[
  {
    "id": 37810144,
    "title": "Flappy Dird: Flappy Bird Implemented in MacOS Finder",
    "originLink": "https://eieio.games/nonsense/game-11-flappy-bird-finder/",
    "originBody": "eieio.games blogwhat's my deal Oct 7, 2023 Flappy Dird: Flappy Bird implemented in MacOS Finder I made a game. It’s called Flappy Dird. It’s Flappy Bird inside MacOS Finder. ad placements start at $2,000 It has instructions, high score tracking, and marquee banner ads. You double-click to start a game and select any file in the window to jump. It runs at 4 frames a second and can’t run much faster. It occasionally drops inputs for reasons that you’ll understand if you finish this blog. I’m going to lay out how Flappy Dird works and how it got there. Head to the github repo if you want to check out the code or play the game yourself. Idea to Prototype The original idea for Flappy Dird came when I noticed that Finder had a “Date Last Opened” field for directories. I knew that the atime (file access time) field was controversial (updating an inode on every file read is expensive!) and wanted to learn how similar date last opened was. I found a few things: The field only updated when opened via Finder; cding didn’t update the timestamp. The field did update if you made a symlink to a directory and then double-clicked that symlink within Finder. The field was accessible (with second-level precision) via mdls This got me pretty excited! I like putting games in weird places, and I realized I could combine those three facts to make a button! The basic idea: Create a directory dir. Inside dir make a directory button that symlinks back to dir. On startup, read the ‘last opened’ timestamp of dir. Repeatedly poll the ‘last opened’ timestamp and do something when it changes. Open button (inside dir) to change the ‘last opened’ timestamp without changing your location in Finder. I brainstormed some ideas for iconic games that could be played with a single button and came up with Flappy Bird1 pretty quickly. And then I started trying to figure out how I’d draw something like Flappy Bird in Finder. I spent a while looking into making Finder’s font monospaced (to make ascii art easier) and measuring out the width of various ascii characters in Finder’s default font before realizing that I had a much better option: emojis have a constant width and if you put them in filenames Finder will display them. emojis in filenames. the future is here. So I had a way to accept clicks and a way to draw to the screen - enough for a prototype! vsync does NOT work for emojis in Finder The basic idea: Set up dir so that it has 15 subdirectories that symlink back to dir Wait until the player double-clicks a directory to start the game. Treat all future double clicks as flaps. Write a function from bird_y_pos,pipe_locations,frame to a 15x15 grid of emojis Every frame, rename every symlink in the directory to a row from our emoji grid. Do some hackery to rename the symlinks in the right order so that we can tell Finder to sort by ‘Date Modified’ This works! But it’s really slow and the screen tears a lot. We can do better. Vsync at home: AppleScript and double buffering The biggest problem with the prototype was that the screen tearing was bad. I figured I’d try to get Finder to “refresh” the file listing in case the tearing was because it wasn’t updating frequently enough. I stumbled upon this Stack Exchange question which pointed me to the AppleScript invocation tell application \"Finder\" to tell front window to update every item. This line helped a little bit (I still saw tearing), but more importantly it planted the seed of using AppleScript. It also delighted me - I find AppleScript totally bizarre. slomobo made this joke on twitter and it feels pretty correct: this is an image; i have no idea how to embed a tweet anymore I shopped the tearing problem around to some smart friends and a bunch suggested that I find a way to do double buffering. The basic idea of double buffering2 is: Have two buffers, buf1 and buf2 On frame 1, display buf1 and start writing your data for frame 2 to buf2. On frame 2, display buf2 all at once! And start writing your data for frame 3 to buf1. Etc. Or something like that. The point is that you avoid the jitter that comes from writing some but not all of the pixels of a new frame to the screen. We came up with several ideas for how to do double buffering. The big ones were: Use symlinks to atomically swap out the directory’s contents. This doesn’t work because you can’t expand the contents of a symlinked dir in Finder (you have to double-click on it, at which point Finder dereferences the symlink and won’t follow renames). Make two directories whose inner symlinks point at each other. This would solve the tearing problem but would mean that we could only advance a frame when the user clicked, which wouldn’t really work for this game. Magically find a way to make Finder display a different directory without changing the “last opened” timestamp. The winning solution came from my friend Jake, who suggested that AppleScript might have a way to control Finder. And it does! tell application \"Finder\" to set target of front Finder window to (\"PATH\" POSIX file) does exactly what you’d think. look at those buffers go I hacked together a double buffering implementation and got the game running smoothly at 1 frame per second! But 1 FPS is slow. We can do better. Tap(pleScript) to Flap(pleScript) The game couldn’t reasonably run above 1 FPS because our input mechanism (double-clicking a file) only had second-level precision - if Flappy Dird ran at 2 FPS you’d only be able to jump every other frame. So I needed a new way to accept input. At this point I figured that AppleScript was all-powerful and could tell me whether an item in Finder was selected. I checked and it totally could! I reworked the code to accept selection of any file in the window as a jump. This worked well and even matched the original game better than double clicking. The game logic became something like: Wait until the user double clicks On every frame, shell out to AppleScript and check whether the user has selected a file in the current window. If they have (or if the “last opened” timestamp has changed), make the bird jump. Shell out to AppleScript to change the directory displayed in Finder. Sleep so that we maintain a constant framerate (e.g. if the above steps took 0.1 seconds and we want to run at 2 FPS, sleep for 0.4 seconds). tap to flap This could…kind of get us to 2 FPS. Except for one problem: AppleScript startup was really really slow. Like 0.2 seconds slow. Which meant: There was no hope of going above 2 FPS, since we had fixed AppleScript costs of 0.4 seconds. When running at 2 FPS there was only a tiny window where you could actually tap a file - if you tapped the file after we shelled out to AppleScript to check whether you had tapped a file we’d miss the tap! So the game wasn’t super playable at 2 FPS. And 2 FPS still felt too slow. We can do better. Rewrite in Rust AppleScript I searched for ways to improve AppleScript’s startup speed. I compiled my AppleScripts (basically useless), entertained ideas like “writing an AppleScript RPC server,” and repeatedly googled “AppleScript improve startup speed” and “AppleScript preload script.” Eventually I posted in the Recurse Center chat and my friend Ian pitched a few suggestions. One of his first suggestions, “can you write the whole game in AppleScript?”, went a little far3 - but he also pitched inverting the control flow between my Python and AppleScript code: I could move my main loop to AppleScript while still shelling out to Python for most of the game logic, allowing me to only pay AppleScript’s startup cost once. I was reluctant to do this because adding any amount of control flow to an AppleScript seemed hard - but I was also pretty excited to get to say “I rewrote it in AppleScript for speed.” Coming up with a way to communicate back from Python to AppleScript was tricky but I landed on something like: do shell script \"game.py await\" set shouldContinue to \"continue\" repeat while shouldContinue = \"continue\" do shell script \"game.py start-frame\" tell application \"Finder\" to set sel to selection set curBuf to do shell script \"game.py tick \" & (number of sel) tell application \"Finder\" to set target of front ¬ Finder window to (\"DIR\" & curBuf as POSIX file) set shouldContinue to do shell script \"game.py sleep\" end repeat That is: Wait for the user to double-click to start the game (await) Record the time at which we’re starting the frame (start-frame) Pass the number of files selected to tick to render the frame. Save the response from tick (which is the name of the directory we just prepared) and navigate to it in Finder. Sleep to achieve our target framerate and emit continue if the player hasn’t lost yet (so that we keep looping). This worked really well! Ian pointed out to me that start-frame and sleep can be easily combined, and I ended up adding another layer of looping to add a way to restart after you die, but this is the basic structure that the game still uses. Adding some flavor The rest of the game was more straightforward - not necessarily easy, but it was just writing Python code to make the emojis on screen look right given some game state. A few notes about that process: To store state during and between runs I made a little state.json file that I read/wrote every frame Adding text was hard because the text I chose was narrower than the emojis I was using. I used a lot of hardcoded spacing to make sure that things lined up correctly. Getting scrolling text across the top was particularly annoying because of the spacing - I ended up writing a function read_n_ad_chars to handle the guesswork around how many characters to show at a given time. I didn’t want to mess with relaying the working directory to AppleScript, so the script has a bunch of template variables that get swapped out by the first-time-setup Python invocation. AppleScript eats anything the python script outputs so I handled logging by appending to a file and catting it afterwards. You jump up an extra row if you tap on two successive frames, which I think makes the game feel a lot better. The hardest bit here was the scrolling banner text. It was particularly tricky because I couldn’t use a debugger since the script was being invoked via AppleScript4. Wrapping up I loved making this. One thing I found particularly delightful was how simple writing the Python for this game was. The prototype was ~90 lines of code (it’s now ~550 lines but like a third of it is boilerplate or constants)! I did it all in vim! No clicking at all! It was great to work without an engine and keep all of my frame state in tiny 2D array. It was easy to keep the whole game in my head from start to finish (even as the control flow got wonkier). The experience made me excited to try making something bigger without an engine. I presented an early iteration of Flappy Dird at Recurse Center and spent a whole lot more time on it because of the response it got during that presentation. Thanks so much to the folks at Recurse for the encouragement, especially since I was presenting 2 weeks after I “never graduated” (Recurse’s word for finishing a batch). If being encouraged to make nonsense like this sounds fun to you you should consider applying! And a special thank you to Kelin for telling me that the banner ads should be pulled by a little plane emoji. 4 frames a second and limited input is a pretty big constraint but I think that it’d be feasible to build some other games this way. Some folks on mastodon suggested building Tetris in Finder and I think that’s hard but feasible. If you want to chat about this please get in touch! I’ve been particularly enjoying cohost for gamedev chatting but I’m all over. I’m currently on vacation (I wrote this on a train from Busan to Seoul) but I’ll be back with some more nonsense in November :) Flappy Bird was probably on my mind because of a reference to it in the excellent game Chants of Sennaar, which I beat last month. ↩ I’m lying a little - there are a few approaches to double buffering / multiple buffering and some require a bunch of copying. Whatever, the idea comes across fine here, don’t worry about it. ↩ Although it was extremely tempting ↩ I bet there’s a clever workaround here. Please tell me if you know of one! ↩ get new posts via twitter, bluesky, mastodon, or rss",
    "commentLink": "https://news.ycombinator.com/item?id=37810144",
    "commentBody": "Flappy Dird: Flappy Bird Implemented in MacOS FinderHacker NewspastloginFlappy Dird: Flappy Bird Implemented in MacOS Finder (eieio.games) 470 points by eieio 15 hours ago| hidepastfavorite51 comments jacurtis 10 hours agoI don&#x27;t have much to say, because I am at a loss for words. I like to think of myself as an \"out of the box\" thinker, but if someone had asked me if this was possible before reading this, I would have said it isn&#x27;t.I absolutely love seeing projects that people put together on a weekend just to challenge themselves and see if something is possible. This is an amazing feat, thoroughly enjoyed the write up.Kudos to the author. reply fingerlocks 6 hours agoparentThis is why I never say anything is impossible in software. It only takes time and creativity reply isometric_8 1 hour agoprevIn a rush to catch up with his team heading out for friday afternoon beers, Bob grabs his jacket and backpack and fails to notice he left Flappy Bird Implemented in MacOS running. It&#x27;s a bank weekend. Bob doesn&#x27;t know it, but his laptop SSD write cycles are being eaten up at a rate of 4 frames per second. Bob returns on Tuesday to find his SSD drive no longer works. reply james_in_the_uk 1 hour agoparentI wonder if anyone who gets Flappy Dird running at work is ever really likely to be &#x27;in a rush&#x27;? reply white_dragon88 48 minutes agoparentprevWell that’s genuinely disturbing reply zoky 10 hours agoprevHave you considered using py-appscript to trigger AppleScript events natively from Python?https:&#x2F;&#x2F;github.com&#x2F;hhas&#x2F;appscript&#x2F;tree&#x2F;master&#x2F;py-appscriptI’ve only ever used the Ruby version of Appscript to be honest, but if the Python version works the same way it would seem to be ideal and would obviate the need for AppleScript entirely. reply eieio 10 hours agoparentOh my goodness I didn’t even think to check if something like this existed! I will certainly check it out (although I do get a bit of delight from the core loop being AppleScript) reply dylan604 10 hours agoprev“this is an image; i have no idea how to embed a tweet anymore”I actually much prefer this. reply amelius 49 minutes agoparentAlso, this will make the post persistent. With an embed, a post can disappear&#x2F;break any time. reply 90-00-09 10 hours agoparentprevAgreed! After blocking Twitter I either see nothing where an embed is supposed to be, or only see emojis rendered in the middle of an otherwise blank area. reply wheybags 10 hours agoprevIf you enjoy this kind of \"games in places they shouldn&#x27;t be\" thing, here&#x27;s some more very silly projects:- Fontemon: a game in a font https:&#x2F;&#x2F;www.coderelay.io&#x2F;fontemon.html- Dungeons & Directories: a text adventure in your file browser https:&#x2F;&#x2F;wheybags.com&#x2F;dungeons_and_directories&#x2F; (disclaimer: I made this one) reply eieio 10 hours agoparentAh thanks for both links! Fun to see that you also chose the filesystem :). Excited to check it out when I am back at a windows machine in a few weeks.I actually played a bit with font games and wrote Hexagone[1] to see what writing a font was like. I didn’t leave the experience that sure what game to build so it’s fun to have some inspiration.[1] http:&#x2F;&#x2F;eieio.games&#x2F;nonsense&#x2F;hexagone-converting-hex-to-rgb-w... reply rahimnathwani 9 hours agoparentprevThe way you did the second is ingenious! Did you consider implementing it with FUSE instead? reply gxqoz 8 hours agoparentprevSee also Defender of the Favicon: http:&#x2F;&#x2F;www.p01.org&#x2F;defender_of_the_favicon&#x2F; reply Geezus_42 7 hours agoparentprevThere&#x27;s also rpg-cli for the command line nerds.https:&#x2F;&#x2F;github.com&#x2F;facundoolano&#x2F;rpg-cli reply mock-possum 7 hours agoparentprevWhat eldritch magic is this… reply Someone 2 hours agoprevThis is a cool hack, but it uses the Finder&#x2F;file system as a graphics system, but isn’t really implemented in it.When I read the title, I expected it to use folder actions (https:&#x2F;&#x2F;developer.apple.com&#x2F;library&#x2F;archive&#x2F;documentation&#x2F;La...) to obtain user input.That, IMHO, would be slightly more “implemented in MacOS Finder”It might be too slow but you could, for example, have the user drag a window around as the game controller, and have the main game loop create a file in a directory to trigger a folder action that runs the main game loop again.Also, can’t you avoid the AppleScript startup problem with a stay-open AppleScript that you send events? reply kccqzy 10 hours agoprevI wonder if OP can solve the AppleScript startup speed problem by rewriting it in… JavaScript? For half a decade now the underlying framework and the command-line tool (osascript) support AppleScript and JavaScript equally well. In JS there are simply a lot of things in the $ object. reply eieio 10 hours agoparentOh I had no idea - thanks! I’ll play around with that.V cool to get two super helpful AppleScript comments in 5 minutes! reply re 6 hours agoprevMy first thought upon reading the title was that it would be done in icon view, using AppleScript to move the icons around within a Finder window like sprites. I was surprised to see it done in list view and (initially) polling the directory \"last opened\" date—definitely a creative approach. reply jedberg 7 hours agoprevI love the \"we can do better\" attitude through the post. Imagine if all developers had this mindset!Of course if we all did, then nothing would ever get done because we could always do better. Sometimes you need someone to say \"this is good enough\". reply eieio 7 hours agoparentHa! You&#x27;re right that it can be a hard balance.I think it&#x27;s easier to stay on the \"we can do better\" track when working on a hobby project like this. At work I&#x27;m typically thinking about whether a marginal day spent polishing something is worth more than all the other stuff that we want to do.But with Flappy Dird I knew that I wanted to push until I was either happy with the project or didn&#x27;t believe that I could push the game any more. reply Cloudef 3 hours agoprevI wonder if you could do this using fuse as well. Have directory read act as a \"jump\". No apple script required and could work outside of OSX as well, as long as the frontend shows the directory contents properly. reply nxobject 10 hours agoprevTFA&#x27;s spirit of bringing little games everywhere they can brings me so much joy. It reminds me of the old Fortune Teller fish taskbar widgets you could get in earlier GNOMEs.(On an entirely unrelated note, the capitalization \"MacOS Finder\" made me think first that this was an extension for MacOS 8 or 9, but this is a lot more delightful...) reply alexwasserman 5 hours agoprevYou can, or could, merge AppleScript and ObjC too. https:&#x2F;&#x2F;developer.apple.com&#x2F;library&#x2F;archive&#x2F;releasenotes&#x2F;Scr...“ Using AppleScriptObjC, your AppleScript code can work with any Cocoa framework, and can function as a first-class citizen in the Cocoa world.”It’s been a long time since I had to, but you could but all your code into a single app, if you could port the python to ObjC. reply eiiot 3 hours agoprevThis reminds me of the excellent \"Bad Apple!!\" series[0] from Junferno, who is an excellent CS video creator. Specifically \"Bad Apple!! played on Windows 10 File Explorer\"[0]: https:&#x2F;&#x2F;www.youtube.com&#x2F;playlist?list=PLsTVaNk5lQHmRy51gyAsV... reply layer8 11 hours agoprevNext challenge: Run Game of Life in Finder emulating a Mac running Finder. reply hoc 9 hours agoprevI&#x27;ve been using AppleScript since its beginnings in combination with all kinds of other scripting tools and system components for all kinds of more or less useful things, and I&#x27;m really happy to it being used like this even 30 years later.My first implementation was using my first Mac&#x27;s speech recognition to implement an interactive chistmas calendar, btw. So this was a nice reminder of that.Thanks for sharing! reply aoms 10 hours agoprevWhat a wonderful work of art! reply dcchambers 10 hours agoprevThese are the kind of weird experiments that keep me coming back to this site. Amazing. reply egoisticalgoat 9 hours agoprevI am absolutely delighted by stuff like this. The fact that this all started by finding out there&#x27;s an atime equivalent in finder is great.Now i wonder, is there any way to react to arrow keys being pressed, or does finder allow you to use wasd to jump to specific files starting with those letters and react to that? That would extend the possibilities of games run in finder, and if you choose a game that only updates when pressing buttons, like rouge, you wouldn&#x27;t even have to worry about the fps limit. reply amelius 1 hour agoprevIs Finder Turing complete? reply Groxx 11 hours agoprevThat&#x27;s... a thing. I am filled with awe. reply Razengan 12 hours agoprev...Everyone&#x27;s just too dumbfounded to comment reply ge96 11 hours agoparentAd placement starts at $2000 reply donkeysquid 7 hours agoprevThe creativity of the human mind knows no bounds - well done reply zacharycohn 5 hours agoprevThis is the dumbest thing and I love it to death. +1000000 internet points . reply GravityLab 7 hours agoprevHahaha this is incredible. Amazing work. How does this affect system resources in terms of cpu and memory usage? reply 90-00-09 10 hours agoprevWhat a wonderful experiment! My favorite type of posts on HN. reply gooderist 9 hours agoprevTodo: DOOM reply hnfong 11 hours agoprevwow> ...Everyone&#x27;s just too dumbfounded to commentSo true. reply 29athrowaway 9 hours agoprevIt is a matter of time until someone does it with the \"Bad apple\" music video. reply egoisticalgoat 9 hours agoparentWith 4 FPS, it would definitely look great reply eieio 6 hours agorootparentfwiw without any interaction I think I could push this to something like 10 FPS! Just can’t do that here since you can’t accept input during the AppleScript Finder calls.(no idea if that’s enough to look ok) reply tambourine_man 10 hours agoprevA labor of love, a crazy idea and well executed. Congratulations reply harryvederci 3 hours agoprevFINALLY! reply ngcazz 11 hours agoprevThis is phenomenal! reply joshuamorton 8 hours agoprevFinder emblem? reply ostenning 11 hours agoprevWow bravo! reply PlunderBunny 11 hours agoprev“I rewrote it in AppleScript for speed.” reply recursive4 5 hours agoprev [–] Creative but ultimately a dog whistle. \"What are the limit of MacOS GUI?\" shouldn&#x27;t be novel. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author has developed a game named \"Flappy Dird,\" a replication of \"Flappy Bird\" that runs within the MacOS Finder interface.",
      "The creation was achieved by manipulating the \"Date Last Opened\" field in Finder, with AppleScript to control the display, running at 4 frames per second, although occasionally dropping inputs.",
      "The development process was discussed and enjoyed by the author, suggesting the possibility of other games built using this approach."
    ],
    "commentSummary": [
      "The project \"Flappy Dird\" is an implementation of the game Flappy Bird within MacOS Finder, showing a unique approach to game development.",
      "The project has garnered positive reactions for its originality and the way it's executed.",
      "This has sparked discussions about potential similar projects and the capabilities of AppleScript and the Finder."
    ],
    "points": 470,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1696770636
  },
  {
    "id": 37813895,
    "title": "Why I can no longer recommend a Mac to fellow blind computer users",
    "originLink": "https://www.applevis.com/blog/we-deserve-better-apple-why-i-can-no-longer-recommend-mac-fellow-blind-computer-users",
    "originBody": "Skip to main content AppleVis Main navigation Apps Forum Blog More Log in Breadcrumb Home Blog We Deserve Better from Apple: Why I Can No Longer Recommend a Mac to Fellow Blind Computer Users By David Goodwin👨🦯, 8 October, 2023 As many of you will know from personal experience, there is a longstanding issue with VoiceOver on Mac where Safari will frequently become unresponsive with VoiceOver repeatedly announcing the message “Safari not responding.” When this issue occurs, the user's Mac may become unusable for up to several minutes at a time. Sometimes it can be resolved by switching away from Safari. Sometimes restarting VoiceOver can resolve the issue. However, far too often, the user is unable to switch away from Safari or turn VoiceOver off, instead having to simply wait for their Mac to become responsive again. This “Safari not responding” behaviour when using VoiceOver dramatically impacts productivity and overall usability of Macs for blind and low vision users. Furthermore, it appears that the issue extends beyond just Safari - many other common applications that utilise Apple's WebKit browser engine can also be affected by the “not responding” problem. It is also important to point out this issue occurs regardless of the specification level of the Mac - it has been widely experienced on the latest Apple silicon-equipped Macs with 16GB or more of RAM, so even owners of top-tier new Mac hardware still face this crippling VoiceOver bug. This critical problem has persisted for years across multiple MacOS versions without a permanent fix from Apple. Given the longevity and level of disruption caused by this bug across Safari and other applications, I can unfortunately no longer in good faith recommend Macs to anyone who relies on using VoiceOver. The impact of this bug when performing routine, and often critical, tasks in Safari and other applications simply makes macOS an unreliable and frustrating platform. It deeply saddens me to feel compelled to say I can no longer recommend Macs to fellow VoiceOver users and that we deserve better. Macs have traditionally been popular within the blind community, and they offer some great accessibility features. I have proudly used a Mac as my primary computer for over a decade. However, frankly, Apple should be utterly ashamed that they have let this issue persist for so long without a permanent fix. It's a failing that raises serious questions about the company's frequently stated commitment to accessibility. There can be no doubt that if sighted users were to experience something similar, that it would receive significant media coverage and an urgent fix from Apple. To be clear - in saying we “deserve better”, I do not mean we are entitled because we are blind or use accessibility features. Rather, we deserve better because we pay the same premium prices for Macs as every other customer. Yet unlike most Mac users, we are forced to accept that multiple times a day, our expensive devices may become useless paperweights for minutes at a time due to the “Safari not responding” issue. We deserve better because we are regular paying customers who have patiently tolerated an unacceptable problem impacting our productivity and independence for far too long. Our disability and use of VoiceOver does not lower the standard we should expect from Apple. If anything, it makes Apple more accountable to deliver an experience equal to what sighted users enjoy. We deserve better simply because we are valued customers of Apple. I want to also note that I am sympathetic to the difficulties Apple's engineering team likely faces in resolving this issue. Based on user reports, there appears to be no consistent way to reproduce the “Safari not responding” behaviour - it can occur randomly and sporadically. The same web page may work fine multiple times before suddenly triggering a freeze. There are also inconsistencies across different users, machines, and configurations. I imagine these factors make it very challenging to isolate and fix the underlying problem. However, given the engineering talent and resources available to Apple, the challenge should not be insurmountable. I believe we need to escalate out urging of Apple to prioritise and permanently solve the “Safari not responding” bug that has plagued VoiceOver users for far too long. To this end, I encourage those of you who use VoiceOver on Mac to directly contact Apple's accessibility team at accessibility@apple.com to share your own personal experiences with and frustrations about the “Safari not responding” issue. It is important we continue putting direct, polite pressure on Apple to prioritise resolving this problem. Please be constructive in expressing your concerns. Consider also copying Apple CEO Tim Cook on your email by using his publicly shared email address tcook@apple.com to ensure he sees the direct impact this ongoing bug is having on Apple's blind and low vision customers. I want to emphasise that the “Safari not responding” bug is far from the only issue effecting VoiceOver users on Mac. As our recent post on problems in macOS Sonoma and the replies outline, there are numerous other VoiceOver frustrations and failures impacting users. However, it has become a yardstick by which Apple's overall performance on and commitment to accessibility is being judged. It is a yardstick against which Apple has failed for some considerable time. I know many of you share my frustration. I welcome your perspectives and discussion in the comments. Collectively, we need to apply consumer pressure by being vocal about this issue and not purchasing or recommending new Macs until the “Safari not responding” issue is fixed once and for all. Apple simply must do better and restore our trust that Macs provide a stable and fully accessible experience for its blind and low vision customers. Tags Advocacy Apple macOS Opinion Options Log in or register to post comments Comments My opinion By Dominic 14 hours 3 min ago The only matter I had the supplier not responded she is on the MacBook Pro 2012. I’ve also had it happen either 2017 MacBook Air, but not with my Apple Silicon MacBook Air. The only reason I use my Mac is for music creation and even Matts becoming a hassle Shared to HackerNews By Devin Prater 13 hours 41 min ago I shared this to Hacker news. If you have an account there, maybe upvote for visibility. https://news.ycombinator.com/item?id=37813895 I agree. By Brad 13 hours 15 min ago I've been a windows user for most of my life and don't plan on switching to a mac at all. I tried it once, never again. This major problum is just not ok By Joshua 13 hours 7 min ago I use windows and never used a mac nor do i want to If i am understanding this correctly vo will say safari not responding and then the keyboard doesnt work, is this correct? If so thats unacceptable Safari Not Responding By Wayne A 12 hours 58 min ago Maybe I have been very lucky, but I have not experienced the same level of frustration with my Mac. I have been using a MacBook Air since 2013, and my current Mac is a 2020 M1 MacBook Air with 8 GB of RAM. While I have occasionally encountered the \"Safari not responding\" issue, I could easily work around this by turning VO off and on. I use my Mac for creating documents, working with spreadsheets, using the Internet, managing my music library, handling emails, among other things. Based on my experience, I would have no problem recommending a Mac to a blind user. Of course, each person must determine their own use case. Over the past ten years, I have encountered numerous bugs but none of them were show stoppers for me. I imagine that Windows with JAWS or NVDA also has its share of bugs. Given that this bug seems to not affect all Mac users with the same severity, this may make it a more difficult issue for Apple to address. However, given the comments that I have read on Applevis, I agree that this should be a priority for their Accessibility team. Upgrade to latest Mac Mini on hold By PaulMartz 12 hours 53 min ago I was looking forward to upgrading my 2018 Mac Mini - just waiting for the M3 to be released. But that's now tabled. I stand in solidarity with David. No more new Apple hardware until this long-standing and unacceptable issue is resolved. My experience with jaws By Joshua 12 hours 28 min ago Hi I used jaws for 5 years and dont remember any major bugs with it I am having a problem with the computer i use at work where jaws sometimes crashes, HP laptop running windows 10 and jaws 2021 but that doesnt happen offten For those of you not experiencing this issue By PaulMartz 12 hours 10 min ago I can't speak for David, but I personally feel this post will have more impact if those of you not experiencing this issue please discuss your system config in any of the numerous posts devoted to trying to find a solution. And I'll warn you in advance, lots of suggestions have been made over the years, and there is no silver bullet. Most of us have been experiencing this issue for years. Please don't pretend this issue doesn't exist. Curious About Safari on Big Sir Vs Now By Scott Duck 11 hours 39 min ago I am still running Big Sir on my 2015 13 inch MBP (with Core I7, 16 gig RAM, and one terabyte SSD). I do not have a lot of trouble with the not responding bug with Safari, enough to be aggravating at times but not enough to significantly effect productivity, and I almost never encounter it with other applications. Overall, I do have some issues with Safari but no more than I had using Internet Explorer under Windows 8 with JAWS 16, before I switched to the Mac in 2016. For me, using the Mac, there are not more internet browsing issues, just different issues, and there will always be issues of some sort. With Safari, there are some aggravations but it is certainly very usable. Again, this is using Big Sir. I am genuinely curious as to whether things have gone down hill significantly with Safari sense Big Sir. I can update my MBP to Monterey but I have stayed with Big Sir in order to remain on the same OS version as my wife, who has a 2014 MBP and cannot update past Big Sir. I had planned, in about a year or so, to purchase a new MacBook pro but I find posts like this one making me wonder if that is wise. If it turns out that Mac OS no longer meets the needs of my use case, then I will not purchase another mac, for myself, until that is resolved. I do have to remain productive. However, I don't plan to boycott Apple, in order to make a statement. I don't think we make up a large enough portion of their user base for that to work. I do not know the statistics but I suspect that Voice Over users probably make up less than one percent of Apple's user base. We are in the overwhelmingly vast minority here and, as such, we have little leverage with which to economically influence them. If every Voice Over user stopped purchasing Apple products tomorrow, I suspect that it would have no meaningful impact on Apple's bottom line. I think making them aware of the issues, respectfully but persistently, is all we can really do. Safari not responding> By Tayo 9 hours 50 min ago I've had this issue, although it hasn't been as crippling as those I've seen posted here. While VoiceOver may say Safar not responding, for example, I find that I can still type my search, which is where the bug usually show up, and press enter as if nothing is happening. Usually, VoiceOver will read the webpage as if nothing happened. My guess is that this is specifically a VoiceOver bug. I'm using a 2016 Mb Pro with 8 gb of RAM and a 256 SSD. I've had this issue By Brooke 9 hours 16 min ago It's the main reason my MacBook Air has been in its case in the drawer underneath my entertainment center for the past several years. I'd been a Windows user for ages, and when I switched to the Mac, I was struggling in general. The Safari not responding bug tried my patience to the point where I gave up and went back to Windows. Any bug tracker available? By jprykiel 1 hour 2 min ago Hi. I’m one of the lucky ones who’ve had this issue rarely and that didn’t impact my workflow that much. I am located in Frantz with a fast Internet connection. Using Ventura on a 2015 MacBook Pro. I’ve seen some bug trackers Software which were specifically designed for windows applications, and could track What was happening on a computer. Would this kind of application help? I'm also one of the lucky ones By David Standen 34 min 19 sec ago Since upgrading to Synoma, the \"Safari not responding\" bug has completely disappeared for me. The worst occurrence of this bug for me was for the last Ventura update. I am using an M2 2022 Macbook pro with 1 TB SSD and 8 GB Ram. I am in Australia, and am connected to the NBN. I feel very sorry for those of you who are very much impacted by this issue. This has not ben my experience and I will continue to support the Mac. Re: Shared to HackerNews By David Goodwin👨🦯 19 min 55 sec ago Hey Devin, Thank you for sharing my blog to Hacker News. It seems to have attracted quite a bit of interest over there and generated a lot of traffic back to the AppleVis post, which is great. I've been following the Hacker News discussion thread and it's been great seeing so many insightful comments. I encourage others reading this to take a few moments to check out the discussion for themselves. Here's the link https://news.ycombinator.com/item?id=37813895 Re: Curious About Safari on Big Sir Vs Now By Brian 15 min 16 sec ago Disclaimer: Stick to Big Sur! While I am sure I will get flamed for stating this, the Safari Not Responding issue is resolvable with Big Sur. I have given both Monterey and Ventura a run for their combined monies, and I can safely say, \"Both are garbage\". With Big Sur you can adjust a number of VoiceOver settings from within the VO Utility (VO + F8) which will pretty much prevent the bug from happening. I made a post about month ago regarding this, have a read: https://applevis.com/forum/macos-mac-apps/edit-safari-busy-bug-practical-simple-work-around HTH. 😃 More Like This Send all your frustration courteously to Apple Accessibility. (Forum Topic) Duel Voiceover Interference (Forum Topic) A bug in Voice Over? (Forum Topic) please join me in contacting Apple and Spotify regarding Spotify no longer reading descriptions of playlists (Forum Topic) Webkit doesn't check for updates on Mac os X 10.11.1 (Forum Topic) An apology to the moderators here (Forum Topic) BaconReader for Reddit (iOS and iPadOS App Directory) VoiceOver and bluetooth keyboard on iOS? (Forum Topic) Site Information About Club AppleVis FAQ Contact Unless stated otherwise, all content is copyright AppleVis. All rights reserved. © 2023AccessibilityTermsPrivacy",
    "commentLink": "https://news.ycombinator.com/item?id=37813895",
    "commentBody": "Why I can no longer recommend a Mac to fellow blind computer usersHacker NewspastloginWhy I can no longer recommend a Mac to fellow blind computer users (applevis.com) 344 points by devinprater 14 hours ago| hidepastfavorite70 comments mltony 10 hours agoBlind developer here. I could never recommend VoiceOver on MacOS to anyone. 10 years ago before I lost my eyesight I was a huge fan of Apple - everything was just working so much smoother than in Windows world. And much more visually appealing. However, after losing eyesight, I had to switch to Windows. I learned VoiceOver on Mac at first, but working with it was so unbearably frustrating,, even back in 2016 when I was trying. Here are some of my complaints that I still remember:* Many actions work every other time. I remember that interacting with text area of terminal app was especially painful since the sequence of commands was non-deterministic.* Hierarchical navigation model is more cumbersome than flat navigation on Windows. In XCode to access some project settings you need to interact with some panel, which has two horizontal subpanels, so you need to interact with the right one, which in turn has two more vertical subpanels, you interact with the bottom one, which has three more subpanels... The recursion depth was 9 levels, I kid you not; and making a single mistake will lead you to wrong place.* Searching webpage in a foreign language doesn&#x27;t work. Because Command+F needs to be presssed in English layout and this would open VoiceOver search window, where switching to another layout doesn&#x27;t work.* No easy way to open a link in another tab in browser - as opposed to Control+Enter on Windows.* Too difficult keystrokes - I remember one of keystrokes involved 5 keys: Fn+Ctrl+Option+Command+Up&#x2F;Down. By now I forgot what it means, but I remember that it gave me plumber&#x27;s disease - pain in my left wrist from having to press too many keys for extended periods of time.I probably forgot a bunch more. Not sure if any of these have been fixed since then. But my general impression was that Apple is not very interested in fixing bugs, but instead, they appear to care a lot about presenting shiny keynote slides every year on WWDC claiming how much they care about accessibility.In Windows world things are considerably better. Jaws is much more convenient to use, even though I&#x27;ve heard many reports of them not willing to fix bugs. NVDA is open source and it is my favorite, since if something doesn&#x27;t work for me - I just go and fix it myself, but in general things are rarely broken to that extent in NVDA.Also if I remember correctly Jaws and NVDA share about 45% of marketshare in the screenreader world, while VoiceOver is about 10%. So judging from this point alone anyone would be much better learning a Windows screenreader. reply tomcam 1 hour agoparent> I remember one of keystrokes involved 5 keys: Fn+Ctrl+Option+Command+Up&#x2F;Down.What the actual fuck. This is ridiculous if you are blind and otherwise abled. Now imagine what it’s like if you have incomplete control over your fingers, or tremors, etc. At that point it’s simply impossible. Apple, with all those billions in the bank, maybe you could spend a few million getting these problems right. reply mouzogu 2 hours agoparentprev> But my general impression was that Apple is not very interested in fixing bugs, but instead, they appear to care a lot about presenting shiny keynote slides every year on WWDC claiming how much they care about accessibility.yes, you could say the same about their sustainability &#x2F;environment&#x2F; (and arguably &#x2F;privacy&#x2F;). it&#x27;s more about marketing and owning the narrative than it is about actual substance imo. reply Dah00n 1 hour agoparentprevThank you for a very insightful comment. reply mycall 9 hours agoparentprevWhat is your opinion of iOS VoiceOver and its&#x27; rotor? reply miki123211 12 hours agoprevI have been a regular Mac user for over two years (MacBook Air M1) and the issue is far less severe than this person claims.It used to be a much bigger problem around Mac OS 12, but even then, Cmd + option + q (quit and keep windows) would solve it pretty quickly.Voice Over has a lot of issues, but so do Windows screen readers, and as a Mac user, I can at least decide to postpone an accessibility-breaking update, something which Windows doesn&#x27;t let me do. Such an update was one of the main reasons why I switched OSes. reply devinprater 12 hours agoparentI&#x27;m glad you don&#x27;t experience this anymore. I wish I could say the same. Of course, I have a 2019 MBP so no M1, and I&#x27;m not rich enough to just go out there and buy it when I hear it effects even M2 models. I don&#x27;t bet an issues that hault productivity that much. reply kccqzy 4 hours agoparentprevIt&#x27;s also possible to switch the default so that Cmd + q will quit and keep windows, while Cmd + option + q will quit and discard windows. I like it this way much better. I have it set up for just Safari however. reply Daveman90 1 hour agoprevThere is a well known bug in the German TTS adding the Word „Homograph“ before and after each number for about a half year right now. It’s a shame for everyone who depends on that technology. reply the_mitsuhiko 11 hours agoprevNot blind, I have no experience at all with any of this. However what I found quite impressive was witnessing an Apple Store employee guide a blind person through voice over. That to me implies that Apple at least trains some folks in the stores on accessibility features. reply devinprater 11 hours agoparentYep. Sometimes they even hire blind people to do it. There was one at the Apple Store I went to when I first was looking into a Mac. Years later, I heard he left though, moving into another field. reply chrismorgan 13 hours agoprevI was expecting some sort of systemic problem across the operating system, but it seems to be just one specific bug in WebKit—though there is also a link to “our recent post on problems in macOS Sonoma and the replies”, and though it does sound to be a rather debilitating bug for affected users, and though this sort of bug making it to production can be a symptom of systemic problems in the organisation. reply tomxor 7 hours agoparentThey mention another post covering broader issues, just highlighting this as the worst:> I want to emphasise that the “Safari not responding” bug is far from the only issue effecting VoiceOver users on Mac. As our recent post on problems in macOS Sonoma and the replies outline, there are numerous other VoiceOver frustrations and failures impacting users.As a web developer who is committed to supporting blind and disabled users in general, I&#x27;m just getting deep into all of this now, and so far I can say that VO is the worst of the three screen readers for web usage. It has fallen far behind on standards support for modern aria attributes, completely ignoring many of them in practice, to the point where it&#x27;s not possible to always achieve intended screen reader behaviour in VO.Much as with Safari, Apple only seems to pays lip service to support of VO, completely blind users seem to only ever use VO if given no other option, but many online stats muddy OS usage with web usage to make this less obvious. reply lelanthran 1 hour agoparentprevA single bug that stops you using the internet at all is far worse than hundreds of bugs that simply inconvenience you. reply callalex 9 hours agoparentprevWould you tolerate a machine that just stops working for several minutes randomly all day while screaming at you? reply yosito 8 hours agorootparentThis perfectly describes my typical experience using the internet in 2023. reply BlarfMcFlarf 8 hours agorootparentprevThat’s what ads are. reply str3wer 7 hours agorootparentprevaren&#x27;t we already doing that everyday? reply McGlockenshire 8 hours agorootparentprevI think you just described compiler output. reply PaulHoule 12 hours agoparentprevOne fault can leave a platform 100% broken. reply x0x0 6 hours agoparentprevThis broadly seems -- for a blind user -- like the monitor periodically taking a break for a sighted user. Seems like it would get real old by, like, the 3rd time or so. reply thathndude 10 hours agoprevThis is an interesting take, and a very salacious headline.I’m a lawyer who represent a number of disabled individuals (including the blind). My clients with vision impairments as all in on Apple products, saying that the accessibility features are head and shoulders above other options. reply hollerith 9 hours agoparentI switched from MacOS to Linux&#x2F;Wayland after 10 years on MacOS because it accommodated my vision impairment better.Because I have cataracts, it helps to make everything on the screen bigger. MacOS can do it, but the result is blurry unless the scaling factor is exactly 1 or exactly 2, and choosing 2 reduces the horizontal resolution too much for some web sites. (My monitor is 1080p, so with a scaling factor of 2, the viewport is only 960 pixels wide.)In contrast, Linux&#x2F;Wayland offers scaling factors 1.0, 1.25, 1.5, 1.75, 2.0 and 2.25. (I&#x27;ve been using 1.75 for months.) Windows works similarly to Linux&#x2F;Wayland (though apps that have not been updated to work with recent versions of the OS end up blurry), so MacOS is definitely a laggard here. reply prewett 8 hours agorootparentMacOS 10 used to let you specify scaled resolution in Control Panel >> Displays >> Resolution (scaled). It doesn&#x27;t give you numbers, though, more like \"larger text\" and \"more space\", with \"effective resolution\" under the image. The effect is a scaling factor, though, you just can&#x27;t see the actual value. I expect macOS 11+ also continues to do this. reply hollerith 7 hours agorootparentSure, I know. That is the blurry option. There is a free utility that causes choices with \"(HiDPI)\" at the end of them to show up in the list of possible resolutions even when using a non-HiDPI monitor, and that is how you get a non-blurry scaling factor of 2.0 on a Mac.(Also, it hasn&#x27;t been called Control Panel for 20 years.) reply oefrha 8 hours agorootparentprev> but the result is blurry unless the scaling factor is exactly 1 or exactly 2I can’t relate. I’ve been using a scaling factor of 1.5 for about a decade now (4K monitor scaled to 2560x1440) and it’s never blurry. reply AdamN 1 hour agorootparentMac really only works in the ~110 or ~220 ppi sweet spots which most people don&#x27;t realize. The only monitors I&#x27;ve seen that fully work are the LG UltraFines and the Mac displays. I&#x27;m on a ThinkVision display now and it&#x27;s middling. reply hollerith 7 hours agorootparentprevIt is not as sharp as it could be, and someone on this site once commented that he notices the difference (between Mac and Windows, where the software doesn&#x27;t introduce blurriness) even on his 4K monitor.(You could ask me why if I hate blurriness so much, why am I still using a 1080p monitor. My answer is my high-DPI monitor is coming soon.) reply oefrha 6 hours agorootparentSounds more like personal preference than something objective. Windows’ 1.5 scaling on the same monitor looks pretty bad to me (font hinting in particular, legibility isn’t great). reply devinprater 6 hours agoparentprevOh we love our iPhones. But then as a computer OS, we get Windows. Some people try to stick with Apple, either because of Logic Pro or the M1 chip, but I&#x27;d say a good 85% of blind people that even have a computer in the first place, have Windows. reply RobMurray 29 minutes agoparentprevThis is definitely true in the case of phones, but not for computers. NVDA on Windows is excellent and improving all the time. reply Dah00n 1 hour agoparentprevI have worked with some people in the local blind community in my city here in Denmark. They all use Windows even though I see lots of iPhones. reply spacedcowboy 13 hours agoprevI am curious what he&#x27;s going to recommend instead. AFAIK, Macs are the best out there (by quite some way) for accessibility. reply shados 12 hours agoparentWindows with JAWS or NVDA is fairly mainstream, and it&#x27;s accessibility features are pretty well understood and supported. VoiceOver less so. That only covers one facet of accessibility of course, but they have like 80 percent market share in that segment. reply lelanthran 1 hour agoparentprev> AFAIK, Macs are the best out there (by quite some way) for accessibility.Well, now you know different :-) reply eddythompson80 12 hours agoparentprevLast time I saw a poll of blind CS&#x2F;engineering students across NA, the wide majority preferred JAWS as a screen reader. That implies Windows. reply bryanrasmussen 34 minutes agorootparentnot someone who needs to use a screen reader but whenever I&#x27;ve tried them my experience is that JAWS and NVDA are infuriating, the learning curve on VO is much more forgiving and I can get done what I want in short order.JAWS is often used in Universities in the U.S having a lock on the market - perhaps this is why the people polled prefer it? reply ChrisMarshallNY 8 hours agorootparentprevJAWS is very well-known, and well-supported. Web sites are often tested against it.There’s something to be said about having a well-known system, as it cultivates a great deal of “tribal knowledge,” and that can be invaluable.That said, I think that we train ourselves on whatever platform we use. Both Windows and Mac have fairly comprehensive support for Accessibility. I think Apple is newer to the game, but the granularity of the options in the latest OSes is pretty crazy. Not just for blind folks; but for all kinds of disabilities. I’m confident that Apple takes accessibility very seriously.I like to leverage accessibility and localization in my programming. reply makeitdouble 5 hours agorootparent＞ I&#x27;m confident that Apple takes accessibility very seriouslyI think Apple gets too much of a pass for being serious and commited, where other players have actual results.On mild level accessibility, the part that surprised me the most was keyboard mapping. While macos got caps lock&#x2F;esc&#x2F;ctl remapping as touted out improvement, there is no blessed way to remap the rest of the keys.That means third party deamons like Karabiner are the only straight option, and since it stopped being a kernel extension it&#x27;s also out of the critical loops and will lag under load. In the worst moment I see keystrokes getting processed with enough lag to miss the combined triggers or go to different applications.Windows has more options out of the box (e.g. 106 key layouts IME languages etc.), Powertoys is blessed and efficient, and AutoHotKey works well even under stress.I&#x27;m still hoping for more improvement on macos land, but I wish there was less talk and more work on Apple&#x27;s part. reply bryanrasmussen 28 minutes agorootparentmy understanding of this is that Windows definitely makes it easier to write keylogging software etc. which of course also turns into making it easy to do all sorts of other keyboard things like hotkeys etc. (or perhaps I should say Windows makes it easy to write all sorts of hotkey, keyboard manipulating tools which means writing bad things like keyloggers etc. is also super easy)This (again my limited understanding) is why AutoHotKey has not been successfully ported to Mac, because it is either not technically possible to achieve or time required would make it not worthwhile. reply eertami 1 hour agorootparentprevI haven&#x27;t used OSX in a while, but in the past I created my own keymap using Ukelele. Would this not still be possible? reply makeitdouble 39 minutes agorootparentUkele seems to be about character output, and doesn&#x27;t interfere with the actual physical key behaviors or modifiers (that&#x27;s what I get from the site and the interface when running it).For instance I need the Left and Right Command keys to each switch on&#x2F;off the IME when single pressed, and act as Command when combined with other keys. Same way the Fn key doesn&#x27;t seem to be mappable.Seems to work at the same level as the Powertoys remapper on windows, minus the shortcuts. reply eproxus 1 hour agorootparentprevI still use Ukulele and a custom Swedish Dvorak keyboard layout without any problems (in fact, the layout I created 10+ years ago still works out of the box, on every release of macOS). reply natch 10 hours agorootparentprevImplies Windows but without information about why Windows was chosen. Sometimes people have to use whatever computer is available.So I take the question more as: given whatever computer platform you are stuck on, what screen reader do you prefer?Feel free to share the poll though, maybe it will clarify. reply zdragnar 9 hours agorootparentN=1, but the only blind engineer I&#x27;ve worked with preferred Windows, I think for JAWS. The company was a little behind the curve on hardware, but was fairly progressive in a lot of ways, and made accommodations without question, so I know he could have gotten a Mac if he&#x27;d wanted one.He even gave a demo to the team at one point of what it was like to navigate editors, screens and browsers as a blind person, starting with the screen reader at a normal speed and ramping it up to what he was accustomed to.I&#x27;m a little sad I didn&#x27;t get to work with him directly outside of a single time he helped me test some custom UI elements, so I don&#x27;t know much more about his take on voiceover versus jaws, but this was also over a decade ago. I&#x27;m sure things have changed more than a little since then. reply amf12 8 hours agoparentprevMicrosoft actually takes accessibility quite seriously. reply devbent 5 hours agorootparentBack when I was at MS I heard of accessibility PMs walking into VP offices and telling them that their newest version wasn&#x27;t shipping until accessibility was 100% done.I don&#x27;t know if it the same now, but when I was there certain aspects of the customer experience were held sacred. reply devinprater 12 hours agoparentprevMost of us use Windows. reply oooyay 7 hours agoprevNon-sequitur to the article, but are there any good utilities for blind users to use on Linux? reply avtar 5 hours agoparentOrca is one of the more prominent screen readers on Linux:https:&#x2F;&#x2F;help.gnome.org&#x2F;users&#x2F;orca&#x2F;stable&#x2F;index.html.enI used it briefly years ago for some tests but in general I&#x27;m guessing NVDA and JAWS on Windows are probably better options. reply rock_artist 12 hours agoprevFrom developer POV this is real frustrating. This seems like something that makes bad experience but at least in the post there are no reproducible steps. Also when a bug shows only sometimes, if it’s rare enough, sometimes it’s really hard to nail down. I guess Apple’s metrics are superior to mine. Yet, sometimes just catching an issue is tough.The other complication with Apple is the fact that sometimes it might be needed for multiple teams to fix an issue (eg. WebKit, Safari and VoiceOver&#x2F;Accessibility) so they all have their own apple ways which might not be trivial as a single team fixing an issue.And least, yeah, it might just be prioritization as there are some bugs that are with us for years or even regressions between each OS iteration that stays forever. reply MegaDeKay 6 hours agoparentThat \"other complication\" should actually be in their favor. Getting an issue fixed can only be more difficult when the various working parts are the responsibility of completely different companies. reply tmpX7dMeXU 11 hours agoprevI certainly don’t want to discount from the point this article is making, but as someone with a significant visual impairment, but that is not completely blind, I legitimately can’t imagine using anything that’s not macOS these days. This is coming from someone that grew up using Windows, and had an extensive Linux phase…including Gentoo.Screen readers are a bit of a lightning rod for accessible technology interest, almost entirely because most people have some sort of sick curiosity. “How can someone use a computer so differently to the way that I do!?”. Of course, most of these people stop here, never bothering to try using a screen reader to navigate whatever they’re making. They might open VoiceOver, realise they don’t know how to intuitively use it, and fumble around with trying to close it again. This tends to have the effect of sucking any motivation out of the room. Most people won’t then go and meaningfully improve their screen reader experience, but they also won’t think to address any other accessibility shortcomings, especially visual ones, because “blind people use screen readers!” Is the pervasive meme.This is part of why I am all in all so happy with how Apple has been going in this space lately. An obvious result of co-design &#x2F; consultation, or dare I say it…hiring people with disabilities. Addressing accessibility concerns that the stereotypical SV techie has never even heard of. reply gerdesj 10 hours agoparentOP is describing a very long standing bug which they describe as: \"This “Safari not responding” behaviour when using VoiceOver dramatically impacts ...\"OP seems to be happy with the functionality of VoiceOver on Mac but not with the stability of VoiceOver on Mac.You don&#x27;t mention what actually works for you (on your Mac - you do mention that), only that you disagree with the article. reply Dah00n 1 hour agoparentprevSo what do you use that is better? reply devinprater 12 hours agoprevI tried using a mac (2019 MBP, 8 GB RAM, 4 Thunderbolt ports) at work. I could create course material in Ulysses. I could manage files and folders with Finder. I could even slowly navigate through Moodle&#x27;s stupid web interface. But then I had to start using Google Docs, and a Salesforce instance. Google Docs works reasonably well on Windows. I even gave a good enough of a bug report for the orca maintainer to make it more responsive on Linux with chrome. But Mac? I can&#x27;t even begin to describe how messy it is. VoiceOver goes in and out of the document, the VoiceOver cursor gets stuck on menu items, that kind of messed up, where even if I want to download the document for offline usage, it&#x27;s just... Frustrating enough for me to throw away the MacBook, not (entirely) literally, and grab the windows machine again.Salesforce is slightly better except when you get to a table of record entries, that table is empty. Simply, nothing there. Luckily, I can use firefox to get around this. Or I can simply go back to windows and not have any of these problems. The web is snappy again, there is no freezing up or not responding or busy messages, nothing. It, amazingly for Windows, just works. And that&#x27;s a shame for Apple. reply alib 12 hours agoprevI’m very sorry to hear about your experience. That really sucks. The fact we have a thriving browser ecosystem has got to count for something though. Could you recommend people use Firefox instead?I have come to respect Apple in recent years for their focus on and promotion of accessibility within the industry. They are leading where others have often abrogated the responsibility to build inclusive products. We should celebrate this.Therefore this headline made me sad. I appreciate sometimes drastic measures are required to persuade those with power to use it to change things, and it does sound like this bug has been around for too long. If Apple are serious about accessibility they need to fix it.I have read that the relentless release cycle at Apple does lead to bugs like this never getting fixed. I hope this post gets upvoted enough that someone with influence sees it, and ensures that this one is different. Good luck! reply Dah00n 1 hour agoparent>They are leading where others have often abrogated the responsibility to build inclusive products. We should celebrate this.What \"others\" are there except Microsoft Windows and Linux - both also having plenty users here saying that they see those OSes as ahead of Apple? Seems to me that at best Apple is on par, not someone to celebrate above others. reply devinprater 14 hours agoprevnext [2 more] Blog post from Applevis Subtitle: Why I Can No Longer Recommend a Mac to Fellow Blind Computer UsersI don&#x27;t mean to editorialize, the title was just too long for the title box. reply dang 13 hours agoparentYup, the limit is 80 chars so it needs to be chopped. In this case the suffix is more informative than the prefix so I&#x27;ve swapped out \"We deserve better from Apple\" above. Thanks! reply fatih-erikli 13 hours agoprev [11 more] [flagged] jitl 12 hours agoparentThere’s so much wrong with your post:- Blind users and other disabled people are often professionals.- Apple is very much a personal computer company. They make iMovie, Photos, Apple TV, Pages, etc for personal end users. If you’ve used some of Apple’s pro apps like Final Cut or Logic, you’d know they can design for pro users - but macOS as a whole is the opposite, getting more simplistic over time as they cater more to consumers. The Mac is a good pro computer IN SPITE of it being designed for consumers.- many people in the blind community are dishonest? Wtf. This just sounds like discrimination to me. reply upon_drumhead 12 hours agoparentprevI&#x27;m confused as to your statement. So you&#x27;re saying blind computer users shouldn&#x27;t be recommending what they feel works for them to fellow blind computer users? Only people who are getting paid to review should recommend experience?Also, are you saying that blind computer users can&#x27;t use a wide assortment of computers? I have a buddy who is blind and is a developer and they happen to have a top end MacBook Pro and uses it quite well. I don&#x27;t think that calling them a -low- user makes any sense at all. reply Almondsetat 13 hours agoparentprev [–] Apple is the best brand accessibility wise, unless we are talking about some insignificant niche products reply pxc 11 hours agorootparentAccessibility isn&#x27;t a homogenous, fungible quantity. Calling something &#x27;the best brand accessibility wise&#x27; is barely even meaningful, let alone useful.Accessibility is a qualitative issue, and disabled people&#x27;s needs are varied enough that many accessibility tools are only helpful or usable to a subset of them. reply Almondsetat 4 hours agorootparentPlease take a stroll through Apple&#x27;s accessibility options and you will see the sheer quantity and variety of them tackling many disabilities. If you actually have any Apple devices, that is reply Dah00n 1 hour agorootparentSo the blind users commenting here saying Apple is not the best are, what, lying? Or maybe you are not the target for those features and could instead listen to both sides of this argument and stay on the sidelines? Apple is not the clear winner (if a winner at all) in the eyes of the people commenting here that use the features discussed. Several are pro Apple but at least as many write long comments about how Linux and Windows are better for them.I think we should listen to these commenters and not just tell others to go away and look at a list of features, with no background to tell if it is good or bad. reply fatih-erikli 13 hours agorootparentprev [–] This is correct. Apple is first when it comes to accessibility. However accessibility isn&#x27;t invented for the blind. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Accessibility advocate David Goodwin is dissatisfied with Macs due to an unresolved issue with the VoiceOver feature in Safari, particularly for blind users.",
      "Goodwin criticizes Apple's lack of action to fix this bug and appeals for enhanced communication with Apple's accessibility team.",
      "He encourages a group response to voice dissatisfaction and to avoid buying or endorsing Macs until the issue is fixed, highlighting mixed experiences among users with some expressing frustrations and others reporting no significant problems."
    ],
    "commentSummary": [
      "The conversation focuses on the accessibility features for visually impaired users across different operating systems, with varied user preference.",
      "Some users commend Apple's accessibility efforts, but others report issues with the VoiceOver feature on MacOS, causing them to switch to alternatives such as Windows and Linux.",
      "This debate underscores the subjectivity of accessibility and emphasizes the need for Apple to address bugs and enhance the stability of its VoiceOver feature."
    ],
    "points": 344,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1696795160
  },
  {
    "id": 37812217,
    "title": "OpenIPC: Alternative open firmware for your IP camera",
    "originLink": "https://github.com/OpenIPC",
    "originBody": "Skip to content Product Solutions Open Source Pricing Search or jump to... Sign in Sign up OpenIPC Alternative IP Camera firmware from an open community 524 followers https://openipc.org @openipc @openipc https://opencollective.com/openipc dev@openipc.org Verified Overview Repositories 69 Projects 1 Packages People 10 README.md Alternative open firmware for your IP camera (based on Buildroot) OpenIPC is an open source operating system from the open community targeting for IP cameras with ARM and MIPS processors from several manufacturers in order to replace that closed, opaque, insecure, often abandoned and unsupported firmware pre-installed by a vendor. OpenIPC firmware comes as binary pre-compiled files for easy installation by end-user. Also, we provide full access to the source files for further development and improvement by any capable programmer willing to contribute to the project. OpenIPC source code is released under one of the most simple open source license agreements, MIT License, giving users express permission to reuse code for any purpose, even as part of a proprietary software. We only ask you politely to contribute your improvements back to us. We would be grateful for any feedback and suggestions. Historically, OpenIPC firmware only supported SoC manufactured by HiSilicon, but as the development continues, the list of supported processors expands. Today, it also includes chips from Ambarella, Anyka, Fullhan, Goke, GrainMedia, Ingenic, MStar, Novatek, SigmaStar, XiongMai, and is expected to grow further. More information about the project is available in our website and on the wiki. Pinned firmware Public Alternative IP Camera firmware from an open community C 716 143 ipctool Public Simple tool (and library) for checking IP camera hardware C 100 27 coupler Public Seamless transition between video cameras firmware 50 16 microbe-web Public Microbe Web UI is a default web interface for OpenIPC firmware. Shell 23 29 smolrtsp Public A lightweight real-time streaming library for IP cameras C 112 25 mini Public OpenSource Mini IP camera streamer C 75 49 Repositories Type Language Sort Showing 10 of 69 repositories atbm_60xx Public AltoBeam atbm WiFi driver C 2 MIT 4 0 1 Updated firmware Public Alternative IP Camera firmware from an open community C 716 MIT 143 90 (3 issues need help) 2 Updated wiki Public The new OpenIPC wiki 92 GPL-3.0 87 0 0 Updated website Public New website of the OpenIPC project HTML 4 GPL-3.0 5 3 0 Updated silicon_research Public GK7205V300 some examples C 12 MIT 7 0 1 Updated u-boot-sigmastar Public U-Boot for Infinity6xx SoC's C 3 1 0 0 Updated realtek-wlan Public Realtek WLAN drivers C 2 1 0 0 Updated microbe-web Public Microbe Web UI is a default web interface for OpenIPC firmware. Shell 23 MIT 29 15 (1 issue needs help) 1 Updated sandbox-fpv Public Sandbox for FPV experiments C 20 MIT 7 0 0 Updated device-mjsxj02hl Public OpenIPC for Xiaomi MJSXJ02HL Shell 37 MIT 9 9 0 Updated View all repositories People Top languages C Shell C++ Python JavaScript Most used topics openipc hisilicon u-boot u-boot-openipc ipcam Developer Program Member Footer © 2023 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
    "commentLink": "https://news.ycombinator.com/item?id=37812217",
    "commentBody": "OpenIPC: Alternative open firmware for your IP cameraHacker NewspastloginOpenIPC: Alternative open firmware for your IP camera (github.com/openipc) 323 points by danboarder 17 hours ago| hidepastfavorite53 comments bastard_op 13 hours agoHaving looked into this myself prior was mostly asking for disappointment. Getting a random Chinese IP camera to convert to open firmware is great in theory, but the problem is actually buying a camera or 10, or buying more over time.You never really know what hardware ships internally at any given time, ie what SoC a camera really is. They change frequently in random Chinese manufacturers with internal SoC hardware update with constant improvement (in their margins at least), if reported by someone at a point in time, buying the same model a 6mo-1yr later you cannot expect the same model number to be the same hardware.It is rarely stated on a site selling them what SoC they use, or the vendors sites, assuming you can find and&#x2F;or translate anything usable, or if the vendor is even still around more than a few months under that name. Therefore there is no repeatable source of products you can get reliably across time without simply testing a sample and hacking your own as you go.I really wish someone would take initiative to sell&#x2F;resell a line of camera hardware with openipc or other open firmware on them. It&#x27;s a business opportunity sorely missing with nothing to offer home&#x2F;business users ip cameras with open sterile and guaranteed secure firmware vs. random pure Chinese jank. reply squarefoot 12 hours agoparentWhen in doubt, I take for granted that every camera with closed firmware contains malware that phones home or waits dormant to be used to steal sensitive video+audio or to take part in a botnet, which already happened several times. The solution as of today is to put a firewall in between that blocks any traffic to the LAN and from the outside, except from trusted applications. There are many multi ETH port mini PCs that can be repurposed as firewalls so that the video surveillance subnet can be physically separated from the rest by connecting all cameras (for obvious reasons I wouldn&#x27;t take WiFi into consideration) to a switch, possibly with PoE support. reply wtf_is_this 11 hours agoparentprevCouldn&#x27;t agree more with this comment. I&#x27;ve had exactly the same frustration trying to get cameras months&#x2F;years apart that will work. You have to open each one up and hack it yourself (and OpenIPC may support it or not), and nowadays they are protected from loading firmware easily (tiny UART&#x2F;debug ports, disabled UBoot loader, etc.).I also wish there were an option for someone to buy cameras with open HW&#x2F;SW, to get away from the random cloud services they want you to open your network up to with cheap cameras. reply h317 4 hours agoparentprevIn case some other folks who are in need, we have a side-project dedicated to the completely open source camera: hardware: https:&#x2F;&#x2F;github.com&#x2F;maxlab-io&#x2F;tokay-lite-pcb firmware and software layer: https:&#x2F;&#x2F;github.com&#x2F;maxlab-io&#x2F;tokay-lite-sdkAnd we will eventually ship them form here Crowd Supply at: https:&#x2F;&#x2F;www.crowdsupply.com&#x2F;maxlab&#x2F;tokay-lite reply neodypsis 13 hours agoparentprev> I really wish someone would take initiative to sell&#x2F;resell a line of camera hardware with openipc or other open firmware on them. It&#x27;s a business opportunity sorely missing with nothing to offer home&#x2F;business users ip cameras with open sterile and guaranteed secure firmware vs. random pure Chinese jank.Manufacturers could do it themselves, but the license prohibits commercial use. reply KennyBlanken 12 hours agorootparent> OpenIPC source code is released under one of the most simple open source license agreements, MIT License, giving users express permission to reuse code for any purpose, even as part of a proprietary software. We only ask you politely to contribute your improvements back to us. We would be grateful for any feedback and suggestions.Edit: since people are arguing with me, the text I quoted is right of the main page for OpenIPC, and everything here is MIT licensed too: https:&#x2F;&#x2F;github.com&#x2F;OpenIPC reply neodypsis 12 hours agorootparent> Majestic code while is not open, provides unprecedented performance and capabilities for a wide range of hardware. The author of Majestic streamer is looking into possibilities to open-source the codebase after he secures enough funds to support further open development. You can help to make it happen sooner.Source: https:&#x2F;&#x2F;openipc.org&#x2F; reply swores 9 hours agorootparentThat comes after this:> OpenIPC Firmware uses Buildroot to build its Linux distro, and utilizes either Majestic, Mini or Venc streamer.Which makes it sound like Majestic isn&#x27;t required? Or is it the only good option, or something? reply neodypsis 9 hours agorootparentI checked the repos for those alternatives. They state that they are not as feature-complete, nor as widely supported, as Majestic is. reply KennyBlanken 12 hours agorootparentprevI can&#x27;t find that text anywhere on the page (the word \"Majestic\" literally doesn&#x27;t appear anywhere). reply chefandy 11 hours agorootparentIt&#x27;s right before the \"Why OpenIPC Firmware?\" headline. reply tpmx 12 hours agorootparentprevExcept they&#x27;re not releasing the source of the core parts. They say (or at least very heavily imply they are), but they aren&#x27;t. See other comments. reply skywal_l 3 hours agoparentprevStupid question from a noob: why not just use some raspberry pi zero, a camera sensor and some ONVIF server? That&#x27;s what most of those cheap chinese IP camera are: a linux board, a camera sensor and some compression chip. reply cfn 3 hours agorootparentBecause the camera comes in a neat box with resonable weather tightness and it is easier to setup and connect. IP cameras also have the advantage of not needing a power brick. reply spease 8 hours agoparentprevI’ve been thinking about something like this. Anyone else interesting in working on this? reply gkhartman 1 hour agorootparentI&#x27;d be interested. Even if it&#x27;s just a Linux SBC + camera module, an OS image preconfigured as an IP cam would really reduce the DIY overhead. reply tpmx 15 hours agoprevSeems like e.g. (most of) the sensor drivers are binary blobs.Some examples:Ambarella S3L: https:&#x2F;&#x2F;github.com&#x2F;OpenIPC&#x2F;firmware&#x2F;tree&#x2F;master&#x2F;general&#x2F;pack...Some HiSilicon chipset: https:&#x2F;&#x2F;github.com&#x2F;OpenIPC&#x2F;firmware&#x2F;tree&#x2F;master&#x2F;general&#x2F;pack...Edit: A smaller subset of sensors have some code here:https:&#x2F;&#x2F;github.com&#x2F;OpenIPC&#x2F;sensors reply alufers 15 hours agoparentYup, also the userspace application that does the actual streaming is closed-source as well: https:&#x2F;&#x2F;github.com&#x2F;OpenIPC&#x2F;majestic(The git repo is for bug reports only, no source-code there) reply jauntywundrkind 14 hours agorootparentOh man I didn&#x27;t realize that when I last ran into this project.I wonder how hard it would be to run your own streamer pipeline or whatnot on these things? How far a starting place do we get when we install our own Linux; do peripherals (camera networking) work or is there a bunch of special sauce in userland running half the hardware? reply the_biot 13 hours agorootparentThat&#x27;s the problem right there: the Linux kernel situtation. As OP noted they just use the vendor&#x27;s (long outdated) Linux kernel, with the vendor&#x27;s (typically crap) kernel drivers. They just don&#x27;t clean up that situation in that project, let alone try to mainline the drivers. It&#x27;s where I thought I could help, but... see my other post on why that didn&#x27;t work out.With mainline-supported cameras using a standard API, a userland application that does streaming and all kinds of neat stuff is just much more doable IMHO.They&#x27;re not the only project that builds a ton of userland stuff on top of a rotten kernel foundation: oe-alliance has nine distros downstream from it, and hundreds of supported hardware platforms. As far as I can tell it&#x27;s all ancient vendor kernels with un-upstreamable drivers. reply protastus 11 hours agorootparentThat&#x27;s the rotten situation with embedded Linux. Nearly all silicon vendors ship a highly modified kernel with proprietary drivers and firmware. Nothing gets upstreamed. The code quality is low and the changes are not architecturally sound, so patches wouldn&#x27;t get accepted by OSS maintainers in their current state. Vendors end up with countless forks of the kernel, system libraries and system services (at least one for each SoC), which they don&#x27;t maintain because it&#x27;s too much work (of course).Security in this space is a absolute disaster. reply AnthonyMouse 6 hours agorootparentThis is such a weird market. Most embedded devices are purchased by randos who don&#x27;t know what they&#x27;re getting themselves into.By contrast, security cameras are disproportionately purchased by, I&#x27;m guessing, security professionals. Often the security staff at large corporations that are buying cameras in volume. If there was any camera that was otherwise competitive and had mainline kernel drivers and open source firmware, would anybody even buy anything else?Do the manufacturers not realize this? reply protastus 5 hours agorootparentThe camera manufacturers don&#x27;t make the silicon and the silicon makers don&#x27;t care about any specific camera manufacturer.Not even Google can get Qualcomm to upstream their drivers. Let that sink in. And it&#x27;s not because of confidential IP, because many of the drivers are already open source.Silicon vendors could save a lot of time by mainlining their code rather than having hundreds of forks of every library. Security would dramatically improve. OEMs could update their products.Maybe it will take legislation or a special interpretation of the 2021 executive order on cyber security [1] to force silicon vendors to take this seriously.[1] https:&#x2F;&#x2F;www.whitehouse.gov&#x2F;briefing-room&#x2F;presidential-action... reply vineyardmike 8 hours agorootparentprevAnd this is one of the big reasons that Raspberry Pis are so popular despite an endless supply of alternatives - it&#x27;s basically the only well-documented and supported embedded linux. Almost every other dev board that competes with them has limited support and outdated kernels and countless other issues. reply scottlamb 11 hours agorootparentprev> I wonder how hard it would be to run your own streamer pipeline or whatnot on these things?Agree with the_biot: The actual streaming component is not too hard. If this were the biggest problem, I&#x27;d be thrilled to contribute to (or just write) an open source streaming server to complement my open source NVR. [1] The driver situation is indeed a bit harder—these things don&#x27;t just have mainline Linux support with v4l2 for the video capture and encoding. Or open source drivers of any kind to crib from AFAIK.The biggest problem IMHO is that there just aren&#x27;t any good cameras to buy, even completely ignoring the software aspect. I want a camera that:1. doesn&#x27;t support genocide. Nothing that involves Dahua, Hikvision, or Huawei. See IPVM articles on the subject. And a lot of available cameras are relabeled Dahua&#x2F;Hikvision stuff and&#x2F;or use Huawei components.2. is legal for sale &#x2F; authorized for use in the US. (See the Secure Equipment Act of 2021.) Mostly this excludes the same companies.3. has good night mode performance: IR&#x2F;day switch, a sensor that is at least 1&#x2F;1.8\", reasonable resolution (somewhere from HD to 4k).4. has an \"eyeball\" or \"turret\" form factor rather than \"bullet\". The latter seems to really attract spiders, so you end up with a really nice video of a web...5. supports PoE.6. is weatherized (IP54 or so).7. is reasonably priced.If you ignore #1 and #2, there&#x27;s some nice hardware out there, but I&#x27;m not willing to do that. If you ignore #3, there are a few options (GeoVision, maybe Reolink, maybe Hanwha.) If you ignore #4 and #7, there might be a couple (Axis, maybe Hanwha.) Nothing that ticks all the boxes.Hard to get excited about investing a lot in the software when the hardware isn&#x27;t there.[1] https:&#x2F;&#x2F;github.com&#x2F;scottlamb&#x2F;moonfire-nvr reply the_biot 10 hours agorootparentI expect you&#x27;ll get flak for #1, but the morality aspect of it bothers me a great deal as well. Not just supporting products of companies that contribute to China&#x27;s surveillance state, but I worry that writing open source software contributes to surveillance everywhere as well. I don&#x27;t have an answer :-( reply scottlamb 10 hours agorootparent> Not just supporting products of companies that contribute to China&#x27;s surveillance state, but I worry that writing open source software contributes to surveillance everywhere as well.This part doesn&#x27;t bother me. Making open source software available for general purpose needs equalizes things, which I consider an improvement. See e.g. what happens when folks have cameras pointed at the cops slowly murdering George Flynn.But directly giving my money to the companies that are writing software specifically to support genocide (e.g. Uighur classification) and are doing the installations for contract at what&#x27;s essentially concentration camps...that&#x27;s way over my personal ethical line, even if my few dollars are insignificant. reply askiiart 7 hours agorootparentSorry, but I think you meant George Floyd? I can&#x27;t find anything about a \"George Flynn\" being murdered by cops. reply bradknowles 6 hours agorootparentprevFor me, I want cameras that will meet most of these same requirements, but are battery powered WiFi cameras, and don&#x27;t require the use of a cloud service to operate.Reolink makes decent cameras, but their battery models don&#x27;t work with the Reolink NVR.Arlo requires a cloud service.Eufy has a home hub, but I don&#x27;t know yet how well it actually works. I&#x27;ve seen some reviews, and I&#x27;ve actually bought a hub plus a couple of cameras, but I haven&#x27;t tried to put it into actual use yet.And I want this system to also support regular WiFi cameras, as well as PoE cameras. reply dietervds 47 minutes agorootparentEufy has been a pretty great experience for me. Except for the fact that they recently used their notifications (normally used for camera&#x2F;motion events) to push a marketing message[1], which really ticked me off. Hence my interest in these threads now :-)[1]https:&#x2F;&#x2F;twitter.com&#x2F;dietervds&#x2F;status&#x2F;1707473093445230803 replystefan_ 12 hours agoparentprevThey are using the ancient vendor kernels too, anything from 3.x to 4.x to 5.x which sort of defeats the purpose. Most of those are pretty small too, they would be better off reversing them..Strong Android homebrew firmware vibe. reply snthd 14 hours agoprevhttps:&#x2F;&#x2F;openipc.org&#x2F;support-open-source>It&#x27;s Open Source>Price of the Firmware>The right to use the OpenIPC firmware and its components is granted to all users free of charge and only for personal, non-commercial purposes. If you are interested in using OpenIPC for your business projects, please contact our team. reply neilv 13 hours agoparentI wonder whether they really want open source, or this is a commercial effort that&#x27;s trying to leverage &#x27;the community&#x27; to get the hard part of the engineering for free.For really wanting open source, the OpenWrt model has seemed pretty good, and the nature of the engineering work seems similar. Some hardware vendors use OpenWrt for their firmware, which seems to help the mainline open source OpenWrt also have good support for that hardware. reply papercrane 14 hours agoparentprevThat&#x27;s confusing, because the GitHub repo has an MIT license. Is there some kind of secret sauce they add to the compiled binaries that the non-commercial clause applies to maybe? reply orra 14 hours agorootparentTheir \"Magestic\" app, which sounds like it provides key functionality, is closed source. reply nirav72 15 hours agoprevFor those curious if their devices are supported. The list is here:https:&#x2F;&#x2F;github.com&#x2F;OpenIPC&#x2F;wiki&#x2F;blob&#x2F;master&#x2F;en&#x2F;guide-support... reply bradknowles 13 hours agoparentWow. I&#x27;ve only heard of two of those brands of cameras.And I didn&#x27;t recognize any that were among the known thousands of re-brands&#x2F;sub-brands of HikVision and Daihua, both of which have been implicated in the suppression of the Uighur people in China (and which have been banned by the NDAA, see https:&#x2F;&#x2F;ipvm.com&#x2F;reports&#x2F;ban-law). reply snvzz 15 hours agoparentprevFinally, a way to decide which IP camera to buy. reply fcpk 15 hours agoprevTotally random question there but... Any recommendations of cameras that are currently easily available(say amazon, aliexpress, etc) and support this while having good&#x2F;verygood image quality? reply s1gnp0st 15 hours agoparentSeconded. I previously bought some that I thought were supported, before finding out that they are not. It&#x27;d be great to know whether anyone has had success with particular hardware. reply obituary_latte 9 hours agorootparentThey have a device&#x2F;chip compatibility list here: https:&#x2F;&#x2F;github.com&#x2F;OpenIPC&#x2F;wiki&#x2F;blob&#x2F;master&#x2F;en&#x2F;guide-support... reply efrecon 14 hours agoprevA long time ago, I built a business around off-the-shelf IP cameras. The field was full of buggy firmware with enormous security holes (biggest I found was to be able to acquire pictures without even being authorised!). Hardly none of these firmware would get updated, they were abondonware right out of the package.Even though this doesn&#x27;t seem to be entirely open, any kind of move into that direction is a bit of fresh air. reply braincode 9 hours agoprevMajestic, the main streaming \"core\" component of the camera (which does RTSP, HLS, etc...) is closed source, the rest is open, except some third party SDK blobs. reply gkhartman 2 hours agoprevI was really hoping that the Pine64 PineCube cameras would gain traction in this space, but it seems like they only ever really shipped small batches of them. They&#x27;d need an enclosure&#x2F;software to make them a reasonable choice.Wiki: https:&#x2F;&#x2F;wiki.pine64.org&#x2F;wiki&#x2F;PineCube reply saidinesh5 12 hours agoprevThe interesting thing about this project is that they&#x27;re repurposing cheap ip camera hardware to work as surprisingly decent, low latency digital video system for fpv drones:https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=wZAHkWHfBF4Mostly a continuation of the OpenHD project. But it&#x27;s quite promising. reply elromulous 13 hours agoprevOpenIPC really didn&#x27;t mean what I thought it did.I was 100% sure it would be some kind of inter-process communication abstraction &#x2F; framework. reply mikepurvis 13 hours agoparentYeah I think IPC has too much of an established meaning here. Something like OpenIPCam might be a little safer. reply petabytes 9 hours agoprevRelated: I was hacking Ambarella action cameras a while back: https:&#x2F;&#x2F;github.com&#x2F;petabyt&#x2F;liemothEventually I stopped not because of how low quality action cameras were, there were hundreds of different vendors publishing rebranded Chinese cheap cams. And not to mention how crap the firmware is. The whole thing gets hot and the battery only lasts a few minutes. reply dang 14 hours agoprevRelated:OpenIPC: Alternative open firmware for your IP camera - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35975383 - May 2023 (1 comment) reply snthd 14 hours agoprevAnother IP Camera thread: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36447024 reply the_biot 14 hours agoprev [–] OpenIPC is one of those projects where the core team is Russian, and their communication is Russian, and they use Telegram (or a bridge) for everything. As a non-Russian speaker you&#x27;re pretty lost trying to contribute, and as a result they attract hardly anyone outside the russosphere. I&#x27;ve tried telling them they&#x27;re artifically limiting their project, but was predictably ignored. This project has so much potential, but it will never be achieved.I&#x27;ve seen this before; projects like that tend to stagnate and die. Openinkpot comes to mind.BTW not a rant against Russians; open source projects do best when they have a lot of contributors from across the world, and that means English. reply 0xDEADFED5 12 hours agoparentFirst of all, software projects in general tend to stagnate and die.If you really want to contribute, I&#x27;m sure you can figure out a way. You can always fork it and do your own thing, that&#x27;s potentially a contribution in itself. It&#x27;s a little selfish to expect other people to become fluent in technical English and rearrange their project to suit your desires. reply rasz 10 hours agoparentprev [–] Something tells me being closed source while naming it OpenIPC and pretending to be under MIT license mighty be a bigger factor.Being russian and clearly trying to steer project towards drone use https:&#x2F;&#x2F;github.com&#x2F;OpenIPC&#x2F;sandbox-fpv doesnt help either, we all know what russian drones are used for right now. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenIPC is an open-source firmware designed to supplant insecure and proprietary firmware provided by IP camera manufacturers.",
      "The firmware, available in pre-compiled form for straightforward installation, also allows access to source files for further tweaks and development. It was initially compatible with HiSilicon processors but now incorporates chips from different manufacturers.",
      "The project encourages user contributions and actively seeks feedback and suggestions, offering several repositories for collaboration."
    ],
    "commentSummary": [
      "The OpenIPC project provides open firmware for IP cameras, catering to issues related to transitioning Chinese IP cameras to open firmware.",
      "However, concerns about some portions of the firmware remaining proprietary and limitations of embedded Linux are raised, signaling skepticism over the complete adoption of open source in the OpenIPC project.",
      "There are also communication and contribution difficulties due to the primarily Russian core team, alongside discussions on the need for more open-source solutions and possible strategies like legislation or adopting the OpenWrt model."
    ],
    "points": 323,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1696784222
  },
  {
    "id": 37815674,
    "title": "My personal C coding style as of late 2023",
    "originLink": "https://nullprogram.com/blog/2023/10/08/",
    "originBody": "My personal C coding style as of late 2023 October 08, 2023 This has been a ground-breaking year for my C skills, and paradigm shifts in my technique has provoked me to reconsider my habits and coding style. It’s been my largest personal style change in years, so I’ve decided to take a snapshot of its current state and my reasoning. These changes have produced significant productive and organizational benefits, so while most is certainly subjective, it likely includes a few objective improvements. I’m not saying everyone should write C this way, and when I contribute code to a project I follow their local style. This is about what works well for me. Primitive types Starting with the fundamentals, I’ve been using short names for primitive types. The resulting clarity was more than I had expected, and it’s made my code more enjoyable to review. These names appear frequently throughout a program, so conciseness pays. Also, now that I’ve gone without, _t suffixes are more visually distracting than I had realized. typedef uint8_t u8; typedef char16_t c16; typedef int32_t b32; typedef int32_t i32; typedef uint32_t u32; typedef uint64_t u64; typedef float f32; typedef double f64; typedef uintptr_t uptr; typedef char byte; typedef ptrdiff_t size; typedef size_t usize; Some people prefer an s prefix for signed types. I prefer i, plus as you’ll see, I have other designs for s. For sizes, isize would be more consistent, and wouldn’t hog the identifier, but signed sizes are the way and so I want them in a place of privilege. usize is niche, mainly for interacting with external interfaces where it might matter. b32 is a “32-bit boolean” and communicates intent. I could use _Bool, but I’d rather stick to a natural word size and stay away from its weird semantics. To beginners it might seem like “wasting memory” by using a 32-bit boolean, but in practice that’s never the case. It’s either in a register (return value, local variable) or would be padded anyway (struct field). When it actually matters, I pack booleans into a flags variable, and a 1-byte boolean rarely important. While UTF-16 might seem niche, it’s a necessary evil when dealing with Win32, so c16 (“16-bit character”) has made a frequent appearance. I could have based it on uint16_t, but putting the name char16_t in its “type hierarchy” communicates to debuggers, particularly GDB, that for display purposes these variables hold character data. Officially Win32 uses a type named wchar_t, but I like being explicit about UTF-16. u8 is for octets, usually UTF-8 data. It’s distinct from byte, which represents raw memory and is a special aliasing type. In theory these can be distinct types with differing semantics, though I’m not aware of any implementation that does so (yet?). For now it’s about intent. What about systems that don’t support fixed width types? That’s academic, and far too much time has been wasted worrying about it. That includes time wasted on typing out int_fast32_t and similar nonsense. Virtually no existing software would actually work correctly on such systems — I’m certain nobody’s testing it after all — so it seems nobody else cares either. I don’t intend to use these names in isolation, such as in code snippets (outside of this article). If I did, examples would require the typedefs to give readers the complete context. That’s not worth extra explanation. Even in the most recent articles I’ve used ptrdiff_t instead of size. Macros Next, my “standard” set of macros: #define sizeof(x) (size)sizeof(x) #define alignof(x) (size)_Alignof(x) #define countof(a) (sizeof(a) / sizeof(*(a))) #define lengthof(s) (countof(s) - 1) While I still prefer ALL_CAPS for constants, I’ve adopted lowercase for function-like macros because it’s nicer to read. They don’t have the same namespace problems as other macro definitions: I can have a macro named new() and also variables and fields named new because they don’t look like function calls. For GCC and Clang, my favorite assert macro now looks like this: #define assert(c) while (!(c)) __builtin_unreachable() It has useful properties beyond the usual benefits: It does not require separate definitions for debug and release builds. Instead it’s controlled by the presence of Undefined Behavior Sanitizer (UBSan), which is already present/absent in these circumstances. That includes fuzz testing. libubsan provides a diagnostic printout with a file and line number. In release builds it turns into a practical optimization hint. To enable assertions in release builds, put UBSan in trap mode with -fsanitize-trap and then enable at least -fsanitize=unreachable. In theory this can also be done with -funreachable-traps, but as of this writing it’s been broken for the past few GCC releases. Parameters and functions No const. It serves no practical role in optimization, and I cannot recall an instance where it caught, or would have caught, a mistake. I held out for awhile as prototype documentation, but on reflection I found that good parameter names were sufficient. Dropping const has made me noticeably more productive by reducing cognitive load and eliminating visual clutter. I now believe its inclusion in C was a costly mistake. (One small exception: I still like it as a hint to place static tables in read-only memory closer to the code. I’ll cast away the const if needed. This is only of minor importance.) Literal 0 for null pointers. Short and sweet. This is not new, but a style I’ve used for about 7 years now, and has appeared all over my writing since. There are some theoretical edge cases where it may cause defects, and lots of ink has been spilled on the subject, but after a couple 100K lines of code I’ve yet to see it happen. restrict when necessary, but better to organize code so that it’s not, e.g. don’t write to “out” parameters in loops, or don’t use out parameters at all (more on that momentarily). I don’t bother with inline because I compile everything as one translation unit anyway. typedef all structures. I used to shy away from it, but eliminating the struct keyword makes code easier to read. If it’s a recursive structure, use a forward declaration immediately above so that such fields can use the short name: typedef struct map map; struct map { map *child[4]; // ... }; Declare all functions static except for entry points. Again, with everything compiled as a single translation unit there’s no reason to do otherwise. It was probably a mistake for C not to default to static, though I don’t have a strong opinion on the matter. With the clutter eliminated through short types, no const, no struct, etc. functions fit comfortably on the same line as their return type. I used to break them apart so that the function name began on its own line, but that’s no longer necessary. In my writing I sometimes omit static to simplify, and because outside the context of a complete program it’s mostly irrelevant. However, I will use it below to emphasize this style. For awhile I capitalized type names as that effectively put them in a kind of namespace apart from variables and functions, but I eventually stopped. I may try this idea in different way in the future. Strings One of my most productive changes this year has been the total rejection of null terminated strings — another of those terrible mistakes — and the embrace of this basic string type: #define s8(s) (s8){(u8 *)s, lengthof(s)} typedef struct { u8 *data; size len; } s8; I’ve used a few names for it, but this is my favorite. The s is for string, and the 8 is for UTF-8 or u8. The s8 macro (sometimes just spelled S) wraps a C string literal, making a s8 string out of it. A s8 is handled like a fat pointer, passed and returned by copy. s8 makes for a great function prefix, unlike str, all of which are reserved. Some examples: static s8 s8span(u8 *, u8 *); static b32 s8equals(s8, s8); static size s8compare(s8, s8); static u64 s8hash(s8); static s8 s8trim(s8); static s8 s8clone(s8, arena *); Then when combined with the macro: if (s8equals(tagname, s8(\"body\"))) { // ... } You might be tempted to use a flexible array member to pack the size and array together as one allocation. Tried it. Its inflexibility is totally not worth whatever benefits it might have. Consider, for instance, how you’d create such a string out of a literal, and how it would be used. A few times I’ve thought, “This program is simple enough that I don’t need a string type for this data.” That thought is nearly always wrong. Having it available helps me think more clearly, and makes for simpler programs. (C++ got it only a few years ago with std::string_view and std::span.) It has a natural UTF-16 counterpart, s16: #define s16(s) (s16){u##s, lengthof(u##s)} typedef struct { c16 *data; size len; } s16; I’m not entirely sold on gluing u to the literal in the macro, versus writing it out on the string literal. More structures Another change has been preferring structure returns instead of out parameters. It’s effectively a multiple value return, though without destructuring. A great organizational change. For example, this function returns two values, a parse result and a status: typedef struct { i32 value; b32 ok; } i32parsed; static i32parsed i32parse(s8); Worried about the “extra copying?” Have no fear, because in practice calling conventions turn this into a hidden, restrict-qualified out parameter — if it’s not inlined such that any return value overhead would be irrelevant anyway. With this return style I’m less tempted to use in-band signals like special null returns to indicate errors, which is less clear. It’s also led to a style of defining a zero-initialized return value at the top of the function, i.e. ok is false, and then use it for all return statements. On error, it can bail out with an immediate return. The success path sets ok to true before the return. static i32parsed i32parse(s8 s) { i32parsed r = {0}; for (size i = 0; i < s.len; i++) { u8 digit = s.data[i] - '0'; // ... if (overflow) { return r; } r.value = r.value*10 + digit; } r.ok = 1; return r; } Aside from static data, I’ve also moved away from initializers except the conventional zero initializer. (Notable exception: s8 and s16 macros.) This includes designated initializers. Instead I’ve been initializing with assignments. For example, this buffered output “constructor”: typedef struct { u8 *buf; i32 len; i32 cap; i32 fd; b32 err; } u8buf; static u8buf newu8buf(arena *perm, i32 cap, i32 fd) { u8buf r = {0}; r.buf = new(perm, u8, cap); r.cap = cap; r.fd = fd; return r; } I like how this reads, but it also eliminates a cognitive burden: The assignments are separated by sequence points, giving them an explicit order. It doesn’t matter here, but in other cases it does: example e = { .name = randname(&rng), .age = randage(&rng), .seat = randseat(&rng), }; There are 6 possible values for e from the same seed. I like no longer thinking about these possibilities. Odds and ends Prefer __attribute to __attribute__. The __ suffix is excessive and unnecessary. __attribute((malloc, alloc_size(2, 4))) For Win32 systems programming, which typically only requires a modest number of declarations and definitions, rather than include windows.h, write the prototypes out by hand using custom types. It reduces build times, declutters namespaces, and interfaces more cleanly with the program (no more DWORD/BOOL/ULONG_PTR, but u32/b32/uptr). #define W32(r) __declspec(dllimport) r __stdcall W32(void) ExitProcess(u32); W32(i32) GetStdHandle(u32); W32(byte *) VirtualAlloc(byte *, usize, u32, u32); W32(b32) WriteConsoleA(uptr, u8 *, u32, u32 *, void *); W32(b32) WriteConsoleW(uptr, c16 *, u32, u32 *, void *); For inline assembly, treat the outer parentheses like braces, put a space before the opening parenthesis, just like if, and start each constraint line with its colon. static u64 rdtscp(void) { u32 hi, lo; asm volatile ( \"rdtscp\" : \"=d\"(hi), \"=a\"(lo) : : \"cx\", \"memory\" ); return (u64)hi<<32lo; } There’s surely a lot more to my style than this, but unlike the above, those details haven’t changed this year. To see most of the mentioned items in action in a small program, see wordhist.c, one of my testing grounds for hash-tries, or for a slightly larger program, asmint.c, a mini programming language implementation. c Have a comment on this article? Start a discussion in my public inbox by sending an email to ~skeeto/public-inbox@lists.sr.ht [mailing list etiquette] , or see existing discussions. « A simple, arena-backed, generic dynamic array for C null program Chris Wellons wellons@nullprogram.com (PGP) ~skeeto/public-inbox@lists.sr.ht (view) Index Tags Feed About Tools Toys GitHub All information on this blog, unless otherwise noted, is hereby released into the public domain, with no rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=37815674",
    "commentBody": "My personal C coding style as of late 2023Hacker NewspastloginMy personal C coding style as of late 2023 (nullprogram.com) 316 points by zdw 9 hours ago| hidepastfavorite228 comments neilv 1 hour ago> While I still prefer ALL_CAPS for constants, I’ve adopted lowercase for function-like macros because it’s nicer to read.\"ALL_CAPS\" in C was not for constants, but for preprocessor macros. It&#x27;s shouting in all-caps, because it means \"Look out! There&#x27;s a cpp macro expansion here!\"Related, please stop using \"ALL_CAPS\" for constants in other languages. Not only does shouting constants as the most prominent syntax in the code make no sense, but there are much better uses for shouting in a programming language.(For an example of a good use of \"ALL_CAPS\": if your language ever acquires Scheme-like template-based hygienic macro transformers, \"ALL_CAPS\" (or \"ALL-CAPS\") is excellent for making template pattern variables stand out within the otherwise literal code blocks.) reply avgcorrection 29 minutes agoparentThanks for bringing this up. There is no reason for modern languages that have proper constants (not preprocessor macros which happen to sometimes be used for them) to use this tedious style.Constants are so innocent and useful. Why indirectly discourage their use by making their usage an eye-bleed? reply jraph 33 minutes agoparentprevInteresting take, but good luck with this fight against all caps (snake case) constants, at this point it&#x27;s almost a consensus, a shared culture element, a deeply ingrained habit that the vast majority of developers have and recognize.Changing this would be a huge undertake I&#x27;d be afraid of engaging, if I cared that much. reply eviks 25 minutes agoparentprevIndeed, these are less readable and harder to type for no benefit reply Uptrenda 7 hours agoprevIMO, defining your own types is one step too far. Now everyone who is already familiar with C types has to learn your own quirky system to understand one program. I think it does probably make sense to be specific about the sizes though e.g. using uint32_t over just uint (and expecting to receive some architecture-dependent size you might not get with uint.) These types should be defined in the right header (I think it depends on compiler?) It&#x27;s been a while since I wrote any amount of C so my apologizes if this isn&#x27;t correct. reply WalterBright 2 hours agoparentThe reality is that C `int` is 32 bits in size.Sure, that&#x27;s not true for 16 bit targets. But are you really going to port a 5Mb program to 16 bits? It&#x27;s not worth worrying about. Your code is highly unlikely to be portable to 16 bits anyway.The problem is with `long`, which is 32 bits on some machines and 64 bits on others. This is just madness. Fortunately, `long long` is always 64 bits, so it makes sense to just abandon `long`.So there it is: char - 8 bits short - 16 bits int - 32 bits long long - 64 bitsDone!(Sheesh, all the endless hours wasted on the size of an `int` in C.) reply Findecanor 1 hour agorootparentYet another issue is that `char` is signed on some platforms but unsigned on others. It is signed on x86 but unsigned on RISC-V. On ARM it could be either (ARM standard is unsigned, Apple does signed).I therefore use typedefs called `byte` and `ubyte` wherever the data is 8-bit but not character data. I also use the aliases `ushort`, `uint` and `ulong` to cut down on typing. On the other hand, the types inare often recognised by syntax colouring in editors where user-defined types aren&#x27;t. reply consp 2 hours agorootparentprevWhich are minima, but in practice they represent the width. reply rkagerer 3 hours agoparentprevThe author did qualify it with personal coding style. Frankly the standard types are too verbose and I wish this guy&#x27;s elegant and clear list had been the one that was adopted way back when. reply iforgotpassword 2 hours agorootparentThat ship has sailed ages ago. There are some things you should just accept about C, or any programming language really. Just because you can do something doesn&#x27;t mean you should do something. I don&#x27;t know how many years of experience in C this guy has, but this is a \"been there, done that\" case for me. I stick to stdint and stdbool today, and even if only half the code&#x2F;libs I interface with do that, it&#x27;s already worth the extra _t-typing all the time. Just the fact that they use the i prefix for signed, and s for string has a high chance that his s8 string type gets confused with an 8-bit signed int.But as you say, it&#x27;s a personal style, and the author seems to be aware of that:> I’m not saying everyone should write C this way, and when I contribute code to a project I follow their local style.Because that&#x27;s by far the most important rule to follow in any language.I think the rest is less controversial, the 0 vs. NULL thing has been going on forever; I didn&#x27;t check recently but I&#x27;d assume \"const somestruct *foo\" would still sometimes help out the compiler to optimize vs. the non-const version. reply lelanthran 2 hours agorootparentprev> The author did qualify it with personal coding style. Frankly the standard types are too verbose and I wish this guy&#x27;s elegant and clear list had been the one that was adopted way back when.They didn&#x27;t adopt it for the same reason that it is a bad idea now - too many programs already contained at least one variable named after his types.If the standard had adopted his convention, too many programs will break, which is why his convention is currently unsuitable for any existing project. reply otikik 2 hours agorootparentWouldn’t existing programs just continue working? What the author did was adding new types, not modifying or removing existing ones. reply iforgotpassword 2 hours agorootparentBecause those existing programs surely don&#x27;t use the same identifiers for other stuff? Certainly there is no code out there using s8 for \"signed char\" instead of \"utf8 string\"? :-) reply lelanthran 2 hours agorootparentprev> Wouldn’t existing programs just continue working?Only ones which don&#x27;t have variables named `i8` or `b32` (which is common, but not for booleans).I&#x27;ve seen many projects which used the pattern [a-z][1-9]+ as variables. Those programs with a variable called `i8` won&#x27;t compile if the standard made a type called `i8`.In particular, the standard reserves entire patterns to itself, so it cannot reserve the pattern of [a-z][0-9]+. They could, and did, reserve the pattern *int*_t for themselves. reply otikik 1 hour agorootparentBut that problem exists for any C project that uses an external library. If the library defines something that the project already uses, then the project will not work.In my mind that&#x27;s not a problem with the decisions taken by the author of the article, it&#x27;s more of a symptom of C&#x27;s limitations. reply lelanthran 1 hour agorootparent> But that problem exists for any C project that uses an external library. If the library defines something that the project already uses, then the project will not work.For libraries, yes, but we&#x27;re talking about why the standard didn&#x27;t do it.The standard did not want[1] to reserve keywords that current programs were already using.A library that conflicts on keywords will only break with those programs that use it. A standard that conflicts on keywords breaks all programs in that language.> In my mind that&#x27;s not a problem with the decisions taken by the author of the article, it&#x27;s more of a symptom of C&#x27;s limitations.One of the constraints of taking decisions is to work within the limits existing framework - if you&#x27;re avoiding the alternatives that don&#x27;t break, then it&#x27;s the decision-makers bug, not the frameworks.The framework has limitations, widely published and known. You make decisions within those limitations.[1] Although, they do do it, it&#x27;s only with relectance, not on a whim to avoid typing a few characters) reply gdprrrr 1 hour agorootparentprevTypedefs and variable namens don&#x27;t live in them same namespace, do they? reply lelanthran 1 hour agorootparent> Typedefs and variable namens don&#x27;t live in them same namespace, do they?Depends. See this snippet: https:&#x2F;&#x2F;www.godbolt.org&#x2F;z&#x2F;5T5jz47q4Cannot declare a variable called `u8` when there is a typedef of `u8`.And even when you can declare a variable called (for example) `int`, that effectively \"breaks\" the program by not being even a tiny bit readable anymore. reply Communitivity 1 hour agorootparentprevNo program should every have variables names according to [a-z][1-9]+ pattern, except perhaps loop indices - and not even then. reply lelanthran 1 hour agorootparent> No program should every have variables names according to [a-z][1-9]+ pattern, except perhaps loop indices - and not even then.What&#x27;s that got to do with not breaking existing programs? replymarwis 2 hours agorootparentprevIn case of winapi you can actually generate your own headers following the style you want, see for example https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;cppwin32 reply colejhudson 6 hours agoparentprevI mean, being a bit glib here, but a lot of programming is dealing with someone else&#x27;s type system.Moreover, for those of us who write C fairly often, the mnemonics here are familiar.Actually, as custom type systems go, this one is pretty elegant. Reminds me of Rust. reply oaiey 4 hours agorootparentYeah but not for basic types. Also, most code mingles sooner or later with other code. Than this is just ugly. reply circuit10 1 hour agorootparentIt’s better than dealing with needlessly long type names like uint32_t though reply kristopolous 5 hours agorootparentprevSpeaking of type systems, I read glib as g-lib a few times and tried to understand how you were talking about the GNU lib in that sentence. reply zombot 3 hours agorootparentWouldn&#x27;t that have to be `glibc`? reply wyldfire 2 hours agorootparentIt is unfortunate that the two have such similar names because there&#x27;s a lot of room for confusion. It doesn&#x27;t help that they have somewhat adjacent functionality almost. reply slondr 2 hours agorootparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GLib?wprov=sfti1 reply the_mitsuhiko 59 minutes agoparentprevThe big issue with custom integer types is that while they are awesome in the implementation files, they are problematic for libraries in headers. And if you want to avoid a divergence between header and implementation files you&#x27;re kinda stuck with the inttypes.h ones in practice. reply petabytes 4 hours agoparentprevIt&#x27;s pretty much the same types you see in Rust or Zig, and I think Linux even uses some of the same types. reply oaiey 4 hours agorootparentYeah, and C&#x2F;C++ has these since three times the age of Rust. I also find them beautiful but not consistent. reply Nursie 1 hour agoparentprev> These types should be defined in the right headerstdint.hIt&#x27;s always been amazing to me how many different projects I&#x27;ve worked on (not that I&#x27;ve been in professional C for about 7 years now)) that include their own painstaking recreation of this file.Reusing them and effectively translating them just to your own name is just annoying to the reader IMHO. I am reminded of a C++ project I worked on, where I questioned the extensive use of typedefs around collections of things, various forms of references and compound objects etc. I was informed by one of the more experienced C++ folks that it made the code easier to comprehend.Later I saw the typedef cheat-sheet sellotaped to the side of his monitor... reply gallier2 1 hour agorootparentFor bool it&#x27;s even worse. reply voxl 7 hours agoparentprevthey&#x27;re not quirky types in the least... reply outsomnia 4 hours agorootparentIn isolation, they&#x27;re not crazy.But much C code is bringing in library headers which contain their author&#x27;s own pet choices for these, which inevitably are not the same and the result is extremely confusing when you have that in play as well as the stdint.h ones.The kernel contains a mixture of \"pet\" types like u32 and stdint ones, it&#x27;s already confusing.He also does make a \"crazy\" choice later to call his string class \"s8\" which clashes with his nomenclature here. reply wiseowise 3 hours agorootparent> which clashes with his nomenclature here.How? reply asalahli 2 hours agorootparentBecause it can be mistaken for 8-bit signed integer. reply guidoism 6 hours agorootparentprevI agree. A lot of languages have settled on those same names or something similar. We don’t live in a world with a single word size anymore so carrying bit length in the name is critical, and so is keeping identifier names short. His trade off is exactly the one I would make. reply WalterBright 2 hours agorootparent> carrying bit length in the name is criticalI beg to disagree. In D: byte - 8 bits short - 16 bits int - 32 bits long - 64 bitsabsolutely nobody is confused about this. reply eviks 15 minutes agorootparentOf course plenty of people are confused, the overhead of \"short&#x2F;long\" just makes no sense, but yet another bad design from the past carefully preserved reply iforgotpassword 2 hours agorootparentprevBecause we&#x27;ve used those names since forever, but that&#x27;s archaic random crap really. Nothing apart from maybe \"byte\" makes sense here, the rest is completely arbitrary historic cruft. Could as well have called the rest timmy, britney and hulk. reply runiq 1 hour agorootparent> Nothing apart from maybe \"byte\" makes sense hereLest we forget: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20170403130829&#x2F;http:&#x2F;&#x2F;www.bobbem... reply layer8 1 hour agorootparentprevJava uses exactly the same, and has a huge developer mindshare. While rooted in historic accidents, it&#x27;s well-established. reply another2another 1 hour agorootparentprevIn that case D should probably start to have an internal conversation about what they&#x27;re going to call 128 bits then, &#x27;cause its going to become a thing sooner or later.stdint already has that covered though: (u)int128_t reply gallier2 56 minutes agorootparentit has defined the type for very long: it&#x27;s cent and ucent. It hasn&#x27;t implemented it completely though, but named and defined it is already forever. reply gallier2 53 minutes agorootparentprevand ubyte - 8 bits ushort - 16 bits uint - 32 bits ulong - 64 bits ucent - 128 bits float - 32 bits double - 64 bits real - maximum precision hardware allows (80 bits on x87). reply lelanthran 2 hours agorootparentprev> they&#x27;re not quirky types in the least...But they are buggy (correct code cannot depend on the sign of `char`), which is usually the result of typedefing primitive types to save typing 3 characters on each use. reply comex 8 hours agoprev> #define sizeof(x) (size)sizeof(x)I&#x27;m guessing this is lacking an outer pair of parentheses (i.e. it&#x27;s not `((size)sizeof(x))`) on the grounds that they&#x27;re unnecessary. In terms of operator precedence, casting binds tightly, so if you write e.g. `sizeof(x) * 3`, it expands to `(size)sizeof(x) * 3`, which is equivalent to `((size)sizeof(x)) * 3`: the cast happens before the multiplication. Indeed, casting binds more tightly than anything that could appear on the right of sizeof(x) – with one exception which is completely trivial.But just for fun, I&#x27;ll point out the exception. It&#x27;s this: (size)sizeof(x)[y]Indexing binds more tightly than casting, so the indexing happens before the cast. In other words, it&#x27;s equivalent to `(size)(sizeof(x)[y])`, not `((size)sizeof(x))[y]`.But you would never see that in a real program, since the size of something is not a pointer or array that can be indexed. Except that technically, C allows you to write integer[pointer], with the same meaning as pointer[integer]. Not that anyone ever writes code like that intentionally. But you could. And if you do, it will compile and do the wrong thing, thanks to the macro lacking the extra parentheses.…On a more substantive note, I quite disagree with the claim that signed sizes are better. If you click through to the previous arena allocator post, the author says that unsigned sizes are a \"source of defects\" and in particular the code he presents would have a defect if you changed the signed types to unsigned. Which is true – but the code as presented also has a bug! Namely, it will corrupt memory if `count` is negative. You could argue that the code is correct as long as the arguments are valid, but it&#x27;s very easy for overflow elsewhere in the code to make something accidentally go negative, so it&#x27;s better for an allocator not to exacerbate the issue.With unsigned integers, a negative count is not even representable, and a similar overflow elsewhere in the program would instead give you an extremely high positive count, which the code already checks for.Personally I prefer to use unsigned integers but do as much as possible with bounds-checked wrappers that abort on overflow. Rarely does the performance difference actually matter. reply jstimpfle 1 hour agoparent> (size)(sizeof(x)[y])Actually, and this is probably surprising to many, this is equivalent to (size)(sizeof ((x)[y]))sizeof is not a function but a unary operator, and indexing (as well as function calling...) binds stronger than the sizeof operator. It is not a function, not even syntactically! Hence why I strongly prefer putting a space after the sizeof keyword, and to not use parens for the operand unless needed.https:&#x2F;&#x2F;en.cppreference.com&#x2F;w&#x2F;c&#x2F;language&#x2F;operator_precedenceSo the \"correct\" way to define the macro is#define sizeof(x) ((size)(sizeof (x))) reply comex 7 hours agoparentprev(self-reply) One more thing.> I could use _Bool, but I’d rather stick to a natural word size and stay away from its weird semantics.This is even more subjective, but personally I like _Bool&#x27;s semantics. They mean that if an expression works in an `if` statement: if (flags & FLAG_ALLOCATED)then you can extract that same expression into a boolean variable: _Bool need_free = flags & FLAG_ALLOCATED;The issue is that `flags & FLAG_ALLOCATED` doesn&#x27;t equal &#x27;0 if unset, 1 if set&#x27;, but &#x27;0 if unset, some arbitrary nonzero value if set&#x27;. (Specifically it equals FLAG_ALLOCATED if set, which might be 1 by coincidence, but usually isn&#x27;t.) This kind of punning is fine in an `if` statement, since any nonzero value will make the check pass. And it&#x27;s fine as written with `_Bool`, since any nonzero integer will be converted to 1 when the expression is implicitly converted to `_Bool`. But if you replace `_Bool` with `int`, then this neither-0-nor-1 value will just stick around in the variable. Which can cause strange consequences. It means that if (need_free)will pass, but if (need_free == true)will fail. And if you have another pseudo-bool, then if (need_free == some_other_bool)might fail even if both variables are considered &#x27;true&#x27; (i.e. nonzero), if they happen to have different values._Bool solves this problem. Admittedly, the implicitness has downsides. If you&#x27;re refactoring the code and you decide you don&#x27;t really need a separate variable, you might try to replace all uses of `need_free` with its definition, not realizing that the implicit conversion to _Bool was doing useful work. So you might end up with incorrect code like: if ((flags & FLAG_ALLOCATED) == true)Also, if you are reading a struct from disk or otherwise stuffing it with arbitrary bytes, and the struct has a _Bool, then you risk undefined behavior if the corresponding byte becomes something other than 0 or 1 – because the compiler assumes that the implicit conversion to 0 or 1 has been done already. reply billforsternz 5 hours agorootparentThis is all very good and very, ahem, true. But (and it&#x27;s a big butt);if (need_free == true)Is such a horrible code smell to me. You have a perfectly good boolean. Why compare it to a second boolean to get a third boolean?if (need_free)orif (!need_free)for the opposite case is so much better.I will admit that in my world this leavesif (need_free == some_other_bool)as something I don&#x27;t have a particularly comfortable way of doing safely. reply hun3 1 hour agorootparentBetter example: #define FLAG_63 (1ULLBecause C&#x27;s precedence rules are damn complicated.This particular part is not actually complicated: the postfix operators bind the most tightly, then the prefix ones, then the infix ones. (The last part is quite messy, though.)So (int)x[y] parses the same way as, for example, *p++, which should be familliar to a C programmer. reply bArray 31 minutes agoprev> typedef float f32;> typedef double f64;Assuming float is 32 bits and double is 64 bits sounds like a foot-gun. OpenCV defines a float16_t [0], CUDA implements half-precision floats [1], micro-controllers implement whatever they want.C++23 introduces fixed width floating-point types [2], but not aware of any way to enforce this in C. What I would suggest it to have a macro to check data is not lost at compile time.Generally I agree with others, it might be better to leave some of these things as default for readability, even if it is not concise.[0] https:&#x2F;&#x2F;docs.opencv.org&#x2F;4.x&#x2F;df&#x2F;dc9&#x2F;classcv_1_1float16__t.htm...[1] https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;cuda-math-api&#x2F;group__CUDA__MATH...[2] https:&#x2F;&#x2F;en.cppreference.com&#x2F;w&#x2F;cpp&#x2F;types&#x2F;floating-point reply lsh123 6 hours agoprevI wrote and still maintain an open source C project for 20+ years. Once a year I get a new guy coming in and telling me I am doing it wrong: you should typedef all data types, you should stop using const, and so on. It stopped being funny after the first couple times. reply laserbeam 5 hours agoparentI&#x27;m getting an \"I don&#x27;t use const, and here&#x27;s my view on it\" vibe from the author much more than \"you shouldn&#x27;t use const\". I&#x27;m really not getting any demand that you change your coding style, just someone reflecting on their work and explaining it to others. And... Whether I agree with their choices or not, I find that very cool and informative. reply SoftTalker 5 hours agorootparentAnd, he also says \"I’m not saying everyone should write C this way, and when I contribute code to a project I follow their local style.\" reply outsomnia 4 hours agorootparentprevIt wasn&#x27;t clear to me if he&#x27;s talking about const as a variable declaration qualifier - I never used it - or const in pointer types, which is very useful. reply jstimpfle 1 hour agorootparentFor me it&#x27;s the other way around -- I use const for global variables because it makes a real difference, the data will be put in a .ro section.Pointer-to-const on the other hand (as in \"const Foo *x\") is a bit of a fluff and it spreads like cancer. I agree with the author that const is a waste of time. And it breaks in situations like showcased by strstr().I use pointer-to-const in function parameter lists though (most of the time it does not actually break like in strstr()): as documentation, and to be compatible with code that zealously attaches const everywhere where there (currently) is no need to mutate.But overall my use of const is very very little and I generally do not waste my time (anymore) with it. I almost never have to use \"const casts\" so I suppose I can manage to keep it in check. In C++ it is a bit worse, when implementing interfaces, like const_iterator etc. That requires annotating constness much more religiously, and that can lead to quite a bit of cruft and repetition. reply outsomnia 53 minutes agorootparentYou&#x27;re right, I also mark array definitions as const to control the section they go in, I was thinking about things like const int a = 5; ... anything I want a simple const var for is done with #define for me.const is indeed viral when eg, used in apis, but it&#x27;s a strong indication at a glance for api users what they can expect to happen to the memory the pointer points to, whether it&#x27;s just for input or is modified... and the virality is only a pain (it can be a pain) if you didn&#x27;t use it from the start so all the things it might call are already kitted out with it. reply jstimpfle 32 minutes agorootparentconst makes sense and rarely causes problems when used in function parameter lists.However, when used for members in datastructures, it&#x27;s more often than not problematic. replyf1shy 25 minutes agoparentprevFunny enough, the take on const in the only thing I agree with in the whole post… reply rewgs 1 hour agoparentprevI&#x27;m not a C developer, so I have to ask: why in the world would you not use const? reply jstimpfle 1 hour agorootparentIt&#x27;s more work. Not only to put all the annotations correctly, but only because it causes some real headaches. It&#x27;s easy (implicit) to transition from non-const to const 1 pointer level deep. But the other way around -- it&#x27;s really awkward to \"remove\" a const.The strstr() signature is probably the shortest example &#x2F; explanation why. To implement strstr(), you have to hack the const away to create the return value. Alternatively, create a mutable_strstr() variant that does the exact same thing. This is the kind of boilerplate that we don&#x27;t want in C (and that C is bad at generating automatically).Think about it this way: Real const data doesn&#x27;t exist. It always gets created (written) somewhere, and usually removed later. One way where this works cleanly is where the data is created at compile time, so the data can be \"truly\" const, and be put in .ro section, and automatically destroyed when the process terminates. But often, we have situations where some part of the code needs to mutate the data that is only consumed as read only by other parts of the code. One man&#x27;s const data is another man&#x27;s mutable data.In C, the support for making this transition work fluently is just very limited (but I think it&#x27;s not great in most other languages, either). reply mmoll 9 minutes agorootparent> Real const data doesn&#x27;t existEver seen a ROM?And the C library‘s hacks around not being able to overload functions (which is the only reason for strstr et al‘s weird signature) wouldn‘t stop me from using const. It can be really useful both for documentation and for correctness. Think memcpy, not strstr. reply gwd 9 minutes agorootparentprev> The strstr() signature is probably the shortest example &#x2F; explanation why. To implement strstr(), you have to hack the const away to create the return value.It seems to me the \"hacking\" is exactly the side-effect that is wanted. It&#x27;s like the requirement in Rust to do certain kinds of things in an `unsafe { }` block (or using the `unsafe` package in Go): not that you want the compiler to prevent you from doing things completely, but that you want the compiler to prevent you from doing things by accident.> One man&#x27;s const data is another man&#x27;s mutable data.Yes; and the point of `const` for function parameters is to make sure that data isn&#x27;t mutated unexpectedly. reply ForkMeOnTinder 9 hours agoprev> To beginners it might seem like “wasting memory” by using a 32-bit booleanMaybe I&#x27;m a beginner then. He lists a few cases where it&#x27;s not worse than sticking to 8-bit bools, but no cases where it&#x27;s actually an improvement. It still wastes memory sometimes, e.g. if you have adjacent booleans in a struct, or boolean variables in a function that spill out of registers onto the stack. Sure it&#x27;s only a few bytes here and there, but why pessimize? What do you gain from using a larger size? reply defrost 8 hours agoparentIt depends entirely on the architecturesCPUs, that said the obvious case from past experience is numeric processsing jobs where (say) you flow data into \"per cycle\" structs that lead with some conditionals and fill out with (say) 51210242048 sample points for that cycle (32 or 64 bit ints or floats) .. the &#x27;meat&#x27; of the per cycle job.My specific bug bear here was a junior who insisted \"saving space\" by packing the structs and using a single 8 bit byte for the conditionals.Their &#x27;improved&#x27; code ground throughput on intel chips by a factor of 10 or so and generated BUS ERRORs on SPARC RISC architectures.By packing the header of the structs they misaligned the array of data values such that the intel chips were silently fetching two 32 bit words (say) to get half a word from each to splice together to form a 32 bit data value (that was passed straddling a word boundary) to pipe into the ALU and then do something similar to repack on the other end - SPARC&#x27;s quite sensibly were throwing a fit at non aligned data.Point being - sometimes it makes sense to fit data to the architecture and not pack data to \"save\" space (this is all for throughput piped calculations not long term file storage in any case) reply xvedejas 8 hours agorootparentThis is the use case for `uint_fast8_t` (part of the C99 standard); it should use whatever width of unsigned integer is enough to store a byte, but fastest for the platform. You always know that the type can be serialized as 8 bits, but it might be larger in memory. So long as you don&#x27;t assume too much about your struct sizes across platforms, it should be a good choice for this. Although, if alignment is an issue, it might be a bit more complicated depending on platform. reply stefan_ 7 hours agorootparent10 years ago when ATmegas were still around and your 32 bit variable was generating 3 instructions for addition I would say „right on“ but now everything is a 32 bit Cortex-M and please stop polluting your code with this nonsense reply xvedejas 6 hours agorootparentPlease understand, I am still in a position where I am writing new code for a platform which only has one compiler, a proprietary fork of GCC from nearly 20 years ago. I assume other C programmers might have similar situations. reply kstrauser 6 hours agorootparent> a proprietary fork of GCCA what now? reply CapsAdmin 6 hours agorootparentI think it&#x27;s not a GPL violation if you keep the fork non-public.Though I&#x27;m entirely sure not when something is considered private or public. You can obviously make changes to a GPL repo, compile it and run the executable yourself and just never release the source code.But what happens when you start sharing the executable with your friends, or confine it to a company?\"I made this GCC fork with some awesome features. You can contact me at joe@gmail.com if you&#x27;re intere$ted ;)\" reply bdw5204 5 hours agorootparentMy understanding is that the GPL only requires the source code to be made available on request for at least 3 years (or as long as you support the software, if more than 3 years). If you want to require people who want the source to write to you via the Post Office and pay shipping+handling+cost of a disc to receive the source code, I believe this is permitted by the GPL as long as you don&#x27;t profit off of the cost.Of course, for almost all practical cases, the source code for a GPLed program is made available as a download off the Internet because the mail order disc route seems really archaic these days and probably would be removed altogether in a GPL version 4 if some prominent company used this loophole to evade the spirit of the GPL. Either that or somebody would jump through your hoops to get the source and just stick it on a public GitHub repo. If you then DMCA that repo, you&#x27;d be in violation of the GPL.If you share an GPLed executable with your friends or with other people at a company, then they&#x27;d presumably be able to request the source code. But if you run a Cloud GCC service with your fork, you could get away with keeping your source code proprietary because GCC isn&#x27;t under AGPL. reply rags2riches 3 hours agorootparentprevMy understanding is the violation happens when you share the binary without the license and sources, or information on how to request the sources. reply rowyourboat 3 hours agorootparentprevAll the GPL says on source code access is that you need to make the source code available to whoever you distributed your program to. If the program never leaves a closed circle of people, neither does the source code. reply Cerium 4 hours agorootparentprevFor example, Microchip XC16 [1]. It is GCC with changes to support their PIC processors. Some of the changes introduce bugs, for example (at least as of v1.31) the linker would copy the input linker script to a temporary location while handling includes or other pre-processor macros in the linker script. Of course if you happen to run two instances at exactly the same time one of them fails.As far as the licensing part goes they give you the source code, but last time I tried I could not get it to compile. Kind of lame and sketchy in my opinion.[1] https:&#x2F;&#x2F;www.microchip.com&#x2F;en-us&#x2F;tools-resources&#x2F;develop&#x2F;mpla... reply seabird 6 hours agorootparentprev\"Everything is 32-bit Cortex-M\" isn&#x27;t true and it&#x27;s not even close. reply paulddraper 7 hours agorootparentprevIDK it seems semantically right reply bandrami 3 hours agorootparentprevAt one point a loooooong time ago we said \"let&#x27;s give every struct its own page\" as a joke but... holy crap, it was so much faster. reply comex 8 hours agorootparentprevBut if you don&#x27;t use packed attributes, then the compiler will still add padding as necessary to avoid misalignment, while not wasting space when that&#x27;s not necessary. reply defrost 8 hours agorootparentThe key part (for myself) of ForkMeOnTinder&#x27;s comment was:> Maybe I&#x27;m a beginner then. He lists a few cases where it&#x27;s not worse than sticking to 8-bit bools, but no cases where it&#x27;s actually an improvement. It still wastes memory sometimesThey key part of my response is sometimes \"wasting memory\" (to gain alignment) is a good thing.If someone, a beginner, is concerned about percieved wasted memory then of course they will use \"packed\".As for the guts of your comment, I agree with your sentiment but would exercise caution about expecting a compiler to do what you expect in practice - especially for cross architectural projects that are intended to be robust for a decade and more - code will be put through muliple compilers across multiple architectures and potentially many many flags will be appied that may conflict in unforseen ways with each other.In general I supported the notion of sanity check routines that double check assumptions at runtime, if you want data aligned, require data to be big endian or small endian etc then have some runtime sanity checks that can verify this for specific executables on the target platform reply junon 3 hours agorootparentprevIf you have three chars next to each other in a struct, there&#x27;s a good chance they&#x27;ll take 4 bytes of memory due to padding. 4 32-bit bools guarantee it&#x27;ll take 12 at least, if not 16. reply eyegor 8 hours agoparentprevMost of the time an easy optimization is to pad fields of your struct to a 32 bit boundary. Almost any compiler will do this for you (look up \"struct alignment &#x2F; padding\"). If the compiler is going to do this anyway, might as well use the memory yourself instead of letting it be empty space. If it doesn&#x27;t happen, you leave performance on the table, so doing this raises the chance that your struct&#x2F;fields will be aligned.Nuance is that each field should be at an address divisible by the fields size or wordline size, not some magic 32 constant. The entire struct should also be padded to a multiple of the largest fields size. In practice this usually means 32 bit alignment.Ref http:&#x2F;&#x2F;www.catb.org&#x2F;esr&#x2F;structure-packing&#x2F; reply gavinhoward 8 hours agoparentprevAnd to add to that, if you use an actual bool type, sanitizers will warn you if they are ever any value other than false (0) or true (1). reply Quekid5 8 hours agoparentprevComputer architecture is optimized for 32+ bit aligned access to most things. The gain is (usually, but not always!) performance. reply gavinhoward 8 hours agorootparentI&#x27;m afraid you are only slightly correct.Architectures are generally optimized for aligned access (or disallow unaligned access), but what counts as \"aligned\" is different for each type.A char type that is used for a bool can be accessed on any byte boundary because the alignment of a char is 1. The alignment of a 32-bit value is 4.However, architectures are generally more optimized for 32-bit operations in registers. If you&#x27;re dealing with a char in a register, the compiler will generally treat it as a 32-bit value, clearing the top bits. (This is one of those places where C&#x27;s UB can bite you.)However, there are architectures where 32-bit access is optimized. reply tedunangst 8 hours agoparentprevWhat&#x27;s an example of a function where a boolean variable spills to the stack and the 3 bytes are important? reply jll29 2 hours agoprevThe notion of \"personal style\" is problematic, even for hobby projects, because (good) programming is ultimately a social activity.Even Linux started as a personal project, but because of its quality and the need it met it quickly spread. So please write your code in such a way that experienced other C programmers can read it easily.In isolation, I like some of his ideas, but some issues with C remain, and he is perhaps just to comfortable with C to jump ship and embrace Rust, which has many things he likes and more (e.g. no buffer overflows by design). reply david2ndaccount 9 hours agoprevI disagree about the structs vs out-parameters thing. I’ve found it makes functions that could return an error much harder to compose and leads to a proliferation of types all over the place. In practice almost all functions can fail (assuming you are handling OOM), so having a predictable style of returning errors is more important. reply staunton 2 hours agoparent> assuming you are handling OOMWhich almost noone ever does. It&#x27;s very hard and almost never has any benefit. At that point you have way different problems than programming style choices... reply Lockal 4 hours agoprev> typedef ptrdiff_t size;This reminds me of \"#define max ...\" in Windows.h. Not as bad, but if you autoreplace `sizeof(ptrdiff_t)` with `sizeof(size)`, good luck, because it will output size of type of size variable, if it exists in the scope. reply hyc_symas 8 hours agoprevtypedef all structs - yes, helps with conciseness. Use typedefs liberally, I say. But only typedef the things themselves, not pointers to the things. You can always use (type *) when you need a pointer. In particular, for function pointers, typedef the function, not the function pointer. Then you can use the function typedef for function declarations too, which gives you parameter type checking without needing to fix declarations everywhere if you change a function signature. I see most C codebases get this one wrong, typedef&#x27;ing the function pointer and still needing to manually write out all function declarations for that pointer definition.I&#x27;m not sold on the structs as return types thing. I prefer just a numeric error code as a return value, and out parameters for any other returns. reply jpcfl 7 hours agoparentI prefer to use typedef&#x27;s for opaque structs to emulate classes with all private fields, and use &#x27;struct&#x27; for plain ol&#x27; data structures. Classes should only be accessed via functions, while structs can be accessed directly.I think this is more-or-less a C&#x2F;POSIX standard convention. E.g., `pthread_t` vs. `struct stat`. reply yvdriess 56 minutes agorootparentIt&#x27;s definitely part of the linux kernel coding style: https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;html&#x2F;v4.10&#x2F;process&#x2F;coding-style.h... reply lelanthran 2 hours agorootparentprev> I prefer to use typedef&#x27;s for opaque structs to emulate classes with all private fields, and use &#x27;struct&#x27; for plain ol&#x27; data structures. Classes should only be accessed via functions, while structs can be accessed directly.Totally agree, I even wrote this as a blog post: https:&#x2F;&#x2F;www.lelanthran.com&#x2F;chap9&#x2F;content.html reply WalterBright 7 hours agoprevInteresting how my experience has led me in a different direction:https:&#x2F;&#x2F;dlang.org&#x2F;blog&#x2F;2023&#x2F;10&#x2F;02&#x2F;crafting-self-evident-code...(The article is crafted around D, but the principles apply to C as well.) reply keyle 5 hours agoparentFun read. What happened to the conditional expressions? Move them to the interiors of doX() and doZ().That was an interesting point. Not sure that it&#x27;s always valid but I guess it depends where you want the abstraction to lay, and how it affects the mental construct around the code.e.g. deleteRecords();is not better than if let x = deadRecords() deleteRecords(x);Sure, it looks messier but there is value is showing upfront that you&#x27;re pruning and not wiping.If the author wisely renames his function e.g. pruneDeadProjects(), yes. But merely moving the the condition within the function can be dangerous for context and be a leaky abstraction. reply WalterBright 4 hours agorootparentFinding the right abstraction isn&#x27;t always easy. Sometimes if I just put it down and let it slosh around in my brain for a few days, it comes to me.Like your idea of pruneDeadProjects()! reply MBCook 9 hours agoprevA lot of this makes sense to me.I’ve started writing a bare metal OS for Arm64. It’s very early but I’ve done some similar things. I’m using pascal strings, I’ve also renamed the types (though I’m using “int8” style, not “i8”).I quickly decided that I never intend to port real software to it, so I really don’t have to conform to standard C library functions or conventions. That’s given me more freedom to play around. C is old enough to have a lot of baggage from when every byte was precious, even in function names.It’s nice to get away from that. Much like the contents of this post, that plus other small renamed just ended up feeling like a nice cleanup. reply vdqtp3 6 hours agoparent> I never intend to port real software to it, so I really don’t have to conform to standard C library functions or conventions.So you&#x27;re just building it as just a hobby, won’t be big and professional like gnu? reply 1f60c 2 hours agorootparentContext: Linus Torvalds&#x27; announcement of Linux to comp.os.minix: https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~awb&#x2F;linux.history.html reply eddd-ddde 8 hours agoparentprevi never thought about that, saving bytes even in symbols reply trealira 6 hours agorootparentYeah, old C compilers would only look at the first 6 characters of a name, and the rest were insignificant. That&#x27;s how you get nanrs like \"strcpy\" and \"malloc\" instead of something like \"string_copy\" or \"mem_allocate\" (I still think \"memory_allocate\" would be long enough to be annoying to type). reply lifthrasiir 4 hours agorootparentOne of last vestiges of this fact AFAIK was libjpeg, which had a macro NEED_SHORT_EXTERNAL_NAMES that shortens all public identifiers to have unique 6-letter-long prefixes. Libjpeg-turbo nowadays has removed them though [1].[1] https:&#x2F;&#x2F;github.com&#x2F;libjpeg-turbo&#x2F;libjpeg-turbo&#x2F;commit&#x2F;52ded8... reply nxobject 25 minutes agorootparentprevIIRC this mirrored the behavior of MACRO-11, DEC&#x27;s first-class PDP-11 assembler. reply Eggsellence 8 hours agorootparentprevI think he is suggesting the opposite - use more verbose names for clarity. reply kazinator 6 hours agoprev> #define sizeof(x) (size)sizeof(x)That breaks any macro that uses sizeof in its expansion, and subtly changes any code snippet you might bring into the code that uses sizeof, even if those macro are defined first.Speaking of which, if you define a macro for a C keyword before including any standard header, the behavior is undefined.It&#x27;s an unparenthesized unary expression, which has a lower precedence than postfix. sizeof(x)[ptr] will turn into (size)sizeof(x)[ptr] which parses as (size) ( sizeof(x)[ptr] ). reply jpcfl 7 hours agoprevParameters and functionsNo const.Please don&#x27;t. `const` is incredibly valuable, not only to the reader, but to the compiler.Take for example: int Foo_bar(Foo const* self);Just looking at this signature, I know that calling `bar()` will not modify the state of the object. This is incredibly valuable information to the reader.Furthermore, if I want to create a `Foo` constant, I can only call this function if it is `const`. static Foo const a_foo = FOO_INIT(&some_params); return Foo_bar(&a_foo); &#x2F;&#x2F; Will not compile without &#x27;const&#x27; in function`const` is valuable to the compiler, since `a_foo` can be placed into ROM on some platforms like MCUs, saving precious RAM. reply david2ndaccount 7 hours agoparentYou only know there are no mutations if Foo itself does not contain any indirections. Additionally, the compiler generally cannot assume that Foo_bar does not modify Foo as it is legal to cast away const as long as it is not originally a variable declared as const (so in your static Foo example it would be UB to cast away const).static + const is valuable, but const parameters are merely a convention, there is no actual enforcement around them and due to aliasing the compiler generally can’t assume the parameter doesn’t actually change anyway. reply lelanthran 2 hours agorootparent> Additionally, the compiler generally cannot assume that Foo_bar does not modify Foo as it is legal to cast away constNo, but it can warn you!The type is meant to capture programmer intention, and if you use `const` the compiler can warn you that your intention does not match the intention of the existing code (like, the intention of the author who wrote Foo_Bar). reply paulddraper 7 hours agoparentprevAgreed; const is one of those features that is so good I wish a lot of other languages (e.g. java) had it. reply epcoa 6 hours agorootparentconst in C and C++ are an abomination. On a pointer they don’t tell the compiler to do shit, because they can’t.That I can agree with TFA. However I agree with the GP that dismissing it entirely is a little misplaced. It serves as a hint&#x2F;documentation and I think the article undersells the value of rodata (not the pointer use of const which is basically shit).I mean I have seen at least a few SIGSEGV&#x2F;aborts due to attempted writes to ro memory. Also like, one of the few modern justifications for C, embedded, const still has important link time meaning. reply skovati 5 hours agorootparentprevFWIW, Java does have the \"final\" keyword. reply flakes 3 hours agorootparentFinal only protects the variable from being assigned a new reference (similar to a const pointer). It doesn’t protect any of the underlying data held by the object from being changed, unless the entire hierarchy has every field declared final as well. I still use final heavily in all of my Java code, but it doesnt convey the full intent I would like it to. reply _old_dude_ 2 hours agorootparentI remember James Gosling saying, a long time ago, that the whole class should be either mutable or not so you do not need to tag some methods with const.The consequence is that you may define two classes, one non-mutable and one mutable like String&#x2F;StringBuilder. reply layer8 1 hour agorootparentIt means you have to triplicate each mutable class, because besides the immutable variant you also need the common interface (e.g. CharSequence), in order to pass mutable instances to read-only functions. reply taylorius 50 minutes agorootparentprevJava has many great qualities, but concision is not one of them. replyglitchc 7 hours agoparentprevI&#x27;m afraid you are mistaken. In particular for pointers, const does not guarantee that the memory at the location pointed to won&#x27;t change. Const only guarantees that the address itself doesn&#x27;t change. reply kibibu 6 hours agorootparentShould perhaps be int Foo_bar(const Foo * self); reply jenadine 4 hours agorootparent`const Foo*` and `Foo const*` are exactly the same and just a question of style (east-const vs. west-const)Not to be confused with `Foo *const` reply PennRobotics 2 hours agorootparentFor anyone thinking, \"wtf const pointer order??\" fall back on the spiral rule:https:&#x2F;&#x2F;c-faq.com&#x2F;decl&#x2F;spiral.anderson.html reply glitchc 6 hours agorootparentprevEven then, some other function can change the memory at the address of self while this one is executing, especially in concurrent systems. Additionally, any other pointer pointing to the same address can also modify self&#x27;s memory. const in this case is really just \"scout&#x27;s honour\". reply __MatrixMan__ 6 hours agoparentprevSorry for going off topic, but something fun I&#x27;ve learned lately is that in Nim (which compiles to C) changing:let x = foo()... to ...const x = foo()...runs foo at compile time to get the value. I dunno I just thought it was neat. reply drpixie 8 hours agoprevWhile there are a few disagreeable points, I like the article.I&#x27;ve always felt that C is unfairly maligned. Yes, it&#x27;s very low level, it&#x27;s meant to be. Yes, it lets you shoot yourself in the foot, but what language doesn&#x27;t?Most of the problems with C are really issues with the standard library, the Unix (now Posix) interfaces, and the string type.None of these are actually part of C, but are part of how C is normally used. So those problems can be avoided, and use C for what it&#x27;s good at. reply slimsag 7 hours agoparentIt&#x27;s not unfairly maligned, it&#x27;s just that everyone remembers their college&#x2F;university &#x27;learning experience&#x27; which made no distinction between C&#x2F;C++, they were told to use the Borland compiler, and when trying to learn printing \"hello world\" they only got a `segmentation fault` error instead of a stack trace. When they asked why it&#x27;s so hard, they were told C&#x2F;C++ is hard - so they dropped the class.Then they picked up a JS or Python class, were told high-level languages are easy and viola! they started to understand programming.That&#x27;s the reason people are spiteful of it. They had a terrible learning experience right out the gate. reply jes5199 5 hours agorootparentdo compilers like gcc support stack traces now? reply ndesaulniers 5 hours agorootparentNo; it&#x27;s up to the program author to link against a library that provided back-traces (and maybe install a signal handler to call into that unwinder). Even then, some kind of information needs to be retained in the binary that&#x27;s normally not (-gmlt comes to mind).Usually folks attach a debugger to capture a stack trace. Usually the debugger uses debug info to determine where the program is, and it&#x27;s stack trace. Or it can walk frame pointers. Depends on if either are even used, which is a compile time decision. reply eldenring 7 hours agoparentprev> Yes, it lets you shoot yourself in the foot, but what language doesn&#x27;t?Good lord. reply drpixie 6 hours agorootparentWell ... Name a language is which you categorically cannot \"shoot yourself in the foot\"! reply eviks 0 minutes agorootparentThe \"categorically\" part is a useless qualification, you don&#x27;t program in a binary world, the ease with which a footgun is possible in a language is very important and can&#x27;t be reduced to isPossible jcrites 6 hours agorootparentprevThe issue is that it&#x27;s a spectrum: how easily you can shoot yourself in the foot, especially on accident, without awareness of the risks. And perhaps what the consequences are when you do. Risk and consequence. C is high risk and also high consequence.In higher level languages, you can&#x27;t shoot yourself in the foot nearly as easily in such a way as to trivially create a correctness problem and security vulnerability (like a buffer under&#x2F;overflow). Languages like Java and C# make it pretty difficult to shoot yourself in the foot this way (though you still can in other ways, like with incorrect concurrency). Rust makes it a lot harder to shoot yourself in the foot across the board, especially on accident (i.e., without being aware that you&#x27;re something dangerous and low-level, viz. `unsafe`). reply LoganDark 6 hours agorootparentAnd even in Rust, the unsafe code guidelines and UB are extremely hotly debated and well-defined whenever possible. reply edvinbesic 7 hours agoparentprev> I&#x27;ve always felt that C is unfairly maligned. Yes, it&#x27;s very low level, it&#x27;s meant to be. Yes, it lets you shoot yourself in the foot, but what language doesn&#x27;tIsn’t it a beauty of lower level languages that creating higher level abstractions provides more value?edit: typo reply quelsolaar 2 hours agoprevYou can declare flexible array member strings with literals like this:#define DECLARE_STRING(variable_name, string) struct{size_t allocated; size_t used; char string[sizeof(c_string)];} variable_name ## internal = {.allocated = sizeof(c_string) - 1, .used = sizeof(c_string) - 1, .string = c_string}; MyString *variable_name = &variable_name ## internalIts a lot of C99 magic, so it may not be what you want but it is possible. reply lionkor 1 hour agoprevIsn&#x27;t defining byte = char a bit wrong? char may be signed or unsigned, and may be more or less than 1 byte, right? So why that? reply travisgriggs 5 hours agoprevPretty much agree with most of this. My own personal evolved style is pretty similar. I&#x27;m suspect this kind of pragmatic style offends the theoretic and the academic. That can be intimidating. I&#x27;m glad at least one other person out there is like me. reply bjourne 8 hours agoprevI like it a lot. Especially the part about ditching const qualifiers. They clutter function declarations, don&#x27;t make the intent any clear, and almost never improve performance. Restrict, on the other hand, I&#x27;ve found makes the compilers emit better code in many cases.But I don&#x27;t like using 1 and 0 instead of booleans. Many standard C functions (fclose for example), return 0 on success. Better to be explicit here. reply pylua 8 hours agoparentI like using the const keyword, and believe it serves a real purpose with readability. I feel like most things are read access by default which is why it seems cluttered. I believe rust gets immutable by default correct . reply dundarious 7 hours agoparentprevI use an exitint typedef to signify \"0 is success, non-0 is failure\" and boolint equivalent to his b32. Not typesafe of course, so it&#x27;s just info for fallible humans. reply zeroCalories 2 hours agoprevI think C really needs an update to the standard library that includes these shorter types. Seems like a fine list though.Some of my own style changes this year:I try really hard to write functional code. Mainly try to keep functions pure, and write declarative code. I find that this makes the code easier to write(not necessarily read), and I&#x27;m less scared of bugs.I also avoid malloc unless I absolutely need it. You can usually preallocate space on the stack or use a fixed length buffer, which pretty much avoids all fears of memory leaks or use after free type bugs. You will sometimes waste memory by allocating more than you need, but it&#x27;s a lot more predictable. reply pif 2 hours agoprev> No const.I stopped reading there. I wish this guy a happy coding (and non-coding) life, but I hope we never work together. reply cryo 1 hour agoparentI&#x27;d love to have him in a team. He truly cares exactly how code works and analyzes&#x2F;fuzzes the hell out of everything.I don&#x27;t agree with all stylistic choices in his code, but the level of experience and skills are far above most C developers. reply Luker88 1 hour agoprevMore than a few of these look like what you will find in Rust by defaultNice to see some evolutive convergence in C programmers, too reply cshenton 8 hours agoprevReally lovely. A lot here reminds me of design in Odin lang. Short integral types, no const, composite returns over out params. Big fan of the approach of designing for a single translation unit and exploiting the optimisations that provides from RVO etc. reply jeffrallen 15 minutes agoprevI came back to C after a good long time in Go, and I found that my C style had picked up some of these same good ideas, which I attributed to Go. In particular I also swore off NUL terminated strings, and started using structure returns to send back multiple values. reply juunpp 7 hours agoprevHow do you not use const for arguments of data types that are too costly to pass by value? reply progfix 4 hours agoparentUsually if it is too costly, then you pass it via a pointer. reply fefe23 1 hour agoprevWhy would anyone care what the favorite whatever style of some dude on the Internet is?I like petunias! Now what? How does that help anyone? reply fredrb 1 hour agoparentSharing preferences and opinions on code ergonomics certainly has value for me, and I bet it has to other people too. This is, after all, a developer&#x27;s forum.I&#x27;m certain your opinion on petunias and your possible distaste for orchids will be welcomed in a flower-news type orange site. :-) reply awestroke 1 hour agoparentprevI&#x27;ll probably copy everything in the post for my next C project. Super nice stuff. reply forgotpwd16 1 hour agoparentprevYour preference doesn&#x27;t help anyone. That&#x27;s true. But coding styles may improve its reader&#x27;s programming. reply nspattak 1 hour agoprevi love this blog, i hold Chris Wellons to a very high estime BUT i utterly disapprove the usage of macros so much, especially to wrap cstd types, functions, etc. reply jstimpfle 1 hour agoparentI&#x27;m currently on a C++ (mostly C with C++ compiler) trip. It does make some things easier and some things harder. It makes it easier to work with C++ developers :-). I sometimes use the more involved C++ features but often regret it after because of complications.But one thing that makes it worth it is the removal of the struct tag space. I have a strong dislike for the struct tag boilerplate in C, but the alternative -- typedef boilerplate -- in C is unbearable to the point that I have a macro to define structs in C that does this automatically. #define STRUCT(name) typedef struct name name; struct name STRUCT(Foo) { int x; int y; };But macros often come with disadvantages. In this case it&#x27;s that many IDEs have trouble finding the struct definitions from a usage site. reply lelanthran 3 hours agoprevIn general, the style the author has adopted is to introduce brevity where he can, and use wrappers over what would have been standard and idiomatic C code. In most situations, these conventions aren&#x27;t good in a non-solo project, because they simply aren&#x27;t as obvious to the programmer.He says so himself:> I don’t intend to use these names in isolation, such as in code snippets (outside of this article). If I did, examples would require the typedefs to give readers the complete context. That’s not worth extra explanation. Even in the most recent articles I’ve used ptrdiff_t instead of size.You require extra work to understand his basic types before reading even a short snippet, so he doesn&#x27;t use it when he wants people to read short snippets.Introducing additional stuff the programmer must remember that does not add any safety is pointless busywork.A non-complete summary of his conventions:1. typedef standard typenames to 3-char symbols,2. remove qualifiers like const,3. use macros to reduce the amount of typing the programmer does,4. typedef all structs (and enums too, I assume)5. A macro-ized string-typed with prefixed-length.> Starting with the fundamentals, I’ve been using short names for primitive types. The resulting clarity was more than I had expected,This isn&#x27;t clear: `int8_t` is a lot clearer to a C programmer than `i8`, because a C programmer has already internalised the pattern of the stdint.h types. This is going to lead to subtle bugs as well: quick, according to his convention, what is the % specifier for `byte`?You can use %c, but that gives you an ascii character (which is not what we think of when we say &#x27;byte&#x27;).If you use PRIu8 the compiler might give warnings because `char` might be signed. The best option is to just not use `byte` and use `uint8_t` instead (or, in his system, `u8`).Same with `b32` vs `i32` - it&#x27;s a distinction without a difference and mixing these types won&#x27;t give compiler warnings, while it is almost certainly an error on the part of the developer. Use `bool` if you don&#x27;t like `_Bool`.In general I try to take advantage of whatever typing C provides; I don&#x27;t try to subvert it because I want the compiler to warn me when my intention doesn&#x27;t match the code I wrote.> No const. It serves no practical role in optimization, and I cannot recall an instance where it caught, or would have caught, a mistake.I disagree with dropping `const`.1. It&#x27;s useful as an indicator to the caller that the returned value must&#x2F;must not be freed. It&#x27;s a convention I use that makes it easy to visually spot memory leaks.Of the two functions below, it&#x27;s clear to me which one needs the returned value `free()`ed and which one doesn&#x27;t. const char *replace_substring (const char *src, const char *pat, const char *replacement); char *replace_substring (const char *src, const char *pat, const char *replacement);2. It actually does catch a lot of problems, because the compiler warns me when I attempt to modify a value that some other code I wrote never intended to be modified. It&#x27;s about intention, and when I know it is safe to modify the `const` value, then I have to explicitly cast away the const to compile my program. Anyone reading the program will know that the modification of the const-qualifed value is intentional, and therefore safe.> #define s8(s) (s8){(u8 *)s, lengthof(s)}This is interesting. I will try this out in my next project. I do think that there&#x27;ll be quite a few compiler warnings for sign-mismatch though. This is the second \"I wonder what the sign is\" question for programmers reading his code - it means that his code has to compile with the flags that he compiles it with (I assume he&#x27;s passing a flag to force chars to a particular sign). You can&#x27;t simply compile his code in another project unless you copy his flags, and those flags may conflict with the new projects flags.I also wish that he&#x27;d showed a few examples of how having the length helps - what is presented in the post doesn&#x27;t show any additional string safety over using nul-terminated strings. All those macros, including the one that creates the struct, could be written to operate on null-terminated strings. In essence, the length can be simply unused for everything! Where&#x27;s the safety!?> It’s also led to a style of defining a zero-initialized return value at the top of the function, i.e. ok is false, and then use it for all return statements. On error, it can bail out with an immediate return.I use a similar pattern, but I use `goto cleanup` on all errors; you can&#x27;t, as a general pattern, return early in a non-trivial C function without leaking resources. You can, as a general pattern, `goto cleanup` in every C function to clean up resources. I prefer the general pattern that I use everywhere rather than having to ensure that all resources acquired up to that particular return statement are released.> rather than include windows.h, write the prototypes out by hand using custom types.I think this is a very bad idea: you can&#x27;t depend on the headers not changing after a compiler or library update. Sure, maybe in practice, all the Windows types and declarations don&#x27;t change all that much, but I wouldn&#x27;t want to be the developer trying to hunt down a bug because the interface to some function has changed and the compiler isn&#x27;t giving me errors.All in all, I dunno if I would look forward to working on a team with these conventions - the code is harder to read, doesn&#x27;t work in isolation, needs custom flags, and introduces a string type without introducing any string safety with it. reply antiquark 9 hours agoprev> #define sizeof(x) (size)sizeof(x)Technically, it&#x27;s illegal to #define over a language keyword. reply wannacboatmovie 8 hours agoparentIf you&#x27;re dead set on doing this, the correct way would be to name the macro in all caps e.g. #define SIZEOF(x) as C is case sensitive. It is somewhat self-documenting to the next guy that SIZEOF() != sizeof(). reply loeg 7 hours agorootparentAny name is fine, as long as it isn&#x27;t literally \"sizeof\". reply mianos 5 hours agoparentprevConsidering &#x27;#defines&#x27; are done in a textual pre-precessing by the C pre-processor, they don&#x27;t know much at all about the C language. You can define out int, long, struct or anything.I have seen many people redefine &#x27;for&#x27; and &#x27;while&#x27;. These people often argue that it is an improvement. cpp #define sizeof(x) (size)sizeof(x) sizeof(UU) ^D # 1 \"\" # 1 \"\" # 1 \"\" # 31 \"\" # 1 \"&#x2F;usr&#x2F;include&#x2F;stdc-predef.h\" 1 3 4 # 32 \"\" 2 # 1 \"\" (size)sizeof(UU) reply chii 8 hours agoparentprevreally? then how do you #define things like types like `int` and `char`? reply tom_ 8 hours agorootparentYou don&#x27;t. reply antiquark 8 hours agorootparentprevFor example, it would be illegal to do the following:> #define int longBecause you&#x27;re replacing the int keyword with something else.The standard says:> 17.6.4.3.1 [macro.names] paragraph 2: A translation unit shall not #define or #undef names lexically identical to keywords, to the identifiers listed in Table 3, or to the attribute-tokens described in 7.6. reply jenadine 4 hours agorootparentThat&#x27;s C++. But I couldn&#x27;t find the same restriction in C. In fact it seems that C allows it as long as you don&#x27;t include any of the standard C header.> 7.1.2 \"Standard headers\" §5 [...] The program shall not have any macros with names lexically identical to keywords currently defined prior to the inclusion of the header or when any macro defined in the header is expanded. reply oh_sigh 8 hours agoparentprevAlso, `#define countof(a) (sizeof(a) &#x2F; sizeof(*(a)))` is unsafe since the arg is evaluated twice. reply f33d5173 7 hours agorootparentThe arg is expanded twice but evaluated zero times, since sizeof gives the size of its argument type without executing anything. reply Izkata 5 hours agorootparentcountof(foo()) looks like foo() is only called once, but would actually be called twice. That&#x27;s what GP is talking about, it&#x27;s evaluated twice after the expansion when the code is actually running, not during the expansion. reply uecker 3 hours agorootparentIt is not evaluated for regular arrays. It is evaluated for arrays with variable size, you need to be careful a bit. But this is rarely happens to be a problem.The general rule for sizeof is to apply it only to variable names or directly to typenames. reply badsectoracula 8 hours agorootparentprevThis is a very common macro to get static array lengths and i&#x27;m not sure there is any other way to do the same thing (i.e. give a static array, get back the number of items in it) in any other way. reply david2ndaccount 7 hours agorootparentprevArguments to sizeof aren’t actually evaluated (except for variably-modified-types, AKA VLAs, but don’t use those). reply uecker 4 hours agorootparentPlease use those. They are useful, make code clearer, improve bounds checking, ...Don&#x27;t let attackers influence the size of a buffer (neither for VLAs nor for heap allocations). reply thesnide 28 minutes agoprevcomments here are a canonical example of bikeshedding... reply da39a3ee 1 hour agoprev> #define countof(a) (sizeof(a) &#x2F; sizeof(*(a)))> #define lengthof(s) (countof(s) - 1)It makes no sense to use the word \"length\" to mean one less than the number of items. You could call it maxindexof perhaps.There may be good arguments for zero based indexing, but we have to also accept that there are downsides. One is that your code has to feature an artificial quantity obtained by subtracting one from a meaningful quantity. reply abareplace 1 hour agoparentThis is length of a null-terminated string, e.g. lengthof(\"abc\"). reply EPWN3D 6 hours agoprevlol \"no const\". This is really groundbreaking stuff. Let&#x27;s take a memory-unsafe language and make it even less safe. reply waffletower 6 hours agoprevWould be great to see the author&#x27;s treatment of memory allocation and lifecycle for complex data types. reply uwagar 5 hours agoprevlooks like u tabstop at 4, i prefer 2. other than that our styles match :) reply bsder 8 hours agoprev> signed sizes are the wayWell, I should probably just say \"We&#x27;re done here.\" and stop reading the rest of the article. \"Signed sizes\" are an extremely surprising abstraction break that are just asking for disaster.> No const. It serves no practical role in optimization, and I cannot recall an instance where it caught, or would have caught, a mistake.Should you even be writing C if you haven&#x27;t hit this? People mix up \"in buffers\" and \"out buffers\" all the time. \"const\" flags this immediately.> Declare all functions static except for entry points. Again, with everything compiled as a single translation unit there’s no reason to do otherwise.And when you go trying to debug something and get at a variable or function that you can&#x27;t find because everything is \"static\", you&#x27;ll curse the one who wrote the code.> Another change has been preferring structure returns instead of out parameters.Which is a great way to accidentally return a pointer to your stack and open a big ass security hole. Passing in the output buffers makes clear the ownership semantics.This guy seems like he mostly writes code for 64-bit systems. The coding advice is ... okay, I guess? Maybe? In that domain?In a 32-bit embedded domain, some of these guidelines are a good way to get youself into a lot of trouble in a real hurry. reply ripe 7 hours agoparent> signed sizes are an extremely surprising abstraction break that are just asking for disaster.Bjarne Stroustrup wrote a detailed memo advocating for signed sizes:https:&#x2F;&#x2F;www.open-std.org&#x2F;jtc1&#x2F;sc22&#x2F;wg21&#x2F;docs&#x2F;papers&#x2F;2019&#x2F;p14... reply josephg 6 hours agorootparentEh, I hard disagree with this memo. He&#x27;s either dismissing or unaware of the biggest advantage of unsigned types, namely they make invalid state unrepresentible. And essentially all of his criticism of unsigned types is really criticism of the sloppy way old C and C++ compilers let you mix signed and unsigned numbers in math operations.Modern C&#x2F;C++ compilers can and will warn you (quite aggressively) if you mix signed and unsigned numbers without thinking about it.A lot of the examples also seem weird. Eg, he gives a negative example of a function: unsigned area(unsigned x, unsigned y) { return x * y; }In this, he complains that you can still write buggy code: area(height1-height2, length1-length2);He&#x27;s right - that is potentially buggy, But, that code would be buggy whether the area function took signed or unsigned numbers as input. However, the signed version of this function is still worse imo because it could hide the logic bug for longer. If the area function should always return a positive number, I&#x27;d much rather that invalid input results in an area number like 4294967250 than a small negative number.Similarly, accidentally passing a negative index to a vec is much more dangerous with signed indexes because v[-2] will probably quietly work (but corrupt memory). However, v[4294967294] will segfault on the problematic line of code. That&#x27;ll be much easier to find & debug.And a lot of the examples he gives, you&#x27;d get nice clear compiler warnings in most modern compilers if you use unsigned integers. You won&#x27;t get any warnings with signed integers. Your program will just misbehave. And thats much worse. I&#x27;d rather an easy to find bug than a hard to find bug any day of the week. reply uecker 4 hours agorootparentThe advantage of using signed types is that you can reliably find overflow bugs using UBSan and protect against exploiting such errors by trapping at run time. For unsigned types, wrap-around bugs are much harder to find and your program will silently misbehave. reply harpiee 1 hour agorootparentWith unsigned you can actually check for overflow yourself very easily z=x+y; if(z#define assert(c) while (!(c)) __builtin_unreachable()This seems like a bad idea, because the whole point of an assert is that something shouldn&#x27;t happen, but might due to a (future?) bug. reply haimez 8 hours agoparent> This seems like a bad idea, because the whole point of an assert is that something shouldn&#x27;t happen, but might due to a (future?) bug.And so it’s a bad idea because…?The whole idea is to notice a bug before it ships. Asserts are usually enabled in test and debug builds. So having an assert hit the “unreachable” path should be a good way to notice “hey, you’ve achieved the unexpected” in a bad way. You’re going to need to clarify in more detail why you think that’s a bad thing. I’m guessing because you would prefer this to be a real runtime check in non debug builds? reply im3w1l 7 hours agorootparentIt&#x27;s undefined behavior if the assert triggers in production. It&#x27;s too greedy for minor performance benefit at the risk of causing strange issues. reply haimez 7 hours agorootparentYikes. I did have to go down a little rabbit hole to understand the semantics of that builtin (I don’t normally write C if that wasn’t immediately obvious from the question) but that seems like a really questionable interpretation of “this should never happen”. I would expect the equivalent of a fault being triggered and termination of the program, but I guess this is what the legacy of intentionally obtuse undefined behavior handling in compilers gets you. reply im3w1l 7 hours agorootparentThe builtin itself is fine. It works exactly as it&#x27;s intended. It says \"I&#x27;ve double and tripple checked this. Trust me compiler. Just go fast\". But you should not use it to construct an assert. reply josephg 6 hours agorootparentEh. I absolutely get what you&#x27;re saying. And this is for sure flying very close to the knife&#x27;s edge. But if your assertion checks don&#x27;t run in release mode, and due to some bug, those invariants don&#x27;t hold, well, your program is already going to exhibit undefined behaviour. Why not let the compiler know about the undefined behaviour so it can optimize better?The nice thing about this approach is that the assertion provides value both in debug and release mode. In debug mode, it checks your invariants. And in release mode, it makes your program smaller and faster.Personally I quite like rust&#x27;s choice to have a pair of assert functions: assert!() and debug_assert!(). The standard assert function still does its check in both debug and release mode. And honestly thats a fine default these days. Sure, it makes the binary slightly bigger and the program slightly slower, but on modern computers it usually doesn&#x27;t matter. And when it does matter (like your assertion check is expensive), we have debug_assert instead. reply LoganDark 6 hours agorootparentprevthis is a true unconditional assert, e.g. \"I assert that this condition is true\". Problem is that&#x27;s too much power to throw at most use cases. reply0xedd 3 hours agoprev> code style > personalYou missed the point of code style. reply thetic 8 hours agoprev> #define sizeof(x) (size)sizeof(x)Undefined behavior[1]> #define assert(c) while (!(c)) __builtin_unreachable()Undefined behavior[1]> I’ll cast away the const if needed.Undefined behavior[2]> The assignments are separated by sequence points, giving them an explicit order.I don&#x27;t believe assignments are sequence points and only the function call is.[1] https:&#x2F;&#x2F;en.cppreference.com&#x2F;w&#x2F;c&#x2F;language&#x2F;identifier#Reserved...[2] https:&#x2F;&#x2F;en.cppreference.com&#x2F;w&#x2F;c&#x2F;language&#x2F;const reply LegionMammal978 7 hours agoparent>> I’ll cast away the const if needed.> Undefined behavior[2]How so? As the page you linked mentions, simply casting &#x27;const T *&#x27; to regular &#x27;T *&#x27; is well-defined; it&#x27;s only modifying a const object through the pointer that&#x27;s UB (C17 6.7.3&#x2F;7).> I don&#x27;t believe assignments are sequence points and only the function call is.Assigments within expressions don&#x27;t create sequence points. However, the expression of an expression statement is a full expression (i.e., not a subexpression of another expression), and there is a sequence point between each pair of full expressions (C17 6.8&#x2F;4). In other words, the semicolons create sequence points. reply WalterBright 8 hours agoparentprev> #define assert(c) while (!(c)) __builtin_unreachable()And people keep telling me that nobody uses the C preprocessor to define their own syntax any more! reply dundarious 7 hours agoparentprev> > I’ll cast away the const if needed.> Undefined behavior[2]To be clear, it&#x27;s only UB if the object was defined const, which is the case given he wrote:> One small exception: I still like it as a hint to place static tables in read-only memory closer to the code. I’ll cast away the const if needed.So you are correct on this point. Funnily enough, such objects are relatively rare IME, so I had to double-check to see that he was advocating it specifically in the rare case where it must not be applied. reply comex 7 hours agoparentprev> Undefined behavior[2]Given that this particular undefined behavior usually causes crashes in practice, I expect the author is talking about casting away the const but not actually writing to the pointer. Which is legal. reply thetic 7 hours agorootparentThe legal cases in which he needs to cast away const could be avoided if the arguments to called functions were appropriately qualified. reply LegionMammal978 7 hours agorootparentHe never said he needs to cast away const to do what he is attempting to do, he just said that he wants to cast away const to reduce clutter, even though the program would have the same semantics as if he kept the const. reply thetic 5 hours agorootparentIf only there were a way to indicate the function argument isn&#x27;t mutated. My spidey senses tingle whenever I see const-ness cast away because it almost always means something is wrong. Either a function is missing a qualifier on an argument, or something very unsafe is happening. Why force callers to cast away const-ness in hopes that everything will be fine when you can just write the correct function signature. reply paulddraper 7 hours agorootparentprevOr a common situation is mutable -> const -> mutable.And that is legal. reply ComputerGuru 8 hours agoprevI might just be a grumpy old dev but a lot of this stuff gets an immediate no from me because it’s so unidiomatic. You have to unlearn the accepted way of doing things and you end up with a codebase that is just so foreign to anyone looking at even a small chunk of it, unless they are committed to really learning to do things your way.Everyone knows what a uint32_t is when they see it. The cognitive overhead (until it becomes second nature, obviously) just feels like a heavy price to pay in order to save yourself a few characters.(Some other stuff in the proposed coding style still gets a thumbs up from me, though.) reply Pannoniae 8 hours agoparentUnpopular opinion: something being unusual does not necessarily mean it is bad. Yes, it will look foreign to random people looking at it, but if someone wants to seriously work with it, it will only take a few days to get familiarised with it. The justification of \"cognitive overhead\" is, from what I have seen, a shibboleth for rejecting \"outsider\" code written by someone not conforming to the language standards by claiming it is harder to understand. Personally, I would say that says more about the person&#x27;s inflexibility and&#x2F;or OCD, not the writer&#x27;s style.I am not saying every style is good (some simply obfuscate things and&#x2F;or make things overly verbose or unreadable) but rejecting a style solely based on it being \"non-idiomatic\" is not a good thing. reply yura 57 minutes agorootparentWell said. This is also something that I don&#x27;t buy from the criticism towards Lisp. Something along the lines of: \"Lisp did not become mainstream because everyone writes their own little language for their project, and so no one can understand other project&#x27;s code.\"pg wrote excellent arguments against this criticism in \"On Lisp\" § 4.8 Density, which apply just as well to the discussion above: “If your code uses a lot of new utilities, some readers may complain that it is hard to understand. People who are not yet very fluent in Lisp will only be used to reading raw Lisp. In fact, they may not be used to the idea of an extensible language at all. When they look at a program which depends heavily on utilities, it may seem to them that the author has, out of pure eccentricity, decided to write the program in some sort of private language. [...] If people complain that using utilities makes your code hard to read, they probably don’t realize what the code would look like if you hadn’t used them. Bottom-up programming makes what would otherwise be a large program look like a small, simple one. This can give the impression that the program doesn’t do much, and should therefore be easy to read. When inexperienced readers look closer and find that this isn’t so, they react with dismay.” reply thetic 4 hours agorootparentprevI don&#x27;t even think that&#x27;s a controversial opinion. Breaking convention isn&#x27;t inherently bad; it has costs, some of which you described. In this case specifically, the novelty is not justified by any significant benefit. reply loeg 7 hours agoparentprevThe u32, i8, etc type aliases are the least offensive parts of this to me, even though I rarely see them in C code. I think those are pretty clear.b32, size (ptrdiff_t), usize (size_t), nothing for ssize_t... what? Those are unidiomatic and also kind of weird. The macros... some are fine, some are weird.If this makes the author more productive in C, it might behoove them to see if a higher level language like Rust would meet their needs. reply benreesman 8 hours agoparentprevI want to both be polite to the OP but also agree.Writing correct C is hard, so I’m not going to knock anyone who found stuff that helps them.But pound defining shit to things you know via your Hungarian notion? Write some elisp. My Haskell programs don’t actually have Unicode lambda in them.Pascal strings? Yeah, that’s probably the better call, but why not use C++ or Rust or something where a bunch of geniuses got it right already? reply mikewarot 3 hours agorootparent>Pascal strings? Yeah, that’s probably the better call, but why not use C++ or Rust or something where a bunch of geniuses got it right already?I&#x27;ll be diving into C fairly heavy for the first time ever next year. I intend to skip right past pascal strings and implement&#x2F;use free pascal&#x27;s AnsiString or UnicodeString, both of which are reference counted, have a length (with no limit) and are guaranteed null terminated. I&#x27;ve stored a gigabyte in them in a few milliseconds. There&#x27;s no need to allocate or free memory either... it&#x27;s like freaking magic. reply saulpw 5 hours agorootparentprevToo many geniuses spoil the soup. We all see many of thousands of recipes and techniques over the course of our careers and it makes sense that each of us are continuously curating the small subset that we reach for in every project. I enjoy seeing the workbenches of other craftsmen, and nothing here looks unfamiliar.Arthur Whitney however is nuts. reply badsectoracula 8 hours agoparentprev> I might just be a grumpy old dev [...] Everyone knows what a uint32_t is when they see it.You might not be old enough then :-P many codebases typedef their own int types. See glib (gint, gshort, gint32, etc), SDL (Sint32, Uint32, etc) off the top of my head and there are many that define types like \"int32\" or \"i32\" like the linked article. reply ComputerGuru 7 hours agorootparentI cut my teeth on DWORD, PHALF_PTR, and friends, so my issue is not so much “don’t know how to grok this” as it is “we finally have sane, universal type names and you’re throwing them away.”Sure, the _t suffix may be an eyesore but I’ll take size_t over “size” any day. reply LoganDark 6 hours agorootparentI like Rust&#x27;s approach of \"isize\", as in \"size\" is in the place of the bit width. \"size\" sounds stupid. reply mberning 8 hours agoparentprevTo be fair they did say that when contributing to a shared project they follow the prevailing standard.I don’t see the harm in following this for your own passion projects. You aren’t doing it for the world, you’re doing it for yourself. reply ComputerGuru 6 hours agorootparentI mean to each their own, but in my own experience, I value being able to easily and reliably copy-and-paste code snippets across projects (and I have a million of them, across several evolutions of my own personal coding styles and conventions) or files without worrying about whether the typedefs are in scope, polluting a namespace with possibly conflicting names or macros, etc.I also have often found myself publishing “for my own use only” code as open source later and like to keep things understandable to maybe help teach someone something someday. reply jeanlucas 8 hours agoprevI wish I could work with C again reply sys_64738 7 hours agoparentC is wonderful so if you an find a project at work with a lot of C code then it&#x27;ll remain forever. All these other fad languages will die before C ever does. reply josephg 6 hours agorootparentI&#x27;m hopeful for Zig as a modern replacement for C. It feels modern like rust, but still with C&#x27;s lightness. reply Quekid5 8 hours agoparentprevI wish nobody would have to.(Haha, only serious.) reply Gibbon1 9 hours agoprev [7 more] [flagged] dmr_92 9 hours agoparentCan you explain why---what is it about this style that puts you off? reply petabytes 9 hours agoparentprev [–] What&#x27;s wrong with it? reply superchroma 9 hours agorootparent [–] Well, from a team perspective, it&#x27;s extremely opinionated and hostile to newcomers and messes with core language features at the expense of readability. If it&#x27;s your personal codebase then do whatever, obviously. reply aportnoy 8 hours agorootparent> extremely opinionatedI have not seen a single codebase that widely uses uint8_t and does not typedef it to u8. It is the exact opposite of \"extremely opinionated\". reply loeg 8 hours agorootparentprevIt doesn&#x27;t mess with a core language feature to alias &#x27;u8&#x27; to &#x27;uint8_t&#x27;. It&#x27;s a reasonable use for the name and one used in other languages (e.g., Rust). There&#x27;s nothing in the C standard that defines or uses the &#x27;u8&#x27; name. reply Dylan16807 6 hours agorootparentprev [–] Opinionated? Was there something else you wanted u8 to mean? replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares their personal coding style for C language, highlighting changes they've made to improve productivity and overall organization.",
      "Techniques used include short names for primitive types, the usage of typedefs for improved clarity, and particular preferences for macros, parameters, functions, and strings.",
      "The author acknowledges that their approach to coding might not appeal to everyone, and they are prepared to adapt their style when contributing to other projects. Preferences for string types, structure returns, initialization assignments are also shared with illustrative examples."
    ],
    "commentSummary": [
      "The summaries provide insights on coding style, conventions, and best practices in C programming, covering aspects like uppercase/lowercase macro usage, custom types definition, naming inconsistency confusions.",
      "They debate on using structs as return types, assess pros and cons of typedefs, discuss potential risks of const and signed indexes, emphasizing the need to balance productivity and personal coding preferences.",
      "It's highlighted that opinions and preferences in these matters can vary among coding practitioners."
    ],
    "points": 316,
    "commentCount": 228,
    "retryCount": 0,
    "time": 1696811403
  },
  {
    "id": 37809276,
    "title": "Why is Debian the way it is?",
    "originLink": "https://blog.liw.fi/posts/2023/debian-reasons/",
    "originBody": "blog.liw.fi → posts → 2023 → Why is Debian the way it is? 2023-10-08 12:14 debian What Debian wants to be The constitution, power structure, governance Social contract and Debian free software guidelines Self-contained No bundled libraries Membership process Release code names Changing slowly Debian is a large, complex operating system, and a huge open source project. It’s thirty years old now. To many people, some of its aspects are weird. Most such things have a good reason, but it can be hard to find out what it is. This is an attempt to answer some such questions, without being a detailed history of the project. What Debian wants to be Debian wants to be a high-quality, secure general purpose operating system that consists only of free and open source software that runs on most kinds of computers that are in active use in the world. By general purpose I mean Debian should be suitable for most people for most purposes. There will always be situations where it’s not suitable, for whatever reason, but it’s a good goal to aim for. Some other distributions aim for specific purposes: a desktop, a server, playing games, doing scientific research, etc. It’s fine to aim to be general purpose, or specific purpose, but the choice of goal leads to different decisions along the way. For Debian, aiming to be general purpose means that Debian doesn’t choose what to package based on the purpose of the software. The only real choice Debian makes here is on whether the software is free and whether it’s plausible for Debian to maintain a high quality package. The constitution, power structure, governance Debian is one of the more explicitly democratic open source organizations. It has well-defined processes for making decisions, and elects a project leader every year. Further, the powers of the project leader are strictly constrained, and most powers usually associated with leadership are explicitly delegated to other people. The historic background for this is that the first Debian project leaders were implicitly all-powerful dictators until they chose to step down. Then one project leader went too far, and a revolt threw them out, and democracy was introduced. As part of this, the project got a formal constitution, which defines rules for the project. The reason Debian has the rules it has, is because less rules, and less bureaucracy, didn’t work for Debian earlier in its history. Social contract and Debian free software guidelines In the mid-1990s, before the term open source had been introduced, what was “free software” was defined by the Free Software Foundation, but in a way that left much to be interpreted. Debian wanted to have clearer rules, and came up with the Debian Free Software Guidelines, and made them part of its Social Contract. The social contract is Debian’s promise to itself and to the world at large about what Debian is and does. The DFSG is part of that. This is a foundation document for Debian, and changing it is intentionally made difficult in the Debian constitution. The more detailed rules have made it clearer what Debian will accept, and have simplified discussions about this. There is still a lot to discuss, of course. The DFSG was later the basis of the Open Source Definition. Self-contained Debian insists on being self-contained. Anything that is packaged in Debian, by Debian, must be built (compiled) using only dependencies in Debian. Also, everything in Debian must be built by Debian. This can cause a lot of extra work. For example, current programming language tooling often assumes it can download dependencies from online repositories at build time, and that is not acceptable to Debian. The main reason for this is that a dependency might not be available later. Debian has no control over third party package repositories, and if a package, or entire repository, goes away, it might be impossible for Debian to rebuild the package. Debian needs to rebuild to upgrade to a new compiler, to fix a security problem, to port to a new architecture, or just to make some change to the packaged software, including bug fixes. If Debian weren’t self-contained, it would be at the mercy of any of the tens of thousands of packages it has, and all their dependencies, being available when an urgent security fix needs to be released. This is not acceptable to Debian, and so Debian chooses to do the work of packaging all dependencies. That means, of course, that for Debian to package something can be a lot of work. No bundled libraries Debian avoids using copies of libraries, or other dependencies, that are bundled with the software it packages. Many upstream projects find it easier to bundle or “vendor” dependencies, but for Debian, this means that there can be many copies of some popular libraries. When there is a need to fix a security or other severe problem in such a library, Debian would have to find all copies to fix them. This can be a lot of work, and if the security problem is urgent, it wastes valuable time to have to do that. As an example: the zlib is used by a very large number of projects. By its nature, it needs to process data that may be constructed to exploit a vulnerability in the library. This has happened. At one point, Debian found dozens of bundled copies of zlib in its archive, and spent considerable effort making sure only the packaged version of zlib is used by packages in Debian. Thus, Debian chooses to do the work up front, before it’s urgent, while packaging the software, and make sure the package in Debian uses the version of the library packaged in Debian. This is not always appreciated by upstream developers, who would prefer to only have to deal with the version of the library they bundle. That’s the version they’ve verified their own software with. This sometimes leads to friction with Debian. Membership process Given the size and complexity of Debian as an operating system, and its popularity, the project needs to trust its members. This especially means trusting those who upload new packages. Because of technical limitations in Linux in the 1990s, every Debian package has full root access during its installation. In other words, every Debian developer can potentially become the root user on any machine running Debian. With tens of millions of machines running Debian, that is potentially a lot of power. Debian vets its new members in various ways. Ideally, every new member has been part of the Debian development community sufficiently long that they are known to others, and they’ve built trust within the community. The process can be quite frustrating to those wanting to join Debian, especially to someone used to a smaller open source project. Release code names Debian assigns a code name for its each major release. This was originally done to make mirroring the Debian package archive less costly. In the mid-1990s, when Debian was getting close to making its 1.0 release, code names weren’t used. Instead, the archive had a directory for each release, named after its version. Developing a new release takes a while, so the directory “1.0” was created well ahead of time. Unfortunately, a publisher of CD-ROMs, prematurely mass-produced a disc they labeled 1.0, before Debian had actually finished making 1.0. This meant that people who got the Debian 1.0 CD-ROM got something that wasn’t actually 1.0. An obvious solution to prevent this from happening again would have been to prepare the release in a directory called “1.0-not-released”, and rename the directory to “1.0” after the release was finished. However, this would’ve meant that all the mirrors would’ve had to re-download the release when the name of the directory changed. That would’ve been costly, given the massive size of Debian (hundreds of packages! tens of megabytes!). Thus, Debian chose to use code names instead. Later, the “pool” structure was added to the Debian archive. With this, the files for all releases are in the same directory tree, and metadata files specify what files belong to each release. This makes mirroring easier. It might be possible to drop the code names and stick to versions, now, but I don’t know if Debian would be interested in that. Changing slowly As implied above, Debian is huge. It’s massive. It’s enormous, It’s really not very small at all, any more. Large ships stop slowly. Large projects change slowly. Any change in Debian that affects large portion of its packages may require hundreds of volunteers to do work. That is not going to happen quickly. Sometimes the work can be done with just a small number of people, and Debian has processes to enable that. As an example, if a new version of the GNU C compiler is uploaded, the work of finding out what fixes in other packages need to be made can usually be done by a handful of people. Often a change takes time because there’s a need to build consensus, and that requires extensive discussion, which takes time and can only rarely be short-circuited. This all also means Debian developers tend to be conservative in technical decisions. They often prefer solutions that don’t require large scale changes. To comment publicly, please use this fediverse thread.",
    "commentLink": "https://news.ycombinator.com/item?id=37809276",
    "commentBody": "Why is Debian the way it is?Hacker NewspastloginWhy is Debian the way it is? (liw.fi) 285 points by brycewray 23 hours ago| hidepastfavorite169 comments rlpb 15 hours ago\"Self-contained\" and \"No bundled libraries\" are two very important concepts that a subset of our ecosystem decided was too much work. Then they re-discovered all the problems that result, and have now coined terms like \"software supply chain\" to describe them.Meanwhile Debian doesn&#x27;t suffer from any of this because it&#x27;s been doing things so as to avoid these issues all along. reply asveikau 14 hours agoparentI think either approach makes sense depending on who you are.If your goal is to distribute software across multiple distros and operating systems, bundling dependencies makes sense.If your goal is to maintain a distro, shared libraries that you can apply a security patch to exactly once is obviously better.But these are two different people with either goal. reply rlpb 12 hours agorootparentWhat about the user?If your goal is to consume software for which you need long term reliability, accepting software that bundles an unmaintainable (to you) set of dependencies does not make sense. Unless you have no better option [edit: or if you&#x27;re paying to delegate your problems to someone else I suppose].As a user, using software sources that make the same choices Debian makes is always preferable for you if that alternative is available. reply jamesgeck0 12 hours agorootparentEngineering tradeoffs are always tradeoffs. One is not strictly better than the other from the user&#x27;s perspective.Mandating shared dependencies means that Debian is often running software against a dependency version that the original author did not develop against or test against. Sometimes the Debian package is effectively a fork. This results in Debian-specific bugs which get reported upstream. Distribution-specific bugs are a crappy experience for upstream developers because it wastes their time, and it&#x27;s a crappy experience for users to be told that their software cannot be supported upstream because it&#x27;s a fork.Maintaining a huge repository of forked software is also an enormous undertaking. It&#x27;s common for Debian users to be running fairly old versions of software. This is also not ideal, particularly for desktop users who read upstream documentation and require support when entire features are missing from their antique Debian version. reply rlpb 12 hours agorootparentYou seem to assume that Debian users are hapless and are ending up in these situations by accident. That&#x27;s not true. Most users choose Debian&#x27;s model because they want to use software maintained by people who care about their use cases. They use old versions of software by choice, because they want a platform that doesn&#x27;t change under their feet. Others use Debian or something Debian-based because it is popular, but it is popular because of its quality as a direct result of making these choices, not despite them.If you&#x27;re an upstream who gets frustrated by Debian users, then it&#x27;s worth considering why they&#x27;re using Debian the first place.There are some users who don&#x27;t want this, and they tend to be the vocal minority. Debian is not the right distribution for them! reply jamesgeck0 10 hours agorootparentI was a contributor to a small Linux desktop application ages ago. We absolutely had a regular flow of hapless users who installed the Debian-provided package and reported bugs that either never existed in upstream builds or had been fixed months before. I believe the situation was that Debian had packaged an obsolete version of the software for stable because of a misunderstanding of the versioning system. When upstream discovered the mistake and contacted them, they refused to update it to a modern version. Instead, they requested that we maintain their fork and backport literal years of fixes.This situation resulted in Debian distributing a broken version of our software for several years. I did not come away with positive impressions of their packaging processes. reply rlpb 10 hours agorootparent> because of a misunderstanding of the versioning system> ...> I did not come away with positive impressions of their packaging processesI am not coming away with positive impressions of your upstream versioning or release processes :) replybscphil 14 hours agorootparentprev> If your goal is to distribute software across multiple distros and operating systems, bundling dependencies makes sense.Of course, an important \"exception to the exception\" is when you&#x27;re making software that can easily be distributed by distributions, e.g. because it&#x27;s end user software and open source.I think the optimal cases for bundled dependencies are (a) large closed source binaries that never change, like games, and (b) self-deployed software, e.g. something like a server written in Go that is compiled and maintained in its running environment by a single developer or company. reply duped 13 hours agorootparent> is when you&#x27;re making software that can easily be distributed by distributions, e.g. because it&#x27;s end user software and open source.I have trouble understanding why this is desirable for either authors or end users. Even for open source end user applications, I want the software that I&#x27;m running to be reflective of the software that was authored and not the software that some distro maintainers think it should be. reply Denvercoder9 12 hours agorootparent> I want the software that I&#x27;m running to be reflective of the software that was authoredI don&#x27;t. As an end-user, I couldn&#x27;t care less about what the author wanted, I want to run the best possible version of the software. Often that&#x27;s the version maintained by my distro, as they&#x27;ve put in the effort to make sure all the different software on my system works well together. reply Aerbil313 14 hours agorootparentprevThe endgame of dependency management is the Nix model. U believe manually packaged software repositories&#x27; era is coming to an end. reply totetsu 9 hours agorootparentprevThis is how I ended up with three separate the Gimp installs on my system. A deb a snap and a flatpack. And I still can’t get plugins to work. reply pabs3 3 hours agoparentprevThe self-contained thing isn&#x27;t always true either, for a long time the open firmware inherited from the linux-firmware repo wasn&#x27;t built from source, we just shipped the binaries. I expect there are other cases in the archive too, Debian doesn&#x27;t systematically strip generated files from all tarballs and regenerate them. Especially with more AI&#x2F;ML stuff, where we probably can&#x27;t even get the training data, let alone afford to train them. reply pabs3 4 hours agoparentprevDebian has a ton of embedded code copies, inherited from all our upstreams who bundle libraries for Windows&#x2F;macOS&#x2F;etc and also sometimes fork them etc.https:&#x2F;&#x2F;wiki.debian.org&#x2F;EmbeddedCopies reply codedokode 13 hours agoparentprevUsually developer of application tests it only with a specific version of a library. If you use another version of library, you need to carefully test it and fix all found bugs and I am not sure if Debian has resources to do it. So we can assume that they simply use untested combinations of libraries and hope that everything will be ok (it won&#x27;t). reply rlpb 12 hours agorootparentAs if this isn&#x27;t an issue with fast-moving \"let&#x27;s bundle everything\" upstream code drops either?Distribution releases have the advantage that they have a large number of followers who share the same set of versions, and so can shake out the issues and fix the bugs together. In practice I think this beats what most upstreams that each pick their own sets of versions can achieve on their own.It only takes one skilled engineer to fix any given issue in a given distribution release, even at today&#x27;s scale. That&#x27;s not a big burden, and is even available to those not skilled with a relatively inexpensive support contract.Corporate upstreams additionally tend to focus on what matters to paying customers; other use cases can often receive a \"not supported\" answer. A community of followers operating on the same set of versions can address these use cases more easily, too. reply hnfong 11 hours agorootparentNot to mention that if you run 10 apps daily and have 50 more installed, you really don&#x27;t want to be running 40 different versions of GTK&#x2F;QT&#x2F;whatever. reply bsder 10 hours agoparentprev> Meanwhile Debian doesn&#x27;t suffer from any of this because it&#x27;s been doing things so as to avoid these issues all along.Except that it&#x27;s clear that Debian is suffering from a manpower problem and has run into fundamental scaling limits with its current architecture.Thus, we&#x27;re seeing things like Nix and Silverblue at the OS level with Snaps and Flatpak at the application level.I don&#x27;t know what the solution is, but it seems to me like Debian is going to need to do something shortly. reply talkingtab 19 hours agoprevSeveral open source software organizations are remarkable. Not little remarkable, big remarkable. As in they show us how alternative models to the typical corporate business model may well be far superior as a way for people to collaborate. For the most part these remarkable are unknown. I have used Debian for (ahem) a very long time without knowing much about the organization and this article was a very good introduction.I have been aware of the IETF for quite a while. What is most amazing is that the internet today was built (more-or-less) by the IETF. See The Tao of IETF (https:&#x2F;&#x2F;www.ietf.org&#x2F;about&#x2F;participate&#x2F;tao&#x2F;). This is an organization with no members. It just works. Hardly anyone really knows about it.Just as interesting is what happened when the corporate world decided to compete with the IETF for control of how the internet worked. Some people call this the protocol wars. (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Protocol_Wars). For a while it seemed like each month the OSI would announce a project to replace parts of the internet, like TCP, with an X.protocol. Of these efforts very few survived and thrived - like X.509.The question that comes to my mind is whether these kind of democratic type collaborative organizations are in fact superior (far superior?) to the traditional corporate model. I personally have watched many corporations act with obvious stupidity. Doing things that can only be described as severely fight-their-way-out-of-a-paper-bag challenged. To put it kindly.Certainly these other-style organizations do not really stack up on an economic basis. The income of most corporations dwarfs that of both the IETF and Debian. And yet as a contributor and creator, I can ask Cuo Bono? Certainly not the contributors, they subsist.And perhaps most interesting to me, and perhaps worth an experiment, is whether it is possible to use an IETF or Debian style model that competes with the corporate model. It did work once with the Protocol Wars, so maybe.(edit to remove markdown syntax, sigh) reply zajio1am 18 hours agoparent> Just as interesting is what happened when the corporate world decided to compete with the IETF for control of how the internet worked. Some people call this the protocol wars. (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Protocol_Wars). For a while it seemed like each month the OSI would announce a project to replace parts of the internet, like TCP, with an X.protocol. Of these efforts very few survived and thrived - like X.509.I would not describe it as &#x27;corporate world decided to compete with IETF&#x27;, than &#x27;governments tried to enforce its power&#x27;. IETF working groups are often full of engineers from corporate vendors trying to collaborate to ensure interoperability, while ISO is traditional top-down governments-led organization. reply talkingtab 16 hours agorootparentYou are correct. A better way to put it. And it is true that the IETF working groups are often from corporate vendors.So perhaps that part of my argument is completely wrong. And distracts from the main question: could other fundamental models of collaboration be significantly more productive (efficient) than corporate models? reply convolvatron 15 hours agorootparentnot entirely. I was involved in the IETF in the early 90s. At that time the old guard were the sort of second wave of internet designers (Clark, Estrin, Zhang, Cerf, Deering, Jacobsen....not going to pretend to list everyone). They primary worked off of (D)ARPA grants, although some of them them did work at places like Parc, and certainly places like Cisco.during that time, alot more money was being dumped into this internet thing, and companies realized that if they could get their widget written into internet standards, it would be really good for business.partially due to that, and partially due to a largely ineffective focus on multicast protocols (PIM, RSVP, etc.), these people became less central over time, and alot of the formative protocol design activity stopped.just my perspective, but it seems odd that we&#x27;re still largely stuck in the early 90s protocol-wise. clearly there have been some changes (http3, bar), but not really much considering the relative timespans.in any case, the point being that corporate involvement in the IETF wasn&#x27;t a given in the early days, and it hasn&#x27;t been an unqualified win. reply SSLy 14 hours agorootparentproto design has been killed by incompetent enterprise firewall vendors and administrators (or, well, their managers).the stupid dance tls1.3 has to do is best case in point. reply convolvatron 13 hours agorootparentyeah. there are plenty of reasons. I do think the shift to the client(nat)&#x2F;server(default-free) model did alot of damage. ATM was also a huge suckhole that killed momentum. and I think ISP just stopped listening to what the ietf had to say for the most part. replychubot 18 hours agoprevI just switched to Debian this year after ~13 years of Ubuntu, and I really appreciate itIt grew on me after a long time. I always thought it was not the most \"technically sound\" way of doing thingsi.e. I don&#x27;t really like the packaging model of global updates where you don&#x27;t know what&#x27;s going on, and sometimes there are version conflictsBut I have come to appreciate the stability and good intentions of the Debian projectSometimes it&#x27;s not technical excellence that matters the most, but the purpose and goals of the project reply dietrichepp 17 hours agoparentYeah. I keep coming back to Debian after trying out another distro for a while.There are some specific complaints I have about technical choices for Debian, like the way daemons autostart post install. But these complaints are outweighed by the benefits of using a distro with coherence across packages and upgrades.Apt is also just such a phenomenal package manager. It is fast out of the box, and supports some relatively tricky scenarios—like using stable for your system, but a newer Nginx from backports. Feels like I can get the newer features for the one or two packages that I really care about, and then use something stable and boring for everything else. reply bostik 17 hours agorootparentI&#x27;m a Debian user since 1998 and have had it as my personal desktop since that time. I&#x27;d think that counts for something.> Apt is also just such a phenomenal package manager. It is fast out of the box,This wasn&#x27;t always the case. There&#x27;s a good reason almost all guides first written before 2015 specifically instructed everyone to use &#x27;apt-get&#x27; directly. For quite some time the more uniform &#x27;apt&#x27; frontend really wasn&#x27;t intuitive or helpful. (Just to be clear: these days it is phenomenal in its simplicity and clarity.)And as someone who has has to dive in to the package managers&#x27; code bases, the overall quality of libapt used to be .. questionable. Figuring out code and control flows back in 2010 was like trying to rub chili out of your eyes with an unsanded wooden spoon.But the sheer bullheaded stubbornness Debian imposes on their package universe and its architecture means it&#x27;s an absolute joy to work with if you&#x27;re doing any kind of distro customisation work. reply natebc 15 hours agorootparentFWIW the current recommendation is still to use apt-get in your scripts. It&#x27;s less about stability and more about intention for backwards compatibility. Perfectly fine to use apt interactively, it is as you say phenominal, simple and clear.from apt(8): SCRIPT USAGE AND DIFFERENCES FROM OTHER APT TOOLS The apt(8) commandline is designed as an end-user tool and it may change behavior between versions. While it tries not to break backward compatibility this is not guaranteed either if a change seems beneficial for interactive use. All features of apt(8) are available in dedicated APT tools like apt-get(8) and apt-cache(8) as well. apt(8) just changes the default value of some options (see apt.conf(5) and specifically the Binary scope). So you should prefer using these commands (potentially with some additional options enabled) in your scripts as they keep backward compatibility as much as possible.https:&#x2F;&#x2F;manpages.debian.org&#x2F;bookworm&#x2F;apt&#x2F;apt.8.en.html#SCRIP... reply pwdisswordfishc 14 hours agorootparentprevFor quite some time the more uniform &#x27;apt&#x27; frontend really didn&#x27;t exist at all. reply jwilk 13 hours agorootparentIndeed. The \"apt\" frontend was added in 2014 and included in a stable release in 2015.So I don&#x27;t know what tool other than apt-get could \"guides first written before 2015\" use. reply colanderman 13 hours agorootparentaptitude. And before that, dselect (which I believe predated APT entirely). reply cassianoleal 10 hours agorootparentOh my, I don&#x27;t miss the dselect days.OTOH aptitude is, to this day, a joy to use. reply jwilk 13 hours agorootparentprev> For quite some time the more uniform &#x27;apt&#x27; frontend really wasn&#x27;t intuitive or helpful.What do you mean? reply kefyras 1 hour agorootparentprev> like the way daemons autostart post installThis can be configured with service-policy.d(5), see https:&#x2F;&#x2F;packages.debian.org&#x2F;bookworm&#x2F;policy-rcd-declarative-... for example. reply chubot 17 hours agorootparentprevYup exactly, I hated the daemon autostart thing.But it&#x27;s a small issue compared to the mess I see in the rest of software these days ...Alpine Linux seems interesting too, although right now Debian suits me well. I guess the problem is that I still don&#x27;t make Debian packages myself, while Alpine&#x27;s APKBUILD seems more approachable -- pure shell, while Debian has an array of tools and formats.But Debian \"lagging\" a bit can be a feature, not necessarily a bug. reply chrisandchris 15 hours agoparentprevSame here, switched to Debian for my servers as OS from Ubuntu. Main reason: uses (boring & old) working technology. No more netplan, snapd, systemd-resolver. reply guerby 14 hours agorootparentNote: debian cloud image use netplan.io reply sgarland 12 hours agoparentprev> version conflictsWhat are you getting conflicts on? Unless you’re pulling from Sid, and did something fun like upgrading libc6, you shouldn’t see version conflicts if everything was installed via apt. reply chubot 6 hours agorootparentIt hasn&#x27;t happened to me in a long time, but when I first started using Debian&#x2F;Ubuntu I ran into it and was confusedDebian does have Conflicts package metadata - https:&#x2F;&#x2F;www.debian.org&#x2F;doc&#x2F;debian-policy&#x2F;ch-relationships.ht...So in theory I don&#x27;t like it, but I now better understand the possible reasons for it, and I haven&#x27;t run into it recentlyI think there is room for other systems that don&#x27;t have this problem, but Debian is good at what it does, and you can build other things on top of it reply dsr_ 19 hours agoprevLIW left out one major chunk: because Debian is a volunteer organization, and nobody can make a volunteer do anything that they don&#x27;t want to do. reply abdullahkhalids 15 hours agoparentFrom the description, the Debian organization seems to be an anarchist one. A bunch of people, not coerced to be there, have created a diffuse rotating democracy for making decisions. Self-sufficiency is key to the organization, that emerges from thoughtful usage of resources. reply rstuart4133 8 hours agorootparent> From the description, the Debian organization seems to be an anarchist one.Then it mislead you. Anarchist organisations aren&#x27;t typically characterised by large, long and complex sets of policies, constantly evolving, that are strongly policed. Often by bots.This is the reaction from one software engineer that stumbled into Debian infrastructure: https:&#x2F;&#x2F;lists.debian.org&#x2F;debian-devel&#x2F;2023&#x2F;09&#x2F;msg00334.htmlTo quote one part of that email: I&#x27;ve been maintaining free software for 30 years so I&#x27;ve got a lot of experience with a lot of different tools, and I&#x27;ve rarely encountered anything that is as comprehensive and well-documented as all this stuff is.This style of organisation is characteristic found in engineering organisations try to deliver high quality products, not anarchist organisations.And while it&#x27;s a flat(ish) style hierarchy, it has leaders (the DPL), a judiciary (the technical committee) and even behaviour police (whoever polices the conduct - it is policed). reply kosherhurricane 14 hours agorootparentprevWhy would you consider an organization with a constitution anarchist? reply abdullahkhalids 14 hours agorootparentContrary to popular beliefs, anarchists have no problem with constitutions or laws or governments. Anarchy is ANti-hierARCHY, which in the extreme case extends to not accepting the hierarchy of the State (i.e. an organization with a license to exert force over all others in society). But,1. Many anarchist movements do not demand this much change. They only demand removal of specific forms of hierarchies they think most problematic. The Occupy movement for instance was demanding the curtailment of the political power of the 1% over the 99%. Its always better to think of political movements as directions of evolution in political space, rather than specific destinations.2. Then how do anarchists propose that laws&#x2F;constitutions be imposed? By consensus and discussion. By making sure everyone is on board. Or by temporarily giving someone conflict resolving power (as in the Debian case). Plenty of societies and organizations operate this way, and work fine. Read The Dawn of Everything for some historical examples. See a region in Syria [1] as a modern example.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Autonomous_Administration_of_N... reply cf100clunk 19 hours agoparentprevLeft out another major chunk: the conflict that arose over the eventual systemd adoption. To me that conflict altered the concept of &#x27;&#x27;What is Debian&#x27;&#x27; permanently, for better or worse (depending on who you listen to). reply VancouverMan 18 hours agorootparentRegardless of whether one likes or dislikes systemd itself, I think that unfortunate debacle can only be seen as causing harm to the entire Debian project.The politics of it certainly generated a lot of distrust and resentment among the users and contributors. The project&#x27;s reputation was undoubtedly hurt.Perhaps most importantly where the technological impacts. It&#x27;s one thing when a user can generally ignore the politics surrounding a Linux distro, and the software still does what it needs to do. It&#x27;s another matter when one routine update after another causes their computer(s) to no longer boot, among other serious problems, all thanks to systemd. Users definitely notice incidents like that, and it decreases, or even eliminates, their trust.So much hard-earned and invaluable goodwill was unnecessarily lost during and after that period of time.If any good did arise from that situation, it was that more people became aware of the BSDs, or tried them again if they&#x27;d used them in the past. FreeBSD and OpenBSD saved users who needed the reliability and trustworthiness that Debian used to offer, before systemd negatively affected the quality of Debian. reply Aeolos 18 hours agorootparent> before systemd negatively affected the quality of DebianIs there any publication quantifying this?I followed the whole debacle with interest, and my personal experience with my servers was the exact opposite: adopting systemd improved reliability and made administration significantly easier. It&#x27;s sad that this was politicized by a small part of the community, but the end result was worth it. reply dietrichepp 17 hours agorootparentYeah, I agree.Systemd is incredibly controversial among a niche group of people who have strong opinions about how init & core system functionality should work, and then there’s an outer ring of people who focus on one or two problems with some relatively minor problems that Systemd caused for which there are viable workarounds. Like how Systemd terminates processes that belong to your session when you log out.I remember writing SysV style init scripts or rc.d &#x2F; BSD style init scripts. It was awful. You had all these copy-pasted shell scripts with various gaps in functionality depending on who wrote them. Getting a service to run in Systemd feels like heaven by comparison. I don’t even care about, like, Docker.I think the reports of problems (like background processes getting termed on logout, how any security problems in Systemd tends to be severe by nature) were just so numerous compared to the reports of the benefits (like the boot time improvements and the massive improvements running daemons). It was some bad decisions and a lot of bad PR, but the overall impact IMO is very positive. reply SoftTalker 16 hours agorootparentHave you looked at OpenBSD?Here&#x27;s the &#x2F;etc&#x2F;rc.d&#x2F;sshd: #!&#x2F;bin&#x2F;ksh # # $OpenBSD: sshd,v 1.7 2022&#x2F;08&#x2F;29 19:14:25 ajacoutot Exp $ daemon=\"&#x2F;usr&#x2F;sbin&#x2F;sshd\" . &#x2F;etc&#x2F;rc.d&#x2F;rc.subr pexp=\"sshd: ${daemon}${daemon_flags:+ ${daemon_flags}} \\[listener\\].*\" rc_configtest() { ${daemon} ${daemon_flags} -t } rc_cmd $1Pretty much any service&#x2F;daemon is similar. You define a few things and you&#x27;re done. reply Aeolos 15 hours agorootparentHere is the systemd unit for comparison: [Unit] Description=OpenBSD Secure Shell server per-connection daemon After=auditd.service [Service] EnvironmentFile=&#x2F;etc&#x2F;default&#x2F;ssh ExecStart=&#x2F;usr&#x2F;sbin&#x2F;sshd -i $SSHD_OPTS StandardInput=socketI leave it to the reader to decide which of those two is easier to understand and maintain. reply SoftTalker 15 hours agorootparentDebatable, but I was mainly reacting to \"... rc.d &#x2F; BSD style init scripts. It was awful. You had all these copy-pasted shell scripts with various gaps in functionality depending on who wrote them.\"Simply not true in (modern) BSDs, well at least OpenBSD; I&#x27;m not familiar with the others. reply dietrichepp 6 hours agorootparentYeah, maybe it’s not inconsistent any more. But meanwhile, systemd has gotten really nice, even a good version of rc.d seems awful by comparison.I had the same feeling when Apple came out with launchd in 2005. It felt like such a massive improvement over the existing state of things. Systemd also feels like a massive improvement. reply abenga 16 hours agorootparentprevA declarative unit file is still more readable for me. reply unixhero 16 hours agorootparentprevWhy in the world korn shell?I stumbled upon an auditor with programming skills, who did all his scripting in Korn shell. To me it was like he was from another planet. reply SoftTalker 15 hours agorootparentIt&#x27;s the standard shell on OpenBSD. reply VancouverMan 17 hours agorootparentprevWhen I was trying to resolve the numerous problems that systemd was causing for me on multiple computers that Debian had previously worked perfectly fine on, I certainly ran across a lot of bug reports, mailing list postings, forum postings, IRC logs, blog articles, and other online communications from people who were also having problems with systemd.Beyond that, I&#x27;ve heard of enough problems involving systemd from my Debian-using colleagues and acquaintances, too.I don&#x27;t know if it&#x27;s been formally studied in any way, but it was clear to me that I definitely wasn&#x27;t alone in experiencing problems involving systemd.The widespread negative sentiment that exists toward systemd, including from well beyond the Debian community, didn&#x27;t just come out of nowhere.From what I can see, it was generated thanks to a lot of people directly experiencing a lot of unnecessary problems caused by systemd. reply klysm 15 hours agorootparentprevI’m pretty convinced it was net positive in the long term. If Debian hadn’t gone with systemd, then where would it be today? Stuck in shell script hell? reply gorgoiler 14 hours agorootparentWe would have been forced to bite the bullet and implement a packaging extension that allowed debs to describe what to start at boot time, when, and how. The compatability layer would have allowed any init system to be used.Instead we welded the systemd engine into the chassis and pray that when it comes to replacing it we aren’t the ones on the hook. reply klysm 13 hours agorootparent> The compatability layer would have allowed any init system to be used.I don’t see how such a compatibility layer would work in a way that doesn’t suck horribly. reply nolist_policy 17 hours agorootparentprevSigh. reply tmtvl 16 hours agorootparentprevFor anyone who hasn&#x27;t yet seen it, there was an interesting adaptation of the Debian systemd discussion into an Ace Attorney-style format:reply loxias 17 hours agorootparentprevYeah, it was annoying for a bit of time, but not anymore! I haven&#x27;t had systemd installed in quite some time. sysvinit works without a hitch on bullseye and bookworm.added: Debian&#x27;s (to me) about (among other things) technical superiority, a robust packaging system, as well as user freedom and choice. It&#x27;s super easy to not use systemd these days, \"what is debian\" didn&#x27;t change, there was just a slight delay in reality catching up to principles. :) reply g232089 17 hours agoparentprevYour comment makes it sound like volunteers have a great degree of freedom and that&#x27;s not the case because, obviously, in these organisations you will be shown the door if you don&#x27;t do what others tell you to do. reply larme 17 hours agoprevSometimes I daydream about getting a fuck-it amount of money.During this thought process I always make a plan of what open source software project I should donate, and debian is always one of the first several candidates.Now I just need the money! (meanwhile I donate to debian anyway) reply tarruda 17 hours agoparentThis article from 2020 says Debian doesn&#x27;t need money: https:&#x2F;&#x2F;www.theregister.com&#x2F;2020&#x2F;09&#x2F;10&#x2F;debian_project_addres... reply samueloph 15 hours agorootparentDebian can&#x27;t really directly pay contributors (there are some rare few cases like lawyers, etc....), so that would be one of the reasons for what the article is talking about.The best thing someone could do in this scenario would to be hire someone to work on&#x2F;improve Debian directly. reply shiroiuma 38 minutes agorootparentBut what improvement does Debian really need, beyond the capabilities it already has?Linux in general could use various improvements, if some billionaire decided to fund people to work on it. But I don&#x27;t really see how any distro could use the lion&#x27;s share of that funding. Instead, much of it probably needs to go to infrastructure things like drivers, Wayland, etc., so that Linux works better on people&#x27;s computers. Other things that could use development funding are various applications. But these are things that all distros share. reply humanrebar 14 hours agorootparentprevSomeone could form a nonprofit org that funds packaging work in Debian. Or maybe even a for-profit one. I&#x27;m pretty sure a lot of big consumers would rather pay for expertise instead of having an in-house Debian \"upstream\" team. reply Karellen 13 hours agorootparenthttps:&#x2F;&#x2F;www.debian.org&#x2F;donations> The easiest method of donating to Debian is via PayPal to Software in the Public Interest, a non-profit organization that holds assets in trust for Debian.https:&#x2F;&#x2F;www.spi-inc.org&#x2F;> Software in the Public Interest (SPI) is a non-profit corporation registered in the state of New York founded to act as a fiscal sponsor for organizations that develop open source software and hardware. Our mission is to help substantial and significant open source projectsEdit: But also, freexianhttps:&#x2F;&#x2F;www.freexian.com&#x2F;lts&#x2F;debian&#x2F;> To achieve the 5 years of support, and properly cover all Debian packages, Freexian organizes a corporate sponsorship campaign with the goal of funding the work of multiple Debian contributors who are established as independent workers.> If you are not yet convinced, here are seven reasons why you should help fund the Debian Long Term Support initiative (LTS): reply IshKebab 15 hours agoparentprevMe too, but I&#x27;d do my own projects rather than donate. Much more fun! reply talent_deprived 17 hours agoprevDebian could be great except for driver support which they only tacitly acknowledge:https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;debian&#x2F;comments&#x2F;paxj85&#x2F;why_debian_w...\"We acknowledge that some of our users require the use of programs that don&#x27;t conform to the Debian Free Software Guidelines. We have created \"contrib\" and \"non-free\" areas in our FTP archive for this software.\"I had it running on a couple of my machines about 1 or 2 years ago and an update came in for WiFi that bricked them. I started looking into rolling back or whatever and just decided to switch those to Ubuntu (or Kubuntu actually) and they work great and have has no issues. reply dartharva 16 hours agoparentHow are they \"only tacitly\" acknowledging it? Looks like they have put in very tangible solutions in place already.Debian 12 even made a dedicated non-free-firmware repo for free software purists who would like to concede having non-free drivers just so they can use their hardware. reply Nextgrid 17 hours agoparentprevThey&#x27;ve now relaxed their (stupid) policy so at least the default ISO includes non-free drivers.When it comes to an already installed system, enabling the non-free repos and installing linux-firmware (or more specific firmware-* package for your hardware) should fix it. reply dartharva 16 hours agorootparentIf the default ISO already included non-free drivers, why would you have to separately enable the non-free repos to get firmware?My Debian 12 install didn&#x27;t come with proprietary Nvidia drivers, nor did it ask me if I wanted them during installation. I had to enable the non-free-firmware repo to get them. reply Nextgrid 16 hours agorootparent> If the default ISO already included non-free drivers, why would you have to separately enable the non-free repos to get firmware?I&#x27;m not 100% sure about this but I believe it may enable it for you automatically if non-free firmware was used during the install. I mentioned it just in case.> My Debian 12 install didn&#x27;t come with proprietary Nvidia driversThe primary problem of the previous non-free driver policy is the lack of network drivers which make it impossible to install nor download the drivers even if you somehow managed to install the OS. This is now resolved.It&#x27;s not a big deal if the ISO doesn&#x27;t include every non-free driver out there as long as you can manually install it after the fact. reply rascul 11 hours agorootparentprevThe default ISO didn&#x27;t come with non-free drivers until the latest release (12, bookworm). Unless you used the \"unofficial\" image which did include them. Now the situation is different.Here is the relevant part from the release notes:> In most cases firmware is non-free according to the criteria used by the Debian GNU&#x2F;Linux project and thus cannot be included in the main distribution. If the device driver itself is included in the distribution and if Debian GNU&#x2F;Linux legally can distribute the firmware, it will often be available as a separate package from the non-free-firmware section of the archive (prior to Debian GNU&#x2F;Linux 12.0: from the non-free section).> However, this does not mean that such hardware cannot be used during installation. Starting with Debian GNU&#x2F;Linux 12.0, following the 2022 General Resolution about non-free firmware, official installation images can include non-free firmware packages. By default, debian-installer will detect required firmware (based on kernel logs and modalias information), and install the relevant packages if they are found on an installation medium (e.g. on the netinst). The package manager gets automatically configured with the matching components so that those packages get security updates. This usually means that the non-free-firmware component gets enabled, in addition to main.https:&#x2F;&#x2F;www.debian.org&#x2F;releases&#x2F;bookworm&#x2F;amd64&#x2F;ch02s02.en.ht... reply FuriouslyAdrift 19 hours agoprevI worked with Ian Murdock at Purdue in the days of the very first release. He was a sysadmin and devloper while I was a web designer for the libraries.The guy truly believed in the GNU&#x2F;Linux &#x27;way&#x27; and &#x27;free as in speech&#x27; software. His initial drive was from the difficulty of packaging and package management and that is probably his biggest contribution. Network-of-Workstations (NOW... think peer-to-peer infratsructure) was his passion that he really never quite got going.Bruce Perens, the guy he handed control over to, is the authoritarian leader being refered to. I like the guy. He&#x27;s definitely in the old guard, aka Linus Torvalds, style of management. In big complex projects with volunteers that syle works.Anyways, the old days of Linux and Debian were a blast. I never quite go tinto like all these other people, but I miss those old days.There&#x27;s way too much money people involved today. So it goes.Ian&#x27;s manifesto explains it all, anyways.https:&#x2F;&#x2F;www.debian.org&#x2F;doc&#x2F;manuals&#x2F;project-history&#x2F;manifesto... reply gautamcgoel 15 hours agoparentCan someone explain the controversy surrounding Bruce Perens? I never heard the story and Google isn&#x27;t being helpful. reply ploum 12 hours agorootparentI’m also interested by any source about that. I’m reading a lot about open source history and can’t find anything about that story (which seems quite important for those who want to understand Debian history) reply Maken 17 hours agoparentprevThe hell happened with Murdock after Debian? His trajectory since he stepped down until his death seems quite erratic. reply layer8 17 hours agorootparentHow so? https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ian_Murdock#Life_and_career reply aleph_minus_one 17 hours agorootparentSee the next section:> https:&#x2F;&#x2F;en.wikipedia.org&#x2F;w&#x2F;index.php?title=Ian_Murdock&oldid... reply layer8 17 hours agorootparentI’m aware of the circumstances of his death. That doesn’t say anything about his trajectory since leaving Debian. reply BruceEel 18 hours agoparentprevThank you for sharing this, it all rings so true. Love Debian and still use it. reply pjmlp 16 hours agoparentprevWith many other non-copyleft alternatives shaping up, and systems like ChromeOS and Android, with the Linux kernel and completely unrelated userspace, I firmly believe when our generation is gone, Linux won&#x27;t stay around on its present form for much longer. reply mfuzzey 14 hours agorootparentBut neither Android nor ChromeOS are self hosting, in the sense that you can&#x27;t use Android to build Android nor ChromeOS to build ChromeOS (well for the later maybe you could with a Debian container...)So I think traditional Linux distributions will remain, at least as development tools.Of course it is true that many end users these days get by with just a phone or a tablet but this is a general thing and also results in less Windows users too. reply pjmlp 14 hours agorootparentA matter of improving existing toolchains.Also my point is about Linux kernel, being as relevant as AT&T UNIX, after the generation that created it is no longer among us. reply hedora 16 hours agorootparentprevI wonder if the end-game is a BSD, or some sort of hard fork of the Linux ecosystem.Ubuntu and RedHat basically don’t work by any of my definitions of “work”.They’re both enterprisey and bloated and flaky in all the ways Windows was in the 90’s, except they add flatpack&#x2F;snap, letting each program be its own flaky OS install, compounding the problem. Want to save a file to ~? Read this 1000 page tome on the 21 successors to SEL first.Anyway, my current heuristic is that if it defaults to systemd or wayland, then I don’t want to use it.Debian was never the default for big sprawling corporations, so it’s not clear to me that just staying on the “suckless ethos” side of such an ecosystem fork would be that bad vs. Linux in its previous heyday. reply starttoaster 10 hours agorootparentPerhaps that is the end game for you. My intent isn’t to say this to stir heated discussion but people have different opinions on flatpak and snap, in fact I even like snap besides the fact that you have to use snap’s store (you can’t just start your own snap repository and configure your local snapd to install from it.) No complaints with systemd, and you can still easily swap to Xorg. No idea what you’re referring to with the trouble of saving a file to ~, perhaps that’s a RHEL thing (it’s been so long since I’ve used RHEL.)All in all, my intent is mostly just to say we’re not in consensus. I doubt a massive revolution is coming if your assumption is that it is because there’s some overwhelming majority with your opinions. reply lionkor 17 hours agoparentprevIts rare that we read a piece of content on the internet from 1994! Thanks for sharing this, its older than me. reply happytiger 19 hours agoprevDebian is Toyota. Reliable but boring. Except it’s also built by volunteers. reply arun-mani-j 17 hours agoprevI personally love and use Debian exactly for its principles and stability.I have heard users of other distros and a few upstream complaint that Debian \"modifies\" their packages?Is it so? If yes, there surely must be a good reason. Can someone tell me about it? reply alphager 14 hours agoparentThere&#x27;s mainly three kinds of patches:* make the software behave like Debian needs it (configuration is stored somewhere in &#x2F;etc&#x2F;, no additional downloads at runtime, use the system libraries instead of vendored ones) * security backports. Debian freezes the functionality at release and only provides security updates. Many software nowadays just includes security fixes in new releases bundled with new functionality.In combination these two kinds of patches lead to growing differences between a Debian released version 1.2 and the \"real\" 1.2, making it harder to handle bug reports (e.g. you get a bug report for version 1.2-Debian, but only support the \"real\" version 1.4 with a whole set of updated libraries).The third kind of patch has mostly gone out of style; it&#x27;s when Debian thinks they can improve the software. That lead to things like removing randomness from SSH keys: https:&#x2F;&#x2F;github.com&#x2F;g0tmi1k&#x2F;debian-ssh reply rlpb 11 hours agorootparentAnother kind of patch is when a common dependency library is being updated, and laggard upstreams need patching to make their current releases work against the newer library; it&#x27;s either that or a Debian release with those packages missing.This type is actually really common. Debian packages something like 30k upstreams, and so some are always behind. reply c-hendricks 8 hours agorootparentDo they just patch the package in Debian or do they submit them upstream? reply pabs3 3 hours agorootparentUsually both, submit upstream and add the patch to Debian while waiting for upstream to accept the patch. reply happymellon 2 hours agoparentprevThere are two main ways of developing your software.1. Incremental versions. Think like Chrome, there are not bug fixes, just new versions that may contain bug fixes.2. Major versions, where you&#x27;ll end up with semver style versioning. You&#x27;ll have version 1 and version 2, but you&#x27;ll also get version 1.1 released after 2 as it is the same as version 1, but with just a bugfix applied.Debian essentially will only work with the second methodology, as they are API stable, which makes software developed by the first method incompatible.To work around this Debian will backport \"fixes\" from version 3 to version 1, and create their own version 1.debian-2. The problem here is that people will now raise bugs with upstream on behaviour that was never released.Yes Debian is allowed to do this, but upstream is also allowed to be unhappy with the additional workload that Debian puts on them. reply rlpb 15 hours agoparentprevDebian has a user-first philosophy as well as a focus on integration between packages. When required, that means patching upstreams that don&#x27;t meet that expectation for whatever reason. The ability to do this is precisely the point of Free Software. reply mcculley 12 hours agoparentprevThis is not always good. See https:&#x2F;&#x2F;www.debian.org&#x2F;security&#x2F;2008&#x2F;dsa-1571 reply klysm 15 hours agoparentprevApplying patches increases the cost of making changes. reply Dwedit 16 hours agoprevDebian&#x27;s policies also led to a heavily restricted version of RetroArch being available instead of the real version. Specifically, RetroArch has its own package management functionality built-in through its \"Core Updater\" feature, which downloads and installs emulators in the form of library files. This is banned by Debian because it sidesteps the whole package manager system.Meanwhile, you can still build the full version of RetroArch from source code by installing the dependencies of Debian&#x27;s source package, but building the original source code instead. reply cogman10 16 hours agoparentDebian is somewhat of a bad fit for a media center PC. I&#x27;ve learned that the hard way trying to get Kodi and retro arch working.It&#x27;s otherwise a great os. reply csande17 12 hours agoparentprevWeird that that was considered an issue while KDE Discover and snapd will both happily install stuff from third-party \"stores\" by default on Debian. reply LeoPanthera 16 hours agoparentprevRetroArch provides a Flatpak, making this mostly a non-issue. reply Barrin92 13 hours agorootparentno, from a distribution standpoint it&#x27;s a pretty grave issue. As the name suggests distributions distribute, and if we start to unbundle the OS from the application layer, Debian loses the very thing people chose it for.It&#x27;s in a sense like a legacy carmaker or newsroom being more concerned with its own control than with the product. Doesn&#x27;t end well over the long term. reply LeoPanthera 11 hours agorootparent> Debian loses the very thing people chose it forI don&#x27;t buy this. People don&#x27;t choose Debian for the third party software in the repo. If they did, it&#x27;s a bad choice.People choose Debian as a rock solid and stable base OS, and it&#x27;s perfectly rational to use it as a base for third party software on top from other sources. reply Barrin92 11 hours agorootparent>If they did, it&#x27;s a bad choice.It&#x27;s an excellent choice. Debian provides about 60k software packages compared to say 15k in the Fedora repos. Debian and derivatives vastly outnumber other distributions in terms of available software, it&#x27;s what drove Ubuntu&#x27;s popularity, everything&#x27;s available on it.You can use it as a stable base OS, but it doesn&#x27;t have any particular advantage, and even some disadvantages compared to RedHat or Suse distributions which offer you many more enterprise tools like Yast out of the box, package managers that are able to do atomic and reversible transactions, which apt still does not do, and so on. replycswhnjidd 18 hours agoprevTIL Debian is mostly packaging.Love it, Debian is amazing. reply RetroTechie 17 hours agoparentPackaging, and more importantly: package dependencies, are (imho) the essence of building a successful distro.Get this wrong, stuff breaks regularly, and one ad-hoc fix follows another, forever.Get this right, and everything Just Works™ (generally).Debian is very good in this regard (along with the BSDs, I&#x27;d say). reply klysm 15 hours agorootparentDebian is very much one ad hoc fix after another, to an incredible degree really. Go dive into the source and patches and you’ll see how much duct tape there really is holding things together. What’s so great about Debian though is it effectively hides all that pain from you (most of the time). reply jowea 13 hours agoparentprevYeah, that&#x27;s 90+% of what a Linux distro is, packaging software from a wide variety of projects and trying to make a coherent whole. In Windows&#x2F;MacOS we have one company making the kernel, basic libraries, desktop manager and some applications, and separate independent software developers making and packaging their own applications (although that changed a bit in the last decade or so). reply aiunboxed 19 hours agoprev> The historic background for this is that the first Debian project leaders were implicitly all-powerful dictators until they chose to step downWhat was this about ? reply Brian_K_White 18 hours agoparentIt simply means what it says. The first few leaders were simply in charge of everything like an owner, they made all major decisions themselves and told everyone else what the plan was, and each one did that job until they decided they didn&#x27;t want to do it any longer and handed it off to the next leader. They were a dictator only in the literal sense that they dictated, not that they were tyrants.They weren&#x27;t literally an owner. That&#x27;s why the \"implicitly\". Everyone was still only volunteers. But everyone volunteerily let them call all the shots.Then later they developed a formal democratic structure and the leader is more of a coordinator than boss. reply qaisjp 8 hours agorootparentBenevolent dictators reply alexwasserman 19 hours agoparentprevI assume the Ian Murdock (founder and Ian in DebIAN) transition to Bruce Perens, then Ian Jackson then annual project managers. reply barumrho 14 hours agoprevI used to choose other distros for more updated dependencies, but now I appreciate the stability at OS level a lot more. Containers also solves the problem for running services. reply klysm 15 hours agoprevI think the next generation of immutable distros is going to eventually make Debian effectively obsolete for many service workloads. reply FpUser 17 hours agoprev>\"Self-contained\"I love that whole paragraph. And in general prefer their philosophy. reply pabs3 3 hours agoparentDebian doesn&#x27;t go far enough on that point, the folks at GNU Guix and Bootstrappable Builds are getting to the point where they can build the entire distro from source starting with only ~500B of manually written machine code.https:&#x2F;&#x2F;bootstrappable.org&#x2F; https:&#x2F;&#x2F;guix.gnu.org&#x2F;en&#x2F;blog&#x2F;2023&#x2F;the-full-source-bootstrap-... reply fsflover 19 hours agoprev> what was “free software” was defined by the Free Software Foundation, but in a way that left much to be interpretedI don&#x27;t understand what the author means here. What is unclear about the four freedoms? To me, Debian&#x27;s definition looks redundant. reply Brian_K_White 18 hours agoparentIt&#x27;s like the US constitution and the bill of rights. Everything in the bill of rights is technically redundant and covered by principles already expressed in general form in the main constitution.And yet that isn&#x27;t actually good enough. Those general principles, being general rather than specific, require the key word, interpretation, in each new specific context. And different people with different goals can and do always ALWAYS warp interpretation in infinite ways that are all perfectly reasonable sounding on their face, and yet someone else can always produce a totally different interpretation, which also holds together. reply mcpackieh 17 hours agorootparentCan you give any examples of software projects or licenses in which there&#x27;s room for genuine disagreement in interpretation of these rules? I&#x27;m having trouble imagining it. reply mjw1007 16 hours agorootparentDoes a licence that says \"you must distribute the source unmodified, but you&#x27;re allowed to distribute patches with it and a build system that applies them\" count as free?The DFSG said yes.Does a licence says \"you may modify this and redistribute it as much as you like, but if you put it on a CD then everything else on there must also be free\" count as free?The DFSG said no. reply reactordev 18 hours agoparentprevYou would have to compare them at the time. Things have changed. At the time Ian felt it didn’t do enough. Revisions and decades later it’s basically parity. reply tbrownaw 14 hours agoparentprevConsidering how the FSF&#x27;s own AGPL conflicts with freedom #1, they clearly can&#x27;t be that clear. reply 16 hours agoprevnext [3 more] [dead] natebc 16 hours agoparentEncourage those sane companies to donate to Debian as well. The more financial stability Debian has the better off we&#x27;ll all be.https:&#x2F;&#x2F;www.debian.org&#x2F;donations.html reply klysm 15 hours agoparentprevI agree it’s probably the most reasonable choice for most, but I’m hoping that changes. I want more immutable infrastructure than what Debian can provide reply badrabbit 18 hours agoprevnext [8 more] [flagged] Karellen 17 hours agoparentThere&#x27;s an \"anti-sysv-init train\"?`sysvinit` is still available in the current version of Debian, Bookworm, released less than 4 months ago:https:&#x2F;&#x2F;tracker.debian.org&#x2F;pkg&#x2F;sysvinitNote that `runit` is also available:https:&#x2F;&#x2F;tracker.debian.org&#x2F;pkg&#x2F;runitAlso, while it&#x27;s not the default init system, there are instructions for setting it as the active init system, on the Debian wiki:https:&#x2F;&#x2F;wiki.debian.org&#x2F;InitNote that while packages aren&#x27;t forced to provide native sysv init scripts (or native runit init scripts either), I hear that sysv init scripts are just shell scripts that are incredibly easy to write even for inexperienced sysadmins (much easier than writing systemd unit files apparently?) so cobbling together any that are missing for a sysv-based system shouldn&#x27;t be much work. reply badrabbit 15 hours agorootparentA .deb package being available is not the same as init scripts maintained or being allowed to select an init system during install.Sysv init scripts for every package already existed since that was the only init system in use by debian until 2014, people volunteered to keep maintaining those but they were overriden in lieu if systemd only approach.It seems you didn&#x27;t use pre-systemd linux. It felt like you controlled the whole OS. Linux has always been about giving control to users. Systemd works great but it&#x27;s geared towards corporate users who want better managability. So in a way, by violating the unix modularity philosophy, major distros sold out to corporations. Now systemd manages not just init but dns, logging, cron type scheduled jobs, fs mounting, system time,etc... it forced architectural changes where either you accepted systemd way or the unix way. And a lot of projects caved in.Openrc and sysvinit manage init scripts&#x2F;services. That&#x27;s it. You know how they work and they are designed to be compatible with anything else. If you need a schedluled managed for example, sysvinit or openrc have no opinions how to do that, you can use cron or your own thing, you have full control. Systemd on the other hand has timers, they work great, but guess what they don&#x27;t play nice with? Openrc and sysv init. Guess what plays nice with all 3? Cron and it&#x27;s many implementations.The modular design of Linux gave users power. The centralized opinionated systemd design gives the few \"elite\" influential people who work for big corporations power and control because their design does not take into account interoperability with arbitrary services. reply bee_rider 16 hours agoparentprevI think it was why they got on the systemD train. It was the init system they thought they could maintain well.Sounds like there were lots of hurt feeling in the whole situation. Did some major Debian maintainers actually leave Debian for Devuan? If Devuan’s contributors were originally just Debian users, it seems like no loss for Debian and a win for the community (more developers). reply badrabbit 15 hours agorootparentThey could have permitted volunteers to maintain sysv or openrc versions of init scripts and made it an installer option. I believe devs and users alike were part of the devuan fork: https:&#x2F;&#x2F;www.devuan.org&#x2F;os&#x2F;announce&#x2F; reply jwilk 16 hours agoparentprev> the leadership teamThere&#x27;s no such thing in Debian. reply badrabbit 15 hours agorootparentWhether it was formal or not, since the decision was not made by a vote of contributors, there is such a thing, even if it is informal. reply pabs3 6 hours agorootparentThere was a vote about systemd in 2019:https:&#x2F;&#x2F;www.debian.org&#x2F;vote&#x2F;2019&#x2F;vote_002 reply zee2345 19 hours agoprevnext [3 more] [flagged] cswhnjidd 18 hours agoparentBecause real diversity comes from people who look different. reply Iwan-Zotow 18 hours agoparentprevPaint yourself purple and join Debian - easy... reply bfrog 19 hours agoprevWell certainly part of it is much of the .deb ecosystem still feels like its stuck in the late 90s linux era to me. But maybe I&#x27;m alone on that gut feel. reply switch007 17 hours agoparentDeb packages just need reinventing by some younger people.I’m thinking: implemented in nodejs with a cli with emojis and animations.The GitHub read me should have a minimum of 80 emojis and a meme or two.The core dependency of the cli should be a 6 month old framework with 92 commits. When that library reaches 1 year old, it should be swapped out for something newer as the sole maintainer will have left their job and&#x2F;or got bored with the project having spent so long on it reply klysm 14 hours agorootparentExcellent straw man takedown reply cswhnjidd 18 hours agoparentprevBetter being in the 90s than using snappacks or whatever. reply hyperman1 17 hours agoparentprevI&#x27;d like you to explain this a bit deeper. As a user from the &#x27;90s I am quite happy with it, but I might be stuck in my ways, who knows? So not saying you&#x27;re wrong, but what are we missing exactly? reply klysm 14 hours agorootparentA few of my gripes:* creating Debian packages is a difficult process to learn. There’s a bunch of tools and wrappers around those tools to handle inadequacies and it’s not clear how to do it “right”. For example, if I just have a binary and want to put it into a Debian package without pulling in some random shell scripts that aren’t part of Debian proper, it’s not immediately clear how to do that.* Non-atomic and imperative install&#x2F;remove&#x2F;etc. hooks make it difficult to robustly handle failures. Since there is no file system manifest or sandboxing, there’s no real guarantees that removing a package will actually remove it.* there’s no spec for a .deb package. This means the only way to correctly generate a Debian package is through the difficult tooling. I understand why this choice is made, and it has a lot of benefits for the continued evolution of the debian project. However, it makes it difficult to build better tooling around deb packages. reply pabs3 3 hours agorootparentIf you \"just have a binary\" then you aren&#x27;t compiling from source, which means you can&#x27;t make modifications to that binary easily, which Debian wants to be able to do to fix security issues and other bugs.There is a large range of documentation, everything from the simple to the more advanced to different packaging niches. The large amount of docs makes that harder to navigate though, and a lot of stuff is moving towards automated packaging these days anyway.https:&#x2F;&#x2F;wiki.debian.org&#x2F;AutomaticPackagingTools reply pabs3 3 hours agorootparentprevThe spec for .deb packages is in the deb(5) manual page:https:&#x2F;&#x2F;manpages.debian.org&#x2F;bookworm&#x2F;dpkg-dev&#x2F;deb.5.en.html reply em-bee 16 hours agorootparentprevwhat deb and rpm and any similar packaging are missing is true version control of the whole packaging ecosystem. at present it is difficult to track which combination of package versions has been tested, and you can&#x27;t easily roll back to a tested combination. the current systems assume that versioning is linear and that a newer version of any package is always better than an older version. downgrading any package so that every user of the distribution can benefit from the downgrade is difficult and confusing.there are distributions that provide rollback. but as far as i can tell they require you to keep the old version to roll back to, stored on your computer. you can&#x27;t roll back otherwise.i really want this to work like revision systems for code, where i can just checkout any old version that was committed.another feature that i would like to see everywhere is stickiness of the packaging source. currently, if i include additional repos they override the main repo, such that always the newest version is picked from any repo.this makes it difficult to include less trusted 3rd party repos. i would like to be able to add 3rd party repos such that only the packages that i explicitly install from that repo will also be updated from that repo, while any other packages in that repo will be ignored unless no other repo has them.in debian it is possible to set priorities for different repos, but that is not easy to manage. the priorities to have each package update stick to the original repo should be default.guix and nix do provide some of this as far as i can tell, but i am not a fan of keeping every package self contained with massive link trees. (i may change my mind on that some time maybe, but that&#x27;s what i feel for now)conary was&#x2F;(is?) a packaging system that did have both of these features, although, according to some of the developers the repository was a bit clunky and could have been better. but that was under the hood, not noticeable to users and packagers. i loved working with it and i wish foresight, the distribution using it had become more popular so that it would have had the manpower to keep going. reply pabs3 3 hours agorootparentWith snapshot.debian.org you can roll back to any version of any package that has been saved. Using that you can even bisect where issues started.https:&#x2F;&#x2F;manpages.debian.org&#x2F;testing&#x2F;devscripts&#x2F;debbisect.1.e... https:&#x2F;&#x2F;wiki.debian.org&#x2F;BisectDebian reply pabs3 3 hours agorootparentprevYou can use pinning to achieve what you want with 3rd-party repos, using the origin feature.https:&#x2F;&#x2F;wiki.debian.org&#x2F;AptConfiguration#apt_preferences_.28... reply reactordev 18 hours agoparentprevYes but at least I don’t see ads on my Operating System for Viagra. I’ll take 90s boring linux over windoze everyday of the week. reply __float 19 hours agoparentprev\"stuck in the late 90s\" in what sense? Building them? Distributing them? (Dependencies?) reply gorjusborg 18 hours agorootparentThat they work reliably? reply bfrog 14 hours agorootparentThey work reliably with a lot of unseen work. Debian is a great distro as a user. The .deb packaging tools I found to be a huge hassle and made packaging up missing or updated or custom software a pain in the ass. reply klysm 14 hours agorootparentExactly, this is why it takes literally years of human effort to get a new version released reply bfrog 14 hours agorootparentprevHave you built a .deb with debuild and debhelpers? Can you recall all the goofy rules about what needs to go in each file and what each debhelper does? How do you keep all of it in version control and build downstream packages that depend on the changed one? Have you managed to accidently sneak a local build machine dependency or piece of information by accident into the final .deb only to realize it much later?I found it be a big ball of easily forgettable twine of rules tools and oddities myself that every time I needed to redo something was a hassle. With many foot guns that lead to odd package issues. reply klysm 14 hours agorootparentCompletely agree with your assessment of the building process. The majority of the complexity involved in the process is not inherent reply Zetobal 19 hours agorootparentprevI would say the process on getting \"maintainer\" status. reply Narann 17 hours agoparentprev> the .deb ecosystem still feels like its stuck in the late 90s linux era to me.Can you elaborate, please? What do you mean by this? reply klysm 14 hours agorootparentQuirky, clunky tooling with a bunch of arbitrary shit you have to memorize to use correctly. People enjoy that kind of thing after they learn it. reply imp0cat 13 hours agorootparentEnjoy? No. But yeah, it gets easier once you&#x27;ve done it a few times. reply klysm 12 hours agorootparentNot really enjoy doing, but enjoy knowing arcane shit even if it doesn&#x27;t make sense. replycodedokode 13 hours agoprev [–] What I don&#x27;t like in Debian:- 3rd-party software is not welcome; there is no mechanism for installing it securely because you are supposed to either install software from official repository or compile what you have written yourself. For example, if you want to install Sublime Text, or VS Code, there is no way to do it securely, without giving untrusted software access to your browser history and SSH keys. Of course, you can ignore security and run sudo curl http:&#x2F;&#x2F;script , but it doesn&#x27;t guarantee that the installer won&#x27;t break something. It is like we are back in 95 when every second program would replace system DLLs in Windows folder and break other software.- there are third-party repositories, but they can cause conflicts and you better not use them, but there is no other way to install third-party software.Third-party software is very important, I install OS to run it, and it surpises me that Linux is so unfriendly to third-party software, including closed-source software and doesn&#x27;t provide means to install and run it securely and reliably and without making developers adapt it to every existing distribution.- their bugtracker is email-based and as I don&#x27;t use email it is completely alien to me. But maybe this is not bad because it stops most of people from posting bugs and saves time to reply to them.I also tried Fedora, and here is what I don&#x27;t like:- they release a new version every 6 or 12 months and it is incompatible with older version, and you have to use a very weird way to upgrade: first, you need to install non-standard plugin (dnf-plugin-system-upgrade), then you need to download packages, then reboot into a temporary OS, then if everything is ok, it will create a new OS, and reboot into it. It looks complicated, easy to break and probably requires a lot of disk space, while Debian can upgrade everything in place.- if a system component like Gnome is crashing, there will be neither log records nor crash dumps and you will never figure out why it has crashedAlso, APT is buggy when dealing with mixed 32-bit&#x2F;64-bit packages: I wanted to install a package once and it suggested to delete half of the system to do it; luckily I have noticed that the package list is too long before agreeing. Why would package manager delete packages when I ask to install something, I don&#x27;t understand. As a bugtracker requires using email, I didn&#x27;t report it, and it would be difficult to reproduce this anyway. reply gsliepen 13 hours agoparentAlmost all software that Debian packages is 3rd-party. The issue is usually that software like Sublime Text or VS Code is non-free. That is not in itself an impediment for being packaged; after all there is the \"non-free\" section of the archive. However, often non-free software is also not free to be distributed by third parties. Thus, Debian would break the law if they did.You don&#x27;t need to adapt your software that much to have it run on Linux distributions; there are standards that the distributions implement that you can rely on. Often software that claims to only support one particular distribution will run perfectly fine on others. Linux distributions are not unfriendly towards third party software, but they have no obligation at all to spend effort to make that software work, it&#x27;s the third parties that should do that work.The bug tracker being email based is because when Debian started, that was the normal way to communicate on the Internet (besides IRC). A lot of tools were built on it, and the Debian developers themselves are used to it, so there is little incentive to change this.The Debian developers would say that apt is not buggy; it&#x27;s just that if there are conflicts, they have to be resolved in some way, which means deleting some of the conflicting packages. It also does ask you to confirm in this case. Although it would indeed be better if it would detect this is a very unsatisfying solution. reply Karellen 13 hours agoparentprev> For example, if you want to install Sublime Text, or VS Code, there is no way to do it securely, without giving untrusted software access to your browser history and SSH keys.First, if you don&#x27;t trust a bit of software, why are you installing it?But more importantly - you don&#x27;t want your text editor to be able to open and edit your browser history files, or your ssh key files?If my text editor wasn&#x27;t able to open and edit those files, I&#x27;d consider it extremely broken! reply codedokode 12 hours agorootparent> First, if you don&#x27;t trust a bit of software, why are you installing it?The more people you trust, the larger is the chance that you get deceived.> But more importantly - you don&#x27;t want your text editor to be able to open and edit your browser history files, or your ssh key files?Only with my permission. reply codedokode 12 hours agorootparentprevEven if you trust the developer (I don&#x27;t), there is a chance that there is a bug or vulnerability in the software. Also you need to install different plugins from random anonymous guys from Github, and it is difficult to trust an anonymous person. reply Karellen 11 hours agorootparent> Also you need to install different plugins from random anonymous guys from GithubDo I? Strange, I&#x27;ve never noticed needing to do that myself. reply Timber-6539 13 hours agorootparentprevIs VS Code a text editor? I wouldn&#x27;t consider anything that has internet access permissions or a suite of 3rd party user plugins&#x2F;extensions a text editor but that&#x27;s just me.Giving VS Code express permission to your home&#x2F;filesystem (the default if you install it traditionally) is a security risk [0] [1] most people rarely think about.[0] https:&#x2F;&#x2F;blog.aquasec.com&#x2F;can-you-trust-your-vscode-extension...[1] https:&#x2F;&#x2F;www.techradar.com&#x2F;news&#x2F;hackers-are-using-malicious-m... reply Karellen 13 hours agorootparent> Is VS Code a text editor?Um, I thought it was?I&#x27;ve not used it, because I&#x27;m happy with (neo)vim for my dev needs, but I thought that&#x27;s what it did?If VS Code isn&#x27;t used to edit text, what is it for?Edit: (neo)vim and emacs both have 3rd party extension ecosystems, with extensions written in languages that can access the internet, so I&#x27;m not sure how that affects your test? reply Timber-6539 12 hours agorootparentNano is a good example of a text editor. reply mdwalters 11 hours agoparentprev [–] > if a system component like Gnome is crashing, there will be neither log records or crash dumps and you will never figure out why it crashedOn Fedora you can use ABRT (AKA Problem Reporting) to view logs and tracebacks of a component that has crashed, and report the problem via Bugzilla. Also, GNOME isn&#x27;t a system component, Fedora would still work without it, but it would use a TTY terminal instead. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Debian is a free, open-source general-purpose operating system with a democratic governance structure where the project leader is elected yearly.",
      "It follows a social contract and guidelines aimed at maintaining commitment to free software, control over dependencies, and offering a self-contained platform. It refrains from using bundled libraries to achieve this.",
      "Debian adopts a methodical membership process for package uploads owing to the significance and trust involved. To prevent confusion and facilitate mirroring, it employs codenames for its releases. Due to its complexity, it evolves gradually, necessitating extensive dialogue and consensus."
    ],
    "commentSummary": [
      "The conversation explores various aspects of Debian, including its packaging, dependency management strategies, and the quality of its package manager's code.",
      "Participants also discuss the effects of systemd, the support and funding for Debian, the future of Linux distributions, and the pros and cons of using Debian.",
      "The dialogue includes a range of views and debates, showcasing the diversity of user perspectives and the inherent challenges associated with Debian use."
    ],
    "points": 285,
    "commentCount": 169,
    "retryCount": 0,
    "time": 1696760519
  },
  {
    "id": 37809721,
    "title": "Homebrew to deprecate and add caveat for HashiCorp",
    "originLink": "https://github.com/Homebrew/homebrew-core/pull/139538",
    "originBody": "Skip to content Product Solutions Open Source Pricing Search or jump to... Sign in Sign up Homebrew / homebrew-core Public Sponsor Notifications Fork 11.8k Star 12.8k Code Issues 21 Pull requests 138 Actions Security Insights New issue hashicorp: deprecate and add caveat #139538 Open iMichka wants to merge 2 commits into Homebrew:master from iMichka:terraform +26 −0 Conversation 37 Commits 2 Checks 12 Files changed 2 Conversation Member iMichka commented Inform users that we might disable this forumula one day given there will be no more version updates in homebrew-core due to the license change Have you followed the guidelines for contributing? Have you ensured that your commits follow the commit style guide? Have you checked that there aren't other open pull requests for the same formula update/change? Have you built your formula locally with HOMEBREW_NO_INSTALL_FROM_API=1 brew install --build-from-source , whereis the name of the formula you're submitting? Is your test running fine brew test , whereis the name of the formula you're submitting? Does your build pass brew audit --strict(after doing HOMEBREW_NO_INSTALL_FROM_API=1 brew install --build-from-source )? If this is a new formula, does it pass brew audit --new ? 👍 21 👎 2 🎉 3 iMichka added the CI-syntax-only label iMichka requested a review from a team github-actions bot added go formula deprecated labels chenrui333 added the maintainer feedback label p-linnane reviewed View reviewed changes Formula/terraform.rb Outdated Show resolved Formula/terraform.rb Outdated Show resolved Formula/terraform.rb Outdated Show resolved ZhongRuoyu reviewed View reviewed changes Formula/terraform.rb Outdated Show resolved Member Bo98 commented • edited Not sure if it's necessarily worth elaborating on: but as it stands future releases prior to 1.6.0 will be acceptable, though it's possible there won't be any such release. carlocab reviewed View reviewed changes Formula/terraform.rb Outdated Show resolved MikeMcQuaid reviewed View reviewed changes Member MikeMcQuaid left a comment Thanks for this @iMichka! Looks great so far. A few thoughts. Formula/terraform.rb Outdated Show resolved Formula/terraform.rb Outdated Show resolved Formula/terraform.rb Outdated Show resolved Formula/terraform.rb Outdated Show resolved iMichka mentioned this pull request qt: rev bump and test QtWebEngineCore presence #140149 Merged 6 tasks chenrui333 mentioned this pull request terraform 1.5.6 #140294 Merged Contributor github-actions bot commented This pull request has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. github-actions bot added the stale label Member fxcoudert commented @iMichka gentle ping on that review feedback fxcoudert removed the stale label iMichka force-pushed the terraform branch from 006cae0 to 4f6db6b Compare iMichka added a commit to iMichka/brew that referenced this pull request terraform: add audit for relicensing … Verified a24d15d iMichka mentioned this pull request terraform: add audit for relicensing Homebrew/brew#15975 Merged 7 tasks iMichka requested review from p-linnane, carlocab, ZhongRuoyu and MikeMcQuaid Member Author iMichka commented Done. Sorry for the delay MikeMcQuaid approved these changes View reviewed changes Member MikeMcQuaid left a comment Thanks @iMichka! BrewTestBot added this pull request to the merge queue github-merge-queue bot removed this pull request from the merge queue due to failed status checks View details chenrui333 reviewed View reviewed changes Formula/t/terraform.rb Outdated Show resolved chenrui333 reviewed View reviewed changes Formula/t/terraform.rb Outdated Show resolved chenrui333 approved these changes View reviewed changes 7 hidden items Load more… BrewTestBot added this pull request to the merge queue github-merge-queue bot removed this pull request from the merge queue due to failed status checks View details Bo98 mentioned this pull request readall: check hash generation works Homebrew/brew#16053 Merged ZhongRuoyu reviewed View reviewed changes Member ZhongRuoyu left a comment These unfortunate dependents also need to be deprecated: atlantis * Dependency 'terraform' is deprecated but has un-deprecated dependents. Either un-deprecate 'terraform' or deprecate it and all of its dependents. cdktf * Dependency 'terraform' is deprecated but has un-deprecated dependents. Either un-deprecate 'terraform' or deprecate it and all of its dependents. envconsul * Dependency 'consul' is deprecated but has un-deprecated dependents. Either un-deprecate 'consul' or deprecate it and all of its dependents. fabio * Dependency 'consul' is deprecated but has un-deprecated dependents. Either un-deprecate 'consul' or deprecate it and all of its dependents. infracost * Dependency 'terraform' is deprecated but has un-deprecated dependents. Either un-deprecate 'terraform' or deprecate it and all of its dependents. stolon * Dependency 'consul' is deprecated but has un-deprecated dependents. Either un-deprecate 'consul' or deprecate it and all of its dependents. terraform-provider-libvirt * Dependency 'terraform' is deprecated but has un-deprecated dependents. Either un-deprecate 'terraform' or deprecate it and all of its dependents. terraform-rover * Dependency 'terraform' is deprecated but has un-deprecated dependents. Either un-deprecate 'terraform' or deprecate it and all of its dependents. tfmigrate * Dependency 'terraform' is deprecated but has un-deprecated dependents. Either un-deprecate 'terraform' or deprecate it and all of its dependents. tfschema * Dependency 'terraform' is deprecated but has un-deprecated dependents. Either un-deprecate 'terraform' or deprecate it and all of its dependents. tfupdate * Dependency 'terraform' is deprecated but has un-deprecated dependents. Either un-deprecate 'terraform' or deprecate it and all of its dependents. Member Bo98 commented • edited Terraform dependents might be resolved when we add OpenTofu, so we could wait a little for that. Not aware of anything for Consul however so those might have to take the hit. 👍 6 p-linnane approved these changes View reviewed changes Member p-linnane left a comment Agreed with @Bo98 on seeing if we can use OpenTofu as a drop in replacement for the Terraform dependents. 👍 4 BrewTestBot added this pull request to the merge queue github-merge-queue bot removed this pull request from the merge queue due to failed status checks View details Member Bo98 commented I've re-run the tap_syntax to show the failure so we don't try and fail to auto-merge this for a 5th time. (It previously wasn't showing because we had removed brew audit from PR checks for a little while while keeping it for merge checks - but I restored it fully as of yesterday.) Might be worth splitting consul and terraform to their own PRs. We'll need to make a decision on the former, and for the latter it looks like we'll wait for a little bit until OpenTofu make their first release - we already are blocking version bumps so no rush. 👍 2 Member ZhongRuoyu commented We may want to remove the livecheck blocks too -- vault 1.15.0 has been released and it's showing up in the output of brew bump. dduugg pushed a commit to dduugg/brew that referenced this pull request terraform: add audit for relicensing … Unverified 9a2eed7 Contributor b-reich commented #149678 I created a formula for the opentofu fork 🚀 6 bschaatsbergen reviewed View reviewed changes Formula/n/nomad.rb Outdated Show resolved thimslugga commented Perhaps include something to inform those users that want to continue using terraform, etc, that they should use the hashicorp homebrew tap? https://github.com/hashicorp/homebrew-tap https://www.hashicorp.com/blog/announcing-hashicorp-homebrew-tap 👍 1 Contributor aliscott commented These unfortunate dependents also need to be deprecated: @ZhongRuoyu I'm the lead maintainer of infracost. It shouldn't have a dependency on terraform, so we can update the formula to remove that dependency. 👍 1 Member ZhongRuoyu commented It shouldn't have a dependency on terraform, so we can update the formula to remove that dependency. @aliscott Could you open a PR to update infracost's dependency then? Thanks! Member MikeMcQuaid commented Perhaps include something to inform those users that want to continue using terraform, etc, that they should use the hashicorp homebrew tap? https://github.com/hashicorp/homebrew-tap https://www.hashicorp.com/blog/announcing-hashicorp-homebrew-tap We shouldn't link to this directly because: we don't want to be seen to specifically recommend a tap we have no influence over homebrew-core is for open-source software only and we want to be careful pointing to a repository that is not open source software 👍 2 baggiponte commented What does this mean for tools like tfenv? Can homebrew be used to install OSS that installs non-OSS software nevertheless? 👍 1 Member Bo98 commented • edited What does this mean for tools like tfenv? Can homebrew be used to install OSS that installs non-OSS software nevertheless? If tfenv itself is able to remain open source that's likely fine. I imagine it'll be able to download OpenTofu in the future too anyway. iMichka mentioned this pull request hashicorp: deprecate some formulae #150192 Merged 6 tasks Member Author iMichka commented I've moved the formulae that could be deprecated right away to another PR: #150192 iMichka force-pushed the terraform branch from b60c160 to a36180b Compare github-actions bot added the automerge-skip label iMichka added 2 commits terraform: deprecate and add caveat … Verified 9559f4d consul: deprecate and add caveat … Verified 9dbb797 iMichka force-pushed the terraform branch from a36180b to 9dbb797 Compare github-actions bot removed the automerge-skip label yhj-zone mentioned this pull request Hacker News Daily Point Above 100 @2023-10-09 yhj-zone/hackernews-daily#10 Open headllines bot mentioned this pull request Hacker News Daily Top 10 @2023-10-09 headllines/hackernews-daily#1180 Open nikaro reviewed View reviewed changes Formula/c/consul.rbdepends_on \"go\" => :build def installsystem \"go\", \"build\", *std_go_args(ldflags: \"-s -w\")end def caveats<<~EOSWe will not accept any new packer releases in homebrew/core (with the BUSL license). Contributor nikaro Suggested change We will not accept any new packer releases in homebrew/core (with the BUSL license). We will not accept any new Consul releases in homebrew/core (with the BUSL license). Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Reviewers nikaro ZhongRuoyu bschaatsbergen MikeMcQuaid p-linnane carlocab chenrui333 At least 1 approving review is required to merge this pull request. Assignees No one assigned Labels CI-syntax-only formula deprecated go maintainer feedback Projects None yet Milestone No milestone Development Successfully merging this pull request may close these issues. None yet 14 participants Footer © 2023 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
    "commentLink": "https://news.ycombinator.com/item?id=37809721",
    "commentBody": "Homebrew to deprecate and add caveat for HashiCorpHacker NewspastloginHomebrew to deprecate and add caveat for HashiCorp (github.com/homebrew) 253 points by mooreds 22 hours ago| hidepastfavorite93 comments tedivm 20 hours agoIt&#x27;s important to note that they are holding off on deprecating dependents to see if they can swap in replacements. For example, programs that depend on Terraform will likely be able to use OpenTofu as a replacement.Unfortunately it doesn&#x27;t seem like there are going to be open source alternatives to Vault, Consul, or Nomad (that last one is hilarious to me: nomad was a good product until hashicorp stopped investing in it, now that it&#x27;s closed source it basically has a snowballs chance in hell of being adopted). reply cipherboy 18 hours agoparentIf anyone does start an open Vault fork, I&#x27;d be interested in contributing as I get time.Edit to add: https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;vault&#x2F;graphs&#x2F;contributors?from=... reply tedivm 15 hours agorootparentI wish I had the time or energy for it- especially with someone like you willing to contribute. I really hope someone takes up this task. reply nanmu42 19 hours agoparentprevI like Nomad a lot. It is even more lightweight than k3s and has served my low-budget projects very well. A little sad to see things are going this way. reply wg0 19 hours agoparentprevNomad was overly ambitious in running everything under the sun. Didn&#x27;t catch on. Setting it up would also require Consul. Docker swarm is simpler and superior solution to Nomad and is built right into docker engine itself. reply speedgoose 19 hours agorootparentI would rather sink in an ocean of kubernetes YAML documents than having to deal with Docker swarm issues ever again. Thankfully Docker swarm is kinda dead so it’s unlikely to be relevant again. Kubernetes won. reply wg0 16 hours agorootparentKubernetes won because of its composability. The API is the killer feature. The notion of objects, labels, pretty much an object oriented, declarative interface.You&#x27;d have to do a lots of custom morphing if you want that kind of interface over docker swarm.For smaller setups, simplicity of docker swarm might be useful but I am curious to know from people who are running clusters of 25 to 50 nodes. reply talent_deprived 18 hours agorootparentprevI like what kube achieves, I don&#x27;t like the power and memory hungry requirements of it, it seems ill optimized. A cluster with no pods uses 5%-15% CPU, that&#x27;s a lot of electricity being used for nothing. It&#x27;s really odd since Google claims to have green datacenters. reply mdaniel 16 hours agorootparentI&#x27;d bet you&#x27;re running a \"stacked\" control plane and if you moved etcd off to its own machines you&#x27;d localize the churn to be that. Which, if correct, doesn&#x27;t exactly refute your \"kubernetes is power and memory hungry\" claim but would more clearly distinguish whyI had high hopes back in my earlier days of them offering pluggable KV stores so folks could make tradeoffs if etcd doesn&#x27;t align with their goals and risks but as best I can tell they&#x27;ve declared it a non-goal reply p_l 15 hours agorootparentThat&#x27;s part of what k3s does reply philipov 17 hours agorootparentprevDoes Google use carbon swaps to conceal their emissions? Maybe that&#x27;s how they reconcile the difference.A quick search turned up this: https:&#x2F;&#x2F;www.greencentury.com&#x2F;google-parent-alphabet-pledges-...Looks like a possible case of greenwashing to me. reply tedivm 15 hours agorootparentI got into an argument with someone at google who said they don&#x27;t use carbon swaps and ended up pulling up a shareholder report that said they did. Google is real big on the greenwashing. reply dilyevsky 16 hours agorootparentprevGoogle doesn’t use kubernetes to run anything production. They use Borg reply teraflop 17 hours agorootparentprevI&#x27;m curious, are you seeing that on cloud instances, or what?My experience with bare-metal Kubernetes hasn&#x27;t been that bad. On my home cluster with very underpowered J4105 CPUs, I get about 5% CPU usage on the master node at idle, and 2% on workers. reply dilyevsky 16 hours agorootparentPeople need to check their logs for any oddities more frequently. A lot of the setups I’ve seen have subtle misconfigurations that leak&#x2F;busy loop resources needlessly reply nixgeek 17 hours agorootparentprevGoogle doesn’t run on Kubernetee. reply tedivm 17 hours agorootparentprevI&#x27;ve actually been using docker swarm for an odroid cluster that runs a lot of services in my house. I really like it- it&#x27;s extremely low overhead and the network mesh works way better than I was expecting considering I put no effort into configuring it.That said this is a three node cluster running a handful of containers, with services accessed by my wife and I. So not exactly a large scale production experience. reply SteveNuts 17 hours agorootparentAs someone whose wife will immediately call me if the important services in our household (Plex, pihole, home assistant), I think those definitely count as production services! reply michens 19 hours agorootparentprevCan you elaborate on its issues? I&#x27;ve never used it on production myself, but it looks like a simplified k8n, which is something I feel a lot of people desire reply da768 19 hours agorootparentLast used it years ago, networking would just die randomly, requiring container restarts when using multiple nodes reply eropple 18 hours agorootparentI have heard (though haven&#x27;t tried) that V2 of Compose&#x2F;Swarm is significantly better. reply drdaeman 18 hours agorootparentprevAnything can and will break. Symptomatically similar thing happened to me with K8s with (IIRC) Calico. And I’ve had issues with Swarm too, although of a different kind (it quietly refused to run a data channel for me over an IPv6-only network, while control channel worked just fine).It was all quite a while (3-4 years) ago, so I don’t remember much details, except that Swarm was then not exactly capable of working in IPv6-only environments. It was significantly easier to navigate Docker’s codebase than Kubernetes’.(Ultimately, my solution was that I understood that I didn’t need any kind of orchestration there, just HA&#x2F;failover. But that’s another story.) reply speedgoose 18 hours agorootparentprevI haven’t used it for many years but in short it just didn’t work reliably. I mostly encountered network iptables issues from memory. reply raphinou 19 hours agorootparentprevI&#x27;ve been very happy with until now, though i must admit i&#x27;m using one node swarms. But i&#x27;d rather use that (1) than sticking together a custom solution using docker compose as i see it mentioned sometimes.(1) https:&#x2F;&#x2F;dockerswarm.rocks&#x2F;traefik&#x2F; reply nunez 18 hours agorootparentprevInfinite issues with overlay networking once you scale past 30 nodes in a swarm. Weave aside, network drivers for anything not CNI are basically deprecated&#x2F;dead. reply kawsper 17 hours agorootparentprevWith Nomad 1.3 and newer, you can run a Nomad cluster without Consul, there&#x27;s support for built-in service discovery: https:&#x2F;&#x2F;www.hashicorp.com&#x2F;blog&#x2F;nomad-service-discovery reply busterarm 15 hours agorootparentprevHaving worked with Nomad, K8s and Docker Swarm at reasonable scale, Docker Swarm is the only one of those three that I would never use again under pain of death. reply mosselman 16 hours agorootparentprevI am a happy swarm user for a long time now. It is pretty simple. The only complicated part was the reverse proxy, but traefik has been a solid solution.You will spend quite some time figuring the labels out, but once you have them for one service, it works for many.A new solution that might be even better is Kamal by 37 Signals. It runs all their production stuff, so it must be capable. Can’t wait for a version 2 where, I’d suspect, by then there will be more community contributions for some more diverse setups merged in. reply jen20 18 hours agorootparentprev> Docker swarm is simpler and superior solution to NomadThis is the voice of absolute delusion. Swarm is a mess. Nomad actually scales. reply chromatin 18 hours agorootparentprevRecent nomad versions replace lots of consul functionality. Consul is definitely not needed at small scale, even for service discovery. reply jacurtis 18 hours agoparentprevHonestly, all Hashi products are generally really good. I do think they do suffer from moving too fast syndrome. They pushed to go public and most or all of their recent shortcomings can be traced back to trying to push stock prices.Early Hashicorp was incredible. They were open source stewards and looked like an up-and-coming Redhat or Canonical. Their products were ground-breaking and truly huge value adds to the open-source eco system. But they got extremely popular (mostly thanks to Terraform, which took off and drew more attention to their other products).But since going public it has been clear that they are trying to get money and enterprise customers at any cost.Terraform itself has felt like it was in maintenance-mode since hitting version 1. Terraform providers frequently break. For production I have found you need to pin providers down to the patch level, because i&#x27;ve had multiple problems just in small patch-level updates over the recent years. They famously started rejecting any open-source contributions that didn&#x27;t provide a business value for Hashicorp. Since TF hitting v1, almost all the attention seems to be towards Terraform Cloud and Terraform Enterprise. At Hashicon, it feels like every talk is just propaganda pushing these products. It is all they care about anymore.Nomad was a product that Hashicorp was very excited about for a while and then they seemed to just abandon on the side of the road in their quest for enterprise dominance (probably after learning that most Enterprises are all-in on K8s and Nomad is more beneficial to fast-moving start-ups).Vault was an incredible tool, especially in the Open-Source space. But in the past few years they have really split the open-source and licensed versions of Vault significantly which made the open-source version feel more like a burden to Hashicorp. The last time I talked to Hashicorp about Vault (we were deeply considering a switch to Vault at work last year), they treated the open-source self-hosted solution as a \"trial\" of the real Vault, and that is what it felt like. Almost everything we ran into during setup, they would respond back with \"oh that&#x27;s fine in the enterprise version\".Overall, they have peeled back on all but the absolute minimum efforts required to support their open source versions of products and have been an entirely enterprise focused company for a while. And I can&#x27;t blame them, sure you need to make money. But I can&#x27;t help but point to organizations like RedHat and Canonical as examples of what Hashicorp could have been.At this point I feel like the parent that watches their kids, talking about how they missed out on reaching their potential, thanks mostly to what seems like greed or over-ambition. \"I&#x27;m not mad, I&#x27;m just disappointed\" comes to mind when I think of Hashi. I have high-hopes for OpenTofu to fill the Terraform void. I&#x27;ve moved past Vault and am using one of the big hyperscalers&#x27; secrets management tools (which I enjoy a lot less, but is cheaper and less complicated). I use kubernetes instead of Nomad, which again is fine, it has become the standard anyway. So i&#x27;ll be just fine... but I&#x27;m dissapointed at you Hashicorp. That&#x27;s all. reply idunno246 17 hours agorootparent> But since going public it has been clear that they are trying to get money and enterprise customers at any cost.And yet, as an enterprise customer, their sales team is terrible. Incredibly unhelpful with problems, not willing to discuss pricing, ultimately lost the deal due to how unresponsive they were, and how they basically told as that they wouldn’t be any more likely to fix all the bugs we were hitting if we paid.The reason they had to go this route is they are getting trounced by their competitors and it’s all their own fault reply hiepph 55 minutes agorootparentprevThe enterprise versions of their products are ridiculously expensive. They&#x27;re always trying their best to charge you with mistakes (e.g. number of connecting clients). The pricing on the landing page is just a fraction of the real price. That&#x27;s a big turn-off for me. It&#x27;s true that I don&#x27;t want to have the hassle of setting up a production-ready Vault, but seriously, if Hashicorp keeps charging like that I&#x27;d would really consider a different solution. reply NetOpWibby 18 hours agorootparentprevI wonder if this downhill trend has been in the works for years. In early 2018 I wrote a post[1][2] about my frustrations with interviewing with HashiCorp.TL;DR: they put me through an interview gauntlet, took my entire day, and ignored me for weeks until I aired my frustrations in their contact us form.I haven’t used their products since but always heard great things about them. That enterprise push is fierce.[1]: https:&#x2F;&#x2F;blog.webb.page&#x2F;2018-01-11-why-the-job-search-sucks.t...[2]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16127697 reply bbarn 17 hours agorootparentNot at HashiCorp, but I try to respect applicants&#x27; time in interviews, and my overall process is less than 3 hours total for interviewing, across 4 people (all of which are self-scheduled). That said, when you have hundreds or thousands of applicants, people inevitably fall through the cracks unless you have an internal recruiting team entirely focused on maintaining that experience.Interviewing sucks for everyone on either side, and efforts to improve candidate experience on my end have often been met with resistance in the company. One thing I&#x27;ve done is try to be as clear as possible at the end of interviews where someone stands. If it doesn&#x27;t go well, I try to relate what didn&#x27;t go well and how it applies to the position we&#x27;re hiring for. If it goes well, my interviewers and I try to send out invites for the candidate to schedule the next interview within 10 minutes of completion, and I tell them that during the call.So many companies are so risk-averse though, that they act like interviews are a poker game. I&#x27;ve generally had nothing but positives with my approach, and yes, someone may sue us at some point for being blunt and upfront, but my experience so far is people appreciate honest feedback if you deliver it kindly on the spot. reply darkwater 16 hours agorootparent> One thing I&#x27;ve done is try to be as clear as possible at the end of interviews where someone stands. If it doesn&#x27;t go well, I try to relate what didn&#x27;t go well and how it applies to the position we&#x27;re hiring for.I tried doing that last time I hired and I think generally speaking makes a good experience for candidates, BUT when you have those 2-3 individuals that have really high opinions of themselves it can get tiring for the hiring manager (myself), because they won&#x27;t accept the answer anyway and will keep asking and ultimately give some bad review in Glassdoor etc reply NetOpWibby 17 hours agorootparentprevI prefer bluntness as that&#x27;s my personal style. I&#x27;m glad you&#x27;re pushing through, even despite resistance.Every person I interviewed with at Hashicorp seemed to enjoy the conversation and the live coding challenge wasn&#x27;t particularly challenging. Ah well.Rejection is protection and redirection. reply tayo42 17 hours agorootparentprevThe second comments section, could have been all from today. Nothing about hiring has changed in 5 years lol reply NetOpWibby 6 hours agorootparentWish I could say I was surprised reply bdcravens 17 hours agorootparentprevI appreciate Hashicorp, but early on I had a bad taste in my mouth. I paid for their first commercial offering, the VWWare adapter of Vagrant. I had a conversation with Mitchell (can&#x27;t remember the exact context) around an issue I was encountering, and remember him being pretty dismissive toward what I was asking. Again, I&#x27;m sure there&#x27;s a lot of context I&#x27;m not remembering, but I had the feeling that they used their early revenue as their launching pad and were more focused on the next big thing than giving treating their customers as the asset that they were. reply wdb 17 hours agorootparentYeah, that was a waste of money. Didn’t work well at all reply mathverse 18 hours agoparentprevWait is Nomad really closed source? reply cipherboy 18 hours agorootparentNomad, along with the rest of Hashicorp&#x27;s flagship products, transitioned to the BUSL-1.1: https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;nomad&#x2F;blob&#x2F;main&#x2F;LICENSE reply c7DJTLrn 17 hours agorootparentAm I misunderstanding something? I thought BUSL means the source is still open, but not free as in freedom. reply cipherboy 17 hours agorootparentPerhaps a more apt term would be freeware.The BUSL is not OSI approved and places restrictions around usage, in an effort to sell some product or service off of a project and prohibit competition. It is not free as in beer either, if you hit such usage.The source is available (hence source available being another terminology) but even Hashicorp has dropped \"open source\" from nearly every relevant page in understanding that their products no longer have an open source core (see instead \"Vault Community\" or \"Terraform Community\").The general consensus is that no, the BUSL is not OSS as OSI has not blessed it as such: https:&#x2F;&#x2F;opensource.org&#x2F;licenses&#x2F; reply c7DJTLrn 17 hours agorootparentThanks. reply nrclark 16 hours agorootparentprevIt&#x27;s true that Hashicorp&#x27;s source is still available, but you might not have permission to use it or modify it any more. Even if you can read it legally. Under their new license, Hashicorp can essentially revoke any commercial user&#x27;s license to run Hashi software, and there is nothing the business can do except pay. They can do it at any time, if the business has any divisions that do anything close to competing with any Hashi product. reply toenail 17 hours agorootparentprev> Am I misunderstanding something?Yes, source being available is not enough to be called open source. reply rwmj 1 hour agorootparentIt&#x27;s worse in some ways because if you read the source and then work on another open source project in the same field you could end up in legal trouble. replyerulabs 19 hours agoparentprevNomad looked like a good product until I used it. I was so looking forward to a simpler take on orchestration, having spent years in k8s-land and generally being a fan of Hashicorp in the past.My tldr now is nomad should be considered harmful. reply ochoseis 19 hours agorootparentI’ve started playing with Nomad recently just to see what it’s all about. There are some annoying things with networking, but overall it’s been pretty fun so far. What made you consider it harmful? reply erulabs 17 hours agorootparentThe number one issue is that deployments are blocking, and scaling is a deployment. That means during a deployment, scaling is disabled. This makes safely deploying to thousands of containers during scale-in hours extremely dangerous. Combined with nomad-autoscaler being a SPOF and crash-happy, the always-be-deploying startup with a daily traffic pattern should not use Nomad.It’s also nontrivial to get nomad to be resilient. For example, if your template uses keys from consul or vault, even with “noop” mode, and consul or vault are unreachable, Nomad will happily start killing running containers that had been rendered correctly in the past, because it can’t re-render the template. This pattern has only recently been addressed by “nomad_retry” but there have been several bugs with it, and 1.6.2 will currently kill all running containers if some template resource can’t be reached. Under the hood this uses consul-template, which does support infinite retry, but getting nomad to use consul template safely is non trivial. Eg: vault tokens expire so infinite retry for vault doesn’t work as-is.Node_pools just landed (2023!), and are still broken when using Levant (another abandoned nomad tool - think kustomize with a much more horrifying DSL).Bonus issues: they’re still trying to get cgroups v2 working, the current version finally doesn’t DOS your backend services in an infinite loop, the UI lies, deployments can get stuck forever because “progress_deadline” is more of a suggestion than a deadline, nomad-autoscaler is not highly available, crashes very easily, often scales faster than its “cooldown” window, and is simply stupid. AWS Karpenter feels a full decade into the future.And all that so I can write my own Nomad specs instead of installing a vendors helm chart in a network isolated namespace. Boo.To reiterate, I’ve liked Hashi for years and years, and I don’t have any ill will towards them. Just shocked at how poor nomad is compared to k8s. It’s definitely more fun than k8s when getting started, for sure. reply nanmu42 19 hours agorootparentprevRecent version of Nomad has introduced many QoL features like secrets and service discovery, though logging collection remains unsupported. reply mdaniel 16 hours agorootparent> though logging collection remains unsupported.Do you happen to have a link to the issue (I guess or docs, if it&#x27;s their official stance)? I&#x27;d enjoy reading how they ended up in that circumstance reply kawsper 12 hours agorootparentLogcollection doesn&#x27;t come out of the box, and you&#x27;re kinda on your own to set it up, this article (although a bit dated) shows you what you must go through:https:&#x2F;&#x2F;atodorov.me&#x2F;2021&#x2F;07&#x2F;09&#x2F;logging-on-nomad-and-log-aggr...It also doesn&#x27;t help that Nomad handles everything from Docker containers to managing regular applications. reply jen20 3 hours agorootparentprevAlso worth noting that nomad’s closest competitor (Kubernetes) does not do this either. It’s a huge oversight in both ecosystems. replybroknbottle 19 hours agoprevHashicorp maintains their own tap..https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;homebrew-taphttps:&#x2F;&#x2F;www.hashicorp.com&#x2F;blog&#x2F;announcing-hashicorp-homebrew... reply cipherboy 18 hours agoparentNote for whatever it is worth that the Homebrew-owned version rebuilds these from source -- https:&#x2F;&#x2F;github.com&#x2F;Homebrew&#x2F;homebrew-core&#x2F;pull&#x2F;139538&#x2F;files#.... This is also typical in the Linux packaging ecosystem, but typically involving packaging dependencies explicitly too, which is likely why Vault &co never landed in a distro&#x27;s package set prior to the license change.The Hashicorp variants copy the release builds: https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;homebrew-tap&#x2F;blob&#x2F;master&#x2F;Formul... reply andix 20 hours agoprevWhy? Isn&#x27;t homebrew just a package manager, and why should a non free license restrict them of including HashiCorp tools? Or do they have a policy only to include free software.Edit: yes they have quite strict guidelines: https:&#x2F;&#x2F;docs.brew.sh&#x2F;License-Guidelines reply vmarchaud 20 hours agoparentThey do have a policy to only include free software [1]:> We only accept formulae that use a Debian Free Software Guidelines license or are released into the public domain following DFSG Guidelines on Public Domain software into homebrew&#x2F;core.[1]: https:&#x2F;&#x2F;docs.brew.sh&#x2F;License-Guidelines reply jameshart 19 hours agorootparentIn home-brew core.And the reasoning is that home-brew isn&#x27;t strictly a package manager, it is a dependency installer and builder - it pulls down sources and everything they depend upon (as source packages too, usually), and builds them locally. So as a user of home-brew you&#x27;re trusting the formulae you install from to be for things you are allowed to pull down and build locally. Licenses matter.Hashicorp (or anyone else) can maintain a source tap of their own - when you opt in to using a third party tap you&#x27;re trusting that tap&#x27;s licensing as well. And there&#x27;s also &#x27;casks&#x27; which are for non-source distributions, which home-brew can install as well, where source licensing isn&#x27;t important. reply INTPenis 19 hours agorootparentprevBut the license hasn&#x27;t changed, so that raises the question of why was it included in the first place? reply Hackbraten 19 hours agorootparentThe license will change to BUSL for all upcoming upstream releases. [1][1]: https:&#x2F;&#x2F;www.hashicorp.com&#x2F;blog&#x2F;hashicorp-updates-licensing-f... reply chmod775 19 hours agorootparentprevThe license did change. From MPL 2.0 to BSL. reply Hackbraten 19 hours agoparentprev> Isn&#x27;t homebrew just a package managerIt’s not just a package manager.It’s a piece of software called `brew` (the package manager) but also a package repository called `homebrew-core` to which the software connects by default. The package repository is carefully curated and only accepts open-source licenses.You’re free to use `brew` to tap into whatever repository you like, but TFPR is concerned with the core repository only. reply mdaniel 16 hours agorootparent> also a package repository called `homebrew-core` to which the software connects by defaultI think that&#x27;s only true if one does an `export HOMEBREW_NO_INSTALL_FROM_API=1` otherwise they default to their new JSON API: https:&#x2F;&#x2F;docs.brew.sh&#x2F;Installation#default-tap-cloningand is the only way if one needs to alter any core Formulae: https:&#x2F;&#x2F;docs.brew.sh&#x2F;FAQ#can-i-edit-formulae-myself (which, for clarity, Brew explicitly disavows) reply Hackbraten 16 hours agorootparentI was referring to the package repository, not a Git repository. reply miki123211 19 hours agoparentprevThis is only partially true.By default, homebrew supports (or, in their terminology, taps into) two repositories, homebrew&#x2F;core and homebrew&#x2F;casks.Core only takes free software, built by the Homebrew developers itself, installed in &#x2F;opt&#x2F;homebrew etc. Casks takes everything under the sun, including commercial software with no available source code. Such software is often downloaded straight from their developers and installed wherever it wishes to be installed, most often in &#x2F;Applications. reply hnlmorg 19 hours agoprevMuch as I love the service homebrew provides, terraform is one of the edge cases that is better being managed outside of brew. I believe tf-switch is now the most popular option?The problem with Terraform is that you often need a pinned version because accidentally updating your state file can be dangerous. (Though in fairness, updating Terraform is significantly less troublesome than it was in the pre-1.0 days) reply Zizizizz 17 hours agoparentI use rtx (rust asdf) https:&#x2F;&#x2F;github.com&#x2F;jdx&#x2F;rtx as it lets me install all the languages with one tool (plus project environmental variables like direnv) reply bloopernova 16 hours agorootparentIs it compatible with asdf? reply mpwoz 16 hours agorootparentFrom the first bullet point under &#x27;features&#x27;:> asdf-compatible - rtx is compatible with asdf plugins and .tool-versions files. It can be used as a drop-in replacement. reply bloopernova 15 hours agorootparentI&#x27;m going to blame being distracted by my dog for not reading the link you posted.asdf + direnv makes my work life so much easier. I have one .envrc that checks what branch I&#x27;m on and switches you to the right Terraform workspace and sets correct env-vars too. reply Zizizizz 13 hours agorootparentYeah I use this same setup with rtx replywwalexander 18 hours agoparentprevYou‘re right, it‘s best managed inside of MacPorts, which allows you to install specific versions of packages and switch between them easily! reply hnlmorg 14 hours agorootparentYou can do that with homebrew too. But in the case of Terraform, where you can have developers on Windows and macOS and CI&#x2F;CD pipeines based on Debian&#x2F;Ubuntu&#x2F;CentOS&#x2F;Alpine&#x2F;etc, it&#x27;s handy having everyone use the same tool for managing Terraform versions.It&#x27;s just like how you wouldn&#x27;t have different people manage the same Python codebase with different package managers. You agree on the standard for that business or project and have every instantiation of that project follow that same standard. reply wwalexander 4 hours agorootparentI definitely agree overall. My last job was standardized on Homebrew and used tfswitch; I personally translated to MacPorts but I do agree that tying a project to a specific package manager (and thus OS, etc.) isn’t advisable. reply soraminazuki 4 hours agorootparentprevI think Nix is better suited for that kind of thing. You can specify multiple versions of packages and expose them on a per-project basis. It eliminates the need to switch packages globally. reply whalesalad 18 hours agorootparentprevtil macports still exists reply wwalexander 16 hours agorootparentIt not only exists but is vastly superior to Homebrew. reply cmclaughlin 18 hours agoparentprev> Much as I love the service homebrew provides, terraform is one of the edge cases that is better being managed outside of brew. I believe tf-switch is now the most popular option?tfenv is also available in homebrew for installing multiple&#x2F;different versions of terraform reply oefrha 19 hours agoprevThey can always live in casks instead. Not terribly impactful practically speaking. reply jacurtis 19 hours agoparentTrue, Hashi can cask it just fine if they wanted to continue to distribute through homebrew. I doubt they would though, they have already been recommending direct binary installation from their servers for the past few years. That is how the installation documentation for Terraform has been for a while.The bigger impact is on other tools that have terraform as a dependency, as seen here: https:&#x2F;&#x2F;github.com&#x2F;Homebrew&#x2F;homebrew-core&#x2F;pull&#x2F;139538#pullre...Tools like atlantis and infracost are also getting removed since they depend on Terraform. So it is going to make distribution a little harder for those smaller tools. The good news is that the thread does say that they are holding off to allow those tools to update their dependencies to an alternative like OpenTofu when it becomes stable or to remove the dependency altogether. But the real impact imho is these other tools. reply shagie 18 hours agorootparent> I doubt they would though, they have already been recommending direct binary installation from their servers for the past few years.September 25, 2020 : Announcing HashiCorp’s Homebrew Tap https:&#x2F;&#x2F;www.hashicorp.com&#x2F;blog&#x2F;announcing-hashicorp-homebrew... ( https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24619346 - though only 2 points and 0 comments so no reason to click there other than to note that it was mentioned here )https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;homebrew-tap (last updated 2 days ago: Bump terraform-ls to 0.32.1) reply hkh 14 hours agorootparentprevThis won&#x27;t impact infracost, Alistair just responded to it: https:&#x2F;&#x2F;github.com&#x2F;Homebrew&#x2F;homebrew-core&#x2F;pull&#x2F;139538#issuec... reply godelski 14 hours agoprevHomebrew is really useful but there&#x27;s some weird design choices. Like why does it install a new dedicated python? And why is that python required to be the latest? But not always because each formula must specify the python version so this never actually happens in practice and you have formulas specifying all kinds of versions. Like why not just do what every other package manager does and uses the system&#x27;s python? Seriously, I already have too many pythons installed, I don&#x27;t need another. Especially when it makes it confusing because I need to install a pip package to get something working properly. reply foolfoolz 20 hours agoprev [–] seems like politics. there are plenty of packages in hombrew that will no longer receive updates but they’re not deprecated reply latexr 20 hours agoparentThere’s a difference between a formula which is not updated because upstream doesn’t release new versions and one which won’t receive new updates in Homebrew because its license is no longer open-source (which the non-Cask part of Homebrew requires). This is the latter. reply Macha 20 hours agoparentprevThe package being dead is one thing, the users should expect that homebrew will not magically create updates that upstream doesn&#x27;t have.That updates exist but homebrew will not legally be able to distribute them, meaning you could be installing an old and vulnerable version that will never get updated on the other hand, seems worthwhile warning people about. reply bogantech 19 hours agorootparentHomebrew is legally able to distribute them, they&#x27;re making a decision not to reply Brian_K_White 18 hours agorootparentPerhaps \"legally\" was the wrong word, but the essence is still that they can&#x27;t redistribute them, even if the reason is not law. The fact that the reason is it would be incompatible with other etsablished policy, and they wrote that policy themselves in the first place, doesn&#x27;t change the fact of \"can&#x27;t\".It&#x27;s not legitimate to say \"no one is stopping you from rewriting all your other contracts, charters, and principles to be compatible with my new license\".And who knows, maybe they even can&#x27;t \"legally\" if everything were fully evaluated.Also, this sounds like an attempt to obfuscate or downplay the essential fact here of who made the breaking change. If someone cared about Terraform being included in brew, and wanted to know who to blame for their orderly world being disturbed, it is not because homebrew has decided to evict Hashicorp, it is because Hashicorp left. reply monsieurbanana 16 hours agorootparentprevThey made that decision (to distribute applications with a specific type of licences) long ago. Nothing to do with terraform in particular. reply neurostimulant 19 hours agoparentprev [–] Homebrew Core only contains apps with opensource license, specifically license that compatible with Debian Free Software Guidelines license (GPL, Apache, BSD, MIT, etc).https:&#x2F;&#x2F;wiki.debian.org&#x2F;DFSGLicenses#DFSG-compatible_License... replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A GitHub pull request proposes to deprecate certain software formulas from Homebrew, a widely used package manager for macOS.",
      "The software formulas being considered for deprecation are tools from HashiCorp, including Terraform and Consul, stemming from a recent alteration in their licensing.",
      "The pull request also suggests a potential replacement for Terraform: OpenTofu, and actively invites feedback from team members in the ongoing process of updating Homebrew following the license change."
    ],
    "commentSummary": [
      "The conversation mainly addresses user experiences and viewpoints about HashiCorp and its products, focusing particularly on licensing, pricing, and some users reporting issues with the interview process.",
      "A negative experience was shared with HashiCorp's VMware adapter, contributing to the overall critique.",
      "Homebrew's design choices are also scrutinized, specifically the decision not to include HashiCorp's Terraform in the package manager because of licensing fears."
    ],
    "points": 253,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1696766408
  },
  {
    "id": 37809516,
    "title": "A Raspberry Pi 5 is better than two Pi 4S",
    "originLink": "https://hackaday.com/2023/09/28/a-raspberry-pi-5-is-better-than-two-pi-4s/",
    "originBody": "Skip to content HACKADAY HOME BLOG HACKADAY.IO TINDIE HACKADAY PRIZE SUBMIT ABOUT October 9, 2023 A RASPBERRY PI 5 IS BETTER THAN TWO PI 4S 187 Comments by: Elliot Williams September 28, 2023 What’s as fast as two Raspberry Pi 4s? The brand-new Raspberry Pi 5, that’s what. And for only a $5 upcharge (with an asterisk), it’s going to the new go-to board from the British House of Fruity Single-Board Computers. But aside from the brute speed, it also has a number of cool features that will make using the board easier for a number of projects, and it’s going to be on sale in October. Raspberry Pi sent us one for review, and if you were just about to pick up a Pi 4 for a project that needs the speed, we’d say that you might wait a couple weeks until the Raspberry Pi 5 goes on sale. TWICE AS NICE On essentially every benchmark, the Raspberry Pi 5 comes in two to three times faster than the Pi 4. This is thanks to the new Broadcom BCM2712 system-on-chip (SOC) that runs four ARM A76s at 2.4 GHz instead of the Pi 4’s ARM A72s at 1.8 GHz. This gives the CPUs a roughly 2x – 3x advantage over the Pi 4. (Although the Pi 4 was eminently overclockable in the CM4 package.) The DRAM runs at double the clock speed. The video core is more efficient and pushes pixels about twice as fast. The new WiFi controller in the SOC allows about twice as much throughput to the same radio. Even the SD card interface is capable of running twice as fast, speeding up boot times to easily under 10 sec – maybe closer to 8 sec, but who’s counting? Heck, while we’re on factors of two, there are now two MIPI camera/display lines, so you can do stereo imaging straight off the board, or run a camera and external display simultaneously. And it’s capable of driving two 4k HDMI displays at 60 Hz. There are only two exceptions to the overall factor-of-two improvements. First, the Gigabyte Ethernet remains Gigabyte Ethernet, so that’s a one-ex. (We’re not sure who is running up against that constraint, but if it’s you, you’ll want an external network adapter.) But second, the new Broadcom SOC finally supports the ARM cryptography extensions, which make it 45x faster at AES, for instance. With TLS almost everywhere, this keeps crypto performance from becoming the bottleneck. Nice. All in all, most everything performance-related has been doubled or halved appropriately, and completely in line with the only formal benchmarks we’ve seen so far, it feels about twice as fast all around in our informal tests. Compared with a Pi 400 that I use frequently in the basement workshop, the Pi 5 is a lot snappier. MORE POWAH! Nothing comes for free. While the Raspberry Pi 5 is more efficient for the same workload than the Pi 4, you can push it still harder. And when you do, it draws a peak 12 W versus the Pi 4’s peak 8 W. And this is where we get to that price asterisk we mentioned in the opening. You might need to fork out for more power coming into the board, and figure out how to handle the heat coming off of it, if you’re computering hard. But first the good news. The Raspberry Pi 5 has an all-new power subsystem, featuring the DA9091 power-management IC, generating eight separate voltages and capable of supplying 20 A to the BCM2712 SoC. Apparently, this chip was co-developed between Raspberry Pi and Renesas, and it includes a real-time clock unit just because they could squeeze it in. It also supports USB-C Power Delivery, so finding a power supply that’s capable of supplying all that juice to the Pi 5 is a lot easier, something that has been a pain point in the past. Will we never see a brownout warning again? We can dream. The star of the new power management system, hands-down, is the power button. How many power button hacks have we seen over the years? We’re happy to bid them adieu. Now the bad news, in the immortal words of Stan Lee: with great power comes great cooling requirements. The Pi 5 runs hot enough that you might require a heatsink, or even an active cooling solution with a fan. Raspberry Pi shipped us an active cooling package to test out, and it plugs into a fan header on the board, so you know they mean business. Raspberry Pi has also re-worked their case for the Pi 5, adding a fan with a removable cover, and vents on the underside. And they haven’t forgotten the power button here either – a small piece of acrylic serves as both a button cap and a power status light. Nice. PCIE, FOR REAL THIS TIME The most exciting new feature for people who wish to use the Pi 5 on the desktop is probably the official support for a real PCIe lane. When the Pi 4 came out, it was discovered that it spoke PCIe between the USB controller and the SOC, and of course intercepting those lines was one of the first hacks that we saw on the then-new Pi 4. Then came the CM4, which forced you to design your own board anyway, so you could choose between USB and PCIe. With the Pi 5, you don’t have to choose, and you won’t have to hack on it either. But you will need an adapter. A single PCIe 2.0 lane is broken out to a flat-flex connector, and from there you’ll need an adapter board to connect it up to whichever peripherals you’ve got in mind. Adapters will doubtless come on the market soon, but if you just can’t wait, we’ve got a tutorial series on making your own PCIe devices to help. Once you get the connections sorted out, you might also try pushing it up to PCIe 3.0 speeds. [Jeff Geerling] got a preview hardware adapter from Raspberry Pi, and found that although it’s not certified for PCIe 3.0, it works most of the time at those speeds. With an NVMe hard drive attached, he found that he could get 450 MB/sec using the sanctioned PCIe 2.0, and almost 900 MB/sec by changing a line in /boot/config.txt, enabling the unsupported PCIe 3.0 mode, and crossing his fingers. That was easy. UNDER THE HOOD: THE RP1 CUSTOM CONTROLLER Power supply tweaks, including the power button, are down to Raspberry Pi’s cooperation with Renesas. More computational grunt comes from Broadcom’s new SOC. But features like the dual MIPI connectors or the dual USB 3.0 and USB 2.0 ports with enough bandwidth that they don’t crowd out each other or any of the other peripherals, are all due to Raspberry Pi’s in-house innovation here: the custom RP1 interface / southbridge chip. According to Eben Upton, Raspberry Pi’s CEO, “It’s basically a chiplet architecture: all the rage now, but very forward-thinking when we started the RP1 development program back in 2016.” Broadcom makes the SOC at a very fine feature scale, while Raspberry Pi can use larger and cheaper processes to handle the rest: Ethernet, USB, MIPI, analog video out, USART, I2C, I2S, PWM, and GPIO – everything but SDRAM, the SD card, and HDMI. The Raspberry Pi 5 uses PCIe for the backbone between the SOC and their RP1 chip. Four lanes of PCIe, to be exact, providing a 16 Gb/s link between the body and the brains. This is interesting because most chiplet designs are entirely proprietary, and both chips need to speak a common secret language. Here, Raspberry Pi and Broadcom can collaborate, but almost at arm’s length, because there’s nothing proprietary about PCIe. And because they had a spare PCIe channel on the SOC, they were able to break it out for the end user. Desoldering the RP1 and doing without all the peripherals it provides, patching the kernel appropriately, and turning the Pi 5 into an all-PCIe, five-channel monstrosity is left as an exercise to the motivated reader. ODDS AND ENDS The big yellow composite video-out is gone from the Raspberry Pi 5, but they broke out the lines for you to solder to if you want to hook it up to something other than HDMI. The old audio output jack has been removed entirely, so you’re probably going to have to rely on HDMI audio out or a HAT if you want hi-fi audio. Other connections include PoE on a four-pin header, an ARM debug / UART on a three pin header, and a JST battery connector to keep the real-time clock module ticking. Since you might want a heatsink, with fan or without, they’ve added mounting holes spaced around the processor. For space reasons, the MIPI camera/display flat-flex connectors use the thinner form factor that we’ve seen on the Pi Zero, rather than the wider one on the Pi 4. RASPBERRY PI EVOLUTION The Raspberry Pi 5 is, in some ways, a modest step forward. A two-times speedup isn’t anything to sneeze at, and the various quality-of-life improvements scattered throughout are great, but none of this is revolutionary when you look at the state of play in the SBC market. Still, the Pi 5 is at least twice as nice as the Pi 4, and at only a small upcharge. If you think back six months ago, where people were paying absurd markups for Pi 4s, the Raspberry Pi 5 is positively a bargain. And while there are faster Linux SBCs on the market these days, they also cost a lot more, so the value proposition of the Pi 5 is still solid. Add in Raspberry Pi’s documentation and software support, and there’s a lot here to like. They’re not available in stores just yet, but Raspberry Pi plans to have “just under a million” Pi 5s produced and in stores over the course of the rest of 2023, so they’re not going to be scarce — we hope! If you need the speed, and can handle the heat, there’s no reason not to get a Raspberry Pi 5. Posted in Current Events, Featured, News, Raspberry Pi, Slider Tagged new part, raspberry pi, Raspberry Pi 5 POST NAVIGATION ← THE QUESTIONABLE BENEFITS OF PAYING MORE FOR AIR QUALITY MONITORS HACKADAY PRIZE 2023: THIS DIFFERENTIAL SCOPE PROBE IS SMARTER THAN IT LOOKS → 187 THOUGHTS ON “A RASPBERRY PI 5 IS BETTER THAN TWO PI 4S” one says: September 28, 2023 at 11:01 am Gigabit Ethernet? Report comment Reply one says: September 28, 2023 at 11:02 am Huge png? Report comment Reply Gino Latino says: September 28, 2023 at 11:56 am my eyes are bleeding. Report comment Reply RW ver 0.0.3 says: September 28, 2023 at 3:01 pm Might even be 1Gbit this time instead of 700Mbit. Report comment Reply Gareth says: September 28, 2023 at 4:10 pm I rarely use my Pi400 but, when I do, it always makes me smile. No mention of a Pi500 and I’d struggle to make an arguement to produce one (other than “I want!”) Report comment Reply RW ver 0.0.3 says: September 28, 2023 at 7:44 pm Yes I’ve been on the fence about a Pi400 since they were released, but a little more oomph might tip me. Report comment Reply Elliot Williams says: September 28, 2023 at 11:55 pm Jenny wanted to know about the Pi 500 too! So I asked, and the answer was “it’s on the radar, but it’s a long way off b/c of other stuff with priority in the pipeline”. (Loosely paraphrased.) The Pi 400 is awesome. I use one as a CNC driver, and my young son uses his to learn computering. It’s perfect for these kind of light duty tasks. (Full disclosure: The one in the basement is the review model Pi sent me. The other, my wife bought for him without even asking me!) But I do think that my conclusion that the Pi 400 was finally the end-goal of the RPi foundation — to make an everyman-computer that’s accessible, friendly, and Linux — still rings true. https://hackaday.com/2020/11/02/new-raspberry-pi-400-is-a-computer-in-a-keyboard-for-70/ Would a Pi 500 really bring all that much more to the table? Speed is only really an issue when opening browsers, or Thunderbird. Non-bloated software runs just fine as-is, with no need for a fan. IME/IMO/etc. Report comment Reply Foldi-One says: September 29, 2023 at 1:08 am Have you overclocked your Pi 400s at all, and are you running them off a fast SD card or external drive? I understand them to be the newer revision of the silicon that handles it better than the original Pi4 and I’ve had one of those cranked up enough the only remotely sensible workload I’ve managed to bog it down noticeably with is a complex FreeCAD model, and its been running that way nearly since the Pi4 launched. But IME the SD cards are often the bottleneck. But no personal experience with a 400 on the CM4 and normal Pi4. So yeah I’m not sure a Pi 500 would really bring anything for its core mission over the older one really. Though definitely would be a few relatively minor quality of life improvements I’d think, and in that form factor making that PCIe Lane directly a M.2 NVME slot probably makes sense, as the data devices speed really does tend to be the biggest bottleneck to using a Pi4 IME. Report comment Steve says: September 29, 2023 at 1:23 am Totally agree with you here. I’ve been seeing the fan and heatsink they’ve included for reviewers, in every article so far, and that’s telling me this thing is gonna run HOT when loaded. I do wonder if undervolting and/or underclocking would help with power consumption. The 400 is amazingly capable on its own though, and even with a 500 on the horizon, it’ll likely fulfill most tasks I’ll be throwing at it. I’ve been having fun with mine for a few weeks now. The only things I can see a RPi 5 or 500 doing that might make me consider upgrading (IFF the price isn’t ridiculous) are the PCIe connection (the possibilities with an adapter board and slot are almost endless), and the RTC. Everything else is “nice to have,” but not strictly necessary. Then again, you’re hearing from a guy who uses an HP 9816 PC from the early 1980s and thinks it’s pretty nifty. :D Report comment Oscar Goldman says: September 29, 2023 at 11:32 am The 400 and the 5 are brought down by their asinine micro-HDMI connectors. Just as we’re starting to be free of micro-USB, we’re seeing the continued proliferation of this trash. Talk about a crippling mistake. Report comment Rasmus Palazzi says: October 2, 2023 at 5:05 am What sets the Raspberry Pi 5 apart for me is its increased power delivery capability through the USB ports. The issue I encountered with both the Pi 4 and the Pi 400 was the requirement for numerous power cables to connect monitors, hard drives, and other peripherals. Report comment Andrew LB says: October 7, 2023 at 10:31 pm Foldi-One: If you want a good micro-sd card for your Pi, get the Kingston Canvas GO Plus. It’s basically the fastest card available. Here is my benchmark running an overclocked Pi4 @ 2.2ghz. https://pibenchmarks.com/benchmark/62297/ Report comment phox says: October 4, 2023 at 2:43 am Well, what I like about my Pi400 is that it’s so much like the Amiga/Atari of my childhood… A computer with integrated keyboard, no cooling fans needed. Sounds like Pi5 runs too hot for that.. what’s it worth if you have to clock it down to make it passive? Report comment Reply William says: September 29, 2023 at 8:33 am Pi 4 and 3 B+ had gigabit too (3 B+ was limited by the USB 2.0 bus however). Would have been nice if they could bump it to 2.5GBe to boost performance of NAS boxes. Report comment Reply CRJEEA says: September 28, 2023 at 11:03 am I think we might see our fair share of augmented reality headsets and robot vision projects with this board. Perhaps someone will find a way to rapidly switch the screen/camera ports to support two of each at the same time or break out one of the other PCIe lanes. Maybe even have two pi5s chatting at full speed. Although that’s a lot of amps to find. Perhaps we’ll just see coffee warmers. Report comment Reply Canuckfire says: September 28, 2023 at 11:47 am That RP1 chipset looks really interesting… If it really is just connected over pcie, you could put that on its own PCIe x4 card to get all of the gpio… if you had software support… in a normal x86 computer? I think I am more excited to see what the CM5 turns out to be and what that RP1 chip might turn up in next… Report comment Reply Canuckfire says: September 28, 2023 at 11:48 am Whoops, wrong stub Report comment Reply Eben Upton says: September 28, 2023 at 12:47 pm We had regular PCIe cards with it on during development. Eerie having an x86 desktop with the full Raspberry Pi I/O subsystem. Report comment Reply Pixelbrick says: September 28, 2023 at 2:30 pm Can you sell us such a thing please? Report comment Reply CRJEEA says: September 28, 2023 at 4:02 pm I second that. Having a desktop gpio card that works with raspi software would be awesome. Those cards exist, but are several hundred thousand pounds each. Having one on a raspi budget would be a game changer for some. Perhaps one that could fit in a laptop PCIe slot or similar, I can see that being great for robotics, even aimed at collages and universities perhaps. Report comment CRJEEA says: September 28, 2023 at 4:04 pm ^ several hundred to a thousand Report comment NiHaoMike says: September 28, 2023 at 4:54 pm If the state machine hardware in that can be used to connect a high speed ADC (more than 12 MSps, the limit for the classic FX2 in 2×8 bit mode), it could potentially be the next basis for a SDR. Report comment thom says: October 1, 2023 at 1:02 am When y’all left the little guys out in the cold(supply) I walked away from the Pi’s. (And I have AT LEAST one of every Pi ever made) “Fool me once” Report comment Reply Andrew LB says: October 7, 2023 at 10:35 pm Agreed. I spent over a year to get a second Pi4 and still haven’t gotten a Pi Zero w 2. Yet every day on youtube i’d see Jeff G showing off the stacks of Pi4’s he’s using to make an absolutely pointless array. Report comment Frank Zhao says: September 28, 2023 at 2:05 pm Coffee cup saucer shaped heat sinks! Make it happen! Report comment Reply Regulus says: September 28, 2023 at 11:13 am The encryption hardware acceleration and PCIe 2.0 x1 (or 3.0×1!!) port is pretty exciting news, as this might make for a nice router. I currently use a few Pi4s as VPN routers, and they have to work pretty hard to route traffic, and since all traffic goes via the same ethernet port (or USB adapters, which shares the same bandwidth anyway), maximum speeds without any packet inspection are 0.5Gb. I can see a future dedicated HAT with four gigabit ports attached to a PCIe 3 controller being a very nice expansion option. Report comment Reply fiddlingjunky says: September 28, 2023 at 11:27 am Jeff Geerling already hooked up a single-port 10Gb/s adapter up to the PCIe and was able to get ~6Gb/s (due to PCIe lane count). Report comment Reply James says: September 29, 2023 at 6:15 am I used to run a Pi4 as a VPN and it was frustratingly slow and glitchy, I recently swapped to an i5 NUC from 2016, with 8 gig of ram and it’s orders of magnitude faster while running two VMs :D Report comment Reply Andrew LB says: October 7, 2023 at 10:39 pm I use a Pi Zero for my Pi-Hole and one of those Lenovo M700 Tiny 1 Liter PCs with an i5-6500t, 8gb DDR4, etc which i got for $75 on ebay. Just had to add a hard drive. I’m running PFSense on it. Report comment Reply PWalsh says: September 28, 2023 at 11:16 am > Priced at $60 for the 4GB variant, and $80 for its 8GB sibling Yowzah. Microcontrollers powerful enough to run linux. Use one of these with an SSD as a desktop computer. Report comment Reply The Gambler says: September 28, 2023 at 11:50 am It really is sad that unlike their competition they refused to put a native m.2 slot or other sata interface. Support daily is growing for the orange and rock pi’s and IMHO they offer way more than the raspberry pi does. Report comment Reply Foldi-One says: September 28, 2023 at 12:19 pm I really don’t see it as a problem – pay a little extra and accept the still lower polish and support level of the other SBC to get that or one of their other differential features or pay a little extra compared to the performance/feature set if the long term proven support of the Pi family is worth it to you. Competition that doesn’t entirely suck is good for us all, but doesn’t mean each company has to produce identical products. Either way you can now the regular Pi family has an available PCIe lane use it for a drive if you want to, and I expect an adaptor and case for such a use will be very very cheap if the Pi5 can be kept in stock enough, as that is serious computing power for relatively low money. Plus for me not going M.2 for the Pi makes sense – it is in theory an (educational/industrial) ’embedded’ computer and electronics project brain SBC. So does it need M.2? (and also which keying, maybe the ‘wifi’ keying would be better?) I’d say no, for most users the cheap USB thumb drive or SD card is the better choice, but I’m happy the option looks to be coming to the regular Pi family. Report comment Reply Elliot Williams says: September 28, 2023 at 11:59 pm The Raspberries already have an M2 HAT demo. I’d guess that someone is making them in quantity by the time the Pi 5s hit the stores. Arya is already looking around for the pinout on the PCIe port. :) Report comment Reply ebastler says: October 1, 2023 at 4:27 am NVMe SSD on PCIe 2.0×1 ain’t particularly fun though. That’s like SATA II level of speeds. Report comment TG says: September 28, 2023 at 12:21 pm I think that’s a tactical decision to keep it true to being an experimentation platform instead of seeing them all bought up as el cheapo desktops Report comment Reply Anton says: September 29, 2023 at 12:15 am To be fair, the RPI 5 announcement blog post leans heavily on marketing these as el cheapo desktops. Report comment Reply Big Poppa says: October 1, 2023 at 4:21 am But will it play Youtube videos? Report comment BLMac says: September 28, 2023 at 3:05 pm I buy the Pi for its complete ecosystem. Other SBCs may offer some better features but time is money, and the Pi just works without me having to spend expensive hours fat-arsing around trying to get it to work. Report comment Reply James says: September 28, 2023 at 3:55 pm I did watch the Jeff Geerling video on the alternative SBC’s and sure they are much faster than the Pi, but only if you can get it to work for starters. Plus the lack of updates to the kernel for most of them. Heck, even the original model B can still run the latest release, albeit not particularly fast. Report comment Reply Johannes Burgel says: October 1, 2023 at 2:34 am Some of the other SBCs have upstream kernel support by now. It’s the Raspberry Pis that are lagging behind in this regard. Report comment Reply sword says: September 28, 2023 at 12:00 pm I mean I have done that with every version…. Report comment Reply Foldi-One says: September 28, 2023 at 12:35 pm for about two mins on the earliest Pi’s waiting for the first windows to open… Though the more recent models can cope with a desktop and probably the entirely of most users desktop needs and this one sounds like the only PC most people would ever need. Report comment Reply Eben Upton says: September 28, 2023 at 12:49 pm Indeed. I find myself forgetting I’m using a Raspberry Pi and not a “regular” PC on a regular basis now. JetStream JavaScript performance is about 2/3 of my MacBook Air. Report comment Reply mythoughts62 says: September 28, 2023 at 11:17 am I hope Argon comes out with an Argon One case for it soon, great cooling and good looks. Report comment Reply Daniel Dunn says: September 28, 2023 at 11:36 am I wish they’d just stop trying to make the Pi into a PC replacement. Take all the cash they spent on PCIe and faster CPUs and give us USB-C video or onboard battery support. Better machine learning capabilities will be cool though. Report comment Reply Thomas McNeill says: September 28, 2023 at 11:51 am Battery support with power management, and power states would be great. Although some of that might have been available as I haven’t looked at the Pi’s in years due to battery and sleep issues. Report comment Reply Anton says: September 29, 2023 at 12:17 am I think that this is actually the perfect thing to cut for cost and leave to third party addon boards. An external BMS requires practically zero support from the Pi, aside from maybe an I2C line. On the other hand, you can’t add PCIe to a chip that doesn’t support it. Report comment Reply Foldi-One says: September 28, 2023 at 12:31 pm USB-C’s DP-altmode video is way to niche still, in 5 years time maybe but for now too few display really supports USB-C video, including heaps of brand new ones. So most folks won’t be able to just use it – and needing an extra dongle to use your SBC is silly… Also better machine learning is exactly what they have given you – PCIe lets you just plug in your choice of ‘AI’ accelerator, most of which are probably just one kernel recompile away from working. I can agree with Thomas better power management would be very nice, and has been the biggest downside to the Pi family. Though it sounds like it is still low power enough at idle but on that in most cases it doesn’t matter, even on battery powered projects for the most part you have space and weight to play with to get the endurance you want. Report comment Reply Eben Upton says: September 28, 2023 at 1:30 pm This x100. I’m baffled by the enthusiasm for a standard that isn’t really a thing yet, when HDMI is basically everywhere and just works ™. If I’m reincarnated I’m coming back as USB-C, because then people will love me for no reason. Report comment Reply wilmore says: September 28, 2023 at 2:34 pm “Isn’t even a thing yet”? USB-C as a monitor input is all over the place. Cheap USB-C docking stations are everywhere and provide adaptation to full sized HDMI sockets–you know, the ones that devices actually use. Micro-HDMI is used by, uhh, *you*. Talk about niche. People only hate USB-C in the Rpi world because you keep releasing broken or cripled versions of it. Can we use proper USB-C chargers with this board? Did you remember to use two resistors instead of just one? Have you pondered supporting USB-PD at higher voltages so you can reduce power loss on your power connector? Report comment Reply Mr Purple says: September 28, 2023 at 8:12 pm Those monitors and docking stations are thunderbolt. I.E. thunderbolt through usb port. They are not compatible with USB-C. Thunderbolt is licensed through intel and I imagine it would not be simple to include it on the pi SOC Report comment Anton says: September 29, 2023 at 12:23 am This, 100%. I have only ever used Micro HDMI for RPI 4’s, and I’ve used USB-C for video output on something like 20 different laptops. I can never find those pesky Micro HDMI adapters, but there’s always a USB-C hub or two on hand. And to Mr Purple – No, wilmore is referring to DP Alt Mode docks/hubs/adapters/dongles, which are ubiquitous. Thunderbolt is only needed if you want an external GPU, AND Intel actually made the standard open as part of USB 4. Report comment willmore says: September 29, 2023 at 9:54 am No, Mr. Purple, they’re not TB, they’re USB. Even my phone support the docking station. Are you saying my phone has a TB port on it? Report comment Dan T says: September 28, 2023 at 3:11 pm If I may, it’s really very simple: I can buy a USB-C to HDMI cable anywhere, and I have several of them lying around at any given time. Best Buy carries ten varieties; every Target and Walmart has them. I know of exactly two stores in my metro area of 5 million people that carries any sort of micro HDMI cable. Report comment Reply Regulus says: September 28, 2023 at 4:31 pm Micro-HDMI is even less of “a thing” than USB-C DP Alt mode, which is supported by basically every monitor, and is trivially and easily converted to HDMI by cables everyone with a USB-C laptop is likely to have. Report comment Reply Oscar Goldman says: September 29, 2023 at 12:00 am 100% on the mark. Micro-HDMI is insulting trash. Report comment fanoush says: September 30, 2023 at 1:01 pm Not just to HDMI but also to DisplayPort (it is DP alt mode) so you can have both, and DP is cool in a way and is more open and can be chained too. Report comment Oscar Goldman says: September 29, 2023 at 12:01 am You should really inform yourself before broadcasting such ignorance. Report comment Reply Anton says: September 29, 2023 at 12:28 am Lol you’re talking to the CEO of Raspberry Pi. I think he’s informed enough on the technical side (USB-C can really be a mess to implement, as seen with the first board revision of the Pi 4), but perhaps it would benefit him him to ask his users what they think about Micro HDMI vs DP Alt Mode. Report comment Chad Page says: September 29, 2023 at 8:55 am IMO it’s where the puck is going – It allows for a one-cable connection to a monitor and other devices. There are already quite a few non-thunderbolt docking stations and devices that work with it (xreal air for example, which doesn’t make sense *without* USB C carrying everything) – and most higher end phones support it. It would be a huge boost to add it to a pi zero (albeit very unlikely/technically difficult) and an absolutely glaring omission by the time pi6 rolls around to not have it. Report comment Reply Chad Page says: September 29, 2023 at 8:56 am (oops, I meant a potential 1-cable connection to a hub-equipped monitor… power+video) Report comment Ostracus says: September 28, 2023 at 2:50 pm Some display tablets use it instead of HDMI. Also I believe it was Jenny that covered HDMI vs DP and why the latter is better. Report comment Reply Elliot Williams says: September 29, 2023 at 12:03 am Arya on that one, but yeah. And anecdotally, I have only VGA and DP on my ancient X220. Gave a presentation a month ago. Guess which one’s easier to get into an HDMI workflow. (Displayport) Report comment Reply spaceminions says: September 28, 2023 at 2:53 pm At work, I’ve been supporting USB-C laptop docks with various IO including 2 displayport outputs. In the last few years that they’ve been in use, they’ve been a big hit for letting people plug in only one cable to their laptop, while no particular earthshattering issues spring to mind. I wouldn’t remove the built-in hdmi output but if you’re going to try and use one of these as a real computer, getting more stuff to fit into the same form factor seems like exactly the sort of thing they’ve been doing this whole time to further that goal. Else they’d use a full-sized HDMI – the only reason I have ever seen a small hdmi was for this; I guarantee every monitor and TV I own requires a full sized connector as input. Or they’d use a more durable form of storage. Or they’d keep the analog audio for people who need basic sounds to come out without an adapter. That said, I am still using a Pi2, because I just need to hit 100mbit for lightweight services, and if I could power it by POE instead of usb I would do that to cut my number of cables in half. *shrug* Report comment Reply Doc Oct says: September 28, 2023 at 9:08 pm Huh? There are hundreds of portable monitors and usb-c “docking” stations on Amazon that support it. I’ve seen a few desktop monitors that have it, but I agree it’s not really everywhere there yet… but you can get cheap hub like units with 1-3 video outputs from usb-c, and things like those Steam Deck docking stations. Report comment Reply Oscar Goldman says: September 28, 2023 at 11:59 pm What are you talking about? Any reasonable monitor supports DisplayPort. This Pi doesn’t even give us competent HDMI, but rather a POS micro-HDMI port. Micro-HDMI is offensive trash. They ruined the 400 with it, and it’s sad to see this brain-dead trend continue. USB-C with DP would be a much better option, seeing as how you can buy cheap and reliable DP-to-HDMI dongles if you need one. Report comment Reply Foldi-One says: September 29, 2023 at 4:18 am There is a rather large gulf between supporting display port (which sadly IME is still rather rare to see done properly as monitors and TV still tend to have 3-4 HDMI inputs and maybe 1 or 2 DP) and supporting USB-C in its displayport alt mode. If you need a dock/dongle that probably requires active logic and external power vs a simple pin to pin copper adaptor if you don’t just have Mini-HDMI cables… And implementing video over USB-C properly isn’t simple or cheap compared to just putting in a DP or HDMI connector. Plus to avoid being another contributor to the barely compatible this port only does x USB-C mess you really should be implementing the entire USB-C spec on every port you provide (at least if your device is capable of doing that part of the spec at all), so USB-PD, PCIe alt mode, display, legacy USB etc… The only Pi product that a USB-C port doing more than just power input might make sense for at the present time is the next model in Zero lineup, as its such a small footprint that one power port doing everything the SOC can do just about makes sense, but it would still add to the cost… Report comment Reply wvbeale says: October 4, 2023 at 5:04 am I’d find those added costs to be well worth it — and there are others just like me. (anyone who might disagree is wrong, and probably a bad person) Report comment ag says: October 5, 2023 at 2:22 am Give us two usbc connectors that are able to do dp- alt mode and proper usb / usbotg where at least on of them accepts power input. instead of the micro hdmi connectors. Would be way more flexible and useful. Report comment Bob says: September 28, 2023 at 11:43 am This is such a coincidence. This morning I was at work, looking for a RPI 4 online because I need a new pi. A colleague just told me that the prices dropped in the last few weeks. Suddenly, I get a popup of a new e-mail, from the pi hut, stating “NEW Raspberry Pi 5!!!”. That was an easy buy. Report comment Reply Andrea says: September 28, 2023 at 11:49 am i was reading that find a power supply will be more hard, because it’s true that support PD, but only at 5 volts… and it will need 5A at five volts… all PD power supply that i have they stop at 3A for 5Volt… so i’ts will be very hard to find a power supply for it :'( Report comment Reply N says: September 28, 2023 at 1:41 pm Agreed, PD sucks ass. Not interested in a universal connector that might just transfer data, might deliver 120 watts. Report comment Reply ebastler says: October 1, 2023 at 4:38 am PD is awesome, but the RPI doesn’t follow PD specs anyway so just buy the official PSU and be happy. Otherwise you’re in for a lot of pain. Report comment Reply Andrew says: September 28, 2023 at 1:49 pm No, it won’t. The official 5V 5A PSU will be affordable and widely available. Report comment Reply filiph says: September 28, 2023 at 9:04 pm see the past year I must admit that I’m scared that they will have very little, as usual and the pice Will rise like hell… as always with raspberry thing Report comment Reply willmore says: September 28, 2023 at 2:36 pm Agreed. If you want a normal USB-PD power supply that will provide 5V/5A, you need to find a >60W one and you’ll need to use e-marked cables. Or you could use the special one the foundation wants to sell you. So, you might as well bundle that into the price of the SBC. Oh, don’t forget you need the HSF assembly or it’ll throttle hard. So, add that in as well. Maybe a few micro-HDMI to normal HDMI cables so it can be used with common displays…. Report comment Reply celmanx says: September 28, 2023 at 9:02 pm my 67W PD Power supply support only 3A on 5 volts, my laptop PD power supply is 65W and i check and tha maximum on 5 volt the maximum is 2A. So it’s not so simple… Report comment Reply willmore says: September 29, 2023 at 9:59 am Exactly. I keep seeing people spouting about the USB-PD spec saying these supplies are legit brecause of table 10.2, but skipping over the four reasons in table 10.1 which say they should not be made. Report comment Reply ian 42 says: September 29, 2023 at 9:31 pm Yep, for something sucking this much power they should have gone to 12v supply.. Report comment Reply ebastler says: October 1, 2023 at 4:34 am PD Specs only allow up to 3 A at 5 V, everything above is put of spec. It also seems that the Pi needs 5.1 V for stable operation once again, so it will not work with most PD supplies in general, as target voltage is 5.00 V and specs allow for 4.75 to 5.25 V for compliant units (most stay within 4.95 to 5.05, but that’s not enough not to get under voltage warnings on a Pi5 according to some people who tested early models on Twitter with a variety of power supplies). The complete disregard for anything USB specs of the RPI foundation is truly impressive, not even hobbyists do as much scuffed non-standard stuff, and this is for mass market… Absolutely insane. Oh, according to a post on the RPI forum it will _default_ to 5 V 3 A in the total absence of negotiation on the USB input. A-to-C cable to an old phone charger, or notebook? Your source better have working protections or the Pi will burn them down. Again, obviously, not following USB Standards in the slightest. No negotiations = 5 V 500 mA. Nothing more. I _really_ hope that’s a wrongly worded post I read there. If not it would be absolutely crazy. Even the myriad of Chinese low cost SBCs manages to follow USB specs. This is inexcusable. Report comment Reply Truth says: September 28, 2023 at 12:20 pm My guess as to the “VideoCore VII”: VideoCore IV @ 250MHz: 250 [MHz] x 3 [slice] x 4 [qpu/slice] x 4 [processor] x 2 [op/clock] = 24 Gflop/s VideoCore IV @ 300MHz: 300 [MHz] x 3 [slice] x 4 [qpu/slice] x 4 [processor] x 2 [op/clock] = 28.8 Gflop/s VideoCore VI @ 500MHz: 500 [MHz] x 2 [slice] x 4 [qpu/slice] x 4 [processor] x 2 [op/clock] = 32 Gflop/s #VideoCore VII @ 800MHz: 800 [MHz] x 2 [slice] x 4 [qpu/slice] x 4 [processor] x 2 [op/clock] = 51.2 Gflop/s “Today, that effort bears fruit, with the launch of Raspberry Pi 5: compared to Raspberry Pi 4, we have between two and three times the CPU and GPU performance; roughly twice the memory and I/O bandwidth; and for the first time we have Raspberry Pi silicon on a flagship Raspberry Pi device.” – https://www.raspberrypi.com/news/introducing-raspberry-pi-5/ VideoCore VII @ 800MHz: 800 [MHz] x 3 [slice] x 4 [qpu/slice] x 4 [processor] x 2 [op/clock] = 76.8 Gflop/s (ref: https://forums.raspberrypi.com/viewtopic.php?t=244519#p1533677 ) Report comment Reply Eben Upton says: September 28, 2023 at 12:51 pm Good guess. Report comment Reply Truth says: September 28, 2023 at 5:13 pm When I see a reply like yours I’m always reminded of a friend in university many years ago looking for a paper by Denis Richie on something or other. He posted a request to one of the news groups and the response was from you guessed it, saying something along the lines of “I sure I have that paper somewhere, I’ll uuencode it and post it here when I find it”. Sometimes it is easy to forget that people who do really cool things are just people. Report comment Reply Truth says: September 28, 2023 at 5:19 pm “Dennis Ritchie” even (Background: Known for creating of the C language with Ken Thompson; One of the developers of a tiny little thing no body have ever heard of called UNIX at Bell Labs, and co-author of the book The C Programming Language with Brian Kernighan – known to programmers as the “C Bible”). Report comment Reply Dave says: September 28, 2023 at 6:21 pm https://hackaday.com/2011/10/17/on-the-life-of-dennis-ritchie/ https://hackaday.com/2022/08/24/coffee-with-kernighan/ Report comment Brad says: September 28, 2023 at 12:42 pm When I can I buy the RP1 chips directly? The abstraction this offers – any core you want with whatever compute/RAM pairing you can manage – in tandem with a unified hardware/software subsystem that allows user applications (like those found in the Pi ecosystem) to speak just over their respective peripheral protocols (I2C, SPI, Ethernet) and PCIe to the main core would be huge. Report comment Reply Paulo says: September 29, 2023 at 12:25 am This! Imagine a PCIe card connected to something like a standard PC, a Zimaboard/Zimablade or even via an M.2 adapter in a mini PC. A few Windows or Raspberry piOS drivers and voilà! Report comment Reply d00med says: September 28, 2023 at 12:44 pm Interesting upgrades, but the increasing price point starts to put it into the territory of NUCs/mini PCs if you don’t need a Pi specific feature. I run my Pi’s headless and don’t do anything terribly computing intensive on them. Hopefully, there will be some lower price point versions coming out. Report comment Reply Foldi-One says: September 29, 2023 at 1:23 am Being the Pi folk if you need cheaper the older models are probably going to be available and cheaper for a very very long time, though price to performance won’t favour them. Though it is probably about time for a new Pi Zero, and for myself a new compute module would be nice. Report comment Reply Jim says: September 28, 2023 at 12:49 pm I only have two questions. 1. Will they actually be AVAILABLE? I spent a year looking to buy a RPi4 8G. 2. Is Mathematica and Wolfram language still available for the Pi for free? Report comment Reply rclark says: September 28, 2023 at 12:56 pm Adafruit has the 8GB boards in stock right now! Noticed they’ve had them off and on for the past month now…. Just a FYI. Report comment Reply rclark says: September 28, 2023 at 12:57 pm rpi-4 8GB boards that is… Report comment Reply Elliot Williams says: September 29, 2023 at 12:11 am Yeah, they’ve come back into wide availability. I wanted to start the piece off with a “just when we could buy Pi 4s again” snark, but I held back. :) Checked a local (German) electronics house and they had 2,000 in stock. Report comment Reply rclark says: September 28, 2023 at 12:52 pm “I wish they’d just stop trying to make the Pi into a PC replacement. ” I agree… But with the power and active cooling, it looks like that is the direction they are going with the SBC. Part of the RPI draw for me was always fanless operation with optional cooling depending on your needs. Luckily we have the Pico, Zeros and older RPI boards for the embedded/electronic/robotic needs. For my use I could care less about the video side of things. 99% of my use is headless RPIs. So all the ‘video’ chatter is just blah, blah, and blah :) . What did peak my interest was the USB 3.0 bandwidth and power. Hopefully now we can run ‘two’ drives (whether portable HDD or SSD) off the ports simultaneous and get full bandwidth with each. The several RPI-4s that I have in operation currently boot from a Samsung T5 or T7 500GB drive. Only one right now uses the SD card and it is a 4 legged robot. Be nice that I could now use ‘any’ available SSD (of any size) and two simultaneously without the PI chocking. The RTC addition was nice. Looking forward to kicking the wheels so to speak these new cheap RPI-5 boards. I see pi-shop already has them up on their website (with ‘Notity Me’ status) . I do wish an RP2040 was on board though. With all the component changes, a new row of headers could have been added :) . But again, it looks more like the ‘focus’ is desktop rather than hobbyist IO interfacing. Rely more on ‘hats’ for that and or interface through USB a Pico board. Report comment Reply Andrew says: September 28, 2023 at 1:50 pm * couldn’t care less Report comment Reply Paul says: September 28, 2023 at 4:10 pm Ah, heck. Pedants unite! * pique my interest. Report comment Reply Andrew says: September 28, 2023 at 5:35 pm * choking Report comment Reply Miles says: September 30, 2023 at 7:11 am I think you missed that there is now a PCIe slot (and rumours of an M.2 Hat). The inexpensive storage has moved on to NVMe drives (I got a 2TB 3,400MB/s for $42 shipped last month). If you have an endless supply of free 2.5″ for a NAS maybe it makes sense (hey maybe you do, I don’t know). I agree that proper power is something that we’ve needed for a long time, maybe next time around (Pi6?) We’ll get 9v power input and standard USB-C with Displayport Alt Mode. Because you can get a breakout for HDMI to USB-C for peanuts anywhere. And 5A at 5V is the crazy, it is non standard and against the spirit of the PD spec, if not technically the letter of it. Report comment Reply rclark says: September 30, 2023 at 10:11 am Not an endless supply … but I do have quite a few lying around both internal 2.5 SATA drives (from upgrading) and several USB 3.0 T5/T7 drives waiting in the wings. USB 3.0 is really fast enough for storage I’ve found. Don’t get me wrong, I like the NVMe slot and plan on using it at some point too, but it isn’t the all to end all :) . I just want to plug in ANY USB 3.0 device and have the power there to make it work properly — all ports simultaneously. Report comment Reply Mike says: September 28, 2023 at 12:57 pm Eben’s been banging that $35 drum for well over a decade, claiming that he wanted to make a machine that students could afford. With these prices, it looks like the Foundation might be starting to move away from that original mission. On the one hand – I can’t blame them. During the shortage (and even before!), they have been watching all of these third-party SBCs pop up with ever-increasing prices. On the other, though … if the Pi Foundation is going to stray from it’s core mission, it’s going to find a VERY crowded field it needs to compete in – and yes, this field is largely one of their own making, but at the same time, their community support will only take them so far. Report comment Reply rclark says: September 28, 2023 at 1:01 pm With current ‘inflation’ $55 is the new $35 … That said, the 1GB boards should come in for around $35-$40. Report comment Reply Mike says: September 28, 2023 at 2:08 pm $55 is quite a jump, but then again, this isn’t a 1GB board, as you pointed out. So far, I don’t know if they’re bothering to produce 1GB boards. Is there any mention of those as of yet? Report comment Reply Dan T says: September 28, 2023 at 3:26 pm The PCB has jumpers for 1, 2, 4, and 8GB configurations. Report comment Reply abjq says: September 29, 2023 at 6:11 am so we can have max 15GB? Report comment Miles says: September 30, 2023 at 2:47 pm The jumpers look to be tied to a single trace on the right, and ground on the left. So any or all pads look the same, if that trace even goes anyway. Jeff Geerling mentions in his video this is just to help you ID it quickly. (without needing to memorize the RAM chip codes). I also wonder if you can swap the ram. If it is a single package for all those GB they have to be somewhat pricey. (4 BGA on your average 16GB PC module would mean only 4GB per chip. This is not a DIMM or SO-DIMM because I don’t think there are LPDDR4X DIMMs) If you could source some LPDDR4, and I assume hot air re-work a 1GB Pi5 into a 4GB whether it would be worth it? Although I’m just hoping for more grunt for emulation, so 1GB might be enough. Especially if they make it into a Zero. But I assume the point of the RP1 chip is to make the platform a little more CPU/SOC agnostic. Love to see a Thunderbolt/USB4 version of that RP1 chip Also curious about if you get a ram error in the top half of a ram chip/stick why you can’t re-configure to use half the address space instead of wasting it or being forced to buy new RAM. Report comment rclark says: September 28, 2023 at 3:05 pm Only mention was they were coming ‘later’ after the initial 4s and 8s. ‘Later’ is kind of open ended word though :) . Report comment Reply James W Williams says: September 28, 2023 at 2:10 pm They may be counting on the 3A+ and the Zero 2 W to address the low cost Pi market. It would be nice to see those get at least 1GB of RAM. Report comment Reply Mike says: September 28, 2023 at 2:23 pm That it would – the Z2W is a surprisingly capable little machine, and the 3A+ is little more than just a slightly larger Z2W w/ USB-A. I suppose I could see those filling in for the Pi’s original role, if they can keep up stock on those two models. Report comment Reply PhillyCheezSteak says: September 29, 2023 at 2:54 am 3A also has full size HDMI, which I for one really appreciate. One less adapter to muck around with. Report comment Reply Toby says: September 28, 2023 at 3:17 pm And usb-c pd for power, I really like the Zero-s, but I would really like them even more w/ usb-c. Report comment Reply JohnU says: September 29, 2023 at 1:27 am Other Pi’s are still cheap, higher performance costs more – and $55 for something that is on par with a cheap PC or laptop is still pretty good. If you want a cheap PC or laptop just look in the nearest e-waste bin or on the various used marketplaces, you can get very capable machines for very little. Report comment Reply Andrew LB says: October 7, 2023 at 11:03 pm You can thank the current administration for the crazy high inflation. Heck, just the other day i spent $15 for a burger, fries, and drink at Carls Jr. Report comment Reply dlcarrier says: September 28, 2023 at 1:01 pm >Desoldering the RP1 and doing without all the peripherals it provides, patching the kernel appropriately, and turning the Pi 5 into an all-PCIe, five-channel monstrosity is left as an exercise to the motivated reader. I want to try the opposite: Put the RP1 on a PCIe card, and patch a desktop computer’s kernel to add MIPI, GPIO, PWM, and the various serial interfaces to a desktop computer. Report comment Reply James says: September 28, 2023 at 3:59 pm Eben said above in the comments that’s how they tested the RP1 Report comment Reply Vinny says: September 28, 2023 at 4:44 pm That would be interesting. Report comment Reply Andrew Dodd says: September 28, 2023 at 1:06 pm One thing not mentioned: They COMPLETELY removed any form of hardware video encode from the Pi5. Rather than fix the reason no one used it (severely outdated – 1080p in a 4k world), they outright removed it. What’s the point of having TWO 4-lane MIPI interfaces if you have no hardware video encode? If you’re shooting stills or low enough resolution that software encoding is viable, you don’t need anything more than a single 2-lane MIPI interface. Report comment Reply willmore says: September 28, 2023 at 2:41 pm Video input for AI processing with the AI engine that the chip also doesn’t have? Report comment Reply RW ver 0.0.3 says: September 28, 2023 at 3:06 pm Hang a nVidia 1030GTX on the PCIe ??? Report comment Reply come2 says: September 28, 2023 at 4:25 pm 4k world ??? Report comment Reply willmore says: September 28, 2023 at 5:00 pm Go to any appliance store and find a cheap TV. It’ll be 4K. I went through Microcenter the other day and I saw one display under 4K. It was 1366×768 and had a DVD player built in. It’s probably ever going to be purchased by dentist’s offices and people with little kids they want to babysit with a Disney DVD. Every other TV was 4K or greater. Report comment Reply Andrew Dodd says: September 29, 2023 at 8:04 am Yup, and you never see 4k cameras any more unless they’re dirt-cheap junkbox dashcams. Sony A6300 was released with 4k capability including oversampling from a 6k sensor in 2016 and that’s an entry-level unit Report comment Reply None says: September 29, 2023 at 2:53 am We use Pis to capture video sequences so, for us, this means we may be stuck with Pi4. Will have to test the performance. Report comment Reply Dodo says: October 1, 2023 at 2:35 am Such CPU can encode 1080p in software about as well as a low-quality hardware encoder. So it might be worthwhile to try that. Report comment Reply N says: September 28, 2023 at 1:35 pm Woohoo! Can’t wait to buy a closed board for $60 from the open source $5 computer company! Report comment Reply George Graves says: September 28, 2023 at 3:25 pm $5 computer Report comment Reply James says: September 28, 2023 at 4:00 pm Reversion of Ethernet placement is nice, hopefully someone can work out how to make it fit in the pi-top 3. Report comment Reply Wolfgang Black says: September 28, 2023 at 4:25 pm So a new Pi no one will have in stock and you can only buy at many times the list price? Report comment Reply Joshua says: September 28, 2023 at 4:36 pm Typical. Everyone talks about performance only. Greedy people.. What about backwards compatibility? Can older releases of Raspbian still run? What about self-booting software projects, do they still work? Report comment Reply Anton says: September 29, 2023 at 12:40 am Greedy? Those all sound like enterprise needs. If your business needs long-term hardware support, they shouldn’t be buying a board meant for educational use. For everyone else, a distro upgrade isn’t a huge deal. For that matter, the RPI foundation was very slow to start releasing 64-bit OS images for their 64-bit boards, specifically for backwards compatibility. Report comment Reply Mojo says: September 28, 2023 at 5:09 pm IMHO Pi as we knew it is dead. Pico is the exception. Being unobtainable for so long, Pico is the successor. Pi 4 and the latest iteration is something that is easily obtained by cheaper an more available boards. I truly wish that they had spent their time producing Pi 3 when people still needed them. Time to move forward. Report comment Reply PhillyCheezSteak says: September 28, 2023 at 10:52 pm Pico is an entirely different beast from the Pi as such. It’s a microcontroller like Arduino or ESP, as opposed to a single-board computer. Report comment Reply Anton says: September 29, 2023 at 12:43 am Sure, but it’s similar in spirit to the original RPI. A lot of people bought the Pi 1 so they could learn Python on a cheap computer with GPIO, and you can do that now on a Pico. It’s inexpensive, it’s easily obtainable, and it’s educational. Report comment Reply rclark says: September 29, 2023 at 9:19 am In one why you are absolutely right. For small (even medium) size projects the Pico W boards can replace the RPI ‘Zero w’ boards. Bonus with the 3 analog I/O on board. In fact I’ve done that with now two of my projects (Zeros went back in the parts box). There is absolutely no need for a full blown Linux system running when all you need is a few DI points to monitor, and/or operate a couple relays and even much more complex things. microPython and circuitPython run fine on the Pico as well as C/Assembly with an IP stack available. I wrote a small redis client for use with the Picos, for example, to get/put data around my internal network. So for me, I’ve really started using the Pico boards instead of Linux based RPI boards. But when you need a camera, hdmi interface, face recognition, logging, USB 3.0 needed, network services, etc. then out comes an RPI 3, 4, or now 5. I think the Pico was one of RPI Foundations best ideas to date. Like potato chips, you can’t have to many! Report comment Reply rclark says: September 29, 2023 at 9:30 am I must add for ‘educational’ school use, the 3, 4, 400, 5 boards make more sense in that you write your Python right on the board, most liking with a GUI, so much easier to ‘use’. The Pico requires a host system to update the code on the Pico like Thonny or rshell. Ie. Why have both, when you have a ‘all-in-one’ for learning. Plus you could get by with one RPI 4, say, with multiple users logged in too. So again for education, the RPI Linux based boards make more sense. Report comment Reply rclark says: September 29, 2023 at 9:41 am And for bonus, for the more advanced students, you can allow them to plug in a Pico to the RPI Linux board to program the Pico for some cool project! Completing the circle so to speak without much cost to the school. Report comment Reply chango says: September 28, 2023 at 7:46 pm Moving everything but the CPU, GPU, and RAM controller into an in-house developed ASIC sure would make it easy to put someone else’s SoC in place of Broadcom’s… Just sayin’ Report comment Reply willmore says: September 29, 2023 at 10:04 am My thoughts are similar. Wonder if we’ll ever see this chip for sale on its own. Report comment Reply PhillyCheezSteak says: September 28, 2023 at 8:57 pm The Pi 5 sounds nice but they’re definitely drifting away from “budget computers cheap enough to throw into random projects”. I just hope we see a Zero 3 or Model A5. Report comment Reply chango says: September 29, 2023 at 8:15 am Factoring in inflation since 2019, the Pi 4 4GB would cost $66 USD in 2023. That makes the new 4GB model $11 less than the 4B while bringing significant improvements. Report comment Reply Chris Nobel says: September 29, 2023 at 12:19 pm You don’t get the point. The B+ is fully capable for headless use, eg. signage, but do use less than 2W. It would be nice if the foundation went back to basics, an upgraded Zero (or a slimmed B+ or whatever) with ethernet, 2 usb A, dc plug (accepting 6-15V unregulated), ½-1G, resonably price and AVAILABILITY! Report comment Reply Olivier Barthelemy says: October 8, 2023 at 6:19 am That’s bunk. General inflation does not apply to IT. THe IBM PC was $1500 in 1981, an N100 x86 PC is $150 today. Report comment Reply C says: September 29, 2023 at 1:34 am Only BLE 5.0? Why not 5.2? No hardware video encoder? Then what’s the point of two camera interfaces? Strange design choices. Also not cheap. Report comment Reply JohnU says: September 29, 2023 at 1:36 am Loving all the comments along the lines of “All very nice but they’re stupid & wrong for not supporting ” It’s a frickin’ computer on a credit card for $55, if you don’t like it you can add a HAT or go make your own computer – Jay Carlson proved it’s pretty doable in fact: https://jaycarlson.net/embedded-linux/ No product is perfect or all things to all people, but the Pi has some pretty great design & engineering in it (and it’s actually made in the UK, which is fantastic) plus the support is way above any of the competition – we’ve just been evaluating the OrangePi 5 as an alternative to the CM4 and it’s immediately obvious the support / available information is way below the standard of the Pi foundation. Report comment Reply Piotrsko says: September 29, 2023 at 8:57 am Amen. Shut up and just take my money. Don’t matter much to me, I am not a power user, but under a franklin for a pc works any day Report comment Reply Miles says: September 30, 2023 at 2:59 pm You can get a Ryzen 2 or Intel 6th/7th gen system /with/ 8 or 16GB of ram, sensible power supplies (Standard, available at local stores, reasonable voltage and currents), a metal case, PCI expansion, GPU options, NVMe M.2 sockets, SATA bay and included copper heatsinks for a Franklin. Lenovo Tiny, HP Mini or Dell Micro. (HP 800G3 Mini starts at $27 barebones, add /any/ 6th or 7th gen processor $6-$20, and a couple sticks of 4GB DDR4 SO-DIMM, $10-15, and a 45-90w power brick $5-15. The math on the Pi as a general purpose desktop computer is lousy.) As some kind of portable or low power device the RPi makes some sense, but add a screen and battery and you can pick up a used netbook that is just better value period. Don’t get me wrong, I have plans to jam a Zero in a Data Frog SF2000 and a Zero2 in a PSP 1000 using some cheap but good screens, but at the end of the day I know for a fact an Ayn Loki Zero would have been better value if I’d kept my pre-order ($224 with tax and shipping) Report comment Reply Johnny says: September 29, 2023 at 3:33 am What RaspberryPi eco system needs right now is some magick-all-in-one-board solution that would handle a bunch of Li-poli cells, power RasPi itself and a bunch of extra stuff, like a keyboard and/or official display. Because Report comment Reply Nick says: September 29, 2023 at 5:11 am Is this the real Eben Upton? Thanks for posting, and nice work on the Pi. While the Pi 4 meets most of my needs, thanks to proper networking and more RAM, I can see how this new one really adds some welcome features. Breaking out the PCIe is a useful one, and better power management is welcome. I hope the Foundation also has a good supply chain these days! Report comment Reply Nick says: September 29, 2023 at 5:12 am Dammit, that was supposed to be a reply to the user claiming to be Eben Upton. Report comment Reply The Commenter Formerly Known As Ren says: September 29, 2023 at 7:10 pm Hackaday’s commenting system is based on the Monte Carlo (or is it the Monty Hall?) algorithm. Report comment Reply Mike says: September 29, 2023 at 7:43 pm I thought it was Money Python, myself. Report comment Reply Mike says: September 29, 2023 at 7:44 pm * Monty – though I suppose Money works too. :p Report comment The Commenter Formerly Known As Ren says: September 30, 2023 at 4:59 pm B^) Report comment Winston says: September 29, 2023 at 6:08 am So, when will these be out of stock for me to not buy? Report comment Reply The Commenter Formerly Known As Ren says: September 29, 2023 at 7:08 pm 3… 2… 1… Report comment Reply jawnhenry says: September 29, 2023 at 9:21 am Eben Upton says, “The RPI 3 makes a fantastic desktop computer.” Eben Upton says, “The RPI 4 makes a fantastic desktop computer.” Eben Upton says… Report comment Reply NiHaoMike says: September 30, 2023 at 5:47 am Even the original Pi makes a pretty good desktop computer if all you do is word processing/text editing and other light office work. It’s not intended for heavy gaming or video editing. Have you heard anyone say that a bicycle is junk because it’s nowhere as fast as a Tesla? Report comment Reply jawnhenry says: September 30, 2023 at 10:05 am Thank you very much for pointing out my error; I stand corrected; — Eben Upton says, “The RPI 3 makes a fantastic desktop computer.” Eben Upton says, “The RPI 4 makes a fantastic desktop computer.” Eben Upton says, “The RPI 5 makes a fantastic desktop computer.”; and, most importantly, A Raspberry Pi afficianado says that even the RPI 1 makes a fantastic desktop computer. Report comment Reply Leo says: September 29, 2023 at 10:40 am For all of you residents of developed countries, I think that 55 USD is generally only about 1/30th of a minimum wage. I would be very happy with that, because here in Brazil this plate will cost at least 1/2 of a minimum wage (taking into account taxes, shipping, etc). Report comment Reply Brenda says: September 29, 2023 at 12:10 pm What are we supposed to do about the lack of audio? Not everyone uses HDMI audio. Does the speakers in your monitor sound better than your stereo or home-theater? I guess they scored one against the Retro-Pi users, too. Report comment Reply rclark says: September 29, 2023 at 6:40 pm Use a USB sound device or an audio hat. I know the USB solution works as I did that for one of my builds. Report comment Reply Joshua says: September 30, 2023 at 7:05 am Stupid workaround, IMHO. Self-booting projects don’t necessarily support USB audio. Not everyone is using Linux on the Pi. There’s RISC OS, too, for example. Report comment Reply Foldi-One says: September 30, 2023 at 2:38 am In fairness anybody that wanted good sound out of a Pi didn’t use the 3.5mm anyway as it most certainly isn’t very good, it has always been HDMI, USB or something like the Wolfson audio card. I guess they could have gone through the effort for the tiny handful of folks that want great audio and not much else to improve that and stuff it in in Pi5 but I don’t think that would have been worth it for anybody. As either other features would be cut or the cost would have to go up some more, for something that can be fixed very very cheaply a number of ways depending on how you wish to use your Pi. Put that decision on the users, honestly I’m surprised they still found the board space to put in the video and that one makes way more sense to keep IMO. Report comment Reply NiHaoMike says: September 30, 2023 at 5:52 am I wonder if the composite output can be reprogrammed into a general purpose high speed DAC. Report comment Reply Joshua says: September 30, 2023 at 7:11 am “In fairness anybody that wanted good sound out of a Pi didn’t use the 3.5mm anyway as it most certainly isn’t very good, it has always been HDMI, USB or something like the Wolfson audio card. ” Yeah, but it was a standard device. Everything else requires especially written drivers. The 3,5mm audio was like the “PC Speaker” or Sound Blaster of the Pi platform. Removing it was so unnecessary. I mean, analog AV out is the #1 way of interfacing homebrew projects. Have an analog video monitor? Use Composite. Have an homebrew audio amp or something similar saved from electronic scrap? Use stereo audio. That’s what the Raspberry Pi is for! It’s a single board computer meant to interface electronic “trash” via analog i/o or GPIO pins. Not just a random Linux computer on the desk to run Chromium and VLC player on an 4k screen! Report comment Reply Foldi-One says: September 30, 2023 at 8:37 am The Pi folks and community put so much work in that it is just enable the device tree overlay and it works for most if not all HAT solutions. No work for you at all. And if you want to test your homebrew audio you will want to actually have a good signal input for it, which the inbuilt Pi audio hasn’t ever been – garbage in garbage out… And these days saying analogue anything is the number one way I’d say is trivial to prove false – almost all projects on HAD are most certainly predominately if not entirely digital, with digital interfaces between absolutely everything, often the only analogue parts at all are the fleshsack using it… If you could have everything it would be great to have it, but equally I’d want it to actually be sound worth having and just having composite at all IMO is kinda of surprising – not sure composite has seen any use outside of legacy security camera type stuff in this millennium almost! But nice to have it, as it is the one that is rather harder to get with a cheap external part, where audio is pretty darn trivial. Report comment Reply Joshua says: September 30, 2023 at 11:42 am “And if you want to test your homebrew audio you will want to actually have a good signal input for it, which the inbuilt Pi audio hasn’t ever been – garbage in garbage out… ” I see, that’s the world we’re living in now. People assume that criticism always is for the own benefit. But that’s not the case here. I’m not affected by the removal of the AV capabilities, I have my old Pi models for experiments. I’m worried about others. I’m worried that the Pi will loose connectivity with the analog world, loose touch with the past and future. That circuits in schools made on humble vero boards nolonger work. If someone needs an USB adapter or HAT/Shield for about everything very basic, then things go ad absurdum. PS: If you meant “we” (as in, everyone) when you said “you” then I’d like to apologize, of course. Report comment Foldi-One says: October 1, 2023 at 1:43 am You can actually find a school that lets the pupils really use vero board?!?! Impressive, round here last I heard anything actually letting kids play and learn with the more Lego style safe enough for toddlers build a circuit educational toys was uncommon. Though possible for a chronic lack of teachers that actually understand sciences well enough. And yes I was meaning people in general with that you – as if you or anybody else want to test their own audio work, well it rather helps for the input signal to actually be good enough to know if the failure is in what you made. Report comment wvbeale says: October 4, 2023 at 5:27 am Excuse me, sir – but let’s leave VLC Player out of this. Report comment Reply bicyclesonthemoon says: October 3, 2023 at 8:46 am It’s disappointing to see no audio output. What do they think people don’t need sound anymore? Ok, if you connect with hdmi to tv, there is high probability that the tv will have sound. But if you connect to normal monitor then there is no sound and then what? Alternative is use, usb, gpio, hat, or bluetooth. Each of these has problems, as it requires either adapter, or device, or is taking some resource. audio jack is cool because it JUST WORKS. with no anything. I don’t want high quality audio (there are better choices for that). I just want to hear what’s going on. Connect any normal speaker and hear! I was already disappointed by lack of audio jack in pi 400 which I have. There, it had enough place for it but they didn’t. I thought well, they will see the mistake and when they make the next version of 400 or however they will call it, they will include audio output. But now I see they made pi 5 without sound output it makes me lose hope if it’s going in good direction. Report comment Reply Edward J Crutchley says: September 29, 2023 at 1:14 pm $5? Ummmmm, but 60-35= 25 NOT $5, even the $45 model, that’s $15 NOT $5. Sure the 4 GB version of the Pi 4 was $55, but that ignores the 2GB and 1GB versions. Why are they dead set on pricing themselves out of the maker market? Report comment Reply rclark says: September 29, 2023 at 6:08 pm ??? I would think in the maker market, the Pico is your buddy with the occasional Linux based board used when there a real need. Even at that, a RPI Zero, Zero 2, is priced low low if you need a Linux system. I personally don’t think the prices are that bad for what you are getting in return for a RPI 3, 4, or 5. What am I not seeing here? Report comment Reply shod says: September 29, 2023 at 6:03 pm Minus 1: Needs a fan and a lot of power Minus 2: Only HW accelerates H265 I hear If only they could have given the stuff it adds without drawbacks. Report comment Reply Foldi-One says: September 30, 2023 at 2:46 am From what Jeff Gearling has said it sounds like it doesn’t need a fan or heatsink to outperform a properly cooled Pi4. I’d call that plenty of performance out of the box with no effort. Sure it will throttle under a real load but if Apple can sell Mac Airs that last less time before throttling, with no option to add cooling for such huge prices I think you can give the Pi5 a pass there… Report comment Reply Joshua says: September 30, 2023 at 11:44 am To be fair, the Pi losts a lot of basic HW decoding already. The Pi 3 still could decode MPEG1+2. Report comment Reply fanoush says: September 30, 2023 at 1:31 pm It does not need a fan. May still run well with those passive radiator-all-heatsink cases that were available also for pi4. And it is said to still run from 3A power supply if you don’t need power hungry stuff plugged into usb 3 ports. Report comment Reply CMH62 says: September 29, 2023 at 6:56 pm Thx for the detailed writeup, Elliot! I want one even though I’m not immediately sure what exactly I’d use it for! :-) Report comment Reply The Commenter Formerly Known As Ren says: September 29, 2023 at 7:01 pm Ditto! Report comment Reply ytrewq says: October 1, 2023 at 2:29 pm Too little too late, and no M.2 slot (can understand) or eMMC on board (just why?). Today there are much more interesting devices from the many competitors. Report comment Reply rclark says: October 1, 2023 at 5:31 pm To late? Naw. I bet they get gobbled up just like the Pi-4s, 3s, 2s, 1s, Zeros did/do. Only western competitor is the Beaglebone that I know of. I know I have a 5 pre-ordered and I bet I am not the only one. I currently have five 4s powered up with one doing real work… Point is I’ll probably buy a few more 5s over the next few years as well. To late? Don’t think so, just an other interesting iteration of the RPI eco-system. Report comment Reply James Reed Feeney says: October 2, 2023 at 6:28 pm The semiconductor industry is still coming out of Covid related supply chain problems. Pi availability will come back soon. Report comment Reply OnceAHubRatatouie says: October 2, 2023 at 3:22 pm Would have been better if M.2 was on the board itself but… 1) VideoCore VII drivers, are they actually open source or just stubs (of some degree)? 2) The whole PCIe issue that Jeff Geerling was working around of BAR for PCIe devices, specifically video cards – (the idea being the CPU can have the entirely of the dGPU’s VRAM in addressable range)… So, hopefully there are options for BAR / reBAR and GPUs of all types can work on RPi5. Report comment Reply James Reed Feeney says: October 2, 2023 at 6:20 pm The RP line of PCs is starting to get interesting. If you need a small controller, you can get a Pi Zero. If you need a bit more computational horse power, you get a Pi3, or Pi4, and finally you want a fire breathing monster Pi, then The Pi5 is for you. I wonder how long it will take for them to get Windows11 on the Pi5? Report comment Reply rclark says: October 5, 2023 at 12:54 pm Hopefully never (get Windoze 11) on RPI-5. Linux is perfectly up to the OS job here as well as all tasks in my home. Windoze = Waste of time. Report comment Reply James Reed Feeney says: October 2, 2023 at 6:25 pm I’ve heard of a Pi-femto. (1) I/O pin. Is that correct? Report comment Reply Olivier Barthelemy says: October 8, 2023 at 6:13 am Calling a Southbridge (a concept from… 30 yrs ago ?) a chiplet is marketingspeak at its worst. Report comment Reply Darkstar says: October 8, 2023 at 6:41 am “Gigabyte Ethernet”? As in 10GbE? That would be awesome but I doubt the little SoC can push out that much data… Report comment Reply John says: October 8, 2023 at 11:33 am If a Raspberry Pi 5 is better than two Raspberry Pi 4s, then is an Orange Pi 5 worth three? It has four cores that’re the same as the Raspberry Pi 5’s (Cortex A76), plus four slower (more efficient) Cortex A55 cores, plus it comes in 16 and 32 gig version, if you need the memory. I wonder how long it’ll be until we can get Raspberry Pi 5 boards with at least 8 gigs. Report comment Reply Leave a Reply Please be kind and respectful to help make the comments section excellent. (Comment Policy) This site uses Akismet to reduce spam. Learn how your comment data is processed. SEARCH Search for: NEVER MISS A HACK Follow On Facebook Follow On Twitter Follow On Youtube Follow On Rss Contact Us SUBSCRIBE IF YOU MISSED IT THIS WEEK IN SECURITY: LOONEY TUNABLES, NOT A 0-DAY*, AND CURL WARNING 6 Comments ROBOTIC MIC SWARM HELPS PULL VOICES OUT OF CROWDED ROOM OF MULTIPLE SPEAKERS 22 Comments CHIP SHORTAGE ENGINEERING: MISUSING DIP PACKAGES 54 Comments WHY WALKING TANKS NEVER BECAME A THING 62 Comments A RASPBERRY PI 5 IS BETTER THAN TWO PI 4S 187 Comments More from this category CATEGORIES Categories Select Category 3d Printer hacks Android Hacks Arduino Hacks ARM Art Artificial Intelligence Ask Hackaday ATtiny Hacks Battery Hacks Beer Hacks Biography blackberry hacks Business car hacks Cellphone Hacks chemistry hacks classic hacks clock hacks cnc hacks computer hacks cons contests cooking hacks Crowd Funding Curated Current Events Cyberdecks digital audio hacks digital cameras hacks downloads hacks drone hacks Engine Hacks Engineering Fail of the Week Featured Fiction firefox hacks FPGA g1 hacks Games google hacks gps hacks green hacks Hackaday Columns Hackaday links Hackaday Store Hackerspaces HackIt handhelds hacks hardware High Voltage History Holiday Hacks home entertainment hacks home hacks how-to Interest internet hacks Interviews iphone hacks ipod hacks Kindle hacks Kinect hacks laptops hacks Laser Hacks LED Hacks Lifehacks Linux Hacks lockpicking hacks Mac Hacks Machine Learning Major Tom Medical Hacks Microcontrollers Misc Hacks Multitouch Hacks Musical Hacks Netbook Hacks Network Hacks News Nintendo DS Hacks Nintendo Game Boy Hacks Nintendo Hacks Nintendo Wii Hacks Nook Hacks Original Art Palm Pre Hacks Parts PCB Hacks Peripherals Hacks Phone Hacks Playstation Hacks Podcasts Portable Audio Hacks Portable Video Hacks PSP Hacks Radio Hacks Rants Raspberry Pi Repair Hacks Retrocomputing Retrotechtacular Reverse Engineering Reviews Robots Hacks Roundup Science Security Hacks Skills Slider Software Development Software Hacks Solar Hacks Space Tablet Hacks Teardown Tech Hacks The Hackaday Prize Tool Hacks Toy Hacks Transportation Hacks Uncategorized Video Hacks Virtual Reality Weapons Hacks Wearable Hacks Weekly Roundup Wireless Hacks Xbox Hacks OUR COLUMNS HACKADAY LINKS: OCTOBER 8, 2023 9 Comments TRY IT OUT 17 Comments ISS MIMIC BRINGS SPACE STATION DOWN TO EARTH 20 Comments HACKADAY PODCAST 239: OVERCLOCKING, OSCILLOSCOPES, AND OH NO! SMD OUT OF STOCK! 1 Comment HACKADAY SUPERCONFERENCE 2023: FIRST ROUND OF SPEAKERS ANNOUNCED! 6 Comments More from this category RECENT COMMENTS Nick on Commodore 64 Web Server Brings 8-Bit Into The Future BrightBlueJim on Sine-wave Speech Demonstrates An Auditory One-way Door Joseph Eoff on Blaupunkt Tube Radio Is The Sultan Of Radios BrightBlueJim on Sine-wave Speech Demonstrates An Auditory One-way Door Timo on Virtual Mini Pinball Cabinet Scores Big Zamorano on Hats Off To Another Weird Keyboard From Google Japan Joseph Eoff on Blaupunkt Tube Radio Is The Sultan Of Radios Richard H on Blaupunkt Tube Radio Is The Sultan Of Radios Nick on Watch Out SiC, Diamond Power Semiconductors Are Coming For You! pelrun on 3D Printed Mini Drone Test Gimbal NOW ON HACKADAY.IO P4trick liked $5 WiFi Busy Light - 2021 HaD Prize Entry. goku wrote a comment on Telegram Control. Mirko has updated the project titled Openthing 2 - RepTrap. Pamungkas Sumasta has added a new log for Generative kAiboard. Paul McClay has updated the log for Minamil 3dp: another minimal CNC mill. Just4Fun has updated details to Z80-MBC2: a 4 ICs homebrew Z80 computer. John Sutter liked PY32F002A F15P Development Board. John Sutter liked ARM Devboard From a Used-up Temperature Logger. mihai.cuciuc liked Stable Cable Table. JP Gleyzes has updated details to Long Range Weather Station (65€). HOME BLOG HACKADAY.IO TINDIE HACKADAY PRIZE VIDEO SUBMIT A TIP ABOUT CONTACT US NEVER MISS A HACK Follow On Facebook Follow On Twitter Follow On Youtube Follow On Rss Contact Us SUBSCRIBE TO NEWSLETTER Copyright © 2023Hackaday, Hack A Day, and the Skull and Wrenches Logo are Trademarks of Hackaday.comPrivacy PolicyTerms of ServiceDigital Services Act Powered by WordPress VIP By using our website and services, you expressly agree to the placement of our performance, functionality and advertising cookies. Learn more OK",
    "commentLink": "https://news.ycombinator.com/item?id=37809516",
    "commentBody": "A Raspberry Pi 5 is better than two Pi 4SHacker NewspastloginA Raspberry Pi 5 is better than two Pi 4S (hackaday.com) 245 points by MrBuddyCasino 22 hours ago| hidepastfavorite185 comments p1mrx 19 hours ago> It also supports USB-C Power Delivery, so finding a power supply that’s capable of supplying all that juice to the Pi 5 is a lot easierIsn&#x27;t this mostly false? The Pi4 uses 15W at 5V3A. The standard says, to exceed 15W you should increase the voltage, but Pi5 increased the amperage instead. 5V5A USB-PD supplies are almost unheard of.I wonder how much a 9V3A -> 5V5A buck converter dongle would cost. reply geerlingguy 16 hours agoparentFrom my anecdotal evidence, I&#x27;ve found so far only the Radxa 30W adapter can negotiate 5v5a, most others only go up to 3A, and a few rare ones to 4a, but none I can find in the US.Even if the power adapter supplies it, without a high quality cable there&#x27;s enough voltage drop to trigger the low voltage warning.I recommend the official supply or eventually the official new PoE+ HAT for now. reply regularfry 18 hours agoparentprevProbably about the same as the official supply, you&#x27;ll probably start seeing them pop up. The current price I can see for the 5V5A supply is £11.90.They go into the trade-off in one of the launch videos. Apparently if you&#x27;ve only got a 3A supply then the downstream USB power gets limited but otherwise it works. reply coder543 19 hours agoparentprevI don’t agree. I found this straightforward statement:https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;16712&#x2F;usbc-power-delivery-hit... > USB-PD R3.1 supports three charging models: > - Fixed voltage > - Programmable power supply (PPS), and > - Adjustable voltage supply (AVS). > In the fixed voltage scheme, the Standard Power Range (SPR) mode supports 3A and 5A at 5V, 9V, 15V, and 20V.That’s about as clear as it can be, assuming AnandTech is correct.Also keep in mind the existence of PPS as another data point. I have an Anker charger sitting in front of me that offers 3.3V-11V at 5A. 5V at 5A falls squarely in that range, as long as you have an e-marked cable that can support 5A. (I have no idea if the Pi 5 supports negotiating PPS, but it would increase compatibility with chargers if it does.)I agree the Pi 5 should have included a buck converter or something, but I don’t think it’s correct to say they’re not standards compliant. It’s just not a common use of the standard. reply KennyBlanken 18 hours agorootparentIt&#x27;s not hard to find at all.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;USB_hardware#USB_Power_Deliver...5v3A is the limit of USB PD SPR. reply coder543 18 hours agorootparentNo, Wikipedia is looking like it&#x27;s out of date or wrong. It’s not a definitive source, especially for highly technical topics like this.Here is another industry source which has a table that also says 5V at 5A: https:&#x2F;&#x2F;www.graniteriverlabs.com&#x2F;en-us&#x2F;technical-blog&#x2F;usb-po...In fact, using Wikipedia’s sources, I found the actual specs and here is another piece of supporting evidence for these other articles:> The Fixed PDOs Maximum Current field Shall advertise at least 3A, but May advertise up to RoundUp (PDP&#x2F;Voltage) to the nearest 10mA. Requires a 5A cable if over 3A is advertised.[0]This is a footnote attached to 5V3A, indicating to me that chargers are allowed to offer more than 3A at 5V.At a minimum, the spec seems to be ambiguously written, but I’ve only spent a few minutes skimming it. Multiple industry sources (previously linked) believe that 5V@5A is within the spec, even without using PPS.[0]: “USB_PD_R3_1 V1.8 2023-04” reply Denvercoder9 18 hours agorootparentYou&#x27;re both correct, but USB Power Delivery, like so many things USB, is a total mess with different revisions, and versions of those revisions, that each extend and deprecate parts of the previous revision. I&#x27;m going to ignore the actual protocol used for negotiation here, but there have also been at least three different protocols.The first revision, rev 1.0, had six fixed power profiles: 5V&#x2F;2A, 12V&#x2F;1.5A, 12V&#x2F;3.0A, 12V&#x2F;5.0A, 20V&#x2F;3.0A and 20V&#x2F;5.0A. These were deprecated by rev 2.0, version 1.2, which instead introduced power rules at four fixed voltages, supporting power supplies with different output powers. The maximum supported currents by the specification were 5V&#x2F;3A, 9V&#x2F;3A, 15V&#x2F;3A and 20V&#x2F;5A. These power rules were retroactively named the Standard Power Range (SPR) in rev 3.1. That revision also added the Extended Power Range (EPR), which raised all the current limits up to 5A, but only when used with electronically marked cables. EPR also added power rules at 28V, 36V and 48V, again requiring (differently) marked cables and up to 5A. So 5V&#x2F;5A is a valid option, but only when used with a USB PD rev 3.1 compatible power supply and cable.Orthogonally to the fixed power rules, rev 3.0 introduced the Programmable Power Supply, which allows a configurable voltage between 3.3V and 21V in steps of 20 mV. This was extended by rev 3.1 with voltages between 15V and 48V in steps of 100 mV, called the Adjustable Voltage Supply. As far as I&#x27;m aware this is not commonly used, and most products advertising Power Delivery support use the predefined power rules. reply Wowfunhappy 14 hours agorootparentI hate USB-C. What a mess.What good is a universal port if it&#x27;s not actually universal? I would actually prefer different shapes, because then you could be pretty confident that if everything fit it was going to work. Like how USB used to be. reply Guvante 12 hours agorootparentIs it really worse now than it was before?Having a plug on say a laptop that can be a charger, a HID, a drive, a monitor, or all of the above with a docking station is vastly superior to the old day IMHO.Unfortunately the cable to do that isn&#x27;t cheap and is massively overkill for say a phone charger. Thus cable compatibility becomes a thing but looking at HDMI cable compatibility is always a thing.I think the problem is what used to be complaints about niche connectors are complaints about USB-C and it feels different and more impactful even if it isn&#x27;t necessarily.After all I know back in the Mini-USB days you had manufacturers skipping the data pins to save some copper leading to frustration when you couldn&#x27;t transfer files. reply yftsui 11 hours agorootparentIt is really worse. I have ton of USB-C cables doesn’t transfer data from flash lights, a lot of USB 2.0 speed cables with USB-C connector, then USB 3.0 cables rated at 5G&#x2F;10Gbps without an easy way to know the difference, then Thunderbolt 3 &#x2F; 4 cables one each.Besides Thunderbolt cables, there is not a standard nor universal way to identity them. I had to test each cable by plug in an SSD then speed test the SSD.Flash back 15 years, a cable with MicroUSB either transfer data, or not. The difference between high speed &#x2F; super speed only lives in the device. reply doubled112 10 hours agorootparentDon&#x27;t forget to mention displays in this discussion.A USB-C docking station will plug into many of my laptops, but only two will display anything on the screen.Being able to pick and choose features like that is not user friendly. It will do everything except when they didn&#x27;t feel like implementing it is not a good standard. reply Guvante 6 hours agorootparentprevHaving to buy yet another adapter cable to make things work wasn&#x27;t great. Nor was having to use a hub because FireWire and Thunderbolt at up space that could have been USB cables.At least you get minimal functionality (why are you hoarding data less cables?) Better than having to go buy one because you have a billion micro USB but no FireWire to hook up that drive. reply p1mrx 10 hours agorootparentprev> I had to test each cable by plug in an SSD then speed test the SSD.That&#x27;s actually not a bad solution. The speed of a cable doesn&#x27;t matter unless you have a device that needs to be fast. reply mschuster91 12 hours agorootparentprev> I would actually prefer different shapes, because then you could be pretty confident that if everything fit it was going to work.The baseline profiles for power delivery (i.e. anything up to 20V@5A) have been all but done for two, three years now - you can readily buy combinations of PD and MUX chipsets that handle everything you can throw at them, and Anker has high-quality chargers to supply the juice.The thing where USB-C still has issues is alternate modes. USBx usually works on all ports of a laptop or PC and virtually all cables, but anything involving TB, display, debug or audio is a hit-and-miss given how incredibly difficult it is to route all these high-frequency signals and properly mux them. reply kylebenzle 8 hours agorootparentprevUsbc is the greatest thing to happen to humanity the last 20 years.The simple fact that we can finally share charging cables across phones is a god send, not having a single cable for all phones was the stupidest thing people had ever done. reply Macha 7 hours agorootparentMicro-USB was previously the phone charging standard for everyone but iPhones for a good ten year run (and even then EU sold iPhones shipped with a micro-USB to lightning converter in the box). reply BlueTemplar 12 hours agorootparentprevSeems to be finally coalescing around something sane with USB4 ? (USB3 having been a \"beta\" version...) reply nullfield 1 hour agorootparentIsn’t USB 4 largely just TB3 without Intel certification and branding requirements? reply simondotau 13 hours agorootparentprev> What a mess.It’s such a mess that the EU took one look at it and said “let’s make that a universal standard and force it upon everyone!” Modern USB truly is the government bureaucracy of standards. reply Guvante 12 hours agorootparentEU has been trying to work with Apple on this almost since the lighting cable was released.I believe there were discussions about potentially standardizing on Lighting if Apple relinquished it&#x27;s tight control over the technology but they were not interested in that path.The EU decided that Apple cannot dictate what cables are used to charge their phones to the degree of at one point charging manufacturers $4 per cable (to be fair it seems like Apple was providing the hardware so it wasn&#x27;t a pure licensing charge). reply reactordev 17 hours agorootparentprevThe standard of evolving standard. Those looking take note, don’t rush to standardize something when the tech itself is still being tweaked and iterated on. Let it iterate, document it, but don’t put standards in place when you clearly can’t track or regulate it. You just look like that boomer who just now got their Windows 10 Upgrade CD to work.The reality is that with variable and programmable power supplies, it’s now up to the device to draw power at a balanced rate from the supply, instead of the “supply” pushing power in. Modern tech has allowed us to have supplies between 3V-48V, 500ma to 10A. All with a usbc head. reply wkat4242 16 hours agorootparentNot standardising gave us the myriad of charging and headphone plugs on mobile phones in the late 90s&#x2F;early 2000s. reply fecal_henge 13 hours agorootparentprevHate to be that guy but tweaking and iterating power delivery systems with no standards sounds like a way to get houses burned down. replybmicraft 2 hours agoparentprevThe Pi 5 does not require not require more than 5V@3A, except if you plan on pulling significant current from its the USB ports. If you don&#x27;t get the 5V@5A supply USB ports get limited to 600mA each and you&#x27;re still good. reply zarzavat 19 hours agoparentprevYeah, ultimately if you want to run a Pi you ought to be buying an approved power supply. Trying to use random USB chargers even with a Pi 4 is a road to unhappiness. It doesn&#x27;t have a battery, after all. reply comboy 12 hours agorootparentYes. But also you can try putting some big cap between 5V and GND before it arrives. reply BlueTemplar 11 hours agorootparentprevPi 4 does come with a power supply because of being even less standardized. reply BlueTemplar 12 hours agoparentprevAt least better than the Pi3-, which could end up drawing up to 15W on a connector that officially only supports up to 2.5W, therefore resulting in lots and lots of bricked SD cards... reply wdb 17 hours agoprevRaspberry Pi has been getting pretty expensive. Maybe I remember wrong but I think the first ones were like $30? Now they want $80+ for it. reply qwerty456127 15 hours agoparentI feel like they probably should diversify their offer and make both cheaper and more expensive models. Many people here including myself probably would love to have an even more powerful and less cheap model (e.g. I consider Raspberry Pi being a common standard extensible compact computer it&#x27;s key selling point, don&#x27;t care much about the price but often find myself handicapped by its CPU and IO performance while we all know similar type SoCs can do much do much better as many smartphones do), at the same time for many users and use cases it indeed probably should stay cheaper than it is becoming. Perhaps selling premium beefy models could have helped sponsoring cheaper ones. reply __jonas 11 hours agorootparent> I feel like they probably should diversify their offer and make both cheaper and more expensive models.They are already kind of doing this by still producing and selling the old models, on the page for the 3b there is this obsolescence statement:> Raspberry Pi 3 Model B will remain in production until at least January 2028https:&#x2F;&#x2F;www.raspberrypi.com&#x2F;products&#x2F;raspberry-pi-3-model-b&#x2F;I can find the 3b for around 40€ new, under 30€ used, which for me is kind of ideal, I don’t have a lot of interest in the more expensive models. reply godelski 14 hours agorootparentprevThey do have the zero, but it hasn&#x27;t been updated in awhile and I wish it was. There are still tons of projects with benefit from these cheap microboards, even smaller than the pi. The Pis have become pretty powerful but also more expensive. It would be nice to have, like you&#x27;re suggesting, a trade-off and it have a clearly good platform to use reply FlagsAreFun 9 hours agorootparentThey did release the Zero 2 W, which is much improved from the first model: https:&#x2F;&#x2F;www.raspberrypi.com&#x2F;products&#x2F;raspberry-pi-zero-2-w&#x2F; reply godelski 1 hour agorootparentThat was released at the end of 2021.For more context:Raspberry pi: April 2014Pi Zero: Nov 2015 (1x ARM1176JZF-S @ 1 GHz, 512 MB RAM)Pi Zero 1.3: May 2016 (now you can use cameras)Pi Zero W: Feb 2017 (Wifi and bluetooth 4.1)Pi Zero WH: Jan 2018 (omg, soldering the gpio pins? Much wow)Pi Zero 2 W: Oct 2021 (4x ARM Cortex-A53 @ 1Ghz, still 512 MB, now bluetooth 4.2)I&#x27;m not at all convinced they are caring about this market. Realistically there have only been 3 models and there really hasn&#x27;t been much push into this area. The Zero 2 upgrade wasn&#x27;t anywhere near the leap that the normal pis are making. I know there is more limitations, but they also have more competitors and it isn&#x27;t like the zeros are sitting on shelves. There&#x27;s till a good market for10 seconds to do speech recognition and speech synthesis. There are approaches out there trying to kind of get something working using Whisper tiny but in our ample internal testing and community feedback we feel that Whisper small is the bare minimum for voice assistant tasks, with many users going all out and using Whisper large-v2 at beam size 5. With GPU it&#x27;s still so fast it doesn&#x27;t really matter.The Raspberry Pi is especially poorly suited for this use case (and even amd64). We have some benchmarks here[1]. TLDR a ~seven year old Tesla P4 (single slot, slot power only, half-height, used for $70) does speech recognition 87x faster, with the multiple increasing for more complex models and longer speech segments. A 3.8 second voice command takes 586ms on the Tesla P4 and 51 seconds on the Raspberry Pi 4. Even with the Pi 5 being twice as fast that&#x27;s still 25 seconds, which is completely unusable. Not fair to compare GPU to Raspberry Pi but consider the economics and practicality...You can get an SFF desktop and Tesla P4 from eBay for $200 shipped to your door. It will idle (with GPU and models loaded) at ~30 watts. The CPU, RAM, disk (NVMe), I&#x2F;O, etc will walk all over a Raspberry Pi anything. Add the GPU and obviously it&#x27;s not even close - you end up with a machine that can easily do 10x-100x what a Raspberry Pi can do for 2x the cost and power usage. You can even throw a 2.5gb Ethernet card in another slot for $20 and replace your router if you want to go really dense.Even factoring in power usage (10-15w vs 30, 2-3x) the cost difference comes down to nearly nothing and for many users this configuration is essentially future-proof to anything they may want to do for many years (my system with everything running maxes out around 50% of one core). Many also gradually grew their self-hosted situation over the years with people ending up with three or more Raspberry Pis for different tasks (PiHole, Home Assistant, Plex, etc). At this point the SFF configuration starts to pull far head in every way including power usage.Users were initially very skeptical to GPU use, likely from taking their experience in the desktop market and assuming things like \"300 watt power usage with a huge > $500 card\". Now they love having a GPU around for Willow and miscellaneous other CUDA tasks like encoding&#x2F;decoding&#x2F;transcoding with Plex&#x2F;Jellyfin, accelerated Frigate, and all kinds of other applications. Willow Inference Server (depending on configuration) uses somewhere between 1-4GB of VRAM so with an 8GB VRAM card that leaves for plenty of additional tasks. We even have users who started with the Tesla P4 and then got the LLM bug and figured out how to get an RTX 3090 working with their setup which also of course leads to absurd performance with Willow - my local RTX 3090 goes from end of speech to command completion in HA to TTS feedback in ~250ms. It&#x27;s \"speak, blink, done\" fast.[0] - https:&#x2F;&#x2F;heywillow.io&#x2F;[1] - https:&#x2F;&#x2F;heywillow.io&#x2F;components&#x2F;willow-inference-server&#x2F;#ben... reply squarefoot 15 hours agorootparentThe lack of GPIOs problem in Mini PCs could be solved with a cheap external USB->GPIO adapter such as this one: https:&#x2F;&#x2F;www.hardkernel.com&#x2F;shop&#x2F;usb-io-board&#x2F; The above board was intended neither for the Raspberries nor for Mini PCs, but the code is Open Source and shouldn&#x27;t be too hard to adapt. reply kkielhofner 12 hours agorootparentIn the Home Assistant community most GPIO and other duties have migrated to ESP devices and the excellent esphome[0]. I have at least 10 devices around my home and it&#x27;s fantastic, I haven&#x27;t wired up GPIO to a Pi or anything other than a $5 ESP8266&#x2F;ESP32 for years.Another instance of using the right tool for the job.[0] - https:&#x2F;&#x2F;esphome.io&#x2F;index.html reply squarefoot 12 hours agorootparentYes, that would likely be an even better and cheaper solution if one doesn&#x27;t need to drive GPIOs from a complex OS such as Linux which would require a fatter board than an ESP one. reply kkielhofner 11 hours agorootparentIf you haven&#x27;t tried it you may be surprised by what HA + esphome offers with the 8266&#x2F;32.That said, the link you provided is potentially a great add-on for someone who wants&#x2F;needs the logic available on a real Linux host! reply philsnow 10 hours agorootparentprevAny leads on the ESP32-S3-BOX-3 SKU (the replacement for the ESP32-S3-BOX that Willow was developed for, IIUC)? I saw someplace say the only place to get it is Espressif&#x27;s Aliexpress page for it, but they show out of stock for me.Thanks! reply kkielhofner 10 hours agorootparentOf course!The initial release of the BOX-3 was essentially a pre-production run with ESP-BOX similar 3D printed plastics.The full production run of the BOX-3 from Espressif with proper injection molded plastics should become available from a retailer&#x2F;distributor near you within the next couple of weeks. reply simfree 7 hours agorootparentprevHave you tried the Radeon MI25 Compute cards? We have had good luck with a few, they perform quite well. reply kkielhofner 7 hours agorootparentWe&#x27;d love to move beyond Nvidia.The issue (among others) is we achieve the speech recognition performance we do largely thanks to ctranslate2[0]. They&#x27;ve gone on the record saying that they essentially have no interest in ROCm[1].Of course with open source anything is possible but we see this as being one of several fundamental issues in supporting AMD GPGPU hardware.[0] - https:&#x2F;&#x2F;github.com&#x2F;OpenNMT&#x2F;CTranslate2[1] - https:&#x2F;&#x2F;github.com&#x2F;OpenNMT&#x2F;CTranslate2&#x2F;issues&#x2F;1072 reply jrockway 16 hours agoparentprev$35 in February 2012 dollars is $47 today. reply burtness 13 hours agorootparentwhat about purchasing power? reply TMWNN 12 hours agorootparentAdjusting for inflation is a representation of purchasing power. reply BlueTemplar 11 hours agorootparentEeeh, better to adjust to the respective minimum wages (if your country had them at both times) or median wages (if not). reply jrockway 6 hours agorootparentI used the BLS&#x27;s consumer price index -based calculator: https:&#x2F;&#x2F;data.bls.gov&#x2F;cgi-bin&#x2F;cpicalc.plThe Federal minimum wage has not changed since 2009, but the CPI captures effects like per-state minimums increasing, less people working minimum wage jobs, etc. No \"adjust for inflation\" calculation will capture the \"pain\" that every individual experiences from making a purchase, but this index is pretty close. replybmicraft 2 hours agoparentprevI very much suspect they will be releasing 1GB or 2GB models later after the initial demand for it has died down and they can keep boards on shelves. Notice the the 1&#x2F;2&#x2F;4&#x2F;8GB indicator on the board itself (and how they didn&#x27;t say there won&#x27;t be those models) reply rldjbpin 2 hours agorootparentgiven their computation power now, how many people do you think would benefit from that offering vs older pis sold at a discount?the main point for the price was to make it more accessible for kids, so that parents can buy one without thinking too much about the costs. the 1&#x2F;2 GB may not support the desktop use cases that might be expected from the performance of the new pi.i think the pi zero is now their main go-to device for the price-consious audience at this rate. reply bmicraft 1 hour agorootparentThe pi zero _2_ still is on 0.5GB of ram. 2GB would be a big step up from that. Use cases like running home assistant or unify network controller don&#x27;t work below 1G, and would benefit from faster cpu and storage of the Pi 5 reply zarzavat 16 hours agoparentprevDepends what you need. You can still buy the old ones, and they are cheaper than at launch. But now have the option to buy a more capable one, for more money. If you&#x27;re in the market for a v4 then they&#x27;re getting cheaper. reply djbusby 17 hours agoparentprevMy first one was $25 (10 years ago?). But I think, from a comment on HN, that in inflation adjusted dollars for performance they are still very good. reply MikusR 5 hours agoparentprevThat&#x27;s like saying that apple watch costs 18 000. reply bsder 10 hours agoparentprev> Now they want $80+ for it.That just means that you&#x27;re seeing closer to the real price instead of subsidized price.I&#x27;m holding out for the RISC-V boards. At least I&#x27;ll be able to get real documentation unlike the RPi boards. reply kristopolous 20 hours agoprevI look forward to snatching up cheap pi 4s when this comes out reply fanf2 18 hours agoparentThey will still be selling new Raspberry Pies 4. https:&#x2F;&#x2F;rpilocator.com&#x2F;?cat=PI4 reply ttyyzz 15 hours agorootparentI live in germany, they really want (EUR) 92.99 for one? That&#x27;s ridiculous. reply fanf2 14 hours agorootparentThere are other places in Germany asking €87.49 and €86.90 which is not far from the nominal $75 price from Raspberry Pi, plus 19% VAT reply checkyoursudo 13 hours agorootparentprevI would have thought it was Raspberries Pi instead of Raspberry Pies. reply paulcole 20 hours agoparentprevWho’s going to go to the trouble to untangle the bread board wires and drag them out of whatever dusty drawer they were forgotten in to sell them? reply blacksmith_tb 17 hours agorootparentI think the implication is the scalpers who are sitting on Pi 4s they&#x27;ve been trying to sell on Amazon for $100 will quickly dump that inventory (and try and snap Pi 5s, no doubt). reply kristopolous 17 hours agorootparentprevI have pi4s I&#x27;m using that I&#x27;d love to swap out for something better. I&#x27;m sure many people feel the same reply stavros 19 hours agorootparentprevPeople who like money. reply smcl 15 hours agoprevAre people really looking to RPi for performance though? Surely the appeal is that it’s good enough, open enough and has a bunch of peripherals and stuff you can tinker with. Or maybe I’m out of touch (obligatory: no it’s the kids… etc) reply ddingus 12 hours agoparentIncreasingly yes.I am in that group now. Basically, my hobby development has moved to Raspi machines. I use a 400 as my primary station and various 3 and 4 models to do projects with.One very nice thing is to package everything up with a Pi and have that project be self-contained. Dev tools, I&#x2F;O whatever all nice and compact.Start another project? Just setup another Pi and go.Having a higher performance model means moving more of what I do onto a Pi. Things like VS Code run reasonably, but not great. On the 4, many things are close to great.The 5 appears capable enough to change that equation.Nice! reply smcl 11 hours agorootparentFair play then. I genuinely thought it was just a little dev&#x2F;experiment&#x2F;prototype platform for people, as it was for me. If this is what you need then god speed! reply ddingus 7 hours agorootparentIt is often that, so fair on your part too.What I have seen happen is people basically get going on Raspian and start to like it. Myself, I have used UNIX flavors since SGI IRIX and I find Raspian simple, well supported and easy to use.A fast Pi, plus a solid load of OSS packs quite a punch!Dev tools for many languages.Open, Libre Office and friends suddenly makes for a very cheap, effective desktop one can use to get a whole lot done.Anyone inclined to work with OSS can grab one of these little Pi computers can do a lot. I use them that way frequently. reply bmicraft 2 hours agoparentprevIf you don&#x27;t want more performance, you can still by older and much cheaper models like the zero 2w for a quarter the price reply lagniappe 21 hours agoprevAs someone who thought they were getting away with something finding Pi4&#x27;s in stock at a normal price... welllll shit. reply PaulRobinson 20 hours agoparentPi4 is still a great piece of kit, it&#x27;s only 8W (the 5 is 12W), and you&#x27;re not going to be as worried with cooling. There are a ton of projects out there for which the 4 is a good choice.For a general purpose computer though, the 5 is getting to the point where it is competitive for a lot of use cases where the 4 was a bit of a struggle. It sounds like I could definitely consider using a 5 as a mini-workstation for web browsing, some light dev work and the like, in those cases where the 4 felt a little clunky. reply sho_hn 20 hours agorootparent> it&#x27;s only 8W (the 5 is 12W)It&#x27;s 12W under high load, but at equivalent load (in terms of output performance) to a 4B draws less power. reply geerlingguy 15 hours agorootparentAt idle it draws .2W more, but at anything above that you get 2x the performance per watt, minimum. reply pja 20 hours agorootparentprevAn uncooled Pi5 is faster than a Pi4 running the same code.A Pi4 is still a perfectly fine microcomputer. reply digitalsushi 16 hours agorootparentespecially in a dusty woodshop reply pmontra 18 hours agorootparentprevAnd a Pi 3b with a TV hat and tvheadend is a little less than 4 W at almost zero load. reply MrBuddyCasino 20 hours agorootparentprevI feel like the Zero 2 W is the new low-power option. Its 4 cores should be enough for most things, the 512MB RAM limit is a bit unfortunate though. reply rcarmo 20 hours agorootparentGet a Radxa Zero, I&#x27;ve been using one with 4GB of RAM and 64GB EMMC: https:&#x2F;&#x2F;taoofmac.com&#x2F;space&#x2F;blog&#x2F;2023&#x2F;10&#x2F;07&#x2F;1830 reply blacksmith_tb 17 hours agorootparentLooks promising, but it&#x27;s also a pretty different machine - I like the Pi Zero because it&#x27;s cheap and just capable enough (basically I want a full OS, but I would never consider using a Zero as a desktop). The Radxa-s look quite a bit more powerful, but they&#x27;re also twice the price, and once you&#x27;re spending that much, the possible gotchas with software support make it a lot less enticing to me. reply rcarmo 17 hours agorootparentWhich is why I only pick boards that run Armbian. reply dark-star 20 hours agorootparentprevCool little device, but will it work with the RGB2HDMI, PiStorm or PiSCSI? I doubt it but if it were, that would be awesome reply MrBuddyCasino 20 hours agorootparentprevLooks interesting. Page doesn&#x27;t say if drivers are mainlined? reply adgjlsfhk1 20 hours agorootparentprevit does seem like there is an in between that might be nice. specifically something at the roughly 4 gigs RAM but 5 watts, lower clocks and worse graphics. I think that&#x27;s just about the right specs for a machine to run Linux without a fan and provide a good entry level experience (i.e. can do some real work in a terminal and watch video in a browser) reply heresie-dabord 20 hours agorootparentprevI enjoy using the R.Pi 4 and R.Pi 400 and from the start I&#x27;ve been a supporter of this class of relatively inexpensive Linux-first device.But the R.Pi 4x series is peak R.Pi for me. In the last couple of years, NUC-class SFF devices with Intel-based chipsets have appeared [1] and they offer a much better performance profile (speed, power consumption) than the R.Pi 5. Some of these devices offer dual gigabit Ethernet.[1] Liliputing is one site that I follow for related information. reply eropple 19 hours agorootparentThe Orange Pi 5 series offers similar in a relatively small board factor too, though, as well as stuff like HDMI video capture, built-in or modular eMMC, Rockchip&#x27;s new(-ish) NPU, dual 2.5Gbps Ethernet, and M.2 support. It&#x27;s a big reason why the RPi5 kind of falls flat for me: it&#x27;s a worse one of those, and not really much cheaper.I have a few NUC-style very small form factor Intel machines around too, of course, but there are a lot of good options out there and ARM shouldn&#x27;t be discounted. reply ThatPlayer 14 hours agorootparentFor most of my use cases (3d printer controller), I don&#x27;t really need any of those. So easily 2x the price for something I won&#x27;t use isn&#x27;t worth it.Not that I don&#x27;t have an Orange Pi 5 for other uses, but that&#x27;s gathering dust in a drawer because I&#x27;m waiting for good GPU drivers. The Raspberry Pi 5 seems like it&#x27;ll have Vulkan 1.2 out of the box. I&#x27;m still waiting for Panfrost&#x27;s new PanCSF drivers for the RK3588 Malis. reply geerlingguy 15 hours agorootparentprevI can&#x27;t find an OPi 5 for less than $100, nearly double the price of the low spec Pi 5, and other good boards like the Rock 5 B start in the $130s.I would love to see an RK3588S board forYeah, as soon as I heard about the 5 needing cooling for most tasks I went ahead and got a 8GB Pi 4.You heard wrong, then.\"For normal usage of your Raspberry Pi, adding cooling is entirely optional. The idle performance of a Raspberry Pi 4 and a Raspberry Pi 5 is about the same, and under typical loads Raspberry Pi 5 will run cooler than a similarly loaded Raspberry Pi 4.\"\"even when fully throttled, a Raspberry Pi 5 is still going to run faster than a Raspberry Pi 4!\"https:&#x2F;&#x2F;www.raspberrypi.com&#x2F;news&#x2F;heating-and-cooling-raspber...A Pi 5 has a higher performance ceiling than the Pi 4 when passively cooled, and an even higher one with active cooling. The Pi 5 does not need cooling for most tasks. reply pcdoodle 9 hours agorootparentGood info. Most Pi&#x27;s are kept slightly warm for later consumption :) reply sho_hn 20 hours agorootparentprevA Pi 5 shouldn&#x27;t require cooling for anything that a Pi 4 doesn&#x27;t require cooling for.Pi 4s require cooling for many workloads (they run notoriously hot).You&#x27;re more likely to run into the needs-cooling envelope of a Pi 4. reply rewmie 19 hours agorootparentprev> Yeah, as soon as I heard about the 5 needing cooling for most tasks (...)I&#x27;m sure those who put a premium on thermals or quietness will be able to run RPi5 with passive cooling if they really want to. The RPi4 already suffered from thermal issues and that didn&#x27;t pose a major problem. Anyone could simply drop by Amazon and pick one of the many passive cooling cases without any issue. reply geerlingguy 15 hours agorootparentI&#x27;m hoping Flirc updates their passive heatsink case for Pi 5. It&#x27;s my favorite case for the Pi, and it should be able to handle the 5&#x27;s slightly increased temps. reply gjsman-1000 20 hours agoparentprevA Pi 4 is still a Pi 4, and who knows how long it will take for it to gain software support for whatever you are intending to use it for. reply rcarmo 20 hours agorootparentWhat do you mean? I run a lot of things in ARM64 boards, and the Pi has an almost insane amount of software support compared to most alternatives (I&#x27;m running an unofficial port of Proxmox on my 8GB Pi 4...) reply myself248 19 hours agorootparentFor pure-software projects, where you could just throw an x86 thin client at it for cheaper anyway, yeah the pi4→pi5 upgrade path is quite straightforward.But for anything involving raw GPIO, you know, the reason you might want a true embedded board instead of a commodity PC, the pi5 has completely changed the architecture. The GPIO pins used to come straight off the BCM CPU with direct register access, now they&#x27;re peripherals of the RP1 southbridge IC which is connected over PCIe.Chief in my mind is the rpitx&#x2F;pifm projects, which (ab)use the DMA pipeline to cram baseband-rate samples into the PWM registers and make a GPIO pin oscillate at radio frequencies. Connect a (bandpass filter, please! and a) piece of wire, and voila, the board becomes a radio transmitter.This simply won&#x27;t work on the pi5 without a complete rewrite of the core. Someone will have to figure out if the RP1 even offers similar capability, figure out if PCIe packet jitter will screw it up or if there&#x27;s some buffering possible in the RP1, and rearchitect the whole project around it.However, neither of those projects has seen much activity lately. In other words, it ain&#x27;t likely to happen. Just keep using 3&#x27;s and 4&#x27;s for this.For some stuff, pifmrds for instance, stereo processing really chews through CPU, and precludes running a station-automation package on the same iron. It would be really nice to have a faster CPU wrapped around the same transmit capability, but that&#x27;s not what the pi5 is. If someone builds a station-in-a-box around this, it&#x27;ll have a pi5 for the front-end but a pi4 as the actual exciter. reply Semaphor 17 hours agorootparentprev> I&#x27;m running an unofficial port of Proxmox on my 8GB Pi 4...)I’m doing the same with Proxmox Backup Server, Proxmox itself runs on a more powerful thin client ;) reply CyberDildonics 20 hours agorootparentprevRasberry pi&#x27;s are by far the best supported boards out there, what software support are you talking about that they don&#x27;t have but will eventually? reply dingosity 14 hours agoprev8 second boot times? That&#x27;s only 2 seconds slower than my Commodore 64. On the less snarky side... I&#x27;m sure it&#x27;s a better system, but just because a core can run at 2.4GHz instead of 1.8Ghz, you have to take power dissipation into account. You have the same problem with overclocking: you can up the clock speed, but there&#x27;s still only so much thermal energy you can remove from the packaging before you have to throttle the core to avoid melting the solder off the board or delaminating the PCB underneath the package. I&#x27;m sure the good people at Broadcom have taken this into account, I&#x27;m just commenting you can&#x27;t just say \"oh. it has a 33% faster clock, it&#x27;s going to chunk 33% more data.\" reply fanf2 14 hours agoparentThey shrunk the process size so the 5 is more power efficient than the 4 reply nektro 14 hours agoprevi wish they&#x27;d kept the $35 price point instead of competing on performance reply zachallaun 13 hours agoparentI think someone demonstrated in the announcement thread that, adjusting for inflation, the Pi5 is going to be cheaper at release than the Pi4.Ultimately though, at least IMO, the Pi isn&#x27;t about being cheap, it&#x27;s about being capable while staying a great value. I bet there&#x27;s a lot of hidden complexity in the pricing, and it&#x27;s very possible that they&#x27;d have to compromise significantly more than 12.5% of the functionality&#x2F;performance to get a 12.5% price reduction (to $35). reply barsonme 19 hours agoprev> But second, the new Broadcom SOC finally supports the ARM cryptography extensionsWonderful, this is long overdue. reply Drblessing 5 hours agoprevWhat&#x27;s a good CLI only OS to run on the Pi 5? Want to take the plunge. reply argulane 3 hours agoparentThe normal Raspberry Pi OS Lite is pretty okay.https:&#x2F;&#x2F;www.armbian.com&#x2F; is also pretty awesome and also supports bunch of other RaspberryPi clones for cheaper price. reply GravityLab 17 hours agoprevAny idea if the cooling solution mentioned, either as heatsink or fan or both, significantly impacts the footprint of the unit? In terms of weight and also dimensions. Can you reasonably cool the unit under load with a fan that is almost not noticeable when it&#x27;s attached to the board?Really cool stuff in any case, glad to see this product continuing to get new features and spec bumps. reply Kirby64 16 hours agoparentThey&#x27;ve shown off fan modules that fit underneath the usual &#x27;pi hat&#x27; size, so it seems the answer is: no in terms of size. You&#x27;d still need cutouts for ventilation though. reply ddejohn 17 hours agoparentprevNot sure about the 5, but I have a heatsink and fan on my 4 and while it was designed to fit inside the official pi case (there is no evidence of active cooling from the outside), it is very noticeable in terms of sound. reply fanf2 13 hours agorootparentI think the PWM fan control on the 5 should make it quieter most of the time. reply 404mm 17 hours agoprevIt’s nice they doubled the speed of SD controller but I still wish they just gave us some eMMC for the system. reply undersuit 15 hours agoparentThere&#x27;s the RaspiKey, an eMMC module on a microSD card breakout board.Or the Compute Module which can be even more compact than the standard Pi format. reply Havoc 10 hours agoparentprevJust boot off a cheap ssd. Improves the experience dramatically reply moffkalast 16 hours agoparentprevIt&#x27;s also nice that they tripled the overall performance, but it would&#x27;ve been cool if there was a non-sketchy way to power the thing. reply 404mm 16 hours agorootparentI know it may sound silly but the need for active cooling is a deal breaker to me. I’m still hoping to see some massive heat sink or a metal case that acts as a heat sink. But the overall Pi evolution leads towards hotter, louder and more expensive, which there are better options where you get more flops per buck. reply geerlingguy 15 hours agorootparentIt&#x27;s not a need, per-se. running a small service on it that only uses 10-20% CPU and just bursts now and then, I don&#x27;t see it throttling. It&#x27;s only running benchmarks or activity that uses one or two cores 100% all the time that runs into throttling (and even throttled, performance is a good deal better than Pi 4 unthrottled). reply johann8384 14 hours agoprevAnd an Orange Pi 5Plus is even better reply FL33TW00D 19 hours agoprevAny data on the FLOPS of the pi5 GPU yet? reply fanf2 18 hours agoparentThe VideoCore VII has 12 QPUs at 900MHz according to https:&#x2F;&#x2F;www.raspberrypi.com&#x2F;news&#x2F;james-adams-and-eben-upton-...The VideoCore VI has 8 QPUs at 500MHz producing 32 Gflop&#x2F;s according to https:&#x2F;&#x2F;github.com&#x2F;Idein&#x2F;py-videocore6So, VideoCore VII should produce about 86 Gflop&#x2F;s reply FL33TW00D 10 hours agorootparentExcellent thank you reply desireco42 17 hours agoprevPi 5 is really nice but it requires a heat sink... this makes it ... whole different game to me...I think as long as there are both 4 and 5, it is good, otherwise this is fantastic they are experimenting as they should, but there is value in being low-power and versatile in my view. reply justin66 17 hours agoparent> it requires a heat sinkIt does not. They’ve made a point of noting that it’s faster that a Pi 4 even if you run it without a heat sink and let it throttle the cpu back when it hits the temperature limit. reply desireco42 14 hours agorootparentTo me that means it does need it, otherwise you have a thing that would run fine in one moment, then crawl to a stop... reply bmicraft 1 hour agorootparentIt doesn&#x27;t \"crawl to a stop\" while throttling though reply kzrdude 14 hours agoparentprevI&#x27;ve got a metal case for the pi 4 (which touches the cpu, like a heat sink) and I hope I can do the same for the pi 5 sooner or later. reply moffkalast 16 hours agoparentprevThe 4 needs a heatsink as well though, otherwise it throttles under load, it just doesn&#x27;t need active cooling. reply 0ct4via 11 hours agoprevIf you&#x27;re going to mess with capitalization for the title, it would be better ending with \"than two Pi 4s\" for pluralization -- there is no \"Pi 4S\" model. reply NotYourLawyer 20 hours agoprevI have to think most Pi’s in the wild are sitting idle most of the time. How many people are using them in compute-intensive projects? I certainly don’t need more CPU to run a pihole or a rain gauge or whatever. reply humanistbot 19 hours agoparentMostly agreed, but \"sitting idle most of the time\" isn&#x27;t quite the right metric. The question is if it can handle the peak loads. And a pi zero can run pihole and a rain gauge.I don&#x27;t plan to upgrade my rpi4 running homeassistant, a Plex video streaming server, a tailscale VPN exit node, and a few other minor services.Plex on the rpi4 can&#x27;t keep up transcoding 4k video to 1080p, maybe the rpi5 can, but I don&#x27;t personally need that. Many Plex users do need 4k transcoding though.Also, I have tried to use the pi4 as a desktop replacement, and I just found it too slow for the modern bloated world of webapps. All the little things just added up to too much and I gave up. The pi5 might be good enough though, but with all the peripherals and pains of arm64, a used old laptop is still probably better on the price&#x2F;performance ratio. reply mort96 19 hours agoparentprevBeing idle most of the time doesn&#x27;t mean you don&#x27;t need compute, there&#x27;s a lot of use cases (usually-low-traffic web servers, for example) where you&#x27;re idle most of the time but when something actually happens, you want to handle it quickly.I use a Raspberry Pi 4 to host a Nextcloud instance. It&#x27;s sitting idle most of the time, but the weak CPU of the Pi 4 (combined with a fast connection and the fact that Nextcloud is a terrible pile of slow PHP) means that syncing lots of small files is actually CPU limited.(Seriously, I set it up on the Pi 4 thinking that Nextcloud would be mostly IO limited. I wasn&#x27;t considering the fact that its file syncing is based on WebDAV, and every single WebDAV operation is a separate HTTP request which goes to a freshly spun up PHP interpreter which has to go through all the work of connecting to databases and handle authentication and what not in order to write a 100 byte file to disk. Still, it works well enough for the small size and quiet of the Pi 4 is worth the trade-off.) reply regularfry 18 hours agoparentprevI&#x27;ve got a zero 2 sat next to me that&#x27;s got uncomfortably warm because I&#x27;ve been pushing too much at it.What I&#x27;ve been trying to do is get it to chew through whisper.cpp at anything like a reasonable rate. It can so nearly do it with the right sized model, the latency isn&#x27;t good enough yet, but the reason I&#x27;m doing this on a pi zero 2 is because of the USB-OTG port. I can take the whisper.cpp output, and have the pi pretend to be a USB keyboard, typing what it hears into a second machine over USB. This gives you dictation capabilities in environments where upgrading the PC it&#x27;s plugged into isn&#x27;t an option.Like I say, it&#x27;s not quite there yet in terms of usability, but it&#x27;s close enough that I think the problem is just a matter of software (and arguably model) optimisation. reply jonincanada 6 hours agorootparentSounds like some learnings around the lora gpt fine tuning that could be applied to the similar whisper transformer architeture &#x2F; reply fanf2 18 hours agorootparentprevThe Raspberry Pi 4 and 5 have USB OTG through their USB-C ports. reply giantrobot 16 hours agorootparentThose require a bit more power than a Pi Zero though. That&#x27;s not necessarily a blocker but if the system OP is talking about only has USB2 ports a Pi 4&#x2F;5 wouldn&#x27;t be suitable. reply johnklos 15 hours agoparentprevI run BIND, Apache, sendmail, SearXNG, npf, and route a class C network over tinc on my 8 gig Raspberry Pi 4 with software mirrored disks. I just bought an Orange Pi 5 because it&#x27;s much faster (it has four Cortex-A76 cores, just like the Raspberry Pi 5 and at the same clock, too, but it also has four Cortex-A55 cores, which are by themselves about the same performance as the Cortex-A72 in the Raspberry Pi 4).Moving to the Raspberry Pi 5 just isn&#x27;t an option, because I&#x27;m already memory limited at 8 gigs, so 4 gigs would be unusable.I bought my Orange Pi 5 with 16 gigs, so I&#x27;m quite looking forward to doing the upgrade! Granted, there might not be too many other people putting this much work on Pis, but when you&#x27;re power constrained, they&#x27;re an excellent option. reply prepend 20 hours agoparentprevI think it’s great to have more cpu. My kids built a Minecraft server on a 4S and it doesn’t perform that well.The small form factor is really convenient for turning on and off.Pis are designed for the edge case and that allows them to sit in drawers. If you design for the median only, then you’ll never get to median because people won’t be able to run and test the edge cases. reply bombcar 17 hours agorootparentFor things like that the Rock boards are much much more powerful.Of course as you go up costs rise and eventually you might as well buy a Mac mini or similar. reply prepend 12 hours agorootparentYes, there’s obviously more powerful computers for running a Minecraft server.But pis are easy to build and manage and that’s the platform my kid likes. reply ep_jhu 19 hours agoparentprevFor my DCSkyCam project, htop shows 1.8 load average on my Pi 4B. It&#x27;s always busy, with resources left to handle spikes based on what the camera detects (or post to mastodon&#x2F;website). I&#x27;d say it&#x27;s right-sized. Best part - not throttled, even without a case, fans, or even heatsinks. That probably wouldn&#x27;t be the case with a Pi 5. reply spacechild1 15 hours agoparentprevI have been using RPIs for multimedia art projects where I am running heavy audio and&#x2F;or video processing, sometimes maxing out all available cores. So I am pretty excited about the new specs :) reply wayfinder 19 hours agoparentprevThat’s because for everyone who needs a beefier CPU, the Raspberry Pi 4 hasn’t even been on the table. reply _joel 20 hours agoparentprevMaybe not the CPU per se but I&#x27;d imagine the extra beef GPU side could be good for digital signage and entertainment. reply humanistbot 18 hours agorootparentThe pi4 is powerful enough to drive multiple 4k displays while doing face detection from a camera. reply varispeed 20 hours agoprevMissing opportunity to release 16GB version or at very least give specs of the memory so someone could swap chips. reply zamadatix 19 hours agoparentThey didn&#x27;t have the 8GB Pi 4 for the first year of launch either. I think it was Jeff G who said they were planning on a 16 GB release down the road. reply fanf2 17 hours agorootparentYeah https:&#x2F;&#x2F;youtu.be&#x2F;nBtOEmUqASQ around 3:00But in this interview https:&#x2F;&#x2F;youtu.be&#x2F;vXYzJ1os4NA at 11:50 Eben says that the chipset does not support 16 GB. reply bmicraft 1 hour agorootparentMaybe it&#x27;ll need a slightly upgraded broadcom soc like they did with the 8gb model of the Pi 4, or as a Pi 5+ variant reply washadjeffmad 17 hours agoparentprevThere are already tons of higher-RAM SBC options - check out PandaBoard.They&#x27;re awesome for prototyping, but there are so many places a Pi isn&#x27;t the right fit or scale. They&#x27;re common, not the only choice! reply magicalhippo 14 hours agoparentprevWhat do you need 16GB for that doesn&#x27;t need a beefier CPU to match?I&#x27;ve been running multiple 2GB Pi&#x27;s and I can&#x27;t think of any reason why I&#x27;d ever need more than 4GB. If I need that also want more IO (PCIe&#x2F;NVME) and more CPU to make use of it. reply zamadatix 12 hours agorootparentAI models are a pain to getting running nearly as good on smaller amounts of RAM (if you can get them running at all) even though the compute itself plenty usable for many. If your use case is always going to be a home dns server or using a few GPIO pins then it doesn&#x27;t really matter. reply rewmie 19 hours agoparentprev> Missing opportunity to release 16GB version (...)This seems to defeat the whole purpose of a Raspberry Pi: cheap, almost disposable computer to tinker around with.A 8GB RPi4 board is already more expensive than some used desktop computers, and until recently some major hardware manufacturers (ahem apple) were still selling high-end laptops with less than 16GB of RAM.I understand asking for features such as PoE and SATA support, but 16GB of RAM feels like an attempt to make the RPi something that&#x27;s not supposed to be. reply geerlingguy 15 hours agorootparentAlso, 16GB LPDDR4x modules are crazy expensive right now, from what I&#x27;ve heard. reply euniceee3 19 hours agoparentprevAny ideas on what it would take to accomplish this? Like how would the extra memory be allocated without a firmware change? reply colejohnson66 19 hours agorootparentOn the Pi 4, the BCM2711 supposedly supports 16 GiB (35-bit address space, with top half for peripherals and bottom half for SDRAM)[0 pg 5]. However, there seems to be some firmware limitation that limits you to 8 GiB of RAM, even if a 16 GiB chip is installed.[1] I would assume the BCM2712 would have the same address space (or bigger), and, as such, could support 16 GiB (or more) of RAM, but the firmware is written with the assumption that only 8 GiB of RAM will be installed.The Pi firmware comes as a binary blob, but has there been any effort to reverse engineer it?[0]: https:&#x2F;&#x2F;datasheets.raspberrypi.com&#x2F;bcm2711&#x2F;bcm2711-periphera...[1]: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=vtym0TAWNto reply zamadatix 18 hours agorootparentIt&#x27;s not a firmware problem, the memory controller only supports 17 bits for the rows. reply colejohnson66 17 hours agorootparentSo the hardware supports a memory map where 16 GiB of SDRAM could be present, but also limits itself to only 8 GiB in another area? Wow. replymort96 19 hours agoprevThe &#x27;S&#x27; in the title should be lowercase, it&#x27;s \"two Pi 4s\". The current title makes it sound like there&#x27;s a model called \"Raspberry Pi 4S\" and the Pi 5 is better than two of those Pi 4S models. reply TaylorAlexander 17 hours agoparentYes also the only reason it is capitalized in the original headline is that the headline is presented in all caps. In the article itself they say “Pi 4s”. reply stavros 19 hours agoparentprevAgreed, that&#x27;s how I parsed it too. reply thiht 17 hours agoparentprevOr maybe « two Raspberry Pis 4 »? It feels weird to have the plural mark on the model number reply guntherhermann 16 hours agorootparentIn English, it would sound strange to say \"Two Raspberry Pis 4\", idiomatically we would probably always say \"Two Raspberry Pi Fours\" reply thiht 16 hours agorootparentOk! thanks for the clarification reply fanf2 15 hours agorootparentprev“Raspberry Pies 4” is arguably correct, if you consider 4 to be a postnominal adjective. https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Postpositive_adjectiveBut it’s weird enough that I would only use it when being jokey. reply MrBuddyCasino 17 hours agoparentprevIt is an artifact of the all-caps original title and should be lowercase. reply undersuit 15 hours agoparentprevThe Pi5 better be better than the Pi4s. https:&#x2F;&#x2F;www.raspberrypi.com&#x2F;products&#x2F;compute-module-4s&#x2F;The s stands for sarcasm. &#x2F;s reply jfk13 19 hours agoprevYes, π⁵ ≈ 306, while 2⋅π⁴ ≈ 195.So pi^five clearly beats two pi^four. Three pi^4 would be close, though. If only we could just define pi as a nice simple integer... reply agilob 18 hours agoparentHow about π=5?https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;physicsmemes&#x2F;comments&#x2F;dxlrw3&#x2F;using_... reply lemper 18 hours agoparentprevthey tried. in Indiana. at the end of 19th century. reply yieldcrv 14 hours agoprev [–] I wish Raspberry Pi&#x27;s came already constructedI just always deprioritized doing itI&#x27;ve built desktops in the past before, its more about disinterest in doing it at all for anythingthe services that will do it move the pricing of raspberry pi&#x27;s to an equally as unattractive territory. might as well just get an old android phone and turn it into a server at that point. reply Beached 14 hours agoparentI&#x27;m confused. a rpi is comes already constructed? they are single board computers. all you have to do is plug in a power cord? reply yieldcrv 14 hours agorootparentI usually have to add another sister board, or put a heatsink on or something discouraging reply TMWNN 12 hours agoparentprev [–] I am all thumbs when it comes to hardware, and Raspberry Pi is about as easy to put together as imaginable. I purchased my Pis in \"kits\" provided by resellers, and assembling the kits takes about 15 seconds. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Raspberry Pi 5 has been launched, boasting enhancements like increased speed, improved WiFi, and support for dual displays. However, there are concerns about it generating more heat and requiring extra cooling.",
      "Users are discussing the device's display compatibility and the use of USB-C. There are mixed reviews, with debates circling around its efficiency, pricing, and the removal of some features.",
      "The community showcases varying suggestions for the Raspberry Pi 5's potential applications, such as using it as a router or for software-defined radio scenarios, demonstrating its versatility."
    ],
    "commentSummary": [
      "The discourse predominantly encompasses the features of Raspberry Pi 5, including its power needs, USB-C standard, performance relative to Raspberry Pi 4, cooling requirement, and updated specs.",
      "There is a range of views on the aptness of the Raspberry Pi 5 for varying projects, available alternatives, and apprehensions over cost and memory constraints.",
      "Raspberry Pi is a series of small single-board computers used in teaching basic computer science and for performing tasks that require more than a basic micro-controller."
    ],
    "points": 245,
    "commentCount": 185,
    "retryCount": 0,
    "time": 1696763535
  },
  {
    "id": 37814748,
    "title": "Forty years of programming",
    "originLink": "https://fabiensanglard.net/40/index.html",
    "originBody": "FABIEN SANGLARD'S WEBSITE ABOUT EMAIL RSS DONATE Oct 8, 2023 FORTY YEARS OF PROGRAMMING I am about to turn forty-six. This means I have been programming for forty years, half of them professionally. During most of that time, I used a \"standard\" setup with 104 keyboard, a flat mouse, and a sitting desk. My home workstation circa 2011 (porting Doom III to mac) Things evolved ten years ago when I started to experience pain in my forearms and shoulders when I programmed. Here is what I did to solve my problem, it may work for someone else. MOUSE Using a vertical mouse improved things a lot. My favorite is the Evoluent VerticalMouse 4. My work workstation circa 2015 Eventually, I opted for the Magic Trackpad from Apple. It is great to switch workspace with three fingers, zoom, and more. Having it in the center allows me to use it alternatively with my left and right hand. My home workstation circa 2015 It used to be annoying to get the drivers for Linux/Windows but now it is all sorted out. KEYBOARD The first ergonomic keyboard I tried was the KINESIS Freestyle2. It allowed me to spread each half as needed, resulting in horizontally straight wrist . However, the standard Control, Shift, and Alt still required wrist gymnastics. Same thing for Esc (I talk about VIM later) which required a left wrist twist. Also the tenting angle was too low. I tried the KINESIS Advantage2. I liked the concept of having so many thumb options. But the fixed width was a step backward compared to the KINESIS Freestyle2. The keyboard which has it all for me is the Ergodox EZ. It can be as wide or narrow as I need. And the custom firmware is a highly customizable gem. Among many features, it can switch all keys into a different layer with a single keystroke. The silver bullet for me is the ability to have a key function change if you keep it pressed. If you look at my layout you can see how capitalization (Shift) can be done by maintaining pressed either D or K. All my symbols {, [, (, ... are on another layer available from a single key maintained pressed. My work keyboard in 2023 With the Ergodox EZ, my wrists never move. They are always in a rest position, on all three axes. Zero pain and I can program all day. Additionally, the Ergodox accepts hardware tuning, like DROP Carbon keycaps, custom cables by pexonpcs.co.uk, and Brown Gateron G Pro. Trivia: As a fan of IBM's Model M clicky keyboard, I tried to build my first Ergo using Cherry MX Blue. That was fine for home but made some coworkers upset. I recommend going for Cherry MX Red which are the most silent, or the Cherry MX Brown which are a good middle ground between Blue and Red. VIM As you will have guessed my goal is to move my hands and twist my wrists as little as possible. That would be a problem to navigate a program since most IDEs require clicking via the mouse. Thankfully, most editors have a VIM mode which allows you to move across a file, goto definition, go back, all that without using the mouse. DESK Standing up improves my posture. I don't slouch when I stand. So I built a motorized standing desk with Topsky legs and a Home Depot butcher counter top. My home desk, at the time used for amazing Diablo 2 Resurrected adventures The three position memory allow to switch from standing to seating within a few seconds. And I try to alternate during the day. STRETCHING I take a break every once in a while and do a bunch of Wall Angel. MEDITATION IN MOTION I manage my stress level by disconnecting from work when I leave it. Rock climbing works well for me. You can't think of anything else when you climb. It is a great way to turn off a brain that keeps on having great optimization ideas when it is not the time anymore. *",
    "commentLink": "https://news.ycombinator.com/item?id=37814748",
    "commentBody": "Forty years of programmingHacker NewspastloginForty years of programming (fabiensanglard.net) 238 points by billiob 12 hours ago| hidepastfavorite127 comments SulphurCrested 10 minutes agoThis is going to be controversial, but what saved me was abandoning a keyboard-intense environment (emacs + command line) for Apple’s Xcode. I do still build much of the time at the command line, but editing code and switching between windows is mouse-heavy.(You can use Xcode as just a text editor if you like.) I cut and paste by selecting with the mouse where possible. I have a gaming mouse with a button programmed for macOS’s “Exposé” so I can find windows back – and I use a single screen, a 27″ iMac.I have an Apple trackpad which I use to give my hand a break from the mouse, but I find that if I use it heavily for a day or two I get sore fingers. I use a mechanical keyboard. That too tends to cut up my fingers, and I’m on the lookout for a better one, or at least better keycaps.I’ve been doing this for 45 years now, since age 19. Along the way I’ve used punchcards, line mode editors, TECO itself and two TECO-inspired full screen editors, various full-screen editors like DEC’s EDT, and emacs. I know vi, and will by habit drop into it when editing config files and so on, but don’t subscribe to the view that it solves the RSI problem. So I do know how to edit efficiently using keyboard shortcuts, but now think it’s the wrong thing to do.I went through a stage where I used emacs and a Happy Hacking Keyboard, and was very sore at the end of it.I am not slow with my mouse. I can churn out ~2000 lines of good C++ in a day. (But I am not a fan of the language from a typing point of view!) reply benreesman 2 hours agoprevForty years is just a crazy amount. I’ve been doing it for 30 and getting paid for 20, and I know my stuff: but not to that level. Fabien is just a demigod. Given the long ass hours I’m pushing 100k hours in this trade, and I feel like I’m really finding my feet in the last 1-3 years.This is one of those trades where you can do amazing shit on the first day, but you’re still a work in progress just decades in.I love this shit. reply lordnacho 1 hour agoparentI don&#x27;t know, I don&#x27;t think I&#x27;ll ever feel hyper competent, the way I think of some of these people you come across.I certainly know a lot more than 20 years ago. But I always feel like there&#x27;s more and more layers. Just browsing this site I find something every few days to add to my reading list.In a way that&#x27;s good, not feeling like I&#x27;ve hit some experience ceiling. reply CapsAdmin 5 hours agoprevI&#x27;m only 33 but never had any issues with joint pains or anything like that whereas my peers, some younger and some older struggle with this and use various methods like ergonomic keyboards, massage sessions, stretching, etc to help ease the pain.I&#x27;ve used a traditional mouse and keyboard since I was around 7 and can easily spend a whole day in front of my computer.Lately I&#x27;ve been wondering if it&#x27;s simply just my inability to sit still which has helped me so far. I feel like I always need to change my sitting position. I can sink down in my chair, put my legs up on something, move my feet, lean on my desk in awkward positions, stretch and twist my body and so on. Not because anything hurts, but because I just feel restless.But sometimes when I&#x27;ve been intensely in the zone while programming, I don&#x27;t move as much and then I might start to feel some stiffness in my arms, fingers, etc. I would also notice my eyes get sore, maybe because I don&#x27;t blink as much? reply ozim 4 hours agoparentI am 36 and I subscribe to notion that no desk or ergonomic chair will help.Only thing that helps is excercise.I go to the gym at least 2x a week.When gyms were closed during lockdowns I did not excercise or had much of movement anyway. Then I got my back hurting and other stuff.Now I am back at the gym and all issues went away. Also I don’t lift super heavy but moderately like I don’t bench press 100kg so I finish at 70kg usually. So it more important to move than lift heavy. All gym bros will tell you to put on more weights but you don’t have to if it is not your goal. My goal is to be healthy. reply musha68k 3 hours agorootparentAlso timers; reminding to constantly switch up positioning. Standing, sitting (especially with legs up), now and then lie down position even (e.g. for when reading specs&#x2F;documentation).Switching chairs is another one for me. I do have a HAG Capisco at my fully ergonomic standing desk setup but also work from the couch or kitchen table in between.But yes, exercise is the main pillar; and AFAIR ergonomics basically means no repetitive or same posture or movement over longer stretches of time; body wants to change position and move about.Lastly, breaking habit of overwork if current work phase allows (always questioning if the case &#x2F; if working efficiently). reply amoss 3 hours agorootparentprevRowing is fantastic for this. There is a good balance between cardio workout and strength. If your day job involves sitting in a chair for eight hours a day then the all over effect finds those muscles that you don&#x27;t use very often and tones them up. reply henrik_w 1 hour agoparentprevI coded professionally for more than 10 years without thinking about ergonomics, and without any problems. But then I started to feel pain in my arms, and it took a lot of effort to get it under control. In the end, what helped me the most was using a break program (to remind me to let go of the keyboard, and to do stretches), in combination with an ergonomic keyboard and mouse.I&#x27;ve written more details about it here:https:&#x2F;&#x2F;henrikwarne.com&#x2F;2012&#x2F;02&#x2F;18&#x2F;how-i-beat-rsi&#x2F; reply CapsAdmin 4 hours agoparentprevSome additional notes from reading other comments. I feel the way I wrote the post make it sound as if I&#x27;m fine with any setup which is not true.- Lower than normal desk height is more comfortable.- Lower than normal chair height is more comfortable.- Using a laptop is tiring unless it&#x27;s on a desk.- Using a laptop on my lap in a couch gives me neck pain.- I should preferably not have to tilt my neck to look at the center of a screen.- Using a macbook trackpad over extended periods of time can give me some wrist discomfort. I find myself often twisting and cracking my wris if I don&#x27;t use a mouse because of the discomfort.- Some chairs work better than others but I&#x27;m not sure what it is. Price does not seem to correlate at all.Somewhat related:- Sleeping on a pillow that is too tall gives me neck pain and headache. (from staying in hotels, friends, family, etc)- Sleeping on very hard mattresses is preferable over soft. (seems more common in asia) reply d3vmax 1 hour agoparentprevAlso have your screen at eye level, neck bent looking over for prolonged periods.. gives you a neck &#x2F; elbow pain. reply spit2wind 4 hours agoprevStenography is the most ergonomic method of computer input I am aware of.Stenography is a mapping of some combination of keys to some output.A particular mapping is called a \"theory\".One such mapping is phonetics, or the sounds of words. Let one key represent the \"Kuh\" sound, another \"ah\", and a third \"tuh\". Press them all down and when released, you get \"cat\". \"Al\" \"Guh\" \"Or\" \"If\" \"Um\" \"Algorithm\". 5 strokes instead 8. And that&#x27;s not even trying to be efficient.Another mapping is to use shapes. Three keys in the top row and one in the middle on the bottom looks like a \"T\". So, map that to something you associate with T.A theory can be any mapping you want. Stenography is based on shorthand which was invented in the late 1800s. There are plenty of theories that already exist. You don&#x27;t need to make your own.When you write, you use some words or phrases frequently. Map those to convenient keys. Such mappings are called \"briefs\". Go crazy with them and you can reach 370 words per minute of real-time dictation.You obviously don&#x27;t have to be that good. I find that 30 wpm is sufficient to be productive at work. You can reach 70 wpm by practicing 15 minutes a day for a few months. You&#x27;re a programmer. You can do it.Expecting a lifetime of computer input? Don&#x27;t optimize for easy key-to-output mappings such as QWERTY or Dvorak. Learn stenography.See Plover and Javelin. reply pjc50 5 minutes agoparentDo you find that you have a particularly document-heavy workflow? Because I find I spend far more time navigating code than writing new code, and I try to let Intellisense do most of the letters. I&#x27;ve never felt that a higher WPM would help me in programming.(Early on in my career someone I worked with had one of those \"spend a whole week testing things and then emit a one character change\" times, and that left an impression on me)Then there&#x27;s the APL programmers, who are all like \"what do you mean &#x27;a word&#x27;\"? reply d-lisp 3 hours agoparentprevWhen I program I write words with two or three strokes thanks to lsp, company, yasnippets andor . On my keyboard tab and return are accessible with my thumbs. I can perform any task related to my window manager, manage multiple monitors and windows with two fingers of the same hand, without moving my hands from the keyboard.My keyboard is a macro-keyboard by default, \"input\" is a specific mode of the keyboard, that&#x27;s vi but for whole desktop.I type 120+ wpm, most of the keys I can type is an abstraction, the rest are characters.All of that said, I do think there&#x27;s no better thing as a split keyboard for controlling ergonomically your computer.I was interested in stenography, but that&#x27;s too much work to probably never reach the velocity and comfort I have right now. reply apitman 10 hours agoprevI started experiencing similar forearm pain a few years ago. The split keyboard seemed to help a ton. I also programmed[0] an Arduino with two foot pedals. Left pedal for CTRL, right for SHIFT, both for ALT. Worked wonders. I mostly have the pain manageable now, but it still flares up sometimes, and it was really scary there for a while wondering if I might have to stop programming. To you young guns out there, an ounce of prevention truly is worth a pound of cure. Invest in an ergonomic setup and stretches&#x2F;exercises now, and don&#x27;t push yourself into pain while typing. Take more breaks. Once you push your body over the edge it may never quite be the same.[0]: https:&#x2F;&#x2F;github.com&#x2F;anderspitman&#x2F;ergo-pedals reply thewakalix 9 hours agoparentWhat if you need to press all three \"keys\" simultaneously? reply Brian_K_White 4 hours agorootparentThe same keys still exist on the keyboard. There is also such a thing as a compose key sequence where you press a few things serially rather than all at once. There are countless answers to this question really. reply devinprater 7 hours agorootparentprevHe probably doesn&#x27;t use Emacs. :) reply yetanotherasian 6 hours agorootparentI know what you mean. I was forced to use emacs for a year. I found it an interesting coincidence the person who forced emacs on us had the worst carpal tunnel syndrome and couldn&#x27;t type on a standard keyboard without extreme pain.I know you can remap your keys but there&#x27;s inertia to just go with what you&#x27;re given.I&#x27;ve also noticed myself switching between Mac, Windows, and Linux, that Cmd-C on my Mac is way less stressful on my hand than Ctrl-C on the other 2 machines. I should probably figure out how to remap those.PS: If you&#x27;re curious how emacs was forced, it was because the lead built the project&#x27;s IDE&#x2F;build&#x2F;debugging system into emacs reply pault 6 hours agorootparentTry a keyboard running QMK firmware. You can map a single key to multiple codes depending on the length of the key press. I use caps lock as escape if released immediately, and control if held down. Putting the modifiers in the bottom corner of the keyboard was a sadistic design choice. reply Brian_K_White 4 hours agorootparent\"escape if released immediately, and control if held down\"Elegant.And I mean for those two particular modifiers with that relationship, not just the idea in general. reply imp0cat 5 hours agorootparentprevOr evil mode. reply dingnuts 6 hours agorootparentprevwow you woke up today and chose violence, didn&#x27;t ya? reply _TwoFinger 42 minutes agoprevJust want to mention sticky keys[1], as it seems often overlooked.Instead of holding the modifier key(s) when activating a shortcut, you can press and release each modifier sequentially.It does come with some of the fancy keyboards&#x27; firmware, but is also a built-in feature of all major OS-es. You can get it without spending a single dollar.[1]: https:&#x2F;&#x2F;www.emacswiki.org&#x2F;emacs&#x2F;StickyModifiers reply cassepipe 9 hours agoprevDear Vim beginner, before getting a fancy keyboard, do remap Esc to some other key.It makes no sense to have your most important key where it is nowadays.The reason the back to normal mode key is ESC is because that key used to be much closer from the home row.My personal preference is to use Caps Lock because each OS has an easy solution for this simple remap and since it&#x27;s system wide you can use vim modes elsewhere too (zsh and gdb for me mostly). Also in general it&#x27;s quite convenient to have escape so close.Do what you will but please don&#x27;t suffer uselessly for stupid historical reasons. reply Zamiel_Snawley 8 hours agoparentCaps lock is such a waste of prime keyboard real estate, even for non-vim users!For those who don’t use vim, just switching caps lock and control is probably better so you can copy and paste with less contortion.For vim users, I highly recommend remapping caps lock to be control when held, and escape when tapped. reply el_benhameen 6 hours agorootparentI jump between using a Mac and a Windows machine for development. Remapping capslock to ctrl&#x2F;cmd feels more ergonomic, and has the added benefit of making it so I don’t have to remember which machine I’m on. reply __MatrixMan__ 8 hours agoparentprevThis is good advice. But be cautioned--at some point you&#x27;ll want \"some other key\" to be the left half of your spacebar or some other such nonsense. And then you&#x27;ll be one of us. Forever. reply 000ooo000 7 hours agoparentprevJust to add to the other ESC remapping suggestions:Insert mode (i.e. i): spaceExit insert mode (i.e. esc): shift+spaceSome terminals have issues with modifiers on keys like space but if you&#x27;re a vim user on only 1-2 machines and can use something like Kitty then you should be OK. reply jLid 8 hours agoparentprevBinding caps lock to control when held and escape if pressed alone is the greatest reply pylua 7 hours agoparentprevI agree with this, but there is another default combination that is escape , right?I realized that esc was difficult in vim when I was forced to change due to the original MacBook Pro Touch Bar. reply jodrellblank 7 hours agorootparentCtrl-[It&#x27;s not bad, but not all that comfortable on my left hand pinky finger. reply pylua 7 hours agorootparentThat’s it! I have come to use this without thinking and I have not noticed any discomfort. reply niederman 6 hours agoparentprevHonestly even if you don&#x27;t use Vim it&#x27;s worth doing. I&#x27;m a big fan of interception-tools&#x27; caps2esc on Linux. reply hcs 8 hours agoparentprevAfter using a keyboard with Esc even further away than usual, I&#x27;ve found myself using Ctrl-[ more often. I should look into remapping capslock. reply hollerith 8 hours agoparentprevHuh. I find ESC easier to type than Caps Lock because it is in the corner of the keyboard.I used to have RSI and what solved it (over 15 years ago) was taking more care to hit the precise center of the key. Using my sense of touch to sense the key&#x27;s exact location before even trying to activate the key helps me do that. When the key is on left edge of the keyboard, it is easier to do the determination of the exact location (of the left edge of the key I want) because I don&#x27;t have to worry about accidentally activating the key to the left of the key I want. Caps Lock of course is also on the left edge of the board, just like ESC is, but ESC is also on the top edge, which is an additional help: the easiest keys to type in my experience are the ESC key and the left control key because they are in corners of the board. (The other two corner keys, \"pause&#x2F;break\" and the right arrow key, are harder because of how far they are from the home row.)I never understood the desire many writers on this site have of moving the hands as little as possible. More precisely, I understand the rationale, but I consider the rationale to be misinformed. If you don&#x27;t move your hands (i.e., because you have a keyboard with only 36 keys or something), you still have to use your arm muscles to hold your hands over the board, and the human brain is better at movement than at using the muscles to statically counteract gravity like that. That is part of it, but there is more. When hitting a ball with a tennis racket, it is ergonomic to take a back swing, i.e., to move the part of the racket that will hit the ball in the direction opposite the direction you want the ball to go, before starting the swing. Very quick back swings are important in typing, too, for preventing RSI (for reasons I don&#x27;t fully understand). And I think moving my hands around the board (i.e., in the 2 horizontal dimensions) makes it less likely my brain will put the relevant muscles in \"freeze mode\", which makes RSI more likely. Or something like that. reply jfdi 8 hours agoparentprevinoremap kj :) reply precompute 48 minutes agorootparentThis is so useful I have it baked into my keyboard! reply qup 7 hours agorootparentprevI use this one, too.I&#x27;ve seen a lot of people recommend jj, but that&#x27;s part of something I type a lot, so I had to choose another. jk works okay too, because I don&#x27;t use that acronym. reply Clever321 7 hours agorootparentprevI thought I was the only one insane enough to do this. reply bbotond 7 hours agorootparentThere are dozens of us! Dozens! reply neilv 9 hours agoprevLong-time programmer here who is a typing machine, without discomfort, thanks in part to heeding Internet warnings about RSI circa 1990, and to finding what works for me.One of the things that matters for me when seated at a desk (I also do standing desk) is that I like to be able to have my feet flat on the floor when seated, and then for the keyboard height to be lower than most desks.If you&#x27;re similar, and you&#x27;re considering those powered height-adjustable desks, note the minimum height in the specs of a desk (and that the actual minimum height might be an inch or two higher, due to the top thickness and&#x2F;or leveling). Importantly, the \"3-stage\" ones on Amazon go lower than the \"2-stage\" ones.You can also occasionally find rare low fixed-height desks. I also once realized that the legs of a university lab&#x27;s white laminate desks were interchangeable with the legs of matching shorter white laminate side tables. (I quietly swapped a set, under the cover of night.)On chairs, I don&#x27;t like armrests in any case, and also, note that, if you have a lower desktop or keyboard shelf, armrests on chairs might bump into it.If you want to remove arms from a chair that has them, check how well that works before you spend a lot of money. Aerons with arms look like they can come off pretty cleanly. I&#x27;ve had good luck with some more conventional commercial-grade office task chairs. I&#x27;ve also seen office chairs that leave unsafe heavy-duty welded steel frame protrusions out the sides, with corners that could rip into someone&#x27;s leg someday. Steelcase Leap V2 arms can come off, but not cleanly, so it looks kinda dumb for how much money you spent. reply jillesvangurp 2 hours agoparent+1 on arm rests. I hate those. They box you in and lift your elbows uncomfortably high thus cutting off nerves in shoulders and arms. The word arm rest implies that these somehow are about resting your \"tired\" arms. I&#x27;ve never experienced any form of arm fatigue while using a chair. Just not a thing. Complete anti feature. I don&#x27;t need them. They come off if I have the choice. I won&#x27;t buy chairs that have them.Some of my other tricks include walking a lot (during my commute for example) and generally getting up regularly. I have some mini life hacks like getting a glass of water and not using things like water bottles or tea pots that \"allow\" you to stay at your desk longer. Anything that forces you to get up is a good thing. A simple stroll to the kitchen is a good thing. Standing desks are great too.Also, because I figured out that a lot of my issues are shoulder related I do some simple exercises to loosen those once in a while. That&#x27;s also why walking helps. Swimming is also great. If you sit still for extended periods of time, shoulders tend to cramp up and the nerves to your wrists and hands go through there. Where you feel pain and where the problems are are not necessarily the same thing. reply edvinbesic 7 hours agoparentprevI can echo this myself, however what made the biggest difference for me was having my elbows resting on something taking the weight off of my shoulders. Whether standing or sitting if I have to keep my elbows “up in the air” it becomes very uncomfortable very fast, otherwise I have no problem doing 12-14 hours sessions. reply marhee 2 hours agoprevHa! Experiences forearm and shoulder pain. Proceeds to mention casually he&#x27;s into rock climbing. DisplayS photos of games where (jerky) mouse mileage is, to put is mildly, excessive.Talking about elephants in the room here... I don&#x27;t think it&#x27;s the keyboard or mouse, dude! ;)I&#x27;ve done rock climbing myself. Obviously, it&#x27;s a huge burden on the ligaments of the upper body. It&#x27;s also very fun (not in the least because it&#x27;s a bit scary), but you will feel those wrists a shoulders a bit the next day. Small price to pay, until it because really painful.I would recommend to the author to try and mix in in some other fun sports like kite-surfing or mountain biking etc. Or dancing (salsa I can recommend if you&#x27;re open to the latin music). And limit mouse-mileage-heavy gaming a bit maybe? There&#x27;s a lot of other options (try Return of the Obra Dinn).Finally, I don&#x27;t know if other have experienced this, but reflecting a bit on how you do programming can also help. Thinking more, resisting that urge to start typing away did a lot for me personally wrt my wrists and shoulders starting to loving me again. But that may be just me, of course. reply pugworthy 11 hours agoprevI too have been programming for 40 years, am in early 60&#x27;s, and amazed that I have no real wrist issues from all that time of typing, etc.I&#x27;ve also been a pretty avid WASD + Mouse gamer for a long time as well, and I think gaming probably causes me more issues than programming. reply naruhodo 6 hours agoparentI&#x27;m in my early-mid 50s, I&#x27;ve been programming for over 40 years and like you I have experienced more cramps and discomfort from Ctrl&#x2F;Shift + WASD while gaming than any programming I have done.I am a rock climber, however, so I have some significant muscular development in my forearms. And the thought of that reminds me that from time to time I have had problems with my knees, which started when I was a child. My knees become painful and collapse under me. And the solution has been to do weighted leg extensions to strengthen the push&#x2F;kick muscles.So I wonder whether these wrist problems can be overcome by strengthening exercises.EDIT: and then I scroll down and see that Fabien is a climber. So I guess that refutes my theory. reply jdjdjdhhd 2 hours agorootparentI&#x27;ve been using a computer for 30 years and have no pain and not particularly strong... Whatever that means reply unnouinceput 11 hours agoparentprevSame here, same setup as his 1st step. No need for fancy standup mouse, ergonomic keyboard or all that crap. This guy deserves a \"your weakness disgust me\" gif on his blog. reply cmpalmer52 10 hours agorootparentThat’s a bit harsh. 57 here and my forearms, elbows, and wrists are f…ouled up without an ergonomic split keyboard and some other accommodations. Bodies are different, and if this guy is a rock climber, that might have something to do with it. But flat, tight keyboards give me tendinitis in as little as a day. I can’t even use my laptop without an external keyboard, which kinda defeats the purpose, but it’s what work gave me and it’s more powerful than my current, out of date, PC. reply zingababba 10 hours agorootparentI climb and the typical position at workstation of hands pronated + shoulders forward sucks for postural health. I currently am dealing with bilateral proximal bicep tendonitis from excessive climbing and even being at the computer for a few minutes aggravates it. I wonder if it&#x27;s possible to rig something up to be able to type in a supinated hand position and if it would be not super infuriating. reply tom_ 10 hours agorootparentprevHahah, oh dear, and you claim to be early 60s too? And yet here you are still acting like a teenager on the internet. It&#x27;s fun to be young, but you have to put it behind you at some point. reply jasoneckert 9 hours agoprevWhile everyone is different, it&#x27;s nice to read about other&#x27;s journey and the hardware they recommend along the way.I&#x27;ve been developing as long as the author, but lucky enough to not have any serious forearm&#x2F;hand pain during that time. There was a time a decade ago where I had some mild pain, but taking frequent breaks helped tremendously. And like the author, I started investigating other keyboards as well.Today I use a Moonlander Mark I and Apple Trackpad on one system, and a Lenovo TrackPoint II and Logitech MX Master 3 on my other system. Both configurations have their merits, and are equally good in my opinion. It&#x27;s also surprisingly easy to move between them. reply city41 10 hours agoprevI am also 46 and can not do much of anything physical with my hands anymore, such as sand, paint, use a screwdriver, etc. Years ago I switched to an ergonomic keyboard (Kinesis Advantage 2) and a vertical mouse. Neither bother me at all, I can type and use the mouse just fine all day. Unsure if the two are related. Does the pain I feel from sanding indicate I&#x27;d feel similar pain with a non ergonomic setup? Not sure. But I am glad I made the switch, just in case. reply nope96 9 hours agoprevI&#x27;ve been using computers since I was a pre-teen, it wrecked both my forearms (and later, my neck) in my early 30&#x27;s. There was NO WARNING, the issues just happened out of nowhere. I always thought RSI, if it happened to me, would happen gradually and I&#x27;d know to stop&#x2F;rest. The neck stuff has taken a decade to heal and it&#x27;s still not back to normal. reply porridgeraisin 5 hours agoprevI am somewhat of an edge caseMy left hands fingers are way more flexible and nimble than my right hand, even though I&#x27;m right dominant.I do basically all typing with all my left hand&#x27;s 5 fingers, accompanied by just two fingers on the right hand.I have looong fingers so it works out on standard keyboards without any need to stretch.But yes, never ever put weight on your wrists while typing, same advice as when playing a violin. reply rr808 9 hours agoprevAt least he has a desktop. I cringe every time I see someone spend all day on a laptop. reply fransje26 2 hours agoprev> Meditation in motionIs that Pierre-Chauve in the background? Nicely climbing the Rocher du Roi Gros Nez then! I can never get enough of the vistas you get from that crest line.. reply mlhpdx 9 hours agoprevAfter a lifetime of programming I feel like I still have a pretty-good “keyboard stamina”. I attribute this to not having any habits, let alone bad ones. Or maybe they’re all bad. I’ll explain.I started programming pretty early, but I didn’t have a desk until my first job after college. I pretty-much made do with whatever situation was available to me. I also started working pretty early, doing physical jobs. I bucked hay for local farmers at about 12 and started shearing Christmas trees at about 15. The latter doing a lot of damage to my shoulders over the next few years. In college I rowed crew, which didn’t make things better as far as my arms went.After college I started programming and I’ve been doing it professionally for going over 30 years now. It’s never really been physically comfortable. Even back then I was good for like an hour-long sit at most before I had to get up and move around and “get the blood flowing“ so to speak. That’s never changed. I still don’t like working at a desk. I stand, I sit on the floor, I sit in comfortable chairs, I lay down on the sofa, I’ve been known to steer a sailboat with one hand and program with the other.I feel like this is why it still works for me - I don’t repeat my posture much. I have a really, really low bar for my programming “situation” and take full advantage of that. reply jasfi 5 hours agoprevSome things that have worked for me:- Fish oil.- Getting enough protein.- Yoga. Especially for tight forearms and shoulders. reply jasfi 1 hour agoparentJust to add, yoga should really be done using a variety of stretches for your whole body. A professional trainer can also show you how to do the stretches properly. I often go straight to what I know works for me though. reply fipar 7 hours agoprevI’m about the same age as the author and also tried the freestyle as my first ergonomic keyboard.I’m now a big fan of my keybordio model 100.If you’re young and you plan to do this for a living, a good ergo keyboard with firmware you can customize is one of the best investments you can do for your hands’ (and arms) health. reply thomasfl 2 hours agoprevLower back pain is probably the worst problem for many developers. I have ended up doing one minute dead hang every morning. reply karmakaze 4 hours agoprevI have experienced shooting pains along the backs of my hands. I had to lay off typing&#x2F;mousing for a few days for it to subside whenever I tried typing. After that experience, I learned an alternate keyboard layout (customized-NIRO) and I haven&#x27;t had any issues since. I don&#x27;t believe my typing speed changed much if at all, it was all about reducing strain and improving comfort. reply 000ooo000 10 hours agoprevI took the opportunity to learn an alternate KB layout (Workman) and I think that has been beneficial for my hands, which are average to slightly smaller than average, so QWERTY movements like T are a slight stretch. Have also got the split KB (Kinesis Freestyle Edge, which is OK. Would like something that can run QMK or similar). Also paying attention to the little nags and working them out is definitely valuable. For, e.g. I run &#x27;g sta&#x27; (git status) often, as well as &#x27;g lscm&#x27; (git log -20 --pretty etc); now I use Bash bindings to run both with Ctrl+j, and &#x27;cls&#x27; with Ctrl+l. I&#x27;ve even started using the KB&#x27;s &#x27;action keys&#x27; (I.e. those ones that aren&#x27;t part of the standard ANSI&#x2F;layout) for Winkey+1..n to spare my left hand the awkward chord from winkey to the number row. I also run a trackball for dev and a mouse for gaming so that I split my use over different muscles. Hoping it all keeps any kind of injury at bay. reply keithnz 8 hours agoprevI&#x27;ve been programming (and gaming) 40+ years now, and weirdly, the main RSI injury I have is actually from when I was a kid using the Atari joystick as a left hander (this one https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;3&#x2F;33&#x2F;Atari-26... )now any fine motion where I need any kind of pinching motion for more than 30 seconds is super painful and by thumb pretty much gives up (using my thumb to tap the spacebar on the keyboard is fine though).Since learning about RSI about 30+ years ago, I regularly exercise and stretch my hands, so I think that has helped a lot. reply gooseyard 9 hours agoprevim a little older than the author. I also play a musical instrument that I guess you would say is physically demanding, and I have a hobby that is sort of hard on the hands. On the forums I inevitably follow about those activities, there is a sort of background of paranoia about how if you don&#x27;t do everything perfectly all the time, your hands will be ruined, you will never have seen it coming, and you should base all your decision-making regarding practice time around avoiding catastrophic injury.I&#x27;m sure there are some other people who share my interests who have been the victims of misfortune when it comes to RSI injuries, but I think for the bulk of us who do anything repetitive, it&#x27;s not incredibly difficult to avoid these issues if you just pay attention to what you feel.For example, on Friday I noticed that, after pretty much a solid week of intense editing of code (I wrote a large thing and then realized I had gotten it all wrong and needed to move a lot of stuff into different files), I kept noticing that my elbows were killing me because they were grinding into the armrests of my chair. It didn&#x27;t dawn on me until the end of the week that the reason I don&#x27;t usually have that elbow discomfort is that years ago I took the armrests off my chair, or as the case with my current chair, I folded them backward and out of the way. I had put them down to clean them or something and forgot about it.If I mind myself, I&#x27;ll notice the stuff that makes me uncomfortable. If I try doing whatever that is less and I feel better, then I&#x27;ve learned something. If I stop doing the thing that I&#x27;ve learned makes me uncomfortable, that&#x27;s great. If I did it too long and I need to visit a doctor or a therapist to help me undo the damage, that&#x27;s great too.So long story short, pay attention to how you feel, recognize that however you feel is probably a result of the choices you&#x27;ve made, intentionally or not, and treat your body as a machine that needs maintenance and either find a good technician or learn to do it yourself when it makes sense. reply rldjbpin 2 hours agoprevi am early in my journey and would like to take preventative measures before i have to overcompensate with fancy equipment when it is too late.what is it that one can do without spending too much or making overly drastic changes in how we work? reply hnthrowaway0315 10 hours agoprevI can confirm that the vertical mouse (exactly same brand) fixes my wrist issue. I used to feel some pain after playing diablo 2 a few days, a few hours each but now this doesn&#x27;t bug me anymore. reply pugworthy 8 hours agoprevI&#x27;m curious if typing speed is any indicator of problems, or lack of problems. Per previous comment, I&#x27;ve been \"typing\" for over 40 years, no real problems, and am a really, really fast typer. My hands don&#x27;t spend a lot of time locked in one particular position, and my fingers are moving all kinds of ways constantly when typing. Just a thought. reply qup 7 hours agoparenthow fast? reply Taniwha 11 hours agoprevI went tough much of the same in my 30s, and essentially went through the same steps and came to essentially the same conclusion, except in my case it means living on laptops with a trackpad reply svennidal 10 hours agoprevAfter less than a decade of programming around 8-10 hours every day, I could barely use my hands anymore. My forarms and shoulders were like marbles. I even stopped playing musical instruments because moving my finders was torture. Vim and split keybord (Dygma Raise) was my solution. I could feel the difference right away now, almost a year later, I don’t feel any pain at all. I feel lucky because I know people who have had to have surgery. reply TurboHaskal 2 hours agoprevStanding desks, split keyboards, photos of outdoor adventures... I see these everywhere. Is this the modern developer&#x27;s midlife crisis at 40? reply wly_cdgr 11 hours agoprevDoesn&#x27;t even mention what a legend he is one time, just straightforward useful clearly presented info.Legend.Also: the part about rock climbing because you can&#x27;t think about anything else resonates. I run for the same reason. reply StopHammoTime 11 hours agoprevSame on the wrist. I have one of those mice with a trackball on top that you use by running your hand over it. Absolute game changer and not at all difficult to work (probably even more sensitive tbh). reply boffinAudio 1 hour agoprev40 years of development here, too .. my solution to the RSI problem, which I discovered in the 80&#x27;s, is quite simple: change my keyboard and mouse setup regularly - simply don&#x27;t use the same keyboard&#x2F;mouse for longer than 6 months, maximum.Adopting this policy has kept RSI problems at bay, at least in my case. I have 3 or 4 favourite keyboards and I just switch them around ... giving my hands a chance to re-train on different weights&#x2F;resistances of keys, etc. reply bobobob420 11 hours agoprevOnly 1 monitor for 40 years? p impressive reply mjoin 11 hours agoparenti&#x27;m the same. i don&#x27;t understand why some fellow programmers need multiple monitors: one is already plenty enough for me. (i&#x27;m using a tiling window manager, switching back and forth between different workspaces, having another monitor would require moving my head while i can instantly switch to another workspace from my fingertips) reply beautron 4 hours agorootparentI&#x27;m the same too. Having multiple monitors makes no sense to me.With a keypress, I can switch between fullscreen workspaces. Or place two windows side-by-side when needed. My desk is free of clutter, and I never have to deal with configuring multiple displays.I believe multiple monitors are less elegant and less effective (though a giant workstation full of monitors is perhaps impressive to family and friends).A similar topic: I also don&#x27;t understand why some fellow programmers need syntax highlighting. Some programmers have never even tried programming without it! And to them it might feel weird to go without at first, but I think it quickly becomes clear that syntax coloring is noisy and superfluous (and you might end up wasting time picking colors, etc.). Imagine if we did that for natural language (verbs are green, nouns are red, etc.), or math notation (numbers are blue, variables are pink, operators are orange, etc.). The silliness becomes clear quickly. reply srazzaque 11 hours agorootparentprev+1I&#x27;ve found that, for me anyway, one centered large monitor with enough real estate for your daily tasks is better than N smaller monitors. This is even if \"total number of pixels\" is larger on the N-monitor setup.It&#x27;s one less decision to make 1000 times a day (which monitor should this thing be on?), and reduces neck strain resulting from switching your focus between monitors. reply bee_rider 8 hours agorootparentprevI usually like 1.But, it can be nice to have one for output; plots or something.I assume folks who work in a corporate environment would want one for slack, outlook, or whatever. reply vivab0rg 6 hours agorootparentprevYou don&#x27;t need to \"move your head around\" if you use TWMs like xmonad or spectrwm. reply keyle 11 hours agoparentprevIt&#x27;s been years I have 2 or 3 monitors and I find them annoying and mostly a waste of power.Every now and again I use them as a reference screen against something I&#x27;m doing, but my neck starts hurting pretty quick. Alt-tabbing rocks. They tend to be mostly empty.Alt-tabbing means improved focus \"one thing at a time\" too.I&#x27;m thinking to keep a large 32\" screen in front of me, ditch the side 32\" and get a \"portable LCD\" to put \"under\" as if my keyboard is a laptop and I&#x27;d have a laptop screen below my main screen. That should be hopefully useful as a reference screen.Having 2 or 3 of the exact same screen makes no sense to me anymore. We&#x27;re not with 19\" CRT Sony trinitron anymore :)I&#x27;m surrounded by a wall of screens and I don&#x27;t know what to do with them (that is useful and not distracting). reply tom_ 9 hours agorootparentPortrait displays are worth trying, in my view. I&#x27;ve gone through various iterations of multi-display setups, and I&#x27;ve settled on having 2 x 27\" portrait displays at 1440p (or scaled 4K equivalent), one directly in front of me and one to the side, plus a landscape display of any size to the other side.Both the portrait displays get a good amount of use. The central one is most important of course but the one to the side is useful too as I don&#x27;t have to turn my head much to look at it. Its usual use is documentation or similar but using it for actual work is no hardship.The landscape display is mainly there because I work in video games, so I kind of need one. I usually use only about half of it at most, though, to avoid having to turn my head too much. If I was in some other line of work I&#x27;d probably have a 3rd portrait display instead.(I did use the large monitor + laptop below arrangement for a while, and that was good too, but one day I knocked my drink over and it destroyed my laptop. So that prompted a rethink.) reply Aeolun 11 hours agorootparentprev> Having 2 or 3 of the exact same screen makes no sense to me anymore. We&#x27;re not with 19\" CRT Sony trinitron anymoreI used to have three 1080p screens. Now I just have one 4K screen, and I still have more screen space :) reply CodeWriter23 11 hours agorootparentprevI got a 43” 4K TV that can do 60hz with reasonable signal input latency. You can check the specs of many TVs on rtings.com. Just over $200 at Costco these days.I personally like to use Divvy to chop my screen into sections but mostly I do 2&#x2F;3 for my IDE (affords me 3 less-than-full-width columns for source) and 1&#x2F;3 for the browser. I have wanted to have an iPad for displaying docs but haven’t gone there yet. reply keyle 10 hours agorootparentIs it as good for your eyes? Aren&#x27;t TV designed to be looked at from a further distance than arm&#x27;s length? reply arcanemachiner 8 hours agorootparentNot the GP, but the pixels on my 43\" 4k TV are just slightly smaller than those on the 1080p monitors it replaced. I have the TV positioned about an arm&#x27;s length away. It&#x27;s farther than the monitors it replaced. And it&#x27;s far enough away the I don&#x27;t have to crane my neck side to side. (To that end, I also use a tiling window manager and have ~3 inch gutters on the side.)I love it and it&#x27;s just a $200 TCL TV that happens to have good response (thanks rtings.com). replyh2odragon 9 hours agoprevThe advice given to pianists is valuable for PC keyboard users: \"wrists up!\"trackballs are possible to use without fatigue. mice, less so. reply gpspake 11 hours agoprevHa, I was having wrist pain and I got that same wrist rest. I&#x27;ve recommended it to a lot of people. (Although I&#x27;ve never thought about cutting one in half for my mouse :) reply moron4hire 11 hours agoparentI&#x27;ve found that changing keyboards between a flat mechanical and a Microsoft wave keyboard every 6 months keeps the pain away. Don&#x27;t need anything fancy, just gotta change the exact factors of the repetitive motion. reply apatry 9 hours agoprevAnother thing that made a difference for me regarding wrist pain was to switch my keyboard layout to Colemak. reply polishdude20 10 hours agoprevI find that using the trackpad my wrist is always working to keep my hand over the trackpad without touching it. reply yosef123 11 hours agoprevIt’s crazy to think that even a rock climber can suffer from RSI. Goes to show it can happen to anyone. reply paulryanrogers 9 hours agoparentI suspect they are especially at risk. Guy I worked for used to climb and switched to a track ball. When I got into chin ups in a big way within a few months I had some new hand pain. Also heard rock climbing is a young person&#x27;s game, not that many gray beards, likely because of the toll. reply notacoward 9 hours agoprevI didn&#x27;t quite make it to forty years, but I think I could say I made it to 37 (1983-2020). I&#x27;ve also spent more time than the average programmer writing on a computer, and I&#x27;ve never had any significant RSI-type issues. This might be a bit controversial, but I think part of the reason is that I&#x27;m not a traditional touch typist. That&#x27;s not to say I&#x27;m slow, either. I&#x27;ve probably worked with well over a thousand other programmers, and the only person I&#x27;m sure was faster wasn&#x27;t any of them. It was my mother, who was a professional typesetter longer than I was a professional programmer. Only a handful of other programmers have even seemed close.So, why do I think that relates to my lack of RSI problems? Because with my seven(ish) finger method my hands constantly rove over the keyboard. What they don&#x27;t do is stay in one fixed location, with the wrists in the same (usually somewhat awkward) position as only the fingers move. That maximizes repetition - the R in RSI. Minimizing hand motion this way is bad. If you want your hands and wrists to stay healthy you have to keep them moving, just like any other muscle&#x2F;tendon&#x2F;ligament complex in any other part of your body. It&#x27;s silly to think that the general rules of exercise and flexibility don&#x27;t apply to hands. reply jacurtis 10 hours agoprevA WARNING for anyone who thinks this article doesn&#x27;t apply to them:If you are reading this post and thinking, \"oh I don&#x27;t have these problems, I&#x27;m going to read something else\", I encourage you to pause.Having been in this industry for a while, I have seen RSI-type injuries happen to HUGE portions of my colleagues. I&#x27;ve met many people who have changed careers over this.The reality is that spending 8-10+ hours a day in front of a keyboard is grueling on the hands, wrist, and arms (not to mention it can be for your back and neck as well).It isn&#x27;t a matter of \"if\", but \"when\". For some the result is more impactful on their daily life than others, but it does affect nearly everyone to some extent.So I encourage anyone who is still young, thinking this is a post for \"old people\", to consider applying some of these principals today in hopes of pushing these problems further down the road or maybe even to put you in the minority of people who never have to deal with them in their career. The apple mighty-mouse thing (whatever they call it now) is horrible for your wrists and hands, throw it in the trash. Consider investing in some of these tools now, so you don&#x27;t suffer later.In a similar vein, take care of your posture which can save you from back problems and neck or spine injuries later. Sit&#x2F;stand desks are great options and readily available now (and relatively affordable). Consider an ergonomic chair as well, and don&#x27;t be afraid to spend good money on it. You spend 8-10+ hours a day in it. It&#x27;s worth spending $1,000 on a desk and $1,000 on a chair that will save you thousands in medical bills and a priceless amount of avoided pain down the road. It&#x27;s funny to me how many engineers making $150k or more a year and won&#x27;t spend $2,000 on a good desk setup (which lasts for many many years). If you have a work-from-home budget, spend it on ergonomic tools, not a fancy monitor with a higher refresh rate. reply polishdude20 10 hours agoparentAlso take frequent breaks! Get out of your chair often. If you&#x27;re literally sitting 8-10 hours a day, you&#x27;re doing it wrong. reply jacurtis 10 hours agorootparentGood advice. When I was young I used to just \"plug in\" for one constant (more or less) session all day long. I don&#x27;t know how I did it to be honest.But yeah now, I NEED breaks. Walk around, get a snack, go for a quick walk outside. Really anything to break it up. The impact is better on you mentally as well as physically. reply strawhatguy 9 hours agorootparentprevI think this is key. I never had too many issues taking breaks, to be honest. Even when &#x27;programming&#x27;, I spend a lot of the time thinking about what to do, and not actually typing. I heard early on that if you&#x27;re constantly typing, you&#x27;re doing it wrong, and I must&#x27;ve taken it to heart.I&#x27;m 44 myself, and fortunately haven&#x27;t had that sort of pain in my hands&#x2F;wrists&#x2F;forearms. Have had back pain though, and that sucks. reply AA-BA-94-2A-56 9 hours agoparentprevI (30m) had only just graduated university (Master of Information Technology, majoring in Software Engineering) when (due to my desk setup that was not ergonomic, sleep position, and other factors) I was hit with an ulnar nerve nerve injury which left me unable to use the computer. It took me six months to recover, and in that time I had to give up my gym habit, all video games, looking for work. I was lucky it happened to me then, and not now. I have a mortgage, and my wife and I are planning children. I can&#x27;t have this happen again in the future.The best thing I ever did was get a really good ergonomic chair after this for A$700. reply angarg12 11 hours agoprev [–] Am I the only one who finds these \"I&#x27;ve been programming since 6\" posts silly? I mean, I used to mess up with computers since I was a child, but I wouldn&#x27;t call that \"programming\". In fact if I&#x27;m completely honest, it wasn&#x27;t until 5 years ago when I truly started to learn and grow as an engineer, more than the rest of my career combined.Rather than focused on quantity let&#x27;s focus on quality. 1 good year of experience is worth 10 bad ones. reply shric 9 hours agoparentThis might be particular to the age of the OP. I just turned 47, so I&#x27;m one year older than him.In 1981 (so when I was 5) my parents bought me a Sinclair ZX81. It came with literally nothing except a BASIC interpreter in ROM and a manual. The manual explained how to program in BASIC and had a bunch of listings that you could type in for rudimentary games etc. So when we got our computers at that era all we could do is \"program\". Was it hardcore software engineering? No. Did we understand it at that age at any meaningful level? No. But it was programming, of a sort.In the next few years I experimented more and more until I started to grasp higher level concepts.My point is there was a particular window in time that met these criteria:- affordable personal computer.- no (or very few) games available, you had no choice but to write programs.This was around 1980-1983. Before 1980 most people didn&#x27;t have access to personal computers. After 1983 you could get computers that could easily read games from tapes or floppies.So I guess there were a bunch of 5 year olds in that era who were almost forced to program. reply briHass 7 hours agorootparentAs someone 4 years younger, I&#x27;d argue the &#x27;good times&#x27; carried on until the early 90s, at least for a kid that didn&#x27;t have any money to buy new games all the time. The games back then were all simple enough that a devoted kid with lots of free time would get bored&#x2F;beat the game in a day or two, and then you have to find something else to do. Now, there are millions of free games on the internet, not counting pirate&#x2F;emulation, and also plenty of other media content distractions.I&#x27;d also say that era (88-92) was great for being able to pick up cheap used computers. Commodore 64s could be found at garage sales, and I was fortunate that my father would bring home old IBM PC XT&#x2F;AT boxes from work that were destined for the dumpster. reply shric 7 hours agorootparentAgree, I did program on the Commodore 64 and later Amiga extensively and I too couldn&#x27;t afford many games. However I&#x27;m not sure if I was the typical kid -- in the late eighties and nineties many of my friends had consoles and I had no interest in them because I couldn&#x27;t program them. reply xorcist 11 hours agoparentprevThe one sentence that isn&#x27;t meaningful to the text, and that&#x27;s the one you wish to comment on?I started programming a few years later, maybe at 8 or so, but I would absolutely call that programming. I still have the note pads somewhere. It&#x27;s nothing to be ashamed of, it&#x27;s a beginner&#x27;s first steps on a computer that does not exist. reply xelxebar 11 hours agorootparentWait. You started out by scribbling down programs on a notepad and no computer? That&#x27;s both adorable and inspiring. Were you working with BASIC? I&#x27;m super curious what this was like for you. reply jslabovitz 10 hours agorootparentI did this too! Not on a notepad, but on a Royal typewriter, age 10. My dad had a telecommunications consulting business, based on a remote Univac mainframe he rented time on. He&#x27;d bring his Texas Instruments Silent 700 printer-terminal home from work, and I&#x27;d sometimes get to play Adventure and what-not if he wasn&#x27;t using it. I vividly remember typing my first BASIC program on the typewriter, one afternoon after school, and then waiting excitedly for him to get home so I could actually type the program into the terminal. (Pretty sure the program was: 10 PRINT \"HELLO\" &#x2F; 20 GOTO 10.)EDIT: This would have been ~1976. reply xdennis 9 hours agorootparentprevI did that too. I thought it was common. I learned programming by reading my older sister&#x27;s Computer Science manual from high school in preparation for me starting high school (2004, Romania).All school work was on paper so it was a necessity.But it was fun too. In pointless classes like French, there was nothing better to do. It was worthwhile to work on my personal programs on paper and type them out when I got home.You had to think about the approach before starting to write it down. I would leave a lot of vertical space in case I needed to put something above. In some cases I would write the code afterwards, draw a rectangle around it, and point to where it needed to move. It was a bit messy sometimes. reply bmacho 11 hours agoparentprev> I mean, I used to mess up with computers since I was a child, but I wouldn&#x27;t call that \"programming\".Well, he does call programming whatever he did.In &#x27;83 they have sold 5 million computers according to the wikipedia[1], I am sure tens of thousands of 6 year olds started programming on them.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Market_share_of_personal_compu... reply bcrosby95 11 hours agoparentprevI&#x27;m not sure why it would be silly. I started playing baseball when I was 6, and if I continued to this day I would say I&#x27;ve been playing for 38 years. reply cableshaft 5 hours agoparentprevI was coding in Q-Basic when I was about 9 years old. It wasn&#x27;t much more than colored text on the screen and some inputs for a text based choose your own adventure for a little while but I was doing it. I was also playing other Q-Basic games and then reading their code to learn how to do certain things, then tweaking certain aspects of the code to change things, like what was said in messages and the hud and how much the score went up, etc.Likely the only reason it wasn&#x27;t earlier is I didn&#x27;t have access to a computer until then. My parents didn&#x27;t buy one until around that age, and I was already bugging them about it because my friend had one and was showing me these cool Q-Basic games he found on Compuserve. I started programming almost immediately after we got a computer in the household.Now granted 5 is pretty young, I didn&#x27;t even really know how to read then. But I probably would have by 7 years old if there was a computer in the house at that point.I was also making Hypercard programs on the school library Macintosh (the original Macintosh) around that time, mainly images I drew with clickable portions to navigate to new screens, like navigating a maze.My more serious programming was when I got a TI-85 calculator in 7th grade, though, so when I was around 12 years old. I started making text-based games on there, then action based games like Breakout, and at that point I was using plenty of for and while loops and checking keyboard inputs and creating menus and slinging variables around and calling functions and everything else. reply saulpw 11 hours agoparentprevThe post is about his environment and computing setup. It has nothing to do with programming. The title and first sentence only mention that he&#x27;s been programming since he was 6 to give context for his long-term computing use and how his life and computing setup have evolved. reply dboreham 11 hours agoparentprevSome people actually did program when children, as opposed to \"messing around\". reply vhodges 11 hours agoparentprevI&#x27;ve been coding for 42 years (since the age of 13), about 33 years as a professional developer. I do tend to downplay the stuff I did on my own for the first 9-10 years because a lot of my peers started then too, but the stuff since late &#x27;89&#x2F;early &#x27;90 counts.More relevant to the post, I have had some pain in the past, mostly neck&#x2F;shoulder from the monitor not being high enough, but some finger pain too (tendinitis - even before mice use was prevalent), switching to a vertical mouse has helped to reduce it quite a bit. The mouse I use currentlyhttps:&#x2F;&#x2F;www.logitech.com&#x2F;en-ca&#x2F;products&#x2F;mice&#x2F;lift-vertical-e... reply onetom 9 hours agoparentprevI&#x27;ve also spent breaks between classes in primary school, at age 6-7, by writing BASIC programs on graph paper. Within a year or 2 I&#x27;ve transitioned to writing in Forth for XZ Spectrum. I used a Roland DX 80 plotter to \"print\" my source code. I&#x27;ve even sold plotted images from AutoCAD 1.0 for DOS.I&#x27;ve also learnt Z80 assembly at the same time, coz the \"4th Forth by Fébert Csaba for ZX Spectrum\" had built-in assembly too.I&#x27;ve seen my father designing and building a ZX Spectrum clone from scratch. He did explain the process too, so I got to know how a CPU+ALU+RAM+BUS+IO made up a computer.So I very much consider those years part of my programming career, because they were very much formative and it was a continuum as programming became my career.Also, don&#x27;t forget, that all this was cutting edge shit, because there wasn&#x27;t anything better available for affordable prices for everyday ppl!even access to the ZX Spectrum was only possible for me, because my father could bring it home from work for the weekends and I could use it a little bit after school, at the University he worked at. my classmates never even saw any kind of computer up close, other than LCD wrist watches... reply spacedcowboy 11 hours agoparentprevMmm. I&#x27;ve been programming since 11. When I built my first computer (back then), it was soldering components to a PCB, not plugging a thing-that-only-goes-in-one-way into slots&#x2F;sockets #1, #2, #3 etc.So I guess I&#x27;ve been programming for longer than him, despite starting several years later in life. What \"programming\" means has changed a lot over the years though, I&#x27;ll say that much. reply keyle 11 hours agoparentprevI don&#x27;t think it&#x27;s silly, I like to see \"how\" people work.The author is not uninteresting. His experience is far greater than most. You should browse his blog and you&#x27;ll find you&#x27;ll learn plenty.Not every post has to be about something extraordinary. We&#x27;re all aging together. He wasn&#x27;t boasting his programming experience onto the viewers. reply fiddlerwoaroof 10 hours agoparentprevI’ve been programming and reading programming-adjacent books since I was about six. It definitely has shaped my views about hyped technologies since I remember the same sort of rhetoric used to promote OOP and XML and Java as is being used today to promote Crypto, AI and Rust.I don’t know if it’s made me a better programmer, but it’s made me realize the importance of focusing on fundamentals and learning the ephemeral stuff just in time. reply mlhpdx 7 hours agoparentprevI started programming in 3rd grade when a teacher kept me in during recess because I was being bullied, and was first paid for programming in 7th grade when I ported a program from BASIC to C and used the money to buy my first computer). Silly? Didn’t feel that way. reply nemetroid 11 hours agoparentprevIt&#x27;s an extremely small part of the post. reply rco8786 11 hours agoparentprevFeels like weird gatekeeping. Doesn’t count as “quality” unless you’re doing it full time as a career? reply antod 8 hours agoparentprevExcept the article is about 40 years of typing&#x2F;mouse injuries and how they dealt with them, rather than 40 years of quality engineering output. reply actuallyalys 11 hours agoparentprevI mean, I also mostly “messed around” at that age and didn’t really consistently program so I probably wouldn’t say that either. But I wouldn’t rule out the possibility that Fabien Sanglard did. reply meiraleal 11 hours agoparentprevHave you had the opportunity to compare two 20 years old developers one with 10 years of experience and another with 2? reply booleandilemma 11 hours agoparentprev [–] I almost always see it as a red flag. When I hear someone irl say it I interpret it as a sort of clout hack.\"I&#x27;ve been programming since I was six years old so listen to me when I say we should use mongoDB\". reply jongjong 10 hours agorootparent [–] It can be meaningful but it depends on how they&#x27;ve been using that time. If they stayed at one company working on one project on a specific feature for 40 years straight, they may be a world class grandmaster in a very specific area which is only relevant for like 5% of all projects but have narrow experience and be incapable of providing advice on most coding topics that affect 95% of projects. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Experienced programmer, Fabien Sanglard, offers tips for ergonomic workstation setups and pain relief strategies to alleviate strain from prolonged programming.",
      "Sanglard's recommendations include using a vertical mouse, the Magic Trackpad, and the Ergodox EZ keyboard, and enabling VIM mode in text editors to limit wrist and arm strain.",
      "He further highlights the advantages of a standing desk and suggests employing stretching exercises and meditation to manage stress levels."
    ],
    "commentSummary": [
      "The forum discussion revolves around programming and physical health, exploring the difficulties presented by continuous keyboard work.",
      "It stresses the importance of ergonomics and strategies to prevent repetitive strain injuries, with commenters sharing their own experiences and recommendations regarding keyboard layouts, exercises, and breaks.",
      "Briefly, it also touches upon the influence of programming skills and experiences on professional growth."
    ],
    "points": 234,
    "commentCount": 126,
    "retryCount": 0,
    "time": 1696802109
  },
  {
    "id": 37817152,
    "title": "Blackmagic Camera for iPhone",
    "originLink": "https://www.blackmagicdesign.com/products/blackmagiccamera",
    "originBody": "Products Resellers Support Developer Company Media Splice Forum Log In Blackmagic Camera Tech Specs Blackmagic Camera Introducing Digital Film for iPhone! Blackmagic Camera unlocks the power of your iPhone by adding Blackmagic’s digital film camera controls and image processing! Now you can create the same cinematic ‘look’ as Hollywood feature films. You get the same intuitive and user friendly interface as Blackmagic Design’s award winning cameras. It’s just like using a professional digital film camera! You can adjust settings such as frame rate, shutter angle, white balance and ISO all in a single tap. Or, record directly to Blackmagic Cloud in industry standard 10-bit Apple ProRes files up to 4K! Recording to Blackmagic Cloud Storage lets you collaborate on DaVinci Resolve projects with editors anywhere in the world, all at the same time! Get the \"Hollywood Look\" with an iPhone! Blackmagic Camera puts the professional features you need for feature film, television and documentaries in your pocket. Now you can create YouTube and TikTok content with a cinematic look, and broadcast quality ENG! Imagine having a run and gun camera on hand to capture breaking news whenever it happens! Or use Blackmagic Camera as a B Cam to capture angles that are difficult to reach with traditional cameras, while still retaining control of important settings. Best of all, recording to Blackmagic Cloud allows you to get your footage to the newsroom or post production studio in minutes. Whatever content you’re creating, Blackmagic Camera lets you capture digital film quality anywhere! Interactive Controls for Fast Setup Blackmagic Camera has all the controls you need to quickly setup and start shooting! Everything is interactive, so you can tap any item and instantly change settings without searching through confusing menus! The heads up display, or HUD, shows status and record parameters, histogram, focus peaking, levels, frame guides and more. Show or hide the HUD by swiping up or down. Auto focus by tapping the screen in the area you want to focus. You can shoot in 16:9 or vertical aspect ratios, plus you can shoot 16:9 while holding the phone vertically if you want to shoot unobtrusively. There are also tabs for media management including uploading to Blackmagic Cloud, chat and access to advanced menus. Camera Media Chat Settings On Screen Heads Up Display The heads up display, or HUD, has the most important camera controls such as lens selection, frame rate, shutter angle, timecode, ISO, white balance, tint, histogram and audio levels. You can adjust settings such as exposure by touching the ISO indicator, or you can change the audio levels simply by touching the audio meters. It's that easy! Everything is interactive, so if you tap any item you can instantaneously change its settings without having to search through complex menus! You can clear the heads up display to reveal the full screen image by swiping up or down with your finger. Status The indicators across the top of the screen show various camera settings and they can also be changed, just by touching the specific setting. An easy to use adjustment palette will then be displayed! Lens Lens let’s you select a lens for your shot. Cell phones often have 3 rear lenses ranging from 13mm, 24mm and 77mm telephoto, plus a front lens. Iris is automatically set based on your lens choice. Frames Per Second The frames per second adjustment is normally set based on the post production delivery format. You can adjust the frame rate from the preset 24 fps up to 60 fps depending on your phone model. Shutter Speed Shutter speed lets you adjust for varying light conditions or control the amount of motion blur in your image. You can set it manually or choose from suggested speeds by scrolling the speed dial. Timecode The timecode indicator will show you either the duration of your recording or the time of day. During recording the timecode will turn red, and display the embedded timecode playing back the clip. Project Name Indicator If you have selected a DaVinci Resolve project to record to from the media tab, the name of this project will be displayed above the timecode in the HUD. ISO ISO adjusts the image sensor’s sensitivity to light. A setting such as ISO 100 is suited to outdoors, with higher values great for low light situations. Tap the ISO icon to access the ISO settings. White Balance/Tint White balance lets you adjust for different temperatures of light. Common presets include sunlight, overcast and fluorescent light, or you can set it manually. You can save settings by locking them. Resolution The resolution you set in your record settings is shown in the status display, so you can check if you are shooting in 720p, 1080p or up to 4K at a glance. Histogram The histogram helps check exposure. The left side shows shadows and the far right shows highlights. If the edges of the histogram come to an abrupt stop, adjust exposure to ensure you’re not clipping. Audio Meters You can monitor the recording levels of the internal microphone or external sources using the audio meters. To avoid distortion, peak audio levels should fall in the upper end of the green zone. Record Button The record button lets you stop and start recording. Tap it once to begin recording and tap it again to stop. When recording, the button and timecode at the top of the screen turn red. Storage Indicator Storage indicators show the remaining recording time on your phone’s internal memory in hours, minutes and seconds. Turn on the storage indicator display by accessing setup menus in the settings tab. Upload Indicator When you are uploading to Blackmagic Cloud, the upload indicator displays the name of the clip as well as the upload speed, the percentage of the clip that is uploaded and time remaining to complete. Zebra The Zebra display will draw diagonal lines over areas of your image that have excessive exposure level. The variable control when used with the histogram lets you see the best exposure for any scene. Focus Assist An extremely powerful feature, focus assist adds a colored highlight to fine image detail, so you can see what parts of the image are in focus. This allows manual focus to be used for cinematic looks. Grids The grid control adds on screen markers for framing and positioning items in a shot. There is a range of on screen markers including a rule of thirds grid, crosshair and a center dot. Frame Guides When shooting digital film, other aspect ratios are often used. On screen overlays include 1.85:1, 2.35:1, 2.39:1 and 2.40:1 for theatre release. Or use 9:16, 1:1 and 4:5 for social media! Safe Area Guides Safe area guides let operators ensure important image content does not get too close to the edge of the screen, otherwise it could be cut off when viewed on consumer grade televisions. False Color False color overlays colors to represent different exposure values. Pink represents optimum exposure for lighter skin tones, while green is good for darker skin tones, and red is overexposed. 3D LUT Indicator LUTs let you preview a specific \"look\" as you are shooting so you can see what it will look like after it has been color corrected. When in use, a LUT icon is displayed in the top left of the LCD. Touch to Focus Touch to focus allows you to auto focus your lens in any region of the image. Tap the screen in the area you want to auto focus. Touch and hold to lock the focus and exposure. Auto Exposure Auto exposure adjusts ISO and shutter angle to achieve the best exposure for your image. You can simply tap anywhere on the screen to quickly set that as your auto focus area. Image Stabilization Image stabilization prevents shaky footage and helps make your video look more professional. Tap to select which level of stabilization you wish to use including cinematic for smoother motion. Zoom Control Zoom gives you up to x8 magnification using presets, or you can manually adjust the zoom by scrolling the scale up to 15x. Tap the zoom icon to hide the display and return to full screen view. Entering Metadata Metadata makes sorting and processing footage in post production easier. The slate lets you add additional details such as shot type or project name. Swipe left or right to change the pages. Camera Setup Menus The settings tab unlocks the full power of the phone’s camera, with quick access to advanced settings such as monitoring, audio, camera setup, recording and more! The record tab allows control over video resolution and recording format including industry standard Apple ProRes or space efficient H.264 and H.265. Plus, you can set anamorphic de-squeeze and lens correction settings. Professional audio options include VU or PPM audio meters and AAC, IEEE Float and PCM formats. You can even add external microphones! Blackmagic Camera also includes professional monitoring tools such as zebra settings for checking exposure, focus assist, frame guides and more. Or add 3D LUTs to recreate film looks! Media The Blackmagic Camera media tab has all the controls needed to browse or scrub clips for quick review, search and sort and view the upload status of your media. You can also link to your photos and select clips to upload to the Blackmagic Cloud. Simply access your media from Blackmagic Camera’s all clips folder by choosing the Media button to see the thumbnails for each clip you have stored. You can save your media to the files folder on the phone, send it to Blackmagic Cloud Storage via Blackmagic Cloud or manually choose which clips to upload to a project library. You can even sync media from Blackmagic Camera directly into a DaVinci Resolve project so you’re ready to edit and color grade! Three Great Workflows! 1. Record to Phone Storage You can record to your phone and export your clips to an external drive. This will allow you to free up valuable storage space on your phone and let you share your files later to other users globally. 2. Record into DaVinci Resolve Log into Blackmagic Cloud and select a DaVinci Resolve project before recording. After each individual recording the media will automatically upload and sync to all members of that project. 3. Upload Selected Clips Blackmagic Camera also has a manual option where you can record your footage to your phone and then select which clips you want to upload via Blackmagic Cloud when you have a network connection. Blackmagic Cloud Creating your Blackmagic ID allows you to have access to Blackmagic Cloud. It’s easy and there’s no charge! Simply tap on Blackmagic Cloud in the media tab to go to the Blackmagic Cloud website to register. Or go to the cloud icon on the Blackmagic Design website. Once you have created your Blackmagic ID you can log in and set up your project library. This is where you create projects to upload to Cloud Storage. With your project library set up, you can then select for Blackmagic Camera to sync directly to Cloud Storage from the app. Invite others using their Blackmagic ID to share the project and instantly collaborate with editors and colorists globally, working on the same project! DaVinci Resolve Project Server Cloud Storage Presentations Settings Live Sync to Blackmagic Cloud Storage When shooting with Blackmagic Camera, the video you capture can be instantly uploaded as a proxy file, followed by the camera originals, and saved to Blackmagic Cloud Storage. These will then automatically sync to all members of the project anywhere in the world. This means you can start editing quickly using your proxies, speeding up your workflow. Media can be constantly added to the project by multiple cameras in different locations which is automatically synced to other project members via Blackmagic Cloud. Everyone can use the proxy media and the colorist or finisher can download the original high res camera originals and render. It’s a fast, seamless and automatic way to collaborate. Download DaVinci Resolve, Free! The best creative tools shouldn’t be limited to Hollywood. With a free version of DaVinci Resolve, you can learn how to use the same tools as professional Hollywood artists. DaVinci Resolve is designed to inspire creativity so you can do your best work. You can sync media from Blackmagic Camera directly into the DaVinci Resolve project via Cloud Storage. Once you learn the software and start using it for more work, you can purchase DaVinci Resolve Studio which adds tons of additional effects, AI tools and more. Plus, you can add an editor keyboard, color control panel, or audio console so you can use both hands at the same time, allowing you to work even faster and be more creative! Sync Bin for Multicam Editing The sync bin is the world’s fastest and most innovative way to select the perfect cutaway when doing multi camera shoots. Using the time of day timecode, sync the clips shot on multiple phones at the same time. Just select the sync bin icon and DaVinci will find all clips that sync to the timeline and display them in a multiview. Simply scroll up and down the timeline and you will see all the clips that sync to the point in the timeline so you can pick the best cutaway. Then click the view with the mouse and then adjust the in and out points to select the cutaway you want. Now use the source overwrite edit mode to add this selected clip to the timeline, perfectly synced to the clip below! Communicate with Editors via Chat Blackmagic Camera features a built in chat workspace so Blackmagic Cloud project members can talk about shots and quickly share creative ideas, all without leaving the app! Simply log in to Blackmagic Cloud and select the project you are working on and start typing a message. You can quickly message editors, assistants, colorists or VFX artists to talk about shot selection or to receive an update on the project! Everyone on that project can see your message and reply instantly. All this means you can keep in touch with other project members without leaving the app and missing a critical shot! NEXT PAGE Tech Specs Blackmagic Camera NEW Blackmagic Camera Get high end digital film camera controls and image processing on your iPhone! Adjust settings such as frame rate, shutter angle, white balance and ISO, then upload to DaVinci editors worldwide! Free Download Download Now on the App Store DaVinci Resolve 18 Hollywood’s most popular solution for editing, visual effects, motion graphics, color correction and audio post production, all in a single software tool for Mac, Windows and Linux! Free Download Download Now Products Professional Cameras DaVinci Resolve and Fusion Software ATEM Live Production Switchers Ultimatte Disk Recorders and Storage Capture and Playback Cintel Scanner Standards Conversion Broadcast Converters Video and Audio Monitoring Network Storage MultiView Routing and Distribution Streaming and Encoding Support Resellers Support Center Contact Us Community Forum Splice Community Company Offices About Us Partners Media Privacy Policy All items on this website are copyright Blackmagic Design Pty. Ltd. 2023, all rights reserved. All trademarks are property of their respective owners. MSRP includes duties, but excludes sales tax and shipping costs. This website uses remarketing services to advertise on third party websites to previous visitors to our site. You can opt out at any time by changing cookie settings. Privacy Policy Follow us: Country or Territory: United States",
    "commentLink": "https://news.ycombinator.com/item?id=37817152",
    "commentBody": "Blackmagic Camera for iPhoneHacker NewspastloginBlackmagic Camera for iPhone (blackmagicdesign.com) 226 points by Lwrless 4 hours ago| hidepastfavorite113 comments herunan 3 hours agoI was surprised to see this was free. On top of that, with an impressive feature set for an initial release. Considering Blackmagic’s reputation, this will easily beat any other half-baked camera apps or paid apps in no time. This is awesome for film school students. Already recommended it to a few of my friends who are into film. reply sneak 3 hours agoparentFree or not free, I am surprised to see that this includes absolutely no phone-home of any kind. “Data not collected.”Kudos to Blackmagic.(Resolve can also work 100% offline, with the license dongle, and is buy-once, not subscriptionware like Premiere. I am a very happy Blackmagic customer.) reply pdpi 3 hours agorootparentThe thing that most impresses me about Blackmagick is how they seem to scale with you from tiny projects to pretty big stuff. From the ATEM Mini all the way to big consoles, from the pocket cameras up to the Ursa etc.This just looks like it’ll drop the low end of that range even lower. reply dharma1 3 hours agoparentprevBlackmagic are awesome, run by a fantastic founder-CEO. They give a bunch of software away for free (Resolve basic version) - I guess enough of it converts to users of their paid stuff like the hardware and Blackmagic Cloud and Resolve Studio.Been a user of their hardware and software for years, nothing but good things to say about it. reply _joel 7 minutes agorootparentSame, it&#x27;s even used in large public UK broadcasters as it&#x27;s good gear. reply dcow 2 hours agoprevAs a non-film person, can someone explain what it means to create the same cinematic ‘look’ as Hollywood feature films? What is Blackmagic doing when recording video to make the video feel more professional? reply javchz 2 hours agoparentMarketing aside, cinematic in this context means more or less \"manual control\".Something that makes a video look amateurish, it&#x27;s the phone trying its best to prioritise a &#x27;clear image&#x27;, but that means changing parameters mid-recording.Now, this isn&#x27;t bad, it&#x27;s ideal for someone who doesn&#x27;t want to lose the moment without worrying about choosing the right setting (imagine a parent recording their child&#x27;s recital or soccer game). But the trade-off is that it looks choppy.But if you&#x27;re in a controlled environment, you can set a fixed exposure (balance between ISO, shutter speed and aperture), framerate, bit-depth, focus distance, colour temperature and microphone gain depending on your intent.As an example, image you want to have a high-contrast image with a dark silluette of someone and a bright background like a sunset, the default phone camera app will try to guess whether you want to focus on the subject or the background, and will switching between the two randomly. With manual control, you can chose, whatch you want. reply zimpenfish 2 hours agorootparent> Something that makes a video look amateurish [...] changing parameters mid-recordingA prime example of this is leaving autofocus on when you&#x27;re moving about. There&#x27;s many YouTubers who haven&#x27;t yet learnt this lesson and it can make the video unwatchable. reply KineticLensman 1 hour agorootparentYes. It’s very rare to see the focus change during a movie or TV show. The main exception is when the focus switches between two people talking, when their positions are known in advance and dialled in so there isn’t any visible hunting reply kuschku 47 minutes agorootparentYou can actually see focus changes quite often in movies and TV shows - but they&#x27;re usually done intentionally to accentuate something, e.g. a focus pull from a foreground object to an actor in the background.But it&#x27;s a slow and smooth motion without any focus breathing intended to highlight an object or an actor, not just autofocus hunting to find something reply abm53 2 hours agorootparentprevI’ve seen enough videos with otherwise high production values to make me suspect there is a valid trade-off to keeping autofocus on. reply astrange 1 hour agorootparentIt depends if you have a cameraman or not. If you don&#x27;t, and you&#x27;re walking away from the camera, it&#x27;s probably best to leave it on and hope it tracks you. reply 2143 41 minutes agorootparentprevTangent that kind of seems relevant here¹:Read the Foreword written by Gerald Sussman (SICP author) of the book The Little Schemer.The most beautiful Foreword I have ever read so far.¹(The Foreword talks about photography a little bit). reply AdamN 1 hour agorootparentprevThe TLDR; version of this is the progression from amateur to expert: 1&#x2F; controls are set wrong in the first place, 2&#x2F; computer changes controls during the shot but it&#x27;s distracting and obvious, 3&#x2F; controls are set right in the first place and everything looks good and consistent, 4&#x2F; expert modifies the controls mid-shot (and the shot requires this) and it looks awesome because everything is changing which allows the shooter&#x27;s expertise to shine through. reply pen2l 2 hours agoparentprevMore than anything it&#x27;s about color correction and color grading.This video explains it nicely I think: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pAh83khT1noIf you are starting out with good data (e.g. 32bit exr workflow), you would be amazed how powerfully and easily you can control what you want and what the possibilities are, with tools like MagicBullet (which offer presets to get you the cinema look with just a mouse click). But if you work long enough in this area you can discover your own workflow and pull it off without these tools, e.g. play with hue&sat, white balance adjustments, the curves, introducing an S-curve for example, color wheels, etc. reply Eduard 1 hour agorootparent> More than anything it&#x27;s about color correction and color grading.to my (literal) perception, using a framerate of 24 frames per second is an even more significant requirement to get the \"cinematic Hollywood look\". reply maven29 1 hour agorootparentIsn&#x27;t the 180 degree shutter angle more crucial than the distinction between 24 and 30? reply slhck 55 minutes agorootparentBoth. The 180 degree rule just makes sure motion blur looks as intended and is a mostly artistic choice that can vary depending on the scene. E.g. for action sequences or particularly smooth motion in a dreamy scene, you can break this rule. Or, if there&#x27;s moving water, you might want to choose a particular shutter in relation to the preset frame rate.The overall frame rate gives you the distinction between a typical movie vs a TV-style documentary. The overall frame rate stays fixed across a movie and should normally not be changed. reply varispeed 38 minutes agorootparentprevWhy 24 fps format is still being used? I personally can&#x27;t stand it. It&#x27;s like watching a slide show.I can&#x27;t wait when Hollywood moves to 120fps or better. reply Joeboy 24 minutes agorootparentEvery now and then somebody makes a high frame rate movie and everybody complains it looks shit, so they don&#x27;t do it again. reply kuschku 2 minutes agorootparentIMO a 48 fps movie at 1&#x2F;48 shutter speed looks just as dreamy as a 24fps movie at 1&#x2F;48 shutter speed, but is much less stuttery.goosinmouse 2 hours agoparentprevIn this case its software that treats the iphone as a camera of their own. Looking at the screenshots, the UI&#x2F;UX is extremely similar to current blackmagic cinema cameras. So you can have two camera operators, or the iphone on a tripod or whatever, and each camera operator will know which settings their camera has and to typically match both the film cameras. Like a quick visual check that both cameras are at the same shutter speed or shutter angle, resolution, white balance and tint, and having the same style of histogram so they can match exposure on both cameras.Its actually fairly neat and cool that they put time and money into this app to further their ecosystem. I guess theres a large overlap of people that film with iphones and also want to buy a legitimately good, budget cinema camera in the pocket 4k&#x2F;6k.I don&#x27;t know HN&#x27;s opinion of blackmagic, but they do some pretty cool stuff. With the purchase of a camera they include Davinci Resolve which is a fully featured Adobe Premier Pro rival. For reference premier pro is $21 a month, and the cheapest blackmagic cinema cam is the pocket 4k which comes in at $1200, after 5 years you have a free camera (thats still actively updated) if you consider Resolve to be equivalent to Premier Pro. Also they&#x27;ve constantly pushed the industry to be more affordable. They were pretty much the first that let you use a consumer usb c SSD to record raw formats. When the camera released, you could get 1tb samsung T5&#x27;s for around $100, while one of their rivals RED cameras made you purchase a proprietary SSD that still costs $1500 for 480GB. Also in terms of affordability, it wasn&#x27;t unheard of for a cinema camera to charge thousands of dollars to be able to use a cinemaDNG raw or ProRes, yet blackmagic cameras came with multiple raw recording options for free. reply KineticLensman 1 hour agorootparentI’ve been using the free version of Resolve for about a year now. It’s absolutely outstanding and well worth the steep learning curve (cos of the massive functionality). Don’t buy a Premiere Pro subscription until you’ve tried it out. Apart from its technical excellence there are zero dark patterns associated with free sign up and use. reply tern 24 minutes agoparentprev1. They are giving you all the tools needed to work in a professional way in a professional setting. This includes many things like being able to set all the camera settings manually, good metering to avoid clipping the sensor, audio metering to avoid clipping the recorder, timecode synchronization with other cameras & audio recorders, LUT preview, etc.2. The \"cinematic look\" comes from a combination of things:- good lighting (using professional lights in most situations)- 180 degree shutter angle (aka \"24fps\"), or slow motion where appropriate- careful and artistic color grading- taking time to set up the scene in advance & good framing- good lenses- good camera sensors (mainly, high dynamic range)- holding the camera still or moving it smoothly through the scene (except when deliberately not, as in for instance The Office)- music- and, more important than you&#x27;d think: very high quality audio (good mics, appropriately mic&#x27;d, low noise, dubbed in post if needed, SFX added)3. In short, what creates the \"cinematic look\" is many factors (and, usually, people) coming together as a system. This app lets your phone be part of that system.4. What makes this app unique: (1) it integrates directly with Davinci Resolve in a way that&#x27;s probably more convenient than Filmic Pro for that workflow and (2) it&#x27;s free.People have been making films and TV shows on iPhones for years, so this is more of an incremental event in the industry. reply ngrilly 1 hour agoparentprevI&#x27;m a non-film person as well, but I&#x27;ve been playing with this a bit. One key ingredient of the cinema look is the shutter speed. The iPhone standard camera app is constantly adjusting the shutter speed and the ISO depending on how much light the camera is getting.Movie cameras work differently with a shutter speed fixed at 24 fps, except for some scenes with specific requirements (for example slow motion). The light is controlled using the ISO, the aperture, the lighting, and ND filters.A nice trick people are using with smartphones to get the cinema look is to use an app like Blackmagic Camera, lock the shutter speed at 24 fps, and mount a variable ND filter on the smartphone to control how much light is received by the sensor, since we can&#x27;t control with the aperture. reply eurekin 1 hour agoparentprevNo attempt at real answer, but some hints from watching youtube videos on the topic:Lightning:- Edge or back lightning, if dramatic- Wraparaound (cradle) lighting, if for pleasantness- Low key look for interiors (no white walls)- Artificial light needs to be motivated as much as possibleSet design- Add bankers light for any money related film, normal table light for anything elseLens- Anamorphics to avoid perspective distortion typical to spherical lenses, also for the \"rich depth of field\" effect- Surprisingly the best lens technically don&#x27;t give the most \"pleasing\" (at least in \"hollywood\" terms) image. They are even called \"clinical\" or too sharp. A lot of DP&#x27;s like lens with a \"character\", altough some artifacts are regarded universally ugly (like the longitudinal chromatic aberration, which pukes green and cyan fringes around the image)Camera- High dynamic range camera, no clipping of highlights or blacks (add light, if necessary)- Must be able to retain true image details, any digital sharpening in the source footage immediately puts things offColor grading:- Good tone mapping: should look \"good\" in black and white, mostly solved with lighting- Pleasing color palette: color harmonies, gradients in good perceptual color space, like okmap. Mostly solved by set design, character and dress design- Even saturation: previous point should cover \"nice colors\", but the saturation is one of the most overlooked aspects. It can be highly or sparingly saturated, but too much variation in a single frame quickly makes for a garbage image. Also, one has to fight most software color manipulation tools, which tends to brighten up highly saturated parts, where in reality, they should go darkerThat&#x27;s a whole package of things, for a camera control specifically, typical operator or AC wants:- Manual focus pull- Way to judge \"exposure\", measured in IRE- Some way to approximate highlight to shadow exposure ratio; 2:1 for \"happy\" look, 4:1 for dark, 5:1 or more for Batman- Highlight clipping warning (especially important on talent&#x27;s skin)- Shutter angle control (typically 180 or 90 degrees), instead of the shutter time used in photography reply Neil44 1 hour agoparentprevImagine the difference between say a sit-com and a movie with the sound off. The movie will have range and intentionality to the scenes. Light, dark, vibrant, dull, perciptible and intentional changes from one to another to match the story. The sitcom is just clear and bright. The camera phone on auto is just going to aim for sitcom all the time wheras this app allows you to be intentional in order to look cool and tell a story. reply nikanj 1 hour agoparentprevTweak the color scale to be all blue&#x2F;orange reply fodkodrasz 2 hours agoprevWhy do project managers always insist adding chat to the app? I wonder if anybody uses the chat feature at all. Personally I find the integrated chat in every util useless waste of resources.Also a further fragmentation of communication platforms for any collaboration simply makes me not want to collaborate unless I&#x27;m paid handsomely to use the yetannotherchatplatform. (just finishing some project and getting rid of several chat platforms i was forced to use because of them) reply have_faith 1 hour agoparentThis one might make sense for production crew. Easy reference of specific clips&#x2F;shots, maybe lossless video clip transfers, things like that reply unfamiliar 2 hours agoprevI am consistently disappointed with the iPhone’s video quality. It looks like the bitrate is simply too low, either to keep file size down or because the phone can’t encode 4K that fast. However shooting in ProRES is simply not practical for me. Can apps like this improve the HEVC video quality, or do they simply reuse the Apple defaults with a new UI? Or is there a ProRES workflow I could be using on the phone? reply coldtea 1 hour agoparent>I am consistently disappointed with the iPhone’s video quality.Compared to what? An Arri? Bebause on its own, as far as smartphone video goes, it&#x27;s quite fine. You can also trivially shoot a higher bitrate (not ProRes) in Filmic Pro and other apps. reply diggan 1 hour agoparentprevMay I ask why ProRes isn&#x27;t practical for you? Would help others to suggest a workflow that would work better than what you tried before. reply Brajeshwar 3 hours agoprevCan someone please ask Blackmagic to have the app rotate and shoot landscape by default even if I have set my phone to never auto-rotate its orientation? I never needed to use my phone in Landscape except for shooting photos and videos. I can orient my phone landscape to shoot photos and videos but not with this App! reply petesivak 1 hour agoparentYou can go into Shortcuts and set up an Automation to automatically lock&#x2F;unlock orientation when you open&#x2F;close any app (including this one). reply andy_ppp 1 hour agorootparentOh that&#x27;s really nice, there are certain apps where this is very useful and I would prefer it but definitely it&#x27;s off for me globally. reply acarabott 2 hours agoparentprevThis doesn&#x27;t totally solve the issue, but if you unlock your phone&#x27;s orientation, rotate to landscape, then go to settings, you&#x27;ll find a \"lock current orientation\" setting. reply omneity 58 minutes agoprevI&#x27;m super excited at timecode support (for automatic timeline alignment) and hopefully gyro data for stabilization in Davinci Resolve! reply paweladamczuk 2 hours agoprevI recorded two takes with exact same settings except one was ISO ~1500 and the other ISO ~3000. Shouldn&#x27;t the second take be around twice as bright as the first one? The change in brightness is hardly noticeable.I suppose this is the same case as every other camera app I&#x27;ve ever tested on Android and iOS, such granular settings like ISO are just not accessible by the system APIs available to the app. In that case though, I&#x27;d expect the app to at least not lie to me.Can someone confirm or deny this? I don&#x27;t know much about photography nor iOS so I might just be confused. reply fragmede 2 hours agoparentYou can see the brightness change when you fiddle with the ISO, so I&#x27;m not sure where you&#x27;d draw the conclusion that the system APIs don&#x27;t give apps access to that.Moving from ISO 1500 to ISO 3000 doubles the sensor&#x27;s sensitivity to light, but doesn’t inherently make the scene appear twice as bright. reply inductive_magic 2 hours agoparentprevTo add to the other commenters: when you tweak the sensors sensitivity for light, the aperture will compensate by letting less light in – unless it is fixed. So there should be no noticeable difference in brightness unless you set a fixed aperture. reply foldr 27 minutes agorootparentThis is a smartphone camera, so of course the aperture is fixed. Playing around with the app, it does not seem to automatically adjust shutter speed to match changes in ISO, and indeed, reducing the ISO does make the image darker – presumably just not as much as OP expected. reply foldr 2 hours agoparentprevISO is linear scale and perception is log scale. Doubling the ISO (while keeping the shutter speed the same) increases the exposure by only one stop, which won’t seem twice as bright. reply coldtea 1 hour agorootparentIsn&#x27;t \"one stop\" defined log-wise to be \"twice as bright\" perceptually? reply foldr 56 minutes agorootparentNo, it&#x27;s twice as bright physically (in the sense that the lux value doubles). For example, doubling the shutter speed increases the exposure by one stop. Similarly, increasing the diameter of the aperture by a factor of √2 (thus doubling the area of the aperture and letting twice as much light in) increases the exposure by one stop.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Weber%E2%80%93Fechner_law#:~:t.... reply eurekin 1 hour agoparentprevProbably auto-exposure adjusted the shutter speed to compensate reply Ballas 2 hours agoparentprevIf all else is kept the same. Usually with auto exposure, it will compensate by changing the integration time (\"shutter speed\") or aperture in order to try and keep the exposure to the same level. reply jemmyw 2 hours agoparentprevISO is supposed to be the sensitivity of the film to light, and the numbers were set by a standards organisation so I&#x27;m not sure you can double the number to double the effect.Also, how does this even work on a digital camera? Surely we can&#x27;t actually adjust how sensitive the sensor is to light, so is it just a simulation? reply frostburg 2 hours agorootparentIt&#x27;s complex. Many modern cameras have dual or triple gain amplifiers, with various range setups. Some ISO settings might be just a digital multiplier on the highest setting of the lower gain stage before switching to the higher gain (which may result in having better snr at higher iso for some settings).Remember that \"digital\" sensors are mostly analog devices (dealing in continuous voltages). reply thih9 2 hours agorootparentprev> Also, how does this even work on a digital camera?It’s signal gain of the sensor.“In digital camera systems, an arbitrary relationship between exposure and sensor data values can be achieved by setting the signal gain of the sensor. (…) For digital photo cameras (\"digital still cameras\"), an exposure index (EI) rating—commonly called ISO setting—is specified by the manufacturer such that the sRGB image files produced by the camera will have a lightness similar to what would be obtained with film of the same EI rating at the same exposure.”Source: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Film_speed#Digital_camera_IS... reply coldtea 1 hour agorootparentprevKind of the same it works on film cameras: you change the sensitivity of the emulsion on them, you change the sensitivity of the sensor by raising the gain on the others.Or think of it like changing the gain on an microphone pre-amp before going into a Analog-to-Digital convertor. reply CorrectHorseBat 2 hours agorootparentprevWe can configure the gain of the analog amplifiers reply justsomehnguy 2 hours agoparentprev> such granular settings like ISO are just not accessible by the system APIs available to the app...what? Every non dumbified enough app has ISO and shutter controls in the manual mode since forever.Eg:https:&#x2F;&#x2F;f-droid.org&#x2F;en&#x2F;packages&#x2F;net.sourceforge.opencamera&#x2F; reply kuschku 51 minutes agorootparentRemember, this is iOS. A system where apps can&#x27;t even record continuous framerate footage. reply zx10rse 54 minutes agoprevInsta download. Kudos to the developers who made it to not collect any data from the app, you deserve a raise. reply 1-6 3 hours agoprevGlad to have a separate camera app for ‘pro’ settings so I can continue to use my regular Camera app for optimal settings when not retouched. reply buro9 3 hours agoprevThis is great.I&#x27;m a native Firefox with extensions away from moving to iPhone after never having one, so every further bit of support by third parties is good.I really hope the EU forcing the opening up of Apple brings Firefox reply dewey 3 hours agoparentHow is one additional camera app related to that? reply buro9 2 hours agorootparentEvery additional third party improvement I see shows me how other companies are still investing in the iPhone. It&#x27;s been a few years now since Google seems to be on a constant decline, and the latest Pixel phones only seem to go further on that path.It&#x27;s the contrast between an ecosystem on a decline, and one still being invested in by others.Not that Apple are perfect, the single biggest blocker to my cohort of friends all replacing their Android with iOS within a few weeks remains the lack of a native third party browser with adblocking extensions. reply abhinai 3 hours agoprevCan someone please ELI5 why this link deserves to be the top on hacker news? reply soultrees 3 hours agoparentI think it’s because BM has a cult following and part of that is because BMs software is notoriously high quality and they seem to be doing things the right way bg avoiding subscriptions and putting real resources into engineering effort for their cameras but also the software.I, for one, was excited to see a BM product available on my iPhone now so I can see why others are just as excited. Google has made the front page for less noteworthy apps before I’m sure. reply dgellow 3 hours agoparentprevPeople vote for it. There is no such thing as “deserve to be at the top”. If the HN crowd show interests and upvote, then it rises. reply dataengineer56 2 hours agorootparentThat&#x27;s only true if you believe that every vote on HN is honest. reply gls2ro 3 hours agoparentprevMy perspective:Blackmagic has great, mostly professional hardware (cameras and more) with high-quality, stable software again focused on pro-market. They also have amateur or entry-level stuff, but even those have excellent quality.So Blackmagic deciding to create an app for iPhone might say they consider the iPhone camera good enough for them. And this is a message worth considering. reply fragmede 3 hours agoparentprev> What to Submit> On-Topic: Anything that good hackers would find interesting. That includes more than hacking and startups. If you had to reduce it to a sentence, the answer might be: anything that gratifies one&#x27;s intellectual curiosity.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html reply moondev 2 hours agoparentprevhttps:&#x2F;&#x2F;medium.com&#x2F;hacking-and-gonzo&#x2F;how-hacker-news-ranking... reply screamingninja 3 hours agoparentprevOur discussion is certainly contributing to keeping it there longer than necessary reply langarus 3 hours agoparentprevhackernews is about what&#x27;s important to the one posting reply lopis 3 hours agoparentprevI&#x27;m surprised too. Isn&#x27;t this essentially an ad? I guess it&#x27;s OK because it&#x27;s a free app. reply franga2000 3 hours agorootparentSo many things on HN could be considered ads. People often post about even non-major releases of SaaS products. The launch of a brand new powerful app and the entry of a big company into a new market seems a lot more newsworthy than most of those. reply k8sToGo 3 hours agorootparentprevIsn’t everything an ad? Even when someone posts a GitHub link to their project? reply Levitating 3 hours agorootparentYes but they&#x27;re not trying to sell their open source project to us reply matsemann 2 hours agorootparentShould we ban new iPhone releases from being discussed here? Other tech released as well? reply qup 2 hours agorootparentprevThe app is free replysecretsatan 2 hours agoprevIs this just... does it just add a filter to give it film grain? reply coldtea 1 hour agoparentNo, that would be some app like \"Super 8\" and others.This one adds pro-like controls (with similar layout and features to expensive professional Blackmagic cameras), and control of the more professional features like log recording, various prores options, 10bit color, LUT preview, and so on. reply tedunangst 1 hour agoparentprevNo. reply usrusr 1 hour agoparentprevBlackmagic isn&#x27;t exactly a company that identifies with film grain... reply brylie 3 hours agoprevPlease remember to put the download link at the top of the article or promotional material for quick access. I had to scroll down to the bottom of this 200 screen article and wasn’t sure I’d even find a download button when I got to the bottom. reply fragmede 1 hour agoparentI didn&#x27;t even find it and resorted to searching the in App store for it. reply danwee 1 hour agoprevIs it only me the one who finds the usability of cameras on smartphones unbearable? I&#x27;m not talking about the software but the hardware: smartphones are thin and not as easy to grab&#x2F;handle as real cameras. Anything that takes more than a few pictures is uncomfortable. reply 20after4 1 hour agoparentThere are specialized gimbals made specifically for phones which solve this really nicely. One example that I&#x27;ve used and found it to be really impressive is the DJI Osmo. reply raincole 3 hours agoprevA bit off-topic: why does Blackmagic keep Davinci Resolve free? Is it a case of commoditizing your compliment because they sell hardware? reply dannyw 3 hours agoparentThere&#x27;s a solid opportunity for a good free video editor for hobbyists and creators, who might not want to spend any money just yet. If you get these users on your software, you&#x27;re much more likely to convert a sale.The free version of Resolve is still limited for more serious &#x2F; professional applications; e.g. not making use of hardware acceleration, not having certain effects, and not processing certain professional workflow codecs.With this strategy, Resolve is essentially the \"go-to\" for newbies into video editing.What&#x27;s so great about Resolve licensing is their lifetime license. Pay $295 once, get it forever. For commercial productions, Blackmagic gets their $$$$$ from their cameras, physical control panels and hardware, etc. These, again range from good value (for entry and mid level) to expensive. reply greenknight 3 hours agorootparentSo I work daily in resolve. I had a project come up at home, where i was like oh ill just use resolve as it has everything we need.Within 2 minutes, I was running into features i needed to pay for. Within 20 minutes I had bought a home copy because of how integral it is to my workflow.They get people in by being able to do the basics, but anything remotely complex, you pay for. reply diggan 1 hour agorootparentI think a lot of users acquire the Studio license similar to myself as well, being a free user for a long time and when I finally went looking for my own shooting device, BlackMagic Pocket Cinema was a no brainer and includes a Studio license. reply dmbche 1 hour agorootparentprevWhat feature? Isn&#x27;t it just gpu acceleration and 4k you get with studio? reply _nalply 3 hours agorootparentprevOne word: Freemium. reply kwonkicker 3 hours agorootparentprevBlender and Unreal Engine comes to mind. Similar formula. reply jcparkyn 2 hours agorootparentI wouldn&#x27;t describe blender as following this formula. The only thing they sell (last time I checked) is blender studio, which as far as I&#x27;m aware is more just another way to donate while getting some things in return. reply ralusek 2 hours agorootparentprevBlender is open source and free, Unreal is paid on the backend. reply sneak 3 hours agoparentprevBlackmagic is primarily a hardware company, yes, but the free version of Resolve doesn’t cut it for professional work. As soon as you start doing anything serious, you need Studio. It is, however, only $299 one time and works for all versions including upgrades. reply fragmede 3 hours agoparentprevThey make their money on very nice, very expensive professional control surfaces, cameras, and lots of other gear. Davinci Resolve is free because they realize funneling people into their ecosystem, having them learn how to use Resolve, instead of Avid&#x2F;FCP&#x2F;Lightworks&#x2F;Premier&#x2F;etc, which means that&#x27;s what they&#x27;re going to go with when they have the money to spend on gear. It&#x27;ll just work better together. Like if you buy all Apple products instead of random company&#x27;s stuff. reply stephen_g 3 hours agoparentprevYes, while many serious users will need to go up to the paid version of Resolve, mainly they want to sell you cameras, capture and playback hardware, vision switchers (like the ISO ones that record the individual streams and then create a Resolve project for you to re-edit it if you want). reply Brajeshwar 3 hours agoparentprevMy friend, please stop giving them ideas. Anyways, I&#x27;m not a professional and Davinci Resolve (free) is the good enough editor for me. I think this is macOS, iOS is free but Apple sells hardware kinda free. reply madaxe_again 3 hours agoprevI don’t get it.Surely if you want to shoot professional footage, you use a professional camera, with an actual lens, with a full frame sensor - not something which has been scaled down to the extent that it relies on digital hallucinations to make an acceptable looking image. reply adlpz 3 hours agoparentI don&#x27;t get it.Surely if you want to watch movies, you go to the cinema, with an actual projector, with a full frame film - not something which has been compressed down to the extent that it relies on digital trickery to make an acceptable looking image.^ that reply usrusr 53 minutes agoparentprevOne thing is brand onboarding: film students don&#x27;t rent the big guns for everything and if they can do a quick \"shot on phone\" but with a blackmagic workflow, it will be hugely attractive for them. Think of it as cosplay if you like, but it can have powerful long term effect for the brand if new generations start out on their tools. Or in terms of computers, ca year 2000, think about it as leaning C++ instead of getting good at Excel VBA.The other thing is consistency: if you do use the big ones, but have a shot where the big camera simply cannot be used, allowing an iPhone to stand in with processing defaults defaults set up to make the footage as consistent with the regular takes as possible can be worth a lot. Sony has created the RX0 line essentially for that, as a support gadget for their film cameras. Sales to consumers are merely opportunistic side income.Same (super convenient to easily get consistent setting) for takes that are what in software development would be considered a bugfix, takes that happen when primary filming is over. All the rental equipment has been returned, the order of the day is getting least bad results with what you have. If blackmagic has tools for this contingency and competitors don&#x27;t, blackmagic will be more attractive. reply baq 2 hours agoparentprevTurns out the recent iPhone camera hardware (especially in the Pro models) is simply good enough to shoot &#x27;professional footage&#x27;. Not everywhere, not all the time and especially not in all lighting conditions - but it is good enough more often than not. reply fragmede 1 hour agoparentprevIf you have a real professional bank account, you own (not rent) all the $100,000 camera systems you could possibly want (5-10?), but for everyone else, money is fairly limited. To be able to take advantage of the video cameras you do have to get extra camera angles for a scene, configured with the proper settings - ISO&#x2F;shutter&#x2F;white balance, is easily worth the $50 tripod and using your phone. reply sacnoradhq 1 hour agorootparent100k is a cheap pro camera such as a 50k will get you a Red 8k package but minus the frame and other gear to go with it. A Sony Venice 2 8K is 60k just for the main body. Lenses aren&#x27;t cheap either.Gyroscopic stabilization for iPhone is cheap and doable: DJI Osmo isfull frame sensorwhy? I mean, with film, anamorphic Super35 is a thing, but so is Super16. With digital, why would all the footage taken on professional MFT cameras be \"not professional\"? reply steve1977 21 minutes agorootparentSuper 35 &#x2F; APS-C is still quite a bit larger than an iPhone camera sensor though. reply foldr 2 hours agoparentprev> with an actual lensThis kind of hyperbole isn’t exactly helping your argument. Smartphone cameras have very sophisticated lenses. reply qup 2 hours agoparentprevUse what you have. reply MrThoughtful 3 hours agoprev [–] If I understand it correctly, this is postprocessing software?Why is it called \"camera\" and tied to the iPhone?Shouldn&#x27;t this be a website where you upload a photo, adjust it, and then download it again? reply altacc 1 hour agoparentThere seem to be 2 types of camera apps for the iPhone: those aimed at adding filters and post-processing; and those removing all processing & automatic controls that the stock camera app wants to add (by default the iPhone software will want to apply lots of corrections, \"improvements\" and change settings on the fly that can make a video or photo look terrible).This is the latter, handing back control of the camera settings to the user, which is what you want for consistency of the look of a video whilst subjects move, the camera pans, etc... reply Closi 3 hours agoparentprevIn that case, you don’t understand it. This isn’t preprocessing software.It’s mostly aimed at getting the right shot in the first place, and giving skilled users enough control to make better shots.So that’s why it’s called camera. reply sunbum 3 hours agoparentprevBecause it is not postprocessing software, it&#x27;s a camera app reply loxdalen 3 hours agoparentprevAlso seems to allow setting manual settings during filming. Not sure if that is possible in the native camera app on iPhone. reply jawngee 3 hours agoparentprev [–] No, it&#x27;s a full on camera app. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Blackmagic Camera app for iPhone provides professional-grade camera controls and image processing capabilities, enabling users to produce high-quality cinematic content.",
      "Users of the app can easily alter settings, record to Blackmagic Cloud for collaborative work, and utilize special features like focus assisting and image stabilization.",
      "The app is integrated with DaVinci Resolve for editing and grading color, offering advanced control and editing functionalities to iPhone users."
    ],
    "commentSummary": [
      "Blackmagic has launched a free camera app for iPhones that provides professional-quality, manual control features, valuable for film students.",
      "The app permits manual adjustments of settings like exposure and frame rate for a cinematic effect, also integrating with Davinci Resolve, leading to positive user reviews.",
      "The dialogues are currently centred around comparison between the capabilities of professional cameras and smartphones, emphasizing the convenience and economical benefits of smartphones for filming."
    ],
    "points": 221,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1696829257
  },
  {
    "id": 37812142,
    "title": "The Tailscale Universal Docker Mod",
    "originLink": "https://tailscale.dev/blog/docker-mod-tailscale",
    "originBody": "~$community Events Blog Docs↗ Changelog↗ Download↗ Login Introducing the Tailscale Universal Docker Mod Xe Iaso (they/them)TailscalaronApril 14, 2023 Imagine a world where you could add applications to your tailnet the same way you add machines to it. This would mean that http://wiki would go to your internal wiki, http://code would take you to an IDE, and http://chat would take you to your internal chat server. This is the world that Tailscale lets you create, but historically the details on how you would actually do this are left as an exercise for the reader. Today, we're introducing a new way to add Tailscale to your Docker containers: our brand new universal Docker mod. This lets you add Tailscale to any Docker container based on linuxserver.io images. This lets you have applications join your tailnet just as easily as machines can. You can set up a wiki on http://wiki, an IDE at http://code, and a chat server at http://chat and have them all be accessible over your tailnet. You can even use this to expose your internal applications to the public internet with Funnel.You can even use this to SSH into containers!You can what into a container?Yep! Tailscale SSH lets you SSH into containers when you enable the TAILSCALE_USE_SSH setting and permit access in the ACLs. This is a great way to get into a container without having to SSH into the docker host and run docker exec -itbash. To add this to your existing Docker containers with linuxserver.io images, add the following environment variables to your docker-compose.yml file: - DOCKER_MODS=ghcr.io/tailscale-dev/docker-mod:main # tailscale configuration # make sure this is persisted in a volume - TAILSCALE_STATE_DIR=/var/lib/tailscale - TAILSCALE_SERVE_MODE=https - TAILSCALE_SERVE_PORT=80 - TAILSCALE_USE_SSH=1 - TAILSCALE_HOSTNAME=wiki ## uncomment to enable funnel ## remember that if you do, it's exposed to the internet, so be careful! #- TAILSCALE_FUNNEL=on # replace this with your authkey from the admin panel - TAILSCALE_AUTHKEY=tskey-auth-hunter2CNTRL-hunter2hunter2 This will add Tailscale to your container so that you can access it over your tailnet. If you run docker compose up -d with the authkey changed out for a valid authkey, you'll be able to access your apps over Tailscale. Image generated by Ligne Claire v1.5, prompt: flat color, shipyard, containers, mountains, no humans, space needle Docker and Docker mods Docker allows you to create snapshots of operating system installs with a given state, such as \"having the Go compiler available\" or \"install this program and all its dependencies\" and distribute those preconfigured images on the Internet. When you consume the same Docker image at two time intervals T0 and T1, you get the same image with the same code, just as you expect. When a Docker container is run, it usually runs on top of an ephemeral filesystem that gets destroyed when the container is stopped. This means that restarting the container will reset it back to the state that was there when the image was created. This is normally convenient when working on applications that make temporary changes to the filesystem, such as an image converter that uses temporary files to do the conversion logic. This is less convenient when you want to run things like database servers in Docker. However, most of the time when you do things that need persistent state, that persistent state is usually limited to a single file or directory. Docker provides external persistent state with volumes. They're basically directories that are plunked into the container at runtime, but it maintains the state between container runs. This is great for things like databases because you wouldn't want to lose all your data when you restart the container. So, from here we can create a hierarchy for docker and statefulness. You expect docker containers to have state for data, and you also expect the docker container to be running the same code every time you run the same image. You don't expect anything else to be running, everything is deterministic at T0, T1, or TN.This is a valid hierarchy because it's what you expect from docker. You expect the same code to run every time you run the same image. What is humor? Humor is a complicated concept that is almost universal throughout human cultures. It's a way of conveying concepts like absurdity, irony, the absurdity of irony, and normally frustrating things in ways that aren't quite as much of a downer. It's really about being able to communicate subtle things like common errors that everyone makes when learning things (such as English and its rule of all the rules having exceptions, even for the exceptions). It's also a tool that you can use to help describe the abstract and nonphysical things like emotions, feelings, ideas, the human condition, and how Kubernetes works.Humor is also really hard to convey properly in a written medium. This is even more difficult when the humor is about technology, which is usually hard to understand in the first place. I'm going to try to explain the humor in this article with these asides so that y'all can follow along, but if you already get why this is funny it may ruin the joke for you. Sorry! In his famous presentation Reverse emulating the NES, fellow philosopher in arms tom7 introduced the idea of a type of humor called \"invalid hierarchies\". In this he does rather abusrd things to an NES using a custom circuit board and a raspbery pi to allow him to (among other things) run an SNES emulator on the NES. This video is quite possibly one of my favorite technical communication videos and is a huge influence to how I write humorous things for this blog.This creates an invalid hierarchy because you expect the NES to only run 8-bit NES games, but not 16-bit SNES games. This is funny. If you've never seen that video before, it's well worth a watch. Another example of an invalid hierarchy is my April Fool's Day post Using Tailscale without using Tailscale. You'd expect to have to use Tailscale if you want to use Tailscale, but \"using Tailscale without using Tailscale\" creates an invalid hierarchy in the mind of the reader. This is also funny. Docker mods Docker mods let you install extra packages and services into containers at runtime. If the ONBUILD hook lets you run a series of commands when an image is built, you can think of docker mods as a missing ONRUN hook that lets you customize an image at runtime.This creates an invalid hierarchy because we think about the code in a container being deterministic between invocations and this allows you to make something nondeterministic. This is funny. Docker mods and s6 At a high level, a docker mod is a series of files that add additional instructions to the start phase of a docker container. It works because the linuxserver.io containers preinstall s6 via s6-overlay and then start it in the background to manage the lifecycle of services in the container.This is also funny because usually Docker containers aren't supposed to have multiple processes running in them for simplicity, but it turns out that when you want to do things like put your wiki seamlessly on your tailnet, you want to have multiple processes running. This is another invalid hierarchy because you expect the container to only have one process running, but it has multiple with a service manager, just like the host OS. When I made the docker mod, I had to create a few s6 services to help it run: One to set a list of packages that Tailscale needs to run (jq to process some data from the packages server, and iptables to configure the firewall inside the container for Tailscale to run in a TUN device). One to download Tailscale to the container. One to start the Tailscale node agent tailscaled. One to authenticate you to the tailnet with tailscale up and set other settings like tailscale serve.This is also hilarious because this roughly mirrors the process that you have to do on your host OS to get Tailscale running. This is another layer of invalid hierarchy because you expect containers to ship with all the software they need, but here is this container that needs to download software at runtime. This is funny because it's like a container that needs to download software at runtime, just like your host OS. As above, so below, eh? Each of these is connected together like this (arrows indicate dependencies):If you've ever worked deeply with the Heroku ecosystem, you can think about Docker mods as akin to all of the hilarous hacks you can do with buildpacks at dyno boot time. Configuration The Docker mod exposes a bunch of environment variables that you can use to configure it. You can see the full list of environment variables in the documentation, but here are the important ones: Environment Variable Description Example DOCKER_MODS The list of additional mods to layer on top of the running container, separated by pipes. ghcr.io/tailscale-dev/docker-mod:main TAILSCALE_STATE_DIR The directory where the Tailscale state will be stored, this should be pointed to a Docker volume. If it is not, then the node will set itself as ephemeral, making the node disappear from your tailnet when the container exits. /var/lib/tailscale TAILSCALE_AUTHKEY The authkey for your tailnet. You can create one in the admin panel. See here for more information about authkeys and what you can do with them. tskey-auth-hunter2CNTRL-hunter2hunter2 TAILSCALE_HOSTNAME The hostname that you want to set for the container. If you don't set this, the hostname of the node on your tailnet will be a bunch of random hexadecimal numbers, which many humans find hard to remember. wiki TAILSCALE_USE_SSH Set this to 1 to enable SSH access to the container. 1 TAILSCALE_SERVE_PORT The port number that you want to expose on your tailnet. This will be the port of your DokuWiki, Transmission, or other container. 80 TAILSCALE_SERVE_MODE The mode you want to run Tailscale serving in. This should be https in most cases, but there may be times when you need to enable tls-terminated-tcp to deal with some weird edge cases like HTTP long-poll connections. See here for more information. https TAILSCALE_FUNNEL Set this to true, 1, or t to enable funnel. For more information about the accepted syntax, please read the strconv.ParseBool documentation in the Go standard library. on Something important to keep in mind is that you really should set up a separate volume for Tailscale state. Here is how to do that with the docker commandline: docker volume create dokuwiki-tailscale Then you can mount it into a container by using the volume name instead of a host path: docker run \\ ... \\ -v dokuwiki-tailscale:/var/lib/tailscale \\ ... If you want to use kernel networking mode, you will need to add the NET_ADMIN and NET_RAW capabilities to the container, as well as pass the /dev/net/tun device into the container. Here is an example of how to do that with the docker commandline: docker run \\ ... \\ --cap-add=NET_ADMIN \\ --cap-add=NET_RAW \\ --device=/dev/net/tun \\ ... In a compose.yaml file, it will look like this: version: '2.1' services: dokuwiki: image: lscr.io/linuxserver/dokuwiki:latest volumes: - /dev/net/tun:/dev/net/tun cap_add: - NET_ADMIN - NET_RAW # ... This can be useful when you are running applications on your tailnet without tailscale serve, and you want the underlying service to know the exact remote IP address (such as when running a Minecraft server). Fun things you can do Normally when I write these articles, I tend to give you one functional example so that you can fill in the blanks here. This time, I want to give you a few functional and genuninely useful examples so that you can get started with our Docker mod right away. If you want to test this with a simple command-line shell, you can run this docker command to create a volume for Tailscale state, and then run a container with the Docker mod installed: docker volume create trap-sun-statetrap-sun is the name of the container that we will be running. You can name it whatever you want, but you should use the same name in both your volume and your container. I'm setting the name here in case you get stuck and need to arbitrarily kill the container with docker kill trap-sun. docker run \\ --rm \\ -v trap-sun-state:/var/lib/tailscale \\ -e TAILSCALE_STATE_DIR=/var/lib/tailscale \\ -e TAILSCALE_SERVE_PORT=3000 \\ -e TAILSCALE_SERVE_MODE=https \\ -e TAILSCALE_FUNNEL=on \\ -e TAILSCALE_USE_SSH=1 \\ -e TAILSCALE_HOSTNAME=trap-sun \\ -e TAILSCALE_AUTHKEY=tskey-auth-hunter2CNTRL-hunter2hunter2 \\ -e DOCKER_MODS=ghcr.io/tailscale-dev/docker-mod:main \\ --name trap-sun \\ -it \\ --cap-add=NET_ADMIN \\ --cap-add=NET_RAW \\ -v /dev/net/tun:/dev/net/tun \\ lsiobase/alpine:3.17 \\ shYou can also base your Docker images on the lscr.io/linuxserver/baseimage-alpine:3.17 image, which is a minimal Alpine Linux with Docker mod support. This can be used to adapt your existing containers into nodes on your tailnet. You can also use Ubuntu with lscr.io/linuxserver/baseimage-ubuntu:jammy as the base image. The cloud's the limit! DokuWiki If you want to set up a wiki for your tailnet with DokuWiki, you can use this Docker compose file: # docker-compose.yaml version: '2.1' services: dokuwiki: image: lscr.io/linuxserver/dokuwiki:latest container_name: dokuwiki environment: - PUID=1000 - PGID=1000 - TZ=Etc/UTC - DOCKER_MODS=ghcr.io/tailscale-dev/docker-mod:main # tailscale information - TAILSCALE_STATE_DIR=/var/lib/tailscale - TAILSCALE_SERVE_PORT=80 - TAILSCALE_SERVE_MODE=https ## uncomment to enable funnel, may be a bad idea for some use cases #- TAILSCALE_FUNNEL=on - TAILSCALE_USE_SSH=1 - TAILSCALE_HOSTNAME=wiki - TAILSCALE_AUTHKEY=tskey-auth-hunter2CNTRL-hunter2hunter2 volumes: - dokuwiki-data:/config - dokuwiki-tailscale:/var/lib/tailscale restart: unless-stopped volumes: dokuwiki-data: dokuwiki-tailscale: Then use docker compose up -d to start the DokuWiki container with Tailscale grafted in. You can then access your DokuWiki instance at https://wiki.yourtailnet.ts.net. You will want to do the setup wizard, and then you can start using your own private wiki! Your private cloud development environment with code-server Want to have all the fun of GitHub Codespaces without having to use GitHub's servers for development? Set up your own private cloud with code-server and Tailscale! version: '2.1' services: code-server: image: lscr.io/linuxserver/code-server:latest container_name: code-server environment: - PUID=1000 - PGID=1000 - TZ=Etc/UTC - PASSWORD=hunter2 - PROXY_DOMAIN=code.shark-harmonic.ts.net - DOCKER_MODS=ghcr.io/tailscale-dev/docker-mod:main|ghcr.io/linuxserver/mods:code-server-nodejs|ghcr.io/linuxserver/mods:code-server-npmglobal # tailscale information - TAILSCALE_STATE_DIR=/var/lib/tailscale - TAILSCALE_SERVE_PORT=8443 - TAILSCALE_SERVE_MODE=tls-terminated-tcp - TAILSCALE_USE_SSH=1 - TAILSCALE_HOSTNAME=code - TAILSCALE_AUTHKEY=tskey-auth-hunter2CNTRL-hunter2hunter2 volumes: - code-server-data:/config - code-server-tailscale:/var/lib/tailscale restart: unless-stopped volumes: code-server-data: code-server-tailscale: Then use docker compose up -d to start the code-server container with Tailscale grafted in. You can then access your code-server instance at https://code.shark-harmonic.ts.net. You may want to change the password from hunter2 to something more secure. code-server also has support for cloning repositories from GitHub directly, so with this you can get started hacking on a project on one machine, then seamlessly pick up where you left off on another! You can start hacking at something in your office and then walk over to the local Tim Horton's to finish it up! There's a bunch of other containers in the linuxserver.io fleet, you can use Tailscale with those as well. You can also check out Awesome-LSIO for more ideas! At Tailscale, we want to recreate the Internet around the idea of small, trusted networks with your friends, family, and coworkers. When you set up applications on your tailnet like this, you can slowly start to use your own private infrastructure instead of relying on the public Internet. This is a great way to start using Tailscale, and we hope that you will find this Docker mod useful. If you have any questions, feel free to reach out to us on Twitter or the Fediverse. We are always happy to help! TAGS docker philosophy PREVIOUS ARTICLE Improving Tailscale via Apple’s open source NEXT ARTICLE Creating a multi-user tailnet with GitHub organizations ← Back to the blog RSS Share Tweet LinkedIn Hachyderm Improve this page The official community site of Tailscale. WireGuard is a registered trademark of Jason A. Donenfeld. © Tailscale Inc.",
    "commentLink": "https://news.ycombinator.com/item?id=37812142",
    "commentBody": "The Tailscale Universal Docker ModHacker NewspastloginThe Tailscale Universal Docker Mod (tailscale.dev) 217 points by notamy 17 hours ago| hidepastfavorite46 comments MrPowerGamerBR 13 hours agoFor those who want to run Tailscale on their Docker containers, but don&#x27;t want to switch to images based off linuxserver.io, you can still run Tailscale as a sidecar container, and use \"network_mode: service:tailscale\"I do that for my containers and it is incredibly useful for cross containers communication, especially for containers that are hosted in different dedicated servers.https:&#x2F;&#x2F;mrpowergamerbr.com&#x2F;us&#x2F;blog&#x2F;2023-03-20-untangling-you... reply whs 1 hour agoparentI run my game servers using `network_mode: service:tailscale` and every time the game server needs to restart (or crash) Tailscale will permanently lose connectivity and needs to be recreated (restart doesn&#x27;t work).To solve this problem I add another container which should never need to be restarted, and both the game and Tailscale use the networking of that container. This is also the exact use case of Kubernetes&#x27; pause containers, so I just use the EKS pause image from ECR public gallery.Another tip I&#x27;d recommend is to run the Tailscale container with `TS_USERSPACE: &#x27;false&#x27;` `TS_DEBUG_FIREWALL_MODE: nftables` (since autodetection fails on my machine) and give it `CAP_NET_ADMIN`. This allow Tailscale to use tun device instead of emulation, and it supposed to be more performant. But the clear benefit is that the game server will see everyone&#x27;s Tailnet IP instead of 127.0.0.1.In Thai: https:&#x2F;&#x2F;blog.whs.in.th&#x2F;node&#x2F;3676 reply rcarmo 12 hours agoparentprevThis seems a lot cleaner than injecting new binaries into existing images or depending on linuxserver.io images. reply tylergetsay 9 hours agorootparentThe \"benefit\" of one tailscale daemon per container is https&#x2F;external access&#x2F;etc can be handled automatically reply iansinnott 8 hours agoparentprevDoes linuxserver.io have a bad reputation? or is it just that it&#x27;s yet another dependency in the stack?Asking because I&#x27;ve been happy with their containers so far reply xena 7 hours agorootparentMany useful images are based on linuxserver.io containers, but most docker images are not based on linuxserver.io. reply crypt1d 14 hours agoprevI am really impressed by what the tailscale folks have been building. I use their product suite regularly and have nothing but good things to say about it. I will be tinkering with this mod as well starting next week ;)keep it up guys! reply upon_drumhead 15 hours agoprevWhile this is super cool, it&#x27;s not Universal. It requires usage of containers based upon LinuxServer.io containers. reply wutwutwat 14 hours agoparentHow does the base image make them not universal? reply upon_drumhead 13 hours agorootparentNot every container is based on the LinuxServer.io stack. I can&#x27;t take any arbitrary container and use the docker mod and have it work.I have over 25 containers running on my home server and not a single one of them is based on a LinuxServer.io image. This \"universal\" mod would work with 0 of them. reply bigmattystyles 13 hours agorootparentWould you list them? I’m always looking for cool new containers for my homelab reply upon_drumhead 13 hours agorootparentWireguard + GUI: https:&#x2F;&#x2F;github.com&#x2F;wg-easy&#x2F;wg-easyManaging all those household docs: https:&#x2F;&#x2F;docs.paperless-ngx.comBackups of mail accounts: https:&#x2F;&#x2F;www.offlineimap.orgCloud storage for phones: http:&#x2F;&#x2F;nextcloud.comMirroring podcasts locally: https:&#x2F;&#x2F;github.com&#x2F;akhilrex&#x2F;podgrabManaging dynamic service dns via plugins: https:&#x2F;&#x2F;coredns.ioMy own matrix instance: https:&#x2F;&#x2F;matrix-org.github.io&#x2F;dendrite&#x2F;Backups: https:&#x2F;&#x2F;restic.netMedia Management: https:&#x2F;&#x2F;jellyfin.orgRelay only tor help: https:&#x2F;&#x2F;www.torproject.orgS3 compatible storage: https:&#x2F;&#x2F;github.com&#x2F;seaweedfs&#x2F;seaweedfsGit + CI: https:&#x2F;&#x2F;about.gitlab.comManaging SSL and container proxying: https:&#x2F;&#x2F;traefik.ioMirror the docker registry locally: https:&#x2F;&#x2F;github.com&#x2F;docker-library&#x2F;docs&#x2F;tree&#x2F;master&#x2F;registrySamba support for the windows hosts: https:&#x2F;&#x2F;github.com&#x2F;ServerContainers&#x2F;sambaHTTP&#x2F;S Proxy with support for modifying results: http:&#x2F;&#x2F;www.privoxy.orgDatabase: https:&#x2F;&#x2F;www.postgresql.orgDatastore: https:&#x2F;&#x2F;redis.ioand a bunch of support software. Paperless has Tika and Gotenberg as deps for example. reply mroche 8 hours agorootparentDo you find GitLab and its Runners to be heavy at all in your home lab? I&#x27;m curious if anyone&#x27;s been using Gitea&#x2F;Forgejo with Actions or Woodpecker (Drone). reply upon_drumhead 8 hours agorootparentI have the runner set to poll every 60 seconds, so it&#x27;s not using any real resources unless it&#x27;s running a build. Gitlab itself is the heaviest consumer of resources, however, having the integrated CI system is worth it for me. I use gitlab at work, so it&#x27;s familiar. reply ehPReth 5 hours agorootparentprevdo you find any blocklisting issues running a relay only? i heard in the long, long ago that people who even just ran relays would find themselves on IP blocklists because of ignorant blocklist builders not knowing the difference between exit and non-exit nodes or grabbing the wrong lists reply upon_drumhead 2 hours agorootparentI have not and I’ve run the relay for about 5 years now. I do get lots of captchas on cloudflare, but generally they’re just the click to prove you are a human style and it’s not too bad for the household. reply bigmattystyles 9 hours agorootparentprevAwesome! Thank you! reply justsomehnguy 6 hours agorootparentprev> Cloud storage for phones: http:&#x2F;&#x2F;nextcloud.comThanks, that sums it up for me.I used OC&#x2F;NC for years but in the last three I mostly abandoned it because the desktop app (for Windows, at least) is atrocious and Android one... isn&#x27;t good either.But as on-demand document download with occasional upload it&#x27;s fine. reply wutwutwat 8 hours agorootparentprevAs others have said, you can run a sidecar container and proxy your current containers through the sidecar, and into the Tailscale network. They are universal in that the docker containers can run on any docker host, not that they are guaranteed to mesh&#x2F;drop in and run inside whatever random containers you already run. Not sure why I am being downvoted for asking a genuine question out of curiosity… reply RockRobotRock 2 hours agorootparentprevLinuxServer.io containers rock for homelab. Not sure why you wouldn&#x27;t use them. reply asmor 14 hours agoprevI would disagree that containers aren&#x27;t supposed to run more than one process. It&#x27;s just discouraged because a lot of people aren&#x27;t well versed in the pitfalls of being PID 1. Fedora&#x27;s toolbox is a great counter-example, as is systemd now being able to boot up as your PID 1 in some container distros without much modification. reply Sayrus 11 hours agoparentTo be fair, even for running a single process the pitfalls are real. I&#x27;ve been seeing Tini[1] a lot for these situations.I just read in the README that Tini is included by Docker since 1.13 if using --init flag.[1] https:&#x2F;&#x2F;github.com&#x2F;krallin&#x2F;tini reply tornato7 13 hours agoprevI wonder if this will fix the issue of appending -n to new ephemeral servers that join the network. For example, if you have a service wiki-1 and that container&#x2F;instance gets restarted, it then appears on your tailnet as wiki-1 making users unable to access it at wiki&#x2F;Their official solution is to run a logout command before shutting down but that&#x27;s not always possible. reply vineyardmike 14 hours agoprevThis is really cool, I didn’t even know Docker mods existed. That’s the best kind of cool.I wonder if the internals will be open sourced? I assume it’s a pretty “simple” go tcp proxy that listens on the tailnet instead of an open port. I had been thinking about writing one for our services at work, so maybe we can use this, but I’d prefer to build the binary directly into our containers. reply asmor 14 hours agoparentIt&#x27;s likely just `tailscale serve https &#x2F; `.https:&#x2F;&#x2F;github.com&#x2F;tailscale&#x2F;tailscale&#x2F;blob&#x2F;main&#x2F;ipn&#x2F;serve.g...And they also support direct embedding:https:&#x2F;&#x2F;tailscale.dev&#x2F;blog&#x2F;embedded-funnelI think this is built on the wireguard-go + gvisor mashup, that allows you to do this with just Wireguard:https:&#x2F;&#x2F;github.com&#x2F;WireGuard&#x2F;wireguard-go&#x2F;tree&#x2F;master&#x2F;tun&#x2F;ne...One of my favorite applications of this is this little tool that turns Wireguard VPNs into SOCKS5 proxies (which you can selectively enable in your browser)https:&#x2F;&#x2F;github.com&#x2F;octeep&#x2F;wireproxy reply debarshri 14 hours agorootparentThis is really cool. Networking in general is full of quirks and what people think is full of \"magic\".Full disclosure, I am founder of Adaptive [1]. We use a similar technique to the one with VPN exposed as SOCK5 proxy but for accessing internal infrastructure resources.[1] https:&#x2F;&#x2F;adaptive.live&#x2F; reply asmor 2 hours agorootparentI think we had the same idea, but I didn&#x27;t get to finish building mine. OIDC tokens being available in most CI systems these days is a nice building block.https:&#x2F;&#x2F;github.com&#x2F;acuteaura&#x2F;tinybastion&#x2F; reply debarshri 1 hour agorootparentMore than happy to chat, if you drop a email at debarshi [dot] adaptive [dot] live. reply figmert 14 hours agoparentprevDocker doesn&#x27;t do mods. As the article says, this is possible due to s6 and s6-overlay, which is included with linuxserver.io docker images, combined with a set up scripts that set it all up. This does prevent your containers from being immutable.All the code for LSIO images is available on their GitHub. reply xena 10 hours agoparentprevYou&#x27;re in luck: it&#x27;s literally normal tailscalehttps:&#x2F;&#x2F;github.com&#x2F;tailscale-dev&#x2F;docker-mod reply xena 10 hours agoprevAuthor of the post here in case you have any questions! reply binwiederhier 7 hours agoparentNo question, just a thanks. I&#x27;ve read a lot of your stuff, and it&#x27;s always incredibly insightful and clever. Thanks for being an amazing Internet citizen! reply RockRobotRock 2 hours agoparentprevyou rock reply wccrawford 11 hours agoprevIt looks like I need to regenerate the auth key every 90 days, which kind of kills this for me. I definitely don&#x27;t want to have to update all my docker stuff every 90 days, and it&#x27;s almost assuredly going to go offline right when I can&#x27;t deal with it. reply zrail 11 hours agoparentThe trick is to persist the tailscale var volume. The auth key is only used when setting up a particular client the first time, once it&#x27;s connected to your network the auth key is irrelevant.If you&#x27;re doing this with ephemeral containers then yes you&#x27;ll need a way to roll auth keys. OAuth credentials don&#x27;t expire and Tailscale has a command line single purpose tool to get an auth key given OAuth credentials, so that can be a viable alternative.https:&#x2F;&#x2F;tailscale.com&#x2F;kb&#x2F;1215&#x2F;oauth-clients&#x2F;#get-authkey-uti... reply elesbao 10 hours agoprevThis post is great as the current state of network mesh is too complex for some users. That led me to write a simple rust daemon to run a TLS proxy and spawn the original app locally, reverse proxying requests as the cost of implementing a full mesh just to have tls across applications was too much for my team at the time. I didn&#x27;t knew about ONRUN, s6 and all that. Also, why not tailscale as the mesh ? reply gerty 3 hours agoprevIs there any reason it wouldn&#x27;t work with podman? reply skippyboxedhero 11 hours agoprevI have never been sure what the security implications are but I just set ports to the tailscale address, and everything is accessible.So if the local tailscale address is 1.2.3.4, I do:ports:- 1.2.3.4:8080:8080This doesn&#x27;t actually add applications to the tailnet as in the OP, but it works. reply eclipsetheworld 14 hours agoprevI just love that this blog post includes an AI-generated image with the caption of course being the name of the model and the given prompt. reply FloatArtifact 13 hours agoprevAll we need now is something for kubernetes reply conradludgate 13 hours agoparenthttps:&#x2F;&#x2F;tailscale.com&#x2F;kb&#x2F;1236&#x2F;kubernetes-operatorIt&#x27;s actually even easier to use. Add `tailscale.com&#x2F;expose: \"true\"` to a kubernetes service annotations and it will be added to the tailnet automatically reply isoprophlex 10 hours agorootparentWow, looks super comfy. Tailscale really seems to be doing everything right lately. reply johng 17 hours agoprevThis is really cool. reply gonzo 11 hours agoprev [–] Article is six months old reply AgentK20 11 hours agoparent [–] And yet some of us are just finding out about this feature now. reply oars 10 hours agorootparent [–] Most of us* replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tailscale has launched a new Docker mod that lets users integrate Tailscale into any Docker container, permitting the addition of apps to their tailnet and SSH into containers.",
      "The mod employs s6-overlay to insert instructions at the kickoff phase of a Docker container, enhancing the user experience and functionality.",
      "The article provides insights into the advantages of using private infrastructure with Tailscale, includes examples of Docker mods usage, and discusses possible support options and forthcoming enhancements."
    ],
    "commentSummary": [
      "The article spotlights Tailscale Universal Docker Mod that facilitates users to operate Tailscale on Docker containers, enabling easier cross-container communication, especially for containers located on disparate servers.",
      "The discussion thread revolves around the use of mods and containers in networking, hinting at potential open-sourcing and integration of the mod into containers.",
      "Networking tools and techniques including Wireguard and SOCKS5 proxies are also brought up in the conversation, emphasizing the substantial interest and value placed on mods and containers in the realm of networking."
    ],
    "points": 217,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1696783888
  },
  {
    "id": 37812113,
    "title": "Zen 5's Leaked Slides",
    "originLink": "https://chipsandcheese.com/2023/10/08/zen-5s-leaked-slides/",
    "originBody": "Skip to content Chips and Cheese The Devil is in the Details Home Memory Bandwidth Data Memory Latency Data Memory Latency Test Privacy Policy About Contact Posts Zen 5’s Leaked Slides October 8, 2023 clamchowder 1 Comment A YouTuber called Moore’s Law is Dead recently leaked a couple AMD slides about Zen 5. I typically find leaks uninteresting as they are impossible to verify and often don’t correspond to reality. One example is leakers expecting RDNA 3 to one-up Nvidia’s Ada architecture. AMD is fighting two larger competitors on two fronts and has not managed a decisive lead over Nvidia for more than a decade. AMD is expected to pull a miracle in the next generation, every generation (or two), and to everyone’s surprise it doesn’t happen. However, this leak is worth a mention because it includes a slide with architecture information. I don’t know whether the leaked slides are genuine, however building a coherent picture with a lot of details is far more difficult than fabricating a few performance numbers. With that in mind, let’s dig a bit into the slides point by point. Instead of trying to validate or disprove the rumors, I’ll try to provide context for each point so you can reach your own conclusions. Branch Prediction Branch predictors steer a CPU’s pipeline, making them vital to both power efficiency and performance. If a branch predictor takes too long to guess where a branch is going, it could hold up the rest of the CPU pipeline. If it guesses wrong, the core will waste time and power doing useless work. The leaked slide brings up three points under the branch predictor, namely zero bubble conditional branches, high accuracy, and a larger BTB. Zero Bubble Conditional Branches “Zero bubble” branching refers to handling a branch without delaying subsequent instructions in the pipeline. If later instructions were delayed, it would be analogous to a gas bubble in a pipe carrying liquid. Too many bubbles reduce how much liquid the pipe is delivering, and can be a problem if your production is constrained by how much liquid that pipe can deliver. A64FX branch prediction and fetch pipeline from the microarchitecture manual. Annotations added in red and green by Clam AMD could already do zero bubble branching since Zen 1, even though few branches could be tracked by the zero bubble predictor. Zen 3 expanded the zero bubble BTB (cache of branch targets) to cover 1024 branches, making zero bubble branches the typical case. Zen 4 carries this forward and expands zero bubble BTB capacity to 1536 branch targets. Therefore, zero bubble branching is nothing new. Zero bubble conditional branches aren’t new either. On all Zen generations, zero bubble branching can happen regardless of whether the branch is conditional or unconditional. Not a lot of difference between unconditional and conditional always-taken branches AMD isn’t alone either. Intel’s Haswell could track 128 branches and handle them with no bubbles. Intel thus made zero bubble branch handling a common case well before AMD did. Since Zen 3, AMD has been able handle more branches with zero bubble speed, but Intel is still very respectable in this area. Therefore “zero bubble conditional branches” is not an exciting point. Existing CPUs from Intel, Arm, and AMD themselves can already handle conditional branches with zero bubbles. Maybe Zen 5 increases zero bubble predictor capacity, but the slide did not say so. High Accuracy and Larger BTB AMD has improved branch predictor accuracy with every generation. Zen 2, 3, and 4 could often achieve better branch prediction accuracy than their Intel competitors. Zen 5 certainly looks to maintain that lead. But saying a desktop CPU has “high accuracy” branch prediction is like saying an airliner has a pressurized cabin. You expect it to, and it’s news if it doesn’t. Even older, simpler branch predictors like the ones on AMD’s Phenom CPUs could correctly predict the vast majority of branches. BTB stands for “branch target buffer”, which is a cache of branch targets. If a branch’s target is cached, the predictor can tell the CPU where to fetch instructions from next without waiting for the branch instruction to reach the core. That reduces frontend latency especially if the branch instruction has to be fetched from L2 or beyond. AMD has tweaked BTB size with every generation, but is a step behind Intel’s best. Golden Cove’s L3 BTB has 50% more capacity than AMD’s last level L2 BTB, and frontend latency is a problem for Zen 4 in games. It’s likely a problem for Intel as well, and both companies will try to expand branch target caching capacity as transistor budget allows. 2 Basic Block Fetch A basic block is a block of code with exactly one entry point and one exit point. A branch will terminate a basic block even if it’s conditional and not always taken. Existing AMD (and Intel) CPUs could already fetch across basic blocks because they could fetch across not-taken branches. The point on AMD’s slide could mean several things. Hypothetical basic block example. Assume nothing can jump into the middle of block1, and that the blocks are laid out consecutively in memory The simplest and most likely explanation is that Zen 5 can fetch across basic blocks just as any high performance CPU made in the last 20 years could. Usually the most boring interpretation of a marketing statement is the correct one. CPUs can generally fetch across not-taken branch boundaries, thus fetching two basic blocks in a single cycle Maybe Zen 5 can fetch across taken branches. Recent CPUs from Intel and Arm have done this. Rocket Lake could unroll small loops within its loop buffer, turning taken branches into not-taken ones from the fetch perspective. Arm’s Neoverse N2 and Cortex X2 can also sustain two taken branches per cycle by using a 64 entry nano-BTB. This capability can help improve frontend bandwidth for high IPC but branchy code. If an architectural feature has been around long enough to be implemented by multiple manufacturers, it has a better chance of showing up in a new core. Without being completely crazy, you could hope that Zen 5 can sustain more than one taken branch per cycle based on the leaked slide. Fetching across taken branches is harder and requires Finally, there’s the daydream category. AMD previously advertised zero-bubble branch handling when it became the common case with Zen 3. They didn’t mention zero-bubble branch handling with Zen 1 or Zen 2, even though both had limited ability to do zero-bubble branches. Maybe Zen 5 can fetch across basic blocks in the common case instead of using a loop buffer or micro-BTB as Intel and Arm did. That likely requires a dual-ported instruction cache or micro-op cache alongside a large BTB capable of delivering two branch targets per cycle. Zen 5 would also need circuitry to merge two fetch blocks into a buffer that downstream stages can consume. I think implementing such a strategy makes little sense. It’d only help in high IPC code bound by frontend throughput. Frontend latency due to instruction cache misses is a bigger issue. Load/Store Every CPU generation tends to see memory subsystem changes to reduce and hide latency. Increased L1D Capacity The leaked slide says Zen 5 has a 48 KB 12-way set associative L1 data cache, giving it increased capacity and associativity compared to Zen 4’s 32 KB, 8-way L1D. Impressively, the slide claims latency stays at 4 cycles. Intel did the same with their L1 data cache in Sunny Cove, but increased latency from 4 to 5 cycles. Zen 5’s larger L1D will enjoy increased hitrate. Higher capacity helps reduce cases where a code sequence’s working set exceeds cache capacity. Higher associativity helps prevent conflict misses where cache capacity is sufficient but too many “hot” addresses clash into the same set. I’m surprised AMD was able to pull this off because 12-way associativity means a cache access involves 12 tag comparisons. Zen uses a micro-tagging scheme where partial tags are compared to predict which cache way (if any) will have a hit, but comparing 12 micro-tags is still no joke. The slide also says Zen 5 can do 4 loads per cycle. That would require 48 tag comparisons. Larger DTLB All modern CPUs use virtual memory. Program memory addresses don’t directly address locations on DRAM chips. Instead, the operating system sets up a map of virtual addresses to physical addresses (page tables) for each process. A misbehaving process therefore won’t run over everything else and force you to reboot the computer because it has limited access to system memory. However, virtual memory addresses have to be translated to physical addresses. If the CPU checked the page tables for each memory access, latency would skyrocket as each program memory access turns into several dependent ones. Therefore, CPUs use TLBs (translation lookaside buffers) to cache frequently used translations. x86-64 4-level paging as described in Intel’s Developer Manual. Plain English comments added by Clam in red. Zen 4 already enjoyed a first level data TLB size increase from 64 to 72 entries. Because of their small size, the first level TLB is fully associative. That eliminates conflict misses, but means any TLB entry could contain the desired translation. Four data cache accesses per cycle could require more than 72 * 4 = 288 TLB tag comparisons every cycle. I’m not sure how Zen 5 would increase the DTLB size without impacting latency unless AMD dropped the fully associative scheme. I could see Zen 5 using a 16-way set associative DTLB with 128 entries or something along those lines. Checking it would be easier than with a 64 entry fully associative TLB, and the larger capacity could be enough to minimize conflict misses. Alternatively, Zen 5 could leave the first level DTLB untouched and increase L2 DTLB capacity. Zen 4 already brought L2 DTLB size up to 3072 entries compared to 2048 entries in Zen 3. Increasing L2 DTLB size would help programs with hot memory footprints in the multi-megabyte range. Larger PWC (Page Walk Cache) An address translation doesn’t have to be an all-or-nothing scenario where you either hit in the TLBs or do a full page walk. CPUs can cache upper level paging structures to reduce page walk latency when the TLBs can’t contain a program’s working set. Page walk cache implementations can vary. You can cache higher levels and cover more address space, or cache lower levels and shorten the page walk more. That lets the page walker start at a lower level, letting a page walk complete with fewer memory accesses. Each page walk cache entry covers a larger region in memory than a TLB entry, making it ideal for handling programs with a larger memory footprint than the L2 DTLB can reasonably cover. For example, a cached page directory pointer table entry would cover 1 GB of address space. I’ve seen a lot of Reddit comments where people said they didn’t understand my articles, so I can explain this in plain English with a real life analogy. Imagine you need to walk your dog down four blocks. You want it done faster. An obvious answer is to build a trebuchet that can throw you and your dog down two blocks. Then, you can start the walk closer to your destination. Because different destinations exist, you build 64 trebuchets facing in different directions. Now, you have a real life walk cache. If you build more powerful trebuchets, you can start your walk closer to the destination. But less powerful ones can throw you places that are a short walk away to many destinations. You can build a lot of more powerful trebuchets but that would consume more area (and trees). CPU architects have to make the same tradeoff. Since Zen 1, AMD has used a 64 entry page directory cache (PDC) that holds page directory pointer table and page map level 4 entries. L2 TLB entries can cache page directory entries. Perhaps Zen 5 finally increases PDC size. Or maybe, AMD gave the load/store unit a stronger preference for caching page directory entries in the L2 TLB compared to direct translations. High Throughput Zen generations have seen modest improvements in core throughput, because core throughput is typically not a limiting factor. Zen 1 and 2 could sustain 5 instructions per cycle, while Zen 3 and 4 could sustain 6 per cycle. AMD made double digit IPC gains every generation thanks to large improvements to instruction and data side memory access performance. Slide from AMD’s Zen 3 Hot Chips presentation showing IPC gains largely coming from memory access improvements from both the data and instruction side But a small minority of high IPC applications might benefit. 8-Wide Dispatch/Rename Every Zen generation had at least an 8-wide frontend and 8-wide retire. However, the dispatch/rename stage was only 6 micro-ops wide. If Zen 5 makes rename/dispatch 8-wide, it would be able to sustain 8 micro-ops per cycle. From Samsung’s paper “Evolution of the Samsung Exynos CPU Microarchitecture” This change will benefit a small minority of high IPC applications capped by core width. Lower IPC applications like games will see little benefit from this change because they’re primarily bound by cache and memory latency. Op Fusion CPUs can achieve higher throughput and make better use of internal buffers by fusing adjacent instructions into single micro-ops. Branch fusion is the most common example. Conditional branching on x86 involves using an instruction that sets flags, and then a branch that jumps (or not) depending on flags. Intel and AMD have been fusing such ALU + conditional branch pairs for many generations. Arm’s recent cores do the same for equivalent ARM64 sequences. Zen 3 improved AMD’s fusion capabilities by allowing simple ALU instructions like ADD, AND, and XOR to be fused with a subsequent branch as well as CMP and TEST. Therefore one micro-op on Zen 3 can perform a math operation, check the result for a condition and branch on it, and write the result back to a register. Zen 4 added NOP fusion and XOR+DIV/CDQ+IDIV fusion. The latter handles common use patterns for x86’s division instructions. Maybe Zen 5 expanded fusion cases, but the slide did not say so. For all we know, the slide could be reiterating the features already present on Zen 4. Prior Zen generations already covered the most common fusion cases (branches). Zen 4’s improvements chase diminishing returns. NOPs are used to align code and should account for a very small percentage of executed instruction. Division is known to be very expensive and avoided by most compilers. If Zen 5 adds fusion cases, it’ll probably pursue further diminishing returns. Larger, More Unified Scheduler Schedulers sit at the heart of an out-of-order CPU and let them achieve high instruction level parallelism. Every cycle, a scheduler has to watch what registers are written to and see if pending instructions need those inputs. It also has to select instructions that have all their inputs ready and send them to execution units. Failing to accomplish all that in a single cycle incurs a ~10% IPC penalty, so large, fast schedulers are very difficult to design. Small schedulers are easy to make fast but can fill quickly and prevent the core from hiding latency. Fortunately, engineers have a lot of scheduler layout options available. In a distributed scheduler, each execution port gets its own private scheduler. That simplifies scheduler design because each scheduler only has to select one instruction for execution each cycle, and only needs enough entries to hold the fraction of pending instructions that are expected to be waiting for that port. However, tuning is difficult because one scheduler can fill and block the renamer even if scheduler entries are available elsewhere. From Henry Wong’s PhD thesis, showing the distributed scheduler design space A unified scheduler avoids that problem by having one scheduler serve multiple ports. Each scheduler entry can hold an instruction destined for any port, so a sudden spike in demand for one execution port can be better tolerated. However, a unified scheduler has to select enough instructions per cycle to feed all the execution ports it’s attached to. There’s no free lunch. AMD’s Zen cores have used a mix of distributed and unified schedulers. There are multiple schedulers like with a distributed scheduler, but some schedulers serve multiple ports as in unified designs AMD, Intel, and Arm’s recent CPUs use a hybrid of the two approaches. Zen 5’s scheduler is both larger and more unified, meaning it has more total entries and can use some of them more efficiently. What filled up and caused a renamer/dispatch stall in a couple of games From a look at a couple of gaming workloads, integer scheduler 0 fills a bit more often than the others. Cinebench 2024 sees similar behavior on the integer side. Scheduler 0 feeds an AGU pipe and an ALU/branch pipe. AMD could chose to make this scheduler bigger or unify it with another scheduler. They could combine both approaches as well. However, scope for improvement may be limited. Integer scheduler related dispatch stalls account for single digit percentages, indicating Zen 4’s distributed scheduler is already well tuned. 6 ALUs, 4 loads, 2 stores The slide says Zen 5 has 6 ALUs, and the ability to do 4 loads/2stores per cycle. ALUs, or arithmetic logic units, are execution units capable of handling the most common integer instructions like adds and bitwise operations. All prior Zen generations had four ALUs, so Zen 5 would increase per-cycle scalar integer throughput by 50%. This change will have minimal effect. I put this section right after the scheduler one because schedulers will fill if the execution units can’t keep up with incoming operations. Schedulers can fill for reasons other than lack of execution ports as well. For example, a sequence of latency-bound instructions will also fill the schedulers. Scheduler-bound dispatch stalls are therefore an upper bound on how often the core is execution unit bound. From above, it doesn’t happen often. Increased load/store throughput might help specific scenarios like memory copies, where the core can sustain 2 loads and 2 stores per cycle but how much this will effect general cases, I don’t know. ALUs and AGUs themselves are tiny, but feeding them is more difficult. Each new execution port needs inputs from the register file, and increasing register file port count will increase area. More execution ports mean schedulers will have to pick more instructions per cycle, requiring more power and area as well. A not insanely expensive way to get to 6 ALUs and 4 loads/2 stores If I were AMD and had to implement 6 ALUs and 4 AGUs, I would do so with the absolute minimum of extra ports. AGU ports can do double duty as ALU ports because AGUs already have to do simple math on register inputs anyway. The branch port can also be upgraded to an ALU, again reusing existing register file ports. Increasing execution unit throughput will result in minimal gains, but minimal gains can be worthwhile if they are achieved at low cost. I suspect AMD is going after that route. Larger Structure Sizes An out-of-order CPU has structures to track instruction state until their results can be made final. Structure sizes tend to increase with every CPU generation. The leaked slide suggests Zen 5 will do so too, but did not go into specifics. In Cinebench 2024 and the tested games, Zen 4’s reorder buffer is responsible for most stalls. The reorder buffer tracks all instructions in the backend until they are committed in-order. It’s a cap on how far ahead the CPU can move ahead of a stalled instruction. Filling the reorder buffer isn’t a bad thing because it means other queues for specific instruction categories are large enough to not become limitations themselves. AMD has increased reorder buffer capacity with every CPU generation. Zen 5 will almost certainly see an increase as well, but we don’t know to what extent. Along with a reorder buffer capacity increase, AMD will have to augment other structures to prevent them from filling before the ROB does. The store queue could already use more entries and is a prime candidate for optimization. However, increasing store buffer size will be difficult because it has to hold pending store data. For Zen 4, that’s up to 32 bytes per store. 64 Byte Fills/Victim This line on the slide talks about caching. “Fills” refers to cache fills, and “victim” refers to lines kicked out of a cache to make room for data being filled in. I’m confused because every CPU in recent history uses 64 byte cache lines, which means caches manage data at 64 byte granularity. Thus, data is evicted 64 bytes at a time, and brought in 64 bytes at a time. It’s not a point worth mentioning. Data Prefetching Improvements Better caching and higher reordering capacity help attack the memory latency problem by reducing latency and allowing execution to proceed past a latency-bound instruction, respectively. Prefetching counters memory latency by trying to get data that the program will need before an instruction asks for it. Again, the slide didn’t go into specifics, so I’ll provide context on what Zen 4 does. In Zen 4, AMD has prefetchers at the L1 and L2 level. Zen 5 may keep the same prefetch methods but allow them to prefetch further, taking advantage of any bandwidth increases offered by more mature DDR5 implementations. AMD may also tune the prefetchers to ensure demand requests get priority when there’s high bandwidth demand, such as during multi-core workloads. Better AVX-512 Zen 4 featured AMD’s first AVX-512 implementation. Unlike AMD’s first SSE and AVX implementations, Zen 4 did not break instructions that operated on 512-bit vectors into two micro-ops. It had full width 512-bit vector registers, and kept AVX-512 math instructions as one micro-op until they were executed 256 bits at a time. From AMD’s presentation at ISSCC Keeping the same FP execution throughput as Zen 2 and Zen 3 helped AMD get the most important AVX-512 benefits (more efficient use of backend resources) without a massive increase in die area and power. The slide says “FP Pipes/Units at 512b”. The most optimistic interpretation is that Zen 5 has 2×512-bit FP vector execution. Even on TSMC’s newer 4 nm process, I feel that’ll cost too much area and power when most consumer applications don’t use 512-bit vectors. Perhaps AMD will create Zen 5 variants with different FP configurations as Intel has done, with client SKUs spending less area and power on vector FP throughput. 512-bit stores are handled less efficiently on Zen 4 because the store queue can only hold 256-bit pending store data with each entry. At Hot Chips 2023, AMD stated that the area overhead of buffering 512-bit store data was not acceptable. The leaked Zen 5 slide says “Load/Store Queues (512 bit)”, so AMD may have changed their stance. Applications that heavily leverage 512-bit vectors should see more performance uplift on Zen 5 thanks to these changes. Final Words From the leaked slides, AMD is pursuing diminishing returns after getting most of the low hanging fruit with prior Zen generations. Zen 2 greatly improved branch prediction accuracy, vector throughput, and cache capacity compared to Zen 1. Zen 3’s improved BTB setup mitigated Zen 2’s frontend latency problem, and a reorganized scheduler avoids situations where Zen 2’s AGU scheduler fills up. Zen 4 brought a bigger micro-op cache, improved L2 capacity, a substantially increased out-of-order execution window, and AVX-512 support. Zen 5 appears to be going after more limited gains by increasing core throughput and providing a stronger AVX-512 implementation. That said I would caution against looking too far into the current leaks. Specific details are rare, leaving plenty of wiggle room. Assuming Zen 5 is set in stone at this point is also perilous. Core behavior can be tuned via microcode updates. A core can be configurable as well, giving AMD the potential to make large changes even when the architecture is “complete”. We’ve seen AMD roll out Zen 2 variants with different FPU configurations. In the same video, MLiD showed another slide that suggests different FP-512 variants exist as well. Second slide shown by MLiD Any performance numbers should be taken with a giant grain of salt too. It’s better to assume they are all guesses at this point. Even if a leaker has a “source”, estimating performance is inherently difficult because different applications will behave differently. An engineer might see a 30% IPC instruction uplift in simulation with a specific instruction trace, but that doesn’t mean other applications will enjoy the same improvement. AMD’s slide That could be mentioned to a leaker, who doesn’t understand that the trace may not be representative of most applications. From a Red Gaming Tech video. 20-30% IPC gains sound very high considering AMD has managed 10-20% with Zen 2, Zen 3, and Zen 4. Certainly not impossible, but I would be skeptical after seeing the wild RDNA 3 rumored performance numbers. Finally, engineers at Intel, AMD, Arm, and other companies put a lot of hard work into their products. It’s only fair to let them get their say when a product is released. If the engineers release a solid product that delivers a typical 10-20% generation on generation gain but everyone’s perception is set based on fabricated or misinterpreted early performance numbers, I think that’s disrespectful to the engineers. It’s also nonsensical when Intel delivered lower generation on generation gains at the top of their game in the early 2010s. AMD is prone to this because they’re an underdog that people expect to one-up its bigger competitors, so fanciful rumors get a lot of attention. Whenever Zen 5 comes out, I would encourage everyone to look at its performance with respect to how Intel and other CPU manufacturers are progressing, and not based on rumors. If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our Patreon or our PayPal if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our Discord. Author clamchowder View all posts Don’t miss our articles! Email Address * Related Posts Intel’s Ponte Vecchio: Chiplets Gone Crazy September 23, 2023 In \"Articles\" Lowering the BAR: AMD’s 6700 XT launch and the Importance of Disclosure March 10, 2021 In \"Articles\" Nvidia’s Ampere & Process Technology: Sunk by Samsung? June 22, 2021 In \"Articles\" Post navigation ← Qualcomm’s Hexagon DSP, and now, NPU 1 thought on “Zen 5’s Leaked Slides” ET3D says: October 9, 2023 at 12:16 am Thanks for the writeup. The explanations and comparisons to past architectures made this a worthwhile read, which the rumour on its own wouldn’t have been. Reply Leave a Reply This site uses Akismet to reduce spam. Learn how your comment data is processed. Patreon Twitter Mastodon GitHub Link Link Search for: Sort by Relevance Newest first Oldest first Archives October 2023 September 2023 August 2023 July 2023 June 2023 May 2023 April 2023 March 2023 February 2023 January 2023 December 2022 November 2022 October 2022 September 2022 August 2022 July 2022 June 2022 May 2022 April 2022 March 2022 February 2022 January 2022 December 2021 November 2021 October 2021 September 2021 August 2021 July 2021 June 2021 May 2021 April 2021 March 2021 February 2021 January 2021 December 2020 Privacy Policy Copyright © 2023 Chips and Cheese",
    "commentLink": "https://news.ycombinator.com/item?id=37812113",
    "commentBody": "Zen 5&#x27;s Leaked SlidesHacker NewspastloginZen 5&#x27;s Leaked Slides (chipsandcheese.com) 201 points by treesciencebot 17 hours ago| hidepastfavorite143 comments Moldoteck 26 minutes agoCan I ask why more manufacturers don&#x27;t implement apple&#x27;s strategy with soc with ram and all and offer option to add additional classic ram as a slower cache buffer between soc&ssd? This should both solve the lack of upgradability and unlock more performance per watt? reply pvg 16 hours agoprevDiscussion last weekhttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37715082 reply snvzz 14 hours agoprev>(fusion) Conditional branching on x86 involves using an instruction that sets flags, and then a branch that jumps (or not) depending on flags.x86 shows its age and accumulated cruft here.RISC-V has test and jump in a single instruction, doing away with flags. reply klelatti 13 hours agoparentThat’s such a strange criticism given how keen the designers of RISC-V have been to push macro-op fusion. reply snvzz 13 hours agorootparentMacro-op fusion is newer than x86.RISC-V is newer than macro-op fusion, and could account for it possibly being present in implementations.I do not remember ever seeing the designers of RISC-V actually push macro-op fusion. reply klelatti 13 hours agorootparentSeriously? You weren’t aware of this?The Renewed Case for the Reduced Instruction Set Computer: Avoiding ISA Bloat with Macro-Op Fusion for RISC-Vhttps:&#x2F;&#x2F;people.eecs.berkeley.edu&#x2F;~krste&#x2F;papers&#x2F;EECS-2016-130... reply snvzz 12 hours agorootparentYes, very aware.Yet, it does not push macro-op fusion in implementations. It can either be done, or not done. But RISC-V creates opportunities where it is possible and not harmful to do so, while increasing code density.>Exploiting this fact, the RV64G and RV64GC effective instruction count can be reduced by 5.4% on average by leveraging macro-op fusion.It is up to the architects behind implementations to decide when and if to take advantage of the opportunities created by RISC-V&#x27;s design to do macro-op fusion. reply klelatti 12 hours agorootparent> Yet, it does not push macro-op fusion in implementationsYes it does. It pushes it as an alternative to adding instructions to the ISA. It says we don’t need these extra instructions because architects can use macro-op fusion to remove the penalty associated with having two instructions rather than one. reply snvzz 12 hours agorootparent>It says we don’t need these extra instructions because architects can use macro-op fusion to remove the penalty associated with having two instructions rather than one.They are correct in saying that. x86 didn&#x27;t have the chance to do so, because back when it was designed, macro-ops were not considered.Yet whether to fuse or not is still up to microarchitecture architects. RISC-V doesn&#x27;t push it. Only creates the opportunity where sensible, while increasing code density which benefits everyone. reply tux3 12 hours agorootparentIn the same sense that a CPU not supporting hard floats gives software the &#x27;opportunity&#x27; to implement soft-floats.With RISC-V&#x27;s current design, you can either not do fusion and eat a penalty, or you can do fusion.Different ISAs may have a greater or lesser need for fusion, and greater or lesser associated penalty. It seems a little obtuse to insist the ISA doesn&#x27;t encourage fusion, when other ISAs would avoid a structural penalty for not having fusion, that RISC-V intentionally doesn&#x27;t try to avoid at all. reply snvzz 12 hours agorootparent>With RISC-V&#x27;s current design, you can either not do fusion and eat a penalty, or you can do fusion.Only a small portion of existing RISC-V hardware uses fusion.This should tell you all you need to know about this mistakenly inferred \"penalty\" and whether RISC-V forces macro-op fusion into anybody.As per why fusion helps even when not leveraged: Not allocating encoding space to a unified opcode does, in average over a large body of code, increase code density, as shown in the paper.This means more code fits in cache, and in hardware this does weight far more than instruction count, which remains low and highly competitive even where fusion isn&#x27;t leveraged. reply klelatti 12 hours agorootparentprevYour definition of ‘push’ here is make compulsory - which doesn’t make sense in the context anyway.I’ll leave others to judge if the quoted paper pushes it and move on. reply snvzz 12 hours agorootparent>Your definition of ‘push’ here is make compulsoryMacro-op fusion is entirely optional, leveraged by few hardware implementations; Fusion is useful mostly (i.e. exceptions exist) only on mid-range cores.High end cores prefer to split everything up (unnecessary on RISC-V), whereas low end cores prefer simpler hardware.Net win in code density weights more in hardware than instruction count, which remains excellent in RISC-V even when the studied microarchitecture does not implement fusion. replymananaysiempre 12 hours agorootparentprevNot a CPU designer by any means, but I imagine mundane RMW ops on regular registers, like the second half of LUI+ADD, are easier to handle than a flags register that is implicitly threaded through the entirety of the program. So the argument against the CMP+BEQ design could be all the other possible occurrences of BEQ the architecture is required to handle, not the one that can be fused (or not) with a preceding instruction.(Then RISC-V goes and introduces a flags register for floating-point ops...) reply fanf2 12 hours agorootparentprevThe first processor I know of that did macro-op fusion was the Inmos T9000 transputer in 1993. reply Scaevolus 13 hours agoparentprevx86 has load and add in a single instruction, doing away with temporary register usage. reply snvzz 11 hours agorootparent>x86 has load and add in a single instructionSure.>doing away with temporary register usageDubious, as that&#x27;s up to implementation either way.A microarchitecture might split it into a load and an add, eliminating any instruction count reduction advantage.Whereas another microarchiture might take a separate load and add and fuse them, which would free opcode space to be used more effectively elsewhere, resulting in overall code size reduction.It&#x27;s a careful balance. reply stavros 13 hours agoparentprevWhy can this not be added to x86? Is the ISA immutable now? reply snvzz 12 hours agorootparent>Is the ISA immutable now?x86 is, by many metrics, not a good ISA.Its value is derived from the software moat, accumulated since the late 1970s.Thus it isn&#x27;t immutable. Opcodes can be added and removed.Yet breaking compatibility with old code would destroy x86&#x27;s value. reply madeofpalk 12 hours agorootparentCan instructions be added without breaking compatibility? reply colejohnson66 11 hours agorootparentFor x86? Yes. There are many holes in the opcode space that are regularly filled. The bespoke encoding comes from there not being (really any) holes in the beginning, and the designers trying to work around it. reply snvzz 11 hours agorootparentprevx86 opcodes can be 15 bytes long, that&#x27;s how.At the expense of code density, and thus hurting PPA (Power, Performance and Area).Wildly variable opcode length does also complicate the decoder considerably, as it has to find where instructions start and end, an inherently serial operation, and yet decode enough of them to feed an n-wide pipeline. reply pritambaral 13 hours agorootparentprevIt could be added with an extension, but software would continue to be compiled without it. For many, many years. reply snvzz 13 hours agorootparentThere&#x27;s also the issue of code size.x86 encoding space already has a lot of clutter in it.The cost of low code density is that less code fits in cache. This cost is paid in performance, power, area or a combination of them. reply stavros 13 hours agorootparentprevSure, but that&#x27;s fine, no? Better late than never. reply phkahler 14 hours agoprevOne thing I never see in pieces like this is comments on how a change might affect hyperthreading performance.It&#x27;s not relevant for single threaded code, but then neither is doubling the core count from 4 to 8 or 16. reply ksec 7 hours agoparentI dont think they care as much as in the case of Multi Core era. It is easier just to sell more cores. reply nullc 5 hours agoparentprevBoth intel and AMD&#x27;s hyperthreading seems pretty lame compared to Power&#x27;s for some reason.One cause may be that with just two threads the chance that both get blocked on a memory access is pretty good. reply dist-epoch 17 hours agoprevOne big difference between CPUs and GPUs is memory bandwidth.Dumb question, but why don&#x27;t Intel&#x2F;AMD massively increase memory bandwidth by adding more memory lanes? For example instead of the usual 4 slots on a desktop motherboard you would have 16 slots surrounding the CPU on all sides.Is it that the CPU would be unable to effectively use all that bandwidth due to less parallelism compared with a GPU? reply kllrnohj 16 hours agoparentConsider how GPUs & CPUs actually use memory:On a GPU, the memory access patterns are often very wide & easily controlled, such as fetching a block of texture memory. This is like streaming reads for a disk, where you can actually just throw essentially RAID 0 at the problem to scale up really easily. So scaling all the way up to ridiculously wide 2048-bit bus widths (ex. AMD Vega 64) is completely viable since you&#x27;re working with relatively bulk array data almost constantly. Now most GPUs aren&#x27;t going quite that wide to hit the speeds they want, but consider that 192-bit is still considered \"low-end\" here and that&#x27;s already a lot wider than you&#x27;ll find on most CPUs.On a CPU, especially in languages like Java, JavaScript, etc..., memory access patterns are often very random and very small. Consider just something like a simple boolean accessor call `if (foo->isEnabled()) doThing();` - you&#x27;re reading let&#x27;s say 8 bytes (foo pointer) to in turn access 1 byte (a boolean). Now the CPU is already reading in chunks of 64-bytes typically (cache line width), so you&#x27;re already blowing 2 memory fetches to get 128-bytes of data just to read 9 bytes total. Going wider, which is how GPUs scale bandwidth, is utterly pointless here. You&#x27;d just be fetching even more data you didn&#x27;t want in the first place just to occupy precious cache space on something you&#x27;re probably not going to use anyway. In fact this is why DDR5 actually essentially shrinks the bus-width, from 64-bit in prior DDR generations to 2x32-bit in DDR5 reply dathinab 14 hours agorootparent> very random and very smalla major part of JVM, JS Engines and even memory allocators, pools and similar is to make this patters less random, more predictable and more local.For example a common optimization for lisp like languages it to layout a linked list in a continuous way in memory.Stuff like that is needed even if you have more memory bandwidth as sometimes even more important then memory bandwidth is cache locality, which is WAY harder to scale then adding more memory channels. And branch prediction a major performance factor for modern CPUs also benefits quite a lot from cache locality.Furthermore having more bandwidth also only helps if you can bring the bandwidth to each core as needed. But if you ad more memory bandwidth it still has a limit per memory region which can be an issue.So in practice nearly all applications benefit from more bandwidth but the degree they do benefit is limited outside of highly paralelizable use-cases (like some graphics and AI use-case).So for most typical consumer use cases it&#x27;s not worth the additional (high) cost this would impose to various aspects (especially motherboards).And for pro-sumer&#x2F;server CPUs _we do have more channels_!But always a balance between added cost and expected need for more bandwidth.> In fact this is why DDR5AFIK the reason they DDR5 has a split channel is way more complicated then that reply kllrnohj 11 hours agorootparent> a major part of JVM, JS Engines and even memory allocators, pools and similar is to make this patters less random, more predictable and more local.They don&#x27;t, though. The JVM memory model doesn&#x27;t even really allow it. This is a big reason for value types, but that keeps getting perpetually delayed... reply YetAnotherNick 14 hours agorootparentprev> especially in languages like Java, JavaScript, etcNot only there is sequential array in both the language, it is very often used. Things like iterating the array is one of the most common operation and is almost always memory bandwidth bound even for interpreted language. reply logicchains 14 hours agorootparent>Not only there is sequential array in both the language, it is very often usedIn Java only primitive arrays are memory-efficient for iteration; ArrayLists are not because every element is boxed, so even when iterating sequentially a lookup of some random memory address is needed for every element. reply cogman10 16 hours agoparentprev> Is it that the CPU would be unable to effectively use all that bandwidth due to less parallelism compared with a GPU?Yes, but also it&#x27;s due to simple physical limitations. If you look at how close CPUs are to RAM as well as the number of PCB lines coming out of CPUs, it becomes pretty readily apparent that there&#x27;s not much room for additional lines.You have to recognize that each of those lines is creating a magnetic field, the closer you stack them, the more likely cross talk is.We could continue to lower the voltage on the memory lanes so you can pack them closer together, but then you have to start worrying about environmental interference on the lines. It likely won&#x27;t be too long in the future (if it&#x27;s not already here) that we&#x27;ll start seeing shielding for the memory lanes so they can be packed closer together and more can be added.It wouldn&#x27;t shock me if this became a part of DDR6.It also wouldn&#x27;t shock me to start seeing more complex memory controllers doing things like lz4 compression to eck out even higher bandwidths (particularly when the memory is highly uniform). We are sort of seeing the precursor of this in DDR5 with commands to set whole blocks to a given value. reply AnthonyMouse 15 hours agorootparent> You have to recognize that each of those lines is creating a magnetic field, the closer you stack them, the more likely cross talk is.PC desktops have 2 memory channels. EPYC servers have 12 channels per socket. It&#x27;s not the physics, it&#x27;s that ~8-core processors generally aren&#x27;t bounded by memory bandwidth and it&#x27;s not worth the cost of increasing the number of pins.The server sockets do it because they have 128-core processors.If this changes what we may start seeing is CPUs (or iGPUs) with a few GB of HBM as L4 cache. reply simcop2387 15 hours agorootparentIntel is doing this today in the Xeon MAX line, up to 64gb of hbm ram, you don&#x27;t even necessarily need ddr5 on them. serve the home has a good article about them but it&#x27;s all very new so it&#x27;s hard to say how it&#x27;ll shake out. reply dathinab 13 hours agorootparentprev> not the physicsno physics matterbecause it makes it much harder and in turn more costly to the more channels you have to have high quality reliable connectionsserver RAM is for example also slower to compensate this e.g. recent EPYC processors run at up to 4800MT&#x2F;s but a Ryzen 9 runs at up to 5200MT&#x2F;s (without overclocking, with running at 6000MT&#x2F;s isn&#x27;t that uncommon)server also tend to always use EEC memoryso it&#x27;s a benefit&#x2F;cost balance calculation reply AnthonyMouse 12 hours agorootparent> server RAM is for example also slower to compensate this e.g. recent EPYC processors run at up to 4800MT&#x2F;s but a Ryzen 9 runs at up to 5200MT&#x2F;s (without overclocking, with running at 6000MT&#x2F;s isn&#x27;t that uncommon)Zen3 EPYC has 8 channels per socket and uses the same 3200MT&#x2F;s DDR4 as Zen3 desktops, so this little explains why desktops couldn&#x27;t have e.g. 4 channels if there was a need for it. They already commonly have 4 DIMM slots.The rated speed for DDR5 is 8% higher for desktops, but the additional channels don&#x27;t seem to be why. Intel uses 8 channels of DDR5 rather than 12 and it&#x27;s still 4800MT&#x2F;s.> so it&#x27;s a benefit&#x2F;cost balance calculationBut that was my point? reply CoastalCoder 16 hours agorootparentprevI&#x27;m curious if anytime soon we&#x27;ll start seeing optical interconnections between CPUs and on-device &#x2F; same-chasis memory.I&#x27;m not sure what that would add for latency and cost, but I&#x27;m guessing that with optical wave guides, crosstalk isn&#x27;t a concern. At least for the optical part of the data path. reply kimixa 16 hours agorootparentThe speed of light in an optical fiber is still only about 2*10^8 m&#x2F;s - ddr5 6000&#x27;s 3ghz clock means it still only travels 6.7cm&#x2F;clock - so there&#x27;ll still be difficulties around latency if it&#x27;s not \"close\", and that&#x27;s ignoring any issues of actually getting optical transceivers on the die itself. reply billti 15 hours agorootparentIt blew my mind when I learned that even if you could hand wire conceptually perfect switches and wires, once your board was over a few inches, the laws of physics (e.g., the speed of light) would mean you still couldn’t beat a modern CPU for clock speed.It really is insane the level at which current hardware works. reply josephg 13 hours agorootparentYeah. Any time I wait for my computer to do something I think about this. In the 4 seconds discord took to open, my computer could do what, 5 billion operations per second per core? Even single threaded that’s 20 billion steps? And my internet connection could have delivered 600 million bytes of data - enough for a complete copy of windows xp. It’s wild. reply steve1977 7 hours agorootparentIt’s wild and a sign how terribly inefficient most software is today reply trealira 15 hours agorootparentprev> I&#x27;m curious if anytime soon we&#x27;ll start seeing optical interconnections between CPUs and on-device &#x2F; same-chasis memory.Isn&#x27;t that kind of what those APUs sold by AMD are? And SoCs? I could be misunderstanding you, though. reply ta988 14 hours agorootparentprevThey are coming yes. They may not bring much in term of bandwidth per line, but will allow much more lines and entirely different geometries for the cpus allowing chips to be cooled on both sides etc. reply HappyDaoDude 10 hours agorootparentprevIt isn&#x27;t so much the physical limits but more the cost of manufacturing a physically more complex board. This is why most low end GPU&#x27;s typically have their memory bus width cut down to save on PCB and RAM complexity costs.An extreme example, the phone I am using has RAM running at 1.8Ghz but on a 16bit bus. Anything to keep costs down. reply TerrifiedMouse 16 hours agoparentprevWith CPUs the bottleneck isn&#x27;t just memory bandwidth only but also memory latency. If prefetch fails to get the correct data loaded into the cache, the CPU just stalls for thousands of cycles waiting for RAM to deliver the data.If I&#x27;m correct, this is the difference between DDR RAM and GDDR RAM - the latter has higher bandwidth but also higher latency, which is great for graphics with its high bandwidth requirements and very predictable memory access patterns (except when you do ray tracing) which allows you to hide the latency, but you don&#x27;t want to use GDDR for system RAM if you value performance. reply Const-me 15 hours agorootparentYeah, the throughput&#x2F;latency tradeoff is real.Here’s an unusual motherboard salvaged from Xbox Series X which combines AMD Zen2 CPU cores, GDDR6 memory, where the integrated GPU is disabled due to defects: https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;4800s-xbox-chip-shows-us-w... The performance is not great.GPUs are good at hiding the latency due to high degree of parallelism. GPU cores switch to another thread instead of waiting for the data. Each CPU core only runs 2 threads, it can’t quite do the same thing.GPU memory access patterns are only great for well written compute shaders (or CUDA kernels) when the developer who wrote that shader managed to do fully coalesced loads and stores. Memory access patterns for pixel shaders are not that great, especially when doing trilinear or anisotropic sampling in these shaders. reply HappyDaoDude 10 hours agorootparentprevWhich is why I always found it funny to see consoles using unified GDDR for CPU and GPU.But you can optimize around the limitation (within reason) and don&#x27;t have to content with a general non-specified OS.This is why I do wonder about the path not taken with the 1T-SRAM that powered the Gamecube&#x2F;Wii. It was designed purely with low latency in mind but I don&#x27;t think it scaled well. reply toast0 16 hours agoparentprevServer sockets have more memory channels and server motherboards usally have more memory slots. Layout limitations probably make ram on four sides not very feasible.But, more memory channels means more pins and larger sockets, and that adds cost and layout issues for motherboards. HBM ram that attaches on top of the cpu addresses the layout issue (and the bandwidth issue), but interferes with heat removal and removes expandability. Apple makes it work, but they limit clock speeds and were never big on expandability. Intel and AMD have a hard time in the market when their top consumer chips don&#x27;t clock to the moon, even if the last 10% of clock speed takes 50% of the power and delivers 5% of performance (percentages made up). Server chips don&#x27;t need to clock so high, so AMD can release clock limited compact cores that have about the same IPC in a bit more than half the area, and the difference in clocks for a max-cores regular zen4 and a max-cores zen4c isn&#x27;t too much (L3&#x2F;core is half for zen4c though, so may not be a benefit depending on your working set) reply AnthonyMouse 15 hours agorootparent> but interferes with heat removal and removes expandabilityEh. RAM generally consumes a single digit number of watts and will do so regardless of where it&#x27;s physically located in the system.If you put HBM on the CPU socket then you have to replace them together, but you can still replace them. It could also make sense to do both: If you have 16GB of HBM and 16GB of DDR5, you have 32GB of RAM and half of it is faster. Or 80GB of RAM and 16GB of it is faster. Cache hierarchies are a good way to strike a balance between performance and cost, and then the latter can still be expanded independent of the CPU. reply ta988 14 hours agorootparentRoughly 3W per 8GB in DDR4. DDR5 is said to use less. reply ksec 17 hours agoparentprev>Is it that the CPU would be unable to effectively use all that bandwidth due to less parallelism compared with a GPU?Yes. That is why you only see more memory channels for higher CPU count. reply hedgehog 16 hours agoparentprevTwo reasons: First, it&#x27;s a lot of wiring which makes everything more expensive, complicated, power hungry, etc. Mobile SoCs (and Apple&#x27;s chips) sidestep this issue by packaging the memory with the CPU so distances are short and the issues are avoided. Second, increasing memory bandwidth wouldn&#x27;t be that much of a performance win because as you point out there isn&#x27;t that much parallelism and normal software would benefit much more from lower latency than higher bandwidth. Together that means a many-slots design would cost a lot but not be proportionally faster.Tradeoffs are different for server boards (many cores in the CPU) and on platforms with integrated GPUs (becoming more common) so none of this is static. I suspect in-package RAM will become more common even for platforms that still have a couple channels of slotted memory, that would create a pool of faster memory available for graphics etc while still leaving room for more if you need to have a lot of Chrome tabs open or whatever. reply mastax 13 hours agoparentprevIn addition to what everyone has already said, memory controllers are relatively big and expensive. System integrators often ship systemsp.s.: I don’t care about power consumption :)Clearly! I don’t know if I’d say it’s “on par” if an Apple A-series part (i.e., low power enough to be shipped in iPhones and not in any of their “serious” computers) is actually getting slightly higher numbers than the fastest, most power hungry core that AMD has right now! reply AnthonyMouse 16 hours agorootparent> I don’t know if I’d say it’s “on par” if an Apple A-series part (i.e., low power enough to be shipped in iPhones and not in any of their “serious” computers) is actually getting slightly higher numbers than the fastest, most power hungry core that AMD has right now!It isn&#x27;t. The A17 Pro gets 7199 on GB6 multi-thread compared to 18778 for the 7950X, and multi-thread workloads are the only thing that will cause the 7950X to use its full TDP. If you give the Apple chip a desktop&#x27;s power budget, the single-thread number barely changes because that isn&#x27;t what sets the TDP.And Zen4 is punching above its weight because it&#x27;s on TSMC 5nm compared to the A17 Pro on TSMC 3nm. Try comparing the chips that use the same process. reply runeks 15 hours agorootparent> The A17 Pro gets 7199 on GB6 multi-thread compared to 18778 for the 7950X [...]But how many cores does the A17 have versus the 7950X? reply AnthonyMouse 14 hours agorootparentThat&#x27;s kind of the point -- the reason it uses more power is that it has more cores, out of which you get higher multi-thread performance. reply ksec 7 hours agorootparentExactly. If we have to explain this in every single discussion we may as well not discuss it. reply asabla 12 hours agorootparentprevAMD Ryzen 9 7950x: 16 cores and 32 threadsApple A17 Pro: 6 cores (4 efficiency and 2 performance)Refs: https:&#x2F;&#x2F;www.techpowerup.com&#x2F;cpu-specs&#x2F;ryzen-9-7950x.c2846 https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Apple_A17 reply SantiagoElf 14 hours agorootparentprevYeah, I don&#x27;t care about power consumption.There is no &#x27;serious&#x27; computers coming out of Apple for quite some time now - just glorified consumption devices :)Can Apple sillicon run Dota2@200fps or Visual Studio (Not CODE!) 2022+ as good as an x86? Can I put a 7900XTX on a Mac and run Windows natively ? reply Hiko0 6 hours agorootparentInteresting that you‘re writing about serious computers followed by referring to running games at 200 fps. It seems like you yourself don’t know anything about actual “serious“ use-cases. reply SantiagoElf 3 hours agorootparentThere is nothing more serious than gaming. reply ksec 7 hours agorootparentprev>p.s.: I don&#x27;t care about power consumption :)I have seen this over and over again and it is tiring. We are discussing ISA, uArch ( and possible Node as well ) in a ChipandCheese article. Not a CPU comparison &#x2F; gamer review site. reply k4rli 16 hours agorootparentprev7900X is pretty close https:&#x2F;&#x2F;browser.geekbench.com&#x2F;v6&#x2F;cpu&#x2F;2407099 reply SantiagoElf 14 hours agorootparentYes, when I was buying this PC beginning of the year I was consdering 7900X, 7950X and 7950X3D.At the end chose the 7950X, super happy about it.Absolute beast. reply gigatexal 16 hours agorootparentprevi do -- higher power consumption leads to more heat and noise and i hate that stuff i find it super distracting. reply Modified3019 12 hours agorootparentHere&#x27;s what I did years ago, and will never look back:1. Get a usb extender and a fiber optic displayport extender.2. Stick your computer in the garage, run the cables to whatever room and hook up your monitor and peripherals.3. Enjoy as perfect silence and low heat generation as can be possibly be achieved. (Monitors generate heat and sometimes electrical noise, and mice and keyboards are what they are) reply vluft 11 hours agorootparentyup. if you are picky about mobo selection, you can also do it all over one fiber thunderbolt cable; I have my desktop rackmounted and could go up to 165ft away for the desk. reply jwells89 12 hours agorootparentprevIt’s a problem with laptops especially, where massive coolers aren’t a possibility (at least if you want the laptop to be more than just technically “portable”). Even worse, tiny whiny fans are the standard there which means anything with much power at all will have times where its fans are screaming. Really annoying. reply tjoff 14 hours agorootparentprevCooling these CPUs silently is not a problem if you have room for a decent cooler. Which you typically do on a desktop. reply hu3 12 hours agorootparentprevIf you don&#x27;t know how to cool these processors silently, you&#x27;re not their target audience. reply gigatexal 12 hours agorootparentAll of these approaches are bandaids to a problem of big, fat, inefficient x86 cpus.I know how to cool CPUs such that they can run under load for days and weeks and be relatively quiet.The point is: the m series chips get you a crap ton of performance with utter silence. The m2 pro Mac minis — amazing. No fans whatsoever. reply hu3 8 hours agorootparentAgain, if m2 is enough for you, you&#x27;re not the target audience.Some professionals have workloads that require more multi core perf (not to mention proper Linux support). And running those dead silent is trivial. reply SantiagoElf 14 hours agorootparentprevBuy good headphones - Audeze, HiFiMan, Meze :) Btw my cpu is watercooled - very silent, I can barely hear it. reply vondur 16 hours agorootparentprevWhat if the A17 was clocked up to the same clock speed as the 7950x, do you think that the A17 would not score higher? reply brucethemoose2 15 hours agorootparentI don&#x27;t think thats even possible, as the A&#x2F;M silicon is specifically designed&#x2F;fabbed for lower clockspeeds. reply ksec 7 hours agorootparentThat is like saying every single Intel CPU cant be overclocked because they were not designed to do so. There certainly is a limit ( It definitely cant do 5Ghz ), but I am willing bet a 20% is easily achievable with enough cooling. reply _0ffh 15 hours agorootparentprevThe A17 is already on the TSMC 3nm process, while the 7950X is on TSMC 4nm. If anything, the A17 has the advantage here and the real question is, where the AMD chip would land if it was produced with the same technology. reply SantiagoElf 14 hours agorootparentprevA17 can&#x27;t be clocked at 4.5-5.0 GhZ. reply ladberg 15 hours agorootparentprev> p.s.: I don&#x27;t care about power consumption :)You should, considering it&#x27;s really the only bottleneck to getting more perf reply runjake 16 hours agorootparentprevnext [–]> I don’t care about power consumptionThat doesn’t change the fact that it isn’t on par. reply robotnikman 14 hours agoparentprevI don&#x27;t think anything will be able to compete with Apple as long as they continue to buy out all the capacity of the latest node offered by TSMC. Unless Intel finally gets their act together. reply _0ffh 14 hours agoparentprevBad comparison. It should look like this:Zen 4, AMD Ryzen 5 7640U, TSMC 4nm, Single Core GB6 Score 2340Apple A17 Pro, TSMC 3nm, Single Core GB6 Score 2914 reply madjam002 16 hours agoparentprevIf I wanted to build a home server with very good single core performance but very low power consumption (storage and upgradability aside), what are the best options right now? Probably a Mac Mini M2 haha reply deagle50 16 hours agorootparentOr a cheap Zen 3&#x2F;4 with ECC compatible motherboard. reply wtallis 11 hours agorootparentFor a home server, idle power consumption is probably more important than efficiency under load. Ryzen desktop processors are certainly better than most server platforms, but their chiplet-based design is quite bad for idle power. AMD has sold laptop SoCs packaged for their desktop sockets, but I don&#x27;t think those come with ECC capability enabled. reply smoldesu 9 hours agorootparentComparing the mobile 7840U[0] to the M2 Mac Mini[1] suggests it&#x27;s kinda a wash. Both idle around 6-7w, and under load they both max out around 50w.There are definitely less efficient chips you could bring into the comparison fold, but the M2 is a mobile chipset. You need another mobile chip if you want to draw more interesting power consumption comparisons.[0] https:&#x2F;&#x2F;www.notebookcheck.net&#x2F;Framework-Laptop-13-5-Ryzen-7-...[1] https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT201897 reply wtallis 7 hours agorootparentI think you lost the plot. Bringing up AMD&#x27;s low-end mobile processors in a discussion of whether their desktop processors are well-suited for a home server with ECC is pretty far off topic. reply zamadatix 14 hours agorootparentprevIf your goal is to get the best single core performance at a given wattage: pick any high end option (13900k, 7950x, M2) and just limit the power target&#x2F;available performance to the power level you want. I.e. there is a massive amount of power variance in getting dozens of cores to eek out slightly more multi-core performance but the power variance for single core performance at a given wattage is really not that much.If your goal is to go cheap but still have decent performance it depends on what exactly you need it to perform at but mini PCs like Beelink usually give otherworldly performance vs what you&#x27;d expect in the 100-200 dollar category. reply newsclues 16 hours agorootparentprevI have been interested in the low power home server space for a while.What I ultimately want, is a collection of SBCs that are able to be remotely booted so I can have Intel and AMD x86 systems, ARM, Risc-5, Apple that can spin up on demand. reply MrFoof 15 hours agorootparentIf you&#x27;re willing to look at 1L form factors (think Mac Mini), you do have options there.Intel has vPRO and AMD has AMD DASH for remote power on. A Mac Mini won&#x27;t have remote power on, but if you&#x27;re willing to just keep it slept, Wake-On-LAN is an option. reply ta988 14 hours agorootparentIf you are willing to mod it a little you can start the mac mini with a tiny solid state relay on an esp32. reply smoldesu 16 hours agorootparentprevDepends on your definition of \"very low power consumption\". The M2 Mac Mini will max out at 50w[0] from the wall, which is several times higher than a Raspberry Pi under load. You genuinely might have better luck with a laptop board in a mini-PC, depending on your particular constraints.[0] https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT201897 reply KingOfCoders 17 hours agoparentprevAMD seems not interested. Intel will go the Apple way though, simpler cores and trading die area for ever larger caches, also on-die memory, downside you can&#x27;t upgrade memory, upside it&#x27;s faster and more power efficient.[Edit] Or someone could have said: I think you meant processor not die, and I would have said: Yes sure, not sure what was on my mind. reply kllrnohj 16 hours agorootparent> Intel will go the Apple way thoughHmm, curious what the \"Apple way\" is here..> simpler coresApple&#x27;s cores are very far from simple. They are much larger than the other ARM CPUs on the market, and this is also why they are faster than other ARM CPUs on the market.> trading die area for ever larger cachesApple has big L1 comparatively, but L2 & L3 are nothing special. Certainly nowhere close to the massive caches that AMD&#x27;s X3D offers.> also on-die memoryApple doesn&#x27;t have on-die memory. AMD is the only company to have shipped on-die memory to consumers (they were also the first to ship it), and they only did so on GPUs so far. Nvidia & Intel are playing with it for datacenter products, though, but that&#x27;s about it (H100 & Xeon Max respectively) reply fanf2 12 hours agorootparentThe Xeon Max memory is in the package but not on the same die. The package contains 4 CPU dice and 4 stacks of HBM dice. reply lunfard000 16 hours agorootparentprevI think simpler cpu meant to simplify x86, like the x86-s proposal [0]0: https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;articles&#x2F;t... reply paulmd 16 hours agorootparentprev> Apple doesn&#x27;t have on-die memory. AMD is the only company to have shipped on-die memory to consumersWhat distinction are you drawing such that Fury X interposers count as “on-die memory” but apple’s literal stacking of the LPDDR on the actual die doesn’t?Apples technique is more advanced than fury actually: fury is just on-package, not on-die, but apple is literally doing v-cache style stacking of the whole memory subsystem right on top of the die.Of course fury x was earlier, but die stacking is a more advanced technology, there is a reason AMD didn’t do that for 5+ years after fury x. And apple actually beat AMD to market with on-die stacking, by several years (epyc v-cache was 2022 or 2021) with a more advanced implementation (the whole memory subsystem rather than just v-cache). reply undersuit 15 hours agorootparentAFAIK Apple only uses a silicon interposer to connect the CPUs. The memory is on standard interconnects through the package like AMD&#x27;s chiplet approach or the ancient Intel Clarksdale. reply kllrnohj 11 hours agorootparentprev> but apple’s literal stacking of the LPDDR on the actual die doesn’t?There&#x27;s pictures of Apple&#x27;s silicon all over the Internet, didn&#x27;t think to bother even looking at one?https:&#x2F;&#x2F;cdn.mos.cms.futurecdn.net&#x2F;EWuFBHNeGSfhjJfoWFxL7D-102...It&#x27;s very obviously not on die. On package maybe, but definitely not on-die.Since you&#x27;re talking about die stacking i assume you&#x27;re now talking about their \"low end\" phone parts. That&#x27;s a quite different technology from V-cache, foveros, or even Apple&#x27;s \"UltraFusion.\" It&#x27;s also not at all a unique technology to Apple, it&#x27;s very common in the phone SoC space where power consumption is so limited that the thermal issues are mostly negated. reply wmf 16 hours agorootparentprevThat&#x27;s not how it works at all. Apple has nothing similar to V-cache. reply paulmd 15 hours agorootparent> Apple has nothing similar to V-cache.Thanks for the second sentence.AMD’s consumer gpus don’t use v-cache. Apple is using the same memory-on-package technique that fury x and Vega use. Rdna3 cache is not v-cache, just another flavor of mcm. reply wmf 15 hours agorootparentNo, HBM on silicon interposer is not the same as LPDDR on an organic package. One&#x27;s an order of magnitude faster than the other. reply paulmd 16 hours agorootparentprev> That&#x27;s not how it works at all.What isn’t how what works? Use your words.(People literally are in such a race to pick a fight and defend their best friend AMD they forgot to make the rest of the post lol. Simply say the most mild thing and AMD people trip over themselves to do this stuff constantly, it’s funny.)Do you think OP is referring to something other than fury x&#x2F;Vega when they said “gpu shipped to consumers”? That’s the only thing that comes to mind, literally there are only less than a half dozen consumer gpus from AMD in the stacking era so I don’t know what else it would be.If you mean true memory in the die, not stacked, not just a cache - AMD has never shipped that to consumers (or professionals) in a gpu. Nobody puts memory in the gpu because it would be a waste of die space.But like, gotta defend AMD, better hammer that downboat and drop a pithy six-word&#x2F;single-sentence reply. Social media is constantly seething with the AMD defense force looking for fights, gets super old.Like, it was literally less than 30 seconds from clicking post on my comment to seeing your debate determinating cliche. What fine discourse we have here. reply appplication 16 hours agorootparentprevUpgrading memory is great in theory (and practice for those who do) but I think for 95% of users this is not a consideration. reply kibwen 16 hours agorootparentIt becomes more of a consideration when accounting for the fact that you can buy a Macbook that ships with an un-upgradeable 8 GB of RAM, which, sadly, is deficient for even moderate tasks these days. reply toast0 16 hours agorootparentprevUpgradable memory also makes things more flexible for OEMs.Most OEMs continue to sell systems built around last years processor (or even a couple year old processors), and they can increase the memory in those systems as needed. Maybe 4GB was enough when they came out, but now you need 8GB or 16GB to sell a system with an older processor. Not a big deal with socketed RAM, not even a huge deal with soldered RAM, but the processors with 4GB on package become very hard to sell. reply throwaway2990 16 hours agorootparentprevCouldn’t you have on die memory and then have a swap using traditional memory rather than the SSD? reply TheRealPomax 15 hours agoprev [–] It&#x27;s not a leak unless the leaker proves its origin. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A YouTuber leaked slides regarding AMD's upcoming Zen 5 architecture, hinting at various improvements including an enhanced branch prediction, basic block fetch, and memory subsystem, and a larger L1 data cache.",
      "According to the leak, Zen 5 could feature a more extensive cache size, DTLB capacity, and PWC size for better memory access performance, as well as a larger, more unified scheduler, expanded op fusion capabilities, a bigger structure size, six ALUs, and better integer throughput.",
      "Despite the exciting leak, the article advises caution as the final design may vary, emphasizing the need to compare performance gains with other CPU manufacturers and not to rely heavily on potentially distorted early performance numbers."
    ],
    "commentSummary": [
      "The discussion thread tackles various processor-related topics, such as the leak of Zen 5 slides, Apple's strategic implementation with SOC and RAM, the strengths and weaknesses of x86, and how hyperthreading affects performance.",
      "It also delves into the constraints and compromises involved in advancing CPU memory bandwidth, the application of optical interconnections, and the difficulties in optimizing memory access patterns.",
      "Notably, there's a comparison between different processors - Apple A17 Pro and AMD Ryzen 9 7950X, backed by discussions on power consumption, performance, critical factors when upgrading memory, understanding the significance of memory for different users, and potential advantages of on-die memory."
    ],
    "points": 201,
    "commentCount": 143,
    "retryCount": 0,
    "time": 1696783766
  },
  {
    "id": 37815945,
    "title": "Impacts of Lack of Sleep",
    "originLink": "https://belkarx.github.io/posts/finished/Impacts%20Of%20Lack%20Of%20Sleep.html",
    "originBody": "home/about-meposts Impacts of Lack of Sleep - rather bluntly listed for those that need to hear them - Sleep is important. People vastly underestimate the extent to which not sleeping is harmful, viewing it as a valid trade off for productivity and overlooking all of the longterm negative effects. Said Effects Being Metabolism slows down, appetite goes up Depression, anxiety, irritability, distress Increased risk of diabetes More substance abuse/addiction (alcohol and caffiene, specifically) [ironically both depressants and stimulents] Lower life expectancy Lower cognitive performance (specifically in reasoning/logic) Long and short term memory issues Increased (+33%) risk of dementia which is something you should be terrified of Fatigued and demotivated Lower libido Impact of Those Being You get fat You might enter a state where consumption of sugar has to be stringently regulated and have to do annoying blood checks which is expensive if you’re in country with a questionable private healthcare system No one likes talking to you or being around you in general because you become asshole-ish and/or a loner You have to deal with expensive and unhealthy addictions More likely to die early! more stupid - can’t unlock your full intelligence and cognitive prowess, so you’re stifling potential Memory also worsens, see above Ooh that decreased cognitive state has a higher chance of being permanent and getting much much worse, making your life miserable You lose all ambition so start following others. you also feel dead inside all the time and just want to sleeeep to top it all off can’t sexually pleasure your partner to their level of desire, leading to strained relations (even more strained, since the aforementioned negative effects on personality likely contribute as well) so they are less useful in helping you deal with all of the above issues. Discussion on Hacker News here and here Sources harvard study[p1] harvard[p2] nih journal of medical health",
    "commentLink": "https://news.ycombinator.com/item?id=37815945",
    "commentBody": "Impacts of Lack of SleepHacker NewspastloginImpacts of Lack of Sleep (belkarx.github.io) 197 points by belkarx 8 hours ago| hidepastfavorite122 comments 000ooo000 6 hours agoFor me, I find my sleep suffers because of what some describe as \"revenge bedtime procrastination\". When my job is shit (i.e. now), I&#x27;m more inclined to stay on the PC later doing things that bring me some level of joy, or play games to escape the mind spinning its wheels. I seem to more easily recognise that my job is unfulfilling because I&#x27;m staying up later or I&#x27;m dreading going to bed. Going to bed early for a good night&#x27;s sleep seems a lot easier a habit to build when my day-life is going well. reply xyzelement 6 hours agoparentQuick advice - see if you can shift that to getting up earlier. You will probably spend the couple of hours in the morning in a way that sets you up for a better day vs staying up late sets you up for a bad tomorrow.I say this as an always \"late\" person who now tries to get up at 5 to work out and reflect, it&#x27;s been very helpful. reply plorkyeran 4 hours agorootparentA life changing thing for me was specifically scheduling time to do fun things in the morning before doing the things I have to do. If I wake up at six, I still don’t start my day until eight, and instead spend those two hours on things I otherwise would have done the night before. This broke the feeling of dreading mornings that lead to a negative spiral. Even if I know most of the day will be awful, if I can look forward to my me time in the morning I can convince myself to go to bed. reply TeMPOraL 2 hours agorootparentWish this worked for me, but my brain straight up ignores[0] any kind of alarm clocks, music, light, etc. until it&#x27;s something like 10:00 in the morning, or until a person wakes me up (and checks 5-10 minutes later if I&#x27;m actually up)[1]. So, shifting time to morning turned out to be impossible to maintain when I was living alone - I could hold such schedule for maybe a week at most, gradually oversleeping more and more. Still, it was possible then and I wish I tried harder, because now it&#x27;s strictly not - I have two small kids, and one true thing about them is that they always wake up earlier than you[2]. So there&#x27;s no space for personal time in the morning.Also I wonder if I&#x27;m the only one here who feels that their mind takes some 2&#x2F;3 of the day to get into high gear?.It&#x27;s probably more about emotional balance than cognitive functions, but I feel like I&#x27;m best able to focus only from late afternoon onward, making late night the only time that&#x27;s useful for anything.--[0] - I tried all kinds of things. Music blaring from the speakers at 06:30 sharp? Within a week, I&#x27;ve mastered getting up, hitting the power button on the amplifier, and falling back to bed in one, smooth move, with ballet-like fluidity. Got an alarm clock that requires you to solve math problems to turn it off? I quickly realized I must&#x27;ve mastered the art of multiplying 2-digit numbers in memory while unconscious, because I would wake up at 11:00 to discover multiple such alarms disabled, with no memory of doing it, or even hearing them ring.[1] - Yes, that means business trips are especially hard for me. When traveling alone, I rely on wake-up calls from my wife (and check-ups afterward). Even with that, I still almost missed a plane once.[2] - Except when there&#x27;s an important appointment coming up that we need to get up unusually early for - on those nights, they&#x27;re near-impossible to wake up. reply abdusco 43 minutes agorootparentDid you try sleeping earlier? If you regularly sleep at 01:00, then you won&#x27;t be able to wake up at 06:00. If you go to sleep at 22:00 then it&#x27;s not as impossible as you think. reply TeMPOraL 8 minutes agorootparentYes. If I go to sleep at 22:00, I&#x27;ll be half-awake tossing in bed from 6:00, but still unable to get up until ~10:00. reply jmkd 53 minutes agorootparentprevI wish people would understand that \"just get up early\" simply does not and cannot work for a significant portion of the population (20-50% depending on which study you choose).Therefore it is aggravating advice to constantly receive, from those for whom it worked, to those for whom it won&#x27;t. reply Zenul_Abidin 41 minutes agorootparentprevThese days I sleep at around 8, and it made it much easier for me to get up at 5 and still get 8 hours of sleep. And then I have energy to do basically anything in those early hours.If I even get just 6 hours, I immediately feel crankiness the next day. reply dgb23 2 hours agorootparentprevI agree with this. I struggled the same thing as GP mentioned, and worse. And from my experience it’s a vicious cycle.I don’t necessarily agree with the specific “get up at T”, but generally working towards getting rid of bad sleeping habits (and nutrition etc.) is incredibly beneficial. Seemingly small improvements can have a positive snowball effect.Negative circumstances (like “my job is shit”) can be overcome more easily the more energy you have.Seems trite, but it’s trivially true. My mental state improved gradually and I gathered more courage, got more resilient and more creative.I’m much happier, helpful and have much more impact on my life and my surroundings than ever before. And it all started with the basics. reply noufalibrahim 1 hour agorootparentprevWas going to suggest the same thing. A great way to break the cycle is to somehow wake up early one day and force yourself to not sleep. You&#x27;ll feel very sleepy early in the night and will fall asleep. reply oulu2006 5 hours agorootparentprevThat&#x27;s some nice advice, thank you. reply oulu2006 5 hours agoparentprevThis^, I&#x27;m very much the same.Going to sleep feels like it brings the next day, and if you aren&#x27;t looking forward to what you have to do the next day....you put it off for as long as possible.I feel you. reply J_cst 4 hours agorootparentI also procrastinate bed time. I&#x27;ve always linked it to anxiety (non necessarily in strict medical terms&#x2F;meaning). That&#x27;s my rationale: In the evening the day&#x27;s over and &#x27;nothing can happen&#x27; anymore for that day, so I feel I can chill out. On the contrary, tomorrow &#x27;anything has yet to happen&#x27; and I can have a feeling of worry. That&#x27;s the reason I&#x27;ve given myself to procrastinate bed time. Not sure of that&#x27;s the case or I&#x27;m just a night owl (possibly that&#x27;s the case as per Occam&#x27;s). Anyway, I feel you. reply TeMPOraL 1 hour agorootparentThis is me. For me, it&#x27;s not about the next day, but exactly about that \"nothing can happen\" feeling - everyone is finally asleep, no one has any expectations towards me, nobody will be calling me (it&#x27;s night, it&#x27;s rude to call people at night), nor will they be texting me (they&#x27;re normal people, they&#x27;re already in beds, sleeping). It&#x27;s the only time my body is able to relax a little. It&#x27;s the only time I can think about things, without interruptions and the nagging feelings I should be doing something else.I tried, I tried really hard to give it up, but I can&#x27;t. Now, with small kids in the mix, the mornings are not even an option.This is what \"revenge\" in \"revenge bedtime procrastination\" is about. That feeling that it&#x27;s the only time that&#x27;s actually yours, the only moment of actual autonomy in your life. Everything else is driven by others - working a job, running errands, being there with your loved ones. When it starts feeling like an unending stream of obligations, those few hours late at night are a form of defiance, showing the middle finger to the universe, reclaiming some time for yourself. reply loveparade 5 hours agorootparentprevI&#x27;m the same too. I know it doesn&#x27;t make logical sense because it starts a bad spiral, but my body (gut?) is trying really hard to procrastinate sleeping because it doesn&#x27;t want the next day to come. Rationally speaking I know what I should be doing, but it feels like my mind is not strong enough to fight the emotional&#x2F;gut instinct. reply m463 51 minutes agoparentprevI created a bedtime nag program and it really helped, especially with hard deadlines like driving someone in the morning.2 hours to bedtime, 1 hour to bedtime, 30 minutes, etc.... reply otikik 18 minutes agorootparentCan you expand on that? Is that just a bunch of alarms in your mobile phone, or something more ellaborate? reply lloeki 2 hours agoprevThe post is mostly covering \"mind\" and \"metabolic\" things, but the effect extends to \"body\" things:- more likely to avoid hurtful situations (lower attention, zone out events, micro sleep events, reflexes diminished).- more likely to hurt yourself when such accidents occur- once hurt, physical recovery is severely hamperedCar accidents are the obvious ones here but it extends to simple things like accidental cuts with knives, slipping in the tub, or sprained ankles or knees just walking down stairs or curbs.And some other \"mind\" and \"metabolic\" things:- immune system effectiveness plummets, so more chances to get sick- REM deprivation causes neuronal death- long term, leads to anxiety, false memories, paranoia, hallucinations, and psychosis (been there, done that; not a good place to be)We all know that water is essential to not dying, but sleep appears to be on equal footing (3 days without either and you&#x27;re in catastrophic states), maybe even more important (the - sad - record of a human surviving without water is 18 days, for sleep it&#x27;s 10) reply iagooar 29 minutes agoprevIf I could go back 10 years and talk to myself from the past, I would definitely tell myself: get away from that screen from time to time, especially on weekends. Buy a cheap 4x4 and GET OUT THERE. Get exercise, get fresh air, get to see the world through your own eyes instead of through a 2D screen, get physically tired, AVOID if possible any alcohol intake (this is a whole different topic, but alcohol might destroy your sleep, even if you only drink occasionally).I wish I had done it 10 years ago, when my sleep quality and quantity were just depressingly low. These days even with small children I have better quality of sleep than I used to have back then when I would spend hours and hours in front of a computer screen (at work and after work as a hobby). reply kungfufrog 6 hours agoprevResonates with anyone that has had children. Biggest impact for both my wife and I was overall sleep quality&#x2F;quantity and sacrificing sleep for a modicum of personal time at the end of the day. reply charles_f 3 hours agoparentFor some time I was driving to the office (or my clients&#x27;), and almost everyday taking a nap in it after dropping the kids at daycare. When we got a minivan I even sneaked a comforter in it. I was going at the lowest level to avoid being seen, and took an extra half hour sleep. It made all the difference. Yet I felt shameful.I remember the sensation of feeling drained of all energy at 8am, after battling to get kids through the door.Time plays in your favor. It gets easier, much faster than you think. reply madaxe_again 4 hours agoparentprevMy wife still manages 12+ hours per day - I get about 4-6. She wonders why I am gradually becoming irrationally irritable, as in her view being overtired is something that is easily controlled. Easily said when you get to sleep in to 1pm or later every day.I find myself picking fights with anyone and everyone, even inanimate objects. I know it’s crazy.We don’t have time together any more - I just fall asleep the moment the child is asleep, as I know I’ll be getting up several times in the night and before the birds in the morning, and I just can’t keep my eyes open beyond a certain point.The flip-side is that she feels that she spends her every waking hour looking after the baby - which isn’t far off the truth, as I look after her while she sleeps - so nobody is happy. My spare time is spent working and building and repairing stuff at home.I cannot for the life of me understand why anyone has more than one child. How can anyone go “that was fun, let’s do it again”.This post was brought to you by Sleep Deprivation™ reply mythhabit 2 hours agorootparentIf she gets to sleep through the night, she gets to wake early and take care of the child while you get a few hours uninterrupted sleep. Sleep is as important for your mental health and longevity as exercise is for your physical health and longevity.I am a heavy sleeper too, so the agreement my wife and I had, was that she attend her son at night, he needed to be breastfed anyway, while I got to sleep. Then I was up at an ungodly early time and took care of my son while she could get some uninterrupted sleep. We did this for both of our children.Her being pregnant is not an argument in favour of you having to do everything at night. Perhaps in the first few weeks while she recover, but after that, it&#x27;s shared in a reasonable way. She does not get to dictate, but in unison you get to find a happy compromise, which includes enough sleep at regular intervals for both of you.If you are willing to listen, listen to this: You are well on your way to a roommate situation, the first and biggest step towards a divorce. If you do not correct that course, both of you, that is with all likelihood what will happen. This path of a little bit of resentment many times, is a relationship killer. reply pfyra 3 hours agorootparentprevSounds like you need to share the night shift. It isn&#x27;t sustainable to work all day and have every night&#x27;s sleep ruined like that. It&#x27;ll get better as the baby sleeps longer and longer, but chances are that you&#x27;ll get burned out before that unless you get to sleep better. We had separate beds so one of us could sleep undisturbed every night. reply madaxe_again 3 hours agorootparentYeah, we have separate beds - she sleeps in what was our room, I sleep on the couch, so I don’t disturb her when I get up to tend to the bairn.She, I feel reasonably, points out that it was not me that spent 10 months being pregnant with our child - and she sleeps extremely soundly, to the extent that she will sleep straight through screaming and crying more often than not, which means I have to be on alert, as I never do much more than a shallow, nightmare-filled doze.To be honest, it’s not much different to the worst periods of technical hell for my startup back in the day, when I would be yanked out of bed repeatedly to deal with Situations while being yelled at by clients.Now my client is eight months old, and is actually considerably more patient than many of the adults I used to deal with. reply hereonout2 3 hours agorootparentI might be wrong, but sleeping in till 1pm with an 8 month old baby sounds like maybe something else may be going on.My partner had a very difficult birth that has led to a series of mental health issues. I didn&#x27;t spot these for months initially, and it took a lot for it all to come to a head and out in the open.It was easy to miss the issues with a child to care for. reply desmosxxx 3 hours agorootparentprevI&#x27;m sure you helped out while she was pregnant as well? Was she sleep deprived during pregnancy? This doesn&#x27;t sound &#x27;reasonable&#x27; to me. reply VanTodi 3 hours agorootparentprevin 2 months she has no excuse anymore. as a father of a 1 1&#x2F;2 year old, i can relate to you. i wish you all the best. reply sshine 3 hours agorootparentprevI’d penguin up like you, too.But it sounds like if you found nap time during the day, you’d slip right into it. reply desmosxxx 3 hours agorootparentprevWe have 3 kids and it was never as hard as you&#x27;re describing, so that probably has something to do with people wanting to have more than one kid.I don&#x27;t want to overstep, but you guys are doing it wrong. Your wife sleeping 12 hours doesn&#x27;t sound right to me. We took shifts, but my wife was going to bed very early (9pm) and waking up around 5 or 6pm and then I&#x27;d sleep. She did this so we had a solution that worked for both of us. If your wife is not willing to do that, then you should be swapping nights at the very least.Your wife sleeping at midnight and waking up so late is not sharing the workload at all and is frankly abusive IMO. I&#x27;d also look into postpartum issues since sleeping 12 hours is not normal. reply refurb 2 hours agorootparentMeh.. couples just need to find what works for them. There is no one right approach.I did nights for all my kids. Unlike my partner, I can fall asleep almost instantly, no matter when I&#x27;m woken up. My partner, if woken up at 3am, is up for the rest of the night.So I did nights. Wake up, feed, change diaper, rock back to bed, fall asleep right away, then rinse and repeat 2 more times at 2-3 hr internals.Though I was sleep deprived, it wasn&#x27;t that bad and still went to work each day. My partner had the kid during the day, and it just worked for us. reply madaxe_again 3 hours agorootparentprevI dunno - single parents manage it somehow.Probably doesn’t help that we live off-grid in total isolation, in a country where childminding doesn’t really exist, as most folks here have multi-generational households, so there’s no demand. reply desmosxxx 2 hours agorootparentWe didn&#x27;t have nearby family either so can understand how hard that is. Good news is it should start getting easier soon. Good luck man and demand some help if you&#x27;re absolutely at your breaking point. Everyone needs sleep. reply sshine 3 hours agorootparentprevPeople who get >1 kids usually don’t sleep 12+ hours per day.My wife will sleep 10 hours. I will usually sleep 6-8 hours. It means I always have at least 2-4 hours to myself in the morning. reply tayo42 3 hours agorootparentprevShe sleeps more then 12 hours a day? reply madaxe_again 3 hours agorootparentYes. I don’t know how, but she always has - bed at midnight, and she rises anywhere between midday and 4pm. I envy her amicability with Morpheus. I used to be able to sleep a decent amount, but I burned that part of my brain out during my startup days - I get by ok on six uninterrupted hours, but when my night is more like three two hour naps punctuated by stress, I start to disintegrate - as I learned in my startup days. reply e40 2 hours agorootparentThat is 12 to 16 hours, not 12. Something is wrong here. I wish you well. reply refurb 2 hours agorootparentprevWell the sleep deprivation stage doesn&#x27;t last that long (although it seems long when going through it) for most babies.And in terms of one child, your brain (and evolution) does a great job at helping you forget and filter out only the positive experiences of a newborn.Plus once you&#x27;ve had one, you don&#x27;t stress nearly as much about the subsequent ones which taxes your brain a lot less. You don&#x27;t feel quite so exhausted by it and you know it doesn&#x27;t last forver. reply p-e-w 5 hours agoprevI&#x27;m highly skeptical concerning the claimed &#x27;effects&#x27; of lack of sleep. I don&#x27;t doubt those are correlated with lack of sleep, but for many of them, obvious confounding factors exist.As a trivial example, overworked people typically get less sleep than average folks. And excessive work is also correlated with several of the &#x27;effects&#x27; from the list, such as \"fatigued and demotivated\" and \"lower libido\". This means that even though lack of sleep is (indirectly) linked to those problems, getting more sleep won&#x27;t necessarily fix them, because the real cause is something else.I find it strange that a post explicitly addressed to HN readers fails to even mention this extremely obvious issue. reply topkai22 5 hours agoparentSome of the effects may not be casual, but there is substantial evidence for many of the cognitive effects in sleep deprivation studies, which just introduce a lack of sleep. There is probably good evidence for the other effects as well, although I’m not immediately familiar with it.While other issues may be also playing a role or even be the primary cause, the physiological effects of sleep deprivation are likely to make almost any underlying cause worse. reply randomcarbloke 1 hour agoparentprevfor sure, I set the Set Daily Puzzle world record during a time of my life when I had been subsisting on an average of 1-2 hours sleep a night, if that.It&#x27;s also the fluffiest thing I&#x27;ve seen on HN in a while. reply Barrin92 22 minutes agorootparentit reminds me of Why We Sleep which turned out to be completely full of scientific errors (https:&#x2F;&#x2F;guzey.com&#x2F;books&#x2F;why-we-sleep&#x2F;)Obviously that book sold like hot cakes. When it comes to sleep and diet people will repeat anything as long as it sounds intuitive. reply charles_f 3 hours agoprev> Depression, anxiety, irritability, distressI&#x27;m now at a point where I manage to take a step back when I feel generally down, and just look into whether I had enough sleep, or if the weather is crap. These are the two most important predictors for whether I&#x27;ll find my job to be terminally depressing, or my situation to be doomed ; and I try to convince myself to take my distress with a fistful of salt then, and wait to be less tired or the weather to get better, before I overanalyze my situation.It does not always work, but sometimes it helps a little with rationalizing reply l5870uoo9y 21 minutes agoprev> More substance abuse&#x2F;addiction (alcohol and caffiene, specifically) [ironically both depressants and stimulents]Is caffeine really a depressant? I have great joy in consuming caffeine either from Japanese Sencha tea, Matcha, or occasionally a non-watery Americano. reply loa_in_ 1 minute agoparentIt&#x27;s not, it&#x27;s neither IIRC. reply hannofcart 6 hours agoprevA key thing to note is that if you have sleep apnea, even if you think you&#x27;re getting 7-8 hrs of sleep, you likely aren&#x27;t.If you (or likely your partner) suspects you have apnea, talk to your doctor.Doing a sleep study&#x2F;test and getting treatment for apnea will likely transform your life for the better. reply justinator 5 hours agoparentRelieving my sleep apnea has been - by far - the biggest impact to my health and well-being. It&#x27;s like a new lease on life. Just took a $60 retainer from Walgreens (this won&#x27;t work for everyone). Memory, word recall, mood, energy - you name it: just much better than before. It was a slow process to get below what I would call normal (to say nothing of, \"optimal\"). It snuck up on me and I just never understood how terrible I was while just trying to live \"normally.I would compare it to having a fairly robust drinking problem, and then finally getting sober. reply all2 4 hours agorootparentFor me it has been nasal strips. That and not sleeping on my back (where my jaw relaxes and closes my throat). reply abrichr 5 hours agorootparentprevCan you please clarify which retainer? reply operatingthetan 4 hours agorootparentLooks like a common one is called \"SnoreRX.\" Lots of random stuff for the same purpose on Amazon. reply justinator 3 hours agorootparentCorrect, but I’m not here to sell you on that brand or say it’ll work for everyone. But for me it is very effective. I can actually have a relationship with someone (and sleep next to them at night!) reply ToValueFunfetti 5 hours agoparentprevShockingly prevalent too, especially if you&#x27;re male, and gets more common with age. ~90% of elderly men and ~80% of elderly women have apnea (I point to these stats because they&#x27;re higher confidence, but somewhere between 1&#x2F;20 and 1&#x2F;3 among the general population). reply c7DJTLrn 38 minutes agoprevReading this after staying awake all night.I wish I had control over my sleep like others seem to. It would be awesome if I could pass out at 10pm and wake up 8am. That kind of schedule is just impossible for me though. Night time is when my brain kicks into gear and then when I finally do sleep, I find it extremely difficult to wake up.These days I&#x27;m sleeping between roughly 7am to 4pm. Recently I slept 18 hours. If I could fix it somehow I would. It&#x27;s completely out of my control. reply l5870uoo9y 24 minutes agoparentI find difficulty sleeping also depends on what I read before going to sleep. If I read books I sleep much easier and deeper than if I browse Reddit. I think the alarmism, sensationalism, and aggression triggers the nervous system and fills the mind with worthless thoughts which in turn prevents the falling a sleep. reply jesterson 24 minutes agoparentprev> It&#x27;s completely out of my control.It is not. I have been in your shoes not once and lots of recommended things failed, some have not. This will require deep dive into reasons and mitigations, but I suggest you to proactively explore it and hopefully - fix. reply iagooar 24 minutes agoparentprevMaybe a good start could be not giving your brain what it wants, when it starts getting that active. Just let yourself become bored.And if possible, get outside, walk, run, hike, enjoy nature, build something with your hands, get TIRED.Reduce blue light and if possible remove any screens past 7-8pm (books, podcasts and audiobooks help a lot here!).In the morning, get as much sunlight as possible.Do not stress over your current schedule too much. Try to wake up early in the morning for a few consecutive days, even if you do not go to bed early enough. After some time, your body might adjust again to a normal rhythm.Good luck reply mmanfrin 6 hours agoprevI&#x27;d wager most people here who don&#x27;t get enough sleep aren&#x27;t doing so out of some sort of grindset mentality, but because they simply cannot. I find it very hard to fall asleep, nearly impossible to sleep in (a consequence of getting older?), and more interrupted sleep. I&#x27;m reticent to try drugs to enable sleep because I&#x27;ve heard that the sleep they provide is not true sleep. reply notsurenymore 6 hours agoparentI have this pattern of getting really tired in the early afternoon, and then by night time, I would be fully awake. I couldn’t just sleep when I was tired because then I won’t sleep at night, and it just become this terrible cycle. I can’t stay awake at night because things need to be done during the day.It’s been better recently, but some years ago, I would come home from work just completely exhausted and pass out immediately. I would only sleep for a few hours, but then wouldn’t get tired again until 4-5am at which point I often would just power through it because I’d have to get ready and go to work in a couple hours anyway. It peaked a few years ago when I completely wrecked my sleep and didn’t sleep for a few days straight. I remember waking up, thinking about something I believe was just utter nonsense, and a second later being unable to recall what I was even thinking about. I ended up getting my sleep schedule back by cycling some light stimulants during the day and crappy sleep aids at night until I was back to a normal schedule, but that probably wasn’t very healthy itself. reply SoftTalker 6 hours agorootparentI&#x27;ve always been sort of a \"night owl\" I can stay up to 2 or 3 am pretty easily and like sleeping until 10 or 11am. I seem to get my best \"deep\" sleep between 6am and 10am. reply notsurenymore 6 hours agorootparentYeah that sounds similar to what my sleep schedule has been since I’ve been out of work. Prior to that I was doing mostly 12a-8a, but I was working remotely so it was easier.I do enjoy the night, it’s cooler and calmer, sadly there’s just not much that can get done. I remember years ago working a night job, where I would often work very long hours, but I would get accusations of being lazy from people who just saw me as sleeping all day. reply nikkwong 6 hours agoparentprevThat may not necessarily be totally true, you&#x27;re most likely referring to hypnotics (ambien, lunesta, etc)—which were used by doctors for patients in some circumstances but are slowly falling out of fashion for the reasons you describe.There are older and newer classes of drugs on the market that are being used as first-line therapies which not only preserve sleep architecture but in many cases even enhance it. Trazodone (a very old SARI) for example increases the amount of slow-wave sleep in healthy individuals and amount of time spent asleep. There are many newer drugs, suvorexant, ramelteon, etc. These medications are of course not without side effects but for many patients they work very well.I&#x27;d say if you&#x27;re suffering from chronic sleep deprivation then the cost-benefit analysis strongly leans in the direction of a pharmaceutical that fixes that since sleep is second to none in improving health and wellness outcomes.There are many insults that can cause disrupted sleep architecture like you&#x27;re describing—impaired gut microbiota, stress, lack of exercise, etc—all of which may respond to their own remedies. I experimented for years and finally have wonderful sleep with desmopressin + trazodone. And I was at a place where my sleep was seemingly uncureable—waking up every hour or every few hours on most nights.If you want great sleep, you might have to fight to find the correct path for your own situation. reply Obscurity4340 21 minutes agorootparentHow did you come across desmopressin, isnt that a bed-wetting thing? How does it help with sleep outside of urinary issues? reply MarkusQ 6 hours agoparentprevPhysical labor. Or exercise. If you push yourself to the point where you&#x27;re physically tired, you&#x27;ll sleep like a rock. If it doesn&#x27;t, you&#x27;re either not pushing hard enough or you ought to see a doctor. reply FirmwareBurner 6 hours agorootparentHeavy exercise, like lifting weights after work(5PM), makes me fall asleep eve harder.I can still feel the adrenaline and elevated heart rate and body temperature that just makes it even more difficult to fall asleep so I have no idea how people who go to the gym at 9PM manage to fall asleep. I wish I knew their secret or had their genetics.Lighter exercises that don&#x27;t stress the body, like long walks and yoga are much better for sleeping but not good for building muscle.My problem isn&#x27;t falling asleep, it&#x27;s staying asleep for long enough to feel rested. I usually wake up after 4-5h and can&#x27;t easily fall back to sleep. By the time I get sleepy again(7-8AM) it&#x27;s already morning and time to get up for work.So far I haven&#x27;t found a (natural) cure for this. There&#x27;s medication that aids with getting long sleep but the side effects (drowsy ness and brain fog) are nearly just as bad as the lack of sleep. reply yahmez 5 hours agorootparentHave you experimented with sleeping while fasted at all? I have heard a few people say they get their best sleep when sleeping on an empty stomach.I also find white noise and a sleep mask have helped a fair bit. I’m a very light sleeper so little background noises wake me easily, so having white noise on helps there. And wearing a sleep mask notably improved my sleep as our bedroom can get quite bright (even with blackout curtains).Anecdotally though, the most important factors for me are sun exposure (particularly morning and evening), weight lifting (or exercise in general), and cutting out caffeine. reply vbarrielle 2 hours agorootparentprevHeavy exercise should be avoided in the evening, it&#x27;s best to do it early in the morning or during your lunch break if you can. reply mmanfrin 4 hours agorootparentprevMy issues sleeping have gotten worse in the time that I&#x27;ve started to seriously weight train. I am having nights where I&#x27;m so exhausted I could cry and still have a hard time getting steady sleep. reply voidhorse 6 hours agorootparentprevI do think sufficient physical activity helps, but timing is important. I find that if I work out too late in the evening remnants of adrenaline are still coursing by the time I try to go to sleep and actually hamper, rather than help getting to sleep. reply dalyons 5 hours agorootparentprevLove me an over simplified solution for a very complex problem. reply jks 4 hours agoparentprevIf you don&#x27;t like drugs, look up Cognitive Behavioral Therapy for Insomnia (CBT-I). It is kind of a collection of tricks, but it goes further than your basic sleep hygiene checklist. One thing it includes that helps many people if they can do it is the Spielman sleep-restriction protocol: https:&#x2F;&#x2F;stanfordhealthcare.org&#x2F;medical-treatments&#x2F;c&#x2F;cognitiv... reply atmosx 6 hours agoparentprevAre you physically tired when trying to sleep? Without a level of physical activity is next to impossible to achieve a good night sleep. reply mmanfrin 4 hours agorootparentYes, I&#x27;ve actually been more physically tired than usual lately (training&#x2F;gym which never used to be part of my routine now are). Last week I was dead tired and had exercised for hours and I still had trouble sleeping. reply topkai22 5 hours agoparentprevYou may want to consider a short course of sleep aids to see if it can help adjust your schedule. This is unscientific, but I’ve found that if I’m dealing with persistent insomnia, taking something to help me fall asleep when I want to can lead me to “readjust” to the new schedule.I used this the most when traveling and dealing with jet lag, but I still occasionally find it useful to take an OTC sleep aid when my schedule gets “off.”Interrupted sleep is tougher. I’ve had some success by just accepting that if I haven’t fallen back asleep in 15-20 minutes I probably should get up and walk around for a bit and maybe try the couch instead, but it’s not great. reply tacocataco 2 hours agorootparentThat&#x27;s what I do when I can&#x27;t sleep. I \"give up\" trying to sleep and do something for a hour or two. Then I go through the usual prebed ritual.This gives me more attempts at sleep. Better to have more opportunities then fewer. reply ekianjo 6 hours agoparentprevMost of it is lack of physical activity. If you use your body enough during the day you are very unlikely to have problems sleeping at night. reply voidhorse 6 hours agoprevMelatonin has been a complete game changer for me in this regard. I&#x27;ve struggled to get decent sleep since my late teens. I&#x27;m plagued by an overactive brain at night. About a year ago I started taking just 3mg of melatonin before bed and haven&#x27;t looked back since. I&#x27;m sure my particular circumstances were such that it just happened to be my cureall and won&#x27;t work for everyone, but if you have trouble sleeping and haven&#x27;t tried it yet, definitely give it a go. It might be all you need. reply jks 5 hours agoparentThat dose may be ten times too high: https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2018&#x2F;07&#x2F;10&#x2F;melatonin-much-more-th...I was prescribed melatonin by a doctor, and the dose went up little by little from 3mg to 10mg before we decided it didn&#x27;t work for me. Sure it helped me fall asleep, but then I woke up a few hours later and couldn&#x27;t sleep again. Years later I have started taking 0.5mg (1mg is the smallest pill I could find, and I split them in two) and it&#x27;s much better that way.Low-dose doxepin (3mg) has also helped, but I find that I have to take breaks from it, because it stops working after a while. This dose is much lower than the usual dose for depression, but a local pharmacy manufactures the pills based on a doctor&#x27;s prescription. reply SoftTalker 6 hours agoparentprevAn ounce or two of bourbon works for me. Not enough to get intoxicated but just enough to feel warm and sleepy. reply 000ooo000 6 hours agorootparentI hope I don&#x27;t come across as the \"well ackshually\" guy, and I&#x27;m not definitely telling you to change, but maybe this could be useful to you: the alcohol episode of Huberman Lab podcast mentioned briefly at about 1:02:40 that in a discussion with a UC Berkley sleep researcher, it was said that having any alcohol at all on board interferes with sleep to the point that those in the field call it pseudo-sleep because it lacks most of the restorative value of sleep. reply quickthrower2 5 hours agorootparenthttps:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=DkS1pkKpILY1:06 on this video reply 000ooo000 5 hours agorootparentCheers - I misremembered that it was a Stanford researcher. Updated my comment. reply quickthrower2 5 hours agorootparentThanks for the recommendation. I definitely will be geeking out on that series now! reply 000ooo000 4 hours agorootparentGlad you are interested. Keep in mind that his talk of supplements should be weighed with the knowledge that he is financially involved with supplement companies. It&#x27;s not a secret, but worth some awareness. replystarbugs 1 hour agoprevI successfully trained myself to fall asleep using deep brown noise.This obviously is not a cure-all against sleep deprivation, but it&#x27;s easy to try out and, at least for me, it made falling asleep so much easier. reply markus_zhang 6 hours agoprevI have been missing sleep since child was born. The post is pretty accurate. I now fully understand why some people don&#x27;t want to get kids. reply Aurornis 6 hours agoparentIt can be difficult, but it&#x27;s also just a blip on the radar.I don&#x27;t know if this is your first or second, but I found that everything was significantly easier the second time around. I didn&#x27;t realize how much my wife and I had learned to handle things more efficiently and split the load better over time.You learn quickly.> I now fully understand why some people don&#x27;t want to get kids.I don&#x27;t know about that. I think the current trend is greatly overestimating the lifetime impact of some of these short-term difficulties of child raising. If someone actually wants kids but avoids it because they don&#x27;t want their sleep inconvenienced for a couple percent of their entire life, I&#x27;d question if they&#x27;re weighing the tradeoffs appropriately. It&#x27;s a choice for the rest of your life, but some people treat it like those first several months are the entirety of the decision making process, which is weird. reply markus_zhang 6 hours agorootparentI also heard that the second is easier, but it is still some drain so we decided not to get a second one. I think it really depends on 1) How much energy one has stored before getting the kid, and 2) How easy&#x2F;difficult the kid could be.It&#x27;s definitely not just a few months. It has been 3 years for me and I don&#x27;t really think it&#x27;s going to end soon. Again this is just personal, not an average experience. reply musha68k 4 hours agorootparentDifference between parental experience can be wild.Sometimes up to literal survivorship bias talk from other parents; can be mildly annoying but ultimately I always feel happy for them and their kid(s) that they just don’t seem to (want to?) really know better. Again, good for them - and us all - lest we’d have gone extinct long time ago ;) reply xyzelement 6 hours agoparentprevAny parent will tell you that having kids is not going to help your sleep, but most parents find sacrifice unequivocally worth it on the net.Not sure if this is truly what you meant but I would find it highly abnormal that a child does not bring your sufficient joy and meaning that would offset sleep loss. reply hnthrowaway0315 5 hours agorootparentI have a few bullet points about this:1. Most people would never say bad things about parenting because it makes them look like demons. I basically tell my friends and colleagues that it is OK and I&#x27;m still not sure whether it worths it or not. That&#x27;s as far honest as I&#x27;m willing, although it is pretty close to truth nowadays. I was a lot more snarky a year ago.2. Humans are very good at adapting. If you told me 3 years ago that I have to go through all these, I&#x27;d never think about getting a kid. But somehow it actually feels OK now. And that&#x27;s what I tell other people.3. Having kids basically means having a completely different life style for me. I can see why some people actually feel so excited about it because their life style actually fit. It&#x27;s about trading one life style to another, and this is something we rarely told others, who nevertheless never bother to ask anyway, because they don&#x27;t know what to ask. Nowadays, I&#x27;d tell any young people to get their dreams done before getting a kid, unless they really really want it. You want to go travel the world? Do it before getting a kid. You want to teach yourself a long chain of Mathematics&#x2F;Physics topics? Do it before getting a kid. I&#x27;m not saying you can&#x27;t do them after having kids, but it&#x27;s a LOT safer to do them before. reply xyzelement 5 hours agorootparentObviously subjective but I don&#x27;t agree with the final bullet point.I believe that human happiness is primarily determined by the amount of meaning and impact we have.If someone is learning math&#x2F;physics because they are committed to furthering the world through some breakthrough then yes kids would be deteminetal to thatBut if you are just randomly amusing yourself learning topics and one day you are 50+ the ship on family has basically sailed. And you look back on your life as a bunch of short term amusement that gets you to a very depressing place. reply elteto 6 hours agoparentprevHonestly, not having children because you don’t want to lose some sleep for a year or two sounds pretty shallow. reply Broken_Hippo 3 hours agorootparentHonestly, no reason for not having children is good enough for some people.I&#x27;m a woman in my mid-40s. I do not have children and have simply never wanted children.I&#x27;ve been called selfish, shallow, and other such things. Something must be wrong with me. No man will ever marry me if I won&#x27;t have children. I&#x27;ll change my mind - on a scale of \"because I&#x27;m young and stupid\" to \"biological clock will kick in\".None of this is true, of course. reply guidoism 6 hours agorootparentprevI’m fairly confident that the lack of sleep during my first child’s first two years permanently altered my brain in way that has seriously affected my ability to do my job. I think there is research to back my claim that sleep can change your brain just as trauma does. reply petemir 6 hours agorootparentAnother anecdote, but I remember someone here commenting that after several days without sleep he got his amygdala (maybe?) physically damaged and that permanently f-ed up his sleep schedule.As a terminal procrastinator who frequently pulls all-nighters (this year I even got to two in 3 days) I was terrified (and apparently decided to never comply with any deadline anymore, but that&#x27;s another story). reply cma 6 hours agorootparentprevThe thing to realize is your (I am presuming) inflexible job was probably as responsible as your kid. They should let you work very flexible hours if you just had a kid unless by nature it just can&#x27;t be done for that job. reply bentcorner 6 hours agorootparentprevIt does sound shallow, that&#x27;s because having children impacts you more than for a year or two and it&#x27;s more than just losing sleep. reply throwaway5959 1 hour agoprevI delay sleep because there are a couple of coworkers that are impossible, and the more I delay sleep, the later I will have to deal with them. reply behnamoh 8 hours agoprevMy partner suffers from this. She tried many solutions but to no vail. But it seems having an intentional period of wind down before bed (e.g., 1 hour on the couch, scrolling social media and listening to podcasts) helps. When she goes to bed after that, she never uses her phone and can sleep much better.I&#x27;m in the same boat except that I only go to bed when I&#x27;m absolutely completely drained and exhausted, which is probably not good... reply Aurornis 6 hours agoparent> She tried many solutions but to no vail. But it seems having an intentional period of wind down before bed (e.g., 1 hour on the couch, scrolling social media and listening to podcasts) helps. When she goes to bed after that, she never uses her phone and can sleep much better.It feels so strange to read posts where people rediscover the importance of basic sleep hygiene. Having some time to wind down and not using phones in bed are two of the most basic sleep hygiene practices around.I&#x27;m curious what \"many solutions\" she tried before getting back to the basics, because the exact sleep hygiene practices you described are at the front of the list for things that doctors explore first with patients.Unfortunately, some doctors and&#x2F;or internet advice places skip straight to medications, supplements, and an assumption that the problem can only be addressed by chemical means. The number of people who jump straight to melatonin or even prescription sedatives without making any attempts to alter their daily practices is worrisome. reply makestuff 6 hours agoparentprevA similar thing works for me. I don’t use my phone in the bed at night anymore and wait until I am tired before going to bed. After doing this for a few months I began to get tired around the same time every night (midnight for me). I wake up around 8:30am and I am rarely tired in the morning.One thing I would like to improve on is staying asleep throughout the night which I think would allow me to wake up earlier. I have tried Benadryl which works well, melatonin works okay, but the thing that works the best for me is THC. However, I am trying to stop that because I have read that you do not get as good of sleep when you use THC before bed.Another thing that helps is having it freezing cold in the bedroom. reply kbenson 6 hours agorootparentFor me a CPAP machine makes a big difference, but it&#x27;s not something I would have thought I needed except for my wife telling me I had breathing problems while asleep. Worth getting tested for if you suspect it in any way.> I have tried Benadryl which works wellThere have been some studies linking anticholinergics like Benadryl to dementia when taken later in life, so that might be worth looking into, if you haven&#x27;t already, and you use it regularly. reply alwayslikethis 6 hours agoparentprevIt&#x27;s not just phones, it&#x27;s the light. Consider turning lights off as much as possible 1-2 hours before bed. reply voidhorse 6 hours agorootparentDefinitely. I live in an area with a lot of light pollution, so much so that even when it&#x27;s supposed to be full dark, there&#x27;s enough ambient light outside that it&#x27;s more like \"late evening\" at best. I find that any time I&#x27;m away from home, I actually sleep better because there&#x27;s no extreme light pollution keeping me up. reply Mawr 2 hours agorootparentPut up black-out curtains on your windows. reply drones 2 hours agoprevI have not slept well since I was 12 years old. I fear I have permanently damaged my cognitive abilities because of this. reply tomxor 3 hours agoprevMissing an important bullet point:* Sleep deprivation reduces immune system function.I was surprised when none of the gov advice during covid included \"improve your sleep\", because that&#x27;s one of the best ways to strengthen your immune system, reduce your chances of contracting illnesses, and improve your chances of a fast and complication free recovery. reply foxfired 6 hours agoprevHallucinations! Add it to the list [1].I&#x27;ve experienced most if not all that was described here. And I look 10 years older. But I&#x27;m so glad I can sleep normal hours again.[1]:https:&#x2F;&#x2F;idiallo.com&#x2F;blog&#x2F;stopped-sleeping-started-hallucinat... reply losvedir 3 hours agoprevAs the father of a 3 week old, I feel this!I&#x27;ve been very interested in getting more data about just how much and what kind of sleep I&#x27;m getting. I know smart watches these days claim to provide that. Anyone know how accurate it is, and which watches are best for it? reply Patternnoticer 5 hours agoprevSleep is a time machine... The sooner I go to sleep, the sooner work comes in the morning. reply foundart 6 hours agoprevThe article itself points to prior discussion on HN from 2022https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30184796 reply operatingthetan 6 hours agoparentThe article feels like it could have afforded a lot more depth. It&#x27;s like it was designed as an HN prompt. reply nehal3m 6 hours agorootparentIs there something you find wrong with that? I generally find comments on HN to be thoughtful and people tend to invest some effort. There’s also a surprising amount of subject matter experts lurking until their subjects come up. A “prompt article” seems a decent way to get some of that going, no? reply operatingthetan 4 hours agorootparentThen it should just be posted as a text post. No need for this to be posted more than once as well. I don&#x27;t find low effort attention grabbing interesting or helpful. reply gavinhoward 5 hours agoprevThank you to the author for this.I struggle to have the discipline to get enough sleep. I&#x27;m going to bookmark this, pin it in a tab, and use it to remind myself that the consequences of my stupidity are greater than what I would accomplish by staying awak. reply totetsu 5 hours agoprevthis post for the same author about their decent into madness and loss of sens of self is an interesting read. Specifically its interesting how a universal experience of ego destruction is made send of afterwards in such a way that is so individualized to that persons age and culture. https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;L9q5iFJAggvoWNrRf&#x2F;that-time-... reply mitko 5 hours agoprevAs someone who currently runs on decreased sleep, I can relate to a bunch of these.But in my case, the root cause seems to be stress from increased load, and the higher cortisol levels it creates reply bman_kg 4 hours agoprevfighting with this problem for long time, so far my sleep got much better than previous months and years, lots of thanks to author for short and to the point plain English explanation, I like the writing style a lot. I learned a lot in just a minute, thats productive for me. reply telegpt 5 hours agoprevmore about this: https:&#x2F;&#x2F;blog.musemind.net&#x2F;the-underestimated-perils-of-skimp... reply 2devnull 4 hours agoprevI only need 2-3 hours. I wish I could get more. You learn to use it as an advantage. reply ramoz 2 hours agoparentI thought the same. I didn’t “experience” any of these things. I’ve always been driven, fit, resilient. Procrastinated about my health, and being better about sleep&#x2F;all-nighters.Then I was diagnosed with cancer at 30.Blessed to have treated it. I don’t take health for granted any longer, sleep is a critical component of my health. Mental and physical state are 10x where they were.Ymmv. There’s no proof that I directly contributed my cancer, but I suspect I did. reply swayvil 6 hours agoprev [–] Weakened immune system.Slower healing. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lack of sleep has numerous detrimental effects on health, such as slowing metabolism, increasing appetite, elevating the risk of conditions like depression, diabetes, dementia and increasing substance abuse.",
      "These health consequences of sleep deprivation can result in weight gain, heightened healthcare costs, strained relationships, and capped intellectual potential.",
      "Studies conducted by respected institutions such as Harvard and the National Institutes of Health confirm these findings."
    ],
    "commentSummary": [
      "The main topics of discussion include the concept of revenge bedtime procrastination, practical strategies for enhancing sleep habits, and the specific sleep-related challenges often faced by parents.",
      "The dialogue covers the detrimental effects of sleep deprivation on both physical and mental health, as well as ways parents can manage such deprivation.",
      "The discussion also explores the impact of having children on lifestyle adjustments, potential solutions for sleep issues, and the reliability of smart watches in sleep tracking."
    ],
    "points": 191,
    "commentCount": 120,
    "retryCount": 0,
    "time": 1696814418
  },
  {
    "id": 37810052,
    "title": "Indoor wood burning raises women’s lung cancer risk by 43%",
    "originLink": "https://www.sciencedirect.com/science/article/pii/S0160412023004014",
    "originBody": "Skip to main content Skip to article Journals & Books Search Register Sign in View PDF Download full issue Outline Highlights Abstract Keywords 1. Introduction 2. Methods 3. Results 4. Discussion CRediT authorship contribution statement Declaration of Competing Interest Acknowledgements Appendix A. Supplementary material Data availability References Show full outline Figures (1) Tables (3) Table 1 Table 2 Table 3 Extras (1) Supplementary data 1 Environment International Volume 178, August 2023, 108128 Full length article Indoor wood-burning from stoves and fireplaces and incident lung cancer among Sister Study participants Author links open overlay panel Suril S. Mehta a, M. Elizabeth Hodgson b, Ruth M. Lunn a, Claire E. Ashley c, Whitney D. Arroyave b, Dale P. Sandler d, Alexandra J. White d Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.envint.2023.108128 Get rights and content Under a Creative Commons license open access Highlights • First prospective study of wood burning and lung cancer incidence among U.S. women. • Higher wood stove/fireplace usage associated with 70 % higher incidence of lung cancer. • Associations were also elevated when analysis was restricted to never smokers. • Suggest even occasional indoor wood burning can contribute to lung cancer. Abstract Background and aim Epidemiological studies conducted mostly in low- and middle-income countries have found a positive association between household combustion of wood and lung cancer. However, most studies have been retrospective, and few have been conducted in the United States where indoor wood-burning usage patterns differ. We examined the association of exposure to indoor wood smoke from fireplaces and stoves with incident lung cancer in a U.S.-wide cohort of women. Methods We included 50,226 women without prior lung cancer participating in the U.S.-based prospective Sister Study. At enrollment (2003–2009), women reported frequency of use of wood-burning stoves and/or fireplaces in their longest-lived adult residence. Cox regression was used to estimate adjusted hazard ratios (HRadj) and 95 % confidence intervals (CI) for the association between indoor wood-burning fireplace/stove use and incident lung cancer. Lung cancer was self-reported and confirmed with medical records. Results During an average 11.3 years of follow-up, 347 medically confirmed lung cancer cases accrued. Overall, 62.3 % of the study population reported the presence of an indoor wood-burning fireplace/stove at their longest-lived adult residence and 20.6 % reported annual usage of ≥30 days/year. Compared to those without a wood-burning fireplace/stove, women who used their wood-burning fireplace/stove ≥30 days/year had an elevated rate of lung cancer (HRadj = 1.68; 95 % CI = 1.27, 2.20). In never smokers, positive associations were seen for use 1–29 days/year (HRadj = 1.64; 95 % CI = 0.87, 3.10) and ≥30 days/year (HRadj = 1.99; 95 % CI = 1.02, 3.89). Associations were also elevated across all income groups, in Northeastern, Western or Midwestern U.S. regions, and among those who lived in urban or rural/small town settings. Conclusions Our prospective analysis of a cohort of U.S. women found that increasing frequency of wood-burning indoor fireplace/stove usage was associated with incident lung cancer, even among never smokers. Previous article in issue Next article in issue Keywords Wood smokeHeatingCookingLung cancerWomen 1. Introduction Wood smoke is a complex mixture consisting of particulate matter, gases, and hundreds of different chemicals, some of which have been classified as hazardous air pollutants and carcinogens (e.g., benzene) (Pope et al., 2011, NTP, 2016, International Agency for Research on Cancer, 2010). The World Health Organization (2018) estimates that 3.8 million people globally will die from an illness resulting from household air pollution by burning of solid fuels and kerosene; of these deaths, 8 % are estimated to be from lung cancer. Lung cancer is the second most diagnosed cancer and the leading cause of cancer-related death among U.S. women (U.S. Cancer Statistics Working Group, 2022). In the United States, wood smoke is emitted primarily from wood stoves, fireplaces, and boilers. Almost two million U. S. households use wood as their primary heating fuel (U.S. Energy Information Administration, 2022a, U.S. Energy Information Administration, 2022b); some restaurants use wood for cooking. Studies in the United States have found higher levels of fine particulate matter (PM2.5) in homes with wood stoves compared to those without a stove; the highest levels were those using older stoves (Fleisch et al., 2020). Wood smoke is also a major contributor to outdoor air pollution, contributing nearly 80 % of wintertime PM in some geographical areas (Ward et al., 2012). The International Agency for Research on Cancer (IARC, 2010) has characterized indoor emissions from household combustion of biomass fuel (primary wood) as probably carcinogenic to humans (Group 2A) based on more than a dozen case-control studies of indoor wood burning and lung cancer, and studies in experimental animals. A pooled analysis from studies in higher income countries in North America and Europe, where wood burning is primarily used for heating rather than both cooking and heating, suggest that wood users have a 20 % higher risk of lung cancer compared to nonsolid-fuel users (Lissowska et al., 2005, Hosgood et al., 2010). Long-term exposure to indoor wood combustion was associated with a 2.5-fold excess risk among non-smokers in Brazil and seven European countries. However, these studies relied on retrospective exposure assessments for wood smoke exposure and may be subject to recall bias. Using the same prospective study population as presented here, the Sister Study cohort, White and Sandler (2017) found that exposure to wood-burning stoves/fireplaces was associated with an increased incidence of breast cancer. Wood smoke exposure may differ by sociodemographic and geographic characteristics, with poorer and rural populations being disproportionately exposed (Noonan et al., 2015, Rogalsky et al., 2014). Therefore, this study aims to address whether exposure to indoor wood smoke from fireplaces and stoves is associated with incident lung cancer in a large prospective cohort of U.S. women with consideration of how the association varies by sociodemographic and geographic characteristics. 2. Methods 2.1. Study population and exclusion criteria The Sister Study is an ongoing prospective cohort of over 50,000 women living in the United States, including Puerto Rico, who have had a sister diagnosed with breast cancer. Participants were enrolled from 2003 to 2009 (Sandler et al., 2017). Sociodemographic, behavioral, and physical characteristics were collected via questionnaires given at baseline. Sister Study participants provided written informed consent and the study is overseen by the National Institutes of Health Institutional Review Board. Sister Study follow-up data collection is on-going (Sandler et al., 2017). This present analysis includes incident lung cancer diagnosed and reported through September 30, 2019 (Sister Study Data Release 9.1). Sister Study participants were excluded from the current study if they withdrew from the study (N = 3), had missing data on presence of a fireplace or stove at the longest-lived adult residence (N = 119), had pre-baseline lung cancer (N = 63), had missing or insufficient data on lung cancer status or timing (uncertain lung cancer diagnosis or unknown timing of lung cancer diagnosis, N = 19), or had missing or zero person-years of follow-up (N = 281). After exclusion criteria, 50,399 (99.0 %) women remained in the study sample. 2.2. Indoor wood burning stove and/or fireplace exposure Indoor wood smoke exposure was collected at enrollment. For the longest-lived adult residence, participants were asked whether there was a fireplace or wood-burning stove (yes/no), whether wood was used as a fuel source for fireplaces or stoves in the longest-lived adult residence (yes/no), and annual usage of wood-burning stove or fireplace (0–365 days). Annual use of wood-burning stove or fireplace was categorized as either not having a stove or fireplace or using it zero days/year, 1–29 days/year of use, or 30–365 days/year of use. Participants were also asked the same questions regarding the presence and usage of wood-burning stoves and fireplaces at their baseline residence, if that was different from the long-lived residence. Also, participants were asked to list the main source of heat (electricity, gas, fuel oil, wood, other fuel sources), main energy source for stove top cooking (electricity, gas, other fuel sources), and other fuel sources used in fireplaces and/or stoves (no fireplace, natural gas/propane, artificial logs, and other fuel sources). 2.3. Incident lung cancer Post-baseline incident lung and bronchus cancer (ICD-10:C34) was identified through September 30, 2019. Lung cancers identified as in situ (D02.2; N = 2) or borderline (N = 1) were categorized as non-events and censored at the time of their diagnosis. Incident lung cancer was self- (or next of kin) reported and confirmed using medical records or the National Death Index (NDI) and death certificates (DC). Due to the low positive predictive value of self-reported lung cancer among Sister Study participants (76.9 %), we further excluded self- or next-of-kin reported lung cancer without any medical or death record confirmation (N = 170), though we note that this level of reporting accuracy was similar to other prospective cohort studies (Bergmann et al., 1998, Colditz et al., 1986). Supplemental Table 1 provides participant characteristics of excluded and included lung cancer cases. After exclusion, 50,226 (98.7 %) women remained in the final study population. For lung cancer cases with histopathology data (N = 199), we grouped tumor subtypes into adenocarcinomas (ICD-O-3: 8046, 8140–8255, 8480–8481, 8550–8551), other non-small cell lung cancer (large cell carcinoma: ICD-O-3: 8012–8031, 8310; squamous cell carcinoma: ICD-O-3: 8050–8076), small cell carcinomas (ICD-O-3: 8040–8045), and other or unknown histologies (ICD-O-3: 8000–8010, 8560, 8720–8980). 2.4. Statistical analysis Hazard ratios (HRs) and 95 % confidence intervals (95 % CI) were estimated using Cox proportional hazards regression models to assess the association between indoor wood smoke from fireplaces and stoves and incident lung cancer. Follow-up began at enrollment, age at entry into cohort was used at the beginning of follow-up, and age was used as the time scale. We assessed Cox proportional hazards assumptions using diagnostic tests including plot analyses and time interaction models. To calculate p-for-trend across tertiles of annual wood burning usage, a median value for each tertile was used as a linear term in models. To identify potential confounders, we used a directed acyclic graph (DAG) (Supplemental Fig. S1) and existing literature. Covariates include: highest educational attainment (high school graduate or below, some college or an Associates’ degree, Bachelor’s degree or above), marital status (never married, married or living as married, or previously married including divorced, widowed, or separated), race/ethnicity (non-Hispanic White, Non-Hispanic Black, Hispanic or other race/ethnicity), total household income (≤$49,999, $50,000–$99,999, ≥$100,000), smoking status (never smoker, ever smoker (i.e., past or current smoker)), total smoking intensity (packs/year), years exposed to environmental tobacco smoke (ETS) (years), and urban status at longest-lived residence (urban, suburban, rural or small town). To account for high missingness for total household income (N = 1,950), values were imputed using the fully conditional specification multiple imputation method. Participants with missing data for other covariates (<3 %) were excluded from complete case analyses. To further explore the data, we stratified by smoking status, duration of longest-lived residence (<15 years, ≥15 years), race/ethnicity, urban status at longest-lived residence, family history of lung cancer, Census region excluding Puerto Rico for longest-lived residence (Northeast, Midwest, South, West) and household income at baseline. Interaction terms for each wood smoke exposure and potential modifying factor were included in models to test for heterogeneity with a p-value < 0.05. All our analysis was performed using data from the entire final study population, producing stratified effect estimates using separate terms for strata specific effects. We also evaluated whether the association varied by histological subtype (adenocarcinoma, other non-small cell lung cancer; excluding small cell, other, unknown, and noncoded histologies due to a limited number of cases). Though few participants reported coal as a primary or concurrent fuel source, we conducted a sensitivity analysis excluding all participants who reported coal as a fuel for their fireplace or stove (N = 325) or as their main heating fuel (N = 32). We included a sensitivity analysis limiting to participants whose current residence was also their longest-lived adult residence. We also examined additional fuel sources reported at the participant’s longest-lived residence, including main source of heat at residence, energy sources for stove top cooking, and alternative fuel sources used in fireplaces and/or stoves. STATA v.14.2 was used for all statistical analyses, DAGitty (https://www.daggity.net) was used to create the DAG, and R v.4.2.1 was used to create the main figures. 3. Results The study population’s baseline characteristics by lung cancer status are summarized in Table 1. Overall, the mean follow-up time for all participants was 11.3 years (range: 0.10–15.9 years), and 347 women were diagnosed with lung cancer. Compared to those without a wood-burning fireplace or stove, women with a wood-burning fireplace or stove were more likely to be Non-Hispanic white, had higher educational attainment, had higher household income, more often married or living as married, more likely to have lived in the Western U.S., less often lived in an urban setting at their longest-lived residence, had higher mean pack-years, and had lower mean exposure to ETS. Table 1. Participant characteristics of Sister Study participants by presence of wood-burning stove/fireplace at longest-lived residence (N = 50,226). Participant characteristic No stove/fireplace (N = 18,949) Has stove/fireplace (N = 31,277) N Mean (range) N Mean (range) Age at baseline, years (mean, range) 18,949 55.0 (35.0–76.5) 31,277 56.0 (35.1–76.5) Follow-up time, years (mean, range) 18,949 11.1 (0.1–15.7) 31,277 11.5 (0.1–15.9) Total pack-years (mean, range) 18,818 7.2 (0.0–130.0) 31,135 5.8 (0.0–120.3) Total years exposed to ETS (mean, range) 18,382 23.5 (0.0–75.0) 30,588 21.1 (0.0–75.0)N % N % Race/ethnicityNon-Hispanic white 14,258 75.3 27,828 89.0Non-Hispanic black 2,397 12.7 1,980 6.3Hispanic/other 2,288 12.1 1,465 4.7 Highest educational attainmentHigh school degree or less 3,878 20.0 3,878 12.4Some college/Associates’ degree 7,103 37.5 9,812 31.4Bachelor’s degree or above 8,061 42.6 17,583 56.2 Household incomeLess than $50,000 7,151 37.7 5,682 18.2$50,000 - $99,999 7,929 41.8 12,612 40.3At least $100,000 3,869 20.4 12,983 41.5 Marital statusNever married 1,677 8.9 1,042 3.3Married/living as married 12,400 65.5 25,185 80.5Previously married 4,869 25.7 5,044 16.1 Longest-lived Census regionNortheast 3,770 20.2 5,508 17.7Midwest 5,730 30.7 8,363 26.9South 5,572 29.9 9,464 30.3West 2,718 14.6 7,824 25.1Puerto Rico 870 4.7 6 0.02 Urban status at longest-lived residenceUrban 5,388 28.6 4,922 15.8Suburban 5,995 31.8 13,751 44.1Rural or small town 7,474 39.6 12,515 40.1 Smoking statusNever 10,335 54.6 17,955 57.4Ever (former or current) 8,611 45.4 13,319 42.6 ETS = environmental tobacco smoke. Missing: total pack years (N = 273), total years exposed to ETS (N = 1,256), race/ethnicity (N = 10), highest educational attainment (N = 7), marital status (N = 9), longest-lived Census region (N = 371), urban status at longest-lived residence (N = 181), smoking status (N = 6). Overall, 62.3 % of the study population reported the presence of an indoor wood-burning stove and/or fireplace at their longest-lived residence, and only 20.6 % reported annual usage of at least 30 days/year. In fully adjusted models (Table 2, Model 2), we found positive associations for incident lung cancer and both presence of a wood-burning fireplace or stove (HRadj = 1.42; 95 % CI = 1.11, 1.81) and wood used as a fuel source in wood-burning fireplaces or stoves (HRadj = 1.43; 95 % CI = 1.14, 1.79). Compared to those without a wood-burning fireplace or stove or zero days/year usage, the highest incidence of lung cancer was seen in women reporting wood-burning fireplace/stove usage of at least 30 days/year (HRadj = 1.68; 95 % CI = 1.27, 2.20; p-for-trend < 0.001). Table 2. The association between the presence and usage of a wood-burning fireplace and/or stove presence and incident lung cancer. Empty CellModel 11 Model 22 N (cases/noncases) HR 95 % CI p-trend N (cases/noncases) HRadj 95 % CI p-trend Presence of wood-burning fireplace/stove No wood-burning fireplace/stove (ref) 119/18,830 1.00 —110/18,075 1.00 —Yes 228/31,049 1.05 0.84–1.31219/30,160 1.42 1.11–1.81Used wood as a source of fuel No wood-burning fireplace/stove or no wood use (ref) 191/30,949 1.00 —179/29,825 1.00 —Yes 156/18,915 1.25 1.01–1.54150/18,396 1.43 1.14–1.79Annual wood-burning fireplace/stove use No wood-burning fireplace/stove or no wood use (ref) 163/25,083 1.00 — 0.002 152/24,110 1.00 — <0.001 1–29 days/year 86/14,458 0.88 0.68–1.1584/14,088 1.12 0.85–1.48At least 1 month/year 98/10,257 1.40 1.09–1.8093/9,968 1.68 1.27–2.201 Age accounted for as underlying time scale. 2 Aditionally adjusted for race/ethnicity (NH black, NH white, Hispanic/other), education (HS or less, some college, Bachelors or above), urban status at longest-lived residence (urban, suburban, rural/small town), smoking status (never, ever), pack years, total years exposed to environmental tobacco smoke, marital status (single, married, previously married), income status (<$50,000, $50,000–$99,999, $100,000+). We did not see evidence of effect modification by smoking status; elevated estimates were seen for both ever- and never-smoking women (Table 3). Compared to those without a wood-burning fireplace or stove, the incidence of lung cancer among never smokers was higher among women using wood-burning fireplaces/stoves 1–29 days/year (HRadj = 1.64; 95 % CI = 0.87, 3.10) and at least 30 days/year (HRadj = 1.99; 95 % CI = 1.02, 3.89), with a suggestive linear exposure–response trend (p-for-trend = 0.08). A 54 % increased incidence of lung cancer was observed among smokers using wood-burning fireplaces/stoves at least 30 days/year (HRadj = 1.54; 95 % CI = 1.15, 2.07; p-for-trend = 0.003). Table 3. The association between wood-burning fireplace and/or stove presence and usage and incident lung cancer, stratified by smoking status.1 Empty CellNever smoker Ever (current or former) smoker N cases HRadj 95 % CI p-trend N cases HRadj 95 % CI p-trend Presence of wood-burning stove/fireplace No (ref) 6 – —107 1.00 —Yes 52 – –167 1.14 0.89–1.48Used wood as a source of fuel No (ref) 28 1.00 —154 1.00 —Yes 30 1.62 0.95–2.75120 1.38 1.08–1.77Annual wood-burning fireplace/stove use No fireplace/stove or zero use (ref) 20 1.00 — 0.08 135 1.00 — 0.003 1–29 days used/year 21 1.64 0.87–3.1063 0.99 0.73–1.35At least 1 month used/year 17 1.99 1.02–3.8976 1.54 1.15–2.07P-for-heterogeneity (used wood as fuel source) = 0.64; p-for-heterogeneity (annual wood-burning fireplace/stove usage) = 0.40. 1 Adjusted for race/ethnicity (NH black, NH white, Hispanic/other), education (HS or less, some college, Bachelors or above), urban status at longest-lived residence (urban, suburban, rural/small town), marital status (single, married, previously married), income status (<$50,000, $50,000-$99,999, $100,000+), total years exposed to environmental tobacco smoke. Length of time living at the longest adult residence did not substantially modify the associations between wood burning and lung cancer incidence (Fig. 1A). The increased estimate associated with a wood-burning fireplaces and/or stoves at least 30 days/year was more pronounced, albeit less precise, for those living in an urban area (HRadj = 2.64; 95 % CI = 1.43, 4.87; p-for trend = 0.003) (Fig. 1B). Statistically significant associations were observed for those in rural/small settings (HRadj = 1.74; 95 % CI = 1.17, 2.57; p-for trend = 0.003) and no increased incidence was found for those in suburban settings, although tests for interaction were not statistically significant (p-for-heterogeneity = 0.26). We also stratified by U.S. Census region (Fig. 1C); the highest incidence of lung cancer in the highest use frequency category was seen for participants in the Midwestern (HRadj = 2.58; 95 % CI = 1.52, 4.38), Northeastern (HRadj = 2.30; 95 % CI = 1.25, 4.26), and Western (HRadj = 1.37; 95 % CI = 0.81, 2.33) U.S. Census regions. Download : Download high-res image (468KB) Download : Download full-size image Fig. 1. Adjusted HRs (95 % CI) of wood-burning fireplace and/or stove usage and incident lung cancer, stratified by residential factors at longest-lived residence, including (A) years lived at residence (<15 years, 15+ years), (B) urban status (urban, suburban, rural/small town), and (C) U.S. Census region (Northeast, Midwest, South, West). All models adjusted for race/ethnicity, education, smoking status, pack years, total years exposed to environmental tobacco smoke, marital status, income status. Models A and C were additionally adjusted for urban status. Test for trend values across tertiles of annual wood burning usage are reported as p-for-trend values in top left of each plot. P-for-heterogeneity for A: 0.87; p-for-heterogeneity for B: 0.26; p-for-heterogeneity for C: 0.34. All plotted values are on a log scale. In race-stratified analyses, lung cancer incidence was elevated among non-Hispanic White participants using wood-burning fireplaces and/or stoves at least 30 days/year (HRadj = 1.71; 95 % CI = 1.29, 2.26; p-for-trend < 0.001), but sample sizes were too small to evaluate associations in other groups. Associations did not vary by household income (Supplemental Table 2) or by family history of lung cancer (Supplemental Table 3). Wood burning was positively associated with lung adenocarcinoma rates, but not the rates of other non-small cell lung cancers (Supplemental Table 4). We did not see a change in the results when we restricted to only those whose longest-lived residence was also their residence at baseline, or when coal users were excluded from final models (not shown). Rate of lung cancer was similarly elevated for non-wood fuel sources used in fireplaces/stoves, including natural gas/propane (HRadj = 1.29; 95 % CI = 0.91, 1.82) and artificial logs (HRadj = 1.33; 95 % CI = 0.97, 1.83) (Supplemental Table 5.1). As shown in Supplemental Table 5.2, compared to using electricity as the main heating source, use of other fuel sources was associated with higher rates (HRadj = 1.32; 95 % CI = 0.83, 2.09); as was use of wood as the main fuel source (HRadj = 1.40; 95 % CI = 0.73, 2.66), but based on only on a small number of exposed cases (N = 11). Lastly, we did not find an association between lung cancer and types of energy sources for stovetop cooking (Supplemental Table 5.3). 4. Discussion In this large cohort of U.S. women, we observed higher rates of lung cancer with higher frequency of use of indoor wood-burning fireplaces and/or stoves in participants’ longest-lived residence, including among non-smoking women. To our knowledge, this is the first prospective analysis of indoor stoves and fireplaces in relation to lung cancer risk. This study contributes to the existing evidence base suggesting that indoor air pollution from stoves/fireplaces may contribute to lung cancer risk and suggests that this holds true even in populations where wood-burning is predominately not used for cooking or heating inside the home. Our prospective study results largely align with previous case-control studies of indoor wood burning and lung cancer in women (Sobue, 1990, Ko et al., 1997, Le et al., 2001, Hernandez-Garduno et al., 2004, Hosgood et al., 2010, Tang et al., 2010, Vermeulen et al., 2019, Phukan et al., 2014). Our findings are also consistent with IARC’s classification of biomass burning (primarily wood) as a probable lung carcinogen (IARC, 2010). Smoke from wood burning contains known and suspected lung carcinogens, including benzene, 1,3-butadiene, polycyclic aromatic hydrocarbons, and other hazardous air pollutants. The mechanistic pathways by which wood smoke exposure causes cancer are not fully elucidated; however, wood smoke has been found to be genotoxic, inducing DNA damage and mutations (Mumford et al., 1993, Öztürk et al., 2002, Musthapa et al., 2004). Wood smoke can cause immunosuppression and chronic inflammation via inflammatory cytokines (IARC, 2010), and induces oxidative stress (IARC, 2010), which may lead to tumorigenesis. Similarly, both natural gas/propane and artificial logs used as a fuel source in fireplaces or stoves were also associated with an increased incidence of lung cancer. Lung carcinogens have been identified in household gas emissions (Lebel et al., 2022, Dutton et al., 2001) and manufactured synthetic logs (Gullett et al., 2003). When examining just fuel sources of stovetop cooking, we did not find an increased incidence in lung cancer for women using natural gas, compared to electricity. The pronounced associations of lung cancer in relation to wood-burning exposure metrics when restricting to never-smoking women in this study provides further evidence of wood smoke as an independent risk factor of lung cancer. In a pooled analysis of four case-control studies of nonsmoking women from Europe and North America (Hosgood et al., 2010), the association of wood use and lung cancer was slightly elevated but not significant (OR = 1.15; 95 % CI = 0.81, 1.64). Particularly among low- and middle-income countries, nonsmoking women may be at increased exposure to wood burning due to greater cooking and traditional household roles (Delgado et al., 2005, Torres-Duque et al., 2008). Overall, our results are consistent with studies of women from other Western countries; the risk estimate of the pooled analyses for all women from Europe and North America (Hosgood et al., 2010) found a similar higher rates of lung cancer using wood as their predominant indoor fuel source (OR = 1.19, 95 % CI = 0.94, 1.51). Globally, indoor wood use patterns significantly differ by country, region, and sociodemographic characteristics. Compared to low- and middle-income countries, the U.S. is much less reliant on wood as a primary fuel source for cooking (Bonjour et al., 2013) and heating (1.7 % of U.S. population, U.S. Census Bureau, 2020); however, use varies by region and socioeconomic status, with those in rural poverty at greatest exposure to residential wood smoke (Rogalsky et al., 2014). Of histopathology data reported, adenocarcinomas represented the largest proportion of lung cancer subtypes and associations were seen between wood burning and adenocarcinoma in our study. Other studies have also found elevated risk for adenocarcinoma associated with wood use (Báez-Saldaña et al., 2021, Figueroa et al., 2012, Hernandez-Garduno et al., 2004). We found elevated associations for wood-burning among women living in urban and rural/small town settings, but not for suburban settings. Women in rural and small-town settings may more heavily rely on wood burning as a fuel type and have been estimated to have greater exposure to wood smoke in the U.S. (Noonan et al., 2015, Rogalsky et al., 2014). Estimates were highest for urban settings, although these results were more imprecise. Including urbanicity as a confounder in our main analyses to adjust for traffic-related air pollution still resulted in elevated effect estimates. Further, adjustment for urbanicity as a proxy for outdoor air pollution may not account for sources of non-traffic related air pollution, including outdoor wood burning. Evidence suggests ambient outdoor air pollution in total is a known risk factor for lung cancer (Loomis et al., 2013). In this population, wood-burning was positively associated with lung cancer incidence in all income groups. Though we were limited by spatial granularity and smaller sample sizes, the positive association was consistently present among participants living in the Midwestern, Northeastern, and Western U.S. Further exploration into these broad geographic regions may yield spatial variations, particularly among rural populations as previously estimated (Noonan et al., 2015). Our study population primarily used electricity and gas as their main fuel sources for cooking and heating in their longest-lived residence. As such, indoor wood burning for many is likely a secondary or tertiary fuel source. Still, an estimated 10.8 million U.S. households used wood as a fuel source in 2020 (U.S. Energy Information Administration, 2022a, U.S. Energy Information Administration, 2022b), and nondaily use can still contribute a consequential amount of wood smoke exposure. To capture women using wood-burning appliances at least seasonally, we categorized “at least one month per year” of wood-burning appliance usage as our highest use frequency category which is consistent with this not being a predominate source of heating in this population. Seasonal indoor wood-burning has been found to be a significant contributor to outdoor air pollution, particularly during colder months (Croft et al., 2017, Tsiodra et al., 2021). Ward and Lange (2010) identified higher PM concentrations in winter months associated with a higher proportion of wood-burning residences. We were also unable to compare the efficiency of wood-burning appliances, a major factor in the contribution to higher indoor air pollution in the United States. Recognizing efficiency of wood-burning appliances is a public health issue, U.S. EPA promulgated updated performance requirements in 2015 for new residential wood heaters to lower harmful emissions (U.S. EPA, 2015), which may ultimately provide substantial reductions in cancer and noncancer hazard from indoor wood burning practices in the United States (Marin et al., 2022). With a prospective study design, we reduced the potential of recall bias compared to previous retrospective case-control studies. The large Sister Study sample and US-wide enrollment allowed for examination of sex- and geographic-specific lung cancer rates. Given the large sample size of the cohort, we were able to assess effect modification by smoking status and household income. The availability of in-depth baseline survey questions on presence, use, frequency, and fuel types allowed for a greater examination of wood smoke exposure. Lastly, we were able to eliminate confounding concerns by coal co-exposure. Demographically, our study participants are more affluent, older, and less diverse than the U.S. population of women, which may limit our generalizability. Use of wood-burning fireplaces and stoves may be an imprecise proxy for actual indoor wood smoke exposure. Since wood burning usage variables were self-reported, there is potential for nondifferential misclassification of wood smoke exposure, which would like attenuate affect estimates. To categorize wood smoke exposure, we relied on longest-lived residence, and were not able to assess lifetime exposure. Further interrogation by more granular geographic regions is needed given regional variability in climate and heating practices. Lastly, other co-exposures were unaccounted for, such as radon, poor housing quality, and other sources of wood smoke (e.g., outdoor wood burning, wildfires). In conclusion, in this large prospective study of U.S. women, we provide evidence consistent with prior case-control studies that even infrequent exposure to indoor wood smoke is associated with a higher incidence of lung cancer, including among never smokers. CRediT authorship contribution statement Suril S. Mehta: Conceptualization, Methodology, Software, Formal analysis, Data curation, Writing – original draft, Writing – review & editing, Visualization, Project administration, Supervision. M. Elizabeth Hodgson: Methodology, Data curation, Writing – review & editing. Ruth M. Lunn: Conceptualization, Writing – review & editing. Claire E. Ashley: Data curation, Writing – review & editing. Whitney D. Arroyave: Writing – review & editing. Dale P. Sandler: Writing – review & editing. Alexandra J. White: Conceptualization, Methodology, Formal analysis, Writing – original draft, Writing – review & editing, Project administration, Supervision. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements The authors would like to thank Aimee D’Alosio, Heather Carroll, and Patrick Ringwald for their assistance in dataset preparation, and Kyla Taylor, Katie O’Brien, Andy Rooney, and Scott Masten for their critical manuscript review. Funding This work is supported by the Divisions of Translational Toxicology and Intramural Research at National Institute of Environmental Health Sciences (Z01ES044005, Z1AES103332). Appendix A. Supplementary material The following are the Supplementary data to this article: Download : Download Word document (223KB) Supplementary data 1. Data availability Data will be made available on request. References Báez-Saldaña et al., 2021 R. Báez-Saldaña, A. Canseco-Raymundo, B. Ixcot-Mejía, I. Juárez-Verdugo, A. Escobar-Rojas, U. Rumbo-Nava, P. Castillo-González, S. León-Dueñas, O. Arrieta Case–control study about magnitude of exposure to wood smoke and risk of developing lung cancer Eur. J. Cancer Prevent., 30 (6) (2021), pp. 462-468 CrossRefView in ScopusGoogle Scholar Bergmann et al., 1998 M.M. Bergmann, E.E. Calle, C.A. Mervis, H.L. Miracle-McMahill, M.J. Thun, C.W. Health Validity of self-reported cancers in a propsective cohort study in comparison with data from state cancer registries Am. J. Epidemiol., 147 (6) (1998), pp. 556-562 CrossRefView in ScopusGoogle Scholar Bonjour et al., 2013 S. Bonjour, H. Adair-Rohani, J. Wolf, N.G. Bruce, S. Mehta, A. Prüss-Ustün, M. Lahiff, E.A. Rehfuess, V. Mishra, K.R. Smith Solid fuel use for household cooking: country and regional estimates for 1980–2010 Environ. Health Perspect., 121 (7) (2013), pp. 784-790 CrossRefView in ScopusGoogle Scholar Colditz et al., 1986 G.A. Colditz, P. Martin, M.J. Stampfer, W.C. Willett, L. Sampson, B. Rosner, C.H. Hennekens, F.E. Speizer Validation of questionnaire information on risk factors and disease outcomes in a prospective cohort study of women Am. J. Epidemiol., 123 (5) (1986), pp. 894-900 CrossRefView in ScopusGoogle Scholar Croft et al., 2017 D.P. Croft, S.J. Cameron, C.N. Morrell, C.J. Lowenstein, F. Ling, W. Zareba, P.K. Hopke, M.J. Utell, S.W. Thurston, K. Thevenet-Morrison, K.A. Evans, D. Chalupa, D.Q. Rich Associations between ambient wood smoke and other particulate pollutants and biomarkers of systemic inflammation, coagulation and thrombosis in cardiac patients Environ. Res., 154 (2017), pp. 352-361 View PDFView articleView in ScopusGoogle Scholar Delgado et al., 2005 J. Delgado, L.M. Martinez, T.T. Sánchez, A. Ramirez, C. Iturria, G. González-Avila Lung cancer pathogenesis associated with wood smoke exposure Chest, 128 (1) (2005), pp. 124-131, 10.1378/chest.128.1.124 View PDFView articleView in ScopusGoogle Scholar Dutton et al., 2001 S.J. Dutton, M.P. Hannigan, S.L. Miller Indoor pollutant levels from the use of unvented natural gas fireplaces in Boulder, Colorado J. Air Waste Manag. Assoc., 51 (12) (2001), pp. 1654-1661 CrossRefView in ScopusGoogle Scholar Figueroa et al., 2012 C.G.S. Figueroa, R. Fernández-Plata, M.S. Rivera-de la Garza, M. de los Ángeles Mora-Pizano, D. Martínez-Briseño, F. Franco-Marina, J.R. Pérez-Padilla Wood smoke as a risk factor for lung cancer in nonsmoking hospitalized population NCT Neumología y Cirugía de Tórax, 71 (4) (2012), pp. 325-332 Google Scholar Fleisch et al., 2020 A.F. Fleisch, L.B. Rokoff, E. Garshick, S.T. Grady, J.W. Chipman, E.R. Baker, P. Koutrakis, M.R. Karagas Residential wood stove use and indoor exposure to PM2.5 and its components in Northern New England J. Expo Sci. Environ. Epidemiol., 30 (2) (2020), pp. 350-361, 10.1038/s41370-019-0151-4 View in ScopusGoogle Scholar Gullett et al., 2003 B.K. Gullett, A. Touati, M.D. Hays PCDD/F, PCB, HxCBz, PAH, and PM emission factors for fireplace and woodstove combustion in the San Francisco Bay region Environ. Sci. Technol., 37 (9) (2003), pp. 1758-1765 View in ScopusGoogle Scholar Hernandez-Garduno et al., 2004 E. Hernandez-Garduno, M. Brauer, J. Perez-Neria, S. Vedal Wood smoke exposure and lung adenocarcinoma in non-smoking Mexican women Int. J. Tuberc. Lung Dis., 8 (3) (2004), pp. 377-383 View in ScopusGoogle Scholar Hosgood et al., 2010 H.D. Hosgood III, P. Boffetta, S. Greenland, Y.C. Lee, J. McLaughlin, A. Seow, E.J. Duell, A.S. Andrew, D. Zaridze, N. Szeszenia-Dabrowska, P. Rudnai In-home coal and wood use and lung cancer risk: a pooled analysis of the International Lung Cancer Consortium Environ. Health Perspect., 118 (12) (2010), pp. 1743-1747 CrossRefGoogle Scholar International Agency for Research on Cancer, 2010 International Agency for Research on Cancer, 2010. IARC Monographs on the Evaluation of Carcinogenic Risks to Humans. Volume 95. Household Use of Solid Fuels and High-Temperature Frying. IARC, Lyon. Google Scholar Ko et al., 1997 Y.C. Ko, C.H. Lee, M.J. Chen, C.C. Huang, W.Y. Chang, H.J. Lin, H.Z. Wang, P.Y. Chang Risk factors for primary lung cancer among non-smoking women in Taiwan Int. J. Epidemiol., 26 (1) (1997), pp. 24-31 View in ScopusGoogle Scholar Le et al., 2001 C.H. Le, Y.C. Ko, L.S. Cheng, Y.C. Lin, H.J. Lin, M.S. Huang, J.J. Huang, E.L. Kao, H.Z. Wang The heterogeneity in risk factors of lung cancer and the difference of histologic distribution between genders in Taiwan Cancer Causes Control, 12 (4) (2001), pp. 289-300 Google Scholar Lebel et al., 2022 E.D. Lebel, D.R. Michanowicz, K.R. Bilsback, L.A.L. Hill, J.S. Goldman, J.K. Domen, J.M. Jaeger, A. Ruiz, S.B. Shonkoff Composition, emissions, and air quality impacts of hazardous air pollutants in unburned natural gas from residential stoves in California Environ. Sci. Technol., 56 (22) (2022), pp. 15828-15838 CrossRefView in ScopusGoogle Scholar Lissowska et al., 2005 J. Lissowska, A. Bardin-Mikolajczak, T. Fletcher, D. Zaridze, N. Szeszenia-Dabrowska, P. Rudnai, E. Fabianova, A. Cassidy, D. Mates, I. Holcatova, V. Vitova Lung cancer and indoor pollution from heating and cooking with solid fuels: the IARC international multicentre case-control study in Eastern/Central Europe and the United Kingdom Am. J. Epidemiol., 162 (4) (2005), pp. 326-333 CrossRefView in ScopusGoogle Scholar Loomis et al., 2013 D. Loomis, Y. Grosse, B. Lauby-Secretan, F. El Ghissassi, V. Bouvard, L. Benbrahim-Tallaa, N. Guha, R. Baan, H. Mattock, K. Straif The carcinogenicity of outdoor air pollution Lancet Oncol., 14 (13) (2013), p. 1262 View PDFView articleView in ScopusGoogle Scholar Marin et al., 2022 A. Marin, L. Rector, B. Morin, G. Allen Residential wood heating: an overview of US impacts and regulations J. Air Waste Manag. Assoc., 72 (7) (2022), pp. 619-628 CrossRefView in ScopusGoogle Scholar Mumford et al., 1993 J.L. Mumford, X. Lee, J. Lewtas, T.L. Young, R.M. Santella DNA adducts as biomarkers for assessing exposure to polycyclic aromatic hydrocarbons in tissues from Xuan Wei women with high exposure to coal combustion emissions and high lung cancer mortality Environ. Health Perspect., 99 (1993), pp. 83-87 View in ScopusGoogle Scholar Musthapa et al., 2004 M.S. Musthapa, M. Lohani, S. Tiwari, N. Mathur, R. Prasad, Q. Rahman Cytogenetic biomonitoring of Indian women cooking with biofuels: micronucleus and chromosomal aberration tests in peripheral blood lymphocytes Environ. Mol. Mutagen., 43 (4) (2004), pp. 243-249 View in ScopusGoogle Scholar Noonan et al., 2015 C.W. Noonan, T.J. Ward, E.O. Semmens Estimating the number of vulnerable people in the United States exposed to residential wood smoke Environ. Health Perspect., 123 (2) (2015), p. A30 View in ScopusGoogle Scholar NTP, 2016 NTP (National Toxicology Program), 2016. Report on Carcinogens, Fourteenth Edition. U.S. Department of Health and Human Services, Public Health Service, Research Triangle Park, NC. https://ntp.niehs.nih.gov/go/roc14. Google Scholar Öztürk et al., 2002 Ş. Öztürk, S. Vatansever, K. Çefle, Ş. Palanduz, K. Güler, N. Erten, O. Erk, M.A. Karan, C. Taşcıogˇlu Acute wood or coal exposure with carbon monoxide intoxication induces sister chromatid exchange J. Toxicol. Clin. Toxicol., 40 (2) (2002), pp. 115-120 View in ScopusGoogle Scholar Phukan et al., 2014 R.K. Phukan, B.J. Saikia, P.K. Borah, E. Zomawia, G.S. Sekhon, J. Mahanta Role of household exposure, dietary habits and glutathione S-Transferases M1, T1 polymorphisms in susceptibility to lung cancer among women in Mizoram India Asian Pac. J. Cancer Prev., 15 (7) (2014), pp. 3253-3260 CrossRefView in ScopusGoogle Scholar Pope et al., 2011 C.A. Pope III, R.T. Burnett, M.C. Turner, A. Cohen, D. Krewski, M. Jerrett, S.M. Gapstur, M.J. Thun Lung cancer and cardiovascular disease mortality associated with ambient air pollution and cigarette smoke: shape of the exposure–response relationships Environ. Health Perspect., 119 (11) (2011), pp. 1616-1621 CrossRefGoogle Scholar Rogalsky et al., 2014 D.K. Rogalsky, P. Mendola, T.A. Metts, W.J. Martin Estimating the number of low-income Americans exposed to household air pollution from burning solid fuels Environ. Health Perspect., 122 (8) (2014), pp. 806-810, 10.1289/ehp.1306709 View in ScopusGoogle Scholar Sandler et al., 2017 D.P. Sandler, M.E. Hodgson, S.L. Deming-Halverson, P.S. Juras, A.A. D'Aloisio, L.M. Suarez, C.A. Kleeberger, D.L. Shore, L.A. DeRoo, J.A. Taylor, C.R. Weinberg, Sister Study Research Team The Sister Study Cohort: baseline methods and participant characteristics Environ. Health Perspect., 125 (12) (2017), Article 127003 View in ScopusGoogle Scholar Sobue, 1990 T. Sobue Association of indoor air pollution and lifestyle with lung cancer in Osaka, Japan Int. J. Epidemiol., 19 (1990), pp. S62-S66 CrossRefView in ScopusGoogle Scholar Tang et al., 2010 L. Tang, W.Y. Lim, P. Eng, S.S. Leong, T.K. Lim, A.W. Ng, A. Tee, A. Seow Lung cancer in Chinese women: evidence for an interaction between tobacco smoking and exposure to inhalants in the indoor environment Environ. Health Perspect., 118 (9) (2010), pp. 1257-1260 CrossRefView in ScopusGoogle Scholar Torres-Duque et al., 2008 C. Torres-Duque, D. Maldonado, R. Pérez-Padilla, M. Ezzati, G. Viegi Biomass fuels and respiratory diseases: a review of the evidence Proc. Am. Thorac. Soc., 5 (5) (2008), pp. 577-590 CrossRefView in ScopusGoogle Scholar Tsiodra et al., 2021 I. Tsiodra, G. Grivas, K. Tavernaraki, A. Bougiatioti, M. Apostolaki, D. Paraskevopoulou, A. Gogou, C. Parinos, K. Oikonomou, M. Tsagkaraki, P. Zarmpas Annual exposure to polycyclic aromatic hydrocarbons in urban environments linked to wintertime wood-burning episodes Atmos. Chem. Phys., 21 (23) (2021), pp. 17865-17883 CrossRefView in ScopusGoogle Scholar U.S. Cancer Statistics Working Group, 2021 U.S. Cancer Statistics Working Group. U.S. Cancer Statistics Data Visualizations Tool, based on 2021 submission data (1999–2019): U.S. Department of Health and Human Services, Centers for Disease Control and Prevention and National Cancer Institute; https://www.cdc.gov/cancer/dataviz, released in November 2022. Google Scholar U.S. Census Bureau, 2020 U.S. Census Bureau, 2020. American Community Survey (ACS). Available at: https://www.census.gov/programs-surveys/acs. Table DP04. Google Scholar U.S. Energy Information Administration, 2022a U.S. Energy Information Administration, 2022a. Residential Energy Consumption Survey. 2020 RECS Survey Data. Available: https://www.eia.gov/consumption/residential/data/2020/#fueluses. Google Scholar U.S. Energy Information Administration, 2022b U.S. Energy Information Administration, 2022b. Office of Energy Demand and Integrated Statistics, Form EIA-457A of the 2020 Residential Energy Consumption Survey. Available: https://www.eia.gov/consumption/residential/data/2020/hc/pdf/HC %201.1.pdf. Google Scholar U.S. Environmental Protection Agency, 2015 U.S. Environmental Protection Agency. Standards of performance for new residential wood heaters, new residential hydronic heaters and forced-air furnaces. 80 FR 13671, March 16, 2015. https://www.govinfo.gov/content/pkg/FR-2015-03-16/pdf/2015-03733.pdf. Google Scholar Vermeulen et al., 2019 R. Vermeulen, G.S. Downward, J. Zhang, W. Hu, L. Portengen, B.A. Bassig, S.K. Hammond, J.Y.Y. Wong, J. Li, B. Reiss, J. He, L. Tian, K. Yang, W.J. Seow, J. Xu, K. Anderson, B.T. Ji, D. Silverman, S. Chanock, Y. Huang, N. Rothman, Q. Lan Constituents of Household Air Pollution and Risk of Lung Cancer among Never-Smoking Women in Xuanwei and Fuyuan, China Environ. Health Perspect., 127 (9) (2019), Article 97001 Google Scholar Ward and Lange, 2010 T. Ward, T. Lange The impact of wood smoke on ambient PM2. 5 in northern Rocky Mountain valley communities Environ. Pollut., 158 (3) (2010), pp. 723-729 View PDFView articleView in ScopusGoogle Scholar Ward et al., 2012 T. Ward, B. Trost, J. Conner, J. Flanagan, R. Jayanty Source apportionment of PM2.5 in a Subarctic Airshed – Fairbanks, Alaska Aerosol Air Qual. Res., 12 (2012), pp. 536-543 CrossRefView in ScopusGoogle Scholar White and Sandler, 2017 A.J. White, D.P. Sandler Indoor wood-burning stove and fireplace use and breast cancer in a prospective cohort study Environ. Health Perspect., 125 (7) (2017), Article 077011 View in ScopusGoogle Scholar World Health Organization, 2018 World Health Organization, 2018. Household air pollution and health World Health Organization. Ambient (outdoor) air quality and health [online], http://www.who.int/mediacentre/factsheets/fs313/en/ (May 8, 2018). Google Scholar Cited by (0) Published by Elsevier Ltd. Recommended articles Parental occupational exposure to chemicals and risk of breast cancer in female offspring Environmental Research, Volume 227, 2023, Article 115817 Julie Elbaek Pedersen, Johnni Hansen Maternal exposure to particulate matter early in pregnancy and congenital anomalies in offspring: Analysis of concentration-response relationships in a population-based cohort with follow-up throughout childhood Science of The Total Environment, Volume 880, 2023, Article 163082 Ronit Nirel, …, Hagai Levine Using the POD sampler for quantitative diffusive (passive) monitoring of volatile and very volatile organics in ambient air: Sampling rates and analytical performance Environment International, Volume 179, 2023, Article 108119 P. Pérez Ballesta, …, E. Woolfenden View PDF Show 3 more articles Article Metrics Mentions News Mentions: 1 View details About ScienceDirect Remote access Shopping cart Advertise Contact and support Terms and conditions Privacy policy We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the use of cookies. All content on this site: Copyright © 2023 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.",
    "commentLink": "https://news.ycombinator.com/item?id=37810052",
    "commentBody": "Indoor wood burning raises women’s lung cancer risk by 43%Hacker NewspastloginIndoor wood burning raises women’s lung cancer risk by 43% (sciencedirect.com) 185 points by geox 21 hours ago| hidepastfavorite184 comments sneak 18 hours agoHalf the comments here are just providing an implementation of the fizzbuzz instead of addressing the article’s point.https:&#x2F;&#x2F;www.samharris.org&#x2F;blog&#x2F;the-fireplace-delusion> Once they have exited your chimney, the toxic gases (e.g. benzene) and particles that make up smoke freely pass back into your home and into the homes of others. (Research shows that nearly 70 percent of chimney smoke reenters nearby buildings.) Children who live in homes with active fireplaces or woodstoves, or in areas where wood burning is common, suffer a higher incidence of asthma, cough, bronchitis, nocturnal awakening, and compromised lung function. Among adults, wood burning is associated with more-frequent emergency room visits and hospital admissions for respiratory illness, along with increased mortality from heart attacks. The inhalation of wood smoke, even at relatively low levels, alters pulmonary immune function, leading to a greater susceptibility to colds, flus, and other respiratory infections. All these effects are borne disproportionately by children and the elderly. reply hulitu 1 hour agoparent> Research shows that nearly 70 percent of chimney smoke reenters nearby buildings.Which research ? Has this guy ever been in a village ? When 70% of \"smoke reenters nearby buildings\" than the chimney from one house gets to the door of the other.Will be nice a little common sense before making such statements. reply dathinab 18 hours agoparentprev> and into the homes of others.I really wish indoor wood burning, especially if not used for heating but luxury, would be banned at least in citieslike wtf. is it legal to harass your neighbors with smell and toxic fumes every day just because you want to feel a bit comfy (to clarify: I mean comfy by having a fancy looking fire place you use additional to your normal heating system because you like how it looks)sure there are countries and areas in countries where for various sociology economical reasons banning it isn&#x27;t acceptable at all and should not be donebut a ~4 million city where one of the _most costly ways to heat_ is by burning wood logs it&#x27;s a bit of a different story ;)and I know some parts of the city are poor and use very old ovens, but funnily they have less of an smell issue (I can&#x27;t judge the toxicity) because they tend to burn preprocessed pellets for heating instead of not necessary well suited or fully dry wood for the looks reply chongli 17 hours agorootparentI really wish indoor wood burning, especially if not used for heating but luxury, would be banned at least in citiesIt doesn’t need to be banned, it just needs stringent regulations to limit the particulate emissions. You can buy wood stoves today that emit less than half a gram per hour (0.5g&#x2F;hr) of PM2.5 particulates. These modern wood stoves are also very efficient and beautiful to look at, so they provide a bit of luxury to go with their heating performance. reply philips 16 hours agorootparentThe testing of many wood stoves was called into question a few years ago. https:&#x2F;&#x2F;energynews.us&#x2F;2021&#x2F;03&#x2F;18&#x2F;report-finds-systemic-failu... reply brnt 13 hours agorootparentprevThe problem with filters is they have a lower limit. PM2.5 cuts out much of the smell, but none of the carcinogenicity (PM0.1 or so). The only filter that really works is a sealed cap. Burning biomatter is just not worth the health impact or the cost and effort to mitigate. It&#x27;s like capturing CO2: a stupid idea. reply bigstrat2003 17 hours agorootparentprev> like wtf. is it legal to harass your neighbors with smell and toxic fumes every dayYou say harass, I say enhance. I love when I can smell a neighbor burning wood, it&#x27;s one of the best things in the world. And therein lies the reason why your wish isn&#x27;t going to come to pass any time soon: we don&#x27;t all agree that the thing you want to ban is bad. reply throwbadubadu 16 hours agorootparentStrange, I just hate it.. I&#x27;m fine with loud parties, your loud stupid music, don&#x27;t care how you run your garden, but nothing is more hatable than someone harassing me with smoke or other headache creating smells, and going for my health. I go into kind of half nature, to enjoy the fresh air, and then there is someone making a fire out of some mix of pure laziness, ignorance, unnecessity and pyromania, despite it being also illegal here.I mean a good campfire is hot, doesn&#x27;t produce much smoke, goes quick up and high, usually not that smelly, ok who needs it.. (though we even these are quite toxic, so please a reasonable romantic evening campfire, and not a 5 hour bonfire on a hot and sunny day?) But unfortunately it is always not just that, but I need to smell a lot of carcinogenic smoke, green waste smell, or even worse plastic or other garbage smell, my cloyhes stink, my furniture stinks.. and we are also just past the time to just burn stuff.(Noone here does this out of poorness bte.) reply SoftTalker 16 hours agorootparentI burn all my yard waste as my locality has regulated all other means of disposal to the point of it being time- and cost-prohibitive.I pile it up and have a bonfire a few times a year. reply jemmyw 14 hours agorootparentThey&#x27;ve regulated chipping mulching and composting? reply SoftTalker 13 hours agorootparentNo, but I don&#x27;t own a wood chipper, and renting one is costly and time-consuming. reply jemmyw 9 hours agorootparentI got the largest electric one I could find. It&#x27;s pretty good, entirely metal housing so it won&#x27;t fall apart, and no gas or oil to deal with. I don&#x27;t know if 120v vs 240v makes a difference in that regard for Americans who want more powerful electric gear, we&#x27;re 240v. reply vel0city 7 hours agorootparentThe US is 240V. reply ShadowBanThis01 5 hours agorootparentYou know that&#x27;s not the general-purpose outlets. Most houses have a few 240V circuits that are wholly devoted to large appliances. reply vel0city 5 hours agorootparentIt doesn&#x27;t change the fact if you&#x27;re in the US and you really want 240V you can (usually) get it. Just about everyone has 240V service at their home. And I think we should normalize offering 240V outlets in garages. reply ShadowBanThis01 2 hours agorootparentYep, I agree about the garage outlets. replyyread 14 hours agorootparentprevwtf, where do you live so I can avoid it? reply hollerith 17 hours agorootparentprevThe PM2.5 not only gets in your lungs, it enters your bloodstream and gets everywhere including your brain. Peter Attia MD says there is no doubt it will shorten lifespan and healthspan.>your wish isn&#x27;t going to come to pass any time soonIt is coming to pass: for example, in the Bay Area, woodstoves and fireplaces have been banned in new construction since 2005. reply ipaddr 15 hours agorootparentIf Peter Attia says it, it must be true. reply joecool1029 9 hours agorootparentprev> It is coming to pass: for example, in the Bay Area, woodstoves and fireplaces have been banned in new construction since 2005.It doesn&#x27;t make much sense to install it there anyway, the climate isn&#x27;t cold enough... reply philjohn 16 hours agorootparentprevBig disagree there - it exacerbates Asthma in sufferers, that alone makes it bad. reply dathinab 12 hours agorootparentprevIf some people get coughing fits from it it really stops mattering weather you like it or not. reply wannacboatmovie 13 hours agorootparentprevMeanwhile Seattle is under a perpetual cloud of choking smoke for like 2 months out of every year because WA and Canada are incompetent when it comes to forest management, and the best people can do to pretend to care is tell you to rubber band a furnace filter to a box fan and wear N95 masks to work.Banning wood stoves is like banning plastic bags. It&#x27;s a superficial, empty gesture that is a drop in the bucket that makes no meaningful impact.I&#x27;ll continue to heat my cabin with wood, thank you. reply dathinab 17 hours agorootparentprev> very old ovensit&#x27;s kinda fascinating, some of this ovens are older then 70 years, some where build in when this houses where build ~120 years ago (but update)they also are all health hazards as in you manually have to control the exhausts when starting them and if you mess that up (e.g. fall asleep) CO will leak into you apartment and might silently kill you .... reply t0bia_s 2 hours agorootparentHave you ever used that oven? reply t0bia_s 2 hours agorootparentprevIf state is not capable bring cheap energy for heating, what can we do? Let people freeze by regulations and ideology of clean CO2 free environment? reply eternityforest 15 hours agorootparentprevIf we care about the climate and want to spend money on it, subsidizing cleaner tech for those countries that need it seems like a great way to start, because even if you don&#x27;t believe in climate change you&#x27;re still preventing poverty and unhealthy air for people.I don&#x27;t think it should be banned entirely though, that seems a bit extreme and likely to stir up controversy, even though it sure would be nice to have less smoke.I don&#x27;t quite get the appeal of indoor fires, most houses are so warm all winter I wouldn&#x27;t want any more heat, but outdoor bonfires are a nice tradition, as long as they&#x27;re not happening enough to kill people with fumes. reply antisthenes 17 hours agorootparentprevWhile we&#x27;re doing that, can we please also ban 2-stroke engines used for landscaping.The gasoline fumes mixed with small particles that get kicked up are absolutely noxious and can&#x27;t possibly be good for you.This is especially egregious in dense suburban areas, where there&#x27;s landscaping going on _every_ single day of the growing season. reply iancmceachern 16 hours agorootparentCalifornia just did this reply imoverclocked 15 hours agorootparentYeah, but my Stihl MS880 is still legal here in California… and likely will be for some time. Good luck getting a usable, heavy-duty chainsaw to work on any tech besides a two-stroke motor of some kind. reply Scoundreller 13 hours agorootparentWhat’s the barrier to making a li-ion powered electric and “usable heavy-duty chainsaw”? reply imoverclocked 3 hours agorootparentI have a Stihl combi-system which includes a polesaw attachment. It’s electric with a lithium ion battery backpack that weighs at least 20 lbs.I love the thing. I’ve felled trees that are up to 8 inches in diameter and done some real cleanup work around my property with it.It might be possible to make something for me but the average heavy chainsaw user needs to be able to trek into the woods and spend significant time away from any power source. Furthermore, I want to be nimble and able to drop everything and run when a tree starts moving. A heavy weight strapped to my back for real tree work is a death trap.Firefighters have even more stringent requirements for their chainsaws in highly demanding environments. If you want to convince anyone that an electric chainsaw is workable, convince them first. Everyone else will be easier.I use my bigger chainsaws for bigger trees (just took down a mature Monterey Pine that dropped its last needle) and some Alaskan milling. The latter takes a lot of power for a single pass. I’m already lugging 22lbs just for the power head, making that electric with similar capacity to a tank or two of gas will be completely impractical in terms of weight. That’s especially so in rugged terrain. reply dzhiurgis 13 hours agorootparentprevAre you using heavy-duty chainsaw for urban landscaping? reply imoverclocked 3 hours agorootparentI have 16-ish acres in the Santa Cruz mountains. Between my property and the aging neighbors who I share a driveway, there is plenty of opportunity for different sizes of chainsaw. I also live in a redwood log cabin with more redwood and other large diameter trees on the property. I originally purchased the bigger chainsaw for some Alaskan milling but it has come in handy elsewhere.I would not recommend this chainsaw to the uninitiated buyer. It’s a lot of chainsaw and you only need money to buy the thing. Seriously, I’ve mentioned it to seasoned loggers and they raise an eyebrow and say as much. If I didn’t do Alaskan milling I wouldn’t have enough acreage to justify it. reply formerly_proven 17 hours agorootparentprev> While we&#x27;re doing that, can we please also ban 2-stroke engines used for landscaping.Why only for landscaping? These fucks need to be banned, period. It was a huge mistake to ban them in vehicles while grandfathering all the old crap in. Unfortunately mechanics keep decade-old two-stroke junk alive, and spare parts aren&#x27;t even banned. reply 3-cheese-sundae 17 hours agorootparentHow exactly would you propose spare parts get banned? reply ethbr1 17 hours agorootparentSpare parts are manufactured and supplied into channels.If you ban that, inventory eventually depletes, and people shift to alternatives.Similar to freon.I would maintain a carve-out for chainsaws. Low use, specialty requirements, and electric alternatives can&#x27;t replicate. reply mcpackieh 17 hours agorootparent> Spare parts are manufactured and supplied into channels. If you ban that [...]Easier said than done. You&#x27;d need to impose BATFE-style regulations and measures. You&#x27;re talking about arresting people for selling partially manufactured parts, parts which they claim are actually for other things, plans for parts paired with uncut sheet metal, etc. Is a clutch illegal to manufacture if it happens to fit on some model of two-stroke motor, even though it could also be used for other things? Have fun getting tangled up in these weeds. And the only way you&#x27;d stand a chance of enforcing this is by threatening people with many years in prison for what are non-violent crimes.People tolerate this sort of thing in the case of the BATFE because nobody really needs a machine gun, very few people have one in the first place, there&#x27;s little parts commonality with other machines and because machine guns kill people in very visceral and obvious ways. But with two-stroke motors you have a whole ton of people who use these machines to save on huge amounts of labor (you&#x27;re fucking with the way they put food on their table and pay rent, not just their fun at a gun range) and the alternatives are subpar. You exempt chainsaws from this, but that merely reflects what you are personally familiar with and consider reasonable. And even if the alternatives weren&#x27;t subpar, you&#x27;d still be asking people to purchase new expensive machines instead of performing cheap repairs on machines they already own. Point is, the incentive and willingness to skirt two-stroke spare part bans would be much higher than with machine guns. Your bans would fail without extremely draconian enforcement. reply Groxx 16 hours agorootparentIf you&#x27;re aiming for perfection, sure.Two stroke engine parts don&#x27;t have a massive legally protected cult following like guns, nor are they high-cost many-times-repeated consumables like drugs. They&#x27;re closer to antiques - some will collect them, most won&#x27;t bother if they&#x27;re not available.Ban it and you&#x27;ll stop major places from selling it, and minor places from widely advertising they have it. You won&#x27;t stop anyone dead-set on continuing to do so, but you will stop most people because the cost will be higher and it&#x27;ll be much harder to find. reply bbradley406 2 hours agorootparent>Two stroke engine parts don&#x27;t have a massive legally protected cult following like gunsYes they do, they&#x27;re called dirtbikes and a large part of America enjoys riding them. Other countries use them in the scooter form factor. Could some of the use cases be covered by alternatives like the electric Surron? Sure. Will you successfully pass a law against them in the next 30 years? Not a chance. reply mcpackieh 14 hours agorootparentprevIf you&#x27;re not shooting for perfectionism and won&#x27;t bother to enforce a spar parts ban, then why propose a spare parts ban in the first place? Just ban the production&#x2F;sale of new two-stroke machines, not the spare parts. As old machines wear the cost and effort of keeping them in service will eventually exceed the activation energy for buying a new machine. An unenforced ban of spare parts would accomplish nothing productive; it would waste your political capital and train a new group of people to have contemptuous disregard for laws.With respect to the antique analogy, you have it completely backwards. Antiques are things which have outlasted their practical usefulness but are kept around anyway because people have some nostalgic or aesthetic affinity for them. Machine guns fall into this category; they are essentially fetishes (in the traditional nonsexual sense of the word). Virtually nobody is earning their living with machine guns. Two stroke motors on the other hand are in active use by thousands if not millions of people to earn a living. If they were in fact antiques, then they wouldn&#x27;t be a problem in the first place because antiques aren&#x27;t put to use. If you banned new production of two-stroke machines, the ones that already exist would gradually become antiques despite the existence of legal spare parts, and eventually the only people keeping them in running order would be the antique geeks who own it because they like it, not because they earn a living with it. The people who are actively earning a living with two stroke motors today are the people you [should] have a gripe with, the lawn care guys who run two-stroke leaf blowers in your neighborhood aren&#x27;t antique collectors doing it for fun, they&#x27;re guys trying to feed their children. replykiliantics 17 hours agorootparentprev> like wtf. is it legal to harass your neighbors with smell and toxic fumes every day just because you want to feel a bit comfySame can be said for car use. Majority of trips are a few miles and could be done by bike or replaced with transit. reply paganel 17 hours agorootparentOnly if you live in urban areas where you don’t have to wait longer than 15 minutes for the bus to come, and, most importantly, where it is safe (especially for women) to take said bus late in the evening&#x2F;at night. I hope that that recent and horrific stabbing in NYC made many people aware of the security aspect. reply dathinab 12 hours agorootparentso if you life in any arbitrary urban area in most of the EUbecause then you often don&#x27;t even need to take the bus due to places you need to go to (e.g. super market) and places you come from (e.g. home) being much more intermixed&#x2F;closer to each othere.g. for me places to buy food, medicine, a doctor, dentist, fast food, train, bus, package station, kiosk, bakery, restaurant, cafe, a park like thing all in ~8min walking distance and I&#x27;m somewhat at the outer are of the citystill if you have disabilities and can&#x27;t properly walk 8min you still need a car reply paganel 12 hours agorootparentI&#x27;m saying that, as a man, if you have to wait for your SO at the bus&#x2F;train station late in the evening when she comes back from work then maybe public transport is not for everyone, in this case for almost 50% of the population (i.e. women). Been there, done that.If you live in an European big city where this problem doesn&#x27;t exist, where women have no problem and especially no fear to use public transport late in the evening all by themselves, then consider yourself (and your SO) lucky. replyAndrewKemendo 15 hours agoparentprevPeople struggle to change their behavior when confronted with data suggesting something they love is measurably worse for themIn my experience, we decide to just ignore the information and find other reasons to give higher weight to benefits, in order to resolve the cognitive dissonance without requiring a change of behaviorAdd up enough of these cognitive dissonances and you find yourself living in a fantasy land where sugar is \"fun!\" and burning wood indoors is \"cozy.\" reply ShadowBanThis01 5 hours agorootparentActually, this article will deter me from using my fireplace this year. I enjoy chopping wood and then having somewhat-frequent fires in the winter. But my fireplace can also burn gas, which I will use now instead but far less frequently because it costs a shitload of money. reply AndrewKemendo 4 hours agorootparentThat’s great! Hopefully more people are inspired to do so and it starts a wave.I know I’ve been advocating a long time since studies have been coming out for the last few decades on this.I have three kids and have lived in multiple houses that we had to retrofit the chimney &#x2F;fireplace because we didn’t use it and they are a huge heat sump.Would be great to have reduced cancer across the population! reply cableshaft 14 hours agoparentprev> The inhalation of wood smoke, even at relatively low levels, alters pulmonary immune function, leading to a greater susceptibility to colds, flus, and other respiratory infections.Well that put a damper on my desire to attend a bonfire later this month. reply ShadowBanThis01 5 hours agorootparentI see what you did there. reply grecy 17 hours agoparentprev> Once they have exited your chimney, the toxic gases (e.g. benzene) and particles that make up smokeKeep in mind that modern wood stoves have catalytic combustors that burn the wood, then the smoke, then usually the smoke again.They extract vastly more heat from the wood, and they emit vastly less toxic gasses out the stovepipe.Also note they are legally required in many, many places now.(Older stoves and fireplaces are grandfathered in, but you can&#x27;t install a new \"old\" stove in many towns around the world) reply SoftTalker 16 hours agorootparentHeating with wood is also carbon-neutral. reply sneak 14 hours agorootparentOnly if you replant the trees. reply AuryGlenz 14 hours agorootparentThis may come as a shock to you but trees do, in fact, replant themselves just fine without human intervention. reply reducesuffering 16 hours agorootparentprevYa, the realistic alternative to carbon-neutral wood burning is carbon-intensive natural gas burning... How is that better? reply KeplerBoy 15 hours agorootparentHave you not heard of heat-pumps? I know they won&#x27;t work according to old wive tales, but they actually do. reply reducesuffering 14 hours agorootparentI&#x27;m not arguing that they don&#x27;t work, but the \"realistic\" alternative is that people will still mostly use natural gas if they end up stopping wood-burning. Heat Pumps are also still using a mostly-fossil-fuel grid at times where there&#x27;s the least solar and the grid is most carbon-intensive. Thus, wood burning is still the only carbon-neutral option. I&#x27;d still rather see heat pumps on a fully renewable grid, but we should have less carbon emissions getting there. reply dathinab 17 hours agorootparentprevyes I&#x27;m mainly complaining about grandfathered in stoves, or such which are incorrectly installed maintained reply gardenhedge 17 hours agoparentprevLink to the fizzbuzz implementation answers? reply echelon 17 hours agorootparentIt&#x27;s one of the most dismissive assertions I&#x27;ve seen on HN to date. The comment would have been much better without it, or simply rephrased as \"To the point: ...\" reply tempaccount420 16 hours agorootparentIt&#x27;s pretty funny and reflects the unfortunate state of culture at the current point in time. reply paulcole 16 hours agoparentprev> Half the comments here are just providing an implementation of the fizzbuzz instead of addressing the article’s point.Are you saying this as an indictment of the commenters or in praise of the progress we’ve made here at HN? reply j45 17 hours agoparentprevCOPD from fire is a real thing.I recall seeing some studies about indoor natural gas stoves not being the greatest either. reply adolph 17 hours agoparentprevsamharris.org: an example of secular intransigence“And I say this to Kai \"Look: We&#x27;ll shape the handle By checking the handle Of the axe we cut with—\" And he sees.”https:&#x2F;&#x2F;www.poetryfoundation.org&#x2F;poems&#x2F;57150&#x2F;axe-handles reply formerly_proven 18 hours agoparentprevScientifically this has been proven quite well, and people who aren&#x27;t dumb notice as much - posh-er neighborhoods have terrible air quality in fall&#x2F;winter evenings because they all fire up their cozy fireplaces. If you can smell it on the street... well, that&#x27;s ground level. Open windows and ventilation inlets are higher up and more affected.The thing is more that people don&#x27;t want to hear it, like at all. These fireplaces are a big status symbol and considered totally green and environment-friendly because it&#x27;s wood, right, so it&#x27;s renewable, so has to be good. That they have no filters and terrible firing, resulting in exhaust that would make an 80s diesel tractor blush in shame, nobody cares. It&#x27;s natural, so it&#x27;s eco, so it&#x27;s green, so it&#x27;s good and it&#x27;s cozy. So fuck off. reply ethanbond 17 hours agorootparentFWIW I think the eco intuition is not very durable. I can’t imagine people holding onto that when confronted, rather it’s just something people don’t consider without it being brought up.But the coziness, positive emotional affiliation is definitely hard to overcome. Especially since it’s pretty much the smell and sound of bad burning itself that is so desirable. reply sneak 15 hours agorootparent> I can’t imagine people holding onto that when confrontedTry it sometime and be surprised! People round up a bunch of common retorts: the smoke goes up, it smells good, it’s an old tradition, thousands of years, etc.People think that smoke isn’t that bad for you if it doesn’t make you cough, because it doesn’t feel bad in the lungs.The fireplace delusion taught me a lot about myself and my own mind, and humans in general. We fucking LOVE to fool ourselves into rationalizing our own preferences. reply ethanbond 15 hours agorootparentNo I am agreeing with you. I’m saying that the issue is not that they refuse to believe it’s bad when explained, it’s that even when they’re convinced it’s bad, they “net out” to keeping them due to all the positive affiliations despite believing the harms. replyTacticalCoder 19 hours agoprevIn my vacation house (where I went to live for one full winter season), I turned the fireplace into an enclosed (still wood-fueled) fireplace: there&#x27;s a fan (if I want to) but it&#x27;s not blowing smoke, it&#x27;s blowing warmth from the warmed up \"firebox\" (whatever that is called) inside the house.It&#x27;s day and night compared to that same house back when it had a regular open fireplace: smoke would drift inside the house, no matter how cautious I&#x27;d be. And the efficiency was terribly bad.Now I still get to enjoy the view and once the fire is well started and the smoke correctly goes through the chimney, I can even open the enclosed fireplace&#x27;s for a while and hear the crackling &#x2F; get the full ambiance.Lovely. I hope to go there in december. Falling asleep in front of the fireplace is one of my greatest joy in life.It did cost about 5 K EUR IIRC (to transform the open fireplace into an enclosed one) but then warming the house with wood is super cheap (an enclosed fireplace is much more energy efficient than an open fireplace).FWIW I warmed the house during the entire summer with five \"stères de bois\" [1] (an old french unit that isn&#x27;t used for anything anymore beside wood logs meant to be used as fuel), paid 60 EUR each. So 300 EUR to warm the house (south east of France, mild winter).BTW: house in the middle of nowhere, hardly any neighbors. So they don&#x27;t get my smoke and I don&#x27;t get their.[1] https:&#x2F;&#x2F;fr.wikipedia.org&#x2F;wiki&#x2F;St%C3%A8re reply ahaucnx 17 hours agoparentI second this. My sister has an enclosed fireplace and last time I visited her, I measured the air quality next to it and was surprised to find no elevated PM2.5 inside the room.However keeping the door just a bit open emitted a lot of smoke into the room and gave a completely picture.So I would recommend measuring the air quality in your home if you run any fire place.I should probably write a blog post about it sometime. reply X6S1x6Okd1st 17 hours agorootparentYup I got a pm2.5 & AQI monitor a couple years ago and was so excited to have a fireplace in my new house, but trying it with the monitor showed me I really shouldn&#x27;t. I am considering an enclosed fireplace to still be able to use it. I am now wondering if just emitting it into the air (out of the chimney) is acceptable.EDIT: also it&#x27;s been shocking to check on it while cooking, our vent hood is mediocre to bad so we end up keeping a door open most of the time we are cooking reply desiarnezjr 16 hours agorootparentIs this a modern wood stove or insert? I recently bought one for a new home we&#x27;re building, and from my research they&#x27;re incredibly efficient these days. Not installed yet so we will see, I suppose. reply X6S1x6Okd1st 16 hours agorootparentOpen hearth reply dzhiurgis 13 hours agoparentprevDoes it have power? 5K buys ducted heat pump with built in HRV. Or you could&#x27;ve heated your cabin for a decade just using space heaters... reply zemvpferreira 19 hours agoprevGreat prospective study. If I&#x27;m reading correctly, using a wood-fired stove&#x2F;fireplace for around a month per year for 10 years or more will increase your chance of lung cancer by ~70%.To put that in perspective, smoking regularly in the same time-span increases your chance of lung cancer by over 20x. reply sampo 18 hours agoparent> smoking regularly in the same time-span increases your chance of lung cancer by over 20x.But what if you were to smoke only for the same amount as you mention using the fireplace, around a month per year? reply zemvpferreira 18 hours agorootparentYou&#x27;ll have trouble finding many people to study.From the evidence I&#x27;ve seen, intensity of smoke exposure is much more problematic than volume. Light smokers still have 5-10x the chance of developing lung cancer than a non-smoker.That might be a good first estimate for smokers who only smoke one month out of the year. reply ghusto 15 hours agoparentprev> using a wood-fired stove&#x2F;fireplace for around a month per year for 10 years or more will increase your chance of lung cancer by ~70%Really?! I just started using a wood fireplace, so this concerns me. I wish I understood things like \"Cox regression\" and other sciency things so I could trust this study. reply adolph 15 hours agoparentprevWhat is the base rate? Increasing a very small number by 1.7x or 20x is still a small number. One might say “oh any increase is too much.” I acknowledge that and counter that there is opportunity cost to addressing topics of small effect. For example, I would expect that if unconstrained atmospheric combustion was a big problem, it would show up in occupations with high likelihood of exposure.“Occupational exposure as a firefighter” has been evaluated by the International Agency for Research on Cancer (IARC) and classified as “possibly carcinogenic to humans”, with strongest evidence for testicular cancer, prostate cancer and non-Hodgkin lymphoma.https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC7254920&#x2F; reply cannonpalms 17 hours agoparentprevYou&#x27;re conflating correlation with causation. Adding only a single factor under study (use of wood-burning fireplaces) will not necessarily produce the measured result; it could have been the result of a confluence of factors correlated with use of wood-burning stoves that you personally may lack. reply jncfhnb 17 hours agorootparentSmoke is a highly likely casual factor. You don’t really win any points for saying IT COULD BE CORRELATION. This one seems like an extremely safe bet. It’s not even a huge effect size. reply zemvpferreira 16 hours agorootparentprevYou&#x27;ve clearly not read the study at all or live in a world where only perfect double-blind studies are acceptable. I wish we could all live there but sometimes you have to make-do with reality. reply this_steve_j 16 hours agoprevI’m really grateful to the HN community for bringing quality research (like this study and others) to my attention.If there are old habits and behaviors, like indoor fires in this case, that can be easily avoided (by me) in exchange for a better chance at avoiding lung cancer, I’m surely glad to have that knowledge so I can avoid unwitting cozying up to a serious ailment later in life.There is so much anecdotal evidence around old habits but a high powered study, followed by more like it, can usually be relied upon. reply hmottestad 19 hours agoprevIn Norway it’s very normal to have a stove and burn wood pretty much every day in the winter season. Open fireplaces are rather uncommon though, essentially because they produce a lot of smoke and very little heat.I hope they do a follow up study that separates fireplace and stove. reply sneak 18 hours agoparentWoodsmoke re-enters buildings within a 500-1000 meter radius of the fire.Burning wood is a very bad idea. It smells nice and we like it, but it is positively terrible for our health and should not be done.Traditions, however, are strong. reply amdolan 18 hours agorootparentProperly operating a wood burning stove will release very little smoke. This is where the center stone is in the range of approx 300-600F (100-300C). The chimney will produce clear heat and there is no detectable smell when standing outside. Much different result than a fireplace.It’s referred to as double combustion, or secondary combustion. reply foobiekr 14 hours agorootparentYou often will not smell small particulate. In addition the problem here is that the stove has to be operated correctly and in good condition which, unfortunately, is not something you can rely on. reply 7952 13 hours agorootparentprevIs there any reason to believe that they are properly operated? reply simonsarris 11 hours agorootparentprev> it is positively terrible for our healthNorway is top 10 in the world for life expectancy. Compared to all other possible ills, it suggests the danger here is a tad histrionic. reply mrtksn 17 hours agoparentprevInteresting, women lung cancer rate in Norway seems to be slightly higher than most of the Europe bu significantly lower in male population: https:&#x2F;&#x2F;canceratlas.cancer.org&#x2F;the-burden&#x2F;lung-cancer&#x2F; reply hmottestad 15 hours agorootparentThat is very interesting. Seems to be the case for all the nordic countries. Sweden stands out a bit more than Norway though, they actually have a higher lung cancer rate for women than for men. reply ksimukka 17 hours agoparentprevYou might be interested in: https:&#x2F;&#x2F;www.fhi.no&#x2F;en&#x2F;cl&#x2F;air-pollution&#x2F;wood-burning&#x2F;I don’t remember the exact details, but the Oslo municipality has a program that will provide home owners a grant to replace their inefficient wood stoves.Statistics Norway might have some interesting data on how many households burn wood. reply mrangle 19 hours agoprevBuy an air filter with an AQI meter. Not only will you never use your wood stove again, you may end up cutting down on your pancake Saturdays. reply nkurz 18 hours agoparentPresuming you&#x27;ve actually done this test, I think it must depend on your woodstove and how it is installed. We heat almost exclusively with wood, and have a modern EPA compliant wood stove connected to a continuous stainless steel liner that runs to the top of the chimney. We got an AQI meter based on comments on like this.The AQI meter showed that there was no perceptible difference in air quality with the wood stove on versus off. Startup shows some smoke that decreases in an hour, but continuous operation shows no effect. Since we are in a cold climate, we burn essentially 24-7 for the entire winter, and thus startup effects can mostly be ignored.You are right that there was a tremendous decrease in air quality when cooking, especially anything that was fried. So I agree that you might cut down on pancakes, but my evidence does not back your assertion you will \"never use a woodstove again\". To the contrary, using the meter tremendously reduced my worries about indoor air quality due to using the woodstove.Did you actually do this test and find different results? What sort of system were you testing? reply newsclues 18 hours agorootparentYeah, comparing an old out of date stove&#x2F;fireplace from peoples memory to modern high efficiency models seems rather disingenuous on the part of commenters. reply jwr 17 hours agorootparentprevDoes the smoke magically disappear once it exists your chimney? Because your neighbors might get AQI results that aren&#x27;t that great — or yours might deteriorate, too, if everybody around you starts heating with wood.It&#x27;s just not a good idea, period. reply chongli 16 hours agorootparentA modern wood stove doesn&#x27;t emit much smoke at all. It recirculates smoke through the combustion chamber (and&#x2F;or through a catalyst) which causes secondary combustion. This greatly increases the energy efficiency of the stove and gives you a much cleaner burn. Not only are you not polluting your house with wood smoke particles but you aren&#x27;t polluting your neighbours either! reply Applejinx 16 hours agorootparentThis. Mine has the recirculation thing with extra air channels and a big ol&#x27; soapstone wall interposed between the combustion chamber and the exhaust, and you can&#x27;t just start it up full of wood, you have to fire it properly at which point it&#x27;s hot enough to burn its own smoke, which you can see happening. And then once it&#x27;s going, engage the catalyst, which also burns the smoke more, until there&#x27;s basically nothing left.No more &#x27;nice cozy smell of neighborhood wood fires&#x27;, but on every other possible metric, way way better. reply nkurz 15 hours agorootparentprevI was responding to the specific claim that woodstoves cause measurable problems with indoor air quality in the houses where they are used. I&#x27;m willing to believe they can, but in my direct experience, mine didn&#x27;t. I live in a rural enough area (less than 1 house per sq km) that my neighbors are unlikely to be affected by my woodburning, and I am not likely to be affected by theirs.The question (and I agree it&#x27;s open) is what effect dispersed burning has on the larger scale environment. My current belief is that local wood harvesting and burning in a modern efficient stove is better than the alternatives currently available to me. If you have evidence to the contrary, please provide it rather than just making blanket statements. reply recursive 16 hours agorootparentprevThere&#x27;s nothing magic about it. Sufficiently high combustion temperature reduces particulates. reply kevinmchugh 16 hours agoparentprevLosing hot breakfast on the weekend is a very strong argument against monitoring my air quality reply stephen_g 19 hours agoparentprevI still feel like there must be a difference in particulate matter from things like combustion products (which we know are very bad) and those from cooking - as long as nothing is getting hot enough to char and smoke… Still a good idea to have a good exhaust fan in your range hood (and pick induction over gas if possible) but still… reply kibwen 17 hours agorootparent> and those from cookingI presume the parent commenter is referring to cooking using a gas stovetop, rather than to any particulate emitted by the food itself. reply mrits 19 hours agoparentprevA good vent hood does wonders here reply nwellinghoff 20 hours agoprevI don’t get it. I thought with a properly functioning stove all the smoke goes outside not into the room. Same thing with a fireplace. reply 2devnull 19 hours agoparentPeople with stoves live around other people with stoves. The air inside houses comes from somewhere. :) they hint at this in the study somewhere. reply graeme 17 hours agoparentprevIf you can smell it, then is spreading through the room. There’s no rule of airflow which says air only flows upwards.And once you can smell smoke the particulate levels are generally very high reply simonsarris 10 hours agorootparent> There’s no rule of airflow which says air only flows upwards.When you are burning a fire there is a rule, it&#x27;s called the stack effect, it is the very point of having a long (and ideally straight) chimney. reply graeme 4 hours agorootparentYou’d be a fool to burn a fire in your home if your fireplace has no chimney. But that doesn’t mean all the smoke goes upwards.Surprisingly I couldn’t find a single study which measured PM 2.5 from fireplaces with chimneys. However, a study on wood stoves (substantially better sealed) found a 30% rise.Which is actually less than I would have predicted. I’d still expect a fire to be higher, but I’m open to being wrong. If I’m ever going someplace with a chimney and fire I’ll bring a pm 2.5 meter for fun. reply xattt 19 hours agoparentprevLighting the fireplace causes some smoke to drift into the room, before the fire is hot&#x2F;large enough to cause a strong draft.There are also some times when the wood is a dud and takes a long time to start. reply aaomidi 18 hours agorootparentSpecifically until the chimney is hot enough. You can speed up the process though by using a fresh air intake for the stove rather than it using the air in the house. reply thelastparadise 19 hours agoparentprevNot all.I grew up in a house that used wood as its primary means of heat (and two smoking parents but they smoked outside exclusively).These days I live in a house primarily heated by a heat pump (in a region where it does get below 0F on a normal winter), but it has a wood stove.I can&#x27;t imagine a woodstove setup that doesn&#x27;t leak some level of particulate to the inside. No matter what, you can tell there&#x27;s wood burning inside. It is a pleasant smell (burning white&#x2F;red oak), but probably not good for health.So even though I grew up with a wood stove burning all winter every winter and have one now, I seriously appreciate the heat pump as a clean and efficient means of heat for 90% of the season.To me, wood is a great emergency form of heat as we have plenty of deadfall where we live. But I would bot recommend it otherwise.And don&#x27;t get me started on firewood processing... reply andtong 19 hours agoparentprevPerhaps some of the smoke leaks out of the stove into the room. Maybe a rocket mass heater[1] could help. It uses a thermal siphon to pull all the exhaust outside.[1]https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rocket_mass_heater reply nwellinghoff 19 hours agorootparentI mean wouldn’t you see it? I use my fireplace all the time in the winter and I would classify the amount of smoke that gets into the room as “negligible”. Especially compared to smoking a cigarette.Would love if this study actually followed up with some measurements and the “why” of actual fireplaces and stove setups. reply mrangle 19 hours agorootparentYou can measure. Buy an AQI meter. But I recommend buying an air filter with an AQI meter attached, as when you see what the supposedly negligible amount of smoke is actually doing to your air quality you are going to want the former. reply 29athrowaway 19 hours agoparentprevWood with radon.A radon meter is a useful thing to have a home. And a moisture meter. reply mcpackieh 18 hours agorootparentRadon&#x27;s most stable isotope has a half life of only a few day, so if there&#x27;s radon in the wood there must also be uranium or thorium in the wood to create that radon, and that must have come from the soil. Not impossible I guess, but I think if trees in your area have that much uranium&#x2F;thorium in them, your primary concern would be radon coming up through the ground into your house.If there&#x27;s a correlation between burning wood and radon related health issues, I suspect it&#x27;s not because of radon coming from the wood but rather smoke from the wood making residents particularly susceptible to injury from radon coming up from the ground. Such an effect is known to exist with tobacco smoke at least. reply 29athrowaway 16 hours agorootparentI am no scientist but I know radon is bad, and it comes from underground.Underground water can contain radon, and it is an issue when it comes to wells and houses. Trees can suck that radon and when you burn wood a radon meter shows an increase. reply mcpackieh 14 hours agorootparentA tree could suck up water containing radon, but after you cut down the tree and let it dry under a shed for a few weeks that radon will be gone. It will have all decayed away because the half life is only 3.8 days. For firewood to still contain radon a few weeks after the tree was cut down, it must have a fairly substantial amount of uranium or thorium in it (constantly producing new radon.) reply 29athrowaway 13 hours agorootparentInsighful, thanks. Then my radon meter must have been affected by something else. replymrangle 19 hours agoparentprevEven a small amount of smoke represents a thick concentration of particulate. reply trappist 16 hours agoprevI&#x27;m not seeing any attempt here to control for obvious confounders such as the relationship between this exposure and the climate you live in.I think I&#x27;m also not a fan of the (editorialized) verb (implies causality) used in this headline, for a purely observational study. reply FreeFlyFreeFall 12 hours agoparent\"...confounders such as the relationship between this exposure and the climate you live in.\"I&#x27;m not sure what this is suggesting, and I&#x27;m curious. reply 2devnull 20 hours agoprevSo if I’m reading this right, for never smokers who lived in a house that used a woodstove between 1-29 days of the year have the same risk as smokers who used one everyday if the year? reply elephanlemon 20 hours agoparentI wonder what the risk level is for people that regularly sit around campfires. reply mrangle 19 hours agorootparentHigh. I never used to notice. But after paying more attention to air quality in my environment, I now notice when air quality is particularly bad when before I hadn&#x27;t. Sitting around a campfire feels like at least pack of cigarettes was smoked. reply lacrimacida 19 hours agorootparentprevYes, am wondering the same. Recently went camping and the fire was lit for effects as we cooked food on mini&#x2F;portable stoves. It’s horrible I’d say, the smoke is thick, gets into everything but all I got is go with the program, it keeps the mosquitoes away and it’s charming. I wonder how long till this culture gets a reality check… reply dimitrios1 19 hours agorootparentprevThis is why I bring a smokeless fire pit with me camping now. I swear it makes a difference, but could be placebo. reply treadmill 17 hours agorootparentWhich one do you use? reply dimitrios1 8 hours agorootparentI have been pretty happy with the Solo Stove, but there are lots of options now. reply 2devnull 19 hours agorootparentprevPeople like to “bbq.” It’s like cancer as a hobby. reply mcpackieh 18 hours agorootparentDoes that seem unusual or alarming to you? Lots of fun and enjoyable things have tradeoffs that people willingly make. There&#x27; a balance to be struck between having fun in the moment and your long-term health, and most people tolerate a fair bit of risk in exchange for fun. Otherwise, nobody would drink alcohol, go skiing, etc. reply washadjeffmad 17 hours agorootparentPeople laugh at the light-proof HEPA cocoon I live in, but I refuse to accept the inevitability of my death at anything other than time&#x27;s hands. reply 2devnull 17 hours agorootparentprevNo it doesn’t. I think my point was that people budget their carcinogenic exposure in non-optimal, hard to rationalize ways. Like living in places with terrible air quality (California) and then being concerned about gas stoves. reply vore 13 hours agorootparentIt&#x27;s way easier to change your gas stove than to move. replygjsman-1000 20 hours agoparentprev1-29 days is… vague. However, the study also does not seem to consider the much, much tighter particulate emission rules that new wood stoves and similar have had to follow. The EPA has tightened it every few years (I think the last one was 2015?)An old wood stove is almost nothing (and I literally mean almost nothing) to a modern wood stove in efficiency or air cleanliness. It would almost be like if you did a study about the toxicity of cars but used data from when we had leaded gas. There’s still a toxicity there but your results are going to be a mess. reply happytiger 20 hours agorootparentIt’s a study in correlation, presuming causation.Research on whether dual burning stoves that reprocess their own exhaust and don’t leak like old ones (the ones you’re referring to) or other systems could be done, but this is baseline research.You can use the same logic to attack the choice of fuels, or species of wood, or types of seals and whether air is taken from the inside or the outside of the structure, and all of it would also be as valid as your argument. You could also try to invalidate this for only focusing on women. But, while valid arguments (as is yours), it sidesteps the purpose of the study.It’s simply not the hypothesis of this particular study, which was simply establish a correlation between the presence & use of wood burning fuel in fire burning devices and lung cancer incidence in women.Don’t throw out the bathwater with the babe. reply fzeroracer 19 hours agorootparentprevIt&#x27;s still an important investigation because unfortunately things are never idiot proof. For example, my parents have a pellet stove and it was not uncommon for it to occasionally leak, followed by my parents putting in a shoddy self-made fix.These kind of studies could indicate some flaw in the way we&#x27;re creating wood stoves, or some consumer-related failure that needs to be better worked around. Or it could be that consumers are just using older wood stoves. But you gotta start with a baseline. reply thaumasiotes 18 hours agoparentprevIf you mean the summary in the abstract, they report three hazard ratios:1. Women reporting more than 30 days of fireplace use per year (smokers and nonsmokers both included) had 68% more lung cancer than women reporting that their home didn&#x27;t contain a fireplace (again, smokers and nonsmokers both included).2. Non-smoking women reporting more than 30 days of fireplace use per year had 99% more lung cancer than non-smoking women reporting that their home didn&#x27;t contain a fireplace.3. Non-smoking women reporting 1-29 days of fireplace use per year had 64% more lung cancer than non-smoking women reporting that their home didn&#x27;t contain a fireplace.Not actually reported, but present by overwhelmingly strong implication:1b. Women (without regard to ever-smoker status) reporting 1-29 days of fireplace use per year had no significant increase in lung cancer compared to women reporting that they didn&#x27;t even have a fireplace. (Or else the results summary would have noted that they did.)These are funny numbers with suspicious lacunae in the reporting of results. I&#x27;m not inclined to take them very seriously. At the same time, it is undoubtedly true that inhaling smoke is bad for you.The study terminology always treats wood-burning stoves and fireplaces as equivalent. I would estimate that of the fixtures in the USA that are either wood-burning stoves or fireplaces, approximately 100% are fireplaces. This makes the study worthless if you want to draw conclusions about stoves, but OK for fireplaces. On the other hand, it doesn&#x27;t really matter that the study won&#x27;t let you draw conclusions about stoves, because there would be no point in having such conclusions. reply 2devnull 19 hours agoprevSo merely owning a stove but never burning wood gets you to HR 1.42. That makes the 1.6 for occasional use less bothersome (ymmv). reply bilsbie 18 hours agoparentMaybe you’re more likely to live near other houses that do use theirs. reply 2devnull 17 hours agorootparentPrecisely. And those houses may be in areas that have relatively more confounding factors (eg gas generator use will be higher). reply jskrablin 19 hours agoprevI heat my house with a wood burning enclosed fireplace connected to an external air supply. Any other heating solution is kind of the less optimal choice when you own wood supply chain (forest and some tools to prepare wood for heating).But yeah all those planet saving heat pumps need some advertising anyway. reply mindslight 16 hours agoparentOutdoor wood boiler. It keeps the wood mess and smoke outside. Plus you can either stack wood near it or use equipment to move it in bulk, rather than carrying wood inside by hand and re-stacking. They got a bad rap because of poor efficiency (the first generation was essentially a barrel inside a barrel) and people&#x27;s tendency to burn trash in them, but newer models are two stage gasifiers just like modern wood stoves.Although honestly I kind of still want a heat pump for the shoulder seasons and the times I&#x27;m away. reply sambeau 15 hours agoprevI recently did a deep dive into woodsmoke as I couldn&#x27;t understand how ~2 million years of evolution with woodsmoke hadn&#x27;t rendered us completely immune to its effects.I couldn&#x27;t at that time find a paper that showed that it was very harmful to humans, just a lot proving that it was harmful to mice, rats, rabbits, and guinea pigs (and a lot of forwards stating the dangers then linking to another animal research paper)—which bothered me—rodents don&#x27;t have ~2 millions years of woodsmoke in their evolution history. I found papers about forest fires that some showed bad effects other were inconclusive and wouldn&#x27;t;t draw conclusions as there were too much smoke from other combustable substances mixed into the smoke. I also found a paper about firefighters exposed to woodsmoke that found no ill effects on them.So I remained unconvinced.But this paper is different: not animals, large study, evidence. SO I will read it in-depth.But I am still baffled. How could over a million years of evolution not have bred this out of us? I&#x27;m genuinely curious. reply cstrahan 15 hours agoparentWhy should you expect ~2 million years of evolution to bread that out of us?The ability to start and control fires for warmth and cooking purposes greatly increases the odds of survival for individuals and their offspring.If I may personify evolution as form of rhetoric: evolution doesn’t care if you limp along through a painful life, or if your genes guarantee a horrific death at the end of it. Evolution only cares about your ability to produce offspring that in turn reach maturity and produce offspring of their own, and so on.Resources are finite in this world, so sure, the fittest tend to outcompete less fit creatures, and the former will perpetuate its genes while the latter won’t. This is where the notion of “survival of the fittest comes from”. But it is a mistake to interpret that implication the other way around: that is, survival does not necessarily imply greater and greater fitness over time.Evolution isn’t about species becoming the best they could be, it’s only about being good enough. reply hollerith 13 hours agoparentprev>I couldn&#x27;t understand how ~2 million years of evolution with woodsmoke hadn&#x27;t rendered us completely immuneThere&#x27;s strong evidence for use of fire going back only .5 million years. More importantly, all existing humans are descended from a population that lived in sub-Saharan Africa till 70,000–50,000 years ago:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Recent_African_origin_of_moder...Although that population probably used fire before migrating out of Africa, they probably did so only for cooking and to keep wild animals away and almost never used fire indoors. reply 7952 13 hours agoparentprevFor evolution to happen a trait needs to effect reproductive success. Diseases that hit later in life will only have a minimal effect. Your kids are already grown by the time you get cancer and probably having children of their own. There may be slightly less resources available to raise the next generation if you die but the effect is minimal. Genes that help&#x2F;hurt the survival of young people are going to have a much a more profound effect and influence evolution more. reply yread 14 hours agoparentprevFor large part of the two million years not having a fire killed people a lot more than lung cancer. reply sambeau 13 hours agorootparentThat was certainly my top theory. Cooking saving many more babies than woodsmoke killed. reply not_your_vase 15 hours agoparentprevMaybe we just devolved.Most of the people have been living close to nature up to like a 100 years ago. And today? I have like 8 allergies, and I have no idea about the root cause of 6 of them. I would die in 2 days on a field of random weed, by sneezing myself to death. And I suspect I am far from the record holder in this site. reply vba616 11 hours agorootparentMaybe it&#x27;s the weeds that are evolving. reply susiecambria 10 hours agoprevInformation like this--the link and the discussion--is why I love HN.Unlike others, though, with posts like this, I&#x27;m thinking about wood stove use and burn piles in my part of rural America. What does the Census data say about the use of central heat? What is the health of those who live around me and rely on wood stoves? What can be done locally to transition folks off of wood stove use? What positive health impacts will we see? What can be done via public education to reduce the installation of fireplaces in new homes? reply curation 10 hours agoprevI&#x27;m aware of this project: https:&#x2F;&#x2F;www.noeton.fi&#x2F; They have a device that goes into your fireplace removes cancer causing substances to EU standards. They are a group of Finnish scientists and friends who wanted to find a way to make something they love less polluting and safer to use. reply Ferret7446 10 hours agoprevAre men immune to lung cancer? This seems like an oddly targeted study, and one cannot help but wonder if there are political reasons for it. reply dathinab 18 hours agoprevthe ways you can (short term) kill yourself by doing indoor wood (or pallet) burning wrong are quite many and even in countries which relative strict regulations when it comes to chimes and similar deaths by it aren&#x27;t that rareif something can kill you just because you used it somewhat wrong it it&#x27;s somewhat broken in some aspect of ventilation you probably should assume that even a more correct long term usage might have a good chance to have serve long term negative health effects reply s3krit 13 hours agoprevIn the UK recently, there has been a designation of &#x27;smoke-free&#x27; areas, specifically as a result of research done on the harm to health caused by PM2.5 particulates. It&#x27;s become quite popular for middle-class homes to fit multifuel stoves (i.e., wood, coal, whatever) in order to heat the main room of their house. Largely because they look nice but there is some argument to be made that with the cost of living increasing, especially around energy, that it is somewhat economical to do so - particularly if you have access to free&#x2F;foraged wood. The idea is you can still use these stoves so long as the fuel you are burning is considered &#x27;smokeless&#x27;, which in reality isn&#x27;t actually smokeless but refers to wood that has a moisture content between 15-20%, as well as a number of commercially-available fuels you can buy from large hardware stores like B&Q and some supermarkets.Another development that personally affects me is that local councils are now able to start enforcing this &#x27;smoke-free&#x27; thing on owners of narrowboats[1]. For those that don&#x27;t know, the UK has a 2000 mile network of inland canals on which thousands of us live, in canal barges of about 7ft beam. In the winter, a lot of these boats&#x27; primary method of heating our homes is using multifuel stoves. It&#x27;s an itinerant lifestyle that can be hard at times but incredibly freeing and requires one to live somewhat more in tune with the seasons. A lot of boaters don&#x27;t have a ton of money and primarily fuel our stoves with foraged wood at varying levels of seasoning (the process of drying wood to reduce moisture content and make it burn better and cleaner). For instance, this year I got a knock on the roof of my boat by a tree surgeon, informing me we needed to move our boat as they had to chop down an ash tree that had succumbed to ash dieback [2]. I asked if I could take some of the wood and he said no problem, saves them a bit of work getting rid of it. And that&#x27;s now my wood for the winter. We are, of course, aware of the health hazards of groups of boats burning not the best wood, but a large percentage of live-aboard boaters really don&#x27;t have any other option.Sorry, not particularly relevant to the article posted but I thought it worth mentioning that there are cases where not burning things to stay warm in winter is not an option for some groups of people.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Narrowboat[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hymenoscyphus_fraxineus reply robg 17 hours agoprevIt’s amazing to me that active air exchanges for filtering and even humidity are not well optimized as part of modern housing HVAC. Is it costs or simply system integrations?For instance, another big indoor pollutant is cooking fumes, from both the heat source and to cook, carmelize, burn, etc - the food and vapors of changing temperatures. The range hood required by fire codes to stove sizes is not turning on automatically with pollutant detection. reply snapplebobapple 19 hours agoprevWhats the absolute risk change? Relative risk is utterly meaningless for actually deciding what to do in the context of alternate choices. reply pmontra 19 hours agoprevThere should be no smoke coming out from a modern stove or fireplace. They are enclosed, take air mostly from outside and blow their exhaust into the sky.Of course some smoke can come inside from the windows or aeration holes. reply dist-epoch 17 hours agoparentSo why do you smell them at street level if the smoke is blown into the sky. reply onecommentman 9 hours agoprevThe statistically significant cancer risk occurs when wood-burning occurs for more than 30 days a year. (Table 2).Burning 1-29 days looks statistically insignificant unless I’m reading Table 2 wrong - HR of .88-1.12 with CIs including 1. Suggesting no statistically significant effect observed from incidental or short-term exposure…roasting chestnuts for Christmas in your fireplace seems a fairly safe bet for most.As a point of public health calibration, women smokers are 13 times more likely to be a lung cancer victim than non-smokers, according to the American Lung Association.https:&#x2F;&#x2F;www.lung.org&#x2F;lung-health-diseases&#x2F;lung-disease-looku...“Men who smoke are 23 times more likely to develop lung cancer. Women are 13 times more likely, compared to never smokers.” 2004 US HHS Surgeon General reportWood smoke use for more than 30 days&#x2F;year is a ~50% increase in lung cancer, according to this one study. Maybe focusing on the order of magnitude greater causal agent would be the wiser public policy choice, in the US at any rate. reply whoopsie 16 hours agoprevTell this to Big Candle reply hirundo 19 hours agoprevI run a wood stove all winter long and some years never turn on the central heat. I wonder how much of the extra risk is offset by * lower fire risk from burning the fuel surrounding my house for the last few years. * more exercise from cutting, splitting and hauling wood * an air purifier running next to the wood stove all winter * about ~$1,200 per year saved on fuel * more open windows during the day during the winter due to cheaper fuel costs.That might not fully compensate for the extra lung cancer risk but it might be significant.Also since heat is cheaper this way, I heat my house more and am more comfortable all winter. This doesn&#x27;t seem like it would offset the health risks, but it is a hedonic compensation. reply natdempk 16 hours agoparentYou could do (most of) the good things from the above and not have the wood burning stove. I think this is kind of a false dilemma. :) reply inglor_cz 13 hours agoprevI understand that smoke is noxious, but I am still somewhat bewildered by the lack of corroborating historical data.My own grandparents grew up in fairly modest rural settings, where burning wood was literally the only way not to freeze (I am talking about the 1930s here). At least one of their native villages was located in a tight valley where smoke tends to stay.But lung disease wasn&#x27;t that much of a problem among them and their neighbors. People typically died of heart attacks, strokes etc., or lived to be fairly old. (Two of my four grandparents made it to 90.)I would expect a much more prominent data peak there. With other toxic substances, we often have it. With asbestos, even ancient Romans knew that slaves who mined asbestos died early of some weird lung disease etc.; the ancients lacked equipment, but were often good observers anyway. reply demandingturtle 10 hours agoprevglad it does no harm to men reply gjsman-1000 20 hours agoprevHave they examined if it is the wood itself; or the (possible - no idea if true) accumulations of microplastics and other chemicals into the wood?I would not be terribly shocked if we found out that wood is more dangerous to burn today than it used to be.Edit: For the downvoters who think it’s unlikely, consider that microplastic and PFAS in rainwater has passed safe levels. What do trees grow from?https:&#x2F;&#x2F;www.popularmechanics.com&#x2F;science&#x2F;a40859859&#x2F;rainwater... reply zdragnar 19 hours agoparentWood is already known to be cancer causing (via inhaled sawdust). Smoke is not going to be any better, and the quantity of microplastics trees available won&#x27;t really move the needle on that. reply gjsman-1000 19 hours agorootparentCan you give me some studies on that, assuming that the sawdust is from straight wood and not any form of wood beam which contains glues and treatments? reply zdragnar 19 hours agorootparenthttps:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;8266936&#x2F;Money quote from the abstract:\"The strongest association of exposure to wood dust and development of nasal cancer is observed in those occupations where workers are exposed to hard wood dust and chemical additives are not used.\"Note that the strongest association is when chemicals are NOT used - I suspect this might be more due to how people work the wood (less likely to sand particles board, more likely to wear protective gear) than the nature of the chemicals themselves.It does go on to say that various respiratory cancers are from both untreated and treated wood sawdust, but wood alone is well known to cause cancer. reply mrangle 19 hours agorootparentprevLook up any of the mountain of studies that connects air particulate, in general, to cancer.If you want to argue variables for air particulate&#x27;s relationship to lung cancer, then more power to you. reply zorrotorro 20 hours agoparentprevLegit interesting question. reply bbor 20 hours agoparentprevDefinitely possible IMO! Seems like it would be surprising tho - consider the disparities in volume. I think the boring answer is that breathing smoke is dangerous, despite the occult and ambience benefits of a hearth ;( it forms minute bits of plastic that are tinier than 5 millimeters long. Their size means they end up everywhere, even in our blood, where they range in size between 700 nanometers and 5,000 nanometers. reply Eumenes 19 hours agoprevBurn wood for many years and couldn&#x27;t care less about these studies. If you&#x27;re burning dry wood with a clean chimney and insure your seals are good, you&#x27;re fine. There has been renewed interest by climate warriors to ban wood stoves (they love to cite women and children being \"most at risk\", the emotional argument must be present to change hearts and minds). Ironic, given it&#x27;s mostly poor and working class people burning wood. Must be a big push by heat pump and geothermal industries to get rid of wood stoves in the name of \"public health\". Despite urban populations living with AQI magnitudes higher than rural populations who use these heating sources. reply abid786 18 hours agoparentThe science speaks for itself, but to a climate skeptic that might not be true. reply Eumenes 18 hours agorootparentThe \"science\" has no mentions of the stove itself, the wood used, how the wood was dried, the physical setup where the stove is being used, they also lumped in \"fireplaces\" which imo, negates the entire study, because those are open to the air. I&#x27;m not about to throw away 500k+ years of human history of burning wood because a few graduate students produced a paper. reply FreeFlyFreeFall 11 hours agorootparentGood points. I do think economical geothermal AC shouldn&#x27;t be too difficult in many places, but money has a way of influencing things; look at the septic industry horror stories. I&#x27;m curious about how urban vs rural AQIs look for different areas. It does seem more and more like climate control is used to control people through moral and consequently legal claims, and some whistleblowers seem to support that idea. Any skepticism is labeled \"conspiracy theory\".Also mentioned here, modern double and triple burning stoves are much more efficient, and catalytic combustor stoves leave almost nothing but CO2 and water vapor coming out of the flue pipe. At least for stoves and enclosed fireplaces, if temporary smoke from an initial burn is still a concern, then I&#x27;m sure a separate pipe with a fan could be set up to draw air through the flue until a thermometer reaches a certain temperature. A fresh air intake should eliminate smoke leakage during startup. A strong flame used to start the fire, and a draft can also reduce the total smoke produced during this process. Alternative formats like rocket mass heaters may help with this as well.Rather than draw attention to a real danger, encourage education, and distribution of better technology to help people burn a renewable resource safely, some would rather ban a medium that supports the survival of the poor completely. And some here would condemn all wood burners as ignorant archaic people who poison everyone around them and are too unintelligent to realize it. Sam Harris&#x27;s post, (https:&#x2F;&#x2F;www.samharris.org&#x2F;blog&#x2F;the-fireplace-delusion) also has this air of incredulity that anyone would burn wood since science shows that it produces harmful byproducts. He seems to fail to realize that wood can burn cleanly, and suggests that we burn gas, which we know also often produces harmful levels of benzene; he suggests that we burn nothing at all, while even the electricity we use comes from coal burning, and alternative \"green power\" still has a large carbon footprint at this point, no? Geothermal seems good, and avoids radon issues, but is a specialized wood-burning system better if geothermal won&#x27;t be enough? How do we move forward knowing that industry often influences studies? He writes, \"The unhappy truth about burning wood has been scientifically established to a moral certainty\". This is yet another \"scientist\" saying, \"The science is settled.\" Unfortunately for him and climate alarmists, no it hasn&#x27;t been, and posturing himself as correct because he believes in \"scientific rationality\" while simultaneously missing simple solutions to simple problems seems typical for him. That said, I do think sharing this info with communities who burn wood inefficiently is a good idea, and I see why their arguments about tradition and it being natural are criticized. He seems to see no exception to this \"traditional\" attitude among those who burn wood. If these communities don&#x27;t care about inefficient burning, and aren&#x27;t regulated, move away; stay mobile. Alternatively, perhaps using an air filter with a high MERV rating helps.Why ban a viable, renewable heat source that can be used to teach people valuable lessons? So that they can become dependent on wind turbines, such as the ones that froze in Texas a year or two ago, leading to the deaths of many who were wholly dependent on electric heat? I&#x27;d rather breath some smoke than freeze to death anyway; banning wood burning is worse than imposing regulations to make it almost completely safe, and seeing conclusive opinions before people consider alternative burning systems concerns me, especially when the alternatives posed are often just as dangerous. The most important thing is educating people though. When they know why a safety regulation exists, they&#x27;re much less likely to hate it and disobey it, as long as it doesn&#x27;t restrict alternatives and freedoms that it shouldn&#x27;t. reply zapdrive 5 hours agoprevnext [2 more] [flagged] dang 5 hours agoparentIf you keep breaking the site guidelines like this, we will have to ban you. Please see https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37202760 and the links back from there.If you&#x27;d please review https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html and stick to the rules when posting here, we&#x27;d appreciate it. reply belval 20 hours agoprev [6 more] [flagged] __alexs 20 hours ago[flagged]| parentnext [2 more] Do you often find a burning desire to just make shit up? reply sokoloff 19 hours agorootparentIt’s cancerous. reply mtVessel 20 hours agoparentprevThis was a study of US residents. reply jsight 20 hours agoparentprevThis study focuses on the US. reply 9991 20 hours agoparentprev [–] Good lord, dude, the study is right there. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A recent study associates indoor wood-burning from stoves and fireplaces with a 70% increase in lung cancer prevalence among U.S. women, including non-smokers.",
      "Not just regular, but even occasional use of wood-burning heating sources are pointed out as potentially harmful, contributing to lung cancer due to the presence of carcinogens in wood smoke.",
      "Notably, this study did not consider other factors like radon exposure and substandard housing conditions, offering a scope for further research on the subject."
    ],
    "commentSummary": [
      "Indoor wood burning, as in wood stoves and fireplaces, has been discovered to elevate lung cancer risk in women by 43% and cause respiratory issues, particularly in children and the elderly.",
      "Stricter regulations or a ban on wood burning are under consideration due to health risks and the potential harm to surrounding residents.",
      "There's ongoing debate about the effectiveness of bans, alternative solutions, their impact on air quality and climate change, and the challenges of enforcing such restrictions, highlighting the need for further investigation."
    ],
    "points": 185,
    "commentCount": 184,
    "retryCount": 0,
    "time": 1696769886
  },
  {
    "id": 37809834,
    "title": "Contour: Modern and fast terminal emulator",
    "originLink": "https://github.com/contour-terminal/contour",
    "originBody": "Skip to content Product Solutions Open Source Pricing Search or jump to... Sign in Sign up contour-terminal / contour Public Sponsor Notifications Fork 103 Star 1.7k Code Issues 184 Pull requests 7 Discussions Actions Security Insights contour-terminal/contour master 38 branches 29 tags Go to file Code Latest commit christianparpart Merge pull request #1239 from contour-terminal/fix/debian_deps … 692e7ab Git stats 3,980 commits Files Type Name Latest commit message Commit time .github Check both ubuntu packages for startup cmake Drop last bits of boost library uses debian Unify and update copyright mark docs Add config option to enable/disable size indicator on resize and make… examples Clean up C++ namespaces per library scripts Move qml package to common list src change names to adhere clang-tidy format test [CI] Adds check against .editorconfig. .clang-format Renamelibrary to . .codecov.yml Renamelibrary to . .ecrc [CI] Adds check against .editorconfig. .editorconfig [CI] Adds check against .editorconfig. .gitignore Rewrite UI to QML/QtQuick with Qt6 (rather than QtWidgets for Qt5). CMakeLists.txt Add build type into cmake summary CODE_OF_CONDUCT.md CODE_OF_CONDUCT.md: Fill the contact info. LICENSE.txt Rename LICENSE to LICENSE.txt README.md Update link to point to release AUR package SECURITY.md Create SECURITY.md TODO.md [terminal] Grid system refactor (with focus on performance). autogen.sh autogen.sh: Default QTVER to 6 metainfo.xml Fix infinite loop when next line does not exist mkdocs.yml update documentation and changelog vcpkg.json [Github CI] Update run-vcpkg action README.md Contour - a modern & actually fast Terminal Emulator contour is a modern and actually fast, modal, virtual terminal emulator, for everyday use. It is aiming for power users with a modern feature mindset. Features ✅ Available on all 4 major platforms, Linux, OS/X, FreeBSD, Windows. ✅ GPU-accelerated rendering. ✅ Font ligatures support (such as in Fira Code). ✅ Unicode: Emoji support (-: 🌈 💝 😛 👪 - including ZWJ, VS15, VS16 emoji :-) ✅ Unicode: Grapheme cluster support ✅ Bold and italic fonts ✅ High-DPI support. ✅ Vertical Line Markers (quickly jump to markers in your history!) ✅ Vi-like input modes for improved selection and copy'n'paste experience and Vi-like scrolloff feature. ✅ Blurred behind transparent background when using Windows 10 or KDE window manager on Linux. ✅ Blurrable Background image support. ✅ Runtime configuration reload ✅ 256-color and Truecolor support ✅ Key binding customization ✅ Color Schemes ✅ Profiles (grouped customization of: color scheme, login shell, and related behaviours) ✅ Synchronized rendering (via SM ? 2026 / RM ? 2026) ✅ Text reflow (configurable via SM ? 2028 / RM ? 2028) ✅ Clickable hyperlinks via OSC 8 ✅ Clipboard setting via OSC 52 ✅ Sixel inline images ✅ Terminal page buffer capture VT extension to quickly extract contents. ✅ Builtin Fira Code inspired progress bar support. ✅ Read-only mode, protecting against accidental user-input to the running application, such as Ctrl+C. ✅ VT320 Host-programmable and Indicator status line support. ✅ and much more ... Installation contour is packaged and available for installation on multiple distributions. Fedora use official package sudo dnf install contour-terminal Arch use AUR package Installing via Flatpak Install from Flathub Click the following button install Contour from the Flathub store. Prerequisites Make sure you have flatpak installed in your system (here is a tutorial on how to install it), and make sure that the version is >= 0.10 (check it using this command: flatpak --version) Add the flathub repository using the following command: flatpak remote-add --if-not-exists flathub https://dl.flathub.org/repo/flathub.flatpakrepo. Proceed with one of the following options: Install from Flathub Install from GitHub release Requirements operating system: A recent operating system (OS/X 12, Windows 10+, an up-to-date Linux, or FreeBSD) GPU: driver must support at least OpenGL 3.3 hardware accelerated or as software rasterizer. CPU: x86-64 AMD or Intel with AES-NI instruction set or ARMv8 with crypto extensions. Configuration In order to set up Contour, it is necessary to modify the configuration file contour.yml, which is initially generated in the $HOME/.config/contour directory. Some features also require shell integration. These can be generated via the CLI (see below), these currently exist for zsh, fish and tcsh. Installing from source Contour is best installed from supported package managers, but you can build from source by following the instruction below. You can Qt 5 or Qt 6, by default contour will be compiler with Qt 6, to change Qt version use QTVER=5 ./scripts/install-deps.sh to fetch dependencies and cmake flag -D CONTOUR_QT_VERSION=5. UNIX-like systems (Linux, FreeBSD, OS/X) Prerequisites ./scripts/install-deps.sh This script might ask you for the administrator password if a package dependency can be insalled via the system package manager. Compile cmake -S . -B build -G Ninja cmake --build build/ # Optionally, if you want to install from source cmake --build build/ --target install Windows 10 or newer Prerequisites For Windows, you must have Windows 10, 2018 Fall Creators Update, and Visual Studio 2019, installed. It will neither build nor run on any prior Windows OS, due to libterminal making use of ConPTY API. Set up vcpkg, preferably somewhere high up in the folder hierarchy, and add the folder to your PATH. cd C:\\ git clone git clone https://github.com/Microsoft/vcpkg.git .\\vcpkg\\bootstrap-vcpkg.bat Install Visual Studio Build Tools (make sure to select the CLI tools for C++, which you might need to do in the separate components tab). Install Qt6 (i.e. to C:\\Qt) Open the developer version of Powershell. In the contour source folder execute .\\scripts\\install-deps.ps1. This step may take a very long time. Compile In the developer version of Powershell: # change paths accordingly if you installed QT and vcpkg to somewhere else cmake -S . -B build -DCMAKE_TOOLCHAIN_FILE=C:\\vcpkg\\scripts\\buildsystems\\vcpkg.cmake -DCMAKE_PREFIX_PATH=C:\\Qt\\6.5.0\\msvc2019_64\\lib\\cmake cmake --build build/ # Optionally, if you want to install from source cmake --build build/ --target install Distribution Packages CLI - Command Line Interface Usage: contour [terminal] [config FILE] [profile NAME] [debug TAGS] [live-config] [dump-state-at-exit PATH] [early-exit-threshold UINT] [working-directory DIRECTORY] [class WM_CLASS] [platform PLATFORM[:OPTIONS]] [session SESSION_ID] [PROGRAM ARGS...] contour font-locator [config FILE] [profile NAME] [debug TAGS] contour info vt contour help contour version contour license contour parser-table contour list-debug-tags contour generate terminfo to FILE contour generate config to FILE contour generate integration shell SHELL to FILE contour capture [logical] [words] [timeout SECONDS] [lines COUNT] to FILE contour set profile [to NAME] References VT510: VT510 Manual, see Chapter 5. ECMA-35: Character Code Structure and Extension Techniques ECMA-43: 8-bit Coded Character Set Structure and Rules ECMA-48: Control Functions for Coded Character Sets ISO/IEC 8613-6: Character content architectures xterm: xterm control sequences console_codes Linux console codes Summary of ANSI standards for ASCII terminals Text Terminal HOWTO (Chapter 7.2, PTY) ANSI escape code in Wikipedia License Contour - A modern C++ Terminal Emulator ------------------------------------------- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. About Modern C++ Terminal Emulator contour-terminal.org/ Topics emoji linux console unicode library terminal xterm terminal-emulators vte hacktoberfest unicode-support grapheme-cluster sixel-graphics Resources Readme License Apache-2.0 license Code of conduct Code of conduct Security policy Security policy Activity Stars 1.7k stars Watchers 19 watching Forks 103 forks Report repository Releases 28 Contour 0.3.12.262 Latest + 27 releases Sponsor this project christianparpart Christian Parpart https://paypal.me/ChristianParpart Learn more about GitHub Sponsors Contributors 31 + 20 contributors Languages C++ 92.5% CMake 4.1% Shell 1.8% QML 0.5% GLSL 0.5% PowerShell 0.2% Other 0.4% Footer © 2023 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
    "commentLink": "https://news.ycombinator.com/item?id=37809834",
    "commentBody": "Contour: Modern and fast terminal emulatorHacker NewspastloginContour: Modern and fast terminal emulator (github.com/contour-terminal) 177 points by ingve 21 hours ago| hidepastfavorite166 comments mk12 20 hours agoThere are lots of terminal projects recently. Ghostty (https:&#x2F;&#x2F;mitchellh.com&#x2F;ghostty) and Terminal Click (https:&#x2F;&#x2F;terminal.click) come to mind. Also Warp and Fig but they don&#x27;t appeal to me because they&#x27;re proprietary (and Fig got acquired by Amazon). I&#x27;ve been very happy with kitty (https:&#x2F;&#x2F;sw.kovidgoyal.net&#x2F;kitty&#x2F;) for years and it would take a lot to make me switch. reply rob74 19 hours agoparentI switched to wezterm (https:&#x2F;&#x2F;wezfurlong.org&#x2F;wezterm&#x2F;) a few months ago and I&#x27;m pretty pleased with it (especially the search function, which is something not many terminals support), although it still has some rough edges. reply alwillis 13 hours agorootparent+1 for WezTerm.Switched from iTerm 2 like a year ago. reply SadTrombone 12 hours agorootparentprevHuge WezTerm fan, the dev is very friendly and responsive on GitHub as well. reply nikolay 10 hours agorootparentprevI love Wez, but I hate the lack of macOS native tabs. reply KolenCh 18 hours agoparentprevI&#x27;ve used kitty for quite a while but it has subtle problems with tmux that I use all the time (one of the primary reason is persistent session).From the FAQ https:&#x2F;&#x2F;sw.kovidgoyal.net&#x2F;kitty&#x2F;faq&#x2F; and other GitHub issues, it seems the recommendation is not to use tmux. Since I need to use tmux, I then change the terminal emulator I use instead. reply hiAndrewQuinn 18 hours agorootparentWhat kinds of problems? I&#x27;ve used tmux with kitty plenty and never noticed anything, but that&#x27;s probably because I just didn&#x27;t care to look. reply KolenCh 18 hours agorootparentThe one I encountered is color problem. I can never get xterm-256color works reliably and has to use screen-256color which supports lesser features such as italic.Running btop within tmux using kitty also has color problems, but Alacritty also fails at that. reply mfontani 17 hours agorootparentI had a long battle with making kitty work with tmux. I settled on the following in ~&#x2F;.tmux.conf, before moving to wezterm for good. Maybe it fixes things for you, too: set -g default-terminal xterm-256color set-option -sa terminal-overrides &#x27;,xterm-kitty:RGB&#x27; set-option -ga terminal-overrides \",xterm*:Tc:smcup@:rmcup@\" set-option -ga terminal-overrides \",screen*:Tc:smcup@:rmcup@\" set-option -ga terminal-overrides \",tmux*:Tc:smcup@:rmcup@\"I honestly don&#x27;t know whether all are needed, or only some. But with these, it worked well for me. reply KolenCh 16 hours agorootparentI tried the first line and in some situations (specifically with some remotes, i.e. kitty -> ssh -> tmux with that 1st line) it still won&#x27;t fix it.I gave up kitty because the author obviously don&#x27;t like tmux and is advising users not to use it. So it is more like a choice between kitty and tmux. Giving up kitty is easier for me in terms of features and time for retraining.What&#x27;s your reason to migrate from kitty to wezterm? reply mfontani 13 hours agorootparent> What&#x27;s your reason to migrate from kitty to wezterm?My reason to no longer using kitty is simple: I don&#x27;t like having to copy kitty&#x27;s terminfo data to every single system I connect to in order to have my terminal work.It&#x27;s fine for those few systems I very often connect to, of course.But I also connect to ephemeral systems, sometimes for a short session, and the toil and friction inherent in having to do that just isn&#x27;t worth it.Sure, \"kitty +kitten ssh ...\" can work in most people&#x27;s scenarios. Didn&#x27;t quite work in mine, due to various intricacies about my ssh setup - multiple ssh keys, handled mostly by ssh-ident.wezterm Just Worked for me. As I got \"back\" to also using other systems like Windows and MacOS, it Just Worked there, too. No fiddling with terminfo, no fiddling with $TERM, either.Happy days. reply KolenCh 11 hours agorootparentYour reasoning resonates with mine very much. I&#x27;ll look at wezterm soon and hope it will works for me too. reply mfontani 13 hours agorootparentprev> I tried the first line and in some situations (specifically with some remotes, i.e. kitty -> ssh -> tmux with that 1st line) it still won&#x27;t fix it.You likely need all of them ;-)IIRC it \"fixes\" terminfo stuff \"within tmux\" for kitty regardless of what $TERM is set. Mind you, I was using the latest tmux at that time. tmux 3.2?Here&#x27;s the issue I had: https:&#x2F;&#x2F;github.com&#x2F;kovidgoyal&#x2F;kitty&#x2F;issues&#x2F;3018Looks like it should be fixed on a very recent tmux; otherwise this MIGHT help: set-option -as terminal-features &#x27;,xterm-kitty:RGB&#x27;... or not. reply KolenCh 11 hours agorootparentI tried your solution using one remote server that I had trouble with before and it works! That&#x27;s really great. This probably solves enough problems with kitty for me that I can use it until I find a better solution. (I am using Alacritty at the moment but it has other problems so that I have to use kitty as fall back for some situations.)Thanks! reply dmos62 17 hours agorootparentprevColors in terminals are tough. There&#x27;s the terminal, then the shell, then tmux, then vim, and you want all of their configs in concert even when you&#x27;re switching between light and dark one or twice a day. Colors are tough in general. Even the browser color story kind of sucks. Dark reader goes a long way, but not enough reply KolenCh 15 hours agorootparentRight, I sort of understand that (not the details). I&#x27;m sympathetic to the kitty author, who explains why other terminals and tmux are doing things wrong, etc.I guess it is especially messy in the terminal, reading a little bit into the issues and it just feels like a ton of legacy hacks between different implementations.And comparing that to the browser is interesting. Both are \"ubiquitous\"—terminal emulator for cli&#x2F;TUI, and browser for GUI. May be because of there generality &#x2F; ubiquity, it is messy and impossible to \"make it right\". replyhn92726819 16 hours agoparentprevIs Kitty&#x27;s startup time (~200ms) not unbearable for other people? Is it faster now or do people not spawn new terminals constantly? reply fn-mote 7 hours agorootparentFrankly, my shell startup time is that long. (fish, some configuration files, no effort at optimization)That&#x27;s a pretty hyper-optimized life, when 0.2 sec to get a command line is a problem. reply lytedev 15 hours agorootparentprevI open new tabs or panes in my single terminal window, personally FWIW. reply vluft 13 hours agorootparentprevyou can run it with `--single-instance` (or `-1`) and you only pay (most of) that startup cost once. reply ljm 19 hours agoparentprevKitty&#x27;s my daily driver on Mac (when I&#x27;m not using eshell, anyway), but on the Windows side I am actually pretty damn impressed with Windows Terminal over WSL2. I don&#x27;t even bother using graphical mode on emacs in WT&#x2F;WSL, it is smooth as butter and super responsive in terminal mode and aside from using Linux, I&#x27;m actually preferring it as a dev environment to a Mac these days.In fact, it feels like there&#x27;s an ever-so-slight latency when on a Mac that Windows doesn&#x27;t have. At first I put it down to the keyboard feel, but even using the same keeb it feels a bit more &#x27;squishy&#x27; on the Mac. reply bitvoid 18 hours agorootparentI really like Windows Terminal too. The one issue I have is that I have Ubuntu as the default session and when I open the terminal from the start menu (i.e., hit command key, type \"terminal\", and hit enter), it opens but loses focus. The only way to remedy it is to disable WSLg (GUI support). It&#x27;s a long standing bug and a weird one at that. reply ljm 18 hours agorootparentYeah, that and sometimes the WSL session takes forever to boot up (a good 10+ seconds for the first time, for me).Once it&#x27;s working it&#x27;s fantastic. I don&#x27;t know what it is with MacOS, unless they&#x27;re subtly animating keypresses or something. Linux is as responsive as Windows is too. It&#x27;s a wired keyboard so nothing to do with bluetooth. reply camdv 17 hours agorootparentI think that&#x27;s the initial boot of the VM. I&#x27;v noticed it too. reply grudg3 15 hours agorootparentCorrect. If you run `wsl --shutdown` as an admin then start up Windows Terminal again the delay is back the first run. reply mardifoufs 13 hours agorootparentprevI love Terminal but I can&#x27;t get glances working on it correctly :(.(I know, the task manager is good enough usually but not for having an overall view of everything, and not really for GPU usage. ) reply p5a0u9l 9 hours agoparentprev> kitty I hate to be the sour consumer, but the kitty dev’s attitude I’d the main thing that turned me off kitty. Wanted to like it, couldn’t do it. reply eej71 20 hours agoparentprevAlso, Microsoft has their own new terminal program.https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;terminal reply danielvaughn 20 hours agoparentprevI wasn&#x27;t crazy about the proprietary nature of Warp, but I tried it and was sold almost immediately. I can use the same modifier-key commands to select and manipulate text that I&#x27;m used to, which feels super nice. They also provide such good sessionization out of the box that I don&#x27;t really feel the need to install tmux or anything like that. reply lobsterslive 19 hours agorootparentThe problem is that Warp is closed source, requires a log in, has extensive telemetry, promised to go open source but never did and has raised 78M from VCs including 50M from Sequoia, the FTX people. reply satvikpendem 19 hours agorootparentIndeed, VCs will want a return on their investment at some point, and it doesn&#x27;t seem like simply sharing some terminal commands in the cloud with your team will be enough to justify that valuation. This is especially true as the Zero Interest Rate Phenomenon of the last 15 years is now over.Personally I&#x27;d never use something VC backed for something so critical to my productivity. reply alchemist1e9 19 hours agorootparentHaving never heard of Warp and wondering what would get VCs to give that much to a terminal project I just took look at their website and it’s obviously the AI&#x2F;LLM aspect.With that said does anyone know which open source projects are looking at LLM integration. It’s not a bad idea. reply satvikpendem 19 hours agorootparentI took a look before this year and it used to be that their value proposition was more so geared towards sharing terminal inputs and outputs easily. Now they seem to be leveraging OpenAI APIs instead as their main value prop.However, I still don&#x27;t really see why someone would pay 12 bucks a month for that when they could just pay a little bit more for full blown ChatGPT and use AI for much more than terminal commands (and there are already terminal and vim plugins to directly query GPT with your API key, which is orders of magnitudes cheaper than Warp). It seems like a product that has no clear target market or target usage, seems more like a feature than a full product. reply jrm4 18 hours agorootparentprevIt&#x27;s not at all a bad idea as something like \"optional plugin,\" but it&#x27;s definitely absolutely a bad idea if we&#x27;re also mentioning VC&#x27;s. reply andrepd 19 hours agorootparentprevHaving an LLM run terminal prompts sounds like a recipe for disaster reply danielvaughn 16 hours agorootparentThankfully it doesn&#x27;t run it, it just inserts the commands into your prompt. It&#x27;s kinda nice but I&#x27;ve also been using the terminal for like 12 years, so I don&#x27;t find myself relying on it very often. reply alchemist1e9 19 hours agorootparentprevIt’s more about sophisticated replacement for tab completion. reply satvikpendem 18 hours agorootparentI can just run Llama locally for that, no VC required (and indeed there are plugins which do exactly this already). Llama and other local LLMs are being optimized to run on the CPU on less than 1 GB of RAM so it&#x27;s not as if only those with beefy GPUs can do so. reply alchemist1e9 18 hours agorootparentAgreed and the part I’m missing is these plugins which do this. Any suggestions or recommendations would be interesting to hear. Which terminals and which plugins etc. replymananaysiempre 18 hours agorootparentprev> I can use the same modifier-key commands to select and manipulate text that I&#x27;m used to, which feels super nice.I know this is not a complete solution, but just in case you weren’t aware, the keybindings in GNU Readline are nearly entirely configurable[1]. Getting some of the keybindings to work could require digging into serial terminal arcana[2], unfortunately, but here a terminal emulator that supports either the Kitty keyboard protocol[3] or the (venerable if impressively badly documented) modifyOtherKeys feature[4] could make things easier (messing with the serial driver[2] seems an overreaction to me, and you can’t get acceptable Ctrl-C behaviour that way anyway). The “new” terminal emulators mentioned in this thread (alacritty, contour, kitty, wezterm) should be able to do the trick, and of course xterm as well (probably rxvt too?); tmux compat needs help, though[5].[1] https:&#x2F;&#x2F;tiswww.cwru.edu&#x2F;php&#x2F;chet&#x2F;readline&#x2F;rluserman.html#Bin...[2] https:&#x2F;&#x2F;susam.net&#x2F;blog&#x2F;from-xon-xoff-to-forward-incremental-...[3] https:&#x2F;&#x2F;sw.kovidgoyal.net&#x2F;kitty&#x2F;keyboard-protocol&#x2F;[4] https:&#x2F;&#x2F;invisible-island.net&#x2F;xterm&#x2F;modified-keys.html[5] https:&#x2F;&#x2F;github.com&#x2F;tmux&#x2F;tmux&#x2F;issues&#x2F;3335 reply angra_mainyu 13 hours agoparentprevMy favorite is alacritty, although kitty is a close second, not a big fan of how kitty handles fonts&#x2F;colors compared to alacritty. reply diath 19 hours agoparentprevSeconding kitty, a while ago I tried like a dozen of different terminal emulators and tested a lot of features including link clicking, escape code rendering, Unicode rendering and so on, and only kitty and Sakura have passed most of my requirements, with Sakura having one issue that I cannot recall that ultimately made me choose kitty. Built-in window splitting without tmux is also great. The only downside is that when you SSH into a remote machine without kitty&#x27;s terminfo installed, you can&#x27;t even use backspace or other common keys, which is annoying. reply snapplebobapple 19 hours agorootparentI went wezterm because kitty wasnt always playing nice with tmux and i dont think i would go back at this point there is something i juat like more about wezterm i cant put my finger on. reply itsTyrion 8 hours agoparentprevI&#x27;ve tried kitty twice, both times I found it confusing and went back to anything else reply mongol 19 hours agoparentprevBoth Ghostty and Terminal Click developed with zig, it seeems... reply Razengan 14 hours agoparentprevI tried to try Warp, out of curiosity, and.... Are they fn kidding? A terminal that requires sign-up???What in the actual hell.Oh my god Fig too! Which of you is encouraging this crap?? reply alwillis 13 hours agorootparentI had the same reaction to having to sign-up to use a terminal?I was this has gone too far—hell no.Been a happy WezTerm for the past year or so. reply nikolay 10 hours agoparentprevSo, Ghostty will be BUSL, too? reply eviks 19 hours agoparentprevThanks for the links! Click is cool&bold in that it tries to get rid of all the obsolete baggage, pity it&#x27;s in C, though reply cout 19 hours agoprevI am always pleased to see code that is reasonably clean and uses modern C++.Sixel support is also nice to see. For all its faults, I think it&#x27;s the only pixel graphics extension that currently has a chance of widespread adoption.I am curious if the authors have made any attempts to quantify \"fast\". Are there any benchmarks? In particular, I would love to see some input latency comparisons. reply malablaster 21 hours agoprevI don’t see the point of pitching “fast” as its first selling point. Are people frustrated by the latency of the incumbents? reply hoherd 20 hours agoparentAfter using iTerm2 as my daily driver for like a decade, and then trying out some of these other \"fast\" terminal emulators, I was amazed at how slow the experience in iTerm2 is. I even kept alacritty around as a second terminal for a while for cases when not having iTerm2 features wasn&#x27;t a deal breaker, but eventually I just went back to iTerm2. I am still aware of how slow it is, and although I tolerate it, I wish it were as fast as some of the others.You can see the speed difference with your eyes by joining a tmux session from different terminal emulators and doing operations that redraw the whole terminal, such as searching in a less pipe. reply kergonath 19 hours agorootparent> eventually I just went back to iTerm2. I am still aware of how slow it is, and although I tolerate it, I wish it were as fast as some of the othersIndeed. It still has the best feature set of all the terminal emulators I tried, though, and I wish it worked on Linux. reply vlovich123 19 hours agorootparentprevI find Konsole very featureful and fast… reply greymalik 15 hours agorootparentI see… reply anta40 19 hours agorootparentprevThese days, I mostly use WezTerm on MacOS. And ocassionaly iTerm2.iTerm2 is slow? Man I guess I&#x27;m not a heavy command line user. I mosty open 5 or 6 tabs. tmux? Very very rarely. reply Macha 17 hours agorootparentThe slowness is noticeable often enough in simple typing latency, but there&#x27;s definitely a spectrum as to how much people notice or find it frustrating. On the one end there&#x27;s the people that won&#x27;t use anything but xterm, while on the other there&#x27;s people who don&#x27;t care about the seconds of latency for some remote development setups. reply ghusbands 20 hours agoparentprevYes. It&#x27;s common on at least Microsoft terminals to be waiting for the terminal to finish displaying the output it has been sent. I expect that it will be the same for many older terminal emulators. reply vidarh 19 hours agorootparentMost older terminal emulators tends to do fairly well. E.g. on Linux, rxvt tends to be one of the fastest. Many older terminals tends to do well not least because many older terminals implements jump-scrolling properly (scroll multiple lines at a time before updating the screen if the rendering doesn&#x27;t keep up) because there are even escape codes to turn that on&#x2F;off.There&#x27;s a weird \"bathtub curve\" when it comes to terminal speed where the oldest tends to be well optimised in terms of tricks to minimise rendering, the newest all tend to concern themselves very much with rendering smart, but often miss the old-school tricks, and quite a few of the ones in the middle are just really naive about everything. reply ku1ik 12 hours agorootparentSpot on! reply kergonath 19 hours agoparentprevShort answer: yes.Longer answer, there are diminishing returns, but speed definitely is a selling point. A terminal being slow is not necessarily something you notice straight away (if the software is competent in the first place; a lot of them are not). However, testing a new one with a lower latency and fewer buffering issues is when you feel it, and then it is difficult to go back. reply roydivision 3 hours agoparentprevLong term Gnome Terminal user, I recently tried Alacritty, ugh, massive difference. Went back to Gnome after a couple of days. Like typing through molasses. Terminals can be slow. reply cobbal 16 hours agoparentprevLatency doesn&#x27;t bother me too much, but throughput can really matter if I&#x27;m running a particularly verbose command. Often program speed is limited by how fast the terminal is able to clear the stdout pipe. This is especially true when running commands inside emacs \"build\" window (which is running who-knows-how-many regexs). If it&#x27;s more than ~10,000 lines long then an instant command can be slowed down to more than 10 seconds. reply porridgeraisin 5 hours agoparentprevI only care about latency from pressing Ctrl+alt+t to typing the first char of the command.xfce4-terminal does just fine for this. So I just use that. reply user3939382 20 hours agoparentprevI’ve never been able to perceive the difference. I use iTerm2 and I can’t perceive how it could be faster. reply vidarh 19 hours agorootparentMost of the time people only notice the slowness if they are in the habit of regularly letting huge amounts of output stream to the terminal, so it really depends on how you use your terminal. I agree with you - for me terminal speed was a solved issue in mid 1980&#x27;s, and has not been an issue since. reply gusfoo 17 hours agoparentprev> Are people frustrated by the latency of the incumbents?Not me, no. And I&#x27;m not even frustrated by pretty much anything. I have some Cygwin SSH sessions on the go and I&#x27;m running &#x27;screen&#x27; in those terminals. It works and has done since the &#x27;90s. reply kristopolous 20 hours agoparentprevSeems to be a windows&#x2F;mac problem. In Linux I&#x27;ve never really been bothered by it in at least 20 years reply tssva 19 hours agorootparentThe speed of Gnome Terminal and Konsole on Linux along with Microsoft Terminal on Windows didn&#x27;t bother me until I tried a couple of the newer faster terminals. Now all 3 seem slow to me. reply cpach 19 hours agorootparentprevIt’s not a Windows&#x2F;Mac thing. There are definitely slow terminal emulators for Linux as well. I’m quite sure you wouldn’t want xterm as your daily driver. reply kristopolous 18 hours agorootparentI use exactly that. It&#x27;s fine for me and I&#x27;m very much a terminal first person.Maybe people have extremely fancy shell setups? Perhaps it&#x27;s a font thing that slows it down? Can you demonstrate?I&#x27;m usual a tmux&#x2F;zsh&#x2F;vim user (although I&#x27;ve promised to switch to emacs for years :( )I just timed my xterm. I&#x27;m displaying about 2.3MB&#x2F;s of characters.Basically here was my test $ dd if=&#x2F;dev&#x2F;urandom bs=10M count=1hexdump -C > &#x2F;tmp&#x2F;test $ ls -l &#x2F;tmp&#x2F;test -rw-r--r-- 1 chris chris 51773449 Oct 8 09:32 &#x2F;tmp&#x2F;test $ time cat &#x2F;tmp&#x2F;test ... wait wait wait .... 51773449 &#x2F; (total time) &#x2F; (2^20)Now this of course depends on the size of the window. I&#x27;m on a 4320x3840 dual 4k monitor and the window geometry was 1334x928 for this test.I acknowledge this computer (i7-8650U ~ 2018 thinkpad with a boring UHD Graphics 620) is capable of faster output but the xterm featureset is vaaassstt (albeit extremely complicated) and I&#x27;ve found the other terminals to be lacking.My only faithless move away from xterm was probably 17 or so years ago with rxvt when I was on a laptop with ballpark 16MB of memory. It&#x27;s memory footprint and startup time was way smaller probably for its total lack of features and unicode support. That made a difference on my Toshiba Protege 120mhz pentium I was using at the time.When I was a macOS and a Windows developer, slow terminals were definitely a problem though reply slotrans 17 hours agorootparent2MB&#x2F;s is glacial. Your computer is very fast, it should be able to print over 1GB&#x2F;s without breaking a sweat.Casey Muratori demonstrated this with his refterm project. It seems like a lot of interest in making fast terminals has followed from that. reply kristopolous 15 hours agorootparentWhy do I care? I can&#x27;t read things that fastThis isn&#x27;t rhetorical. I can&#x27;t answer the question reply SoftTalker 14 hours agorootparentprevI use xterm. Seems fine. Not sure what I&#x27;m missing I guess but I get my work done. reply ur-whale 20 hours agoparentprev> Are people frustrated by the latency of the incumbentsYes. If you write code in vim as I do, terminal latency (snappiness) is very important. s a matter of fact, that&#x27;s the main reason I paid attention to this project. reply sodapopcan 19 hours agorootparentYa, there was a very noticeable VImprovement (lol, sorry) switching from iTerm to Alacritty. I&#x27;d never heard of this project before, looks really interesting. reply IshKebab 19 hours agoparentprevOn Windows, yes. Not on Mac or Linux though. reply blueflow 18 hours agoprevnext [–]Prerequisites: .&#x2F;scripts&#x2F;install-deps.shPlease don&#x27;t do this! List your dependencies normally. Packaging OSS projects is difficult enough already. reply jraph 16 hours agoparentI don&#x27;t know. If the script is legible, that&#x27;s an executable (and therefore probably tested), exact documentation.edit: yep, I just took a look and can confirm, it takes a few seconds to get the list of dependencies for several distributions. It&#x27;s way better than most readmes. reply fefe23 15 hours agoprevI would love it if it became standard practice for projects to list their dependencies.I was going to try this but it wanted to pull in Catch2 (some unit testing framework) and apparently has a dependency on Qt.It is good and nice if your project has unit tests, and by all means use a framework if you think you need one. But don&#x27;t make me install your framework just so I can do a standard release build. Only require it for building and performing the unit tests.If the home page mentioned the Qt dependency, I wouldn&#x27;t have downloaded the source code. reply amitizle 21 hours agoprevI&#x27;m not trying to annoy, I swear. I am using Alacritty for a few years. With tmux config that is moving with me for +- 8 years, except for speed of a terminal emulator, I can&#x27;t understand the difference between them (other than iTerm2 which is nice but has way too many features) reply eviks 20 hours agoparentwezterm has a unique great feature of programmable configuration (lua), which allows (among many things) to have custom keybindings depending on the foreground process, then there is also some keybinding modality (in Contour as well), then some have quick command panels, then there are various levels of tab support, then there are a bunch of other UI improvement...But if iTerm2 has too many features, implying you don&#x27;t care that much about them, you might not be invested enough to learn about the difference (there are many little things and not a great comparison of various terminals for an easy read) reply bloopernova 21 hours agoparentprevI was going to comment something similar. I use iTerm2 day to day for work, and gnome-terminal on my personal Linux box.What&#x27;s a compelling reason to switch terminals? Or maybe: is there a compelling reason not to switch? reply sergioisidoro 21 hours agorootparent> is there a compelling reason not to switch?Your terminal emulator is one of the most security sensitive things you use. Sudo password? SSH keys? logs? A lot goes through your terminal, so I think about 5 times before trying out new terminals. reply sodapopcan 19 hours agorootparentprevThe major reason I switched from iTerm to Alicritty is the config. I use cmd as my tmux key. This was really annoying to get working in iTerm and was brittle. It required all of macos overrides, iTerms overrides, Tmux config, and Karabiner Elements to get it how I liked it. With Alacritty, it&#x27;s all done with a clean Alacritty config and a couple of macos-level overrides (I use cmd-q and cmd-h differently in Terminal). Also, the vim+tmux combo is noticeably faster in Alacritty. I&#x27;m very interested in Contour. reply NetOpWibby 18 hours agorootparentI currently use iTerm AND Alacritty. If the latter supported tabs, I&#x27;d use that exclusively. reply 77pt77 13 hours agorootparentTilling window manager?I use i3. reply NetOpWibby 6 hours agorootparentI&#x27;ve never been able to get into window managers. I prefer the system default. reply da39a3ee 13 hours agorootparentprev> The major reason I switched from iTerm to Alacritty is the config.Exactly the same for me! My terminal config *must* be in version control. But iTerm2 keeps it in some sort of Apple Plist crap. It has a \"dynamic JSON profile\" feature but it&#x27;s hard to use correctly.Switching to Alacritty from iTerm2 has been fantastic; I don&#x27;t miss anything. I don&#x27;t use tabs; I use tmux. I need an OS-wide \"hotkey\" but a few lines of hammerspoon seem to do that perfectly. reply akdkfe223 20 hours agorootparentprevIt depends what kinds of things you find compelling but I would guess probably nothing if you haven&#x27;t already been sucked into the terminal emulator rabbit hole. iTerm2 and gnome terminal are both perfectly functional for 100% of the tasks you will actually need them for. reply eviks 20 hours agorootparentprevfor one, being able to have an identical user experience across all platforms reply bloopernova 17 hours agorootparentGood point. I share my shell config between Linux and macOS so my UX is mostly the same. Gnome-terminal and iTerm2 act similarly enough that it doesn&#x27;t really bother me moving between the two. reply angra_mainyu 13 hours agorootparentprevalacritty&#x27;s configurability is incredibly good, also the scrolling and crispness in rendering is a godsend, particularly if you&#x27;re a heavy user or even just do long tails. reply whalesalad 19 hours agoparentprevI like native tabs so that I can compartmentalize multiple tmux sessions. Alacritty with tabs would be perfect. Kitty seems to be that - but it’s got some oddities of it’s own. reply amitizle 3 hours agorootparent> I like native tabs so that I can compartmentalize multiple tmux sessions. Alacritty with tabs would be perfect. Kitty seems to be that - but it’s got some oddities of it’s own.Gotcha. I&#x27;m using tmux sessions for that. reply mgaunard 19 hours agoprevI use terminator because it&#x27;s reasonably lightweight (despite being written in python) and has good tab and split screen support.I&#x27;m not interested in tmux since I work on my computer directly.Is there anything with a comparable feature set that&#x27;s faster?I couldn&#x27;t care less about Mac or Windows support. reply whalesalad 19 hours agoparentTmux is great even for local dev. I use it locally and remotely.You might like to try kitty. reply snapplebobapple 18 hours agorootparentKeep am eye on zellij, its almost there for me for making me gice up my tmux that i put a lot of work getting just right reply mhitza 19 hours agoparentprevI&#x27;ve been using terminator as well for who knows how many years at this point, and aside from the startup I don&#x27;t percieve it as slow.What, or in which circumstances exactly do you feel your terminal emulator is slow and would need to be faster? reply abnry 17 hours agoparentprevThe GUI style choices turn me off to terminator. Silly reason. Maybe I should look at changing defaults. reply czottmann 18 hours agoprev> Available on all 4 major platforms, Linux, OS&#x2F;X, FreeBSD, WindowsI know it&#x27;s nitpicky, but it&#x27;s \"macOS\" since 2016, i.e. it stopped being \"OS X\" or \"OSX\" about 7 years ago. reply masto 19 hours agoprevWhat’s modal about it?I’m also in the “I’ve never wished my terminal were faster” camp. Certainly I’ve wished the things I run in it were faster, but that’s a different story.I split my time between macOS and ChromeOS, and I find it easier to just stick to what’s built in. The things that add friction to my development experience are not waiting for tty characters to draw, they are hopping around between the mess of tabs and windows between the browser, VS Code, and terminal sessions, all tied for a distant third; in second place, waiting for builds; and by far and away #1, trying to understand how this goddamn maze of abstraction and indirection and microservices and frameworks works so I can figure out where to actually put my code. reply maccard 12 hours agoparent> I’m also in the “I’ve never wished my terminal were faster” camp.I have an application that I run daily that is noticeably (15-20%) faster if I redirect terminal output to a file, or if I use a non-default terminal emulator. In this day and age, not being able to keep up with drawing 80 x 24 even with unicode is just not acceptable. reply mistercow 16 hours agoparentprev> I’m also in the “I’ve never wished my terminal were faster” camp.In the long long ago, I remember having to limit my terminal scrollback to avoid things getting laggy. That hasn’t been the case for a long time, because terminals started taking performance seriously.So I think advertising speed is more of a table stakes situation than a competitive advantage. I want to see that a new terminal emulator is thinking about speed because if they’re not, I can probably rule them out as an option immediately. reply ElCapitanMarkla 9 hours agorootparentI hadn’t had this issue for years, about 4 months ago I started at a new company which had a big rails monolith. I’m now super conscious of ever resizing iterm, if I leave a terminal running for more than a day resizing the terminal brings the entire laptop to its knees. reply ftyers 20 hours agoprevDoes it support Arabic&#x2F;Devanagari and not do crazy things with BiDi? If so I&#x27;m sold! reply Tmpod 1 hour agoparentIt looks like it uses Qt under the hood, so it likely leverages it. Haven&#x27;t tried yet though. reply slim 20 hours agoparentprevdo you think mlterm is slow? reply ftyers 20 hours agorootparentI&#x27;ve not used it, is it good? reply kps 19 hours agorootparentNot that commenter, but I also use mlterm and have for years, even though I don&#x27;t normally use BIDI or other ‘exotic’ language capabilities. It is just a terminal, in the do-one-thing-well sense. Its downside is its somewhat idiosyncratic configuration (although there is now a GUI for most of it, so newer users might not notice). reply Tempest1981 19 hours agoprevHow do I check if my older CPU has AES-NI?> Requirements: CPU: x86-64 AMD or Intel with AES-NI instruction set or ARMv8 with crypto extensions.Edit: maybe https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;AES_instruction_set#Intel reply SushiHippie 19 hours agoparentThe lscpu command will you show you. IIUC you need to have \"aes\" under the flags entry. reply konart 19 hours agoprevHow does it compare to Alacrtitty, Kitty, Wezterm, Rio and other moder and actually fast terminal emulators? reply souvlakee 13 hours agoparentRecently discovered Rio and it works faster than everything GPU accelerated I&#x27;ve ever tried. Believe that&#x27;s because WebGL. reply AceJohnny2 20 hours agoprevDoes it support `tmux -CC` ?Sorry for being that HN commenter but, if not, I&#x27;m bored with yet another terminal that just equates established features.iTerm&#x27;s killer feature is `tmux -CC` support, so that tmux&#x27;s sessions are integrated natively. I find it shocking that, years after tmux established this feature, iTerm remains the only terminal emulator to use it. reply felixding 20 hours agoparentTmux control mode is one of the keys reasons I can&#x27;t leave Mac. I also feel it&#x27;s unbelievable that iTerm 2 is still the only terminal that supports this feature.For anyone who is asking why this feature is good: it&#x27;s really not that easy to see the benefits from the documentation, blog or even videos, until you personally try it. At least that&#x27;s the case for me. reply iamjackg 18 hours agorootparentI worked on a pull request to implement it in Terminator a long time ago, but eventually gave up. Control mode was contributed to tmux by the author of iTerm2, and if you explore the iTerm2 codebase you can definitely see that they&#x27;re very tied together in terms of implementation choices and general design. It&#x27;s not that simple.In order to have two-way synchronization, your terminal needs to not only have an architecture where you can add a background thread that can control the \"main\" terminal interface, but also be flexible enough to properly size all windows and panes, and match font sizing. If your terminal supports different font sizes for every pane&#x2F;split, those have to be turned off in tmux mode. There&#x27;s all kinds of corner cases. reply riddley 20 hours agoparentprevI read the Control Mode section of the man page, but the utility didn&#x27;t leap off the page. Would you share what&#x27;s cool about this mode? reply lanza 19 hours agorootparentsshing into a server only happens within one shell. Thus you can open tmux there and that&#x27;s the only level of granularity you can have.iterm window->iterm tab->ssh->tmux windows->tmux panesWith `tmux -CC` it lets you hoist that chain upwards and map the entire ssh session into an iterm window.iterm window (ssh session) -> iterm tabs (tmux windows)The biggest benefit for me is that you can then open another tmux session within the CC one and use tmux as normaliterm window (ssh session) -> iterm tabs (tmux windows) -> tmux2 windows -> tmux2 panesAnd then the normal feature of tmux is that it persists across ssh sessions. This also means that the entire iTerm2 window and all its tabs and splits are also maintained across sessions. I think my current layout on my main development machine that I remote into is over 3 months old.Another point of view is that it decouples the state of your tmux tabs&#x2F;panes from your terminal emulator itself. e.g. if you use this locally instead you could then terminate the entire iterm program and reopen it and restore the same exact window&#x2F;tab&#x2F;pane splits you previously had. This is a nice perk for restarting&#x2F;updating iTerm. reply jordemort 18 hours agorootparent> f you use this locally instead you could then terminate the entire iterm program and reopen it and restore the same exact window&#x2F;tab&#x2F;pane splits you previously had. This is a nice perk for restarting&#x2F;updating iTerm.iTerm seems to be able to pull this off itself during upgrades, even without tmux. The first time it happened, I was surprised, and now I just install iTerm updates with abandon, knowing that all my 40 terminal windows and all the stuff in them will come back after the update. reply riddley 18 hours agorootparentprevI appreciate your explanation, but I&#x27;ll confess that I still don&#x27;t get it. I assume I&#x27;m just not understanding; please don&#x27;t take offense.The state of my splits etc are already distinct from the terminal emulator as is&#x2F;are the session(s) without CC. I can restart my terminal emulator or add new sessions at will without needing any special support from the terminal. reply AceJohnny2 16 hours agorootparentprevFirst, some background: Tmux is great but (like GNU Screen) introduces a layer of abstraction between you and the shell. You have to use its special keybindings to create new windows, switch window&#x2F;pane, or most annoyingly to scroll through history.Control Mode removes that layer of abstraction, so (with iTerm) you can use all the normal terminal emulator functions to do what you expect: click on panes to select them, tmux windows are just native tabs, and you can scroll the terminal nativelyIn effect, all the UI compromises you had to do to make use of tmux... go away.It’s a huge benefit, because it means all the friction of using tmux disappears. Instead of thinking “do i really want to deal with tmux’s UX for this quick ssh session?” (it’s never as quick as you think) I just do it. All my terminal sessions are now in tmux. If I’m doing something on a remote machine, i can confidently close my laptop and know i can restore later. reply shmichael 20 hours agorootparentprevthe iterm mode allows one to create, split, resize remote terminals as of they were native terminal windows, instead of all of them being contained within one terminal window and controlled using completely different keyboard-only shortcuts (via tmux) reply singwhenurdown 19 hours agorootparentdoes this make it faster? or is it used for the native feel reply drbaba 19 hours agorootparentMainly the native feel. For example, you can use standard Cmd-based keybindings to control tabs and panes, and get smooth mouse scrolling in each pane. reply throwaway290 20 hours agoparentprevHow is it useful? I could never understand why I would want something like that as opposed to just using Tmux splits & terminal emulator windows. And this is the only selling point of iTerm! Everything else is strictly worse (lag, CPU&#x2F;memory usage) compared to plenty of emulators. reply akdkfe223 20 hours agoprevDoes anybody know if it&#x27;s actually fast in terms of low latency? I personally don&#x27;t care about GPU acceleration or any of those things that Alacritty and others claim makes them fast when the latency is noticeably worse than older non accelerated emulators. reply sprash 18 hours agoparentIf latency is your biggest concern you have to use a terminal emulator without double buffering. So far xterm and mlterm are popular choices. reply angra_mainyu 13 hours agoparentprevthe latency is excellent, I immediately noticed a huge difference, particularly when seeing the output of long commands (e.g: tail -f) reply muhammadusman 15 hours agoprevOnce in awhile I’ll try these new emulators but I keep going back to iTerm2. Maybe having a comparison of the two or similar would help convince me and others in the same boat. reply tupolef 16 hours agoprevI just tried it and I will keep an eye on it for later. Currently it is a work in progress and there is a lot of issues with the dependencies and compatibility with legacy setups, but it is way better than Warp and other GPU terminals that I tried.I am currently a happy user of Urxvt and intend to continue until I am forced to switch to wayland (in a long time, I hope). In comparaison I have to add that the rendering in more defined in Urxvt that Contour even after tinkering, and like always it uses at least 5 times less memory. reply kelsey9876543 21 hours agoprevi don&#x27;t see the point of adding vi input mode to the emulator when we have it inside bash with `set -o vi` reply drbaba 19 hours agoparentMany interpreters don’t use Readline (e.g. IPython and Zsh), and enabling inconsistent Vi keybinsings in each one can be a bit annoying. From that perspective, adding it to the terminal seems a possibly easier way to get it “everywhere”? reply mhitza 19 hours agorootparentPossibly more universal, but there are also tools like rlwrap [1] that adds readline support to programs that don&#x27;t have it. From the docs apparently the readline library ships a similar tool ootb nowadays but I haven&#x27;t tried that and just noticed now when I wanted to share the rlwrap link.[1] https:&#x2F;&#x2F;github.com&#x2F;hanslub42&#x2F;rlwrap reply williadc 18 hours agorootparentprevIPython absolutely uses readline reply drbaba 14 hours agorootparentNot anymore. Since IPython v5 (5+ years ago?), readline was replaced with prompt_toolkit.EDIT: Here’s an official source: https:&#x2F;&#x2F;ipython.readthedocs.io&#x2F;en&#x2F;stable&#x2F;whatsnew&#x2F;version5.h... reply Gualdrapo 21 hours agoparentprevAs it supports Windows, I guess it would be useful for them reply kelsey9876543 20 hours agorootparentbash runs on windows reply akdkfe223 20 hours agorootparentFeels a little bit misleading to say that honestly. It runs on windows in what is effectively a chroot&#x2F;jail&#x2F;container esque monstrosity. I personally consider that to qualify as a totally separate userspace and therefore not the same operating system as one would assume when you use the term windows without any additional qualifiers. reply miloignis 15 hours agorootparentAre you referring to WSL&#x2F;2? The msys2 &#x2F; Git Bash integration is more integrated with Windows then how I interpret your description. replyConscat 16 hours agoprevhttps:&#x2F;&#x2F;github.com&#x2F;contour-terminal&#x2F;contour&#x2F;issues&#x2F;382This apparently does not support the Kitty graphics protocol, just Sixel, which makes it look fairly unattractive to me, personally. reply tmtvl 18 hours agoprevI like that they reference the VT510 manual, if the terminal implements the more advanced control codes then individual applications can do their work without needing to rely on their own implementations of the needed functionality. I&#x27;m gonna give Vttest a whirl in Contour and if it does well I&#x27;ll try daily driving it for a while. reply tiffanyh 20 hours agoprevA prominent blog post benchmarking latency of various terminals.https:&#x2F;&#x2F;danluu.com&#x2F;term-latency&#x2F;TL;DR; terminal.app on macOS is typically the fastest. reply idoubtit 19 hours agoparentYour benchmark is more than 6 years old, on a 2014 Macbook, with a strange choice of terminals. LWN&#x27;s article on the subject is almost as old, but with a better comparison: https:&#x2F;&#x2F;lwn.net&#x2F;Articles&#x2F;751763&#x2F; (2018)For a recent take, though by one of the competitors, see https:&#x2F;&#x2F;tomscii.sig7.se&#x2F;2021&#x2F;01&#x2F;Typing-latency-of-Zutty> TL;DR; terminal.app on macOS is typically the fastest.Even on macOS, that&#x27;s probably not true, at least nowadays. See https:&#x2F;&#x2F;www.lkhrs.com&#x2F;blog&#x2F;2022&#x2F;07&#x2F;terminal-latency&#x2F; (2022) reply 4death4 18 hours agorootparentIt says Terminal.app follows closely behind the fastest. This level of discussion around “fast” terminals is funny to me. I’ve always used Terminal.app and never had a single problem. It turns out the default option is likely the best in this case. reply IshKebab 19 hours agoprevIt would be nice if someone worked on actually improving the terminal experience. Selecting, copying and pasting text is still pretty awful (why is the cursor an entire block rather than a I bar?). Editing multiline inputs is awful. Navigating history is so-so (even with McFly etc.). Anything more complicated than left&#x2F;right&#x2F;up&#x2F;down fails half the time and dumps control characters instead.Why are terminals always stuck in the 70s? Can I get a modern terminal? reply Macha 17 hours agoparentThere are better experiences (e.g. my shell has vi mode which is much nicer for editing multi-line inputs, I&#x27;m sure if I could be bothered to learn emacs&#x2F;readline shortcuts that&#x27;d be another set) but they&#x27;re not easily discoverable and a bit unevenly distributed. reply indymike 16 hours agoparentprev> Editing multiline inputs is awful.Outside of \"line at a time\" i&#x2F;o (a rarely used mode where an entire line is edited locally and then sent to the host), most of what users see is as interactive is controlled by the program you are interacting with. The terminal just takes commands from the host and does what it is told. BTW, line at a time mode isn&#x27;t used that much. The only thing I use that uses line at a time mode is telenet in LINEMODE.> Navigating history is so-soYes, that is because the program you are likely interacting with where history is relevant implements it&#x27;s own repl or command line (i.e. bash, zsh, python, etc...) and it is responsible for it&#x27;s own history and may implement it completely differently than say, bash or zsh.> Why are terminals always stuck in the 70s? Can I get a modern terminal?We do have a modern terminal: the web browser... and it&#x27;s pretty nice.There have been a ton of tries at more modern terminals, but ultimately, they end up really being limited by the software running in the terminal session. In the 90s we had a ton of commercial terminal emulators that would allow you to create full guis, complete with dialogs and forms. In the 00&#x27;s there were a few tries at terminals that would allow html output and embedding of html forms for input (can&#x27;t remember the names of them). I suppose there&#x27;s also the whole X11 thing... which is so good enough that it&#x27;s really hard to kill.Let&#x27;s get back to character mode:A lot of interactive terminal software is built using different libraries - so sometimes you get a terminal gui based on ncurses, terminal.gui, or something else... here&#x27;s a list: https:&#x2F;&#x2F;github.com&#x2F;rothgar&#x2F;awesome-tuis#libraries. Most of these libraries try to use most of the features in your terminal emulator, but often, just use stuff that is in everything.For command line programs (i.e. just type a command), a lot of the experience is dictated by the parser used by the tool and whatever the underlying operating system has for passing arguments. Some shells and terminal emulators (like iTerm2 on mac) try to smooth this out, but again, there&#x27;s a lot of variety in command line parsers.Probably the biggest modern improvement in the shell world was gettext and various command-line completion libraries which allows command parameter completion if the developer supports it or uses a parser that supports completion. But none of this is the terminal itself doing the work. reply IshKebab 3 hours agorootparentYou&#x27;re doing the classic thing of explaining why things are bad and thinking that that is the end of the story.> The terminal just takes commands from the host and does what it is told.> that is because the program you are likely interacting with where history is relevant implements it&#x27;s own repl or command line (i.e. bash, zsh, python, etc...) and it is responsible for it&#x27;s own historI know!!!My point was why is nobody working on fixing these issues?The answer is not because it can&#x27;t be done. That just shows lack of imagination. reply paradox460 17 hours agoparentprevFwiw, iTerm2 has a \"copy mode\" which lets you navigate around and yank the visible terminal and scrollback with vim bindings reply pmarreck 19 hours agoprevWezterm is my jamhttps:&#x2F;&#x2F;wezfurlong.org&#x2F;wezterm&#x2F; reply cormullion 16 hours agoparentOverall I think Wezterm has the best font support (and the best support). reply sprash 18 hours agoparentprevReally cool, the parallax background scrolling feature:https:&#x2F;&#x2F;wezfurlong.org&#x2F;wezterm&#x2F;config&#x2F;lua&#x2F;config&#x2F;background.... reply pmarreck 5 hours agorootparentI didn&#x27;t even know it had that feature until now, LOL, nice! reply politelemon 15 hours agoprevAny idea why nvtop wouldn&#x27;t work on it? I run nvtop and get \"Error opening terminal: contour.\" reply Stem0037 19 hours agoprevI remember the days when I had to juggle between different emulators just to get a fraction of these features... reply da39a3ee 13 hours agoprevI&#x27;ve in the camp of never noticed my terminal emulator being slow. Used iTerm2 for 10 years, switched to Alacritty recently for reasons unrelated to performance. Before iTerm2 on Linux I don&#x27;t really remember what I used but I don&#x27;t remember it being slow. Haven&#x27;t noticed scrolling speed being slow, and I don&#x27;t get this \"insert latency\" obsession. reply cmrdporcupine 17 hours agoprevLooks nice, but seeming lack of tabs support (https:&#x2F;&#x2F;github.com&#x2F;contour-terminal&#x2F;contour&#x2F;issues&#x2F;90) makes it a hard sell for me. reply ur-whale 20 hours agoprev [–] > contour ... actually fastand yet, borderline impossible to install without using flatpakSo which is it ? fast or needs flatpak ? reply JCWasmx86 19 hours agoparentIt&#x27;s easy to install on ArchLinux and Fedora. The build process seems to be just standard CMake, so calling it \"borderline impossible\" is really a stretch. reply yjftsjthsd-h 17 hours agoparentprevWait, does flatpak have a runtime performance penalty? reply doubled112 17 hours agorootparentIf I can run Steam and play games packaged with Flatpak, hopefully my machines can handle a terminal emulator too. reply ur-whale 14 hours agorootparentWith games, you don&#x27;t have startup time issues. Not so for something as ubiquitous as a terminal emulator. reply doubled112 13 hours agorootparentDo people have startup time issues with Flatpaks?I can&#x27;t say I&#x27;ve ever noticed a difference, but everything I use has an NVMe. Perhaps it matters. reply diath 19 hours agoparentprevHow come? It doesn&#x27;t seem to have any dependencies that you wouldn&#x27;t be able to get, they&#x27;re pretty common. reply worksonmine 19 hours agoparentprev [–] > So which is it ? fast or needs flatpak?Are they mutually exclusive?If you don&#x27;t like flatpaks it seems to be available on the AUR, fedora or homebrew if you&#x27;re using mac. Debian repositories are always lagging behind for new stuff so that&#x27;s no surprise but the dev provides a script for installing dependencies on the rest of the distributions somewhat customized to your flavor if you&#x27;re willing to build from source. There is a debian directory so it seems like they started the process of getting included but you&#x27;ll have to verify in the mailing-lists to see what the current status is.I&#x27;d say that&#x27;s plenty of options to choose from. The simplest being flatpak. Are you a package maintainer yourself? Do you know how much work is involved to get into the main repositories? This is not npm. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Contour is a cutting-edge terminal emulator accessible on numerous platforms, boasting features such as GPU-accelerated rendering and support for font ligatures, Unicode, and grapheme clusters.",
      "The application has diverse customization options and additional features like clickable hyperlinks, terminal page buffer capture, and a built-in Fira Code inspired progress bar.",
      "Contour can be obtained via package managers or constructed from source code, and is licensed under the Apache License, Version 2.0."
    ],
    "commentSummary": [
      "The discussion revolves around terminal emulators, assessing their performance, features, and compatibility.",
      "Users voiced concerns like slow startup times, color problems, and latency, and highlighted the potential of AI and low-level machine (LLM) capabilities in terminals.",
      "Despite numerous individual experiences with specific emulators like Contour and WezTerm, there is no unanimous agreement on the best emulator, indicating different preferences and requirements."
    ],
    "points": 177,
    "commentCount": 166,
    "retryCount": 0,
    "time": 1696767623
  },
  {
    "id": 37812332,
    "title": "Writing down unfiltered thoughts enhances self-knowledge",
    "originLink": "https://www.scientificamerican.com/article/know-yourself-better-by-writing-what-pops-into-your-head/",
    "originBody": "SKIP TO MAIN CONTENT Subscribe Latest Issues Scientific American Sign In|Newsletters COVIDHealthMind & BrainEnvironmentTechnologySpace & PhysicsVideoPodcastsOpinion Nobel Prize Sale! Save 40% PSYCHOLOGY Know Yourself Better by Writing What Pops into Your Head The exercise of writing down unfiltered thoughts enhances self-knowledge By Christiane Gelitz on October 6, 2023 Credit: Ems-Forster-Productions/Getty Images For decades, physician and author Silke Heimes has been leading groups in therapeutic exercises to put thoughts and feelings down on paper. Heimes, a professor of journalism at Darmstadt University of Applied Sciences, points to abundant evidence that writing for five to 20 minutes a day can improve health, diminish stress, increase self-confidence and even kindle the imagination. A writing routine, she argues, is a form of mental hygiene that almost anyone can benefit from. So how do you start? What happens if—as every writer fears—the page remains blank? And how do you get rid of an overcritical inner censor? Heimes, director of the Institute for Creative and Therapeutic Writing in Darmstadt, explains how to overcome inhibitions and open up your inner world. [An edited transcript of the interview follows.] ADVERTISEMENT If you want to write in order to understand yourself better, what's the best way to start? There are writing exercises, for example in so-called fill-in journals, where you directly answer a question. But if I just want to get started without any aids, the best way is to use the method of automatic writing. That means I set myself a short time window, maybe five minutes, in which I write continuously without thinking, without putting down the pen or rereading what I’ve written. The goal is to get thoughts down on paper as unfiltered as possible so that an inner censor can't switch on—or at least doesn't get too loud. It helps not to set the goal too high—not to expect too much—but to understand this writing as a time-out, so to speak, or as a kind of warm-up exercise. Wouldn’t it be helpful to ask yourself specific questions? If you want to, you can follow programs that, for example, organize specific questions into topics. But that can also be inhibiting at times because such questions primarily get your head working to produce rational answers. Questions often steer thoughts along preconceived paths. Sometimes it is almost easier without them to let the gut lead the way. What if you just can't think of anything? ADVERTISEMENT The half-sentence method can help. With this approach, you complete a given half-sentence such as \"When I woke up this morning” or “What happened to me today.” If you write in the morning, the [first example] is a good choice. Because everyone wakes up in the morning, everyone can think of something to say about it. The same applies to [the second example] if you write in the evening because you inevitably experienced something during the day by then. To start, you can also write down words that begin with the letters of your name and then create a text using those words. Can anything go wrong using these methods? Not really. Just as with thinking, you can of course get tangled up in your own thoughts or get stuck in brooding loops when writing. But that’s not the fault of the writing itself; it’s just something that becomes obvious on paper. Writing often deals with emotional issues, so you also might temporarily feel bad because something is stirred up or triggered. In that case, you should take a break and do something else or talk to someone about it. If the feeling persists, it is best to seek professional help. Sign up for Scientific American’s free newsletters. Sign Up Does it make a difference whether you write by hand or on a keyboard? Writing by hand is a very complex movement that activates more areas in the brain, which leads to being more creative. It also usually means slowing down, which invites you to pause and take a breath. In addition, there is something sensual and unique about writing by hand. because, for one thing, our handwriting is very individual. And for another thing, it tells us something about our state of mind. In fact, handwriting usually becomes rounder and livelier when we are in a good mood and smaller or tighter when we are not feeling so well. Typing on the keyboard, on the other hand has a soothing quality because it is very rhythmic. Further, it has the advantage of allowing you to share your writing more quickly. I think it’s always good to have both skills and to use them. ADVERTISEMENT You have guided many groups in this type of writing. How does that typically work? We first do little writing exercises to warm up. Many people come with the expectation that they’ll sit down, and the writing will flow right away—that they’ll perform brilliantly almost off the cuff. But no athlete, no musician would expect that of themselves. Professional writers know better. And there are other common misconceptions. The biggest one is “I can't write.” A lot of people come to my seminars with this attitude. But we can all write. Rather the problem is the often exaggerated demands we place on ourselves. I like to quote French writer André Breton, who invented automatic writing. He said, mutatis mutandis, that if you want to write, find a nice place, sit down in peace and quiet and forget about seeking out brilliant thoughts. Is there anything else that people particularly struggle with when it comes to writing? We’ve already talked about your own performance expectations. But what can also lead to inhibitions is the fear of emotions or of your personal history—fear of confronting possibly painful topics. And further problems usually arise when people want to put their thoughts into a literary form in order to publish them. ADVERTISEMENT What if someone only produces platitudes? What if they sound banal or superficial? Who decides that? That is a judgment that should be unacceptable in creative and therapeutic writing. Everyone expresses what is important, right and possible for them at that moment, and I think that is precisely what deserves appreciation. What do people in your groups write about most often? They write about the topic of self-worth—that is, the fear of not being good enough—about not being heard or seen and about the topic of freedom versus security, especially at work. And what insights do they go home with? ADVERTISEMENT That varies greatly. But they often take home a lot of pieces of paper, and that’s how they recognize that they can definitely write. They have produced something and are justifiably proud of it. This increases their self-esteem, and they develop more confidence. Writing also sharpens perception and promotes mindfulness. People notice more quickly when something is not good for them and find better ways to deal with those problems. And when thoughts go round in circles, putting them down on paper clears the mind. After that you have more capacity for other things in your life. Can these benefits be probed empirically? There are [hundreds] of studies on the effect of expressive or therapeutic writing. Many of them come from the psychologist James Pennebaker, who did research on this primarily with students. Do you write a lot yourself? Yes, every day. I work on a novel or nonfiction book every day, and I also jot down my thoughts for three minutes in the morning. These few minutes of mental hygiene are as important and natural to me as brushing my teeth every day. This article originally appeared in Spektrum der Wissenschaft and was reproduced with permission. Rights & Permissions ABOUT THE AUTHOR(S) Christiane Gelitz is a psychologist and an editor at Spektrum der Wissenschaft. Recent Articles by Christiane Gelitz Allergic to Your Pet? This Immunotherapy May Help How Certain Gestures Help You Learn New Words Misophonia Might Not Be about Hating Sounds After All READ THIS NEXT CLIMATE CHANGE Climate Disasters Displaced 43 Million Children in Just Six Years Chelsea Harvey and E&E News PUBLIC HEALTH 'Morning After' Antibiotic Could Reduce STIs Sara Reardon CLIMATE CHANGE Journey to the Thawing Edge of Climate Change Jocie Bentley PSYCHOLOGY Know Yourself Better by Writing What Pops into Your Head Christiane Gelitz CONSERVATION Millions of Mosquitoes Will Rain Down on Hawaii to Save an Iconic Bird Sarah Wild ASTRONOMY The Sky Is Full of Stars--and Exoplanets, Too Phil Plait ADVERTISEMENT NEWSLETTER Get smart. Sign up for our email newsletter. Sign Up Support Science Journalism Discover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners. Subscribe Now! FOLLOW US instagram youtube twitter facebook rss SCIENTIFIC AMERICAN ARABIC العربية Return & Refund Policy About Press Room FAQs Contact Us Site Map Advertise SA Custom Media Terms of Use Privacy Policy Your US State Privacy Rights Your Privacy Choices/Manage Cookies International Editions Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers. © 2023 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC. ALL RIGHTS RESERVED. SCROLL TO TOP",
    "commentLink": "https://news.ycombinator.com/item?id=37812332",
    "commentBody": "Writing down unfiltered thoughts enhances self-knowledgeHacker NewspastloginWriting down unfiltered thoughts enhances self-knowledge (scientificamerican.com) 171 points by elorant 16 hours ago| hidepastfavorite66 comments NalNezumi 10 hours agoI think a lot of people put too much pressure on themselves when it comes to the advice \"writing your thoughts down\". Perfectionism, or the memories of being forced to take notes in school, kicks in and you start to look for how-to:s, format and guides.Very similar to meditation. The best meditation advice I got was to just sit down, focus on doing nothing and be OK with being bored. Not some breathing techniques, how to focus your mind, posture etc. That comes later, very naturally, after you get curious about improvements.Same goes for writing down thoughts. Just take a blank A4 paper from the printer and write away. Not sure what to write? Write about that, the fact that you&#x27;re not sure what to write. Throw the paper away if you don&#x27;t like the thoughts you see, or at least do it with that mindset. It&#x27;s not about writing something novel, just as meditating is not about reaching some enlightenment reply mtalantikite 7 hours agoparent> Very similar to meditation. The best meditation advice I got was to just sit down, focus on doing nothing and be OK with being bored.Similarly, no matter how much instruction I&#x27;ve gotten in meditation over the years, I still go back to Tilopa&#x27;s advice all the time [1]. Just rest.[1] https:&#x2F;&#x2F;unfetteredmind.org&#x2F;tilopas-advice&#x2F; reply rramadass 4 hours agorootparentNice!In case you didn&#x27;t know of them already, you might find the following books interesting;1) Mind Training: The Great Collection translated by Thupten Jinpa.2) Essential Mind Training translated by Thupten Jinpa - This is a subset from the above collection. reply somsak2 1 hour agorootparentcan&#x27;t tell if this is satire or not reply satvikpendem 6 hours agoparentprevThere was an app or site I remember using that, every time you typed a sentence, it disappeared so that you only focused on the sentence in front of you. After you&#x27;re done, you could export the entire document. I wonder if we could add some LLM features to that too. reply prometheon1 1 hour agorootparentI knew I remembered this app, but almost couldn&#x27;t find it. Was it this one?https:&#x2F;&#x2F;write.sonnet.io&#x2F; reply numtel 9 hours agoparentprevBefore I graduated from college, my aunt gave me the book, The Artist&#x27;s Way, which prescribes this basic task as you&#x27;ve described as the foundational exercise for creativity. The suggestion given was 3 pages a day no matter what. reply electrondood 6 hours agorootparentBingo. The Morning Pages technique is also the cheapest therapy on the planet. The effect, for me anyways, is virtually identical to the catharsis I feel from a $200 therapy session. reply criddell 8 hours agorootparentprevDid you pick up the practice? If so, what are your thoughts after doing it for some time? reply mtalantikite 8 hours agorootparentNot OP, but I went through the Artist&#x27;s Way a while back and kept morning pages up for two or so years after it. I&#x27;d still probably do morning pages every day if I didn&#x27;t move on to sitting at my piano in place of them (and would do both if I had time in the morning). So in that regard I think it was a success -- it got me back to having a healthy relationship with music making and artistic practice.The content of the workbook at places doesn&#x27;t totally resonate with me, but I&#x27;d say there&#x27;s enough in there that it&#x27;s worth trying out. It definitely asks you to confront and reflect on a lot of things that have happened in your life, so it can be uncomfortable and bring up a lot. But that&#x27;s of course an important part of art making.I&#x27;d say just try it out, it&#x27;s only 12 weeks and you can always bail if it doesn&#x27;t work for you! reply anon-3988 7 hours agoparentprev> Very similar to meditation. The best meditation advice I got was to just sit down, focus on doing nothing and be OK with being bored. Not some breathing techniques, how to focus your mind, posture etc. That comes later, very naturally, after you get curious about improvements.Chilling != meditation. Like I get that chilling is helpful for you but words have meaning and that is not meditation. reply kortilla 5 hours agorootparentChilling is not “doing nothing”. When people use that word they usually mean consuming content (listening to music, watching shows, reading book) or doing some other relaxing thing (knitting).You’re right that words have meaning, and purposefully doing nothing at all to the point of being uncomfortable with it is antithetical to “chilling”. reply anon-3988 3 hours agorootparentOP says that:> The best meditation advice I got was to just sit down, focus on doing nothing and be OK with being boredWithout any proper technique or guidance, this is just chilling. What does it mean to \"focus on doing nothing here?\". I can apply this sentence to binge watching 10 episodes of Friends, getting stoned, just slouching on the couch, staring at walls etc etc. That is what it means to chill.OTOH, to meditate is a very explicit, focused action which requires technique and discipline. reply iammjm 2 hours agorootparentWatching Netflix or getting stoned is clearly not doing nothing. If you take it literally then it becomes clear what is meant: just being, without indulging in anything, not even our thoughts. I agree this requires focus, but i don&#x27;t think it requires technique but rather practice. As for discipline, i don&#x27;t even know what that is reply anon-3988 59 minutes agorootparent> I agree this requires focus, but i don&#x27;t think it requires technique but rather practice.Tell me how you would do this without a technique. Your mind wanders and a random thought appears, what do you do? Whatever you are going to say next is by definition a technique. The fact that there a multiple answers to this questions means that the problem is open to a variety of techniques that have a range of effectiveness. This implies teaching and guidance. reply tomjakubowski 6 hours agorootparentprevBecoming able to tolerate boredom still could be a good step on the way to learning to meditate. reply qwerasdf5 6 hours agoparentprevTry taking an edible, sit down in a quiet room in front of your text editor of choice, and write down ideas.The next day, look at the stuff you wrote and pick out the good stuff. reply barapa 7 hours agoparentprevWhich stock A4 should I use? reply p1mrx 7 hours agorootparentNo one can be told which stock to use. Buy reams from at least 3 different manufacturers, take a random sample from each, and have a neutral third party number and randomize the specimens. Sharpen your pencil and write one full page, noting how the graphite glides against the wood fibers. Leave a margin of at least 3 centimeters, and if you must erase, use only natural rubber. reply zeroCalories 12 hours agoprevI like writing because it forces you to turn vague feelings into concrete ideas. You can barf your thoughts onto paper and then read it with a critical eye, allowing you to evaluate your own thoughts. It helps me to find the gaps in my knowledge, notice sloppy arguments, etc. Sometimes that leads to a better understanding of the topic, but most of the time I realize that I have no understanding of the topic, which is still useful information. For example I&#x27;ve found myself eager to drop a hot take on a recent international event, but after writing out my comment I realize that I&#x27;m an idiot, so I make a mental note and post it anyway. reply bobbylarrybobby 6 hours agoparentThey say you don&#x27;t truly understand something until you&#x27;re forced to explain it to someone else. Writing your ideas down is essentially “rubber duck debugging” applied to your thoughts: you can get clarity without needing a human third party. reply mcmoor 8 hours agoparentprevI don&#x27;t like it because I feel my vague ideas \"disappear\" when becoming concrete. It feels like trying to project a multidimensional creature into 2D; you inevitably lose some of it. In contrast, in my brain I can rotate or pinch the idea anyway I like and even turn it into further ideas. reply SamPatt 6 hours agorootparentI enjoy writing but I also feel this strongly. For me it feels like writing is forcing me to carry on one particular train of thought much longer, and in a more focused way, than I do in my head.Thoughts sometimes have a parallel aspect to them, and since writing cannot capture this, it&#x27;s necessarily a pale imitation. But that&#x27;s what makes it challenging, and rewarding if I&#x27;m actually able to get something across. reply darkerside 7 hours agorootparentprevThat&#x27;s exactly the point. You can do that in your own head, but you can&#x27;t communicate that to other people. For every amazing thing you&#x27;ve ever seen in a movie screen, imagine how much MORE amazing it was in someone&#x27;s head before that. But you can&#x27;t capture that in full.And, much like the scientific method can take something that \"everyone knows\", and turn it into the basis for a branch of science, the process of getting this down in paper forces you to document all the nuances in your head, making that final idea even richer and fully consistent. reply PhilipRoman 12 hours agoparentprev>I like writing because it forces you to turn vague feelings into concrete ideas.Yep. I&#x27;ve started painting my dreams and it is surprisingly difficult to turn vague memories into concrete scenes. It&#x27;s almost physically exhausting. reply Geee 1 hour agorootparentI did this one time when I saw a particularly beautiful scenery. I could remember the elements mostly, but didn&#x27;t quite manage to capture the beauty of it. reply johnchristopher 12 hours agorootparentprevDo you paint them as you remember them or do you write them down before ? reply PhilipRoman 12 hours agorootparentI try to paint them as soon as possible after waking up.Usually I just write down a list of keywords, that seems enough to keep my brain from erasing them. I don&#x27;t think it&#x27;s really possible to capture a scene in text. The text only serves as memory aid. reply lacrimacida 12 hours agorootparentThat’s interesting, care to share them? reply kelseyfrog 11 hours agoparentprevTo add, it helps to have some record of inner thoughts to look back and reflect with greater fidelity than memory.Done enough, one can start to make connections at a greater rate than if one was to try to keep it all in one&#x27;s head. I&#x27;ve had several experiences in therapy where I had the spontaneous thought, \"I&#x27;ve written about this before!\" I was able to go back and thread through all previous reflections in a way that was much more than the sim of their parts. It really accelerates self discovery and growth. reply SamPatt 6 hours agorootparentOn a similar note - I often have the sense that I&#x27;m a very different person today than I was in the past.Reading old writing of mine is fascinating, because it will sometimes confirm this - I often have very different beliefs - but it also shows how similarly my thought processes were in the past.I do recommend it, with the caveat that if you journaled during dark times, don&#x27;t reread those parts if you&#x27;re struggling again. Can be contagious. reply Geee 42 minutes agoprevI&#x27;ve written down (typed) my thoughts for almost 20 years now. They&#x27;re mostly just some sort of ideas that pop in my head; usually inspired by something I&#x27;ve read. Lot of them are just app &#x2F; business ideas. Usually a few ideas a day.Not sure if it&#x27;s been helpful in any way. reply javawizard 12 hours agoprevI have a pretty strong natural filter. Years of therapy and three different therapists have yet to get me to a place where I can truly open up, even when writing on a page I know no-one else will see.Two days ago I did weed for the first time. I accidentally took a much larger hit than I meant to. For the first 15 minutes I had a complete out-of-body experience; nothing seemed real, I felt like I was floating through space, able to peer into reality at will, still not quite trusting that it wasn&#x27;t all a dream. I&#x27;m pretty sure the term of art for what I experienced would be a psychotic episode.After that... I lost my filter. Like, it was gone. And my mouth was a conveyor belt connected to the emotional part of my brain. No logic, just uninhibited speech, for 45 minutes, all while sobbing harder than I have in my entire life.I exposed every last deep, dark secret I possessed. My fear of never being good enough. My fear that everyone in my life will leave me at some point. My fear that I&#x27;ve done so many hurtful things over my life that I&#x27;m unworthy of love and of the friends I have. And many more things I won&#x27;t be sharing with the world, at least not yet.Inside, it felt like my brain and my mouth were connected by a pipe, and all I could do was sit back and watch in horror as the very depths of my mind were laid bare for all to see.A good friend of mine was with me while I did it. She heard everything. It&#x27;s a mark of our friendship that she held me, reassured me that she loved me for who I am, not who I want people to think I am. Our friendship is even stronger now, something I would never have thought possible before having that experience.It was terrifying, and yet oddly theraputic. I&#x27;m seriously considering cannabis-assisted psychotherapy now.---I guess what I&#x27;m trying to say is: there are a number of substances that induce mind-altering states in ways that are relatively safe and free from long term effects. If you&#x27;re someone who can&#x27;t seem to open up naturally, don&#x27;t be afraid to try them. They just might change your life.(As with everything, consume appropriately and safely. Have someone experienced in the substance you&#x27;re consuming keep an eye on you. And for god&#x27;s sake, don&#x27;t do anything known to be addictive or to have severe negative side effects.)Also, find friends who you can truly be yourself around, who love you even when they know the absolute worst about you. It makes all the difference to know that someone loves you not for who you want the world to think you are but for who you actually are. reply omscs99 55 minutes agoparentI remember I was at a party few years back, some guy did way more acid than what he could handle and had a breakdown. I remember there was another guy there who was “tripsitting” but just something about him seemed kind of off, like he was trying to control the whole situationSo here was this one guy having a mental breakdown, really suffering, and there’s this other guy who keeps aggressively “checking in” on him every other minute, nobody stepped in to stop him because he was “tripsitting”.(I know normally tripsitters can do this, but the way the guy was doing it was just awful, like he was invading the guy’s space and pestering the shit out of him, it freaked him out even more)You’re definitely fortunate to have had a good friend nearby, some people are too fucking crazy to ever get high around reply jiggybling 3 hours agoparentprevI tried weed once a few years ago and took a bit too much as well. When the euphoria hit, I realized I had never felt that good in my entire life and I became scared of the idea that I would lose control. My thoughts also sped up and I would get into these metacognitive&#x2F;self-analytical loops. I&#x27;d start thinking about something and before I could finish I&#x27;d have another thought analyzing what I had just thought, and so on. And I experienced the loss of filter. At the time, I was with a friend in his basement and we were surrounded by his mom&#x27;s amateur paintings. I&#x27;ve never been an art person but I remember looking at one of her paintings and talking nonstop about all the different visual details that kept popping out at me.I didn&#x27;t have as complete as loss of inhibition as you described however. The friend I was with wasn&#x27;t someone I completely trusted - many of my thoughts I decided not to share. Still, I think the experience was valuable and I&#x27;m glad I tried it. Although, in retrospect any self-insight I gained I don&#x27;t think I truly took to heart, as it didn&#x27;t lead to any meaningful behavioral change. It&#x27;s only been two days, but I wonder what kind of long term changes you will see. Has the loss of filter persisted in any way or did it wear off when you came down?Either way, your story, outside of being well written and incredibly personable, gives me hope for my own healing. I&#x27;d love to connect and hear more about where your experience takes you, especially if you end up trying cannabis assisted therapy. I&#x27;m planning on trying psychedelic assisted therapy at some point so it would be cool to have someone to discuss with. My email is in my profile if you&#x27;re interested. reply Onawa 11 hours agoparentprevI&#x27;ve been using marijuana for too long for it to have that effect on me unfortunately, but actual psychedelics are great at expanding the mind. If you can find someone that you trust to be a &#x27;sitter&#x27; (at least for the first couple of times), it can be a life changing experience with positive long-term effects. Micro dosing psilocybin has been starting to show positive results in trials in recent years. reply coffeebeqn 11 hours agoparentprevWriting is one of my favorite things to do high. Nothing that interesting just whatever pops in my mind. Pen to paper so I don’t get distracted by shiny internet things reply tambourine_man 7 hours agoparentprevI don’t have many favorite comments on hacker news, especially not if this kind, but this one goes on that list. Thanks for sharing. reply layer8 11 hours agoprevThe problem I have with this is that my thoughts come 10 or 20 times quicker than I can write them down. And while doing this writing exercise I’ll have meta-thoughts about the writing and the exercise. The thoughts hopelessly outrun the writing, and also the train of thoughts will be wildly different from what it would be when not attempting to write it down. I guess one still learns something from the exercise, but it’s quite a frustrating experience. reply dwaltrip 10 hours agoparentIt sounds like you are putting too much pressure on yourself. You don’t have to write everything down. Just writing something, anything at all, no matter what it is, is great. That’s something to be proud of.I find it very helpful to slow down, breathe, and regain connection to my body.And you can stop whenever you want. Journaling is something you do for yourself — it shouldn’t feel like a chore.Your journal is your safe place where you can dump your thoughts. It will never judge you :) It can receive anything you offer to it. reply layer8 2 hours agorootparentMy point is that I don&#x27;t find it remotely feasible to write down unfiltered thoughts. They are necessarily dramatically filtered in terms of volume, because the speed of thought is magnitudes faster than the speed of writing. They are also filtered because putting things into words constitutes a transformation and distortion of the original thoughts, a crafting into words that are not quite the same as the original thoughts. Thirdly, the writing process introduces meta-thoughts about the writing, which when following \"write down what pops into your head\" very quickly leads to just writing about the writing.Maybe other people are less conscious of the distortions and of their meta-thoughts. But for me it makes the exercise a rather artificial process, where it&#x27;s difficult to produce anything authentic, anything close to the original thoughts. reply iammjm 2 hours agoparentprevIf writing is something new for you then it is normal that it will feel unnatural and thus attract more attention to the activity itself. It&#x27;s totally okay to start off writing about writing to get things going. In no time you will naturally move on to writing about other things. So don&#x27;t worry and just write reply prepend 11 hours agoparentprevI few similarly, but writing both makes me edit in real time and then again as I move stuff around.I don’t think it’s useful for literally capturing every idea, but it captures as many as you can get down.It’s kind of like how when you’re reading a book aloud you get a few seconds ahead in your head. reply jbm 13 hours agoprevSure it does, you aren&#x27;t getting me this time.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hundred_Flowers_CampaignJokes aside, this definitely helped when I was in college, especially when I had a follow up session where I could ask questions to my professors or the TA reply motohagiography 12 hours agoparentIt&#x27;s unbelievable how that campaign and even the hint of something like it here with internet mobs can obliterate the creative capacity of huge numbers of people. I write and make to understand things, and by example, I recently wrote about 12,000 words to get a yield of about 3,000 words of my best work. The stuff in that other 9,000 words will probably never see a screen again, but it had to come out to mine it for the single ideas and turns of phrase that become like library functions for dense, concise writing.If you can&#x27;t produce the raw material, you can&#x27;t make anything new or good. reply prepend 11 hours agorootparentThere’s a black mirror episode in the future about how LLMs trained on people’s discarded drafts start outing them for having disfavored opinions.Maybe through people’s digital AI ghosts (AIs made from people’s content after they die) start spouting racist stuff.So people are surprised to learn their friends and heroes were racist&#x2F;capitalist&#x2F;nickel back fans&#x2F;etcEnds up being a programming error that didn’t properly account for boundaries.That’s the end of my expected story idea. But I best google and Microsoft are training off every character written, not just what makes it to your final 3,000. reply motohagiography 10 hours agorootparentNot giving a fuck is a superpower. reply cableshaft 13 hours agoprevFrom the article:> What if you just can&#x27;t think of anything?Just write about your day if you need to. What you did, what you read about online (including reflecting on news if you read that), what thoughts you had over the course of the day.When I have good writing habits I can easily write 500-3000+ words about my day, and I don&#x27;t have what most people would consider an interesting life. I just work from home, work on my hobbies (mostly gamedev), read articles, go to one or two social events a week (often just a family party or casual game night or meetup, nothing crazy), go to a few conventions throughout the year, and take one weeklong trip.One year I managed to write 120,000+ words with this approach, and it likely would have been higher if I hadn&#x27;t skipped a decent number of days (maybe not a lot higher, I sometimes made up for the skipped days by reflecting back as much as I could).I&#x27;m not currently that good about it, though. In general it&#x27;s been a bad year for me for writing these things. Maybe this article will help me get back into it. reply riidom 11 hours agoparent> Just write about your day if you need to.In past, I had the habit of laying in bed and not being able to fall asleep because I was pondering about \"bad\" things that happened that day, or pondering about things going to happen the next day where I didn&#x27;t feel confident about the outcome, or my role (or similar).I kept this a little vague, I think the kind of events that keep you away from sleeping may be different for everyone. And if you have the habit of being in bed awake for longer time, you know why that is anyway.To get to the point: Doing a daily writing basically wiped out the problem entirely for me. Nowadays, when I can&#x27;t sleep, it is because I ate too much too late.I do my writing once per day, usually close to bed time. I write about relatively noteworthy things (\"another day of being too lazy to progress on hobby thing XY\" is a pretty common one), then go a bit on a tangent maybe, why that is and if it&#x27;s actually that bad. I try to be mostly gentle to myself here and keep it positive. If things happen that annoy me (neighbours annoying, cashier rude, big train delay, whatever), I of course write them down too, and also go on a tangent. Can I do something about this? Are there maybe good reasons for it, which I didn&#x27;t think or care of the moment it happened, etc?Basically, do the thing you would do while laying in bed and pondering things - the difference is, when you are not in bed, and writing, you do it in a more constructive, brief way. For events the next day (difficult meeting etc.) nail down the worst case, the way it will likely go, the things you for sure do not wanna do&#x2F;say.And it weeds out the desire to unwrap all that crap once more as soon as you lay down. Which is the whole point of the exercise, basically.Do I read these things ever again? Most often not, honestly. But I maybe change my opinion on that, and if so, they are available.FWIW, I write these on computer, not pencil and paper. Lowers the friction for me, and is fine. Most often it&#x27;s in the range of 150-400 words, roughly guessed. reply nuancebydefault 13 hours agoprev> Writing What Pops into Your HeadI believe this is what I usually do when placing comments on this HN forum.Bonus: enhanced self-knowledge and in a lot of cases, instant feedback! reply hyperthesis 13 hours agoparentThe feedback-evaluation of you also must be evaluated. reply zingababba 12 hours agoprevI&#x27;ve been journaling for something like 25 years now. The hardest yet most rewarding part for me is going back and reading my entries. Sometimes I have to force myself because I&#x27;m often revolted by what I&#x27;ve written during various periods however it has really helped me understand my moods. reply SoftTalker 10 hours agoparentIs it important to go back and read them?The few times I&#x27;ve felt motivated to write down my thoughts were during periods of high personal stress and once they were on paper I put the notebook on a shelf. It sat for a while, and one day when I thought about it I tore up the pages and threw them away. They weren&#x27;t anything I needed anymore, and I certainly didn&#x27;t want anyone else to come across them. reply jseliger 14 hours agoprevA lot of people suffer a lot of anxiety around writing, and it seems like trying to lower the stakes helps: https:&#x2F;&#x2F;bessstillman.substack.com&#x2F;p&#x2F;on-writing-or-not reply adaml_623 13 hours agoparentNow I&#x27;m anxious about cancer and I didn&#x27;t even get to the secret about lowering the stakes :-&#x2F; reply voisin 12 hours agorootparentI read it and can’t find what the secret about lowering the stakes is. It seems like it takes the impending death of a loved one to put into perspective the resistance of writing as an emperor with no clothes. That if you’re meant to write you will when it realize it.Is my reading comprehension poor today or was there advice I missed on how to lower the stakes outside of a serious situation like she faces? reply thomasfuller 11 hours agoprevI find this to be true for me personally. I try to set aside 5 minutes every day just to write what ever I’m thinking about. I find it forces me to confront my anxieties directly. Sometimes even just writing about them reduces them.I think this style of writing is intentionally not meant to be perfected so I never go back and edit what I’ve written.Earlier this year I built https:&#x2F;&#x2F;chronofile.co to help me with this specific practice. reply slowhadoken 12 hours agoprevIt’s my preferred method of communication but persuasion is popular. Self-help rhetoric propagates quickly through large groups. reply rajataghi 13 hours agoprevRead through the article - fill-in journal seems like a good idea for a small app. Does anything like that exist? reply criddell 8 hours agoparentThere are lots of journals out there that provide a daily prompt. Day One (from Automattic) is pretty nice and you get basic functionality for free. reply extasia 13 hours agoparentprevFill-in in what sense? Like a physical journal but digital? reply iammjm 2 hours agorootparentI imagine a survey-like questionnaire: 1. Today as i woke up ... 2. In the course of the day i ... 3. One thing that particularly occupied me today was ... reply extasia 12 minutes agorootparentAh gotcha. Yeah that sounds neat actually. reply cyanydeez 10 hours agoprevGood to hear my livejournal was more than just bad teenage poetry reply ckyngcybra 10 hours agoprev [–] I started doing this like 3 years ago. Actually have like 30+ 100page sketchbooks filled so far. Keep meaning to catalogue it all, but I keep adding to it instead. reply iammjm 2 hours agoparent [–] I think it&#x27;s nice to see my own library grow. It will also be interesting to revisit it one day. It&#x27;s like a time capsule. Plus it&#x27;s thousands of pages of input to create an ai-version of myself one day ;) replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "According to physician and author Silke Heimes, daily writing of unfiltered thoughts for 5 to 20 minutes can enhance health, lessen stress, boost self-confidence, and improve imagination.",
      "Techniques for writing include automatic writing, completing half-sentences, and usage of personal names to overcome inhibitions. However, emotions triggered through writing if persistent may require professional assistance.",
      "Studies highlights expressive or therapeutic writing's positive effects. The key benefits range from higher self-esteem and heightened perception to increased mindfulness and cognitive clarity, making it a vital part of mental hygiene."
    ],
    "commentSummary": [
      "Unfiltered writing serves as a way of self-knowledge, stimulating creativity, providing therapeutic benefits, and allowing individuals to delve into their thoughts and perceptions.",
      "Some find writing restrictive, whereas others relish in it and use it as an effective tool for coping with emotions, mitigating anxiety, and enhancing sleep.",
      "Journaling, a subset of writing, is highlighted as a beneficial practice for self-reflection and discovery."
    ],
    "points": 170,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1696784891
  }
]
