[
  {
    "id": 40172033,
    "title": "Balancing Quality and Speed in Rust gamedev",
    "originLink": "https://loglog.games/blog/leaving-rust-gamedev/",
    "originBody": "Leaving Rust gamedev after 3 years Once you get good at Rust all of these problems will go away Rust being great at big refactorings solves a largely self-inflicted issues with the borrow checker Indirection only solves some problems, and always at the cost of dev ergonomics ECS solves the wrong kind problem Generalized systems don't lead to fun gameplay Making a fun & interesting games is about rapid prototyping and iteration, Rust's values are everything but that Procedural macros are not even \"we have reflection at home\" Hot reloading is more important for iteration speed than people give it credit for Abstraction isn't a choice GUI situation in Rust is terrible Reactive UI is not the answer to making highly visual, unique and interactive game UI Orphan rule should be optional Compile times have improved, but not with proc macros Rust gamedev ecosystem lives on hype Global state is annoying/inconvenient for the wrong reasons, games are single threaded. Dynamic borrow checking causes unexpected crashes after refactorings Context objects aren't flexible enough Positives of Rust Closing thoughts Disclaimer: This post is a very long collection of thoughts and problems I've had over the years, and also addresses some of the arguments I've been repeatedly told. This post expresses my opinion the has been formed over using Rust for gamedev for many thousands of hours over many years, and multiple finished games. This isn't meant to brag or indicate success, but rather just show there has been more than enough effort put into Rust, to dispel the the commonly said \"once you gain enough experience it'll all make sense\" argument. This post isn't a scientific evaluation or an A/B study. It's my personal opinion after trying to make Rust gamedev work for us, a small indie developer (2 people), trying to make enough money to fund our development with it. We're not one of those developers that have infinite money from an investor and years to burn. If you're in that category and are happy to build systems for years, none of the below will apply. I'm looking at things from the perspective of \"I want to make a game in 3-12 months maximum and release it so that people can play it and I can make some money from it.\". This is not written from a perspective of \"I want to learn Rust and gamedev seems fun\", which even though is a valid goal, is not in any way aligned with what we want, which is doing gamedev in a commercially viable and self-sufficient way. We've released a few games across Rust, Godot, Unity, and Unreal Engine, and many people played them on Steam. We also made our own 2d game engine with a simple renderer from scratch, and we also used Bevy and Macroquad for many projects over the years, some being very much non-trivial. I've also used Rust full-time at work as a backend developer. This post is not based on a short-sighted opinion of just going through a few tutorials or trying to make a small game for a game jam. We're well over 100k lines of Rust code written over 3+ years. The goal of this post is to serve as a reference to dispel commonly said arguments that get repeated over and over. But again, this is a subjective opinion, and in big part being written so that I don't have to continually explain the same things over and over again when people ask. I'd like this to be a reference for why we're likely abandoning Rust as a gamedev tool. We're in no way stopping with game development, we're just stopping with game development in Rust. If your goal is to learn Rust because it seems interesting and you like the technical challenge, that's completely fine. Part of what I want to appeal with this post is however how Rust gamedev is often presented, and advice that people often give out to others, without knowing whether they're building a tech demo or attempting to ship something. The community as a whole is overwhelmingly focused on tech, to the point where the \"game\" part of game development is secondary. As an example of this, I remember one time a discussion around the Rust Gamedev Meetup, that while was probably done jokingly was still imo illustrative of the issue, with something like \"someone wants to present a game at the meetup, is that even allowed?\" ... I'm not trying to say that people should have the same goals as we do, but I think maybe the way some things are communicated could be clearer, and that people should be more honest about what it is they're doing. Once you get good at Rust all of these problems will go away Learning Rust is an interesting experience, because while many things initially feel like \"this is a special problem only I'm having\", later one realizes that there's a few fundamental patterns that are universal, and that everyone learning has to re-discover and internalize in order to be productive. This may include simple things like &str vs String, or .iter() vs .into_iter() and having to constantly use those, or just the realization of how partial borrows often go against certain abstractions. Many of these things are just learning pains, and once enough experience is acquired the user can fully anticipate them without thinking and be productive. I've very much enjoyed my time writing various utilities and CLI tools in Rust, where I found it more productive than Python for anything but a few lines of code. That being said, there is an overwhelming force in the Rust community that when anyone mentions they're having problems with Rust the language on a fundamental level, the answer is \"you just don't get it yet, I promise once you get good enough things will make sense\". This is not just with Rust, if you try using ECS you're told the same thing. If you try to use Bevy you'll be told the same thing. If you try to make GUIs with whichever framework you choose (be it one of the reactive solutions or immediate mode), you'll be told the same thing. The problem you're having is only a problem because you haven't tried hard enough. I believed this for years. I tried, very hard, for years. I've definitely seen this happen on many levels with the language, and I've found myself to be very productive in certain areas, and learned to be able to anticipate what the language and type system wants in order to avoid these issues. But, and I say this having spent the past ~3 years and written over 100k lines of game-related code in it across the whole ecosystem of frameworks/engines and having made my own, many if not most of the problems don't go away if one isn't willing to constantly refactor their code and treat programming as a puzzle solving process, rather than just a tool to get things done. The most fundamental issue is that the borrow checker forces a refactor at the most inconvenient times. Rust users consider this to be a positive, because it makes them \"write good code\", but the more time I spend with the language the more I doubt how much of this is true. Good code is written by iterating on an idea and trying things out, and while the borrow checker can force more iterations, that does not mean that this is a desirable way to write code. I've often found that being unable to just move on for now and solve my problem and fix it later was what was truly hurting my ability to write good code. In other languages one can write code with \"I can throw this away later\" in mind, which I've found to be the most useful approach in terms of getting good code. An example being say that I'm implementing a player controller. I just want the player to move and do things, so that I can start building my level and enemies. I don't need a good controller, I just need it to do things. I can surely delete it and make a better one later. In Rust, sometimes just doing a thing is not possible, because the thing you might need to do is not available in the place where you're doing the thing, and you end up being made to refactor by the compiler, even if you know the code is mostly throwaway. Rust being great at big refactorings solves a largely self-inflicted issues with the borrow checker It's very often said that one of Rust's greatest strengths is ease of refactoring. This is most definitely true, and I have had many experiences where I could fearlessly refactor significant parts of the codebase, with everything working afterwards. Everything works as advertised? The thing is, Rust is also a language that will force the user to refactor much more often than other languages. It doesn't take a lot to suddenly be backed in a corner with the borrow checker and realize \"wait I can't add this new thing because things will no longer compile, and there's no workaround other than code restructuring\". This is where experienced people will often say that this becomes less of an issue once you get better at the language. My take is, while that is 100% true, there's a fundamental problem of games being complex state machines where requirements change all the time. Writing a CLI or a server API in Rust is a very different experience than writing an indie game. Assuming the goal is to build a good experience for players rather than an inert set of general purpose systems, the requirements might change from day to day just after having people play the game and you realize some things need to fundamentally change. Rust's very static and overly-checked nature fights directly against this. Many people would counter-argue that if you end up fighting the borrow checker and have to refactor your code it's actually good, because this makes your code better. I think this is a valid point to raise for something where you know what you're building. But in the majority of cases, I don't want \"better code\", I want \"game faster\" so that I can test it sooner and realize if the idea was good. It's not uncommon to be forced to make a choice between \"do I break my flow and spend the next 2 hours refactoring this to test an idea, or do I make the codebase objectively worse?\". I'd argue as far as maintainability being the wrong value for indie games, as what we should strive for is iteration speed. Other languages allow much easier workarounds for immediate problems without necessarily sacrificing code quality. In Rust, it's always a choice of do I add an 11th parameter to this function, or add another Lazy>, or do I put this in another god object, or do I add indirection and worsen my iteration experience, or do I spend time redesigning this part of code yet again. Indirection only solves some problems, and always at the cost of dev ergonomics One fundamental solution that Rust really likes and that very often works is adding a layer of indirection. A canonical example of this is Bevy's events, which are the go-to suggested solution for anything related to \"my system needs to have 17 parameters to do its thing\". I've tried to be on both sides of this, even specifically in the context of Bevy of trying to use events more heavily, and trying to just put everything in a single system. That being said, this is just one example. Many issues with the borrow checker can simply be worked around by doing something indirectly. Or by copying/moving something out, then doing the thing, then moving it back. Or by storing it in a command buffer and doing it later. This can often lead to interesting discoveries in terms of design patterns, for example one thing I found quite neat is how a very large portion of issues can be solved by reserving entity ids ahead of time (e.g. World::reserve in hecs, note the &world and not &mut world), combined with a command buffer. These patterns are amazing when they work, and they solve otherwise very difficult problems. Another example is a seemingly very specialized get2_mut in thunderdome that seems like a random idea at first, until one realizes that this is something that comes up all the time and that solves many unexpected issues. I won't get into arguing whether the learning curve to being productive is reasonable. It is certainly not, but this whole post is about the problems persisting on a fundamental level even after enough experience is acquired. Back to the point, while some of the above can solve specific problems, there's very often going to be a situation that can't be solved with a specialized and well-thought-out library function. This is where solving problems with \"just do the problematic thing later\" with a command buffer or event queue becomes something many will suggest, and it certainly works. The problem with games specifically is that we often care about inter-connected events, specific timings, and just overall managing a lot of state at once. Moving data across an event barrier means the code logic for a thing is suddenly split into two part, where even if the business logic might be \"one chunk\", it has to be cognitively regarded as two. Anyone who's been in the community long enough have had the experience of being told that this is actually a good thing, separation of concerns, code is \"cleaner\", etc. You see, Rust was designed in a smart way, and if something can't be done, it's because the design is wrong, and it just wants to force you down the right path ... right? What would be 3 lines of code in C# suddenly becomes 30 lines of Rust split into two places. The most canonical example here is something like: \"while I'm iterating over this query I want to check a component on this other thing and touch a bunch of related systems\" (spawn particles, play audio, etc.). I can already hear people telling me well duh, this is obviously an Event, you shouldn't be writing that code inline. Just imagine the horror of wanting to do something like (Unity code coming, brace yourselves, or just pretend it's Godot): if (Physics.Raycast(..., out RayHit hit, ...)) { if (hit.TryGetComponent(out Mob mob)) { Instantiate(HitPrefab, (mob.transform.position + hit.point) / 2).GetComponent().clip = mob.HitSounds.Choose(); } } This is a relatively simple example, but it is something one might want to write. And especially when implementing a new mechanic and testing things, it is something that you can just write. There is no maintainability to think about, I just want to do very simple things, and I want to do them in the place where they are supposed to happen. I don't want a MobHitEvent, because maybe there's 5 other things I may want to check the raycast against. I also don't want to check \"is there a Transform on the Mob\"? Of course there is one, I'm making a game. All of my entities have a transform. But Rust won't let me have a .transform, let alone in a way that would never crash with a double borrow error if I'm accidentally inside queries with overlapping archetypes. I also maybe don't want to check if the audio source is there. Sure I could .unwrap().unwrap(), but the more observant 🦀's will notice the lack of world being passed around, are we just assuming a global world? Aren't we using dependency injection to write out our query as another parameter in the system with everything laid out up front? Is .Choose assuming a global random number generator? What about threads??? And where exactly is the physics world, are we seriously assuming that to be a global too? If you're thinking \"but this won't scale\" or \"it might crash later\" or \"you can't assume global world because XYZ\" or \"what if multiplayer\" or \"this is just bad code\" ... I hear you. But by the time you finished explaining to me that I'm wrong I've already finished implementing my feature and moved on. I wrote my code in single pass without thinking about the code, and as I was writing it I was thinking about the gameplay feature I was implementing and how it affects the player. I wasn't thinking \"what's the right way to get a random generator in here\" or \"can I assume this being single threaded\" or \"am I in a nested query and what if my archetypes overlap\", and I also didn't get a compiler error afterwards, and I also didn't get a runtime borrow checker crash. I used a dumb language in a dumb engine and just thought about the game the whole time I was writing the code. ECS solves the wrong kind problem Because of the way Rust's type system and borrow checker works, ECS comes up as a naturally occurring solution to the problem of \"how do we have stuff reference other stuff\". Unfortunately, I think there's quite a bit of terminology mixup, and not only do different people mean different things, but also that the large part of the community attributes some things to ECS that aren't actually ECS. Let's try to separate things out. Firstly, let's mention a few things which we can't really do for various reasons (there's some nuance, but simplified since this article is already way too long): Pointer-y data with actual pointers. The problem here is simple, if character A follows B, and B gets deleted (and de-allocated), the pointer would be invalid. Rc> combined with weak pointers. While this could work, in games performance matters, and the overhead of these is non-trivial due to memory locality. Indexing into arrays of entities. In the first case we'd have an invalid pointer, in this case if we have an index and we remove an element, the index might still remain valid, but point to something else. Now comes a magic solution that gets rid of all these problems, generational arenas, as best shown by thunderdome, which by the way is a library I'd highly recommend as it's small and lightweight and does what it's supposed to while keeping its codebase readable, the last point being quite rare in the Rust ecosystem. Generational arena is basically just an array, except instead of having our id be an index, it's a tuple of (index, generation). The array itself then stores tuples of (generation, value), and we to keep things simple we can just imagine that every time something is deleted at an index we simply bump up the generation counter at that index. Then we just need to make sure that indexing into the arena always checks if the generation of the provided index matches the generation in the array. If the item was deleted, the slot would have a higher generation, and the index would then be \"invalid\" and act as if the item doesn't exist. There's some other reasonably simple problems to solve, such as keeping a free list of slots where we might want to insert to make insertion fast, but none of that is really that relevant for the user. The key point being, this allows a language like Rust to completely side-step the borrow checker and allow us to do \"manual memory management with arenas\" without actually touching any hairy pointers, and while remaining 100% safe. If there was one thing to point at that I like about Rust, it'd be this. Especially with a library like thunderdome it really feels that this is a great match, and that this data structure very well fits the language as it was intended. Now comes the fun part. What most people attribute as benefits of ECS are for the most part benefits of generational arenas. When people say \"ECS gives me great memory locality\", but their only query around mobs look like Query, what they're doing is basically equivalent to Arena where the struct is defined as struct Mob { typ: MobType, transform: Transform, health: Health, weapon: Weapon } Now of course defining things in this way doesn't have all the benefits of ECS, but I feel it should be very explicitly pointed out that just because we're in Rust and just because we don't want to have everything be Rc> does not mean we need ECS, it could just mean that what we really want is a generational arena. Back to ECS, there are few different ways to look at ECS that are very different: ECS as dynamic composition, allowing combinations of components to be stored and queried and modified together without having to be tied in a single type. The obvious example here that many people end up doing in Rust (because there's no other good way of doing this) is tagging entities with \"state\" components. One example might be that we want to query all Mobs, but maybe some of them have been morphed into a different type. We could simply do world.insert(entity, MorphedMob), and then in our query we can either query (Mob, MorphedMob), or something like (Mob, Not) or (Mob, Option) or check the presence of said component in code. Those might end up doing different things depending on different ECS implementations, but in practice we're using this to \"tag\" or \"split\" entities. Composition can be much richer than this. The example before also fits in this, where instead of having one big struct Mob, we can have this be a separate Transform, Health, Weapon, and maybe other things. Maybe a mob without a weapon doesn't have the Weapon component, and once it picks up a weapon we insert it into the entity. This would allow us to iterate over all mobs with a weapon in a separate system. I'll also include Unity's \"EC\" approach in the dynamic composition, as while it may not be the traditional purist \"ECS with systems\", it very much uses components for composition, and performance concerns aside, it ends up allowing for very similar things as if it was \"pure ECS\". I'd also like to give an honorable mention to Godot's node system, where child nodes are often used as \"components\", and while that has nothing to do with ECS, it has everything to do with \"dynamic composition\", as it allows nodes to be inserted/removed at runtime, and the behavior of entities to be altered because of this. It should also be noted that the approach of \"splitting up components into as small as possible for maximum reuse\" is something that is very often cited as a virtue. I've been in countless arguments where someone tried to convince me how I absolutely should be separating Position and Health out of my objects, and how my code is spaghetti if I'm not doing that. Having tried those approaches quite a few times, I'm now very much on the hard disagree side in complete generality, unless maximum performance is of concern, and then I'd concede on this point only for those entities where this performance concern matters. Having also tried the other approach of having \"fat components\", both before and after the \"separated\" approach, I feel that the \"fat components\" approach much better fits into games that have lots of logic that is unique to what is happening. For example, modelling Health as a general purpose mechanism might be useful in a simple simulation, but in every game I end up wanting very different logic for player health and enemy health. I also often end up wanting different logic for different types of non-player entities, e.g. wall health and mob health. If anything I've found that trying to generalize this as \"one health\" leads to unclear code full of if player { ... } else if wall { ... } inside my health system, instead of having those just be part of the big fat player or wall systems. ECS as dynamic structure of arrays, where due to how components are stored in ECS we get the benefit of iterating over say just Health components and having them next to each other in memory. For the uninitiated, this would mean instead of Arena, we'd instead have: struct Mobs { typs: Arena, transforms: Arena, healths: Arena, weapons: Arena, } and where values at the same index would belong to the same \"entity\". Doing this by hand is annoying, and depending on your background and which languages you've used in the past you may have had to do this by hand at some point. But thanks to modern ECS, we can kind of get this for free by just writing out our types in a tuple, and have the underlying storage magic put the right things together. I'd also call this use case ECS as performance, where the point of doing things this was is not \"because we want composition\", but \"because we want more memory locality\". This may actually have some valid applications, but I'd say for the vast majority of indie games that get shipped, this is not necessary. I'm intentionally saying \"that get shipped\", because of course it's easy to build mindblowingly complex prototypes that will require this, but those will also be infinitely far from ever being \"played\" by other people, and thus aren't of concern for this article. ECS as a solution to the Rust borrow checker, which is what I think most people using ECS are actually doing, or rather the reason why they're using ECS. If anything, ECS is a very popular solution and recommendation to give in Rust, because it tends to work around a lot of the issues. We don't need to care about lifetimes of things if we're just passing around struct Entity(u32, u32), since it's all nice and Copy, just like Rust likes it. The reason I have this as a separate point, is that many times people use ECS because it solves the particular problem of \"where do I put my objects\", without really using it for composition, and without really needing its performance. There's nothing wrong with that, only when such people end up getting into arguments all over the internet trying to convince other people that their approach of doing things is wrong, and that they should be using ECS a certain way because reasons mentioned above, without actually needing it in the first place. ECS as dynamically created generational arenas, which is something I kind of wanted to exist and tried to hack together, only to realize that to truly get what I'd want I'd have to re-invent many ugly interior-mutability related things that I wanted to avoid doing in the first place, just to allow doing things like storage.get_mut::() and storage.get_mut:: at the same time. Rust has this nice property that while you do things the way you want it to it's all fun and pretty, but once you want something it doesn't really like, things quickly turn into \"I need to re-implement my own RefCell that does this specific thing\" or worse. What I really mean by this point is that while generational arenas are nice, one of the big annoying downsides is that one has to define a variable and a type for every arena they intend to use. This can of course be solved by ECS if one just uses a single component in every query, but it would be nice and neat if one didn't need a full archetypical ECS just to get an arena-per-type on demand. There are ways to do this of course, but I'm way past burnout on trying to re-invent parts of the ecosystem to do this myself, and way past caring enough to force myself to do it. ECS because Bevy, which is partly meant as a joke, but I think due to Bevy's popularity and it's all-encompassing approach it should be mentioned as a separate view of ECS. Because for most engines/frameworks, ECS is a choice, it's a library that one decides to use. But as far as Bevy games are concerned, this isn't something optional that is used only for some things, the whole game is ECS. It should be noted in the most positive way, that while I may disagree on many things, it's hard to deny how much improvement has Bevy done to ECS APIs and the ergonomics of ECS itself. Anyone that has seen or even used things like specs understands how much better Bevy is at making ECS nice to use and approachable, and how much it improved over the years. That being said, I think this is also the core cause of the problem I have with how the Rust ecosystem views ECS, and especially how Bevy does. ECS is a tool, a very specific tool that solves very specific problems, and that does not come for free. I'll take a sidestep here and let's talk about Unity for a second. Regardless of what happened with its licensing, leadership or what its business model is, it'd be foolish to think of Unity as anything but one of the main things that made indie gamedev the success that it is. Looking at SteamDB charts there are now almost 44 000 games in Unity on Steam, with the second engine being Unreal at 12 000, and the rest falling well behind. Anyone that has been following Unity in the recent years knows about Unity DOTS, which is essentially their \"ECS\" (and other data oriented things). Now as a past, present and future user of Unity, I'm very excited by this, and one of the main reasons I find it exciting is that this co-exists with the existing game object approach. There's many intricacies, but at its core, things are what one could expect. A single game can use DOTS for some things, while also using the standard game object scene tree as it was before, and these two go well together. I don't think one would find a person in the Unity space who understands what DOTS things and thinks it's a bad feature that shouldn't exist. But I also don't think one would find a person who thinks DOTS is all there should be in the future, and that game objects should be erased from existence, and all of Unity should be moved to DOTS. Even ignoring maintenance and backwards compatibility, this would be remarkably stupid, as there are so many workflows that naturally fit into game objects. Those that have used Godot can probably see a similar view, especially those that used gdnative (e.g. via godot-rust), where while node trees maybe not the best data structure for everything, they for sure are extremely convenient for quite a few things. Taking this back to Bevy, what I don't think many people realize, is just how all-encompassing the \"ECS everything\" approach is. An obvious example and in my opinion a large failure point here is Bevy's UI system, which has been a pain point for a while, especially combined with the \"we'll start working on the editor this year for sure!\" types of promises. If you take a look at Bevy's UI examples it becomes very quickly obvious that there isn't much there, and taking a look at the source code for something simple as a button that changes color when hovered and clicked quickly reveals why. Having actually tried to use Bevy UI for something non-trivial, I can confirm that the pain is even greater than it looks, as the amount of ceremony required by the ECS to do anything UI related is just completely insane. As a result, Bevy's closest thing that exists to an editor is a 3rd party crate that uses egui. I'm simplifying things a bit, and of course there's more that goes into making an editor than just UI, but I do think that the insistence of putting everything into ECS, including UI, is definitely not helping here. ECS in Rust has this tendency of turning from something that is considered a tool in other language to become almost a religious belief. Something that should be used because it is pure and correct, and because doing that is the right way. Programming language communities often have certain tendencies, and having been a serial language hopper over the years I find it interesting to compare these. The closest thing to Rust's view on ECS I can think of is Haskell, except, and I know this is an oversimplification but I'll say it anyway, I do feel that the overall community in Haskell is a lot more mature, and that people in general tend to be more reasonable about the existence of other approaches, and view Haskell as a \"fun tool to solve problems where it fits well\". Rust on the other hand often feels like when you talk to a teenager about their preference about anything. What comes out are often very strong opinions and not a lot of nuance. Programming is a very nuanced activity, where one has to often make suboptimal choices to arrive at a result in a timely manner. The prevalence of perfectionism and obsession with \"the correct way\" in the Rust ecosystem often makes me feel that the language attracts people who are newer to programming, and are easily impressionable. Again, I understand this doesn't apply to everyone, but I think the overall obsession with ECS is in some sense a product of this. Generalized systems don't lead to fun gameplay A very commonly offered solution to many issues preventing here is more generalization through systems. If only components were more granularly split up and proper systems were used, surely all those special-cased problems would've been avoided, right? Strong argument that is tough to say much against, other than \"general solutions lead to boring gameplay\". Having been quite active in the Rust gamedev community I've seen a lot of projects others were building, of course often the suggestions they offer do actually correlate with the game they're working on. People who tend to have neatly designed systems that operate in complete generality tend to have games that aren't really games, they're simulations that will eventually become a game, where often something like \"I have a character that moves around\" is considered gameplay, and where the core focus is on having one or more of the following: Procedurally generated world, planets, space, dungeons. Voxel based anything, with deep focus on voxels themselves, rendering voxels, world size and performance. Generalized interactions where \"anything can do X with anything else\". Rendering in the most optimal way possible, if you're not using draw indirect are you even making a game? Having good types and \"framework\" for building games. Building an engine for making more games like the one that is about to be built. Multiplayer. Lots of GPU particles, the more particles the better the VFX. Well structured ECS and clean code. ... and many more All of these are fine goals in terms of playing around with tech and learning Rust, but I want to re-iterate what was said at the top of this article. I'm not evaluating Rust from the perspective of technical curiosities or \"this scratches the right brain itch\". I want to make real games that will get shipped to real people (not developers) in reasonable amount of time, that those people will pay for and play, and have an actual chance of hitting the front page of Steam. To clarify, this isn't a cold blooded \"make money at all costs\" scheme, but it's also not a \"I'm just doing this for the lulz\". The whole article is written from a perspective of wanting to be a serious game developer who cares about games, gameplay and players, and not just tech enthusiasm. Again, nothing wrong with tech enthusiasm, but I think people should be very careful about what their actual goals are, and above all be honest with why they're doing what they're doing. Sometimes I feel like the way some projects present themselves are the way people talk about those projects is false advertising that creates an illusion that commercial goals can be attained with said approaches, instead of making it more clear that \"I'm just doing this for the tech itself\". Now back to generalized systems. Here's a few things that I think create good games, that are going directly or indirectly against generalized ECS approaches: Mostly hand-designed playthrough of a level. This does not mean \"linear\" or \"story\", but it does mean \"lots of control over when the player sees what\". Carefully crafted individual interactions throughout the levels. VFX that are not based on having lots of same-y particles, but time synchronized events (e.g. multiple different emitters firing on a hand-designed schedule) working across all the game's systems. Iterated playtesting with multiple passes on gameplay features, experimentation and throwing away what doesn't work. Shipping the game to players as fast as possible so that it can be tested and iterated on. The longer nobody sees it, the bigger chance nobody cares about it when it comes out. Unique and memorable experience. I understand that reading this many people would think I'm imagining an artsy-fartsy game made by a painter and not a real programmer who wants to make a game like Factorio, but this isn't true. I still like systemic games, I like code, I want to make something that is driven by programming, because I do feel like I'm mainly a programmer. What I think most people get wrong is mistaking carefully thinking through player interactions and designing them as something artistic. I'd argue that this is what game development actually is. Game development isn't building a physics simulation, it's not building a renderer, or building a game engine, or designing a scene tree, or a reactive UI with data bindings. A good example game here would be The Binding of Isaac, which is a very simple roguelike with hundreds of upgrades that modify the game in very involved, interactive and deeply complex ways. It's a game with many systems that play into each other, but it's also something that's not at all generic. It's not a game with 500 upgrades of the \"+15% damage\" variety, but many upgrades are of the \"bombs stick to enemies\" or \"you shoot a laser instead of projectiles\" or \"first enemy you kill each level will never spawn again\". Looking at a game like this retrospectively may make it look like it's something you could design up front with general purpose systems, but I think this is also something where most people go completely wrong with game development. You don't make a good game like this by sitting in a dungeon for a year, thinking through all the edge cases and building a general system and then PCGing all the upgrades. You build a prototype with some small amount of mechanics and have people play it, see that the core things work, and then add more things and have people play again. Some of these interactions have to be discovered through deep knowledge of the game after playing a lesser version of the game for many hours and trying many different things. Rust is the type of language where wanting to do a new type of upgrade might lead you down a path of refactoring all of the systems, and many would even say \"that's great, now my code is much better and can accommodate so many more things!!!\". It sounds like a very convincing argument, one that I've heard many times, and one that has also caused me to waste a lot of time chasing down solutions to the wrong problems. A more flexible language would allow the game developer to immediately implement the new feature in a hacky way, and then play the game, test it and see if the feature is actually fun, and potentially do a bunch of these iterations in a short amount of time. By the time the Rust developer is finished with their refactoring, the C++/C#/Java/JavaScript developer has implemented many different gameplay features, played the game a bunch and tried them all out, and has a better understanding of which direction should their game be taking. Jonas Tyroller explains this extremely well in his video on game design as a search, which I'd 100% recommend every game developer to watch, because it feels like the best explanation of why so many games people make (myself included) are profoundly terrible. A good game is not made in a lab where careful types are crafted, it is made by a developer who is a grandmaster player at the genre, and who understands every aspect of the design and has tried and failed many things before reaching upon the final design. A good game is made through scraping a lot of bad ideas, through a non-linear process. Making a fun & interesting games is about rapid prototyping and iteration, Rust's values are everything but that To better define this point we have to define what is meant by \"game development\" in this article. We're not talking about AAA, or large scale very long term projects in general. I don't think anyone can realistically think they're going to build a successful 5 year game project unless they already have a lot of prior experience with both game development and the tooling they're using. We're talking about indie games made by individuals or small teams on relatively tight budgets/timelines. Secondly, there are many reasons one could make a game, but our intention is to make something other people will play and consider to be good, without knowing which technology/engine/framework/ideology was used to create it, without knowing or relating to the author, and without having any prior exposure. I feel like this especially needs to be stressed out, because while the Rust community is overall very supportive, it often creates a very false idea that \"this is so cool, people will love a game like this\". It's not a problem only Rust struggles with, and many gamedevs end up showing their games to other gamedevs and gamedev communities, and thus fall for the same fallacy. Because of the general vibes in the Rust community it's very common for people to receive very positive reinforcement on what they're building. This is nice in terms of mental health and short term motivation, but having gone through the process of releasing something on Steam publicly more than once, I feel like many people are headed for a bitter realization once people who aren't in their friend group/community see their game. The reason I'm saying this is that I think the community as a whole has adopted this idea of relentless positivity and praise towards everything Rust related, shielding itself completely from the outside world. But the real world of gamers is not as nice. Gamers on Steam don't care if something is made in Rust, they don't care if it took 5 years to make, they don't care if the code is opensource. They care about looking at the game, and within a few seconds being able to tell if this is going to be a waste of time, or something potentially interesting. I've seen many people dismiss these things as the young generation and attention spans and ADHD this/that and people should appreciate XYZ. I don't find any of these views helpful, because everyone does this, we as game developers are just biased when it's about our games. When you're shopping at a grocery store and look at bananas and some of them have a slightly ugly color or look a bit damaged you'll pick the ones that look better instead. When going to a restaurant you'll pick one that looks like they'll have good food at a good price, or at least delivers an experience you care about. I'd even say that it is correct and desirable that players do not care about the developer and just look at the game for a few seconds, but at least that keeps us honest. It keeps the games be about the game itself and nothing else, because ultimately, it is the game and the experience of playing it that matters. It also reveals the values one as a game developer should appeal to. If you're showcasing your game and the response is anything but \"can I please play this?\", the game was not interesting to the person who you showed it to. At least not in the sense that truly matters for the purposes of making commercially successful games. People would often argue that Rust appeals to values like \"maintainability\" and how this leads to better games that don't crash, but I think the problem here is completely different scales. Surely we can all agree that a game crashing when someone presses play is bad, and it is definitely bad when you corrupt a save file and the player loses progress. But I think all of this completely misses the point of what matters to players. There are many cases where people would get their progress wiped and they'd still come back to the game and play it again, because the game was that good. I've done this more than once as a player. Rust as both language and community is so preoccupied with avoiding problems at all cost that it completely loses sight of what matters, delivering an experience that is so good that whatever problems are there aren't really important. This doesn't mean \"ship crap games\", it means focusing on the game being a good game, not on the code being good code. Procedural macros are not even \"we have reflection at home\" Game development as a domain of programming often requires one to write more than one type of code. We have system-y code for things like collisions, physics, particles. We have gameplay code for \"scripting\" entity behaviors. We have UI, VFX, audio. And then we also have tools. Depending on the game that is being built the size of each category may vary, but after working on enough of different genres of games I'd say it's generally universal that some amount of effort will have to be spent on every aspect. Rust fits very nicely in the low level algorithmic areas where one knows exactly what the problem is and just needs to solve it. Unfortunately, a lot of gamedev requires more dynamic approaches, and this becomes especially painful around level editing, tooling and debugging. Even something as simple as \"print this object\" is not a problem that can be reasonably solved without either writing code, or creating procedural macros. Now many languages have macros, and for those who haven't used Rust for long enough, they might not know that there's two types of macros in Rust: Declarative macros: These are relatively simple to create and very useful, but unfortunately quite limited. As many things in Rust, \"safety\" is above all else, and things that would be completely fine in C preprocessor macros become an impossible issue. The simplest example here is concatenating tokens, which now has a famous paste crate that gives a partial solution using a procedural macro. At a surface level you'd think great, problem solved, right? ... but unfortunately not even close, for example things like nesting and mixing procedural and declarative macros together isn't always going to work, and it's not even obvious what's possible and why until a lot of time is spent on figuring out the technicalities. Procedural macros: As a core idea procedural macros basically allow the programmer to run code at compile time, consume Rust's AST, and generate new code. There are many issues with this unfortunately. Firstly, proc macros aren't really cached and get re-run on recompiles. This ends up forcing your code to be split up in multiple crates, which isn't always possible, and if you rely on proc macros more heavily your compile times will suffer by a huge amount. There's many convenient proc macros like profiling's function macro which are very very useful, but ultimately unusable, because they destroy incremental build times. Secondly, procedural macros are incredibly difficult to write, and most people end up using very heavy helper crates, such as syn, which is a very heavy Rust parser that eagerly evaluates everything it's applied to. For example, if you want to annotate a function and just parse its name in your macro, syn will end up parsing the whole function body regardless. There's also the case where the author of syn is also the author of serde, a popular Rust serialization crate, which at some point last year started shipping a binary blob with its installation in a patch release, rejecting the community backlash. This isn't really a case against Rust, but I feel it should be mentioned, because it shows how a big part of the ecosystem is built on libraries made by single developers who can make potentially dangerous decisions. Of course this can happen in any language, but in terms of procedural macros this is very important, because almost everything in the ecosystem uses crates made by this specific author (syn, serde, anyhow, thiserror, quote, ...). Even ignoring the above, procedural macros have a very steep learning curve, and they have to be defined in a separate crate. This means that unlike with declarative macros where you can just create one as if you were making a function you can't easily just make a new procedural macro. In contrast, using reflection in C# is extremely easy, and if performance is of no concern (which it often isn't in cases where reflection is used) it can be a very quick and useful option for building tools or debugging. Rust doesn't offer anything of the sort, and the last approach for compile time reflection has been basically cancelled in one of last year's Rust dramas. As this article aims to remain technical I don't see much value in explaining the drama in detail or trying to take sides, because while all those are of varying importance to different people, practically the consensus in the community is that there is no more compile time reflection in sight in the near future, which is incredibly sad for everyone involved with the language. Procedural macros are a big and powerful tool, but their utility for indie game development is incredibly low, as their development cost and complexity is a bit too high to be used to solve minor issues that could've been solved by reflection with little to no effort. Hot reloading is more important for iteration speed than people give it credit for Before we get into Rust and hot reloading, I'd like to mention a few things. Firstly, if you haven't seen Tomorrow Corporation Tech Demo, I would 100% recommend every single game developer to watch this video to see what is possible in terms of hot reloading, reversible debugging, and overall tooling for game development. If you think you know what these things are, watch the video anyway. I have long felt that hot reloading was important at least to some extent, but seeing what these guys have built on their own really makes me feel ashamed of ever feeling that certain workflows were adequate for developing interactive experiences. For those who haven't watched the video, here's what the guys at Tomorrow Corporation have done: Built their own programming language, code editor, game engine, debugger, and games. Built support for hot reloading across the whole stack. Reversible time-travel debugging with a timeline that can scrub across game states. ... just watch the video :) I promise you won't regret it I understand that building something like this into an existing platform like .NET, or into a native language like C++ or Rust is borderline impossible in complete generality, but I also refuse the argument that just because it's hard and won't work 100% we shouldn't strive to want these things. There are many existing platforms/languages that support hot reloading to various extents. During my exploration I went as far as making a game in Common Lisp in order to get a feel for its hot reloading capabilities. I wouldn't necessarily advise people do that, but one does not have to go that far. Since .NET 6, it is now possible to do hot reloading in any C# project. Now I've heard people report mixed experiences on this, but I've also tried it myself, and it's a bit tough for me to take some of the arguments seriously, especially when they're from more recent times and not from \"I tried it when it came out\". In the context of Unity, there now is hotreload.net, which is a custom implementation made specifically for Unity, which I've been using for about 4 months now, and which has been completely amazing in terms of productivity. This is actually the #1 reason we're moving back to Unity. It's not the reason we're abandoning Rust, but it is a reason we're going to Unity and not Godot or UE5. (At the time of writing Godot does not support .NET hot reload, and UE still only has blueprints and C++.) For the purposes of this article, we can just focus on hot reloading bodies of functions, that is the only valid operation would be changing code inside of a function, and hot reloading that. Somehow this is a controversial topic in the Rust ecosystem, and many people will happily argue that it's not useful if it doesn't do everything, or that it's too restricted to be useful, or that the potential for bugs outweighs any possible benefits. I have a very hard time emphasising with any of this in the context of game development. Games are anything but stateless data processors. Few cases where hot reloading becomes incredibly useful: Immediate mode anything, be it UI or drawing. Even with fast compile times the iteration speed is significantly improved as one doesn't have to constantly re-enter the same state. Debugging with immediate mode drawing/geometry. This is probably my favorite use case, especially around debugging character controllers and physics, where I might enter an unexpected/buggy state, and with hot reloading I can simply add a line few lines to draw the relevant values in-game to see what's happening without having to reproduce the issue again. Tweaking constants that affect gameplay. While in some cases restarting the game with a new value would lead to a different result, games aren't scientific experiments. We don't need reproducibility, we need fun. It's much easier to optimize for fun when I can tweak values while playing. Crates like inline_tweak are useful here, but they require foresight. Hot reloading allows me to work on an unrelated feature and randomly get an idea \"I wonder what if\" and just do it, without it being a detour in what I was doing before. It should be noted that Rust does in fact have a solution in the form of hot-lib-reloader, but having tried it it's nowhere near perfect, even for the very simple use case of just reloading functions. I've had it break on many random occasions, and ultimately gave up as it was causing me more effort to play around with it than it was saving. Even if this crate worked without any issues it doesn't solve the issue of randomly tweaking things, as it still requires planning and foresight, which reduces potential creative usage. Many people counter hot reloading with \"but the compiler does XYZ\", to which I'd love to suggest something that would never get merged, but would be nice to have. What if there was a compiler flag ... and yes, I can already see people scream \"the poor compiler team\" ... I guess we'll never have this. There many partial workarounds, but none of them get close to the utility of true hot reloading, which is what I'd call what .NET and Unity currently have. Scripting languages are a partial solution and problematic in Rust for many reasons, manually implemented hot reloading with dylibs is limited, and any form of state serialization and restarting again only works for big code changes, and not just tweakability. Not to say these things aren't useful, but I think we as game developers should desire higher level of tooling than just \"I can reload a few structs in my code\", especially when other mature platforms can support much more general workflows. Abstraction isn't a choice This section is motivated by a very simple code sample I just wrote while working on our game. I have a UI with a list of characters, and a detail page that appears when a character (duck) is selected. The way the UI is structured I just have helper functions for each state of the UI, as we're using egui and immediate mode requires most things to be available in most places. This actually works great, because things like this will work egui::SidePanel::left(\"left_panel\").frame(frame).show_inside( ui, |ui| { ui.vertical_centered(|ui| { character_select_list_ducks(egui, ui, gs, self); }); }, ); egui::SidePanel::right(\"right_panel\").frame(frame).show_inside( ui, |ui| { character_select_recover_builds(ui, gs, self); }, ); egui::TopBottomPanel::bottom(\"bottom_panel\") .frame(frame) .show_inside(ui, |ui| { character_select_missing_achievements( egui, ui, gs, self, ); }); But let's say some of these have conditional state, and their implementation is actually quite non-trivial. This is the case when selecting a specific duck. Initially, my code was the following egui::CentralPanel::default().frame(frame).show_inside( ui, |ui| { character_select_duck_detail(ui, gs, self); } }); fn character_select_duck_detail(..., state: ...) {if let Some(character) = state.selected_duck { // some UI} else { // other UI} } This again works fine, problem is egui will often require very deep nesting just because almost every layout operation is a closure. It would be very nice if we could reduce the nesting and move the if outside. As a result we'd also separate out two clearly separate things ... first instinct: if let Some(character) = &self.selected_duck { character_select_duck_detail(.., character, self); } else { character_select_no_duck(...); } But here we get slapped on the wrist, did I actually think I could get away with passing self around while also borrowing a field on self? Even years into using Rust I still sometimes use too much of my brain thinking about the UI or game, and too little thinking about how I should be structuring my code, and end up with a problem like this. The Rust-y instinct would say \"clearly you need to separate your state and not pass around a big struct\", but this is a great example of how Rust clashes with the most natural way of doing things. Because in this case we're building a single UI window. I don't want to spend any of my brain cycles thinking about what parts of the UI need what parts of the state, I just want to pass my state around, it's not that big. I also don't want to spend extra time passing around more fields when I add more fields 15 minutes down the line, which I'm almost certain I'll do. I also don't want to be separating things into more than one struct, because there's more than one thing I might want to do an if on, and having gone down the \"splitting structs\" path before, it rarely works out on first try. The solution? As many things in Rust, we feel a bit of the 🤡 emotion (clown emoji for those who don't have the right installed), and then change the code to this: if let Some(character) = &self.selected_duck.clone() { character_select_duck_detail(.., character, self); } else { character_select_no_duck(...); } Everything now works, the borrow checker is happy, and we're cloning a string every frame. It won't show up in the profiler, so it really doesn't matter in the grand scheme of things. But it's especially sad for a language that aims to be so fast and optimal to have to resolve to wasting cycles on re-allocating memory more often than one would like, just to stay productive. I only mention this specific case because it's quite indicative of my overall experience writing Rust, and where many problems are simply solved by extra copying or cloning. It's something most Rust developers are familiar with, but that came as a surprise to many people I was helping learn Rust. Their usual response is \"wait I thought Rust was supposed to be very fast and efficient\" ... all one can say to that is \"oh it is fast, don't worry, in this case cloning the string every frame is totally harmless\" and then feel the 🤡 emotion again. GUI situation in Rust is terrible Just like there's a running joke of Rust having 5 games and 50 game engines, we probably need another joke for GUI frameworks. People are trying many different approaches, which in the complete generality of Rust as a language makes sense. But in this article we're focusing on gamedev, and I feel like this is something where we're not only seriously lacking, but I don't even see a way out. Now when I say UI, I don't mean UI to build an editor, I mean in-game UI specifically. Something that has to be highly stylized and visual. At least in my experience, the hardest part about building game UI isn't figuring out how to do data binding, or how to make things reactively update, or even how to best describe my layout. It is customizing the look and feel of the UI. This doesn't even touch on things like particles in UI, or various effects the user might want. Obviously a GUI library that is completely agnostic of everything can't have fancy shader effects and particles, but I think that's also part of the overall issue in approach. GUI libraries push all of this onto the user to figure out, and then every user is left to re-invent the wheel in their own framework/engine of choice. We ended up doing the majority of our UI in egui, which while sub-optimal and confusing in many ways at least provides a decent Painter interface for completely custom UI. When mentioning this and saying how much better the UI situation is in Unity or Godot people always say something like oh I tried Unity, it was terrible, I'm so much happier doing things in pure code. A very common response, and one that I also used to say, which completely misses the point that building a UI is a skill, and doing so in a complex UI toolkit like Unity or Godot provide is complex and annoying because it is something that has to be learned. Reactive UI is not the answer to making highly visual, unique and interactive game UI There are many GUI libraries in Rust, with many different approaches. Some are bindings to existing GUI libraries, some are immediate mode, some are reactive, and some even retained mode. Some try to use flexbox, while others don't really deal with layout on a fundamental level. The problem is that as far as game development is concerned, I'm not really sure if we have anything that approaches things the correct way. The reason we have so many libraries is the same reason we have so many game engines, it's because very few people in the Rust ecosystem are actually making games. At least in my view, game GUI doesn't really care that much about data being updated the fastest, about having reactive re-rendering, data bindings, or the fanciest declarative way to describe a layout. What I'd want instead is to have a very pretty GUI, with lots of custom sprites, animations, vector shapes, particles, effects, flashes, etc. I want my button to wiggle when it's clicked, I want my text to animate as it's hovered, I want to be able to use a custom shader and distort it with a noise texture. I want particles to fly around when a character box is selected. I understand that some games might want to render a table with a million elements, but I don't think that should be a goal of a game GUI. I also understand many if not all of the ones linked above are not marketing themselves as a game GUI, but that is partly my point in this section. As far as I'm aware, there isn't a single solution in the Rust ecosystem that would make it its goal to \"be good at making game GUIs\". I understand that having something like \"particles and shaders\" in a GUI is not going to be easy for a library that probably wants to be engine-agnostic, but this again might be another reason for why the situation is unlikely to improve. I do think that most games want to have buttons that wiggle, text that is animated, boxes that rotate in all the weird ways, and maybe even some kind of ungodly blur effect for when that happens. Is that crazy to want such things? Orphan rule should be optional This section can probably be quite short, because I think anyone who's tried to write a decent amount of userland Rust will feel the pain of orphan rule. It's a great example of something I'd call \"muh safety\", a desire for perfection and complete avoidance of all problems at all costs, even if it means significantly worse developer ergonomics. There are mostly valid reasons for wanting the orphan rule for things such as libraries uploaded to crates.io, and I am willing to concede that crates published there should obey this. But I have a very hard time caring about this rule for applications and libraries developed in end products. I'm explicitly not saying binary crates, because most bigger projects will be composed of more than one crate, and many will be more than one workspace. Practically, I'd say this should be something we could disable even for published libraries, as some are not really libraries that are consumed by further downstream libraries. Game engines and frameworks are a good example of this, as people using libraries like Macroquad or Comfy really don't need those to uphold the orphan rule in their codebase. It'd be very beneficial for \"framework-y\" libraries to be able to extend existing things without forking, and provide more unified experience to end users. But unfortunately, like many things in Rust, \"perfection is only in the absolute\", and just because there is chance that someone could possibly implemented a conflicting trait we must prohibit this for everyone in every circumstance with no option to disable it. Coroutines/async and closures have terrible ergonomics compared to higher level languages Debugging in Rust is terrible no matter what tools you use or what OS you're on Compile times have improved, but not with proc macros It's been a few years since Rust had truly terrible compile times, and the situation as a whole has certainly improved, at least on Linux. Incremental builds on Windows are still significantly slower, to the point where we initially ended up migrating to Linux (3-5x difference), but alas, at least after purchasing a new high end desktop it only takes a few seconds to build our 10k LoC codebase. That is, after having spent extensive amounts of time optimizing compile times, removing proc macros, and moving things into their respective crates. As a good example here, the only reason comfy-ldtk exists is to wrap a single file and ensure serde's monomorphisation happens in a separate crate. This might seem like a petty detail, but at least on my desktop this has resulted in incremental times of +10s instead of just 2s on Linux. A pretty gigantic difference for 1600 lines of struct definitions. Now I understand, serialization isn't a trivial thing, and I understand serde has a lot of feature. But I also don't think there's any universe where paying 8 second to compile 1600 lines of code is anywhere near reasonable. Especially when you look at the code and see it's all just simple structs. There's no complex generic magic, all of this comes down to serde being slow. I've seen many people not care about things like this, and having personally raised the issue of incremental compile times many times in many different contexts, and there's always a decent chunk of people who will convince me that it's fine, that their build takes 20-30 seconds or longer and that they're still being productive. At the risk of angering some, I can only attribute this to lack of experience with better tooling, or simply their game not having reached a stage where they actually need to iterate quickly. Or at the very least, I feels like some people realize how much more polish could their games have if their compile times were 0.5s instead of 30s. Things like GUI are inherently tweak-y, and anyone but users of godot-rust are going to be at the mercy of restarting their game multiple times in order to make things look good. If your experience here differs, I'd love to see an example of a very well polished and non-trivial amount of GUI that was built with a +30s incremental build time. Rust gamedev ecosystem lives on hype It's no news that the Rust gamedev ecosystem is young. When you ask around inside the community most people will admit this when issues are mentioned, and I'd say at least in 2024 we don't have an awareness issue as much anymore. I would say that the outside world has a very different view though, and I will attribute this to very good marketing on the side of Bevy and a few others. Just a few days ago Brackeys released their video about coming back to gamedev to do Godot development. As I've been watching this and started hearing about all the amazing opensource game engines I already had a feeling. At around 5:20 a picture of a Game Engine Market Map is shown, and I can only say I was truly shocked by seeing three Rust game engines there, and specifically which three: Bevy, Arete and Ambient. Now I want to make this extra clear, this blog post is not an attempt to take a stab at any specific project, and I understand those projects are not responsible for what other people are doing with their videos. But at the same time, this has become such a theme, or maybe even a meme, in the Rust world, that I feel it should be talked about. The way the Rust ecosystem generally works is whichever project can make the most amount of promises, shows the best website/readme, has the flashiest gifs, and most importantly appeals to the right abstract values, gets widely praised, regardless of the usability of said project. Then there are other projects which are often under the radar, because they're not sexy and are not promising undeliverable features, but instead are just trying to do a thing in a way that works, and those end up almost never being mentioned, or when they are they're mentioned as second class choices. The first example here is Macroquad, which is a very practical 2D game library, which runs on basically all platforms and has very simple API, compiles incredibly fast and has almost no dependencies, and was built by a single person. There's also an accompanying library miniquad which provides a graphics abstraction on top of Windows/Linux/MacOS/Android/iOS and WASM. Macroquad has however committed the one of the highest crimes in the Rust ecosystem, and that is using global state, and even being potentially unsound. I say potentially, even thoughI understand that purists will say \"no this isn't a question, it is wrong\", because for all intents and purposes it is completely safe to use unless you decide to use the lowest level API to touch the OpenGL context. Having used Macroquad for almost 2 years now, I've never ran into this being an issue. It is however something that will forever be mentioned whenever it is suggested, because it does not appeal to the ultimate Rust value, 100% safety and correctness. The second example is Fyrox, which is a 3D game engine with an actual full 3D scene editor, animation system, and seemingly everything needed to make a game. This project was also made by a single person, who is also making a full 3D game in said engine. Personally I have not used Fyrox, because just like this section mentions, I've been personally guilty of falling for the hype and picking projects that have pretty websites, lots of github stars, and present themselves a certain way. Fyrox has been gaining some traction on reddit lately, but it is truly sad for me how it almost never gets mentioned in any videos, despite having a full editor, which is something Bevy has been repeatedly promising for years now. The third example is godot-rust, which are Rust bindings to the Godot Engine. The most serious crime committed by this library is that it's not a pure Rust solution, but instead just bindings to a filthy C++ engine. I'm exaggerating a bit, but those that are looking at Rust from the outside may be surprised how close to reality this sometimes is. Rust is pure, Rust is correct, Rust is safe. C++ is bad and old and ugly and unsafe and complex. That's why in Rust gamedev we don't use SDL, we have winit, we don't use OpenGL, we have wgpu, we don't use Box2D or PhysX, we have rapier, we have kira for game audio, we don't use Dear ImGUI, we have egui, and above all we surely can't use an existing game engine that's written in C++. That would be a violation of the sacred crab code that everyone who uses rustup default nightly to get faster compile times agrees on in the license (the same one that prohibits us from using the logo (tm)(c) officially endorsed by the Rust foundation). If anyone is actually serious about making a real game in Rust, especially in 3D, my #1 recommendation would be to use Godot and godot-rust, because at least they end up having a fighting chance of delivering all the features they need to, because they can lean onto a real engine to help them deliver. We spent a year building BITGUN with Godot 3 and gdnative using godot-rust, and while the experience has been painful in many ways, it wasn't the fault of the bindings, but rather trying to mix large amounts of GDScript and Rust in all the possible and dynamic ways. This was our first and biggest Rust project and what lead us down the Rust path, and ultimately I'd say every game we made using Rust afterwards was less of a game, simply because we spent a lot of time trying to figure out irrelevant technical issues with Rust-the-language, some part of the ecosystem, or just some design decision that ended up being difficult to solve because of the rigidity of the language. I'm not going to say GDScript and Rust interop was easy, it was definitely not. But at least there was the option of \"just do the thing and move on\" provided by Godot. I feel this is something most people who try code-only solutions don't value, especially in Rust where the language can get in the way of creativity in so many inconvenient ways. I don't have much to say about Ambient because it is fairly new, and I have not used it, but again, I don't know of anyone else who has used it, and yet it made it into Brackeys video. Arete came out a few months ago with version 0.1, and actually received a relatively negative response from the Rust community due to being very vague about its claims and being closed source at the same time. Despite that, I've seen it mentioned by outsiders on many occasions, often with very bold claims. As far as Bevy is concerned, I do believe it being showcased as the \"main\" Rust game engine is mostly justified, if anything just because of the scale of the project and the number of people involved. They have managed to build a remarkably large community, and while I may disagree with their promises and some choices of the leadership, I can't deny the fact that Bevy is popular. The purpose of this section is nothing but to bring some awareness to the strange state of things, where outsiders will often just look at how well is each engine marketing itself and what it says in their announcement blog posts. The reason I feel the need to mention all these things, is because I've followed this path more than once, and more than once saw very convincing things people would say, only to later realize that they're just very good at talking, but not as good at delivering on those features. One notable mention that isn't a game engine is rapier, a physics engine that is very often recommended, as it promises to be a pure Rust solution to physics, a great alternative to the ugly outside world of Box2D, PhysX, and others. After all, Rapier is written in pure Rust, and thus enjoys all the benefits of WASM support, while also being blazingly fast, parallel at its core, and of course very safe ... right? My experience here mostly comes from 2D, where while basic things do work, some of the more advanced APIs are fundamentally broken, for example convex decomposition crashing on relatively simple data, or multibody joints causing a crash when they're removed. The latter being especially funny, because this makes me feel like I was the first person to try to remove a joint, which doesn't seem like such advanced usage. These might seem like edge cases, but overall I've also found the simulation to be quite unstable, to the point where I ended up writing my own 2D physics engine, and at least in my testing found it to cause less issues on simple things like \"prevent enemies from overlapping\". This isn't an ad for my physics library, please don't use it, as it's not very well tested. The point is that if a newcomer to Rust asks for a recommendation for physics, they will be recommended rapier, and many will say it's a great and popular library. It also has a nice website and is widely known in the community. Having been that person and having really struggled for months and thinking it must be me, I must be the one doing something wrong the only reason I feel like I \"found out\" was because I tried to re-implement it myself. A lot of the Rust ecosystem has a property of making the user feel like they're doing something fundamentally wrong, that they shouldn't be wanting to do a certain thing, that the project they want to build is undesirable or incorrect. It's a feeling similar to using Haskell and wanting to do side effects ... it's just not a thing \"you're supposed to want\". Except in Rust's case, the problem is that very often libraries that end up causing the user to feel this way will get universal praise and recognition, because most of the ecosystem lives on hype, rather than shipped projects. Global state is annoying/inconvenient for the wrong reasons, games are single threaded. I know that just by saying \"global state\" I'm immediately triggering many people who have strong opinions on this being wrong. I feel this is one of those things where the Rust community has created a really harmful and unpractical rules to put on projects/people. Different projects have vastly different requirements, and at least in the context of game development I feel that many people are mis-judging what are the actual problems. The overall \"hate\" towards global state is a spectrum, and most won't be arguing 100% against it, but I still feel there's many things where the whole community is just going in a wrong direction. Just to reiterate, we're not talking about making engines, toolkits, libraries, simulations, or anything of the sort. We're talking about games. As far as a game is concerned, there is only one audio system, one input system, one physics world, one deltaTime, one renderer, one asset loader. Maybe for some edge cases it would be slightly more convenient if some things weren't global, and maybe if you're making a physics based MMO your requirements are different. But most people are either building a 2D platformer, a top down shooter, or a voxel based walking simulator. Having actually tried the pure approach where everything is injected as parameters multiple times over the years (starting with Bevy 0.4, up to 0.10), and having tried building my own engine where everything is global and playing a sound is just play_sound(\"beep\"), my stance on what is more useful is quite clear. This isn't meant to be specifically against Bevy, I do think a large part of the ecosystem is guilty of this, with the only exception being macroquad, but I'm using Bevy as an example because it sits on the other end of the spectrum where everything is passed around explicitly. Here's some things I found very useful to have in Comfy that I use all the time in our games, that make use of global state: play_sound(\"beep\") for playing one off SFX. If more control is needed, one can use play_sound_ex(id: &str, params: PlaySoundParams). texture_id(\"player\") for creating a TextureHandle to refer to an asset. There is no asset server to pass around, because at worst I could use paths as identifiers, and since paths are unique, obviously the identifiers will be too. draw_sprite(texture, position, ...) or draw_circle(position, radius, color) for drawing. Since every non-toy engine will batch draw calls anyway, it's not like any of these would do much more than just push a draw command into a queue somewhere. I'm more than happy to have a global queue, because why would I care about passing around anything just to push a \"draw circle\" into a queue. If you're reading this as a Rust developer who isn't necessarily a game developer, you might be thinking \"but what about threads???\", and yes, this is also where Bevy servers as a good example. Because Bevy asked this question and tried to answer it in the most general way possible, what if we just made all our systems run in parallel. This is a neat theoretical idea, and might seem appealing to many who are new to gamedev, because just like in backend land where things are all async and run on threadpools it might seem this would lead to free performance. But unfortunately, I feel this is one of the biggest mistakes Bevy has made, and having been asking about this I feel many are starting to realize it too, although few really admit it. Bevy's parallel systems model is so flexible it doesn't maintain consistent ordering even across frames (at least last time I checked). If one wants to maintain ordering, they should specify a constraint. This again seems reasonable at first, but having tried to make a non-trivial game in Bevy on more than one occasion (months of dev time, tens of thousands of lines of code), what ended up happening is the user ends up specifying a ton of dependencies anyway, because things in a game tend to need to happen in a specific order in order to avoid stuff being randomly delayed by one frame depending on what runs first, or even worse things just sometimes behaving weird because you got AB instead of BA. When you raise an issue about this, you'll be heavily argued against because what Bevy does is technically correct, but for the purposes of actually making a game ends up being a huge amount of pointless ceremony. Now surely there must be an upside to this? Surely, all of this free parallelism is useful and makes games run blazingly faster? Unfortunately, after all the work that one has to put into ordering their systems it's not like there is going to be much left to parallelize. And in practice, what little one might gain from this will amount to parallelizing a purely data driven system that could've been done trivially with data parallelism using rayon. Looking back at all of gamedev over the years, I've written a lot more parallel code in Unity using Burst/Jobs than I have ever achieved in Rust games, both in Bevy and in custom code, simply because most of the work on games ends up being the game, with enough mental energy left to solve interesting problems. While in almost every Rust project I feel most of my mental energy is spent fighting the language, or designing things around the language, or at least making sure I don't lose too much developer ergonomics because something is done in a specific way because Rust requires it to be that way. Global state is a perfect example in this category, and while this section is long, I feel like it really has to be explained a bit further. Let's begin by just defining the problem. In Rust as a language, there's generally a few options: static mut, this is unsafe, meaning every usage needs unsafe, which gets very ugly and in the case of accidental misuse leads to UB. static X: AtomicBool (or AtomicUsize, or any other supported type) ... a decent solution that while a bit annoying at least isn't too annoying to use, but only works for simple types static X: Lazy> = Lazy::new(|| AtomicRefCell::new(T::new())) ... this ends up being necessary for the majority of types, and is not only annoying in terms of defining it and using it, but also leads to potential crashes at runtime due to double borrows. ... and of course \"just pass it around, don't use global state\" I can't count the number of cases where I've accidentally caused a crash because of a double borrow on something, and not because the code was \"poorly designed to begin with\", but because something else in the codebase forced a refactor, and as I was refactoring I ended up needing to also restructure my use of global state, leading to unexpected crashes. Rust users would say that this means my code was doing something wrong and that it actually caught a bug for me, and that this is a good example of why global state is bad and should be avoided. This isn't completely false, and there are bugs that can happen and that would be prevented by this sort of checking. But practically speaking, and in terms of the types of errors I run into when using a language with easy global state like C#, I'd say that in the context of gamedev it's quite rare that any of these problems actually occur in real code. On the other hand, crashes due to double borrows when doing anything with dynamic borrow checking are something that can happen very easily, and very often for the wrong reasons. One example being queries on overlapping archetypes with ECS. For the uninitiated, something like this is going to be a problem in Rust (simplified a bit for readability): for (entity, mob) in world.query::().iter() { if let Some(hit) = physics.overlap_query(mob.position, 2.0) { println!(\"hit a mob: {}\", world.get::(hit.entity)); } } The problem being, we're touching the same thing from two different places. An even easier example would be iterating over pairs by doing something like this (again simplified) for mob1 in world.query::() { for mob2 in world.query::() { // ... } } Rust's rules prohibit having two mutable references to the same object, and anything that could potentially lead to this can't be allowed. In the above cases we'd get a runtime crash. Some ECS solutions work around this, e.g. in Bevy one can at least do partial overlaps when the queries are disjoint, e.g. Query and Query)>, but that only solves the case where nothing overlaps. I'm mentioning this in a section on global state, because the existence of such limitations becomes especially apparent once things are made global, because it becomes very easy to accidentally touch a RefCell that another part of the codebase is touching through some global reference. Again, Rust developers will say this is good, you're preventing a potential bug!, but I'll again defer to saying that I don't think I've felt many cases where this has actually saved me from doing something wrong, or where doing this in a language without such restrictions would cause an issue. There's still the question of threading, but I think the main fallacy is where Rust game developers assume that games are the same as backend services where everything must run async in order to perform well. In game code one ends up having to wrap things in a Mutex or AtomicRefCell not to \"avoid issues they'd run into otherwise if they were writing C++ and forgot to synchronize access\", but rather just to satisfy the compiler's all encompassing desire to make everything threadsafe, even when there isn't a single thread::spawn in the whole codebase. Dynamic borrow checking causes unexpected crashes after refactorings As I'm writing this I just discovered yet another case of our game crashing because of an overlapping World::query_mut. We've been using hecs for about 2 years now, these aren't the trivial sort of \"oh I accidentally nested two queries, oopsie\" you run into when you first start using the library. But rather one part of the code being top level that runs a system that does something, and then an independent part of the code doing something simple with ECS somewhere deep down, and then through a large scale refactoring these end up overlapping unexpectedly. It's not the first time I've had this happen, and the commonly suggested solution is \"your code is just poorly structured, that's why you're running into these issues, you have to refactor and design it properly\". Countering such arguments is relatively difficult, because at the core they're not wrong, this happens because some parts of the codebase were suboptimally designed. The problem is, it's yet another case of Rust forcing a refactoring where no other language would. Overlapping archetypes aren't necessarily a crime, and non-Rust ECS solutions like flecs are happy to allow this. But this issue isn't limited to just ECS. We've had it happen time and time again with the use of RefCell, where two .borrow_mut() end up overlapping and causing an unexpected crash. The thing is, these aren't always just because of \"bad code\". People will say \"borrow for the shortest amount you can\" to work around the issue, but this isn't free. Obviously this again depends on having the code structured in the right way, but at this point I hope we've established that gamedev isn't server development, and code isn't always organized optimally. Sometimes one might have a loop that needs to use something from a RefCell, and it makes a lot of sense to extend the borrow over the whole loop instead of just borrowing where it's needed. This can immediately lead to an issue if the loop is large enough and calls a system that might need the same cell somewhere inside, usually with some conditional logic. Once could again argue \"just use indirection and do the conditional thing through an event\", but then again we're taking a tradeoff of having the gameplay logic spread over the codebase instead of just having 20 lines of obviously readable code. In a perfect world everything would be tested on every refactoring, every branch would be evaluated, and code flow would be nicely linear and top down where these things don't ever occur. One wouldn't have to use a RefCell but would carefully design their functions so they can pass down the right context object or only the needed parameters. Unfortunately, I don't see this being even remotely realistic for indie game development. Time spent refactoring a feature that might get removed 2 weeks down the line is time wasted, making RefCells a desirable solution to partial borrows where otherwise data would have to be re-organized into differently shaped context structs, or function parameters would have to be changed all over the place to drill down the right parameters, or indirection would have to be employed to separate things out. Context objects aren't flexible enough Since Rust has a relatively unique set of constraints on programmers, it ends up creating a lot of self-inflicted issues that end up having solutions that don't necessarily come up in other languages as often. An example of this is a context object that gets passed around. In almost every other language it's not a big problem to introduce global state, be it in the form of global variables or singletons. Rust does unfortunately make that quite a bit more difficult for all sorts of reasons mentioned above. First solution that one would come up with is \"just store the references to whatever you need for later\", but anyone who has used Rust for more than a few days will recognize that this is just not going to be possible. The borrow checker will require every reference field to have its lifetime tracked, and because lifetimes become generics that poison every single usage point of the type, it's not even something that can be easily experimented with. There's more than one problem here, but I feel the need to point this out a bit more explicitly, as it may not be obvious to those who haven't tried. On a surface level, it may seem that \"what if I just use the lifetimes?\", e.g. struct Thing x: &'a i32 } The problem is, what if we now want a fn foo(t: &Thing) ... well of course we can't, Thing is generic over a lifetime, so this has to become fn foo(t: &Thing) or worse. Same thing if we try to store Thing in another struct, now we end up with struct Potato, size: f32, thing: Thing, } and even though Potato might not really care about Thing, being in Rust lifetimes are to be taken with utmost seriousness, and we can't just ignore them. Things are actually much worse than they seem, because lets say you do end up going down this road, and try to figure out things with lifetimes. Rust also does not allow unused lifetimes, so say that you have struct Foo { x: &'a i32, } but as you're refactoring your codebase you end up wanting to change this to struct Foo { x: i32, } now that is of course completely prohibited, because you'd have an unused lifetime, and we can't have that. This may seem very minor, and in some languages this is somehow desired even in simpler cases, but the problem is lifetimes often require a decent amount of \"problem solving\" and \"debugging\" where one tries a few different things, and trying things with lifetimes often means adding or removing lifetimes, and removing lifetimes very often means \"oh this is now unused, you have to remove it everywhere\", leading to gigantic cascading refactorings. I have tried going down this road a few times over the years, and honestly, one of the most infuriating things is trying to iterate on a very simple change with lifetimes only to be forced to change 10 different places on every single change. But even if the above wasn't the case, in many cases we can't just \"store a reference to something\" anyway, because the lifetimes wouldn't work out. One alternative that Rust provides here is shared ownership, in the way of Rc or Arc. This of course works, but is heavily frowned upon. One of the things I've realized after using Rust for a while is that using these can actually save one quite a bit of sanity, although it does require not telling your Rust friends about the code you write anymore, or at least hiding it and pretending it doesn't exist. Unfortunately, there are still many cases where shared ownership is just the bad solution, possibly for performance reasons, but sometimes you just don't have control over the ownership and can only get a reference. The #1 trick in Rust gamedev is \"if you pass in references top down every frame, all your lifetime/reference problems disappear\". This actually works very well, and is similar to React's props being passed top down. There's only one issue, and that is that now you need to pass everything into every function that needs it. At first it seems obvious and easy, just design your code correctly and you won't have any issues, lol. Or at least that's what many would say, and specifically \"if you're having issues with this, your code is ugly/wrong/bad/spaghetti\" or \"you shouldn't be doing it that way\" and you know, the usual. Lucky for us, there is an actual solution, which is to create a context struct that is passed around and that contains all those references. This ends up having a lifetime, but only one, and ends up looking something like this: struct Context { player: &'a mut Player, camera: &'a mut Camera, // ... } Every function in your game can then just accept a simple c: &mut Context and get what it needs. Great, right? Well, only as long as you don't end up borrowing anything. Imagine you want to run a player system, but also hold onto the camera. The player_system just like everything in the game wants c: &mut Context, because you want to be consistent and avoid having to pass 10 different parameters into things. But when you try doing this: let cam = c.camera; player_system(c); cam.update(); you'll just be met with the usual \"can't borrow c because it's already borrowed\", as we touched a field, and the rules of partial borrows say that if you touch a thing the whole thing is borrowed. It doesn't matter if player_system only touches c.player, Rust doesn't care what's on the inside, it only cares about the type, and the type says it wants c, so it must get c. This may seem like a dumb example, but in larger projects with larger context objects it becomes unfortunately quite common to want some subset of the fields somewhere, while also conveniently wanting to pass the rest of the fields elsewhere. Now lucky for us, Rust isn't completely dumb, and it would allow us to do player_system(c.player), because partial borrows allow us to borrow disjoint fields. This is where defenders of the borrow checker will say that you simply designed your context object wrong, and that you should be splitting it up either into multiple context objects, or group your fields based on their usage so that partial borrows can be utilized. Maybe all the camera stuff is in one field, all the player stuff is in another field, and then we can just pass that field into player_system and not the whole c and everyone is happy, right? Unfortunately, this falls under the #1 problem this article tries to address, and that is that what I want to be doing is working on my game. I'm not making games to have fun with the type system and figure out the best way to organize my struct to make the compiler happy. There is absolutely nothing I'm gaining in terms of maintainability of my single threaded code when I reorganize my context object. Having done this exact thing quite a few times I'm very much certain that the next time I have a playtest and get new suggestions for my game I'll probably have to change the design again. The problem here is, the code is not being changed because the business logic is changing, it's being changed because the compiler isn't happy with something that is fundamentally correct. It may not follow the way the borrow checker works because it only looks at types, but it is correct in the sense that if we instead passed all the fields we're using it would compile just fine. Rust is making us make a choice between passing 7 different parameters or refactoring our struct any time something is moved around, where both of those options are annoying and wasting time. Rust doesn't have a structural type system where we could say \"a type that has these fields\", or any other solution to this problem that would work without having to redefine the struct and everything that uses it. It simply forces the programmer to do the \"correct\" thing. Positives of Rust Despite the whole article being very much against Rust, I'd like to list a few things that I found as positives, and that really helped us during development. If it compiles it often kinda just works. This is both a meme but in some sense not really. There have been many times where I was surprised by how far one can take \"compiler driven development\" and actually succeed. Rust's biggest strength by far is that when one is writing code that is fitting for Rust, things go very well, and the language guides the user along the right path. From my perspective the biggest strength here are CLI tools, data manipulation and algorithms. I've spent a non-trivial amount of time basically writing \"Python scripts in Rust\", where what would usually be small utilities that most would use Python or Bash for I chose to use Rust (both for learning and seeing if it'd work), and quite often I was surprised that this actually worked. I definitely would not want to be doing the same in C++. Performance by default. As we're moving back to C#, I ended up looking a bit into Rust vs C# performance at the more granular level, trying to match specific algorithms 1:1 between the two languages and tried to get performance as close as possible. Still, Rust would come out on top by a rough order of 1:1.5-2.5 after some efforts on the C# front. This probably isn't too surprising to those who consume benchmarks on a daily basis, but having gone through this myself and having really tried, I was very pleasantly surprised how naturally occurring Rust code would just be really fast. I do want to point out that Unity's Burst compiler improves on C#'s performance quite a bit, but I don't have enough A/B data to provide specific numbers, and have only observed significant speedups on C# alone. That being said, in all the years of Rust I've been continually pleasantly surprised how well the code runs, even despite doing very stupid things, which I often like to do. I will note that this is all predicated on having the following in Cargo.toml [profile.dev] opt-level = 1 [profile.dev.package.\"*\"] opt-level = 1 As I've seen many many many people asking about things being slow, only to find they're just making a debug build. And just as Rust is very fast with optimizations turned on, it is very slow with optimizations turned off. I use opt-level = 1 instead of 3 because in my testing I haven't noticed a difference in speed, but 3 compiled a bit slower, at least on the code I tested on. Enums are really nicely implemented. Everyone using Rust probably knows this, and I will say that as time passes I tend to move to more dynamic structuring of things rather than strictly with enums and pattern matching, but at least for the cases where enums fit, they end up being very nice to work with, and probably my favorite implementation across the languages I've used. Rust analyzer. I wasn't sure if I should put this in the positives or negatives. I'm putting it in the positives because I would 100% not be able to write Rust without it anymore. Having first started with Rust in 2013 or so, the tooling around the language has improved dramatically, to the point where it's actually very very useful. The reason I considered putting it in the negatives is that it is still one of the more broken language servers I have used. I understand that this is because Rust is a complicated language, and having spoken to a lot of people about this, I think my projects might be just a bit cursed (it's probably my fault) because it tends to crash and not work for me all the time (yes I have updated, it's been happening for over a year across machines/projects). But despite all that, it's still incredibly useful and helpful, and s",
    "commentLink": "https://news.ycombinator.com/item?id=40172033",
    "commentBody": "Leaving Rust gamedev after 3 years (loglog.games)1219 points by darthdeus 16 hours agohidepastfavorite740 comments Animats 14 hours agoThat's a good article. He's right about many things. I've been writing a metaverse client in Rust for several years now. Works with Second Life and Open Simulator servers. Here's some video.[1] It's about 45,000 lines of safe Rust. Notes: * There are very few people doing serious 3D game work in Rust. There's Veloren, and my stuff, and maybe a few others. No big, popular titles. I'd expected some AAA title to be written in Rust by now. That hasn't happened, and it's probably not going to happen, for the reasons the author gives. * He's right about the pain of refactoring and the difficulties of interconnecting different parts of the program. It's quite common for some change to require extensive plumbing work. If the client that talks to the servers needs to talk to the 2D GUI, it has to queue an event. * The rendering situation is almost adequate, but the stack isn't finished and reliable yet. The 2D GUI systems are weak and require too much code per dialog box. * I tend to agree about the \"async contamination\" problem. The \"async\" system is optimized for someone who needs to run a very large web server, with a huge number of clients sending in requests. I've been pushing back against it creeping into areas that don't really need it. * I have less trouble with compile times than he does, because the metaverse client has no built-in \"gameplay\". A metaverse client is more like a 3D web browser than a game. All the objects and their behaviors come from the server. I can edit my part of the world from inside the live world. If the color or behavior or model of something needs to be changed, that's not something that requires a client recompile. The people using C# and Unity on the same problem are making much faster progress. [1] https://video.hardlimit.com/w/7usCE3v2RrWK6nuoSr4NHJ reply kibwen 14 hours agoparent> I'd expected some AAA title to be written in Rust by now. I'm disinclined to believe that any AAA game will be written in Rust (one is free to insert \"because Rust's gamedev ecosystem is immature\" or \"because AAA game development is increasingly conservative and risk-averse\" at their discretion), yet I'm curious what led you to believe this. C++ became available in 1985, and didn't become popular for gamedev until the turn of the millenium, in the wake of Quake 3 (buoyed by the new features of C++98). reply busterarm 10 hours agorootparentLamothe's Black Art book came out in '95. Abrash's black book came out in '97. Borland C++ was pretty common and popular in 93 and we even had some not-so-great C++ compilers on Amiga in 92/93 that had some use in gamedev. SimCity 2000 was written in C++, way back in '93 (although they started with Cfront) An absolute fuckton of shareware games I was playing in the 90s were built with Turbo C++. reply pjmlp 3 hours agorootparentKind of true, however they had endless amounts of inline Assembly, as shown on the Black Book as well. I know of at least a MS-DOS game, published on Portuguese Spooler magazine, that was using Turbo C++ basically as a macro assembler. One of the PlayStation selling points for developers was being the first home console with a C SDK, while SEGA and Nintendo were still doing Assembly, C++ support only came later to the PlayStation 2. While I agree C++, BASIC, Turbo Pascal, AMOS were being used a lot, specially in the Demoscene, they were our Unity, from the point of view of successful game studios. reply zonovar 2 hours agorootparentprevI also remember by videogame magazines I was reading back in early 90s that another C++ compiler that was a favourite among devs was Watcom C++ that was released in 88. reply flohofwoe 43 minutes agorootparentThat doesn't mean that it was used primarily with C++ though. IIRC Watcom C/C++ mainly became popular because of Doom, and that was written in C (as all id games until Doom 3 in 2004 - again IIRC though). The actual killer feature of Watcom C/C++ was not the C or C++ compiler, but its integration with DOS4GW. reply wqweto 23 minutes agorootparentBtw, dont’t remember Turbo C or Borland C++ to be able to compile to 32-bit x86 on DOS reply hamilyon2 13 hours agorootparentprevI really hope that C++ evolves with gamedev and they become more and more symbiotic. Maybe adoption of rust by gamedev community isn't the best thing to wish to happen to language. Maybe it is better to let other crowd to steer evolution of rust, letting system programming and gamedev drift apart reply lumost 6 hours agorootparentprevI sometimes wonder if the problem with rust is that we have not yet had a major set of projects which drive solutions to common dev problems. Go had google driving adoption, which in turn drove open source efforts. The language had to remain grounded to not interfere with the doing of building back-end services. Rust had mozilla/servo which was ultimately unsuccessful. While there are more than a few companies uinf rust for small projects with tough performance guarantees - I haven't seen the “we manage 1-10 MM sloc of complex code using rust” type projects. reply doublepg23 6 hours agorootparent? I believe the Rust efforts in Firefox were largely successful. I think Servo was for experimental purposes and large parts were then added to Firefox with Quantum: https://en.wikipedia.org/wiki/Gecko_(software)#Quantum reply danielheath 5 hours agorootparentMy recollection was that those were separate changes - servo didn’t get to the stage where it could be merged, but it was absolutely the plan to build a rendering engine that outperformed every other browser before budget cuts hit. reply emmelaich 2 hours agorootparentprev> Go had google driving adoption This is commonly said but I think it's only correct in the sense that Google is famous and Google engineers started it. Google never drove adoption; it happened organically. reply zozbot234 3 hours agorootparentprevServo is an ongoing project, it has not \"failed\" or been unsuccessful in any sense. reply bjconlan 2 hours agorootparentI think the original poster is perhaps speaking to previous articles (ie https://news.ycombinator.com/item?id=39269949) which from the outside looking in made me feel that perhaps this infact was the case (at least for a period). reply hot_gril 13 hours agorootparentprevExactly, it's all about the ecosystem and very little about the language features reply jacobgorm 12 hours agorootparentC++ classes with inheritance are a pretty good match for objects in a 3D (or 2D) world, which is why C++ became popular with 3D game programmers. reply otikik 2 hours agorootparentThis is not at all my experience. What I have experienced is that C++ classes with inheritance are good at modeling objects in a game at first, when you are just starting and the hierarchy is super simple. Afterwards, it isn’t a good match. To can try to hack around this in several ways, but the short version of it is that if your game isn’t very simple you are better off starting with an Entity Component System setup. It will be more cumbersome to use than the language-provided features at first, but the lines cross very quickly. reply __turbobrew__ 4 hours agorootparentprevThis is how I feel about golang and systems programming. The strong concurrency primitives and language simplicity make it easier to write and reason about concurrent code. I have to maintain some low level systems in python and the language is such a worse fit for solving those problems. reply hot_gril 11 hours agorootparentprevYeah, OOP makes sense for games. The language will matter a bit for which one takes off, but anything will work given enough support. Like, Python doesn't inherently make a lot of sense for data processing or AI, but it's good enough. reply throwaway2037 2 hours agorootparentTo be clear, the reason why Python is so popular for data wrangling (including ML/AI) is not due to the language itself. It is due to the popular extensions (libraries) exclusively written in C & C++! Without these libraries, no one would bother with Python for these tasks. They would use C++, Java, or .NET. Hell, even Perl is much faster than Python for data processing using only the language and not native extensions. reply kagakuninja 11 hours agorootparentprevOOP kind of goes out the window when people start using entity component systems. Of course, like the author, I'm not sure I'll need ECS since I'm not building a AAA game. reply hot_gril 10 hours agorootparentHad to look up ECS to be honest, and it's pretty much what I already do in general dev. I don't care to classify things, I care what I can do with something. Which is Rust's model. reply remram 8 hours agorootparentSorry I got lost in that sentence. What is Rust's model? reply hot_gril 7 hours agorootparentRust has traits on structs instead of using inheritance. Aka composition. reply zozbot234 2 hours agorootparentYou can also have structs be generic over some \"tag\" type, which when combined with trait definitions gets you quite close to implementation inheritance as seen in C++ and elsewhere. It's just less common because usually composition is all that's required. reply juleiie 7 hours agorootparentprevPython makes sense because of accessibility and general comfort for relatively small code bases with big data sets. Those data scientists at least from my experience are more into math/business than interested in most efficient programming. Or at least that was the situation at first and it sticked. reply meheleventyone 13 hours agorootparentprevDisagree the adoption of C++ was more about Moore's law than ecosystem, although having compilers that were beginning to not be completely rubbish also helped. reply pavlov 13 hours agorootparentAlso C++ could be adopted incrementally by C developers. You could use it as “C with classes”, or just use operator overloading to make vector math more tolerable, or whatever subset that you happened to like. So there’s really three forces at play in making C++ the standard: 1) The Microsoft ecosystem. They literally stopped supporting C by not adopting the C99 standard in their compiler. If you wanted any modern convenience, you had to compile in C++ mode. New APIs like Direct3D were theoretically accessible from C (via COM) but in practice designed for C++. 2) Better compilers and more CPU cycles to spare. You could actually count on the compiler to do the right thing often enough. 3) Seamless gradual adoption for C developers. Rust has a good compiler, but it lacks that big ticket ecosystem push and is not entirely trivial for C++ developers to adopt. reply pcwalton 13 hours agorootparentI'd say Rust does have that big ticket ecosystem push. Microsoft has been embracing Rust lately, with things like official Windows bindings [1]. The bigger problem is just inertia: large game engines are enormous. [1]: https://github.com/microsoft/windows-rs reply withinrafael 12 hours agorootparentRepo contributor here, just to curb some expectations a bit: it's one very smart guy (Kenny), his unpaid volunteer sidekick (me), and a few unpaid external contributors. (I'm trying to draw a line between those with and without commit access, hence all the edits.) There's no other internal or external Microsoft /support/ that I'm aware of. I wouldn't necessarily use it as a signal of the company's intentions at this time. That said, there are Microsoft folks working on the Rust compiler, toolchain, etc. side of things too. Maybe those are better indicators! reply heavyset_go 12 hours agorootparentThat's disappointing on Microsoft's part, because their docs make it seem like windows-rs is the way of the future. Thanks for your work, though! reply pjmlp 3 hours agorootparentDon't be, they also killed C++/CX, even went to CppCon 2016 telling us how great future C++/WinRT would bring to us. Now almost a decade later, VS tooling is still not there, stuck in ATL/VC++ 6.0 like experience (they blame it on the VS team), C++/WinRT is in maintenance, only bug fixes, and all the fun is on Rust/WinRT. I would never trust this work for production development. reply smileson2 10 hours agorootparentprevI wish Microsoft had any direction on the 'way of the future' for native apps on Windows reply travisgriggs 4 hours agorootparentIf they did publish a “way of the future” direction, would you believe them? Fool me N times then shame on them, fool me N+1 times, then shame on me sort of thing. reply tracker1 2 hours agorootparentI'd have bought into MAUI if there was Linux support in the box. reply meheleventyone 13 hours agorootparentprevI'd say the inertia is far more social than codebase size related. Right now whilst there are pockets of interest there is no broader reason to switch. Bevy as the leading contender isn't going to magic it's way to being capable of shipping AAA titles unless a studio actually adopts it. I don't think it's actually shipped a commercially successful indie game yet. Also game engines emphatically don't have to be huge. Look at Balatro shipping on Love2d. reply cableshaft 11 hours agorootparent> Also game engines emphatically don't have to be huge. Look at Balatro shipping on Love2d. Balatro convinced me that Love2D might be a good contender for my next small 2D game release. I had no idea you could integrate Steamworks or 2D shaders that looked that good into Love2D. And it seems to be very cross-platform, since Balatro released on pretty much every platform on day 1 (with some porting help from a third party developer it seems like). And since it's Lua based, I should be able to port a slightly simpler version of the game over to the Playdate console. I'm also considering Godot, though. reply meheleventyone 10 hours agorootparentThere’s a pretty big difference between the Playdate and anything else in performance but also in requirements for assets. So much so I hope your idea is scoped accordingly. But yeah Love2d is great. reply cableshaft 7 hours agorootparentIt is. I've already half ported one of my games to the Playdate (and own one), I'm pretty aware of its capabilities. The assets are what I struggle with most. 1-bit graphics that look halfway decent are a challenge for me. In my half-ported game, I just draw the tiles programatically, like I did in the Pico-8 version (and they don't look anywhere near as good as a lot of Playdate games, so I need to someday sit down and try to get some better art in it). reply pcwalton 13 hours agorootparentprevThere are a few successful games like Tunnet [1] written in Bevy. [1]: https://store.steampowered.com/app/2286390/Tunnet/ reply meheleventyone 12 hours agorootparentLooks cool and well received but at ~300ish reviews hardly a shining beacon if we extrapolate sales from that. But I'll say that's a good start. reply TillE 11 hours agorootparentSpeaking as a Godot supporter, I don't think sales numbers of shipped games are relevant to anyone except the game's developer. When evaluating a newer technology, the key question is: are there any major non-obvious roadblocks? A finished game (with presumably decent performance) tells you that if there are problems, they're solvable. That's the data. reply meheleventyone 10 hours agorootparentGame engines are tools not fan clubs. It’s reasonable to judge them on their performance for which they are designed. As someone who cares about the commercial viability of their technology choices this is a small but positive signal. What it tells me is someone shipped something and it wasn’t awful. Props to them! reply lelanthran 4 hours agorootparentprev> A finished game (with presumably decent performance) tells you that if there are problems, they're solvable. It doesn't tell you anything about velocity, which is by far the most important metric for indie devs. After all, the studio could have expended (maybe) twice as much effort to get a result. reply pcwalton 3 hours agorootparentOr maybe Rust allowed them to develop twice as fast. Who knows? We're going by data here, and this data point shows that games can be made in Bevy. No more and no less. reply pcwalton 10 hours agorootparentprevAgreed. We've learned a lot from Godot, by the way. I consider all us open source engines to be in it together :) reply jacobgorm 12 hours agorootparentprevSo far I am way less productive in rust than in any language I've ever used for actual work, so to rewrite an entire game engine would seem like commercial suicide. reply zozbot234 13 hours agorootparentprevYes, the Google folks are also funding efforts to improve Rust/C++ interop, per https://security.googleblog.com/2024/02/improving-interopera... reply Sn0wCoder 2 hours agorootparentThanks for the link. This one was also posted awhile back in a rust comment and when I first read it, I thought Google had used Rust in the V8 sandbox, but re-reading it seems that the article uses Rust as an ‘example’ of a memory safe language but does not explicitly say that it uses Rust. Maybe someone with more knowledge can confirm that Rust was (or was not) used in the V8 Google Chrome sandbox example…. https://v8.dev/blog/sandbox reply SunlitCat 9 hours agorootparentprevTheoretically accessible describes the experience of trying to use D3D from C very well! Was trying to use it with some kind of gcc for windows. The C++ part was still lacking some required features, so it was advised to use D3D from C instead C++. There were some helper macros, but overall I was glad when Microsoft started to release their Express (and later Community) Editions of Visual Studio. reply IggleSniggle 12 hours agorootparentprevThat description of problems bodes well for Zig reply synergy20 13 hours agorootparentprevnot true anymore, c11 and c17 are either supported or coming https://devblogs.microsoft.com/cppblog/c11-and-c17-standard-... reply meheleventyone 12 hours agorootparentNot really relevant to 30 years ago though. reply georgeecollins 12 hours agorootparentprevI worked on many of Activision's games 1995-2000 and C++ was the overwhelming choice of programming language for PC games. C was more common for console. In 1996 the quality of MSFT IDE/ Compiler, plus the CPUs available at the time was such that it could take an hour to compile a big game. By 1998 it was a few minutes. As I recall I think MSFT purchased another companies compiler and that really changed Visual Studio. reply philiplu 10 hours agorootparentI was a developer on the Microsoft C++ compiler team from 1991 to 2006. We definitely didn't purchase someone else's compiler in that time. We looked at the EDG front end at various times but never moved over to it while I was there. Perhaps the speed-up you remember had something to do with the switch-over from 16 bits to 32, which would have been the early to mid 90s. Or you're thinking of Microsoft's C compiler starting from Lattice C, back in the 80s before my time. There was also a lot of work done on pre-compiled headers to speed compilation in the latter half of the 90s (including some that I was responsible for). reply throwaway2037 2 hours agorootparentI heard that early versions of C++ IntelliSense from Visual Studio used Edison Design Group's (EDG) front end. Is that true? No trolling here -- honest question. If yes, are they still using it now? reply meheleventyone 10 hours agorootparentprevI was a teenager at that point. I learnt C in the early 90s and C++ after 96 IIRC. Didn’t start professionally in games until 2004 though! reply delfinom 13 hours agorootparentprevKind of both in my opinion. But rust is bringing nothing to the table that games need. At best rust fixes crash bugs and not the usual logic and rendering bugs that are far more involved and plague users more often. reply pcwalton 13 hours agorootparentThe ability of engines like Bevy to automatically schedule dependencies and multithread systems, which relies on Rust's strictness around mutability, is a big advantage. Speaking as someone who's spent a long time looking at Bevy profiles, the increased parallelism really helps. Of course, you can do job queuing systems in C++ too. But Rust naturally pushes you toward the more parallel path with all your logic. In C++ the temptation is to start sequential to avoid data races; in systems like Bevy, you start parallel to begin with. reply NBJack 12 hours agorootparentAside from a physics simulation, I'm curious as to what you think would be a positive cost benefit from that level of multithreading for the majority of game engines. Graphical pipelines take advantage of the concept but offload as much work as possible to the GPU. reply vvanders 5 hours agorootparentWe were doing threading beyond that in 2010, you could easily have rendering, physics, animation, audio and other subsystems chugging along on different threads. As I was leaving the industry most engines were trending towards very parallel concurrent job execution systems. The PS3 was also an interesting architecture(i.e. SPUs) from that perspective but it was so distant from the current time that it never really took off. Getting existing things ported to it was a beast. Bevy really nails the concurrency right IMO(having worked on AA/AAA engines in the past) it's missing a ton in other dimensions but the actual ECS + scheduling APIs are a joy. Last \"proper\" engine I worked on was a rats-nest of concurrency in comparison. That said as a few other people pointed out, the key is iteration, hot-reload and other things. Given the choice I'd probably do(and have done) a Rust based engine core where you need performance/stability and some dynamic language on top(Lua, quickjs, etc) for actual game content. reply pcwalton 4 hours agorootparent> That said as a few other people pointed out, the key is iteration, hot-reload and other things. Given the choice I'd probably do(and have done) a Rust based engine core where you need performance/stability and some dynamic language on top(Lua, quickjs, etc) for actual game content. I fully agree that this will likely be the solution a lot of people want to go with in Bevy: scripting for quick iteration, Rust for the stuff that has to be fast. (Also thank you for the kind words!) reply vvanders 4 hours agorootparentYeah, it's a fairly clean and natural divide. You see it in most of the major engines and it was present in all the proprietary engines I worked on(we mostly used Lua/LuaJIT since this predated some great recent options like quickjs). We even had things like designers writing scripts for AI in literate programming with Lua using coroutines. We fit in 400kb of space for code + runtime using Lua on the PSP(man that platform was a nightmare but the scripting worked out really well). Rust excels when you know what you want to build, and core engine tech fits that category pretty cleanly. Once you get up in game logic/behavior that iteration loop is so dynamic that you are prototyping more than developing. reply Animats 12 hours agorootparentprevIn big-world high-detail games, the rendering operation wants so much time that the main thread has time for little else. There's physics, there's networking, there's game movement, there's NPC AI - those all need some time. If you can get that time from another CPU, rendering tends to go faster. I tend to overdo parallelism. Load this file into a Tracy profile, version 0.10.0, and you can see what all the threads in my program are doing.[1] Currently I'm dealing with locking stalls at the WGPU level. If you have application/Rend3/WGPU/Vulkan/GPU parallism, every layer has to get it right. Why? Because the C++ clients hit a framerate wall, with the main thread at 100% and no way to get faster. [1] https://animats.com/sl/misc/traces/clockhavenspeed02.tracy reply pcwalton 12 hours agorootparentprevAnimations are an example. I landed code in Bevy 0.13 to evaluate all AnimationTargets (in Unity speak, animators) for all objects in parallel. (This can't be done on GPU because animations can affect the transforms of entities, which can cause collisions, etc. triggering arbitrary game logic.) For my test workload with 10,000 skinned meshes, it bumped up the FPS by quite a bit. reply Keyframe 1 hour agorootparentprev\"Fearless concurrency\" reply summerlight 11 hours agorootparentprevYeah, gaming industry has become mature enough to build up its own inertia so it will take some time for new technologies to take off. C# has become a mainstream gamedev language thanks to Unity, but this also took more than a decade. reply augusto-moura 9 hours agorootparentprevThe concept of AAA games didn't even exist back in 1985, very few people were developing games at that era, and even fewer were writing \"complex\" games that would need C++. The SNES came on 1990 and even then it had it's own architecture and most games were written in pure assembly. The PlayStation had a MIPS CPU and was one of the first to popularize 3D graphics, the biggest complexity leap. I believe your are seeing causation were only correlation should be given. C++ and more complex OOP languages just joined the scene when the games themselves became complex, because of hardware and market natural evolution reply dimitrios1 9 hours agorootparentprevComparing the time it takes for a prog language to spread from the 80s to today is a bad vantage point. Stuff took much longer to bake back then -- but even so the point is moot, as other commentors pointed out, it took off roughly the same amount of time between 2015 and today. reply holoduke 13 hours agorootparentprevMany tried c++ in early 90s, but wasnt it too slow/memory intensive? You had to implement lots of inline c/assembly to have a bit of performance. Nowadays everything is heavily optimized, but back then not. reply ovao 12 hours agorootparentIf you’re referring to game dev specifically, there have been (and continue to be) concerns around the weight of C++ exception handling, which is deeply-embedded in the STL. This proliferated in libraries like the EASTL. C++ itself however is intended to have as many zero-cost abstractions as possible/reasonable. The cost of exception handling is less of a concern these days though. reply justinhj 8 hours agorootparentException handling is easy enough to disable. Luckily, or C would probably still be the game developers go to. reply pcwalton 13 hours agoparentprev> I'd expected some AAA title to be written in Rust by now. Why? Those kinds of game engines are enormous amounts of code, and there's little incentive to rewrite. I do strongly disagree that we aren't ever going to see large-scale game development in Rust; it just takes time. Whether games adopt an engine is largely about that engine's maturity rather than anything about the language. Bevy is quite young; 0.13 doesn't even have support for animation blending yet (I landed that for 0.14). reply VelesDude 13 hours agorootparentIt was a few years back that the question came up to the developers of a Call of Duty title. \"Is there still code from Quake 3 in COD?\". They dodge around it by saying something like \"we cannot deny this but e use the most appropriate tech where needed\". While not confirmation, I wouldn't be surprised if there is a few nuggets of Q3 in that code base still doing some of the basics. That would be really cool if it is true. It seems like unless you are someone like John Carmack or most of Nintendo, game dev tools are about what can get the best results quickest rather than any sort of technical specifics. It is a business after all. reply jsheard 12 hours agorootparentA neat real-world example of ancient Quake code surviving to this day is visible in Valves games - the hardcoded patterns for flickering lights in Quake 1 survived into GoldSrc and then into Source and then into Source 2, most recently showing up in Half Life Alyx, 24 years on from their original appearance in Quake 1. https://www.alanzucconi.com/2021/06/15/valve-flickering-ligh... Basically all of the bigger systems will have been Ship-of-Theseus'd several times over by now, but little things like that can slip through the cracks. reply suby 1 hour agorootparentThat light flickering is quite cool, thanks for sharing. It reminds me of the Wilhelm scream, but on a much smaller scale of course. reply scruple 12 hours agorootparentprevIf that's the question... Let me assure you that there are decades-old pieces of code inside of, and used to assemble, many modern AAA games coming out of mature studios. The systems and tooling is typically carried forward. I don't think this is some big secret and you've intuited exactly the reason why: > game dev tools are about what can get the best results quickest rather than any sort of technical specifics. It is a business after all. reply VelesDude 9 hours agorootparentNot surprised at all that this stuff sticks around. I find it very endearing actually. Ain't broke, don't fix it! reply meheleventyone 12 hours agorootparentprevA lot of big projects have amazing longevity to their older architectural decisions. Unreal still has a lot of stuff in it people that used UE1 would recognize, I did most of my professional development on UE3 and a bunch of that is still pretty recognizable. Similarly Chrome is a product of the time it was first created. And looking into the Windows source is probably like staring into the stygian abyss. There is a lot of legacy and tech debt out there! reply VelesDude 9 hours agorootparentI remember years back someone form Microsoft calling the windows code base \"The Abyss\" because of how much technical legacy there was in it. I think it was Steve Gibson who said that the Windows code base had some very questionable things in it. For instance they had work experience high school students working on code that made it into the final build that was less than spectacular. Like how Windows used to stall when you put a CD in and wouldn't proceed until the disc spun up and started reading data. Windows 11 probably would still do that but I don't know because I don't have a disc drive any more. reply Negitivefrags 7 hours agorootparentIt wasn't really windows lagging, it was explorer. There used to be more things in explorer that were blocked on something ultimately blocked by I/O. This tends to not be the case so much any more, so I doubt it would happen today. Instead you get the dreaded \"Working on it....\". It seem's like hard drives can be just as slow to spin up these days as CDs were back in the day. reply qalmakka 3 hours agorootparentprevDamn I forgot about explorer hanging when you put a CD in. That was especially terrible when you didn't have DMA reply johnnyanmac 4 hours agorootparentprev> game dev tools are about what can get the best results quickest rather than any sort of technical specifics. It is a business after all. Bingo. Rust's biggest strength is correctness. But games aren't mission critical, and gamers are very tolerant towards bugs (maybe not on social media, but very few buggy games have had their sales impacted). Your biggest sale to AAA game devs are to engine programmers to minimize tech debt. But as we are seeing with the current industry, that's not exactly something companies care about until it's too late. Then on the indie level we get articles like this. Half the article ultimately came down to \"it's faster to break things and iterate than to do it right once\". Again, similar lack of need for bug-free games. In addition, few indie games are scoped to a point where they need a highly disciplined ECS solution to scale with. The author even criticizes the \"tech specs\" community part of rust gamedev. Different tools, diferent goals, different needs. IMO, I think Rust will help make for some very robust renderers one day, but ultimaely the scripting will be done on another language. Similar to how Unity uses C# scripting to a C++ engine, that they IL2CPP to bring back to a full C++ game. reply qalmakka 3 hours agorootparentThis, exactly. As an embedded turned Unreal developer the first impression I had while using Unreal is how little concern for correctness there is overall. UB is used liberally, and there's clearly a larger focus on development speed and ease off use compared to safety and correctness. If a game has integer overflow or buffer overflows nobody cares. Viceversa, you need to keep the whole thing usable enough for the various 3D artists and such who have a hard time understanding advanced programming. reply duped 14 hours agoparentprev> The \"async\" system is optimized for someone who needs to run a very large web server, Even there it's very problematic at scale unless you know what you're doing. async/await isn't zero cost, regardless of what people will tell you. reply zamalek 13 hours agorootparentAbsolutely. Async/await typically improves headroom (scalability) at the cost of latency and throughput. It may also make code easier to reason about. reply duped 13 hours agorootparentI disagree with this, you're probably not paying much (if at all) in latency or throughput for better scaling. What you're paying for with async/await is a state machine that describes the concurrent task, but that state machine can be incredibly wasteful in size due to the design of futures and the desugaring pass that converts async/await into the state machine. That's why I said it's not \"zero cost\" in the loosest definition of the phrase - you can write a better implementation by hand. reply dboreham 10 hours agorootparentprevDefinitely makes code harder to reason about. reply treyd 8 hours agorootparentIf you were to write the same code without using async you'd be trudging through a mess of callbacks and combinators. This is what writing futures code before 2018 was like. It was doable if you needed the perf but it sucked. Async is a huge improvement to readability and reasoning that we didn't have before. reply jeremyjh 5 hours agorootparentNo, actually that was just javascript. Programming environments with threading models don't have to live that way. Separate threads can communicate through channels and do quite well for themselves. See how it works is, you do something like let data = file.read(); and the it just sits there on that line until the read is done and then your data has the actual bytes in it and you just use them and go on with your life. reply eklavya 37 minutes agorootparentMaybe you are both right but your scales are orders of magnitude apart. reply littlestymaar 13 hours agorootparentprev> at the cost of latency and throughput. Compared to what? Doing epoll manually? reply zamalek 11 hours agorootparentA reactor has to move the pending task to some type of work queue. The task has to pulled off the work queue. The work queue is oblivious as to the priority of your tasks. Tasks aren't as expensive as context switching, but they aren't free either: e.g. likely to ruin CPU caches. Less code is fewer instructions is less time. If you care enough, you generally should be able to outdo the reactor and state machines. Whether you should care enough is debatable. reply cmrdporcupine 9 hours agorootparentThe cache thing is a thing I think a lot of people with a more... naive... understanding of machine architecture don't clue into. Even just synchronizing on an atomic can thrash branch prediction and L1 caches both, let alone working your way through a task queue and interrupting program flow to do so. reply littlestymaar 6 hours agorootparentprevSo yeah, you're thinking about the comparison between async/await and manual state machines management with epoll. But that's not what most people have in mind when you're saying async/await have performance impact, most of them would immediately think you're talking about the difference with threads. reply jvanderbot 13 hours agorootparentprevThreading, probably. reply astrange 12 hours agorootparentAsync/await isn't related to threading (although many users and implementations confuse them); it's a way of transforming a function into a suspendable state machine. reply jvanderbot 11 hours agorootparentI know. But threading, and earlier processes, were less scalable but potentially faster ways of handling concurrent requests. reply otterley 11 hours agorootparentIt's also much easier to reason about, since scheduling is no longer your problem and you can just write sequential code. reply littlestymaar 6 hours agorootparentThat's one way to see it. But the symmetric view is equally valid: async await is easier to reason about because you see were the block points are instead of having to guess which function is blocking or not. In any case you aren't writing sequential code, it's still concurrent code, and there's a trade-off between the writing simplicity of writing it as if it was sequential code, and the reading simplicity of having things written down explicitly. This “write-time vs read-time” trade of is everywhere in programming BTW, that's also the difference between error-as-return-values and exception, or between dynamic typing and static one for instance. reply eddd-ddde 13 hours agorootparentprevThreading is compatible with async reply jvanderbot 11 hours agorootparent\"threading alone\" as in a thread per request. reply littlestymaar 13 hours agorootparentprevI don't think so, because there isn't a performance drawback compared to threads when using async. In fact there's literally nothing preventing you from using a thread per task as your future runtime and just blocking on `.await` (and implementing something like that is a common introduction to how async executors run under the hood so it's not particularly convoluted). Sure there's no reason to do that, because non-blocking syscalls are just better, but you can… reply cmrdporcupine 12 hours agorootparentprevIf I'm not doing slow blocking I/O, I'm not doing epoll anyways. But the moment somebody drops async into my codebase, yay, now I get to pay the cost. reply littlestymaar 6 hours agorootparentEither you are doing slow IO (in some of your dependency) or you don't have anyone dropping async in your code though… reply andersa 11 hours agoparentprev> I'd expected some AAA title to be written in Rust by now. That hasn't happened, and it's probably not going to happen, for the reasons the author gives. The main reason is that you can't ship that Rust code on PS5 in a sensible manner. People have tried, got useless toys to compile, but in the end even Embark gave up. I remember seeing something from them that they had moved Rust to server-only. reply FridgeSeal 10 hours agorootparent> The main reason is that you can't ship that Rust code on PS5 in a sensible manner. Really - why’s that? reply tormeh 10 hours agorootparentSony requires that you use their tooling, which you can only get under NDA. reply justinhj 7 hours agorootparentIf there was significant pressure from developers Sony would allow Rust. I doubt there is any. reply vintermann 2 hours agorootparentprevReally a shame that there's that sort of thing going on in 2024 too. reply rr808 7 hours agoparentprev> I tend to agree about the \"async contamination\" problem. Argh I have the same issue. Sure if you write JS or Python you probably need async. My current Java back end that has like 5 concurrent users does not need async everything making 10x the complexity. reply int0x29 14 hours agoparentprev> * There are very few people doing serious 3D game work in Rust. There's Veloren, and my stuff, and maybe a few others. No big, popular titles. I'd expected some AAA title to be written in Rust by now. That hasn't happened, and it's probably not going to happen, for the reasons the author gives. At one point the studio behind the Finals was writing game server code in Rust with an Unreal engine client. Not sure if that's true still reply TheRoque 14 hours agorootparentThe studio you're talking about is Embark studios, and is openly pretty big on Rust [1] I think it was rumored that their next project will use a Rust game engine, but I am not sure how it's going now. [1] https://github.com/EmbarkStudios/rust-ecosystem reply jsheard 13 hours agorootparentTheir creative sandbox project is full Rust from client to server I believe. I haven't kept up with it after trying the closed alpha a while ago but it looks like it's still going, and has a name now: https://wim.live It's still only listed as coming to PC, Mac, Linux and Android so I guess they haven't broken through the barrier of shipping Rust on consoles. reply droopyEyelids 14 hours agorootparentprevBackend 3d code? reply xboxnolifes 12 hours agorootparentI'm not familiar with the domain, but wouldn't 3D collision checking be considered backend 3D code? Even if it's not rendered, it still needs to be calculated. reply ReleaseCandidat 14 hours agorootparentprevServer side rendering for games. reply internetter 13 hours agorootparentThat's a thing? reply pjmlp 2 hours agorootparentYep, Stadia might have failed, but GeForce Now and XBox Cloud Gaming have enough customers to keep them going. reply spiderice 2 hours agorootparentThat’s complete different. They are rendering the client and streaming it to users. That doesn’t make the client side code “server side” any more than you streaming Fortnite on Twitch does. reply pjmlp 1 hour agorootparentNope, XBox XDK has facilities for code to be aware of rendering server side. reply chris37879 12 hours agorootparentprevAbsolutely! Any sort of multiplayer game needs a source of authority if you want to prevent cheats like a hacked client lying about its position, and a really good way to do that is load the geometry of your level and run physics checks server side at a lower frequency than once per frame. Godot and Unity both support headless builds for exactly this reason, it's basically the whole game engine, minus the renderer, audio, and UI systems, usually. reply dj_mc_merlin 11 hours agorootparentThat is not server side rendering. Per your own comment: > minus the renderer (Otherwise you are completely correct.) Closest I can think of is server side ragdolls that are rendered the same on all screens and similar stuff. reply cmrdporcupine 12 hours agoparentprev\"I tend to agree about the \"async contamination\" problem. The \"async\" system is optimized for someone who needs to run a very large web server, with a huge number of clients sending in requests. I've been pushing back against it creeping into areas that don't really need it.\" 100% this. As I say elsewhere in these threads: Rust is the language that Tokio ate. It isn't even just async viral-chain-effect, it's that on the whole crates for one async runtime are not even compatible with those of another, and so it's all really just about tokio. Which sucks, if you're doing, y'know, systems programming or embedded (or games). Because tokio has no business in those domains. reply hgomersall 3 hours agorootparentIt does in my domain of systems programming with async data handling. Tokio works like a dream - slipping into the background and just working so I can concentrate on the business logic. reply dboreham 10 hours agorootparentprevDisappointing to hear this after battling the same nonsense in JS for years. reply cmrdporcupine 9 hours agorootparentIt's just endemic to the industry. Framework-itis reply otabdeveloper4 1 hour agorootparentprevRust is a language made and used by Dunning-Kruger people who violently react to having to learn the prior art. What did you really expect? reply retrocryptid 8 hours agoparentprevI'm happy to see someone still doing some work in second life. reply Animats 6 hours agorootparentThere's a lot going on. Someone is doing a new third party viewer, Crystal Frost, in Unity. Linden Lab has a mobile viewer in alpha test. Rendering is PBR now for new objects. There are mirrors! Content upload is moving to glTF, to be compatible with everybody else. Voice is switching from Vivox to WebRTC. Game controller support is in test. New users get better avatars. The dev staff is larger. None of this is yet increasing Second Life usership much, but it remains the best metaverse around. I thought the metaverse thing was going to be bigger. Meta spent so much money to produce so little. reply vaylian 3 hours agorootparent> There's a lot going on. I'd like to use the opportunity to ask: What happened during the covid pandemic? I haven't heard/read anything about second life during the pandemic even though this was probably a once-in-a-lifetime opportunity? Are there any news sources that you can recommend to keep an eye on second life, because it doesn't seem that it gets that much press coverage? reply Animats 2 hours agorootparent> What happened during the COVID pandemic? Usage went up about 10%, and then leveled off. Logged in right now, at 0020 PDT: 32084 users. Varies between 30,000 and 50,000 around the clock. > News sources * https://modemworld.me/ * https://ryanschultz.com/ reply matheusmoreira 12 hours agoparentprev> The \"async\" system is optimized for someone who needs to run a very large web server, with a huge number of clients sending in requests. Can you please elaborate on this? I see a lot of similar concerns in other contexts too. Linux kernel's scheduler for example. Is it a throughput/latency tradeoff? reply cmrdporcupine 12 hours agorootparentThe current popularity of the async stuff has its roots in the classic \"c10k\" problem. (https://en.wikipedia.org/wiki/C10k_problem) A perception among some that threads are expensive, especially when \"wasted\" on blocking I/O. And that using them in that domain \"won't scale.\" Putting aside that not all of use are building web applications (heterodox here in HN, I know)... Most people in the real world with real applications will not hit the limits of what is possible and efficient and totally fine with thread-based architectures. Plus the kernel has gotten more efficient with threads over the years. Plus hardware has gotten way better, and better at handling concurrent access. Plus async involves other trade-offs -- running a state machine behind the scenes that's doing the kinds of context switching the kernel & hardware already potentially does for threads, but in user space. If you ever pull up a debugger and step through an async Rust/tokio codebase, you'll get a good sense for what the overhead here we're talking about is. That overhead is fine if you're sitting there blocking on your database server, or some HTTP socket, or some filesystem. It's ... probably... not what you want if you're building a game or an operating system or an embedded device of some kind. An additional problem with async in Rust right now is that it involves bringing in an async runtime, and giving it control over execution of async functions... but various things like thread spawning, channels, async locks, etc. are not standardized, and are specific per runtime. Which in the real world is always tokio. So some piece of code you bring in in a crate, uses async, now you're having to fire up a tokio runtime. Even though you were potentially not building something that has anything to do with the kinds of things that tokio is targeted for (\"scalable\" network services.) So even if you find an async runtime that's optimized in some other domain, etc (like glommio or smol or whatever) -- you're unlikely to even be able to use it with whatever famous upstream crate you want, which will have explicit dependencies into tokio. reply karmarepellent 1 hour agorootparent> Putting aside that not all of use are building web applications Perfect moment to mention \"rouille\" which is a very lightweight synchronous web server framework. So even when you decide to build some web application you do not necessarily have to go down the tokio/async route. I have been using it for a while at work and for private projects and it turned out to be pretty eye-opening. reply collinvandyck76 8 hours agorootparentprev>now you're having to fire up a tokio runtime I've been developing in (mostly async) Rust professionally for a about a year -- I haven't written much sync rust other than my learning projects and a raytracer I'm working on, but what are the kind of common dependencies that pose this problem? Like wanting to use reqwest or things like that? reply Animats 2 hours agorootparent> Like wanting to use reqwest or things like that? Yes. Reqwest cranks up Tokio. The amount of stuff it does for a single web request is rather large. It cranks up a thread pool, does the request, and if there's nothing else going on, shuts down the thread pool after a while. That whole reqwest/hyper/tokio stack is intended to \"scale\", and it's massive overkill for something that's not making large numbers of requests. There's \"ureq\", if you don't want Tokio client side. Does blocking HTTP/HTTPS requests. Will set up a reusable connection pool if you want one. reply stephc_int13 14 hours agoprevAs a game developer for about two decades, I've never considered Rust to be a good programming language choice. My priorities are reasonable performances and the fastest iteration time possible. Gameplay code should be flexible, we have tons and tons of edge cases _by design_ because this is the best way to create interesting games. Compilation time is very important, but also a flexible enough programming structure, moving things around and changing your mind about the most desirable approach several times a day is common during heavy development phases. We almost never have specifications, almost nothing is set until the game is done. It is a different story for game engines, renderers, physics, audio, asset loaders etc. those are much closer to system programming but this is also not where we usually spend the most time, as a professional you're supposed to either use off-the-shelf engines or already made frameworks and libraries. Also, ECS is, IMHO, a useful pattern for some systems, but it is a pain in the butt to use with gameplay or UI code. reply fmbb 4 hours agoparent> It is a different story for game engines, renderers, physics, audio, asset loaders etc. those are much closer to system programming but this is also not where we usually spend the most time, as a professional you're supposed to either use off-the-shelf engines or already made frameworks and libraries. But this is where industry interest (the little there is) lies for Rust, is it not? This is what the AAA studios that are researching and prototyping are working on. C++ is not a popular language to implement the actual game in for all the reasons you list. It is too slow to compile and too rigid. The people who actually build the games, make them tick, are all working in visual scripting languages. reply hi-v-rocknroll 13 hours agoparentprevAs a non-game dev who uses Rust and Elixir, Rust wouldn't be my first pick for a large gamedev studio for multiple reasons. As for alternatives worth evaluating: Crystal, Cython (compiled Python), or Nim could result in increased gamedev productivity over C++ or C#. Maybe even Go because the iteration and compile times are very fast, and the learning curve is very low. reply ClimaxGravely 8 hours agorootparentOften in the past Lua has been used and in my experience it's been quite nice. It's very easy to bind, there's some nice editors out there and the performance is decent. There's some other game-specific scripting languages that have popped up (angelscript and wren come to mind but there's more). I've not used them in full production products though. Mostly just kicked the tires. Now that I think about it though, it's been almost 6 years since I've worked on an engine with lua support. Mainly because in the last few years I've been working with unity or unreal. reply prisenco 13 hours agorootparentprevnext [–]Go because the iteration and compile times are very fast Safety is important and for certain applications, Rust is unrivaled. But for games, like web apps, where time to market and innovation can be just as if not more important than being free of runtime errors, Go is more suited to rapid development than Rust on compile times alone. Of course, the libraries and support for both aren't quite there yet, so at this point neither is well suited to game dev. reply hi-v-rocknroll 12 hours agorootparentI agree. We almost have a paradox of choice nowadays because it's easier than ever to create new language platforms. Rust is something different because its thesis is safety and performance by default, more or less optimized for systems development primarily, but at the bargain of making dangerous things more complicated to accomplish somewhat intentionally. Unconventional languages are sometimes used as a conspicuous challenge to attract developers or to attempt to move some parts of an industry into new territory. reply KaiserPro 1 hour agorootparentprev> Cython (compiled Python), or Nim could result in increased gamedev productivity over C++ or C# If you're starting from scratch, then maybe. Having had to crash learn games dev (ex VFX systems person) Unity + c# is just so nice to use. most of the easiness of python, but with proper strict typing. (which you can turn off, if you want) plus the wealth of documentation, its great. I imagine unreal is quite good in that regard too. reply fuzztester 3 hours agorootparentprev>As for alternatives worth evaluating: Crystal, Cython (compiled Python), or Nim could result in increased gamedev productivity over C++ or C#. I read on a recent HN thread that Crystal compilation is slow due to its type inference, IIRC. reply Nuzzerino 13 hours agorootparentprevDoes Crystal support Hot Reloading? The slow compilation speed is a non-starter for me. reply sam0x17 10 hours agorootparentThey have an interpreter mode now that is quite good and should be well-suited for these situations reply neonsunset 8 hours agorootparentprevGamedev industry already settled on almost perfect language for this task (C#) so there is little profit in trying to reinvent the wheel. And by perfect I mean not the way Unity uses it but the way pure C# engines use it. reply hi-v-rocknroll 12 hours agorootparentprevHaha. Nope. Maybe Nim, V*, Go*, or Elixir would be a better choice for such a use-case. * So fast, they really don't need HCR. reply nox101 5 hours agorootparentHCR provides changing things while th game is running in it's current state. fast recompile does not. Start game, wait for engine to initialize, select level, wait for it to load, move player or camera to desired location. Now iterate on something at that location via HCR. If you have to recompile and restart the game you're not going to have fast iteration reply misswaterfairy 10 hours agorootparentprevNim does, when tied in with Unreal Engine 5. https://github.com/jmgomez/NimForUE https://www.youtube.com/watch?v=Cdr4-cOsAWA reply Nuzzerino 11 hours agorootparentprevI haven't tried it yet but I've wondered if Elixir might be a good choice for a game server with many concurrent players. reply hi-v-rocknroll 10 hours agorootparentDefinitely and for chat. BEAM/HiPE VM allows native linking using NIFs so it's possible to integrate Erlang or Elixir with C-compatible projects for critical code sections, library interfacing, and perhaps even the majority of a performance-critical game engine as native code. Rustler also exists to write NIFs in Rust. Recall how VMware ESXi core tech was implemented mostly as Linux kernel modules and heavily-modified Linux to turn it inside-out as a type-1 hypervisor. reply paulddraper 9 hours agorootparentprevGo is infamous for its gc latency spikes, which is the thing that games cannot tolerate. Though 1.18 helped a lot, you'd have to do some major persuasion to game devs that Go's gc is the kind of thing they'd want in their game. --- EDIT: Not sure the downvote, Go is know for its (historically at least) unsuitability for RTC or game dev. reply hi-v-rocknroll 6 hours agorootparentThat's true. Go ain't C4 (JVM), ORCA (Pony), HiPE (Erlang/OTP BEAM), or CLR (C#). The JVM and CLR runtimes have been beaten on for years at immense scale in server-side business settings. I wished Go supported embedded work (without a GC), had an alternative allocator a bit more like Erlang's, and had alternative implementations that transpiled to other languages, but it doesn't. Ultimately, I left when zillions of noobs poured in because it was seen as \"easy\" and started wasting my time rather than searching for answers themselves. If performance were such a huge concern, I don't see any valid resistance to Rust that completely lacks a GC and makes it easy to call C code other than \"it's something different\", \"there's too much hype\", or \"I don't like it\". Recent development tools like RustRover make is really damn easy to see whats a move value or a borrow, debug test cases, run clippy automatically, and check crates versions in Cargo.toml. Throw Copilot in there and let it generate mostly correct, repetitious code for you. reply philosopher1234 6 hours agorootparentprevI’ve heard that go has very low latency gc, i haven’t heard of it having spikes reply neonsunset 5 hours agorootparentThe problem with Go is its inadequate FFI, which is important for gamedev which tends to be FFI and syscall-heavy due to embedding another gamescript language and/or calling into underlying rendering back-end, sometimes interacting with input drivers directly, etc. Which is why C# has been chosen so often (it has performance not much worse than C++ (you can manually optimize to match it), zero or almost zero-cost FFI, and can also be embedded, albeit with effort). There are also ways to directly reduce GC frequency by writing less allocation-heavy code, without having to resort to writing your own drop-in GC implementation (which is supported but I haven't seen anyone use that new API just yet aside from a few toy examples, I suppose built-in GC is good enough). reply Tade0 1 hour agoparentprev> Also, ECS is, IMHO, a useful pattern for some systems, but it is a pain in the butt to use with gameplay or UI code. Not a game developer, but each time I tried to make one not using ECS(or something at least similar in spirit) I quickly found myself not being able to proceed due to the sheer mess in the codebase. How does one normally avoid that? reply harpiaharpyja 9 hours agoparentprevI had similar thoughts, about Rust being a good match for game engines but not games. Maybe it suggests Rust game engines might want to include an interpreter for some higher level language to actually do the gamedev in. Rust is pretty good for writing PL interpreters (and similar tooling) too, actually. reply Zambyte 8 hours agoparentprevI know you're not asking for recommendations, but Lisp, particularly SBCL, really seems to check all your boxes. I say this as someone who generally reaches for Scheme when it comes to Lisps too. There are a few game engines[0] for CL, but most of them seem to be catered specifically to 2D games. [0] https://github.com/CodyReichert/awesome-cl?tab=readme-ov-fil... reply lmm 10 hours agoparentprev> a flexible enough programming structure, moving things around and changing your mind about the most desirable approach several times a day is common during heavy development phases. That's the kind of code for which Rust-like languages shine. Rich type systems make it easy to change your mind about things and make large changes to your code with confidence. (Whether Rust tooling is actually at a level to take advantage of that is another question) reply Kamq 5 hours agorootparent> That's the kind of code for which Rust-like languages shine. Rich type systems make it easy to change your mind about things and make large changes to your code with confidence. I don't think this is true. Rust makes it easy to get the refactor right (generally speaking 100% right). But that's not what they're describing. They're describing where the ability to make the refactor fast, even if it doesn't work correctly (in the formal sense of correctly). That is to say, memory leaks and race conditions and all sorts of horrible nastiness may be tolerable during the dev process in exchange for trying out an idea more quickly. This is, of course, significantly more work at the end to patch up all of the things you did, but if you don't have to do the full work on 99/100 iterations, or got to try out more iterations because of the quick turnaround time, that would be considered a win here. reply wredue 6 hours agorootparentprev>Rich type systems make it easy to change your mind about things and make large changes to your code with confidence. To be fair, they need to be able to make large changes with confidence because what would be small changes in other languages tend to end up being very large changes in rust like languages. reply mrkeen 13 hours agoparentprev> My priorities are reasonable performances and the fastest iteration time possible. I bought Mount & Blade II Bannerlord in 2020-03-30. I love it to death, but come on... // 2024-02-01 $ curl https://www.taleworlds.com/en/News/552grep \"Fixed a crash that\"wc -l 29 // 2023-12-21 $ curl https://www.taleworlds.com/en/News/549grep \"Fixed a crash that\"wc -l 6 // 2023-12-14 $ curl https://www.taleworlds.com/en/News/547grep \"Fixed a crash that\"wc -l 101 Maybe feeling like you're iterating fast isn't the same as getting to the destination faster. Edit: Lol guys calm down with the down-vote party. I was counting crashes, not bugs: $ curl https://www.taleworlds.com/en/News/547grep \"Fixed a bug that\"wc -l 308 Does your C++ not crash, just theirs? reply bogwog 13 hours agorootparentThat game (currently) has 88% positive reviews on steam and a 77 metacritic score with over 15.5k people playing the game right now (according to steamcharts.com) Thats a lot of happy customers. reply kbenson 8 hours agorootparentprevI can't really comment on the quality of the game or experience or how buggy it feels because I've never played it, but I will say that counting fixed crash situations is a somewhat arbitrary and useless metric. If each of those crashes affected and was reported by a single person or even nobody because no regular person could really encounter it is a vastly different situation than if each of those crashes was experienced by even 1% of the users. The criteria by which something is decided to mention in the patch notes is not always purely because the users care. Sometimes it's because the developers want to signal effort to user and/or upper management. Maybe Mount and Blade was super boggy in the past and is still super buggy now so all the crashes fixed are just an indicator of how large the problem is for them and how bad the code still is. I dunno, you didn't really give any information to help on that front. reply Repulsion9513 6 hours agorootparent> If each of those crashes affected and was reported by a single person or even nobody Then do you really think they'd be spending time fixing it? (Actually, you know what, they probably would.) reply kbenson 2 hours agorootparentThat's why I had a paragraph mentioning different reasons things might be mentioned. I don't think it's uncommon to find a bug that could cause a crash while working something else, confirm it does crash, and then fix it. If the culture is to mention those things in patch notes even if you're not sure it actually ever caused a user problem, then it will be listed. That doesn't mean all, or even any, of the listed crashes were like that, but it does illustrate that it's hard to know what they actually mean without additional info. (for what it's worth, I'm a long time Tarkov player, so I'm definitely familiar wroth buggy games and apparent development problems with rushing, so this is more a devils advocate position on my part) reply lionkor 13 hours agorootparentprevWith Rust and the exact time iteration times, management and deadlines, you end up with the same amount, just theyre panic!() instead. Thats an improvement, sure, but its fighting a symptom. reply sam0x17 10 hours agorootparentThere are a bunch of useful clippy lints to completely disable most forms of panicking in CI. We use this at my work since a single panic could cost millions of $ in our case. reply mrkeen 13 hours agorootparentprevWith modern languages that take safety more seriously, it's a lot easier to spot places where the code 'goes wrong'. In an older language, you have nothing to tell you whether you're about to dereference null: foo.bar.baz = ...; Even if you've coded it 100% correctly, that line of code still looks the same as code which will segfault. You need to look elsewhere in codebase to make sure the right instructions populated those fields at the right time. If I'm scrolling past, I'll slow down everytime to think \"Hey, will that crash?\" Compare that with more safety focused languages where you can see the null-dereferences on the page. Unwrap() or whatever it is in Rust. Since they're visually present, you can code fast by using the unsafe variants, come back later, and know that they won't be missed in a code review. You can literally grep for unsafe code to refactor. reply CJefferson 6 hours agorootparentprevI love Rust, but a crashing released game is better than a half-finished \"perfect\" game, or a game where you couldn't iterate quickly, and ended up with a perfectly tuned, unfun game. reply Repulsion9513 6 hours agorootparent> a crashing released game is better than a half-finished \"perfect\" game For who? I, and I'm pretty sure most other gamers, would rather a fully-finished \"perfect\" game that took twice as long. reply lelanthran 3 hours agorootparent> For who? I, and I'm pretty sure most other gamers, would rather a fully-finished \"perfect\" game that took twice as long. Evidence suggests otherwise. Of all demographics, gamers appear to be the most tolerant of buggy software. I'm playing a 2020 game right now that has (in about 30 hours of gameplay): 1. Crashed twice 2. Froze once 3. Has at least ONE reproducible bug that a player would run into at least once every mission (including the first one). Since this game is now so old it's not getting any more patches, these bugs are there for all eternity, because they just do not move the needle on enjoyment by the gamer. Searching forums for Far Cry 5 Bugs gives results like this: https://www.reddit.com/r/farcry/comments/1ai4jzx/has_far_cry... Gamers just don't care about bugs unless it stops them playing the game at all! In order for bugs to have an effect on gamer enjoyment, it literally needs to make the game unplayable, and not just make the player reload from the last savepoint. reply KronisLV 1 hour agorootparent> 1. Crashed twice 2. Froze once 3. Has at least ONE reproducible bug that a player would run into at least once every mission (including the first one). Sounds about on par even for enterprise software, in cases where shipping quickly is prioritized over overall quality, doubly so for gamedev which is notorious for long hours and scope creep. reply bcrosby95 2 hours agorootparentprevHell no. Lots of these games take 5-7 years to make. You want to turn that into 10-14? I can live with the rare crash bugs. reply eviks 1 hour agorootparentWhat if it's 5-7, but only after there is a deep enough dev pool and language tooling to address some of the productivity issues mentioned in the blog? Why make up arbitrary x2 factors? reply StressedDev 4 hours agorootparentprevThe problem is we would have a lot less games and the games we would get would not be as fun. Rust appears to have the following problems: 1) As the article pointed out, game developers are less productive in Rust. This is a huge problem. 2) Game budgets are not going to get bigger. This means that if Rust reduces productivity, games are going to be less polished, less fun, etc. if they are written in Rust. 3) Game quality is already fine. 99% of the games I play have very few noticeable bugs (I play on an Xbox Series X). Even the games with bugs are still fun. Basically, gamers are looking for fun games which work well. They are not looking for perfect software which has no bugs. reply lelanthran 3 hours agorootparent> As the article pointed out, game developers are less productive in Rust. This is a huge problem. I don't think it's limited to just game developers though. Unless you are writing something in which any GC time other than 0ns is a dealbreaker, and any bug is also a dealbreaker, you're going to be less productive in Rust than almost any other language. reply pcwalton 3 hours agorootparentOh, come on, we're yet again extrapolating from \"Rust is bad at rapid iteration on an indie game\" to \"Rust is bad at everything\". If Rust were really that astoundingly unproductive of a language, then so many developers at organizations big and small wouldn't be using it. Our industry may be irrational at times, but it's not that irrational. reply lelanthran 2 hours agorootparent> Oh, come on, we're yet again extrapolating from \"Rust is bad at rapid iteration on an indie game\" to \"Rust is bad at everything\". I am saying that Rust development has a lower velocity than mainstream GC'ed languages (Java, C#, Go, whatever). I didn't think that you are disputing this claim; if you are disputing this, I'd like to know why you think otherwise. reply pcwalton 1 hour agorootparent> I am saying that Rust development has a lower velocity than mainstream GC'ed languages (Java, C#, Go, whatever). That's not what you said: you said you're going to be less productive in Rust than nearly any other language, not \"mainstream GC'd languages\". > I didn't think that you are disputing this claim; if you are disputing this, I'd like to know why you think otherwise. Depending on the domain, I am disputing that, because of things like the Cargo ecosystem, easy parallelism, ease of interop with native code, etc. There is no equivalent to wgpu in other languages, for example. reply lelanthran 41 minutes agorootparent> That's not what you said: you said you're going to be less productive in Rust than nearly any other language, not \"mainstream GC'd languages\". I feel that you're selectively reading only what you have talking points to respond to. Here is exactly what I said: > Unless you are writing something in which any GC time other than 0ns is a dealbreaker, and any bug is also a dealbreaker, you're going to be less productive in Rust than almost any other language. I mean, I literally carved out an exception use-case for Rust; viz for software that can't handle GC. I wrote a single sentence with a single point, not a a single point diluted over multiple paragraphs. You have to literally read only half-sentences to interpret my point the way you did. If you aren't going to even bother reading full sentences, why bother engaging at all? reply CJefferson 3 hours agorootparentprevNo, the game doesn’t take twice as long. It just gets abandoned half-finished. The world is full of half-finished games, it takes time and money to push to a finish. reply KaiserPro 1 hour agorootparentprevperfect is the enemy of good. You never release anything thats perfect. Perfect is impossible. reply Const-me 3 hours agorootparentprev> I, and I'm pretty sure most other gamers, would rather a fully-finished \"perfect\" game that took twice as long I have recently completed Cyberpunk Phantom Liberty. The game crashed 4-5 times during 100-150 hours of gameplay. The crashes were pretty much painless because I quick save often. The game was amazing. The development of the game started in 2012, 12 years ago. I’m not sure you or most gamers would rather want a fully-finished \"perfect\" Cyberpunk 2077 game released in 2036. reply raincole 6 hours agorootparentprevPhotoshop does crash. Trust me if you do enough image editing you'll know it's not even a super rare event. They're generally doing a poor job handling the situations where you have no enough storage or RAM. It didn't stop Adobe from being worth 200B. reply thom 13 hours agorootparentprevHard to know what TaleWorlds are actually optimising for because half the features of Bannerlord feel like they’ve never been played by a dev let alone iterated on. reply int_19h 2 hours agorootparentprevAnd yet the fact that Bannerlord game logic is entirely in C# makes this possible: https://github.com/int19h/Bannerlord.CSharp.Scripting which in turn makes it a lot easier and more convenient to mod. Try that with Rust... reply stephc_int13 13 hours agorootparentprevYeah this is a common problem in the industry, we rarely have enough time to refactor what should be considered prototype-level code into robust code. reply hi-v-rocknroll 12 hours agorootparentThe game dev industry could form a consortium to launch its own dedicated general purpose language built from scratch to compile very fast like V or Go, run predictability, be much safer, be more reusable, and be extremely productive with the lessons learned from C, C++, C#, and more. Also, I think LLMs will be able to run against code bases to suggest mass codemods to clean things up rather than having humans make a zillion changes or refactoring fragile areas of tech debt. LLMs are already being applied to generate test cases. reply jibe 11 hours agorootparentJonathan Blow’s Jai is an attempt at something like this. It’s looking promising so far! reply hi-v-rocknroll 10 hours agorootparentInteresting. I went through the primer spec. Appears to be a different kind of D or Go with some key points. Any new language should begin with a specific thesis of specific competitive advantages and problems it solves over existing customary and alternative tools. Kai appears to fulfill this property, so that's a good sign. reply stephc_int13 12 hours agorootparentprevI believe that better tooling can help, yes. With refactoring, debugging, creating performance and style reports, updating documentation and a ton of other stuff. reply neonsunset 9 hours agorootparentprevC# is that language (see Godot, Stride, FNA, Monogame). reply hi-v-rocknroll 9 hours agorootparentNot really, it was adopted. It originated from Microsoft as their post-J++ Java alternative for CLR for the purposes of making it easier to write banking server software and Windows apps. reply neonsunset 9 hours agorootparentDoes it matter what it was 20 years ago? It is the go-to language for gamedev today and only keeps getting better at it. reply hi-v-rocknroll 7 hours agorootparentBoth things can be true. I'm saying it wasn't designed to be as such. I don't what you're arguing about. reply tedajax 13 hours agorootparentprevThis comment is nonsense reply meinersbur 13 hours agorootparentprevMy impression is that this is due to their non-robust programming style. They do not add fallback behavior when e.g. receiving a null object. It would still be a bug, but could be a log entry instead of crash. reply mrkeen 13 hours agorootparent> My impression is that this is due to their non-robust programming style. It's been 50+ years. I don't think that it's worthwhile just telling the programmer to do a better job. > They do not add fallback behavior when e.g. receiving a null object. It would still be a bug, but could be a log entry instead of crash. This is a pretty big feedback loop: * The programmer puts the null into the code * The code is released * The right conditions occur and the player triggers it * IF DONE SKILLFULLY AND CORRECTLY the game is able to recover from the null-dereference, write it out to a log, and get that log back to the developers. * The programmer takes the null out of the code. If you don't do the first step, you don't get stuck doing the others either. reply nurettin 10 hours agorootparent50+ years and people still fail to grasp this. You have to put something (an optional, or a default constructed object in a useless state) and all you did was to skip the null check. In case of optional, you introduced a stack rewind or a panic. Everything else stayed the same. Maybe that default even deleted the hard drive instead of crashing. Coding is hard. \"just don't code\" is not the answer. You can avoid something, that doesn't mean it won't show up in some other fashion. reply sam0x17 10 hours agorootparentAgain, if you disallow unwrapping and panicking at the CI level, you actually force your developers to properly handle these situations. reply astrange 12 hours agorootparentprevArbitrary recovery to null pointers isn't a good way to do robust programming. I recommend doing the exact opposite actually. https://en.wikipedia.org/wiki/Crash-only_software https://medium.com/@vamsimokari/erlang-let-it-crash-philosop... reply 0x457 11 hours agorootparentA crash of an actor in BEAM is incomparable to a crash of a video game. reply epr 8 hours agorootparentIs it? Is there no reasonable case where you have a subsystem in a game crash, then restart itself? Unless I'm mistaken, I've experienced this myself in video games more than once. Anything beats a full crash with a pointless error message. reply astrange 9 hours agorootparentprevBut if your video game uses a DSL for actors then you can do it in the DSL, which avoids special arbitrary bug-hiding behavior. reply popcar2 15 hours agoprev> Rust gamedev ecosystem lives on hype I've been saying this for years. I've tried to get into Rust multiple times the past few years and one of the things I've tried was gamedev with Rust (specifically the library ggez when it was still being worked on, and a little bit of Bevy). I admittedly never got far, but I gave it a solid shot. My experience was instantly terrible. Slow compile times and iterations, huge package downloads (my project folder was roughly 1gb for a simple 2D project), and of course Rust itself was difficult to get into with lifetimes and having to wrap and unwrap my variables constantly and getting into wrestling matches with the borrow checker. I kept telling myself that everyone loves Rust and the community loves to rave about anything Rust-related and maybe I just don't get it, but it took some time to realize that no... It's just a terrible choice for it. I even tried to make UI with eGUI and was still miserable. Rust is a systems programming language but the community is trying to convince everyone should be used for general purpose stuff. And my other biggest problem is that they keep painting other non-Rust things as being fundamentally flawed for not being Rust. \"It's not memory safe\" is the biggest one thrown around, but when was the last time memory safety was actually a big problem in games? Unity uses C# which is garbage collected, Godot uses its own scripting language which makes it nigh impossible to leak memory, Unreal AFAIK has its own tools that makes memory management trivial. Rust game development feels like a solution looking for a problem to fix. I am curious about Bevy when it becomes mature and has its own editor, but for now I'm just not convinced gamedev with Rust will ever take off. reply pcwalton 13 hours agoparent> And my other biggest problem is that they keep painting other non-Rust things as being fundamentally flawed for not being Rust. \"It's not memory safe\" is the biggest one thrown around, but when was the last time memory safety was actually a big problem in games? Unity uses C# which is garbage collected, Godot uses its own scripting language which makes it nigh impossible to leak memory, Unreal AFAIK has its own tools that makes memory management trivial. Rust game development feels like a solution looking for a problem to fix. Memory safety may or may not be important in games, but the ability of engines like Bevy to analyze system dependencies and automatically scale to multiple CPUs is a big deal. Job queuing systems have been popular in gamedev for a very long time, and Rust's insistence on explicit declaration of mutability is a big part of the reason that \"just works\" in Bevy. reply kbenson 8 hours agorootparent> but the ability of engines like Bevy to analyze system dependencies and automatically scale to multiple CPUs is a big deal Is it? The article addresses that, and basically calls it a pointless feature that is almost never used and when it is the benefits are mostly lost because of real world needs and constraints, and that the problems it solves are easier solved through other solutions and add-on systems that are well understood. I think this might be a case where explaining the real-world benefit instead of the theoretical benefit is needed, if only to counter what are very pointed criticisms that are definitely deeper than at the theoretical level. reply pcwalton 7 hours agorootparentHere's a trace of a Bevy demo: https://i.imgur.com/oXUxC2h.png You can see that all the CPUs are being maxed out. This actually does result in significant FPS increases. Does it matter for every game? No. But it does result in better performance! reply lelanthran 3 hours agorootparent>> but the ability of engines like Bevy to analyze system dependencies and automatically scale to multiple CPUs is a big deal >> Is it? The article addresses that, and basically calls it a pointless feature > You can see that all the CPUs are being maxed out. You're missing the forest for the trees - the poster above basically said \"seeing all the CPUs being maxed out is a pointless feature\" and you reply with \"but see, all the CPUs are being maxed out\". You're literally ignoring the complaint and replying with marketing. reply pcwalton 1 hour agorootparentNo, the original article said that you don't get parallelism from Bevy in practice: > Unfortunately, after all the work that one has to put into ordering their systems it's not like there is going to be much left to parallelize. And in practice, what little one might gain from this will amount to parallelizing a purely data driven system that could've been done trivially with data parallelism using rayon. It's not saying \"yes, you get parallelism, but I don't need the performance\"; it's claiming that in practice you don't get (system-level) parallelism at all. That's at odds with my experience. reply tumdum_ 45 minutes agorootparentprevThat makes no difference if the game is boring. reply omniscient_oce 12 minutes agorootparentYou could say that about any game engine. Are you suggesting Bevy should try and not optimise performance because perf and fun are not correlated? reply VS1999 6 hours agorootparentprevThe problem is that most of the gameplay code is linear, and people have already gotten good at splitting parallel work across threads. Serious physics engines (see jolt) are already designed to run on another thread and distribute the work across multiple cores. The main part of graphics drivers when using opengl or vulkan run on another thread and the UI you access just passes data to it. Rust's parallelism hasn't proven to be faster than C/C++, let alone less annoying to achieve. reply pcwalton 6 hours agorootparentAmong those who have tried both, I can confidently say that the idea that C/C++ parallelism is as easy to achieve as parallelism in Rust is very much a minority view. There's a reason why nobody tried to parallelize CSS styling in a production browser before Stylo came along. reply VS1999 6 hours agorootparentI'm talking about games specifically. I don't know much about the needs of web browsers. reply pcwalton 6 hours agorootparentI've parallelized emulators in C++ and work on parallel parts of Bevy now, which is probably the closest you're going to get to someone who has worked on parallelizing parts of large game engines in both C++ and Rust. It was far easier in Rust. reply KaiserPro 1 hour agorootparentprev> automatically scale to multiple CPUs We've been promised automatic CPU scaling in programming languages since at least 2001, and I've yet to see any practical version of it. reply raincole 6 hours agorootparentprevIt's a good feature, but still a niche one. It's a bit like choosing Unity only because of DOTS. For a few projects perhaps it make sense. But just a few ones. reply pcwalton 6 hours agorootparentNobody said that every game needs that level of performance. But saying that it's a solution looking for a problem is not true. I'm fully in favor of having Bevy support dynamic languages, as implemented in for example bevy_mod_scripting [1], for projects that don't need that parallel performance. [1]: https://github.com/makspll/bevy_mod_scripting reply vaylian 2 hours agorootparentGood scripting support is probably the way to go for Rust game development anyway in order to achieve high iteration/idea testing velocity. We could have a script engine that memory-manages various in-game objects and the scripts call into Rust functions to do the heavy lifting. Those Rust functions will typically take things by reference from the script engine so that memory-management is mostly a non-issue. reply fleventynine 15 hours agoparentprevI'm a Rust fan (mostly for embedded firmware with minimal deps), but even after 10 years of playing with the language it's not clear to me that advanced GUI or gamedev fits well with the borrow checker. It requires a significant paradigm shift in architecture, and I'm not convinced it's worth making that shift, especially if your application can tolerate a garbage collector (which many games and most UI apps can). reply iknowstuff 14 hours agorootparenthttps://dioxuslabs.com/blog/release-050 Seems promising, very React-esque with little boilerplate reply alimnes 5 hours agorootparentDevelopment speed is many times lower than with Typescript frameworks, while the result is not faster or significantly more stable. Why should anyone choose Dioxus over Sveltekit, Next or Nuxt? I never had an issue with a frontend app that the borrow checker would have catched. Error handling was an issues some years ago but is solved by now when using one of those modern frameworks. (I don't know if Dioxus has error boundaries, though.) Those Rust fullstack frameworks make sense only for people wanting to use Rust, not for people looking for the right tool for the job. reply vacuity 14 hours agorootparentprevI hope Rust does gain mature options for its GUI ecosystem, but the author of the article makes a very good point that in other languages, there would be mature options in use already. \"Seems promising\" is too little, too late. reply iknowstuff 14 hours agorootparentFor sure! I would not write a game in Rust in 2024. reply hi-v-rocknroll 13 hours agorootparentprevAgreed. Multiple languages exist. They can be part of {your, your team's} toolbox for different specific purposes. Some languages are set by other tools or by team members' backgrounds. Popularity also lends itself to greater availability of tools and Q&A forums. In the end, it's a better decision-making process to select what is most likely to be long-term productive for a specific project and team. reply jvanderbot 13 hours agoparentprevThis might be controversial, but \"Safety\" and \"Speed\", in the same ecosystem, are not free. The cost is heavy syntax and heavy cognitive climbs. Why Rust was ever sold as a language for the masses is beyond me. A safe, fast, hard language is something you use for operating systems, aircraft, etc. I adore Rust because it does all the things I remember being told to do in C, but without me remembering to do them: Error codes from all functions, Ownership models, etc. But those are not good reasons for me to use it for anything I wouldn't use C for. reply DarkNova6 15 hours agoparentprevJust a small addition: Godot also has great C# support. It is a real charm to work with. reply leduyquang753 5 hours agorootparentLast time I tried Godot with C# in Visual studio, when I debugged I could not see the console output, and when I ran with the console output I could not debug (the breakpoints weren't hit). A Google search later and turns out it wasn't just me. reply treyd 8 hours agorootparentprevThe godot-rust project crates take a minor amount of adaptation to understand how it exposes the Godot object system in Rust but it's also pretty well developed. reply Quothling 13 hours agoparentprevWe're doing more and more of our back-end work with Rust. The main reason is the performance it provides. It's not just great for our end-users it's also so much cheaper in the modern world where we pay per mileage in the cloud. Part of what we really like about Rust, however, is actually exactly the variable ownership because it makes it very straight forward to enforce and control data-integrity and avoid race conditions. Even for programmers who would struggle to do so in C or C++. I'm not sure whether or not that's even useful in game development. I've never done any form of game development beyond some Chess game I programmed in my first year of CS 30 years ago. But I'm actually really curious as to why you've struggled with variable ownership, because I'd frankly like to improve our on-boarding processes even more for new hires. > my other biggest problem is that they keep painting other non-Rust things as being fundamentally flawed for not being Rust Rust has a cult and it's best not to pay too much attention to it. Don't get me wrong, we're seeing great benefit in not just using Rust over C/C++ but also replacing more and more of our C# and Python services with it, but it's a very immature language and like any other programming language it's still just a tool. If it works for you, use it, if not... Well, use something that does. reply vacuity 14 hours agoparentprevNow I'm wondering how far people could go a hypothetical Rustscript* that transpiles to Rust (or hooks into rustc?), introduces extra features such as reflection, removes lifetimes, and changes the defaults around things like monomorphization. * name intentionally made to make people angry reply suby 1 hour agorootparentIf you're removing lifetimes from the script, I'm not sure how you're then transpiling to Rust, unless you wrap everything with reference counting, at which point you're better off using a language with GC. reply hi-v-rocknroll 13 hours agoparentprevRust ain't Go but anything Go has can be used as an argument that Rust should try to do better in certain areas. ;) Perhaps learn another language like Haskell, Swift, or Kotlin before Rust. Get cargo-bloat, cargo-cache, and cargo-outdated. Setup a memcache server and use sccache to accelerate Rust, C, and C++ compilations. It's not 100% but it's pretty awesome for things compiled at a stable build location. Just like any platform, avoid dependencies wherever possible and use minimal crate features. Some Rust crates have an npm-like problem of dragging in zillions of dependencies. reply nialv7 10 hours agoparentprev> but when was the last time memory safety was actually a big problem in games? Unity uses C# which is garbage collected, Godot uses its own scripting language which makes it nigh impossible to leak memory, Unreal AFAIK has its own tools that makes memory management trivial. So.... Sounds like memory safety is indeed a problem? Otherwise why do so many solutions exist for it? Yeah, Rust definitely is not the only solution, or perhaps not even a good solution to this problem in the context of game development. But let's not pretend the problem itself doesn't exist? reply ciwolsey 10 hours agoparentprevGarbage collection causes performance issues. reply logicprog 10 hours agoparentprevAs I said in my own comment down thread, despite being a huge rust advocate, I sincerely agree with you here. Rust is not a good language for actually writing games, and the fact that it is being sold as such is really detrimental to it in my opinion, because it is holding the ecosystem back. Rust is being pushed as a language for game logic, so people try out and realize it isn't very good at that, and so they just give up on Rust in the game development industry at all and leave, understandably! If Rust were more strategically positioned, it could get a lot farther. Where it should be focusing in the games industry is on game engines, where flexibility and quick iteration and easy prototyping and being able to just reach out and directly touch and control things isn't as important, but where concerns like the clarity and maintainability of the code base, stability of the software, resource ownership and management, and eeking out every ounce of performance all become important, and so the type system and static analysis guarantees of Rust are actually useful. This is where, I'm disappointed to say, I think things like Bevy and Amethyst have severely hurt the Rust game development ecosystem. They aren't really game engines in the traditional sense, they are more like game frameworks like Love2D except written in Rust: they force you to statically link your game code to the engine code, and write your game logic in the same language your engine is written in. This means that game developers who just want to quickly prototype game mechanics and want to be able to iterate on them in order to refine them are forced to use a language that is far too focused on correctness, safety, static verifiability, and concerns like that to actually be usable as a programming language, and worse, it forces them to compile their game logic and the entire engine together and link them together in order to build their actual game and test it, massively increasing the weight of the process and basically ruling out hot reloading or making your game independent of any specific version of the engine, or its license. It puts them between a rock and a hard place, between using some other ecosystem, or using a language that simply unsuitable for a game development. I think the far better solution (one which I plan to very slowly feel out with my embryo engine project, which is born out of my frustration of looking at the existing rust game engines and feeling like they are all kind of lying about what they are) would be to stop with the vaporware and the hype with Bevy and Amethyst and such, and actually build a proper game engine, like they are promising to be but are not, that is its own separate pre-compiled executable that game developers don't even need to mess with at all, that picks up game assets and game code written in a more flexible, dynamic, language that's better for prototyping, and runs them, something like what Unity or Godot or even Gamebryo do. Only then will the rust game development ecosystem take off, because it will no longer be forcing a language that just isn't good for that on to people. reply fire_lake 2 hours agorootparentBut people want to write Rust and a game seems like a fun way to do it. They can already use Godot or Unity with this approach. reply alimnes 5 hours agoparentprev> Rust game development feels like a solution looking for a problem to fix. The same can be said for ordinary CRUD backends. Java, C#, Go and Typescript (Node, Deno or Bun) are all memory safe with good type systems and more than good enough performance. Evangelism around Rust is unfortunately still a thing. A good example is the latest hype in the community because some Google Manager said at a Rust conference that writing Rust is as fast as writing Go. Anyone having done more than a toy program in Rust and Go knows how wrong this statement is. The reasons are given in the article. reply neonsunset 5 hours agorootparentThis is not necessarily a bad thing. Especially given that Rust is an immediate upgrade with no downsides when moving away from C or C++. It is easy to see with people never wanting to go back, which also involves getting companies and products to adopt it as you would otherwise be forced by the market to work with inferior tools. As a counterexample, .NET suffers a lot from the lack of evangelism - big chunk of community that started out back in .net framework days still thinks of it as poorly as people outside the ecosystem because they never bothered to drop old and obsolete tools and targets and give new versions a proper try (as the code is often vastly simplified and performance is vastly better). Other programming languages, not only Rust, also do better at self promotion - take for example Go that managed to convince everyone to put it in the same bucket as Rust (which, personally, I find absolutely insulting as C# is a much closer alternative to Rust both in performance, features and access to low-level bits). reply obdev 2 hours agorootparentI mean, if we are allowed to lie in order to promote Rust, why don't we just smear all the C/C++ code bases in the world as security hazard needed to be sorted out ASAP? Unless we already do... reply neonsunset 2 hours agorootparentI doubt security is the matter everyone is concerned with but rather the quality of tooling and developer experience. It is, of course, difficult to convey to developers who only experienced C and C++ build systems, or Ruby tooling and brittleness, or Python way of managing dependencies, or setting up the packaging when using Java, that fast and easy to use solutions do not come from trade-offs but from just better ways of doing so - using cargo and Rust or dotnet and C# is night and day difference compared to options listed above. I said it here in the past and will say it again: it's not that Rust (or .NET for that matter) are that good, it's a lot of other popular languages and platforms are that bad at one or another aspect (or many at the same time), that make it sufficiently painful to never tolerate a downgrade when you worked with a tool that offers better all-around experience. reply obdev 1 hour agorootparentI value good tooling as much as the next software engineer. We have good IDEs, build systems, package managers in Java and .NET lands; but we also have a decent environment of established, well-maintained libraries and frameworks. Rust is deemed to have good tooling, but the third-party library ecosystem is following the NPM/RubyGems culture with all the fragmented dependencies, plus the added complexity of compile times due to lack of ABI compatibility. Meanwhile, monolithic projects like Tokio also keep strengthening their reign among the small peasant crates. I'm learning Rust, after decades of various languages with garbage collector, and I believe in the language itself and its tooling. But everything else about Rust irks me. reply LarsDu88 15 hours agoprevI've done hobby gamedev in Bevy/Rust, Godot/C#, and Unity C#. It's honestly somewhat baffling to me that folks will choose Rust for gamedev right now. The state of the open sourced tools are just not there yet, especially when compared to Godot, and at the same time these games are running on PC hardware which tends to get faster every year. Also for ECS... one thing I tended to realize is that when developing a game, pigeonholing everything into an ECS can seriously tend to get in the way. A lot of (efficiently written) game code is best handled in an infrequent event-driven way. An ECS backed game engine like Bevy can make big mobs more efficient, but few games will actually leverage this effectively for fun gameplay and at the same time modern PCs are fast as hell. I think about Starcraft from 1998, created when virtually all PCs only had one core, and its 200 unit per faction cap. Blizzard hasn't increased this cap because it doesn't necessarily make the game more fun. Now should a gamedev today, 26 years later, making a 2d isometric game for the PC be worried about performant multithreading???? reply Nuzzerino 13 hours agoparent> I think about Starcraft from 1998, created when virtually all PCs only had one core, and its 200 unit per faction cap. Blizzard hasn't increased this cap because it doesn't necessarily make the game more fun. Ah... Starcraft. It's 200 supply per player (hero units take 0 supply, zerglings are 0.5, and the supply cost goes up to 8 for battlecruisers for example). The limit is enforced when building a unit from a building. Map triggers can grant",
    "originSummary": [
      "An indie developer decided to step away from Rust gamedev due to challenges with the borrow checker, slow prototyping, and GUI development, critiquing the community's focus on technical aspects over practical game creation.",
      "The post delves into the utilization of generational arenas, Entity-Component-System (ECS), and the constraints of GUI libraries in Rust, stressing the importance of balancing code quality, iteration speed, efficient data management, and enhancing compile times in Rust game development.",
      "Despite obstacles like hot reloading, UI structure, and global state integration, the author values Rust's compiler-driven methodology, high performance, and usability for CLI tools and data handling."
    ],
    "commentSummary": [
      "Developers using Rust in game development encounter challenges such as slow compile times, ecosystem limitations, and struggles implementing advanced features like async systems.",
      "Participants discuss the pros and cons of Rust versus other languages like Go and C++, debating its suitability for game engines and systems programming.",
      "The conversation highlights the significance of memory safety, parallelism, and productivity in game development, exploring alternative languages and approaches to enhance efficiency and flexibility in projects."
    ],
    "points": 1219,
    "commentCount": 740,
    "retryCount": 0,
    "time": 1714152836
  },
  {
    "id": 40168519,
    "title": "Free High-Quality CC0 Textures and 3D Models for Commercial Use",
    "originLink": "https://www.sharetextures.com/",
    "originBody": "Home Textures Models Blog About Sign In CC0 Textures and 3D Models by sharetextures for more, please scroll down. Artwork by: Olga Antonenko High Quality & Quantity “We provide high-quality 3D models and 1000+ textures that you can use on your projects.“ Latest Atlases 37 atlases and increasing every day Latest Textures 1502 textures and increasing every day Latest Models 184 models and increasing every day Completely Free “All of our content is copyright-free. It means, you can use them anywhere you want which includes commercial projects too.” 0 patrons 0 assets $0 per month a month agoglenneroo 6 days agoGanicuus 7 days agoAdar Shalev 7 days agoAdham Taha 9 days agoKristian 9 days agommd yy 12 days agoBarış Manav 15 days agoMartynas Marozas 16 days agoJJ JJ 17 days agoPeter Moonen 19 days agoDaiki Sugioka 19 days agoKoloredKaboose 19 days agoJames Byerly 20 days agoSara, Sophie COUIGNOUX 23 days agoWilliam VANEGAS ROJAS 23 days agoNick Petro 23 days agodanjely emilio vargas almanzar 24 days agoWhat A World News 25 days agoMoThe Person a month agoLaura a month agoLennart (ambientCG) a month agoPoly Haven a month agoBotak Plotak a month agoJeff Nyte a month agoY-Phil a month agoglenneroo Become a Patron Supported By Created With ofPatrons Keep in touch:) Home Assets Textures 3D Models Blog License",
    "commentLink": "https://news.ycombinator.com/item?id=40168519",
    "commentBody": "I'm creating PBR Textures and 3D models since 2018 and sharing them for free (sharetextures.com)406 points by tolgaarslan 21 hours agohidepastfavorite86 comments catapart 20 hours agoI've had my eye on this for a couple of months now because you guys are doing exactly what I'm doing when it comes to modular asset/utility development: putting it in the public domain, where it belongs. I'm all for making money on specialization or convenience, but I really can't find it in myself to build a perfectly useful something and then only use it for myself unless someone else can pay for it. As long as it's fully modular, I just have to give it away for free. Some things - no matter how much work they took to make - are just not worth paying for. Or, at the very least, I would never pay for them. So rather than just keep everything to myself so I can use it the one time, I can't see any reason not to just make it entirely available to the public. And, good god, I would be so embarrassed to see my name in the credits of something with a label like \"provided image formatter\", or something. So attribution is something that I really couldn't care less about. It's always a nice gesture, but some things just aren't worth attributing. All of which is to say: I love your interest in releasing these things to the public domain, and I'm very eager to join you! I've worked in games since before the original THQ went under, and have been using Unreal for the last 6 years or so. I'd be happy to get into whatever process you guys are using, and provide assets to whatever specifications you enforce. I find the most important thing about assets is that they be uniform (so large-scale changes can all happen in the same way, per asset). So I'm happy to conform, just so I can make assets that will act well in-editor. Unfortunately, I'm mired in some side-project dev work, and won't be able to work on games, or game assets, probably for the rest of the year. So I've got to put off helping until I can clear my plate some. But I have bookmarked you guys, and will follow up to see if you have any interest in additional help with this kind of work, as soon as I can! Aside from all that, thanks again for providing this. It really is a useful and altruistic endeavor! reply tolgaarslan 16 hours agoparentCome to our discord and say hi. I'll love to discuss the idea reply hwbunny 17 hours agoparentprevnext [2 more] [flagged] squigz 15 hours agorootparentHow does this ruin anything? reply aarongeisler 19 hours agoprevThis is great! I added the link to my list of free game dev resources: https://github.com/aaron9000/c-game-resources Is there a tool you used to create these materials that you would recommend? reply tolgaarslan 16 hours agoparentThanks. Substance Designer, Substance Painter, Metashape, Blender, Marmoset. We are mainly using these softwares. For the atlases we are using Details Capture from VFX Grace reply AvieDeckard 13 hours agorootparentWould you be willing to consider sharing the substance source files on your site as well? I've been learning material design off and on for a while now and the ability to learn from and modify them would be cool, but obviously not required if that's too much effort or just not something you'd want to share. Either way, thanks a lot for the resource, stuff like this always gets me excited! reply tolgaarslan 4 hours agorootparentI'm not planning to share them because they are so complicated and need to be edited and standardized one by one. I'm focused on creating scanned assets right now. reply AvieDeckard 4 hours agorootparentThanks for the response. Sounds great! Again, thanks so much for sharing these resources, it's always good to have more free assets out there. reply aarongeisler 14 hours agorootparentprevThank you, I will check these out - love what you are doing. Please keep it up. reply lurkingmba 14 hours agoprevI guess \"PBR\" means something different to you. reply trillic 13 hours agoparentProfessional Bull Rider? reply zamadatix 13 hours agoparentprevWait, we're not all on HN to discuss Policy Based Routing? reply junon 14 hours agoparentprevPabst Blue Ribbon? Yeah. Been a mindfuck ever since I started 3D stuff. reply wnoise 12 hours agoparentprevPeanut Butter and Radishes? reply noworld 14 hours agoparentprevPatrol Boat, River reply jandrese 18 hours agoprevBrowsing the models is a bit of a trip. Potato, chair, onion, lamp, medieval torture device, banana, couch... reply anticorporate 19 hours agoprevLove this! One nit. The homepage says “All of our content is copyright-free. It means, you can use them anywhere you want which includes commercial projects too.” That's not how copyright works. All of these works are copyright. That copyright is what allows the owner of the copyright to place them under a CC0 license. What CC0 really means is \"the copyright holder has waived the rights they have under copyright by granting you a non-exclusive license to use this work pretty much however you choose.\" reply dahart 19 hours agoparentCC0 is “a tool for relinquishing copyright and releasing material into the public domain”, and the official icon for it says “public domain” [1] mainly created because actual public domain is problematic across different countries, I believe. Actual public domain really is ‘copyright-free’, and it makes sense to describe CC0 as making things ‘copyright-free’ to a general audience that may not be familiar with the subtle intricacies of copyright law. It is true that only the original copyright holder has the authority to release their works into the public domain, but once they’ve done that, copyrights are no longer held, and the work is no longer subject to protection under copyright law. https://en.wikipedia.org/wiki/Creative_Commons_license#Zero_... https://creativecommons.org/public-domain/cc0/ reply anticorporate 18 hours agorootparentRight... CC0 exists because there is no such thing as relinquishing copyright in many countries, including the United States. It's a license that allows the work to be used as if copyright were relinquished. If I create a work and license it under CC0, I still own the copyright, I've just given everyone a license to use the work in such a way that I cannot enforce most or all of the rights associated with my ownership of that copyright. reply dahart 17 hours agorootparentCC0 a license that relinquishes copyrights. You’re right that it’s a license and not public domain, but otherwise making a distinction without a difference. The stated explicit intent, and the rights granted by the license, are to provide a version of public domain that is unambiguous and works globally. Your terminology is a bit funny when you say “I still own the copyright” or “all of these works are copyright”. Works aren’t copyright, works are protected by copyrights that authors have… unless the author waives those rights. The copyright one has by default is the exclusive right to copy and distribute the work. Once you give that away, either via license or public domain attribution, it’s irrevocable and permanent, and there’s nothing of value in the idea that you’re still the copyright holder, since there are no longer any copy rights retained nor copyright protection under any laws. In short, it’s perfectly fine to call CC0 attributed works “copyright-free” because that’s what the license actually does, it “waives” all copyrights and “related rights”, and allows the public to copy at will, forever. BTW I don’t think it’s true to say that there’s no such thing as relinquishing copy rights in many countries, that’s too strong of a claim. It is true to say there’s no such thing as public domain, but copy rights (or “related rights”) can be transferred and/or waived pretty much everywhere. reply kubanczyk 16 hours agorootparent\"To the greatest extent permitted by, but not in contravention of, applicable law, Affirmer hereby overtly, fully, permanently, irrevocably and unconditionally waives, abandons, and surrenders all of Affirmer's Copyright and Related Rights [...]\" It being \"to the greatest extent permitted by applicable law\" I think you are imprecise here: > there’s nothing of value in the idea that you’re still the copyright holder, since there are no longer any copy rights retained nor copyright protection under any laws reply starkrights 17 hours agorootparentprevAnd their point is that saying “copyright free” will probably be more immediately understandable to people who don’t know that and don’t want to read a small comment about copyright ‘intricacies’ (even if it’s not that intricate) By saying copyright free, more people who need freely-licensed works like this are likely to use it instead of being warded off by “this does has a license, it’s copyrighted, but it’s actually free because of the license” Colloquially, it’s the same thing. reply regularfry 19 hours agoparentprevThis does depend on the jurisdiction. In some legal jurisdictions the effect of putting something in the public domain is to assert that no copyright exists in the work. In other jurisdictions that's not legally possible. reply dahart 17 hours agorootparentExactly. This is why CC0 is was created- to make public domain available to jurisdictions that don’t already have it. reply mminer237 15 hours agoparentprevIn the US you can disclaim your copyright and place works in the public domain. It's only countries with strict authors rights like Germany that that isn't possible. reply squigz 15 hours agoparentprevWhat would you replace that line with? reply qarl 17 hours agoparentprevI sorta think that for a large banner appealing to a naive audience, \"copyright free\" is more appropriate than \"the copyright holder has waived the rights they have under copyright by granting you a non-exclusive license to use this work pretty much however you choose.\" But hey, maybe there are more IP lawyers in the free texture community than I realize. reply r1chardnl 20 hours agoprevWhat are atlases used for? https://www.sharetextures.com/atlases reply whizzter 20 hours agoparentBefore bindless became a thing (and it's not yet standard with for example WebGL) then doing extra render calls just to change active textures could be an quite expensive operation. Thus a texture atlas is useful to be able to batch a lot of geometry into the same call (Also useful for 2d-like animations). https://en.wikipedia.org/wiki/Texture_atlas reply tolgaarslan 20 hours agoparentprevAtlases are a set of small elements grouped together in one asset. It's usually used for trees and terrain objects. reply nightowl_games 20 hours agoprevGreat timing. I'm currently playing around with 3D in Godot 3. We've made a bunch of 2D games, and have a huge system that we have no real incentive to port to Godot 4, so I'm seeing how far I can push Godot 3. Looking forward to trying some of these. reply Etherlord87 1 hour agoparentBeware those assets are high poly and are better suited for rendering work than for realtime rendering in games: https://i.imgur.com/KX2FVzv.jpeg reply tolgaarslan 20 hours agoparentprevI'm also working on Unreal Engine and creating my assets Unreal Engine Material versions on that. I'm hearing too much about Godot and I hope, I can find a way to make a game using my own assets. reply goodcjw2 17 hours agoprevGreat work to publish your work into public domain. I saw a couple more people in the thread trying to do the same thing. Just curious: is there a reason to create your own site for this? Instead of listing on things like Sketchfab? They seem to support public domain for a long time already: https://sketchfab.com/blogs/community/sketchfab-launches-pub... reply tolgaarslan 16 hours agoparentMy wife and I have been architects since 2013, and back in 2018, it was almost impossible to find high-quality free materials. Polyhaven (formerly Texture Haven) and AmbientCG (CC0Textures) mainly focused on more natural assets. We decided to give it a try, and within just two weeks, we reached 100 patrons. So, ShareTextures is funded by the patrons, and as a result of this support, we continue creating. We have our website because we want to provide some advantages to our patrons. Additionally, it helps us promote our supporters, partners, etc. reply lofaszvanitt 16 hours agorootparentDon't you have a feeling that your actions will break down the market and kills the revenue for those who depend on this kind of income? I mean, you people are architects, not some digital beggars. You should ask a minimal price for it, but should not offer it for free, as others have also pointed it out. reply tolgaarslan 16 hours agorootparentWe understand your concern, but we believe in the power of sharing and accessibility. We aim to support the community by offering free materials and 3D models. Our creations are simply tools to aid talented artists in creating their main products.There are unlimited options to create a digital asset. (style, design, year, condition, etc.) We can't create all of them. reply ugh123 5 hours agorootparentprevMuch of open source software is given away free, as in beer. Would you rather that be paid for and possibly also closed source? reply tolgaarslan 1 hour agorootparentYes, much open-source software is free, enabling more people to use and contribute to it. Paid options might offer extra perks. Personally, I appreciate the accessibility and collaborative spirit of open-source software. It fosters innovation and community involvement, which I value. For example, if you check ArtStation's or Unreal Engine's Learning sections, you will see free tutorials from real experts. This exemplifies the power of the internet. reply baobabKoodaa 16 hours agorootparentprevYes, how dare they give out their work for free. The audacity of these people! reply carlosjobim 15 hours agorootparentArchitects get paid for making models, so why is it not fathomable for the poster that they should pay for the textures they use? Or, why do some workers deserve to get paid and others not? Edit: And reading the reply from the architects, you can almost smell the entitlement. reply baobabKoodaa 12 hours agorootparentI don't get it. How is this any different from programmers who create open source software that they release for free? reply carlosjobim 11 hours agorootparentThere is no difference. Programmers keep making open source software for free, and people making money on that software keep acting entitled to it. reply squigz 2 hours agorootparentprevI'm confused. Who do you think is not being paid here? reply baobabKoodaa 16 hours agoparentprevThis is like \"old web\" versus \"platforms\". Me personally, I like the old web. Websites that people make and put stuff on. Platforms are generally great... until they aren't. Your own website will be great as long as you want. reply softfalcon 17 hours agoparentprevI think it's so they can provide a quality web experience curated to facilitate their Patreon goals. Their work is free, but they likely want to ensure folks know \"who\" is making all these assets and that they have a Patreon so you can help support their efforts. That seems fair to me considering the generosity at play here. reply rbanffy 11 hours agoprevThis is amazing. Back in the 1990's, when I worked in 3D, a resource like this would have been an immense force multiplier. Back then we had BBSs to share models, but nothing like this. reply folli 14 hours agoprevSlightly (?) on topic: I'm looking for some collaborators for an opensource project using Babylon.js (where PBR textures could also be helpful): https://github.com/r-follador/CubeTrek_Babylon It's a GPS track visualization web app for outdoor sports; any help would be appreciated (see here for the hosted app: https://cubetrek.com) reply WickedSmoke 18 hours agoprevThe website is completely broken with Firefox. Search results do not appear on the screen - you must scroll down to see results. Clicking on the asset images does nothing so they cannot be downloaded. When using Chrome the layout and interaction works, but even then browsing is a poor experience as only four items can be seen at once so lots of scrolling is needed. Sheesh... modern web design is a disaster. This sort of collection could really use static pages with labels as links and small thumbnails showing at least 50 items per page. [Edit] I recommend https://ambientcg.com/ instead as that site is much more responsive. reply jandrese 16 hours agoparentOther than being a bit slow to load and lacking a \"please wait, still loading...\" message it works fine on Firefox 125.0.2. reply tolgaarslan 15 hours agoparentprevSorry to hear that. We don't have a developer on the team. Lennart (from AmbientCG) has great skills in development, and he perfected their website. We're hiring freelancers to update our website, but with limited resources. Still, we're always looking for ways to improve our website for our users. reply jayess 17 hours agoparentprevWorks fine for me on firefox 125. reply cptskippy 17 hours agorootparentWorks on 124.0.2 as well. reply WickedSmoke 17 hours agorootparentI'm running 120 (released five months ago). If people wrote web pages rather than web programs these sort of issues could be avoided. reply at_ 21 hours agoprevFantastic resource! I actually stumbled across it organically a few months ago, and couldn't believe my luck. There's really nothing else out there as high quality that's CC0. So thank you. reply fodi 2 hours agoparentQuaternius[0] and Kenney[1] also has lovely low-poly 3D models, all under CC0. [0] https://quaternius.com/ [1] https://kenney.nl/assets/category:3D reply r1chardnl 20 hours agoparentprevYou could try https://polyhaven.com/ reply tolgaarslan 20 hours agoparentprevYou can also use 3dassets.one It let's you search 3D assets based on the license or creator. reply waynecochran 14 hours agoprevIs there a high demand for models of gallows, electric chairs, and head stocks? I now know where to go if need to create a model of a torture chamber with some added fruits and vegetables. reply Tomte 21 hours agoprevHow is that done? What more than a camera and a computer do you need to create those textures and associated maps? reply tolgaarslan 21 hours agoparentI've been creating for 6 years and adding new assets every couple of days. There are several technic to make them. You need a computer indeed but some creation methods don't require a camera. You can generate textures procedurally using software. reply Tomte 21 hours agorootparentLet's say I want to \"texturize\" an existing brick wall of a building. I make high-quality photos, but then what? How do I create bump maps and so on? Do I need a stereoscopic setup? Do I need polarizing filters or something like that? What software is used? reply tolgaarslan 20 hours agorootparentIF you want to make it high-quality, there is a technic called photogrammetry. You need to take hundreds of photos under same light, without shadow or reflection. Then you create the 3D model of the facade using photogrammetry software like metashape or realitycapture. After you have the 3D model, you need to transfer 3D data to 2D texture. It's called baking. After baking is done you endup with basecolor(diffuse/color), height(displacement), normal and ao(ambient occlusion) maps. If you are looking for a basic way, just take one clean photo(without visible shadows or reflections) There is method called Bitmap to material. Basically, you grayscale your image and using that grayscale data to create other maps like bump, roughness height etc. If you've experience in that area you can make them in Photoshop. Adobe Substance Sample(Substance Alchemist) has great abilities to generate using that method + AI. To clear reflections you need circular polarization filters. You can also use cross-polarization method to clear all unwanted reflections. But It require addinational light source and linear polarization filter. reply Tomte 19 hours agorootparentThank you, these are great keywords to google. reply Culonavirus 18 hours agoparentprevSubstance Designer is a defacto industry standard in procedural texture creation. As of 5.4 UE also has a texture graph editor. They're all node based editors combining a bunch of PCG techniques and patterns to produce textures in parametric, non-destructive way. reply rcarmo 19 hours agoprevLovely. Just about any sort of texture map I could think of, and great variety. I especially like the glassy/stone textures. Any Blender add-ons that integrate with the site? reply tolgaarslan 16 hours agoparentYou can find our assets on BlenderKit. reply FrostKiwi 19 hours agoprevSeem like this project's goals and the goals of polyhaven.com are aligned. Is a cooperation possible? Each other's libraries could totally benefit from more 3D Models under CC0 reply tolgaarslan 15 hours agoparentPolyhaven creates textures using only photogrammetry(scan). It's doubling their quality also, equipment, and human resources. I'm adding some scanned textures too but it's not my main focus. reply jppope 17 hours agoprevthe problem with acronyms... his PBR is a completely different PBR than I was thinking... reply tmaly 18 hours agoprevI love how it is supported by patrons rather than charging everyone. reply Joel_Mckay 17 hours agoprevI used to wonder why studios had their own foley sound stage, musicians, photography and concept artists... Then one takes a trip down the deep rabbit hole of modern copyright liabilities. Finds a web marketplace filled with potential copyright submarine lawsuits, DMCA take-downs waiting to happen (youtube contentID will eventually flag the work of that cheap asset pack off Unity marketplace), and draconian EULA. People are often unaware resale/transfer of many production Asset licenses is prohibited even if you buy the old CD/DVD media sets, \"royalty free\" can also mean hidden per-user seat-fees and can't cover public performances of sheet-music that must be cleared with local performing arts organizations to make sure the original rights holders get paid. What this means, is even if you pay for something 3rd party... you don't own the primary rights to the content. And there is usually a bunch of per-file legal conditions that are impossible to keep track of in a production environment. Ultimately, even CC0 may not necessarily protect you if the source makes the wrong judgement call on the original primary work. There is a similar argument about ML generated works as algorithms cannot own IP, and therefore cannot be transferred or licensed properly... This is one of the primary reasons I financially support OSS related projects that do procedural textures/shaders that can be baked in 4k later. It really sucks the fun out of building anything aesthetically beautiful for cheap... as your budget ends up mired in licensing fees even before 1 line of code is written.. I supported Polyhaven as they curated a truly unrestricted library of CC0 content, and offer it without any hidden fees/subscription BS for everyone. Remember to have fun, =) reply hahamrfunnyguy 18 hours agoprevWhat is PBR? Not Pabst Blue Ribbon! Here's a brief description from Adobe: Physically based rendering (PBR), sometimes known as physically based shading (PBS), is a method of shading and rendering that provides a more accurate representation of how light interacts with material properties. Depending on which aspect of the 3D modeling workflow is being discussed, PBS is usually specific to shading concepts while PBR refers to rendering and lighting. Both terms describe the process of representing assets from a physically accurate standpoint. reply weinzierl 18 hours agoparentThat is true but not whole story. I believe when people talk about PBR Textures they most often use PBR synonymously to \"Principled Shader\", \"BSDF Shader\" or \"Disney Shader\" (all meaning roughly the same thing). reply weinzierl 3 hours agorootparentHere is my whole comment. Somehow the last part I posted in an edit magically disappeared: That is true but not whole story. I believe when people talk about PBR Textures they most often use PBR synonymously to \"Principled Shader\", \"BSDF Shader\" or \"Disney Shader\" (all meaning roughly the same thing). When it comes to 3D data, exchanging geometry has been possible for a long time. Essentially and most of the time we just use triangles as the lowest common denominator. Most of the difficulties are more or less accidental complexity because of different formats. When it comes to textures this was not possible for a long time, because every renderer used its own algorithm and they all had different parameters. There was no lowest common denominator. When Disney invented BSDF it allowed the exchange of realistic materials for the first time. The \"format\" won and is what is sold or given away as \"PBR-Textures\".j reply cubefox 17 hours agoparentprevPBR textures don't include just the usual colored images, but also other surface properties that influence how specific surfaces interact with light. Like roughness, glossiness, elevation (surface normal vectors), whether the surface is a metal, whether it has specular highlights, whether it appears fuzzy, whether it is partly translucent (like skin) and so on. The combination of such textures is called a (PBR) \"material\". It's called \"physically based\" because there exist simple physics formulas for these properties, so it luckily isn't necessary to simulate the all microphysical details that cause them in real objects. Similar to how one can describe a gas with a few parameters from thermodynamics without considering the molecular details that explain those phenomena. In old 3D renders everything looked like plastic because they had only very primitive surface properties. reply DannyPage 19 hours agoprevIs there any software meant for casual users for displaying and walking around these objects? I'd love to have some sort of VR room with the simplicity of \"The Sims\" where I could add these, plus perhaps some connections to the rest of the internet. A fake TV that displays the frontpage of HN or a 24-hour Twitch feed. A radio object that can play Spotify streams, with 3D audio enabled. reply squigz 13 hours agoparentI've considered something like this for years. A simple, casual, social space to just decorate, hang out, do basic stuff like what you outline. I think there's a lot of potential there! Not so much money though, I imagine... reply sooperserieous 13 hours agorootparentSecond Life is still a thing, or OpenSimulator if you really do want to do it all yourself :) FWIW, SL is finalizing the addition of PBR textures now and may go further WRT scene imports. reply squigz 13 hours agorootparentI know one shouldn't judge an entire community like this, but every interaction I've had with SL has been... strange, to put it lightly. Still, I have a lot of respect for those devs and their commitment to the game OpenSimulator looks like a cool idea too! reply catapart 18 hours agoparentprevDon't know of anything like this, but this is a really good idea! It would be sweet to have a scene and renderer with well-known properties (which could be adjusted to account for style; realistic/toon/dynamic lighting/baked lighting, etc) which could be dropped in to a webpage so that users could mess with it. That way, developers could independently include it and write their own ways of injecting the assets. The end result being a web widget that is a 3D environment with whatever the developer wants to put in it (with some defaults like \"sunny outside\", \"sun room\", \"basement\", \"cave\", etc), and drag-and-drop asset selection for users to view the showcased assets in the scene. Could even double as a code-guide, based on your idea of including functioning TV and Radio assets (an example of how to include this functionality into the assets). Overall, just a really great idea! reply tolgaarslan 3 hours agoparentprevThat Virtual Room can easily created with Unreal Engine. I believe you can find some ready environments like this on Unreal Engine Marketplace. reply hwbunny 17 hours agoprev [5 more] [flagged] ddsgtuuuu 17 hours agoparent [–] Somebody's pissed off that their own crappy attempt at selling assets failed. Get good noob. reply hwbunny 15 hours agorootparent [–] Nah, I have no stakes in this market. Just the recent I do work for free in exchange for clicks movement baffles me. People are idiots, ruining complete markets because some bored individuals dilute the market with their free offerings. reply tolgaarslan 15 hours agorootparent [–] Even Adobe has a free platform. That's all I wanted to say reply hwbunny 15 hours agorootparent [–] Yeah, they also killing the market. Once they established themselves they just nuke it, so you will need to look out for another ladder to climb higher. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "sharetextures' CC0 Textures and 3D Models offer copyright-free resources including 184 models and 1502 textures for commercial projects.",
      "The platform is sustained by patrons and constantly expands its collection with frequent additions.",
      "Access over 37 atlases on the website for diverse project needs."
    ],
    "commentSummary": [
      "Tolgaarslan has been sharing PBR textures and 3D models for free on sharetextures.com since 2018, offering software tool recommendations and seeking collaborations.",
      "Users engage in conversations about software tools, copyright, licensing, copyright-free works, and public domain materials, as well as optimizing texture atlases and experiences with game development engines.",
      "ShareTextures, initiated by a group of architects to provide free materials, triggers a debate on whether creators should receive payment, while users share feedback on the site and suggest resources for 3D models, exploring topics like using photogrammetry, node-based editors, and PBR textures in 3D modeling."
    ],
    "points": 406,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1714134195
  },
  {
    "id": 40171183,
    "title": "Bun Introduces Efficient Crash Reporting in Zig and C++",
    "originLink": "https://bun.sh/blog/bun-report-is-buns-new-crash-reporter",
    "originBody": "At the time of writing, Bun has over 2,600 open GitHub issues. We love having users and feedback, but some issues are really hard for us to reproduce and debug. Apps and SaaS products get to use wonderful crash reporting services like Sentry, but for CLI tooling like Bun, uploading core dumps has privacy, performance, and executable size tradeoffs that are harder to justify. That's why in Bun v1.1.5, I wrote a compact new format for Zig and C++ crash reports. The crash report fits in a ~150 byte URL containing zero personal information. Why not just use the OS crash reporter? Some operating systems like macOS have built-in crash reporters, but that usually means shipping debug symbols with the application. For Linux, these debug symbols are around 30 MB and macOS around 9 MB. du -h ./bun 60M ./bun llvm-strip bun du -h ./bun 51M ./bun And on Windows, the .pdb file is over 250 MB (gi bun.pdb).Length / 1mb 252.44921875 30 MB - 250 MB is a huge amount of bloat to add to every install of Bun. But without debug symbols, crashes are pretty limited. And with Address space layout randomization in the mix, all of the function addresses are made useless. uh-oh: reached unreachable code bun will crash now 😭😭😭 ----- bun meta ----- Bun v1.1.0 (5903a614) Windows x64 AutoCommand: Builtins: \"bun:main\" Elapsed: 27msUser: 0msSys: 0ms RSS: 91.69MBPeak: 91.69MBCommit: 0.14GBFaults: 22579 ----- bun meta ----- Search GitHub issues https://bun.sh/issues or join in #windows channel in https://bun.sh/discord thread 104348 panic: reached unreachable code ???:?:?: 0x7ff62a629f17 in ??? (bun.exe) ???:?:?: 0x7ff62a907a83 in ??? (bun.exe) ???:?:?: 0x7ff62a61f392 in ??? (bun.exe) ???:?:?: 0x7ff62ade7ff1 in ??? (bun.exe) ???:?:?: 0x7ff62ab2193c in ??? (bun.exe) ???:?:?: 0x7ff62ab21166 in ??? (bun.exe) ???:?:?: 0x7ff62cd3ddeb in ??? (bun.exe) ???:?:?: 0x7ff62b7a4bb6 in ??? (bun.exe) ???:?:?: 0x7ff62b7a33bd in ??? (bun.exe) ???:?:?: 0x1bab9ca115d in ??? (???) ???:?:?: 0x1bab9ca111f in ??? (???) The new crash reporter In Bun v1.1.5, when a crash or panic occurs, Bun prints a message like this: Bun v1.1.5 (0989f1a) Windows x64 Args: \"C:\\Users\\dave\\.bun\\bin\\bun.exe\", \".\\crash.js\" Builtins: \"bun:main\" Elapsed: 40msUser: 15msSys: 15ms RSS: 92.80MBPeak: 92.80MBCommit: 0.14GBFaults: 22857 panic(main thread): Internal assertion failure oh no: Bun has crashed. This indicates a bug in Bun, not your code. To send a redacted crash report to Bun's team, please file a GitHub issue using the link below: https://bun.report/1.1.5/wa10989f1aAAg6xyL+rqoIwzn0F+oqC0v5R+52pGkr6Om7h+Oy3voK+9qoKA0eNrzzCtJLcpLzFFILC5OLSrJzM9TSEvMzCktSgUAiSkKPg This bun.report link, when clicked, redirects to open a pre-filled GitHub issue form, with the remapped stack trace encoded in the URL. Making addresses useful The function addresses are pointers in memory to where the application code is loaded, which includes a randomized offset for security reasons. This means if we try and demangle these, we get nothing. llvm-symbolizer --exe ./bun.pdb 0x7ff62a629f17 0x7ff62a907a83 ?? ??:0:0 The trick is to simply subtract the address from the base address of the binary. pub fn getRelativeAddress(address: usize) ?usize { const module = getModuleFromAddress(address) orelse { // Could not resolve address! This can be hit for some // Windows internals, as well as JIT'd JavaScript. return null; }; return address - module.base_address; } In reality, this function is a lot more complicated, as there are different APIs for each platform. Note – What I referred to as a \"module\" above is only the case for Windows. It called an \"image\" on macOS, and a \"shared object\" on Linux. They all refer to the same concept of a loaded library or executable in memory. For simplicity, I'll continue to refer to them as \"modules\". Windows: Call GetModuleHandleExW with the GET_MODULE_HANDLE_EX_FLAG_FROM_ADDRESS flag. The base address is the pointer of the module. Linux: Use dl_iterate_phdr to iterate over the loaded modules, once you find one that the raw address is contained in, .dlpi_addr on the dl_phdr_info struct will be the base address. macOS: The functions _dyld_image_count and _dyld_get_image_header can be used to iterate over modules, and then _dyld_get_image_vmaddr_slide gets the ASLR slide. The resulting address still includes an offset to the image (for Bun it is 0x100000000, can list these in lldb with image list). To encode a shorter URL, this offset is removed, but it must be re-added before remapping or else llvm-symbolizer will fail. For Linux and MacOS, the first module refers to the main application binary. On Windows, you can compare the module's name to peb.ProcessParameters.ImagePathName to determine if it is the main binary. Normally, once the module and relative address are resolved, the application will immediately open debug symbols and demangle the function. To avoid the cost of downloading and parsing debug symbols, let's offload demangling to a server. This server could cache all of the debug symbols, and demangle stack traces within seconds. At the same time, it can serve as the link to open a new GitHub issue. bun.report's URL Structure Let's take another look at this URL, and break down how it is been encoded: Platform: A single character indicating the platform. w is for x86_64 Windows, M is for aarch64 macOS, and so on. Subcommand: A single character indicating the subcommand, such as bun test, bun install, or bun run. Commit SHA: The commit SHA of the current version of Bun. This is used to fetch debug symbols later. Feature Flags: Indicators for what APIs and features were used before Bun crashed. Stack Trace Addresses: The addresses calculated earlier. Crash Type: A single character indicating the type of crash. Crash Message: The message from the crash, the format of this depending on the type. Note – The version number in the URL is actually just for show. This is so that given just the info above, one can figure out a lot about the crash by hand. For example, you can quickly identify a windows crash by the w platform identifier. Less trivially, you could identify a segmentation fault by looking for A2 near the end of the string. VLQs are fun To keep the URL reasonably short, the stack trace addresses are encoded using base64 Variable Length Quantity numbers. This allows small numbers to be encoded with less characters, while still being able to encode large numbers. This is the same technique used in JavaScript source maps for storing line numbers. The transformation looks like this. Notice how the VLQ encodes smaller addresses as smaller numbers. The server can decode these back into relative addresses, download debug symbols using the commit hash and platform, and use llvm-symbolizer to demangle the function names. It now becomes plainly obvious what happened: There is a tripped assertion in dirInfoCachedMaybeLog, which came from part of the module resolver code on Windows. What are \"Features\" The URL also encodes a 64-bit integer, where each bit corresponds to if a certain feature in Bun was used. These flags give a hint as to what APIs and systems could have led the crash. For example, the dotenv feature is set when any .env file is automatically loaded, fetch is set when fetch() is used, and so on. (Full list) Zig's compile-time metaprogramming makes creating this bitfield easy. We already had a container of global variables for tracking features. pub const Features = struct { pub var bunfig: usize = 0; pub var http_server: usize = 0; pub var shell: usize = 0; pub var spawn: usize = 0; pub var macros: usize = 0; // ... and so on }; And inside various APIs, we would increment these numbers to mark usage of a feature. For encoding these into a single u64 integer, we can use std.meta to iterate over the list of features and create a list. pub const feature_list = brk: { const decls = std.meta.declarations(Features); var names: [decls.len][:0]const u8 = undefined; var i = 0; for (decls) |decl| { if (@TypeOf(@field(Features, decl.name)) == usize) { names[i] = decl.name; i += 1; } } const names_const = names[0..i].*; break :brk names_const; }; Then, a packed struct can be created dynamically derivied to use one bit per feature. This structure functions like an integer, but interacts like a struct. // note: some fields omitted for brevity pub const PackedFeatures = @Type(.{ .Struct = .{ .layout = .@\"packed\", .backing_integer = u64, .fields = brk: { var fields: [64]StructField = undefined; for (feature_list, 0..) |name, i| { fields[i] = .{ .name = name, .type = bool }; } fields[feature_list.len] = .{ .name = \"__padding\", .type = @Type(.{ .Int = .{ .bits = 64 - feature_list.len } }), }; break :brk fields[0..feature_list.len + 1]; }, }, }); And finally, when Bun crashes, the bitfield can be constructed very trivially using inline for, a way to iterate over something at compile time, but perform the inner contents at runtime. pub fn packedFeatures() PackedFeatures { var bits = PackedFeatures{}; inline for (feature_list) |name| { if (@field(Features, name) > 0) { @field(bits, name) = true; } } return bits; } Now, adding a new feature to the original struct Features will properly handle it in the crash reporter, without needing to repeat ourselves. Doing this sort of thing is possible with C or Rust via macros, but I feel like it's so much simpler and readable with Zig comptime. How does this compare to a core dump? Core dumps have a lot more information, but they are massive, need debug symbols to be useful, and include lots of potentially sensitive or confidential information. We wanted to avoid the possibility of sending any JavaScript/TypeScript source code, environment variables, or other sensitive information in the reports. This is why we only send the Zig/C++ stack trace and a few other details. Instead of sending everything by default, this approach sends only what we (probably) need to diagnose the issue. If we need more information, we can ask the user to provide it, but this is so much better than nothingness of a bunch of unmapped addresses we had before. Demo To put it all together, I wrote a small webapp that lets you test out the crash reporter, which is available at the homepage, bun.report. It is also where you end up if you append /view to the end of any crash report URL. Bun is hiring in San Francisco If you're interested in working on projects like this, we're hiring engineers in San Francisco! We're looking for systems engineers to help build the future of JavaScript. Apply here",
    "commentLink": "https://news.ycombinator.com/item?id=40171183",
    "commentBody": "Bun's New Crash Reporter (bun.sh)299 points by zackoverflow 17 hours agohidepastfavorite69 comments LiamPowell 15 hours agoSo the argument for using this over a regular stack trace is that they don't have to ship megabytes of debug symbols. However they have seemingly just ignored the better option of only including function names in the debug table, which is obviously a much nicer option than having to use a web service to view you stack trace. This isn't just a theoretical solution either, it's already implemented in LLVM: https://clang.llvm.org/docs/UsersManual.html#cmdoption-gline... reply Jarred 17 minutes agoparent> the argument for using this over a regular stack trace is that they don't have to ship megabytes of debug symbols No, it’s because almost nobody has enough patience to upload a crash report for a GitHub issue. It has to be easy. Making it a URL that autofills the form with almost everything we need makes it easy. The size matters too, we didn’t want it to have downsides for our users, but the important thing is making this whole process really easy for the user so that enough developers actually upload crash reports. reply LewisJEllis 12 hours agoparentprev\"they have seemingly just ignored the better option...obviously much nicer\" This comes off a bit presumptuous. I would assume that they are aware this is a possibility. \"having to use a web service to view you stack trace\" This is just not a downside that matters for this usage scenario. It's almost the same story as minifying your frontend JS bundle, uploading source maps to Sentry, then using Sentry to view an unminified stack trace from a user's browser. The user was never going to view that stack trace anyway, and I am not bothered by having to use Sentry to view it - I never would have seen it at all otherwise. reply AnthonyMouse 7 hours agorootparent> The user was never going to view that stack trace anyway Speak for yourself. The ability to understand and affect what's going on in your own user agent is important not just to users having control over their own devices, it changes the social fabric. You can't get interested in how something works because to you it's an opaque blob whose priests have declared you unworthy. Curiosity suppressed. That's bad for the kids. reply raziel2p 1 hour agorootparentif you want that you can always just run the application with debug symbols, no? reply AnthonyMouse 29 minutes agorootparentThe user is all too often not provided with debug symbols. reply LiamPowell 11 hours agorootparentprev> This comes off a bit presumptuous. I would assume that they are aware this is a possibility. They don't present it here, they justify their solution by claiming that the existing solution is bad because it includes several megabytes of debug symbols. reply hitekker 2 hours agorootparentprevOn the note of awareness of possibilities vs ignorance, I found this thread on HN to be gold: https://news.ycombinator.com/item?id=37592471 The creator of Vue pointed out a new feature for Svelte was a similar feature that Vue had tried and discarded. The creator of Svelte showed up to explain their position; I thought their exchange was illuminating. I was also impressed by Vue's research & experimentation. reply tambourine_man 11 hours agorootparentprev> I am not bothered by having to use Sentry to view it I am. And by source maps in general. reply dewey 6 hours agoparentprevIf you criticize something where you have no context, were not part of the discussions and aware of the trade offs that were made it comes across a bit arrogant to say they should “obviously” “just” do something else. There’s other ways to suggest an alternative solution. reply LiamPowell 4 hours agorootparentMaybe it comes off that way a bit, but I can only go off what's in the blog post where they begin by calling the existing solution bloated in order to justify their new solution without ever mentioning that the debug table contains much more than just function names and line number mappings. I do think it should be obvious that displaying text directly in the console is better than relying on a web service to display that same text. reply simscitizen 14 hours agoparentprevAnother option on macOS/iOS is to ship just the LC_FUNCTION_STARTS section in the Mach-O binary. This is how symbolication can discover function names from system libraries even without full debug symbols on those platforms. reply saagarjha 14 hours agoparentprevThese can still be large. I generally include them unconditionally because I think they bring value but most software does not. reply cryptonector 14 hours agoprevThis is great. Very creative. Many should copy this scheme. The key is the relative-to-executable/shared object base stack trace program counters. bun is statically linked, yes? In a dynamically linked system one would need to prefix every normalized program counter with a small numeric shared object ID. reply andersa 11 hours agoparentIt's not really anything new. Very common in environments where you can't ship symbols such as games crashing on player's computers. For example, the Unreal Engine crash reporter has been capable of sending such a simple format for many years. You can restore a reasonably accurate function/line number from it for each stack frame. Though minidump is usually preferred as having the stack variables can give additional hints to what happened. reply elliotlarson 11 hours agoprevBun seems really compelling. I tried it out for a couple of small example projects and I like the speed and the fact that it combines package management and a JS runtime. However, I use Dependabot on most of my serious projects. I know work is under way, or at least there is some discussion in a couple of repo issues, for Bun support in Dependabot. I'm kind of holding off on using it until support for it has been rolled out. reply heldrida 13 hours agoprevAfter years following bun, (seen first tweets about from zig) have recently started using it. Stuff just works without any hassle! Thank you reply herpderperator 15 hours agoprevBun is amazing, but I recently tried to make an http/2 server through fastify and was not able to: user@host:~/d/temp/server$ bun run index_fastify.js 14warned.add(feature), console.warn(new NotImplementedError(feature, issue)); 15}, $; 1617class NotImplementedError extends Error { 18code; 19constructor(feature, issue) {^ NotImplementedError: node:http2 createServer is not yet implemented in Bun. Track the status & thumbs up the issue: https://github.com/oven-sh/bun/issues/887 code: \"ERR_NOT_IMPLEMENTED\" at new NotImplementedError (internal:shared:19:27) at internal:shared:2:69 at node:http2:48:53 at getServerInstance (/Users/user/d/temp/server/node_modules/fastify/lib/server.js:342:16) at createServer (/Users/user/d/temp/server/node_modules/fastify/lib/server.js:25:18) at fastify (/Users/user/d/temp/server/node_modules/fastify/fastify.js:198:30) at /Users/user/d/temp/server/index_fastify.js:4:13 The linked issue is actually about implementing support for http/2 clients, which was already released in v1.0.13 (https://bun.sh/blog/bun-v1.0.13#http2-client-support). The NotImplementedError message should be updated to point to the issue for the server variant: https://github.com/oven-sh/bun/issues/8823 Implementing http/2 server support is in the top few feature requests (https://github.com/oven-sh/bun/issues?q=is%3Aissue+is%3Aopen...). It looks like once they ship this, a lot more people will be able to move over to Bun. reply drewbitt 13 hours agoparentThat's just the status of Bun. Wait for implementations -> turns out you need more implementations of other APIs once that's done -> wait some more -> it comes out but will crash at various edge cases -> wait some more -> repeat. Bun is just too early in its lifecycle. Very hopeful for the project though! reply alimnes 4 hours agorootparentYes, but http2 is one of the few features Bun is missing. Overall it's pretty complete. reply nikita 15 hours agoprevMicrosoft is really good at this BTW. At SQL Server we had mini dumps they were tiny stripped out of personal info and incredibly useful. And a full dump of a production SQL Server even at that time (15 years ago) would be a huge file - too big to move around. reply jallmann 14 hours agoparentCurious - was this for Microsoft internal services or customer deployments? If the latter, how did they know what was PII? reply malkia 11 hours agorootparentPossibly this was used MiniDumpFilterTriage (from https://learn.microsoft.com/en-us/windows/win32/api/minidump...) and some of other stripping/scrubbing data fields. This one fills all non-null ptr in the callstack (and other areas?) with 0xAAAAAAAA I actually had to fix this for us two weeks ago, as our internal tools were crashing on the CI with this, and it wasn't helpful (to us), but at the same time understand how important is for this if shipped to external customers. Crashdumps are underrated field that needs more eyes to solve the big data problem there. reply nikita 14 hours agorootparentprevI believe you had to opt in and it has some legal language before it. All the data pages were stripped and SQL Servers stores all data in the buffer pool. But of course you could find some stuff on the stack and other caches. reply tredre3 15 hours agoprevDid you know that bun needs to download 37 packages before the repl becomes available? No internet no repl for you. PS C:\\Users\\anon> bun repl bun-repl [6/6] error: FailedToOpenSocket downloading package manifest bun-repl error: bun-repl@latest failed to resolve Not a big deal, but I was expecting (and frankly excited) to have a single no-install executable to drop in my PATH and have it just work! reply Jarred 15 hours agoparentWe haven’t prioritized implementing a repl yet. The current repl is a community-implemented bun-repl npm package. `bun repl` internally does the equivalent of `bunx bun-repl` reply Kwpolska 2 hours agorootparentSounds like a security nightmare. Does `bun foobar` translate to `bunx bun-foobar` as well? reply Jarred 2 hours agorootparent> Does `bun foobar` translate to `bunx bun-foobar` as well? No. reply lelo_tp 15 hours agoprevFew people would notice how much attention was put into it. Love it, really tells how much the folks behind bun care about their craft reply vvpan 16 hours agoprevSo, anybody using Bun? Does it live up to hype? reply Stoids 15 hours agoparentI have not used it in production yet, but it's been great for one-off scripts and side projects. Setting up a TypeScript Node environment with ts-node, ts-jest, ESM support, top level await, etc. is more annoying than it should be. More recent Node releases have alleviated some of this pain, but not as trivial as running bun init. I've enjoyed using the bun shell [1] API. [1] https://bun.sh/blog/the-bun-shell reply lvncelot 3 hours agorootparentOhh bun shell looks interesting. I was looking at zx[1] for some frontend pipeline scripting tasks that were just beyond maintainable bash, but maybe I'll give bun shell a go. [1] https://github.com/google/zx reply drewbitt 3 hours agorootparentBun shell has less features than zx or dax https://github.com/dsherret/dax. I tried it out but had to change tooling. reply basil-rash 15 hours agoparentprevIt’s good unless you want a repl (they pretend to have one but it’s miserable: >6s latency all the time when it updates), or you plan to use any native modules. The error messages are also significantly worse than node’s. I used it for a bit, but node with ‘—loader tsx’ does everything I want nowadays with none of the downside. If I was building a simple server, perhaps with websockets, and I was sure it need native modules, I’d consider using Bun. I have several such services live now actually. reply robxorb 14 hours agorootparentI too would use it if the REPL worked properly. JS developers are often used to working interactively - the browser/console workflow - and have come to rely on testing ideas or problems / solutions in the REPL interactively. It's efficient, as you can test and narrow things down in isolation and figure out what is working or not-working very quickly. It's also a good way to try a new module, and this is reflected in docs that eg, use node's REPL as a demo. One bizarre problem with bun's REPL at the moment, which can be quite the unexpected \"gotcha\", is the REPL itself seems to be cached, and it goes stale. It actually expects to be able to update itself online every day or so, or it breaks! Yeah, well cool - unless you ever work offline. Eg, had you expected to work on a long flight and now half your dev workflow was broken, for no good reason. (And unfortunately bun's \"--prefer-offline\" flag had no effect.) I don't want to criticise bun too much - it is incredibly fast, and has made other significant workflow enhancements. But a REPL is a dealbreaker for some of us. I guess it's similar to hot code-reloading in compiled languages, you don't go back to the minutes-long compile-wait cycle for the kinds of problems that kind of workflow makes instantaneous. reply Jarred 13 hours agorootparentCurrently, the REPL is an alias of `bunx bun-repl`, and bun-repl is a community-maintained npm package. We haven't had the time to do our own REPL. Honestly, we really need to hire more engineers. We're a small team and there's so much to do reply M4v3R 15 hours agoparentprevStarted using it as soon as they hit 1.0 and never looked back, we’re implementing it in every project now. reply fellowniusmonk 15 hours agorootparentBun + uwebsocket cut my server costs and requirements significantly for my websocket app. It's a real delight to work with. reply Alupis 15 hours agorootparentHow did it cut your server costs - increased performance and therefore reduced load? Bun looks quite promising, but I've heard mixed feedback on it being a \"drop in\" replacement of node. Any experience with that aspect? reply fellowniusmonk 14 hours agorootparentPerf gain mostly from uwebsocket is what it looks like. reply alimnes 4 hours agorootparentprevDo you use uwebsocket with Bun explicitly? I thought Bun has websocket support built in by internally using uwebsockets? reply ivanjermakov 11 hours agoparentprevI'm using it for my programming language (~15kLOC) as a dev and test runner and haven't had any Bun specific issues so far. It still surprises me with its instant startup. reply aprilnya 7 hours agoparentprevBeen using it recently and it’s great, the quality of life stuff is really nice (like, not having to worry about compiling TS) and it’s really fast. Some things are still missing though, but for me it’s already better than Node reply afavour 11 hours agoparentprevIt’s super fast and nice. But I can’t bring myself to use it with anything other than side projects because of the VC funding model. reply k__ 15 hours agoparentprevPretty awesome, but I hope they get the node:crypto module to 100% soon. reply verisimilidude 14 hours agoparentprevThe Bun libraries made it very easy to create my own static site generator. Not a huge lift, I know, but it’s been a delight to work with. reply numbers 16 hours agoparentprevoh yeah, it's amazing! The speed is great but just the DX is so much nicer than npm, yarn, or node itself. It took us a few tries to getting it working for our prod environments, but nothing we couldn't figure out within the same day. I thought having a binary for a lock file was weird (don't know the technical reasons behind it besides maybe speed) but after using it for months now, it never has caused an issue for us. reply vvpan 15 hours agorootparentThanks for the feedback. What kind of product are you using it for? Any issues with library compatibility? reply yieldcrv 15 hours agoparentprevusing it on all my side projects, some of which made it to production. some of which had lots of dependencies in the past and seamlessly switched over one employer/client has a big project I'm afraid to attempt changing anything on, but I wonder reply toxik 15 hours agoprevI feel like this post was a great case study of Zig. Interesting! reply ctoth 15 hours agoprevTIL: Bun runs on Windows and can be installed with scoop. reply nosefrog 7 hours agoprevThis is crazy. Amazing work! reply sgammon 12 hours agoprevThis is awesome. Great work Bun team reply bluelightning2k 16 hours agoprevThis guy ships reply tiffanyh 10 hours agoprevUnpopular opinion … Please use the same TLD for all your services. Don’t do: Bun.sh, and now Bun.report It’s confusing for users and also a security vector. Just make it report.bun.sh reply inopinatus 10 hours agoparentThe obvious long-term solution is to obtain the .bun gTLD in the next round of applications (expected 2026) reply Jarred 1 hour agorootparentthere have been discussions about this reply thanksgiving 10 hours agoparentprevAnother vote for same TLD. I’d dare say this should be the default position for everything (except toy projects). reply twelvechairs 10 hours agoparentprevI don't know if it's unpopular. That's literally how domains are designed to function and have done by and large for decades reply thanksgiving 9 hours agorootparentI think marketing folks like to send emails from a different domain so they can send transactional and marketing emails from different domains altogether (not just subdomains). Not sure if this is evidence based or just plain voo doo / monkey-see-monkey-do but I’ve read this in multiple places. But of course, that doesn’t apply here. reply Rucadi 16 hours agoprevA lot of people trash about bun, but I think that they are highly motivated people bringing a lot to the table. reply ComputerGuru 12 hours agoparentHaving VCs kind of forces that motivation on you, for better or worse. reply dzogchen 12 hours agoprevI just cannot get excited for this VC funded experimental piece of technology. 10 years ago, maybe. Node.js is not going anywhere anytime soon. reply inopinatus 10 hours agoparentThey used to say \"perl is not going anywhere\", too, and it turns out they were right. reply pjmlp 4 hours agorootparentTurns out a full rewrite turns people away. Still plenty of UNIX admins keep using Perl 5. reply lolinder 9 hours agorootparentprevPython and PHP weren't VC-funded projects. reply cryptonector 14 hours agoprev> Linux: Use dl_iterate_phdr to iterate over the loaded modules, once you find one that the raw address is contained in, .dlpi_addr on the dl_phdr_info struct will be the base address. Er, just use `dladdr()`. reply AbuAssar 7 hours agoprev [–] > That's why in Bun v1.1.5, I wrote a compact new format for Zig and C++ crash reports. why did the author use the pronoun I instead of we? isn't it a team work and there is no I in team as they say? reply Jarred 7 hours agoparent [–] We are a team, but Dave wrote the code for this project so \"I\" makes sense reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bun software introduces a new crash report format in Zig and C++ to address the challenge of reproducing and debugging crashes from over 2,600 GitHub issues.",
      "The compact format minimizes personal data and eliminates the need for large debug symbols, featuring encoded stack trace addresses and feature flags for efficient server analysis.",
      "The system showcases the advantages of sending essential data for issue diagnosis over traditional core dumps, demonstrating privacy-conscious and high-performance crash reporting."
    ],
    "commentSummary": [
      "Users are discussing Bun's New Crash Reporter as an alternative to debug symbols, debating the inclusion of function names in the debug table on Hacker News.",
      "Different perspectives on debugging tools and package management between Vue and Svelte are highlighted in the conversation, with users praising Bun's functionality but suggesting improvements.",
      "Feedback includes positive experiences with uwebsocket and Bun, alongside concerns about REPL function issues and using Bun as a replacement for Node.js."
    ],
    "points": 299,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1714148430
  },
  {
    "id": 40168242,
    "title": "PEP 686: Python defaults to UTF-8 Mode",
    "originLink": "https://peps.python.org/pep-0686/",
    "originBody": "PEP 686 – Make UTF-8 mode default Author: Inada NaokiDiscussions-To: Discourse thread Status: Accepted Type: Standards Track Created: 18-Mar-2022 Python-Version: 3.15 Post-History: 18-Mar-2022, 31-Mar-2022 Resolution: Discourse message Table of Contents Abstract This PEP proposes enabling UTF-8 mode by default. With this change, Python consistently uses UTF-8 for default encoding of files, stdio, and pipes. Motivation UTF-8 becomes de facto standard text encoding. The default encoding of Python source files is UTF-8. JSON, TOML, YAML use UTF-8. Most text editors, including Visual Studio Code and Windows Notepad use UTF-8 by default. Most websites and text data on the internet use UTF-8. And many other popular programming languages, including Node.js, Go, Rust, and Java uses UTF-8 by default. Changing the default encoding to UTF-8 makes it easier for Python to interoperate with them. Additionally, many Python developers using Unix forget that the default encoding is platform dependent. They omit to specify encoding=\"utf-8\" when they read text files encoded in UTF-8 (e.g. JSON, TOML, Markdown, and Python source files). Inconsistent default encoding causes many bugs. Specification Enable UTF-8 mode by default Python will enable UTF-8 mode by default from Python 3.15. Users can still disable UTF-8 mode by setting PYTHONUTF8=0 or -X utf8=0. locale.getencoding() Since UTF-8 mode affects locale.getpreferredencoding(False), we need an API to get locale encoding regardless of UTF-8 mode. locale.getencoding() will be added for this purpose. It returns locale encoding too, but ignores UTF-8 mode. When warn_default_encoding option is specified, locale.getpreferredencoding() will emit EncodingWarning like open() (see also PEP 597). This API was added in Python 3.11. Fixing encoding=\"locale\" option PEP 597 added the encoding=\"locale\" option to the TextIOWrapper. This option is used to specify the locale encoding explicitly. TextIOWrapper should use locale encoding when the option is specified, regardless of default text encoding. But TextIOWrapper uses \"UTF-8\" in UTF-8 mode even if encoding=\"locale\" is specified for now. This behavior is inconsistent with the PEP 597 motivation. It is because we didn’t expect making UTF-8 mode default when Python changes its default text encoding. This inconsistency should be fixed before making UTF-8 mode default. TextIOWrapper should use locale encoding when encoding=\"locale\" is passed even in UTF-8 mode. This issue was fixed in Python 3.11. Backward Compatibility Most Unix systems use UTF-8 locale and Python enables UTF-8 mode when its locale is C or POSIX. So this change mostly affects Windows users. When a Python program depends on the default encoding, this change may cause UnicodeError, mojibake, or even silent data corruption. So this change should be announced loudly. This is the guideline to fix this backward compatibility issue: Disable UTF-8 mode. Use EncodingWarning (PEP 597) to find every places UTF-8 mode affects. If encoding option is omitted, consider using encoding=\"utf-8\" or encoding=\"locale\". If locale.getpreferredencoding() is used, consider using \"utf-8\" or locale.getencoding(). Test the application with UTF-8 mode. Preceding examples Ruby changed the default external_encoding to UTF-8 on Windows in Ruby 3.0 (2020). Java changed the default text encoding to UTF-8 in JDK 18. (2022). Both Ruby and Java have an option for backward compatibility. They don’t provide any warning like PEP 597’s EncodingWarning in Python for use of the default encoding. Rejected Alternative Deprecate implicit encoding Deprecating the use of the default encoding is considered. But there are many cases that the default encoding is used for reading/writing only ASCII text. Additionally, such warnings are not useful for non-cross platform applications run on Unix. So forcing users to specify the encoding everywhere is too painful. Emitting a lot of DeprecationWarning will lead users ignore warnings. PEP 387 requires adding a warning for backward incompatible changes. But it doesn’t require using DeprecationWarning. So using optional EncodingWarning doesn’t violate the PEP 387. Java also rejected this idea in JEP 400. Use PYTHONIOENCODING for PIPEs To ease backward compatibility issue, using PYTHONIOENCODING as the default encoding of PIPEs in the subprocess module is considered. With this idea, users can use legacy encoding for subprocess.Popen(text=True) even in UTF-8 mode. But this idea makes “default encoding” complicated. And this idea is also backward incompatible. So this idea is rejected. Users can disable UTF-8 mode until they replace text=True with encoding=\"utf-8\" or encoding=\"locale\". How to teach this For new users, this change reduces things that need to teach. Users don’t need to learn about text encoding in their first year. They should learn it when they need to use non-UTF-8 text files. For existing users, see the Backward compatibility section. Copyright This document is placed in the public domain or under the CC0-1.0-Universal license, whichever is more permissive. Source: https://github.com/python/peps/blob/main/peps/pep-0686.rst Last modified: 2023-09-09 17:39:29 GMT",
    "commentLink": "https://news.ycombinator.com/item?id=40168242",
    "commentBody": "PEP 686 – Make UTF-8 mode default (python.org)224 points by GalaxySnail 22 hours agohidepastfavorite96 comments nerdponx 19 hours agoDefault text file encoding being platform-dependent always drove me nuts. This is a welcome change. I also appreciate that they did not attempt to tackle filesystem encoding here, which is a separate issue that drives me nuts, but separately. reply Dwedit 13 hours agoparentWith system-default code pages on Windows, it's not only platform-dependent, it's also System Locale dependent. Windows badly dropped the ball here by not providing a simple opt-in way to make all the Ansi functions (TextOutA, etc) use the UTF-8 code page, until many many years later with the manifest file. This should have been a feature introduced in NT4 or Windows 98, not something that's put off until midway through Windows 10's development cycle. reply sheepscreek 12 hours agorootparentI suspect that is a symptom of Microsoft being an enormously large organization. Coordinating a change like this that cuts across all apps, services and drivers is monumental. Honestly it is quite refreshing to see them do it with Copilot integration across all things MS. I don’t use it though, just admire the valiant effort and focus it takes to pull off something like this. Of course - goes without saying, only works when the directive comes from all the way at the top. Otherwise there will be just too many conflicting incentives for any real change to happen. While I am on this topic - I want to mention Apple. It is absolutely bonkers how they have done exactly the is countless times. Like changing your entire platform architecture! It could have been like opening a can of worms but they knew what they were doing. Kudos to them. Also..(sorry, this is becoming a long post) civil and industrial engineering firms routinely pull off projects like that. But the point I wanted to emphasize is that it’s very uncommon in tech which prides on having decentralized and semi-autonomous teams vs centralized and highly aligned teams. reply samus 3 hours agorootparent> While I am on this topic - I want to mention Apple. It is absolutely bonkers how they have done exactly the is countless times. Like changing your entire platform architecture! It could have been like opening a can of worms but they knew what they were doing. Kudos to them. Apple has a walled garden approach to managing their ecosystem, and within the confines of their garden they just do what's necessary. AFAIK, Apple doesn't care about the possibilty to run binaries from the '90s on a modern stack. Edit: even though it's expensive, it's possible to conduct such ecosystem-wide changes if you hold all cards in your hand. Microsoft was able to reengineer the graphical subsystem somewhere between XP and 8. Doing something like this is magnitudes more difficult on Linux (Wayland says hi). Google could maybe do it withij their Android corner, but they generally give a sh*t about backwards compatibility. reply kevin_thibedeau 9 hours agorootparentprev\"UCS-2 is enough for anyone\" reply Dwedit 8 hours agorootparentUCS-2 is why we have the WTF-8 encoding standard, which allows mismatched UTF-16 surrogate pairs to survive a round-trip through an 8-bit encoding. https://simonsapin.github.io/wtf-8/ reply layer8 17 hours agoparentprevHistorically it made sense, when most software was local-only, and text files were expected to be in the local encoding. Not just platform-dependent, but user’s preferred locale-dependent. This is also how the C standard library operates. For example, on Unix/Linux, using iso-8859-1 was common when using Western-European languages, and in Europe it became common to switch to iso-8859-15 after the Euro was introduced, because it contained the € symbol. UTF-8 only began to work flawlessly in the later aughts. Debian switched to it as the default with the Etch release in 2010. reply da_chicken 16 hours agorootparentIt's still not that uncommon to see programs on Linux not understanding multibyte UTF-8. It's also true that essentially nothing on Linux supports the UTF-8 byte order mark. Yes, it's meaningless for UTF-8, but it is explicitly allowed in the specifications. Since Microsoft tends to always include a BOM in any flavor of Unicode, this means Linux often chokes on valid UTF-8 text files from Windows systems. reply tialaramex 15 hours agorootparentThe BOM cases are at best a consequence of trying to use poor quality Windows software to do stuff it's not suited to. It's true that in terms of Unicode text it's valid for a UTF-8 string to have a BOM, but just because that's true in the text itself doesn't magically change file formats which long pre-dated that. Most obviously shebang (the practice of writing #!/path/to/interpreter at the start of a script) is specifically defined on those first two bytes. It doesn't make any sense have a BOM here because that's not the format, and inventing a new rule later which says you can do it doesn't make that true, any more than in 2024 the German government can decide Germany didn't invade Poland in 1939, that's not how Time's Arrow works. reply tremon 10 hours agorootparentpoor quality Windows software to do stuff it's not suited to Depends how wide your definition of \"poor quality\" is. All powershell files (ps1, psm1, psd1) are assumed to be in the local charset unless they have a byte order mark, in which case they're treated as whatever the BOM says. reply nerdponx 15 hours agorootparentprevInterestingly, Python is one of those programs. You need to use the special \"utf-8-sig\" encoding for that, which is not prominently advertised anywhere in the documentation (but it is stated deep inside the \"Unicode HOWTO\"). I never understood why ignoring this special character requires a totally separate encoding. reply duskwuff 14 hours agorootparent> I never understood why ignoring this special character requires a totally separate encoding. Because the BOM is indistinguishable from the \"real\" UTF-8 encoding of U+FEFF (zero-width no-break space). Trimming that codepoint in the UTF-8 decoder means that some strings like \"\\uFEFF\" can't be safely round-tripped; adding it in the encoder is invalid in many contexts. reply thayne 7 hours agorootparentprevReally? In my experience it's pretty rare for Linux programs not to understand any multibyte utf-8 (which would be anything that isn't ascii). What is somewhat common is failing on code points outside the basic multilingual plane (codepoints that don't fit in 16 bits). reply Dylan16807 16 hours agorootparentprev> Not just platform-dependent, but user’s preferred locale-dependent. Historically it made sense to be locale-dependent, but even then it was annoying to be platform-dependent. One is not a subset of the other. reply layer8 15 hours agorootparentNot sure what you mean by that with regard to encodings. The C APIs were explicitly designed to abstract from that, and together with libraries like iconv is was rather straightforward. You only needed to be aware that there is a difference between internal and external encoding, and maybe decide between char and wchar_t. reply Dylan16807 10 hours agorootparentNot everything is C, and nothing like that saves you when you move your floppy between computers. reply hermitdev 16 hours agorootparentprev> platform-dependent. It's 2024 and we still can't all agree on line endings. Mac vs Win vs Unix... reply Y-bar 16 hours agorootparentMac OS and Unix agreed about twenty years ago to use the same ending: https://superuser.com/a/439443 reply Dylan16807 15 hours agorootparentBy which time XP was already in the middle of releasing, so it was too late to get Windows on board. It's too bad, with a bit more planning and an earlier realization that Unicode cannot in fact fit into 16 bits then Windows might have used UTF-8 internally. reply jmb99 12 hours agorootparentUnless I’m mistaken, Rhapsody (released 1997) used LF, not CR. At that point it was pretty clear Mac was moving towards Unix through NeXTSTEP, meaning every OS except windows would be using LF. Microsoft would’ve had around 6 years before the release of XP, and probably would’ve had time to start the transition with Win2K at the end of 1999. reply mixmastamyk 3 hours agorootparentEvery OS except the one that had 95% market share in late 90s. Apple was only propped up “Weekend at Bernies” style to appease regulators. reply Longhanks 15 hours agorootparentprevIt's 2024, everything but Windows is UTF-8 since twenty years. reply int_19h 12 hours agorootparentLinux was definitely not uniformly UTF-8 twenty years ago. It was one of the many available locales, but it was still common to use other encodings, and plenty of software didn't handle multibyte well in general. reply andrewshadura 3 hours agorootparentprevEtch came out in 2007, not 2010. reply anthk 16 hours agorootparentprevEmacs was amazing for that; builtin text encoders/decoders/transcoders for everything. reply hollerith 16 hours agorootparentMy experience was that brittleness around text encoding in Emacs (versions 22 and 23 or so) was a constant source of annoyance for years. IIRC, the main way this brittleness bit me was that every time a buffer containing a non-ASCII character was saved, Emacs would engage me in a conversation (which I found tedious and distracting) about what coding system I would like to use to save the file, and I never found a sane way to configure it to avoid such conversations even after spending hours learning about how Emacs does coding systems: I simply had to wait (a year or 3) for a new version of Emacs in which the code for saving buffers worked better. I think some people like engaging in these conversations with their computers even though the conversations are very boring and repetitive and that such conversation-likers are numerous among Emacs users or at least Emacs maintainers. reply fbdab103 16 hours agoparentprevA different one that just bit me the other day was implicitly changing line endings. Local testing on my corporate laptop all went according to plan. Deploy to linux host and downstream application cannot consume it because it requires CRLF. Just one of those stupid little things you have to remember from time to time. Although, why does newly written software require a specific line terminator is a valid question. reply selimnairb 17 hours agoparentprevYeah, this has bitten me several times as soon as a people use the code on Windows. reply jillesvangurp 20 hours agoprevNot relying on flaky system defaults is a good thing. These things have a way of turning around and being different than what you assume them to be. A few years ago I was dealing with Ubuntu and some init.d scripts. One issue I ran into was that some script we used to launch Java (this was before docker) was running as root (bad, I know) and with a shell that did not set UTF-8 as the default like would be completely normal for regular users. And of course that revealed some bad APIs that we were using in Java that use the os default. Most of these things have variants that allow you to set the encoding at this point and a lot of static code checkers will warn you if you use the wrong one. But of course it only takes one place for this to start messing up content. These days it's less of an issue but I would simply not rely on the os to get this right ever for this. Most uses of encodings other than UTF-8 are extremely likely to be unintentional at this point. And if it is intentional, you should be very explicit about it and not rely on weird indirect configuration through the OS that may or may not line up. So, good change. Anything that breaks over this is probably better off with the simple fix added. And it's not worth leaving everything else as broken as it is with content corruption bugs just waiting to happen. reply ok_computer 8 hours agoparentI was using .gitignore generated by an aliased touch function in powershell. Despite my best efforts, I could not get git to respect its gitignore. Figured out the touched text file was utf-16 and basically not respected at all. Lesson learned I uuchanged a system default to utf-8 but just rely on my text editor now. reply anordal 13 hours agoprevThe following heuristic has become increasingly true over the last couple of decades: If you have some kind of \"charset\" configuration anywhere, and it's not UTF-8, it's wrong. Python 2 was charset agnostic, so it always worked, but the improvement with Python 3 was not only an improvement – how to tell a Python 3 script from a Python 2 script? * If it contains the string \"utf-8\", it's Python3. * If it only works if your locale is C.UTF-8, it's Python3. Needless to say, I welcome this change. The way I understand it, it would \"repair\" Python 3. reply Euphorbium 19 hours agoprevI thought it was default since python 3. reply lucb1e 18 hours agoparentYou may be thinking of strings where the u\"\" prefix was made obsolete in python3. Then again, trying on Python 2.7 just now, typing \"éķů\" results in it printing the UTF-8 bytes for those characters so I don't actually know what that u prefix ever did, but one of the big py2-to-3 changes was strings having an encoding and byte strings being for byte sequences without encodings This change seems to be about things like open('filename', mode='r') mainly on Windows where the default encoding is not UTF-8 and so you'd have to specify open('filename', mode='r', encoding='UTF-8') reply jcranmer 16 hours agorootparentPython has two types of strings: byte strings (every character is in the range of 0-255) and Unicode strings (every character is a Unicode codepoint). In Python 2.x, \"\" maps to a byte string and u\"\" maps to a Unicode string; in Python 3.x, \"\" maps to a unicode string and b\"\" maps to a byte string. If you typed in \"éķů\" in Python 2.7, what you get is a string consisting of the hex chars 0xC3 0xA9 0xC4 0xB7 0xC5 0xAF, which if you printed it out and displayed it as UTF-8--the default of most terminals--would appear to be éķů. But \"éķů\"[1] would return a byte string of \\xa9 which isn't valid UTF-8 and would likely display as garbage. If you instead had used u\"éķů\", you'd instead get a string of three Unicode code points, U+00E9 U+0137 U+016F. And u\"éķů\"[1] would return u\"ķ\", which is a valid Unicode character. reply aktiur 17 hours agorootparentprev> strings having an encoding and byte strings being for byte sequences without encodings You got it kind of backwards. `str` are sequence of unicode codepoints (not UTF-8, which is a specific encoding for unicode codepoints), without reference to any encoding. `bytes` are arbitrary sequence of octets. If you have some `bytes` object that somehow stands for text, you need to know that it is text and what its encoding is to be able to interpret it correctly (by decoding it to `str`). And, if you got a `str` and want to serialize it (for writing or transmitting), you need to choose an encoding, because different encodings will generate different `bytes`. As an example : >>> \"évènement\".encode(\"utf-8\") b'\\xc3\\xa9v\\xc3\\xa8nement' >>> \"évènement\".encode(\"latin-1\") b'\\xe9v\\xe8nement' reply lucb1e 17 hours agorootparent> `str` are sequence of unicode codepoints [...] without reference to any encoding I guess I see it from the programmer's perspective: to handle bytes coming from the disk/network as a string, I need to specify an encoding, so they are (to me) byte sequences with an encoding assigned. Didn't realize strings don't have an encoding in Python's internal string handling but are, instead, something like an array of integers pointing to unicode code points. Not sure if this viewpoint means I am getting it backwards but I can see how that was phrased poorly on my part! reply tialaramex 16 hours agorootparentThere are two distinct questions here, to which implementations can provide different answers 1. Interface: How can I interact with \"string\" values, what kind of operations can I perform versus what can't be done ? Methods and Operators provided go here. 2. Representation: What is actually stored (in memory) ? Layout goes here. So you may have understood (1) for Python, but you were badly off on (2). Now, at some level this doesn't matter, but, for performance obviously the choice of what you should do will depend on (2). Most obviously, if the language represents strings as UTF-8 bytes, then \"encoding\" a string as UTF-8 will be extremely cheap. Whereas, if the language represents them as UTF-16 code units, the UTF-8 encoding operation will be a little slower. reply lucb1e 15 hours agorootparentAlright, but don't leave us hanging: what does Python3 use for (2) that you say I was badly off on? (Or, in actuality, never thought about or meant to make claims about.) Now we still can't make good choices for performance! https://stackoverflow.com/questions/1838170/what-is-internal... says Python3.3 picks either a one-, two-, or four-byte representation depending on which is the smallest one that can represent all characters in a string. If you have one character in the string that requires >2 bytes to represent, it'll make every character take 4 bytes in memory such that you can have O(1) lookups on arbitrary offsets. The more you know :) reply samus 11 hours agorootparentSince Java 9, the Java JRE does something similar: if a string contains only characters in ISO-8859-1 then it is stored as such, else the usual storage format (int16) is used. reply tialaramex 15 hours agorootparentprevYeah, I started writing about what you found (the answer to (2) for Python) and I realised that's a huge rabbit hole I was venturing down and decided to stop short and post, so, apologies I guess. reply d0mine 14 hours agoparentprevThe Python source code is utf-8 by default in Python 3. But it says nothing about a character encoding used to save to a file. It is locale-dependent by default. # string literals create str objects using utf-8 by default Path(\"filenames use their own encoding\").write_text(\"file content encoding uses yet another encoding\") The corresponding encodings are: - utf-8 [tokenize.open] - sys.getfilesystemencoding() [os.fsencode] - locale.getpreferredencoding() [open] reply Animats 12 hours agoprevIs the internal encoding in CPython UTF-8 yet? You can index through Python strings with a subscript, but random access is rare enough that it's probably worthwhile to lazily index a string when needed. If you just need to advance or back up by 1, you don't need an index. So an internal representation of UTF-8 is quite possible. reply rogerbinns 11 hours agoparentThe PyUnicode object is what represents a str. If the UTF-8 bytes are ever requested, then a bytes object is created on demand and cached as part of the PyUnicode, being freed with the PyUnicode itself is freed. Separately from that the codepoints making up the string are stored in a straight forward array allowing random access. The size of each codepoint can be 1, 2, or 4 bytes. When you create a PyUnicode you have to specify the maximum codepoint value which is rounded up to 127, 255, 65535, or 1,114,111. That determines if 1, 2, or 4 bytes is used. If the maxiumum codepoint value is 127 then that array representation can be used for the UTF-8 directly. So the answer to your question is that many strings are stored as UTF-8 because all the codepoints areAnd many other popular programming languages, including Node.js, Go, Rust, and Java uses UTF-8 by default. Oh, I missed Java moving from UTF-16 to UTF-8. reply hashmash 20 hours agoparentWith Java, the default encoding when converting bytes to strings was originally platform independent, but now it's UTF-8. UTF-16 and latin-1 encodings are (still*) used internally by the String class, and the JVM uses a modified UTF-8 encoding like it always has. * The String class originally only used UTF-16 encoding, but since Java 9 it also uses a single-byte-per-character latin-1 encoding when possible. reply rootext 20 hours agoparentprevIt seems you are mixing two things: inner string representation and read/write encoding. Java has never used UTF-16 as default for the second. reply Dwedit 7 hours agorootparentOr possibly confusing it with JavaScript, which treats strings as sequences of UTF-16 characters? reply cryptonector 18 hours agorootparentprevNot even on Windows? reply layer8 17 hours agorootparentNo, file I/O on Windows in general doesn’t use UTF-16, but the regional code page, or nowadays UTF-8 if the application decides so. reply int_19h 12 hours agorootparentDepends on what you define as \"file I/O\", though. NTFS filenames are UTF-16 (or rather UCS2). As far as file contents, there isn't really a standard, but FWIW for a long time most Windows apps - Notepad being the canonical example when asked to save anything as \"Unicode\" would save it as UTF-16. reply layer8 11 hours agorootparentI'm talking about the default behavior of Microsoft's C runtime (MSVCRT.DLL) that everyone is/was using. UTF-16 text files are rather rare, as is using Notepad's UTF-16 options. The only semi-common use I know of is *.reg files saved from regedit. One issue with UTF-16 is that it has two different serializations (BE and LE), and hence generally requires a BOM to disambiguate. reply TheCycoONE 7 hours agorootparentPowershell use to output utf-16 by default on Windows. It might still but it's been awhile since I needed to try. reply PurpleRamen 20 hours agoparentprevSeems it happened two years ago, with Java 18. reply a-french-anon 18 hours agoprevWhy not utf-8-sig, though? It handles optional BOMs. Had to fix a script last week that choked on it. reply shellac 17 hours agoparentAt this point nothing ought to be inserting BOMs in utf-8. It's not recommended, and I think choking on it is reasonable behaviour these days. reply BoingBoomTschak 1 hour agorootparentOnly reason I used it was to force MSVC to understand my u8\"\" literals. Should've forced /utf8 in our build system, in retrospective. For UTF-16/32, knowing the endianness doesn't seem to be a frivolous functionality. And in fact, having to use heuristics-based detection via uchardet is a big mess, some kind of header should have been standardized since the start. reply Athas 17 hours agorootparentprevWhy were BOMs ever allowed for UTF-8? reply josefx 17 hours agorootparentSome editors used them to help detect UTF-8 encoded files. Since they are also valid zero length space characters they also served as a nice easter egg for people who ended up editing their linux shell scripts with a windows text editor. reply stubish 5 hours agorootparentprevAn attempt to store the encoding needed to decode the data with the data, rather than requiring the reader to know it somehow. Your program wouldn't have to care if its source data had been encoded as UTF-8, UTF-16, UTF-32 or some future standard. The usual sort of compromise that comes out of committees, in this case where every committee member wanted to be able to spit their preferred in-memory Unicode string representation to disk with no encoding overhead. reply plorkyeran 17 hours agorootparentprevWhen UTF-8 was still very much not the default encoding for text files it was useful to have a way to signal that a file was UTF-8 and not the local system encoding. reply da_chicken 16 hours agorootparentprevSome algorithms can operate much easier if they can assume that multibyte or variable byte characters don't exist. The BOM means that you don't have to scan the entire document to know if you can do that. reply Dwedit 13 hours agorootparentprevBasically every C# program will insert BOMs into text files by default unless you opt-out. reply neonsunset 9 hours agorootparentWhere did you get that from? reply Arnavion 9 hours agorootparentIt's the behavior when using the default `Encoding.UTF8` static. You have to create your own instance as `new UTF8Encoding(false)` if you don't want a BOM. reply neonsunset 7 hours agorootparentThis is true for `UTF8Encoding` used as an encoder (e.g. within transcoding stream, not often used today). Other APIs, however, like File.WriteAllText, do not write BOM unless you explicitly pass encoding that does so (by returning non-empty preamble). reply orf 18 hours agoparentprevBecause changing Python to silently prefixing all IO with an invisible BOM isn’t a good idea. reply int_19h 12 hours agorootparentThe expectation isn't for it to generate BOM in the output, but to handle BOM gracefully when it occurs in the input. reply shpx 7 hours agorootparent> On encoding the utf-8-sig codec will write 0xef, 0xbb, 0xbf as the first three bytes to the file https://docs.python.org/3/library/codecs.html The codec you're imagining would also make reading a file and writing it back change the file if it contains a BOM. reply otteromkram 18 hours agoparentprevOnly one script? Out of how many? reply hermitdev 16 hours agorootparentNot the OP, but I see this pop up quite frequently in ETL, usually handling csv files. reply anthk 16 hours agoprevOn UTF-8, the Linux framebuffer should had had a good utf8 support (a proper one, not 256/512 glyphs) long ago. Even GNU Hurd since 2007 or so it had a better 'terminal console' with UTF8 support. It's 2024. reply Aerbil313 13 hours agoprevNice. Now the only thing we need is JS to switch to UTF-8. But of course JS can't improve, because unlike any other programming language, we need to be compatible with code written in 1995. reply shpx 7 hours agoparentThis is about when you ask Python to open a file \"as text\", what encoding it will use by default. The internal representation of strings is a different matter and, like JavaScript, Python doesn't \"just use UTF-8\" for that. reply lexicality 20 hours agoprev> Additionally, many Python developers using Unix forget that the default encoding is platform dependent. They omit to specify encoding=\"utf-8\" when they read text files encoded in UTF-8 \"forget\" or possibly simply aren't made well enough aware? I genuinely thought that python would only use UTF-8 for everything unless you explicitly ask it to do otherwise. reply aktiur 17 hours agoparentIt actually depends! `bytes.decode` (and `str.encode`) have used UTF-8 as a default since at least Python 3. However, the default encoding used for decoding the name of files use ` sys.getfilesystemencoding()`, which is also UTF-8 on Windows and macos, but will vary with the locale on linux (specifically with CODESET). Finally, `open` will directly use `locale.getencoding()`. reply Affric 20 hours agoprevMake UTF-8 default on Windows reply johannes1234321 20 hours agoparentSince Windows Version 1903 (May 2019 Update) they push for Utf-8. But Windows is a big pile of compatible legacy. reply pjc50 18 hours agoparentprevIn addition to ApiFunctionA and ApiFunctionW, introduce ApiFunction8? (times whole API surface) Introduce a #define UNICODE_NO_REALLY_ALL_UNICODE_WE_MEAN_IT_THIS_TIME ? reply cryptonector 18 hours agorootparentApiFunctionA is UTF-8 capable. Needs a run-time switch too, not just compile-time. reply sebazzz 12 hours agorootparentYes: https://learn.microsoft.com/en-us/windows/win32/sbscs/applic... > On Windows 10, this element forces a process to use UTF-8 as the process code page. For more information, see Use the UTF-8 code page. On Windows 10, the only valid value for activeCodePage is UTF-8. > This element was first added in Windows 10 version 1903 (May 2019 Update). You can declare this property and target/run on earlier Windows builds, but you must handle legacy code page detection and conversion as usual. This element has no attributes. reply garaetjjte 18 hours agorootparentprevIt's now possible, but for years the excuse was that MBCS encodings only supported characters up to 2 bytes. reply ComputerGuru 14 hours agorootparentprevOnly under windows 11, I believe. And that switch is off by default. reply int_19h 12 hours agorootparentYou're thinking of the global setting that is enabled by the user and applies to all apps that operate in terms of \"current code page\" - if enabled, that codepage becomes 65001 (UTF-8). However, on Win10+, apps themselves can explicitly opt into UTF-8 for all non-widechar Win32 APIs regardless of the current locale/codepage. reply layer8 17 hours agoparentprevThat would break so many applications and workflows that it will never happen. reply numpad0 11 hours agoparentprevlots of apps can't even handle non-ASCII username on Windows reply Affric 6 hours agorootparentI have seen the worst of it. Too many companies running franken-software from decades ago. reply tedivm 20 hours agoparentprevThat's exactly what this proposal (which has been accepted) is going to do. reply lolinder 20 hours agorootparentI think they mean that the Windows operating system should default to UTF-8. reply anonym29 15 hours agoprevnext [2 more] [flagged] terr-dav 15 hours agoparentIt sounds like you're building an empire over there, albeit a tiny one with very large gates. reply Myrmornis 20 hours agoprev [–] Hm TIL, I thought that the string encoding argument to .decode() and .encode() was required, but now I see it defaults to \"utf-8\". Did that change at some point? reply LeoPanthera 18 hours agoparent> ChatGPT4 says it's always been that way since the beginning of Python3 This is not a reliable way to look up information. It doesn't know when it's wrong. reply _ache_ 20 hours agoparentprev [–] You can verify on the documentation by switching the version. So ... since 3.2: https://docs.python.org/3.2/library/stdtypes.html#bytes.deco... In 3.1 it was the default encoding of string (the type str I guess). https://docs.python.org/3.1/library/stdtypes.html#bytes.deco... reply aktiur 17 hours agorootparent [–] > In 3.1 it was the default encoding of string (the type str I guess). No, what was used was what sys.getdefaultencoding(), which was already UTF-8 in 3.1 (I checked the source code). At that time, the format used for representing `str` objects in memory depended on if you used a \"narrow\" (UTF-16) or \"wide\" (UTF-32) build of Python. Fortunately, wide and narrow builds were abandonned in Python 3.2, with a new way of representing strings : current Python will use ASCII if there's no non-ASCII char, UCS-2 –UTF-16 without surrogate pairs — if there is no codepoint higher than U+FFFF, and UTF-32 else. But that did not exist in 3.1, where you could either use the \"narrow\" build of python (that used UTF-16) or the \"wide\" build (that used UTF-32). See this article for a good overview of the history of strings in Python : https://tenthousandmeters.com/blog/python-behind-the-scenes-... reply _ache_ 17 hours agorootparent [–] Thank you ! The documentation was misleading about \"default encoding of string\". reply int_19h 12 hours agorootparent [–] The simple thing to remember is that for all versions of Python going back 12 years, there's no such thing as \"default encoding of string\". A Python string is defined as a sequence of 32-bit Unicode codepoints, and that is how Python code perceives it in all respects. How it is stored internally is an implementation detail that does not affect you. reply Dylan16807 9 hours agorootparent [–] 32 bit specifically? The most expansive Unicode has ever been was 31 bits, and UTF-8 is also capable of at most 31 bits. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "PEP 686 proposes switching Python's default encoding to UTF-8 to align with industry standards and enhance compatibility with other technologies, with the option to opt-out if necessary.",
      "The proposal addresses backward compatibility on Windows and offers guidelines for error management, aiming to ensure consistency in locale encoding usage.",
      "Similar to Java and Ruby, Python aims to streamline text encoding for newcomers while supporting existing users, rejecting alternatives like deprecating implicit encoding and PYTHONIOENCODING for PIPEs."
    ],
    "commentSummary": [
      "The discussion on PEP 686 on python.org proposes making UTF-8 the default text file encoding to address platform-specific challenges and ensure consistency in specifying encoding in Python.",
      "Contributors highlight the importance of understanding the differences between byte strings and Unicode code points, along with concerns about legacy application compatibility when switching Windows to default to UTF-8.",
      "The debate includes topics like using Byte Order Marks (BOMs), transitioning to UTF-8 across various operating systems, and the impact of encoding standards on performance and memory usage in Python and Java."
    ],
    "points": 224,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1714132514
  },
  {
    "id": 40167742,
    "title": "Pharo 12: Object-Oriented Simplicity & Enhanced Tools",
    "originLink": "https://pharo.org/news/2024-04-26-pharo12-released.html",
    "originBody": "Toggle navigation News Features Download Documentation Community Contribute Stories About Pharo 12 Released! Dear Pharo users and dynamic language lovers: We have released Pharo version 12! What is Pharo? Pharo is a pure object-oriented programming language and a powerful environment focused on simplicity and immediate feedback. Simple & powerful language: No constructors, no types declaration, no interfaces, no primitive types. Yet a powerful and elegant language with a full syntax fitting in one postcard! Pharo is objects and messages all the way down. Live, immersive environment: Immediate feedback at any moment of your development: Developing, testing, debugging. Even in production environments, you will never be stuck in compiling and deploying steps again! Amazing debugging experience: Pharo environment includes a debugger unlike anything you've seen before. It allows you to step through code, restart the execution of methods, create methods on the fly, and much more! Pharo is yours: Pharo is made by an incredible community, with more than 100 contributors for the last revision of the platform and hundreds of people constantly contributing with frameworks and libraries. Fully open-source: Pharo full stack is released under MIT License and available on GitHub ... more on the Pharo Features page. In this iteration of Pharo, we continue working on our objectives of improvement, clean-up and modularization. Also, we included a number of usability and speed improvements. A complete list of changes and improvements is available in our Changelog Some highlights of this amazing version: Highlights New breakpoint system The debug point system is a breakpoint model that supersedes the previous implementation of breakpoints and watchpoints. They are configurable, composable, and extensible. The traditional breakpoints remain available, including conditional breakpoints, one-time breakpoints, and object-centric breakpoints. Additionally, there are new types of breakpoints, such as chained-breakpoints, which condition the activation of certain breakpoints on the triggering of others (e.g., breakpoint B only activates if breakpoint A is hit first). Debug points also feature a dedicated browser and integration options. Tools Scalable fluid class syntax is now the default one Preparing the introduction of the Bloc graphic system by migrating more tools to Spec2 widgets Spec2 UI framework enhancements to support GTK 4 Leaner version of the Metacello package manager More robust and strict mode for FFI System New architecture for refactorings and domain specific transformations Code loading speed improvement Fast browsing via fully optimized package tags Optmized memory usage via optimized method protocols Compiler simplifications and improvements Virtual machine Massive image support with permanent space String/ByteArray comparison speed up Development Effort This new version is the result of 1895 Pull Requests integrated just in the Pharo repository. We have closed 865 issues and received contributions from more than 70 different contributors. We have also a lot of work in the separate projects that are included in each Pharo release: http://github.com/pharo-spec/NewTools http://github.com/pharo-spec/NewTools-DocumentBrowser http://github.com/pharo-spec/Spec http://github.com/pharo-vcs/Iceberg https://github.com/pharo-graphics/Roassal http://github.com/pillar-markup/Microdown http://github.com/pillar-markup/BeautifulComments http://github.com/pharo-project/pharo-vm Contributors We always say Pharo is yours. It is yours because we made it for you, but most importantly because it is made by the invaluable contributions of our great community (yourself). A large community of people from all around the world contributed to Pharo 12.0 by making pull requests, reporting bugs, participating in discussion threads, providing feedback, and a lot of helpful tasks in all our community channels. Thank you all for your contributions. The Pharo Team Discover Pharo: https://pharo.org/features Try Pharo: http://pharo.org/download Learn Pharo: http://pharo.org/documentation 26 April 2024 <-- [Pharo 12] will enter feature freeze on March 1st Pharo is developed by an international community of open-source developers, coordinated and maintained by the pharo consortium and receives essential support from Inria, RMOD, CNRS, UDL, Cristal and many others.",
    "commentLink": "https://news.ycombinator.com/item?id=40167742",
    "commentBody": "Pharo 12 (pharo.org)217 points by xkriva11 23 hours agohidepastfavorite99 comments mifa201 11 hours agoI started a new job in Smalltalk one year ago without any previous experience with the language (background in Scheme, a bit of Common Lisp / Clojure, C, C++, Java, Python, Haskell and a couple of other languages). I had three pair programming sessions in the beginning and that was it, it took me one month to get a grasp of the codebase I have to maintain/develop further. The application is huge, but the tooling and discoverability of the programming environment (including best debugging experience I've ever seen) made it super easy to dive into the system and learn everything practically by myself. This and the fact that everything follows the same basic design principles (objects all the way down) make it a perfect match for solving complex problems. Honestly my experience at work was mostly with Visualworks. But I've been using Pharo in two side projects and I'm loving it. It became one of my top 3 programming languages I've ever used (together with Scheme and CL). It's impressive how much this rather small community achieved, thanks for the awesome work and this new release! reply pheatherlite 6 hours agoparentThis top comment is stark contrast to the one right below it. I guess pure oo is a love hate thing reply mifa201 1 hour agorootparentI used to dislike OOP and favor functional programming languages, until I started working with Smalltalk. I guess the fact that blocks are closures and also first-class objects make up the difference somehow. I learned that programming paradigm is not the main factor I appreciate in a language, but also the elegant/coherent design, simplicity, liveness of the programming environment etc. Also since most of the time developers spend debugging programmings, today I think languages should be designed with improved debugging features in mind. Statically programming languages are doing great progress here (always heard great things about error messages from Rust). Smalltalk and Common Lisp OTH, besides having nice error messages, also have exceptions with resumable semantics. The hability to always get a debugger during an exception without unwinding the stack is such a huge help (specially for dynamic languages), allowing one to fix a running program and continue working. I maintain/develop a complex Smalltalk GUI desktop application and rarely restart it during development (which always take a while, since stuff like connecting and loading data from the database takes some time). Also I can always fire up the debugger to inspect objects I want to change (like dialogs etc), edit code and run it in any frame of the stack (like a REPL everywhere) etc. Maintaining this codebase in a dynamic programming language with exceptions that use the traditional termination model would be a nightmare, every crash would require a restart, and you don't have a compiler to help you. It's no surprise that people just turned to statically typing for this kind of system. reply no_time 21 hours agoprevWeirdest programming language/IDE/Runtime/everything I've ever attempted learning. The sheer oddity + lack of real world example code floating around made it feel impenetrable. To put it into perspective, picking up Rust and writing entry level but real world applications was a walk in the park after coming to terms with the ownership system. reply melvinroest 20 hours agoparentI agree with you. I've worked for a while at a company using it in production [1]. For learning Pharo (while not working at a company), I've learned that going to ESUG [1] and Pharo Days [2] (2 conferences) is the best way to actually learn. On ESUG there are many professional Smalltalkers, including people that write Pharo. And on Pharo Days there are many OG Pharo devs. They can teach you certain things much quicker than any course can. For example, on ESUG, I was shown how to write a debugger extension on the spot for a particular debugging case (with animations) that had no good working debugging support yet. It was amazing to see how quick people can develop it when they have strong knowledge on it. The fact that the language is inspectable helps a ton. Another way to learn is by asking many questions on their Discord channel [4]. The community seems really active there and I found them to be really friendly. The Pharo website [5] sort of understates how active their spun off communities are as they simply mention that they exist, but the site doesn't really convey the vibe of how lively the communities are. I'm not sure how one would go about that, but it's a shame you can't directly see that from a website. [1] https://yesplan.be [2] https://esug.org [3] https://days.pharo.org [4] https://discord.gg/QewZMZa [5] https://pharo.org/community reply nsm 18 hours agorootparentI think Pharo would benefit a lot from having lots and lots of screen casts, since so much stuff is visual. I don't think something like ESUG is a sustainable way to spread techniques. A lot of stuff that is well documented in the Pharo ecosystem is old. A lot of the new stuff doesn't have great docs. Just reference docs and code is not enough. There has to be a coherent narrative around an API to put it together. Also, I'm all onboard having a UI environment for programming, but it needs to have great keybindings that follow platform HIGs, and at least my Pharo 10 experience of the window management was really bad. I wrote about this some https://nikhilism.com/post/2021/experiencing-smalltalk/ reply em-bee 13 hours agorootparenti agree. here is a screencast i made: https://news.ycombinator.com/item?id=38995507 i don't know how much has changed since then. but if pharo has changed so much that this screencast can no longer be used then that's a problem in itself. we are not going to gwt more screencasts if their halflife is to short. reply azinman2 12 hours agorootparentLooking over the syntax, it seems that lines end with a period, much like in English. This makes much more sense than semicolon! That said, it looks like you end up with a million windows to do anything. Seems like the UI could be better UX'd, no? Guessing this a community that would treat such a comment as harassment. reply em-bee 11 hours agorootparentwhy would it be considered harassment? it's not like your comment is any more critical than many of the others here. as for the million windows, if you go that from my videos, then i'd like to point out that the interface has indeed improved since. for one it added the ability to group windows with tabs: https://youtu.be/GGJZeajjWGU?list=PLqbtQ7OkSta0ULYAd7Qdxof85... (interestingly, that video is older than mine, so it seems that i just hadn't discovered this feature yet) reply batman-farts 14 hours agorootparentprevMy initial burst of enthusiasm was dampened by the big gap between the entry-level introductions (ProfStef tutorial, the introductory MOOC) and the sheer complexity of everything the image includes. I'm not a big-time Java developer, I certainly appreciate the elegance of the language and dev environment compared to, say, Python, but that huge list of packages and their Baselines in the left pane of the system browser makes it really difficult to tell where to get started. I attended an online Smalltalk meetup recently, and one of the veteran Smalltalkers there was preferentially rebasing his code on Cuis because he felt Pharo had become too heavyweight. I also fear that leaning so heavily on a closed, corporate platform like Discord as the community hub may lead to tears in a few years. If you're leaning into the idea that \"the community is the documentation,\" you're at Discord's mercy for community sustainment, on top of the already hairy problem of surfacing solutions from within the depths of a long-running discussion forum. Sure, running everything off of mailing lists + IRC like older open source projects do would be a clear step backwards, but being stuck with Discord has been a mild turn-off for me. Finally, it's worth noting that development is spearheaded by folks in France and Latin America for whom English may not be their primary language. That doesn't affect their ability to do good work! It's totally worth reflecting on how something attempting to approximate natural-language programming in English ended up forked outside the Anglosphere! But I also feel like it'd be worth having an editor take a cleanup pass at future versions of the main ebooks. I've got both the books that Alexandre Bergel published through Apress, and they're both solid, but if the first-resort resources were up to the same standard, I think perhaps fewer people would come away with an unfavorable impression. Of course, that's over and above simply keeping them up to date as development progresses - I believe Pharo by Example is still on version 9? reply cess11 13 hours agorootparentStart with the Finder and the Examples-category if you want to know how to do something, don't browse the full package list in the hope you'll stumble over something useful. There are small communities outside the Discord having meetups and whatnot. Would probably be nice to have a Discourse-instance for a more documentation-like meeting place, but that requires money and volunteers doing moderation. I enjoy that some docs and other resources aren't expressed in US:ian advertising lingo. It's not a bug, it's a feature. reply peatmoss 18 hours agoparentprevSqueak / Pharo are alternate timeline technologies. Maybe the one where Betamax won and Al Gore read the intelligence briefing. Their lineage goes back to the Alto when people were imagining what interacting with computers even meant, and what metaphors from the real world make sense to apply to collections of bytes. Rust is radical in some ways, but it's fundamentally a creature of the vaguely Unixy paradigm we all live in today. I miss the weird world of possibilities we used to have. reply charlysl 18 hours agorootparent> Al Gore read the intelligence briefing. Wasn't that George W. Bush? reply cess11 18 hours agorootparentNot in the alternate timeline. reply charlysl 18 hours agorootparentHa ha, I see, nice reply derefr 16 hours agorootparentprev> Their lineage goes back to the Alto when people were imagining what interacting with computers even meant, and what metaphors from the real world make sense to apply to collections of bytes. I would argue that this lineage of computing isn't as arcane and out-of-reach as people might think. Much of the \"obvious\" promise of Smalltalk / object-based runtime environments — specifically, all the UI stuff it enabled — was too expensive / high-overhead initially, for it to have much penetration in the microcomputer or the mainframe/batch processing space; thus relegating those specific ideas to academic experiments in workstation productivity. But fancy object-based UIs weren't the whole of what this lineage of computing was about. Microcomputer and mainframe systems were built as descendants of this lineage, repeatedly, and many of them were even in common use; but it might be harder to recognize them as such. It's the less-obvious, more low-level/internal architectural things they inherited. If you ignore the specific assumption of a UI or \"strict\" OOP, and instead just consider this lineage as anything fitting these criteria: 1. systems that booted into a live runtime bytecode VM, usually de-hibernating the VM state from a memory image; 2. and then exposing a shell that was more of a REPL than a command language, allowing interoperation with the data that defined the state of the VM on a high level, 3. where the \"operating system\" within the runtime is fully exposed to you (rather than being a black box with a whitelisted FFI API); but where each data structure within that \"operating system\" is protected due to the common runtime of the OS + userland, enforcing ADT-defintion-time abstraction layers in the way it allows clients (including the REPL) to interact with any given object/ADT... ...then you could say that all of the following are part of the lineage: • BASIC — especially the BASICs on microcomputers that booted to BASIC, or had BASIC on ROM, and never ran DOS (published software for these computers, was usually just precompiled BASIC bytecode!) • Object Pascal / Delphi • Emacs • most SQL databases, but especially Ingress • MOOs (object-oriented MUDs) • Plan 9, despite its Unix roots. (Especially applicable insofar as you could consider \"a runtime and OS as toolkit, and applications as LEGO with clear exposed seams that the user can pick apart and remix\" an additional criterion.) You can usually recognize these systems, because there's no way to get anything like a machine-code monitor / debugger on them; instead, the runtime itself usually exposes bytecode-level (or interpreter-level) monitoring / debugging, in a way that doesn't allow you to break the runtime's assumptions through it. reply jdougan 12 hours agorootparentAlso Inferno on DisVM, various Forths, and modern web browsers (especially running Lively JS) reply cmrdporcupine 15 hours agorootparentprevDon't threaten me with a good time! And I still to this day enjoy combining your #3 (emacs) with your #5 (MOO) via e.g. https://github.com/toddsundsted/rmoo You might appreciate my project: https://github.com/rdaum/moor (Though it's in a bit of a slow period because of Real Life(tm)) reply mark_l_watson 20 hours agoparentprevI would like to disagree with you, but I can’t. Pharo, and Squeak that it is derivative from, is an odd development experience. A long time ago I experimented with making web apps with Pharo, fun, but I wouldn’t use it in production. When I saw this announcement I was motivated to update my Pharo NLP library https://github.com/mark-watson/nlp_smalltalk but I may not. Many NLP tasks are now done infinitely better using deep learning and LLMs. I just looked for OpenAI API support and found 2 year old Pharo project library https://github.com/pharo-ai/open-ai and maybe more promising 1 year old project by Bracken https://github.com/brackendev/OpenAI-Pharo EDIT: to be fair to Pharo, I am not really a Smalltalk person. In 1983 someone at Xerox arranged for me to get a trial Smalltalk system on My Xerox 1108 Lisp Machine, and I removed it within a few weeks. reply andsoitis 21 hours agoparentprevFwiw, you can inspect the Pharo code from within the running Pharo itself. If you’re looking for a good real life code base that is not the bare Pharo, you can check out Glamorous Toolkit: https://gtoolkit.com// reply krylon 21 hours agoparentprevI had a similar experience when I tried to learn Smalltalk. Pharo By Example is a thing. I haven't read it, but I assume it is forked from or inspired by Squeak By Example, which made things click for me (at least a little). https://books.pharo.org/pharo-by-example9/ reply 0x445442 21 hours agorootparentIIRC, Squeak By Example is heavily Morphic centric, which is going to immediately diverge from other language Getting Started sections. Not sure if one exists but a Getting Started that focuses on just the Workspace, the Transcript and data processing might be a better introduction to those with experience in more traditional languages. reply igouy 18 hours agorootparent\"Quick-UI-Tour\" https://github.com/Cuis-Smalltalk/Learning-Cuis/blob/master/... \"The Cuis Book\" https://cuis-smalltalk.github.io/TheCuisBook/ reply guessbest 6 hours agoparentprevHow would you compare this to say, writing WinForm applications in Visual Basic 6? reply fizfaz 19 hours agoparentprev> The sheer oddity + lack of real world example code floating around made it feel impenetrable not sure if I understand you, but you have the code of the whole system at your fingertips. Which is certainly \"real world\" because you are running it :) reply em-bee 19 hours agorootparentreal world example means end user applications, not developer tools. i want to see how an application looks like that my mother could use, or that i could sell to my customer. websites. desktop applications that hide the IDE... reply igouy 17 hours agorootparentYour applications would look however you wanted them look. https://pharo.org/success/ reply em-bee 14 hours agorootparentthat's not the question. we are looking for examples that come with source that can be studied. the success page lists 53 projects. only one of them came with a direct link to the source. one included an un-clickable link. one linked to a non-english website where i could figure out that it was licensed under the LGPL, but i could not find the link to the source. a surprise was that DrGeo which is known to be Free Software links to a dead website. grafoscopio which i also believe to be FOSS as well has a dead download link on its website. several other projects had dead links too. the only source i found was for HoneyGinger: https://github.com/tomooda/HoneyGinger OpenPonk https://github.com/OpenPonk and record: https://github.com/estebanlm/record (7 years old) btw, DrGeo is here: https://github.com/hilaire/drgeo that is three source examples under active development for a project as old and as large as pharo is that is surprisingly little. more accessible source examples are needed to attract developers. especially given the difficulty to get used to the pharo developer tools. i have actively explored working with pharo. i just could not find any useful apps that i could use and contribute to. and i had no ideas for an app that i'd be interested enough to create from scratch. for a while i even tried to use it as a desktop and used an app that provides a commandline inside pharo. the primary problem was that upgrading to a new version of pharo each year was difficult. given the image based development you tend to start with a current version of pharo and then keep to that version until you are done. reply igouy 11 hours agorootparent> i have actively explored working with pharo. Then you must already know more than me, about what's available now. Too much? https://github.com/feenkcom/gtoolkit > … given the image based development you tend to start with a current version of pharo and then keep to that version until you are done. I think of it as image based development: not image based version control. reply em-bee 8 hours agorootparentwhat do you mean by image based version control? i mean the lack of version control inside the image can be considered a problem, as you have to connect to external tools go get it, lest you save a copy of the image as a version (which is what i would call image based version control), which that is not practical at all. but that is not what i meant. i was talking about the problem that when i develop an application in pharo 11, but then i want to move the development to pharo 12, that amounts to a lot of work, so i don't do it but i'll stick to pharo 11 until my app is done. reply igouy 8 hours agorootparent> that amounts to a lot of work Why? Are you making a lot of changes that conflict with the distro? \"Guideline 120 Avoid modifying the existing behavior of base system classes.\" :-) 1996 Smalltalk with Style page 95 https://rmod-files.lille.inria.fr/FreeBooks/WithStyle/Smallt... reply em-bee 7 hours agorootparentit's not the code conflicts, but all the modifications i made to the environment. addons i installed, configurations i changed, windows i opened, code snippets i have in a workspace/playground. pharo is to much like a desktop, and switching to a new version of pharo is like reinstalling my computer and setting up my desktop from scratch. there is no tool that would just take every change i made to the original pharo image and apply it to the new one. you know like docker where your base image is immutable and changes to that image are saved in a separate image that is layered on top. so that you can replace the base image while keeping your changes. reply Qem 21 hours agoparentprevDid you try the MOOC? At first the environment may feel a bit confusing, but the videos in the MOOC help to walk through it, visually, and get familiar: https://mooc.pharo.org/ Also there's lots of free books at https://books.pharo.org/ reply 0x445442 21 hours agoparentprevLearning Rust is at least an order of magnitude harder than learning Smalltalk. This is not to dismiss what you have said. On the contrary, it highlights what must be an enormous miss in how Pharo presents documentation on its landing page. reply whartung 18 hours agorootparentRust the language is certainly more difficult than the Smalltalk language. The language isn't the issue. You have Smalltalk the language, which is this [] big, and Smalltalk the environment and class libraries, notably the GUI system, which is this [.....**.....] big. And, sure, you have all of the source code, but, for me, the source code may as well be organized in a stack of index cards. You get the individual methods, but not the sweeping picture. I can learn a lot more scanning a file full or source code, compared to the little snippets of code you're presented with screen by screen. Just being able to scroll and absorb is useful. But even then, especially being OO, with lots of abstraction, tracing through the GUI code, blind, is very difficult. You end up at top level, \"do nothing\" abstraction classes. Much like in Java, where everything you click on is an interface, which doesn't tell you a whole lot. Navigating a Smalltalk image is a skill all its own. reply cess11 18 hours agorootparentWhen I realised there was a difference between inspecting a class and an instance it became much easier to find the things I was looking for. reply selykg 18 hours agorootparentprevI don't think that's necessarily what the OP is saying. I agree with OP. Finding documentation to learn to use this absolutely obscure system is near impossible. I seen an announcement about a Pharo release a couple years ago and was like \"huh, that sounds cool.\" Proceded to download it and had no clue about anything. It is not at all like any other IDE. Learning it might not be hard, but when the IDE is absolutely different from anything else you've ever used, combined with very little documentation that speaks to how to do the basics, it can be a very mysterious and difficult thing. Rust is like any other programming language, in that you write code in a file, then compile it. Yes, there's other stuff to deal with in between and it can get way more complicated. But the IDE is the real culprit, combined with documentation for a developer to learn to develop real applications with, that makes Pharo infinitely more difficult in my personal opinion. I guess to simplify this a little with an edit. If you already know another programming language, learning Rust is not fundamentally different. Some parts of it will be difficult like the borrow system and stuff that's very Rust centric. But in most ways it behaves like a lot of other languages in terms of using it at a bare bones basic level. Pharo is ... otherworldly in that nothing else really compares, you have to learn a completely different paradigm for how to program it, and that is the difficult part imo. reply xkriva11 18 hours agorootparentThis is a complex issue. In fact, you can work with Pharo in a Unix-like manner. Use an external editor to modify the source code and run the image without any GUI. However, by doing so, you lose the most interesting and enriching aspects of it. The Pharo team has worked hard to integrate better with the world outside the Pharo image. People still complain that it is not like anything else they know, but if it were, they would miss the opportunity to learn how to do things differently and possibly better. There is also a lot of excellent documentation available, especially the MOOC, but it requires newcomers to invest some time. reply selykg 17 hours agorootparentThat totally makes sense. I'm not saying there aren't advantages to what is being done, but what is being done does add a level of complexity to it all. I think there is a valid reason for tools like Pharo to exist, I don't know the answer to how to improve the situation outside of a (selfish) desire to see better beginner documentation that talks about using it for people that have zero knowledge of Pharo and similar tools. Explain it like I'm 5 type stuff. I suspect it all becomes easier once the general primitives are explained but until they are there's an inscrutability to it. reply cess11 17 hours agorootparentprevWhen it starts up there's a welcome-window, in it or like a few clicks down there's a tutorial called ProfStef that gives a quick tour through some data types and ways to execute code. I think that's a pretty neat introduction to the very basic basics. reply selykg 17 hours agorootparentInteresting, I'll have to take a look at that when I have some time. I may have missed that the first time around. Thanks! reply cess11 15 hours agorootparentProbably should have mentioned that it's interactive, and invites experimenting with the content. reply agumonkey 17 hours agoparentprevWithout an online course or a mentor I'd be lost too. But you only need a two hour introduction to have fun IMO. reply DeathArrow 21 hours agoparentprevYou can try to look at Rosetta Code if you want examples. reply cess11 18 hours agoparentprevOthers have linked to the book collection, there you'll find walkthroughs of the basics, building web applications, tracking code in git, and so on. A lot of the common packages also have examples bundled that you can play around with and read documentation in. Once one gets over the initial hurdle of the syntax, message passing and how to use the GUI to create new projects and objects one can usually just surf around in the image and look for examples to figure things out. reply znpy 19 hours agoparentprevI had the same experience. Spent a couple of (vacation) weeks devoting 2-3 hours a day to Pharo and going through all of one of the books from the website (Pharo by example? can't really remember). It's really one of those things that will die out on their own, because of the many layers and the little applicability (outside of playing the gimmicks). reply beefnugs 8 hours agoprevWithout ever practically using this system, my first impression is that this is how mech operating systems would have to be. At a certain complexity of vehicle, you would need all of: manufacturer base features/diagnostics, safe-mode control, an open market of source code to add in, pilot preferences, and then real nerd stuff where you customize in your own routines. All the while being able to explore a live running system as it is running without having to trust closed binary only software. Of course the world we live in would never go this way, proprietary unknowable, pre-government hacked, unrepairable trash it is for us. reply throwaway918274 17 hours agoprev7 minutes of Pharo smalltalk for Rubyists: https://youtu.be/HOuZyOKa91o Is a pretty good overview how the environment works. reply ejflick 6 hours agoprevI check every release hoping that they finally fixed rendering for HiDPI. Guess I'll have to keep waiting. reply isr 1 hour agoparentYou want cuis smalltalk. It's a heavily-simplified smalltalk (like pharo, it was also forked from squeak), with a redesigned morphic system - which is ENTIRELY vector graphics based (plus a few other niceties). Yes, even the fonts themselves are vector graphics. You can use a fancy cursive font, rotate a text window, and then zoom it in or out - with total clarity. reply xkriva11 1 hour agoparentprevIn Settings Browser, check \"Set canvas scale factor automatically\" reply em-bee 5 hours agoparentprevi ran into this problem too. i was able to make it mostly work by changing the font size. reply Decabytes 22 hours agoprevHow does Pharo compare to Squeak? I'm interested in doing more with Smalltalks but Pharo just does not play nice on Fedora 40 with Wayland right now reply xkriva11 17 hours agoparentCuis [1] is a simplified fork of Squeak, trying to make it more closely resemble the original Smalltalk-80 (while using a custom version of Morphic). Squeak itself initially tried to not be bound by Smalltalk-80 heritage. Now, it focuses on preserving of the experiments that Alan Kay, Dan Ingalls, and others did, being very conservative in this regard. Pharo forked to have a system that can actually evolve. Pharo should generally work on Wayland, so there will be more things going on, probably. [1] https://cuis.st/ reply orthoxerox 22 hours agoprevIs not calling it a Smalltalk implementation still a marketing decision, or has Pharo diverged sufficiently from Smalltalk-80 to become incompatible? reply xkriva11 22 hours agoparent+Traits, +Slots, different class definition syntax and its processing, different file API, full block closures, different source file syntax, ByteArray literals, different comments syntax, many differences in the standard and UI library, etc. No, Pharo is not Smalltalk-80. reply igouy 17 hours agorootparentI think incompatible Smalltalk implementations were one of the bigger barriers to Smalltalk adoption. In contrast, Java standard libraries have been a huge benefit. reply pjmlp 22 hours agoparentprevOne thing that comes to mind is that Smalltalk-80 doesn't do traits, or the UI is completly different. reply andsoitis 19 hours agoprevPharo features - https://www.pharo.org/features reply sigzero 13 hours agoprevSmalltalk is one of those \"I'd like to learn\" but haven't found a good resource that I like (oh and the time, sigh). reply whartung 10 hours agoprevFirst, congrats to the Pharo team, I mean they do a lot of work. But a singular testament of Pharo is that it this amazing environment, with all this heritage, tickling many of the more popular \"CS folk\" buttons. It's an active, busy project, with lots of committers. And yet, \"nobody\" uses it. The Pharo folks live in their sandbox, eating their dog food, making Pharo more Pharo than ever, but it seems to only be uplifting folks making Pharo. Pharo is built for Pharo makers to make Pharo, and they continue make Pharo a better place for them to live and do their work. I'm certainly not going say that its because of X or Y or Z. Just that, it \"is\". It is \"not used\" in the large. Sure, folks use it, they have their success stories like any project does to some extent. But the larger \"hive mind\" of the \"internet\" hasn't seemed to have caught on, or have tasted it and moved on to something else. So, at 30,000 feet, despite all their work, something is not quite clicking. reply lolinder 10 hours agoparent> The Pharo folks live in their sandbox, eating their dog food, making Pharo more Pharo than ever, but it seems to only be uplifting folks making Pharo. Some of this is just down to the philosophy of Smalltalks. To program in a Smalltalk is to make Smalltalk. You can't really separate the creation of the system from the creation of the application(s). With that in mind, it's hardly surprising that few people who aren't contributing to Pharo use Pharo—it may not be just that it hasn't found mainstream appeal (which is true) but also that most people who pick it up and start using it seriously end up becoming contributors. reply dmpk2k 21 hours agoprevIs any thought being put into adding parallelism in the future? reply batman-farts 13 hours agoparenthttps://github.com/smarr/RoarVM is the main thing that comes up when trying to learn about this, and it hasn't been touched in over 10 years. I have also seen people working on spawning child VMs to handle parallelism, though unfortunately I don't have links to those discussions immediately at hand. It seems like perhaps an unnecessarily heavyweight approach, too, not a canonical solution that you would want to use in production. I assume that step 1 towards parallelism, at least on the image side, would be going through the class library and making sure everything is thread-safe. I'd love to know where one would even get started with that effort. The Roar project claims to support Pharo 1.2, which doesn't seem to be very far after they forked from Squeak, but obviously a lot has changed since then. And the challenge is that Pharo is still rapidly developing all the overhauled classes that distinguish it from Smalltalk-80. Meanwhile, if I want to play with parallel image/REPL-based programming, I can go over to Common Lisp and, while lacking an equally coherent GUI, be able to load up bordeaux-threads and off I go. reply andsoitis 20 hours agoparentprevNo, but you can do concurrent programming in Pharo: https://books.pharo.org/booklet-ConcurrentProgramming/pdf/Co... reply ginko 20 hours agorootparentSo the entire dev environment and any programs you have running in it only uses a single thread? reply andsoitis 19 hours agorootparentPharo uses green threads https://www.pharo.org/features https://en.wikipedia.org/wiki/Green_thread reply packetlost 18 hours agorootparentSo does it use symmetric multiprocessing at all? reply pjmlp 17 hours agorootparentGreen threads only mean a M:N mapping to OS threads. reply packetlost 17 hours agorootparentRight, is the N (OS threads) == 1 in this case? It's a specific, technical way of asking if it can run parallel workloads across modern multi-core CPUs. reply rscho 15 hours agorootparentAs far as I understand it, no parallelism at all in Pharo itself. reply ginko 18 hours agorootparentprevThat feels very limiting. It's one thing to have a program run only on one core, but you're supposed to have your entire dev environment, including editor and debugger inside the VM. Won't that end up with the env freezing on compute heavy tasks? reply pjmlp 17 hours agorootparentThe runtime does the mapping, it only means IS threads aren't directly exposed. reply 0x445442 15 hours agorootparentprevYes, you can lock the UI quite easily if you're not careful. reply peter_d_sherman 22 hours agoprevRelated: Pharo Syntax in a Nutshell: http://rmod-pharo-mooc.lille.inria.fr/MOOC/PharoMOOC/Week1/C... https://en.wikipedia.org/wiki/Pharo >\"Pharo is an open source, cross-platform implementation of the classic Smalltalk-80 programming language and runtime.[3]\" https://en.wikipedia.org/wiki/Smalltalk reply anilakar 21 hours agoparentSyntax is the easy part. The real difficulty is in trying to make sense of the whole ecosystem, including the strange runtime-IDE-hybrid approach. reply twixfel 20 hours agorootparentYes last time I checked they had their own versioning system as well... it's one thing to learn a language but having to learn a whole new set of tools put me off in the end. I guess I can see the argument that the Smalltalk files are not plain text but rather images (or something), but having to basically download a second OS within the OS just to write Hello World was off putting. I realise that that's also can make Smalltalk so powerful, at least in principle. Also lack of retina display support put me off. Of course it is purely cosmetic but when it's literally the only app I used that looks like shit, it is sufficient to nudge me away. Maybe fixed now, not sure. reply andsoitis 18 hours agorootparent> Yes last time I checked they had their own versioning system as well... Using git with Pharo: https://books.pharo.org/booklet-ManageCode/pdf/2020-05-12-Ma... reply igouy 15 hours agorootparentprev> to learn a whole new set of tools The tools are how we find functionality that can be re-used and re-purposed to do what we need. A lot of exploring and reading existing stuff, less writing new stuff. > Smalltalk files are not plain text but rather images (or something) Mostly Smalltalk files are plain text files! There's a plain text log file with a replayable record of what you've been doing. There's a sources file with the source code. There are plain text file outs and change sets. There's the VM like the JVM — not a plain text file. There's the image, a cache of byte code (like Java .class files, Python .pyc files) and application state — not a plain text file. With a previous Pharo version: $ bin/pharo --headless Pharo10-SNAPSHOT-64bit-502addc.image hello.st pharo is the VM. Pharo10-SNAPSHOT-64bit-502addc.image is the image. hello.st is a plain text file. $ cat hello.st Stdio stdout nextPutAll: 'hello world'; nextPut: Character lf.! SmalltalkImage current snapshot: false andQuit: true! reply twixfel 12 hours agorootparentThat's fine, I understand that a lot of my criticisms are arguably actually advantages, but after doing the MOOC about 8 years ago I just couldn't stick with it. Fun though, and a great way to really learn OOP at the time when I was just starting out programming. reply igouy 12 hours agorootparentThat's fine, people have some strange ideas about Smalltalk, both +ve & -ve. reply alexisread 11 hours agorootparentprevFunnily enough, I'd compare the image and versioning differences to managing Postgres rather than non-image based languages. Postgres has to deal with images (backups) and versions (migrations). Maybe selling image-based systems in both cases needs a little work or formalisation (best practices and all that)? Image-based systems seem more about data management rather than code management. Just some random thoughts reply igouy 7 hours agorootparentIn the beginning source code in text files :-) \"Within each project, a set of changes you make to class descriptions is maintained. … Using a browser view of this set of changes, you can find out what you have been doing. Also, you can use the set of changes to create an external file containing descriptions of the modifications you have made to the system so that you can share your work with other users.\" 1984 \"Smalltalk-80 The Interactive Programming Environment\" page 46 \"At the outset of a project involving two or more programmers: Do assign a member of the team to be the version manager. … The responsibilities of the version manager consist of collecting and cataloging code files submitted by all members of the team, periodically building a new system image incorporating all submitted code files, and releasing the image for use by the team. The version manager stores the current release and all code files for that release in a central place, allowing team members read access, and disallowing write access for anyone except the version manager.\" 1984 \"Smalltalk-80 The Interactive Programming Environment\" page 500 https://rmod-files.lille.inria.fr/FreeBooks/TheInteractivePr... reply elviejo 21 hours agorootparentprevJust imagine that is: - docker image - vscode preibstalled - and everything used the same programming language. reply john1203 14 hours agoprevIt seems too research-oriented programming IDE+language, where there is not a big market compared to other areas. Who does Pharo want to compete with? Maybe developers think they shouldn't compete with other platforms? With the amount of real Pharo apps right now even on GitHub, it would be extremely difficult to \"sell\" Pharo to any decision maker. reply cess11 13 hours agoparentWhy would that be hard? Once you're fluent in it development in Pharo is very, very fast, you can build a GUI application while a prospect or customer is watching and supplying inputs about what they need. Many companies that buy software don't care about programming languages, they care about functionality and price. With Pharo you can show them immediately what the application could look like and how their processes could be implemented. Fast development means you can sell at a lower price and reach customers that can't afford more 'classic enterprise'. You can also reach customers that do work in quickly changing regulatory environments, like insurance. When you've banged out the frontend and some logic you return to your office and figure out things like database schema. Pharo is specifically designed for commercial and industrial applications and not as a research environment. Partnering with private capital has been one source of funding for the project. reply krmboya 17 hours agoprevMany comments in the vein that pharo is weird and different from what they're used to. I'd say programming is such a young field programmers should dedicate time to try out the weird stuff that they aren't used to, opportunities to rewire your brain for better. Let's not get stuck in a local minima on how to do programming. There could be better ways not yet popular enough. Smalltalk came out of an environment of innovations that were ahead of their time. EDIT: while doing the Pharo mooc, I was able to create a DSL for what I understand to be D&D dice representation without using special metaprogramming syntax of the language or parsing and tokenizing strings. Just plain old OOP of the Smalltalk/Pharo flavour. This were just basics btw reply igouy 16 hours agoparentSmalltalk had a moment at the transition from green screen to PC's, fumbled the everything is a string internet, and never showed-up for the multicore + GPU party. Meanwhile the nasty statically type checked languages lowered the pain level with non-nullable and type inference for local vars. reply cess11 14 hours agorootparentPretty sure Pharo has had GPU-support for a long time, and there's RoarVM for \"manycore\" applications in Smalltalk. reply igouy 13 hours agorootparent> Pharo has had GPU-support for a long time For highly parallel general-purpose computations? Tell us more! https://chapel-lang.org/docs/technotes/gpu.html As-already stated in this discussion RoarVM \"hasn't been touched in over 10 years\". This discussion is about the Pharo 12 release and supposedly RoarVM is \"compatible with Squeak 4.1 and Pharo 1.2 \". https://github.com/smarr/RoarVM reply cess11 4 hours agorootparentKinda weird to move the goal posts like this. reply kingspact 16 hours agoparentprevI don't like Pharo because they changed it too radically from Squeak, which I DO like very much. reply wk_end 14 hours agorootparentAs a curious outsider - what are the key differences between Squeak and Pharo that make it worse in your eyes? reply kingspact 10 hours agorootparentThey removed a lot of the capabilities of Squeak and most of the nice legacy code included for educational and historical purposes, and neutered the object inspection tools. I understand that Pharo is no longer Squeak and the whole idea was to simply take Squeak and change not just the UI but the actual language. But I'm just a Squeak person. reply em-bee 10 hours agorootparentactually, from pharo's perspective the goal was to remove bloat, things that didn't help active new development. as a result pharo images were smaller, and didn't have to support lots of old legacy code or objects. another problem that squeak had was that it was built on top of old images, containing objects that date back to the beginning of squeaks history, and possibly even older than that, and because of that it was (at least at the time) not possible to verify the ownership and license of all the source that the image was made of. for some objects the source was even lost altogether. this made squeak incompatible with FOSS licenses and it was a reason why it was not included in linux distributions. which is one factor that limited its spread among developers. one of pharo's goals was to remove those old objects and unverified source and make it possible to build new images purely from a verifiable source. from a certain perspective, the inclusion of those old objects creates a certain sense of awe, considering who were the people that worked on smalltalk that created these objects, whereas pharo in that respect feels more sterile. reply anothername12 14 hours agoprevMan the responses here are most interesting. So many people in hacker news complaining about its “unconventional” approach, the perceived inconveniences of it, or how it wouldn’t be useful in corporate environments. It’s like listening to a bunch on middle managers deciding on the tech platform rather than bunch of hackers contemplating the possibilities. reply pvg 14 hours agoparentPlease don't sneer, including at the rest of the community. https://news.ycombinator.com/newsguidelines.html reply germandiago 19 hours agoprev [–] Great idea but not pragmatic enough. I did try. reply tasuki 19 hours agoparent [–] In what way is it not pragmatic? I don't know Pharo at all, but \"not pragmatic\" often means just \"too different from what I'm used to\"... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pharo 12, a user-oriented programming language focusing on simplicity and immediate feedback, has been launched with a live environment and exceptional debugging capabilities.",
      "Key highlights of version 12 encompass a new breakpoint system, tools enhancements, and advancements in the system and virtual machine.",
      "The release, backed by 1895 Pull Requests and contributions from 70+ contributors worldwide, showcases Pharo's open-source nature and community-driven development."
    ],
    "commentSummary": [
      "Users discuss the challenges and benefits of working with the Smalltalk programming language Pharo, noting difficulties in understanding its unconventional nature and lack of examples.",
      "Suggestions for improving the Pharo learning experience include attending conferences, using community resources like Discord, and creating more educational materials.",
      "The conversation covers the importance of better documentation, enhanced user interface, and the potential obstacles of implementing parallelism in Pharo, while also comparing it to other Smalltalk systems like Squeak and Cuis."
    ],
    "points": 217,
    "commentCount": 99,
    "retryCount": 0,
    "time": 1714128065
  },
  {
    "id": 40174424,
    "title": "Court upholds New York $15 broadband law",
    "originLink": "https://arstechnica.com/tech-policy/2024/04/court-upholds-new-york-law-that-says-isps-must-offer-15-broadband/",
    "originBody": "No preemption — Court upholds New York law that says ISPs must offer $15 broadband New York obtains significant win for states' ability to regulate broadband. Jon Brodkin - 4/26/2024, 9:10 PM Enlarge Getty ImagesCreativeye99 reader comments 48 A federal appeals court today reversed a ruling that prevented New York from enforcing a law requiring Internet service providers to sell $15 broadband plans to low-income consumers. The ruling is a loss for six trade groups that represent ISPs, although it isn't clear right now whether the law will be enforced. New York's Affordable Broadband Act (ABA) was blocked in June 2021 by a US District Court judge who ruled that the state law is rate regulation and preempted by federal law. Today, the US Court of Appeals for the 2nd Circuit reversed the ruling and vacated the permanent injunction that barred enforcement of the state law. For consumers who qualify for means-tested government benefits, the state law requires ISPs to offer \"broadband at no more than $15 per month for service of 25Mbps, or $20 per month for high-speed service of 200Mbps,\" the ruling noted. The law allows for price increases every few years and makes exemptions available to ISPs with fewer than 20,000 customers. \"First, the ABA is not field-preempted by the Communications Act of 1934 (as amended by the Telecommunications Act of 1996), because the Act does not establish a framework of rate regulation that is sufficiently comprehensive to imply that Congress intended to exclude the states from entering the field,\" a panel of appeals court judges stated in a 2-1 opinion. Trade groups claimed the state law is preempted by former Federal Communications Commission Chairman Ajit Pai's repeal of net neutrality rules. Pai's repeal placed ISPs under the more forgiving Title I regulatory framework instead of the common-carrier framework in Title II of the Communications Act. 2nd Circuit judges did not find this argument convincing: Second, the ABA is not conflict-preempted by the Federal Communications Commission's 2018 order classifying broadband as an information service. That order stripped the agency of its authority to regulate the rates charged for broadband Internet, and a federal agency cannot exclude states from regulating in an area where the agency itself lacks regulatory authority. Accordingly, we REVERSE the judgment of the district court and VACATE the permanent injunction. Advertisement Be careful what you lobby for The judges' reasoning is similar to what a different appeals court said in 2019 when it rejected Pai's attempt to preempt all state net neutrality laws. In that case, the US Court of Appeals for the District of Columbia Circuit said that \"in any area where the Commission lacks the authority to regulate, it equally lacks the power to preempt state law.\" In a related case, ISPs were unable to block a California net neutrality law. Several of the trade groups that sued New York \"vociferously lobbied the FCC to classify broadband Internet as a Title I service in order to prevent the FCC from having the authority to regulate them,\" today's 2nd Circuit ruling said. \"At that time, Supreme Court precedent was already clear that when a federal agency lacks the power to regulate, it also lacks the power to preempt. The Plaintiffs now ask us to save them from the foreseeable legal consequences of their own strategic decisions. We cannot.\" Judges noted that there are several options for ISPs to try to avoid regulation: If they believe a requirement to provide Internet to low-income families at a reduced price is unfair or misguided, they have several pathways available to them. They could take it up with the New York State Legislature. They could ask Congress to change the scope of the FCC's Title I authority under the Communications Act. They could ask the FCC to revisit its classification decision, as it has done several times before But they cannot ask this Court to distort well-established principles of administrative law and federalism to strike down a state law they do not like. Coincidentally, the 2nd Circuit issued its opinion one day after current FCC leadership reclassified broadband again in order to restore net neutrality rules. ISPs might now have a better case for preempting the New York law. The FCC itself won't necessarily try to preempt New York's law, but the agency's net neutrality order does specifically reject rate regulation at the federal level. Page: 1 2 Next → reader comments 48 Jon Brodkin Jon has been a reporter for Ars Technica since 2011 and covers a wide array of telecom and tech policy topics. Jon graduated from Boston University with a degree in journalism and has been a full-time journalist for over 20 years. Advertisement Channel Ars Technica SITREP: F-16 replacement search a signal of F-35 fail? Footage courtesy of Dvids, Boeing, and The United States Navy. SITREP: F-16 replacement search a signal of F-35 fail? Sitrep: Boeing 707 The F-35's next tech upgrade US Navy Gets an Italian Accent SITREP: DOD Resets Ballistic Missile Interceptor program SITREP: DOD's New Long-Range Air-to-Air Missile Aims to \"Outstick\" China Army's New Pistol Has Had Some Misfires Army's Next (Vertical) Lift En Route SITREP: President Trump's Missile Defense Strategy Hybrid Options for US's Next Top Fighter The Air Force’s Senior Citizen Chopper Can’t Retire Yet Ars Live #23: The History and Future of Tech Law Police re-creation of body camera evidence - Pueblo, COArs Technica Visual Labs body camera software with the Dos Palos PDArs Technica He knew his rights; he got tased anyway More videos ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=40174424",
    "commentBody": "Court upholds New York law that says ISPs must offer $15 broadband (arstechnica.com)213 points by nateb2022 12 hours agohidepastfavorite179 comments infecto 12 hours agoSerious question here. Is this law more or less stating that the business gets no revenue back from local or state government? I have no issue with the argument that internet service is a basic right but if that is the case, I would expect the government to either offer the service or pay a market/agreed price to cover the cost for low income users. reply singlow 12 hours agoparentI think the logic here is that the government has already subsidized them by granting privileges to use public infrastrucure that was built with tax dollars. There can only be so many wires running on the poles or under the streets so in return for that privilege, you have to provide this in return. reply hughesjj 9 hours agorootparentNot even granting public infra, we straight up paid BILLIONS for them to expand broadband coverage and the ISP's didn't do jack most of the time https://www.huffpost.com/entry/verizon-pennsylvanias-com_b_7... > By the end of 2003, Teletruth estimates that every household in Pennsylvania has paid in excess of $1,135.00 for a fiber optic service they will never get. Teletruth estimates that the total overcharging to be $3.9 billion or more in excess profits, tax deductions, and other financial perks, including funding other business ventures through cross-subsidization. And it's far from just PA https://newnetworks.com/bookbrokenpromises/ reply cogman10 11 hours agorootparentprevThis is where I think we are all wrong. In the UK, the lines are owned by the government and ISPs/telcos rent access to provide service. This allows for pretty strong competition from the ISPs and stops there from being a billion dead lines buried underground. To me, government owned lines and connections makes the most sense for society. I think that's also a big part of why the US train system sucks. Private entities generally don't want to own that infrastructure, they want the trains that can haul the goods. So make them rent the line and have the government manage access just like they do with airplanes and roads. reply matt-p 10 hours agorootparentThat is not, and has never been, true. BT was privatized in 1984. Over the years since then, they've been forced by regulators to open up more and more of the stack to competing ISPs. At one point, you could buy wholesale services like ISDN lines or p2p E1 circuits. Later, ISPs could rent space in the telephone exchanges and individual copper wires. In the current FTTP world, they can either rent a layer 2 service or deploy their own fiber in BT ducts and pay a few pence per meter rental. reply Latty 11 minutes agorootparentNever forget that BT was rolling out fibre across the country when they got privatised, had production in the UK running. The public BT foresaw the need for fibre and the cost of copper in the 70s, and was preparing, once Thatcher sold them off and tried to woo American ISPs that couldn't offer an equivalent, they sold it all off on the cheap and communication infrastructure in the UK was set back decades. The full story: https://www.techradar.com/news/world-of-tech/how-the-uk-lost... reply cogman10 9 hours agorootparentprevUpvoted. I thought BT was owned by the government (particularly because every ISP ends up renting from them). I still think that a centralized network that is rented out ends up working better than one that's deployed by different ISPs. reply matt-p 9 hours agorootparentJust to be clear, I agree. Despite its faults, I still think we are quite fortunate to have BT Openreach in its current form as a country. I can understand that perception. Essentially, Openreach is regulated so heavily by the government that it might as well be considered public, from the perspective of 'what services it must offer, on what basis, to whom, and the prices it must set.' However, it is a private company, being a 'firewalled' part of BT reply hedora 9 hours agorootparentThis type of system is called “common carrier”. (The firewalled part of BT acts as a common carrier). Because of the way physics and fiber optics work, having a common carrier that rents out access to wavelengths or bandwidth on the fiber strands is the best economic model, by far. reply matt-p 8 hours agorootparentI disagree since BT/OR * don't rent out fibre or wavelengths. They rent out layer 2 vlans (they own the OLT at the exchange and the terminal in people's homes) this means they set the package speeds, price points and use whatever technology they like. For that reason many ISPs have decided to instead rent duct space from BT/OR and each install thier own fibre. That is not really a common carrier, more like common duct. *except in very rural areas for highly specific use cases, or in the case of wavelengths for medium range high capacity backhaul. reply robertlagrant 3 hours agorootparentAlso, lots of parallel fibre was laid by NtL and friends in the early 2000s, which is the useful stuff. I think Virgin Media bought it all, and that's what they offer as their broadband. reply mnw21cam 1 hour agorootparentFun fact - if you try to purchase broadband from a service that runs over BT infrastructure, the price you'll be charged will vary depending on whether there is also a Virgin Media service that would also be available to you. As in, if there's no competition, you'll be charged more. That's price gouging, and I think it should be illegal. reply matt-p 42 minutes agorootparentThat's not true either! Market 1/2/3 has been dead for well over a decade. Openreach and BTW pricing is consistent across the whole country. reply multjoy 2 hours agorootparentprevThere's currently a lot of non-BT fibre rollout taking place. Mostly in cities (Hyperoptic et al), as well as B4RN and similar non-profits infilling the rural locations. reply WarOnPrivacy 8 hours agorootparentprev> they've been forced by regulators to open up more and more of the stack to competing ISPs. The 1996 US Telecom act did this and all kinds of local ISPs popped up to compete. We suddenly had cheap DSL all over the place; people left dial-up in droves. But citizens don't write laws, lobbyists do. Within 10 years, ISPs had clawed back all their monopolistic control. Now most of the US is under a monopoly/duopoly again. reply matt-p 8 hours agorootparentBT also employ lobbiests, and there's regular controversy in the industry over ofcom not regulating them hard enough, or in why exactly they won the original 2010 era rural fibre contracts but overall I guess they simply don't work as hard as thier ATT counterparts;) reply WarOnPrivacy 7 hours agorootparent> I guess they simply don't work as hard as thier ATT counterparts;) Since at least the 1950s, AT&T has been so tightly tied to US Gov+NatSec, it's practically another agency. To illustrate that: AT&T once faced a lawsuit for aiding US Gov's unconstitutional, warrantless wiretapping (also for cloning backbone traffic for the NSA). When Congress presented a bill to grant retroactive immunity to AT&T, both Obama and Clinton paused their presidential campaigns. They flew back to DC to cast their Yea votes and protect AT&T from any possible accountability. US news orgs weren't able to spot anything out of the ordinary in that. reply smcin 6 hours agorootparent> When Congress presented a bill to grant retroactive immunity to AT&T, both Obama and Clinton paused their presidential campaigns. They flew back to DC to cast their Yea votes and protect AT&T from any possible accountability. Your facts are a little scrambled: are you referring to 2008 [0] or 2012 [1][2] or both? I guess you meant 2008 because both Hillary and Obama were running for President; but Hillary was absent from the 2/12/2008 vote, [2] claims Obama voted against it. I located the bill: [3] \"S.2248 - FISA Amendments Act of 2008\" and that Senate vote record from 2/12/2008, although it says \"Clinton (D-NY), Not Voting\", \"Obama (D-IL), Not Voting\"(?). You'd expect journalists to get important facts right. Meanwhile US media barely covered that vote. [0] 2/2008 \"Telecom immunity remains intact as Democrats split on vote\" \"Obama voted with 30 fellow Democrat Senators, but not the absent Hillary Clinton, to allow the telecom companies to face lawsuits... on the [Bush] admin's programme of warrantless wiretapping. But the immunity survived, with 18 Democrats crossing over to support George Bush.\" https://www.theguardian.com/politics/2008/feb/12/terrorism.u... [1] \"US Supreme Court finalizes gift of immunity to the telecom giants\" - Glenn Greenwald https://www.theguardian.com/commentisfree/2012/oct/10/suprem... [2] TheAtlantic: \"The Supreme Court Isn't Bothered By the NSA’s Warrantless Wiretapping\" ... \"refused to hear a case that holds telecom companies accountable for letting the warrantless NSA spying\" https://www.theatlantic.com/politics/archive/2012/10/supreme... [3]: 02/12/2008 \"S.2248 - FISA Amendments Act of 2008\", click through on \"14 roll call votes\" https://www.congress.gov/bill/110th-congress/senate-bill/224... reply smcin 6 hours agorootparentLook on the bright side, the AT&T logo already looks like the eye-in-the-sky. Or the Death Star. Hardly needs any reworking. reply idle_zealot 11 hours agorootparentprevIf the government owns the lines, what are private telcos... actually doing? Operating the webpage that lets you put in a credit card to pay for service? reply gregmac 8 hours agorootparentDon't know how it works, but I do think that's how it should be. Municipality providers dark fiber from the CO(s) to every house. From there, any ISP can rent rack space, and arrange their own backbone/uplink, and get wired to whatever customers they can. Municipality charges a fee to use the fiber, and that's it. Muni provides the line to the CO, ISP is responsible after that. They can compete on their internet speed, price, support, and whatever else they want (Netflix box, email, low latency to game servers). Muni could even add a \"no-internet\" free service that just provides access to city services, library, etc. reply kube-system 7 hours agorootparentIt is worth noting that in the US, there's a not-insignificant part of the country that isn't in a municipality. reply WarOnPrivacy 7 hours agorootparentprev> I do think that's how it should be. Municipality providers dark fiber from the CO(s) to every house. We have a variant of that here. An indy company just laid fiber here but they don't provide service. Instead I have a choice of 8+ competing fiber companies. Before fiber, I had 1 'choice' of ISP and paid $124/mo for 1Gb/40Mb w/ 40ms latency to the IX. Now my choices include 1Gb/1Gb $49, 2Gb/2Gb $79, 10Gb/10Gb $199 w/ 4ms latency to the IX. reply storyinmemo 10 hours agorootparentprevThe line is a piece of copper or glass that ends in a port in a rack at central point. The ISP is providing the connection from that point in the building to their equipment that does switching, routing, and yes billing you. reply AnthonyMouse 9 hours agorootparentprevLast mile ISPs bring your traffic between your house and an internet exchange somewhere in the nearest major city. Transit ISPs bring it from there to some other internet exchange in a different city where the opposite endpoint is. The premise here is that the government would handle the physical part of the last mile and then customers would either buy \"internet service\" from a transit ISP or from someone reselling transit because the transit provider doesn't want to deal with customer support and retail billing. reply Affric 10 hours agorootparentprevIn Australia the government a private company owned by the government largely owns the last mile (except where competitors built fibre first). All the backhaul between \"points of interconnect\" is owned privately. As a user though you pay for bandwidth at the interchange because that's how the old privatised and divested government monopoly did it. The model, last mile public utility but all the speed private wouldn't be all that awful if it weren't butchered by various vested interests. In theory the current last mile company will be sold once it becomes profitable. reply throw0101d 8 hours agorootparentprev> If the government owns the lines, what are private telcos... actually doing? Providing Layer 3. Just like the government owns the roads and highways, and lets UPS and FedEx use them for delivery. The theory is that roads/fibre are natural monopolies, but the services that use them can be competitive. There just needs to be a fee for the upkeep of the infrastructure (but it doesn't necessarily need to make a (hufe) profit). reply alexriddle 11 hours agorootparentprevSome operators do have their own infrastructure and most large ISPs have equipment in local exchanges and operate their own backhaul. Lots of it is just about marketing though. reply Wowfunhappy 10 hours agorootparent> Lots of it is just about marketing though. But in that case, doesn't it make more sense for the government to just cut out the middleman and offer internet to consumers directly? What is the company's value-add? reply hedora 8 hours agorootparentThe ISP company offers customer support, installation services and the fiber modem, and owns the equipment in the network closet that peers with the backbone. They might even own the peering network closet (which is often a weatherproof enclosure with a backup generator). reply 6510 5 hours agorootparentprevYES, roads, water, sewage, electricity, it could all work with this formula. It could all be added to the long list of things you pay for that are build and maintained by taxes. road service providers, water service providers, sewage service providers, electric service providers etc Picture it! reply AndrewDucker 2 hours agorootparentprevThat's not true. The majority of British lines are owned by OpenReach, which is a subsidiary of BT, which used to be run by the government, but was privatised back in the 80s. But they have not been owned by the government in decades, and there are competitors who have run their own lines. reply metalspoon 7 hours agorootparentprevThis is cool, but the government wouldn't be keen on updating the cables, would they? You'd be locked in to whatever cable the local government can afford. reply WarOnPrivacy 7 hours agorootparent> This is cool, but the government wouldn't be keen on updating the cables, would they? They charge ISPs for use of the infrastructure and that funds maintenance and upgrades. For the sort of core upgrades that occur every few decades, they can do a bond referendum to cover shortfalls. It's the same setup our private infra does here. They own the fiber in the ground and ISPs pay then to sell services. It can be harder for a private infra company to get in the ground because they're limited to municipalities that aren't captured by telecom/ISP lobbying orgs - but still have a dense enough population. reply admax88qqq 6 hours agorootparentprev> To me, government owned lines and connections makes the most sense for society. I used to think that but now I'm not sure I still believe it. I worry that government owned lines doesn't result in \"everyone gets fiber to the home now\" but rather \"nobody gets fiber yet cause we're still getting cable in Internet to the last X rural users.\" reply wredue 6 hours agorootparentWhy would it mean that? I fear that private IsPs will collude to keep internet slow and prices high, while never delivering to rural. reply tgsovlerkhgsel 10 hours agorootparentprevYou don't need it to be government owned. While that is certainly one effective way of doing it, another can be to let whoever wants to build it build it, but require them to give access to competitors for a regulated fee. reply dools 9 hours agorootparentOh yeah that’s bound to be as equitable as having the government provide it! reply matt-p 8 hours agorootparentIt depends how empowered your regulators are. In many European countries that's how it works and it is literally a better system than the government doing it themselves. reply WillAdams 11 hours agorootparentprevTell that to the judge who oversaw AT&T's breakup. reply legitster 11 hours agorootparentprevI make this point all of the time. In the US, the rails are private and the government owns the passenger service. In Europe, the government owns the rails and the passenger service is private. People try to frame these issues as \"socialist/capitalist\" or some nonsense all of the time. But the reality is that neither system is more or less capitalist than the other. It's just one is better structured than the other. However, having lived in a town in the US that has had a public ISP for decades, it was abhorrent. Municipalities only care about cost cutting - so there was no investment in wiring infrastructure or customer service. So I think there is an argument to be made that we may take for granted a bit the difference between rails/roads and fiber optics. reply pseudalopex 10 hours agorootparent> Municipalities only care about cost cutting - so there was no investment in wiring infrastructure or customer service. So I think there is an argument to be made that we may take for granted a bit the difference between rails/roads and fiber optics. Municipal ISPs and ISPs leasing municipal infrastructure dominated PCMag's speed ratings until they excluded local ISPs. Many have excellent satisfaction ratings. Large private ISPs consistently have some of the worst of any business. reply cogman10 11 hours agorootparentprev> Municipalities only care about cost cutting - so there was no investment in wiring infrastructure or customer service. Well, there's two aspects at play here. For starters, you do need to be a certain size as an ISP to really offer cheap services. Imagine, for example, being a town of 500 with a municipal ISP. You'd end up with a lot of equipment to store and maintain and to really be effective, you'd have to employ a bunch of people that spend most of their time doing nothing. (After all, how often would the customer service rep for a small town ACTUALLY be answering any calls?). That's why a state, county, or national? run ISP would probably make more sense, especially in more rural locations. That being said, in the UK with BT and the line rentals I described earlier those worked unreasonably well. ISPs could complain to BT to fix problems which ultimately left the various ISPs in a region to compete on tech and price. That was a win for the customer. reply legitster 10 hours agorootparentThis was not at all a small public ISP. And it was managed by the city power company, so it had the economies of scale already. There's a lot of nitty gritty policy weeds of what went wrong. But the main one was that after the public ISP incorporated in the 90s as a complimentary service to a public cable network, the state passed a legislation regulating public utility rates. Good for power/water/sewer/cable customers. But it meant that the city could not charge customers differently even with wildly different internet speeds. Going into the 90s, as the internet aspects became more important, they couldn't keep up with upgrading the lines, especially given the costs would be born by all customers regardless of what speed they were getting. reply peteradio 10 hours agorootparentThat was 30 years ago (sorry for the reminder) but don't you think times changed how something like that would be deployed? reply legitster 9 hours agorootparentIt could be! But I think one lesson is that internet and other utilities are not quite as functionally comparable as people claim. Per capita consumption of water has not quadruped in the last 10 years, but home internet has! So the case for internet as a public utility will be stronger when the need for constantly upgrading infrastructure plateaus. reply nobody9999 10 hours agorootparentprevSo the issue was poor government regulation and not public ownership of the infrastructure? reply actionfromafar 11 hours agorootparentprevThe general image I get from the US is that public infrastructure and customer service in general is lacking in investment. reply legitster 11 hours agorootparentMaybe, infrastructure. But having spent time in both the US and in Europe, I cannot begin to even describe how much better average customer service in America is compared to Europe. But I think that is also what holds up the adoption of public services - the difference between typical customer service in the US and say, the Post Office or DMV, turns a lot of people off from the idea of public ISPs or etc. (Although, based on some time spent in Munich makes me think the quality/availability of broadband in Europe is also not all that). reply matt-p 8 hours agorootparentI'm sorry but you cannot begin to compare ATT or Comcast customer service with say even BT in the UK. And if I value say customer service highly I get to choose from one of a hundred or more ISPs. Zen, or A&A for example probably have some of the best ISP customer support in the world. In many other spheres I will concur America has a better customer service culture. But I think alot comes down to taste and incentives. I am happy with not getting fussed over at a restaurant for example, and the odd frustration comes weighed against the fact I'm not really socially obligated to tip (although usually do). reply CogitoCogito 10 hours agorootparentprevIf we’re talking ISPs specifically, I’ve definitely had better customer service in Germany, Sweden and the Czech Republic than I ever had in the US. reply hedora 8 hours agorootparentI’ve had great experiences with customer support with 7 of the 8 local US ISPs I’ve dealt with. Starlink, comcast, hughes.net and at&t customer support can take my RJ45 crimper and stick it where they don’t need network connectivity (sideways). reply awad 10 hours agorootparentprevI struggle to think of places that are more customer friendly overall than the US. There are certainly some companies with generally less favorable reputations like, say, Comcast but that's often a result of having a captive audience. One might even argue that our largest consumer oriented companies span so many different industries but all share good customer service and experience as a cornerstone. reply actionfromafar 9 hours agorootparentI meant public customer service, but, point taken. reply matt-p 8 hours agorootparentprevAs apposed to private businesses who love making big capital heavy investments they don't need to make? The point stands though that the feedback loop between customer wants and the government is much much longer than between consumer and business. reply wbl 10 hours agorootparentprevThe US would have lots of constitutional problems enforcing rules that we like ISPs to enforce. reply philwelch 11 hours agorootparentprevThe US train system is great; we move a much higher proportion of freight by rail as opposed to truck than Europe does for that very reason. reply barney54 11 hours agorootparentThe freight system is great, the passenger train system in the U.S., not so much. reply philwelch 9 hours agorootparentBack when passenger rail still made sense, and was run by private industry, it was great. But it’s been mostly obsoleted by automobiles and airlines. There’s a certain population density where it’s still competitive with airlines, but in the US that only exists between Boston and DC, which is also the only profitable part of Amtrak and would work a lot better if Amtrak could be privatized and focus their investments on that route instead of being forced to run a bunch of subsidized nostalgia routes for train geeks. reply epcoa 8 hours agorootparent“Train geeks” have no meaningful public policy pull, it is silly and counterproductive to insinuate they have much to do with lobbying and promoting long distance rail especially east of Chicago where most of the routes aren’t particularly touristy. > would work a lot better if Amtrak could be privatized That’s a bold claim. While I’m not knee jerk anti-privatization, I’m extremely skeptical that privatizing the NEC will be anything but a fleecing of the public since most of the problems are due to aging infrastructure requiring billions of dollars of investment - which even with your beloved cars and airplanes is not something handled much by the private sector. People who argue it’s bad now just assume you’re too much of an idiot to imagine it could be worse. reply philwelch 3 hours agorootparentUnder their former CEO Richard Anderson, Amtrak achieved profitability for the first time in its history and had a solid long term business plan that entailed focusing on Acela and investing in improvements there at the expense of the unprofitable outlying routes (many of which are west of Chicago). We could privatize Amtrak next year and they could follow that same business plan—in fact, they’d probably have to. But unfortunately Anderson’s tenure and plan did not last very long, since “Amtrak CEO” is a three year political appointment. > “Train geeks” have no meaningful public policy pull, it is silly and counterproductive to insinuate they have much to do with lobbying and promoting long distance rail So who do you think managed to stuff $22 billion of Amtrak subsidies into the 2021 infrastructure act? Fairies? There are tons of small, dedicated lobbies that are organized and effective enough that they can pressure the government into these sorts of policies, and the train geeks are certainly one of them. Or maybe they just managed to infiltrate the ranks of policy staffers. These people hated Anderson because he was threatening their favorite “historic” long haul routes, like the Seattle-Chicago “Empire Builder”, in favor of focusing on Acela. reply kube-system 7 hours agorootparentprev> would work a lot better if Amtrak could be privatized and focus their investments on that route instead of being forced to run a bunch of subsidized nostalgia routes I don't think that's a fair assessment. There's a public non-monetary value that is gained by having broader accessibility to routes that aren't necessarily profitable. USPS has a universal service obligation for the same reason. We also do the same thing (even more explicitly) with air travel: https://en.wikipedia.org/wiki/Essential_Air_Service reply philwelch 3 hours agorootparentI don’t think anyone actually relies on Amtrak service in any meaningful way. The only line that anyone might rely on, Acela, is profitable by itself. USPS has a universal service obligation but also a legal monopoly. Meaning that if you try and start your own postal service, the feds will literally shut it down and throw you in prison. Its probably no coincidence that USPS also has abominably nonexistent spam filtering, to the degree that the majority of USPS service I’ve received in my entire adult life consisted of myself throwing away and/or shredding the vast majority of crap that they shove in my mailbox six days a week. And any suggestion that maybe they cut down to shoving crap in my mailbox 5 or even 4 days a week is treated like it would be the end of Western civilization. reply ClumsyPilot 9 hours agorootparentprevMost of your railway is not electric, India has electrified 95% of its rail, so did China. It makes sence reply philwelch 8 hours agorootparentThe US has reliable access to cheap fuel, and diesel locomotives are still very efficient, especially compared to diesel trucks. reply ClumsyPilot 58 minutes agorootparentelectric trains are like 2x more powerful, less maintenance and carbon-free reply lukevp 10 hours agorootparentprevI generally consider myself very politically liberal but the idea of the government controlling all internet infrastructure is truly bone-chilling. At least with many ISPs owning the lines we have some semblance of possibility that government tampering with information would be noticed. If all traffic passed through a single entity’s control, we’re only a slippery slope away from the Great Firewall of USA. Probably justified by either preventing terrorism or CSAM. reply PhilipRoman 29 minutes agorootparentThey already tap into every cable, no matter whether they own it or not. See NSA breaking into Google datacenters and wiretapping cables. Granted, they were only able to do this because the internal traffic was not encrypted. Still, global traffic analysis is nothing to scoff at, and with enough surveilance points privacy becomes almost nonexistent, even in the presence of encryption. reply roywiggins 10 hours agorootparentprevOther countries with filtering regimes just obligate their private ISPs to block, eg, torrent trackers or Facebook or whoever. Governments have plenty of leverage over ISPs already, namely that ISPs have employees, addresses and hardware they can seize. reply manicdee 9 hours agorootparentprevThere's no reason to believe that commercial entities would be any less likely to spy on you than a government entity. The government can just pass security regulations that require access to infrastructure with gag orders in place so the infrastructure owner isn't allowed to talk about government access requests or actions. The trick is simply to ensure you pick a government that isn't going to pass that kind of invasive legislation, or will remove it and retrospectively revoke all access granted under the legislation that they're repealing. As for Great Firewall of USA, what makes you think it doesn't exist already? reply zeroonetwothree 9 hours agorootparentIn the US at least this would require a court order, the government cannot just blanket search all private communications a priori. reply Qwertious 6 hours agorootparentThe Snowden leaks proved otherwise. That was over a decade ago, it's wild that everyone just forgot it all and then went back to framing the same fears as hypothetical. reply deadbunny 8 hours agorootparentprevThey can and do though... reply greyface- 10 hours agorootparentprevThis ownership model doesn't necessarily give the government access to the contents of your communication. It could be gov-owned dark fiber, rented and lit by private parties at each end. reply kristopolous 10 hours agorootparentprevEdward Snowden revealed this was all already happening under robust private industry. Privatizing publicly built infrastructure did not inoculate us from surveillance. To actually do it we need to exercise control through the collective mechanism we call government. Laws, regulation, oversight, audits, sunshine policies. It's the only way to curb it. Mistrusting your only mechanism of control and putting blind faith into private markets did not work. We tried it. It failed. reply AnthonyMouse 9 hours agorootparentWhen the government is the one pressuring AT&T to have a secret room, how is the government going to save you from it? The actual problem is the centralization, regardless of whether it's public or private. There shouldn't be two ISP options, there should be a thousand, so that anyone wanting to compromise all the traffic has to compromise a thousand independent entities, some of which can then be operated by stubborn curmudgeons who would rather loudly go to jail than silently betray the public. reply kristopolous 2 hours agorootparentSo the options are A. Break up the companies, assume mergers and acquisitions don't happen, consolidations never happens and that someone will be defiant of the law in the public interest at a crucial point B. Change the law It's important to note that AT&T was broken up and there once was many mobile carriers and there were many surveillance and spy programs going on during that time unabated and then they just bought each other up. So instead of wishing on a hope and a dream that some invisible hands will orchestrate unexplainable market magic, let's just change the law. There's actual historical evidence of that working. reply AnthonyMouse 32 minutes agorootparent> assume mergers and acquisitions don't happen Or just prohibit them. That's a change to the law you can actually see is being enforced. The problem with changing the spying law is that the spying is happening in secret, so there is no way to verify that they're not lying to you as they have in the past. Laws with no accountability mechanism are a farce. They need to be backstopped by verifiable structural inhibitions on the practice, like diverse decentralized infrastructure subject to competitive pressure and plausible independent reimplementation by anyone who doesn't trust the incumbents. > someone will be defiant of the law in the public interest at a crucial point This is required to turn back injustice. But it's more likely to be achieved the more people have the opportunity to do it. reply kube-system 7 hours agorootparentprevIronically, the internet is a US government project that was opened up to external participation. The US government has historically controlled significant parts of core internet infrastructure, and didn't fully hand over control of the internet until October 1, 2016. reply faeriechangling 9 hours agorootparentprevGovernments already control private ISPs and compel them to have the capability to tap massive amounts of data, IIRC 1%. Were also talking about owning the lines not the actual switching and routing, in either case the taps would likely be placed at the ISPs offices. reply robbiep 10 hours agorootparentprevWhy build 4 sets of lines when you can build 1? Isn’t this literally the reason why broad swaths of the US have zero competition? reply matt-p 10 hours agorootparentIn london I used to have 4 different physical fibre providers available plus one which is fibre to the building and cat5e to the prem. At a retail level there were hundreds of providers to choose from. Most of the country has more than one physical fibre (or FTTB/FTTC/CATV) provider available. reply mhb 10 hours agorootparentprevResilience? Exactly the thing which is being optimized out of much critical infrastructure. reply matt-p 10 hours agorootparentIt's easier to build resilience as a single network or organization than by \"hoping\" to have that with two different ISPs (who will obviously both of taken the cheapest route dig wise between A and B). If I need two diverse fiber paths between points X and Y, I can go to one provider who can contractually guarantee diversity. For example, they can ensure that maintenance work on Circuit 1 is not scheduled at the same time as Circuit 2, and that the paths are never less than X metres between them, as they are fully aware of the two paths. reply AnthonyMouse 9 hours agorootparentThe one ISP is not going to do that automatically, because it costs more. Then only banks and others willing to pay a massive premium get it. And sometimes even they don't, because they run completely redundant fiber paths along different streets and then use the same OEM's equipment on both of them and a bad software update takes them both out at once, or their union goes on strike and cuts power to the whole operation before they walk out. Meanwhile diverse ISPs wouldn't be using the same A and B. ISP A has a central office in a high rise downtown, ISP B primarily targets single family homes and has their switching equipment on a piece of land next to a substation in suburbia, so if you subscribe to both you not only have links coming from opposite directions, they're each operated by independent organizations instead of a monoculture. reply matt-p 9 hours agorootparentThat might be your perception, but let me give you real life state of affairs in the UK. In the last decade a number of ISPs have popped up and decided to fibre up areas. They are invariably buying OLTs from Nokia or Adtran (the same two vendors as BT OR) putting them in a BT OR exchange because that is cheap and very convenient, good access to backhaul etc and then renting BT OR ducts and poles to install the fibre in/on (PIA). To top it off they often are using the same fibre vendor as BT and sometimes even the same contractors to install it. Worked example of this; netomia/youfibre (though there are dozens). What resilience are we really gaining here? Organisational, and that's pretty much it. reply AnthonyMouse 8 hours agorootparentYou've created an environment where there is a single large incumbent and the ability and incentive for anyone else to piggyback on their existing infrastructure. Now suppose that anyone could run an ISP out of their house. They wire up a few of their neighbors and then make a single long-distance run to one of many backhaul providers, none of which has a dominant market position like BT. The backhaul providers connect to their customers and each other in telco hotels, but they're smaller and more numerous because each of the largest providers has their own, so the city has three or four instead of one. Meanwhile even if you have a BT, organizational independence in itself is better than nothing. reply matt-p 8 hours agorootparentAre there western countries with no incumbent telecom provider? It is a fine concept but you'd have to actually ban infrastructure sharing between ISPs and be building a completely Greenfield network. Quite theoretical. I'm aware of former eastern bloc countries that have dynamics somewhat reminiscent of what you describe though. reply AnthonyMouse 7 hours agorootparentIt's not that you have to prohibit it, it's that you have to somehow break their existing monopoly if it already exists, and it's easy to choose rules that don't actually break it but instead cause it to be a de facto utility again. So for example, if the incumbent has a monopoly on transit or interconnection, you have a problem because they could just charge prohibitive rates and bankrupt all their competitors. And prohibitive rates are same thing as banning them from using it, aren't they? It's what happens by default. But then the competitors can't get off the ground because the incumbent has a vertically integrated monopoly. You could require them to provide those services for their competitors at regulated rates, but then you're not actually breaking that monopoly, you're just regulating it while cementing it in place. If you have an existing monopolist then first you have to thoroughly break them up, not just mitigate the continued existence of the monopoly. reply icehawk 6 hours agorootparentprevAnd then Circuit 1 and Circuit 2 are taken down at the same time because even though they have diverse paths they're part of the same underlying DWDM system which has just encountered a fault. Ensuring there isn't a single point of failure is not just as simple as 'putting it in the contract.' Carriers can and have groomed their primary and backup circuit on to the same L1 path without realizing it. reply matt-p 1 hour agorootparentThat should only happen if your protection is at the optical layer which hasn't been common for a decade or more, partly for that reason. I've never seen it happen in 20 years. Although of course small parts of the path change over time and sometimes that means bits get put in the same duct for a short run but that also happens when you buy from different providers to be honest. With some carriers you can insist they send you updated route kmls when they change either path and you can detect the change as you'll see different losses/otdr traces before and after a maintenance/fix. reply Dylan16807 10 hours agorootparentprevLaying down competing bundles along the same path gains you very little resilience. For the smallest issues, you can expect a single bundle to already have spare connections. For bigger issues, almost anything that takes out one bundle will take out the neighbors too. reply p1mrx 10 hours agorootparentprevThe government could own the physical fiber without controlling the crypto at the endpoints. reply matt-p 10 hours agorootparentExactly. Govt owning physical fibre in a P2P network (i.e one fibre back to the pop per user) is perfectly fine because an ISP can encrypt between the pop and the user (that comes as standard with pon for example) but anything* involving active equipment is a massive no no in my view. reply eru 1 hour agorootparentprevThat seems pretty silly. If only a few people can use a limited resources like cable space on a pole, the government should auction off the privilege to the highest bidder. And then let the winner decide how they want to use what they bid for. The government can use the revenue from the auction (and general tax revenue, too) to help poor people. Though I would suggest to just give poor people money so they can decide for themselves, instead of deciding for them that they should spend their limited resources on eg internet access. reply AnthonyMouse 9 hours agorootparentprev> There can only be so many wires running on the poles or under the streets This seems like saying \"there can only be so many condos in a city, so we have to ration them.\" Hypothetically if you had billions of fiber optic cables running along the same street you could physically run out of space, but that's an implausibly high number. Meanwhile you could easily have hundreds to thousands without any trouble. > so in return for that privilege, you have to provide this in return. The point of even wanting to do that is in order to charge a sustainable price for the service. If you had to provide the service below cost, why would you do it at all? reply ClumsyPilot 9 hours agorootparent> This seems like saying \"there can only be so many condos in a city, so we have to ration them.\" Broadband is a natural monopoly, housing is not. These are well understood economic terms. reply AnthonyMouse 8 hours agorootparentRoads are a natural monopoly. Pulling fiber through a pre-existing conduit along the side of the road is not. reply legitster 11 hours agorootparentprevGovernments already charge ISPs to use said infrastructure: https://www.fcc.gov/sites/default/files/ad-hoc-commitee-surv... So for those poles running down the street in front of your house, ISPs are already paying ~$20 per pole per year. Places like NYC probably also take a cut of the revenue on top. In some regions, this is the primary gatekeeper against competition! It's really hard to argue that this is a subsidy - if anything it's the municipalities using the power of their natural monopoly. reply bbanyc 10 hours agorootparentThe revenue cut for use of public streets is known as the \"franchise fee.\" For the legacy cable companies, this was fixed by federal law in the 1980s at 5% of total revenue from cable television. For legacy phone companies, it varies by locality, but it's based on landline phone revenue. There's a federal law from the 1990s banning any such taxes or fees from being charged on internet service, so cities don't collect anything based on that. reply legitster 10 hours agorootparentThis may vary based on municipality, but franchise fees are usually in addition to specific pole attachment fees. Your franchise fee, for example, could buy permission to build your own poles. But it would not pay for you to put lines up on city-owned poles. reply BugsJustFindMe 11 hours agorootparentprev> $20 per pole per year So low? That's approximately 0% of what the ISP makes from the homes served by that pole. I figured it would be more. reply legitster 11 hours agorootparentI think you are exaggerating a bit. I have a pole in front of my house that just serves me and my neighbor. I pay $480 a year for Comcast, my neighbor gets CenturyLink. So the cost for the pole rental works out to about 5% of Comcast's service cost to me. Keep in mind some poles cost more than others (up to $250 according to the report), and that there are also hookup costs for the pole are ~$1000. I'm not saying ISPs are suffering here, but cities are not providing this infrastructure at a loss. reply BugsJustFindMe 11 hours agorootparent> I have a pole in front of my house that just serves me and my neighbor Hah. We live in veeeeery different housing densities I guess. I was thinking this was NYC, which would be closer to mine. If it's the whole state then probably somewhere in between. reply dmurray 10 hours agorootparentSome reasonable sources [0] put the number of telephone poles in the US (175m) a little above the number of households (125m). [0] https://www.srperspective.com/post/telephone-poles-a-high-wi... reply awad 10 hours agorootparentprevCheck out this fun map curtesy of NYC who do a pretty good job exposing a lot of these type of data https://maps.nyc.gov/sandbox/poletop-finder/ reply kbenson 7 hours agorootparentprevDepending on what infrastructure is already in place, I think $15/mo might be many times the break even price of supportable customers per unit of equipment/infra if amortized over a couple years. I work for an ISP. We actually build our own fiber networks by stringing fiber along aerial infrastructure to cover whole neighborhoods and then doing drops to individual houses on orders and supplying the ONT, and we can make that work and be profitable after a few years at ~$50/mo, but that's what I would consider many multiples more expensive to deliver than providing service over existing infrastructure, if that's actually what's being talked about here. And this is in CA and in the bay area and LA areas, so I doubt the regulatory and cost difference factors significantly, unless there's some cost required to be paid to the city to use their infra. reply tshaddox 11 hours agorootparentprevThe term of art is \"natural monopoly,\" and ISPs (at least landlines) are a textbook example. reply tzs 11 hours agorootparentprevThat logic works, perhaps, for wired ISPs but not for wireless ISPs. Does the New York law cover wireless ISPs or only wired ISPs? Yes, wireless spectrum is a limited public resource that requires government permission to use, but the government in that case is federal government. reply infecto 12 hours agorootparentprevYeah I was thinking that might be the case as well and I can see that argument making sense too. This especially makes sense if they get assigned regions with zero competition. On the other hand these types of rules seem like they would be hard to perfect I would almost rather have municipal run internet. reply throwaway48476 11 hours agorootparentIt would be nice if there was as much interest in making laying new fiber cheaper as there seems to be in shuffling pieces around via regulation. reply ranger_danger 11 hours agorootparentprev> There can only be so many wires running on the poles or under the streets True, but I think using DWDM this is mostly a non-issue. reply csomar 32 minutes agoparentprevMy limited experience seeing how some of these deals play from the inside, it's usually some sort of a conflict or hostile take over; and the \"poor\"/\"rights\" argument is leveraged to attack the other party. This applies also to politicians buying votes and getting random companies to pay for it. It's easy when it's someone else money. reply ceejayoz 12 hours agoparentprev$15 probably does cover the costs of these plans. 25 megabit? reply infecto 12 hours agorootparentI don't know what the costs are but my point is more that its setting price for a for-profit entity. It gets sticky because I don't know NY state law and perhaps these groups have lobbied for getting regions where you are unable to compete as n independent and with that defacto monopoly they should play by any additional rules. I would just assume that if you want to provide broadband service to low-income that the government would be making up for some of the lost revenue. reply outofpaper 11 hours agorootparentThe government is letting you provide service using tax-payer fiber. That makes for a fair bit of profit. Take away the fiber n try operating an ISP. Oh I know there's StarLink but that's not going to giver everyone in a city broadband. reply toast0 11 hours agorootparentprev25 mbps isn't much, but... $15/month probably doesn't cover any line to your house. A low cost landline was $10/month 10+ years ago, and usage was extra. I think that's an ok baseline for a connection that needs maintenance and service. reply munk-a 11 hours agorootparentI think there's also the factor that the US government has already paid ISPs for last mile installations several times now - the line to everyone's house has been paid for... if an ISP diverted that money into stock buy backs I'm sure they can explain that to the court and instead be found to have fraudulently expended that subsidy. reply toast0 11 hours agorootparentSome of that is one time cost, but you've got ongoing costs like pole maintenance and replacement, repairing breaks, trouble tickets, call centers, etc. I just don't think you can do that for $15/month, even if the capex of build out has already been paid for. Maaaybe if you had near 100% uptake, like landlines used to have. But no ISP is getting that. Most (certainly not all) people can choose between wired internet from a telco and a cable company, and 4g/5g is available in a lot of places, starlink in less dense places, and a lot of people don't need a separate home internet plan because the only networked device is their phone. reply awad 10 hours agorootparentMy understanding is there are already federal incentives to get less dense areas wired, though I'm not up to speed on the state of that - a quick Google tells me that USDA has loan and grants for it but unclear how well funded that is and how that would impact economics at $15 and $20 a month. Given that more than half of the population of NY is in NYC and Long Island, I would imagine that there is enough density and economic diversity at least downstate to make this at minimum cost neutral though this is total conjecture. As an aside, I remember the days of companies partnering with CLECs taking advantage of rural connectivity loopholes to create free conference calling services by billing larger upstream carriers and splitting that revenue with service providers, which was pretty clever. reply matt-p 9 hours agorootparentprevwell wholesale copper line rental in the uk is under £10 a month, and that includes capex pay back. So it is possible for an internet service but it will probably not be enough to allow for much if any allocation of the capex payback. (and that is sort of OK if you are able to take a position that they would never of taken a full price service anyway. 1 dollar a month back to the capex of building out to that house is better than 0) reply ImPostingOnHN 10 hours agorootparentprevI think $180/year/person would more than cover what little maintenance is needed in non-disaster situations. I've maybe made a few phone support calls over the last 1 or 2 decades of broadband service. That would add up to $1,800 – $3,600 for 5 or 10 minutes worth of work. If it's cheaper to build out their own fiber and not have these plans, that is an option these companies have, too. reply ikiris 11 hours agorootparentprevit managages it just fine in other developed nations. reply matt-p 9 hours agorootparentprev25Mb doesn't cost any more or less to provide than say 500Mb in reality. If your last mile infrastructure supports those speeds then it supports those speeds. Even the cheapest router possible to buy will do 500Mb. This comes down to an argument about what \"cost\", it will cover the marginal cost of an additional subscriber sure (e.g additional customer support, sending a router out, taking a payment each month, 500GB/month across the backbone etc) but will not really be enough to payback the capital cost of the original fibre or coax rollout. reply throw0101d 8 hours agorootparent> 25Mb doesn't cost any more or less to provide than say 500Mb in reality. If your last mile infrastructure supports those speeds then it supports those speeds. Even the cheapest router possible to buy will do 500Mb. It can once you start adding up all the customers and worrying about 'upstream' connectivity. One thousand customers at 500Mb can potentially saturate a 400Gb link; one thousand at 25Gb cannot. One has to do capacity planning and average and worst case scenarios to worry about, especially at peak times. reply matt-p 7 hours agorootparentIt's only slightly more peaky, and there is no more, or very little additional aggregate bandwidth used. I promise you if you've got 2 x 100G links out of an exchange you will not be able to visually tell from the traffic profile which is 5000 500Mb customers Vs 4000 25Mb customers and 1000 500Mb customers. reply jballer 5 hours agoparentprev> have no issue with the argument that internet service is a basic right I am curious how you would define “a right”? reply AndrewDucker 2 hours agorootparentSomething society says that you have to be allowed. reply KingOfCoders 4 hours agoprevIs this like everyone has the right for a bank account? In Germany public libraries have free internet access, I rarely see people there though (and in libraries in general, they have the most expensive magazines to read for free!) reply ssl-3 3 hours agoparentI don't know if it's codified anywhere (and unlike Germany, we've got 50 states' worth of laws), but public libraries in the US also [almost always] provide free Internet access. ...and somehow, I'm not at all sure what that has to do with a New York City law requiring ISPs to sell Internet access for people's homes at a price of $15 monthly -- regardless of where in the world a public library might be. reply KingOfCoders 2 hours agorootparentIf the reasoning is, ISP need to be $15 so everyone has internet, and everyone gets free internet at the library, that argument does not make sense. If the reasoning is, ISP need to be %15, so everyone has convenient internet, that argument makes sense. reply Novosell 2 hours agorootparentprevGermany has 16 states, why is 50 special? reply jmyeet 9 hours agoprevNational retail ISPs should not exist. All Internet should be municipal broadband that should be subsidized or free by the state. I welcome any ISP who is unhappy about this giving back the broadband infrastructure that governments have already subsidized and paid for as well as given legal monopolies to by, for example, banning municipal broadband. I also welcome eminent domain to solve this problem. reply fy20 3 hours agoparentI think the UK could be taken as an example here. For the past couple of decades internet access at home has usually been provided over copper phone lines. That infrastructure was originally laid across the country by BT, which was government owned, until a decade ago. The actual digging and trenching work was often done by private contractors, but the resulting infrastructure was owned by the government. When ADSL broadband rolled out in the 2000s, BT offered their own service, but they were also forced to allow other companies to provide internet access over their infrastructure. At the end of the day the speeds were the same regardless of who you chose, but they competed against each other to gain customers. Unlimited broadband quickly because a thing, and the prices stayed low (compared to the US). Over time they did upgrade the service, and now you can get close to 100mbps over the same, often 40+ year old, cables. In large cities fibre is rolling out, but it's being done by private companies. The UK has a lot of small towns and villages where that won't make sense, so it will be interesting to see how the market looks in the next 20 years. reply IshKebab 1 hour agorootparentYeah I don't think the UK is the best example. Not as bad as America, sure. But we've been very very slow to roll out fibre or even broadband, especially outside big cities and towns, as you noted. That's exactly why it makes more sense for a single government entity to do this. In fact BT were going to roll out fibre in 1990 (yes really, I didn't believe this when I first heard but it's true) but Thatcher killed it because it would have given them a monopoly... Yeah. That's exactly as dumb as it sounds. Thanks Tories. reply ssl-3 3 hours agoparentprevPerhaps. But what of those people who live outside of municipalities? Are they to transfer data with RFC 2549? reply sershe 3 hours agoparentprevYeah yeah, I remember growing up at the tail end of municipal essential services (USSR). For example, plumbers! You call up the municipal services on a Saturday with a water leak, and they tell you \"the plumbers are out till Monday, and actually Monday is fully booked so see you Tuesday\". I am not sure if private plumbers were fully illegal or a gray area, but we didn't \"know a guy\", so my mom used to be pretty good with plumbing fixes. As long as competition exists or can be enforced, municipal services should not exist. reply bobs_salsa 2 hours agorootparentIn this case how ever the US ISPs do not have any competition. You mention the solution to this already, there’s nothing wrong with a municipal service so long as you allow private industry to service it. reply grecy 9 hours agoparentprevNow do education, prisons & healthcare ! reply anon291 6 hours agoparentprevAs long as we make sure that the public pensions that will collapse after the expropriation of the utility companies are not bailed out, I am in full support of this. At the end of the day, the largest investors in most utilities are unfunded pension programs (Usually government) desperately looking for a higher rate of return after legislatures overpromised. reply cryptonector 4 hours agoparentprevHello DMV. reply loceng 12 hours agoprevNot accounting for government-produced inflation is interesting. I wonder if city could sue Federal government for printing money and causing such consequences? Reminder that deflation should be occurring with the benefits of automation technology, where the buying power of the dollar should be increasing. reply bobthepanda 11 hours agoparentThis has happened before. Most cities capped transit fares at one nickel when streetcars got started and the US was on the gold standard. These laws remained in effect even through massive inflation through two world wars, and usually didn't get repealed until the companies failed, the government took ownership of these services and then realized how much they cost. reply rizzom5000 11 hours agorootparentHave price controls ever ended with a positive outcome for consumers, or anyone for that matter? reply gms 11 hours agorootparentNo. Unfortunately wielding them is always an act of either ignorance or malice. reply loceng 6 hours agorootparentprevIs this an argument for having or eliminating the gold standard, where currency is anchored to a physical good? reply cryptonector 4 hours agorootparentClearly GP's argument has nothing to do with how a currency is run. reply cryptonector 4 hours agorootparentprev90% of NYC's subways were built by two private companies in the 19teens and 20s, then they got regulated and their fares got capped until in the late 1940s they were going bust and the city \"nationalized\" them. The streetcars between Queens and Brooklyn got killed this way too, and not even replaced. It's horrible. But public transit yo! It's funny. In the U.S. the federal government doesn't get to nationalize industry thanks to the Steel Cases. The States don't do it either because they compete with each other. But the cities do do it, and what can they nationalize? Public transportation is what they have nationalized. In other countries it tends to be the inverse. So Buenos Aires has an amazing privately-run public transit (bus) system that is the envy of any American city, and that's because in Argentina they nationalize big companies, not piddly ones, because the politicians who can do it are at the federal level, and they \"dream big\". I wish we could put all public transit in the U.S. in the private sector. Then finally we would have real public transit options. reply legitster 11 hours agoprevThis seems... excessive. I'm all for cheaper internet, but why even bother with $15 at this point? Why not just make it free? Both companies and consumers are going to be completely at the whim of regulators. If they start jiggering around with the means testing and or the completely arbitrary price point, I'm not confident the quality won't deteriorate or ISPs pull out of the state altogether. reply MathMonkeyMan 11 hours agoparentAt this point, internet is like electricity. Except that I have more choices for power providers than I do for internet. In my building in New York City, I have one option (cable via Spectrum), and the cheapest plan they offer is $80 per month. It recently got a lot faster, which is nice, but I'd happily pay $30/month instead for a small fraction of the bandwidth. But I can't. I wouldn't qualify for the hypothetical state enforced $15 option anyway. reply legitster 11 hours agorootparent> Except that I have more choices for power providers than I do for internet I've lived in over 15 different places in the US, and I have never, ever had more than 1 option for power provider. reply illusive4080 10 hours agorootparentIn many places your delivery is fixed but you can choose who to buy the power from. It’s all mixed in the grid of course but they do buy proportionate power from the generation companies. It’s more common in gas service in my experience than in electric. Personally I have 10+ choices for gas supplier, 2 choices for buried internet (eg excluding Starlink, etc), and 1 choice for power (co-op) reply nobody9999 10 hours agorootparentprev>> Except that I have more choices for power providers than I do for internet >I've lived in over 15 different places in the US, and I have never, ever had more than 1 option for power provider. In NYC, Con Edison[0] delivers power to customers. It also generates power, but other power generation companies share their delivery infrastructure in NYC. As such, there are at least half a dozen folks I can buy my power from. But all of it is delivered by Con Edison -- which, by the way, charges more for delivery than it does for the power itself. This may well be different from other places in the US/world, but that's how it is here. N.B.: I too, have only two choices (spectrum and RCN) for broadband where I live, but (as I mentioned) at least half a dozen power providers. [0] https://www.coned.com/en Edit: Added the missing link reply legitster 9 hours agorootparentThis seems like a specific feature to NYC. reply jessriedel 8 hours agorootparentIn SF I have two power generation options and one delivery option. It’s stupid because one of the generation options is both cheaper and greener (I don’t know of any reason to prefer the other), and delivery is literally 3x the generation cost so it doesn’t matter much either way. reply ssl-3 3 hours agorootparentprevIt may seem specific to NYC, but it's also that way in many other places in the US -- places like Ohio, and Texas. https://www.cnet.com/home/energy-and-utilities/energy-deregu... A combination of some napkin math and 2020 Census data suggests that ~35% of the US population lives in a state that the above article declares to be \"fully deregulated\". reply mckn1ght 8 hours agorootparentprevIt was also possible when I lived in Boston, see for example: https://www.energyswitchma.gov/#/compare/2/1/02108// reply buzer 8 hours agorootparentprevIn bay area I can choose between PG&E and Silicon Valley Clean Energy for power. PG&E even handles the billing for SVCE. In Finland there was local monopoly for delivery and power could be bought from numerous companies. I just checked offers for my old place and I could choose between 250 different products, some from same companies (e.g. fixed term/permanent, guaranteed wind power, market rate). In there you would get separate bill from your power company rather than it being handled by the local delivery company. reply throwaway290232 6 hours agorootparentprevUS Law often requires power generators to operate separately from distributors. Even when it's the same company, they have to operate as two separate businesses, and it's illegal for them to even mention certain information to each other as it's considered anti-competitive. The purpose is to allow the market to provide competition. This is not specific to NYC. An Introductory Guide to Electricity Markets regulated by the Federal Energy Regulatory Commission: https://www.ferc.gov/introductory-guide-electricity-markets-... Deregulation and competition: https://en.wikipedia.org/wiki/Electricity_sector_of_the_Unit... reply gruez 10 hours agorootparentprev>At this point, internet is like electricity. Except that I have more choices for power providers than I do for internet. \"choices for power providers\" in this context is probably the choice of the company that generates your electricity, not the company that delivers those electrons to your house. Given how the electricity grid works, the exact electrons you're getting might not even be related to the company you're supposedly buying power from. In this respect the electricity provider can't really be compared to an ISP. reply mindslight 5 hours agorootparentElectrons aren't being delivered. The drift velocity of electrons at typical currents is very low, alternating current all but guarantees they're not going anywhere anyway, and even if they did it would only be to the local distribution transformer. reply qaisjp 9 hours agorootparentprevAnd Spectrum has interruptions all the time. reply rconti 10 hours agoparentprevConsidering I pay $50 for 10 gig internet here in the US, I suspect those poor downtrodden ISPs can afford to add a small percentage of marginal customers at $15/mo and still at least break even on all of them. reply datascienced 9 hours agorootparent$50 for 10g sounds dreamy awesome. I think in Australia that’d be US$3k at least. reply gruez 10 hours agorootparentprevYou paying $50 for 10 gig doesn't mean internet costs $5/gig to provide and you can give the poor broadband access for 13 cents. Most of the cost is fixed, $15 is uneconomical for any ISP in the US. reply internetter 11 hours agoparentprev$15 is a fair price for 25mbit reply travoc 6 hours agorootparentMy guess is you've never had to provide network support for 10,000 households, with their busted up routers, virus-filled PCs, and the helpless people who own them. reply Glant 10 hours agorootparentprev100mbps now https://arstechnica.com/tech-policy/2024/03/fcc-scraps-old-s... reply HeyLaughingBoy 7 hours agorootparentprevI agree. That's almost 1/10th what I pay. reply Wowfunhappy 9 hours agoparentprev> I'm all for cheaper internet, but why even bother with $15 at this point? Why not just make it free? If we were talking about 15¢ per month, I would ask the same question. But, like... $180 every year seems like a lot of money to me? For comparison, ad-free Netflix costs $15.50 per month. Youtube Premium is $14 per month. And while I haven't looked into the details, I keep hearing Mint Mobile podcast ads for a cell phone plan with unlimited data for $15 per month. reply m3kw9 9 hours agoprevPretty much required infrastructure now broadband reply hanniabu 12 hours agoprev [–] > For consumers who qualify for means-tested government benefits, the state law requires ISPs to offer \"broadband at no more than $15 per month for service of 25Mbps, or $20 per month for high-speed service of 200Mbps,\" the ruling noted. The law allows for price increases every few years and makes exemptions available to ISPs with fewer than 20,000 customers. Okay, so they're either going to make the application process an extremely painful experience so nobody will want to go through it, or make a ton of shell companies where each one is dealing with less than 20k customers. reply swatcoder 11 hours agoparentYou think it's more cost effective for a company to artificially restructure itself into a obscured collage of vanishingly small cells than for them to offer a fixed-price minimal-service tier to a specific set of low-opportunity customers who probably have high collections issues under normal circumstances? reply BugsJustFindMe 11 hours agorootparent200Mbps isn't minimal service. It's more than most people need for anything they do online. Netflix suggests 25Mbps for 4K streaming. reply cbg0 2 hours agorootparentThis assumes you only have one TV doing 4K streaming, but most households might have more devices consuming bandwidth. Also, between $15 for 25mbps and $20 for 200mbps, the telco is making more profit off the latter, because at that scale bandwidth is dirt cheap and most services like Netflix deploy servers on-premise inside the telco's network. reply crazygringo 10 hours agoparentprevThe application process is trivial, because all you have to do is show you're already a recipient of an existing government program such as Medicaid or free school lunches. And courts will see right through shell companies. It's such a non-starter no company would be dumb enough to even try it. reply kotaKat 8 hours agorootparentAnd for what it's worth right now, Charter Spectrum in the state already sells a 50mbps access package for $25/mo all-in with similar requirements. So they just gotta save face and drop the price another ten bucks and figure out what broadcast fee they'll slowly increase over time to compensate across the rest of the customer base. reply vidarh 12 hours agoparentprev [–] I don't know New York law, but pretty much every jurisdictions has mechanisms for determining that a company is a subsidiary that is not an independent entity. Whether or not the law currently takes that into account, if anyone plays stupid games with that expect the law to be changed accordingly. This is hardly difficult to untangle. reply hanniabu 11 hours agorootparent [–] > expect the law to be changed There's tons of loopholes everywhere allowing companies to get away with ridiculous stuff, getting it fixed is the last thing I'd expect reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A federal appeals court has affirmed a New York law mandating ISPs to provide $15 broadband plans for low-income users, a decision not superseded by federal rules and supported by trade groups representing ISPs.",
      "This ruling is a crucial victory for states' authority to oversee broadband services, potentially influencing upcoming net neutrality legislation."
    ],
    "commentSummary": [
      "The discussion explores ownership, pricing, and regulation of internet infrastructure, emphasizing government vs. private ownership in the US and globally.",
      "Key topics include challenges in breaking telecom monopolies, government impact on internet privacy, and providing affordable internet for low-income groups.",
      "Main themes cover the balance between government regulation and privatization, nationalization effects on essential utilities, and the significance of competition and accessibility in the telecom sector."
    ],
    "points": 213,
    "commentCount": 179,
    "retryCount": 0,
    "time": 1714166317
  },
  {
    "id": 40168765,
    "title": "UK's 'Snooper's Charter' Bill Becomes Law Amid Tech Industry Opposition",
    "originLink": "https://www.theregister.com/2024/04/26/investigatory_powers_bill/",
    "originBody": "Security 51 UK's Investigatory Powers Bill to become law despite tech world opposition 51 Only minor changes from original proposals that kicked up privacy storm Connor Jones Fri 26 Apr 2024 // 12:00 UTC The UK's contentious Investigatory Powers (Amendment) Bill (IPB) 2024 has officially received the King's nod of approval and will become law. Dubbed the \"snooper's charter\" by critics, it aims to widen the digital surveillance powers of the existing Investigatory Powers Act 2016 (IPA) used by UK intelligence services, the police, government, and some emergency services. Before the latest amendments came into force, the IPA already allowed authorized parties to gather swathes of information on UK citizens and tap into telecoms activity – phone calls and SMS texts. The IPB's amendments add to the Act's existing powers and help authorities trawl through more data, which the government claims is a way to tackle \"modern\" threats to national security and the abuse of children. \"The world-leading Investigatory Powers regime is crucial to keeping the public safe,\" said security minister Tom Tugendhat. \"That's why we're making urgent, targeted changes to the Investigatory Powers Act to ensure our laws keep pace with rapidly changing technology and to guard against modern threats to national security. \"These changes mean that not only will our citizens be better protected from serious dangers such as terrorism and child sexual abuse online – their privacy will be better protected too.\" The UK government has positioned the changes, which include an expanded remit to collect data on UK citizens en masse, as a means to afford intelligence agencies and the National Crime Agency (NCA) \"greater agility and speed\" in responding to threats. Among those alterations is the ability for authorities to surveil targets by gathering their internet connection records. This will allow investigators to determine who connected to what service – such as an app or website – what phone number they dialed, where they were at the time, and when they did so. The amendments also expand authorities' ability to gather bulk datasets of personal information on individuals who have a low or no expectation of privacy. This includes data such as CCTV footage or images posted to social media. There was hope among the Bill's opposers that some of the more controversial changes would be repealed following loud concerns over privacy infringements, but the UK's hardline stance on national security has prevailed. Will Richmond-Coggan, privacy and data protection partner at national law firm Freeths, told The Register: \"The amendments made to the Investigatory Powers (Amendment) Bill were ultimately welcomed by the House of Lords, but are unlikely to be welcomed by campaigners or tech companies who were concerned about the wide-ranging scope of notice provisions in relation to the introduction of new privacy-enhancing technologies which would also have the effect of impeding lawful surveillance. \"Additional safeguards have been introduced – notably, in the most recent round of amendments, a 'triple-lock' authorization process for surveillance of parliamentarians – but ultimately, the key elements of the Bill are as they were in early versions – the final version of the Bill still extends the scope to collect and process bulk datasets that are publicly available, for example.\" Naturally, privacy campaigners strongly oppose the IPB and the changes that it brings to UK law, saying they expand an already robust arsenal of tools to collect data on UK citizens in bulk. Tech trade body techUK said in a March statement that it had \"substantial concerns\" about the Bill, which was being \"rushed\" through parliament without proper scrutiny. It believes the IPB will weaken the safety rails that guide the intelligence services when collecting data in bulk, and that it could lead to the wider data harvesting of millions of facial images, internet records, and social media data. techUK told The Register this week: \"As the Investigatory Powers (Amendment) Bill receives Royal Assent, we are disappointed that the government did not address the widespread concerns about its potential negative impacts. \"We remain concerned that these reforms will weaken privacy protections, expand surveillance powers, hinder security innovation, and risk exacerbating international conflicts of law without sufficient safeguards. \"As we look towards the next steps for this legislation, with consultations on how these regulations will work in practice, we look forward to further engagement to ensure a more workable and proportionate regime.\" UK Online Safety Bill to become law – and encryption busting clause is still there Wah, encryption makes policing hard, cries UK's National Crime Agency UK admits 'spy clause' can't be used for scanning encrypted chat – it's not 'feasible' Letters prove GCHQ bends laws to spy at will. So what's the point of privacy safeguards? Privacy International said: \"Sadly, but not surprisingly, [the IPB] has changed little from the government's original proposal, which means its becoming law is a major concern. \"The Bill waters down already insufficient safeguards in the Investigatory Powers Act. It makes mass surveillance easier and gives the UK the option to attempt to control, and perhaps lessen, the security and privacy of internet services used by billions of people around the world.\" Potential threat to security updates in commercial software Other key concerns revolve around the IPB's amendment that would force tech companies to consult the UK government before rolling out security updates to software. It's a big one that opposers of the Bill think will undermine the security posture of the UK, and potentially lead to unnecessarily protracted delays in rolling out key security features, thus making the country a more popular target for cybercriminals. Apple, for example – a company that famously refused to bend even to the FBI after they wanted to crack open the San Bernadino shooter's iPhone, said it would consider pulling iMessage and FaceTime from the UK over fears they would be forced to weaken security. The company branded the IPB's rule \"an unprecedented overreach by the government,\" adding it believes the changes are an \"attempt to secretly veto new user protections globally, preventing us from ever offering them to customers.\" Abigail Burke, platform power program manager at the Open Rights Group, previously told The Register, before the IPB was debated in parliament, that the proposals amounted to an \"attack on technology.\" The IPB, of course, goes hand-in-hand with the long-running calls to break end-to-end encryption (E2EE), which the government also claims impedes its efforts to tackle national security threats. It once again echoed these views [PDF] just this week, in fact. The Online Safety Bill was also passed last year after a rocky process that garnered equally loud concerns from privacy campaigners about a so-called \"spy clause\" that aimed to capture encrypted private messages. Although the UK government admitted that scanning encrypted chats wasn't \"technically feasible\", it didn't rule out the possibility of invoking a request for companies to do so in the future, perhaps at a time when, or if, E2EE becomes illegal in the UK, for example. ® Sponsored: Has the ever-present cyber danger just got worse? Share More about Parliament of the United Kingdom Privacy More like these × More about Parliament of the United Kingdom Privacy Narrower topics cookies Privacy Sandbox Broader topics United Kingdom More about Share 51 COMMENTS More about Parliament of the United Kingdom Privacy More like these × More about Parliament of the United Kingdom Privacy Narrower topics cookies Privacy Sandbox Broader topics United Kingdom TIP US OFF Send us news",
    "commentLink": "https://news.ycombinator.com/item?id=40168765",
    "commentBody": "UK's Investigatory Powers Bill to become law despite tech world opposition (theregister.com)194 points by rntn 21 hours agohidepastfavorite164 comments Lio 19 hours agoAs far as I can tell most UK media has hardly covered this. It's sad but expected that the BBC have ignored this. reply jasoncartwright 19 hours agoparentThey wrote about it last month, and in January. https://www.bbc.co.uk/news/technology-68625232 https://www.bbc.co.uk/news/technology-68128177 reply pera 17 hours agorootparentUnfortunately this sort of news rarely show up in the home page though: https://web.archive.org/web/20240322155020/https://www.bbc.c... https://web.archive.org/web/20240323161503/https://www.bbc.c... https://web.archive.org/web/20240129162322/https://www.bbc.c... https://web.archive.org/web/20240130140028/https://www.bbc.c... Maybe because they are categorised as \"Technology\" instead of \"UK\"\"Society\"\"Politics\" reply walthamstow 17 hours agorootparentThe front page is too full of reality TV, US news/sports, and garden variety human interest stories to fit this kind of thing in reply permo-w 15 hours agorootparentslightly tangential, but at this point I think BBC News just needs to spun off into a visibly separate body from creative production. I wish it wasn't the case - I personally have no problem with BBC News - but it's just too big of an attack surface, and it's not worth sustaining if it risks the loss of probably Britain's greatest cultural asset for the sake of fucking news. amputate the limb before it kills the host reply ode 18 hours agorootparentprevI haven't seen any TV coverage of this at all. reply chgs 10 hours agorootparentNeither have I, but that’s because I don’t watch TV. At least 80% of people in the U.K. don’t watch tv news. reply random9749832 19 hours agorootparentprevWell that's ok, I guess they don't have to report on it anymore. reply mynameisvlad 18 hours agorootparentThe bar set by GP comment is that the BBC “ignored this”. Any amount of coverage proves that to not be the case. Have they reported on it enough? No. But that’s not what was being claimed. reply lupusreal 17 hours agorootparentThat claim was preceded by the language \"hardly covered this\". You're being uncharitably literal. reply ruszki 43 minutes agorootparentFascists are gaining power because people are allowed to be not literal enough, and they are allowed to use dog whistles and euphemism. reply Gud 28 minutes agorootparentSorry, I don’t understand what you are saying. Are you claiming that the person criticising the UK state media is a fascist? Or are you saying that the state media not covering these law changes adequately make them fascist? reply ruszki 24 minutes agorootparentNone of those. reply Gud 14 minutes agorootparentOk, would you clarify for me? slackfan 13 hours agorootparentprevSoviet media hardly covered Soviet labor camps, but it absolutely covered them!!11 reply Supermancho 17 hours agorootparentprevHey it's mentioned here on HN. That counts as being reported!? reply cjk2 19 hours agoparentprevThey probably don't want to draw attention to the fact that they use RIPA legislation to catch TV license evaders. reply deepsun 19 hours agorootparentI don't understand, if BBC is a government agency, why cannot they be sponsored from taxes? Why the hassle of TV licences? reply TillE 19 hours agorootparentMostly for dumb historical reasons, currently justified as slightly protecting them from the government cutting their funding. Objectively European TV licenses are just a terrible wasteful idea, basically creating an entire parallel tax collection system. Of all the important, vital things a government funds, I'm not sure why a public broadcaster should have any special insulation from the usual democratic decision making. reply ben_w 17 hours agorootparentThe UK TV license is much less tax-like than the German one. I never paid for the UK one, by the cunning and devious scheme of… never buying a TV once I moved out of my parent's home. Now I'm in Germany, and here we have to pay for a TV license regardless of if we have a TV or not. (Also, my partner here has a TV). reply iknowstuff 17 hours agorootparentlol Germany. Do you all still live off 1GB of mobile data there too? reply ben_w 17 hours agorootparentFor the very cheapest (€4.99/4 weeks) Vodafone pre-pay rate, sure, 1 GB. The discount supermarket Lidl does 30 GB/€18.99/4 weeks. O2 has a bunch of different \"unlimited\" offers with various caveats and extras at various different price points. Now, the actual mobile network coverage? That's still all over the place, even in Berlin. reply tetris11 14 hours agorootparentLIDl Talk Smart XS package, representing... reply toyg 17 hours agorootparentprevIn Italy it's now a part of your energy bill, by law. Basically too many people were opting out (legitimately or not), so they made it non-negotiable: \"if you have energy, you must be powering screens! Pay up!\" Meanwhile, mainstream TV (both state-run and privately owned) is more and more unwatchable, and more and more people just tune out - but they are still taxed. reply alfiopuglisi 16 hours agorootparentYou can still opt out, just fill out a declaration that you don't have a TV. It can be done online in two minutes (it must be repeated every year tho). The difference is that before it was basically opt-in, with spotty enforcement, and as a result about 30% of the population was not paying anything. Today it requires lying on an official form, making it much more serious. reply thaumasiotes 16 hours agorootparentprev> currently justified as slightly protecting them from the government cutting their funding. How does that work? Who sets the TV license rate? If I hate the BBC and want to eliminate their funding, and I also want to be popular, what's stopping me from eliminating the license regime altogether? reply lozenge 19 hours agorootparentprevHistory. It was a way of letting people who don't have TVs not pay a license. There was also an extra charge for colour TVs. Edit: Oh, the BBC isn't a government agency, it's operationally independent. But always becoming less so, as the board is mostly appointed by the government now. reply bpye 10 hours agorootparentA black and white TV license is still cheaper than colour, £57 vs £169.50. reply pjc50 19 hours agorootparentprevThe BBC is a \"quango\": quasi-autonomous non government organisation. And the TV license .. well, it's really a tax, as much as vehicle excise duty is, but with outsourced collection. I think it's unsustainable long term but the BBC is still pretty popular for its non-news output. reply pipes 18 hours agorootparentAlso it allows for endless pointless online arguments about it not being a tax!(I think it is a tax in everything but name). reply robertlagrant 18 hours agorootparentIn 2024 it's a subscription service with the world's worst UX. I only have internet-delivered TV, and so I'd be very happy with an iPlayer that just unlocks more content behind a subscription fee. reply lupusreal 17 hours agorootparentWhat other subscription service are you legally required to be subscribed to merely for owning a television, computer or any other sort of appliance? Like, suppose your car comes with a subscription service for satellite radio... you're allowed to just not pay it. Owning the object associated with the service doesn't normally oblige you to subscribe to it. reply ben_w 17 hours agorootparent> What other subscription service are you legally required to be subscribed to merely for owning a television, computer or any other sort of appliance? Things may have changed since I left the country, but last I heard you only needed one if you owned specifically a TV or actually used iPlayer (i.e. more than merely having the capability to use it). reply garblegarble 16 hours agorootparentNot quite - the current legislation[1] is that you must have a TV license if you watch/record live TV (however you do it - online, with a TV, etc.), or if you use the BBC's iPlayer app/site. You do not require a TV license if you have a TV but do not use it to watch live television (e.g. using it for YouTube/Netflix apps) 1: https://www.gov.uk/find-licences/tv-licence reply robertlagrant 17 hours agorootparentprevI think it's more restricted than that. You have to have it to watch live TV on iPlayer. That's the legal requirement. reply andrewaylett 16 hours agorootparentI'm happy to report that even with a TV license, you're not compelled to watch live TV. To your actual point, you do now need a license to use iPlayer at all, not merely for live TV -- too many people (like me) were not bothering with a license and relying on catch-up. We stopped using iPlayer as a result. reply robertlagrant 13 hours agorootparent> I'm happy to report that even with a TV license, you're not compelled to watch live TV. Er. Yes? : - ) And I didn't know the other one. Even then, I'd be happy with a subscription in iPlayer that I could sign up to, rather than one enforced by law. reply robertlagrant 12 hours agorootparentprev> What other subscription service are you legally required to be subscribed to merely for owning a television, computer or any other sort of appliance? I have to pay road tax for owning a car :) reply Lio 3 hours agorootparentNot in the UK; Winston Churchill abolished “Road Tax” in 1937. We do have Vehicle Excise Duty but it’s not a road tax. reply pipes 4 hours agorootparentprevOnly if you drive the car on public roads! :) You can declare it off the road or whatever the term is. reply robertlagrant 17 hours agorootparentprevI don't think your analogy is correct. As I understand it, you only pay the licence fee if you watch live TV. reply dazc 18 hours agorootparentprevNews, reporting of things which have happened on this day in the country which I reside. I don't recall the last time this occurred? reply dazc 18 hours agorootparentprevIt's a quaint anachronism, like afternoon tea and cricket, of the 1st, 2nd, 3rd, 4th, 5th, 6th richest nation on the planet. Maintained by witless fools who's sole achievement in life was to marry a wealthy woman. reply denton-scratch 12 hours agoparentprevIndeed. I hadn't noticed it. I'm angry. reply cantagi 11 hours agoprevUK resident here. The original version gave me the push I needed to get a rPi 2B+, subscribe to a VPN, and use it as a wifi AP that routes all traffic from my house through it. Can you trust a VPN who say they don't log? No, but more so than an ISP who might be legally required to at any moment without you ever finding out. Also, I will now never start a tech company in the UK, and this is because I will never put myself into a position where I am forced to add backdoors to a product. reply will0 11 hours agoparentDo you exit the vpn in the UK, or somewhere else? reply FredPret 19 hours agoprevSometimes, when I read the news, I can hear the theme music to Deus Ex reply trinsic2 17 hours agoparentI'm actually reminded of the movie Children of Men, which seems much more of a realistic view of a dystopian landscape in UK over the next 20-50 years. reply logicchains 16 hours agorootparentOr V for Vendetta, we're practically already there. reply yoyohello13 18 hours agoparentprevI’ve been playing cyberpunk recently and I can’t help the feeing of “this is just our current world with more biotech.” reply er4hn 17 hours agorootparentAll good fiction is a statement on reality. reply JohnMakin 17 hours agorootparentprevI'd way rather live in the cyberpunk dystopia. reply tetris11 14 hours agorootparentone good thing about these dystopias is that crime is high enough to keep the rent low, or at least keep the landlords from collecting regularly. Silver linings. reply FredPret 16 hours agorootparentprevThat way at least you get a brain implant that remembers everybody's birthday! reply matheusmoreira 4 hours agoparentprevAll of the dystopia but none of the sweet nanotechnological augmentations which make you an invisible unkillable superman. What a timeline. reply roody15 14 hours agoprevThe older I get the more I see these bills and trends are really based on economics. Politics is more a less a controlled narrative to maintain this style of wealth distribution. The wealthy elite class draws most of its income by exploiting labor. This is done in a variety of ways but essentially most UK Billionaires make their money in private equity (over half.. same in US)... which really means they don't actually do much real labor. Instead they need to extract wealth which often means creating a system where people's labor is exploited. To keep this system in check a control of the narrative is required so you can keep a lower class of workers \"occupied\" so they don't wise up to the reality of the situation. One common theme is fear.. war on terror, axis of evil states.. some sort of plague..Y2K.. you can probably pull up many themes from the past. The other theme is divide by culture.... you see this today with gender issues.. gay rights, etc. For example in most modern western states... the vast majority of citizens don't really care if people are gay .. they may or may not approve personally but honestly don't mind what people do with their private lives etc... So the narrative has to flipped and made more extreme ..it doesn't matter the issue it has to something that gets people riled up enough that they don't have much mental or emotion energy left to focus on the small group that is exploiting their labor. (So gender issue has to get extreme to the point where we talk about surgical altering children for example) Notice both parties... somehow keep migrants flowing into Europe (and US).. Almost like pressure needs to be kept on keeping wages low. In the US the federal reserve openly states this as the goal ..(https://canadiandimension.com/articles/view/us-federal-reser...). Little coverage is given to this and instead a strange spin is attached on how somehow the worker class will actually benefit by being paid less. This is the same with survelliance bills. We must keep an eye on the worker class to make sure they don't organize and wise up to the reality of their situation. My two cents reply outlore 10 hours agoparentChannel 5 on YouTube does a good job presenting this same view point Whether it's the border crisis, or drug addiction, the numerous wars fought, or the polarization of society, the aim is for the ruling elite to divide and rule, and continue extracting wealth reply Pfhortune 11 hours agoparentprevYou're right for the most part, except for maybe the transphobia bit in the middle. No one wants to surgically alter kids. reply akomtu 8 hours agoparentprev\"Politics is a concentrated expression of economics\" - Lenin. He started on a high note of freeing the labor from the capitalists, but quickly realised it's more profitable to put the labor class into cages (gulags) and extract wealth from them in a more direct fashion. reply SillyUsername 19 hours agoprevTor is about to become a lot more mainstream in the UK. reply azalemeth 19 hours agoparentHonestly, although VPNs get a bad rep, there's (in my opinion) a good argument for using one here on privacy grounds. Use two or three-hop routing to frustrate correlation attacks, route your entire internet traffic to one or two other places chosen at random, use DNScrypt / DOH and have the freedom to choose who gets to slurp your data -- and the ability to change them easily. Mullvad's move towards open firmware, diskless servers, and proving verifiability to end users is something I can get towards. [C.f. https://mullvad.net/en/blog/system-transparency-future] I'm sure nothing wrong will happen with delaying security updates if they ever actually use that clause... reply londons_explore 15 hours agorootparentI have little confidence in the security of TOR. I believe any competent adversary can do packet timing correlation attacks, and it turns out just 30 seconds of web browsing gives enough timing data to narrow down to just 1 tor user, since the tor userbase is fairly small. I'm guessing there are ~10 nation states who can deanonymize tor, and I strongly suspect the UK is on that list. reply tetris11 14 hours agorootparentref for the timing attacks, please reply Eisenstein 15 hours agorootparentprevAs a bonus you get to fill out 500 captchas every day. reply hazeii 15 hours agoparentprevCheck out VPN's in Australia (Australian citizen currently in the UK) reply stranded22 19 hours agoprevWhat are people's thoughts around vpns for select devices at home for nafarious activity? So most traffic goes through open net, and tv (for example) goes through a privacy first vpn provider with an exit in, say, Switzerland. And Proton mail for a small amount of email with the rest via apple. Essentially - would it be less of a flag. And would they even care about a bit of streaming reply azalemeth 19 hours agoparentI do this the other way around. All my traffic goes through a privacy-first multi-hop VPN provider, and data from things I don't trust and don't care about (set-top boxes, smart TVs [which I detest!], games consoles, house guests, etc) goes through the clearnet and provides a reasonable degree of cover. Literally everything else gets onion routed to some degree, to frustrate correlation analysis. I know that TOR users are logged specifically; I don't know about VPNs directly. All I want is for people to not spy on me. I'm not a criminal and I do nothing wrong(!) -- apparently an impossibly hard ask! Something someone on HN might be able to answer is this: is there a reason why you couldn't try to configure Wireguard to have a constant bandwidth mode? Continually transmit UDP packets and again frustrate timing or correlation analysis -- if every client spoke to the server on one of a predetermined number of speeds continually then the burstlike connections coming from that server to a potentially identity leaking service (e.g. gmail.com) could not easily be correlated with the origin of the traffic. I imagine that would make it easier to have some stronger degree of privacy protection in aggregate. reply tomxor 18 hours agorootparentInteresting idea. It would only be effective against timing analysis if all clients connected to the wg server use this constant bandwidth mode (otherwise your traffic could be identified through negative correlation against other connections). It would also have to be aggressively rate limited to make it practical for anything with a battery over wifi, even a constant 2Mbit is not ideal for wifi and will cause a lot of battery drain since radios are most efficient when they can do burst communication. Or maybe it could be limited to bursts at some interval for better bandwidth, or better efficiency (but not both)... that would also make it easier to manage the traffic since it's no longer real-time. I doubt there is any way to configure wireguard to behave like this, it has quite a specific purpose and wireguard's design focus is performance and security rather than obfuscation from traffic analysis. Maybe it's not necessary to modify WG if the traffic can be manipulated just before the WG interface... [edit] Talking of timing analysis, this was the side channel in specter/meltdown that was demonstrated to extract cryptographic keys from JS in the browser. Browser responded by just lowering the resolution of JS timers and introducing noise. Maybe this would also be sufficient for a wireguard connection at the cost of slightly worse latency performance. The tricky part is how the timing resolution would be tied to activity on the WG server, i.e you would need the packets to be spread out further and further the less activity there is on the server, so you actually end up with better performance the busier the server is. It would also come at a performance cost to routing which would be forced to hold on to packets artificially. reply cjk2 19 hours agoparentprevI actually don't care if I'm honest. I will be a good citizen, use all the normal services and blend in with the noise. Data about you looks much more suspicious if there is none. If I wanted to do something nefarious I'd do it completely offline. reply pipes 15 hours agoparentprevI'm thinking about ditching my VPN sub. It just looks suspicious. Sad times. reply wkat4242 12 hours agorootparentWho cares if it looks suspicious though? You're still innocent unless proven guilty. I'm not giving up mine (and to be fair I do use it for Torrents mostly :) ) I'm not in the UK though, and I refuse to even visit it for work now since Brexit. Google had some convention there a while ago and I just refused to go. If they don't want us I don't want them. reply deadbabe 20 hours agoprevTruly, I believe we live in the medieval era of the technological adoption curve. Such laws should be seen as barbaric by future governments. reply bluescrn 17 hours agoparentIt's not just about tech. People simply don't care about freedom or privacy any more. They're happy to give it all up if it means that an ever-expanding nanny state will care for them and protect them from all harm, whether it be international terrorism, cigarettes, or insults being hurled online. The campaign to brand freedom ('freedumb') as a far-right ideal certainly isn't helping. reply ben_w 17 hours agorootparentThe UK's left/right split is very different to that of the US. In particular, surveillance is extremely popular with both of the big UK parties. Well, I say \"big\", we'll see if the Tories even manage to win a single seat at the next election — I think they're currently expected to get just under 100 seats, but they're so dysfunctional that total disintegration is absolutely possible between now and then. reply Silhouette 17 hours agorootparentprevPeople simply don't care about freedom or privacy any more. I don't believe that's true. Our younger generations are way more savvy about those things than our older ones. What I do see is a sense of hopeless inevitability. People don't feel they can escape the surveillance and other technological dangers because everyone is at it. Both of our major political parties are generally supportive of the police and surveillance state and no-one else has any realistic prospect of forming a government at national level. Unless you give up a large part of modern life and decide not to participate in large parts of our society you have little real choice but to pick your poison from the big tech firms as well. So people resist in less obvious ways like moving between services and creating new accounts frequently with false details. People from younger generations hardly ever commit to any single account on any service the way their parents or grandparents did. They have no time for terms and conditions that require real identities. (Or the service will do what exactly? Terminate the account they didn't really care about or expect to use for long anyway?) I imagine a lot of young people already know how to use a VPN and probably more will start doing so over time. I wonder how many phones the average teenager really has today. Of course this attitude creates other problems of its own. With modern tech but little personal accountability we see issues like cyberbullying becoming real problems for our schoolkids for example. Unfortunately the only answer the politicians can seem to think of to deal with a problem like that is trying to be even more authoritarian. We need to do better than that. reply deadbabe 17 hours agorootparentYoung people these days do NOT know how to use VPN, they barely know how to even use computers outside of their walled garden app based devices. reply kimixa 12 hours agorootparentI think some of this is selection bias - for many their local peers of similar age are often in the same industry, and of similar education backgrounds. I assume on a site like this it's likely tech-aligned. Most people do NOT know how to use a VPN, and barely know how to use computers outside of their walled garden app based devices. I'm not sure if there's really much of a difference by age. reply graemep 18 hours agoparentprev> the medieval era You mean the era in which we got things like the Magna Carta, rights of due process, habeas corpus, the gradual abolition of slavery (at least in western Europe), the gradual transfer of power from monarchs to parliaments etc. I think our direction of travel is a lot worse now! reply ben_w 17 hours agorootparentMagna Carta, which was about the relationship between a king and his own feudal lords, which was violated by both sides and annulled by Pope Innocent III, leading to the First Barons' War — one of the many UK civil wars besides the one commonly known by the title \"The\" Civil War? > the gradual abolition of slavery (at least in western Europe) Even with that caveat, didn't that really only happen after the medieval era? reply denton-scratch 12 hours agorootparentprev> things like the Magna Carta, rights of due process, habeas corpus I think Habeas Corpus is one of the few clauses of Magna Carta that still has legal effect (the former is part of the latter). And remember that Magna Carta was a deal between the \"barons\" and King John; the rights that Magna Carta purported to confer, only applied to barons (landowners, basically). In the time of King John, the law as most ordinary people encountered it was simply the local baron. I think it wasn't until Henry II that there were royal courts set up, to deliver the same law everywhere. reply jjgreen 18 hours agorootparentprev... we got things like the Magna Carta Did she die in vain? reply sandbags 17 hours agorootparentBrave Hungarian peasant girl that she was. reply linearrust 16 hours agoparentprev> Such laws should be seen as barbaric by future governments. Only because future governments will have far better methods of surveillance and population control. reply hogepiyo 20 hours agoprevUK leading the way on this draconian bullshit as usual. You know its bad when your hope rests on big tech throwing down the gauntlet. reply A_D_E_P_T 19 hours agoparentThe UK government seems hell-bent on the control and micromanagement of its own population. Their recent smoking ban -- which is sure to make cigarette smoking cool again with young people -- was a joke. This hyper-surveillance measure is a lot less funny. reply simonbarker87 18 hours agorootparentThe smoking ban is incredibly popular and supported by most people across all generations, also given we have tax payer funded healthcare this is a clear win for the nation. reply lonelyasacloud 18 hours agorootparentPopular but stupid. Surely we've learnt by now that outright prohibition is only good for funnelling money to criminals? reply simonbarker87 17 hours agorootparentOnly if smoking is seen as cool, which it isn’t any more. Vaping is the way to go for kids these days and whilst still not great it seems like it’s better than smoking. reply A_D_E_P_T 15 hours agorootparentSpare some thought as to why it's no longer \"seen as cool.\" That's about to be turned on its head. Banning smoking will make it seem edgy, transgressive, and alluring. Still worse, banning it only for people below a certain age will make it seem mature and sophisticated to those below the cutoff. The forbidden fruit. The UK Govt couldn't possibly devise a better way to popularize cigarettes with young people. And they'll create a nice little black market, to boot. reply nmca 13 hours agorootparentsuppose we design a reasonable experiment here, like taking a random sample of teenagers and mitigating for anonymity concerns. are you seriously claiming that this would show an increase after the ban? or to put it another way --- the phenomenon you point to is real but completely swamped by the more direct impacts reply A_D_E_P_T 11 hours agorootparentIf we were talking of the USA, or of a European nation like Croatia, I'd bet that smoking rates in youth show a slow but gradual increase in the years following a UK-style ban. In the UK, however, the way to bet is that the government enforces the ban with maniacal zeal, so smoking rates will probably go down slightly in the near term. (For better or worse -- usually worse -- the UK has more state capacity to enforce a ban than the US or indeed the vast majority of other countries.) But smoking will become, once again, a prestigious activity. All the cool kids will smoke, if only for the social signaling benefits. Over more than a decade, youth smoking rates might surpass what they were before the ban. reply BriggyDwiggs42 1 hour agorootparentWorth noting that the harm done, both economically and personally, by overpolicing smoking would probably be worse than the consequences of the smoking it manages to prevent reply posix_monad 16 hours agorootparentprevThe taxes paid on nicotene products more than offset the cost of the healthcare. reply simonbarker87 15 hours agorootparentIf it were additive to the base cost of healthcare for the nation but it’s not, taxation isn’t ring fenced like that. reply extraduder_ire 8 hours agorootparentWould that matter outside of optics? Tax money is fungible. reply trinsic2 17 hours agorootparentprevThe Postmaster debacle is the biggest example of this I have ever seen. Something has dramatically changed when people are not being held accountable for their crimes. reply denton-scratch 12 hours agorootparentprevI smoke 40 a day. I don't want my granddaughters to do the same. I strongly support the ban. All the smokers I know agree with me. reply red_admiral 17 hours agorootparentprevCannabis is technically also banned in the UK. As is selling vapes to underage people. Where there's a market, capitalism finds a way. reply xyst 19 hours agoparentprevUSA is not too far behind. FISA was reauthorized despite outcry from many people. https://archive.is/3vKoO In a post-911 world, the people leading this country still leading by fear. reply cjk2 20 hours agoparentprevWe do indeed lead the way in draconian bullshit. We also lead the way in making it utterly unworkable and useless through incompetence, cost cutting and cases getting shot down by ECHR though. Think of us as an example to the rest of the world on how to do a shitty job of consultation, legislation and implementation! reply kwhitefoot 18 hours agorootparent> cases getting shot down by ECHR though. Weren't the Tories going to get the UK out of the European Convention on Human Rights? And thus remove the possibility of appealing the the European Court of Human Rights. reply cjk2 18 hours agorootparentYes they are. They have a valid point leaving it regarding immigration, but the side effects of it are far more advantageous to them personally and this is not being discussed. reply EGreg 20 hours agoprevLest you think it's just the UK, we have such laws in USA now making their way through Congress, and already in place around the world: https://community.qbix.com/t/the-global-war-on-end-to-end-en... reply anonzzzies 20 hours agoparentIt’s just a matter of time for it to happen everywhere unfortunately: it’s possible, it’s easy, most people don’t know or care or understand, so keep trying and it’ll happen. reply chrisfinazzo 19 hours agoparentprevShort of a law that forbids the math which makes encryption possible, this is noise. Wake me up when there's an actual threat to privacy in... (checks notes........) OH WAIT The heat death of the Universe will come first -- Carry on, nothing to see here... reply tivert 16 hours agorootparent> Short of a law that forbids the math which makes encryption possible, this is noise. So your \"solution\" is to break the law and suffer the consequences? I think you misunderstand law. It's not about making things impossible, it's about making possible things unappealing so you won't do them. reply chrisfinazzo 15 hours agorootparentIt’s impossible to break laws that are impractical and unenforceable? That’s cute. reply tivert 14 hours agorootparent> It’s impossible to break laws that are impractical and unenforceable? Huh? When did I ever say it was \"impossible to break laws\"? My entire point was it very possible to break laws, which is why those laws exist. The government could pass a law banning encryption tomorrow and there would be nothing besides fear of the law stopping you from continuing to use it, if you so chose to so do (just like the law against battery doesn't mean there's a physical barrier making it impossible for you punch the guy standing next you on the subway). The actual and indented effect of the law would be the technology would be driven underground, and your choice to use it illegally would carry far greater risks to you. Also, a law against using encryption wouldn't be unenforceable. At the very least they could punish you with a sentencing enhancement or use it as an alternative charge if it successfully cloaked whatever else you were doing (e.g. like how they got Al Capone for tax evasion, not any of the other stuff he did). And anyway, computers make surveillance and enforcement easier, not harder. reply EGreg 12 hours agorootparentThat's not even true. There is a lot more than A's fear of the law. There is also B, C and everyone else's fear of the law, compounding and creating chilling effects, to the point where there is simply no one who will carry your message, transact with you, provide a platform, or whatever, for the type of thing you want to break the law for. reply kmeisthax 18 hours agorootparentprevhttps://xkcd.com/538/ reply akomtu 7 hours agoprevUK is a dying state, like the Roman empire, once mighty, now is a small tourist destination. When the good will leaves a nation to its own devices, what's left is a disorganized group of men and women held together by tyranny of a dying dragon of bureaucracy. That bureaucracy doesn't know how to keep a nation prosper, and all it cares about is self-preservation by any means possible. reply EVa5I7bHFq9mnYK 5 hours agoparentTourist and money laundering destination. reply Silhouette 17 hours agoprevThe Conservatives who have been in government for a long time will most likely lose a general election within a year so we should also be looking at the opposition Labour party's views on this kind of legislation. Perhaps surprisingly given they are currently led by a former human rights lawyer the Labour side also seem to be authoritarian when it comes to technology and surveillance. reply dmix 16 hours agoparent> Perhaps surprisingly given they are currently led by a former human rights lawyer the Labour side also seem to be authoritarian That distinction was lost long ago in the west. Canada's left wing liberals/NDP are doing it under the guise of hate speech and protecting kids: https://globalnews.ca/news/10317040/online-harms-bill-canada... Conservatives more stereotypically spin it for national security/terrorism. But it will come in some form or the other each administration like clockwork. Generally it seems the intelligence services get a free pass from the entire political class on all sides, regardless of pretence, as we've seen in every Five Eyes countries, including Australia. reply Dig1t 14 hours agoprev>\"Additional safeguards have been introduced – notably, in the most recent round of amendments, a 'triple-lock' authorization process for surveillance of parliamentarians What does this mean exactly? I'm not familiar with UK government terminology. reply fullspectrumdev 14 hours agoparentIt means spying on members of parliament requires a significantly higher level of authorisation than spying on a normal citizen. One law for them, another for us. reply denton-scratch 12 hours agorootparentThat clause stood out to me too. This sounds like a huge step back towards Spycatcher territory, where MI5 thought it was their job to undermine the Labour Party, because they were all communists. reply smashah 15 hours agoprevIt's funny how our next PM will be the person who let off Jimmy Saville yet this bill claims to care about CSAM (signed into law by the brother of a known peadophile). 1st amendment is being killed in America, privacy rights are being killed in the UK. Freedom? Democracy? Will of the people? Lol. reply francis-io 20 hours agoprev\"Additional safeguards have been introduced – notably, in the most recent round of amendments, a 'triple-lock' authorization process for surveillance of parliamentarians ...\" Thank god, I'm glad the politicians that passed this law will be protected from it /s reply causal 17 hours agoparentThis, and requiring tech companies to consult the government before rolling out security updates, make it seem evident that safety isn't really the motivation. reply BoxOfRain 17 hours agoparentprevIt brings to mind the fact the Inner Party of Oceania had the privilege of turning their telescreens off. reply kmeisthax 18 hours agoparentprevI bet you in 10 years there will be a scandal where the Tories spied on an opposing party MP, or vice versa. Doesn't even need malicious intent because the mere act of doing this carries the stench of malicious incompetence. reply kypro 15 hours agoparentprevI'm a cool headed person, but reading that sentence was rage inducing for me. There's literally no good argument to protect politicians other than corruption. If politicians are worried these powers could be used to target them then it could be used to target literally anyone, and the concern shouldn't have been that they could be target, but that anyone could be targeted. Plus, if this bill is only going to stop bad guys then they should have anything to worry about anyway right? I mean isn't this what we are told!? Should we be worried about these powers being abused or not!? What a fucking joke. reply chaz6 16 hours agoprevI have forewarned my employer that if this bill passes I will be considering my position as I am idealogically opposed to being forced to comply with state surveillance. reply pacifika 11 hours agoparentThe possibility of state surveillance, I think that is a distinction worth making. reply kypro 16 hours agoparentprevI wish there were more people like you. Big respect for this. reply hollow-moe 14 hours agorootparentUnfortunately, someone not like them will happily take the job, possibly with a lower pay as bonus. reply cjk2 20 hours agoprevOh brilliant, I am sure our utterly fucking incompetent and inept security services having more data will lead to better outcomes. choke Salman Abedi choke. (for anyone not in the UK, he was already well known to the security services [MI5] and managed to kill 22 people in a suicide bombing) reply tgv 12 hours agoparentOTOH, new mass surveillance in combination with optimistic use of new tech will lead to ruining thousands of innocent lives. It's as if they thought that the Horizon scandal was just a bit of bad publicity for a good idea. reply Akronymus 19 hours agoparentprevIf I had a buck for every time someone like that was \"known to the authorities\" I'd have quite a bit of money. This is a pretty blatant power grab, rather than anything actually positive. reply echelon_musk 19 hours agorootparentInstead of admitting blame failures are used to increase budgets. The simple arguments of \"we didn't have enough funding\" or \"we weren't allowed to collect more data\" will be used. More tax payer money is then spent to cement power through mass surveillance and the cycle repeats. reply balderdash 19 hours agorootparentIt’s all about competence - it’s why a couple have of competent programmers can stand up and website/program/app for orders of magnitude less time and money than government or large corporate IT can. I’m sure the same is true in law enforcement / intelligence, more power/money is simply the crutch for the incompetent to be more productive reply csmattryder 18 hours agoprev> \"These changes mean that not only will our citizens be better protected from serious dangers such as terrorism and child sexual abuse online [...]\" Sometimes you just love it when the band knock out one of their classics. reply mrandish 13 hours agoprevWhen I was kid I read Bradbury, Asimov, Clarke and Heinlein while dreaming of how by the time I was \"old\", I'd for sure be living in a sci-fi future. Humans had landed on the moon only the decade before, NASA had launched Skylab (a REAL space station) and was developing a space shuttle that could go to orbit anytime. Personal computers with modems were actually a thing in stores. Sadly, I now realize I was correct except that I was reading the wrong sci-fi authors. I should have been reading Orwell and Huxley. reply cynicalsecurity 20 hours agoprevIs anyone still surprised the UK doesn't respect the privacy of their citizens? If the UK still was in the EU, this would have never happened. reply switch007 20 hours agoparenthttps://en.wikipedia.org/wiki/Regulation_of_Investigatory_Po... We were in the EU in 2000 https://en.wikipedia.org/wiki/Investigatory_Powers_Act_2016 And in 2016 reply echelon_musk 19 hours agorootparentRIPA 2016 was to legalise practices exposed by Snowden. The \"regulation\" part in the title is just newspeak smokescreen. They regulated themselves. reply chmod775 20 hours agoparentprevThe UK is still subject to the European Court of Human Rights, which is distinct from the EU. Relevant past rulings: https://dpglaw.co.uk/european-court-of-human-rights-declares... https://privacyinternational.org/press-release/5120/judgment... reply matthewdgreen 19 hours agorootparentThe typical government response to European Court rulings against surveillance is just to interpret them in some favorable way, then ignore them. So if the court outlaws \"mass surveillance\" and you've instituted a mass surveillance system, just call it \"targeted surveillance\" or ignore the ruling altogether. Maybe in 6-7 years you'll get another ruling against that, but it doesn't matter. reply basisword 20 hours agoparentprev>> If the UK still was in the EU, this would have never happened. I wouldn't be so sure about that. The original act passed in 2016 and was already an abomination. reply ben_w 20 hours agorootparentIndeed; that even the Welsh Ambulance Service was ever given nationwide no-warrant-needed access to internet connection records still irks me. However, the 2016 act was ruled incompatible with EU law: https://www.bailii.org/ew/cases/EWHC/2018/975.html reply Symbiote 20 hours agorootparentprevThe EU court ruled against this law in 2020: https://www.blueprintforfreespeech.net/en/news/eu-court-rule... reply tgv 12 hours agorootparentIndeed. A state can pass whatever legislation, but it's not the highest authority in EU member states. States still have to obey the various agreements, treaties and rulings, although enforcing that falls back onto the member states themselves. So a sufficiently determined government (Poland/PiS, Hungary) can still get away with a lot. reply duxup 20 hours agoparentprevIs there something about the EU that would prevent government spying? Most privacy efforts I've seen in the EU are commercial facing legislation that I find kinda of piecemeal and naive (granted I'll take naive over nothing, sometimes...). I don't know that I've seen efforts that would seriously curtail government surveillance. reply ben_w 20 hours agorootparent> Is there something about the EU that would prevent government spying? Limit, rather than outright prevent. \"\"\"Respect for private and family life Everyone has the right to respect for his or her private and family life, home and communications.\"\"\" — Chapter II, Article 7, Charter of Fundamental Rights of the European Union, https://www.europarl.europa.eu/charter/pdf/text_en.pdf Which is part of the Treaty of Lisbon. reply duxup 20 hours agorootparentHas that prevented any laws? Been used in a challenge? Honest question as I really don't closely follow this area in the EU. reply ben_w 20 hours agorootparentThe principle certainly has, but without being a lawyer I can't be more detailed than \"this sounds like the right vibe\". Off the top of my head, it's connected to all of: 1) the 2016 act being ruled incompatible with EU law: https://www.bailii.org/ew/cases/EWHC/2018/975.html 2) all the GDPR popups people seem to hate even more than thing GDPR is trying to get under control 3) an ongoing battle over transatlantic data sharing by private corporations: https://en.wikipedia.org/wiki/Max_Schrems reply denton-scratch 11 hours agorootparentprev> Is there something about the EU that would prevent government spying? Presumably you mean laws and treaties; those exist. But I wouldn't expect government spies to pay excessive attention to the law. The problem is the clauses about requiring distributors to get state clearance before deploying an update [ahem - nooo, that disables our backdoor], and increased costs and obligations on ISPs. Thwy won't ever try to enforce the law against the spooks, but for big tech companies, it's just regulatory clutter. They have staff lawyers out the wazoo, all specializing in regulatory clutter. Not good for small ISPs and tech companies. reply hgyjnbdet 20 hours agoparentprevAre you aware of sone of the proposed laws the EU wants to enact? reply graemep 18 hours agorootparentHardly anyone is. One of the problems with the EU is that because the media focus on national governments, and because the EU often acts indirectly (through directives, requiring national governments to pass legislation, rather than direct legislation) people really have little idea of what the EU does legislatively. I am aware of the current attempts to push surveillance laws, although AFAIK the EU parliament did reject the worst of the legislation the commission tried to push. reply ethbr1 20 hours agoparentprev> If the UK still was in the EU, this would have never happened. Why do you think people in the government wanted them to leave? The UK has never squared the \"free speech, even when it's inconvenient to those in power\" circle. reply rasengan 20 hours agoprev [–] King Charles has cancer so he won’t have to deal with the backlash of this. It is truly ridiculous that this has become a law in the UK. The internet is weakening and fracturing at intense speed - faster than it grew. reply stoobs 20 hours agoparentThe government creates these laws, not the reining Monarch. Royal assent is a ceremonial process at this point as the powers to refuse a bill are very, very rarely exercised. reply goodcanadian 20 hours agorootparentRoyal assent is a ceremonial process at this point as the powers to refuse a bill are very, very rarely exercised. From Wikipedia (https://en.wikipedia.org/wiki/Royal_assent): \"The last bill that was refused assent was the Scottish Militia Bill during Queen Anne's reign in 1708.\" Even that was only done on the advice of government ministers who had second thoughts after the bill was passed. reply hgyjnbdet 20 hours agoparentprevPretty sure even without cancer Charlie wouldn't be aware of any backlash, even if there was one. His role is ceremonial at best. I agree the law is ridiculous though. I hope big internet companies boycott the UK, speaking as a UK citizen. This law won't catch anyone remotely tech sazzy. reply pch00 19 hours agorootparent> I hope big internet companies boycott the UK So at least there's a potential bright side for people in the UK :-) reply dogleash 19 hours agoparentprev> The internet is weakening and fracturing at intense speed - faster than it grew. The writing was on the wall a decade ago. You'll get Cable TV 2.0 and you'll like it. An army of sycophants and chumps are working to ensure it. reply basisword 20 hours agoparentprev>> King Charles has cancer so he won’t have to deal with the backlash of this. It has nothing to do with him in reality. The monarch has to give ascent to new acts of parliament and by convention always does. They don't (overtly) interfere politically. reply alephnerd 20 hours agoparentprev> King Charles has cancer so he won’t have to deal with the backlash of this. What backlash? Are you going to go lobby in Westminster? Or hit the street and riot/direct action against it? The reality is most people don't care, and complaining on a forum online does nothing. reply robxorb 20 hours agorootparent> complaining on a forum online does nothing. This probably isn't true. Forums are voluminous online and regularly contain such complaints, partly as the anonymity gives people a feeling of being more free to express themselves, versus other types of social group interactions. Given that cohesive and cooperative direct action from organised groups is relatively rare, it could be seen as statistically likely direct action at least sometimes resulted from a group unifying over discussion on a forum. reply rasengan 20 hours agorootparentprev> The reality is most people don't care, and complaining on a forum online does nothing It has to start somewhere. Last time the King faced adversity from the people, it was due to an anonymously distributed pamphlet. reply switch007 20 hours agoparentprevThe monarch signing a bill in to law is just a formality. Nobody holds them accountable for it. Agree with your other points reply gnuser 20 hours agoparentprev [–] “Ceremonial” etc: Behind Closed Doors: Oaths to the Monarch (who, by the way, was besties with necro-pedo-murderer Jimmy Saville) Gtfo of here brits - the Irish were on the right side of history and it’s high time for a revolution and you know it’s true! reply adammichaelc 19 hours agorootparent [–] What you say is triggering. Triggered by the darkness. It’s okay to be afraid of the unknown. But it’s also time to take a look within ourselves and see that we are co-creating something. Do we want it? Is this the best we can do as a human race? The Irish people have a bit of luck on their side. They follow their hearts and speak up. The ancient Celtic DNA knows. It’s a magical people, and I mean that in the sense of the Magi; enlightened men and women of knowledge who brought the sciences and the arts to humanity. The Irish were conquered, but the power and wisdom within the DNA cannot be conquered. It’s all within. Like an ancient quantum computer. It sits. Waiting to be tapped by the brave one who dares to look outside the program. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The UK's Investigatory Powers Bill, known as the \"snooper's charter,\" has been approved with minor changes, expanding digital surveillance powers to combat national security threats and online child abuse.",
      "Critics worry the bill will compromise privacy, enable mass data collection, and potentially slow security updates, possibly making the UK more prone to cybercrime.",
      "Tech companies, privacy advocates, and organizations like techUK and Privacy International have voiced concerns about the bill's implications on privacy, security innovation, and international legal conflicts, including the government's stance on breaking end-to-end encryption for national security reasons."
    ],
    "commentSummary": [
      "The UK's Investigatory Powers Bill has become law, facing opposition from the tech industry.",
      "Critiques are directed at media coverage, notably the BBC, for not sufficiently highlighting this crucial matter.",
      "Discussions cover privacy worries, government surveillance, EU legislation effects, societal behavior, historical context, and the monarchy's involvement in lawmaking."
    ],
    "points": 194,
    "commentCount": 164,
    "retryCount": 0,
    "time": 1714135547
  },
  {
    "id": 40167905,
    "title": "Veilid: Decentralized App Launch at DEF CON 31",
    "originLink": "https://cultdeadcow.com/tools/veilid.html",
    "originBody": "....:::::::::::::::::::::::::: :::::::::::::::::::::::::.... ..::'''' '` ````::.. .::' .__.__ .__ .___ `::. .:: ___ __ ____ |__||__| __| _/ ::. .:: \\ \\/ // __ \\|||/ __::. .:. \\ /\\ ___/||_| / /_/.:. :: \\_/ \\___ >__|____/__\\____:: .. \\/ \\/ ..TAKE BACK CONTROL _ __ _ ((___))((___)) [ x x ] _ _/ cDc GRAND IMPERIAL DYNASTY \\_ _ [ x x ] \\ / _ _ MCMLXXXIV A.D. _ _ \\ / (' ') \\ VOLANDO, REPTILIA SPERNO / (' ') (U)(U) [about][veilid foundation][contributors][press releases][launch events][veilid.com][code][discord][links]..:-: :=+++++++. :-..++++++++= =%:*@--++++++++: :@--@#+++++++++ *+:@%.-++++++++: #*.%@-+++++++++ .* #@+ =#* :#@@- :##. +#@@. :++++++++: +%.+@# %@- @@. +++++++++ %@:-@%. .---: :::: %@- .::: .:---@@. .++++++++- .@=.%@= -%%+=+%@= =*@% %@- -+@@. =@@+==+@@. =++++++++. -#+ #@* =@% @@. -@% %@- .@@. +@# @@. .++++++++= =@* *@%. #@%#######: -@% %@- .@@. %@= @@. -++++++++=@# +@@: *@# -@% %@- .@@. #@+ @@. ++++++++*%:#@@+ .%@#-.:-+* =@@. .%@= :@@: :@@+:.-*@@- -++++++++#@@@% :+***+- +++++-:+++++ =++++= =***=.-*+= ++++++++*@@@= .___ .:++++++++#@* [__ ._. _.._ _ _ . , _ ._.;_/ ++++++++*#[ (_][)(/, \\/\\/ (_)[\\ .+++++++=....:::::::::... ::::: about :::::```:::::::::''' Veilid (pronounced Vay-Lid, from \"Valid and Veiled Identification\") We built Veilid because when the Internet was young and new, we viewed it as an endless and open realm of possibility. Instead, the Internet we know now has been heavily commercialized, with users and their data being the most sought-after commodity. The only ways to opt-out of becoming the product for billionaires to exploit are either too technical for the average user, or to simply not go online. We believe that people should be able to forge relationships, learn, create, and build online -- without being monetized. (top) ...:::::::::::::::::::::... ::::: veilid foundation ::::: ```:::::::::::::::::::::''' * Christien Rioux * Katelyn Bowden * Paul Miller (top)...::::::::::::::::... ::::: contributors ::::: ```::::::::::::::::''' Veilid contributors include coders, admins, writers, legal, and more. * TC Johnson * Jun34u, cDc * Deth Veggie, cDc * Beka Valentine * signal9 * Obscure, cDc * Kirk 'Teknique' Strauser * Alice 'c0debabe' Rhodes * Abbie 'antijingoist' Gonzalez * snowchyld, NSF * John 'Wrewdison' Whelan * Robert 'LambdaCalculus' Menes * Glenn Kurtzrock * Daniel Meyerson * CylentKnight * Robert 'Slugnoodle' Notarfrancesco * Yer mom (top) ...:::::::::::::::::... ::::: launch events ::::: ```:::::::::::::::::''' We're releasing Veilid at DEF CON 31 in Las Vegas. You should come hang out. 2023-08-12 - \"Veilid Demo Lab,\" 10:00 – 11:55, Committee Boardroom, Caesars Forum 2023-08-11 - \"CULT OF THE DEAD COW Breaks The Internet (and you can too!),\" PAR-TAY!, 20:00 - ??:??, Track 1, Caesars Forum 2023-08-11 - \"The Internals of Veilid, a New Decentralized Application Framework,\" 09:00 - 09:45, Track 1, Caesars Forum (top) ...::::::::::::::::::... ::::: press releases ::::: ```::::::::::::::::::''' 2023-06-22 - CULT OF THE DEAD COW Breaks The Internet (and you can too!) (top)...:::::::::... ::::: links :::::```:::::::::''' https://cultdeadcow.com https://veilid.com https://gitlab.com/veilid/veilid https://discord.gg/5Qx3B9eedU Twitter: @cdc_pulpit & @veilidnetwork Bluesky: @cultdeadcow.bsky.social Fediverse: @VeilidNetwork@hackers.town (top) xXx / RULE BOVINIA \\ xXx .. ____ ____ .. .. __| |__ __| |__ .. :: .\\_\\__ __/_/___________\\_\\__ ___/_ :: ____ ____|____ \\| \\_ \\.____ _____ .. _/ __\\__ \\. __|:/ |\\ : :|/ __/ ____ \\ ./ _//| / |_\\__.: _) ./ _ \\:/| / _/\\\\ \\| \\| /|/ /|| \\ \\| \\ \\| . |/ /|.________/|.. \\|\\_________|________/|_ |b5!ACiD |___||____| |\\_________| .. - ---------------|_____/ |-------------------| :----|_____/ --- - .. .. _____ |____| : _____ .. :: \\_ \\. _____ _______ . \\_ \\. :: .. ______| |/ __/ _______\\__ \\ _____|.. :: _( ___ : _) ./ _/ ___. _______ :: :: \\ \\\\: \\\\|| / :: :: \\ \\\\\\\\|| _/ :: :: \\_ \\| . \\| . \\| . \\| ||_) :: :: (_________|\\_________|\\__________|\\_________| :: .. .. .. :: :: :: :: _____ _ ________________ ______ :: :: _/ _/ _____ ____ \\_ \\../ _/ :: :: __\\ \\ ./ _// : ||__ __ :: :: _ _\\\\_\\\\ \\:/ || |/ //_//_ _ :: ::. (_\\\\ \\| / . /\\ //_) .:: .:. \\ \\| . |/ /| /\\ _/ .:. ::. \\_________|_________/ |___ / \\ ___) .:: ::. . \\/ \\/ . .:: `::. || .::' `:::... || ...:::' ` ```:::::.. .. .::::::::::::::::::::. .. ..:::::''' ' `. ```::::::::::::::::::::::''' .' `-._. :::::::::::::::::::::: ._. ```---...___ _|_ . :::::::::::::::::::::: . _|_ ___... : : :::::::::::::::::::::: : : : :::::::::::::::::::::: : . . .......: :::::::::::::::::::::: :.. . .. . . ................::::::::::::::::::::::................ :::'''''':'::':::::::::::::::::::::::::::::::::::::::::: :: ::::::::::::::::::::::::::::::::::::::::::::::::::::: :: ::::::::::::::::::::::::::::::::::::::::::::::::::::: ::'::::::::::::::::::::::::::::::::::::::::::::::::::::: :::::::::::::::::::::::::::::::::::::::::::::::::::::':: ::.::::::::::::::::::::::::::::::::::::::::::::::::::::: :::::::::::::::::::::::::::::::::::::::::::::::::::::.:: ::::::::::::::::::::::::::::::::::::::::::::::::::::: :: ::::::::::::::::::::.:::::::::::::::::::::::::::::::: :: :::::::::::::::::::::::::::::::::::.:::::'::':'''''''.:: `::::::::::::::::::::::::::::::::::::::::::::::::::::::' . . ........ :::.::::::::::::::.::: ........ . . : ::: :::::::::::::: ::: : : ::: .............. ::: : . : ::: :::::::::::::: ::: : . . ::: .............. ::: . . : ::: ::: . :. ::: ::: .: ___|.::' . .......... . `::.|___ ________\\ __| .:::' `:::. |__ /________ ________\\ _______ \\ ..:::' `:::.. / _______ /________ \\ _______ \\ \\ \\ ::: ::: / / / _______ / __ \\ \\ \\ \\ \\ ::: ::: / / / / / __ \\ \\ \\ \\ \\ \\ ::: ::: / / / / / / \\ \\ \\ \\ \\ \\ ::: ::: / / / / / / \\ \\ \\ \\ \\ \\ ::: ::: / / / / / / \\ \\ \\ \\ \\ \\ ::: ::: / / / / / / \\ \\ \\ \\ \\ \\ \\ ::: ::: / / / / / / / \\ \\ \\ \\ \\ \\ \\ ::: ::: / / / / / / / \\_____\\ \\ \\ \\ \\ \\ ::: ::: / / / / / /_____/ \\ \\ \\ \\ \\ ::: ::: / / / / / \\ \\ \\ \\ \\ ::: ::: / / / / / \\______\\ \\ \\ \\ ::: ::: / / / /______/ \\ \\ \\ ::: ::: / / / \\ \\ \\ :::::: / / / \\______\\ \\ :::: / /______/\\ :: /\\ /\\ /\\/",
    "commentLink": "https://news.ycombinator.com/item?id=40167905",
    "commentBody": "Cult of the Dead Cow – Veilid (2023) (cultdeadcow.com)180 points by dp-hackernews 22 hours agohidepastfavorite111 comments freedomben 20 hours agoThis was actually announced/released last year. It is absolutley fascinating and deserving of HN (IMHO). > Veilid is an open-source, peer-to-peer, mobile-ﬁrst, networked application framework. Website: https://veilid.com/ Overview: https://veilid.com/docs/overview/ Slides from the Defcon presentatino: https://veilid.com/Launch-Slides-Veilid.pdf Code: https://gitlab.com/veilid/veilid reply yellow_lead 15 hours agoparentMan, the website really needs to be updated then. I checked over a year ago and it still has this > The code for VeilidChat will be available on Gitlab in the coming weeks. reply smusamashah 18 hours agoparentprevI have seen other similar tools/tech on hn https://briarproject.org/ https://github.com/berty/berty There were 1 or 2 more like these but don't remember there names. reply jszymborski 16 hours agorootparentI know MaidSAFE started out this way in 2006, but I think the I had a crypto digression https://maidsafe.net/ reply programmernews3 10 hours agorootparentIts a crypto grift reply jszymborski 8 hours agorootparentSadly it looks like it. Wasn't always that way, having started in 2006! reply khimaros 17 hours agorootparentprevveilid is meant to be a bit more general purpose than briar or berty, which are primarily chat apps. veilid is more similar to freenet in some sense. note: veilid does not currently support bluetooth transport. reply ganoushoreilly 19 hours agoparentprevDefcon Launch party was fun too! reply egypturnash 20 hours agoprevHas VeilidChat (or anything else running on top of Veilid) been released? The page for that (https://veilid.com/chat/) says its code will be released “in the coming weeks”; the whole Veilid site looks unchanged since its initial publication back in 2023. Edit: ah, some bouncing around through their FAQs found a repo for it that has commits within the last week/month: https://gitlab.com/veilid/veilidchat - looks like “hand this to your non-technical friends” is still a very long way away. reply khimaros 17 hours agoparentunfortunately most of their up to date documentation is on discord. there is a channel there with a list of active projects. this one caught my eye: https://github.com/cmars/distrans/ reply squigz 13 hours agorootparentThere's more than a little irony in a group complaining about the commercialization of the Internet putting documentation/information on Discord reply khimaros 11 hours agorootparentit's a bummer and limits my engagement with the community. however, it seems to be working pretty well for the people who are most involved and the community is pretty active. personally, i hope some of the use cases move to veilid chat once it is available. worth noting that their target audience is \"normal humans\", so being on discord may be helping them engage with that audience. reply fullspectrumdev 13 hours agorootparentprev> documentation on discord Disgusting. I have been looking for solid example code to play with building stuff on top of Veilid, but I’m absolutely unwilling to waste time on that shit chat platform tbh. reply fabrice_d 13 hours agorootparentNot sure how up to date these are, but they also have docs hosted at https://veilid.com/docs/ reply khimaros 11 hours agorootparentthere is a lot missing reply jeroenhd 19 hours agoprevI had high hopes for Veilid when it was unveiled (ha) but I stopped hearing about it soon after it was published online. Veilid Chat didn't really seem to work once I found the source code and except for a few \"hello world\" networking programs I haven't seen anything use the protocol yet. The official website doesn't seem to be getting any updates anymore. A shame, because this has a lot to offer, in my opinion. reply Retr0id 17 hours agoparentI think it basically released before it was ready, to much fanfare. Not an undeserved fanfare, but a premature one. I think the concept is solid and they're still actively developing it, but it's not really something end-users can play with yet. reply andoando 7 hours agoparentprevThe issue as I see it is free open source software can't normally compete with commercial simply because it relies on the good will of developers to contribute. People need money to live, most of us can't dedicate hours a day to working on something like this. For every 20 devs you can get to do this theres a 1000+ working for the commercial equivalent. We need a paid, non corporate model, not free. reply ganoushoreilly 19 hours agoparentprevThey're still updating it regularly, but it hasn't grown nearly as fast as they wanted it to. Last I heard they're working on doing nightlies and weeklies for release too. I don't really think that matters as much as just having a good release schedule and tools that leverage it. reply gnuser 10 hours agoparentprevlove cDc but they drop more projects than google - people just remember the ones that stick/stuck reply 31337Logic 21 hours agoprevWow. Today I learned CdC and ACiD are still a thing. ;-) Thanks for posting this. Even though slightly old, still timely and relevant. reply hypercube33 20 hours agoparentSimilar to the cDc but instead of hacking it goes after culture and religion is the Church of MOO which I think came out of the same bbs/usenet and irc era of the Internet if you could even call it that http://www.textfiles.com/occult/MOOISM/ reply broost3r 13 hours agoparentprevwhat a great username to go with this comment reply HDPDV 16 hours agoparentprevCULT OF THE DEAD COW. TODAY. TOMORROW. FOREVER. reply LastNevadan 17 hours agoprevI'm amazed that CdC is still around. I remember dialing into their BBS, Demon Roach Underground, in the mid 80s. That was nearly forty years ago! https://en.wikipedia.org/wiki/Demon_Roach_Underground reply netsharc 16 hours agoparentOne of their members had presidential hopes a few years ago: https://arstechnica.com/information-technology/2019/03/it-tu... reply HDPDV 16 hours agoparentprevThe phone number is seared into my brain for life, I think. I can't remember my girlfriend's phone number, but the number of BBSs I called 30+ years ago? That's stuck in there until the day I die. reply ergonaught 7 hours agorootparentCan’t remember the last time I called Ripco or Lunatic Labs but I still know their numbers better than any other phone numbers, including my own. reply dgellow 20 hours agoprevThe project website has more information https://veilid.com/ reply gryfft 21 hours agoprevThe web needs more sites hosting raw .html pages formatted as plain text decorated with ASCII art with zero regard for mobile. I say this with complete sincerity. reply gwern 14 hours agoparentYou can support mobile with ASCII art if you render at different widths and use a HTML+CSS wrapper for media-queries! I have a whole proposal about this: https://gwern.net/utext reply pyinstallwoes 11 hours agorootparentLove the thinking here. Thanks for the research and suggestions. reply nulbyte 21 hours agoparentprevIt's not even that much of a problem for me on mobile after zooming out just a tad. The green on black is really what makes this page for me. reply oytis 20 hours agoparentprevNo cookies, no frameworks (no JS at all really), plain unobfuscated html. Amazing! reply ganzuul 16 hours agoparentprevStill waiting for an LLM to communicate exclusively in this format. reply flipdot 20 hours agoparentprevWhat about accessibility? reply solardev 20 hours agorootparentThat's what 8-bit text to speech was for, played from the PC Speaker for maximum effect. It sounded like Stephen Hawking choking on vodka, but that somehow fit the mood. reply freedomben 20 hours agorootparent> It sounded like Stephen Hawking choking on vodka I'm probably way, way overthinking this, but this seems philosophically quite deep and interesting, much like \"what is the sound of one hand clapping\" (ignoring Bart Simpson's masterful destruction of the ancient question). reply ganzuul 16 hours agorootparentLooking forward to your zine publication. reply the_real_cher 20 hours agorootparentprevWouldn't raw html be better for accessibility than a JS framework? Saying this as a non-front end dev. reply ivan_gammel 20 hours agorootparentIf done correctly it doesn’t matter. SSR can yield an accessible HTML page and you won’t notice the difference. Client-side JS can adapt the web site to your needs -a personalization that is hard to achieve in static. reply marcosdumay 19 hours agorootparentWhen was the last time... Or rather, when have you ever seen a framework-based page with accessibility done correctly? reply ivan_gammel 19 hours agorootparentI have seen many sites scoring well on various accessibility metrics. There’s no inherent technical limitation of frameworks that would make accessibility impossible, even when dealing with text to speech. We will see many more next year with EAA coming into effect (and many European companies do care about compliance). reply jilijeanlouis 2 hours agorootparentDespite regulation I don’t Believe it will actually get implemented even governments have regulations around audio accessibility for education in particular, for years now, and still nothing moves. reply Conlectus 20 hours agorootparentprevRaw HTML: potentially. Big blocks of undifferentiated ASCII art: no. reply garfij 16 hours agorootparentaria-hidden=true reply joemi 13 hours agorootparentNot present on the page in question, though. reply oytis 20 hours agorootparentprevAccessibility of this page is pretty bad as far as I can tell, but not because it's plain HTML. And if I understand correctly you can mark ASCII art as an image (role=\"img\") with alternative text too. reply troyvit 19 hours agorootparentprevOooh good point. W3c accounts for it, and it's not that tough. Just a case of putting in the raw html: https://www.w3.org/TR/WCAG20-TECHS/H86.html reply wddkcs 19 hours agorootparentprevNot everyone has to access everything. reply warkdarrior 20 hours agorootparentprevFuck accessibility. We're hackers, if it was hard to write, it should be hard to read. reply ivan_gammel 20 hours agorootparentnext [3 more] [flagged] freedomben 20 hours agorootparentHackers come in all ages, colors, shapes, and sizes. The ethos is one that is the very opposite of identitarianism. I would highly recommend getting to know more people in the community. Also, I would guess that GP is making a joke based on a classic (and funny) stereotype. reply ivan_gammel 20 hours agorootparentThere’s no such thing as single community of hackers with some ethos. Exactly because “identitarianism” isn’t their thing. The hats are of different colors. GP may be making a joke, but you should read the irony in my comment too. From my experience people who disregard accessibility are often the ones who were never disadvantaged or discriminated. Hence the attributes I mentioned. reply deadbabe 19 hours agoparentprevUse HTMX. reply colecut 19 hours agorootparentHTMX is cool but not necessary for static html pages.. I prefer to use HTMX in place of other frameworks, but if you don't need a framework at all, even better! reply deadbabe 16 hours agorootparentSorry :( reply beardog 19 hours agoprevI went to the launch party they had during defcon, it was a lot of fun. I like that Veild is oriented toward being an application framework. In the same way that we have things like libsodium to use cryptography in our apps without being a master of it, we need frameworks/libraries to help build privacy oriented apps as well. reply hi-v-rocknroll 11 hours agoprevWoah. A blast from the past from the era of l0pht, shmoo, and w00w00. I'm wondering how they defend against tor's problem where large % of nodes are malicious. More doc: https://veilid.gitlab.io/developer-book/index.html reply dustfinger 19 hours agoprevOh man, does this bring back memories. I loved BackOrfice. I installed it on the network at the University where I was a student. I had a friend call me on a payphone from a vantage point where he could view the layout of the computer terminals being used by students doing research. At first, I would open the cd trays of a couple of the computers and he described the students confusion as they kept closing the trays that would open again moments later. After sharing a good laugh, I popped up a dialog on one of the computers with a message similar to: > Hey, I am the guy at station #7. I think you are really hot. Want to go out tonight? Then, I sent similar messages to other stations until we were nearly in tears laughing at the chaos we caused. Ahh, those were good memories. reply uhoh-itsmaciek 18 hours agoparentThat sounds like a crummy thing to do to the people in the lab. reply devjab 17 hours agorootparentThe internet was different back then. We used to put trojans into image files which we distributed through a fake dating site peofile. For shits and giggles. We never did anything “serious” just stupid things like opening CD rom drives or moving their mouse around. We’d UDP “nuke” teacher computers. We’d use open networks, and download warez… all sort of silly stuff. Looking back on it… well a lot of it obviously wasn’t cool at all. I’m just happy that my “teenagers do stupid things” happened in a time where the internet crime was basically not taken serious unless you hacked a bank. reply uhoh-itsmaciek 16 hours agorootparentIt's just a different kind of bullying. reply dustfinger 15 hours agorootparentIt wasn't meant as bullying and certainly wasn't seen that way back when I did this. It was mischevious practical jokes that people laughed at. The world we live in now is not as happy of a place. I feel bad for youth of all ages growing up in the world we have now. reply orthecreedence 12 hours agorootparentprevLabeling practical jokes as bullying really kind of waters down the idea of actual bullying, where emotional or physical harm is caused to someone over a long period of time. Let's not let bullying be the new \"trauma.\" reply a_vanderbilt 19 hours agoprevThey gave a great presentation on veiled for us at BSides Orlando last year. At the afterparty, I had a chance to discuss the protocol with Paul over some drinks. They've really thought through the design and he had answers for almost all of my what-ifs. reply RyJones 13 hours agoprevThey recently became a member of PQCA, too. Disclosure: I work for LF, assigned to PQCA. https://pqca.org/#members https://veilid.org/ https://github.com/pqca reply klaussilveira 18 hours agoprevThe amount of people in this comment section, on \"Hacker News\", that are completely oblivious to one of the most iconic groups of hacker culture is... depressing. I wonder how can we spread more zeitgeist about it to newer generations? reply DoreenMichele 18 hours agoparenthttps://en.m.wikipedia.org/wiki/Cult_of_the_Dead_Cow I dunno, but maybe try being informative instead of critical that losers like me never got the memo? reply klaussilveira 18 hours agorootparentI'm sorry if it felt critical or condescending, it certainly was not the intent. If anything, I just want people to experience that coolness the same way I did. reply elzbardico 14 hours agoparentprevA frequently overlooked issue about Gen-X online culture is that it was always have on gatekeeping, snobbism and hierarchy. It is not surprising that newer generations don't know it, because at this time, people did the most to keep things on their small clubs of initiated folks, newbies NOT welcome. Frankly, millenials and zoomer have a far more open and welcoming approach, and probably their culture will survive better because of that. reply UberFly 13 hours agoprevI watched them unveil Back Orifice at defcon in the late 90s. Those were fun times. reply pyinstallwoes 11 hours agoprevThis is kind of exactly what I’ve been looking for, and have shared at various times features of on hnews and elsewhere. Nice! Thank you! reply tylershuster 15 hours agoprevIt borrows some of its marketing directly from Urbit, which HN famously has a bias against. reply seomint 12 hours agoprevThat beautiful phosphorus green... reply trustno2 20 hours agoprevThe demo VeilidChat app doesn't lead anywhere, but there is a gitlab. I haven't tried it yet. https://gitlab.com/veilid/veilidchat edit: I did, I failed to build it with \"version solving failed.\", I am not learning how to debug Dart builds right now edit2: well, build.sh finished with some installing, but I still don't see any binary. Eh, other day. reply badgersnake 17 hours agoparentThere is a discord, which kinda misses the point. reply cdchn 12 hours agorootparentThink this is something they'd be trying pretty hard to self-host. reply Aerbil313 13 hours agoprevAlso check out https://freenet.org. Ian is going to launch in a few weeks if all goes well. reply orthecreedence 12 hours agoparentI think Veilid is closer to https://iroh.computer/ than Freenet. That said, I'm watching all three extremely closely. reply underseacables 18 hours agoprevThis brings back so many BBS memories! reply solardev 20 hours agoprevWeren't these the guys who released BackOrifice back in the day? It was a simple to use remote control trojan with a nice GUI... had a blast with it :P https://en.wikipedia.org/wiki/Back_Orifice?wprov=sfla1 As a kid, I attached it to some shareware game and sent it to a friend, letting it lurk. I called him up a few days later. And then once we started playing the game, I waited for a suspenseful moment to suddenly play back a loud scream .WAV that I uploaded previously. My friend jumped out of his chair and screamed himself and ran out of the room. He eventually came back, hyperventilating, and sat down to try to tell me what happened, only for his CD tray to start opening and closing at random. He ran away again, swearing about his haunted PC... Eventually he told the school principal, who sat us down and made us explain what modems and trojans and ports were. Then he asked us if we knew what an orifice was, and how that was connected to ports... sigh, the kind of discussion you never wanted to have with a grown-up. We were young. The internet was young. Things were wild and free and not so hypercommercialized and buttoned down yet. Google wasn't around and Apple was for homework and Hypercard. Microsoft still had flight simulators in Excel. Good times... reply Terr_ 18 hours agoparent> to a friend I think with my friend I pitched it more like \"I tricked you for your own good to show you not to trust random EXE files\"... though I don't think it was quite that altruistic a prank. :p I recall another similar trojan (perhaps a little later) was Netbus, both showed up a lot when volunteering on an IRC help channel to help diagnose and walk victims through removal. reply justanother 19 hours agoparentprevAround that time, maybe a little bit earlier, I was a Sun nerd surrounded by other Sun nerds, but this worked for Linux too: We'd FTP into each other's machines and upload things like .au files of lonely whale cries into /dev/audio for an endless supply of WTF Moments. reply gruturo 19 hours agorootparentOh the good times in the Un*x lab (Mostly AIX, sigh) in my first year of Uni. Telnet (What? SSH in 1995-6? nah) to friend's workstation DISPLAY=0:0; export DISPLAY xwininfo -root -all *find some candidate window or control* xkill -id XXXXXX (or just fire a perl oneliner to allocate the sum of memory and swap, and then repeatedly scan it in a random pattern. But that wasn't me).* reply kevindamm 19 hours agorootparentAnd `talk`, the best chat client created ever. reply nsxwolf 20 hours agoparentprevMy friend pulled a similar prank on me a few years earlier than that. Involved different tech like BBS software, ZMODEM file transfer, and Sound Blaster command line utilities but same effect. I nearly died. reply solardev 20 hours agorootparentHe was a pioneer :D The granddaddy of us script kiddies. The BBS door game Legend of the Red Dragon (https://legendreddragon.net/) was how I learned about everything from protocols to sex to RPGs. Such an innocent time. The media was afraid of Doom corrupting the youth. reply MikeTheGreat 13 hours agorootparentI was curious about how you'd learn about sex in an RPG and followed your link (it's a slow day :) ). This is hilarious, and reasonable tasteful, all things considered: https://nuklearlord.fandom.com/wiki/Lay https://nuklearlord.fandom.com/wiki/Pregnancy https://nuklearlord.fandom.com/wiki/Venereal_Disease https://nuklearlord.fandom.com/wiki/Children reply mtillman 20 hours agoparentprevIt’s worth reading their new book which is a history of the group. The chat was launched at defcon last year at their birthday party and there will be more this year. reply solardev 19 hours agorootparentI didn't know they wrote a book! Just bought a copy. It's probably gonna pwn my ereader now, but worth it for the lulz. reply 20 hours agoprevnext [2 more] [dead] kstrauser 17 hours agoparentDon’t make it weird. reply josephd79 21 hours agoprevyer mom. haha reply debo_ 21 hours agoprevThis feels like it would have been edgy and cool 30 years ago. reply jrochkind1 20 hours agoparentI don't know if you're talking about the design or content. But on design... So Cult of the Dead Cow has literally been around for 40 years. And this is what their \"pages\" looked like before the web, when ascii on TTY was all you had. Ironically, however, the first actual web page of theirs IA has, in 1998, does _not_ look like this, they didn't actually think at that point it would be edgy and cool to make the brand new web look like an ASCII tty. https://web.archive.org/web/19980209125729/http://cultdeadco... It actually wasn't until 2019 they decided it would be maybe edgy and cool to make their webpage look like an ascii tty from 1990. https://web.archive.org/web/20190530041326/https://cultdeadc... reply shawn_w 20 hours agorootparentI remember reading the text files section of cDc's page back in the 90's. There was some sick and twisted stuff in that collection. Thanks for the flashbacks? reply debo_ 20 hours agorootparentprevNot the page design, the method itself. \"Bovine mother?\" Come on. reply wyck 18 hours agorootparentThe CDC was formed at an actual cow slaughterhouse. Know your history https://en.wikipedia.org/wiki/Cult_of_the_Dead_Cow reply debo_ 16 hours agorootparentYes, I got it, thanks. reply jacoblambda 20 hours agoparentprevFYI: Cult of the Dead Cow is a hacker group that has been around since the 80s. Hell a number of members actually have testified in congress before (i.e. the CdC members that were also part of L0pht). reply parpfish 20 hours agorootparentone of them went beyond testifying in congress and just became a politician [0] https://en.wikipedia.org/wiki/Beto_O%27Rourke reply jacoblambda 16 hours agorootparentOh damn I didn't even connect that O'Rourke was Psychedelic Warlord. reply Terr_ 18 hours agorootparentprevHoly crap, that is a surprising connection between two names I individually knew in a social graph. reply Terr_ 18 hours agorootparentprev> L0pht Oh man, that reminds me of NTLM passwords: https://en.wikipedia.org/wiki/L0phtCrack reply glonq 18 hours agoparentprevEspecially to those of us who were edgy and cool 30 years ago. reply realce 20 hours agoparentprevWhat have you done lately? reply debo_ 20 hours agorootparentQuite a lot, thank you! reply realce 20 hours agorootparentedgy! reply debo_ 19 hours agorootparent<3 reply tetris11 21 hours agoprev [–] Is this in reference to tucows? reply rmi_ 21 hours agoparentNo: https://en.wikipedia.org/wiki/Cult_of_the_Dead_Cow reply riffic 16 hours agoparentprev [–] TUCOWS for those that don't know was an acronym for \"The Ultimate Collection of Winsock Software\" (fun fact; let's please have a bit more fun on orange site the debby downer thing here is something sometimes). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Veilid is a new decentralized application designed to offer online interactions without exploiting user data for profit.",
      "The platform will be introduced at DEF CON 31 in Las Vegas, featuring events, demos, and details about the foundation, contributors, and press releases.",
      "Emphasizing privacy, Veilid's launch signifies a move towards more ethical approaches in the tech industry."
    ],
    "commentSummary": [
      "The Cult of the Dead Cow unveiled an open-source application framework named Veilid in 2023, sparking comparisons with Briar and Berty.- Users have raised worries about Veilid's accessibility and documentation, reflecting concerns amid Internet commercialization.- Discussions include nostalgia for early internet tech, website accessibility using JavaScript, hacker culture, and the significant history of groups like the Cult of the Dead Cow, reminiscing about its influence on the hacking community and links to figures such as Beto O'Rourke."
    ],
    "points": 180,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1714129569
  },
  {
    "id": 40173237,
    "title": "Master Coding Problems with Spaced Repetition Tool",
    "originLink": "https://www.lanki.xyz/",
    "originBody": "As you solve LeetCode questions, you can mark them as hard, medium, or easy. The tool will then recommend questions you should review based on (1) how hard the question was for you and (2) how much time has passed since you last reviewed it. I&#x27;d recommend normally attempting LeetCode problems and just marking them as hard, medium, or easy for you at first so the tool knows which problems to recommend you review!Here&#x27;s the theory behind spaced repetition and learning if interested: https:&#x2F;&#x2F;www.codecademy.com&#x2F;article&#x2F;spaced-repetition",
    "commentLink": "https://news.ycombinator.com/item?id=40173237",
    "commentBody": "I made a spaced repetition tool to master coding problems (lanki.xyz)158 points by cubemaster 14 hours agohidepastfavorite67 comments As you solve LeetCode questions, you can mark them as hard, medium, or easy. The tool will then recommend questions you should review based on (1) how hard the question was for you and (2) how much time has passed since you last reviewed it. I'd recommend normally attempting LeetCode problems and just marking them as hard, medium, or easy for you at first so the tool knows which problems to recommend you review! Here's the theory behind spaced repetition and learning if interested: https://www.codecademy.com/article/spaced-repetition personjerry 10 hours agoI feel like spaced repetition is exactly what you shouldn't do. Spaced repetition is great for rote memorization of arbitrary facts but programming is about how to think about and solve problems. If you master the methods and ideas, you should be able to derive the answers on the spot. That's better than memorization because then you can actually deal with real interviewers who 1) Want to hear your train of thought and 2) Will give you random changes or variants to problems. It's not about memorizing solutions, because in reality when are you ever going to reverse a linked list or balance a red-black tree? No, so if you're going to put in the effort anyway, you should learn the concepts and understand when to apply them so your knowledge is actually applicable. This will actually make you a better programmer, and you'll do better with the interviews that are actually good (I speak with experience giving interviews at FB). reply ammasant 5 hours agoparentLearning the concepts and using SRS are not only not mutually exclusive, they're complementary. There's no quicker way to \"grok\" a deep technical or mathematical insight than to simply expose yourself to it in a diversity of cognitive contexts. SRS works incredibly well because it generates this diversity as a function of regularly scheduled time. Think of it as entropy generation coupled with heavy reinforcement. reply Mc91 5 hours agorootparentRight. I learned what binary trees were in college and didn't directly use them much afterward. Since interviews sometimes ask about trees, I started my practice of solving tree problems by practicing how to invert a binary tree. I practiced it so much that I can invert a binary tree from memory now, so that I won't get bounced from an interview like the inventor of Homebrew was from a Google interview. Most of the concepts I learned, although I don't remember learning Kadane's algorithm for solving the maximum subarray problem in school, so I've learned some things like that. I can also implement a solution for an associated Leetcode problem in one line of code, so I wound up memorizing that too. OP says \"If you master the methods and ideas, you should be able to derive the answers on the spot\", but I hear from a lot of people that does not happen, even (implicitly) in this thread. Any how, even if I use OPs method, if I do the same problem over and over and over again I'm going to wind up memorizing it any how. reply eru 1 hour agorootparent> [...] so that I won't get bounced from an interview like the inventor of Homebrew was from a Google interview. Just for the record, that never actually happened. It's just a thing the homebrew guy made up. (I know, the rest of your points are unaffected by this. I just feel annoyed when we casually warm up old myths.) reply luc4sdreyer 20 minutes agorootparentHe admitted to lying about it? Do you have a source for that? I couldn't find anything. reply ffsm8 3 hours agorootparentprevLearning is like nutrition. There is no ”one true way\", instead there are lots of people and each one has their own approach to learning. Some people are closer to each other, and upon realizing that they believe they've found a pattern. But it's like when people tell you that you need to supplement x in your diet. It's possible that x helped you. And it's even possible that you'll find other people with the same nutritional deficit, creating the illusion that everyone would benefit from supplementing. But that's just what it is: a premature extrapolation from anecdotes reply kqr 1 hour agoparentprevI disagree. To be fair, I used to think this too, but then when I started to actually used spaced repetition in the process of learning more complex subjects I became surprised by how much more fluid my reasoning became. Writing good flashcards is about - noticing similarities and differences, - exploring variations of a concept, - finding generalisations of related concepts, - learning properties of a concept, etc. All these things are things you use when reasoning also about more complex ideas. In other words, spaced repetition is an efficient way to raise the baseline for what counts as a \"fundamental fact\" which makes it possible to think at a higher level of abstraction. reply gibolt 8 hours agoparentprevSpaced repetition is a good way to remember anything, including methods. Just review the method (maybe with a small practice problem) on an SRS-based schedule, to hone your skill. When you encounter an issue in the wild, having committed to memory ways to solve it will speed up the actual execution. reply moneywoes 5 hours agorootparentAnd timing is key during an interview reply BeetleB 7 hours agoparentprev> Spaced repetition is great for rote memorization of arbitrary facts Traditionally, yes. But several people use the SR algorithm to simply do the same problems over and over (practice), without the memorization component. Looking at a typical SR algorithm, you're not really going to memorize doing the same medium/high Leetcode problem unless you actively try to memorize it. The interval sequence will be something like: 2, 5, 13, 35, 120, etc. By repeatedly solving the problem so few times in such a long period (and while solving other problems at the same time): You're not really going to memorize it accidentally. reply waprin 9 hours agoparentprevThat’s a fair perspective but consider that spaced-repetition can be an “MVP” of the broader concept of adaptive / personalized learning. You track some state about your knowledge of the world, in this case your performance on a flash card, and optimize the next piece of educational content around it. It feels obvious to me that, in theory, we could do a lot more to leverage your current world knowledge to recommend the next piece of educational content, to optimize not just your understanding but also other things like how engaged you are. For example, a system could throw you an easier question if it helps you focus longer. When I see the state of current interview prep it’s mostly “here’s a big list of questions” , perhaps tagged by difficulty and grouped under related topics such as “graph problems” or “tree problems” but I’m personally convinced a more sophisticated system could serve you the best possible next problem to stretch your brain in the right way. Spaced rep is simply the proven starting point. The rest of adaptive learning has a bit of a troubled past because it was trendy to VCs , but pushed on educators and presented more as an alternative to a teacher rather than something to augment a teacher. I worked for a company that raised 100M+ to work on it but the CEO was great at terms sheets but uninterested in actually building a great education project. But the reason that I joined the company was the high level idea still resonates with me. Surely many HN users have a big list of things they want to learn- perhaps about LLMs. But a typical course will have zero knowledge on where you’re starting as a student . It might bore you with stuff you know already or take for granted key prerequisites and skip them. Anyway, I’m currently building my own adaptive learning platform but focused on helping professional poker players learn game theory . My idea being that it’s an easier ed tech app to bootstrap as the knowledge very directly translates to money. And really the same criticism applies because you can’t truly memorize a game tree , it’s more important that you build a high level conceptual understanding. But , as much as rote memorization deserves to be maligned when done in isolation, it’s not so bad when done as part of a broader learning strategy. For example, you can’t memorize vocab to learn Spanish but certainly knowing 5000 words in Spanish is a very nice starting point compared to not knowing any words. And tools like spaced rep have been proven by research to help with that goal and I view as a pathway to more broad adaptive learning strategies. reply Sakos 1 hour agoparentprevI thought we were done with the myth that once we understand something, we don't need to memorize. I've come to the realization that memorization is a core competency. The more I have memorized, whether it's syntax or concepts or problems and how to solve them, the better equipped I am and the more productive I am in the future when faced with even novel problems. Memorization saves me insane amounts of time in day to day programming and gives me an edge in being able to solve and understand problems. I don't just have a deep understanding of the code bases I work with, I have a lot of details memorized about what parts there are, how they work, how they're interconnected, certain technical details, etc. And sure, I could just check, but when having meetings about technical problems and planning our projects, it's an insane help to know so much off the top of my head and that I only need to check a handful of things. Spaced repetition is nothing other than intentionally refreshing something you currently know, to ensure that in a month or a year or longer, you'll still know it. It's not your place to judge what to memorize or not. I'd suggest you should also use spaced repetition on the methods and ideas, since we're very prone to forgetting that too. I would also never criticize anybody for memorizing something, since it's literally just doing intentionally what we would otherwise do haphazardly and by accident. reply adamtaylor_13 6 hours agoparentprevI believe this is incorrect, primarily because this approach has never worked for me. Incidentally, just today I found an article that has put this feeling into words really well: https://nautil.us/how-i-rewired-my-brain-to-become-fluent-in... reply lupire 6 hours agorootparentIncredibly long rambling story just to say that exercise helps learning. Who would ever claim otherwise? reply reaperman 6 hours agoparentprevA lot of my personal opinion follows, and I'd be very, very happy to hear dissenting opinions! I believe there are at least two valid goals for learning that are relevant to memorization: 1) Learning for mastery / competency to apply the skills to real life endeavors 2) Learning so you can meet some standard necessary to enable certain opportunities in life It seems your opinion comes from a personal value bias towards #2 and I share that bias, but perhaps not as dominantly as you do (at the moment). For both of these, memorization can be very helpful, even optimal. But the application of spaced repetition would be different depending on the goal. On memorization applied to learning for genuine personal mastery: I believe memorization is optimal for foundational axiomatic knowledge. A good example is quick addition and times tables (and other arithmetic). I believe: 1) People should consider it a necessary life skill to be able to perform reasonably complex arithmetic manually in your head. For example, 4462+9241=13703 or 25120=3000. Many people will disagree on the nuances on where this begins and ends, and that's okay. For example some people will feel that people should be able to multiple 27123 in their head, others would feel it's okay for something like that to be done on paper. But for me, the point is that there exists a category like \"things you should be able to do, but it's fine not to have memorized\". More importantly, I believe it would be suboptimal to spend a significant amount of your life memorizing an increasing sample of these, but it's reasonable to spend time maintaining the skill of manual mental arithmetic and probably reasonable to spend time improving the speed and accuracy with which you do it. -- 1a) A nuanced subset of this skill example is \"estimation\", for situations where it's appropriate. It might take longer for some people to manually calculate 27123 (or whatever) than for them to pull out their cell phone or ask their Apple Watch. If the situation only needs them to have an upper and lower bound within they might break it down to {lower_bound = 25120 = ((256)2)10 = (150210) = 3000; upper_bound = 30125 = 3055 = 150510/2 = 75010/2 = 7500/2 = 3750} and they could determine the answer is around 3350 or so, but absolutely 100% definitely between 3000 and 3750. And they could still do this type (if not degree) of estimation faster (and more stealthily, which can be important sometimes, like during negotiations/haggling) than pulling out a cellphone and calculating. 2) People absolutely cannot do arithmetic if they do not have certain foundational axioms memorized. If you do not have 1+1=2, 2+1=3, 3+1=4, etc memorized (counting), you literally cannot do arithmetic. That would be an absurdly low-functioning adult and essentially means you don't know how to count. But technically* you only need to be able to count to add, subtract, and multiply (by maintaining two counts simultaneously) any real numbers. Obviously just counting is a helpful, but not optimal, level of memorized knowledge to be competent at arithmetic. It would plainly be helpful to also memorize things like 7+6=13 and 152 = 30, and have instant recall for these rather than re-calculating them manually every time you need them. Which brings me to: -- 2a) I believe it is helpful to memorize more than what many people think it is helpful to memorize. We generally are taught in primary school to memorize times tables up to 10x10. But perhaps it would be useful to extend this beyond 10x10, at minimum for some additional prime numbers which might show up frequently and cannot be quickly factored to smaller numbers. I could see it being super nice to have my times tables memorized up to 100x100, and additional addition/substraction memorized. I feel this way because despite having done well in three semesters of calculus and differential equations, I have always had a few \"holes\" in my brain's lookup table for certain simple addition and multiplication - specifically it really slows me down to need to manually calculate 86 (I have to do 832) or add 16+7 (I personally have to do 20+4+3). It's very frustrating to hit these basic roadblocks. Often I wonder how much happier I'd be if, at a young age, I had personally chosen to endeavor to memorize an unusually large amount of basic integer combinations, rather than just focus on memorizing the minimum necessary to get 100% on my K-5 coursework. Why do I not know 86? Because I took the less ambitious route of just breaking it down to 446 or 823 every time. Which brings me to: On memorization for learning so that you can meet some standard necessary to enable certain opportunities in life: 1) I think we can all agree that some things are worth striving for, but are gated behind formal or informal tests which aren't relevant to the skills and temperament that actually enable people who do those things to do them well. For many people, that's \"leetcode\", but it can also be things like a college graduate memorizing and practicing ad nauseam a powerpoint delivery for a technical role for which they'll never formally present anything in their entire career. It could also be memorizing certain key motifs in a history or politics or English class which are actually wrong. The student may even know* its wrong, but it must still be memorized and regurgitated to get the best possible grade for their transcript. 1a) I think we can all agree that when cramming for an exam for which knowledge is not particularly desired to be retained, the best memorization technique is the one that takes the least amount of time to remember the facts just long enough to regurgitate them for the exam. I've discovered that this method is not the most commonly evangelized \"spaced\" repetition, but rather a Leitner system implemented in a way such that you actually minimize the time spacing as much as possible. For me, this uses digital or physical flash cards. Make a deck containing all facts you want to be able to maccess n days later during the exam. Go through the deck one at a time and any you get correctly, move to a new pile on the right; any you get wrong, move to a new pile on the left. Keep going repeating this with the left-most pile without breaks or pauses until you are getting enough % correct to get the desired score on the exam (with some safety margin), or you get exhausted or stop improving (then sleep and do it again the next day). This takes up far less actual time than spaced repetition, but you'll also forget the information fairly quickly as well and retain much less of it permanently. -------------- To bring it back to the actual topic of memorizing leetcode: For me personally, I think memorizing some leetcode can be useful in three ways. 1) (For both mastery and credentialism) This is stretching the definition of leetcode a lot, but it's worth mentioning. I think it's a good idea to memorize syntax and functions/methods of the language, libraries, frameworks, and tools that you intend to use. I switch languages a lot at work, and I switch jobs a lot. It genuinely shocks me how much I can accomplish in some languages for which I can't even write a syntactically correct for-loop. Like if you put a blank paper and a pen in front of me, I could not write a for-loop -- but I can copy/paste one loop from language documentation or google result or ChatGPT and then copy/paste that line from my source code over and over until I have a working API or application or website from scratch in under a day. But I hate it, and not knowing all the syntax makes me slower. Not knowing what methods are available for a certain class is very frustrating. I don't need to have it memorized to do my job, but when I'm working in a language that I do know super-well, I have a much happier time doing the job. 2) (For mastery) Some leetcode could be considered \"merely slightly more complex fundamental concepts\". Nobody would argue that you should be able to write a for loop from memory in your language, but I think it's reasonable to say that it could very well be useful to be able to have instant recall of some amount of slightly more complex algorithms rather than needing to google them, like implementing sliding windows using trees. Or (for low-resource embedded programming) things like exponentially weighted moving average. Etc. What these are depends on what you desire to have mastery over, but memorization via spaced repetition can ensure that these \"leetcode\" techniques are always available to you the instant you need to implement them. 3) (For credentialism) Yes, it would be wonderful if we all had the brainpower to instantly figure out all leetcode problems fluently on the fly without awkward pauses. But most programmers can't, and it's better for the interviews if you can quickly, decisively, and confidently present everything that the interviewers are looking for. For this, I believe it can be genuinely worth spending some of your life memorizing leetcode if it means the difference between making $120,000 vs. $225,000 for the next five years. You probably won't ever use two-sum or flood-fill in your real job, but if it's the requirement to get that job, it can make a massive difference in your quality of life. Offloading brain-processing by memorizing leetcode also allows you to spend more of your higher-level thinking on connecting with your interviewers, figuring out the best presentation style tailored to them, and optimizing emotional expression during the leetcode portions of the interviews. Basically, leetcode isn't the only \"hard\" part of interviews, and it's useful to free up as much brainpower as possible available for the other challenging things that you need to handle simultaneously as well. Most of those other things don't have a \"ceiling\", any additional marginal efficacy might make the difference. I believe spaced repetition is best here instead of time-minimized Leitner method, because not only is it helpful to still have much of this memorized two years later when you might desire to interview again, but also because the complexity of the problems makes a time-minimized approach fairly infeasible. reply kqr 58 minutes agorootparentOn the topic of memorising arithmetic, I have found it surprisingly useful to memorise a few base ten logarithms. While most people have a good sense of numbers as an arithmetic progression, the geometric perspective is often more useful. (Things like how 3 is halfway from 1 to 10, but 10 is halfway from 1 to 100. (This is just the square root, of course, since that is a power of a half. But it's also useful to know that 2 is one-third of the way to 10, and 1.6 is a tenth of the way to 100. I.e. knowing some logarithms, you can mentally compute arbitrary fractional powers in your head.)) For the curious, I write a little more about it here: https://two-wrongs.com/learning-some-logarithms.html reply andrewp123 3 hours agoprevI've been working on a more effective leetcode site - https://deriveit.org/coding/roadmap. The goal of our site is to teach smart people how to quickly master leetcode. We put in a year's worth of thought to make things as simple as possible, and are super proud of our content. Recently, two people used our site to land Amazon. Check it out! reply mrg3_2013 10 hours agoprevI continue to get puzzled with leetcode type things. Will coding exercises like this become obsolete someday or will the old guards continue to push for such things to be used during interviews? Today coding Copilots can easily replace leetcode \"winners\" reply richardw 9 hours agoparentIt replaced “did you go to a top university” and “do you know our exact stack and tools” as the top hiring practice. Now anyone can compete for the same jobs from anywhere in the world. It’s a leveller, in my mind. I applied to Atlassian as a 50 year old from South Africa who has very broad and deep experience but has never worked in big tech. Failed at the last interview (of 5) but I found the process and feedback very fair and much better than others I’ve seen elsewhere. The focus was on collaboration and communication and not being a jerk, along with the problems to be solved. I enjoyed it. reply madamelic 9 hours agoparentprevIt's a cyclical problem. The people / companies using it to hire will tend to hire people who studied leetcode. The companies that don't use leetcode / don't use it as a strong signal will continue not using it. In my opinion, leetcode is a poor signal if used as a binary decision (\"were they right or wrong?\"). The more important thing is communication and how they worked through the problem. I've heard this multiple places and I absolutely agree: attitude over aptitude. You can teach knowledge, you can't teach attitude or ability to problem solve. The companies that use them as a strong signal are the ones that will be absolutely demolished in engineering because they are on a fast track for a staff full of rote memorization rather than strong creative problem solving. They'll be handicapped when it comes to solving problems that aren't covered by leetcode because no one there bothered to learn it. reply dr_dshiv 9 hours agorootparent> I've heard this multiple places and I absolutely agree: attitude over aptitude. You can teach knowledge, you can't teach attitude or ability to problem solve. Seems like society should try to figure this out.. if attitude is so important, why can’t we cultivate it systematically? reply madamelic 9 hours agorootparentI think we can but a lot of people don't / won't. The way to cultivate it is letting people figure things out on their own and rewarding atypical solutions that arrive at the same conclusion. A lot of education tries to deliver in a specific box and a specific way and any other method is punished. Society would likely get more value if education was more diverse in perspective and methods. It took me way too long to realize that I didn't care about anything but computers. I started doing better in my classes when I framed everything through my lens rather than viewing it through the instructor's lens. I don't know the correlation but I think a lot of entrepreneurial families tend to have good problem-solving children or maybe it is a selection bias among friends. It could be genetic but I think these families tend to reward / not punish ingenuity. In addition, I believe part of the equation is leaving children alone to figure things on their own. It sucks but I think that letting your kids jump into the deep end of the proverbial pool makes them better for it even if they flail for a bit. Give them boundaries but give them the space to figure it out and make mistakes. I know it drove my parents crazy that me and my siblings took everything apart and argued systematically but it paid off in our adulthood. reply idontpost 9 hours agorootparentprev> Seems like society should try to figure this out.. if attitude is so important, why can’t we cultivate it systematically? We do. It's called culture. But culture doesn't optimize for something so narrow since it exists in a much wider context than just \"what's good for knowledge workers in a capitalist system\". reply saddd 9 hours agoparentprevIt's become the software industry's equivalent of standardized testing. reply DelightOne 10 hours agoparentprevThe problem starts when its a new problem. reply mrg3_2013 10 hours agorootparentNot sure what you meant. Leetcode type things solves a problem of past. Future will be copilots doing lot of grunt coding work and human value add would be instructing copilots the right way, bringing broader contextual information. Solving binary search tree problems during interviews will eventually go away (once old guards go away) reply DelightOne 10 hours agorootparentWe agree, solving binary search tree problems do not need solving. That's an old problem. Problem is if you need people to choose and adapt algorithms for a specific new problem, copilot will have a hard time and so you will have a hard time if you don't know how the algorithms work and never adapted one. Because copilots are not good at choosing the correct algorithm and adapting it for new problems, as far as my experience with the competitive programming course I took goes anyway. If you have an old problem, copilot can solve it, true. reply downWidOutaFite 5 hours agoparentprevIf candidates know going in that it's a leetcode interview it filters for \"do you want to work here so much that you'll spend a month or two drilling on these exercises\". Which can be a useful filter if there are thousands of qualified applicants. reply cubemaster 14 hours agoprevHey guys! I'm Shreya. I recently noticed that many people that solve leetcode style problems to learn programming patterns tend to save links to problems they'd like to try again later on google docs/notes. However, this doesn't help them remember to try these problems again or differentiate between problems that are more urgent to revise. Built this tool to help solve this inefficiency! reply xandrius 12 hours agoparentOr just use this public Anki deck as a base and add your own questions to it: https://ankiweb.net/shared/info/1223170466 reply rahimnathwani 7 hours agorootparentThe GitHub repo linked there says it will automatically create cards for your solved (AC) leetcode problems. reply cubemaster 11 hours agorootparentprevdo you think it adds friction to go between the anki screen and the leetcode screen? reply davepeck 9 hours agoprevRelated: executeprogram.com is an excellent learning platform that combines spaced repetition with interactive lessons targeted at professional developers. reply lycopodiopsida 3 hours agoprevYou've made a SR tool to solve LeetCode problems, not coding problems. reply AlexErrant 9 hours agoprevFYI I made Anki's spaced repetition algorithm, FSRS, run in the browser. Might be useful to you https://github.com/open-spaced-repetition/fsrs-browser Edit: I just remembered it takes like 500(?) data points to train the algo... so maybe not. reply jarrett-ye 6 hours agoparentIn the latest version, it only needs 64 review logs to full-optimize the model reply AlexErrant 6 hours agorootparentNice! (Jarrett's the author of fsrs, fyi.) reply tibbar 10 hours agoprevJust wanted to say that this is a really beautiful landing page - way nicer than I expected for a product focused on Leetcode ;) reply aster0id 11 hours agoprevJust fyi for the dev - on mobile, the slide in menu that opens on clicking the burger icon on the top right is empty. Cool idea btw. What do you think about incorporating the \"patterns\" associated with problems into the repetition algorithm? For example, if I find 2 pointer problems hard, then the next 2 pointer problem will be recommended sooner. reply cubemaster 11 hours agoparentyes, was thinking of adding this in! will start adding possible features here: https://alpine-aspen-2ab.notion.site/Lanki-da1481129bfa4d3ca... reply aster0id 11 hours agorootparentI can totally see what you're solving for - the friction of switching between an anki deck and leetcode to achieve the same thing with existing tools. If you want one data point about how I prepare for coding interviews - I just solve Neetcode's list and the blind 75 list (minus the overlap between the two). For reference I got 5 offers in quick succession after being laid off in early 2023. reply xzel 11 hours agoprevI'm about to start the leet code grind for interviews coming up and I'll give this a shot. I found your feedback form on your website but would be nice to post it in your post here as well. Cheers. reply cubemaster 11 hours agoparentposted! thanks for the advice :) reply reddit_clone 11 hours agoparentprevI keep hearing this. What exactly does 'leetcode grind' mean? I take it you solve puzzles by writing code. How does it help with job interviews really? reply linguae 11 hours agorootparent“Leetcode grind” means solving Leetcode problems. Interviewing for many software engineering positions in Silicon Valley these days require doing well on Leetcode-style coding exercises where one has a limited amount of time to solve a problem. What makes software engineering interviews a grind these days is that many employers don’t care about your thinking process; they want an optimal solution to the problem within the time limit without errors, since chances are high that an applicant will come up with the optimal solution. In addition, the high compensation for many Silicon Valley employers has made these positions very desirable and thus ultra-competitive. The same type of applicants who grinded for high SAT and AP scores in high school and who grinded in college for 3.7+ GPAs don’t feel discouraged grinding some more for a six-figure job with life-changing amounts of RSUs once vested and other perks. You can grind hard, but chances are high that somebody else spent even more time studying than you. These types of interviews are similar in spirit to the employment exams that some companies have in Japan during job-hunting season for upper-level college students. So, aiming for a FAANG position, as well as software engineering positions at many other companies in Silicon Valley, essentially require studying not unlike preparing for a GRE subject test for graduate school admissions. reply jacobsimon 10 hours agorootparentHey I just want to add, in addition to your advice here, there are other important aspects to interviewing beyond performance on Leetcode questions. In other words, it's sometimes necessary, but not sufficient. Most senior jobs require system design interviews, conversations about leadership, and more. I think \"grinding\" on Leetcode also entails learning and practicing a lot of irrelevant topics for many jobs, but there are newer resources out there that help you focus on what you need to know for specific roles. I don't like to plug my own company, but Exponent[1] is designed to help with this more 'well-rounded' interview prep for different types of engineering paths, and we also have a mock interview feature that helps you iron out your communication skills with other candidates. 1. https://www.tryexponent.com reply greymalik 9 hours agorootparentI would like to try exponent, but the only free \"lessons\" I see are the intros to each section. I want to see a real, representative sample of the content I'd get before I'm willing to open my wallet. reply jacobsimon 9 hours agorootparentFeedback received! We do have free lessons deeper in the course and lots on our YouTube channel, but will pass this on to the team. reply sovnwnt 10 hours agorootparentprev> for many software engineering positions in Silicon Valley these days Do US most companies outside of Silicon Valley not use leetcode style interviews? Every single software interview I've done in Canada had at least one round of leetcode programming exams. reply madamelic 9 hours agorootparentI've hardly dipped my toe in SV and have almost entirely worked in the Eastern half of the United States. Leetcode questions do happen but typically they are guidelines for a further discussion rather than the entire signal. By that I mean, they are used to investigate problem solving, communication, and personality. It tends to be the bigger places have more rote tests. One company posed the \"If you were re-making Instagram, design me the ability to handle millions of page views over the span of an hour\" or something like that. Apparently CDNs, caching, and load balancers wasn't the answer... Never bothered to look it up because I spent what felt like an hour trying to figure out why this guy thought a CDN wasn't good enough and whether he was _trying_ to get a rise out of me. What only made it funnier is this place had nothing to do with images. reply daemonologist 9 hours agorootparentprevI think outside of silicon valley (and companies emulating that culture), US companies still give coding problems but they're usually not leetcode style, more of just an implementation problem, if that makes sense. Instead of checking that you have some obscure algorithm memorized they simply want to see you code a bit. reply mardef 11 hours agorootparentprevMany engineering interviews include literal \"solve this problem on leetcode\" sections. reply nateburke 9 hours agoprevLove it! Technique is memory, those who espouse abstraction have forgotten their own humble beginnings. reply cubemaster 11 hours agoprevhere's the feedback form from the landing page: https://docs.google.com/forms/d/1Nqq9xj13ZZHlih16xsTp2Nsl4Zc... reply orsenthil 12 hours agoprevWill it stop recommending if you have mastered the problem? Or will it give a new problem to solve ? reply cubemaster 11 hours agoparentfor now, it will always 5 recommend problems you have attempted before, prioritizing the ones you solved both (1) longest time ago and the ones that were (2) hardest for you. so it will not recommend anything you haven't tried. would you want it to stop recommending a problem once you've mastered it? maybe an indication on the tool so you can mark it as mastered? reply FailMore 3 hours agoprevSaving for later reply nico 12 hours agoprevOff topic: is spaced repetition ever used for training LLMs? Does that even make sense? Thank you reply mpmisko 12 hours agoparentTraining for multiple epochs is a bit like that :) reply SpaceManNabs 12 hours agoprevhow is this different from sharing an anki deck? reply xandrius 12 hours agoparentSo many people coming up with different Anki's. For me Anki is so flexible that I don't think anything will ever easily beat it. reply cubemaster 11 hours agorootparenthow do you use anki for code/technical skills? reply xandrius 37 minutes agorootparentThe anki decks/cards are fully customisable to add any fancy extra feature. Otherwise one can make their own cards (I often use LLMs to convert knowledge into cards and then import them into my account) for just about anything. reply cubemaster 11 hours agoparentprevsuper similar - was inspired by anki! this extension shows up directly on the leetcode website as a starting point, but looking to expand to all of ankis-type features soon reply sirobg 12 hours agoprevSeems nice, congrats! How does the recommendation algorithm work? reply cubemaster 11 hours agoparentcurrently recommends 5 problems using a ranking algorithm that calculates a score similar to this: score = ( timeSinceLastAttempt ^ 2 ) * difficulty; Anything else you'd want factored in? reply thorncorona 11 hours agoprev [–] Who memorizes leetcode problems.. this seems like an extension looking for a problem. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The LeetCode question-solving tool enables users to categorize questions as hard, medium, or easy and suggests review based on difficulty and time since last review.",
      "To benefit from personalized recommendations, users should attempt and categorize problems initially to enhance the tool's effectiveness.",
      "An explanation of the theory supporting spaced repetition and learning is available through a provided link."
    ],
    "commentSummary": [
      "The discussion delves into the effectiveness of spaced repetition, memorization, and problem-solving methods in mastering coding concepts for job interviews, notably utilizing platforms like LeetCode.",
      "Various perspectives are shared on the role of memorization in education and personal growth, with an emphasis on tailoring traditional memorization approaches to individual learning styles.",
      "The importance of combining understanding with memorization and the influence of standardized LeetCode-style questions in technical interviews are highlighted in enhancing coding skills and performance during job interviews."
    ],
    "points": 158,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1714160325
  },
  {
    "id": 40169578,
    "title": "Jon Pretty vindicated in Scala community court battle",
    "originLink": "https://pretty.direct/statement.html",
    "originBody": "A Statement Jon Pretty, 26 April 2024 I am a Scala developer and speaker who was cancelled three years ago. Yesterday I attended the High Court in London to hear an apology from several prominent members of the Scala community for making untrue claims about me on 27 April 2021. I sued them for libel, and they admitted fault and settled, paying me costs and damages. Their allegations were sensational and squalid, but unfounded. Their source was the resentment of one woman following a relationship in 2018, which I ended against her wishes. She fabricated or was offered an alternative narrative, which developed into claims of a pattern of behaviour, and culminated in the defendants’ publication of an open letter, which they now agree is defamatory. In two years of legal action, the defendants never presented any evidence to support their allegations, and admitted in court that they had no proper reason to make them. They have given undertakings to the court not to publish further or similar defamatory statements, or have anyone else do so on their behalf. No signatory contacted me about the allegations before publication. I received no warning, and had no knowledge of the claims’ substance. I only discovered what I was accused of at the same time as I learned of my indefinite exclusion from the community; at the same time everyone else found out. I had no opportunity to defend myself. It is no coincidence that the absence of due process led to an abject injustice. The experience of cancellation and enduring the online hysteria was traumatic. I responded by withdrawing from the life I knew. Its consequences hurt me and people close to me, and have been immiserating. My employment opportunities were obliterated. My charitable and educational projects, and my small business, could not continue. Despite my transferable skills, the allegations were a transferrable red flag recognised across programming communities and industries, and I have barely earned a living since. It has taken two years of legal action to receive fair scrutiny in a forum reliant on facts. This outcome finally vindicates me. Many familiar with the open letter will be surprised by this outcome, given the certainty expressed by some of the letter’s supporters. But they should question why no defence of my libel claim was ever offered. They should seek answers for how so many people became so convinced without evidence. And the defendants’ co-signatories should reassess the credibility of everything that compelled them to support the open letter. Nonetheless, I believe the open letter’s authors were convinced it was the right thing to do. I believe that they wished to show their kindness in response to a story which shocked them. Many of them I have known for years, and know to be good, moral people who were mistaken, and would not wish to be mistaken. Good people can make mistakes, without becoming bad people. So, I offer them the respect and compassion I believe they deserve. I recognise their good intent, and I interpret its consequences charitably. We owe each other civility in ceding the time and space necessary to reflect upon this in open enquiry, without coercion or urgency, so everyone can reach a better understanding of this outcome, and of one another. I have avoided naming any third parties here, and I urge others to do the same. I know too well how it feels to be the subject of online harassment, and I do not wish it upon anyone. I reject the culture of blame and escalation. I have not been idle since 2021. I’ve spent much of my time writing Scala 3, which I believe is an exceptional platform for software development, and deserves far greater success. I am motivated to be part of that success, and I will start sharing my work again soon. █ Consent Order and Statement in Open Court (The Honourable Mrs Justice Steyn DBE)",
    "commentLink": "https://news.ycombinator.com/item?id=40169578",
    "commentBody": "Jon Pretty wins in court against sexual harassment claims by Scala community (pretty.direct)130 points by skilled 19 hours agohidepastfavorite90 comments braza 8 hours agoSeeing from a distant place and not having a horse in this race, I see several lessons to unpack: 1. The borderless and ambiguous relationships in technical communities is officially over, and everyone trying to do the opposite is just risking him/herself in terms of reputation. 2. Do the “I condemn X, let put my name in a public letter” nowadays in that kind of dispute without have access to all information it’s just a dangerous way to gain clout. You do not condemn anything, your 7K followers in twitter and 250 followers does not transform you in a public celebrity that put you in a position to condemn someone. 3. The side that settled with the person that committed the allegedly behaviour should celebrate the outcome because it was way light in comparison with other jurisdictions where it could converge to criminal charges. 4. More or less like aviation, you will not train to fly in the clouds in visual flight, but you will train to not enter in that situation in the first place. What I mean by that is everyone needs to operate defensively and cautiously when the social borders are not totally explicitly displayed. Questions like “Should I engage in that behaviour that can be misinterpreted?”, “is something goes south can I suffer reputational damage?”, “do I have the disposition and resources to defend myself and my position in court?” needs to be answered before engage in open letters and in interpersonal relationships in communities. reply dmw_ng 13 hours agoprevHe did not \"win in court\", a consent order means both parties agreed to a mutual resolution prior to a judgement occurring. If I had to guess it was settled this way because of the time, misery and tremendous cost that would otherwise be involved for all parties. \"Wins in court\" suggests some thorough process leading up to a final evidential determination made by a judge. That is not what happened here, it'd probably be more accurate to say the parties were motivated to cooperate via their solicitors under looming threat of enduring that process. No skin in this game, but looking at it from the respondents' perspective, 20k split between 4 is incredibly attractive: 5k and an apology to put an end to any threat relating to the issue, given the alternative of potentially unbounded costs to demonstrate innocence with a tortuous and uncertain outcome. From the claimant's perspective it is more confusing, triggering such a heavyweight process then settling for so little given the claimed harms suggests all kinds of things, not least including the kind of advice he may have received given the strength of the case as his solicitors understood it. reply skilled 13 hours agoparentI am not against having the title changed if mods want to do it, and if there are enough people who think that it should be changed. In my opinion, he did win. The document clearly favours Jon as the other side admitted they have no evidence and made baseless claims - stuck their nose where they shouldn’t have. Make no mistake that this was a serious character assassination based on zero proof or involvement from authorities. This was not a bunch of IRC friends doing a prank for the lolz but an open letter with signatures from prominent community members. I can understand why someone would feel inclined to do something like that, but you ought to consider the repercussions of your actions when it comes to attacking a person based on “word of mouth” evidence. Everyone gets hurt in the process. reply singleshot_ 12 hours agorootparentHe may have won, depending on what he was seeking through the action and what he received in the consent order. But he did not win in court. He won out of court, and used the court as a mechanism to record and eventually enforce what he won. reply latchkey 18 hours agoprevPreviously: https://medium.com/@yifanxing/my-experience-with-sexual-hara... https://scala-open-letter.github.io/ reply andrelaszlo 17 hours agoparentSo if I got this right it's the open letter (second link) that was deemed defamatory, and not the story from the first link? > The Defendants accept that they have never had any evidence to support the allegations apart from the two unverified claims published in coordination with the Open Letter. They were never in a position to make any informed judgement on the truth of the allegations, and did not seek clarification on any of the allegations from the Claimant. How does this vindicate him? reply ZeroGravitas 16 hours agorootparentThe first link also links to a second person's first hand experience (both are noted at the bottom of the signed letter): https://killnicole.github.io/statement/ reply gedy 17 hours agorootparentprevInnocent until proven guilty? reply ceejayoz 17 hours agorootparentUnder English defamation law, libel defendants are actually functionally guilty until proven innocent. reply exe34 17 hours agorootparentBecause the libel defendants are the ones who have pronounced somebody guilty until..... well there was no due process. reply ceejayoz 17 hours agorootparentYou think there should be due process required to speak? reply exe34 17 hours agorootparentDid they merely speak, or did they ban him from a community and made him unemployable? reply ceejayoz 16 hours agorootparentFundamentally, here's the difference. In the US, getting someone convicted of libel or settling for it is pretty solid proof. It means the person being libeled likely could prove that the person making the claims knew they were false and maliciously spread them anyways. In the UK, it's more akin to an allegation. It provides us substantially less information than a similar proceeding in the US would. reply baryphonic 15 hours agorootparent> In the US, getting someone convicted of libel or settling for it is pretty solid proof. It means the person being libeled likely could prove that the person making the claims knew they were false and maliciously spread them anyways. Proving actual malice is only a requirement if the plaintiff is a public figure per New York Times v Sullivan. People who are not public figures have a lower burden of proof, though it is still substantially more stringent than in the UK. reply dragonwriter 13 hours agorootparent> People who are not public figures have a lower burden of proof, though it is still substantially more stringent than in the UK. While state rules for private figures differ, the Constitutional limit (as articulated in Gertz v. Welch) prohibits liability without fault, that is, there must be at least negligence even if the statement is factually false, and also prohibits punitive damages without actual malice. In the UK, defamation is strict liability (there is no fault requirement) in addition to truth being an affirmative defense rather than falsity being an element of the tort. reply singleshot_ 12 hours agorootparentprevThey have a lower burden of proof? Or they have to prove fewer elements? reply dragonwriter 9 hours agorootparentThey (can) have different, less stringent elements, not fewer (state rules vary, but there can be a less stringent fault element than actual malice, though there must still be fault — defamation cannot be strict liability in the US.) reply exe34 15 hours agorootparentprevSo in the US it's easier to make unfounded allegations and ruin somebody's life, and it's on them to prove you knew you were lying? reply ceejayoz 15 hours agorootparentBoth Weinstein and Cosby provide good examples of where hard evidence was extraordinarily difficult to come by. Much of the evidentiary value comes from the large volume of credible (time, place, access, similar stories, etc.) accusations; those accusations largely required an initial few to be brave enough to come out with their stories for the rest to come forward. The UK's setup permits such abusers to prevent press coverage of these initial victims, and to significantly chill the climate for subsequent accusers. Both setups have their upsides and downsides, but I'm less a fan of the one that lets serial predators evolve their strategies to be more and more immune from consequence. reply dragonwriter 9 hours agorootparentprevIn the UK, you can be held liable for stating a reasonable conclusion based on the facts available to you after reasonable investigation that turns out to be (viewed by a court as) false, while in the US there must in any defamation case be some degree of fault as well as falsity. reply nailer 15 hours agorootparentprev> In the UK, it's more akin to an allegation. No. It means the libeller couldn't prove what they were saying. reply ceejayoz 15 hours agorootparentThe alleged libeler. Which can mean, sometimes, that the perpetrator is good at covering their tracks and selects victims who can't sustain a prolonged legal and public opinion battle trying to prove their case. They need only allege that someone libeled them to impose significant legal costs on that person, costs that for many are not feasible. You don't get a public defender. reply exe34 14 hours agorootparentThe \"alleged\" libellers lost the case though. They apologised and paid costs and damages. I don't think \"alleged\" applies after you lost the case. reply ceejayoz 13 hours agorootparentIf you read the document, the allegation that they made false statements remains unproven. They admit only to not being able to support the claims with evidence other than the original allegations. The original source of the accusations that they were supporting in the open letter is also notably not named in the suit. In most court proceedings, it must be positively proven (\"beyond a reasonable doubt\" in some cases, \"by the preponderance of the evidence\" in others) that you committed the crime or tort in question. That's a good requirement, and English defamation law is a glaring exception. reply exe34 13 hours agorootparentSo you believe it's possible that at the time they had evidence to prove the allegations, and thus acted in good faith, and now suddenly they don't have any memory of said evidence? That's an interesting case that I expect we'll be seeing medical papers written about soon! reply dragonwriter 13 hours agorootparentGood faith (that is, having evidence from which one would reasonably believe the truth of the statement at the time it was made) is not a defense to libel in British law. Libel in the UK is a strict liability tort, the statement can be 100% justified based on the information in hand when it is made, and even if the court would agree with that were it permitted to consider it, if the court finds it is false based on information that the defendant did not have (or even could not possibly have had, no matter how hard they sought it) at the time of the statement, the defendant will still be liable. EDIT: while the NY Times v. Sullivan rules for public officials (extended by other cases to public figures) get more attention, the Gertz v. Welch rule that, as a matter of Constitutional law, requires fault for defamation liability for private figures is arguably more fundamental difference between the US and UK on defamation law. reply ceejayoz 13 hours agorootparentprevThe claims in https://scala-open-letter.github.io/ are that the signers \"have become aware\" \"based on multiple, independent, well-substantiated reports showing a systematic pattern of behavior over an extended period\" of harrassment, and links those very multiple, independent reports (of which the second is indeed a substantiation of the first). It's hard to see how any of this is false under the US standards. In the UK, though... https://en.wikipedia.org/wiki/English_defamation_law > everyone involved in the dissemination of the defamation is liable as having published it (That wouldn't fly here; retweeting a false claim, barring unusual circumstances, tends not to be actionable.) The actual allegations weren't litigated in this case, it's a settlement, and I don't doubt it made more sense to say \"sorry\" and pay £5k/person than to spend years in court litigating a crime that is typically done in a way that leaves little physical evidence that someone else asserted happened. Even when you win, you lose! https://en.wikipedia.org/wiki/Irving_v_Penguin_Books_Ltd Millions in legal fees, two years spent proving the case, and he declares bankruptcy when fees are awarded. reply tristor 10 hours agorootparent> It's hard to see how any of this is false under the US standards. You are taking their claims at face value. It doesn't matter what standards you apply other than truthfulness, and these claims could still be false. There's no substantiation provided, merely the claim that such exists. A court case in the US would still require them to provide that substantiation, otherwise their claims could be considered libel. While the libel laws in the US and UK differ, it is not legal either in the US to defame someone either knowingly or negligently. reply ceejayoz 8 hours agorootparent> You are taking their claims at face value. They are entitled to that presumption until evidence emerges otherwise. (This is not the case in the UK!) > A court case in the US would still require them to provide that substantiation, otherwise their claims could be considered libel. No. A court case in the US would require Pretty to show evidence they had reason to believe the information was false for them to be liable for defamation. reply nailer 7 hours agorootparentWhy are people entitled to say bad things about others they cannot prove are true? reply stewoconnor 11 hours agorootparentprevIt's possible that we no longer remember ALL of the evidence we had when we drafted this letter, but the letter links to two first hand accounts of wrongdoing. I also have also talked to multiple other women in the community that say that Jon was someone that women generally knew to warn each other about since there were enough believable accounts of his wrongdoing. reply tristor 17 hours agorootparentprevTo be exactly correct, he sued for defamation four (4) of the signatories of the open letter, because the open letter was the explicit call to action to cancel him. He can only file suit against persons within the jurisdiction of his home court in the UK, the majority of the signatories, as well as the two accusers are not UK persons and cannot be sued in that context. Whether or not that vindicates him is an exercise to the reader. reply aeurielesn 16 hours agorootparentIf four (4) of the signatories can't prove the allegations, how will five (4+1) be able to do so? reply tristor 14 hours agorootparentI make no claims or judgement either way. I just acknowledge that information asymmetry exists as do jurisdictional limits. It's not possible in this case for him to ever be fully vindicated due to the limits of the legal system, but whether it's enough to vindicate him for you is up to you. reply rafram 17 hours agoprevTwo curiosities to note: - He did not sue his accuser. He sued people who trusted her word and amplified her allegations. England's (draconian) libel laws would require them to prove that the allegations were true. That's obviously very difficult. - He lives in Germany, but he chose to sue in England. Germany has more typical libel laws that would require him to prove that he was defamed. reply throwaway11460 16 hours agoparentMaybe he chose England so he could understand the language and system of justice. reply junto 13 hours agoprevLooking at the history here on HN, it seems he’s been accused by multiple women. Regardless of him winning this case, as a woman I’d avoid being alone with this guy at all costs and warn other women to do the same. reply borski 3 hours agoparentSo, once accused, one can never be exonerated, even if innocent? Because that sure sounds unfair and undefendable. I wouldn't want to be alone with you either, in that case, and I'd warn other men to do the same, given that they may evidently be accused for... having previously been accused? I'm the last person to defend anyone who has laid a finger on or harassed anyone without their consent. But I also believe that anyone can accuse anyone of anything, since that's quite literally what rumors are. reply why_only_15 27 minutes agorootparentThe law has strict standards for fairness, for good reason. It excludes things like rumor, for good reason. But when you as a private person are deciding how to act, you make the best decision available to you with the information you have. The information we have is fairly compelling: multiple women have deeply regretted their interactions with him, enough to risk lawsuits to publish their regret publicly. Why interact with him unless absolutely necessary? reply spullara 18 hours agoprevTurns out due process was invented for a reason. reply ceejayoz 18 hours agoparentBritish defamation law is notoriously friendly to plaintiffs. https://en.wikipedia.org/wiki/English_defamation_law > English defamation law puts the burden of proof on the defendant, and does not require the plaintiff to prove falsehood. For that reason, it has been considered an impediment to free speech in much of the developed world. In many cases of libel tourism, plaintiffs sued in England to censor critical works when their home countries would reject the case outright. reply kayodelycaon 17 hours agorootparentI think they have sufficient evidence to win the suit in the US. From the court: The Defendants accept that they have never had any evidence to support the allegations apart from the two unverified claims published in coordination with the Open Letter. They were never in a position to make any informed judgement on the truth of the allegations, and did not seek clarification on any of the allegations from the Claimant. reply rafram 17 hours agorootparentMost likely not. There are two parts of US case law that would be at issue here, but the key is actual malice [1]. The defendants here believed the claims they were repeating, even if they themselves hadn't seen the evidence. That isn't defamation. [1] https://reason.com/volokh/2018/11/07/is-accurately-repeating... reply f33d5173 17 hours agorootparent\"actual malice\" means either they knew it was untrue or they published it with reckless disregard for the truth. That would be the test he would attempt to prove they failed. reply mminer237 15 hours agorootparentprevActual malice (which, reckless disregard isn't all that hard to get) it's only required if a person is a public figure. For most people, you only have to show negligence. reply rm_-rf_slash 17 hours agorootparentprevCounterpoint: when the burden of proof is on the plaintiff, people can lie about terrible things with little to no consequence. I personally have been at the receiving end of some particularly heinous lies by an individual who is unfortunately not getting the mental assistance they need. It was hard enough to secure a restraining order but my lawyer said there was pretty much nothing I could do about the whoppers being blasted over social media. Fortunately nobody believed the crazy person’s crazy lies but if they had I could have been put in a really bad situation. reply jsheard 18 hours agorootparentprevIt's a big part of why Jimmy Savile got away with it despite everyone apparently knowing what he was up to, nobody wanted to run the story because they anticipated it would get shot down as libel. The BBC greenlit an investigation into the allegations immediately after he died, they knew full well there was dirt there but they wouldn't touch it until he was in the ground. reply afavour 18 hours agorootparentprevAgreed. I know absolutely nothing about this particular case so I'm not passing judgement on it specifically. But as a British person I've learned over the years (and the endless court cases by football players etc) to pretty much totally disregard the outcome of these kind of cases. reply nailer 16 hours agorootparentprev> > British defamation law is notoriously friendly to plaintiffs. > English defamation law... does not require the plaintiff to prove falsehood. That's not 'notoriously friendly to plaintiffs'. It's simply reasonable behaviour: if someone says something horrible about someone else, the person saying it must be able to prove it was true. reply TheCoelacanth 13 hours agorootparentIt is completely unreasonable that you are not allowed to speak factually about events that you personally experienced simply because you are not able to conclusively prove that they happened. reply ceejayoz 16 hours agorootparentprev> It's simply reasonable behaviour: if someone says something horrible about someone else, the person saying it must be able to prove it was true. https://en.wikipedia.org/wiki/Libel_tourism The UK's system required a historian to spend two years and millions defending against a libel claim from calling someone a prominent Holocaust denier a Holocaust denier. https://en.wikipedia.org/wiki/Irving_v_Penguin_Books_Ltd That's to argue one of the most clearly documented historical facts we have. Allegations - testimony - are evidence. Multiple people alledging something is evidence. Even under US libel law, accusing someone like Cosby or Weinstein of misconduct is risky enough; in the UK, it led to things like https://en.wikipedia.org/wiki/Jimmy_Savile being untouchable. reply nailer 14 hours agorootparentNothing you have written refutes the item you have quoted. Someone was able to prove that David Irving said the holocaust did not happen, but it did happen, and David Irving lost. Good. The court being slow and expensive is orthogonal and applies across all of law. reply ceejayoz 14 hours agorootparentThis is the absolute best case scenario - the easiest of wins, on a factual level - and it was still hugely expensive in time and money. (He went bankrupt, too, so they didn't even get to recoup those fees.) If you're one of the kids Jimmy Savile raped, do you think they've got those resources to fight a multi-year court battle with someone who's been covering their tracks successfully for over sixty years? The BBC won't risk talking about it, what hope do you have? Note that once he died and libel suits couldn't be raised any longer, hundreds of victims came out within the year. reply mjburgess 18 hours agorootparentprevWell in the case of defamation the damage has already been done, necessarily. So the defendant is really a prosector, and they are asked for their evidence. See, eg., this case. Also,... > The court ruled that Irving's claim of libel relating to Holocaust denial was not valid under English defamation law reply ceejayoz 17 hours agorootparent> Also,... > The court ruled that Irving's claim of libel relating to Holocaust denial was not valid under English defamation law Getting this ruling on one of the most heavily documented historical facts in existence cost millions and two years of litigation. That has an obvious chilling effects to alleging misconduct that isn't documented in a few thousand scholarly books. reply hardlianotion 11 hours agoprevThat is a very generous letter by Jon Pretty, considering the rush to judgement made by people who really should know better. reply cbeach 11 hours agoprevIn terms of ecosystem, it reflects well on the leadership of Zio (John De Goes) that he refused to join the cancellation mob on principle: https://twitter.com/jdegoes/status/1783878876726219016 Whereas Typelevel, on the other hand, were only too happy to join the mob, and then to stick the knife in with their own lynching-by-blog post: https://typelevel.org/blog/2021/04/27/community-safety.html Which they still haven't retracted. The legacy of Travis Brown lives on at Typelevel. They're a political organisation masquerading as a technical organisation. reply coolThingsFirst 18 hours agoprevWhen I read the initial post I chalked it up to there's always another side of the story. The problem is while we want to really punish rapists we can't just do witch hunts based on hearsay and throw innocent people under the bus. Also the importance given to 21 and 35 seems indirectly to imply something shady which is not the case, it can be a relationship of 2 consenting adults. I wish all the best and may you even further clear your name. You went through a horrible ordeal. reply AzzieElbab 17 hours agoparentSame. This came amid a civil war in the Scala community, too. reply stewoconnor 11 hours agoparentprevThis was absolutely not based on hearsay. We made our decision to sign the letter based on direct evidence, not hearsay. The open letter links directly to evidence which is not hearsay. reply tristor 10 hours agorootparent> The open letter links directly to evidence which is not hearsay. If you're referring to the two blog posts, that is testimonial evidence only. While it /is/ possible to convict on the basis of only testimony, it's very rare, as testimony usually needs to corroborated by documentary or physical evidence, or at minimum be supported by a preponderance of other types of evidence which makes the claims against a defendant more likely to be true than that they are false. I am not you, I don't know you, and I don't have any particularly strong opinion about this situation. That said, you're all over the comments here, trying to defend the contents of the open letter. If you believe in this so strongly, I'd advise that you provide stronger evidence to support your position. Simply believing someone else who told you something is not substantial or corroborative. reply VirusNewbie 17 hours agoprevI’m glad something like this happened, it felt very orchestrated. If you look at the accused, he also owned a UK Scala consultancy that directly competed with Miles Sabin. His MO seemed to be finding people who would support his public accusations to ruin reputations. He tried to get me to do it as well. It was weird. reply rafram 16 hours agoparent> Miles Sabin was always whispering in people’s ears to get them to publicly denounce his consulting rivals. > He tried to get me to do it as well. It was weird. Responding to a post about a libel case based on he-said-she-said allegations… by making your own potentially libelous allegations? Careful! reply VirusNewbie 16 hours agorootparentI have chat logs. reply ceejayoz 16 hours agorootparentProving the Holocaust happened in a UK libel proceeding required two years and millions of dollars of litigation. https://en.wikipedia.org/wiki/Irving_v_Penguin_Books_Ltd ; a US court would've likely summarily dismissed it on day one. You'd likely need a lot more than raw text logs in defense. reply vezycash 17 hours agoprevWouldn't suing quicker have saved him a lot of grief? reply throwaway042624 17 hours agoprevHmm. Google seems to remember several threads on this debacle. HN seems to have memory holed them. Complete empty pages. https://news.ycombinator.com/item?id=26961595 https://news.ycombinator.com/item?id=26967283 What's going on, dang? reply ceejayoz 17 hours agoparentHN has been going up and down all day (I've been getting partial page loads on my profile). No conspiracy needed; there's just something busted. reply function_seven 17 hours agoparentprevAll stories on the site this morning were lacking any comments. Not specific to this story. reply gedy 17 hours agoparentprevI do like this exchange from the first: calylex on April 27, 2021unvoteYou have no evidence to be smearing someone's name and defaming them on HN in this way. Stop this behavior until there is legal grounds for doing so. rodgerd on April 28, 2021parentIt's not defamation if it hasn't been judged so in a court of law. I guess it is now, but 3 years later seems damage has been done. reply underseacables 16 hours agoprevJustice at last. reply VirusNewbie 17 hours agoprev@dang can you unflag this? There’s more to the story and a think it’s suspicious its getting flagged. reply lolinder 16 hours agoparent@dang is a no-op, click the contact link in the footer. reply nailer 16 hours agoparentprev+1. It seems important to allow this person to at least partially address the damage to their reputation. reply nailer 16 hours agoprevDirect link to the consent order: https://pretty.direct/consentorder.pdf > My [Lord/Lady], on behalf of the Defendants Miles Sabin, Zainab Ali, Noel Welsh and Bodil Stokke, I wish to associate myself with everything that has been said by Counsel for the Claimant. They wish to apologise unreservedly for the damage and distress caused to the Claimant and for any damage to his reputation by their publications and express their profound and unreserved regret for all of the harm for which they are responsible. reply cbeach 12 hours agoprevI honestly don't know how Jon survived this saga without killing himself. The kangaroo court, featuring many prominent members of the \"community\" was brutal in its rush to judgement. It delivered summary justice to a member of the community and ruined his life in a matter of hours. I was an observer at the time and whilst I didn't know Jon, I was aware of his contributions to the Scala ecosystem. I arranged a video call with him to see how he was holding up, and to try to understand things from his perspective. The man I spoke to was visibly shellshocked and blindsided by the whole experience. He had a brief, consensual relationship with Yifan, and vehemently denies the wrongdoing that she insinuated in her blogpost. But the community apparently knew better. I was already aware of dark political maneuvours in the Scala community thanks to the well published exploits of Travis Brown (who was himself cancelled when the nature of his actions became apparent). Brown didn't just habitually engage in cancel culture. He actually SYSTEMISED it. He wrote tools, published on his github profile to maintain lists of people within the Scala community whose politics he disagreed with, and then he used those lists to attempt to smear those people by association and damage their careers. One of the two accusers of Jon Pretty happened to be Victoria Leontieva (AKA killnicole), Travis's girlfriend at the time. And the other accuser, Yifan Xing, is Travis's new girlfriend now. I'm sure it's all just a coincidence.... While Travis was eventually himself cancelled from the Scala community, it seems his legacy lives on. The whole witchhunt brought deep shame on the community. I think even the founder of the language, Martin Odersky, tacitly endorsed the open letter. That's a terrible way to lead. An ethical leader would have called for an end to the emotionally-charged witch-hunt, and would have called for due process instead. Jon - I hope this vindication gives you the personal closure that you need on the matter. Obviously, I can't imagine you wanting to return to the Scala \"community,\" but please know that the community wasn't united against you at the time. There were many people like me who could see what was happening, and who knew you deserved better than the treatment you received. reply hobotime 17 hours agoprevnext [2 more] [flagged] stewoconnor 11 hours agoparentI signed the letter and I fully stand by everything it says. I have talked to Yifan and I feel sorry for her and believe her. It was well known, and well discussed that women in the scala community knew to warn other women about Jon Pretty. I have heard this both directly and indirectly from multiple prominent women in the Scala community. reply rscho 18 hours agoprev [–] So they got scared and jumped the gun in an attempt to protect the Scala community? reply C0mm0nS3ns3 17 hours agoparentProbably a more accurate statement would be: \"So they got scared and jumped the gun and in so doing risked ruining someones life in an attempt to protect the Scala community.\" reply coolThingsFirst 17 hours agorootparentTechnically speaking is he recovering fully from this? reply tristor 17 hours agorootparentDoes anyone ever recover fully from years of income loss? \"Time in the market beats timing the market\" and time is our most precious resource. It doesn't sound like this settlement (based on the attached court order) makes him whole financially in any way from the impact. reply coolThingsFirst 17 hours agorootparentYeah, good years and bad years kind of a thing. People recover from bankruptcies. I meant in the way of recovering his social image because this is brutal. reply stewoconnor 11 hours agoparentprevwe absolutely did this to protect the scala community and we weren't scared and didn't jump the gun. People had been talking about him as being problematic for years before the open letter reply andras_gerlits 4 hours agorootparent\"I stand by spreading hearsay, even if a court happens to disagree\" reply VirusNewbie 17 hours agoparentprev [–] Not to protect the scala community. To tank a rival. Miles Sabin (the accuser and organizer or the letter) runs a competing consulting service that directly competes (competed?) with Jon Petty, who ran another consulting service also in the UK. reply tsss 15 hours agorootparent [–] I don't think it was out of competition. To borrow some language from the aforementioned open letter: This is very much a \"pattern of behaviour\" with these people, directed against anyone and everyone that makes a compelling political target. Travis Brown must be the worst of them, who isn't even really a part of the \"Scala community\" anymore since he dedicates 100% of his time not on Scala development but on coordinating lynch mobs against his personal enemies within the Scala community. Coincidentally, he is also the new boyfriend of Yifan Xing (the accuser) and a sworn enemy of Jon Pretty. reply outof 12 hours agorootparentTravis Brown is famous for outing \"libsoftiktok\" (much to Tucker Carlson's on tv ire) and other far right and anti-trans hate speech influencers. I can see how a certain type of person would hate him, but i think he's a hero. reply cbeach 11 hours agorootparentTravis Brown worked for Twitter and almost certainly had access to personally-identifying information on @libsoftiktok that he used to dox her. You might think he's a hero, but I'm not sure you'd like it if the situation were reversed, and rightwing tech workers abused their position to expose your online identities because they disagreed with your political opinions. reply outof 3 hours agorootparentit was way after he left and he showed how it was done. i dont remember the details but there was a clear public trail of breadcrumbs. zero inside knowledge needed. you should do your research before making such wild claims. i would never post inflammatory hate speech anonymously so the reversal isn't possible. reply cbeach 12 hours agorootparentprev [–] Yifan is Brown's new girlfriend? Pieces of the puzzle are now falling neatly into place. There were two blogposts written about Jon Pretty, insinuating bad behaviour on his part towards woman. One by Yifan. One by Victoria Leontieva (AKA killnicole). Victoria was Travis's girlfriend too. So, both Victoria and Yifan dated Travis. That's way too coincidental to be coincidence, especially when Travis was known for systematically cancelling people based on his perception of their political opinions (e.g. https://github.com/travisbrown/cancel-culture, https://github.com/travisbrown/twitter-watch) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Scala developer and speaker Jon Pretty faced false allegations from prominent Scala community members, resulting in his cancellation three years ago.",
      "Pretty took legal action, sued for libel, and after a two-year battle, the parties settled, with the accusers admitting fault and apologizing.",
      "This incident had a negative impact on Pretty's career and personal life, but he has now received vindication and aims for reconciliation and continuation of his work with Scala 3."
    ],
    "commentSummary": [
      "Jon Pretty has settled sexual harassment allegations within the Scala community using a consent order, sidestepping prolonged legal proceedings.",
      "Debates emerge on defamation laws disparities between the US and UK, focusing on burden of proof and accountability variations.",
      "The discourse delves into cancel culture, allegations of coordinated accusations, and possible agendas among Scala community members, with Travis Brown, known for uncovering hate speech and cancelling individuals for their political views, at the center, facing accusations of doxxing and personal connections to parties in the case."
    ],
    "points": 130,
    "commentCount": 90,
    "retryCount": 0,
    "time": 1714140316
  },
  {
    "id": 40171551,
    "title": "Exposing Amazon's Aggressive Business Tactics",
    "originLink": "https://www.vanityfair.com/news/story/inside-amazon-business-practices",
    "originBody": "Q & A Inside the Brutal Business Practices of Amazon—And How It Became “Too Toxic to Touch” In an interview with Vanity Fair, reporter Dana Mattioli reveals how the company systematically stifles criticism, squeezes out competitors, and even pits its own employees against one another. “People tend not to last,” she says, “because it’s very aggressive and it can be bruising.” BY JACK MCCORDICK APRIL 23, 2024 BY DIMITAR DILKOFF/AFP/GETTY IMAGES. In May of 2020, seven members of the House Judiciary Antitrust Subcommittee penned a letter to then CEO of Amazon Jeff Bezos. “On April 23,” their message began, The Wall Street Journal “reported that Amazon employees used sensitive business information from third-party sellers on its platform to develop competing products.” The article contradicted previous sworn testimony from the company’s general counsel, possibly rendering the testimony “false or perjurious,” the seven congressional leaders wrote. The Journal’s exposé, which ultimately spurred Bezos’s first-ever congressional testimony, was written by Dana Mattioli as part of the paper’s wide-ranging investigation into Amazon’s business practices. At the time, Mattioli, a longtime business reporter, had recently moved into the Amazon beat, her interest piqued by the corporation’s tentacular infiltration of nearly every aspect of American economic life. Now, four years later, she’s out with The Everything War, a new book-length examination of Amazon that explores everything from its rise to power to its lobbying efforts and the brewing backlash against it. Hive Where Wall Street, Washington, and Silicon Valley meet. Enter your e-mail SIGN UP By signing up you agree to our User Agreement and Privacy Policy & Cookie Statement. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. In this interview with Vanity Fair, edited for length and clarity, Mattioli and I spoke about the challenges of reporting on an infamously secretive and combative company, Amazon’s forays into political-influence peddling, its new foe in the Biden administration, and which candidate she thinks Amazon execs want to see back in the White House come January 2025. Vanity Fair: What first got you interested in covering Amazon? Dana Mattioli: I was The Wall Street Journal’s mergers-and-acquisitions reporter for six years, and in that role, my job was to cover which companies are buying other companies across industries globally. Something fascinating happened during my tenure in that role. It wasn’t just retail companies that were nervous about Amazon. I’d speak to the bankers, the lawyers, the CEOs, the board members at different companies, and they started talking about how they were worried about Amazon invading their industry. Over the course of those six years, those questions got louder. It started bleeding into other sectors where you wouldn’t even really think about Amazon at the time. The company seemed to stretch into every vertical and its tentacles kept spreading. It occurred to me that this was the most interesting company, but also one of the most secretive companies in business history. That to me seemed like such a fun challenge to dig in and see what was going on behind the scenes. What are the sorts of challenges reporters covering the company face? Read the Book Here All featured products are independently selected by our editors. However, when you buy something through our retail links, Vanity Fair may earn an affiliate commission. BUY ON AMAZON BUY ON BOOKSHOP I would say that, as it relates to me, they didn’t provide access, but that doesn't mean I didn’t get access. I spoke to 17 S-team members—the most senior people at the company—for this book, without the company knowing. I spoke to hundreds of people in and around the company. I had hundreds of pages of internal documents. They didn’t really cooperate for the book in setting up interviews, and I understand why. Some of my investigations at the Journal had been very hard-hitting. One of them was the basis for Jeff Bezos’s being called to testify to Congress for the first time in his career. So they didn't participate on an official basis, but I of course did a full fact-check. Out of fairness, I incorporate their PR statements and rebuttals very generously throughout. But it is an interesting company from a PR standpoint. There was an investigation from Mother Jones about the company bullying reporters, how they have lied to reporters in the past, and how that makes things difficult for reporters trying to cover the company. And that investigation questions whether that’s a tactic to get people to back off and not even want to cover them in the first place. What do you think it is about Amazon’s internal culture that made so many employees willing to talk to you? Amazon is the most interesting company culture and the most aggressive one I’ve ever covered. It’s a giant company. More than a million people work there. The turnover and the burnout is much higher than at most other companies. People tend not to last, because it’s very aggressive and it can be bruising. As a result of that, a lot of people have come to me—both people still there and people that have left—to tell me their experiences. When I delve into what goes on behind the scenes and the anticompetitive business behaviors that make Amazon win so often, a lot of it is the product of this culture. A lot of the shocking behaviors are because of this company’s culture. If you’re auditioning for your job every day, and you’re auditioning against every other brilliant employee there, and you know that at the end of the year, 6% of you are going to get cut no matter what, and at the same time, you have access to unrivaled data on partners, sellers, and competitors, you might be tempted to look at that data to get an edge and keep your job and get to your restricted stock units. If you’re at [Amazon] and you’re meeting with [outside companies] on the dealmaking side or the Alexa venture capital side, you might be tempted to not forget what you learned in those meetings and use it on a product to have a home run. MOST POPULAR 13 Undersung TV Gems to Binge Right This Second BY MAUREEN RYAN Supreme Court Justice Samuel Alito Argues Presidents Must Be Allowed to Commit Federal Crimes or Democracy as We Know It Will Be Over BY BESS LEVIN Poppy Harlow Is Leaving CNN BY CHARLOTTE KLEIN There’s a moment in the book where you describe the company’s business practices as liable to “shift from facilitator or partner to mobster” in “the blink of an eye.” Can you say a bit more about what is Mob-like about the way Amazon conducts business? They’re one of the most dominant companies across industries. Retail, cloud computing, digital advertising, smart devices—you name it. It’s this giant conglomerate with a staggering number of industries in which it’s the number one, two, or three player. That creates this dynamic where sellers are beholden to them, partners are beholden to them, and even competitors have to work with them. If you are on Amazon in some way, as a seller or even a competitor, you need to use them. Amazon has the leverage to say, “Well, if you don’t want to do things on our terms, then no more access.” We also uncover that Amazon threatened some partners or rivals with predatory pricing. Under the battle cry of being customer-obsessed, they sold customers dangerous goods, like carbon monoxide alarms that don’t detect carbon monoxide, or fake tourniquets. They touted all the good that they do for small businesses in order to rehab their image while crushing many of them. Let’s talk a bit about Amazon’s PR efforts. The company wasn’t always as invested in burnishing its public image as it is today. When did that start? Amazon was sort of late to realize how big they were. It’s like 2013, their market cap is over hundreds of billions of dollars, people are starting to cry foul about their behavior, their tactics with book publishers and everything else, but they still think that they’re this scrappy start-up, which is totally not the case. At that point, they’re not really investing in lobbying. They don’t really have a great government relations system set up. The board starts to tell Jeff, “We got to start taking this seriously.” Bezos was really disinterested in DC. In 2015, they brought in someone everyone thought would be a heavy hitter: Jay Carney. He was Barack Obama’s press secretary, and was also very close to then vice president Joe Biden. They bring him on to head up government relations and public relations for the firm. The company really bulks up its forces: thousands of people on government relations and public relations. The team grows really massively, and so does their spending. They start spending like a company of their size. Today, they’re one of the top companies by lobbying spending in the country. And what sort of narrative do they seize on? That team focuses on a narrative that they’re doing so much good for small businesses. They start putting out this small-business impact report, where it talks about all the ancillary jobs they’re creating through their sellers and whatnot. But behind the scenes, their business tactics were hurting those very sellers. The very people they were quite literally parading around on Capitol Hill to say, “Look how many jobs we’re creating in your backyard. Mr. Senator,” behind the scenes felt like they were stuck on this hamster wheel, where every year they’re making less and less money on Amazon, where the company could just extort them for fees, and it was becoming less viable. MOST POPULAR 13 Undersung TV Gems to Binge Right This Second BY MAUREEN RYAN Supreme Court Justice Samuel Alito Argues Presidents Must Be Allowed to Commit Federal Crimes or Democracy as We Know It Will Be Over BY BESS LEVIN Poppy Harlow Is Leaving CNN BY CHARLOTTE KLEIN There’s a pattern that plays out time and time again—more than I could even fit in this book; I had to really be choosy with examples—where entrepreneurs would go to Amazon in good faith. Since Amazon’s one of the top players in so many areas, if you’re selling your company, or if you’re looking for fundraising, you sort of have to talk to them. These entrepreneurs would open the kimono and reveal all of their road maps, financials, give them access to what their patents look like, all under the guise of maybe getting an investment or an acquisition. And then they’d get ghosted, and Amazon would turn around with a very similar product. There are a lot of those contradictions throughout the book as Amazon is trying to create this narrative around small businesses. One thing that stuck out to me is that even as the company plowed so much more money into PR and lobbying efforts in Washington, its executives kept embarrassing themselves publicly and putting their feet in their mouths. What do you make of that? I’m glad you picked up on that, because it was really funny to me. The DC team that they hired knew Capitol Hill. They knew how things get done in Washington, DC, where it’s not scorched earth. These were people who came from other parts of government to join Amazon’s lobbying effort, and they were doing something called “watering the flowers,” which refers to making friends on the Hill so that people would be receptive to your talking points. But back in Seattle, where Jeff Bezos was not at all interested in politics and meeting with different legislators, there was a much more pugnacious temperament toward DC and government and any sort of criticism. Their snap reaction to a lot of criticism is to fight back, or “punch back,” as Bezos would tell his team. So on the one hand, the DC team is walking around the Hill, taking meetings with all these different legislators like Elizabeth Warren. And then Jeff Bezos and his team of top executives are writing nasty tweets at them and undercutting all their efforts to build those relationships. There’s a really fun scene in the book where it’s Jeff and his top deputies—it’s the middle COVID, so they’re not in the same room, but they’re all on a conference call—ginning up nasty tweets to active legislators who could be determining their fate. Stuff like that played out all the time. This aggressive culture, this drive to win at all costs, went beyond their business practices. It pervaded the entire culture of the company, including the way it handled PR and government relations. The book opens with what one of your sources calls “the most important law review article written in our lifetimes.” What was the article, who wrote it, and why is it so important? In 2017, a law school student at Yale named Lina Khan wrote a law review article called “Amazon’s Antitrust Paradox.” She was an unknown person at this time. The article makes the claim that Amazon is a monopoly, and that the change in the way that antitrust has been enforced in the last few decades means that companies like Amazon are not being regulated nearly the way they should be and that it’s creating monopolies. This law review article does something remarkable. These things are usually not widely read, but it goes viral. Politicians read it. Heads of corporate development and legal counsels at big companies read it. Reporters read it. It really raises her profile, and people start questioning Amazon’s power. MOST POPULAR 13 Undersung TV Gems to Binge Right This Second BY MAUREEN RYAN Supreme Court Justice Samuel Alito Argues Presidents Must Be Allowed to Commit Federal Crimes or Democracy as We Know It Will Be Over BY BESS LEVIN Poppy Harlow Is Leaving CNN BY CHARLOTTE KLEIN Khan’s law review note was published right as Trump was being inaugurated. Can you talk a bit about how the company responded to Trump? It’s just so fascinating. They were not at all prepared for a Trump win. They were in shock. This was really bad news for the company, because even before he was nominated, Trump is saying, “I’m going to break this company up. They’re a monopoly. They don’t pay their taxes.” And that animosity just persists and gets louder throughout his four years in the White House. There’s a series of nasty tweets about the company. Behind the scenes, his billionaire friends are in his ear saying, like, “You’ve got to do something about them. They’re ruining the economy.” There’s a scoop in the book where Nelson Peltz of Trian, an investment company, has a white paper written that gets to President Trump’s inner circle about Jeff Bezos and Amazon being this unheard-of monopoly that needs to be stopped. So that’s going on behind the scenes. But there’s also this personal animus that cannot be understated. I spoke to a lot of Trump’s top aides, and they said that this stemmed from him just being jealous of Bezos. He is jealous of people with more wealth than him. This is a constant bête noire for him. What’s fascinating to me is that this happens not very long after Jay Carney is brought on to head up government relations. He essentially sits out for the four years of the Trump White House. He determined that it was too far afield from his politics to engage with. By the time that Biden is elected president, which Amazon was really excited about, Carney was excited since these were his people in the White House. They think things are going to change. But it gets arguably worse from there. There’s actual teeth to the policies that the Biden administration puts forward. It’s true—you have some great moments in the book when Carney is texting Biden chief of staff Ron Klain, his old friend, and getting the cold shoulder. Looking ahead to November, from a purely business perspective, who do you think Amazon execs hope wins the election? It’s really tough to say. I don’t think it’s a great outcome for them either way, to be honest. The Trump years had a lot of rhetoric, but not much teeth. Although I did speak to Trump’s director of the White House National Trade Council, Peter Navarro, and I asked him, “If you guys were so worried about Amazon, why didn’t you do anything?” And he said, “Oh, we would have gone after them in a second term.” Who knows? Whereas for the Biden White House, they placed Amazon’s biggest foe—Lina Khan—at the head of the Federal Trade Commission, the agency regulating them. Biden has backed this antitrust reform movement. He’s aligned with Lina Khan on antitrust and Jonathan Kanter at the DOJ. He said he’s the most pro-union president in American history, and Amazon is notoriously anti-union. And he’s embraced their enemies like Chris Smalls and Walmart’s CEO. There’s a scene in the book where Amazon learns that the Biden presidency just views Jeff Bezos and Amazon as too toxic to touch. They’re getting shunned from the White House, essentially. So I don’t know if that’s great for them, either. MOST POPULAR 13 Undersung TV Gems to Binge Right This Second BY MAUREEN RYAN Supreme Court Justice Samuel Alito Argues Presidents Must Be Allowed to Commit Federal Crimes or Democracy as We Know It Will Be Over BY BESS LEVIN Poppy Harlow Is Leaving CNN BY CHARLOTTE KLEIN The book ends with some of the legal and legislative efforts to chip away against Amazon’s power, including the recent lawsuit brought against the company by Khan’s FTC. How would you evaluate the strength of the emerging antitrust movement in Washington? Lina Khan has a lot of momentum behind her. There are even Republicans that are on her side when it comes to antitrust reform. But ultimately you need the courts to side with you, and they’ve had some big losses. So it’s really hard to know where this will end up. When I speak to antitrust experts, they also point out that even if she were to be successful and the FTC breaks up Amazon, is it too late? Do you run into a Standard Oil–type problem, where the Supreme Court breaks up Standard Oil in 1911 into 30-something pieces, and those pieces become market leaders in their own right, and they become the predominant force, and the value of those separate pieces is actually greater than when they were all together? It’s really hard to tell where this nets out. Update: An Amazon spokesperson provided Vanity Fair with the following response to Mattioli's book: “Amazon’s success is the result of continually innovating for consumers and small businesses over three decades to make their lives better and easier every day. The facts show Amazon has made shopping easier and more convenient for customers, spurred lower prices, enabled millions of successful small businesses, and significantly increased competition in retail.” More Great Stories From Vanity Fair Anne Hathaway on Tuning Out the Haters and Embracing Her True Self Scenes From the Knives-Out Feud Between Barbara Walters and Diane Sawyer Eddie Redmayne, Liza Minnelli, and the Untold History of Cabaret Deprived of His 12 Daily Diet Cokes, Trump Falls Asleep (Again) at Trial While Melania Thinks the Hush Money Trial Is a “Disgrace”: Report The 25 Best True-Crime Documentaries to Binge Right Now From the Archive: The Devil in Bette Davis Stay in the know and subscribe to Vanity Fair for just $2.50 $1 per month.",
    "commentLink": "https://news.ycombinator.com/item?id=40171551",
    "commentBody": "The brutal business practices of Amazon (vanityfair.com)129 points by makerdiety 17 hours agohidepastfavorite55 comments cxr 15 hours agoThe writer Rob K. Henderson has covered the hypocrisy of the upper and upper-middle classes in several of his pieces on what he calls \"luxury beliefs\". The case of Amazon def. fits into a similar theme. You can broach the topic with uppercrust folks (without even really trying) of whether supporting Walmart by shopping there is conscionable or not, and you'll get one set of responses/reactions. Meanwhile, it's nowhere near socially unacceptable to talk about all the stuff you order from Amazon, despite Amazon appearing to be by and large worse in its compensation/treatment of its employees and its effect on the economy and the environment—all things that aspiring bougie types ostensibly profess to rank highly in their meditations when it comes to Walmart. reply xanderlewis 13 hours agoparentThe notion of luxury beliefs isn’t just about plain old hypocrisy (though of course there’s plenty of that) — I think it’s more about beliefs one can only hold because of one’s position of privilege. The usual example given is ‘defund the police’. If you’re rich enough to hire your own security or otherwise live safely in a world without police, you can advocate for this and win social points for doing so. Meanwhile, the actual consequences of such an idea, were it to be implemented, would be catastrophic. Unlimited immigration/open borders is another one: if you’re relatively poor, your ability to make a living (among other things) is potentially threatened by this. If you’re rich, you needn’t worry. You can live in your fancy neighbourhood far detached from the consequences of such policies, so you can freely hold such positions and even frown upon those who don’t. Luxury beliefs are just like tangible luxuries [*] — you have them not because you want them or really believe in them, but because they signal to others that you’re of a certain class. Poor people can’t have them, so they’re desirable as status indicators. [*] OK — some ‘luxuries’ are worth it and aren’t just for showing off. But I’m talking about the ones that are. reply ScoobleDoodle 13 hours agorootparent\"Defund the police\" doesn't mean what you think it means here. It doesn't mean removing all funds from the police as the name implies. It actually means reallocating funds from the police to trained professionals for appropriate situations. Like having a government division that hires psychologist / mental health workers to respond to mental wellness checks. Having mental health / homelessness services workers respond to calls about vagrants. So instead of getting a boot to the teeth or death as seen in the news, those at risk groups and people get the directed help and support they need. https://en.wikipedia.org/wiki/Defund_the_police reply rpjt 12 hours agorootparentI'm not sure this distinction matters much to most people, though. People hear \"defund the police\" and draw their own conclusions as to what that means, and it's not a far leap to go from \"defund the police\" to \"welp, guess they want to tie the hands of cops and take away all of their funding so they can't do their jobs\" reply Eddy_Viscosity2 12 hours agorootparentI think what your saying is that people are drawing the wrong conclusion when they hear 'defund the police'. But, if they knew what was really meant they may actually support it. So the distinction actually matters a lot. Like if they heard that it meant sending trained mental health professionals to deal with a mental health crisis called into 911 instead of just sending some cops who may very well just shoot them, that might change their minds about it. reply _DeadFred_ 10 hours agorootparentNo, what we are hearing is that a group for some reason chose a horrible catch phrase that they now say does not mean what the phrase specifically, on it's face, means, and that the group now wants to tell everyone it's not them it's us. Edit: I think the USA needs to completely change how we approach mental health. My grandfather spent his life cruisading for that. Allowing the catch phrase to distract from that point to the extent that the catch phrase is now pretty much a central focus shows that 'defend the police' very much is a problem. reply IG_Semmelweiss 9 hours agorootparentYou are right. But, let's complete the idea to make it bulletproof. Regardless of moving goalposts due to changing definitions, the luxury belief still remains: Reallocate (vs defund) police still moves funds around. Funds are not infinite. - Less police means more crime. This means fewer personnel to combat crime. Crime strikes directly at the poorest. - Less police means more mental care. This means more personnel and facilities to combat mental cases. This had been tried already, with no meaningful decrease in mental problems. reply Eddy_Viscosity2 9 hours agorootparent>This had been tried already, with no meaningful decrease in mental problems Has it? reply Eddy_Viscosity2 9 hours agorootparentprev> Allowing the catch phrase to distract It isn't a good catchphrase, I fully agree. But the reason its been derailed is because there are people actively derailing it and deliberately misleading its meaning. They fight any plan that would diminish the authority and power of police. That's the problem here. reply dude187 7 hours agorootparentprevThey correctly understood it to mean abolish the police. As was made very clear by those who created the statement as they carried it the BLM riots in 2020 reply xanderlewis 12 hours agorootparentprevFair enough. I've heard 'abolish the police' and 'all cops are bastards' (whatever that means) too, so such a sentiment definitely exists. Either way, those usually espousing such things don't seem to have much of a concrete plan in mind (and don't have to, because offering concrete solutions isn't the purpose of such rhetoric). > According to the New York Times, the slogan and movement failed to result in any meaningful policy change. This was attributed to the slogan having no clear definition of its goals. (from the Wikipedia page you linked to) reply Terr_ 6 hours agorootparent> I've heard [...] 'all cops are bastards' (whatever that means) too Is that an honest question? Usually it asserts that police institutions operate similar to organized crime, where some level of bad acts (e.g. perjury, evidence tampering, abuse of power) are a de-facto requirement of continued membership. Thus the corollary that anyone who survived there long-enough to be \"a cop\" must have become \"a bastard\" to do so. Compare to: \"All mafia members are bastards.\" Such systems are self-sustaining because each cohort has the dilemma of defending itself against being denounced by the next. Forcing incoming members to commit the same crimes means they are \"stuck in the same boat\" , changing incentives from \"reveal their crime\" to \"hide our crime.\" reply rufus_foreman 8 hours agorootparentprev\"Yes, We Mean Literally Abolish the Police\" -- New York Times, 6/12/2020, https://www.nytimes.com/2020/06/12/opinion/sunday/floyd-abol... reply eszed 12 hours agorootparentprevThat's a thoroughly sensible proposition. However, right there in the Wikipedia article you post: > some [\"Defund the Police\" advocates] seek modest reductions, while others argue for full divestment as a step toward the abolition of contemporary police services. \"Defund the Police\" is my nomination for the worst political slogan of the 21st century (so far). It contains such multitudes that it's become a Rorschach test for both users and hearers. I wish the reformers (like yourself - whom I fully support) and the abolitionists (I think they're wack jobs) would decide to march under different banners. reply spacemanspiff01 8 hours agorootparentprevIt's also one of the most idiotic slogans in history. Any slogan that requires a paragraph or Wikipedia article to explain what it \"really means\" is public relations malpractice. And I'm coming from the side were I agree with much of the actual policy proposals behind it. It boggles the mind of who came up with the line and thought, yes this is a political winner. Sorry rant over. reply dude187 7 hours agorootparentIt came out of the BLM riots, and very much meant complete police abolishment. All the long winded after the fact redefinitions was just people wrapped up in it stepping in to save face. Since at this point they're linked to it and know it looks bad now that they've had a second to step back and think about it reply mrangle 9 hours agorootparentprevThis feels like the equivalent of mansplaining on this topic, with a heavy dose of gaslighting for bad measure. We were all around for the 2020 riot period, wherein what it means explicitly was made clear over and over and over again. reply VirusNewbie 12 hours agorootparentprevThis is certainly not the case for the NPO BLM groups that clarified they want to abolish the police. reply jjav 1 hour agorootparentprev> The usual example given is ‘defund the police’. > live safely in a world without police That is not at all what the ‘defund the police’ is about. I know the tagline is confusing. What it really means is to fund the various necessary services instead of only the police. So if someone is in need of mental health help, you send a mental health professional instead of the SWAT team. reply Terr_ 6 hours agorootparentprev> Luxury beliefs [...] they signal to others that you’re of a certain class I suppose we need a different term for \"norms which mainly exist because our civilization is doing well enough.\" For example, \"killing everyone in a conquered village is bad\", or \"don't eat fallen foes as food\", etc. reply silverquiet 15 hours agoparentprevI think Amazon is actually among the best compensated of the \"shit jobs\" out there, but it is about the shittiest (certainly it's the pissiest; I've seen the bottle bombs their drivers like to leave since they don't really accommodate bathroom breaks). I don't even really find much hypocrisy amongst the upper class here; they'll outright say that these are shit jobs not intended to provide a living wage; certainly not something you can raise a family on. Probably said as, \"those people should have gone to school\". reply _DeadFred_ 10 hours agorootparentAmazon warehouse jobs are so bad the company has to have contingency plans for when they have burned through the entire available labor force. Walmart does not have to have such plans. reply cxr 14 hours agorootparentprevThe Walmart distribution center built around where I grew up was not known as a hub of shit jobs. It was a source of envy. Amazon warehouse worker pay, in contrast, lags behind Walmart by several dollars per hour and offset several years. And that's assuming you're actually hired by the company directly, rather indirectly as a second class worker brought in through the timeless scam involving contractors employed by staffing agencies. reply StressedDev 13 hours agorootparentI work for staffing agencies at I like it. Please do not assume everyone wants a traditional 9-5 job. reply ghaff 13 hours agorootparentTotal anecdote but had the opportunity to chat at length with an Amazon warehouse worker (in the UK) over the holidays. They said it’s hard work but they got various perks, decent pay, they didn’t even want to be a supervisor which they had been offered, and liked it. Would I want the job? Hell no. But this is an intelligent person who is doing this because they have no other option. reply kevin_thibedeau 12 hours agorootparentThe UK has social safety nets that Americans don't get when they have a job. Amazon isn't allowed to play the same game in countries with worker protections. reply ghaff 12 hours agorootparentI can certainly believe there are issues for Amazon workers in the US that don’t apply in the UK to the same degree. Although Amazon does seem to have various heath insurance plans for US workers. I assume workers have to pay some amount in as is the case with most jobs, including white collar ones, in the US. reply _DeadFred_ 10 hours agorootparentprevAmazon management has to routinely come up with contingency plans after having burnt through/burned out the local available workforce. I have never heard of any other company ROUTINELY has that issue. reply tootie 13 hours agorootparentprevDidn't Walmart get caught locking people inside their facilities? https://www.nbcnews.com/id/wbna4146540 reply jakjak123 12 hours agorootparentprevAmazon does not have a physical whip yet, but they do have a mental whip of tracking every minute of every day you do at work, and ranking you among all the others working there. reply keiferski 14 hours agoparentprevThose kinds of people don’t shop at Walmart because they perceive themselves as being in a higher social class than the “people of Walmart.” I don’t think they care much about the compensation of the employees. reply cxr 13 hours agorootparentRight. The key word in my comment was \"ostensibly\". reply boredinstapanda 15 hours agoparentprevThat seems to reason. Unless you buy all your stuff at Walmart online, you have to go in and see the people working and their working conditions. Amazon, you only see the checkout page. It makes it much easier to ignore the problems. reply lotsofpulp 14 hours agorootparentShopping in Whole Foods is shopping at Amazon, where you also have to go in and see the people working and their working conditions. reply skhunted 13 hours agoparentprevI care about worker’s rights but there is only so much I can do. The Clinton administration refused to include a requirement on standards of employment in order to benefit from free trade with the U.S. Walmart did a lot of harm while it grew to its current size. That memory is still fresh. The damage caused by Amazon is more opaque but it is a loathsome company. What is needed are standards of employment that all companies should be forced to adhere to. reply Nevermark 13 hours agoprevTraditional business value comes from some innovation, and a lot of execution/logistics delivering the innovation. Logistics for product companies being manufacturing, and sales/delivery. But now most small product companies have outsourced production on the \"input\" side, and outsourced logistics on the \"output\" side. Which leaves harsher more direct competition for their innovation core, since that is a fraction of the original business and more easily copied. High componentization of any economy ratches up competitive pressure exponentially, whether it was Amazon, or a collection of service companies offloading the downstream logistics. But since Amazon is protecting itself by doing the opposite, by de-componentizing, i.e. preferencing and tying different sales and delivery logistics services to each other, that centralized leverage is going to squeeze small product companies even more. Especially those that don't have a unique manufacturing process, unique branding, or some other differentiating moat. reply acl777 13 hours agoprevhttps://archive.ph/vTO3D reply jimt1234 12 hours agoprevHow does Amazon operate in countries outside the US. Does Amazon apply the same anti-competitive business practices? Is the workplace equally cut-throat? Just curious. reply flerchin 15 hours agoprevWhatever happened with the \"false or perjurious\" testimony at the beginning of the article? Nothing? reply neodypsis 5 hours agoprevCan I read this on Kindle? reply WalterBright 13 hours agoprev> Dana Mattioli reveals how the company systematically stifles criticism, squeezes out competitors, and even pits its own employees against one another. “People tend not to last,” she says, “because it’s very aggressive and it can be bruising.” Wow! Wait till she does an investigation of professional sports! reply aidenn0 11 hours agoparent> Wow! Wait till she does an investigation of professional sports! At least those are union jobs. reply mehulashah 16 hours agoprevI was Amazon for quite some time, and also outside of Amazon trying to compete against them. The company is both decentralized and has a culture of delivering. While that sometimes has unintended consequences, the people there are quite aware that they can sometimes be an elephant in a china shop. There are practices in place to rein that in. Nonetheless, investigations like this book hardly ever give a balanced view. reply brevitea 15 hours agoparentBy \"balanced view,\" do you mean the one where Amazon lawyers attempt to erode the ever-growing list of credible reports outlining Amazon engaging in and encouraging the tactics outlined in the article/book? From what I can tell, the article is remarkably spot on. It begs the question, without government contracts[1], where would Amazon actually be? Would Amazon be out of business? Hmm... Makes you wonder. [1] https://watson.brown.edu/costsofwar/files/cow/imce/papers/20... reply woooooo 12 hours agorootparentThey file all sorts of forms for where their revenue comes from, they're very profitable without government help, it's an easily checkable fact. reply brevitea 12 hours agorootparentThat's actually a point of contention. Aside from AWS, how successful is Amazon actually? reply mangamadaiyan 15 hours agoparentprevWhat sort of investigations provide a balanced view, then? reply kmeisthax 15 hours agorootparentThe ones that don't give him moral injury for working at Amazon, presumably? reply StressedDev 13 hours agorootparentPlease do assuming that the original poster is acting in bad faith. I have seen a lot of reporting about disfavored organizations, and it is often unfair. I do not know if this reporter has done a good job or not but I have seen a fair number of anti-Amazon articles which basically attack Amazon for being successful. The other problem with the article is it does not point out some obvious problems Amazon has. First, Amazon is expensive, and people can and do go else ware. Second, Amazon's retail side is mediocre. Amazon's retail web page is basically a product search engine. Outside of search, Amazon does very little work to help people find products they are interested in. Amazon also does almost no curation. This is why it is often hard to distinguish between good non-fiction books and books which spread disinformation. Third, a lot of Amazon initiatives have failed. Examples include Alexa (a giant money pit), the Fire Phone, Amazon's video game division (over a billion and basically only produced 1 moderate hit), Prime Video (a mediocre streaming competitor), etc. I have also heard Amazon has been destroying Twitch. My main point is, the article paints Amazon as an unstoppable machine when in fact it is very fallible. It does some great things (AWS), some good things (Kindle), some OK things, and sometimes some very bad things. reply samatman 10 hours agorootparentI was sure you were going to get to the counterfeit and substandard goods problem, but you cited plenty of things in your comment, so I'll just add it. Amazon's retail market is absolutely lousy with counterfeit goods, as well as products blindly imported from China (usually) which don't meet the standards required by regulation for such goods. This regularly causes injury, especially due to electrical fires, and Amazon has not been held to account for any of these practices. Try heading to a street corner and selling counterfeit products: it's a crime, you'll be charged accordingly. Amazon has gotten away with this for more than a decade. reply karaterobot 13 hours agorootparentprevThat's not exactly the most generous interpretation of that comment. reply NathanKP 14 hours agorootparentprevAmazon employee here, seven years at the company so far. This is my personal opinion only: I don't know that there are many articles I've read about Amazon that have a balanced view. I will say that many people who are anti-Amazon have this picture of Amazon as being more ruthless (and smarter) than it actually is. For example, an Amazon seller can make the claim that Amazon used it's access to marketplace data to research info about their product sales and then compete with them. But as an insider I believe the reality is probably far more stupid: a seller who had their product \"copied\" by Amazon was likely just a middleman who was using a factory in China to make their product. The middleman is basically just extracting value from being a middleman. But most Chinese factories produce extra product on the side. Once they setup the toolchain and processes for building your product why not keep the assembly lines running a bit longer and produce a few thousand more copies of your product? Or maybe the factory keeps a big stockpile of the rejected product that was slightly bad quality, so they couldn't send it to the original middleman. And if you are this Chinese factory then why not sell this extra product (mixing in the lower quality rejects) directly to Amazon for Amazon to list as an Amazon Basic? The factory can make more money by cutting out the middleman, and some ignorant Amazon product acquisition worker will buy that stuff, not even realizing it is what it is. No \"sensitive business information\" needed. So \"Amazon copied my product as an Amazon Basic\" is probably way more dumb than it sounds. Or the \"Amazon copied my product idea or roadmap\" claim from the article. Do you realize how many ideas there are floating around in Amazon when you have >1 million employees? Any idea you can come up with has already been suggested dozens of times by Amazon employees and was probably also requested by Amazon's own customers via support a couple thousand times. The only question is whether that idea is worth building now versus worth building later. And as it turns out when there is real demand for an idea there tends to be multiple people who start building that idea at the same time. In fact sometimes there are multiple teams within Amazon who start trying to build the same thing at the same time, and they didn't even know about each other. Often one team fails to build it while another team succeeds. The duplicated effort eventually gets sorted out, but it happens a lot. This is the type of scenario that happens time and time again. It's fun to write a click attracting story about Amazon as this big brutal force when the reality is most likely much dumber than it seems. reply eks391 13 hours agorootparentAs a former employee of a 100K+ person company, I've had these exact thoughts several times. I worked in acquisitions, and constantly was finding out that we were reinventing (or rebuying) the wheel internally in independent departments. reply _DeadFred_ 10 hours agorootparentprevAmazon is the only company I know of that has to have contingency plans for blowing through the available local workforce, their treatment of people is that bad. I have been at the very bottom of society, and everyone chose to work somewhere else than an Amazon warehouse, including the local refuse recycling plant sorting putrid trash that paid less. reply rqtwteye 15 hours agoprev [2 more] [flagged] euroderf 15 hours agoparent [–] Maybe \"nothing new under the sun\" and/or \"nothing to see here\" are not sensible reactions to the article. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Dana Mattioli's interview with Vanity Fair reveals Amazon's aggressive tactics, such as stifling criticism, eliminating competitors, and fostering internal competition.",
      "Her book, \"The Everything War,\" delves into Amazon's ascent to dominance, lobbying endeavors, and challenging reporting experiences due to the company's secrecy.",
      "The article discusses Amazon's lobbying, public relations strategies, influence on small enterprises, the feud with President Trump, and potential antitrust hurdles under the Biden administration amid the rising antitrust movement in Washington."
    ],
    "commentSummary": [
      "The article exposes the hypocrisy of the upper and upper-middle classes supporting Amazon but criticizing Walmart, discussing \"luxury beliefs\" stemming from privilege.",
      "It delves into the misunderstood concept of \"defund the police,\" exploring its various interpretations and implications.",
      "The impact of Amazon and Walmart on employee compensation, working conditions, competition in small businesses, and concerns about Amazon's practices are highlighted, including insights from Amazon employees and former employees' negative experiences."
    ],
    "points": 129,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1714150149
  },
  {
    "id": 40174316,
    "title": "Factorio: Enhancing Game Statistics and Linux Support",
    "originLink": "https://factorio.com/blog/post/fff-408",
    "originBody": "Factorio.comForumsWikiMod PortalAPI Docs Log inSign up 🇺🇦 We support the Ukrainian Red Cross. Game Screenshots Videos Content Artwork About us Buy Demo Merch Blog Support Help FAQ Presskit Contact Friday Facts #408 - Statistics improvements, Linux adventures Posted by Klonan, raiguard on 2024-04-26 Hello, welcome once again to the world of facts. StatisticsKlonan Do you love watching your production graphs grow as much as we do? Accumulator graph It is a little bit tricky when you transition from Steam power to Solar panels and Accumulators, to know if you have enough capacity in the system to survive the cold dark nights. Generally you might just wait until nighttime and see if your factory blacksout, if so, build more solar and accumulators. It would be helpful and convenient to see the statistics of accumulator charge levels, so we added such information: The main reason it was more critical, was that on Fulgora the production from lightning at nighttime is much less predictable, so it is much more important to see the timeline of the accumulator charge. Science graph You can track science pack consumption in the production GUI, but that does not account for things like productivity modules and the new research productivity technologies. So we added a new special item in the production statistics, that shows the total final 'Science' that is produced. Per surface production It was tolerable in the first days of playtesting with planets and platforms that all the production statistics were global. However when you want to get more and more precise with your gameplay and trying optimize each part, it becomes quite necessary. For instance on platforms, we need to know if we are producing enough fuel and ammo to keep the ride going: And its super helpful when checking if a specific planet producing enough when some items are crafted in many places. We also added a checkbox to switch to a 'Global statistics' view, so all the possibilities are available for the player. Quality graph Going deeper still, we want to dissect our production by what quality the produced items are. So what do you think? Are there any other statistics improvements you can think about for 2.0? Linux adventuresraiguard I have appeared in a few FFFs by now but I have never formally introduced myself. My name is raiguard. I have been playing Factorio since June 2017, making mods for the game since the 0.17 release in March 2019, and I finally joined Wube in March 2023. My primary roles at the company are expansion programming and Linux support, as well as being an advocate for the modding community. I have been daily-driving Linux for multiple years and have fallen ever deeper into the black hole of customization and minimalism. \"Why don't most games support macOS and Linux?\" is a sentiment I often see echoed across the internet. Supporting a new platform is a lot more than just changing some flags and hitting compile. Windows, macOS, Linux, and the Nintendo Switch all use different compilers, different implementations of the C++ standard library, and have different implementation quirks, bugs, and features. You need to set up CI for the new platform, expand your build system to support the new compiler(s) and architecture(s), and have at least one person on the team that cares enough about the platform to actively maintain it. If you are a video game, you will likely need to add support for another graphics backend (Vulkan or OpenGL) as well, since DirectX is Windows-exclusive. Many developers will take one look at the Windows market share and decide that it is not worth the trouble to support other platforms. Also, with the meteoric rise of the Steam Deck and Proton, it is easier than ever for game developers to ignore Linux support because Valve does some black magic that lets their game run anyway. Factorio supports macOS and Linux so well because there has always been someone at Wube who actively uses these platforms and is willing to take on the burden of supporting it. Our native Apple Silicon support is a great example of this. Today, I will take you through some of the adventures I've had with maintaining Factorio's Linux support. Wayland My first self-appointed task after joining the team was to add Wayland support to the game. Wayland is a new display protocol that has been in development to replace the antiquated and insecure X11 system. Modern Linux distributions are beginning to switch to Wayland as default, so supporting it in Factorio is paramount. We utilize the SDL library which neatly handles most low-level system interactions and abstracts them into a common interface. SDL has support for Wayland, so all that I theoretically needed to do was build SDL with Wayland enabled and it would \"just work.\" However, it's not quite a simple plug-and-play. Wayland provides \"protocols\" in the form of XML files that you then use the wayland-scanner binary to convert into C program and header files. Being relatively new to C++ at the time, my initial solution was convoluted and involved checking the generated Wayland protocols into our source tree, to be manually regenerated every time we updated SDL. A few months ago, armed with a years' worth of experience, I improved this workflow to automatically generate the files as a part of the build process, so they are always up-to-date with the protocol XML files that SDL ships with. Factorio has supported Wayland since 1.1.77, but it needs to be explicitly enabled by setting SDL_VIDEODRIVER=wayland in your environment. For Factorio 2.0 I added a dropdown to select your preference in the GUI: X11 (left) vs. Wayland (right) with the desktop display scale set to 125%. Notice how the game renders at the display's native resolution when running under Wayland. Client-side window decorations Once Wayland support was implemented, I received a bug report that the window was missing a titlebar and close buttons (called \"window decorations\") when running on GNOME. Most desktop environments will allow windows to supply their own decorations if they wish but will provide a default implementation on the server side as an alternative. GNOME, in their infinite wisdom, have decided that all clients must provide their own decorations, and if a client does not, they will simply be missing. I disagree with this decision; Factorio does not need to provide decorations on any other platform, nay, on any other desktop environment, but GNOME can (ab)use its popularity to force programs to conform to its idiosyncrasies or be left behind. To fix this, I had to bring in another dependency, libdecor. It functions, and SDL even has support for it, but a video game shouldn't have to supply window decorations in the first place. The game has decorations now, but the theme doesn't match. Thanks GNOME! Window resizing seizures A video is worth more than a thousand words: PHOTOSENSITIVITY WARNING: Rapid flashing images. I use the Sway window manager, and a particularity of this window manager is that it will automatically resize floating windows to the size of their last submitted frame. This has unveiled an issue with our graphics stack: it takes the game three frames to properly respond to a window resize. The result is a rapid tug-of-war, with Sway sending a ton of resize events and Factorio responding with outdated framebuffer sizes, causing the chaos captured above. I spent two full days staring at our graphics code but could not come up with an explanation as to why this is happening, so this work is still ongoing. Since this issue only happens when running the game on Wayland under Sway, it's not a large priority, but it was too entertaining not to share. Dynamically linked libraries In a C++ program there are three ways to load/include a library: By including it in your source binary (static linking). Having the system load it when your program starts (dynamic linking). Your program loads it explicitly after startup (\"dynamic loading\" or what I call \"runtime linking\"). We have many libraries, such as SDL, FontStash, and Lua, that are statically linked, but Factorio 1.1 also has many dynamically linked libraries: rai@tantal ~/games/factorio $ ldd bin/x64/factorio linux-vdso.so.1 (0x00007ffc123b1000) libdl.so.2 => /lib64/libdl.so.2 (0x00007fc182f70000) librt.so.1 => /lib64/librt.so.1 (0x00007fc182f6b000) libresolv.so.2 => /lib64/libresolv.so.2 (0x00007fc182f5a000) libX11.so.6 => /lib64/libX11.so.6 (0x00007fc182e13000) libXext.so.6 => /lib64/libXext.so.6 (0x00007fc182dff000) libGL.so.1 => /lib64/libGL.so.1 (0x00007fc182d78000) libXinerama.so.1 => /lib64/libXinerama.so.1 (0x00007fc182d71000) libXrandr.so.2 => /lib64/libXrandr.so.2 (0x00007fc182d64000) libXcursor.so.1 => /lib64/libXcursor.so.1 (0x00007fc182d57000) libasound.so.2 => /lib64/libasound.so.2 (0x00007fc182c43000) libpulse.so.0 => /lib64/libpulse.so.0 (0x00007fc182bf1000) libpulse-simple.so.0 => /lib64/libpulse-simple.so.0 (0x00007fc182bea000) libm.so.6 => /lib64/libm.so.6 (0x00007fc182b07000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fc182b02000) libc.so.6 => /lib64/libc.so.6 (0x00007fc182920000) /lib64/ld-linux-x86-64.so.2 (0x00007fc182f91000) libxcb.so.1 => /lib64/libxcb.so.1 (0x00007fc1828f5000) libGLX.so.0 => /lib64/libGLX.so.0 (0x00007fc1828c2000) libGLdispatch.so.0 => /lib64/libGLdispatch.so.0 (0x00007fc18280a000) libXrender.so.1 => /lib64/libXrender.so.1 (0x00007fc1827fc000) libXfixes.so.3 => /lib64/libXfixes.so.3 (0x00007fc1827f4000) libpulsecommon-16.1.so => /usr/lib64/pulseaudio/libpulsecommon-16.1.so (0x00007fc18276f000) libdbus-1.so.3 => /lib64/libdbus-1.so.3 (0x00007fc18271a000) libXau.so.6 => /lib64/libXau.so.6 (0x00007fc182714000) libsndfile.so.1 => /lib64/libsndfile.so.1 (0x00007fc182694000) libsystemd.so.0 => /lib64/libsystemd.so.0 (0x00007fc1825a1000) libasyncns.so.0 => /lib64/libasyncns.so.0 (0x00007fc182599000) libgsm.so.1 => /lib64/libgsm.so.1 (0x00007fc18258a000) libFLAC.so.12 => /lib64/libFLAC.so.12 (0x00007fc182524000) libvorbis.so.0 => /lib64/libvorbis.so.0 (0x00007fc1824f5000) libvorbisenc.so.2 => /lib64/libvorbisenc.so.2 (0x00007fc182448000) libopus.so.0 => /lib64/libopus.so.0 (0x00007fc1823ec000) libogg.so.0 => /lib64/libogg.so.0 (0x00007fc1823e2000) libmpg123.so.0 => /lib64/libmpg123.so.0 (0x00007fc182385000) libmp3lame.so.0 => /lib64/libmp3lame.so.0 (0x00007fc18230d000) libcap.so.2 => /lib64/libcap.so.2 (0x00007fc182303000) liblz4.so.1 => /lib64/liblz4.so.1 (0x00007fc1822df000) liblzma.so.5 => /lib64/liblzma.so.5 (0x00007fc1822ac000) libzstd.so.1 => /lib64/libzstd.so.1 (0x00007fc1821f0000) libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fc1821cc000) Among these libraries are X11 and PulseAudio, which are being deprecated in favor of Wayland and PipeWire respectively. This causes a compatibility nightmare because if any dynamic dependencies are missing, the game will not launch. This obviously will not do! The presence of these dependencies confused me because we utilize SDL for most of the low-level syscalls, audio, and video, and SDL relies entirely on runtime linking. An investigation revealed the source of most of these dependencies to be Allegro, the low-level library that we utilized for most of Factorio's alpha phase but we have since replaced with SDL. The only remaining use of Allegro in 2.0 was as a secondary audio backend in case a user experienced issues with the SDL audio backend, but the SDL backend has been stable for a very long time, so the time was ripe for its removal. This eliminated 123,024 lines of code from the game and drastically reduced the number of dynamic dependencies: rai@tantal ~/dev/wube/factorio (master) $ ldd bin/FinalReleasex64Clang/factorio linux-vdso.so.1 (0x00007fff96ff2000) libresolv.so.2 => /lib64/libresolv.so.2 (0x00007fd2df8a9000) libm.so.6 => /lib64/libm.so.6 (0x00007fd2df7c8000) libc.so.6 => /lib64/libc.so.6 (0x00007fd2df5e6000) /lib64/ld-linux-x86-64.so.2 (0x00007fd2df8d6000) Clipboard woes It turns out that Allegro was not the only thing requiring us to link against X11. Back in 2017, we received a bug report that a user could not paste large blueprint strings into the game, and Oxyd fixed this by adding support for X11 incremental clipboard transfers to our GUI backend's clipboard handler. I was hoping to utilize SDL's built-in clipboard functionality, but unfortunately SDL does not support incremental transfers. This means there are three options: Continue linking against X11, requiring users to install X11 on their system to be able to run the game (I don't want to mess with static linking). Figure out how to do runtime linking and implement that. Upstream our incremental transfers code into SDL so we can leverage SDL's clipboard functions and other SDL-based games can benefit from our work. As you might guess, I chose the third option. The work to upstream our code is ongoing but should be done in time for Factorio 2.0's release. Asynchronous saving Many of you might not be aware that Factorio has support for saving your game in the background, without freezing while it does so. This feature is tucked away in the hidden settings and only works on macOS and Linux. This is one great example of taking advantage of a platform's features to benefit the game, which would not be available to us if we simply went through Proton. Asynchronous saving works by using the fork syscall to essentially duplicate the game. The primary instance - the one you interact with - continues playing, but the newly forked child runs the saving process then exits on completion. I have used it for many years and have never had issues, but the setting remains hidden because there are a few unsolved problems with it and it requires a significant amount of RAM to work. I would love to promote this feature away from its hidden status in 2.0. If you are playing on Linux or macOS, please enable asynchronous saving (ctrl+alt+click Settings -> \"The rest\" -> non-blocking-saving) and report any issues you find. I am particularly interested in reproducing a seemingly random freeze that occurs at the end of the process. Thank you in advance! Continuing development This has been but a glimpse of the work I have been doing to ensure Factorio on Linux is the best that it can be. There are still many open bug reports and other issues but I am generally happy with the state of things and can confidently say that Factorio has great Linux support. As always, submit a text buffer with your feedback to the usual places. Discuss on our forums Discuss on Reddit Subscribe by email Terms of Service|Privacy|Imprint|Presskit|Contact|RSS|Jobs Copyright © 2015 - 2024 Wube Software - all rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=40174316",
    "commentBody": "Factorio – Statistics improvements, Linux adventures (factorio.com)124 points by ibobev 12 hours agohidepastfavorite47 comments doesnt_know 10 hours ago> Most desktop environments will allow windows to supply their own decorations if they wish but will provide a default implementation on the server side as an alternative. GNOME, in their infinite wisdom, have decided that all clients must provide their own decorations, and if a client does not, they will simply be missing I'm sure there was/is a good reason why this design decision was made, but it certainly seems a bit odd. Imagine if every game had to write decorations for every platform and desktop environment... reply Arnavion 9 hours agoparentTheir reason was that applications want to control everything about how they look, so they don't want compositor-provided decorations that clash with the rest of the application window. Whether that's a *good* reason... Well lots of electrons have been used up in that discussion already. https://gitlab.gnome.org/GNOME/mutter/-/issues/217 https://gitlab.gnome.org/GNOME/mutter/-/issues/1143 ... and probably many other places. reply naikrovek 8 hours agorootparentOk, so make it optional to use custom window chrome/decorations, rather than make it mandatory. GNOME devs are very arrogant. “Do it our way or go away.” They make it very hard to hold onto any hope that there will ever be a reasonable, cohesive, Linux desktop experience. reply mksybr 8 hours agorootparentKDE is reasonable and cohesive reply failbuffer 5 hours agorootparentI really like the hearty amount of configurability they make available thru the control panels. They're not afraid of giving you a lot of options, yet it's organized well, easy to use, and the defaults are sensible. reply ikiris 7 hours agorootparentprevI think them and fyne fight over who can make a worse gui library experience. reply replyifuagree 11 hours agoprevI covet that guys job big time. However, I'm actually too busy playing factorio to do his job so there's that. reply hoten 12 hours agoprevI'm a heavy user of and minor contributor to the Allegro library. We thought Factorio had got rid of Allegro long ago, but apparently some vestigial component remained. So long! reply philsnow 11 hours agoparentAllegro has been around for decades. I still remember using it with DJGPP in the mid-90s, which is at least four forevers ago. One thing I remember making was a fire/heat visualization, not optimized at all but that didn’t matter much because I was targeting 320x240x8bpp. reply raiguard 8 hours agoparentprevThanks for making such a great library! It served us well for many years. Edit: My brain skipped over the \"user of and minor\" part of the sentence. Still, thanks for contributing to Allegro! reply SCUSKU 11 hours agoprevWill never cease to be amazed by the performance that Factorio gets. reply KeplerBoy 11 hours agoprevUsing fork to save the game state in the background is really clever. love it. reply bno1 10 hours agoparentI wonder how they deal with multi-threading. Forking a multi-threaded process is very error-prone (e.g. allocator locks can remain orphaned). reply gizmo686 9 hours agorootparentThe good thing about this usecase is that they do not need to solve the general problem of forking a multithreades program. The child process is doing a very specific job that amounts to serializing the contents of memory and writing it to a file. Once that is done is can simply exit, and all the orphans are cleaned up. While this is admittadly more complicated, it is the same general idea behind fork+exec. Sure, your serialization logic cannot call malloc, but how often do you really need dynamic memory allocation. And if you really do need to, you can use a simple mmap backed bump allocator (or any other custom allocator you want) For datastructure locks protecting gamestate; just wait until the gamestate is in a consistent state before forking. Of course, if you try reusing existing code, or forget about the limitations while writing new code; it would be easy to introduce non-deterministic bugs. reply nh2 6 hours agorootparentIt still seems super error prone: It is very difficult to write general purpose C++ code that does not call malloc somehere. Something fails and you want to display a message `\"Error: \" + reason`? Bam, malloc right here, serialisation process may hang forever in a malloc lock. You quit the game, the parent process exits, and the serialisation process gets reparented to init, invisibly using up your RAM until you reboot. fork()+exec() works in C because C has no invisible memory allocation, and even there you'd usually try to not call any function in between the two to be very sure. Using fork() without being 100% sure there can be no malloc usually means inviting years of rare, hard-to-reproduce random weird bug reports. Beyond that, as the post mentions, fork() needs \"requires a significant amount of RAM to work\" if many pages are touched due to copy-on-write, and copy-on-write also slows down the main game. It seems much safer to use a thread for saving the game state. reply gizmo686 6 hours agorootparentIn principle, using a thread for saving the game state is a much more difficult problem, since the game state itself will mutate. You need to be extremely careful to assure that the state you serialize is consistent and is at least a plausible game state (even if there is a bit of wiggle room to allow for game states that technically never existed). This complexity invades every aspect of game logic; and bugs here tend to be subtle corruption of your save data that will take a while to notice; thereby obscuring the relationship to the corruption, and the save/reload cycle. In contrast, forking with its COW semantics is conceptually easy. You just fork. The main process can continue running, and the child process gets a frozen snapshot. There is a bunch of overhead from the copy part of copy-on-write. However, most of that overhead will likely be spent in the first frame; which is still a significant improvement over the pause time associated with stop the world saving. In practice, coding for the child process is tricky. However, it is self contained and responsible only for a relatively simple problem. No complex problems to solve, just a relatively small amount of code that needs to be written carefully. The RAM usage is a real trade-off inherent in the approach. > You quit the game, the parent process exits, and the serialisation process gets reparented to init, invisibly using up your RAM until you reboot. Or until the short-lived child process finishes its work and exits on its own. reply nh2 5 hours agorootparent> Or until the short-lived child process finishes its work and exits on its own. I was talking about the malloc-deadlocked child. That will not finish. reply nh2 5 hours agorootparentprevHow do you know that the game state is in a consistent state in the child? A stopped thread may be in the middle of mutating an array or struct. reply seba_dos1 26 minutes agorootparentBecause you forked while being in a consistent state? reply atq2119 7 hours agorootparentprevThey mention that there's a rare freeze bug. That does sound like it's most likely one of those orphaned lock problems or similar. reply raiguard 6 hours agorootparentThe post was already getting long so I didn't mention this, but I already found an fixed a potential deadlock due to each process not closing the redundant ends of the pipe, which was specifically touted as being a potential cause for freezing. However, someone reported the bug again after that change went out, so there's still SOMETHING else causing it to hang that I haven't tracked down yet. reply xboxnolifes 10 hours agorootparentprevIs that relevant if the forked process only has to save the game-state to disk and then die? I'd think reading the game-state wouldn't require some mutex lock. reply lucb1e 8 hours agoprevIs this about the upcoming expansion or the base game? I am not reading FFF to avoid spoilers and instead get the full wow effect the first time I play the 2.0 Edit: expansion I guess, since the last 3 releases don't mention statistics (https://forums.factorio.com/viewforum.php?f=3) reply positr0n 7 hours agoparentEvery FFF since they started doing them again has been about the expansion or base game updates that won't be released until the expansion, so I would continue to not read them :) The way they are doing it is a bunch of engine changes to the base game that will be released as 2.0 for free. Then the actual expansion is implemented as mods. So some of the FFF topics like this are about things that will come to the base game when the expansion launches. reply raiguard 8 hours agoparentprevEverything I talked about here will apply to Factorio 2.0 as a whole, whether or not you have the expansion DLC. reply reaperman 8 hours agoparentprevMany of the changes affect the base game as well. Accumulator charge graphs were very overdue. reply milliams 12 hours agoprevWhen they say they removing their Allegro dependency \"eliminated 123,024 lines of code from the game\" I assume this is because they'd vendored in the code? reply raiguard 8 hours agoparentYes, we vendor and statically link as much as possible to avoid dependency hell across distros and platforms. reply Arnavion 9 hours agoparentprevYes. I believe they also vendor SDL, because they've mentioned in the past making local changes to SDL for their needs. There's also a comment in the Reddit discussion by the author of the Linux about deleting a large number of lines of code when he removed their vendored FreeType, so it's likely the case for Allegro too. https://old.reddit.com/r/factorio/comments/1cdifrh/friday_fa... reply xboxnolifes 11 hours agoparentprevVendored or not, it's code that ends up as part of the game. reply db48x 11 hours agorootparentBut they probably wouldn’t have bothered to count the lines of code in the library if they hadn’t checked a copy of it into their source code. reply nubinetwork 11 hours agoprev> The game has decorations now, but the theme doesn't match. Thanks GNOME! Yeah good luck with that one :) reply IAmLiterallyAB 11 hours agoprevMandatory CSD was always a bad idea imo and this is yet another reason reply lupusreal 12 hours agoprev [–] If they drop X11 support that will finally be the end of my cracktorio addiction. reply raiguard 8 hours agoparentApparently I didn't word that section very well, because many people took away this impression. My apologies. Factorio will continue to support X11 for as long as SDL does, in other words, essentially forever. The only change here is that X11 is no longer required for the game to launch at all. SDL will load whichever video driver is available at runtime, or if you have both, you can choose which one to use in the graphics settings. reply jprete 10 hours agoparentprevI don't see anything to indicate that they're dropping X11 support? It sounds like they're dropping dependencies that would require X11 to be installed. reply db48x 10 hours agorootparentNo, it doesn’t sound like they’ll be dropping X11 support any time soon. At least, not before the 2.0 release. It is nevertheless one of those worrisome things. reply bombcar 9 hours agorootparentThey’re dropping a hard dependency on X11 and instead using a library that interfaces with X11 or Wayland for it. It still runs fine on X11. reply Arnavion 9 hours agorootparentprevThere is a comment in the forum discussion by the author of the Linux section that X11 support is not going away, simply because SDL supports it. https://forums.factorio.com/viewtopic.php?p=609071#p609071 reply earthling8118 11 hours agoparentprevIt's time to make WaylandX, for running Wayland applications on X. Well, I happily use Wayland so I won't use it, but maybe you would. reply glandium 3 hours agorootparentLooks like your wish has been granted. https://news.ycombinator.com/item?id=40175731 reply diath 8 hours agorootparentprevThat's already a thing, called XWayland, and it's the only way to make Wayland actually usable on Linux desktop. reply glandium 6 hours agorootparentXWayland is the exact opposite of what GP is asking for. reply endigma 9 hours agoparentprevI don't think they are planning to do this; they are at least maintaining support for whatever MacOS (Metal?) and the Nintendo Switch use in addition to Wayland, which suggests to me they are dropping X11 as a hard dependency not as a usable backend. reply Kerbonut 10 hours agoparentprevJust run the last version that supports X11? reply talldayo 10 hours agoparentprevYou could use Proton to play the Windows version in x11 just fine, I'm pretty sure. reply db48x 11 hours agoparentprev [–] Same here. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Wube team's Friday Facts #408 highlights enhancing game statistics in Factorio, like accumulator charge and science pack usage.",
      "Raiguard is now part of the team, emphasizing Linux support, addressing challenges like diverse compilers and libraries.",
      "Factorio 2.0's migration from Allegro to SDL lowers dependencies and code size, with efforts ongoing to rectify window resizing problems in Sway and enhance Linux performance."
    ],
    "commentSummary": [
      "Factorio developers are enhancing the game for version 2.0, removing the X11 dependency and incorporating a library compatible with X11 or Wayland, ensuring X11 support for Windows users via Proton.",
      "Users are voicing dissatisfaction with GNOME's window decorations and praising KDE's customization options, sparking discussions on Factorio's state-saving complexities, including multi-threading challenges and memory allocation problems.",
      "The post addresses statistical advancements in Factorio gameplay and highlights concerns over GNOME's usability, garnering attention for Factorio's upcoming release changes and desktop environment comparisons."
    ],
    "points": 124,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1714165754
  },
  {
    "id": 40167933,
    "title": "Sacred Modernity: Exploring Europe's Brutalist Churches",
    "originLink": "https://www.dezeen.com/2024/04/24/sacred-modernity-brutalist-churches-book-jamie-mcgregor-smith/",
    "originBody": "Sacred Modernity showcases \"unique beauty and architectural innovation\" of brutalist churches Share: Amy Peacock24 April 2024 Leave a comment Photographer Jamie McGregor Smith has spent the last five years capturing brutalist and modernist churches across Europe. Here, he picks his 12 favourites from his Sacred Modernity book. With 139 photographs of 100 churches, McGregor Smith created the book to showcase the sculptural and unique forms of some of the churches built in the post-war period in countries including Italy, Germany, Austria, Poland and the UK. Published by Hatje Cantz with essays by writers Jonathan Meades and Ivica Brnic, Sacred Modernity: The Holy Embrace of Modernist Architecture aims to bring attention to the unconventional buildings. Chiesa di Santa Maria Immacolata in Italy (above) and L'église Saint-Nicolas in Switzerland (top) are some of the brutalist churches in Sacred Modernity \"Many are surprised to discover the thought-provoking nature of brutalist architecture and are drawn to its challenging and unconventional qualities,\" McGregor Smith told Dezeen. \"In essence, the experience of encountering brutalist churches often involves a transformation from scepticism to appreciation, as individuals are confronted with the unique beauty and architectural innovation that these structures represent.\" McGregor Smith recalled that his work on the book began when he visited the brutalist Wotruba Church in Vienna, which sparked his interest in modernist church architecture. Since then, he has been driven to discover more churches like it. The Wotruba Church in Austria sparked Jamie McGregor Smith's interest in modernist church architecture \"One of the driving forces behind my project was the realisation that many of these remarkable spaces were not fully appreciated within the architectural community and often remained unknown to their local populations,\" said McGregor Smith. \"I felt a sense of excitement and purpose in rediscovering these hidden gems that so freely express creativity,\" he continued. \"These architectural marvels evoke within me a profound sense of awe and curiosity, thanks to the architects' masterful use of form and light.\" Read: Brutalist Italy book showcases \"sometimes surprising\" concrete architecture The churches in Sacred Modernity have sculptural concrete forms that break away from the mould of conventional churches, which typically have a floor plan in the shape of a cross. McGregor Smith claimed this was part of a trend after the second world war, which sought new styles separated from traditional architecture of the past. \"While traditional churches evoke a sense of familiarity and reverence through their classic designs, brutalist and modernist churches challenge these norms with their bold, austere and provocative aesthetic,\" he said. \"These architectural styles emerged in the post-war period as a rejection of the past's orthodoxy and a pursuit of a new social order free from associations with opulence, authority and war.\" Many of the churches have unconventional shapes, like St Matthew's Church in the UK \"In response to the reformed liturgy following the Second Vatican Council, church commissioners were granted unprecedented freedom to depart from traditional church typology,\" McGregor Smith continued. \"They eschewed nostalgic replication, resulting in spaces that excluded functional areas and symbols prevalent since medieval times, retaining only the essential elements of the altar, cross, and font.\" He explained that while early modernist churches adopted familiar rectangular or cross shapes, they quickly evolved to incorporate different geometries such as squares, circles and octagons. Many relocated the positions of altars, which would usually be situated at the top of the cross-shaped church, to the centre of the building. McGregor Smith also described that modernist and brutalist churches exhibited a move away from traditional forms of religious symbolism in decorative elements, and instead used the material and shape of the building to recall the same feelings. Tempio Mariano di Monte Grisa in Italy features creative geometries made from concrete \"Traditional mediums of painting, craft, and sculpture, which once adorned medieval and baroque churches to elevate divine power and beauty, gave way to weightless abstract forms made possible by steel and reinforced concrete,\" he said. \"This departure from traditional symbolism shifted creative expression away from supernatural narratives of heaven and hell, embracing instead earthly qualities of materiality,\" McGregor Smith added. \"Sanctuaries assumed primordial cavern-like forms, reflecting the inherent qualities of earth and stone, while industrial and military architectural influences transformed church interiors into bunker-like shelters.\" Below, McGregor Smith highlights 12 of his favourite churches from Sacred Modernity: St Reinold Kirche in Düsseldorf, Germany, by Josef Lehmbrock (1957) \"This small unassuming church has an austere yet majestic beauty that defines how simple design and materials can create subtle elegance. \"Although narrow, the shuttered windows create depth and volume providing a calming soft light that hints at the vastness of space beyond its walls.\" Santuario della Beata Vergine della Consolazione in San Marino, Italy, by Giovanni Michelucci (1967) \"Giovanni Michelucci has sculptured a splendidly creative and joyful interior that celebrates simultaneously the organic and supernatural. \"The form nods to the sanctuary cave and the symbolism of new life represented by the egg. The unreachable windows and walkways invite visitors' eyes to explore and contemplate mysteries.\" St Mauritius Kirche in Munich, Germany, by Herbert Groethuysen (1967) \"Germany's post-war churches are often brutal and austere. Their designs reflect a rejection of the opulence and pride of the pre-war period and serve as a place of sanctuary and reflection. \"Stripped of their symbology and place in time, they are spaces to forget the horrors of history, war and shame and focus on the hope and light of the future.\" St Paulus Kirche in Neuss-Weckhoven, Germany, by Fritz Schaller, Christian Schaller and Stefan Polónyil (1968) \"Here the concept of incarnation is integrated through the abstraction of organic forms and geometry as a sacred message. The omnipresent roof structure serves as a conduit, forging a connection between the celestial and physical realms. \"Working with his father, this was Christian Schaller's first commission after qualifying, and I had the pleasure of asking him personally for permission to include this image as the book cover.\" Osterkirche in Oberwart, Austria, by Gunther Domenig and Eilfried Huth (1969) \"Perhaps the most radical church in Austria, the building shows all the signs of a culture excited and influenced by the science fiction of the day. \"It is at once ancient and futuristic, a medieval cavern furnished for the space age. This was an unexpected discovery early in the project – a beautifully serene sanctuary and a personal favourite.\" St Matthäus Kirche in Düsseldorf, Germany, by Gottfried Böhm (1970) \"A lesser-known work of Gottfried Böhm's, this church's cavernous interior cascades above you, interspersed with interjections of sculpturing light. \"Symbols of industrial architecture, such as the rounded ovens, represent the Ruhr region's manufacturing heritage and of the furnaces that reside at the base of humanity.\" Chiesa di San Nicolao della Flue in Milan, Italy, by Ignazio Gardella (1970) \"Like the skeleton of a whale or Christ's embodiment, the ribbed ceiling appears to hold the weight of the world above you. \"While the artwork and organ appear teleported from a previous century, the basilica's traditional frame is given a futurist interpretation using novel engineering.\" Chiesa di Santa Maria della Visitazione in Rome, Italy, by Saverio Busiri Vici (1971) \"Perched high up amongst the alpine hills, the winding journey through Austria and Italy made the visit to this church evermore special. \"It was built as a memorial church to commemorate the loss of over 2,000 local people and their parish church, who were killed and swept away when a landslide breached the dam in the mountains above the town. The shape of the church emulates the curve of the damn wall, the flow of water and perhaps a stairway to heaven.\" L'église Saint-Nicolas in Heremence, Switzerland, by Walter Maria Förderer (1971) \"Maria Förderer's chaotic and abstract expressionism illustrates an ethereal reality beyond our earthly experiences. \"The theological idea of the apathetic is considered here, where divinity cannot truly be considered with the known language or ideas of human existence.\" Cathedral Church of Saints Peter and Paul in Bristol, UK, by Ronald Weeks, Frederick Jennett and Antoni Poremba (1973) \"Perhaps the UK's finest example of modernist sacred architecture, Ronald Weeks has created an extraordinarily rich atmosphere in the Cathedral Church of Saints Peter and Paul, choosing form over liturgical function. \"Architecture critic Jonathan Meades described the church thus: 'The last of the [UK] mega-churches, Clifton Cathedral seems embarrassed by its function, but the building would rather be the national theatre. Its brute, board-marked concrete and jagged discords come straight off the Southbank.'\" Heilig-Kreuz-Kirche in Vienna, Austria, by Hannes Lintl (1975) \"Hannes Lintl adopts an overbearing mix of structural form and light design to reflect the power and omnipresence of the divine. \"This concrete 'bunker' offers visitors sanctuary and security, guiding visitors along a clear path towards spiritual enlightenment.\" Kościół świętego Dominika in Warsaw, Poland by Władysław Pieńkowski (1994) \"Too recent to be described as post-war, this Polish church can rather be considered post-soviet. Church construction during the Russian occupation of Warsaw was almost entirely banned, influencing the flourishing of sacred architecture that followed independence. \"This church reminds one of the vaulted medieval churches of France and Britain, yet its geometry and tunnelling light give it a timeless energy.\" The photography is by Jamie McGregor Smith. Read more: Photography Books Churches Worship Architecture books Brutalism Roundups Modernism Architecture Cultural Highlights Subscribe to our newsletters Your email address Dezeen Debate Our most popular newsletter, formerly known as Dezeen Weekly. Sent every Thursday and featuring a selection of the best reader comments and most talked-about stories. Plus occasional updates on Dezeen’s services and breaking news. Dezeen Agenda Sent every Tuesday and containing a selection of the most important news highlights. Plus occasional updates on Dezeen’s services and breaking news. Dezeen Daily A daily newsletter containing the latest stories from Dezeen. New! Dezeen In Depth Sent on the last Friday of every month, Dezeen in Depth features original feature articles, interviews and opinion pieces that delve deeper into the major stories shaping architecture and design. Dezeen Jobs Daily updates on the latest design and architecture vacancies advertised on Dezeen Jobs. Plus occasional news. Dezeen Jobs Weekly Weekly updates on the latest design and architecture vacancies advertised on Dezeen Jobs. Plus occasional news. Dezeen Awards News about our Dezeen Awards programme, including entry deadlines and announcements. Plus occasional updates. Dezeen Events Guide News from Dezeen Events Guide, a listings guide covering the leading design-related events taking place around the world. Plus occasional updates. Dezeen Awards China News about our Dezeen Awards China programme, including entry deadlines and announcements. Plus occasional updates. We will only use your email address to send you the newsletters you have requested. We will never give your details to anyone else without your consent. You can unsubscribe at any time by clicking on the unsubscribe link at the bottom of every email, or by emailing us at privacy@dezeen.com. For more details, please see our privacy notice. Thank you! You will shortly receive a welcome email so please check your inbox. You can unsubscribe at any time by clicking the link at the bottom of every newsletter. Share and comment Share: Leave a comment More Architecture Interiors Design Technology Recommended stories Foster + Partners designing Sierra Leone island city for Idris Elba \"Bowing out gracefully is a rare thing in the starchitect firmament\" \"Strong focus on aesthetics\" contributed to collapse of Norway timber bridge World's largest museum captured ahead of opening in Egypt Video reveals construction progressing on The Line in Neom Sagrada Familia announces 2026 final completion date Saudi Arabia lowers predicted number of residents at The Line by 2030 Timber Aquatics Centre completes in Paris for 2024 Olympic Games Latest from Dezeen Showroom Sonia and Sonya rug collection by Sonya Winner Rug Studio Origata collection by Nao Tamura for Porro Hut lounge bed by Marco Lavit for Ethimo Flamingo Mini lighting by Antoni Arola for VibiaDezeen Showroom Poetica furniture system by Scavolini BetteAnti-Slip Sense treatment by Bette Oblò pendant light by Paola Navone for Lodes Future tiles by Kaolin Comments Visit our comments pageRead our comments policy",
    "commentLink": "https://news.ycombinator.com/item?id=40167933",
    "commentBody": "Brutalist Churches (dezeen.com)122 points by surprisetalk 22 hours agohidepastfavorite40 comments nabla9 12 hours agoIt seems like most new churches in Finland are brutalist concrete slabs at least from outside. They are called \"defense bunkers against devil\" Some pictures: https://twitter.com/sorjonen_fi/status/916606080154767361 https://twitter.com/ArtoNatkynmaki/status/168386825764259020... https://fi.wiktionary.org/wiki/piruntorjuntabunkkeri#/media/... reply polemic 8 hours agoparentNot just the new ones - it's a very scandinavian lutheran trend. Tapiloa Church for e.g. https://www.google.com/maps/@60.1783778,24.8091179,3a,75y,15... However there's also the church in the rock which is absolutely stunning and worth a visit. https://www.temppeliaukionkirkko.fi/en/index/nimi.html# reply analog31 6 hours agoprevHere's one in Madison WI, that was taken down and rebuilt in a more traditional style a few years ago. https://isthmus.com/arts/goodbye-st-pauls/ reply mayormcmatt 12 hours agoprevOur family used to attend Newman Hall Church in Berkeley, California back in the '90s -- a very brutalist building of a church. Although religion never took for me, I had fond memories of the after-church donut feasts in the community space. I don't know if the intention of the architecture was to get me to focus on the mass, but young me just spent the entire time taking in the strange geometries of the place. https://maps.app.goo.gl/3TtT716k3bUkdAVh6 reply jonah 10 hours agoparentI was just going to mention that one! https://duckduckgo.com/?q=newman+hall+holy+spirit+parish+ber... I've been there once and personally, quite like that aesthetic. But the thing that sticks with me was the priest proclaimed the Gospel passage from the bible word-for-word from memory rather than reading it. reply phito 1 hour agoprevBrutalism always makes me feel anxious and unsafe for some reason. reply egypturnash 9 hours agoprevThese all look like churches that I would encounter with blood everywhere in the middle of a first-person shooter. They also look like total acoustical nightmares. All that echo-ey concrete, yikes. reply bombcar 9 hours agoparentSometimes the concrete is exceptionally well designed and you can fill the church with a single singer. Others are so badly designed you need literally millions in audio equipment to have a simple choir. reply deadbabe 8 hours agoparentprevQuake?? reply egypturnash 6 hours agorootparentMostly I was reminded of the upside-down church zone of System Shock 2. That's one of the few FPSs I bothered playing. reply temp0826 10 hours agoprevShouldn't churches evoke feelings of divinity instead of oppression? (I know it's popular to hate on brutalist architecture, but really isn't a place of worship the last place you'd want it?) reply dragonwriter 7 hours agoparentMany of the brutalist churches I’ve seen do not, IMO, “evoke feelings of oppression” (aside from any one might have as a result of personal conflict with the religious institution behind them, which is a separate issue.) Including, I should mention, most of those in TFA. reply wrp 4 hours agorootparentI suppose we just have to chalk it up to different tastes in art. To me, my first thought on seeing most of those pictures is how they look like settings for some implausibly dystopian sci-fi movie. reply Gare 10 hours agoparentprevWhy are you finding brutalist architecture oppressive? reply snowpid 1 hour agorootparentIt's not natural, grey is a stupid colour, no ornaments. Only architects like brutalism. But I am happy to know that brutalist churches in Germany will be destruct at first than older churches. reply SideburnsOfDoom 1 hour agorootparent> It's not natural A point of brutalism is \"form follows function and \"showcase the bare building materials\" (1) - in other words, that's how the material looks, naturally. Bare, unpainted concrete is in fact more natural and honest than painted concrete. And brutalism deliberately leans into that naturalism (of modern materials). 1) https://en.wikipedia.org/wiki/Brutalist_architecture reply snowpid 10 minutes agorootparentThe impression of Unnatural comes from the fact, that brutalist forms don't exist in Nature. Concrete is everything but not a natural material. Showing it isnt natural. I am still buffed they are people defending brutalism. It is one of the biggest mistake in Architecture and one day these buildings will vanish. reply vundercind 10 hours agorootparentprevThey tend to look like something someone who didn’t give a shit made. “I just need a box for people to be in, who cares what it looks like?” The example brutalist churches in another post here look like someone doing a horrible experiment in the Sims made buildings without bothering to use anything but some default wall texture they found buried in the dev tools that doesn’t even tile very well, because they just needed a place to torture Sims in and don’t care what it looks like. Yeah, that comes off as oppressive. reply bombcar 9 hours agorootparentThe oldest churches direct you to the heavens and God. Early modern churches direct you to the preacher. Modern churches direct you to the architect. (Or building repair services depending on whom you ask.) reply djur 5 hours agorootparentprevI don't see it that way at all. A lot of care and thought went into these designs, and they evoke the kind of awe you would expect from a god's house. These aren't just hulking concrete bunkers like some of the institutional brutalism you see. reply jltsiren 7 hours agoparentprevDepends on who you ask. It's a somewhat common belief that churches should be plain and unadorned and that religious art can lead to idolatry. Ideas like that were particularly popular among early Protestants. reply keeganpoppen 2 hours agoparentprevour perhaps evoking the feeling of being a part of something bigger than yourself? which brutalist architecture absolutely does. reply deadbabe 8 hours agoparentprevThey could soften it up by adding some plants. Eco-brutalist! reply thetruckgoes 10 hours agoparentprevHaven't the churches been [historically] oppressive? reply temp0826 10 hours agorootparentI get that it's popular to hate on religion too ;), so I suppose that's ironic to some. reply randohostage 10 hours agorootparentis it though like really? >6 billion people in the world actively identify with one religion or another but in terms of ratio there are more memes on social media about athesim than religions at least on my AI curated feed reply thrance 9 hours agorootparentThere is plenty of hate speech towards non-believers if you look a little bit for it. In fact, I'd even say that as unfunny and overseen atheists memes are, they are the most tame of all religious critic online. reply lIl-IIIl 2 hours agoprevReminds me of this Buddhist monastery: https://architectureprize.com/winners/winner.php?id=6300 reply thom 9 hours agoprevNot as attractive as those in the article, but still quite striking near me: https://modernmooch.com/2018/08/19/trinity-united-reform-chu... Somewhat terrifying when they light up the front in red at night. reply nikolay 9 hours agoprevThese remind me of styling in Raise by Wolves [0] of the Mithraic [1]. By the way, it's a great sci-fi show and I was very saddened when I heard HBO Max canceled it (during its transition to Max). I unsubscribed from them and I'm boycotting them due to this hostility. [0]: https://www.imdb.com/title/tt9170108/ [1]: https://raised-by-wolves.fandom.com/wiki/Mithraic reply jonah 10 hours agoprevIn Los Angeles, CA, the Cathedral of Our Lady of the Angels is pretty brutalist: https://duckduckgo.com/?q=Cathedral+of+Our+Lady+of+the+Angel... https://www.olacathedral.org/overview A favorite part of mine are the tapestries: https://www.olacathedral.org/tapestries reply jszymborski 7 hours agoprevMontreal has some great brutalist buildings, and there are few brutalist churches in the east-end of the island. I grew up with many buildings of this style and I must say I've grown fond of them. I know they are seen as cold, but they bring me warmth. reply fredguth 7 hours agoprevI would add Catedral Metropolitana do Rio de Janeiro. reply sebtron 4 hours agoprevThe first church in the article (second after the banner picture), \"Santa Maria Immacolata\"[0] has an important historical meaning. It is located in the village of Longarone, which was completely wiped out in 1963 due to the disaster of the Vajont dam[1]. Tl;dr the dam should not have been built there because of the geology of the area, but they did anyway; an enormous piece of the mountain fell into the lake, causing a huge wave. Nearly 2000 people died. The church was built as part of the rebuilding of the whole village. [0] (Italian) https://it.wikipedia.org/wiki/Chiesa_di_Santa_Maria_Immacola... [1] https://en.wikipedia.org/wiki/Vajont_Dam reply Andrex 7 hours agoprevI want to read this article but the website has other ideas. Horrible UX and horrible ads (redundant, I know). reply metadat 5 hours agoparentFor what it's worth Andrex, gusto sucks ass. Their customer service sucks and is a sham. reply gnabgib 12 hours agoprevSome discussion 2 months ago [0](34 points, 15 comments) {Different source, same photos} [0]: https://news.ycombinator.com/item?id=39670217 reply SideburnsOfDoom 1 hour agoprevIf you're ever in Reykjavík, check out the Hallgrímskirkja. Completed in 1986, located visibly, on a hill. From outside it's very impressive, with the soaring spire shaped (IMHO) like a supersonic aircraft ascending vertically. A stone angel. https://en.wikipedia.org/wiki/Hallgr%C3%ADmskirkja https://www.re.is/is/blog/guide-to-hallgrimskirkja-church-re... https://yourfriendinreykjavik.com/hallgrimskirkja-a-tribute-... reply philwelch 9 hours agoprevHorrifying and monstrous. reply WalterBright 8 hours agoprev [–] I'd hate to be in one in an earthquake. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Photographer Jamie McGregor Smith's book \"Sacred Modernity\" showcases 100 churches across Europe, focusing on brutalist and modernist architectural styles in countries like Italy, Germany, and the UK.",
      "The book aims to highlight the innovative designs of these unconventional churches, challenging traditional church aesthetics and incorporating sculptural concrete forms and unconventional shapes.",
      "McGregor Smith emphasizes the post-war rejection of traditional church design in favor of new architectural styles that evoke creativity, form, and light to create a sense of wonder and curiosity."
    ],
    "commentSummary": [
      "The conversation focuses on the rise of brutalist architecture in churches, notably in Finland and Scandinavian Lutheran churches.",
      "Opinions vary on brutalist churches, with some feeling anxious or uneasy, while others admire the modern aesthetic.",
      "Divided views exist on whether brutalist churches convey divinity or oppression, with comparisons to sci-fi settings, and examples from different locations are discussed, including a historical church in Longarone, Italy."
    ],
    "points": 122,
    "commentCount": 40,
    "retryCount": 0,
    "time": 1714129886
  }
]
