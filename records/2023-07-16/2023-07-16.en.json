[
  {
    "id": 36739920,
    "timestamp": 1689447796,
    "title": "Every time you click this link, it will send you to a random Web 1.0 website",
    "url": "https://wiby.me/surprise/",
    "hn_url": "http://news.ycombinator.com/item?id=36739920",
    "content": "Feeling BloatedPURGE!Click Here! Got Some BucksBlow Your WadClick Here! Down In The DumpsGet Toasted!Click Here! Bread AddictNoodle CureClick Here!Official Home Page of the Ban Bread Now FoundationDisclaimerLearn the facts. Join our cause. Make a huge contribution.\"Smell of baked bread may be health hazard.\" --from an article in the Cincinnati Enquirer. The main danger, apparently, is that the organic components of this aroma may break down ozone. When are we going to do something about bread-induced global warming? Sure, we attack tobacco companies, but when is the government going to go after Big Bread?More facts. Join our cause. Make a huge contribution.-- More than 98 percent of convicted felons are bread users.-- Fully HALF of all children who grow up in bread-consuming households score below average on standardized tests.-- More than 90 percent of violent crimes are committed within 24 hours of eating bread.-- Bread is made from a substance called \"dough.\" It has been proven that as little as one pound of dough can be used to suffocate a mouse. The average American eats more bread than that in one month!-- Bread has been proven to be addictive. Subjects deprived of bread and given only water to eat begged for bread after as little as two days.-- Bread is often a \"gateway\" food item, leading the user to \"harder\" items such as butter, jelly, peanut butter, and even cold cuts.-- Newborn babies can choke on bread.-- Bread is baked at temperatures as high as 400 degrees Fahrenheit! That kind of heat can kill an adult in less than one minute.-- Most American bread eaters are utterly unable to distinguish between significant scientific fact and meaningless statistical babbling and many have even been known to vote Democratic.-- Bread has been the catalyst for political upheaval and dietary disaster worldwide, the militant \"Whole Grain Cult\", for instance.-- After Jesus broke bread with his disciples the resulting betrayal changed world history forever. (Incorrect fundamentalist thinking blames it on the wine.)-- Money is sometimes called \"bread\". Money is the root of all evil. Coincidence?Frightening statistics! We propose the following:-- No sale of bread to minors.-- A nationwide \"Just Say No To Toast\" campaign, complete with celebrity TV spots and bumper stickers.-- A 300 percent federal tax on all bread to pay for all the societal ills we might associate with bread.-- No animal or human images, nor any primary colors (which may appeal to children) may be used to promote bread usage.-- The establishment of \"Bread-free\" zones around schools.-- Eliminate subsidies for farmers engaged in the growing of wheat.-- Health hazard warning labels to be placed on all packages of bread.How can you help?-- Make a huge contribution.-- Put us in your will.-- Link to this site to spread the word.-- Tell your friends, strangers and coworkers.-- Contact your Congressman and demand action now.-- Organize \"bread lines\" in your neighborhood.-- Use the email link to send your credit card and pin numbers.-- Join this organization for a mere one-time fee of $1000. **-- Don't forget that huge contribution. PayPal button for your convenience.  Ban Bread Now FoundationHoffer, PAEmailThanks to haleyscomet for the inspiration.Site created 10 Jun 02, fixed for Netscape 24 Jun 02Check out the lunatic responsible for this!copyright \u00a9 rejection 2002 all rights preservedlast site update 8 March 2005** Charter members receive a Ban Bread membership card and a really cool Dough Boy Decoder.   Quantities of the decoder are limited so act now!Related LinksThe Toaster OracleYeast TerrorAnti-onion LeagueFood JokesAntitomato.comMoron's Guide to ToastWar on Junk FoodBread Addicts!Withdrawal HelpRamen RecipesHabit Smartmore wingee lunacywingee's funny stuffSave the Polyesterswingee widgets pcDrunks Against Mad MothersPennsylvania HistorySave the Plywood ForestsHonest Wingee's Used CarsCell Phone Biz Opportunity!FAQ About Your HMOCheap PennsylvaniaPossum Hollow, PAeBayt Online Auctionwingee award for tasteless websitesDork of the Day 06/21/02",
    "summary": "- The Ban Bread Now Foundation's website promotes a campaign against bread consumption, claiming it has negative health and societal effects.\n- The foundation proposes various measures to combat bread usage, such as a nationwide \"Just Say No To Toast\" campaign and health hazard warning labels on bread packages.\n- Readers can support the cause by making contributions, spreading the word, contacting their congressmen, and joining the organization for a fee, with charter members receiving a Ban Bread membership card and a Dough Boy Decoder.",
    "hn_title": "Every time you click this link, it will send you to a random Web 1.0 website",
    "original_title": "Every time you click this link, it will send you to a random Web 1.0 website",
    "score": 968,
    "hn_content": "- Web 1.0 websites had a different set of UI idioms and did things in a simple way.\n- Web 1.0 sites accomplished amazing results through simple means.\n- Web 1.0 sites are a source of inspiration for modern design.\n- Modern web design may not be preferred by the masses and can be broken or unusable.\n- User tests often favor simpler options, but they can become impractical for actual work.\n- The use of UIs in movies often features flashy, noisy interfaces that are not representative of real interfaces.\n- The Hacker News community showed interest in the post because it provides a nostalgic look at Web 1.0 sites that were known for their simplicity and usability.\n- Wiby.me is a curated search engine that takes users to random Web 1.0 websites.\n- Users are finding interesting and unique websites on Wiby.me, including personal projects, cave directories, and chemistry lab equipment collections.\n- Web 1.0 sites are often more readable and easier to navigate compared to modern sites.\n- Web 1.0 sites have a charm and personality that is often missing from modern websites.\n- Wiby.me is generating excitement among tech-savvy readers due to its ability to showcase the simplicity and creativity of Web 1.0 sites.",
    "hn_summary": "- Web 1.0 websites were known for their simplicity and usability, which can serve as inspiration for modern design.\n- Wiby.me, a curated search engine, takes users to random Web 1.0 websites, generating excitement among tech-savvy readers.\n- Users are finding interesting and unique websites on Wiby.me, showcasing the charm and creativity of Web 1.0 sites."
  },
  {
    "id": 36737733,
    "timestamp": 1689435244,
    "title": "Signal president says company will not comply with U.K. 'mass surveillance' law",
    "url": "https://fortune.com/2023/07/13/signal-president-mass-surveillance-uk-law/",
    "hn_url": "http://news.ycombinator.com/item?id=36737733",
    "content": "SEARCHSubscribe NowSIGN INHomeNewsTechFinanceLeadershipWellRecommendsFortune 500TECH \u00b7BRAINSTORM TECHSignal president says company will not comply with proposed U.K. \u2018mass surveillance\u2019 law requiring mandatory message scanning before encryptionBYKYLIE ROBISONJuly 13, 2023 at 8:01 PM UTCSignal president Meredith Whittaker, rejects U.K. law mandating pre-encryption message scanning.STEVEN VARGO/FORTUNEMeredith Whittaker, president of encrypted chat service Signal, doubled down on her criticism of proposed British online safety legislation, calling the government\u2019s plan to require a special back door to access encrypted messages \u201cmathematically impossible\u201d and vowing to exit the U.K. market if it becomes law.\u201cThey would order us to implement it. We would not,\u201d Whittaker said at the Fortune Brainstorm Tech conference in Deer Valley, Utah, this week.In an onstage conversation with Roy Bahat, head of Bloomberg Beta, Whittaker said one of her primary missions is to prevent the dangerous trend of socially accepted surveillance.\u201cI think what has happened over the, you know, handful of decades in which the surveillance business model has interpolated our core infrastructure\u2014to the point that we\u2019re surveilled in to an extent we don\u2019t have a sense of\u2014is that that choice has been made for us,\u201d Whittaker said.Signal\u2019s unwavering mission, echoed by Whittaker herself, is to prioritize user privacy and security above all else. The nonprofit\u2019s end-to-end encryption protocol ensures that messages remain strictly confidential, accessible only to trusted recipients. Journalists, government officials, and tech entrepreneurs like Elon Musk and Marc Andreessen rely on Signal for secure communication. However, Signal, like its counterparts Telegram and WhatsApp, are not immune to bad actors who desire privacy for illicit activities, like drug dealers, weapons traffickers, or hackers distributing malware.Thus, the U.K. believes a back door is necessary to prevent such illicit activities from occurring. The country\u2019s online safety bill, first drafted in May 2021, aims to give the government the authority to demand backdoor access to all end-to-end encryption systems. While the government claims it enhances online safety by ensuring social media platforms remove illegal content like revenge porn and hate speech, the bill is fiercely criticized by tech giants, security experts, and privacy advocates\u2014potentially leading to an encryption exodus in the U.K., TechCrunch wrote.\u201cYou cannot create a back door that only the good guys can go through,\u201d Whittaker said, adding that it\u2019s \u201cmathematically impossible\u201d and would \u201cundermine the security of our core communications, core infrastructures.\u201dWhittaker, who joined Signal in 2022, has been an advisor on A.I. for the U.S. Federal Trade Commission. She also was one of the leaders of the 2018 walkouts at Google, in protest of the company\u2019s privacy practices and its response to internal cases of sexual misconduct. She resigned from Google in 2019.In her discussion with Bahat at Brainstorm Tech on Tuesday, Whittaker cited a multitude of examples of the perilous situation that is created when allowing a government access to user\u2019s private messages. One instance that Whittaker expounds on is a mother in Nebraska, who faces up to two years in prison along with her 18-year-old daughter, for giving abortion pills to her daughter. Detectives served Facebook (whose parent company also owns WhatsApp) with a search warrant for access to their DMs, which aided in the prosecution, Vice reported.\u201cWhat is right is we don\u2019t let one massive corporation with very little oversight determine the surveillance society we live in, [in] collaboration with governments,\u201d Whittaker said.Despite Signal\u2019s modest user base of 125 million downloads, significantly dwarfed by WhatsApp\u2019s 2 billion users, both companies have made a resolute commitment: They will exit the U.K. if the Online Safety Bill is enacted. Whittaker said onstage that \u201cthe anti-encryption, sort of regulatory agenda,\u201d which \u201csnuck in around 2018\u201d has alarmed her.Subscribe to Well Adjusted, our newsletter full of simple strategies to work smarter and live better, from the Fortune Well team. Sign up today.Most PopularHEALTHBryan Johnson sold his company to PayPal for $800 million. Now, he\u2019s spending $2 million a year to stay young...July 14, 2023BYFORTUNE EDITORSPERSONAL FINANCEAmericans are \u2018hunkering down\u2019 and buying less food as inflation bites, one grocery giant says\u2014instead they\u2019re raiding...July 14, 2023BYELEANOR PRINGLEHEALTHTech CEO who takes 61 pills a day and eats 70 pounds of veggies a month to stay young forever says he\u2019s never been...July 12, 2023BYSTEPHEN PASTISRelated ArticlesCONFERENCESFor business executives, A.I. is a force that can bring customers closer, or take them awayJuly 14, 2023BY ALEXANDRA STERNLICHTCONFERENCESAs A.I. spreads through the office, companies are rethinking how they deal with dataJuly 14, 2023BY STEPHEN PASTISNEWSLETTERS4 takeaways from Fortune Brainstorm Tech conferenceJuly 13, 2023BY JESSICA MATHEWSNEWSLETTERSTech deals may take even longer to close in light of recent FTC scrutiny, predicts one VCJuly 14, 2023BY ANNE SRADERSTECHYuga Labs\u2019 new CEO is betting on \u2018audacious\u2019 metaverse plans as Bored Ape prices collapse\u2014but details of this ...July 13, 2023BY JESSICA MATHEWSRankings40 Under 40100 Best CompaniesFortune 500Global 500Most Powerful WomenWorld\u2019s Greatest LeadersWorld\u2019s Most Admired CompaniesSee All RankingsSectionsFinanceLeadershipSuccessTechAsiaEuropeEnvironmentFortune CryptoHealthWellRetailLifestylePoliticsNewslettersMagazineFeaturesCommentaryMPWConferencesPersonal FinanceRecommendsCustomer SupportFrequently Asked QuestionsCustomer Service PortalPrivacy PolicyTerms of UseSingle Issues for PurchaseInternational PrintCommercial ServicesFortune Brand StudioFortune AnalyticsFortune ConferencesAdvertisingBusiness DevelopmentAbout UsAbout UsEditorial CalendarWork at FortuneBehavioral Advertising NoticeTerms and ConditionsSite Map\u00a9 2023 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information | Ad Choices FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.S&P Index data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Terms & Conditions. Powered and implemented by Interactive Data Managed Solutions.",
    "summary": "- Signal president Meredith Whittaker opposes the proposed U.K. online safety legislation that would require backdoor access to encrypted messages.\n- Whittaker argues that implementing a backdoor is mathematically impossible and would undermine the security of communications.\n- Signal and other encrypted messaging apps may exit the U.K. market if the Online Safety Bill becomes law.",
    "hn_title": "Signal president says company will not comply with U.K. \u2018mass surveillance\u2019 law",
    "original_title": "Signal president says company will not comply with U.K. \u2018mass surveillance\u2019 law",
    "score": 500,
    "hn_content": "- Signal's president states that the company will not comply with the UK's 'mass surveillance' law.\n- In discussions about governments' desire to have access to private communications, comparisons are made to the encryption debate of the 90s.\n- The concept of the \"spirit of the law\" is discussed, with arguments made about the interpretation and preservation of laws.\n- Suggestions are made to refer to historical documents like the Federalist Papers for understanding the intent of the Constitution and Bill of Rights.\n- The importance of understanding the collective opinion of different viewpoints is emphasized.\n- The argument is made that established governments should have a proactive system in place to enshrine the desires of the public into law.\n- The limitations and challenges of the current legislative process are acknowledged.\n- The need for a government department that gathers public sentiment and initiates constitutional amendments is proposed.\n- The role of the public in shaping constitutional amendments is discussed.\n- The need for a proactive branch of government is highlighted and its potential impact on addressing public interests over time is explored.\n- The influence of gerrymandering on the democratic process is mentioned.\n- The idea of state sovereignty and the power of state legislatures is debated.\n- The limitations of relying solely on the legislative process for constitutional amendments are discussed.\n- The role and importance of the Supreme Court in interpreting and clarifying laws is mentioned.\n- The challenges and complexities of maintaining a balance between anonymity and surveillance in a democratic society are explored.\n- The potential dangers of government surveillance and the implications for privacy are highlighted.\n- The desire for anonymous digital cash and the challenges in achieving it are discussed.\n- The concept of class struggle and the influence of class dynamics on societal power structures are mentioned.\n- The role of governments and their responsibility to protect the rights of the people is emphasized.\n- The need to actively participate in the political process and fight for the rights of the working class is mentioned.- GCHQ's claims about encryption have been criticized as unproven government propaganda by some individuals.\n- The UK government's proposed surveillance legislation has sparked debate and concern over privacy and civil liberties.\n- Encryption has become a central issue in the ongoing debate, with arguments for both privacy and national security.\n- The public opinion on encryption and holding tech platforms accountable for scanning messages and posts for child sexual abuse varies, with some supporting legal requirements and others opposed.\n- The discussion of encryption and privacy has been influenced by recent events, such as Apple's CSAM scanning tool and the ongoing encryption debate in various countries.\n- The potential impact on privacy and the role of technology companies in protecting user data has become a key concern.\n- Encryption alone may not be sufficient to protect privacy, as other data collection methods and surveillance practices can still compromise personal information.\n- Public petitions and online activism may be a way for individuals to voice their concerns and engage in the conversation surrounding encryption and privacy.\n- The conversation around encryption and privacy is ongoing and is likely to continue as technology and society evolve.",
    "hn_summary": "- Signal's president announces that the company will not comply with the UK's 'mass surveillance' law.\n- The debate over encryption and privacy in relation to government surveillance has been sparked.\n- The role of technology companies in protecting user data and the potential impact on privacy are key concerns."
  },
  {
    "id": 36741910,
    "timestamp": 1689463991,
    "title": "A third of North America's birds have vanished",
    "url": "https://nautil.us/a-third-of-north-americas-birds-have-vanished-340007/",
    "hn_url": "http://news.ycombinator.com/item?id=36741910",
    "content": "Login SubscribeNautilus Members enjoy the site ad-free. Login or Join NowZOOLOGYA Third of North America\u2019s Birds Have VanishedAt first Adam Smith couldn\u2019t believe his calculations. Then it sank in.BY ANDERS GYLLENHAAL & BEVERLY GYLLENHAAL June 30, 2023ShareExploreFor weeks, Adam Smith had been crunching the raw data from more bird statistics than anyone had ever tried before\u2014thirteen different bird counts and millions of radar sweeps. Suddenly he heard the musical chime that tells him his results are ready. He leaned across his desk, surrounded by enough high-powered computers to heat up his entire office, and stared at what could only be an impossible conclusion: Over the past fifty years, his calculations found, a third of North America\u2019s birds had vanished. \u201cWell, that can\u2019t be right,\u201d he thought. \u201cI must have made a mistake somewhere.\u201dSmith, one of the hemisphere\u2019s top specialists in bird populations, just sat for a while in his cluttered cubicle at the Canadian Wildlife Service, which was decorated with caribou antlers, a musk-ox skull, and early drawings from his twin boys. Then it dawned on him. \u201cThis would be a massive change, an absolutely profound change in the natural system,\u201d he said. \u201cAnd we weren\u2019t even aware of it.\u201dGLOBE TROTTER: Teams of researchers in both South and North America are trying to protect the Cerulean Warbler, one of scores of tiny world-travelers whose ranges span the hemisphere. Photo by Anders Gyllenhaal.Up until that point, counting the abundance of individual birds throughout the entire continent was impossible. At any given time, many species number in the tens of millions in North America\u2014adding up to billions of birds\u2014and they\u2019re constantly on the move. But the science of bird study was advancing, and a close-knit group of scientists was experimenting with using radar imagery, satellite photos, and citizen science to add precision to the dozens of conventional bird counts done for groups of species.CITIZEN SCIENCE: The Great Blue Heron, shown here with its decorous mating plumage, is one of the species most often reported by birders as part of the massive citizen science project powered by the eBird smartphone app. Photo by Anders Gyllenhaal.The computation Smith had just finished that day in May 2019 combined individual population estimates for 529 bird species, from the most common sparrows and robins to rarities hardly ever seen. When Smith pulled these estimates together and adjusted each for its degree of certainty, the findings came down to a single ski slope of a chart. It showed a precipitous drop in nearly all these species in every part of the continent. At the bottom sat four lone digits\u20142.913. That\u2019s the number of breeding birds in billions that had disappeared since the early 1970s. He had documented an accelerating churn of seasonal losses that slowly took their toll on the abundance of birds. And it translated to an astounding third of the adult birds that not long ago filled North America but now are gone.BYE, BIRDS: The graphic that Adam Smith sent out to his fellow scientists, showing the seismic drop in bird populations over the past fifty years. Credit: Adam Smith.The hardest hit were grassland birds, down by more than 50 percent, mostly due to the expansion of farms that turn a varied landscape into acres of neat, plowed rows. That equates to 750 million birds, from bright yellow Eastern and Western Meadowlarks with their incessant morning songs to the stately Horned Lark with black masks across the male\u2019s eyes and tiny hornlike feathers that sometimes stick up from their heads. Forest birds lost a third of their numbers, or 500 million, including the compact, colorful warblers and speckle-breasted Wood Thrushes that sing like flutes. Common backyard birds experienced a seismic decline. That\u2019s where 90 percent of the total loss of abundance occurred, among just twelve families of the best-known birds\u2014including sparrows, blackbirds, starlings, and finches. There\u2019s been relatively little research on these species, and there\u2019s no sense of urgency when resources are already stretched thin for so many other birds in more dire need.The possibility of such losses was too startling to share with his colleagues until Smith checked every step of his calculations, particularly since he\u2019d never attempted this analysis before. \u201cIt always takes a couple of times to get these numbers right,\u201d he said. After a day and a half of painstaking scrutiny, Smith realized there was no mistake. \u201cI was speechless. We\u2019ve lost almost 30 percent of an entire class of organisms in less than the span of a human lifetime, and we didn\u2019t know it.\u201dNautilus Members enjoy the site ad-free. Login or Join NowSUCCESS STORY: The Bald Eagle is North America\u2019s great conservation success story, more than quadrupling its population in the past decade under an all-out effort to protect the iconic raptor. Photo by Anders Gyllenhaal.SOUND ON: The California Spottled Owl is at the center of the world\u2019s largest research project that uses sound to determine how to protect this storied species. Photo by Anders Gyllenhaal.HUM HALLELUJAH: The Ruby-throated hummingbird delivers one of the core services that birds provide in the balance of nature. Hummingbirds alone help pollinate more than 8,000 species of plants and flowers in North and South America. Photo by Anders Gyllenhaal.FOLLOW THE BIRD: Scientists hope to save the Florida Scrub-Jay, the state\u2019s lone native bird, with the help of a new tracking technique that follows the birds wherever they go around the clock. Photo by Jay Gyllenhaal.From A Wing and a Prayer: The Race to Save Our Vanishing Birds by Anders Gyllenhaal and Beverly Gyllenhaal. Copyright \u00a9 2023 by Anders Gyllenhaal and Beverly Gyllenhaal. Reprinted by permission of Simon & Schuster, Inc.Lead image: Bonnie Taylor Barry / ShutterstockAnders GyllenhaalPosted on June 30, 2023Anders Gyllenhaal was an investigative reporter at The Miami Herald and executive editor at The News & Observer, The Star Tribune (Minneapolis), and The Herald. He also served as the editorial vice president for the McClatchy Company\u2019s 30 newsrooms and 2,000 journalists. He served on the Pulitzer Prize board for nine years.Beverly GyllenhaalPosted on June 30, 2023Beverly Gyllenhaal was a reporter, features writer, and food editor at The News & Observer and The Miami Herald. She coauthored a syndicated column that appeared in 100 newspapers around the US and produced three books with nearly half a million copies in print.Get the Nautilus newsletterCutting-edge science, unraveled by the very brightest living thinkers.Newsletter Signup (LP) \u2013 In Page MobileEmail: *Sign up for freeView / Add CommentsExploreRequiem for the FoghornBY KRISTEN FRENCHJULY 14, 2023TECHNOLOGYAcross the US and around the world, foghorns have disappeared from the coastal landscape.ExploreThis WWII Story Made Us Better ThinkersBY DANIEL SIMONS & CHRISTOPHER CHABRISJULY 13, 2023PSYCHOLOGYBullet-ridden B-17 bombers taught us to look for what\u2019s missing.ExploreRescuing the FrogsBY TAMARA MARIA BLAZQUEZ HAIKJULY 12, 2023ZOOLOGYIndigenous conservation practices in Mexico are helping endangered frogs to recover.ExploreThe 5 Most Common Delusions WorldwideBY BRIAN GALLAGHERJULY 11, 2023PSYCHOLOGYThese bizarre beliefs are strikingly prevalent across a diversity of cultures. Why?ExploreHow to Drive a Car Through a WallBY KRISTEN FRENCHJULY 10, 2023NEUROSCIENCEThe winner of 2023\u2019s Best Illusion of the Year contest on the nature of science and magic.ExploreWhen Sowing Fear Is a Public ServiceBY BRIAN GALLAGHERJULY 7, 2023COMMUNICATIONNuclear winter is scary as hell. Spread the word.ExploreWhy Electric Cars Are Taking OffBY BRIAN GALLAGHERJULY 6, 2023ECONOMICSIt\u2019s not that we like EVs more. They\u2019re just way better now.ExploreWhy E-Bikes Catch FireBY ELENA RENKENJULY 5, 2023TECHNOLOGYThe dangerous chemistry of cheaply made lithium batteries.ExploreA Scientific Breakthrough in Noise ReductionBY SABINE HOSSENFELDERJULY 4, 2023TECHNOLOGYIonized air and wires may take headphones to the next level.ExploreThe Dark Side of FungiBY KRISTEN FRENCHJULY 3, 2023HEALTHThey\u2019re not all magical and entangled. These fungi will kill you.NAUTILUS: SCIENCE CONNECTEDNautilus is a different kind of science magazine. Our stories take you into the depths of science and spotlight its ripples in our lives and cultures.Get the Nautilus newsletterCutting-edge science, unraveled by the very brightest living thinkers.Newsletter Signup \u2013 FooterEmail: *Please check the box below to proceed.Sign up for freeQuick linksHomeAbout UsContactFAQPrimeEbookShopDonateAwards and PressPrivacy PolicyTerms of ServiceRSSJobsNewsletterEthics PolicySocial\u00a9 2023 NautilusNext Inc., All rights reserved.SUBSCRIBE TO THE NAUTILUS PRINT EDITION!Subscribe",
    "summary": "- Raw data analysis reveals that a third of North America's birds have disappeared over the past 50 years.\n- This finding is significant because it represents a massive and profound change in the natural system that was previously unknown.\n- The decline in bird populations is particularly severe for grassland and forest birds, as well as common backyard birds. The loss of these birds has important implications for biodiversity and ecosystem health.",
    "hn_title": "A third of North America\u2019s birds have vanished",
    "original_title": "A third of North America\u2019s birds have vanished",
    "score": 420,
    "hn_content": "- A third of North America's bird population has disappeared due to habitat loss and other factors.\n- Individual actions, such as creating bird-friendly yards, can have a positive impact on bird populations.\n- Influential individuals like architects and urban planners can also play a role in creating bird-friendly environments.\n- Small groups, such as birdwatchers and schools, can contribute to bird conservation efforts.\n- The issue of bird population decline is not solely an individual responsibility and requires action from governments and corporations.\n- Cats, both owned and feral, are a major threat to bird populations and responsible pet ownership can help reduce bird mortality.\n- The decline of bird populations is part of a larger problem of ecological collapse and loss of biodiversity.\n- The post highlights the need for systemic solutions to address the complex and widespread issue of bird population decline.\n- The focus on individual actions may provide a sense of empowerment and inspire people to take tangible steps to help bird populations.\n- The post challenges the notion that government regulation alone is sufficient to address the problem and suggests the need for bottom-up solutions.- The non-grass plants in lawns are important food sources for insects, which are then eaten by birds.\n- There has been a decline in bird populations due to factors like West Nile virus and loss of habitat.\n- Planting native plants and creating wilder sections of properties can attract the right insects and birds.\n- Conservation efforts should focus on eliminating poverty, reducing resource consumption, addressing government corruption, and controlling population increase.\n- Creating bird-friendly yards with natural elements like trees and water sources can attract more birds.\n- Government regulation and campaign finance reform are important for solving environmental problems.\n- The impact of human actions on the environment, such as pollution and habitat destruction, contributes to wildlife decline.\n- Overpopulation and over-consumption are major factors in environmental problems.\n- Climate change and the destruction of ecosystems are significant challenges.\n- The media and the public tend to focus on human impacts rather than the loss of wildlife and biodiversity.\n- Municipal governments and individuals can make a difference through local initiatives and support for conservation.\n- The need for electoral reform to ensure that politicians represent the views of their constituents.\n- There is a connection between economic development, poverty reduction, and environmental protection.\n- The long-term survival of humanity requires addressing the root causes of environmental issues and making sacrifices as a society.\n- The solution to environmental problems is complex and requires a combination of top-down and bottom-up approaches.\n- The importance of educating and engaging people in environmental issues to make meaningful change.\n- The need for comprehensive solutions that address the interconnectedness of environmental problems.\n- The role of technology in monitoring and addressing environmental issues.\n- The importance of public support and participation in environmental initiatives.\n- The influence of money and lobbying on environmental policy and the need for campaign finance reform.\n- The limitations of individual actions in solving large-scale environmental problems.\n- The need for cultural shifts to prioritize the environment and sustainability.\n- The impact of human activities on climate change and wildlife habitats.",
    "hn_summary": "- A third of North America's bird population has disappeared due to habitat loss and other factors, highlighting the problem of bird population decline and the loss of biodiversity.\n- Individual actions, such as creating bird-friendly yards with natural elements and responsible pet ownership, can contribute to bird conservation efforts.\n- The post emphasizes the need for systemic solutions to address the complex issue of bird population decline, including action from governments, corporations, architects, urban planners, and schools."
  },
  {
    "id": 36740921,
    "timestamp": 1689454997,
    "title": "PostgreSQL: No More Vacuum, No More Bloat",
    "url": "https://www.orioledata.com/blog/no-more-vacuum-in-postgresql/",
    "hn_url": "http://news.ycombinator.com/item?id=36740921",
    "content": "BlogContactFAQPostgreSQL: No More VACUUM, No More BloatPostgreSQL, a powerful open-source object-relational database system, has been lauded for its robustness, functionality, and flexibility. However, it is not without its challenges \u2013 one of which is the notorious VACUUM process. However, the dawn of a new era is upon us with OrioleDB, a novel engine designed for PostgreSQL that promises to eliminate the need for the resource-consuming VACUUM.The Terrifying Tale of VACUUM in PostgreSQLThe VACUUM process in PostgreSQL is a historical artifact that traces its roots back to the Berkley Postgres project, which implemented a concept known as infinite time-travel. The concept, while innovative at the time, was eventually dropped by the PostgreSQL community. However, it led to the implementation of a Multi-Version Concurrency Control (MVCC) system prone to table bloat.The PostgreSQL MVCC system, while beneficial for handling concurrent transactions, introduced the need for manual vacuuming. This was a process in which old, unneeded data was purged to free up space and ensure efficient database operations. Manual vacuuming, however, was a labor-intensive task and a potential source of inefficiencies in the system.The PostgreSQL community, in their continued efforts to improve the system, introduced autovacuum - an automatic vacuuming process designed to alleviate the need for manual vacuuming. This was a significant step forward, but it was not a perfect solution. The autovacuum process, while automatic, still consumed substantial system resources. This is one of the reasons why Uber decided to migrate from PostgreSQL to MySQL and one of the 10 things that Richard Branson hates about PostgreSQL.Further enhancements came with the implementation of Heap-Only Tuples (HOT) updates and microvacuum, both significant improvements that reduced the need for full table vacuums. However, despite these advancements, the VACUUM process still remained a resource-intensive operation. Furthermore, PostgreSQL tables remained prone to bloat, an issue that continues to plague many users today. This is the part of PostgreSQL that the team at OtterTune hates the most.Despite these challenges, many organizations and developers continue to use and support PostgreSQL. Its robustness, extensibility, and strong community are just a few reasons why. For instance, OtterTune, despite acknowledging PostgreSQL\u2019s problems, has decided to stick with it. They explain their reasons in a separate blog post, highlighting the importance of considering the overall benefits and drawbacks of a system before making a decision.Enter OrioleDB: The Engine of the FutureOrioleDB is a groundbreaking new engine for PostgreSQL, developed with a primary goal: to save tables from bloat and eliminate the need for regular maintenance like VACUUM. It achieves this through the implementation of row-level and block-level undo logs, as well as automatic page merging.The undo logs at the row and block level provide a more granular level of control, allowing for more efficient handling of data changes. The automatic page merging feature works tirelessly in the background to consolidate fragmented data, further enhancing the efficiency of the system.The figure above illustrates this techniques. Row-level undo log allows in-place updates. Block-level undo log allows to evict tuples, which are deleted but visible to some transactions, from the primary storage leaving more space for new tuples. Automatic merge of sparse pages saves tables and indexes from bloat after many deletes.The implementation of these features in OrioleDB results in a system that requires less manual intervention, consumes fewer resources, and is less prone to table bloat. This promises a significant improvement in the performance and user experience of PostgreSQL.BenchmarksThe following synthetic benchmark can illustrate the OrioleDB advantages of the above as well as some others. The following initialization script creates a table and 5 indexes on it.CREATE TABLE test (  id integer primary key,  value1 float8 not null,  value2 float8 not null,  value3 float8 not null,  value4 float8 not null,  ts timestamp not null);CREATE INDEX test_value1_idx ON test (value1);CREATE INDEX test_value2_idx ON test (value2);CREATE INDEX test_value3_idx ON test (value3);CREATE INDEX test_value4_idx ON test (value4);CREATE INDEX test_ts_idx ON test (ts);The pgbench script is given below. It\u2019s an upsert that performs sparse updates of the one of indexes on conflict. The sparse update causes this index to bloat when using regular heap PostgreSQL tables.\\set id random(1, 10000000)INSERT INTO test VALUES(:id, random(), random(), random(), random(), now() - random() * random() * 1800 * interval '1 second')ON CONFLICT (id) DO UPDATE SET ts = now();This benchmark illustrates the following advantages of OrioleDB design.Thanks to undo log and in-place updates, OrioleDB needs to update only one index, whose value has been changed. With the PostgreSQL heap engine, the update of a single indexed field disables HOT, so all indexes get updated.Automatic page merge saves sparse index from bloat. Sparse pages are automatically merged.Row-level WAL takes much less space than block-level WAL. That saves IOPS on WAL writing.See the results of the benchmark on the graphs below.As the cumulative result of the improvements discussed above, OrioleDB provides:5X higher TPS,2.3X less CPU load per transaction,22X less IOPS per transaction,No table and index bloat.Embrace the Future: Try OrioleDB TodayWith the introduction of OrioleDB, the PostgreSQL community stands on the brink of a new era where the haunting specter of VACUUM is a thing of the past. This novel engine offers a compelling solution to one of PostgreSQL\u2019s longest-standing challenges, promising users increased efficiency and fewer maintenance headaches.So why wait? Visit our github and try OrioleDB today and join the revolution to a more streamlined, efficient, and VACUUM-free PostgreSQL experience.Alexander Korotkov at 15 Jul 23 12:14 UTCCompanyContactFAQ",
    "summary": "- PostgreSQL is a popular database system known for its robustness and flexibility, but it has a challenge with the resource-consuming VACUUM process.\n- OrioleDB is a new engine for PostgreSQL that aims to eliminate the need for VACUUM and reduce table bloat by implementing row-level and block-level undo logs and automatic page merging.\n- OrioleDB offers significant improvements in performance, efficiency, and user experience, providing higher transaction throughput, lower CPU load, reduced IOPS, and no table or index bloat.",
    "hn_title": "PostgreSQL: No More Vacuum, No More Bloat",
    "original_title": "PostgreSQL: No More Vacuum, No More Bloat",
    "score": 328,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginPostgreSQL: No More Vacuum, No More Bloat (orioledata.com)328 points by pella 13 hours ago | hide | past | favorite | 96 commentsmattashii 15 minutes ago | next [\u2013]Oriole's design seems to require transaction-aware indexes with point entry removal, which has its own cost.E.g. a GiST equivalent (for e.g. spatial indexes) would be a hassle to maintain due to its nature of having no precise knowledge about the location of each index tuple, GIN (e.g. FTS indexing) could be extremely bulky due to a lack of compressibility in posting trees, and I can't imagine how they'd implement an equivalent to BRIN (which allows for quickly eliminating huge portions of a physical table from a query result if they contain no interesting data), given their use of index-organized tables. Sure, you can partition on PK ranges instead of block ranges, but value density in a primary key can vary wildly over both time and value range.Does the author have any info on how they plan to implement these more complex (but extremely useful) index methods?This doesn't even consider the issues that might appear if the ordering rules (collation) change. Postgres' heap and vacuuming is ordering-unaware, meaning you can often fix corruption caused by collation changes by removing and reinserting the rows that are in the wrong location after the collation changed, with vacuum eventually getting rid of the broken tuples. I'm not sure Oriole can do that, as it won't be able to find the original tuple that it needed to remove with point lookup queries, thus probably requiring a full index rebuild to fix known corruption cases in the index, which sounds like a lot of additional maintenance.replyivoras 7 minutes ago | prev | next [\u2013]Yeah, but...- Row-level anything introduces write alignment and fsync alignment problems; pages are easier to align than arbitrary-sized rows- PostgreSQL is very conservative (maybe extremely) conservative about data safety (mostly achieved via fsync-ing at the right times), and that propagates through the IO stack, including SSD firmware, to cause slowdowns- MVCC is very nice for concurrent access - the Oriole doc doesn't say with what concurrency are the graphs achieved- The title of the Oriole doc and its intro text center about solving VACUUM, which is of course a good goal, but I don't think they show that the \"square wave\" graphs they achieve for PostgreSQL are really in majority caused by VACUUM. Other benchmarks, like Percona's (https://www.percona.com/blog/evaluating-checkpointing-in-pos...) don't yield this very distinctive square wave pattern.I'm sure the authors are aware of these issues, so maybe they will write an overview of how they approached them.replywokwokwok 7 hours ago | prev | next [\u2013]Good job! That's cool.How do you plan to make your new project keep up to date with the release cadence of the parent project?...because otherwise, I can't see how this is a good idea.Look, I have the same reaction whenever someone does this.If someone goes and forks rust and creates a new programming language call dust that solves I dunno, the fundamental async compatibility story, or adds (somehow) a zero cost native GC type back into the language, I'd say the same thing.You've taken a big open source project, forked it and laid some significant changes on it, which you don't believe this be accepted upstream.Ok...is this a toy that you made for fun?...or a serious project you expect to maintain?If the answer is 'serious project', please make explicit your plans to avoid becoming abandonware in the future, your plans to fold future release from (original project) into yours, or your plans to diverge henceforth into an entirely new project.To be fair, I get it, this is an extension that seems like it could... probably... receive changes that are made upstream in postgres; but, if it was that easy, it belongs as part of the postgres projecct; so, I guess, it's not that easy.So, serious? Or just for fun?replytommiegannert 2 hours ago | parent | next [\u2013]The author wrote this, answering the question in a reply on the post:> Yes, sure! But that's the long way to go. Right now OrioleDB is an extension, which comes with PostgreSQL core patch. The mid-term goal for OrioleDB is to become a pure extension. The long-term goal is to make OrioleDB part of PostgreSQL core.replyznpy 1 hour ago | root | parent | next [\u2013]> The long-term goal is to make OrioleDB part of PostgreSQL core.That would be really the perfect outcomereplysomsak2 2 hours ago | parent | prev | next [\u2013]are you considering using this in production somewhere in the next few days? your reply comes off as absurdly aggressive, especially when you mention no intention of supporting the project monetarily. and that's on top of this question being addressed already, as other commenters pointed out.maybe don't come in so hot next time.replymst 35 minutes ago | root | parent | next [\u2013]It could likely have been better phrased but I found it more pointed than aggressive.There've been quite a few postgres forks that have either died or stayed based on 7.x versions and when you're replacing the entire storage engine - and hence also the on-disk format - migrating away from it if circumstances require it later is going to be annoyingly non-trivial.So while I think I agree with \"don't come in so hot\", \"absurdly aggressive\" may nonetheless be over-egging it slightly given the context.replyjustinclift 7 hours ago | parent | prev | next [\u2013]As a data point, that's already been answered: :)https://news.ycombinator.com/item?id=36742001replyfshbbdssbbgdd 6 hours ago | parent | prev | next [\u2013]Reasonable points. As an onlooker who has run into VACUUM in years past, I have wondered: is this a fundamental necessity, or could it possibly be fixed? Seeing an example of it being fixed is certainly helpful. If I work at a company with the resources to maintain a Postgres fork, great! If not, we can evaluate whether the challenges of using this fork are worth the performance benefit.replyklysm 5 hours ago | root | parent | next [\u2013]> a company with the resources to maintain a Postgres forkThat sounds like a monumental featreplyhlandau 1 hour ago | prev | next [\u2013]Is OrioleDB interested in committing to a stable on-disk format removing the need for an upgrade process between Postgres major versions? Seems like an opportunity to solve this problem.replymjlawson 7 hours ago | prev | next [\u2013]I'm excited about the title, but I have to say that my initial impression has left me frustrated. The main README on GitHub[1] smells of corporate-speak. So far I've learned that:- OrioleDB is a new storage engine for PostgreSQL- PostgreSQL is most-loved (whatever that means)- OrioleDB is an extension that builds on.. other extensions?- OrioleDB opens the door to the cloud!In the wake of crypto and other Web 3.0 grift, this is not the tact that I'd take to release something that extends and improves on something as important as PostgreSQL.[1] https://github.com/orioledb/orioledbreplynewaccount74 5 hours ago | parent | next [\u2013]> OrioleDB is an extension that builds on.. other extensionsI assume you are referring to this part:> OrioleDB consists of an extension, building on the innovative table access method framework and other standard Postgres extension interfaces.I don't know how they could be more clear? Table access methods were introduced in PostgreSQL to support alternative storage methods (like zheap, which tries to do something very similar, or possibly columnar data stores).Mentioning this fact is important, because there are a bunch of forks of PostgreSQL with alternative data storage systems; this is designed to work as an extension for an unforked PostgreSQL. (It doesn't yet)The Readme seems very clear if you are familiar with PostgreSQL.replynextaccountic 38 minutes ago | root | parent | next [\u2013]> this is designed to work as an extension for an unforked PostgreSQL.That's pretty cool!> (It doesn't yet)What's missing?replyctippett 8 hours ago | prev | next [\u2013]The article makes some convincing arguments and the benchmarks seem to corroborate their performance claims, but I don't understand the dichotomy between this proposed new storage engine (OrioleDB?) and PostgreSQL itself.Besides the commercial motivations and wanting to profit from the innovations discussed in the article, is there any reason why this needs to be a whole new database marketed as OrioleDB versus contributing these improvements upstream?replyakorotkov 7 hours ago | parent | next [\u2013]I'm seeing OrioleDB as a future engine for PostgreSQL. I'd like to see it as the default engine. However, the changes in OrioleDB are too big to be made incrementally. This is why I'm comparing the current PostgreSQL engine (with more than just heap, but many other subsystems as well) with OrioleDB.replytarasglek 3 hours ago | root | parent | next [\u2013]Curious if you could share further roadmap. Potential interesting directions: 1. plans for integration with object store ala neon?2. Columnar?3. Async-io oriented redesign4. Interesting new features ala subscriotions to table changes5. Zero copy client bindingsreplytarasglek 3 hours ago | root | parent | prev | next [\u2013]Just curious, are you ukrainian?Never thought I would see a fellow ukrainian rewriting my fav db.replyfuy 8 hours ago | parent | prev | next [\u2013]Alexander Korotkov (OrioleDb author) idea, - based on his Postgres committer experience, I believe, - is that these changes are way too big to be ever accepted upstream, hence separate engine. More info https://www.socallinuxexpo.org/sites/default/files/presentat..., see esp. slides 9-11replywaplot 8 hours ago | parent | prev | next [\u2013]These changes are way too big to integrated into postgresql's engine itself. It fundamentally changes how MVCC is done.replyeyegor 9 hours ago | prev | next [\u2013]Since this is an engine extension, I wonder if it would have any effect when combined with others. For example, timescaledb [0] acts on underlying tables. I wonder if this would have some effect if you did something like  create table xyz(...) using orioledb;  select create_hypertable(xyz, ts);[0] https://github.com/timescale/timescaledbreplydaenney 12 hours ago | prev | next [\u2013]I love the whole \u201c2.3x less CPU overhead per transaction\u201d where Postgres scales from 5% to 65% CPU usage and Oriole sits constantly at 90%. That doesn\u2019t seem like a huge success to me? The predictability sure is nice, but moving the lower end up by 85% is something I\u2019d be rather worried aboutreplyacjohnson55 11 hours ago | parent | next [\u2013]You generally want to keep your CPU fully utilized. It looks like Oriole is doing significantly more transactions and is CPU-bound, due to much lower IO requirements. The good news is that it implies you could get even more performance out of Oriole by vertically scaling to a more powerful CPU, whereas Postgres would not continue to increase in performance this way.Those idle times on the Postgres server could be used for something else, if you're thinking in a desktop OS mindset. But for servers, you tend to want machines that are doing one thing and are optimized for that thing.replytanelpoder 11 hours ago | root | parent | next [\u2013]> You generally want to keep your CPU fully utilized.Not in real life concurrent systems where latency matters. In addition to the queuing/random request arrival rate reasons, all kinds of funky latency hiccups start happening both at the DB and OS level when you run your CPU average utilization near 100%. Spinlocks, priority inversion, etc. Some bugs show up that don\u2019t manifest when running with lower CPU utilization etc.replynewaccount74 1 hour ago | root | parent | next [\u2013]This is a benchmark that tries to execute as many queries as possible, so the interesting stat is transactions per second, not CPU usage. This benchmark is testing top speed, not real world behaviour.If you tested both systems with the same workload (eg. a specific number of queries per second), then the average CPU usage would be much lower for the more efficient engine.The low CPU usage in this benchmark is just a sign that the performance is not CPU bound, but limited by other factors like locking or IO.replymlyle 2 hours ago | root | parent | prev | next [\u2013]Here the system is doing a pretty consistent 750k TPS instead of oscillating between 0 and 225k-- often sitting near 0TPS for tens of seconds. Which system do you think will have better latency for any given loading?replydeadbeeves 8 hours ago | root | parent | prev | next [\u2013]So what you're saying is that an acceptable way to compensate the system's bugginess is by making it more inefficient? I'd rather use a system that's stable under load.replyignoramous 4 hours ago | root | parent | next [\u2013]OP has a point [0], though I'm unsure if Kernels under load are really that unstable / untested.[0] https://brooker.co.za/blog/2021/05/24/metastable.html / https://archive.is/6Qtetreplyarghwhat 1 hour ago | root | parent | prev | next [\u2013]A service should use 100% CPU when loaded fully, anything else means you are suffering from bottlenecks that are actively limiting your throughput. Having lower CPU load because your disks are barely hanging in there is certainly not better.If you want lower max CPU load, just limit its resources (e.g., CPU quota, cpuset limitation) or load it less.replythayne 8 hours ago | root | parent | prev | next [\u2013]> You generally want to keep your CPU fully utilizedOnly if your load is very predictable. If there is a chance of a spike, you often want enough headroom to handle it. Even if you have some kind of automated scaling, that can take time, and you probably want a buffer until your new capacity is available.replymlyle 2 hours ago | root | parent | next [\u2013]I think many here is misunderstanding what was likely meant: postgresql was not able to use all the available CPU under this situation, in that it was oscillating from 10% to 70% CPU use. That 40% average cpu use isn't an asset on a dedicated database server: it just means that the other 60% of available cycles are a perishable resource that are immediately spoiling.In that sense, you want to be able to have your database be able to use all the resources available: all the IOPS, all the CPU cycles, etc.And, of course, the real thing is the amount of work you get done: this thing does more work-- partially by using more CPU cycles, and partially by doing more work per CPU cycle.replyraggi 11 hours ago | root | parent | prev | next [\u2013]It\u2019s hard to generalize on these points. In a situation where the throughput was inverted but the proportional system usage was the same, you would instead say \u201cyou can still vertically scale by adding more disks\u201d, rather than saying adding bigger cpu. It\u2019s not meaningful in isolation.It may be reasonable to suggest that for a new code base that is cpu bound there\u2019s a good chance there is low hanging fruit for cpu optimizations that may further increase the throughput gap. It\u2019s also the case however that the prior engines tuning starting life on much older computer architectures, drastically different proportional syscall costs and so on, it very often means that there\u2019s low hanging fruit in configuration to improve baseline benchmarks such as these. High io time suggests poor caching which in many scenarios you\u2019d consider a suboptimal deployed configuration.It\u2019s not just the devil that\u2019s in the details, it\u2019s everything.replyraggi 11 hours ago | root | parent | next [\u2013]To be a little more clear on what the detail of the benchmark in question is: it\u2019s a benchmark that explicitly exercises a pathological use case for postgresqls current design, one that nonetheless functions, and demonstrates that the advertised engine does not have that pathology. A key takeaway should probably be, if you\u2019re a Postgres user: if your workload looks exactly like this (sparse upserts into a large data set at a high rate) then you might want to evaluate your the runway of your architecture before the geometric costs or latency stalls become relevant - just as for cost analysis of any other system. What is somewhat interesting in this article, and not super clearly presented, is that this workload is actually fairly pathological for most existing engines offering this set of structural and query facilities, and that\u2019s interesting, if this is the niche you need. Most people do some amount of this, but not always at a super high rate, and there are ways to get the same effective writable/readable data using a different schema, while avoiding it. Nice thing here is you can do the one-liner version.replygary_0 11 hours ago | root | parent | prev | next [\u2013]> you can still vertically scale by adding more disksParallelizing IO is a lot different from scaling up CPU power, though. I'd imagine DB server IO performance has a lot less lower-hanging fruit than CPU/software performance.replyraggi 8 hours ago | root | parent | next [\u2013]That depends, the ratio of free bus capacity for data fetch, and free capacity for inter-CPU synchronization is skewed _massively_ in favor of capacity for data fetch. An x86 system is already under-capacity at the cpu/bus interface, which is why we keep throwing more and more cache at the problem and it works.Similarly in the cloud on AWS fro example, you have publicly available scalability options starting from 5k IOPS up to 2M IOPS, >400x or 3 orders of magnitude. By contrast you're going from 1vcpu to 192 cores, about half the raise, and a lower performance scaling due to the increased cost of cross-package shootdowns.Yup, they're different, for sure, but the implication that CPU is easier is not all that clear. In either case, with a database style workload, and with either of these engines in practice you're going to hit a limit at the bus in practice long before you hit a limit on compute or disk io, for any sustained workload - bursts are different.replyacjohnson55 11 hours ago | root | parent | prev | next [\u2013]That makes sense. I'm mostly just trying to explain the counterintuitive reason that the high CPU usage shouldn't be interpreted as a flaw.replyjklehm 12 hours ago | parent | prev | next [\u2013]My read is that it's at 90% because they are saturating the CPU to that point with the TPS threshold they use for comparison, the TPS of Oriole is constant and way higher than pg in these charts at least.I'd think the CPU will drop proportionally to the TPS, they just want to show how high it can go here.replyaseipp 11 hours ago | parent | prev | next [\u2013]Yes, but now that your CPU utilization is uncapped, you can more easily scale the utilization down and retain some form of proportional performance, so it doesn't matter. If you capped the system to 60% of your CPU, it might change the overall numbers, but say you're doing 1.8x more TPS at the same usage, it's a win either way. It's not a marketing trick; those numbers come across as \"Very good\", to me.If Expensive Server CPU = X dollars per unit, and it's only used at 60% capacity and can realistically only be used at that capacity, then you have effectively just set .4*X amount of dollars on fire, per unit. If you can vertically take a workload and scale it to saturate 90% of a machine, it's generally easy to apply QOS and other isolation techniques to achieve lower saturation and retain some proportional level of performance. The reverse is not true: if you can only hit 60% of your total machine saturation before you need to scale out, then the only way to get to 90% or higher saturation is through a redesign. Which is exactly what has happened here.replyavianlyric 8 hours ago | parent | prev | next [\u2013]It was a performance test, where presumably the objective was to apply the maximum possible load each DB engine could handle, and apply that load continuous for a long period of time.The CPU load jumping up and down isn\u2019t Postgres \u201cscaling\u201d it Postgres hitting performance bottlenecks on a regular basis, presumably driven by the need to perform vacuums which are very IO insensitive. So instead of using IO to serve queries, Postgres is using IO for janitorial work, and TPS (and thus CPU usage) crater.Oriole on the other hand manages much higher throughput, and much more consistently than Postgres.What would you prefer a car that does a constant 100mph when your foot\u2019s down. Or one that wildly oscillates between 40mph and 70mph, despite you trying to put the pedal through the floor?replypella 12 hours ago | parent | prev | next [\u2013]With the same equipment, your performance is now five times better. (5X higher TPS) We need to test again with more hardware, but if you can maintain 3 times the performance at the lower end, it could be a good alternative for some users.\"As the cumulative result of the improvements discussed above, OrioleDB provides:- 5X higher TPS,- 2.3X less CPU load per transaction,- 22X less IOPS per transaction,- No table and index bloat.\"replywaterproof 11 hours ago | parent | prev | next [\u2013]Eyeballing the tps graph, OrioleDB is doing 5x tps while using 2x the CPU. So about 5/2=2.5x the CPU per transaction.Checks out.replygary_0 11 hours ago | root | parent | next [\u2013]5x tps with 2x CPU is 2/5 = 0.4x the CPU (ie. it's more efficient per transaction).replyadsharma 12 hours ago | parent | prev | next [\u2013]It's not clear if the CPU cost per tx is any worse. Was OrioleDB doing 5x the transactions at this CPU usage?replycolanderman 7 hours ago | parent | prev | next [\u2013]Yes, because they are performing more transactions per second, by virtue of performing less I/O per transaction. This is a good thing.replyrickette 11 hours ago | prev | next [\u2013]The article contains a link with the rather curious title \"10 things that Richard Branson hates about PostgreSQL\".... Turns out the guy who wrote that blog is called Rick Branson, not Richard.replyrcme 11 hours ago | parent | next [\u2013]Rick is a nickname for Richard.replyjeffparsons 10 hours ago | root | parent | next [\u2013]True words can still be clickbait.In fact, I'd argue that many of the most effective ways to mislead people involve sticking rigidly to literal truth, because it makes them so much harder to counter. When there's no literal untruth to correct, it's natural to end up implying bad faith _without having any definitive proof_, and that is mighty unstable ground from which to argue.replyrcme 9 hours ago | root | parent | next [\u2013]I get what you\u2019re saying, but imagine your name was Richard Branson. You\u2019d hear no end to the jokes. At what point can you consider this an internalized behavior of the author? Is it still clickbait if the author believes his main raison d\u2019etre is to have a meme name?replyjeffparsons 9 hours ago | root | parent | next [\u2013]I think I basically agree with you. And this example is pretty benign \u2014 I'm not actually meaning to criticize anyone here.However I will not that the author in question refers to himself as \"Rick Branson\", and the article title is \"10 Things I Hate About PostgreSQL\". So I think it's just the person who made the link who is being a bit cheeky.My comment was going off on a wild tangent. :)replymst 29 minutes ago | root | parent | next [\u2013]They may have simply been intending to be relatively formal as a mark of respect. (there's enough language and culture dependencies in how one decides such things that 'may' is very much load bearing in that sentence, mind)Certainly it wouldn't've occurred to me to think it was the businessman rather than a name collision.But, eh, agreed on tangent, and I'm not intending to criticise either.replytaneq 7 hours ago | root | parent | prev | next [\u2013]It's like that album by Pete Best, who was a drummer in The Beatles. He published a solo album called \"Best of the Beatles\".replymattl 9 hours ago | root | parent | prev | next [\u2013]See the guy on Bluesky who is called Steve Wozniak and isn\u2019t trying to pretend to be Woz and yet has issues while Bluesky also let someone with a racial slur username get an accountreplykstrauser 11 hours ago | parent | prev | next [\u2013]That caught my eye, too. What do I care what the Virgin CEO thinks of a database?Oh, not that one.reply29athrowaway 9 hours ago | parent | prev | next [\u2013]\"Richard Branson\" is not a unique identifier. Maybe we should all go by UUIDsreplypmontra 4 hours ago | prev | next [\u2013]If this engine is so much better than the internal one shouldn't we expect that at least the big cloud providers will use it on their managed servers? They have an economic incentive to do so. If that happens eventually the PostgreSQL project itself will replace the default engine, or am I wrong?replyklysm 4 hours ago | parent | next [\u2013]I\u2019m not sure the risk of an immature engine is worth it to them. Customers pay for hosted Postgres because they want to not worry about doing it themselves for cheaper. They are paying for reliability.I think you\u2019re correct about the existence of an economic incentive for the cloud providers, but I anticipate it would be offered as a distinct product to \u201cvanilla\u201d (at least in the sort term).Things get interesting though because this space of database products has trended towards restricting who can host in their license terms (TimeScale, ClickHouse, etc). If that\u2019s Orioles cash-in play then maybe cloud providers can\u2019t use it anyway.I suspect the fate of the engine will be determined by its funding sourcereplyRapzid 37 minutes ago | root | parent | next [\u2013]> I\u2019m not sure the risk of an immature engine is worth it to themHeh, about that.. Hasn't AWS already crossed that threshold with Aurora RDS, Redshift, and etc?replykunley 48 minutes ago | prev | next [\u2013]Thanks fof the benchmark graphs, but the conclusion from the graphs is bogus:- CPU load on a graph is actually higher for OrioleDB, not lower- the factors of supposed speedup are not matching what we see on the graphs.replywryanzimmerman 34 minutes ago | parent | next [\u2013]I\u2019m not sure the cpu load being higher is a bad thing, isn\u2019t that basically showing that it is using less IO so it can use more of the cpu?The throughput is way, way higher, so it\u2019s using less cpu per transaction. If this were showing equal numbers of transactions the CPU usage would be lower.Ideally, in a benchmark, I think we\u2019d be seeing basically 100% cpu usage because that would mean the test hardware is being fully utilized and the software being tested isn\u2019t being bottlenecked in some way.replyjpgvm 41 minutes ago | parent | prev | next [\u2013]I think you might have misread the graphs. The graph is showing a 4x peak/~6x average improvement in TPS. Because of this the load is less I/O bound and thus CPU is able to be fully utilised. If you want to measure efficiency you would instead measure at constant TPS.replymst 34 minutes ago | parent | prev | next [\u2013]Given the higher TPS I think you'd be better conceptualising it as \"CPU utilisation\" rather than \"CPU load.\"replyruuda 11 hours ago | prev | next [\u2013]Vacuum does more than removing dead tuples though, there is still a need to update statistics and summarize BRINs.replywild_egg 11 hours ago | parent | next [\u2013]You're thinking of ANALYZE which is a separate operation that's commonly run during vacuuming but can be invoked independentlyreplyjavajosh 11 hours ago | parent | prev | next [\u2013]Yes. The (psql 15) docs are well written: https://www.postgresql.org/docs/15/routine-vacuuming.htmlreplyccleve 12 hours ago | prev | next [\u2013]Is there any documentation on the \"extensibility patches\"? What did you have to do to core Postgres to get this new approach to work?replyakorotkov 10 hours ago | parent | next [\u2013]Please, check this. https://supabase.com/blog/postgres-pluggable-strorage https://www.pgcon.org/events/pgcon_2023/schedule/session/470... Pushing patches to PostgreSQL Core requires a lot of work. But there is a progress already: 5k lines patchset to PG 14, and 2k lines patchset to PG 16.replyksec 5 hours ago | root | parent | next [\u2013]>Please, check this. https://supabase.com/blog/postgres-pluggable-strorageSo 60% of code committed to PG 16 already?replymlyle 2 hours ago | root | parent | next [\u2013]60% of the code needed to run it as a normal outside extension.replyjustinclift 7 hours ago | root | parent | prev | next [\u2013]Awesome. Thanks heaps for your efforts on this!Hopefully it all gets through the hurdles eventually, becoming a new storage engine shipped by default in PG. Maybe even becoming the new default. :)replyoaiey 11 hours ago | prev | next [\u2013]I read object relational? Can someone enlighten me? Entity relational fine but what makes it object relational? Has someone flipped on the buzzword in the years I did not pay attentionreplyfeike 11 hours ago | parent | next [\u2013]PostgreSQL has used this term for decades!The oldest I can find is from 1998 (PostgreSQL 6.3), but it was probably in use even before.> Postgres offers substantial additional power by incorporating the following four additional basic concepts in such a way that users can easily extend the system:classes inheritance types functionsOther features provide additional power and flexibility:constraints triggers rules transaction integrityThese features put Postgres into the category of databases referred to as object-relationalhttps://www.postgresql.org/docs/6.3/c0101.htmreplyoaiey 36 minutes ago | root | parent | next [\u2013]Thanks. Very much a buzzword then.replygrzm 11 hours ago | parent | prev | next [\u2013]It's been object relational (and described as such) going way back. I think the most visible (if infrequently used) object-oriented feature that it has is inheritance: https://www.postgresql.org/docs/current/tutorial-inheritance...replystubish 10 hours ago | parent | prev | next [\u2013]Object Databases were once a thing, and PostgreSQL PostgreSQL used the term Object Relational to indicate it could be used as both an Object Database (it supports table inheritance) and/or a Relational Database. Not that you should ever use the feature, being a historical artifact full of historical gotchas and your clever design becomes a maintenance burden.replyzetalyrae 11 hours ago | parent | prev | next [\u2013]I looked this up the other day because I was similarly surprised, I think it refers to Postgres' ability to do table inheritance: https://www.postgresql.org/docs/current/tutorial-inheritance...replynieve 11 hours ago | parent | prev | next [\u2013]It's because PostgreSQL has inheritance and has almost certainly used the term object relational since before you heard of it.replybrazzy 11 hours ago | parent | prev | next [\u2013]https://stackoverflow.com/questions/45865961/what-does-postg...replyglogla 11 hours ago | prev | next [\u2013]I with people would stop with the \"Uber migrated from Postgres to MySQL\" thing. Uber migrated from Postgres used as relational database to something that is basically their own non-relational database using MySQL as distributed key-value store. It is not really situation applicable to most users of Postgres.Anyway, this design of MVCC which moves older data into undo logs / segments is used by Oracle DB, so it definitely works. The common challenge with it is that reading older versions of data is slower, because you have to look it up in a log, and sometimes the data is removed from the log before your transactions finishes, getting the dreaded \"Snapshot Too Old\" error.E: I don't see in the article when rows get evicted from the undo logs. If when they are no longer needed, I'm not sure where the improvement comes from because it should be similar amount of bookkeeping? If it's a circular buffer that can ran out of space like Oracle does it that would mean under high write load long-running transactions starts to fail which is pretty unpleasant.replyakorotkov 10 hours ago | parent | next [\u2013]> E: I don't see in the article when rows get evicted from the undo logs.The undo records are truncated once they aren't needed for any transaction.> If when they are no longer needed, I'm not sure where the improvement comes from because it should be similar amount of bookkeeping?It depends on what exactly is \"bookkeeping\". If we consider amount of work, then improvement comes because old undo records can be just bulk deleted very cheap (corresponding files get unliked). No vacuum scan is needed. If we consider amount of space occupied, then indeed the same amount of versions take the same amount of space. But saving old versions of rows in the separate storage can save their primary storage from long-term degradation. Also, note that OrioleDB implements automatic merging of sparse pages.> If it's a circular buffer that can ran out of space like Oracle does it that would mean under high write load long-running transactions starts to fail which is pretty unpleasant.OrioleDB implements in-memory circular buffer for undo logs. Once circular buffer can't handle all the undo records, least recent records are evicted to the storage. Currently, we don't place limitation on the site of undo logs. Undo records are kept while any transaction can need them. So, no \"Snapshot Too Old\" errors. However, we can consider implementing this Oracle-like error as an option, which allows to limit the undo size.Also, please, check the architecture documentation of github (if didn't already). https://github.com/orioledb/orioledb/blob/main/doc/arch.mdreplypaulddraper 11 hours ago | parent | prev | next [\u2013]SQL Server avoids vacuum as well, it might be this way, I can't recall.And of course MySQL avoids vacuum by giving a giant middle to concurrency considerations.replytomnipotent 10 hours ago | root | parent | next [\u2013]That's because they don't store non-current versions of rows in the table itself, so why would they need a vacuum? MySQL does need to vacuum indexes, however.replyglogla 11 hours ago | root | parent | prev | next [\u2013]I'm pretty sure SQL Server and MySQL use locking instead of MultiVersion Concurrency Control so they don't keep more copies of data around. No vacuum needed but there's a possibility of things blocking.But I might be out of date.replyevanelias 11 hours ago | root | parent | next [\u2013]InnoDB (MySQL's default storage engine) implements MVCC using undo logging and background purge threads. It scales to highly concurrent OLTP workloads quite well. It doesn't work well with OLAP workloads / long-running transactions though. The oldest active transaction will block purging of anything newer than that transaction's snapshot.replypaulddraper 11 hours ago | root | parent | prev | next [\u2013]Yes, MySQL has read locks.But I don't believe SQL Server does.replyarbitrix 11 hours ago | root | parent | prev | next [\u2013]SQL Server has had MVCC since 2005.replyDaiPlusPlus 9 hours ago | root | parent | next [\u2013]Yes, but doesn't it require opt-in to enable snapshopt isolation? Most T-SQL devs will probably default to locking (TABLLOCK, etc) becuase that's what the bulk of google search results for \"how do I fix my broken query?\" tell people to do: it's only very, very rarely do I see a stackoverflow or dba.se answer that mentions MVCC-related topics.replypella 12 hours ago | prev | next [\u2013]simple OrioleDB docker build tutorial :https://github.com/orioledb/orioledb/blob/main/doc/docker_us...replyemmanueloga_ 10 hours ago | prev | next [\u2013]Experimental format to help readability of a long rant:1.According to the OP, there's a \"terrifying tale of VACUUM in PostgreSQL,\" dating back to \"a historical artifact that traces its roots back to the Berkeley Postgres project.\" (1986?)2.Maybe the whole idea of \"use X, it has been battle-tested for [TIME], is robust, all the bugs have been and keep being fixed,\" etc., should not really be that attractive or realistic for at least a large subset of projects.3.In the case of Postgres, on top of piles of \"historic code\" and cruft, there's the fact that each user of Postgres installs and runs a huge software artifact with hundreds or even thousands of features and dependencies, of which every particular user may only use a tiny subset.4.In Kleppmann's DDOA [1], after explaining why the declarative SQL language is \"better,\" he writes: \"in databases, declarative query languages like SQL turned out to be much better than imperative query APIs.\" I find this footnote to the paragraph a bit ironic: \"IMS and CODASYL both used imperative query APIs. Applications typically used COBOL code to iterate over records in the database, one record at a time.\" So, SQL was better than CODASYL and COBOL in a number of ways... big surprise?Postgres' own PL/pgSQL [2] is a language that (I imagine) most people would rather NOT use: hence a bunch of alternatives, including PL/v8, on its own a huge mass of additional complexity. SQL is definitely \"COBOLESQUE\" itself.5.Could we come up with something more minimal than SQL and looking less like COBOL? (Hopefully also getting rid of ORMs in the process). Also, I have found inspiring to see some people creating databases for themselves. Perhaps not a bad idea for small applications? For instance, I found BuntDB [3], which the developer seems to be using to run his own business [4]. Also, HYTRADBOI? :-) [5].6.A usual objection to use anything other than a stablished relational DB is \"creating a database is too difficult for the average programmer.\" How about debugging PostgreSQL issues, developing new storage engines for it, or even building expertise on how to set up the instances properly and keep it alive and performant? Is that easier?I personally feel more capable of implementing a small, well-tested, problem-specific, small implementation of a B-Tree than learning how to develop Postgres extensions, become an expert in its configuration and internals, or debug its many issues.Another common opinion is \"SQL is easy to use for non-programmers.\" But every person that knows SQL had to learn it somehow. I'm 100% confident that anyone able to learn SQL should be able to learn a simple, domain-specific, programming language designed for querying DBs. And how many of these people that are not able to program imperatively would be able to read a SQL EXPLAIN output and fix deficient queries? If they can, that supports even more the idea that they should be able to learn something different than SQL.----1: https://dataintensive.net/2: https://www.postgresql.org/docs/7.3/plpgsql-examples.html3: https://github.com/tidwall/buntdb4: https://tile38.com/5: https://www.hytradboi.com/replynecovek 6 hours ago | parent | next [\u2013]> I personally feel more capable of implementing a small, well-tested, problem-specific, small implementation of a B-Tree than learning how to develop Postgres extensions, become an expert in its configuration and internals, or debug its many issues.It gets harder as you delve into high concurrency and ensuring ACID: if you are using an established database, these are simply problems you don't have to deal with (or rather more truthfully, there are known ways to deal with them like issuing an \"UPDATE x=x+1\" instead of fetching x and then setting it to x+1).Still, writing an application expecting the datastore to ensure consistency is one thing, and ensuring that consistency are different problems requiring a different mindset (you are thinking of hard problems of your business logic, but you also have to think of hard problems common to db engines at the same time?).> But every person that knows SQL had to learn it somehow. I'm 100% confident that anyone able to learn SQL should be able to learn a simple, domain-specific, programming language designed for querying DBs.The benefit of languages as ubiquitous as SQL is that once you need something that you did not think of, SQL already enables it. But plenty of non-relational databases provide their own non-SQL APIs already (ElasticSearch, Redis, MongoDB, DynamoDB...), and as you suggest, developers cope with them just fine.However, people used to expressiveness of SQL (even if we all know it's imperfect), always miss what they can achieve with a single query moving performance (and some correctness) considerations to the database. The idea is as old as programming: transfer responsibilities for accessing data performantly to whatever is managing that data, even if we know that there are always cases where it's an uphill battle.It's that combination of good-enough performance, good-enough expressiveness, impressive consistency and correctness, and relational databases (and SQL) are a great choice for most applications today.replyemmanueloga_ 6 hours ago | root | parent | next [\u2013]The ACID and concurrency aspects are definitely harder to deal with, but it also depends on what you need. I wonder if many people would find a nice perf increase by running a simpler, well designed db that runs in a single process of a beefy modern computer in a compiled language. In any case, writing any multithreading or multiprocess code is hard, and I doubt a multi-million LoC codebase makes it any easier.> you are thinking of hard problems of your business logic, but you also have to think of hard problems common to db engines at the same time?YES! everyone is complaining these days about slow software in our beefy machines. I guess the core of my rant is that it feels like all of us programmers should start caring a lot more about data organization, code size, minimizing dependencies, data oriented design and \"mechanical sympathy\". Advances in languages, tooling and accessibility to information should demystify the how-to of managing our own application data ourselves.replynecovek 4 hours ago | root | parent | next [\u2013]I symphatise with your last point! And I agree that great developers should understand how to build a sufficiently performant database for their app, even if they won't build one.However, I think our applications are not slow due to database access, but one too many layers of indirection otherwise: eg even ORMs usually introduce a huge performance and complexity cost.Just like we are trying to come up with better and less error prone concurrency models in code (async/await, coroutines...), I get that you are trying to come up with better tooling support for data access, and we should.But we also need to be aware that some people simply want to solve a problem more efficiently, but not most efficiently (look at most ML code and you can barf at it \u2014 yet it still makes a huge progress in one area they care about).replyriku_iki 8 hours ago | parent | prev | next [\u2013]> A usual objection to use anything other than a stablished relational DB is \"creating a database is too difficult for the average programmer.\" How about debugging PostgreSQL issues, developing new storage engines for itthat's exactly what OP company is doing: they are building storage engine for postgres.replyemmanueloga_ 6 hours ago | root | parent | next [\u2013]I doubt this initiative is gonna make Postgres easier to use, smaller in terms of dependencies, simplicity of its codebase or resource usage.replycuu508 4 hours ago | root | parent | next [\u2013]Regarding resource usage, the benchmarks in the article show reduced IO usage. Are you doubting the validity of those benchmarks?replyeduction 9 hours ago | prev [\u2013]Oh I see it\u2019s notOracle dbIt\u2019sOriole dbTotally differentOracleOriolecoughreplyresist_futility 9 hours ago | parent [\u2013]But also not to be confused with OLEDBreplyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- OrioleDB is a new storage engine for PostgreSQL that aims to solve the issue of vacuuming and table and index bloat.\n- The performance benchmarks show that OrioleDB provides higher throughput and lower CPU load per transaction compared to standard PostgreSQL.\n- OrioleDB uses a different approach to handling old versions of data, storing them in separate storage and automatically merging sparse pages."
  },
  {
    "id": 36734214,
    "timestamp": 1689403348,
    "title": "Wireshark Is 25: The email that started it all & lessons learned along the way",
    "url": "https://blog.wireshark.org/2023/07/wireshark-is-25/",
    "hn_url": "http://news.ycombinator.com/item?id=36734214",
    "content": "Wireshark Is 25: The email that started it all and the lessons learned along the way5 Replies25 years ago, I sent this email, which ended up changing the course of my life From: Gerald Combs - Unicom Communications <gerald [at] \u2026 >To: gtk-list [at] redhat . comSubject: ANNOUNCE: Ethereal 0.2.0Date: Tue, 14 Jul 1998 21:47:01 -0500 (CDT)Ethereal is a network analyzer that lets you capture and interactivelybrowse the contents of Ethernet frames. Packet data can be read from afile, or live from a local network interface.More information, including the source distribution, can be found athttp://ethereal.zing.org . Comments and patches are welcome.I remember being nervous and excited at the time, wondering what the reaction would be. I had spent the previous few months working on a protocol analyzer, which was something I needed at work.There are a couple of things to note: the name wasn\u2019t Wireshark (we changed the name in 2006) and at the time of this email, protocol analyzers were rare, you could even say a precious thing. Back then, if you wanted to see what was happening on your network, command line tools like tcpdump and snoop were available at no cost, but if you wanted a GUI analyzer, you had to pay for a product that might cost the equivalent of a luxury car. I needed an analyzer to do my job, so I ended up writing a simple one and releasing it as open source.As it turned out, other people needed an analyzer too. A couple of days after I sent this email, I received a patch. Then another, and another. This quickly blossomed into a thriving developer community which has kept me busy ever since. In the intervening years, the project and its community have grown beyond any expectation that I could have had. Today, Wireshark is used by people around the world to make their networks fast, reliable, and secure, and educators use it to teach the next generation of security and network engineers about the low-level aspects of networks.What does it take to keep a project successful for so long? It would be nice to be able to say that from the very beginning, there was a grand, clear vision of how everything would evolve and unfold. However, as with any significant endeavor, you have to learn to adapt and apply the resources at hand as effectively as you can. In retrospect, Neils Bohr\u2019s quote\u201cAn expert is a person who has made all the mistakes that can be made in a very narrow field.\u201drings especially true in our case. I did manage to learn that in order for an open source project to grow and thrive, you have to ensure that needs are met in the following categories:User support. Living, breathing people make use of your project, and sometimes need assistance. They might be daily power users or novices, and their needs vary.Educator support. Experts and power users can teach others how to get the most out of your project, and need assistance as well.Developer support. The easier you can make it for people to contribute code or otherwise improve the project, the healthier the project and community will be.Attorneys and accountants. Until GitHub and GitLab add \u201cintellectual property law\u201d or \u201ctravel reimbursement\u201d buttons, you\u2019re going to need to find outside help for the regulatory, legal, and financial aspects of your project.Infrastructure. You need servers, containers, and services that provide an online presence and allow for collaboration. GitHub or GitLab may or may not be sufficient for this depending on your specific needs.This isn\u2019t quite a Maslow-style hierarchy of needs for open source, but it\u2019s close. The lines between users, educators, and developers can be blurry, but in general, users and educators depend on the output that developers produce, and the project as a whole needs a solid infrastructure in order for everyone to collaborate.It also doesn\u2019t apply uniformly to all projects. For example, a small, single-purpose image processing library might only need infrastructure and developers, and its user community would likely be other developers. A hosting service like GitHub or GitLab would be sufficient for the project\u2019s infrastructure. On the other hand, a large, feature-rich image editing application might have extensive needs in all five categories along with a business model to fund the project.Wireshark falls into the latter group, and needs quite a bit of care and feeding. Until recently, the business model for meeting these needs has been \u201cask my employer if they\u2019ll host the project,\u201d which worked surprisingly well. My employers (CACE Technology, Riverbed and now Sysdig) provided the resources for Wireshark that go way beyond what GitHub and GitLab can offer, such as SharkFest, a conference dedicated to protocol analysis.In recent years, it became clear that the project would best meet its needs by standing on its own, and with the help of Sysdig, we moved the project to the Wireshark Foundation earlier this year so that the project can continue to grow and serve its community.25 years ago when I sent that email, I never thought I would be blogging about the project today! If you\u2019ve ever thought about starting an open source project, don\u2019t hesitate to do so. Supporting and managing Wireshark has given me a wonderful career, and opportunities to meet and work with brilliant, inspiring people. If I had to distill everything I\u2019ve learned down to a couple of pieces of advice, it would be this:The community has valuable insights to share. Let them do that.Don\u2019t be afraid to start something new. You never know where it might take you 25 years from now.Finally, I can\u2019t thank you all enough for using and contributing to Wireshark over the years. I can\u2019t wait to see what the future brings. Cheers!This entry was posted in Announcement.",
    "summary": "- 25 years ago, Wireshark, a network analyzer, was created and released as open source, changing the course of the developer's life.\n- Wireshark quickly gained popularity and developed a thriving developer community, becoming a widely used tool for analyzing networks and teaching network engineering.\n- The success of Wireshark can be attributed to meeting the needs of users, educators, and developers, as well as maintaining a solid infrastructure and support system.",
    "hn_title": "Wireshark Is 25: The email that started it all and lessons learned along the way",
    "original_title": "Wireshark Is 25: The email that started it all and lessons learned along the way",
    "score": 324,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginWireshark Is 25: The email that started it all and lessons learned along the way (wireshark.org)324 points by ingve 1 day ago | hide | past | favorite | 69 commentswolrah 20 hours ago | next [\u2013]Wireshark is the proverbial hammer that makes all networking problems look like nails. Even if there's a more specific tool available there's a good chance I can swing Wireshark at the problem and figure it out.It continues to blow my mind how many people there are in the world who consider themselves networking professionals but have never used or do not understand Wireshark. It is possibly the most important tool for actually understanding what's really happening in your network, without it you're effectively blind to so many things.Just yesterday I used it to troubleshoot a weird behavior in a recently upgraded Asterisk/FreePBX system which would have probably taken me days to guess my way through without packet captures, but with them I was able to see clearly what was happening on the network and then track that back from there.Congrats to everyone on the Wireshark team on 25 years of making network troubleshooting infinitely easier! I would 100% not be where I am today without it.replyseiferteric 19 hours ago | parent | next [\u2013]It's like a debugger for networking, and surprisingly many programmers don't know how to use debuggers either.replyx86x87 17 hours ago | root | parent | next [\u2013]It's more like strace but yeah.replyauguzanellato 16 hours ago | root | parent | next [\u2013]And a lot of programmers don\u2019t know about strace eitherreplymaximus-decimus 16 hours ago | root | parent | next [\u2013]I can confirm, I didn't know about strace until this very moment. Looking at it, it basically only intercepts system calls? How often is that useful? What do people use it for?replyTristanBall 11 hours ago | root | parent | next [\u2013]It answers, or at least gives the definitive first clue behind a huge number of slow downs or apparent hangs, given how many of those are actually blocking resource waits or retry loops gone mad.It's probably most useful to sysadmins working with binaries, or even if you do do have the source, it's usually a shorter path to the solution for any app/os interaction problem.It's useful for certain classes of optimisation and tuning, because it will give timings and aggregate timings.I'll use it for things as simple as \"where is this program reading it's config files\" - often useful when doco is poor and/or there are multiple config locations selected by conditional logic.There's an \"ltrace\" as well, for share library tracing, although I've personally found that less useful - bugs that that shows are more likely to be code/logic problems rather that os/infrastructure interaction - which is to say, usually outside my job scope.On commercial unix, the equivalent to strace is truss, and it's been around forever.Like many, wirewhark, strace/truss are my go-to tools for a huge amount of troubleshooting.replyaseipp 14 hours ago | root | parent | prev | next [\u2013]Program shits itself randomly during execution, crashes, and doesn't tell you why. strace it. Oh, it turns out it's trying to execve() a binary that doesn't exist on the system, which wasn't documented as a dependency, so I didn't install it. Fixed.Lots of little things like that. Why is this program acting slow at startup when it should be fast? Oh, because it's opening and timing out on a socket connection with an unusually long timeout. Et cetera...replymitchs 15 hours ago | root | parent | prev | next [\u2013]The most fun I've had with strace was debugging a 3-process deadlock. An snmp daemon was blocked waiting for a cli child process to finish, the cli was waiting for a response to a message on a socket it had open with a routing protocol daemon, which was waiting for a response from the snmp daemon.It is also a great way to figure out why programs without useful debug output die. Ie. after a program opens and reads a config file it doesn't like, it starts cleaning up and exits.replyjeffreygoesto 15 hours ago | root | parent | prev | next [\u2013]I recently fired it up to quickly check which headers a crosscompiler used on a specific compilation unit. strace, grep, sort, done. I also use it as first check if something seems to hang. Sometimes you can see lock files trying to be acquired or access to wrong paths.replyklysm 4 hours ago | root | parent | prev | next [\u2013]It lets you see what a program is doing, where doing means any \u201ceffects\u201d of the program that touch the system. Want to see whatS happening to files? You can strace for certain operations to certain files. Weird shitting the bed involving IO? Strace will illuminate the problemreplypdw 13 hours ago | root | parent | prev | next [\u2013]`strace -f -eopen,openat` to see which files a programs opens. Very often useful, even if just to check which config file(s) a program reads.replyx86x87 13 hours ago | root | parent | prev | next [\u2013]it's useful when everything else you've tried failed and you have no clue what is going on. it's extremely helpful in figuring out why a program hangs or crashes. it's a good tool to have in the toolboxreplyel_benhameen 19 hours ago | parent | prev | next [\u2013]Do you have any favorite sources for really understanding Wireshark? I\u2019m not a networking professional per se, but I\u2019m network-adjacent and I\u2019ve dabbled in Wireshark from time to time. I can see the power, but it\u2019s also one of those tools that\u2019s totally overwhelming when I first approach it unless I have a very small, very specific problem. Or is it one of those tools that you learn as you need it?replybaby_souffle 19 hours ago | root | parent | next [\u2013]There\u2019s a million little features and tricks you can do but you\u2019ll never stumble into them unless you\u2019re actively googleing \u201chow do I \u2026\u201d.You might look for some pcap based CTFs with walkthroughs to get exposure to some of the more unique things you can do.Just letting it run for a few min on your router and then powering a device up can also yield some interesting captures\u2026replywolrah 19 hours ago | root | parent | prev | next [\u2013]Unfortunately I can't really help there, I'm a \"learn by doing\" type of person who just jumps in the deep end and hopes he figures out how to swim.Most of my learning was just \"capture the problem happening, capture what happens when it works right if possible, open up the relevant RFCs, then try to understand what's different and why.I work in the VoIP industry so I'm dealing with a lot of NAT problems (insert rant here about lazy ISPs that still haven't enabled IPv6 on their networks) and my main protocol (SIP) is heavily inspired by HTTP and as a result is more or less human readable plaintext, so it was a relatively easy learning curve to just have Wireshark open on one side of the screen and the relevant RFCs on the other side.All I can really say is have a problem you want to solve and start from there.replyel_benhameen 19 hours ago | root | parent | next [\u2013]Sounds reasonable to me, thanks. That\u2019s how I always end up learning, but sometimes I wonder if there\u2019s a better way.replyzvmaz 18 hours ago | parent | prev | next [\u2013]> Just yesterday I used it to troubleshoot a weird behavior in a recently upgraded Asterisk/FreePBX system which would have probably taken me days to guess my way through without packet capturesDo you mind sharing with us what was the problem and how you solved it with packet captures, if you have time? A blog post would be very interesting too.replytoast0 17 hours ago | root | parent | next [\u2013]For something like this firefox bug [1], getting down to pcaps helps determine where the problem is. Client is spinning on a request and server doesn't know about it could be a server problem or a client problem or a network in the middle problem.In this case, the problem was the client wasn't actually sending the request, and with a sizable request that's visible even without decoding the https; although to be totally clear on what was happening, decoding was needed.I've also debugged issued in remote networks where iirc, connections were being reset by some equipment local to the user. Seq/ack sequencing showed the resets were in response to a specific client sent packet and the timestamps showed it was impossible for that to have come from anywhere but equipment near the user.For this bug [2], it took a lot of luck and patience to get a good capture, but once I did, the immediate problem became obvious: the machine I controlled was getting an icmp needs frag but DF set at the same mtu it was already using, and responding by sending the whole sendqueue at once, packetized to the new MTU that was the same as the old one. There's actually three problems here: a) there's no reason for the other side to send this packet (I found this is an already fixed linux bug with forwarding and large receive offload, but no way to contact the administrator of that router), b) our side shouldn't resend the whole sendqueue when the mtu changes, c) if the mtu didn't change, then there's no need to take any action. We only fixed c, but that solved the major problem: these resends would trigger more resends and we'd have periods of unavailability as the network was really busy.This is pretty common when looking at wireshark; unless you work somewhere with full control of all clients and servers and a very network aware developer team, you're going to find lots of non-optimal or semi-broken stuff, and you've got to ignore it and focus on the majorly broken bit.[1] https://bugzilla.mozilla.org/show_bug.cgi?id=1740856[2] https://reviews.freebsd.org/rS288412replylicebmi__at__ 14 hours ago | root | parent | prev | next [\u2013]I had to troubleshoot an issue where several network routers restarted in group without a cause but only when connected to the big wan. The problem was a network discovery software which when poorly configured, would send ssh connection attempts to the management interface and a bug on the specific firmware would crash the router.A network capture was the only good clue.replywolrah 14 hours ago | root | parent | prev | next [\u2013]I'm not much of a blogger, but here's the short version. If anyone happened to be on #freepbx yesterday morning they might already have seen this.I had just upgraded and migrated one of my clients from an on premise FreePBX system that was a few years out of date and running on a repurposed desktop computer with a failing fan to a brand new instance running on a VPS. Everything was working fine with basic phone functionality, but their main ring group was taking a few seconds to stop ringing when answered. Calls would ring in to all phones effectively simultaneously as expected, but when someone answered the call certain phones kept ringing for almost four full seconds after that point.In the past I had seen similar behaviors on AT&T DSL caused by their mandatory modem/router device having an anti-flood filter enabled by default which saw a bunch of nearly identical UDP packets hitting at once and dropped them after the first few. This site has cable internet through a dumb modem so I knew it wasn't that, but they had recently had their IT side taken over by a new company who put in a new firewall so that was a plausible answer.Their IT however had been taken over from us so I wasn't about to go accusing them of getting it wrong without strong evidence. I'm also just that kind of person, I hate when someone blames me or my gear for problems we're not causing so I do my best to never be that guy either. I'll waste an extra few hours of mine any day of the week to be sure I'm not accusing someone else of getting it wrong without a reason.I fired up sngrep on the server, waited for a call to come in, and saved all the SIP sessions that resulted. Download that file, load it up in Wireshark, and I see that while the INVITE messages to start ringing all went out more or less simultaneously (27 phones in ~5ms) the CANCEL messages that stop them from ringing once one answered were sent out sequentially, with the PBX waiting for the first one to respond and confirm it had stopped ringing before sending the next. Clearly this wasn't right, and it obviously wasn't a problem with the firewall either.At that point I started looking at the Asterisk logs and saw that an AGI script was being run for each line that was ringing which wasn't there previously. That script was associated with a new FreePBX module for missed call notifications which was installed but unconfigured on the new server. It didn't indicate it was doing anything in the UI, but it sure seemed to be doing something in the logs.I uninstalled that module and the next call all the CANCEL messages went out in ~5ms just like the INVITEs. I then filed a bug with FreePBX documenting what happened because I'm pretty sure it's not expected or desired for simply having that module installed to cause massive delays in ring groups.---In this case the packet captures demonstrated conclusively that the problem was on the server itself and not in the network. If the capture at the server had looked reasonable my next step would have been to have the IT vendor capture traffic on their firewall at the same time as I was capturing at the server so we could compare and see if it's getting messed with along the way, but here it was not necessary.Like toast0 mentioned, captures help you narrow down where the problem is.replyhartator 16 hours ago | parent | prev | next [\u2013]Fiddle is awesome too.replyprotonbob 15 hours ago | root | parent | next [\u2013]Fiddler classic is what I always go for.replydrmpeg 1 day ago | prev | next [\u2013]Back in 1983, when Ethernet was still fat coax and vampire taps, I was working at a military contractor in Silicon Valley. We had built an Ethernet bridge product that linked the DECnet LANs at DSCS (Defense Satellite Communications System) ground stations around the world (over 9600 bps encrypted circuits).As part of the code, I wrote a packet dumper that put the Ethernet card (a Multibus card from a company called Exelan) into promiscuous mode. It didn't have a dissector like Wireshark, but just being able to dump raw packets in hex to a terminal was a huge advantage for debugging networks.I love Wireshark and it's one of the first things I install on a new system.replyomginternets 22 hours ago | parent | next [\u2013]How did Ethernet go from fat cables and vampire taps to RJ35? Is it still exactly the same protocol (is that even the right word?) as it was back then?I really need to dive deeper into networking\u2026replyHankB99 21 hours ago | root | parent | next [\u2013]> How did Ethernet go from fat cables and vampire taps to RJ35?Layers. The physical signaling is vastly different but the content that rides on it can remain the same. If you study the OSI model (https://en.wikipedia.org/wiki/OSI_model) you will know more about it than me.I don't know how faithfully modern (or ancient) Ethernet follows this model - it might predate this work. Some layers might be blended for the sake of efficiency, but there are definitely layers.replyrubatuga 20 hours ago | root | parent | next [\u2013]The layer with MAC addresses is the most successful part (layer 2)replyremram 19 hours ago | root | parent | next [\u2013]How do you measure \"success\"? Everywhere you are running L2, you are running L1, so it's just as common.L2 switching also requires the Spanning Tree Protocol, which is definitely not a well-liked part of the stack.replygizmo686 2 hours ago | root | parent | next [\u2013]There are a lot of L1 layers that have been used under Ethernet. There were several revisions of the coaxial cable used for ethernet before we switched to the rj45 terminated twisted copper cable everyone thinks of when you say \"ethernet\". Again, there are several iterations for the twisted copper physical layer.More recently, we also run ethernet over optical fibers. The varience within the fiber family of cables is probably greater than the variance in either copper or coax.Ethernet has also been run over power cables.replytoast0 17 hours ago | root | parent | prev | next [\u2013]At 10mbps half duplex, the protocol is nearly the same. Just twisted pair uses differential pair signalling and and coax uses a shared ground and high or low on the center conductor relative to the shield (IIRC). And twisted pair relies on a hub to create a bus. From there you go to 10M/full duplex where the rx and tx pairs are fully separated so collision detection can be disabled.100base-tx increased the symbol rate, added speed and duplex negotiation (layered into the existing link pulse signaling), but otherwise kept things the same; you can even run a 100base-tx hub.1000Base-T is a wide departure at the signalling level; all 4 pairs are used simultaneously, bidirectionally, the symbol rate is the same as 100base-tx, but each symbol carries more bits. But the ethernet frames are pretty much the same. (Larger frames started appearing around the same time as gigE, as I recall, but that might not be accurate)replytrelane 20 hours ago | root | parent | prev | next [\u2013]> How did Ethernet go from fat cables and vampire taps to RJ35?At least, at university: students like me that got hired cheaply and rewired everything. :)That was not a fun summer, but I learned a lot.> Is it still exactly the same protocol (is that even the right word?) as it was back then?I would be surprised, given that coax is equivalent to 3 conductors, and catX cables have 8. And that's before we get into fibre. I would expect they have sime high-level protocol (frames etc.) that gets mapped onto the physical signaling, but I don't know much about that (resource suggestions welcome!).I do know that going from a broadcast medium to switched point-to-point is a lot more efficient etc.Plus the taps were notoriously unreliable (variable connection quality). And would cause reflections in the cable as well, which is fun.replyjohannes1234321 19 hours ago | root | parent | prev | next [\u2013]Yes it is the same protocol. (Minor revisions change details, jumbo frames etc)The cables form the physical connection, on that Ethernet defines a way to determine who may send a message (essentially anybody can send while quiet, and if a conflict is detected everybody retries after a random time)The big thing which changed is that we are often using switched networks, instead of all nodes attaching to the same cable, but that's a change in a higher layer.Ethernet Designers where smart not tontine the spec to properties of a specific material for transport, but abstract ether where signals travel.replyK0balt 19 hours ago | root | parent | prev | next [\u2013]I know I\u2019m gonna get schooled for asking, but did you mean RJ45/48? Or is RJ35 something that just snuck in under my nose?replyhotpotamus 21 hours ago | root | parent | prev | next [\u2013]Every once in awhile you'll hear the odd story about someone tracking down a bottleneck in their network and finding an old 10 or 100mbps ethernet link somewhere. I doubt it happens much anymore, but your 10 gigabit gear should still be able to talk to your 10 megabit gear no problem, which I do find impressive.replymyself248 20 hours ago | root | parent | next [\u2013]I've done it deliberately. I had to test a cellular device from somewhere in Asia, but I'm in the US. The Asian provider had sent us a femtocell with developer's firmware that bypassed the GPS check at startup, which would create a little bubble of their coverage in our RF test chamber, and we could put the DUT in the same chamber and do the testing.Trouble is, the femtocell wanted a network connection, and our RF chamber didn't have an RJ45 passthrough. Some emails got sent, the chamber vendor could sell us a new passthrough module but it was on backorder, ETA two months or something.So the following evening, I swung by the e-waste recycler where I used to volunteer years prior, which meant I could just give the proprietor a wave and then let myself into the back room and pick the pile. And sure enough, I found a couple of 8-port 10base-T ethernet hubs, with 10base-2 connections on the back for connection to a coax segment. I talked him up to twenty bucks so I'd have an expense to submit; the company did not deserve to get this for free.Back in the RF lab the following day, it was a trivial matter to convert the BNC connector on the hubs to the N connector in the chamber wall, locate one of the hubs inside the chamber, and connect the femtocell to it. The one outside got the internet connection, which had been running at gigabit speeds but now found itself negotiating at 10/half! (I wonder if the campus networking folks get alerts when that happens. Because it's almost surely not what's intended, unless I'm around.)The younger techs in the lab mere MYSTIFIED at this exotic hardware that could send Ethernet signals over coaxial cable! That must be expensive! How did you come up with it so fast! Whoever made that must've had this application in mind, but what a niche application! Amazing!replyMacha 21 hours ago | root | parent | prev | next [\u2013]More often it's that a failing cable has caused a connection to renegotiate to a lower speed than actually 10/100 equipment these days thoughreplyrejectfinite 22 hours ago | root | parent | prev | next [\u2013]Study CCNAreplygsich 22 hours ago | root | parent | prev | next [\u2013]Yes. Truly a standard ahead of its time.replygeraldcombs 17 hours ago | prev | next [\u2013]Wireshark's creator here. Waking up to everyone's kind words and reminiscing made my morning. Thanks!replyjiggawatts 10 minutes ago | parent | next [\u2013]Thanks for all the troubles shot with Wireshark over the years!Sadly, I've largely stopped using it because it appears to be unable to keep up with the data rates typically seen from servers these days. I believe the analysis is single-threaded, and doesn't seem to cache anything either. It struggles with captures \"mere\" gigabytes in size, which is just seconds on a 10 Gbps link.replyiamflimflam1 4 hours ago | parent | prev | next [\u2013]You\u2019ve saved my bacon on too many occasions to mention. Thank you.replyktm5j 17 hours ago | parent | prev | next [\u2013]You deserve it! You made something awesome that's been incredibly useful for a ton of people, myself included!replyajsnigrutin 1 day ago | prev | next [\u2013]Wireshark is like a multimeter in electronics world.... the whole world can run without having one, but once something fails, without it, you're fucked.replygeek_at 23 hours ago | parent | next [\u2013]wireshark and tcpdump have helped me find so many strange things in our campus networkreplynikau 7 hours ago | root | parent | next [\u2013]Its even better once you learn how to run tcpdump session over ssh dumped into a pipe, and then use wireshark locally via the pipe to get a nice gui for the remote capture.replyRoark66 20 hours ago | prev | next [\u2013]Thank you. It has been an indispensable part of my work from the very beginning. Tcpdump was fine, but being able to right on a packet and do \"follow TCP stream\" then see the entire conversation in a second was a game changer. Same with the \"right click->filter out this stream\".Also the fact ethereal/wire shark could read files saved by Tcpdump meant I could ssh onto a remote server, fire Tcpdump, run wire shark in a client and when something failed I was able to look at the network stream \"from both ends\". It saved me hours and hours, from dodgy ISP Nat being evident at first glance, to misconfigured MPLS networks being provable (no more the routing team could just say : it looks good for us). No, there was proof... I bet countless people continue having the same experience with this software :-)However, I have to correct one statement made in the article. Ethereal wasn't the first free gui network packet analyzer. There was a Microsoft tool I forgot the name of that was available even in Windows NT days, perhaps \"netmon\"? It was a long time ago. It was free and it predates ethereal. It only worked on Windows and it used it's own file format.replymuststopmyths 17 hours ago | parent | next [\u2013]>There was a Microsoft tool I forgot the name of that was available even in Windows NT days, perhaps \"netmon\"?Network Monitor, also called netmon (or Bloodhound internally), which actually had a documented (maybe unsupported IIRC, but still easy to tap into) API. I wrote a tcpdump wrapper around it, before Ethereal was a thing. The API, and hence netmon, became invalid with the \"next-gen\" TCP stack of Longhorn/Vista.Eventually, MSNA (Microsoft Network Analyzer) came along, which worked on ETW and was able to analyze network and other ETW traces. You could write handlers for any protocol in a supported DSL. You could even make it parse log files and filter/analyze the data.The New Microsoft being what they are, they killed MSNA because it was too powerful and useful to Windows developers. It probably wasn't used by a lot of people, but if you knew how to use it it was one of the most powerful analysis tools of its time.Edit: Microsoft Message Analyzer, not Network Analyzer.replyc0nsumer 17 hours ago | root | parent | next [\u2013]I believe it was Message Analyzer, and what was super cool was its ability to correlate ETW stuff. So you could literally see the interplay between... say... a webserver log, an OS level NIC driver log, and a network capture.I still don't get why MS stopped its public distribution, although I do know it was pretty buggy as released...And yeah, netmon is great. I still use it when I want to filer Windows captures on PID, since Wireshark won't do that. (Even though netsh or pktmon -- built in Windows tools for recording captures -- have it in the header...)replymuststopmyths 16 hours ago | root | parent | next [\u2013]you're right Message Analyzer. My mistake.replynikau 7 hours ago | root | parent | prev | next [\u2013]I recall having to do a native windows packet dump and it reinforced how windows and Linux are opposites.Linux has a lot of friction for normal users, but easy for sysadmins.Windows has a lot of friction for sysadmins, but easy for users.Thankfully the tide seems to be shifting finally with simple succinct tools like curl being installed by default.replydylan604 19 hours ago | parent | prev | next [\u2013]>It was free and it predates ethereal. It only worked on Windowsso...was it really free? sounds like it came with the OS that you paid forreplyarafalov 21 hours ago | prev | next [\u2013]This tool (Ethereal at the time) was absolutely invaluable to my job as Senior Tech Support of Weblogic family of products. I even got clients to run it and was able to provide solutions like \"your large JDBC connection pool had all its connection silently dropped by a network firewall (that client was not aware of) and that's why you having 1 hour transaction delay on first one in a morning. Every pooled connection had to timeout and reset\". And \"Internet Explorer would abort a TCP connection for already-cached resource and that generates non-standard network level errors on your IBM server' Weblogic installation\"I lost half of my hair on that job. Without Ethereal, I am sure I would have lost all of it and a lot more of my sanity too.replybeardedwizard 20 hours ago | prev | next [\u2013]I used wireshark every day for 10+ years supporting load balancers in customer networks. Between pcaps and core dumps, it was some of the most interesting data to work with. Learning libpcap and eventually writing my own version enabled me to pivot out of tech support for the product into development. I joined as a support engineer and left as a principal software engineer writing the code I was previously supporting. Wireshark and gdb let me teach myself so much, I never had to go to college.replyHikikomori 19 hours ago | parent | next [\u2013]I like to call it the \"I don't know what is wrong so it must be the network\" problem.replyhansc 23 hours ago | prev | next [\u2013]Such a great tool and 100% free. Use it often to debug network issues and see where devices connect to. Like someone else said: a multimeter for networks.Also used it to learn about WiFi connection setup with acces point. Can see all the beacon packets and WiFi packetsreplymartyvis 22 hours ago | parent | next [\u2013]I was a very early adopter of Ethereal. My team had a Sniffer PC, but either it was being used by someone else or it didn't adequately decode protocols.replyc0nsumer 21 hours ago | prev | next [\u2013]Gosh, I feel old. I remember when Ethereal was released and it got me excited. I've sure learned a ton since then thanks to it, and solved a /lot/ of problems. It really changed the world of network traffic analysis, moving network captures away from the world of special laptops and tools (needing to ask the network team to schedule and do a capture) to something that any competent tech could grab.replynunez 21 hours ago | parent | next [\u2013]Which is crazy because wireshark used libpcap, which does like 90% of the work of building the packets for you; the other 10% is parsing headersreplyc0nsumer 18 hours ago | root | parent | next [\u2013]It's the header parsing and post-capture filtering and analysis that is the heavy lifting, though.Just like lots of things, you can collect all the data you want... But getting an actionable result is the trick. Wireshark does that.replyquijoteuniv 1 day ago | prev | next [\u2013]People often have this fantasy about changing the world\u2026 and then there is people that just changes the world.replyStayTrue 20 hours ago | prev | next [\u2013]Absolutely terrific software. One good memory is being stuck at a client site (20 years ago) trying to figure out an interop issue with our network equipment. In the two weeks I was there I found it helpful to write a protocol decoder plugin and it was easy work. In the end it was our bug, a bitmask applied for select() was not removed when the implementation changed to epoll() ... in essence a 1-bit memory corruption error that could have very delayed consequences. Funny what memories stand out.replyWastingMyTime89 20 hours ago | parent | next [\u2013]I admire your ability to find joy in these things.At the beginning of my career, I once spent a week in a secure facility trying to understand an annoying network bug using tcpdump because we weren\u2019t allowed to install wireshark. The whole thing turned out to be a combination of the worst bug I have ever seen in a standard library in our decade old version of GNAT (Ada lib - admittedly it had been corrected seven years before) and an ARP misconfiguration.The whole week was awful and largely responsible for me moving on to greener pastures. It takes a special kind of character to enjoy these things.replyHankB99 20 hours ago | prev | next [\u2013]It's not something I use often, but the value of it to me is that I can \"hook it up\" and immediately see what's going on. The generally intuitive interface and the way it decomposes packets make it so easy to pick up and use.25 years of effort has produced a really useful tool.replykrylon 18 hours ago | prev | next [\u2013]Both for learning about networks and in my work as a system and network admin, I found Wireshark (and Ethereal before that) one of the most useful tools around. I once diagnosed a networking problem a friend had by getting him to install it on his laptop and record a packet dump of his traffic, then send it to me via email.replyHikikomori 21 hours ago | prev | next [\u2013]As a network engineer I've used Wireshark weekly for most of my career, but not as much anymore as we moved to the cloud. The ISP I worked for paid for a Wireshark training [0], though I didn't learn anything new we did set up a profile that helps a lot with troubleshooting, still use it 8 years later.Wanted to learn Go so recently started working on a CLI packet capture tool like tcpdump that parses packets received on a raw socket. Got support for ethernet, ipv4, icmp, arp and udp so far.[0] https://www.bettydubois.com/replynunez 21 hours ago | parent | next [\u2013]A few folks and I built one in C++ for our capstone project at Stevens Tech in 2009. Frontend was GTK. It was much slower than Wireshark, but I was surprised by how easy it was to parse the packets (for normal packets speaking the usual protocols anyway)replyHikikomori 19 hours ago | root | parent | next [\u2013]It's a fun way to learn networking or a language as you have to do low level parsing and you have to deal with things like endianness of larger header fields. It does get tedious to write types and parsers for each protocol and you're not learning anything new after doing a few of them so I started using Chatgpt to generate code and tests for me which work surprisingly well (also paid for copilot but didn't find it very useful so far).replytempodox 1 day ago | prev | next [\u2013]Happy anniversary! And thank you for all the great work.replyhansc 15 hours ago | prev | next [\u2013]Also fun debugging VoIP traffic to the local network, and seeing dns-queries from collegues PCs on the network, then asking them: Are you visiting xyz-webpage, and seeing their reaction!?replynabogh 1 day ago | prev [\u2013]Can't say I use wireshark much day to day but it has been an incredible learning tool over the years. What a great project.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- Wireshark is a valuable tool for understanding and troubleshooting network issues.\n- It is often compared to a debugger for networking or strace for networking.\n- Many programmers and networking professionals find Wireshark essential for their work."
  },
  {
    "id": 36737567,
    "timestamp": 1689434157,
    "title": "Our paying customers need X, when will you fix it?",
    "url": "https://twitter.com/maximilianhils/status/1680193548212228097",
    "hn_url": "http://news.ycombinator.com/item?id=36737567",
    "content": "Due to Twitter's new pricing structure, we made a difficult choice to restrict the option of unrolling tweets on the web to Premium members only. You can still unroll tweets for free by visiting Twitter and replying to the tweet with \"@threadreaderapp unroll.\" We appreciate your understanding!Thread ReaderOne-click sign-up and loginSign up or login to access your unrolled and bookmarked threads (or PDF archives if you are a Premium member!)Login with TwitterLogin with EmailLogin above to accept Thread Reader App'sTerms of Service and Privacy PolicyHelp | About | TOS | Privacy | Twitter Files",
    "summary": "- Twitter has implemented a new pricing structure that restricts the option of unrolling tweets on the web to Premium members only.\n- Non-Premium members can still unroll tweets for free by visiting Twitter and replying to the tweet with \"@threadreaderapp unroll.\"\n- This decision has been made by Thread Reader in response to Twitter's new pricing structure.",
    "hn_title": "\u201cOur paying customers need X, when will you fix it?\u201d",
    "original_title": "\u201cOur paying customers need X, when will you fix it?\u201d",
    "score": 319,
    "hn_content": "- IBM is requesting a timeline for a new release of the mitmproxy software to communicate to their customers.\n- The request itself is seen as reasonable, but the way it was phrased in the email comes across as entitled and demanding.\n- The maintainer of mitmproxy does not owe any support or release timeline to non-paying users.\n- The issue revolves around the misconception that the software has a security vulnerability when it's actually a dependency that the project doesn't use.\n- The conversation highlights the challenges of maintaining open-source projects and dealing with entitled requests from large companies.\n- The importance of clear communication and respectful interactions in the open-source community is emphasized.\n- More experienced individuals in the industry share their thoughts and experiences with similar situations.\n- The incident sparks a broader discussion about corporate entitlement and the need for appropriate licensing and usage restrictions in open-source projects.- Users are discussing the usage of mitmproxy and its relevance in information security.\n- There is speculation about the sender's address being from @us.ibm.com, and it sparks surprise.\n- The discussion shifts towards IBM's ownership of Red Hat and their culture of using open source software without giving back.\n- Some users argue that there are open source GitHub repositories owned by IBM that counter the claim of not giving back.\n- The conversation delves into a debate about entitlement, customer expectations, and the responsibility of open-source maintainers.\n- Some users suggest that paying for support or acquiring a commercial license could be a solution for business critical issues.\n- Others argue for a mixed licensing model where software remains free up to a certain revenue threshold and then requires a commercial license.\n- The conversation explores the ethics of open source projects and the challenges faced by maintainers in terms of expectations and demands.\n- A user raises the possibility of forking the repository to build and use the product/library independently.\n- Some commenters express their frustration with the interaction, emphasizing the unhelpful nature of the responses and the perceived lack of communication and empathy.\n- The discussion touches on the concept of forking and maintaining internal company releases.\n- The idea of a support contract for the software is brought up as a potential solution.\n- There is a debate about the propriety of the initial request for a release timeline and the perceived response as extortion.\n- The exchange between the individuals involved is critiqued, with suggestions for better communication strategies and understanding.\n- The conversation highlights the challenges of interpreting tone and intent in written communication.\n- Users express different opinions on the appropriateness of language and responses in the conversation.\n- The role and responsibility of open source maintainers are discussed, along with the need for clear communication regarding contributions and expectations.\n- The importance of maintaining a respectful and constructive dialogue when discussing open source projects is emphasized.\n- The conversation raises questions about entitlement, responsibility, and the realities of open source development.",
    "hn_summary": "- IBM requests a timeline for a new release of mitmproxy software for their customers, sparking a conversation about entitlement and demands from large companies in the open-source community.\n- The issue revolves around a misconception of a security vulnerability, highlighting the challenges of maintaining open-source projects.\n- The discussion delves into IBM's ownership of Red Hat and the culture of using open source software without giving back, raising questions about entitlement and responsibility in open source development."
  },
  {
    "id": 36738347,
    "timestamp": 1689438232,
    "title": "Another World ported to FPGA",
    "url": "https://github.com/sylefeb/a5k",
    "hn_url": "http://news.ycombinator.com/item?id=36738347",
    "content": "a5k: How I remade Another World / Out of This World in hardware on the UP5K FPGAJust want to test?Simulation: Checkout the repo, install Silice put the game data in GAMEDATA and from a command line type in make simul1 (no need for an actual FPGA, this will run the intro in simulation).Hardware: Current supported boards are the icebreaker + VGA PMOD, mch2022 badge and ULX3S over HDMI. Pre-built bitstreams are included but the game data is needed in GAMEDATA so the data packs can be prepared. Jump here if you'd like to test on hardware first.  ForewordThis project is my personal homage to Another World. This game is not only a graphical and gameplay masterpiece, it is also a technical marvel: The entire game runs on a beautifully designed Virtual Machine (VM) that calls only a blitter and rasterizer to produce the graphics in four framebuffers! (what's a vm, blitter or rasterizer anyway?)The VM is quite minimalistic, and both the blitter and rasterizer are good candidates for hardware designs. Therefore it was very tempting to create a hardware implementation of the entire framework : no standard CPU, a truly native hardware version of the Another World VM, blitter and rasterizer. While I'll keep referring to the \"VM\" in the following, keep it mind it will become an actual custom processor, implementing on an FPGA all of Another World opcodes in hardware. This is not a 6502, not a Z80, not a 68000: this is an Out-Of-This-World-chip!As I started to explore hardware remakes of various game render loops (e.g. Wolfenstein3D, Comanche, Doom, Quake), the idea of doing a hardware version of Another World was very tempting. What really made it click is when I realized, reading @fabynou excellent blog, that the four framebuffers were 4-bits 320x200. That meant 128KB of data in total which precisely fits the Lattice UP5K SPRAM! This was too good to pass on!SPRAM is a special fast memory embedded within the FPGA, more on this later.The entire design revolves around four major components:The VM that becomes an actual processor implemented in hardware,The (hardware) blitter that copies data between framebuffers,The (hardware) rasterizer that draws polygons in framebuffers,The SOC that glues everything together, adding the display refresh.The UP5K Lattice FPGA runs comfortably at 25 MHz. This makes it easy to meet game performance requirements (recall the original was running on 7-8 MHz machines). The challenge is more in terms of fitting memory and FPGA resource requirements. Indeed, an FPGA is a grid of programmable logic cells with programmable interconnect. There is a fixed number of cells, and even though a design could theoretically work in simulation it might not fit a given FPGA. The 'logic' - think of it as the code - has to fit within a budget. This budget is often expressed in number of 'LUT's which are the small programmable units of the FPGA fabric (LUT stands for Lookup Up Table, this is a simple yet powerful concept behind FPGAs).To give you an idea, 1K LUTs lets you do a fairly capable dual core RISC-V 32 bits CPU and 5K LUTs (plus some DSPs) a SOC that can run Doom and a CPU+GPU capable of perspective correct 3D rendering (Comanche, Doom, Quake). Thus it seemed 5K LUTs ought to be enough for Another World.Overall it really feels the design wanted to fit 5K LUTs, this is more or less where it ended without too much optimization effort. But it could, for sure, be a lot smaller.However the framebuffers really are the 'hot piece'. They need to be accessed quickly for both display (screen refresh) and rendering (raster/blitter). It just happens that the UP5K features 128KB of SPRAM memory, a memory that can be accessed (read or write) in 1 cycle. And it features a write mask to write the memory by groups of 4-bits (nibbles), exactly matching a 4bpp palette. This makes it a perfect candidate.So, let's get started!If you'd rather skip the details and just want to run the design, jump here.The making ofWhile I do not assume any prior knowledge in the following, let me recommend Fabien Sanglard excellent blog series on Another World before we get started. His posts give an in-depth overview of the engine. A lot here is based on Fabien's fork of the C++ port by Gregory Montoir.Getting startedSo how did I get started on this all? At first, the task of doing a remake can be intimidating. But as always I started from the data. Before anything you have to understand the data, how it is loaded, how large it is, how it flows through the code at runtime and how you can load it into your own version. This was greatly facilitated by the C++ port.My first task was to check whether there were any dynamic loading of data during the game execution. A favorable scenario is to have a single data pack comfortably stored in ROM -- in our case the SPI-memory (see note below) -- and avoid any sort of runtime dynamic allocation. Because we won't have malloc or any sort of such luxury here :-D. By the way, data here means both the instructions for the VM, the graphics and sound. The graphics are (almost) entirely polygons, stored in a specific way that makes the rasterizer simpler. More on this later.We are targeting FPGAs, and most boards typically feature a SPI-flash memory. This is a relatively large memory (e.g. 16 MB) that is slow to write to but quite fast to read from (think 10-20 cycles for random access). So from the design point of view it acts as a ROM. It also retains its content when powered off.To track the data I started to dive into the C++ port. The VM is easily identified as vm.cpp, which essentially contains one method per operand plus the global orchestration of game threads (only one is active at a time, there is no true parallelism within the VM). From the code, we can see that there are 256 VM variables (VirtualMachine::vmVariables) as well as 64 records for threads (each record is 64 bits : two 16 bits data fields in threadsData, one 16 bits call stack pointer in _scriptStackCalls and two 8 bits active fields in vmIsChannelActive). This is already great news, as this means the entire VM state could fit into BRAMs -- another special type of memory distributed across the FPGA fabric that answers to read/write requests in just one cycle. So think of these variables and thread records as VM internal registers: it means the hardware 'VM' has 256 registers and 64 threads.BRAM, contrary to SPRAM, can be initialized at power up. There are also variants allowing multiple read/write ports, we'll come back to this later. There is however not a lot of BRAM on FPGAs, the UP5K provides 120 Kbits.Back to tracking game data. One operand stands out as being for data manipulation: VirtualMachine::op_updateMemList. Looking at the code, a resourceId is given as operand and loaded by Resource::loadPartsOrMemoryEntry (from resource.cpp). So this is where I started, adding \"printf's\" (the C++ port as a nice logging system that I used) and working my way down the chain. Beyond Resource::loadPartsOrMemoryEntry this led me to Resource::loadMarkedAsNeeded where actual loading occurs from the game resource management system -- exactly the type of runtime system I'd like to bypass :)Looking at the logs confirmed that after an initial loading stage, the op code was no longer called until the end of a long section of the game. In addition, we can see in Engine::init (from engine.cpp) that single game sections -- or parts -- can be launched independently calling VirtualMachine::initForPart. For instance, the famous intro section is the second part (the first is the protection screen). Selecting various parts confirmed that loading indeed occurs only at the beginning of each of these sections.This gave me hope that each game part could be stored into a single, separate game package loaded once at the start of the part. To try this I instrumented the calls to op_updateMemList such that a data pack would be dumped after each call, the idea being that the full pack for the part is dumped on the last call. The start address is Resource::_memPtrStart, after which loaded data is copied by calls to Resource::loadMarkedAsNeeded. The end address is Resource::_scriptCurPtr which advances each time a resource is loaded from Resource::loadMarkedAsNeeded.After obtaining a first data pack, I commented out the content of op_updateMemList and loaded the pack once at the start of the game part, in Resource::setupPart. It worked!To create the data packs the Makefile compiles and runs my fork of the C++ port, which outputs the data package of a part.A data pack contains different sections of data, and their start offset is different for each game part. For now, these offsets are baked into the hardware design, which means we have one design for each game part. Not ideal. I did experiment with loading the offsets from the ROM (code is commented) but this incurs a relatively significant increase in LUTs. Quite sure this can be improved!Time for a 'hardware VM'Once the data pack obtained, I created a first minimalist SOC with SPI-memory and a skeleton of the VM. The overall execution pattern of the VM is to fetch the next instruction operator (a byte) from memory at the program counter pc address, select what to do, load the necessary operands from memory and act upon it. The number and size of operands varies between operators. Many operators only modify the VM state (registers, threads and pc) while others have side effects such as drawing polygons or using the blitter to transfer data between framebuffers.SPI-memory is such that there is a latency to start reading from an address, but once started the data is streamed in every cycle (Quad SPI at 2x the clock). So to avoid a slow byte per byte reading and to amortize latency, I start by fetching some sufficient number of bytes (64) into a small BRAM cache, getting both the operator and all operands that might be required.The overall process of fetching instructions is enclosed into an outer loop that deals with threads. For this I simply reproduced the logic from vm.cpp, adjusting for the use of BRAMs instead of arrays.I first implemented all basic operands manipulating registers, leaving out operands regarding polygons, rasterizer and blitter. The key idea here is that the VM execution is not influenced by what happens in the blitter and rasterizer: these are outputs only from the VM. Thus, having these initial operands is enough to run the VM in simulation and compare its trace to the C++ version. I instrumented both, using __display in Silice to output a trace from simulation. Silice has a verilator framework and SPI-memory controllers that allow for simulation, reading data from a data.raw binary file in the current directory, so I could use the actual data packs in simulation.From there on I spent some time looking a traces, comparing any divergence between simulated and reference C++ versions. Here's an excerpt of a trace (the number between [ ] is the cycle counter):hostFrame() i=0x03 n=0x22d1[  3722016] op_jmp 22cc[  3722096] op_Polygon (vid_opcd_0x80) @0cb8[  3730573] op_pauseThread[thread  3] donehostFrame() i=0x3c n=0x0091[  3730824] op_jmp 007f[  3730904] op_addConst [199]+=   1 (before:   33)[  3730982] op_blitFramebuffer ff (swap:0)[  3731691] op_selectVideoPage ffop_condJmp b[250]:   0 a:   0 expr:0 [  3731773][  3731854] op_copyVideoPage 40 => ff[  3731932] op_pauseThread[thread 60] donehostFrame() i=0x02 n=0x248aGetting to this point was relatively easy, and soon I had a fully functional VM in terms of going through instructions. Only problem, it was not drawing anything!The full VM hardware design is in vm.si.The framebuffersTime to prepare for some graphics! The first step was to set up the four framebuffers.The framebuffers are using the four SPRAM blocks of the UP5K. A SPRAM is similar to a BRAM but larger. Contrary to a BRAM, a SPRAM cannot be initialized with a specific content, but that is not something we need for framebuffers. Note that while BRAMs are very common in FPGAs, SPRAM blocks are rarely found so this is a nice feature of the UP5K.Each SPRAM block is 32KB, with a 16 bits data interface and a write mask allowing to write nibbles (4 bits) independently. This is perfect for us as Another World uses a 16 colors palette (4 bits per pixel) in 320x200 resolution buffers. This makes each framebuffer 32KB, so one framebuffer maps perfectly onto a SPRAM block.The videopage unit in a5k.si implements a framebuffer in a SPRAM block, and four videopage units are instanced (page0 to page3) in the SOC main unit, one per framebuffer.The videopage unit exposes a 4 bits interface with per-pixel addressing, and translates these addresses into 16 bits wide access in SPRAM with a write mask and adequate shifts.This is slightly inefficient as accessing four neighboring pixels requires four accesses, while the 16 bits interface would allow to read/write all four at once. I chose to not optimize this as there is no strong pressure on performance. However, other designs like the hardware terrain project exploit this fully.On the ULX3S the videpage unit is implemented with BRAM, a big luxury only possible on the much larger ECP5 FPGA.The displayLet us first discuss the display controller. As we'd like to target VGA, HDMI or SPI-screens we'll settle on a common interface. We are going to assume that the display controller has the following requirements:A frame is sent by streaming pixels at 25 MHz (the SOC frequency, and as it just happens this matches the pixel clock of 640x480 for VGA/HDMI).Between two frames a blanking interval exists, as indicated by a vblank signal (high in between frames).The display controller thus has to be consistently fed with pixels outside of the blank interval, and this process cannot be paused within a frame. Two framebuffers are special: page1 and page2. They are the one being sent to the display. Why two? Overall one is being sent while the game draws in the other, and the buffers are swapped when ready. This prevents visible blinking while new shapes are drawn, an approach known as double buffering.However, the game sometimes reads from the buffer being displayed. The videopage (and underlying SPRAM) cannot support this: at a given cycle a single read or write is possible. To deal with this situation we have to restrict access to the displayed page, allowing other reads to occur only during the blanking interval, when the screen is not being refreshed. This is detected by setting the variable display_conflict that is used to disable the blitter and rasterizer when they should not access a videopage.The display section accessing the pages can be seen in a5k.si (for VGA/HDMI).Some special BRAMs, called dual-port, support two accesses within the same cycle. Our SPRAMs are hower single port and perform a single access within a cycle. This is actually the meaning of SP- in SPRAM!The blitterNow that we have framebuffers and a display, it is time to look into the blitter. The role of the blitter is simple: it copies the content of one framebuffer to another, or fills a framebuffer with a solid color.The VM has several outputs and one input regarding the blitter, all prefixed blitter_. The VM starts the blitter by pulsing blitter_start high, after setting blitter_src and blitter_dst to the index of one of the four framebuffers. The blitter may be already busy, so before starting it the VM waits on blitter_busy.In simulation the blitter checks that there are no start pulses while busy, and uses __display and __finish to stop. A runtime assert!The VM indicates a color fill by setting blitter_src[2,1] (bit two) in which case the blitter uses blitter_color instead of a source buffer.The blitter unit in a5k.si implements (surprise!) the blitter. The logic is simple: a count goes from 0 to 63999, visiting all pixel addresses for a 320x200 framebuffer. The count is triggered by the start pulse and increases every cycle while enabled is high. The count value is output in src_addr as well as in dst_addr but with a one cycle latency (prev_count). These addresses are used in main to access the pixels in the pages selected by blitter_src and blitter_dst. The one cycle latency is necessary as it takes one cycle to get the data from src_addr into src_data (through the videopage SPRAM). So the data written at dst_addr is the one that was accessed at src_data at the previous cycle.The access logic is outside as it is shared with the rasterizer and display controller, so main acts as an arbiter. Understanding the 4 framebuffers and their interations with the blitter and rasterizer was a difficult part of the project, but I'll skip these details for now.The blitter reports busy high as long as prev_count does not reach the last pixel. When it does, busy goes low. Note how busy also controls dst_wenable; when busy is low no writes can occur, so even though count keeps going nothing changes in the destination videopage anymore.It might seem cleaner to stop count from increasing when done. But this adds extract logic -- and hence uses more LUTs -- without any gain. A typicaly case of hardware design where trying to not do something is more expensive than always doing it.I mentioned earlier that count is only increasing when enabled is high. Looking into main we see that enabled is bound to vblank (see the line blitter blit( enabled <: video.vblank ); ). This ensures that the blitter only copies during vblank, such that the screen controller has access priority. The blitter pauses during display.The rasterizerAlmost done! (famous last words - @sylefeb)Now onto the rasterizer. There are two parts to this. First, how to get polygons from the data package, which is something to do in the VM. Second, the rasterizer itself. I'll focus on the first part and simply explain a nice aspect of the Another World rasterizer that I realized working on the engine. Detailing the rasterizer would go beyond the scope of this write up, but please see my tinygpus and flame writeups for more details on rasterization.Again the crux of the matter is the data, in this case the polygons data. The rasterizer will need to read polygon vertices from somewhere. We can expect that polygons have a small number of vertices, so a BRAM seems again like a good choice. We will be using a dual-port BRAM called polygon in the VM. The dual-port is comfortable as it lets us have the VM always write vertex data on port 1, while the rasterizer reads data on port 0. The rasterizer (visible in the C++ port as Video::fillPolygon) needs a few other things like a translation vector, zoom factor and color, which are also passed from the VM to the rasterizer in all the rasterizer_ outputs.So, now we only have to fill in polygon with vertex data, set the parameters and trigger the rasterizer from the VM, right?Well, sure. Let's take a look. All VM opcodes having bits 6 (0x40) and 7 (0x80) set trigger polygon drawing. The other bits are then used for various options on how to obtain translation, zoom and color (from data or registers, etc.). (See virtual machine opcodes table in @fabynou's blog).In the C++ port the VM then calls Video::readAndDrawPolygon. This function in turn calls Video::fillPolygon (the rasterizer) but it also calls ... Video::readAndDrawPolygonHierarchy.Uh-oh.I don't like to see hierarchy in this name. This smells like recursion... and it is! Video::readAndDrawPolygonHierarchy in turn can call Video::readAndDrawPolygon and so on. So our simple process of gathering the vertices in polygon just took quite a different turn. We are now looking and making a hardware version of this recursive function. Well fear not! We'll do just that!Why is there a recursion here? Many polygon-based graphics engine have a notion of groups so that they can animate entire shapes at once. To create hierarchical animations -- think of a shoulder-elbow-wrist hierarchy -- a group contains polygons but also other sub-groups, each with a local transform. This is exactly what happens here.How do we deal with recursion? We implement a stack! Another BRAM to the rescue and voil\u00e0. The process is in the subroutine readPolygons in vm.si. A BRAM stack called polygonStack is used to keep track of the recursion status. Each time a polygon (leaf) is found it is sent to the rasterizer as soon as it is free (a loop waits on rasterizer_busy in case a previous polygon is still being drawn). Each time a recursive call occurs (node) the calls are pushed on the stack. This proceeds until the stack is emptied.A tricky question is the stack size. I did set it up manually at 128 polygons. At this point we are using quite a lot of BRAM and there is not much left.In my tests the stack size was enough, but I haven't done an exhaustive playthrough yet!Alright, so now we have the VM visit polygons recursively, computing translations and colors. The VM triggers the rasterizer.But the rasterizer is empty ...Drawing polygons!Finally we get to the rasterizer. The rasterizer unit is described in a5k.si and is instanced in the main unit.We are given a list of vertices in the polygon BRAM and have to draw the corresponding polygon in the framebuffer selected by rasterizer_dst.The rasterizer itself does not know which framebuffer is selected -- again the main unit is acting as arbiter on the framebuffers. Instead it outputs pixels by setting pix_waddr, pix_wenable and pix_palid (the color index in the palette).As most rasterizers, the polygon is decomposed into spans going from a left edge to the right edge. So the polygon is drawn one horizontal span at a time, going down vertically.In general a polygon could be concave, producing more than one span along a horizontal line. However, all polygons in Aonther World are convex! This is ensured by construction, by the authoring tool.Nice. But it does get better. Diving into the details of the rasterizer I realized something was not quite right: when arriving at the end of an edge, the rasterizer would go to the next edge on both sides at once, without checking whether the other edge extremity was actually reached. That could only mean that the polygons are specially constructed to have a specific property: starting from a top edge and going down, a vertex always exists on both the left/right side at a same horizontal location (see drawing below).The vertex coordinates for this polygon are below. Note how matching *y* coordinates can be found on both sides:(25,0) (34,4) (41,15) (32,19) - (8,19) (0,15) (6,4) (13,0)^^^^ right side, goes down    ^^^^ left side, goes back upThis is why the rasterizer can be kept quite simple. Still, it has to deal with various cases of clipping as well as sub-pixel precision. But all in all it is relatively simple. My hardware version is very close to the C++ version in Video::fillPolygon, with some adjustments for BRAM access and latencies.One last thing ...Are we done yet? ... Transparency? What d'you mean transparency??Yeah well of course there are details. The engine uses a nice palette trick to give the illusion of transparency (glass, cast light). Some polygons being drawn will only write the top bit in the framebuffer, switching the color already there to a higher part of the palette, giving the illusion of a transparent overlay.This introduces some complexity in the rasterizer. To flip only one bit we need to know the prior value of the pixel at this location. This is why the rasterizer inner loop -- drawing a span -- proceeds in multiple cycles: read the previous value of the pixel, modify it (or override it), and write it back.This is done in a slightly different order in the loop, for compactness and due to latencies of registers in the path between the rasterizer and framebuffer.And, by the way, the engine does an even more impressive trick: sometimes a polygon is not drawn as a solid color or transparent, but instead copies its interior pixels from a source framebuffer! Like a simple form a texturing from a render target! This is used for instance in the intro to create the water 'shimmering' effect.Adding this possibility is actually not too difficult. Since we are already reading pixels for transparency, we can simply read them from another source framebuffer instead of the current one.Ah, we're done ... right?At this stage we have a fairly competent graphical port!Everything started great in the intro sequence.Until text was reached.Because yes, the game has a VM, a blitter, a rasterizer, and a font drawing engine.Uh-oh. (didn't I say this already?)But wait, there's yet another issue, which albeit seemingly unrelated can be solved elegantly with the same approach. In one particular part (part 6) the backgrounds are not drawn from polygons. Instead they are pre-rendered and loaded from data.Well, at this stage I had only a small LUT budget left on the FPGA. One thing I do have plenty of, however, is ROM. The SPI-flash is generously large on most FPGA boards. So I though, what can you do with few LUTs and a lot of memory? Brute force of course! I went on to pre-render all fonts into pixel buffers stored in ROM. I then highjacked the op_drawString opcode to lookup a pre-rendered buffer address in a ROM table from the string id. The VM then copies over the data into the target framebuffer. That did the trick! And the same mechanism can be reused for pre-rendered framebuffers in part 6.Left: Part 6 uses a pre-rendered buffer. Without it we get random garbage when the level starts! Right: After implementing loading pre-rendered buffers.Using the same mechanism for both was luck, as I first implemented the op_drawString hack, before realizing the need to copy entire buffers from data to a framebuffer. Hey, sometimes there are good surprises!Loading up the pre-rendered background required another trick, involving patching the corresponding calls to op_updateMemList in the game code and redirecting op_updateMemList to op_drawString.Testing timeAfter getting the game data, you can test in simulation with make simulN (use N=1 for the intro). This is slow but still fun to see! For running on actual hardware, see next.Really? That's it?Are you kidding me? There's no sound and no music!!Well, obviously I focused on graphics and making it playable. I think the sounds can be squeezed in, after regaining some LUTs. Music I am less certain, but perhaps we can also brute-force it with pre-rendered plain waves in ROM, as for op_drawString? Tempting!There is another obvious limitation: each part has to be synthesized as a separate bitstream with a separate data pack. Can we stitch everything in a single coherent game? I think yes, but this will be a topic for future work!I also skipped some gritty details in this write up that maybe I'll add at a later time:adding inputs and such,adjustments for SPI-screen,the arbiter between blitter and rasterizer in main,how colors are read from the palette and palettes are switched,redirection of op_updateMemList to op_drawString.See also my notes on future work.Thanks for reading this far! Hope you enjoyed the read through and will have fun experimenting with this design. I am hoping to keep working on it, after taking a good break ;) Please feel free to reach out, @sylefeb on Twitter or Mastodon.Next are details on the build process and various notes.Where's All the Data?This repository does not contain any game data, and of course the game data is needed to build the data packages required by the bitstreams. The game data comprises the following files:BANK01, BANK02, BANK03, BANK04, BANK05, BANK06, BANK07, BANK08, BANK09, BANK0A, BANK0B, BANK0C, BANK0D, MEMLIST.BINThese are easily copied from an official version of the game. I use the files from the Another World DOS version (dated 19/3/92 in the README). Simply copy these files to the GAMEDATA folder before anything else.You might run into issues using different game versions, please let me know.Building a5kMake sure you have placed the game data where needed. For building from source you have to install Silice and its dependencies (yosys, nextpnr, verilator, etc.), but otherwise the Makefile will use the pre-build bitstreams. Please refer to Silice's getting started guide.Important: The design enables Quad SPI on the flash memory. This can lead to difficulties in reflashing the board. Normally this is a solved issue on the icebreaker, a non issue on the mch2022, but can still be problematic on the ULX3S. My scripts do not flash the design on the ULX3S, only the data gets to flash, the design is programmed through SRAM. So there will be no trouble unless you manually flash the design on the board. In case of trouble there is a simple solution to exit Quad-SPI.In simulationOpen a command line in the root directory of this repo, type in:make simul1This will run the intro sequence in simulation. Feel free to replace simul1 by simulN with N the game part.On the icebreakerFor the icebreaker version you will need the Diligent VGA PMOD as well as ... a VGA screen (!!). Plays best on good old CRT! Plug the PMOD onto the icebreaker and run (the Makefile defaults to the icebreaker):make play1If necessary, adjust the serial port for sending data adjust SERIAL_PORT:make play1 SERIAL_PORT=/dev/ttyUSB1Under Windows it will be one of the COM ports. In any case, keep in mind the icebreaker exposes two ports: one for programming and one for UART. The second one (UART) has to be specified.Of course, the game is not playable for lack of inputs. There is a way to plugin an Amiga joystick on the icebreaker to play the game. It does however require snipping away two pins from the VGA PMOD and a small breadboard for pull-up resistors. Besides, I am not entirely sure this is all safe for the icebreaker. So for now I'll keep that feature 'hidden', but please reach out if you feel adventurous ;)On the mch2022 badgeIf you have the badge, simply runmake play1 BOARD=mch2022for a beautiful rendition of the intro. Replacing play1 by playN with N the game part you'd like to play, using the mch2022 stick and button.If necessary, adjust the serial port for sending data adjust SERIAL_PORT. Under Linux this will typically be /dev/ttyACM1 or /dev/ttyUSB1:make play1 BOARD=mch2022 SERIAL_PORT=/dev/ttyACM1Under Windows it will be one of the COM ports. In any case, keep in mind the badge exposes two ports: one for programming and one for UART. The second one (UART) has to be specified.On the ULX3SPlug in an HDMI screen, connect the board to the computer and runmake play1 BOARD=ulx3sReplacing play1 by playN with N the game part you'd like to play.If necessary, adjust the serial port for sending data adjust SERIAL_PORT. Under Linux this will typically be /dev/ttyUSB1:make play1 BOARD=ulx3s SERIAL_PORT=/dev/ttyUSB1Under Windows it will be one of the COM ports. In any case, keep in mind the board exposes two ports: one for programming and one for UART. The second one (UART) has to be specified.What's left to do?Plenty! Here's a list in (sort of) order of increasing complexity:Play the full game and see is there are any problems! I could not really do that for lack of time ... and skill.Adjust timing so that it matches the original more accurately. The game tends to be too fast, even though I tried to roughly adjust it.Fix the tiny dots sometimes appearing below text rendering (watch out for them in the intro).Add the music. At least on the ULX3S the extra logic should be no problem. Music tracks can be pre-rendered as full waves and streamed.Find a way to link parts together. This could be either keeping the designs separate and using warmboot (or equivalent) to jump to the next, or by implementing next part loading from a design properly dealing with data section offsets stored in ROM (might be easiest if not for the constrained LUT budget).On the badge, I started experimenting with saving the game state. This actually works at a technical level (see ENABLE_GAMESTATE_EXPERIMENT in vm.si), however this would also require saving the framebuffers in SPI-memory (QSPRAM) and reloading them when restarting. The mechanism to load pre-renderered buffers could be reused for that, which likely requires patching the game code too. This one's quite difficult.Something else that would be fun is to build other games or demos on top of this hardware. Given the tools now available, this seems very doable!Easter eggOn most boards one button resets the main VM thread. However some threads typically survive the reset, creating interesting results. Have fun!LinksThere are many great resources out there on Another World. Here are a few I used or came across:Fabien Sanglard excellent blog series on Another World.C++ port by Gregory Montoir, and javascript version.Fabien's fork of the C++ port by Gregory Montoir.VM tools by Felipe Sanches.Another World Suite is an in-browser debugger and resource viewer by C\u00e9sar Botana.LicenceThe Silice design is all MIT License. This write up is under Creative Commons CC BY-NC-SA 4.0. The modified C++ port retains its original GPL license of course. The game data is copyrighted so please go buy the game if you don't own multiple copies already. It's well worth it!LexicA framebuffer refers to a piece of memory storing data meant to be displayed on screen, in this case this will be indices in a color palette.A rasterizer is a device (or piece of code) to draw triangles or polygons into a framebuffer. Another World draws convex polygons.A blitter is a device (or piece of code) to quickly copy data between memory locations, in this case an entire framebuffer into another.A Virtual Machine (VM) is a piece of software that mimicks a CPU: it has a program counter and goes through instructions that change the values of its registers and write to memory, with various side effects. In the case of Another World, this was meant to help port the game: any complete implementation of the VM operands can run the game code correctly. We are about to turn the VM into an actual hardware design running on FPGA!Go back",
    "summary": "- The author has successfully remade the game \"Another World\" in hardware using an FPGA (Field Programmable Gate Array).\n- The game runs on a custom processor implemented in hardware, with a hardware blitter and rasterizer.\n- The design makes efficient use of the FPGA resources and utilizes special memory blocks for pixel data storage.",
    "hn_title": "Another World ported to FPGA",
    "original_title": "Another World ported to FPGA",
    "score": 280,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginAnother World ported to FPGA (github.com/sylefeb)280 points by retro_guy 17 hours ago | hide | past | favorite | 67 commentstombert 15 hours ago | next [\u2013]I remember playing Out of this World on my Sega Genesis as a kid, and I could not believe that graphics would ever get better than this; I had never seen fully animated cutscenes like that on my Sega and thought it was incredible.Obviously graphics did get better, but I feel like Another World still holds its own artistically. The graphics are still very stylish and distinct, and upon replaying the game again about a year ago, I am still really impressed by it.The game isn\u2019t perfect; a lot of the puzzles and stuff sort of just come down to trial and error and it\u2019s extremely short, but I also don\u2019t think I would change a thing about it.It\u2019s sort of in a different category, but for fans of Another World, I recommend also checking out Flashback: The Quest for Identity. It\u2019s got a similar cinematic vibe, and while I didn\u2019t like it at first it\u2019s really grown on me in the last decade or so.replychongli 9 hours ago | parent | next [\u2013]I really really love Flashback. The cyberpunk setting vibes with me! It's just a beautiful setting and a cool story.Another World is a work of art though! The poster for the game seems to be an oil painting and it's just gorgeous [1].[1] http://www.anotherworld.fr/download/AnotherWorld_Poster.jpgreplylacrimacida 8 hours ago | root | parent | next [\u2013]Oh yeah, that game was one of my favorites back then. Also Abe\u2019s Oddyssey, in a similar vein but even more cyberpunk or otherworldlyreplytheragra 13 hours ago | parent | prev | next [\u2013]I liked flashback so much, I tried to complete it on expert. Stuck somewhere closer to the end, though, cannot defeat slime manreplyekianjo 9 hours ago | parent | prev | next [\u2013]The genesis version was fairly late. In 1991 I played the Amiga version soon after it was released and it was absolutely mind blowing. A game like that had never been seen before.replyekianjo 9 hours ago | parent | prev | next [\u2013]Flashback is from Delphine Software and they also used some of the same polygon tech in Cruise for a Corpse, a detective game.replyefnx 14 hours ago | parent | prev | next [\u2013]I found flashback first, on sega. I loved it instantly. Later on I played Out of this World and it took a bit before I got into it. The two are great works of art though.replytombert 9 hours ago | root | parent | next [\u2013]Flashback is a bit more approachable; it is a bit more actioney and honestly might be a better game in an \u201cobjective\u201d sense, if such a thing can exists, but for some reason I\u2019ve always liked Another World a bit better.Maybe it\u2019s just because of the bizarre artistic nature of the entire thing, or maybe it\u2019s because I played Another World first, but something about it has always just stuck with me.replyslim 12 hours ago | parent | prev | next [\u2013]Flashback has a sequel : Fade to blackreplytombert 10 hours ago | root | parent | next [\u2013]To be honest I was not a big fan of Fade to Black. It\u2019s not incompetent or anything but I feel it comes from an era of 3D gaming that has not aged terribly well. I want to like it because I like Flashback a lot, but unlike Flashback I don\u2019t think it holds up very well.replyekianjo 9 hours ago | root | parent | prev | next [\u2013]And it was awfulreplyretro_guy 17 hours ago | prev | next [\u2013]See as well:- Infernal Runner for Amstrad CPC reverse-engineering and JavaScript port by cyxx [title from creator of Another World, both games utilizing virtual machine architecture]: https://github.com/cyxx/infernal_js- The Virtual Machine Architecture of Infernal Runner presentation by Norbert Kehrer (in German with English slides): https://media.ccc.de/v/vcfb20_-_146_-_en_-_202010111400_-_th...- The Story of Another World on the Amiga | MVG: https://www.youtube.com/watch?v=0iz9PJbs5rE- Nintendo 64 port of Another World: https://github.com/jnmartin84/aw64- Another World PlayStation 1 port: https://github.com/fgsfdsfgs/rawpsxreplynickelpro 14 hours ago | prev | next [\u2013]For anyone confused by the HDL, it's the author's custom language: https://github.com/sylefeb/Silice/tree/masterIt provides a compiler to Verilog that then can be fed to traditional design flows.replyburglins 17 hours ago | prev | next [\u2013]I wonder how common were full game VMs in the 90s. For a game older than myself, wouldn't a VM layer incur a great performance penalty on PCs from that time?replydragontamer 17 hours ago | parent | next [\u2013]It was far more important to have the same software work on Amiga, x86 (DOS), Mac and the whole slew of different machines than came and went.Today we have fewer machines than the great explosive growth of the 80s.Consider that most 'software' today is JavaScript interpreted by the Web Browser. It's not like those portability concerns didn't exist in the 80s, if anything, it was harder because you had to make your own interpreter back then.---------Many (maybe most?) video games seem to have been written in a VM, at least before Doom / high performance 3d graphics.I think console games were in C/Assembly for performance.But 'computer' games at that time was before the standard IBM PC or at least, before the PC won and Microsoft achieved dominance. When you didn't know if Amiga, PC98, IBM PC, Mac, or others would win it only made sense to write a VM.SCUMM (Monkey Island and many others) comes to mind.replyPaulHoule 15 hours ago | root | parent | next [\u2013]The Infocom text adventures (e.g. Zork) were based on a VMhttps://en.wikipedia.org/wiki/Z-machinereplyanthk 12 hours ago | root | parent | next [\u2013]Which is stll working with libre implementations (frotz) and OOP based compilers targeting the Z-machine (inform6 +inform6lib).replyeinpoklum 17 hours ago | root | parent | prev | next [\u2013]> Consider that most 'software' today is JavaScript interpreted by the Web Browser.I thought most software was MS Excel sheets with interacting formulae :-)replypdw 12 hours ago | root | parent | next [\u2013]And as it happens, early versions of Excel used a bytecode running on a VM instead of native code. Though the motivation was not portability, but rather memory requirements:> In most cases, p-code can reduce the size of an executable file by about 40 percent. For example, the Windows Project Manager version 1.0 (resource files not included) shrinks from 932K (C/C++ 7.0 with size optimizations turned on) to 556K when p-code is employed.> Until now, p-code has been a proprietary technology developed by the applications group at Microsoft and used on a variety of internal projects. The retail releases of Microsoft Excel, Word, PowerPoint\u00ae, and other applications employ this technology to provide extensive breadth of functionality without consuming inordinate amounts of memory.http://sandsprite.com/vb-reversing/files/Microsoft%20P-Code%...replyPhasmaFelis 8 hours ago | root | parent | prev | next [\u2013]> Many (maybe most?) video games seem to have been written in a VM, at least before Doom / high performance 3d graphics.This was not terribly common, for the obvious performance reasons. Another World ran at around 10-20FPS on most of the systems it was released for, which is fine for a methodical game like that (and for adventure games like Monkey Island, etc.) but doesn't work for fast action games.And of course VM games were basically impossible for the entire 8-bit era, with the exception of things like Zork (and the rest of Infocom's Z-Machine games) whose performance needs were so small that the gigantic overhead of an 8-bit VM was hardly noticeable.Even into the 16-bit era, the majority of multi-platform games were fully rewritten ports.replygwern 15 hours ago | parent | prev | next [\u2013]Yes, but you weren't doing things like _Elite_ in these sorts of special-purpose VMs. Aside from the portability issue (extremely important when platforms had the lifespan of mayflies back then as Moore's law blazed along full speed), VMs also got you compression. A full compiled binary might be exorbitantly expensive in disk/tape space, not to mention RAM. But a very small VM could make a custom-tailored language to interpret on the fly, and save a ton of space where you needed to sweat every kilobyte. (Think about the different in size between `print \"Hello world!\"` and the default compiled binary.) It didn't matter how fast your text adventure ran if it couldn't fit in X kb of space.replysyntheweave 13 hours ago | parent | prev | next [\u2013]The trick to these earlier VMs, from the Infocom Z-Machine and Wizardry's interpreted Pascal code, through SCUMM, Sierra AGI and SCI, Another World, the Horrorsoft games, etc., is that they recognized that the games they were making were primarily going to be \"content-delivery mechanisms\": lots of text and graphical assets, driven by relatively simple computations: the authoring constraint is only related to the hardware in terms of I/O and data compression. So the code that was being run by the interpreter was mostly run-once \"initialize the scene\" and then some animation timers.The opposing idea is represented more by arcade gaming, and later, stuff like Doom and Quake: The game is relatively intimate with the hardware in what it simulates, while the kind of definition that makes up a scene is more on the order of \"put a monster here and a health pickup there\", which aligns it towards being map data, instead of scripted logic.replyashleyn 17 hours ago | parent | prev | next [\u2013]During the heyday of assembly language, VMs were common in business software as well. It made porting to different types of systems easier in a time when standards-compliant C-language compilers targeting a variety of systems did not yet exist or were very expensive.replyburglins 17 hours ago | root | parent | next [\u2013]Oh, I know about the emulation layers of early computers! But I'd assume those programs rarely required frame-perfect input unlike video-games. Wouldn't that be too wasteful and needlessly limit the playerbase?Edit: after reading through wikipedia, I think maybe a VM wouldn't be that wasteful, since the game is very simple mechanically.replydragontamer 16 hours ago | root | parent | next [\u2013]Consoles (and arcades) back then had far better graphical performance than computers. So good computer games didn't have frame perfect inputs at all ... Or at least, not good games (bad games like TMNT for PC / DOS did exist but we're horribly buggy and broken)Computer games had explosive inputs available, like Civilization or needed the use of a mouse.Not so much action / frame perfect stuff. Not until a bit later anyway. Eventually computers were fast enough for arcade ports but computer games just didn't really target that action niche.------The 'computers' with good graphics were like Amiga, not x86 based DOS with mode 13h graphics. So it was all the fallen / failed computers that had the decent action games IIRC.replyjtolmar 7 hours ago | parent | prev | next [\u2013]I don't know any more full game examples, but...Earthbound (SNES, 1994) contains TWO complete scripting systems, one for the dialog system (which is occasionally used for things it shouldn't be; most of shop logic is in it), and one for scripting sprite movement. The dialog script is actually quite impressive and easy to use; I'd consider implementing a similar system even in a modern RPG. The sprite movement script is trash, significantly harder to work with than games that use raw assembly. Apparently that movement script system was actually a common in-house library at HAL, dating back to the NES era, but I don't know too much about that history.Also most of the game's assembly was actually compiled from C, which was almost unheard of for console games at the time.replybzzzt 16 hours ago | parent | prev | next [\u2013]Depends on what you consider 'full game VM'. Adventure games from Infocom ran all game code on a VM, and so did the graphical adventures from Sierra and LucasArts. The latter two used some native graphics primitives of course.replyjoshvm 14 hours ago | root | parent | next [\u2013]Scumm is a good example. There was a port for the original DS that ran well enough.https://wiki.scummvm.org/index.php/Nintendo_DSreplyjmiskovic 12 hours ago | root | parent | next [\u2013]Another World is on a whole other level. SCUMM is from '89 and the NDS came out in 2004. Another World game came out in 1991, and because it used the VM it could be back-ported to Apple IIGS (1986), the computer that's 5 years older than the game itself!The graphics exclusively used real-time rendered polygons with support for transparency, which nobody knew was even possible at the time. Along with researching the new rendering tech, the same person created everything else except the music - the memorable & immersive world, an original story, concept and cover art, strong cinematics that were SoTA at time, graphics and animation, innovative level design, puzzles, the game logic - over just 2 years. It also defined a new 'cinematic platformer' genre, with later titles like Flashback, Blackthorne, Oddworld, and recent LUNARK. It's simply incredible feat.replyegypturnash 11 hours ago | root | parent | next [\u2013]real-time rendered polygons with support for transparency, which nobody knew was even possible at the timeAegis Animator was doing pretty much the same sort of rendering on the Amiga, in 1985.I never did much with it, what with being a kid at the time, but it was fun to play with and looked pretty cool. I don't think its rendering was as tightly optimized as Another World's was, though.replyandrewf 12 hours ago | parent | prev | next [\u2013]Memory was often the constraint on low-end computers \"back in the day\", so code density was a reason to have a VM. This is why Wozniak shipped a VM in the Apple II's ROM.https://archive.org/details/byte-magazine-1977-11/page/n147/...replystevefan1999 15 hours ago | parent | prev | next [\u2013]VM already existed since PL/0 which is the prototype of Pascal. It is also known as P-code and to.be honest it is fine. Especially when you can leverage JIT which trades memory space for gains in speed.replypjmlp 14 hours ago | root | parent | next [\u2013]If you mean bytecode as executable format, that originates already in the late 1950's, early 1980's, with microcoded CPUs as the interpreter, from which Burroughs Large Systems is one of the most famous ones.replythepawn1 17 hours ago | parent | prev | next [\u2013]\"older than myself\"#rightinthefeelsreply7speter 5 hours ago | parent | prev | next [\u2013]Well the idea of a vm wasnt to foreign if my reading of computing history is right. Consider that java launched in 1993 or 94 and its big claim to fame was its portability between systems, and that was because of the jvm, or java virtual machine.I don\u2019t think virtual machines and emulation are that new of a thing. Virtualizing x86 at full speed on consumer hardware has been a thing for, what, 15 to 20 years? And sure that requires special processor features, but remember that systems that came before that that would need to be emulated had even less computational demands. Iirc, a widely used pos software from the 80s has been running in emulation on pos hardware that far exceeds its requirements for the last 25 years, at least.Also, my understanding is that lots of crucial government and business software runs on many layers of virtualization.And my last recollection from what I\u2019ve gathered is that, really until around the mid 90s a lot of operating systems made until then were pretty much hypervisors that ran programs that were virtual machines themselves. Multitasking was simply being able to route hardware resources to a given program, which was sorta its own environment.replyerickhill 17 hours ago | prev | next [\u2013]\"Only Amiga makes it possible\" is being replaced by FPGA 30 years later.replywzdd 8 hours ago | prev | next [\u2013]> [Transparency] introduces some complexity in the rasterizer. To flip only one bit [of the colour value] we need to know the prior value of the pixel at this location.This is like one of only three advantages to using planar graphics, like the Amiga did. With bitplanes you don't need to read back video memory: you just assume the top bit is for transparency effects only and blat out your spans.[The other two advantages are cool moire effects because you can move the planes relative to each other, and memory / bus bandwidth efficiency for awkward colour depths which don't fit nicely into a byte or a nybble, like 8 (3 bits per pixel) or 32 (5 bits per pixel)]reply29athrowaway 17 hours ago | prev | next [\u2013]The intro was fantastic. Even with a PC speaker / no sound card.Having to swim away as your first action, and then escape from some kind of lion is one of the most brutal game experiences ever.Even if you played that game for 1 minute you will remember that game forever.replyguiambros 15 hours ago | parent | next [\u2013]> Even if you played that game for 1 minute you will remember that game forever.I don't remember playing for much more than a few minutes (maybe a couple of hours), and barely making any progress, but that intro and the initial escape got etched in my memory for all those years. The dramatic camera effects of this game were indeed out of this world for the time.replythe_af 15 hours ago | root | parent | next [\u2013]The game itself is barely a couple of hours. It's very, very short.I wonder, if the intro made such an impression on you, why didn't you finish it?replyrecursive 2 hours ago | root | parent | next [\u2013]I'm in the same boat. I could not figure out how to get past the second screen. But I still remember those few minutes very clearly these decades later.reply29athrowaway 15 hours ago | root | parent | prev | next [\u2013]I also didn't finish it.The game is hard and requires more patience than usual to figure it out.replysnvzz 3 hours ago | root | parent | next [\u2013]Definitely a short, and not hard at all, game.I beat it without difficulty when I was single-digit aged.It's absolutely worth it, and better late than never.replyrecursive 2 hours ago | root | parent | next [\u2013]Maybe you're just a good gamer. I was absolutely stumped.replyekianjo 8 hours ago | root | parent | prev | next [\u2013]It was on par with games at the time though.replyXenoamorphous 13 hours ago | parent | prev | next [\u2013]> Even if you played that game for 1 minute you will remember that game forever.I\u2019ve never player that game, but it\u2019s been engraved in my mind for 30 years just from a 2 min? segment in a promotional VHS tape that came with a videogames magazine.Also the artwork in the cover is amazing.replyChainOfFools 16 hours ago | parent | prev | next [\u2013]I'll never forget the \"dialogue\" in the game either, right after you get rescued from that lion thing by your new friend.konanaka beetzai! motsuubo! /wavereplythe_af 15 hours ago | root | parent | next [\u2013]If I remember correctly, your alien friend never rescues you from the beast. The ones who shoot the beast (and then you) are the \"bad\" aliens ;)replyjakemauer 15 hours ago | root | parent | prev | next [\u2013]Mykaruba!replycrtasm 17 hours ago | parent | prev | next [\u2013]and not just the intro but the entire game fit on a single floppy disk, blew my mind.replydougmwne 12 hours ago | parent | prev | next [\u2013]I have the same memory of playing for one minute and getting stuck. I went back to it 25 years later and beat it. Once you get over the reaction-time hump, it\u2019s a glorious and memorable game.replypbj1968 14 hours ago | prev | next [\u2013]Because of this thread, I finally admitted to myself I was never going to figure this game out and watched a play through on YouTube. 22 minutes! I think I got past the lion once.replypjmlp 2 hours ago | prev | next [\u2013]This is quite cool project.replypengaru 16 hours ago | prev | next [\u2013]The original developer of Another World did a GDC postmortem, it's one of the better GDC talks:https://www.youtube.com/watch?v=JFaOYYSxSEAIIRC He shows some of his development tools including how he could modify and step through animation directly line by line in VM bytecode.reply83457 9 hours ago | prev | next [\u2013]The Story of Another World on Amiga, Technical Wizardryhttps://youtu.be/0iz9PJbs5rEreplythepawn1 17 hours ago | prev | next [\u2013]Loved this game. Still holds up...replymsephton 16 hours ago | prev | next [\u2013]I can't tell if this is just the intro rather than the full game?replyderbOac 16 hours ago | prev [\u2013]Off topic, but I was excited to find out Another World is on Steam... but isn't available to play on macOS 10.15 Catalina or above.The incompatibility with 32-bit makes me extremely frustrated; it seems as if in 2023, with quantum computing, DL models, and people porting games like this to FPGA, there should be a way to get 32-bit software to run on modern hardware. It's bizarre to me.Steam specifically recommends not upgrading your mac OS to play these older games.Is all this software going to become unusable because of this sort of thing?replyegypturnash 11 hours ago | parent | next [\u2013]Yes, yes it will. How many other Amiga games can you run on anything besides an Amiga, real or emulated? What happens when you try to run the 2013 version of anything you still depend on?There is a ton of stuff that broke when Apple moved to 32-bit and didn't bother making any kind of compatibility layer. Games are probably hit the worst because (a) almost nobody gives a shit about updating a game they finished working on a decade ago, and (b) almost nobody gives a shit about making a game work on the Mac anyway. I've been a Mac user since 2000 and I've been doing the vast majority of my gaming on consoles for all that time. The 2013 remake of Another World's available on my PS4, as well as the PS3, Vita, Wii U, Switch, Xbone, Android, iOS (well, assuming Apple didn't break that too), and more.Steam says the remake works fine on the Steam Deck I just got, too, thanks to the work they put into their fork of WINE. Old software ends up in virtual machines, one way or another.replybzzzt 15 hours ago | parent | prev | next [\u2013]There's no business case in making sure those old titles persist and publishers mostly forget about them after initial sales are made. You can play the MS-DOS port in QEMU, so I presume there will eventually be some emulation layer capable of running the HD release.replybdhcuidbebe 16 hours ago | parent | prev | next [\u2013]Only macOS dropped 32bit compat. Join linux my friendreplydopidopHN 12 hours ago | root | parent | next [\u2013]True, I run a vanilla fedora, never touched the driver and buy most games on steam without thinking about compatibility. ( I play low demanding games, but still )replyVHRanger 16 hours ago | root | parent | prev | next [\u2013]Android is dropping 32bit support.I can't play planescape: torment on my phone because of this.replyanthk 12 hours ago | root | parent | next [\u2013]Yes, you can. Find a gemrb build for Android, maybe from F-Droid.replyVHRanger 11 hours ago | root | parent | next [\u2013]Link?I spent 2h the other day trying to patch the apk to use APIs that would be compatible,and it still wouldn't startreplystriking 14 hours ago | parent | prev | next [\u2013]You can run Steam in Crossover Games to get some Windows games on your Mac. And Apple has a new compatibility layer themselves, though it takes finagling for the end user to use.replylightedman 13 hours ago | parent | prev [\u2013]Play the SNES version emulated - it had the best musical scorereplyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- Another World, a classic video game, has been ported to FPGA.\n- The game is known for its impressive graphics and innovative gameplay.\n- Fans of Another World are recommended to check out Flashback: The Quest for Identity, which has a similar cinematic vibe."
  },
  {
    "id": 36734423,
    "timestamp": 1689405933,
    "title": "The theory versus the practice of \"static websites\"",
    "url": "https://utcc.utoronto.ca/~cks/space/blog/web/StaticWebsiteTheoryPractice",
    "hn_url": "http://news.ycombinator.com/item?id=36734423",
    "content": "Chris Siebenmann :: CSpace \u00bb blog \u00bb web \u00bb StaticWebsiteTheoryPracticeWelcome, guest.The theory versus the practice of \"static websites\"July 14, 2023A while back I read Wesley Aptekar-Cassels' There is no such thing as a static website (via) and had some hard to articulate feelings about it. As I read that article, Aptekar-Cassels argues that there is less difference between static and dynamic websites because on the one hand, a static website is more dynamic and complicated than it looks, and on the other hand, it's easier than ever before to build and operate a dynamic web site.This is the kind of article that makes me go 'yes, but, um'. The individual points are all well argued and they lead to the conclusion, but I don't feel the conclusion is right. Ever since I read the article I've been trying to figure out how to coherently object to it. I'm not sure I have succeeded yet, but I do have some thoughts (which I'm finally pushing out to the world as this entry).The first thought is that in practice, things look different on a long time scale. The use of static files for web content has proven extremely durable over the years. Although the specific web servers and hosts may have changed, both the static file content and the general approach of 'put your static files with .type extensions in a directory tree' has lived on basically since the beginning of the web. One pragmatic reason for this is that serving static files is both common and very efficient. Since it's commonly in demand even in dynamic websites, people who only have static files can take advantage of this. Being common and 'simple' has meant that serving only static content creates a stable site that's easy to keep operating. This is historically not the case with dynamic websites.The second thought is that one reason for this is that static websites create a sharp boundary of responsibilities with simple, strong isolation. On the one side is all of the complexity of the static web server (which, today, involves a bunch of dynamic updates for things like HTTPS certificates). On the other side is those static files, and in the middle is some filesystem or filesystem like thing. What each side needs from the other is very limited. Any environment for dynamic websites necessarily has no such clear, small, and simple boundary between the web server and your code, and on top of that we're unlikely to ever be able to standardize on a single boundary and API for it.(This is not an accident; the web was, in a sense, designed to serve static files.)As a result, I believe it will always be easier to operate and to provide a static files web server than a dynamic web server and its associated environment. In turn, this makes static files web servers easier to find. This leads to the durability of static websites themselves; they're easier to operate and easier to re-home if the current operator decides to stop doing so. Or at least it leads to the durability of moderate sized or small sized static websites, ones that can fit on a single server.This leads to my final thought, which is that the distinction between static and dynamic websites is not blurry but is in fact still quite sharp. The distinction is not about how much work is involved in building and operating the site, or how much changes on it on a regular basis (such as HTTPS certificate renewal). Instead, the distinction is about where the boundaries are and what the sides have to care about. A static website has two clear sides and draws a sharp boundary between them that allows them to be quite independent (even if they're operated by the same people, the two sides can be dealt with independently). A dynamic website has no such sharp boundaries or clear sides, although you can create them artificially by drawing lines somewhere.(3 comments.)Written on 14 July 2023.\u00ab Some notes on errno when tracing Linux kernel system call resultsStatic websites have a low barrier to entry \u00bbThese are my WanderingThoughts(About the blog)Full index of entriesRecent commentsThis is part of CSpace, and is written by ChrisSiebenmann.Mastodon: @cksTwitter: @thatcks* * *Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, webAlso: (Sub)topicsThis is a DWiki.GettingAround(Help)Search:Page tools: View Source, Add Comment.Search:Login: Password:Atom Syndication: Recent Comments.Last modified: Fri Jul 14 23:00:35 2023This dinky wiki is brought to you by the Insane Hackers Guild, Python sub-branch.",
    "summary": "- The article discusses the difference between static and dynamic websites, arguing that static websites are more dynamic and complicated than they appear, while also being easier to build and operate.\n- The use of static files for web content has remained common and efficient over the years, creating a stable and easy-to-operate website.\n- Static websites have a clear boundary and limited dependencies, making them easier to operate and maintain compared to dynamic websites. The distinction between static and dynamic websites is still sharp, with static websites being more independent and easily re-homed.",
    "hn_title": "The theory versus the practice of \u201cstatic websites\u201d",
    "original_title": "The theory versus the practice of \u201cstatic websites\u201d",
    "score": 267,
    "hn_content": "- The author discusses the theory versus practice of \"static websites.\"\n- The author shares their personal experience of switching from a CMS to a static site generator.\n- Static sites offer benefits such as easier maintenance, reliability, and the ability to work offline.\n- The distinction between sole-author sites and sites with multiple contributors is discussed.\n- The challenges of using more complex static site generators for marketing sites are mentioned.\n- Various tools and frameworks like Hugo, Webflow, and Framer are mentioned in the comments.\n- Readers discuss the benefits and drawbacks of static websites and dynamic websites.\n- Security considerations and the advantages of having a smaller attack surface are mentioned.\n- The trade-off between dynamic sites and static sites is explored.\n- The Baked Data pattern, which combines dynamic server-side code with static site generation, is introduced.\n- The limitations of dynamic sites and the benefits of static sites are highlighted.- Static site generators are a popular choice for building websites because they are easy to host and have a smaller attack surface compared to dynamic sites.\n- Static websites are simpler to maintain and require less code and infrastructure.\n- Hosting options like Netlify and Cloudflare Pages make it easy to deploy and manage static sites.\n- CMS options for static sites, like Decap CMS and Surreal CMS, allow for easy content editing and management.\n- There is a growing interest in using Git-based static site generators with Markdown editing and live previews.\n- Static sites offer faster performance, better SEO, and lower hosting costs compared to dynamic sites.\n- Static sites can be easily archived and preserved for long-term accessibility and compatibility.\n- Some developers prefer to build their own custom static site generators for more control and flexibility.\n- The simplicity and reliability of static sites make them a popular choice for personal blogs and portfolio websites.\n- While static sites may not be suitable for all use cases, they offer many advantages and are worth considering for certain projects.",
    "hn_summary": "- Static websites offer easier maintenance, reliability, and the ability to work offline compared to dynamic sites.\n- Static site generators like Hugo and Webflow are popular tools for building and managing static sites.\n- Static sites have benefits such as faster performance, better SEO, and lower hosting costs compared to dynamic sites."
  },
  {
    "id": 36734445,
    "timestamp": 1689406062,
    "title": "The Uxn Ecosystem",
    "url": "https://100r.co/site/uxn.html",
    "hn_url": "http://news.ycombinator.com/item?id=36734445",
    "content": "MenuAbout UsMissionPhilosophyPinoVideosSupportStorePressknowledgeOff The GridLiveaboardSailingCookingCostsEngine CareWeatherResourcesblogLogBoat ProjectsWeathering Software WinterWorking Offgrid EfficientlyTools EcosystemLost LogbookBuying A SailboattoolsNoodleOrcaDotgridRoninLeftNasuUxnAdeliegamesNijuMarklOquonieDonsolParadiseHiversairesVerrecielbooksThousand RoomsWiktopherBusy Doing NothingLibrarytravelWestern CanadaUS West CoastMexicoFrench PolynesiaCook IslandsNiueTongaNew ZealandFijiMarshall IslandsJapanNorth Pacific OceanMetaIndexThe Uxn ecosystem is a little personal computing stack, created to host ours tools and games, programmable in its own unique assembly language.It was designed with an implementation-first mindset with a focus on creating portable graphical applications, the distribution of Uxn projects is akin to sharing game roms for any classic console emulator.To learn more, read about the uxn design, see the VM specs, or the IO specs.Desktop VersionsTo run Uxn programs, you'll need an emulator for your operating system. Below are links to download the latest versions of the Uxn emulators for major systems, bundled with a few roms. You can find the source code, in the repository. Alternatively, you can find emulators for more obscure systems, or try and you implement your own!Linux Windows MacOSOnce equipped with an emulator, you can pick among these toys, games and tools, write your own programs or try our own:donsol, card gameoquonie, puzzle gameleft, writing toolnasu, sprite editornoodle, sketch toolorca, livecoding toolturye, font editorcatclock, desktop clockdexe, hex editorUxn GuideThe same Uxn rom can be used on a variety of desktop computers, on tiny electronics, modern handhelds, and even in your, ugh, browser.Our general cross-platform desktop emulator requires SDL2, alternatively, you could download the plain Win32 or X11 versions, but we will not cover these in this guide. If you don't have SDL2, here's how to get it:sudo pacman -Sy sdl2       # Archsudo apt install libsdl2-dev   # Ubuntusudo xbps-install SDL2-devel   # Void Linuxbrew install sdl2        # OSXdoas pkg_add sdl2        # openBSDStartupDouble-click on uxnemu to launch it, on some Linux distribution, it might not be possible to do so, if for some reason uxnemu is not be clickable, navigate to the downloaded files in the terminal and launch it from there. To launch the ROM from the terminal, point the emulator to the target .rom file:bin/uxnemu path/to/example.romThere are many ways to launch ROMs:With the launcher program, see preview image aboveBy dragging .rom files onto the emulator windowVia the terminalEmulator ControlsF1toggle zoomF2toggle debuggerF3take screenshotF4load launcher.romButtonsL-CtrlAL-AltBL-ShiftSelectHomeStartOther SystemsUxn can also run on classic consoles and on old electronics. Currently, there are ports(not all are complete) for GBA, Nintendo DS, Playdate, DOS, PS Vita, Raspberri Pi Pico, Teletype, ESP32, Amiga, iOS, STM32, IBM PC, and many more.See the full list of emulators.Need A Hand?The following resources are a good place to start:AwesomeUXN listUxntal(XXIIVV)Uxntal reference(XXIIVV)Uxn tutorial(Compudanzas)You can also find us in #uxn on irc.esper.net.Edited on Sat Jul 8 08:43:41 2023 [edit]Hundredrabbits \u00a9 2022 \u2014 BY-NC-SA 4.0",
    "summary": "- The Uxn ecosystem is a personal computing stack that allows users to create and run their own tools and games.\n- It is designed with a focus on portable graphical applications and can be programmed using its unique assembly language.\n- Users can download the Uxn emulators for different operating systems and explore various toys, games, and tools, or even create their own programs.",
    "hn_title": "The Uxn Ecosystem",
    "original_title": "The Uxn Ecosystem",
    "score": 251,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginThe Uxn Ecosystem (100r.co)251 points by tonyg 1 day ago | hide | past | favorite | 47 commentsenricozb 1 day ago | next [\u2013]Everything about this group is fascinating. They are digital nomads on a boat. Every article [0] is a peek into an existence I didn't think or even consider was possible. I always get thrown into a rabbit hole when I visit their site.In another life maybe.[0]: https://100r.co/site/dry_toilet_installation.htmlreplyreidrac 1 day ago | parent | next [\u2013]Some of the stuff is truly fascinating. My favourite: Solar Collector Tubes (cooking using the sun, including baking bread).https://100r.co/site/solar_evacuated_tube_cooking.htmlreplystefncb 1 day ago | parent | prev | next [\u2013]For anyone interested in this, Devine also has a personal wiki at https://wiki.xxiivv.com/site/home.htmlThere's so much cool stuff in there.replyakavel 1 day ago | root | parent | next [\u2013]Also, they post on Mastodon (since way before the recent migration waves), at:https://merveilles.town/@neauoirehttps://merveilles.town/@rekand the instance created by them hosts a rather fine community of people:https://merveilles.town/public/localreplychubot 16 hours ago | parent | prev | next [\u2013]I'm really impressed that they have the energy / motivation / time to write a virtual machine after dealing with all these \"real world\" issues !Also impressed that it runs on a lot of real hardware apparently. I have seen many VM projects but they seem kind of isolated within other computers.The extreme constraints definitely push the design in a different directionreplyanthk 15 hours ago | root | parent | next [\u2013]Yeah, the same witht the Z-machine for text adventures (and terminal games like tetris, trek or rogue for the Z-machine) . Once you compile the game with Inform6 (an easier OOP lang than Python, rooms and objects are described almost as if they were a config file), the game runs from DOS/Amiga/Atari machines to Win/Lin/Mac/Android/iOS and even Haiku.replyttepasse 22 hours ago | parent | prev | next [\u2013]There is a huge subculture of sailors on Youtube, which is part of my escapism entertainment.Of course Youtube\u2019s algorithm being Youtube\u2019s algorithm and capitalism being capitalism a large part of those sailing videos is bikini content. But at the same time a large part of the sailing existence is repairing, renovating and upgrading their small yachts, from fiberglassing to solar to watermaking. As you say, a peek into a fascinating existence.replysausagefeet 20 hours ago | root | parent | next [\u2013]As someone who recently moved onto a boat after discovering all those YouTube channels, I agree! I've found the less produced channels to be the most exciting.replyttepasse 19 hours ago | root | parent | next [\u2013]Yes. The trick is to find these channels early and then bear to unfollow them when they get to streamlined.replyapgwoz 18 hours ago | root | parent | next [\u2013]My favorite recent channel is called \u201cBoring Sailing,\u201d which the algorithm served up, but has very few subscribers. It\u2019s actually\u2014truly\u2014been boring to watch, at times, but you have to root for him!\u201cWilding sailing,\u201d on the other hand, is a much more ambitious labor fest, with the boat owner restoring a catamaran he bought for 2k Euros. That\u2019s been inspiring, entertaining, and really worth subscribing to.replyuserbinator 1 day ago | parent | prev | next [\u2013]into a rabbit holePun intended, I suppose.replyenricozb 1 day ago | root | parent | next [\u2013];)replyanthk 22 hours ago | parent | prev | next [\u2013]Also, they are active 9front users because they got fed up on dependencies (on any OS) and bloat. 9front it's very small and everything it's statically compiled while having a very small API. Crosscompiling it's a breeze and it automatically makes you competent on CS on reading then Plan9 Intro book from Ballesteros.replyswah 1 day ago | parent | prev | next [\u2013]Shitting in the seas is more complex than I thought.replypSYoniK 17 hours ago | prev | next [\u2013]I think 100r reminds me of the thing we're missing the most nowaydays - bravery.We're afraid to take chances, to explore ideas, to take a slightly harder path or to explore difficult challenges. We're risk averse and we're unwilling to try new things. Reading about their trips, about all the work they do on the boat and their approach to code (love the concept of making software for one person!) makes me feel like a coward and how I would love to be more brave about how I deal with code, life and the environment around me...I appreciate all your work Rek and Devine. I'm glad yo see you busy doing nothing...replyimmibis 16 hours ago | parent | next [\u2013]What's to gain from it, other than bravery itself? Most alternate paths basically guarantee capitalism will crush you in the end.replypSYoniK 16 hours ago | root | parent | next [\u2013]This has got to be one of the saddest replies possible. We have everything to gain from it. If we're unwilling or unable to explore the parts of existence that scare us we cannot possibly come up woth new things. It is the very essence of our evolution and growth, going beyond the comfortable and known.Capitalism is easily the worst thing that has happened to us.replylioeters 14 hours ago | root | parent | prev | next [\u2013]> capitalism will crush you in the endIt's already crushing the majority of us. Unless some people are brave enough to forge alternate paths and live different lives, it will crush the rest of us too eventually.replyimmibis 8 hours ago | root | parent | next [\u2013]Living your life on a boat in the middle of the ocean means you are not spending your life crushing capitalism back.I saw a quote today somewhere on the fediverse: \"It\u2019s not the barricade but the rifle that you have to hold onto.\" - Buenaventura DurrutiIf you defend your one boat from capitalism, while it subsumes everything around it, what have you really won?replyzetalyrae 1 day ago | prev | next [\u2013]I admire this team very much. See also this great article about searching for simpler and simpler models of computing: https://100r.co/site/weathering_software_winter.htmlreplydang 1 day ago | parent | next [\u2013]Discussed here:Weathering Software Winter - https://news.ycombinator.com/item?id=34219654 - Jan 2023 (28 comments)replysph 23 hours ago | prev | next [\u2013]The author's account on Mastodon (https://merveilles.town/@neauoire) is a treasure trove of many cool little projects and screenshots from the parallel computing universe that is Devine's computer.I strongly recommend the podcast episode with him at Future of Coding: https://futureofcoding.org/episodes/044.html and the follow up where he talks about Orca, his incredible live music environment: https://futureofcoding.org/episodes/045.htmlHe's, along with the old Lispers on these forum, the reason why I'm spending so much time thinking about alternative computing and niche programming languages. Uxn is on my todo list, I'm busy with the Smalltalk family these days before diving in on Forth and Uxn :)replytyrust 10 hours ago | parent | next [\u2013]>himIt says \"they/them\" right there on the mastodon profile you linked.replyNeutralForest 1 day ago | prev | next [\u2013]I always love to see them on the front page, not only is the software they write really interesting, all the works they're doing to make the boat as self-sufficient as possible is incredible.replyemmanueloga_ 1 day ago | prev | next [\u2013]Any Blame! fans? [1] [2]1: https://wiki.xxiivv.com/site/devine_lu_linvega.html2: https://blame.fandom.com/wiki/Davine_Lu_Linvegareplywint3rmute 1 day ago | parent | next [\u2013]Both Blame! and Ergo Proxy[1] :)1: https://100r.co/site/pino.htmlreplyzellyn 21 hours ago | prev | next [\u2013]Maxine Chevalier-Boisvert (of note here due to her Basic Block Versioning JIT work and more recently yjit) has been working on similar ideas recently\u2026 https://pointersgonewild.com/2023/02/24/building-a-minimalis...replyKlonoar 1 day ago | prev | next [\u2013]One of their sites I actually check often is their recipes one: https://grimgrains.com/site/home.htmlSome fun stuff in there, for the chef/cooking types around here.replyzzo38computer 9 hours ago | prev | next [\u2013]I had done some stuff with uxn and I wrote my own emulator too (called uxn38); uxn is not too difficult to implement. I think that is is better than some similar designs. I am glad that it does not use Unicode. There are other advantages, too. But, there are some problems, such as the lack of seeking files, and the friend port is no good because it ends up being too complicated (even if it seems simple).replytonyg 1 day ago | prev | next [\u2013]This is a fascinating and elegant little virtual machine design that I think many here would find interesting.replyrollcat 1 day ago | parent | next [\u2013]What blows me away is how practical it is. They've ported it to every piece of junk they had laying around, and wrote tons of software. It's not just a design on someone's web page, they're making tools, games, art.replyanthk 22 hours ago | root | parent | next [\u2013]For gaming it's good, but sadly for software it lacks Unicode support. I wouldn't mind basic support with GNU unifont in further revisions.Altough the complexity would skyrocket.Then, if I wanted to write an adventure in Spanish I would omit the tildes, starting question/interrogation symbols and I would map \u00f1 to ny.replyTazeTSchnitzel 17 hours ago | root | parent | next [\u2013]> for software it lacks Unicode supportThat's not an inherent limitation of the VM at least, just of the current software ecosystem. It would be possible and practical to make a tiny unifont-based Unicode text library. Though handling bidirectional text, joined scripts and vertical scripts would be something else\u2026 :(replyrollcat 17 hours ago | root | parent | next [\u2013]This illustrates just how complex even the \"basic\" stuff turns out to be in practice. Text rendering hates you[1]. Even if you take obvious shortcuts/compromises like limiting yourself to monospace, you'll get bitten by \"double width\" characters. Kudos to those who try, rather than bundling a copy of Chrome with their chat app.[1]: https://faultlore.com/blah/text-hates-you/replyzzo38computer 9 hours ago | root | parent | next [\u2013]I tried to tell other people that (and more; there are even more problems with Unicode than mentioned in that article) but they don't believe me and they believe that Unicode is good anyways.I design my own programs and specifications to avoid Unicode as much as possible, even when multilingual text (sometimes even in languages that Unicode does not have) is desirable.replyanthk 15 hours ago | root | parent | prev | next [\u2013]Yeah, that's why ASCII with a few of accenter chars and glyphs [\u00e1\u00e9\u00ed\u00f3\u00fc\u00f1] would be enough with just a small extended western table. Optional as a library, OFC. By default you write ASCII chars from a table with a \"display\" device. I prefer uxn's simplicity over my own complex locales. Anyway, as I said, a tiny .tal code for extended chars wouldn't be very big.Some fonts for uxn (look at the ~rabbit repos at git.sr.ht) already bring extended chars.replyrollcat 13 hours ago | root | parent | next [\u2013]> [...] ASCII with a few of accenter chars and glyphs [\u00e1\u00e9\u00ed\u00f3\u00fc\u00f1] would be enough [...]Making a bespoke ASCII extension would be a step back - by about 30 years. You don't need a lot of code to support UTF8; if you're concerned about runtime memory usage, you can make your rune type take 8 bits and support only the U+0000-00FF range[1]. It happens to cover all of [\u00e1\u00e9\u00ed\u00f3\u00fc\u00f1] and a whole bunch of other languages - unfortunately, not my native one, which would leave me gravely upset ;P[1]: https://www.unicode.org/charts/PDF/U0080.pdfreplyzzo38computer 9 hours ago | root | parent | next [\u2013]Then you could just implement ISO-8859-1 instead. It is the same range but a simpler encoding, and external programs could be used for conversion if necessary.replyrollcat 3 hours ago | root | parent | next [\u2013]1. UTF8 encoding is trivial and everything already speaks it by default nowadays; 2. it is useful to explicitly differentiate byte arrays from text; 3. if you'd change your mind later on and decide you do want to support Japanese (like here: https://100r.co/site/niju.html), you haven't dug yourself into a hole.replyanthk 2 hours ago | root | parent | next [\u2013]The problem it's uxn it's ported to tons of devices. Would utf8 work under platforms like the Nintendo DS or DOS with a simple header file?replykouteiheika 23 hours ago | parent | prev | next [\u2013]For a more mainstream take on the concept it's also worth to check out RISC-V. Implementing a baseline RISC-V interpreter VM without any extensions is a ~day of work (I've done it; it's a little more complex than their ISA, but not by much), and it can essentially run any piece of software you can throw at it as real compilers can target it. (You still need to supply the rest of the VM though to get I/O and such.)replygenpfault 22 hours ago | root | parent | next [\u2013]https://github.com/cnlohr/mini-rv32imareplyburglins 22 hours ago | prev | next [\u2013]uxn is one of those things I know of and read about for quite a long time, but never end up trying myself. Those \"limited on purpose\" community-supported systems are a really cool concept.replyesoprogramming 11 hours ago | prev | next [\u2013]2021 interview with 100r https://esoteric.codes/blog/100-rabbitsreplyforward-slashed 1 day ago | prev | next [\u2013]A very inspiring duo.replyBookhouseGames 19 hours ago | prev | next [\u2013]Devine and Rekka are great and I love what they do and how they do it.replyanthk 22 hours ago | prev [\u2013]Retro style gaming based in Forth, low specs, virtual machine ported to nearly everywhere. Kinda like the Z-Machine, but for pixels.Good for GB style games without battling the Z80-like instructions and without worring if you would burn the physical LCD on a weird cycle timing bug on displaying sprites.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- The Uxn ecosystem is a fascinating and elegant virtual machine design that has been ported to various hardware platforms.\n- The team behind Uxn is actively creating software, games, and art using the virtual machine.\n- Uxn is particularly appealing for retro-style gaming and provides a low-spec environment for creating Game Boy-style games without the complexity of Z80-like instructions."
  },
  {
    "id": 36735777,
    "timestamp": 1689422365,
    "title": "The shady world of Brave selling copyrighted data for AI training",
    "url": "https://stackdiary.com/brave-selling-copyrighted-data-for-ai-training/",
    "hn_url": "http://news.ycombinator.com/item?id=36735777",
    "content": "ARTIFICIAL INTELLIGENCEThe shady world of Brave selling copyrighted data for AI trainingI'm fairly certain that I was not the only person in the world who thought to himself, \"Did they just yoink the entire Internet and bundle it together into a glorified copy and paste machine?\" upon the release of ChatGPT. And even though there are some concerns about the type of data that was used [\u2026]Alex IvanovsJuly 15, 2023 Updated6 min Read | Reader DisclosureI'm fairly certain that I was not the only person in the world who thought to himself, \"Did they just yoink the entire Internet and bundle it together into a glorified copy and paste machine?\" upon the release of ChatGPT.And even though there are some concerns about the type of data that was used to train OpenAI's latest model, it seems that the overall stance of OpenAI and other companies working on similar projects is that it is fair use. Whether or not that is going to hold up in the long run, remains to be seen.After Google published an announcement saying they're interested in exploring alternatives to robots.txt to provide broader control over AI-related content issues, I was curious to see what other search engines are doing in regard to AI, both for dealing with AI-generated content but also handling data.Personally, I'm not a big fan of these conglomerates ingesting other people's work and then reselling it, which also leads me to the story I'm going to talk about today.Brave gives you \"rights\" to use data for AI inference/trainingAs you may have noticed, I used the word copyrighted for the title of this story. And it's not without reason. I think this story could have been fairly decent even without the copyright part, so before we get to the nitty gritty stuff - I can 100% confirm that Brave lets you ingest copyrighted material through their Brave Search API, to which they also assign you \"rights\".Brave offers numerous API products, some of which are specifically designed for AI. This one, Data for AI, lets you \"Feed results to AI models for inference\", while their premium version of this same API lets you \"Cache/store data to train AI models\" not only with \"regular\" rights but also \"storage rights\".Rather than talking about it too much, I thought the logical thing to do would be to sign up for the API and see what kind of data we can find. For its Data for AI product, Brave offers something called \"Extra alternate snippets\", which are very similar to what we know as Google's Featured Snippets.An example of a typical Google Featured SnippetGoogle's featured snippets tend to be rather short (no more than 50 words), which from a copyright point of view can be classified as fair use.Fair use is a doctrine in the law of the United States that allows limited use of copyrighted material without requiring permission from the rights holders. It provides for the legal, non-licensed citation or incorporation of copyrighted material in another author's work under a four-factor balancing test:The purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposesThe nature of the copyrighted workThe amount and substantiality of the portion used in relation to the copyrighted work as a wholeThe effect of the use upon the potential market for or value of the copyrighted workSo, after doing a few queries with Brave's Search API - I was rather surprised to see how generous their snippets are; in this example below - the \"extra_snippets\" range from 150 to 260 words.Here is the cleaned-up JSON response from the API; this particular response was from a query \"Brave Search\", and the \"extra_snippets\" are extracted from this Wikipedia page. Mind you; this is for a single query from a single site, not taking into account the other (mentioned below) search features that Brave provides through its Data for AI API.      \"extra_snippets\":[        \"Brave Search is a search engine developed by Brave Software, Inc. and released in Beta in March 2021, following the acquisition of Tailcat, a privacy-focused search engine from Cliqz. Brave Search aims to use its independent index to generate search results. However, the user can allow the Brave browser to anonymously check Google for the same query.\",        \"In October 2021, Brave Search was made the default search engine for Brave browser users in the United States, Canada, United Kingdom (replacing Google Search), France (replacing Qwant) and Germany (replacing DuckDuckGo). In June 2022, Brave Search ended its beta stage and was fully released.\",        \"In June 2022, Brave Search ended its beta stage and was fully released. In addition to the launch, the new Goggles feature was added, allowing users to apply their own rules and filters to search queries. Brave search has various features designed to enhance users' searching experience:\",        \"Brave search has various features designed to enhance users' searching experience: Brave Search uses its own web index. As of May 2022, it covered over 10 billion pages and was used to serve 92% of search results without relying on any third-parties, with the remainder being retrieved server-side from the Bing API or (on an opt-in basis) client-side from Google.\",        \"Brave Search is a search engine developed by Brave Software, Inc., which is set as the default search engine for Brave web browser users in certain countries. Brave Search is a search engine developed by Brave Software, Inc. and released in Beta in March 2021, following the acquisition of Tailcat, ...\"      ]I know for a fact that Wikipedia operates under a CC BY-SA 4.0 license, which explicitly states that if you're going to use the data, you must give attribution. As far as search engines go, they can get away with it because linking back to a Wikipedia article on the same page as the search results is considered attribution.But in the case of Brave, not only are they disregarding the license - they're also charging money for the data and then giving third parties \"rights\" to that data.One might argue that even 260 words are not useful enough for any real impact, but I'm not sure that is the case (besides the whole copyright thing) because not only can you manipulate these results and fine-tune the output based on domains, type, date, and other metrics - Brave also offer additional API features for paid customers, such as:Schema-enriched Web resultsInfoboxFAQDiscussionsLocationsAll of which can be used to extract very specific information, and then be used to fine-tune LLM's without any worry for copyright infringement because Brave acts as a middleman.Brave doesn't disclose its own robot crawlerI get anywhere from 30 to 50 visitors a day from Brave's search engine. But, if I go through my access.log files, I won't find any indication that a Brave crawler is regularly crawling my content.They do have something called the Web Discovery Project, but from what I gather - it's an opt-in feature, so you must explicitly agree to it before you partake in the initiative.The Web Discovery Project is a privacy-preserving way for you to contribute to the growth and independence of Brave Search. If you opt in, you\u2019ll contribute some anonymous data about searches and web page visits made within the Brave Browser (including pages arrived at via some, but not all, other search engines). This data helps build the Brave Search independent index, and ensure we show results relevant to your search queries. By \u201cdata\u201d we mean search queries, search result clicks, the URLs of pages visited in the browser, time spent on those pages, and some metadata about the pages themselves.After some more digging, I was able to find a Reddit comment from Jonathan Sampson, Senior Developer Relations at Brave, who said the following:We do indeed have our own crawler, actively building our own index. Presently the index consists of over 8 billion pages, with more than 40M crawled each day. The crawler, which does not contain a unique user-agent string, respects robots.txt.They don't mention their crawler anywhere in their docs, either. So, if you wanted to block Brave from crawling and indexing and ultimately selling your content to third parties, your only option for the time being would be to block all crawlers, which is how Brave would be able to \"respect robots.txt\".And don't get me wrong, I love Brave, and I've given them credit where it's due; it's also my understanding that the Brave Search API feature is new as a whole (released in May 2023), so perhaps it wasn't or hasn't been thought through completely.I've asked for a comment from the Brave team on their thought process, so as soon as I have a statement, I will make sure to include it here.WRITTEN BYAlex IvanovsAlex is a full-stack developer with more than 15 years of experience. After many years of threading the self-taught path, he discovered a natural passion for writing. His past work includes helping build the Huffington Post Code column and working with publishers such as Entrepreneur, TheNextWeb, and many prominent tech startups.Post navigationPrevious PostPREVIOUSWordPress Security Plugin AIOS Logs Plaintext PasswordsRead alsoWordPress Security Plugin AIOS Logs Plaintext PasswordsCheapest Web Hosting Providers: Top 9 HostsHow to Handle Date & Time with JavaScriptUncaught TypeError: Cannot read property of undefined11 Best Prototyping Tools for UI & UX Designers",
    "summary": "- Brave, a search engine, offers an API called Data for AI that allows users to use copyrighted material for AI training and inference.\n- Brave's API provides generous snippets of information from websites, potentially violating copyright laws.\n- Brave's search engine may crawl and index websites without explicit disclosure, potentially selling content to third parties.",
    "hn_title": "The shady world of Brave selling copyrighted data for AI training",
    "original_title": "The shady world of Brave selling copyrighted data for AI training",
    "score": 239,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginThe shady world of Brave selling copyrighted data for AI training (stackdiary.com)239 points by rand0mx1 22 hours ago | hide | past | favorite | 117 commentsniemandhier 18 hours ago | next [\u2013]This discussion on fair use are always quite anglocentric.Atricle 3 and 4 of the EU 'Copyright in the Digital Single Market' give data miners quite extensive rights.Move operation to the EU, train a foundational model, than train a constitutional model based on that.As much as I hate the upcoming AI regulation, the CDSM is solid.https://academic.oup.com/grurint/article/71/8/685/6650009 https://eur-lex.europa.eu/eli/dir/2019/790/ojUpdate: Fixed wrong linkreplypedrocr 17 hours ago | parent | next [\u2013]It's not clear that \"data mining\" covers this use. These models are huge, big enough that they can just contain direct copies of copyrighted works. They've been shown to reproduce them relatively easily. The argument is that they've actually generalized enough or learned enough that they're now no longer the sum of the dataset. I can definitely see that being possible but the way the technology works it's really hard to know if that has happened or if what's happening instead is a bunch of copyright washing.There are some things that would make for good faith displays by the players in the space. For example, Microsoft has been investing a lot and yet their code offering is not trained on their internal code base. Same for Google. Start by doing that and I'll entertain the argument that your tools are fair use or data mining.replyniemandhier 17 hours ago | root | parent | next [\u2013]My reading of the relevant laws would actually lead me to believe that this is not a problem, as long as those reproductions are not returned and the eights holder did not opt out. But courts might decide differently.Regarding the copyright of returned material here is a good discussion:https://copyrightblog.kluweriplaw.com/2023/05/09/generative-...replyJumpCrisscross 16 hours ago | root | parent | next [\u2013]> as long as those reproductions are not returnedThat\u2019s the author\u2019s entire gripe. Brave reproduced a Wikipedia entry without attribution and then slapped a copyright on it to boot.replypedrocr 14 hours ago | root | parent | prev | next [\u2013]That's clearly not enough. There's a continuum between producing exact input copy and having genuine creativity because the model actually learned something. A model that just reformats code and changes all the variable names would pass your test and yet be clearly a copyright violation. This whole argument requires that the neural network weights do something creative because they learned from the code instead of just transforming it. We're even careful about this with humans with things like clean room reimplementations to make sure.replyhakfoo 8 hours ago | root | parent | next [\u2013]The window of possible actual creativity may be limited and variable.There are a lot of pretty complex prompts, where if you asked a group of reasonably skilled programmers to implement, they'd produce code that was \"reformatted and changed variable names\" but otherwise identical. Many of us learned from the same foundational materials, and there are only a handful of non-pathological ways to implement a linked list of integers, for example.With code it may be more obvious, in that you can't as easily obfuscate things with synonyms and sentence structure changes. Even with prose, there is going to be a tendency to use \"conventional\" language choices, driving you back towards a familiar-looking mean.replyhartator 19 hours ago | prev | next [\u2013]> Simply observe the event in which a user does a query q in Brave and then, within one hour, does the same query on a different search engine. What we do is to move the script that detects bad-queries to the browser, run it against the queries that the user does in real-time and then, when all conditions are met, send the following data back to our servers.Wait. Brave browser sends back to Brave Search engine about your browsing? Other search engines usage, but also crawl pages on your computer to help build their search index?Ref: https://github.com/brave/web-discovery-project/blob/main/mod...replyw0ts0n 18 hours ago | parent | next [\u2013]This is (importantly) opt-in.\"Brave doesn\u2019t follow the sneaky practices of other big tech search engines. The Web Discovery Project is opt-in, and the data collected under the Web Discovery Project has specific protections to ensure anonymity.\" per https://support.brave.com/hc/en-us/articles/4409406835469-Wh...replyfredoliveira 10 hours ago | root | parent | next [\u2013]I think mentioning your affiliation with Brave might go a long way in contextualizing why you are defending/rationalizing this (even if opt-in).Editing to add that I don't mean to imply ill will on your part, but that I think being affiliated with Brave might have you taking this type of practice a little more lightly than it probably should be taken.replychoppaface 1 hour ago | root | parent | prev | next [\u2013]Opt-in or not, \u201csneaky\u201d is a marketing term and not a UX principle. E.g. showing the user an example of real ROI from providing their data.That said, stuff like Jedi Blue and Project Bernanke suggest Brave could just disclose they support competitive markets.replyjrmg 19 hours ago | parent | prev | next [\u2013]If you don\u2019t trust Brave then, yeah, they could be doing anything in the browser or on their servers - but that snippet you quoted is a slightly out of context statement from a big document about how they collect data like this, but _don\u2019t_ collect or store it in a way that they could associate it with a user.If you don\u2019t trust that they\u2019re doing what they say they are, then the document doesn\u2019t mean anything. Although that would also mean the quote is kind of meaningless\u2026replyhartator 18 hours ago | root | parent | next [\u2013]The rest of the document is worst. They say they are using your computer to crawl pages you visit and report back to their server. Even Google doesn't do that.replyw0ts0n 17 hours ago | root | parent | next [\u2013]This is opt-in only. https://support.brave.com/hc/en-us/articles/4409406835469-Wh...replyDonHopkins 2 hours ago | root | parent | next [\u2013]If you're shilling for Brave, you should reveal you affiliation with them.What is it?replyyjftsjthsd-h 16 hours ago | root | parent | prev | next [\u2013]> Even Google doesn't do thatAt least Bing did, though. https://news.ycombinator.com/item?id=2169793replymx20 18 hours ago | root | parent | prev | next [\u2013]How do they detect if someone poisons their data, if they not at least associate IP addresses to the data?replydrusepth 17 hours ago | parent | prev | next [\u2013]This specific feature is already opt-in, but historically the answer has always been \"yes\" for dozens of 'features' like this that fly under the radar until users start complaining, and then eventually get converted to opt-in or removed in order to save face.replychoppaface 1 hour ago | parent | prev | next [\u2013]And Google gets the same data joining your cookies ever since Google Plus unified auth across their properties a decade ago. Wait you mean you thought G+ was supposed to compete against Facebook-the-product and not just Facebook-the-ad-network? OopsBrave is perfectly OK with having oopsies tooreply411111111111111 19 hours ago | parent | prev | next [\u2013]It's always surprising to me when I hear people using the brave browser... It's by a company that initially tried to replace their blocked ads with their own \"safe and non-intrusive\" ads as far as I remember, until they backpaddled because of the outrage.It's also a for-profit company and you're not the customer, as you're not paying them money.I'd be way more worried how they're using the data they're collecting on you vs Google or MSreplynicce 19 hours ago | root | parent | next [\u2013]People still like to defend Brave when it gets caught on shady things over and over again. I guess there are no too many other options. For some people it is already too difficult to install uBlock or know its existence.replylalaland1125 17 hours ago | root | parent | next [\u2013]It's because a lot of people are bought into BAT (Brave's cryptocurrency) and have a strong financial incentive to shill Brave.replyDaSHacka 16 hours ago | root | parent | next [\u2013]BAT gives like no money, especially after the crypto crash. Its far more likely its just the browser wars of old, but with even less options to choose from people are going to be more adamant their choice is the best.replytokai 17 hours ago | root | parent | prev | next [\u2013]Ah that makes sense why brave fans post a bit like cryptobros.replyboondoggle16 15 hours ago | root | parent | prev | next [\u2013]I do not use BAT or any crypto. Brave just works, and it blocks ads automatically when I tell friends to install it on their computers.I used to recommend Firefox, but Mozilla has totally jumped the shark (privacy violations [multiple], wastes too much money, blocks APIs that are useful with no real security risks while approving APIs with little use that do have security risks, etc, very user hostile).Chromium is obviously not trustworthy at this point, let alone Chrome. So that leaves like, Safari and Opera?Brendan Eich is the CEO of Brave, and I trust him. Mozilla was good until he was ousted for political reasons.replynicce 15 hours ago | root | parent | next [\u2013]> Chromium is obviously not trustworthy at this point, let alone Chrome. So that leaves like, Safari and OperaBrave is like 99% of Chromium + uBlock\u2026replyboondoggle16 14 hours ago | root | parent | next [\u2013]Right, I should have said \"the main Chromium branch is obviously not trustworthy\". It is possible to remove the untrustworthy bits, however, and there are a variety of de-googled Chromium builds.Chromium is a great browser, unfortunately the official branch has been poisoned by Google.replysiquick 9 hours ago | root | parent | prev | next [\u2013]There are multiple ways you can pay Brave.https://brave.com/firewall-vpn/ https://account.brave.com/?intent=checkout&product=search https://brave.com/search/api/reply2Gkashmiri 18 hours ago | root | parent | prev | next [\u2013]We have these cropping up like ants.MullvadBraveOperaVivaldiMicrosoftHeck zoho is in on a browser nowWhat net gain does each of these companies provide over skinning chromium that isn't in Firefox?Last time I asked brave fanboys why they don't redskin Firefox and the response was \"Firefox is pita to build\" all the while we have projects like palemoon and waterfox that are hobby projects. If they can work with firefox, so could someone else but noreplyArclightMat 12 hours ago | root | parent | next [\u2013]Mullvad is actually a Firefox fork and it directly uses Tor's privacy enhancements[0] to Firefox for a private web browsing experience. As a matter of fact, it really looks like Tor Browser but with a VPN baked in instead of Tor.[0] https://mullvad.net/en/browserreplyrglullis 18 hours ago | root | parent | prev | next [\u2013]Have you worked in any project that required a forked browser?I did. When we folded less than two years later, one of the CTOs biggest stated regrets was that he went with Firefox instead of Chromium. The extension story in Firefox was easily 10x harder. Interfacing with the OS as well. Getting dbus services to work was a fool's errand.reply2Gkashmiri 18 hours ago | root | parent | next [\u2013]Cool so your company folded but as I said, palemoon and waterfox seem to be running just fine.Thunderbird also works.I happen to own a brwoser extension and have both chromium and Firefox extensions. I kinda know myself.replyrglullis 17 hours ago | root | parent | next [\u2013]> palemoon and waterfox seem to be running just fine.GNU/Hurd is also a very interesting alternative OS, the design is a lot more elegant than GNU/Linux, it's still under active development and it has a surprising number of active users.It's still a very bad idea to build the foundation of your tech stack on it.replythrowawayadvsec 18 hours ago | root | parent | prev | next [\u2013]opera offers a free vpn and builtin adblockeri would use it daily if the UI/UX was better, or more similar to firefoxreplyTimber-6539 16 hours ago | root | parent | next [\u2013]If am not wrong, Opera is owned by some Chinese company and they are known for doing some really shady stuff [0][1] in African countries.[0] https://blogs.opera.com/africa/2022/05/free-data-with-opera-...[1] https://www.androidpolice.com/2020/01/21/opera-predatory-loa...replyKORraN 16 hours ago | root | parent | next [\u2013]True, right now lots of folks from the original Opera team (including CEO) work on Vivaldi. If one day Mozilla forces me to ditch Firefox, I will probably switch to this browser.replysoundnote 17 hours ago | root | parent | prev | next [\u2013]Brave literally started on Gecko.reply2Gkashmiri 17 hours ago | root | parent | next [\u2013]Is brave currently forking chromium or not?By your logic, opera was having their own engine till 2013. So what?replycempaka 18 hours ago | root | parent | prev | next [\u2013]> I'd be way more worried how they're using the data they're collecting on you vs Google or MSWhy? They don't even have access to my emails and texts like those other companies do. I also don't see the names of their top executives and founders showing up in articles about connections to Jeffrey Epstein every few months.replyrglullis 18 hours ago | root | parent | prev | next [\u2013]You are a victim of the Mandella effect. There never was anything related to replacing ads in-page, yet if you ask all detractors what they don't like about it, that's the first point they bring up.replytokai 17 hours ago | root | parent | next [\u2013]Seems you're wrong. Here's an archive link to Braves page in '16 were their planned add replacement is explained:https://archive.md/W0k4jreplylalaland1125 17 hours ago | root | parent | next [\u2013]One minor note: they made a slight change in plans from that initial design.How it works now is that when Brave replaces an ad, they put the new ad in a popup, not in-pagereplyrglullis 16 hours ago | root | parent | next [\u2013]Correct. I've been using Brave since their very first versions on the desktop, and there never was any in-page ad insertion.The one type of in-page modification they used to do is that they would add a \"tip\" button to the content creator of some social networks like Twitter or reddit. That had nothing to do with \"replacing ads\" though.> replaces an ad, they put the new ad in a popupIncorrect. There is no 1:1 replacement. You as the user can define how often you want to receive notifications, and even then the notifications only come when you are switching context between any action. It won't interrupt you while you are watching a video, working on google doc spreadsheet or reading though HN.reply411111111111111 16 hours ago | root | parent | next [\u2013]You're responding to a comment that gave you a link to their inital plan, which was literally replacing the ads.click on it, your horizon might be broadened by the added knowledge.replyDylan16807 16 hours ago | root | parent | next [\u2013]That was their original plan, yes.lalaland1125 is making claims about what they actually did, and those claims are not correct.replyDylan16807 16 hours ago | root | parent | prev | next [\u2013]\"replaces an ad, they put the new ad\"That's not how it works. If you turn on Brave ads, they show up every once in a while, completely independently of webpage ads. And they work whether your ad blocker is on or off.replylalaland1125 17 hours ago | root | parent | prev | next [\u2013]They don't replace ads in-page, but they do something very very similar.They block the in-page ads and instead provide their own ads through popup notifications.So they are replacing advertisements on websites.replyrglullis 16 hours ago | root | parent | next [\u2013]Every time this comes up, the argument is the same. People always forget:- The ad blocker works separately from their own ad service.- Their own ads are opt-in.- People receive 70% of the revenue from the ads they see.- The ads from Brave do not track you and whatever personalisation happens in-device, no data is mined.So, no. They are not \"replacing\" anything. They are not stealing anyone's revenue (and no matter how much Linus from LTT argues, he is not entitled to any revenue just because I watched any of his videos) and Brave's own ads are from deals that they closed themselves and a essentially fraud-proof compared with whatever payouts are given by largest ad networks.In other words, they are just offering something that happens to be infinitely more user-focused than the status quo. Every attempt at framing this as unethical came from an uninformed or biased source.replymtlmtlmtlmtl 17 hours ago | root | parent | prev | next [\u2013]As a detractor and therefore a part of the set \"all detractors\", I do not believe this. I just don't buy their shady marketing and try not to support engine monoculture.replyimpissedoff1 17 hours ago | root | parent | prev | next [\u2013]I think some initial news articles claimed this and everyone went with it. Which is basically how it worked, replacing ads, but in a different way, and actually more annoying.. block everyone else's ads...and have their own little popup ads, and if you enabled that, you'd get paid in BAT tokens per view too.reply6gvONxR4sf7o 18 hours ago | prev | next [\u2013]> Fair use is a doctrine in the law of the United States that allows limited use of copyrighted material without requiring permission from the rights holders. It provides for the legal, non-licensed citation or incorporation of copyrighted material in another author's work under a four-factor balancing test:> 1) The purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes> 2) The nature of the copyrighted work> 3) The amount and substantiality of the portion used in relation to the copyrighted work as a whole> 4) The effect of the use upon the potential market for or value of the copyrighted work[emphasis from TFA]HN always talks about derivative work and transformativeness, but never about these. The fourth one especially seems clear in its implications for models.Regardless, it makes it seem much less clear cut than people here often say.replyFishInTheWater 18 hours ago | parent | next [\u2013]The entire fair use claim is derived not from any legal basis, but rather, that \"it has to be fair use\" because it would be legally catastrophic for OpenAI et al if it weren't true.If you look at the core argument in favour of fair use, it's that \"LLMs do not copy the training data\", yet this is obviously false.For Github copilot and ChatGPT examples of it reciting large sections of training data are well known. Plenty can be found on HN. It doesn't generate a new valid windows serial key on the fly, it's memorized them.If one wants to be cynical, it's not hard to see OpenAI/etc patching in filters to remove copyrighted content from the output precisely because it's legally catastrophic for their \"fair use\" claim to have the model spit out copyrighted content. As this is both copyright infringement by itself, and evidence that no matter how the internals of these models work, they store some of the training data anyway.replytwoodfin 17 hours ago | root | parent | next [\u2013]It actually doesn\u2019t even matter if LLMs reproduce copyrighted data from their training. The issue is that a human copied the data from its source into memory for use in training, and this copy was likely not fair use under cases like MAI Systems.The Supreme Court hasn\u2019t ruled on a software case like this, as far as I know. But given the recent 7-2 decision against Andy Warhol\u2019s estate for his copying of photographs of Prince, this doesn\u2019t seem like a Court that\u2019s ready to say copying terabytes of unlicensed material for a commercial purpose is OK.I\u2019m going to guess this ends with Congress setting up some kind of clearinghouse for copyrighted training material: You opt in to be included, you get fees from OpenAI when they use what you added. This isn\u2019t unprecedented: Congress set up special rules and processes for things like music recordings repeatedly over the years.https://scholarship.law.edu/cgi/viewcontent.cgi?referer=&htt...replyluma 14 hours ago | root | parent | next [\u2013]How does that align with Google Books scanning libraries full of copyrighted text, offering full reproductions of sections of the work, and then having the supreme court declare it all to be Fair Use? I think that is a far more relevant precedent here: https://en.m.wikipedia.org/wiki/Authors_Guild,_Inc._v._Googl....replytwoodfin 14 hours ago | root | parent | next [\u2013]The Supreme Court declined to hear the case on appeal, which is a shade different from endorsing the decision after a hearing.That being said, it doesn\u2019t take a lot of effort to differentiate these cases. Google was indexing copyrighted works and providing access to limited extracts. They weren\u2019t transforming them into new works and then selling access to those new works over APIs.replyluma 12 hours ago | root | parent | next [\u2013]OpenAI is also providing access to limited extracts. Google wasn't selling this over an API, they were providing \"free\" access to it while displaying ads to the user. Would the courts see this manner of monetization to be different enough that settled case law wouldn't apply?replytwoodfin 10 hours ago | root | parent | next [\u2013]OpenAI isn\u2019t doing anything like what Google was doing with Books. It\u2019s not hard for laymen to see that, and it\u2019s going to be obvious to any judge who hears a case.Imagine OpenAI had invented a software program that turned any written text into an animated cartoon enacting the text. That would obviously be creating a derivative work and outside fair use bounds. That they mix a bunch of works (copyrighted and otherwise) into a piece of software doesn\u2019t allow them to escape that basic analysis.Google showed a \u201cclip\u201d of the original work, no different in scope than Siskel & Ebert showing a clip of a film as they reviewed it. The uses are not comparable.reply6gvONxR4sf7o 11 hours ago | root | parent | prev | next [\u2013]Google also bought copies of each book, I believe, which makes it another step removed from standard ML practice.replygyudin 16 hours ago | root | parent | prev | next [\u2013]So how is that supposed to work with people sending it legally obtained copyrighted materials for an analyze?replytwoodfin 15 hours ago | root | parent | next [\u2013]That copy (the \u201csend\u201d) would be evaluated under the same fair use criteria.\u201cWrite a review of this short story: \u2026\u201d \u2013 probably fine.\u201cRewrite this short story to have a happier ending: \u2026\u201d \u2013 probably not.replykmeisthax 16 hours ago | root | parent | prev | next [\u2013]OpenAI's bias research on DALL-E revealed that most examples of regurgitation come from repeated copies of the same image in the training set. When they filtered out duplicates, DALL-E stopped drawing training examples.The problem is that filtering the training set is naively O(n^2) and n is already extremely large for DALL-E. For LLMs, it's comically huge, plus now you have to do substring search. I've yet to hear OpenAI talk about training set deduplication in the context of LLMs.As for the legal basis... nobody's ruled on AI training sets in the US. Even the Google Books case that I've heard cited in the past (even by myself) really only talks about searching a large corpus of text. If OpenAI's GPT models were really just a powerful search engine and not intelligent at all, they'd actually be more legally protected.My money's still on \"training is fair use\", but that actually doesn't help OpenAI all that much either, because fair use is not transitive. Right now, such a ruling would mean that using AI art is Russian roulette: if your model regurgitates, the outputs are still infringing, even if the model is fair use. Novel outputs aren't entirely safe, though. A judge willing to commit the Butlerian Jihad[0] might even say that regurgitation does not matter and that all AI outputs are derivative works of the entire training set[1].This logic would also apply in the EU. Last I checked the TDM exception only said training is legal, not that you could sell the outputs. They don't really respect jurisprudence the way the Anglosphere obsesses over \"precedent\", so copyright exceptions are almost always decided by legislatures and not judges over there, and the likelihood of a judge saying that all outputs are derivative works of the training set regardless of regurgitation is higher.[0] In the sci-fi novel Dune, the Butlerian Jihad is a galaxy-wide purge of all computer technology for reasons that are surprisingly pertinent to the AI art debate.Yes, this is also why /r/Dune banned AI art. No, I have not read Dune.[1] If the opinion was worded poorly this would mean that even human artists taking inspiration to produce legally distinct works would be violating copyright. The idea-expression divide would be entirely overthrown in favor of a dictatorship of the creative proletariat.[2] \"Music and Film Industry Association of America\" - an abbreviation coined for an April Fools joke article about the MPAA and RIAA merging together.replyrichk449 14 hours ago | root | parent | next [\u2013]> A judge willing to commit the Butlerian Jihad[0] might even say that regurgitation does not matter and that all AI outputs are derivative works of the entire training set[1].A judge can\u2019t \u201ccommit\u201d the butlierian jihad. A jihad is a mass event caused by some fraction of the population believing in some cause.Which kinda gets to a point that seems to be missed. Copyright law is not \u201cintrinsic\u201d - nobody thinks that copyright is a natural law - it is just a pragmatic implementation which balances various public and private goods. If the world changes such that the law no longer does a good job of balancing the various goods, then either the law will get changed or people will ignore the law.replykmeisthax 13 hours ago | root | parent | next [\u2013]Copyright is a unique case in which the law represents a bargain struck in the 1970s that hasn't been updated since. Everyone ignores it because it's nearly impossible to actually enforce copyright on individual infringers. But that doesn't mean copyright is meaningless: any activity which is large enough to be legible[0] to the state will be forced to bend itself to fit within the copyright bargain.And AI training is extremely legible. This is not like a bunch of people downloading stuff off BitTorrent. All of the large foundation models we use were trained by a large corporation with a source of venture capital funding which could be easily shut off by a sufficiently motivated government. Weights-available and liberally licensed models exist, but most improvements on them are fine-tuning. Anonymous individuals can fine-tune an LLM or art generator with a small amount of data and compute, but they cannot make meaningful improvements on the state of the art.So our sufficiently motivated copyright judge could at least effectively freeze AI art in time until Big Tech and the MAFIAA agree on how to properly split the proceeds from screwing over individual artists.\"Butlerian Jihad\" is a term from a book, so you don't need to take \"jihad\" literally. However, I will point out that there is a significant fraction of the population that does want to see AI permanently banned from creative endeavors. The loss of ownership over their work from having it be in the training set is a factor, but their main argument is that they specifically want to keep their current jobs as they are. They do not want to be replaced with AI, nor do they want to replace their existing drawing work with SEO keyword stuffed text-to-image prompts.[0] https://en.wikipedia.org/wiki/Seeing_Like_a_Statereplyrichk449 5 hours ago | root | parent | next [\u2013]Butlerian jihad is a good reference point. Something so bad happened that a large enough portion of the population was convinced to destroy thinking machines, and this no-computer norm was held in human society for a crazy long time (been too long for me to remember how long elapsed before Chapterhouse, which I think is the book where thinking machines start returning). It was a core belief of humanity that computers were bad, not a law imposed by a judge or legislature.So say a US judge did impose severe restrictions on LLMs through US copyright law. The giant companies that are using LLMs will just move to another country. And just like tax law, others will be happy to have them. Would the US start blocking inbound internet traffic from countries that don\u2019t have the same interpretation of copyright? That seems very unlikely.The point is that the only way LLMs get the butlerian jihad treatment is if the people rise up against them. Right now, that is nowhere close to happening.reply6gvONxR4sf7o 11 hours ago | root | parent | prev | next [\u2013]> The problem is that filtering the training set is naively O(n^2)There are standard ways to do it that are O(n), FYI.replyamluto 18 hours ago | parent | prev | next [\u2013]I would look at #1 here. Crawling the Internet to collect information is one thing. (And people putting text on the web without requiring authentication seem to be granting at least some kind of license to anyone who sends a GET request.). But crawling the Internet (via centralized robots or users\u2019 browsers), then storing that data and charging money to others for rights to that data (as Brave seems to be doing, quite explicitly) seems like it deserves a very different evaluation under factor #1.replybewaretheirs 18 hours ago | parent | prev | next [\u2013]Unpopular opinion time:A ML model is clearly a derivative work of its input.Here's what I think would be fair:Anyone who holds copyright in something used as part of a training corpus is owed a proportional share of the cash flow resulting from use of the resulting models. (Cash flow, not profits, because it's too easy to use accounting tricks to make profits disappear).In the case of intermediaries (e.g., social media like reddit & twitter) those intermediaries could take a cut before passing it on to the original authors.Obviously hellishly difficult to administer so it's unlikely to happen but I don't see a better answer.replymattbee 18 hours ago | root | parent | next [\u2013]I don't know what a fair settlement would be but I'm looking forward to a copyright-holder suing OpenAI to obtain one. These companies have no value if copyright can be enforced on their training data.replyvisarga 16 hours ago | root | parent | next [\u2013]I think there are ways around it. The simplest would be to generate replacement data, for example by paraphrasing the original, or summarising, or turning it into question-answer pairs. In this new format it can serve as training data for a clean LLM. Of course the public domain data would be used directly, no need to go synthetic there.An important direction would be to train copyright attribution models, and diff-models to detect when a work is infringing on another, by direct comparison. They would be useful to filter both the training set and the model outputs.replymattbee 14 hours ago | root | parent | next [\u2013]Would automated paraphrasing not be a derivative work of the original?replyDylan16807 16 hours ago | root | parent | prev | next [\u2013]> A ML model is clearly a derivative work of its input.Do you mean this in a copying sense or a mathematical sense?What if it's only storing 1 byte per input document?replycivilitty 18 hours ago | parent | prev | next [\u2013]That\u2019s not at all clear to me. IANAL but first of all it\u2019s a balancing test, not a bright-line test. The judge could focus on any one factor and make an argument for either side quite easily.Second, \u201cuse\u201d here could mean one of two things: training or inference. It\u2019s publishing the results of inference that can lead to actual effects on the market, not the training.At the end of the day, someone has to prove tangible harm.replyflangola7 18 hours ago | parent | prev | next [\u2013]Microsoft is gambling on the hope that model training will be ruled fair use. This makes it seem that outcome is unlikely.replybrookst 18 hours ago | root | parent | next [\u2013]Do you think a human learning something from reading is fair use? Or are we all copyright violators because reading that article altered our connectomes, and we may recall parts of it later?replyethanbond 18 hours ago | root | parent | next [\u2013]The point being raised is quite specific. Not sure if you\u2019re willingly ignoring it or what?The answer is no, because you reading the article didn\u2019t dramatically degrade its market value.An AI ingesting all content on the internet and then being ultra-effective at frontrunning that content for a large number of future readers does degrade its market value (and subsumes it into the model\u2019s value).replyivalm 18 hours ago | root | parent | next [\u2013]I disagree. People learning how to draw does degrade the future value of copyrighted work. Imagine the future where nobody was allowed to learn to draw, existing copyright value would skyrocket!replyethanbond 18 hours ago | root | parent | next [\u2013]Arguments like this are great for getting your side to go \"rah rah got 'em\" and really, really bad for convincing anyone else.Legal judgments generally focus on actual impacts rather than quirks that might exist in hypothetical universes.replytharkun__ 17 hours ago | root | parent | next [\u2013]While that may be what your parent intended I'm not entirely sure and there does exist the philosophical level discussion here. Or market economics level I guess.If your pool of people that can learn about topic X is restricted the outputs or their labor are more expensive. Now lift a continent of billions of people out of poverty, get them access to schooling, safety etc and see the market forces do the rest.Now equate ChatGPT et al with said billion people. Just that it runs on electricity. If quality is good enough of course. Which is hard to decide right now because of hype.replyivalm 9 hours ago | root | parent | next [\u2013]Your sentiment is exactly what I intended, albeit I was terse and a little facetious. ChatGPT is like introducing a bunch of new skilled labor, it\u2019s just for the first time this skilled labor isn\u2019t human. The fact that this skilled labor learned from copyrighted material is like saying human labor learned from copyrighted material.replyeverforward 14 hours ago | root | parent | prev | next [\u2013]This applies to so many things, though.The most obvious parallel to me is YouTube. There are a ton of people ingesting books, then transforming that information into a roughly paraphrased video for people to watch for free (ish). That devalues the books they read and paraphrased, because other people don't need to read them.Spark Notes devalue actual books in a way, because a lot of high schoolers read those instead of buying the actual book.Search engines have also supplanted books in large part, because I don't need a whole book to answer a specific question. I don't know anyone that owns an encyclopedia anymore.This is the next iteration of these processes. Non-novel information's market value has been degrading for decades now. A series of questions that would have cost thousands of dollars in books to answer in the 70's/80's is now free, with or without AI.replyethanbond 13 hours ago | root | parent | next [\u2013]LLMs are attracting so much positive attention because they are likely to be a huge, huge step change improvement than all those methods you mention.For that same exact reason, it\u2019s totally reasonable they\u2019re attracting unique amounts of negative attention too.You can\u2019t have it both ways: yes LLMs are going to change information retrieval the way nothing else has before, but no it\u2019s actually just like all the other things in terms of their impact on incentive structures.FWIW I don\u2019t really know where I land on this issue. I just find it totally incoherent to believe in the bull case of \u201cthis will transform everything\u201d while also portraying it all as par for the course when discussing potential negatives.Just because Spark Notes didn\u2019t obviously manage to kill valuable parts of our information ecosystem and economy does not mean that Spark Notes x 10,000,000 will not.replycma 15 hours ago | root | parent | prev | next [\u2013]> The answer is no, because you reading the article didn\u2019t dramatically degrade its market value.How about if you read a news article to write a competing one rewording and possibly citing it (one of the most common practices in news)?replyethanbond 13 hours ago | root | parent | next [\u2013]How about it? Do you not think it incurs a lot of negative effects?replysnickerbockers 10 hours ago | root | parent | prev | next [\u2013]Yes it is considered fair use but it's also completely irrelevant because we're talking about a computer program not a person.replyxp84 19 hours ago | prev | next [\u2013]From article:> without any worry for copyright infringement because Brave acts as a middleman.This isn\u2019t how law works. Unless Brave is explicitly indemnifying all their customers (which their lawyers would have to be insane to let them do), any trouble you could get in, is going to be 100% your problem. Pointing the finger at Brave could theoretically get them in trouble too, but would in no way let you off the hook.replyisodev 20 hours ago | prev | next [\u2013]I firmly believe that corps like these don't deserve the benefit of the doubt. Google, Brave and really anyone big enough to allow themselves to do bad things and get away with it must adhere to a standard where they proactively show their stuff doesn't have malicious intents.replysourcecodeplz 20 hours ago | parent | next [\u2013]As always, if the product is free, you are the product...replywoah 20 hours ago | root | parent | next [\u2013]Did you read the article? This is about a paid web crawling api that the author thinks is too good or something. Nothing about a free productreplyisykt 20 hours ago | root | parent | prev | next [\u2013]\u201cAlthough the saying tells us \u201cIf it\u2019s free, then you are the product,\u201d that is also incorrect. We are the sources of surveillance capitalism\u2019s crucial surplus: the objects of a technologically advanced and increasingly inescapable raw-material-extraction operation. Surveillance capitalism\u2019s actual customers are the enterprises that trade in its markets for future behavior.\u201dExcerpt From The Age of Surveillance Capitalism Shoshana Zuboffreplyk__ 18 hours ago | prev | next [\u2013]The websites a Brave user browses are anonymously relayed to their servers for indexing/training. So, they crawl the web without a crawler and the website operators can't do anything about it.That's genius!replyjonathansampson 15 hours ago | parent | next [\u2013]I'm Sampson, from the Brave team. The Web Discovery Project is a clever approach. For Brave to compete with Google, and offer a truly novel index of the Web, a novel approach must be taken. The WDP is an opt-in, privacy-preserving approach which gives Brave a fighting chance against the Search incumbants. Due to our preference of \"Can't be evil\" over \"Don't be evil,\" the WDP is not only designed with privacy and anonymity as a prerequisite, but it is also open-source for public scrutiny and evaluation: https://github.com/brave/web-discovery-project.replyyamsamnow 10 hours ago | root | parent | next [\u2013]It's not a clever approach, it's basically scraping Google results because that's where your users are searching. You follow the bread crumbs from Google searches.Cliqz entire history was based on this kind of thing, milking off other search engines by just deducting their ranking methods, it's parasitic. There's no cleverness about it.replythrowaway675309 10 hours ago | root | parent | next [\u2013]I don't know a lot about this particular approach but your comment that it's just using Google results is blatantly false. It all depends on the search engine that the brave user is leveraging, or no search engine if they type in the URL directly into the header.replyyamsamlater 8 hours ago | root | parent | next [\u2013]Nonsense. This naive idea that Brave innocuously looks at user's traffic patterns.Google owns 95% of the market in most Western markets. There's no \"blatantly false\" about that.They scrape search engine results and present them as their own.Do 10,000 searches on Google and Brave and you'll see how similar they are. It's as simple as that, scraping by sleight of hand.Why can't they be a normal search engine - because they need to scrape others. Simples.replyk__ 11 hours ago | root | parent | prev | next [\u2013].es?replythrowaway72762 20 hours ago | prev | next [\u2013]I think this title is overstated. It seems like Brave is trying to do the right thing here vs other companies that don't even make the attempt. (Also, crawling as a service has been a thing for a while.)replyjsnell 20 hours ago | parent | next [\u2013]> It seems like Brave is trying to do the right thing here vs other companies that don't even make the attemptI feel like I'm missing something. What the article claims they're doing is:1. Misrepresenting what rights they have, and selling access to those rights.2. Stealth-crawling the web, hiding from the webmasters just how much Brave is crawling their site, and making it impossible to block just their crawler.How is either of these the right thing? I mean, for somebody besides Brave. What \"attempt\" are they making that other companies aren't?replythrowaway72762 20 hours ago | root | parent | next [\u2013]I think the first one seems to be a case where Brave just has incomplete information about licensing so for the Wikipedia data and other CCthey need to provide a link.The second doesn't seem like a problem to me as long as they respect robots.txtreplyjsnell 19 hours ago | root | parent | next [\u2013]You didn't actually answer the question, at best you've sidestepped it by claiming that the dodgy shit is either by accident, or really not that bad. Maybe so.But your original claim wasn't just \"Brave are technically not doing anything illegal\" or \"they're no worse than the others\". It was praising them for being better than the others, that they're the only ones trying to do the right thing. And for these example it's just not true, they're outright worse than the industry standard.So, to repeat, what makes you think that \"Brave is trying to do the right thing while other companies aren't even attempting\"?replyskilled 19 hours ago | root | parent | prev | next [\u2013]I think you're missing the point. This is one example that uses a specific license, there are countless other licenses.And you don't seem to have read the article either, because clearly it was explained that they don't respect robots.txt because they have no user-agent.replywoah 20 hours ago | root | parent | prev | next [\u2013]Is there something wrong with accessing information that someone has posted for public access?replyJumpCrisscross 19 hours ago | root | parent | next [\u2013]> Is there something wrong with accessing information that someone has posted for public access?The Wikipedia example is glaring. They\u2019re scraping content, stripping attribution and reselling it with a right to lock it down in a way that is not allowed by the original license.Brave is laundering copyleft content while lying to their customers by selling a license they can\u2019t give. If you\u2019d like, you can sidestep the morality of copyright entirely and focus on the plagiarism and fraud.replytheamk 19 hours ago | root | parent | prev | next [\u2013]Yes. Legalities aside, stripping attribution (author names) from contents which specifically requires keeping it, it a really shitty thing to do.(The fact that they include original URL does not change much, given that they explicitly market it as \"Data for AI\" and those systems never have attribution)replylopatin 19 hours ago | prev | next [\u2013]Why use brave if my info is already being leaked by third parties? E.g. experian. Is it worth the inconvenience and their repeated tricky attempts at monetizing their security conscious niche? Not being facetious, just a real question from a non security conscious person.replyasynchronous 18 hours ago | parent | next [\u2013]It\u2019s built in Ad blocker and other features are heads and tails above anything else I\u2019ve used before, personally.replysoundnote 16 hours ago | parent | prev | next [\u2013]You get degoogled Chromium with e2ee bookmarks etc. sync and a lot of nice convenience features like vertical tabs and mobile background video playback.And if it's your cup of tea, they let you straight up pay money for the search engine.replykodah 18 hours ago | prev | next [\u2013]Unpopular opinion: the next iteration of privacy laws needs to factor in AI. If AI is allowed to slurp up PII or derogative works and the people defending it defend it with the zeal of cryptobros then we're in for a decade of real pain in terms of both copyright law, PII, and IP exposure.replyasynchronous 18 hours ago | parent | next [\u2013]AI is going to do that irregardless- the debate is essentially going to revolve around how and what people can make new commercial works from that data.replyFishInTheWater 18 hours ago | parent | prev | next [\u2013]The fun part is that the GDPR already does. The answer is you're not allowed to use personal data for AI. (And \"personal data\" here covers things like all public social media posts)Facebook recently got told by the CJEU that, no, they can't use people's posts to target advertisements. Even if those ads are what's paying for the platform. That you can't claim such processing as \"part of the contract\" unless it is absolutely necessary in the same way the post office needs an address to send a parcel.If Facebook can't even do that, there is no way LLMs will be allowed. (And remember. The GDPR does not care if your system doesn't distribute personal data. Any kind of processing at all falls under the GDPR's requirements)OpenAI is already being chased by the EU's privacy agencies. Right now they're in the process of asking pointed questions, things will heat up after that.replyBeFlatXIII 14 hours ago | root | parent | next [\u2013]End result: EU AI enjoyers use a VPN plus a US-based credit card borrowed from a friend.replyricardo81 11 hours ago | prev | next [\u2013]My entirely biased opinion is https://www.mojeek.com/ - a traditional search engine crawler (as in, follow links on the web) that identifies its user agent. Dead Simple. The open web, you can search on it.replylern_too_spel 18 hours ago | prev | next [\u2013]Brave continues to be shady. They claim to respect robots.txt but don't identify their crawler if you want to block it.> They don't mention their crawler anywhere in their docs, either. So, if you wanted to block Brave from crawling and indexing and ultimately selling your content to third parties, your only option for the time being would be to block all crawlers, which is how Brave would be able to \"respect robots.txt\".replyverisimi 20 hours ago | prev [\u2013]How long until IP works its way onto ai training data or ais themselves? Ie that for some specific instance, the training is intentionally wrong, so as to check and prove that there has been a breach of IP.replyhomeless_engi 20 hours ago | parent | next [\u2013]Do you mean like trap streets? Seems like a good idea for model makershttps://en.wikipedia.org/wiki/Trap_streetreplylacrimacida 20 hours ago | root | parent | next [\u2013]Yeah, something like that may be already happening and various actors building their cases as we speak.replyDesktopMonitor 20 hours ago | parent | prev | next [\u2013]While not intentionally wrong, Van Halen's brown M&M's rider comes to mind as an example of a similar measure.replythe8472 19 hours ago | parent | prev [\u2013]Depends, how do you distinguish humans acquiring knowledge by ingesting copyrighted content vs. a human using an AI that ingested copyrighted content?replyJumpCrisscross 19 hours ago | root | parent [\u2013]> how do you distinguish humans acquiring knowledge by ingesting copyrighted content vs. a human using an AI that ingested copyrighted contentDoesn\u2019t matter when the content is reproduced verbatim, as Brave is doing. If I memorise your content and then repeat it as my own, I\u2019m not somehow off the hook for copyright violation and plagiarism.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- Brave, a popular browser, is under scrutiny for allegedly selling copyrighted data for AI training.\n- The company is accused of misrepresenting their rights to the data and selling access to it without proper attribution.\n- Brave's crawler, used for web indexing, does not identify itself to website operators, making it difficult to block."
  }
]
