[
  {
    "id": 40368016,
    "title": "Exploring Adobe Photoshop 1.0.1 Source Code",
    "originLink": "https://computerhistory.org/blog/adobe-photoshop-source-code/",
    "originBody": "Menu Explore Overview Stories Timelines Collections Playlists Blogs Publications Connect Overview Events K-12 Students & Educators Families & Community Groups Volunteers Visit Overview Plan Your Visit Exhibits Public Tours & Group Reservations Venue About Overview This Is CHM Centers Programs News Leadership Join & Give Overview Ways to Give Donor Recognition Institutional Partnerships Buy Tickets Donate Hours & Admission Subscribe Upcoming Events Search for: Or search the collection catalog Chm Blog From the Collection Adobe Photoshop Source Code By Leonard J. ShustekFebruary 13, 2013 Share Software Gems: The Computer History Museum Historical Source Code Series pho·to·shop, transitive verb, often capitalized ˈfō-(ˌ)tō-ˌshäp to alter (a digital image) with Photoshop software or other image-editing software especially in a way that distorts reality (as for deliberately deceptive purposes) — Merriam-Webster online dictionary, 2012 When brothers Thomas and John Knoll began designing and writing an image editing program in the late 1980s, they could not have imagined that they would be adding a word to the dictionary. Thomas Knoll John Knoll Thomas Knoll, a PhD student in computer vision at the University of Michigan, had written a program in 1987 to display and modify digital images. His brother John, working at the movie visual effects company Industrial Light & Magic, found it useful for editing photos, but it wasn’t intended to be a product. Thomas said, “We developed it originally for our own personal use…it was a lot a fun to do.” Gradually the program, called “Display”, became more sophisticated. In the summer of 1988 they realized that it indeed could be a credible commercial product. They renamed it “Photoshop” and began to search for a company to distribute it. About 200 copies of version 0.87 were bundled by slide scanner manufacturer Barneyscan as “Barneyscan XP”. The fate of Photoshop was sealed when Adobe, encouraged by its art director Russell Brown, decided to buy a license to distribute an enhanced version of Photoshop. The deal was finalized in April 1989, and version 1.0 started shipping early in 1990. Over the next ten years, more than 3 million copies of Photoshop were sold. The box and disk for the original version of Photoshop on Mac. That first version of Photoshop was written primarily in Pascal for the Apple Macintosh, with some machine language for the underlying Motorola 68000 microprocessor where execution efficiency was important. It wasn’t the effort of a huge team. Thomas said, “For version 1, I was the only engineer, and for version 2, we had two engineers.” While Thomas worked on the base application program, John wrote many of the image-processing plug-ins. The splash screen of Photoshop 1.0.7. With the permission of Adobe Systems Inc., the Computer History Museum is pleased to make available, for non-commercial use, the source code to the 1990 version 1.0.1 of Photoshop. All the code is here with the exception of the MacApp applications library that was licensed from Apple. There are 179 files in the zipped folder, comprising about 128,000 lines of mostly uncommented but well-structured code. By line count, about 75% of the code is in Pascal, about 15% is in 68000 assembler language, and the rest is data of various sorts. To download the code you must agree to the terms of the license, which permits only non-commercial use and does not give you the right to license it to third parties by posting copies elsewhere on the web. Download Photoshop version 1.0.1 Source Code 1990 version of Adobe Photoshop User Guide 1990 version of Adobe Photoshop tutorial Commentary on the source code Software architect Grady Booch is the Chief Scientist for Software Engineering at IBM Research Almaden and a trustee of the Computer History Museum. He offers the following observations about the Photoshop source code: “Opening the files that constituted the source code for Photoshop 1.0, I felt a bit like Howard Carter as he first breached the tomb of King Tutankhamen. What wonders awaited me? I was not disappointed by what I found. Indeed, it was a marvelous journey to open up the cunning machinery of an application I’d first used over 20 years ago. Architecturally, this is a very well-structured system. There’s a consistent separation of interface and abstraction, and the design decisions made to componentize those abstractions – with generally one major type for each combination of interface and implementation – were easy to follow. The abstractions are quite mature. The consistent naming, the granularity of methods, the almost breathtaking simplicity of the implementations because each type was so well abstracted, all combine to make it easy to discern the texture of the system. Having the opportunity to examine Photoshop’s current architecture, I believe I see fundamental structures that have persisted, though certainly in more evolved forms, in the modern implementation. Tiles, filters, abstractions for virtual memory (to attend to images far larger than display buffers or main memory could normally handle) are all there in the first version. Yet it had just over 100,000 lines of code, compared to well over 10 million in the current version! Then and now, much of the code is related to input/output and the myriad of file formats that Photoshop has to attend to. There are only a few comments in the version 1.0 source code, most of which are associated with assembly language snippets. That said, the lack of comments is simply not an issue. This code is so literate, so easy to read, that comments might even have gotten in the way. It is delightful to find historical vestiges of the time: code to attend to Andy Herzfield’s software for the Thunderscan scanner, support of early TARGA raster graphics file types, and even a few passing references to Barneyscan lie scattered about in the code. These are very small elements of the overall code base, but their appearance reminds me that no code is an island. This is the kind of code I aspire to write.” And this is the kind of code we all can learn from. Software source code is the literature of computer scientists, and it deserves to be studied and appreciated. Enjoy a view of Photoshop from the inside. Early Photoshop screenshots* The home screen, showing the available tools. Photoshop allowed you to select brush color as well as size and texture. (The first color Mac was the Macintosh II in 1987.) There were some sophisticated selection tools, and a good assortment of image filters. One important missing feature, which came with version 3 in 1994, was the ability to divide an image into multiple layers. The preferences page allowed for some customization of the features. There was a limited choice of fonts, font sizes, and font styles. The text was entered into this dialog box, then moved into the image. Screen shots courtesy of creativebits, www.creativebits.org. Historical Source Code Releases MacPaint and QuickDraw Source Code, July 18, 2010 APL Programming Language Source Code, October 10, 2012 Adobe Photoshop Source Code, February 13, 2013 Apple II DOS Source Code, November 12, 2013 Microsoft MS-DOS Early Source Code, March 25, 2014 Microsoft Word for Windows Version 1.1a Source Code, March 25, 2014 Early Digital Research CP/M Source Code, October 1, 2014 Xerox Alto Source Code, October 21, 2014 Electronic Arts DeluxePaint Early Source Code, July 22, 2015 About The Author Len Shustek is the founding chairman emeritus of the board of trustees of the Computer History Museum. Join the Discussion Related Articles View all articles CHM BLOG A Computer for the Rest of Us December 15, 2023 CHM BLOG How Old Is Your Furby? December 13, 2023 CHM BLOG Turtles, Blocks, and Memories December 05, 2023 View all articles Share Terms of Use Privacy © 2024 Computer History Museum CHM is a registered 501(c)3 organization. Your donation is tax-deductible to the extent permitted by law. Tax ID 77-0507525. Press Careers Venue Events Stories Donate Hours and Directions Hours & Directions 1401 N. Shoreline Blvd. Mountain View, CA 94043 (650) 810-1010 More Contact Info",
    "commentLink": "https://news.ycombinator.com/item?id=40368016",
    "commentBody": "Adobe Photoshop Source Code (2013) (computerhistory.org)534 points by PaulHoule 18 hours agohidepastfavorite198 comments pmcjones 14 hours agoIn the aughts I worked at Adobe and spent time trying to archive the source code for Photoshop, Illustrator, PostScript, and other apps. Thomas Knoll's original Mac floppy disk backups were available, so I brought in my Mac Plus, with a serial cable to transfer the files to a laptop via Kermit. The first version was 0.54, dated 6 July 1988. The files on the floppies were in various ancient compressed archive formats, but most were readable. I created an archive on a special Perforce server of all the code that I found. Sadly, the earliest Illustrator backups were on a single external disk drive that had gone bad. reply xd1936 14 hours agoparentThank you for your service. Super cool project. Hopefully they make their way to archive.org or Github someday. reply pmcjones 14 hours agorootparentAdobe has the only copy, and they have donated early versions of PostScript (https://computerhistory.org/blog/postscript-a-digital-printi...) and Photoshop; people should ask Adobe to release more. Everything I find in the public domain I post at https://www.softwarepreservation.org/projects . reply jart 13 hours agorootparentWow are you the one that posted the original LISP 1.5 source code? I colorized that and used it to good effect in my blog posts. https://justine.lol/sectorlisp/#listing reply pmcjones 12 hours agorootparentI beat the bushes for the source code, documenting my finds (https://mcjones.org/dustydecks/archives/category/lisp/) and posting them (https://www.softwarepreservation.org/projects/LISP/lisp15_fa...), but the early work was done by Jack Harper, Pascal Bourguignon, Rich Cornwell and Bob Abeles, Andru Luvisi, Angelo Papenhoff, Al Kossow, and others. reply mistrial9 13 hours agorootparentprevisn't the topic the Patents, not the code? The code is mired in Mac toolbox details, no? reply userbinator 9 hours agorootparentPatents expire after 20 years at most, I believe. Everything from before 2004 has expired already. reply reaperman 13 hours agorootparentprevEven the parents from 1988-2000 would be well expired now reply irrational 11 hours agorootparentPoor moms and dads. reply 3abiton 10 hours agorootparentprevCan you patent open-source code? reply p_l 2 hours agorootparentArguably patents as intended should require description of patented device/technique in detail that allows replicating it - effectively open sourcing it. The patent then serves as temporary moat on applying that specific technique or producing that specific device to refund the inventor. In practice... well, sometimes patents were used to recover details of closed-source software or hardware. reply simondotau 8 hours agorootparentprevOf course you can. If you invented it, and nobody else has patented it, you can patent it. Opening the source doesn’t invalidate your rights as an inventor or copyright holder, though it can add confusion and/or complexity to the enforcement of both the patent and the open source license. reply andyferris 7 hours agorootparentYes, but my understanding is that publically detailing how your invention works _before_ trying to patent it means the invention becomes public knowledge/prior art. That is, so long as you submit a patent application before releasing the open-source code, it should be OK, but there's no much you can do once the cat is out of the bag. reply simondotau 5 hours agorootparentI imagine the rules and best practices would vary between jurisdictions, but basically yeah. But as soon as you file for the patent, you can release source and enjoy the confusion. (Based on 30 seconds of googling, it seems that the USA and Australia gives inventors a 1 year grace period after publishing, but the granted patent might not be valid in other countries.) reply p_l 2 hours agorootparentUSA also used to not consider publications or patents published outside USA as prior art, to the point of granting patents that were rewritten from someone else's patent in another country. Not sure if it got better or worse with WTO patent rules. reply Shinchy 1 hour agoparentprevWow, oddly enough that kind of sounds quite fun. reply mistrial9 13 hours agoparentprevthe Illustrator guy was in Palo Alto and approachable .. at the time the feedback was that the interface interactions were not great .. hard to say now, but Freehand became popular quickly, then folded. reply mannyv 10 hours agorootparentFreehand still hasn't been beat, even after all these years. Never did like Illustrator, but like everyone learned to use it once Freehand bit the dust. reply yelling_cat 7 hours agorootparentI hadn't thought about Freehand in a decade and now I'm angry at Adobe for killing it all over again. It never got in your way and let you fully focus on your work. Illustrator never lets you forget that you're using a tool to create things like Freehand did. reply p_l 2 hours agorootparentIs there some good summary somewhere? I remember having, ahem, access to both as a teen but Illustrator had more brand effect behind it. reply pmcjones 13 hours agorootparentprevMike Schuster, who by the way is a superb programmer. reply nunez 8 hours agoparentprevThat's awesome. reply butchlugrod 17 hours agoprevGreat write-up here about what it takes to build the app from this source code: http://basalgangster.macgui.com/RetroMacComputing/The_Long_V... reply lelandfe 14 hours agoparentBlown away as I read more posts on this site. Not many people out there with this sort of knowledge. Thanks for the link. reply butchlugrod 13 hours agorootparentI know! I have read every piece on the site, and they really go into some fantastic detail about old Apple stuff. No idea who this person it, and they have not posted in a few years, but would love to know more about their background. Almost certainly a developer inside Apple in the 80s and 90s. reply internetter 13 hours agoparentprevAnyone having trouble adding this to their feed reader? The RSS works fine on my end, but Miniflux says This website is too slow and the request timed out: Get \"http://basalgangster.macgui.com/RetroMacComputing/The_Long_View/rss.xml\": dial tcp 209.182.219.107:80: i/o timeout reply hexagonwin 12 hours agorootparentseems to work just fine for me. maybe it was a temporary issue? reply kasajian 16 hours agoprevI remember traveling to Adobe in the mid-90s to exchange source-code with them, 'cause PhotoShop was MacApp based, and they had a layer working on Windows. And we traded an in-process SQL engine. I recall brining home some of the code, there were definitely parts of PhotoShop that were included, but not a lot. Just some funky color-space calculations that we ignored. I'm looking forward to looking at the source to see if there's any remnants of MacApp in the mix. They may have changed everything since the mid 90s. Who knows. reply irq 14 hours agoparentI love this story - code trading is such a cool idea, and one I haven't heard of much before. Anyone else have any code trading stories? reply exe34 13 hours agorootparentin academia/research, it's quite normal. often you wish they hadn't given you the code and provided an equation instead. reply mk_stjames 14 hours agoparentprevThey call that out as an exception specifically actually: \"All the code is here with the exception of the MacApp applications library that was licensed from Apple\" reply jaredwy 6 hours agoparentprevWorked on photoshop for many years. It’s still there today. reply mistrial9 13 hours agoparentprevMacApp on Windows ?!! of course.. what a bloatware.. Think Class Library saved the life of lots of devs. Greg Dow might still work for Adobe today. ps- PowerPlant was even better than TCL now thinking on it.. https://en.wikipedia.org/wiki/PowerPlant reply TIPSIO 17 hours agoprevIncredible that the UX is still generally the same. What a vision the original engineers had. I am annoyed today though every time I open the app. The only time it has ever felt snappy on desktop was a sweet period when the MacBook Pro M1 first came out and Adobe Photoshop had a Silicon beta out. Those days are long gone and we are slow again. reply ompogUe 16 hours agoparentNot sure if you worked with it in the early '90's, but on a Mac w/4MB of RAM, it took ~5-10 minutes to undo a Guassian Blur. The pain was real. The way to go back then was the SGI Indigo w/96MB. It worked best for me in the late '90's on a 9500, and even then needed an entire GB of RAM. reply alamortsubite 16 hours agorootparentHa! In the early '90's the way to go was Live Picture [1]! Your undo would have been instantaneous! Unfortunately, Live Picture only ran on Mac. Photoshop was a bit janky on SGI back then, IIRC, but still the better of the two platforms overall. [1] http://lensgarden.com/uncategorized/live-picture-software-th... reply huxley 14 hours agorootparentHahaha that’s Old School. Live Picture was one of several photo compositor tools that focused on Photoshop’s pain points. Fauve Matisse was a little earlier than Live Picture and I believe it introduced layers to Mac photo editing. They ended up getting acquired by Macromedia (or perhaps even Macromind) after a rewrite to compete with Live Picture it was renamed Xres and then abandoned. reply Daub 5 hours agorootparentprevI believe that Livepicture was fast because they loaded the full image as a set of tiles. I also believe that Photoshop was 'inspired' to introduce layers in version 3 in response to Livepicture's layers. It was layers which caused Photoshop to explode in popularity. Adobe then went on to sue Macromedia for using tabs in their interface. Bummer. reply ompogUe 15 hours agorootparentprevYes, I remember Live Picture! It was slick. I actually spent more time in that and Fractal Design Painter, than Photoshop back then. reply Daub 5 hours agorootparentprevPeople forget that Photoshop worked on a Silicon Graphics box. It was indeed the way to go, so long as you could afford it. reply apercu 14 hours agorootparentprev\"SGI Indigo\". I had one of these. Not for Photoshop but still... reply DonHopkins 14 hours agorootparent\"Indy: an Indigo without the 'go'\". -- Mark Hughes (?) http://www.art.net/~hopkins/Don/unix-haters/tirix/embarrassi... reply yjftsjthsd-h 13 hours agorootparent> There are too many daemons. In a vanilla 5.1 installation with Toto, there are 37 background processes. Comparing the output of `ps aux` on a default install of Debian and OpenBSD still gives me this feeling:) reply nullhole 11 hours agorootparentprevExcellent joke. Indy still had the best looking case, though, I think. There's something about that sliced-box appearance that's so unexpected and interesting. reply boffinAudio 3 hours agorootparentI have my SGI O2 sitting on top of my Indy, just because the contrast in design ethos inspires me, somehow. reply aredox 17 hours agoparentprevIs it because the UX is good or because changing it is impossible without the users rebelling en masse? reply kjellsbells 17 hours agorootparentI cant speak for all PS users, but it's not that it is a special UX so much that it is embedded in the muscle memory of the user community, and that degree of familiarity contributes mightily to people being able to get work done quickly. The closest example I cam think of, which people inside Adobe most certainly know about, is the failed attempts by Quark Xpress to update their product in the late 90s/early 00s, which led to them losing a 95% market share position to Adobe InDesign. You do not mess with the tools that a loud and creative community rely upon to get their jobs done. reply bonaldi 14 hours agorootparentis the failed attempts by Quark Xpress to update their product in the late 90s/early 00s There were a number of factors here - outsourcing engineering leading to a disastrously buggy 4.0, then failing to move to OS X for years after the market was ready to, hostile and arrogant approach to customers (\"where else will they go?\") and finally the misbegotten attempt to turn a DTP app into a web design tool. InDesign 1 was fairly clunky, but everyone was desperate to escape. It's an Amiga-like shambles of mismanagement that wasted an early lead; I am still nostalgic for both tbh. reply frankharv 14 hours agorootparentI don't know if I agree about InDesign being clunky. The problem was everybody liked PageMaker7 and nobody wanted something new. How about Audacity? The clowns simply bought CoolEdit and renamed it. Very innovative. reply frankharv 12 hours agorootparentWhoops I meant Adobe Audition=CoolEditPro reply invalidlogin 12 hours agorootparentprevAudacious. reply brazzledazzle 16 hours agorootparentprevAdobe actually changed a bunch of shortcuts at least a couple of points between photoshop 7 and creative cloud. I remember how I'd developed muscle memory that took a bit to fully overwrite. reply tambourine_man 14 hours agorootparentThere are settings to revert them all reply brazzledazzle 14 hours agorootparentYeah but I wanted to maintain \"compatibility\" with others using the software whether for discussion's sake or so I can hop on their workstation and not have to think about changing anything. Turning those legacy settings on and having that survive restarts could be flaky/buggy. I got the impression keeping that functionality well tested wasn't the highest thing on their development priorities. reply tambourine_man 12 hours agorootparentAdobe needs an easier and broader “settings” in the cloud. It should be as easy as login in to have your completely bespoke Photoshop greeting you. reply pests 5 hours agorootparentSlightly different experience, but logged into my friends Google TV the other day and it had all my apps and I was correctly signed into everything, background and screensaver all set up. Very smooth experience. reply philistine 16 hours agorootparentprevThe only way Adobe can get out of this conundrum is by announcing a transition to a new interface, finding ways to incentivize schools to teach the new interface, while keeping the old one around for as long as possible to give time for the oldies to slowly retire. We're talking decades. reply basch 15 hours agorootparentThe user interface is extremely customizable. You can have a default layout and still keep legacy ones around. You wouldn’t need to kill the legacy layout unless you are removing the cuetomizability. reply nineteen999 8 hours agorootparentprevIt can also work the other way though on rare occasions. The Blender UI revamp had the opposite effect, it helped drew more users to the platform (although so did the addition of Cycles and later EeeVee renderers). reply nprateem 13 hours agorootparentprevIntellij are about to learn this lesson unfortunately. reply biofox 17 hours agorootparentprevNo point fixing something that isn't broken (someone please tell Microsoft) reply turnsout 16 hours agorootparentIn Photoshop there are at least three completely different dialog boxes [0] for saving an image as a JPEG, each with totally different UI widgets and functionality. They simply refuse to revise anything in the interface—they just keep adding. It's the software equivalent of hoarding. [0] Save/Save as, Export As, Save for Web (Legacy) reply bombcar 16 hours agorootparentI'm ride-or-die on Save for Web (Legacy), it's the way to go by far. Now if they'd just integrate the tinypng plugin that was deprecated in 2023 - https://help.tinify.com/help/deprecation-of-the-photoshop-pl... reply turnsout 14 hours agorootparentSame—I just wish they'd either drop the \"(Legacy)\" and admit that they can never remove it, or add those same features to the Export dialog! reply reddalo 1 hour agorootparentprevHow long has the Save for Web been \"Legacy\"? I feel like it's been there for a long time. reply Rinzler89 17 hours agorootparentprev>No point fixing something that isn't broken But how do you know a UX isn't broken, when you've only seen one iteration of it you're whole life and have nothing else to compare it to? Kind of like Plato's Cave Allegory. You have to try new things, and if you see them fail, then you know which one was the best. reply beau_g 15 hours agorootparentUse gimp, add a new layer, paste something into it, and resize it, then you will know reply frankharv 14 hours agorootparentHow about PhotoShop's Magic Lasso? I have not found many tools that work as well and make productivity great. reply Dylan16807 16 hours agorootparentprev\"try\" doesn't imply \"ship to millions of customers\" And sometimes there's good enough and you should leave it alone. QWERTY isn't optimal but it's not very far from it. reply Rinzler89 16 hours agorootparentBut if you're not gonna ship it to all your millions of users and receive the outrage as feedback, how will you ever know if it works or not? A/B testing to a few users only works in web-app front-ends, not in professional tools where all single end-user releases need to look and act exactly the same. Shipping to only a handful of users wasn't a thing in the era of \"Gold CD\" releases, and even in the era of the internet updates, nobody will want to take part in A/B testing and end up with a different Photoshop UX than what his colleague is using, so you either ship to all or none. So it seems Photoshop's UI is more of a cause of inertia and resistance to change, rather than nailing right the first time. reply marginalia_nu 16 hours agorootparentA/B testing does very little to improve any UX. It's got merits in performance optimization, where the implementation differs but the contract is static between A and B, but with user interfaces, it generally leads to pessimizations as usage is not proportional to usefulness. The rare exception is single-purpose interfaces where increasing one singular interaction is an end in itself, e.g. a marketing page, but that's a pretty unique case that is very far removed from a productivity tool. reply Dylan16807 15 hours agorootparentprevYou bring in testers and UI experts, and you have the experts watch the testers. Shipping to mass market is a bad way to get feedback. reply Karunamon 15 hours agorootparentAre there such a thing as UI experts anymore? It seems like we only have designers left, and I am none too thrilled about their influence. reply aredox 13 hours agorootparentprevAZERTY is very bad and France is still stuck with it, despite Québec having a variant of QWERTY for decades, ditto for Switzerland having a custom QWERTZ, and BÉPO being heads above. reply eimrine 15 hours agorootparentprevQwerty is a significant brake in learning English from scratch. reply Dylan16807 15 hours agorootparentHow so? And I'm pretty sure an alphabetical keyboard would do much more harm than good, if that's the implied alternative. reply eimrine 18 minutes agorootparent> How so? I am not native but I feel pleasure from using Dvorak and chatting with no need to look at keyboard. I am trying to spread acknowledgement about Dvorak among young folks and those who agreed to learn feel the same boost. The difference is unimaginable for those who already has some muscle memory for anything related to Qwerty such as Ctrl-C. So the difference is between creating the layout for touchtyping goals and creating the layout for any other ones. reply pests 5 hours agorootparentprevEvery time I'm confronted with an alphabetical keyboard my brain malfunctions. It should be easy but I'll be on R or S looking for the T and spend 5 minutes scanning for it. Usually via remote with some crappy app trying to login or search and its on a big TV huge, usually people watching making the awkwardness even worse since everyone else can obviously see where the T is. /end rant reply navjack27 16 hours agorootparentprevHard to do when the power users for the most part try to block analytics and the insider testers have very fluid workflows and there is no such thing as death by a thousand papercuts to them. They aren't getting the signal because the people that should be telling them the signal are actively denying sending the signal. reply eimrine 15 hours agorootparentАre you saying this about OS which shows ads in Start menu? reply bufferoverflow 16 hours agorootparentprevAs someone, who used Photoshop a lot, the UI/UX is good. It would be pretty hard to make it significantly better. And yes, even if you somehow made it better, many users would complain, because they have muscle memory of the UX, and are extremely efficient with it. reply maxglute 16 hours agorootparentprevIt would be curious to see UX timeline of what PS influenced, and what influenced it, in mouse age. A lot of desktop derivative products seem to hold on PS-like UI, it's all very mutually reinforcing. I'm not sure what iPad UX is like. I remember autocad products also adding ribbon system and it wasn't end of the world, but also very few ppl that I know end up migrating. Part of me feels like it's... either very optimal for masses to learn because very few PSers (outside of photography) I know have professional peripherals (some have hotkey stickers on keycaps), vs lots of other creative fields have specialized decks/hardware to make streamline workflow. Like part of me feels like there is a better physical hardware implmentation to manipulate all the curves/histograms other than moving around with mouse, but mouse+keyboard is... good enough. reply bonaldi 14 hours agorootparentA lot of PS 1.0's UI (2-col toolbox on the left etc) owes its heritage to MacPaint, which was a launch app for the Mac. Even the iPad shares keyboard shortcuts set by the original Mac, though has considerably broken away in other aspects. reply BeFlatXIII 16 hours agorootparentprevConsidering how many complaints about GIMP UI being bad with no more substance than \"Just compare it to Photoshop!\", I'd bet 65% on option B. reply bonestamp2 13 hours agorootparentprevWhen Photoshop went subscription I bought the full version of CS6 (or whatever the last non-subscription version was). It was very expensive. Then when that stopped working on Mac, I tried using every reasonable competitor, paid for several. I'm sure some of them are very competent tools, but it was a nightmare trying to learn a new UI. I bit the bullet and started paying the subscription. reply ljm 13 hours agoparentprevThe Messy Middle is an incredible book that essentially details how the CEO of BeHance, back in the day, rewrote Adobe's offering for the cloud, and detailed how he'd do it. Scott Belsky - now investor himself - writing how he sold both BeHance and Adobe down the road for the rent economy. I say The Messy Middle is an incredible book, but it is shelf help for dwindling execs. To their generic credit, the open source scene for artistry and imagery is better than it ever was, because everybody has been priced out of the pro tools that actually can't keep up without community support. reply grishka 16 hours agoparentprevJust downgrade? I still use some version from 2022, the first M1-compatible one that was cracked. Still as snappy as it was 2 years ago. reply darknavi 17 hours agoparentprevI still use an old CS6 license and while it's snappy in the app, it still takes its time to boot. reply vondur 11 hours agoparentprevBack in 1997-98 we had Pentium II machines (450mhz) with fast SCSI drives and 128 MB of ram that were fast Photoshop machines. I also remember it being pretty fast on the G3 Mac's when they first came out. reply MenhirMike 11 hours agorootparent> I also remember it being pretty fast on the G3 Mac's when they first came out. One of the comments that Steve Jobs made in the Boston 1997 speech was \"No one at Apple has reached out to Adobe to ask how to build the ultimate Photoshop machine\" - and in the next few years, Photoshop benchmarks were a key Mac vs Intel comparison during his keynotes. I don't know if Jobs already had influence on the original beige Power Macintosh G3, but he really seemed to care about Photoshop performance when he arrived. reply ChiperSoft 8 hours agoparentprevBack when PS6 was the current release I deliberately downgraded to copies of 2 and 3.5 that I found on a Hotline server, because they were extremely fast and did 90% of what I used photoshop for. reply Exuma 15 hours agoprevI looked at the source code but I wish I could understand what makes it beautifully elegant. I was pondering this question before as I was learning rust, and how tricky it was (decision overload) to make just a snake game (regarding code structure). I then was thinking how one would build a UI, functions which operate on a \"space\", and I thought of photoshop specifically, or 3d studio max. So finding this repo was really cool, except I simply just don't understand it. If anyone knows of good resource I could learn code structure LMK! I find it interesting just from a learning perspective, as I always try to increase my design pattern chops reply mannyv 10 hours agoparentIt uses MacApp, which was one of the first frameworks that tried to handle all the boilerplate for you. The basic structure of MacApp apps is a document, and the MacApp framework dispatches events to your handlers. It's been forever since I worked on a MacApp app, but I think that's the basic structure. It sounds like the MacApp stuff isn't included, but it's probably out there somewhere. I know at some point Adobe ported MacApp to Windows so they didn't have to rewrite everything. I expect at some point they replaced MacApp with their own abstraction layer. reply kalleboo 8 hours agorootparent> it's probably out there somewhere https://archive.org/details/macapp2cdrom reply Exuma 10 hours agorootparentprevAhh interesting... so that would explain the function definitions that were missing for certain functions that were called! reply logdahl 13 hours agoparentprevI can't say much about this code or your personal background, but my honest opinion is to take a step back and examine the principles. I used to be very bothered by abstractions, design patterns and structure. But I realized that when I worked with 'true' imperative code (forget classes for a while), keeping all code in the same file, the code started to structure itself. I am not saying this is the only way, but I feel like OOP can be a hinderance, as you get bogged down by alternatives. reply stockhorn 18 hours agoprevAn article from 2013 with an adobe photoshop version 1.x from 1990.... reply boomskats 18 hours agoparentI'm pretty sure half of that code is still running in WASM on photoshop.adobe.com reply msk-lywenn 18 hours agorootparentYou mean current photoshop includes pascal code? reply callalex 17 hours agorootparentTools used for art often get irrationally preserved for the sake of it. For example I have had a conversation with more than one person (well 2 but still) who believed unironically that the wiring inside vintage guitars and amps must be coated with asbestos insulation or it would change the tone/texture of the sound. reply PaulHoule 12 hours agorootparentDon’t crush that in a hydraulic press. reply wongarsu 17 hours agorootparentprevWhat's wrong with Pascal, apart from the ability to hire developers for it? reply PaulHoule 12 hours agorootparentI hated the dialects of Pascal we were using at school in the early 1980s because they didn’t really support systems programming but after I got a 286 machine I got into Turbo Pascal which did have the extensions I need and that I preferred greatly to C but I switched to C in college because I could write C programs and run them on my PC or on Sun workstations with a 32 bit address space. reply miohtama 12 hours agorootparentprevTurbo Pascal and later Delphi were really nice, but I guess in the same vertical C won due to its UNIX legacy. You can pretty much transform 1:1 between C and Pascal code. reply p0w3n3d 14 hours agorootparentprevWriting in Pascal itself is a Job Preservation Pattern reply madeofpalk 17 hours agorootparentprevI would not be surprised if it does. Photoshop is big and has a lot of legacy. reply dlachausse 17 hours agorootparentI have a feeling that much of it was translated to C or C++ at some point for portability and maintainability reasons. There are several automated Pascal to C translators out there, such as the following... http://users.fred.net/tds/lab/p2c/ Also the languages are similar enough that a programmer with knowledge of both could translate it manually without too much difficulty. reply dunham 17 hours agorootparentTypically TeX is translated from Pascal to C too, via web2c. But there also is a Pascal to WASM compiler out there, which was written specifically for TeX: https://github.com/kisonecat/web2js TeX itself is only about 500kb of wasm, uncompressed, but the memory images with LaTeX loaded are quite a bit larger. reply smburdick 16 hours agoprevJohn Knoll was the FX lead for the Star Wars prequels, and went on to direct Rogue One. The behind the scenes documentaries for the prequels have aged well: https://youtu.be/da8s9m4zEpo?si=5y5gHUMxztwVzMny reply hondo77 16 hours agoparentVFX supervisor, exec producer, and story by, but not director of Rogue One: https://www.imdb.com/title/tt3748528/reference/ reply dylan604 14 hours agoparentprevThere's a multi-part series \"Light & Magic\" on ILM available on Disney+ that I really enjoyed. https://www.imdb.com/title/tt19896784/?ref_=nv_sr_srsg_0_tt_... reply acidburnNSA 3 hours agoparentprevI met his dad, who was a professor emeritus at University of Michigan's nuclear engineering department. He wrote the classic textbook on radiation detection. reply thih9 18 hours agoprev> they could not have imagined that they would be adding a word to the dictionary. Adobe tries to fight that, as this leads to genericization[1]. Their trademark guidelines[2] state a number of examples, like: \"Always capitalize and use trademarks in their correct form. Correct: The image was enhanced with Adobe® Photoshop® Elements software. Incorrect: The image was photoshopped.\" [1]: https://en.wikipedia.org/wiki/Generic_trademark [2]: https://www.adobe.com/legal/permissions/trademarks.html reply deusum 18 hours agoparentI believe it's now well into the realm of genericization.[1] Xerox lost a major lawsuit relatedly, iirc. [1] e.g, https://www.consumerreports.org/consumerist/15-product-trade... reply andai 18 hours agoparentprevI understand the pressure they're under, but nobody's going to say that... reply afavour 17 hours agorootparentI’m sure they know that. The text is there so that they can stand up in court and point to it, not because they think people will actually follow the instructions. reply electroly 17 hours agorootparentprevIt's just like \"LEGO® bricks.\" They're desperately trying to avoid genericization but it's way too late and nobody is going to say that informally. All companies want you to use their trademarks as capitalized adjectives but nobody can make you, personally, do that. But it does help with their official corporate partners who will follow the guidance if they want to stay in Adobe/LEGO's good graces. reply chias 18 hours agorootparentprev\"Oh you're not actually using Linux, that's GNU/Linux\" reply DaiPlusPlus 17 hours agorootparentTIL, \"Linux\", without the \"GNU/\" prefix is a registered trademark of Linus Torvalds. https://ubuntu.com/legal/trademarks#:~:text=Linux%C2%AE%20is.... So (in the US, at least), it's \"Linux(TM)\" and not \"GNU/Linux\" - I'm going to love using this the next time anyone goes uhmackshully to me. reply medmunds 16 hours agorootparentUhmackshully, since it's registered, it's \"Linux®\". The ™ is for unregistered trademarks. reply DaiPlusPlus 16 hours agorootparentHoisted by my own petard! reply ian-g 16 hours agoparentprevMuch more effectively, Velcro's been trying the same thing: https://www.youtube.com/watch?v=rRi8LptvFZY It still won't work in the long run, but I'm very aware now that Velcro is a trademarked name. reply cynicalsecurity 17 hours agoparentprevI photoshopped an image with Gimp. reply hallarempt 14 hours agorootparentI, as the Krita maintainer, hereby give everyone the right to verb the trademarked name \"krita\". Whether it's I \"krittered that concept\" or \"I kritaed that sketch\" -- it's fine! The only thing you cannot do with the trademarked name krita is publish rip-off, spyware-laden versions in places like eBay. reply tagawa 3 hours agorootparentSide note: Thank you for your work! My non-technical partner was able to create and print postcards that had to be in CMYK format, thanks to Krita. You made her very happy :-) reply Moru 3 hours agorootparentprevExcept Krita is a word in Swedish so good luck trademarking that one here :-) reply ThrowawayTestr 17 hours agorootparentprevI gimped an image with Adobe Photoshop® reply aceazzameen 7 hours agorootparentThat sounds accurate. reply downrightmike 17 hours agorootparentprevsuch a terrible name reply dclowd9901 16 hours agorootparentNobody’s ever accused open source of being good at naming stuff reply aragonite 14 hours agorootparentprevDo the users find the name terrible though? I'm pretty sure on at least 3 different occasions I heard someone excitely yelling \"time to bring out the GIMP!\" or some such when they needed to do some quick photo editing. Case in point: https://www.youtube.com/watch?v=T4CjOB0y9nI&t=2518s reply bigstrat2003 11 hours agorootparentYeah, GIMP is an awesome name. It's fun and playful, one of the better named programs out there imo. reply Zambyte 17 hours agorootparentprevWhile using GIMP reply resource_waste 17 hours agorootparentprevNo you didn't. No one actually uses Gimp. We just say 'Gimp is a replacement for photoshop' and pretend that is actually an acceptable solution for people using Linux. (Btw I switched to Krita and I'm never going back to Gimp. Even the things Gimp should be good at, Krita is better.) reply rvense 17 hours agorootparentPersonally I crop screenshots with GIMP twice a year and it's absolutely fine for that. Not sure what your problem is. reply NovemberWhiskey 16 hours agorootparentIf your use-case is \"crop screenshots\", your competition isn't Photoshop, it's MS Paint. reply bmacho 15 hours agorootparentThere aren't many image editors that are able to crop pictures in a usable way. MS Paint for example can't do that. I wonder if the \"move this rectangle\" method is under patents. reply taskforcegemini 2 hours agorootparentMaybe you mean something different by \"cropping\", but drawing a rectangle followed by ctrl+c then ctrl+n is fairly quick / good enough reply yjftsjthsd-h 13 hours agorootparentprevIf there was a linux port of paint, I'd consider it. Until then, GIMP is fine. reply bigstrat2003 11 hours agorootparentCheck out Pinta. It does basic image editing pretty well, imo. reply nunez 3 hours agorootparentprevjspaint.app has you covered reply wizzwizz4 17 hours agorootparentprevGIMP is the screenshot cropping tool, or for when you want to write a Lisp program to do a single, technically-precise thing to an image. Krita for everything else! I'm still waiting for the Krita equivalent of Inkscape. reply Zambyte 16 hours agorootparentI use Lisp extensions all the time for things people claim GIMP can't do, like draw certain shapes. GIMP is to Emacs as Photoshop is to Intellij. Both GIMP and Emacs are fairly lean out of the box; it is meant to be molded into what the user wants. The problem is the target audience of Emacs is much more keen on programmatically modifying their systems than the target audience of GIMP. reply ltlnx 12 hours agorootparentprevWhat issues do you have with Inkscape? I've used it for both (semi-)professional and personal work, and the UX is quite pleasant. reply harrison_clarke 14 hours agorootparentprevsounds like exactly what ronin is for tutorial/example video: https://www.youtube.com/watch?v=SgAWGh1s9zg reply tjoff 17 hours agoparentprevIs genericization really a problem though? reply caseyohara 17 hours agorootparentYes, companies can lose the exclusive right to their mark if the brand is sufficiently genericized. Just ask Frisbee, (Kawasaki) Jet Ski, ChapStick, Velcro, Lego, Band-Aid, Jacuzzi, the list goes on. https://en.wikipedia.org/wiki/List_of_generic_and_genericize... https://en.wikipedia.org/wiki/Generic_trademark reply quesera 15 hours agorootparentMost annoyingly, IMO: Sriracha. The Huy Fong guy decided not to trademark the term, and consequently in the last few years, everyone is selling a Sriracha sauce, all of which are grossly inferior to the original. I've tried many of them, being lately in a Huy Fong desert, and esp during their period of production issues. There are a couple of also-rans, rating maybe 7 stars out of 10. They do not taste like real Sriracha, but they're OK. If they didn't call themselves Sriracha, I might appreciate them more. reply jimbobthrowawy 13 hours agorootparentI don't think that's any more annoying than \"ketchup\" or \"barbecue\" sauce not being trademarked. I hear the sauce made by their original pepper suppliers is pretty good though. reply quesera 6 hours agorootparentI've tried it, and I do not like it. The flavor is boring. Checking up on Huy Fong today, I discovered that they have announced another production disruption this month, expected to last until Labor Day. Their pepper supply is too green. I appreciate their dedication to product! Yes it's a serious supply chain management failure, but I can accept that their requirements are difficult for vendors to meet. A substandard Sriracha might be better than no Sriracha, but there are plenty of substandard vendors already. I'll wait for Huy Fong to get the good stuff sorted out. I hope they resolve this issue soon and permanently. Maybe they and their old pepper grower can make amends, for the good of humanity. reply sgerenser 14 hours agorootparentprevYes, just ask Velcro™: https://www.youtube.com/watch?v=rRi8LptvFZY reply edmara 17 hours agorootparentprevOf course. A trademark exists to mutually protect consumers and businesses from deceptive advertising. When a term referring to a specific product becomes a term for a product category etc, trademark protections then becomes harmful to consumers, but they still benefit the business. If you're building a brand generally you want to be as close to the legal limit as possible without exceeding it reply somat 16 hours agoparentprevehh... I am not sure, A photo shop was a thing long before adobe made some software that could replace an entire photo shop and called it... Photoshop. Verb your nouns and that thing you do in a photo shop becomes \"to photoshop\" I think the insistence on using the \"Adobe® Photoshop®\" is more that the term is already sort of generic and they are on shaky ground from the start. Sort of like windows, or dos, Microsoft goes hard always calling it \"Microsoft Windows®\" or \"MS DOS®\" because just windows, or disk operating system are already very generic terms. https://youtube.com/v/BR6F0EdyulA?t=404 (dave plummer) Not that this will stop them from trying to sue you if you release products using those terms, Gotta give the lawyers something to do after all. Otherwise they would just be sitting around wasting money. This is in contrast to Xerox a term invented specifically for a new invention and the company that invented it. reply deaddodo 16 hours agorootparentIt doesn't necessarily matter if you follow their guidelines or not, this is all legal facade so that they can retain their trademark. In the majority of instances, they simply have to show they made efforts to retain their unique trademark. They don't care that you say \"I photoshopped X\" they just care that GIMP isn't marketed as \"GIMP: Open Source Photoshop\" (or similar instances). reply maurosilber 16 hours agoparentprev\"Always capitalize and use trademarks in their correct form. Incorrect: The image was photoshopped. Correct: The image was enhanced with GIMP software.\" reply ChrisMarshallNY 17 hours agoprevI remember seeing Photoshop, when it was pre-Adobe, in a hospital, in Ann Arbor. I thought it was amazing. One note: I'm almost certain that the version of MacApp (the Apple Pascal app framework) was still in beta, at the time. I used some of Tom Knoll's code (a B-spline algorithm), as a base for a curve editor. He had done some work as a contractor for the company I worked at. reply astrange 5 hours agoparentWhat was it doing in a hospital? reply ChrisMarshallNY 3 hours agorootparentOne of the tech people in the hospital was friends with Tom Knoll, and had it running on a Mac II (I think). I was taking a class there, and the teacher took us on a field trip, to see it. This was 1988 or ‘89. reply dlachausse 17 hours agoprevKudos to companies that are releasing the source code to antique versions of their software. I hope more companies do so in the future. Unfortunately I fear that much of this source code has been lost to time and multiple serial acquisitions over the years. Also, wide spread use of version control is a fairly recent phenomenon, so much of this source code if it still exists at all is on random tape backups and floppy disks or printouts in binders. reply fermigier 17 hours agoparenthttps://www.softwareheritage.org/ \"We collect and preserve software in source code form, because software embodies our technical and scientific knowledge and humanity cannot afford the risk of losing it. Software is a precious part of our cultural heritage. We curate and make accessible all the software we collect, because only by sharing it we can guarantee its preservation in the very long term.\" (Founded by a friend, Roberto Di Cosmo). reply derefr 16 hours agoparentprevI feel like, if some organization like the Internet Archive were to offer a \"software source-code time-delayed-publication escrow service\" (with real boilerplate legal contracts punishing early leaks), a lot of companies would take them up on it. I imagine such a service could be pretty automated/low-touch. One way it could work: 1. you mirror your git repos to a private server the software-conservation org controls. 2. The software-conservation org then sets up matching public repos, initially empty. 3. Every hour, an agent runs, that scans all the private repos for commits with commit timestamps older than ten years (or whatever each company has signed on for as a release period); and syncs just those commits, into that repo's matching public repo. 4. Refs are then also synced, but rewritten, as if `git filter-branch` had been run to remove all commits less than ten years old. Any refs that are empty after filtering are dropped. reply schlauerfox 9 hours agorootparentwhy is source code submission to the LOC not necessary like a book to register copyright? Seems reasonable they hold it in escrow for 30 years or whatever reasonable term copyright should be. reply sys_64738 9 hours agoprevEarly Photoshop was junk compared to Deluxe Paint on the Amiga. History only remembers the winners so it’s unfortunate DPaint gets lost the midst time. reply aceazzameen 7 hours agoparentDPaint was incredible. But Photoshop was still pretty good on its own too. It just happened to advance at a greater rate than DPaint did. reply nunez 8 hours agoprevI took an image processing class during my Comp Eng undergrad. We learned about and implemented (in C, or maybe Java; I think it was C) some of the bitmap processing algorithms that Photoshop incorporated. Some of that math is no joke, and making it tight in the late 80s must have been harder still. reply dang 16 hours agoprevRelated: Adobe Photoshop 1.0.1 Source Code (2013) - https://news.ycombinator.com/item?id=17132058 - May 2018 (200 comments) Photoshop 1.0 Source Code - https://news.ycombinator.com/item?id=5215737 - Feb 2013 (78 comments) reply NewsaHackO 17 hours agoprevWhich do you think has more features, this or current GIMP? reply FdbkHb 38 minutes agoparentGIMP. But it doesn't take many later versions of Photoshop to start becoming more productive than using GIMP because of functionality it has that GIMP does not: Adjustment layers were introduced in 4.0 (1996, GIMP didn't even exist yet) Layer styles were introduced in 6.0 (2000) Smart Filters were introduced in CS3 (2007) They're all invaluable tools that provide a non-destructive workflow where you can go and edit a change you made without having to undo everything you did after that change and redoing things again. If I had to use an ancient version of a program and have nothing but that program until the end of times, I would pick Photoshop CS3. This entire class of functionality still does not exist in GIMP. A lot of modern tools can be added to GIMP through the G'MIC plugins (like the healing tool), but the core editing loop functionality, what is in my opinion the most important thing, is extremely primitive and outdated. All of the competition provides non destructive editing. Including other open source software like Krita (which focuses more on painting tools rather than photo editing, leaving a hole in the open source ecosystem). reply resource_waste 17 hours agoparentprevWhat has accomplished more work? Photoshop prior to 2013, or Gimp all time? Lol we all know. Why is Gimp the knee jerk reaction when its rarely used in the real world? Did we learn it in the 2000s and just keep repeating it? (I say this as a Krita fan) reply MayeulC 16 hours agorootparentThe source code in the linked article is for Photoshop v1.0.1, published in 1990. Though I don't think Gimp is as rarely used in the real world as you seem to think. We all live in different bubbles, but I know more people that use GIMP than Photoshop. reply Zambyte 16 hours agorootparentprevWhy are you comparing it to Photoshop in 2013? The article is about Photoshop in 1990. reply Zambyte 17 hours agoparentprevGIMP has a plugin system and this does not AFAIK, so you're comparing unbounded features vs bounded features. reply mrKola 8 hours agoprevI would love to see the code of Fireworks. Adobe bought macromedia just to kill the apps. If I could bring one app back to life, it would be Fireworks. I was soooo good as it, no other software compares. reply kibibu 8 hours agoparentI don't think that's entirely why. They kept Flash around for a long time reply Rufus_Tuesday 17 hours agoprevAnybody remember BarneyScan XP? reply Rufus_Tuesday 8 hours agoparentBarneyscan was a 35mm slide scanner. https://petapixel.com/2018/05/25/the-story-of-how-photoshop-... reply kls0e 5 hours agoprevexcellent read, how tangible. love the praise of the code structure. impressed on how consistent photoshop's UI is, up to contemporary versions. reply shivanshu120 15 hours agoprevGreat article written on some of the best code out there in the market. reply dylan604 14 hours agoparentbest code? Have you ever read people's thoughts on the PSD format? I know the two are not the same, but it does make you wonder how the PSD issues do not present in the app's code as well. reply PaulHoule 12 hours agorootparentIf I was going to complain about Photoshop it is that it does most operations in the chosen color space (say sRGB) instead of linear light. This is certainly wrong for operations that are physically motivated like blurs even if people sometimes like the result. reply anemoknee 12 hours agorootparentprevI haven't myself, but I'm interested to see what folks are thinking. Any resources you can share? reply dylan604 12 hours agorootparentI'll just leave these here: https://www.reddit.com/r/copypasta/comments/n7auu6/psd_is_no... https://news.ycombinator.com/item?id=575122 reply vsuperpower2020 10 hours agorootparentOne of these links is locked behind an account and the other only redirects to a dead repository. Thanks, I guess. reply dylan604 10 hours agorootparentI don't have a reddit account, and I was able to read it after agreeing that I was over 18. After that, it displayed the diatribe just fine for me.However, feel free to google it yourself reply ge96 17 hours agoprevwonder if anybody has it up on github reply peterjmag 16 hours agoparentSomebody pushed it up here: https://github.com/amix/photoshop But that might be violating the Computer History Museum's license: https://github.com/amix/photoshop/blob/2baca147594d01cf9d17d... reply ge96 9 hours agorootparent12 yrs old wow. I'm surprised everything is in the root folder... no subfolder/groupings, probably uploader choice not actually how it was written? reply SushiHippie 14 hours agorootparentprev> But that might be violating the Computer History Museum's license: Yep, TFA includes this sentence: > To download the code you must agree to the terms of the license, which permits only non-commercial use and does not give you the right to license it to third parties by posting copies elsewhere on the web. reply HumblyTossed 17 hours agoparentprevDoes the zip file not work? reply ge96 9 hours agorootparentI just didn't want to download it, just view it, like a PDF that opens in the web vs. auto downloads reply wezdog1 11 hours agoprevThat pronunciation of Photoshop bugs me. Not everyone has an American accent. reply mentos 15 hours agoprevI wonder what the biggest semantic similarities are between the source code of the first 1990s Photoshop and today’s. reply supportengineer 14 hours agoprevI prefer Photon Paint or DeluxePaint reply mdaniel 18 hours agoprev(2013) reply Eduard 16 hours agoparentI find the addition of \"(2013)\" to the title misleading. \"Adobe Photoshop Source Code (2013)\" I thought it is about Photoshop source code from around 2013. reply divyenduz 16 hours agoprevIs there a youtube channel or something that does deep dives into antique source code like this or windows XP? reply dukeofdoom 11 hours agoprevI was looking for a freeish alternative for mac, but so far only found Photopea which is online but has an almost identical interface. Works pretty good basic things, but kind of bad at removing a background. So still searching ... reply ilrwbwrkhv 13 hours agoprev [–] \"We developed it originally for our own personal use…it was a lot a fun to do\" I honestly do not think anything cool has ever been built due to capitalism. Great ideas to great products are just musings. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Computer History Museum features exhibits, publications, and events delving into computer technology's history.",
      "Adobe Photoshop 1.0.1's 1990 source code, crafted by Thomas and John Knoll, is now accessible for non-commercial use, highlighting the software's sophisticated design.",
      "The museum offers valuable perspectives from software architect Grady Booch, presenting a special view into Photoshop's early evolution."
    ],
    "commentSummary": [
      "The discussion explores the significance of preserving source code for Adobe design software such as Photoshop, Illustrator, and PostScript from the 90s, highlighting its historical importance.",
      "It compares Adobe products with open-source alternatives like GIMP and Krita while examining the evolution of user interfaces, software preferences, and trademark guidelines.",
      "Efforts by organizations like Software Heritage to safeguard source code as a cultural heritage for future generations are emphasized."
    ],
    "points": 535,
    "commentCount": 198,
    "retryCount": 0,
    "time": 1715785935
  },
  {
    "id": 40367331,
    "title": "Apple Introduces New Accessibility Features: Eye Tracking and More",
    "originLink": "https://www.apple.com/newsroom/2024/05/apple-announces-new-accessibility-features-including-eye-tracking/",
    "originBody": "Apple Newsroom needs your permission to enable desktop notifications when new articles are published PRESS RELEASE May 15, 2024 Apple announces new accessibility features, including Eye Tracking, Music Haptics, and Vocal Shortcuts Coming later this year, Apple’s new accessibility features include Eye Tracking, a way for users to navigate iPad and iPhone with just their eyes. CUPERTINO, CALIFORNIA Apple today announced new accessibility features coming later this year, including Eye Tracking, a way for users with physical disabilities to control iPad or iPhone with their eyes. Additionally, Music Haptics will offer a new way for users who are deaf or hard of hearing to experience music using the Taptic Engine in iPhone; Vocal Shortcuts will allow users to perform tasks by making a custom sound; Vehicle Motion Cues can help reduce motion sickness when using iPhone or iPad in a moving vehicle; and more accessibility features will come to visionOS. These features combine the power of Apple hardware and software, harnessing Apple silicon, artificial intelligence, and machine learning to further Apple’s decades-long commitment to designing products for everyone. “We believe deeply in the transformative power of innovation to enrich lives,” said Tim Cook, Apple’s CEO. “That’s why for nearly 40 years, Apple has championed inclusive design by embedding accessibility at the core of our hardware and software. We’re continuously pushing the boundaries of technology, and these new features reflect our long-standing commitment to delivering the best possible experience to all of our users.” “Each year, we break new ground when it comes to accessibility,” said Sarah Herrlinger, Apple’s senior director of Global Accessibility Policy and Initiatives. “These new features will make an impact in the lives of a wide range of users, providing new ways to communicate, control their devices, and move through the world.” Eye Tracking Comes to iPad and iPhone Powered by artificial intelligence, Eye Tracking gives users a built-in option for navigating iPad and iPhone with just their eyes. Designed for users with physical disabilities, Eye Tracking uses the front-facing camera to set up and calibrate in seconds, and with on-device machine learning, all data used to set up and control this feature is kept securely on device, and isn’t shared with Apple. Eye Tracking works across iPadOS and iOS apps, and doesn’t require additional hardware or accessories. With Eye Tracking, users can navigate through the elements of an app and use Dwell Control to activate each element, accessing additional functions such as physical buttons, swipes, and other gestures solely with their eyes. Music Haptics Makes Songs More Accessible Music Haptics is a new way for users who are deaf or hard of hearing to experience music on iPhone. With this accessibility feature turned on, the Taptic Engine in iPhone plays taps, textures, and refined vibrations to the audio of the music. Music Haptics works across millions of songs in the Apple Music catalog, and will be available as an API for developers to make music more accessible in their apps. Music Haptics is a new way for users who are deaf or hard of hearing to experience music on iPhone. New Features for a Wide Range of Speech With Vocal Shortcuts, iPhone and iPad users can assign custom utterances that Siri can understand to launch shortcuts and complete complex tasks. Listen for Atypical Speech, another new feature, gives users an option for enhancing speech recognition for a wider range of speech. Listen for Atypical Speech uses on-device machine learning to recognize user speech patterns. Designed for users with acquired or progressive conditions that affect speech, such as cerebral palsy, amyotrophic lateral sclerosis (ALS), or stroke, these features provide a new level of customization and control, building on features introduced in iOS 17 for users who are nonspeaking or at risk of losing their ability to speak. On iPhone 15 Pro, a screen reads “Set Up Vocal Shortcuts” and prompts the user to choose an action and record a phrase to teach their iPhone how to recognize their voice. On iPhone 15 Pro, a screen reads “Say ‘Rings’ One Last Time,” and prompts the user to teach iPhone to recognize the phrase by saying it three times. On iPhone 15 Pro, a user gets an alert from Vocal Shortcuts that says “Open Activity Rings.” With Vocal Shortcuts, iPhone and iPad users can assign custom utterances that Siri can understand to launch shortcuts and complete complex tasks. With Vocal Shortcuts, iPhone and iPad users can assign custom utterances that Siri can understand to launch shortcuts and complete complex tasks. With Vocal Shortcuts, iPhone and iPad users can assign custom utterances that Siri can understand to launch shortcuts and complete complex tasks. previous next “Artificial intelligence has the potential to improve speech recognition for millions of people with atypical speech, so we are thrilled that Apple is bringing these new accessibility features to consumers,” said Mark Hasegawa-Johnson, the Speech Accessibility Project at the Beckman Institute for Advanced Science and Technology at the University of Illinois Urbana-Champaign’s principal investigator. “The Speech Accessibility Project was designed as a broad-based, community-supported effort to help companies and universities make speech recognition more robust and effective, and Apple is among the accessibility advocates who made the Speech Accessibility Project possible.” Vehicle Motion Cues Can Help Reduce Motion Sickness Vehicle Motion Cues is a new experience for iPhone and iPad that can help reduce motion sickness for passengers in moving vehicles. Research shows that motion sickness is commonly caused by a sensory conflict between what a person sees and what they feel, which can prevent some users from comfortably using iPhone or iPad while riding in a moving vehicle. With Vehicle Motion Cues, animated dots on the edges of the screen represent changes in vehicle motion to help reduce sensory conflict without interfering with the main content. Using sensors built into iPhone and iPad, Vehicle Motion Cues recognizes when a user is in a moving vehicle and responds accordingly. The feature can be set to show automatically on iPhone, or can be turned on and off in Control Center. Vehicle Motion Cues is a new experience for iPhone and iPad that can help reduce motion sickness for passengers in moving vehicles. CarPlay Gets Voice Control, More Accessibility Updates Accessibility features coming to CarPlay include Voice Control, Color Filters, and Sound Recognition. With Voice Control, users can navigate CarPlay and control apps with just their voice. With Sound Recognition, drivers or passengers who are deaf or hard of hearing can turn on alerts to be notified of car horns and sirens. For users who are colorblind, Color Filters make the CarPlay interface visually easier to use, with additional visual accessibility features including Bold Text and Large Text. Updates to CarPlay include Sound Recognition, which allows drivers or passengers who are deaf or hard of hearing to turn on alerts to be notified of car horns and sirens. Accessibility Features Coming to visionOS This year, accessibility features coming to visionOS will include systemwide Live Captions to help everyone — including users who are deaf or hard of hearing — follow along with spoken dialogue in live conversations and in audio from apps. With Live Captions for FaceTime in visionOS, more users can easily enjoy the unique experience of connecting and collaborating using their Persona. Apple Vision Pro will add the capability to move captions using the window bar during Apple Immersive Video, as well as support for additional Made for iPhone hearing devices and cochlear hearing processors. Updates for vision accessibility will include the addition of Reduce Transparency, Smart Invert, and Dim Flashing Lights for users who have low vision, or those who want to avoid bright lights and frequent flashing. visionOS will offer Live Captions, so users who are deaf or hard of hearing can follow along with spoken dialogue in live conversations and in audio from apps. These features join the dozens of accessibility features already available in Apple Vision Pro, which offers a flexible input system and an intuitive interface designed with a wide range of users in mind. Features such as VoiceOver, Zoom, and Color Filters can also provide users who are blind or have low vision access to spatial computing, while features such as Guided Access can support users with cognitive disabilities. Users can control Vision Pro with any combination of their eyes, hands, or voice, with accessibility features including Switch Control, Sound Actions, and Dwell Control that can also help those with physical disabilities. “Apple Vision Pro is without a doubt the most accessible technology I’ve ever used,” said Ryan Hudson-Peralta, a Detroit-based product designer, accessibility consultant, and cofounder of Equal Accessibility LLC. “As someone born without hands and unable to walk, I know the world was not designed with me in mind, so it’s been incredible to see that visionOS just works. It’s a testament to the power and importance of accessible and inclusive design.” Additional Updates For users who are blind or have low vision, VoiceOver will include new voices, a flexible Voice Rotor, custom volume control, and the ability to customize VoiceOver keyboard shortcuts on Mac. Magnifier will offer a new Reader Mode and the option to easily launch Detection Mode with the Action button. Braille users will get a new way to start and stay in Braille Screen Input for faster control and text editing; Japanese language availability for Braille Screen Input; support for multi-line braille with Dot Pad; and the option to choose different input and output tables. For users with low vision, Hover Typing shows larger text when typing in a text field, and in a user’s preferred font and color. For users at risk of losing their ability to speak, Personal Voice will be available in Mandarin Chinese. Users who have difficulty pronouncing or reading full sentences will be able to create a Personal Voice using shortened phrases. For users who are nonspeaking, Live Speech will include categories and simultaneous compatibility with Live Captions. For users with physical disabilities, Virtual Trackpad for AssistiveTouch allows users to control their device using a small region of the screen as a resizable trackpad. Switch Control will include the option to use the cameras in iPhone and iPad to recognize finger-tap gestures as switches. Voice Control will offer support for custom vocabularies and complex words. The new Reader Mode in Magnifier is shown on iPhone 15 Pro. The Hover Typing experience is shown on iPhone 15 Pro. The Personal Voice experience is shown in Mandarin Chinese on Mac. Additional accessibility updates include a new Reader Mode in Magnifier. Hover Typing shows larger text when typing in a text field, and in a user’s preferred font and color. Personal Voice will be available in Mandarin Chinese for users at risk of losing their ability to speak. previous next Celebrate Global Accessibility Awareness Day with Apple This week, Apple is introducing new features, curated collections, and more in celebration of Global Accessibility Awareness Day: Throughout the month of May, select Apple Store locations will host free sessions to help customers explore and discover accessibility features built into the products they love. Apple Piazza Liberty in Milan will feature the talent behind “Assume that I can,” the viral campaign for World Down Syndrome Day. And available year-round at Apple Store locations globally, Today at Apple group reservations are a place where friends, families, schools, and community groups can learn about accessibility features together. Shortcuts adds Calming Sounds, which plays ambient soundscapes to minimize distractions, helping users focus or rest. Visit the App Store to discover incredible apps and games that promote access and inclusion for all, including the accessible App Store Award-winning game Unpacking, apps as tools for augmentative and alternative communication (AAC), and more. The Apple TV app will honor trailblazing creators, performers, and activists who passionately share the experiences of people with disabilities. This year’s theme is Remaking the World, and each story invites viewers to envision a reality where everyone is empowered to add their voice to the greater human story. Apple Books will spotlight lived experiences of disability through curated collections of first-person narratives by disabled writers in ebook and audiobook formats. Apple Fitness+ workouts, meditations, and trainer tips welcome users who are deaf or hard of hearing with American Sign Language, and Time to Walk now includes transcripts in the Apple Podcasts app. Fitness+ workouts always include Audio Hints to support users who are blind or have low vision, as well as modifiers so that users of all levels can participate. Users can visit Apple Support to learn how their Apple devices can be customized using built-in accessibility features. From adapting the gestures to customizing how information is presented on a device’s screen, the Apple Accessibility playlist will help users learn how to personalize Apple Vision Pro, iPhone, iPad, Apple Watch, and Mac to work best for them. Share article Media Text of this article May 15, 2024 PRESS RELEASE Apple announces new accessibility features, including Eye Tracking, Music Haptics, and Vocal Shortcuts CUPERTINO, CALIFORNIA Apple today announced new accessibility features coming later this year, including Eye Tracking, a way for users with physical disabilities to control iPad or iPhone with their eyes. Additionally, Music Haptics will offer a new way for users who are deaf or hard of hearing to experience music using the Taptic Engine in iPhone; Vocal Shortcuts will allow users to perform tasks by making a custom sound; Vehicle Motion Cues can help reduce motion sickness when using iPhone or iPad in a moving vehicle; and more accessibility features will come to visionOS. These features combine the power of Apple hardware and software, harnessing Apple silicon, artificial intelligence, and machine learning to further Apple’s decades-long commitment to designing products for everyone. “We believe deeply in the transformative power of innovation to enrich lives,” said Tim Cook, Apple’s CEO. “That’s why for nearly 40 years, Apple has championed inclusive design by embedding accessibility at the core of our hardware and software. We’re continuously pushing the boundaries of technology, and these new features reflect our long-standing commitment to delivering the best possible experience to all of our users.” “Each year, we break new ground when it comes to accessibility,” said Sarah Herrlinger, Apple’s senior director of Global Accessibility Policy and Initiatives. “These new features will make an impact in the lives of a wide range of users, providing new ways to communicate, control their devices, and move through the world.” Eye Tracking Comes to iPad and iPhone Powered by artificial intelligence, Eye Tracking gives users a built-in option for navigating iPad and iPhone with just their eyes. Designed for users with physical disabilities, Eye Tracking uses the front-facing camera to set up and calibrate in seconds, and with on-device machine learning, all data used to set up and control this feature is kept securely on device, and isn’t shared with Apple. Eye Tracking works across iPadOS and iOS apps, and doesn’t require additional hardware or accessories. With Eye Tracking, users can navigate through the elements of an app and use Dwell Control to activate each element, accessing additional functions such as physical buttons, swipes, and other gestures solely with their eyes. Music Haptics Makes Songs More Accessible Music Haptics is a new way for users who are deaf or hard of hearing to experience music on iPhone. With this accessibility feature turned on, the Taptic Engine in iPhone plays taps, textures, and refined vibrations to the audio of the music. Music Haptics works across millions of songs in the Apple Music catalog, and will be available as an API for developers to make music more accessible in their apps. New Features for a Wide Range of Speech With Vocal Shortcuts, iPhone and iPad users can assign custom utterances that Siri can understand to launch shortcuts and complete complex tasks. Listen for Atypical Speech, another new feature, gives users an option for enhancing speech recognition for a wider range of speech. Listen for Atypical Speech uses on-device machine learning to recognize user speech patterns. Designed for users with acquired or progressive conditions that affect speech, such as cerebral palsy, amyotrophic lateral sclerosis (ALS), or stroke, these features provide a new level of customization and control, building on features introduced in iOS 17 for users who are nonspeaking or at risk of losing their ability to speak. “Artificial intelligence has the potential to improve speech recognition for millions of people with atypical speech, so we are thrilled that Apple is bringing these new accessibility features to consumers,” said Mark Hasegawa-Johnson, the Speech Accessibility Project at the Beckman Institute for Advanced Science and Technology at the University of Illinois Urbana-Champaign’s principal investigator. “The Speech Accessibility Project was designed as a broad-based, community-supported effort to help companies and universities make speech recognition more robust and effective, and Apple is among the accessibility advocates who made the Speech Accessibility Project possible.” Vehicle Motion Cues Can Help Reduce Motion Sickness Vehicle Motion Cues is a new experience for iPhone and iPad that can help reduce motion sickness for passengers in moving vehicles. Research shows that motion sickness is commonly caused by a sensory conflict between what a person sees and what they feel, which can prevent some users from comfortably using iPhone or iPad while riding in a moving vehicle. With Vehicle Motion Cues, animated dots on the edges of the screen represent changes in vehicle motion to help reduce sensory conflict without interfering with the main content. Using sensors built into iPhone and iPad, Vehicle Motion Cues recognizes when a user is in a moving vehicle and responds accordingly. The feature can be set to show automatically on iPhone, or can be turned on and off in Control Center. CarPlay Gets Voice Control, More Accessibility Updates Accessibility features coming to CarPlay include Voice Control, Color Filters, and Sound Recognition. With Voice Control, users can navigate CarPlay and control apps with just their voice. With Sound Recognition, drivers or passengers who are deaf or hard of hearing can turn on alerts to be notified of car horns and sirens. For users who are colorblind, Color Filters make the CarPlay interface visually easier to use, with additional visual accessibility features including Bold Text and Large Text. Accessibility Features Coming to visionOS This year, accessibility features coming to visionOS will include systemwide Live Captions to help everyone — including users who are deaf or hard of hearing — follow along with spoken dialogue in live conversations and in audio from apps. With Live Captions for FaceTime in visionOS, more users can easily enjoy the unique experience of connecting and collaborating using their Persona. Apple Vision Pro will add the capability to move captions using the window bar during Apple Immersive Video, as well as support for additional Made for iPhone hearing devices and cochlear hearing processors. Updates for vision accessibility will include the addition of Reduce Transparency, Smart Invert, and Dim Flashing Lights for users who have low vision, or those who want to avoid bright lights and frequent flashing. These features join the dozens of accessibility features already available in Apple Vision Pro, which offers a flexible input system and an intuitive interface designed with a wide range of users in mind. Features such as VoiceOver, Zoom, and Color Filters can also provide users who are blind or have low vision access to spatial computing, while features such as Guided Access can support users with cognitive disabilities. Users can control Vision Pro with any combination of their eyes, hands, or voice, with accessibility features including Switch Control, Sound Actions, and Dwell Control that can also help those with physical disabilities. “Apple Vision Pro is without a doubt the most accessible technology I’ve ever used,” said Ryan Hudson-Peralta, a Detroit-based product designer, accessibility consultant, and cofounder of Equal Accessibility LLC. “As someone born without hands and unable to walk, I know the world was not designed with me in mind, so it’s been incredible to see that visionOS just works. It’s a testament to the power and importance of accessible and inclusive design.” Additional Updates For users who are blind or have low vision, VoiceOver will include new voices, a flexible Voice Rotor, custom volume control, and the ability to customize VoiceOver keyboard shortcuts on Mac. Magnifier will offer a new Reader Mode and the option to easily launch Detection Mode with the Action button. Braille users will get a new way to start and stay in Braille Screen Input for faster control and text editing; Japanese language availability for Braille Screen Input; support for multi-line braille with Dot Pad; and the option to choose different input and output tables. For users with low vision, Hover Typing shows larger text when typing in a text field, and in a user’s preferred font and color. For users at risk of losing their ability to speak, Personal Voice will be available in Mandarin Chinese. Users who have difficulty pronouncing or reading full sentences will be able to create a Personal Voice using shortened phrases. For users who are nonspeaking, Live Speech will include categories and simultaneous compatibility with Live Captions. For users with physical disabilities, Virtual Trackpad for AssistiveTouch allows users to control their device using a small region of the screen as a resizable trackpad. Switch Control will include the option to use the cameras in iPhone and iPad to recognize finger-tap gestures as switches. Voice Control will offer support for custom vocabularies and complex words. Celebrate Global Accessibility Awareness Day with Apple This week, Apple is introducing new features, curated collections, and more in celebration of Global Accessibility Awareness Day: Throughout the month of May, select Apple Store locations will host free sessions to help customers explore and discover accessibility features built into the products they love. Apple Piazza Liberty in Milan will feature the talent behind “Assume that I can,” the viral campaign for World Down Syndrome Day. And available year-round at Apple Store locations globally, Today at Apple group reservations are a place where friends, families, schools, and community groups can learn about accessibility features together. Shortcuts adds Calming Sounds, which plays ambient soundscapes to minimize distractions, helping users focus or rest. Visit the App Store to discover incredible apps and games that promote access and inclusion for all, including the accessible App Store Award-winning game Unpacking, apps as tools for augmentative and alternative communication (AAC), and more. The Apple TV app will honor trailblazing creators, performers, and activists who passionately share the experiences of people with disabilities. This year’s theme is Remaking the World, and each story invites viewers to envision a reality where everyone is empowered to add their voice to the greater human story. Apple Books will spotlight lived experiences of disability through curated collections of first-person narratives by disabled writers in ebook and audiobook formats. Apple Fitness+ workouts, meditations, and trainer tips welcome users who are deaf or hard of hearing with American Sign Language, and Time to Walk now includes transcripts in the Apple Podcasts app. Fitness+ workouts always include Audio Hints to support users who are blind or have low vision, as well as modifiers so that users of all levels can participate. Users can visit Apple Support to learn how their Apple devices can be customized using built-in accessibility features. From adapting the gestures to customizing how information is presented on a device’s screen, the Apple Accessibility playlist will help users learn how to personalize Apple Vision Pro, iPhone, iPad, Apple Watch, and Mac to work best for them. About Apple Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, AirPods, Apple Watch, and Apple Vision Pro. Apple’s six software platforms — iOS, iPadOS, macOS, watchOS, visionOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, iCloud, and Apple TV+. Apple’s more than 150,000 employees are dedicated to making the best products on earth and to leaving the world better than we found it. Press Contacts Will Butler Apple willbutler@apple.com Apple Media Helpline media.help@apple.com Copy text Images in this article Download all images About Apple Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, AirPods, Apple Watch, and Apple Vision Pro. Apple’s six software platforms — iOS, iPadOS, macOS, watchOS, visionOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, iCloud, and Apple TV+. Apple’s more than 150,000 employees are dedicated to making the best products on earth and to leaving the world better than we found it. Press Contacts Will Butler Apple willbutler@apple.com Apple Media Helpline media.help@apple.com Latest News UPDATE The redesigned iPad Air and new iPad Pro are available today May 15, 2024 UPDATE Apple Watch is the perfect golfing companion May 15, 2024 UPDATE App Store stopped over $7 billion in potentially fraudulent transactions May 14, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40367331",
    "commentBody": "Apple announces new accessibility features, including eye tracking (apple.com)399 points by dmd 19 hours agohidepastfavorite220 comments Shank 16 hours ago> Vehicle Motion Cues is a new experience for iPhone and iPad that can help reduce motion sickness for passengers in moving vehicles. This excites me so, so much! I can't really use my phone as a passenger in a car without getting motion sick after 1-2 minutes. This seems like it might be a promising thing to try. reply nulld3v 11 hours agoparentA similar app that's on Android: https://play.google.com/store/apps/details?id=com.urbandroid... Unsure if it actually works though, my personal test results are mixed. reply ErigmolCt 13 hours agoparentprevI have motion sickness... It's so hard to movearound for me and I am still not able to find what works best for me reply droopyEyelids 15 hours agoparentprevHave you noticed any correlation between how hungry you are and how fast motion sickness kicks in? reply deinonychus 12 hours agorootparentYes, sort of. I don’t necessarily have to feel hungry but if I’m on an empty stomach or just haven’t eaten in a while, the odds I get motion sickness are much higher. If I’m riding somewhere to go get dinner, I have to sit in the front passenger seat. After dinner with a full belly? Throw me in the back and I’ll be fine. reply astrange 14 hours agorootparentprevI'm not sure why, but I feel like I only get motion sickness in the back of Priuses. It must be something about their braking curve. I don't sit in enough EVs to tell if they're the same. reply rootusrootus 13 hours agorootparentSome people never really learn how to use one-pedal driving, so they end up just going back and forth between accelerating and decelerating. That'll make me motion sick in a hurry, and I bet that is fairly universal (among people prone to motion sickness in cars, that is). So in that sense, any EV or hybrid is potentially a problem, depending on the driver. reply teekert 48 minutes agorootparentAh yes, I never get motion sick, except for when I'm in the car with just such a driver: A person with sine-foot. reply 121789 13 hours agorootparentprevTeslas are especially bad for me. I think it’s the rough suspension and fast acceleration/deceleration reply rootusrootus 13 hours agorootparentThe instant-on power and braking takes some getting used to. For the folks who have trouble mastering it, my recommendation is chill mode. It has a much softer acceleration profile, mostly eliminating the harsh starts you might be experiencing. reply kylehotchkiss 12 hours agorootparentprevI suspect most people's interaction with Prius' are Uber rides. Maybe Uber drivers just get bad habits from the platform incentives (drive fast = get more rides) reply yc-kraln 13 hours agorootparentprevToyota's hybrids are the worst. I never get motion sick except as a passenger in any Toyota hybrid reply Shank 15 hours agorootparentprevIt’s really interesting you say this. Is this a known correlation? I feel like now that you mention it, it’s incredibly fast if I’m hungry. reply toast0 15 hours agorootparentI went on a cruise, and had significant (for me) motion sickness that only got better once I ate --- of course, I was avoiding eating because I didn't feel well, so that seems like the wrong choice. reply KolmogorovComp 14 hours agorootparentprevIt is a known correlation. reply ErigmolCt 13 hours agorootparentprevI have not. For me, it does not matter. The ride begins - the motion sickness kicks in reply danaris 13 hours agorootparentprevI regularly drive two family members around—one gets motion sick much faster and more frequently when hungry, while the other gets motion sick the same either way. Does make me wonder what the difference is there. reply devinprater 17 hours agoprevI wonder what new voices will be added to VoiceOver? We blind people never, ever thought Eloquence, an old TTS engine from 20 years ago now, would ever come to iOS. And yet, here it is in iOS 17. I wouldn't be surprised to see DecTalk, or more Siri voices. More Braille features is amazing, and they even mentioned the Mac! VoiceOver for Mac is notoriously never given as much love as VoiceOver for iOS is, so most blind people still use Windows, even though they have iPhones. I was expecting to see much better image descriptions, but they've already announced a ton of new stuff for plenty other disabilities. Having haptic music will be awesome even for me, adding another sense to the music. There are just so many new accessibility stuff, and I can't wait to see what all is really new in VoiceOver, since there's always new things not talked about in WWDC or release notes. I'm hoping that, one day, we get a tutorial for VoiceOver, like TalkBack on Android has, since there are so many commands, gestures, and settings that a new user never learns unless they learn to learn about them. reply asadotzler 12 hours agoparentMy friends that use synthetic voices prefer cleanliness of the older and familiar voices. One friend listens at about 900 WPM in skim mode and none of the more realistic voices work well at those rates. reply bombcar 16 hours agoparentprevThe image description stuff is already surprisingly good - I noticed when I got a photo text while driving and it described it well enough for me to know what it was. reply skunkworker 14 hours agorootparentSame, a family member send a photo while I was driving and over Carplay it was described fairly accurately. reply slau 13 hours agorootparentIt’s sometimes awesome, and often extremely basic. “Contact sent you a picture of a group of people at an airport”. Amazing. “Contact sent you a screenshot of a social media post”. Useless. We know iOS can select text in pictures, so Siri can clearly read it. It knows it’s SoMe, so why not give me the headline? reply zimpenfish 4 hours agorootparent> It knows it’s SoMe, so why not give me the headline? There's certainly a non-zero number of times this could go horribly wrong (I'm guessing CarPlay doesn't have any sense of who is in the car with you) and defaulting to not reading them out is the safest option (but they could definitely add a toggle for this, yeah.) reply slau 1 hour agorootparentThe same is true with reading out text messages. I’ve disabled it for CarPlay now after receiving a mildly raunchy text with a car full of colleagues. It’s still useful on the headphones though. reply MBCook 6 hours agorootparentprevI’m hoping this shows up in iOS 18. reply bcx 11 hours agoprevThis is a good time to remind everyone that tomorrow, May 16th is Global Accessibility Awareness Day (GAAD) (https://accessibility.day), and that there are over 176 events worldwide going on to celebrate the process we are all making at improving accessibility in our products -- any plenty of learning opportunities for beginners and experts. reply ein0p 4 hours agoprevIf accurate enough it seems like eye tracking could be useful beyond accessibility to eg control iPads without dragging one’s greasy fingers all over the screen. iPad already supports pointers. reply superb_dev 3 hours agoparentI was thinking the same, it would make having an iPad under my monitor much less cumbersome reply roughly 16 hours agoprevAccessibility settings are really a gold mine on iOS for device customization (yes, I agree, they shouldn’t be limited to accessibility). I’m particularly interested in the motion cues and the color filters for CarPlay - I have color filters set up to enable every night as kind of a Turbo-night shift mode (deep orange-red color shift), would love to do the same for CarPlay. I also completely forgot iOS had a magnifier built in! reply ryandrake 15 hours agoparentAccessibility features tend to be superpowers though, and I'm glad Apple gates them behind permissions and opt-ins. We all know of applications who try to trick the user into granting them inappropriate access to the device through the Accessibility APIs. I think DropBox still begs you to grant them Accessibility access so its tendrils can do who-knows-what to your system. With great power comes great responsibility. reply burntwater 12 hours agorootparentGuaranteed that marketers are salivating at the idea of eye tracking on apps and website. It's an amazing feature that absolutely needs to be gatekept. reply klausa 2 hours agorootparentI wonder if it'll use the same architecture as visionOS; where the vision tracking events and UI affordances are processed and composited out-of-process; with the app never seeing them. reply roughly 13 hours agorootparentprevIt varies. Things like keyboard control or that kind of thing, absolutely, but mostly I've used it for stuff like \"don't make an animated transition every time I change pages like an overcaffienated George Lucas\" or \"actually make night shift shift enough to be useful at night\". I also use the background sounds to augment noise cancellation while taking a nap. All of those are just useful things or personal settings, not necessarily attack vectors. reply gnicholas 4 hours agoparentprevMy favorite is the \"allow ANC with just one AirPod in\". I have no idea why this would be an accessibility feature. If I turn on ANC, then I don't want it to be disabled just because I'm listening with one ear! reply SSLy 2 hours agoparentprevFYI you can also make turbo night shift by scheduling toggling of white point balance, yep, in accessibility settings reply astrange 14 hours agoparentprevWell, they aren't really limited to accessibility, but they are hidden there. It's sort of like a convenient excuse to get UI designers off your back if you want to ship customization. reply xnx 16 hours agoprevI love accessibility features because they might be the last features developed solely with the benefit of the user in mind. So many other app/os features are designed to steal your attention or gradually nerf usefulness. reply jonpurdy 15 hours agoparentI often use them to get around bad UI/UX (like using Reduce Motion), or to make devices more useful (Color Filters (red) for using at night). Even outside of this, even able-bodied folks can be disabled due to illness, surgery, injury, etc. So it's great to see Apple continuing to support accessibility. reply gerry_shaw 14 hours agorootparentThe red color filter for outside when trying to preserve night vision is a great tip. Some apps have this built-in but much better to have the OS change it everywhere. Recommend creating a Shortcut to toggle this setting. reply rpastuszak 1 hour agorootparentI think you can just add it to Control Centre, no need for shortcuts. I made an app for reading in the dark, minimising the amount of light hitting your eyeballs, but I'm still using the red color filter every night. The app is overall darker and more \"strict\" with how it handles content (esp. images) though: https://untested.sonnet.io/Heart+of+Dorkness and midnight.sonnet.io Overall, reduced the number of photons is a fun metric to play with when building something/messing with prototypes. reply PaulStatezny 11 hours agorootparentprevNot just for outside, but also at public gatherings like concerts! I went to a concert last month and used a red color filter to record a couple short videos without being a big distraction to the audience behind me. Dim backlight + Red color filter can make the screen almost invisible to those around you. reply wishfish 7 hours agorootparentThanks for the reminder. Wish I had remembered this feature during the aurora photography I was doing. I set the phone brightness to the lowest setting but the red filter would have helped even more. reply dylan604 14 hours agorootparentprevThe issue I've seen when the app itself offers a red filter is if that app calls an OS native widget like a keyboard does not get filtered. The system level accessibility feature does filter the OS widget. I would almost rather the app's setting to just enable the OS filter, but I can understand why that might not be possible. reply throwanem 14 hours agorootparentprevYou can also add it to the accessibility shortcut, available anywhere by triple-clicking the power button. reply justsomehnguy 14 hours agorootparentprev> Recommend creating a Shortcut to toggle this setting. Huh? It's not built-in? Android (or at least Moto) has it for years, auto enable on the schedule or the sunrise/sunset. reply ben_w 14 hours agorootparentThere's a built in \"night shift\", but that just changes the colour temperature, it doesn't make everything monochrome red. reply throwanem 6 hours agorootparentNo, that's (part of) what Color Filters is for. reply carl_dr 14 hours agorootparentpreviOS has also had “Night Shift” for several years. The parent is talking about a full on red colour filter, like astronomers might use. reply cqqxo4zV46cp 11 hours agorootparentprevKinda feels like you could’ve done more of a cursory glance to see what functionality was actually being talked about before going for the “Android already does this!!” comment. reply ljm 13 hours agoparentprevI don't love that solid UX gets pushed under the accessibility rug, as an option you might never find. I don't care how cynical it sounds, user experience became user exploitation a long time ago. Big Tech have been running that gimmick at too-big-to-fail scale for the last decade or so. reply dmazzoni 8 hours agorootparentLet's say you're a developer at a big software company (not necessarily Apple, this happens everywhere) and you want to add a new optional setting. The bar is pretty high. There are already hundreds of settings. Each one adds cost and complexity. So even if it's a good idea, the leadership might say \"no\". Now let's say this same setting makes a big difference in usability for some people with disabilities. You just want to put it in accessibility settings. It won't clutter the rest of the UI. You just turned your \"no\" into a \"yes\". reply hbn 12 hours agorootparentprevThe iPhone has a hidden accessibility setting where you can map and double and/or triple tap of the back of your phone to a handful of actions. I use this to trigger Reachability (the feature that brings the entire UI halfway down the screen so you can reach buttons at the top) because phone screens are so damn big that I can't reach the opposite top corner with my thumb even on my 13 mini without hand gymnastics. And the normal Reachability gesture is super unreliable to trigger ever since they got rid of the front Touch ID home button. reply ornornor 12 hours agorootparentDouble tap is reachability for me and triple tap is to make the display very dim so that at night at the lowest brightness setting, I can get it even lower. It resets after a while so even if I forget to switch it off my screen won’t stay dim for the next few days while I wonder why it’s so damn dark. reply happyopossum 10 hours agorootparentprev>The iPhone has a hidden accessibility setting This isn’t “hidden”. It was even called out and demonstrated in the iOS 14 keynote. reply square_usual 9 hours agorootparentprevUnironically calling this feature \"hidden\" is why things are the way they are now. It's not hidden! You can find it if you go through the settings app! But because it isn't in your face all day every now and then people will talk about this \"super secret feature\" and then a PM somewhere has to make a nag feature to advertise it. reply philistine 12 hours agorootparentprevAccessibility benefits everyone, but in the basics you’re right. Too many simple straightforward options are now strictly inside accessibility. At least on the Apple side. And don’t get me started on hidden command line settings. reply PaulStatezny 11 hours agorootparentYou're getting a lot of agreement from other HN users, but I'm not sure it's fair to criticize Apple for putting these kinds of features under Accessibility. There's nothing that inherently \"locks out\" people who don't have a recognized disability from exploring these features. Furthermore, most of Apple's \"accessibility\" features are related to Vision/Hearing/etc (and categorized as such), so I think it's reasonable to consider them accessibility features. Clearly based on other comments here, plenty of people discover these features and find them useful. reply gumby 12 hours agorootparentprev> Too many simple straightforward options are now strictly inside accessibility From outside, it feels like these are the only people with the freedom to improve the user experience at all. So they have to hide their work in the Accessibility preferences. reply nottorp 12 hours agorootparentprev> Too many simple straightforward options are now strictly inside accessibility.Reduce Motion. Is it an accessibility feature or does it just get rid of an annoyance and is good for everyone? reply alwillis 6 hours agorootparent> Reduce Motion. Is it an accessibility feature Yes--some people can become ill with certain types of motion on a screen [1]. [1]: https://www.a11yproject.com/posts/understanding-vestibular-d... reply nottorp 4 hours agorootparentYes they can, but I don't and I still hate needless animations and turn them off. The point is, why is it in \"accessibility\" when it should be more visible? reply ornornor 12 hours agorootparentprevSaves battery too along with cross fade transitions :) reply a_wild_dandan 12 hours agorootparentprevI'm here to intentionally get you started on hidden CLI settings. Learn me somethin'! reply ibll 11 hours agorootparentnot OP, but macOS has a ton of options available with arcane commands. my favorites are the auto-hide dock speed setting, the third hidden window minimise animation, and the hidden system accent colours usually locked to the colourful iMacs reply stacktrust 16 hours agoparentprev> developed solely with the benefit of the user in mind Hopefully accessibility features are never artificially segmented to higher priced devices. reply miki123211 5 hours agorootparentSome of them are, at least on Apple's side, but it's always for a good technical reason. Screen recognition is only available on devices that have a neural chip, things that require lidar don't work on devices that don't have lidar and so on. Google is worse at this, Talkback multi-finger gestures used to be Pixel and Samsung exclusive for a while, even though there was no technical reason for it. Apple has a different problem, many accessibility features aren't internationalized properly. Screen recognition still has issues on non-english systems, so do image descriptions. Voice Over (especially on Mac) didn't include voices for some of the less-popular languages until very recently, even though Vocalizer, their underlying speech engine, has supported them for years. Siri has the same problem. reply Loughla 16 hours agorootparentprevAt least in the US, they kind of can't be. The disability community is pretty up front about lawsuits. reply stacktrust 15 hours agorootparentiOS 17 audio image descriptions for blind people via Image Magnifier should work on all iPhones, but do not work on iPhone SE3 and iPhone 11 Pro. Audio image descriptions do work in iPhone 12 Pro. Lidar in 12 Pro increases accuracy, but should not be mandatory. Hopefully this is a bug that can be fixed, since text descriptions were still functional on the lower-end devices. Source: purchased devices until finding one that worked, since Apple docs indicated the feature should work on all iPhones that can run iOS 17. Edit: audio descriptions in Magnifier are non-functional on iPad Air, working on M2 iPad Pro. reply pxc 12 hours agorootparentprevThat's because the ADA has no enforcement mechanism other than lawsuits, isn't it? Our whole legal disability rights infrastructure is designed to be driven by lawsuits, and sits inert if nobody sues. reply miki123211 4 hours agorootparentThat's actually a good thing, especially if there are people specializing in bringing these lawsuits en-masse. Over here in Europe, where there's no lawsuit culture, some laws (not necessarily accessibility-related, our legislation in that area is far weaker) are violated almost without repercussions. When the government is the only party that can bring charges and the government is complacent / ineffective, nobody actually gets charged and nothing gets done. There's also the problem of incentives, if you can get a lot of money from a lawsuit, you have a far better incentive to sue and find a competent lawyer. You may even deliberately look for and sue violators as a moneymaking scheme, some companies in the US do this. This puts pressure on businesses to comply with the law. Even if you're piss-poor and never sue anybody, you still benefit. If this lawsuit culture doesn't exist, the best you can do is write a report to the government and hope they actually act on it. Many people don't know how to write these, and since there's no money in it, getting a lawyer to do it for you is an expense that nobody is going to help you recoup. The people handling these reports don't help either, they're usually 9-to-5 salaried employees who aren't judged too hard on performance, so they have far less of an incentive to actually pursue cases. reply corps_and_code 16 hours agorootparentprevI wonder if that would be legal, at least in the US. That feels like it'd be a violation of the ADA? reply diebeforei485 6 hours agorootparentIt would only be a violation if it's purely software locked. If it requires a chip that supports specific operations, and entry tier devices have an older chip, that wouldn't be a violation. reply lucb1e 11 hours agoparentprevWouldn't a lot of the companies that build in accessibility do it from a viewpoint of gaining an even wider reach and/or a better public image? I don't see optimizing for that as bad. If they think we'll love the product more by making it better for a given audience, especially if I'm in that audience, I'm happy. Does that mean this company now gets richer? Perhaps, and that's fine by me reply snoman 15 hours agoparentprevEvery attention thief is absolutely thrilled at the idea of tracking your eyes. Let’s all imagine the day where the YouTube free tier pauses ads when you’re not actively looking at them. Shit. I’m turning into one of those negative downers. I’m sorry. I’ve had too much internet today. reply Tagbert 15 hours agorootparentIf this is at all like the eye tracking in Vision Pro, it is only available to the OS and apps are not given access to the data. reply bun_at_work 13 hours agorootparentUntil people complain that Apple is being anti-competitive by not making vision tracking open, or allowing third-party eye-tracking controls, etc. etc. reply ben_w 14 hours agorootparentprevSystem one is, but advertisers could always roll their own and see if they can get away with \"you can only view this content if you give us permission to use your camera\". reply favorited 13 hours agorootparentAt least on iOS, I can't imagine that happening - apps are not allowed to demand you grant permissions unrelated to the actual functionality. From App Review Guidelines (5.1.1) Data Collection and Storage, (ii) Access: > Apps must respect the user’s permission settings and not attempt to manipulate, trick, or force people to consent to unnecessary data access. For example, apps that include the ability to post photos to a social network must not also require microphone access before allowing the user to upload photos. Lots of iOS apps today really want to mine your address book, and constantly spam you with dialogs to enable it, but they don't go as far as disabling other features until you grant them access, because they'd get rejected once someone noticed. reply burnerthrow008 11 hours agorootparentFortunately apps in the EU don’t have to pass though Apple’s anticompetitive review process, so developers are free to ignore that rule if they simply distribute the app via an alternative store. Unfortunately, poor Americans cannot taste the freedom that Europeans have to be abused by developers. reply favorited 3 hours agorootparentFortunately, the Americans who create these products extract more value from the EU than they invest, or they wouldn’t bother trading with you :( Surveillance technology with the word \"accessibility\" in the title: :) reply simonw 14 hours agoprevmacOS has had a version of eye tracking for a while, it's really fun to try out. System preferences -> Accessibility -> Pointer Control Then turn on the \"Head pointer\" option. reply kridsdale1 12 hours agoparentVisionOS has this too. I mapped it to a triple click of the button for when eye tracking becomes inaccurate. reply emehrkay 15 hours agoprevEye tracking coupled with the show grid feature would seem like using a computer the way that people do in movies https://www.youtube.com/watch?v=UxigSW9MbY8 reply badbart14 16 hours agoprevI definitely get a good amount of motion sickness when using my phone while in a car so I'm super interested about the motion sickness cues and if they'll work. The dots look like they may get in the way a bit but I'm willing to take that tradeoff. My current car motion sickness mitigation system is these glasses that have liquid in them that supposedly help your ears feel the motion of the car better (and make you look like Harry Potter) reply yreg 11 hours agoprevAccessibility is for everyone, including you, if you live long enough. And the alternative is worse. So your choice is death or you are going to use accessibility features. – Siracusa reply notJim 10 hours agoparentSee also https://en.wikipedia.org/wiki/Curb_cut_effect reply jiggawatts 11 hours agoparentprevI aimed for the upvote button but they’re so tiny that my fat finger hit the downvote button by accident and then I had to retry the action. This is what people mean by accessibility is for everyone all of the time. reply pseudosavant 10 hours agorootparentI have a tremor and I run into this issue on HN all the time. I need to zoom in a lot to be sure I'll hit it. reply datahack 10 hours agorootparentI don’t (yet) have accessibility challenges beyond glasses but hitting the tiny arrows is incredibly difficult. How come HN doesn’t update to be more accessible? It’s been a long time… I’m surprised it hasn’t been talked about by the team there. reply avtar 10 hours agorootparentAs someone who has carried out accessibility audits, I can unfortunately attest to this topic being a blindspot in tech circles. I remember hanging out with fairly senior frontend devs from a FAANG company who didn't know what purpose skip links served on websites. It can also be an uphill battle to advocate for remediation once design and development work is already baked in. reply brookst 6 hours agorootparentYep. And I think there’s an interesting implicit bias where younger / mornjunior developers often get tasked with things like that, so they see no problem reply ipqk 10 hours agorootparentprevDo you think Paul Graham or Garry Tan give a shit about accessibility? reply dgfitz 9 hours agorootparentNo. reply avtar 10 hours agorootparentprevTry https://www.modernhn.com if you haven't already. UI elements have more spacing around them, especially if zoomed in. reply ebcase 8 hours agorootparentHadn’t heard of this before, it looks great! Need it on mobile though, and would be happy to pay a reasonable fee for it. reply llm_trw 5 hours agorootparentYou can install the addon on mobile firefox. Ironically because of addons I now use firefox exclusively on mobile. reply spike021 8 hours agorootparentprevSame here with essential tremor. Rarely do people think of us. reply contravariant 10 hours agorootparentprevI've hidden and reported so many posts on the front page. I can only hope that the algorithms take into account that there's a decent chance someone trying to hit a link or access the comments will accidentally report a post instead. reply furyofantares 10 hours agorootparentEvery once in a while I wonder where a post disappeared to. Eventually I worked out I was accidentally hiding them, found dozens of hidden posts I was interested in when I looked at my profile. I still do it all the time. It's a problem on both desktop and mobile. I've send mail about it to dang before and did get a response, definitely a known issue. reply rkagerer 7 hours agorootparentprevThis may be an unpopular opinion but I'm just happy zoom works as expected on this site and don't mind if once in a while I have to correct a misvote. I appreciate the information density, and that HN hasn't adopted some \"contemporary\" UI that's all whitespace and animations. (And yes I agree the buttons are awkward) reply vasco 11 hours agorootparentprevZoom the website in, the browser has accessibility built in and hackernews zooms in fairly well. Edit: I seem to be missing something as this is getting downvoted. I genuinely cannot use HN under 150% zoom so thought this was a basic comment I was making. reply uoaei 11 hours agorootparentAccessibility isn't just about possibility, it's about ergonomics. You could integrate a differential equation by lining rocks up in a large desert as a computer, but you wouldn't say that solution is \"accessible\" to a human in the same way it would be with a CPU, a monitor, and functioning sight. reply vasco 10 hours agorootparentAh, so people are annoyed that the zoom isn't appropriate by default, I get it. Thanks I was getting extremely confused. That being said I use different zoom levels for different websites, and the browser remembers them for me, I like how the feature works right now, and I have loads of myopia. If people made it \"big enough\" to be inclusive by default in some websites I'd have to zoom out as well. So my point is that to me is more important to zoom in correctly (many websites don't), than be \"big enough\" to start with. reply Tijdreiziger 7 hours agorootparentI mean, even at 175% zoom, the vote arrows are pretty close together on a touchscreen. And I’m barely 30. I always end up double-checking whether the ‘unvote’ link is present. If it says ‘undown’, I know I’ve made a mistake (or vice versa). reply uoaei 9 hours agorootparentprevI agree about zoom levels on different websites when on desktop. Zoom works differently on mobile vs desktop which is its own challenge as far as ergonomic use. On mobile, to get the up/downvote buttons to the size I want them, I'd have to scroll side-to-side to read comments. reply Ocha 14 hours agoprevMusic haptics can be a cool way to teach someone how to dance and “feel the beat” reply burntwater 11 hours agoparentI'm severely hearing impaired and enjoy going to dance classes - swing, salsa, etc. If I'm standing still, I can easily tune into the beat. But once I start moving, I quickly lose it on many songs; dance studios aren't known for having large sound systems with substantial bass. I don't know that this specific setup would fix anything -- it would need some way of syncing to the instructor's iPhone that is connected via bluetooth to the studio's little portable speaker. But it's a step in the right direction. While on the topic, I can hear music but almost never understand lyrics; at best I might catch the key chorus phrase (example: the words \"born in the USA\" are literally the only words I understand in that song). A few months ago I discovered the \"karaoke\" feature on Apple Music, in which it displays the lyrics in time with the music. This have been game changing for me. I'm catching up on decades worth of music where I never had any idea what the lyrics are (filthy, that's what they are. So many songs about sex!). It has made exercising on the treadmill, elliptical, etc actually enjoyable. reply pests 6 hours agorootparent> A few months ago I discovered the \"karaoke\" feature on Apple Music, in which it displays the lyrics in time with the music. JSYK Spotify has had this for years too, its just under the \"View Lyrics\" button but it does highlight the sentence/word karaoke style. It used to be a \"spotify app\" back in that genre of the service. reply NeuroCoder 14 hours agoparentprevThere's a lot of interesting things we can do with haptics since they're relatively cheap to put in stuff. Hopefully accessibility gets the software and applications further along soon reply ErigmolCt 13 hours agoparentprevUsing haptics in music to enhance rhythm perception and dance skills. Sounds really cool! reply ChrisMarshallNY 11 hours agoprevI'm a believer in accessibility features. The difficulty is often in testing. I use SimDaltonism, to test for color-blindness accessibility, and, in the last app I wrote, I added a \"long press help\" feature, that responds to long-presses on items, by opening a popover, containing the label and hint. Makes testing much easier, and doubles as user help. reply trilorez 6 hours agoprevHaptics in the Music app will be great but it’s not exactly “new”, considering my haptic music app has been out for several years now: https://apps.apple.com/us/app/phazr/id1472113449 reply cush 12 hours agoprevAll these features look amazing! That car motion sickness feature especially. Can’t wait to try it! reply petre 12 hours agoparentAt least I put off the phone while I was in the car. Not the case now. Thank you Apple but I'd rather be sick while looking at your phone in a car. reply gpm 11 hours agoprevI wonder if Vision Pro has enough microphones to do the acoustic camera thing? If so you could plausibly get \"speech bubbles over peoples heads\", accurately identifying who said what. I imagine that could be pretty awesome for deaf people. reply stubish 2 hours agoparentDeaf people I know certainly wouldn't want to be wearing a headset, say, in a restaurant. But a phone app or tablet that can do live text-to-speach would be good enough in many cases if it can separate voices into streams. Anything like this available? reply stacktrust 16 hours agopreviOS 17 Image Descriptions are quite good, but audio descriptions don't seem to work on non-Pro devices, even though text descriptions are being shown on the screen and the audio menu is present and activated. Is that a bug? Even on Pro devices, audio image descriptions stop working after a few cycles of switching between Image Magnifier and apps/home. This can be fixed by restarting the app and disabling/enabling audio image descriptions, but that breaks the use case when an iPhone is dedicated to running only Image Magnifier + audio descriptions, via remote MDM with no way for the local blind user to restart the app. On-device iOS image descriptions could be improved if the user could help train the local image recognition by annotating photos or videos with text descriptions. For a blind person, this would enable locally-specific audio descriptions like \"bedroom door\", \"kitchen fridge\" or specific food dishes. Are there other iOS or Android AR apps which offer audio descriptions of live video from the camera? reply Baeocystin 7 hours agoprevOne of the most important accessibility features they could bring back are physical home buttons on at least one ipad. I am completely serious. I work with a lot of older adults, many of whom have cognitive challenges, and the lack of a physical button to press as an escape hatch when they get confused is the #1 stumbling block by a mile. reply astrange 6 hours agoparentWhen there were physical buttons, it was very popular in Asia to enable the accessibility option to put a virtual one on screen instead, because they were afraid the physical one would break. So it was kind of useless in the end. reply diebeforei485 6 hours agorootparent> because they were afraid the physical one would break On early models it was actually quite common for the button to stop working after a year or two. The durability has since improved, but habits die hard. reply Terr_ 5 hours agorootparentI have a pet-theory that certain older folks are cautious with technology because they grew up with stuff that would break expensively if you pressed the buttons in the wrong order. Then when they see younger people just randomly explore-clicking to get things working--because their experience is that it's a safe tactic--that can gets misinterpreted as expertise, leading to: \"Wow, kids these days just know technology.\" reply astrange 5 hours agorootparentprevIt actually improved by no longer being a real button, instead it was a fake indent with a pressure sensor that did a haptic \"click\". But it still took up a lot of space on the front. reply JimDabell 7 hours agoparentprevIt’s not physical, but if the problem is they need something visible rather than a gesture, then you can put an always-on menu button on the screen that has a home button in. https://support.apple.com/en-sg/111794 reply lowkey 7 hours agoparentprevI agree. I have a last gen base iPad with Touch ID and a home button. I am pretty tech savvy but actually prefer this form factor. reply foooorsyth 7 hours agorootparentTouch ID is so drastically superior to Face ID in so many common “iPad scenarios”, eg. laying in bed with face partially obscured. I don’t understand Apple’s intense internal focus on FaceID only. FaceID with TouchID as an option when a device is flat on the table or when your face is obscured is so much nicer. reply mortenjorck 6 hours agorootparentI actually can’t use Touch ID, at least year-round. I have naturally very dry skin, and no matter how much moisturizer I used in colder months, I had to constantly have it relearn my fingerprint, to the point where I would just give up and use a passcode instead. Face ID, on the other hand, has been flawless since my first phone with it in 2018 (minus some mask-related challenges during the pandemic). It and ProMotion are basically the two reasons I bought an iPad Pro over an iPad Air. reply aikinai 5 hours agorootparentMy mother had the same issue. TouchID would stop recognizing her finger less than a day after setting it up. reply diebeforei485 6 hours agorootparentprev> I don’t understand Apple’s intense internal focus on FaceID only. It isn't. Just a few days ago the new iPad Air has Touch ID. Only iPad Pro uses Face ID. For iPad Pro users who use it for work and unlock it hundreds of times a day when in an office or during a commute, Face ID is vastly superior. reply sleepingreset 6 hours agorootparentprevFaceID builds profitable user habit loops. More real-estate on the screen to show things, easier to thoughtlessly purchase things if your password is a glance at the camera, etc. I don't think this is a user-focused decision, I believe it's a profit-focused one. reply astrange 6 hours agorootparentYou can't buy things just by looking at the camera, there are forced button presses and the App Store tends to make you enter your password. reply rsingla 7 hours agoprevWhile it's a significant step forward for accessibility, it also invites us to consider how such technologies could integrate into everyday use for all users. This could enhance ease of use and efficiency, but it also requires careful consideration of privacy safeguards. reply alphabetting 14 hours agoprevgoogle yesterday also open sourced their accessibility feature for android and windows that controls cursor using head movements and facial gestures https://github.com/google/project-gameface reply dagmx 19 hours agoprevThis is one major advantage to Apple sharing a foundation across all their devices. Vision Pro introduced eye tracking to their systems as a new input modality, and now it trickles down to their other platforms. I am surprised CarPlay didn’t have voice control before this though. reply jsheard 16 hours agoparentIs this really likely to be downstream of the Vision Pro implementation? I would think that eye-tracking with specialized hardware at a fixed position very close to the eye is very different to doing it at a distance with a general purpose front facing camera. reply dmicah 15 hours agorootparentTypically eye-trackers work by illuminating the eyes with near infrared light and using infrared cameras. This creates a higher contrast image of the pupils, etc. I assume Apple is doing this in the Vision Pro. Eye-tracking can also be done with just visible light, though. Apple has the benefit of knowing where all the user interface elements are on screen, so eye-tracking in this on the iPhone or iPad doesn't need to be high precision. Knowledge of the position of the items can help to reduce the uncertainty of what is being fixated on. reply neverokay 15 hours agorootparentSo there isn’t much more to it than getting a good resolution image of the eye from any distance, every millisecond. Precision is the issue because we are mostly moving our eyes in about 8 directions, there’s no precision because we don’t know how to measure focusing of our eye lens with a camera yet (unless that too is just a matter of getting a picture). Squinting would be the closest thing to physically expressing focusing. So the camera needs to know I’m looking left with my eye, followed by a squint to achieve precision. Seems stressful though. Gonna need AI just to do noise cancelling of involuntary things your eyes do like pupil dilation, blinking. reply dagmx 8 hours agorootparentprevNot the hardware side, but the software side. Implementing all the various behaviours and security models for the eye tracking and baking it into SwiftUI, means that it translates over easier once they figure out the hardware aspect. But the iPads and iPhones with FaceID have had eye tracking capabilities for a while, just not useful in UI. reply iloveyouocean 16 hours agorootparentprevThe 'tracking eyes' part is different, but once you have eye position data, the 'how the eyes interact with the interface' could be very similar. reply callwhendone 16 hours agoparentprevIt would be amazing if it gets carried over to the Mac. reply simlevesque 16 hours agoparentprevCarPlay devices aren't really powerful. reply Tagbert 15 hours agorootparentCarPlay devices (car components) are essentially playing a streaming video of a hidden display generated by the phone. CarPlay also lets those devices send back touch events to trigger buttons and other interactions. Very little process is done on the vehicle. BTW if you are plugged in to CarPlay and take a screen shot, it will include the hidden CarPlay screen. reply andrewmunsell 15 hours agorootparentprevCarPlay is rendered by the phone itself, so it's not strictly a function of how powerful the car infotainment is. You've been able to talk to Siri since the beginning of CarPlay so additional voice control is really just an accessibility thing reply joezydeco 15 hours agorootparentSome cars already have a voice control button on the wheel for their existing system which, if done correctly, is overriden by Siri+CarPlay. Which is really nice when it works. reply C-Loftus 10 hours agoprevThis is awesome and I love the UX, although I can't help but feel a bit sad that we need to always rely upon Apple and Microsoft for consumer accessibility. It would be so great if more resources could be allocated for such things within the Linux ecosystem. reply s3p 16 hours agoprevI wonder if this announcement had anything to do with the bombshells OpenAI and Google dropped this week. Couldn’t this have been part of WWDC next month? reply terramex 16 hours agoparentTomorrow (today in some timezones) is Global Accesibility Awareness Day: https://en.m.wikipedia.org/wiki/Global_Accessibility_Awarene... reply Tagbert 15 hours agoparentprevAs Terramex pointed out this is tied to a particular, relevant event. It’s also pretty common for Apple to preannounce some smaller features that are too specialized to be featured in the WWDC announcements. This gives them some attention when they would be lost and buried in WWDC footnotes. It is also probably an indication that WWDC will be full of new features and only the most impactful will be part of the keynote. reply kergonath 14 hours agoparentprevThey do it every year at the same time. Also, it’s a small announcement, not a keynote or the kind of fanfare we have at WWDC or the September events. This does not seem calibrated to be effective in an advertising war with another company. All this to say, probably not. reply joshstrange 13 hours agoparentprevI think it's more of \"clearing the decks\" for stuff that didn't make the cut for WWDC. I assume WWDC is going to be all about AI and they couldn't find a good spot to put this announcement. \"Clearing the decks\" isn't a very kind way to refer to this accessibility tech since Apple has always been better than almost everyone else when it comes to accessibility. I don't see this as \"we don't care, just announce it early\" as much as \"we can't fit this in so let's announce it early\". reply mdm_ 14 hours agoprevMy first thought upon seeing the Haptic Music feature is to wonder how long until they make compatible headphones and I can relive my high school years, walking around listening to nu-metal and hip-hop with a Panasonic Shockwave walkman. reply hprotagonist 12 hours agoprevVery curious to see how well eye tracking works behind a motorcycle visor. Thick leather gloves, bike noise, and a touch screen and audio interface are not much fun. reply GiorgioG 13 hours agoprevMy wife is a hospice nurse and from time to time she'll have a patient without any ability to communicate except their eyes (think ALS) - for these folks in their final days/weeks of life this will be a godsend. There are specialized eye-tracking devices, but they're expensive and good luck getting them approved by insurance in time for the folks in need near the end of their lives. reply tap-snap-or-nap 8 hours agoprevThis will have just enough capability limits and annoyances for you to be convinced to purchase a Vision Pro. It also makes eye tracking more widely accepted by making it available to everyone by using their existing devices. reply killjoywashere 8 hours agoprevImagine an eye-tracking loupe function on mobile: fit the same amount of text but bubble up the part under foveolar gaze. Save on readers* everywhere. * readers are those glasses you can pick up at the drug store for $17.99. reply skygazer 2 hours agoparentIt took me a couple read-throughs to understand what you meant, but yes, on-screen gaze-sensitive magnification would be amazing, I agree. reply xyst 9 hours agopreveye tracking feature is interesting … for ad-tech companies reply rado 15 hours agoprevThey keep announcing these bombastic accessibility features while simple things like tabbing remain frustratingly broken. The macOS “allow this app to access that” dialog supports shift+tab, but not tab. reply Jtsummers 15 hours agoparenthttps://support.apple.com/guide/mac-help/use-your-keyboard-l... - Keyboard navigation https://support.apple.com/guide/mac-help/navigate-your-mac-u... - Full Keyboard Access (accessibility feature, goes beyond just tabbing between elements) It's annoying that tabbing between UI elements is off by default on macOS. It's one of the first things I turn on with a new mac. reply devmor 15 hours agoprevI am excited for Vocal Cues - my main frustration with Siri is how poorly it comprehends even explicit instructions. One thing I wish Apple would implement is some kind of gesture control. The camera can detect fine details of face movement, it would be nice if that were leveraged to track hands that aren't touching the screen. For an example, if I have my iPhone or iPad on the desk in front of me, and a push notification that I don't need obstructs the content on the screen, I would love to be able to swipe my hand up towards the phone to dismiss it. reply myth_drannon 15 hours agoprevEye Gaze devices(tablet with camera + software) cost around $20K, even if it offers 1/4 of the features this is good news for those who can't afford it. reply asadotzler 11 hours agoparentDon't be ridiculous. Solid hardware and software combos for windows cost a small fraction of that. The convenient and decent Tobii PCEye costs like $1,250 and a very nice TMS5 mini is under $2,000. Your bullshit was off by at least an order of magnitude. reply boneghost 10 hours agorootparentLet's be fair and compare similar products. Do you have any examples of $2000 mobile devices that support eye tracking on the OS level? The products you mention look like they're extra hardware you strap to a Windows PC. Certainly useful, but not quite as potentially life-changing as having that built into your phone. reply chaostheory 9 hours agoprevNice to see features from Vision Pro make it onto other Apple products reply toasted-subs 10 hours agoprevGotta make surf they got all the eye balls then make sure they don't get to date anybody and become homeless. reply tootie 16 hours agoprevQuibble but this isn't \"eye tracking\" it's \"gaze tracking\". Eye tracking is detecting where your eyes are. Gaze tracking is what you're looking at. reply bogtog 9 hours agoparentWhere are you getting this language point from? If I look up any company selling \"eye trackers\", their products are all meant to track where you're looking, e.g., https://www.tobii.com/ reply tootie 7 hours agorootparentInteresting. I picked it up working with Tobii devices a few years ago actually. I guess they updated their vocabulary. reply astrange 14 hours agoparentprevThis is how I feel about \"face recognition\" (it should mean recognizing whether something is a face or not), but it is common to use eye tracking this way. reply crancher 15 hours agoparentprevWell then it's both? reply tootie 14 hours agorootparentCould be, but not necessarily. Eye tracking usually means it's tracking the eyes of one or more people in a video. Gaze tracking usually requires your eyes stay pretty steady and close to the tracker. reply syngrog66 9 hours agoprevwhile we're on topic of accessibility I'd like to point out the following lovely facts: * on Android, in the builtin popup menu for copypasta, the COPY button is wedged in tightly (by mere millimeters) between the CUT and PASTE buttons on either side of it. A harmless one packed in between two destructive, lossy ones, and usually irreversible. * the size of an adult human fingertip is no secret (and in fact 99.99% of humans have at least one!) * Google's staff consists of humans * Google supposedly hires ONLY \"smart\" people and their interview process involves some IQ-like tests * Android has had this obvious anti-pattern for many years now. perhaps 10+? reply fitsumbelay 15 hours agoprevmy first thought regarding eye-tracking: \"whose accessibility to what|whom?\" reply devmor 15 hours agoparentDo you have another idea beyond the accessibility of Users without fine motor control to the phone's screen? reply smegsicle 13 hours agorootparentwhich ads you are looking at reply EasyMark 13 hours agoprevhow can I make sure it's off? Is it off by default? reply yreg 12 hours agoparentYes reply dakial1 15 hours agoprevEye tracking is not an accessibility feature, it is an advertising optimization feature disguised as an accessibility feature. reply joshstrange 13 hours agoparentYou can say a lot of negative true things about Apple but this is just silly. There is no way Apple is going to expose that data to underlying apps in the same way they refused to do it in Vision Pro. I'd bet a good bit of money it works the same way where it's a layer on top of the app that the app can't access and from the video it looks like that's exactly how it works. reply simonw 14 hours agoparentprevApple are very good about not making this kind of thing available to apps that don't have an explicit reason to need it. reply resource_waste 14 hours agorootparentnext [5 more] [flagged] jshier 14 hours agorootparentNo optimism, simply facts based on the lack of APIs for accessing any of this data. Since this is just a preannouncement (APIs will come at WWDC next month), we can't tell yet, but Apple's previous system attention tracking was never available in public APIs. reply simonw 14 hours agorootparentprevI stand by my statement. Have you seen the limits placed on VisionOS apps regarding eye tracking data? reply chefandy 11 hours agorootparentprevBaseless pessimism is exactly as useful as baseless optimism. reply rootusrootus 13 hours agorootparentprevIt is preferable to the reflexive cynicism and nihilism that is so prevalent online. At least the supporters have put some thought into their argument. reply jcotton42 15 hours agoparentprevMy presumption is that apps will not be able to access this data, at least without some sort of permission gate. reply sroussey 14 hours agorootparentIndeed they haven’t for all the years it was limited to FaceId’s attention option. reply devmor 15 hours agoparentprevEye tracking is absolutely an accessibility feature. Just because you don't need it, and it can be abused, does not mean it isn't an absolutely game changing feature for some people. reply nofunsir 14 hours agorootparentHe didn't say it wasn't an accessibility feature, just that it was disguised as one. Just because it's not a game changing feature for some people, doesn't mean its primary function isn't advertising. reply yreg 12 hours agorootparentThe user did say verbatim “Eye tracking is not an accessibility feature” reply devmor 14 hours agorootparentprevSomething being disguised as something explicitly implies it is not that thing. reply Aloisius 12 hours agoparentprevIt allows people with ALS to navigate their device with their eyes. Microsoft added a similar feature to Windows about seven years ago. reply tallytarik 13 hours agoprev [–] > “We believe deeply in the transformative power of innovation to enrich lives,” said Tim Cook, Apple’s CEO. Does this line actually mean anything? Press releases are so weird. reply sussmannbaka 4 hours agoparentSure it means something, it’s pretty low on the PR talk scale even. Let’s see: > We believe deeply in \"We\", in this case, refers to Apple and as a company can’t believe in anything, it specifically refers to their employees. > the transformative power of innovation A new feature will bring forth change > to enrich lives In order to make someone’s life better. So far so good, at least no synergies being leveraged for 10x buzzword transformation. Let’s see if it passes the test: There’s a bunch of new accessibility features, that certainly fits the bill for innovation. As anecdata, at least one of these will drastically change how I interact with my devices, so there’s the transformative power. I will be able to use these new accessibility features to improve both the way I work and how I privately use technology, so one could argue my life is being positively enriched by this transformative power brought about by innovation. Do they \"believe deeply\" in this? That is only for Tim Cook and his employees to know, but they’re one of the few companies pushing advances in accessibility past the finish line, so they might! reply astrange 6 hours agoparentprev [–] I always wonder why press releases include claims that a person said a paragraph or more of text that noone would ever say out loud, but this one actually does sound like something he'd say. In a keynote video, anyway. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple unveils new accessibility features like Eye Tracking and Music Haptics to aid users with physical disabilities, deafness, and motion sickness.",
      "The tech giant remains devoted to inclusivity by updating its devices to enhance the user experience for all customers.",
      "Initiatives such as free sessions at Apple Store locations and promoting apps, games, and workouts emphasize accessibility and inclusion, coinciding with Global Accessibility Awareness Day."
    ],
    "commentSummary": [
      "Apple introduces new accessibility features such as eye tracking for their devices, sparking discussions on motion sickness, hunger, vehicle types, and correlations with accessibility.",
      "The dialogue delves into topics like VoiceOver for iOS 17, ethical use of eye tracking, improving accessibility in technology, and sharing tips on utilizing Apple's accessibility features based on personal preferences.",
      "Users also touch on legal frameworks for disability rights, challenges in mainstream tech for accessibility, and potential advancements in music and tech accessibility, discussing Touch ID, Face ID, and the benefits, affordability, and privacy concerns related to eye-tracking technology."
    ],
    "points": 400,
    "commentCount": 220,
    "retryCount": 0,
    "time": 1715782925
  },
  {
    "id": 40366962,
    "title": "User's Frustration: Navigating Broadcom's Unfriendly Website",
    "originLink": "https://matduggan.com/the-worst-website-in-the-entire-world/",
    "originBody": "What if you set out to make the worst website you possibly could? So poorly designed and full of frustrating patterns that users would not only hate the experience of using this website, but would also come to hate your company. Could we make a web experience so terrible that it would express how much our company hated our users? As a long-time Internet addict, I've encountered my fair share of terrible websites. Instagram where now half my feed is advertisements for stupid t-shirts and the other half is empty black space. Who in the fuck would ever wear this Or ARNGREN.net which is like if a newspaper ad threw up on my screen. But Instagram still occasionally shows me pictures of people I follow and ultimately the stuff on ARNGREN is so cool I still want to buy it regardless of the layout. No, I believe it is the crack team at Broadcom that have nailed it for the worst website in existence. Lured in with free VMware So through social media I discovered this blog post from VMware announcing that their popular virtualization software is free for personal use now. You can read that here. Great, I used VMware Fusion before and it was ok and maybe it will let me run Windows on an ARM Mac. Probably not but let's try it out and see. This means that everyday users who want a virtual lab on their Mac, Windows or Linux computer can do so for free simply by registering and downloading the latest build from the new download portal located at support.broadcom.com. With the new commercial model, we have reduced our product group offerings down to a single SKU (VCF-DH-PRO) for users who require commercial use licensing. This simplification eliminates 40+ other SKUs and makes quoting and purchasing VMware Desktop Hypervisor apps, Fusion Pro and Workstation Pro, easier than ever. The new Desktop Hypervisor app subscription can be purchased from any Broadcom Advantage partner. I don't want to register at support.broadcom.com but it looks like I don't have a choice as this is the screen on the VMware site. Now this is where alarm bells start going crazy in my head. Nothing about this notice makes sense. \"The store will be moving to a new domain\". So it's...not...down for maintenance but actually is just gone? Or is it actually coming back? Because then you say \"store will be shutdown\" (just a quick note, you want \"the store\" and \"will be shutting down on April 30th 2024\"). Also why don't you just redirect to the new domain? What is happening here? Broadcom So then I go to support.broadcom.com which is where I was told to register and make an account. Never a great sign when there's a link to an 11 page PDF of how to navigate your website. That's the \"Learn how to navigate Broadcom Support\" link. You can download that killer doc here: https://support.broadcom.com/documents/d/ecx/broadcom-support-portal-getting-started-guide Alright let's register. First the sentence \"Enhance your skills through multiple self-service avenues by creating your Broadcom Account\" leaps off the page as just pure corporate nonsense. I've also never seen a less useful CAPTCHA, it looks like it is from 1998 and any modern text recognition software would defeat it. In fact the Mac text recognition in Preview defeats 3 of the 4 characters with no additional work: So completely pointless and user hostile. Scoring lots of points for the worst website ever. I'm also going to give some additional points for \"Ask our chatbot for assistance\", an idea so revolting normally I'd just give up on the entire idea. But of course I'm curious, so I click on the link for the \"Ask our chatbot\" and..... It takes me back to the main page. Slow clap Broadcom. Imagine being a customer that is so frustrated with your support portal that you actually click \"Ask a chatbot\" and the web developers at Broadcom come by and karate chop you right in the throat. Bravo. Now in Broadcom's defense in the corner IS a chatbot icon so I kinda see what happened here. Let's ask it a question. I didn't say hello. I don't know why it decided I said hello to it. But in response to VMware it gives me this: Did the chatbot just tell me to go fuck myself? Why did you make a chatbot if all you do is select a word from a list and it returns the link to the support doc? Would I like to \"Type a Query\"?? WHAT IS A CHATBOT IF NOT TYPING QUERIES? Next Steps I fill in the AI-proof CAPTCHA and hit next, only to be greeted with the following screen for 30 seconds. Finally I'm allowed to make my user account. Um....alright....seems like overkill Broadcom but you know what this is your show. I have 1Password so this won't be a problem. It's not letting me copy/paste from 1Password into this field but if I do Command + \\ it seems to let me insert. Then I get this. What are you doing to me Broadcom. Did I....wrong you in some way? I don't understand what is happening. Ok well I refresh the page, try again and it works this time. Except I can't copy/paste into the Confirm Password field. I mean they can't expect me to type out the impossibly complicated password they just had me generate right? Except they have and they've added a check to ensure that I don't disable Javascript and treat it like a normal HTML form. Hey front-end folks, just a quick note. Never ever ever ever ever mess with my browser. It's not yours, it's mine. I'm letting you use it for free to render your bloated sites. Don't do this to me. I get to copy paste whatever I want whenever I want. When you get your own browser you can do whatever you want but while you are living in my house under my rules I get to copy/paste whenever I goddamn feel like it. Quickly losing enthusiasm for the idea of VMware So after pulling up the password and typing it in, I'm treated to this absolutely baffling screen. Do I need those? I feel like I might need those. eStore at least sounds like something I might want. I don't really want Public Semiconductors Case Management but I guess that one comes in the box. 44 seconds of this icon later I'm treated to the following. Broadcom, you clever bastards. Just when I thought I was out, they pulled me back in. Tricking users into thinking a link is going to help them and then telling them to get fucked by advising them to contact your sales rep? Genius. So then I hit cancel and get bounced back to......you guessed it! Except I'm not even logged into my newly created account. So then I go to login with my new credentials and I finally make it to my customer portal. Well no first they need to redirect me back to the Broadcom Support main page again with new icons. Apparently my name was too long to show and instead of fixing that or only showing first name Broadcom wanted to ensure the disrespect continued and sorta trail off. Whatever, I'm finally in the Matrix. Now where might I go to...actually download some VMware software. There's a search bar that says \"Search the entire site\", let's start there! Nothing found except for a CVE. Broadcom you are GOOD! For a second I thought you were gonna help me and like Lucy with the football you made me eat shit again. My Downloads was also unhelpful. But maybe I can add the entitlement to the account? Let's try All Products. Of course the link doesn't work. What was I even thinking trying that? That one is really on me. However \"All Products\" on the left-hand side works and finally I find it. My white whale. Except when I click on product details I'm brought back to.... The blank page with no information! Out of frustration I click on \"My Downloads\" again which is now magically full of links! Then I see it! YES. Clicking on it I get my old buddy the Broadcom logo for a solid 2 minutes 14 seconds. Now I have fiber internet with 1000 down, so this has nothing to do with me. Finally I click the download button and I get.....the Broadcom logo again. 30 seconds pass. 1 minute passes. 2 minutes pass. I'm not sure what to do. No. No you piece of shit website. I've come too far and sacrificed too much of my human dignity. I am getting a fucking copy of VMware Fusion. Try 2 is the same thing. 3, 4, 5 all fail. Then finally. I install it and like a good horror movie, I think it's all over. I've killed Jason. Except when I'm installing Windows I see this little link: And think \"wow I would like to know what the limitations are for Windows 11 for Arm!\". Click on it and I'm redirected to... Just one final fuck you from the team at Broadcom. Conclusion I've used lots of bad websites in my life. Hell, I've made a lot of bad websites in my life. But never before have I seen a website that so completely expresses just the pure hatred of users like this one. Everything was as poorly designed as possible, with user hostile design at every corner. Honestly Broadcom, I don't even know why you bothered buying VMware. It's impossible for anyone to ever get this product from you. Instead of migrating from the VMware store to this disaster, maybe just shut this down entirely. Destroy the backups of this dumpster fire and start fresh. Maybe just consider a Shopify site because at least then an average user might have a snowballs chance in hell of ever finding something to download from you. Do you know of a worse website? I want to see it. https://c.im/@matdevdug Share Topic broadcom Apple The Time Linkerd Erased My Load Balancer Due to a combination of issues with GKE and Linkerd, I ended up deleting my load balancer routes when I removed the Linkerd helm chart.… 03 May 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40366962",
    "commentBody": "The Worst Website in the Entire World (matduggan.com)362 points by speckx 20 hours agohidepastfavorite181 comments blantonl 18 hours agoThis is literally enterprise software in a nutshell. If you've ever wondered what the \"Enterprise Application Server v21.5™ now with AI, Chatbots, LDAP, Active Directory Integration, Orchestration, and Web 3.0\" experience looks like, this is it. This is what happens when you bring enterprise software into the general public's view. This is what enterprise software customers see every day. Remember, at some point in the rollout of this dog, the team sat in a conference room and came to the right conclusion that the portal is terribly difficult to navigate, and thus the bright idea to write an 21 page PDF instruction manual for the portal was handed off to a 18 person team. Edit update: The \"Enterprise Chatbot Integration Plugin v2.1™ for Enterprise Portals - Enterprise Application Server v21.5\" was an add on kicker for $1.6MM license revenue and $3.8MM for 21 years of support. This plugin was developed by one person who works for EnterpriseSoftwareCorp Inc at the behest of sales and marketing and management that decried \"we must have a Chatbot AI offering for our enterprise customers because they are asking why we don't.\" The sales exec who inked the contract after the Broadcom merger ended up #4 in the company for sales, went to Hawaii for the EnterpriseSoftwareCorp President's club awards presentation. The Broadcom engineer who was forced to implement this plugin into the Portal just copied the example from the docs (a template of links) and realizes he'll really have to roll his own LLM to add any real capabilities to the bot. But, he was able to check the box that says \"we have a chatbot\" reply yungporko 17 hours agoparenthonestly this isn't that bad imo. i've seen, used, and been forced to make stuff that is way worse than this. it's just the natural result of the corporate development process and it's virtually impossible to not end up with something that strongly resembles the broadcom site. you get one guy who just comes up with ideas in the shower and then drops a message on microsoft teams at 9:30pm telling the team to make it so, and you also have any manager even remotely involved with anybody who uses the product able to dictate features and functionality too, none of these people have experience in technical roles and are either sales, ex-sales or ex-scrum masters. then finally at the end of the human centipede, you have a bunch of .NET-brained pseudoprogrammers sitting in a circle nitpicking and debating the most \"correct\" way to split up and size the current thing and then cram it into the existing mess until you end up with a plan of action that is a combination of multiple ideas which may have once been half decent in isolation but the result is a steaming pile of human shit. reply nogridbag 17 hours agorootparentI've been building enterprise software for a while, but in smaller startups. In all cases we've taken pride in our UX. In the current product I'm building, the domain experts are a generation older than myself and the mockups and designs they produce reflect that. If we just recreated their spec to the pixel, our application would fit right in on a Windows 95 desktop. Yet if you were to look at our application, it has a clean, modern, user friendly design. To accomplish that required me to occasionally push back when they were set in their ways or some cases just ignoring the requirements and building out certain functionality my way. The domain is sufficiently complex that we don't have a ton of time to focus on UX. So the most important thing was setting the general UX patterns from day 1 and mandating developers follow that early on. reply ghusto 13 hours agorootparent> I've been building enterprise software for a while, but in smaller startups. These words you are using, I do not think they mean what you think they mean ;) reply antisthenes 18 hours agoparentprevI don't think all enterprise websites are this bad. Certainly my utility websites (e.g. electric/gas) are a lot more functional and a lot less user hostile, because...those companies would really like it if you paid your bill on time, so at least that workflow is pretty polished. reply kodama-lens 16 hours agorootparent> Certainly my utility websites (e.g. electric/gas) are a lot more functional and a lot less user hostile, because...those companies would really like it if you paid your bill on time, so at least that workflow is pretty polished. Your utility websites are customer facing and everything that the user can't do themselves will result in a phone call or a ticket wich will directly drive up cost. In enterprise it is the opposite. Whatever the costumer cant do themselves requires a ticket. Any ticket or fast ticket response requires support wich increases revenue. I just had a meeting with someone from IBM last week about API Connect, they admit that their docs suck and are wrong in places. It is typical enterprise software, slow and cumbersome, just as reported by OP. reply Vilian 37 minutes agorootparent>In enterprise it is the opposite. Whatever the costumer cant do themselves requires a ticket. Any ticket or fast ticket response requires support wich increases revenue. this make so much sense, but i never thought about it reply eitally 18 hours agorootparentprevI think the point they were trying to make is that enterprise software (served as SaaS for internal-to-the-business users) frequently has awful UX. Think things like SAP & Oracle (or anything Oracle has acquired, like Cerner, AgilePLM, etc) -- those big, heavy, complicated enterprise softwares rife with decade(s) of technical debt and no-longer-understood features that were tied to long forgotten business or technical requirements and created by commodity developers who weren't particularly skilled or particularly knowledgable about their domain. I ran an Enterprise Apps org for a F500 where IT was purely a cost center and we created crap like this all the time. reply fein 18 hours agorootparentprevYou aren't dealing with the enterprise site at that point - rather a public frontend that uses some enterprise-y backend. The real fun begins when you get into the actual enterprise frontends for internal use like SAP Netweaver and Sailpoint, which end up being quite a lot like the broadcom experience in the article. reply not2b 18 hours agorootparentIt's a racket. It might not be as common today, but I remember when there were lots of people whose career was based on their SAP expertise, and the reason they got hired was that no one else could deal with that crap if anything went wrong. Once a lot of those people get into big companies, their career is based on preventing their employer from dumping SAP (or equivalent) for something better. So, it's like they have agents inside all the large companies that use their stuff. reply eitally 18 hours agorootparentWhen I worked in manufacturing IT for a F500, a full 20% of our IT organization was various flavors of Oracle support. reply duozerk 17 hours agorootparentprev> Sailpoint Oh gods, the painful flashbacks. reply Sesse__ 18 hours agorootparentprevWait, you need to go to a website to pay your electric bill? Mine is auto-deducted from my bank (up to a certain monthly maximum that I have set myself). You get bills directly in your bank, and then approve (possibly allowing auto-deduction for future cases) or modify or ignore them as you see fit. reply MH15 18 hours agorootparentOne would figure you'd set up this autopay through the utility provider's website. reply GrinningFool 17 hours agorootparentMany banks let you set this up directly with them - through their own site - via partner integrations with utility providers. reply Sesse__ 17 hours agorootparentprevUh, why? Why would I need to go through a different procedure between each entity I want to pay? After all, the point of a bank is to make unified transfer procedures between entities. reply dsr_ 16 hours agorootparentprevI bet a shiny nickel that you live in a place where the liability for a mistake in this procedure is between the bank and the utility company, not on you. Over in the USA, setting up a bill autopay for a variable amount generally involves a credit card intermediary with a 2-4% rake or a lot of risk to you. reply xnorswap 26 minutes agorootparentPeople in the USA need to lobby for better and modern banking https://en.wikipedia.org/wiki/Direct_debit#United_Kingdom Not only does it not cost anything to the payer, the \"Direct Debit Guarantee\" also protects the payer from mistakes and fraud. This isn't a new system, it's over 50 years old. Paying bills shouldn't be a risky or expensive thing to do. reply BizarroLand 17 hours agorootparentprevIt's only if you don't set up autopay, but you usually have to go to the website for the first time at least to set up the autopay. reply fkyoureadthedoc 19 hours agoprev> Hey front-end folks, just a quick note. Never ever ever ever ever mess with my browser. It's not yours, it's mine. I'm letting you use it for free to render your bloated sites. As if any front end developer came up with this. Anyone who has ever had job in the industry knows this is straight from management. reply airstrike 19 hours agoparentI would think management can't be that adamant about not letting users copy-and-paste... I would also think front end folks should try saying \"no\" to at least some of those silly requests reply kbolino 18 hours agorootparentLarge corporate/government IT lives on another plane of existence. Rules are made in some far-flung office and enforced through edicts that can't be challenged, partly because nobody knows exactly who created them, partly because nobody wants to stand out, and partly because yes-men surround the upper levels of management. Anyway, somebody somewhere about a decade ago seems to have injected into the heads of such rule-makers that users who paste their password confirmations defeat the purpose of the confirmation mechanism, which was leading to excess support requests for forgotten passwords. So, therefore, pasting into the confirmation box (or even better, both boxes) should be disabled. Never mind that password rules have gotten more complex, that allowing users to temporarily preview their passwords instead is now recommended, or that the use of password managers and online password resets means even if the original concern were valid, it's now moot. The rule exists, and so it must be followed. At some point these corporations do lurch forward (or die), so eventually this will get changed, but it'll happen way slower than it should. reply bornfreddy 18 hours agorootparentHonestly, 1Password (& co.) should have an option to \"Type password\" next to \"Paste password\". Prevent that, you stupid website, I dare you! reply jszymborski 18 hours agorootparentFwiw keepass and keepassxc allow you to do this. reply sparky_z 18 hours agorootparentprevIt does, actually. (At least the old self-hosted version that I still use does. Don't know about the newer one.) reply benhurmarcel 13 hours agorootparentprev1password can do that reply pavlov 19 hours agorootparentprevDisabling copy-paste is exactly the kind of thing that a higher-level manager sees on some website, decides immediately that it’s very important for content protection and IP and trade secrets and whatnot, then emails a middle manager to have this implemented ASAP. A week later the request has filtered into a ticket that lands in the front-end developer’s inbox. What should the developer do exactly? Ignore the ticket? Educate the manager who’s perhaps three steps up in the hierarchy and doesn’t even know the person’s name who is charged with implementing the misfeature? Neither would go down well. reply ghusto 13 hours agorootparentNo reason to ignore it, but it's pretty easy to argue against. I realise in some places you'd not be talking directly to the person it came from, but if the person you _are_ able to talk to says \"That all may be true, but just do it\", it's time for a new job. reply duxup 18 hours agorootparentprevI do some front end work. I push back on things and win some battles re-directing them, but ultimately if the client pays to do a stupid thing, they get the stupid thing. It is their website, not mine. reply trustno2 19 hours agorootparentprevThis shit is usually from \"security\" which, in corps, is just endless list of boxes that you need to check and are handed over manager to manager. Everyone is scared to actually remove anything from the list because nobody knows who is actually responsible for maintaining it; getting through the hierarchy to even find such a person would take a month; if you find him, he will tell you \"oh it's for compliance with , safer to keep it there\" reply bonestamp2 17 hours agorootparentExactly, good luck convincing management not to do something that the infosec team suggested even though it provides an insignificant amount of security. The hackers you really have to worry about aren't using your front end, they're submitting directly to your endpoint to bypass exactly these kinds of things. reply npteljes 19 hours agorootparentprevNot in my experience, and not for trivial things like this. I'm sure this varies widely with employer, location and life situation, but generally these kinds of annoyances are both far from the worst that people need to do / tolerate, and that they don't have any say in what goes into the product, they either implement it, or someone else implements it and they can go work at someplace else if they don't like it. reply bongoman42 13 hours agorootparentprevIt absolutely can be, and can even go higher. Recently the Reserve Bank of India updated rules around what is considered security, many banks interpreted them to mean copy and paste into /any/ field is not allowed. Yes, you have to fully type in account numbers etc. reply surfingdino 18 hours agorootparentprevThat one may be coming from the InfoSec guys. reply pixl97 18 hours agorootparent\"Infosec mill agency that looks for easy wins to justify the high price tag they charge enterprise clients like Broadcom\" FTFY. reply bonestamp2 18 hours agorootparentprevI can't imagine any front end person spending extra time blocking the paste function when nobody asked them to do that. This may also come as a surprise, but sadly management and infosec doesn't always take advice from the front end developers. reply balls187 18 hours agorootparentprevThis isn’t a front end dev problem. You can say no but management isn’t under any obligation to capitulate, and often won’t. More over, it’s often solutioneering as a result to some other management identified issue that devs have pushed back on. reply NoMoreNicksLeft 19 hours agorootparentprevThis is the imbecile's solution to people pasting in their passwords from a text file. Except some people paste them in from their password manager. Also, the error he got when he tried to put in the password the first time is likely because there's a mismatch between what it claims the password rules are, and what they really are. He might have exceeded the maximum password size (yes, I know they're supposed to be salted in the backend, and maybe then even are, but you still run into this). Or it might be that he used disallowed punctuation (some sites seem to dislike anything other than question marks and the ones over the 1-2-3 keys... I've personally seen the percent sign and ampersand both cause problems. If there were some little embedded xml file that my password manager could pull from the page automatically that would tell it what the rules are, then I wouldn't have to debug your shitty account creation systems, nameless developer drones out there working for big companies! Not that you care. reply qart 19 hours agoparentprevI wonder. In my experience, all Indian news media outlets (except two) hijack the clipboard. If you select and copy an entire paragraph, in your clipboard, you get only the first few words and a link to the article. While I hate it, and think they are being hostile to me. I think they are catering to a usage pattern, that if you paste that stuff in WhatsApp, the readers would definitely get a link to the article. Traffic guaranteed. reply charles_f 18 hours agoparentprevSide note on this clever work around https://github.com/aaronraimist/DontFuckWithPaste reply trustno2 13 hours agoparentprevfrom what I can tell, this comes from IE6 days, where evil site could potentially see your clipboard . Maybe. reply micromacrofoot 18 hours agoparentprevI think we can go another level up though: why are browser vendors allowing it if it's verboten — if they make it possible, someone will use it. No one's going to risk their job over their boss's inane request to break copy & paste. reply fkyoureadthedoc 18 hours agorootparentyeah I can't really think of a good use case for blocking paste. the Clipboard API is useful in general though and a good addition overall even if some people misuse it. reply sparky_z 18 hours agorootparentAs I understand it, you need to be able replace the paste command with your own custom thing for stuff like Google Docs. But then you can always just replace it with a no-op. reply _wire_ 16 hours agoparentprev> Hey front-end folks, just a quick note. Never ever ever ever ever mess with my browser. It's not yours, it's mine. I'm letting you use it for free to render your bloated sites. This edge is a greatly under-acknowledged and under-represented boundary of propriety, and is routinely flagrantly and hypocritically overrun by organizations with legions of attorneys who fight tooth and nail to stake their claims in the providence of others. The close cousin is the \"click-wrap\" agreement, which should be the very first point of engagement for access to any resource that employs it, but is perennially represented as an afterthought which a priori deprives the visitor of recourse from his later exploitation using the form of a \"contract\" which is fully understood by everyone to not be read, is written in gibberish, and placed at the very end of a primrose path of necessity for access to one's own labors. A huge warning sign of the intrinsic rentier dynamic of the high technology industry has been built into every PC since the dawn of the era and on prominent display: the \"Welcome\" screen. You think you are being warmly greeted upon arrival to the cusp of a vibrant commons, but you are actually being told in no uncertain terme that the PC you just bought was pre-appropriated by its software. The purchase price is rent. The device is your property only in the sense that you own the direct costs of its failure and disposal. You are given an account with limited access to its capabilities and being permitted to access it under the auspices of your hosts. Your work is without value to your hosts. The device is a conduit of your continuing consumption, controlled as tightly as possible, which with every step into its labyrinth further reduces, limits and degrades the value of your work to you, and shifts its value to the device purveyors. This hazard is conventional to the structure of every web service today, including this one: your data (work) goes in and never comes out. It's trapped in the dynamic and context maintained by the host. No social media architecture today respects your work in context, including this one. Your comments should belong to you, be hosted by you, and maintained in a mutually shared and beneficial context. But instead your comments go into a black box which you are permitted to review, in exchange for locally issued currency called (tragically) \"karma\" which is a simply a mechanism for limiting your visibility within a hopelessly regressive and passé format of a reverse-chronologically ordered list of the popular. Everyone on the social web is a serf, tilling a text box, and sharecropping status. My making an example of HN not to call it out for being egregious. HN is completely ordinary. I'm merely offering an example for how totally indoctrinated the technogentsia is to these dark patterns of social networking architecture and how blind everyone is to them. It's pretty weird that these dark patterns are so pervasive when you consider that the ideological bent of most computer technologists is \"libertarian\". But I should note that California ideology is inherently Randite, and Ayn Rand was a deeply disturbed person. With transformer AI we have now seen that every human input on the web has specific economic value which is being aggregated and harvested towards the creation and consolidation of enormous kingdoms of social wealth and privilege. This is being done completely without regard for the principles of propriety that software and MSM content publishers have represented through law as being essential to the construction of a commonwealth. Every output of a transformer is a derivative work without even attribution, much less royalties. And the AI technologists seem poised to have transformers run interference at every level of \"customer\" interaction with new architectures. The more you look into it, the more you will see that high technology has been an epic swindle to transfer control of a commons to narrow silos of exceptional privilege, in which not only does the commonwealth shrivel in exchange for the tech's very limited public advantages, but the vehicle you use for your contributions endlessly deprives you of the just fruits of your own labors, encircles you with infrastructure beyond your reckoning, and enforces your conformance to alien protocols via dark patterns. Much as automobiles make every destination into a parking lot, so the web browser has made every avenue to knowledge end in a gate which is ever further obfuscated into an opportunity to withhold something of value from the visitor, including the value of your own work in context. \"Welcome.\" reply mvkel 19 hours agoparentprevHm? No, it really did come from a front-end person. There was a period in the late-aughts when people wanted to emulate the iPhone's inertial scrolling on the desktop. Most modern sites had it and it was infuriating. That's probably around the time when this site was built. reply fkyoureadthedoc 18 hours agorootparentI'm thinking you may not have made it all the way to this part of the article when you were reading it, but here's the rest of the context > Don't do this to me. I get to copy paste whatever I want whenever I want. When you get your own browser you can do whatever you want but while you are living in my house under my rules I get to copy/paste whenever I goddamn feel like it. Forcing the user to type the password manually rather than letting them paste something in. I think the original idea was to not allow them to mistype the first one, then paste the typo in the second field. But it's a dated practice and very annoying. I once worked on a project for a Pharma company and this one guy tried very hard to push his password requirements and no pasting stuff, but luckily we convinced someone with final say that we should just follow the NIST guidelines for password reqs and leave the UX of the password field up to the UX people lol. I do agree though that smooth scrolling was a front end developer offense, luckily it went out of style pretty quickly. reply jiveturkey 17 hours agorootparent> the original idea was to not allow them to mistype the first one, then paste the typo in the second field. correct. the other analysis here is wrong. we see similar for payments where user is not allowed to paste in ACH info. but this isn’t exactly about user error per se. this is about support cost for bad entries. if the user types a wrong password during registration the recovery of such is very hard. the common user (even of a product like fusion) is VERY unsophisticated and will have severe problems recovering. the more advanced user will have plugins that disable paste disabling. the middle skill user (like in the post) will get past it on their own. so net net this is just another case of this is why we can’t have nice things. they “have to” address that bottom (skill) level of users. personally i can excuse this. the rest, not so much! reply gorkish 18 hours agoprevIn for a comment on the premise that led to this article: Broadcom didn't make vmware desktop apps free because they want you to use them; they made them free because they don't want to sell or support them anymore. They only still exist because they have to ride out existing commercial support agreements and customers need the software while they transition their workflows. Do not use Workstation or Fusion anymore; these products are dead-ended. reply bguebert 17 hours agoparentIt used to be that ESXi and vSphere were free to use for a single server too, but not anymore. reply andrewmutz 19 hours agoprevI would love to hear anyone defend the practice of disabling paste on password fields. I run into it relatively frequently and it both angers me and blows my mind that some developer or team thought this made sense reply JimDabell 18 hours agoparentI’ve had very expensive pen testers tell me to do this as recently as last year. They folded instantly at the first sign of pushback, so it seemed to me something that had been sitting in a checklist for years without anybody questioning it, making every website and app they audited worse. reply pksebben 17 hours agorootparentThat's awful. It's such a security antipattern, obvious as soon as you take into account, you know, _Humans_... Site: Please make a password Human: 7#hs&_suiE2KcS0 Site: No copy and pasting Human: mydogisagoodboy123 Site: Needs special characters Human: Pa$$w0rd12345 Site: Looks great thanks reply bguebert 17 hours agorootparentIts like those policies where the password needs to change every 60 days that was found to actually reduce security because of the count-up-by-one passwords people would use. For places where that's been a rule forever it is really get it removed. reply simcop2387 18 hours agoparentprevI don't agree with it either, but the reasoning i've always seen has been along the lines of, \"it prevents people from putting passwords into the clipboard which can be stolen by other programs\", and then similarly for disabling browser or password manager autofill \"because it prevents people from making a mistake and letting a field get filled with a password when it shouldn't\". Basically leading to \"users should just manually type in and remember all their passwords\" at the extreme end of the reasoning. reply nehal3m 18 hours agorootparentThat doesn't make sense. If I wanted to type a password into some other program for whatever reason I wouldn't find out that I can't paste into the password field until I had already tried. reply mtlynch 16 hours agorootparentprevI know it's not your argument but that doesn't make sense either. Processes on desktop OSes can read each other's memory anyway if they're being run by the same user. reply antisthenes 11 hours agorootparentprev> \"it prevents people from putting passwords into the clipboard which can be stolen by other programs\" But how does this logic work when a keylogger can basically do the same thing to a typed password? reply PaulHoule 18 hours agoparentprevThat and overly restrictive password rules. I often generate passwords using a PRNG or hash function that I know are pretty strong, even if it didn’t actually pick a number. Most common are the passwords that don’t allow certain characters which leaves me thinking: (1) they must have SQL injection bugs all over the app and (2) they probably aren’t hashing passwords. Either way it’s a clear confession of malpractice. A weird one in that example is that you aren’t allowed to use any trigraph that appears in your email. I find it amusing because last month I was working on an application that has a large number of autocomplete boxes that start showing options when you enter the first three characters and I must have filled out the form hundreds or not thousands of times so I wrote a little Python script that would compute the trigraph frequencies for any set of names. I found out the most common trigraph in country names is “and” for instance. reply Pxtl 18 hours agorootparent>(1) they must have SQL injection bugs all over the app and (2) they probably aren’t hashing passwords. Actually, one I've run into is web-framework-level security systems that are hard to disable. Stuff that prevents users from keying in, like XSS attacks. It's not that the password field is being used unsafely, it's that the web framework they're stuck on makes disabling security on a certain text field more complicated than it needs to be and telling the users \"screw it, don't use this character in that password\" is easier than figuring out how to get the Rube Goldberg machine to do what you actually want. Back-end languages aren't hot garbage like html+js+css so usually it's normal proper BCrypt in the back. Obviously a modern web framework won't have this problem, but a lot of sites are old and still running on messy cobbled-together piles of JQuery. reply PaulHoule 18 hours agorootparentI had that problem with ASP.NET back in the day. The creators seemed to think it was impossible to properly escape user HTML against Javascript injections and sometimes you just had to destroy bad strings completely. It was trashing API keys and passwords which is a problem when \"the customer can't log in\". I didn't have a hard time disabling this behavior at all though. My feeling is that it is impossible to \"live with it\" because I didn't know exactly what rules I had to follow to not get strings corrupted. reply malfist 17 hours agorootparentprevThere's also the chance that this is because \"security made me\". More than a few times I've written properly sanitized and parameterized applications, and security came along after the fact and told me we had to prevent input of certain characters. Didn't matter that we handled it just fine, didn't matter that it was safe to put it in. Security's argument was that some other team, some where, at some future time might somehow reuse our data and not follow the same best practices. So no special characters in your password because some engineer in the future might possibly introduce a bug. reply edwinjm 18 hours agoparentprevYou know you should memorize all your passwords? And have a unique one for every website? All 100+ of them? reply salad-tycoon 18 hours agoparentprevWhat about all these sites now only showing you login input boxes one by one, what’s up with that??? Enter username, click, now enter password is revealed. Some sites, password manager manages to do both even if only one is shown but usually not. It’s a common known fact that every person is born with a certain max number of clicks and taps. We all only have so many clicks left in our lives, that’s one less click that I’ll be able to use doing what makes me happy like doom scrolling Twitter. Dammit. reply elbac 18 hours agorootparentThe single username and password fields usually allow the site to determine whether some sort of federated login is in place for your domain. reply fkyoureadthedoc 18 hours agorootparentprevFor many it's to support SSO. so if you put in an email ending in `@company.com` and Company signs in to that site with SSO they direct you to the right place. reply legohead 18 hours agoparentprevEmail or other \"ID\" fields as well. I use 1password, and besides the password part, it's nice to just click on a field and copy/paste knowing you wont make a typo. reply malfist 17 hours agoparentprevYou should check out the government's website for buying treasury bonds. Paste and keyboard inputs are disabled, you HAVE to click on an on screen keyboard to enter your password. reply notfed 10 hours agorootparentI'm with you...THAT is the worst website! reply julienreszka 18 hours agoparentprevI hate it too, the reason might be that there are vulnerabilities where the virus hijacks the clipboard reply throwup238 19 hours agoprevI nominate anything run by Workday as the worst website in the world. Anyone looking for work can probably empathize. All the other websites mentioned are distant runners up to that monstrosity. reply drewg123 19 hours agoparentMy theory is that companies choose workday because it saves them money. If I have an expense below a certain threshold, I just eat it rather than dealing with workday's insanely complex expense report flow. I was railing against workday for a different reason last week. I had a qualifying event and needed to add a dependent to my health insurance. The first screen in the flow was to change my coverage, but it only offered \"self\" plans (not the self + dependent I was trying to change to). I finally learned (after 2 screenshot laden emails with HR) that I had to \"submit my choice and continue\" for the wrong plan before I'd be allowed to choose the correct self + dependent plan on some future screen that I had no idea even existed. The \"submit my choice and continue\" felt rather final. reply throwup238 18 hours agorootparent> If your expenses and reimbursements are difficult to file, that's OK, because the people above you don't actually care if you get reimbursed. If it takes applicants 128% longer to apply, the people who implemented Workday don't really care. Throttling applicants is perhaps not intentional, but it's good for the company. [1] That was also the thesis from an article that made it to HN’s front page a week ago [2]. [1] https://www.businessinsider.com/everyone-hates-workday-human... [2] https://news.ycombinator.com/item?id=40273637 reply bglazer 19 hours agoparentprevMy favorite part of Workday applications is the fixed list for “field of study”, which doesn’t include my field of study or an “other” option. Or maybe its the “autofill from resume” which always, always fails in different unexpected ways. Or maybe its requiring me to manually enter my name and the current date >3 times. reply derefr 15 hours agorootparent> the fixed list for “field of study”, which doesn’t include my field of study or an “other” option If it's anything like the \"employment sector\" options that banks ask you to pick from, then they're not trying to collect accurate info, but rather asking you to bucket yourself into a categorization system used by some very popular credit/risk-scoring heuristics. My guess for why an HR platform is asking such a thing: it probably populates a field that can be fetched through an API, by corporate spending platforms (Float et al) that integrate with Workday, to determine (or at least \"recommend\") the employees who should be issued spend cards. reply eitally 18 hours agorootparentprevMy favorite part is not having the ability to create a single Workday applicant profile that they can persist across all their customer companies. For that matter, Peoplesoft isn't any better. reply aleksiy123 18 hours agorootparentsingle tenant architectures strike again. reply jszymborski 17 hours agoparentprevWorkday is a seemingly universal evil. That being said, some of my emotions toward Workday might be entangled with my feelings towards HR. reply beau_g 18 hours agoparentprevAgreed, I recall a short time some arm of my company used workday learning for training courses. To do a course, you had to add it to a shopping cart for some reason, then \"check out\", which opens a popup. If you somehow managed to complete the course, the popup would just close with no indication that the course was actually done. reply geraltofrivia 19 hours agoparentprevI got aggravated, physically aggravated just by reading the cursed hellspawn’s name. I hate this website and everything it stands for. reply brnt 19 hours agoprevThey have (forgotten to turn off) some sort of ftp service: https://softwareupdate.vmware.com/cds/vmw-desktop/ reply thesuitonym 19 hours agoparentThat's not ftp, it's https. reply brnt 19 hours agorootparent\"sort of\" reply buildsjets 17 hours agorootparentProve it to us. Upload a file. reply lickmygiggle 19 hours agoparentprevI am so thankful for your comment. I was fighting with their captcha entry screen not showing me anything for the better part of an hour this morning before I gave up. reply luma 19 hours agoparentprevNot FTP, just a web server with directory listing enabled. edit: downvotes? That's literally the situation here, look at the friggen URL. reply t0astbread 15 hours agoparentprevTwo sides of the enterprise coin. reply gnatman 19 hours agoprevI actually really like the ARNGREN.net site- reminds me of the funky product classified ads that you used to see in the back of magazines like Popular Mechanics. reply kraussvonespy 18 hours agoparentWhat that arngren.net is missing is the cheesy Johnson Smith ads for X-ray specs! and Sea Monkeys! Johnson Smith was like the cheap claw machine of magazine ads. You knew that all you were going to get was crap, but it was fun crap. Maybe it helped that it took like 2 months to come and you were imagining how great it would be the whole time. reply pelagicAustral 19 hours agoparentprevI personally think this website is amazing... I mean, how do you even maintain something like this? reply brnt 19 hours agorootparentAbsolute positioning and manual html editing :) reply adverbly 17 hours agorootparentOkay hear me out: instagram, but instead of infinite scroll you just show a blank canvas. When you post you include an xy position used to absolutely position it on the wall. Everything is 100x100 pixels max. Epoch time of post date determines zIndex. reply pelagicAustral 19 hours agorootparentprevAbsolute insanity. Commendable. reply thedrbrian 18 hours agoparentprevyou'd love https://www.lingscars.com reply danielvaughn 19 hours agoprevAm I the only one who...kinda likes https://arngren.net? It makes me feel like I'm looking around at a garage sale, and it's somewhat enjoyable. reply buggeryorkshire 18 hours agoparentReminds me of Lings Cars which is actually awesome https://www.lingscars.com/ reply zorrolovsky 18 hours agoparentprevI get what you're saying. It has character, that's for sure :) But have you tried to actually perform a task? Ie \"I want to buy an animal-shaped robot\". Your eyes don't have anchor points in such a chaotic layout, it's very easy to get lost, miss items, and forget which items you already checked and which ones not. Users probably get a brain seizure after 1 minute trying to actually find a product. reply LM358 12 hours agoparentprevIt's one of my favorite websites. Complete madness. It's been like this for something like 20 years. A friend of mine told me that he apparently has a physical store as well, which has exactly the same vibe as the website. reply adverbly 17 hours agoparentprevI was on board until I realized that I could scroll off the right side of the world because of the footer background. reply duxup 18 hours agoparentprevYeah I like that too. I don't know if it is accidental or what but it might be chaotic... but it is chaotic in a way that looks like it is governed by some very specific rules that sets your expectations and makes it pretty fun. reply NoMoreNicksLeft 18 hours agoparentprevSometimes unpolished design makes something feel authentic. Which I guess it is, if it's selling things people actually want to buy. reply tetris11 18 hours agoparentprevIt's predictable, navigable, and fast. 10/10 compared to most other websites reply matthew-molloy 19 hours agoprevMy personal favourite is the old New Zealand Studylink website. You had to log in with both a password and a 'passcode'. You didn't type the passcode though, it told you to enter two or three random characters using dropdown boxes. I always had to write the passcode on paper to figure out which characters were needed (mine was long). reply masfuerte 18 hours agoparentSome sites put the right number of asterisks between the boxes so you can count off the characters. If they wanted the 2nd, 5th and 6th characters of eight you would see (where B is a dropdown box): * B * * B B * * reply huskyr 18 hours agoparentprevReminds me of this classic 'feature' from Lotus Notes (scroll all the way down on the page): https://web.archive.org/web/20120123085307/http://homepage.m... reply veeocho 18 hours agorootparentDirect link to the relevant paragraph: https://web.archive.org/web/20120123085307/http://homepage.m... reply bArray 19 hours agoprevIs this website still actively being used? One of the items is for something that appears only 4 years old: https://www.youtube.com/watch?app=desktop&v=0ci2860tpRU Some interesting comments in the source: I dread to think what garbage that is. Is it Yahoo SiteBuilder? https://www.youtube.com/watch?v=84zfRBcFb9I reply sigspec 18 hours agoparentTerrifying reply callalex 18 hours agoprevCan anybody help me understand why browsers even allow disabling paste? It’s such a universally hated and ableist function. Why can’t the browsers just force a fix by…not supporting this “feature”? reply graeber_28927 16 hours agoparentProblem is, the idiot customer copies and pastes the wrong thing, and then goes on bothering customer support, who go and bother me, the dev, to fix the account. Obviously there are solutions to fix all of this, but that's not how management dreamed up their website, so I'm stuck between supporting idiots and disabling paste. reply callalex 12 hours agorootparentWhat about all the disabled people that you are silently rejecting? They don’t typically have it in them to make as much noise as your one idiot customer but please be aware you are actively harming them. reply amatecha 18 hours agoparentprevright, if I can type into it, I expect to be able to paste into it, like every other text field in every OS GUI of the past, what, 40 years? reply VyseofArcadia 19 hours agoprevI'd like to just respond to the caption on the first image. It's me. I'd wear that shirt with a cat samurai on it. reply probably_wrong 17 hours agoparentBecause I looked into it in the past, I would like to point out two things. First, the shirt is very easy to find. If you want it, you can easily find the store online with the information from the post alone. Second, Instagram is chock full of shady sellers like this one selling t-shirts with AI-generated pictures. You can order from them and the product will probably arrive (eventually), but their websites are copy-pasted versions of each other (I just found at least six stores with identical \"About Us\" text) with different t-shirt designs whose reviews are uniformly poor. So don't count on excellent customer support. Then again, maybe you are the type of person who always wanted to maybe receive a badly-printed, misaligned polyester shirt of a cat carrying a deformed sword. If that's the case then today is your lucky day. reply dj_mc_merlin 19 hours agoparentprevI would also wear it. Why does he get more relevant advertisments than me? I only get advertisements for clothes I'd never wear. I just checked out their website and I'm seriously considering buying some cat samurai shirts. reply b6z 17 hours agoparentprevDammit. This was also my first thought. Might consider it, if customs doesn't make it too expensive. reply huskyr 19 hours agoprevThat looks awful. Unfortunately you could probably write a very similair article about many other corporate or governmental websites. E.g, applying for an ESTA felt like registering for some kind of scam. Or the systems to let friends and family park for a reduced fee in many municipalities also seem to be designed by people who hate humanity in general. reply darkwater 19 hours agoprevYou can apply this do many Governments, banks and insurance websites. Last example I witnessed: my home insurance forced me to re-register in their website due to some (clearly half-assed) migration. The way to force that was giving you a login form with user/password but no clickable \"Submit/Login\" button! And then a mini (like 50px tall) banner at the top of the page telling you that you had to recreate the account. reply robofanatic 19 hours agoprevI am experiencing similar frustration while trying to publish my App on Google Play! Publishing my app on Apple Store was smooth but Google Play is nightmare. reply amir734jj 19 hours agoparentI totally agree. Their UI is hot garbage. reply robofanatic 19 hours agorootparentAfter going through all the pain now I am stuck at the last step where I need to find 20 unique testers before they will allow me to go to production! reply withinboredom 19 hours agorootparentIs that a thing? Can you go stand in a mall and spend an afternoon getting people to test it? reply devsda 17 hours agorootparentThey are referring to the requirements discussed here: https://news.ycombinator.com/item?id=38258101 > Can you go stand in a mall and spend an afternoon getting people to test it? Does that really work ? If a stranger at mall asks us to install a random app out of regular play store flow, only a small number of people will oblige. That number should ideally be zero. reply AlienRobot 19 hours agorootparentprevJust share it on reddit. reply for_i_in_range 19 hours agoprevI can just imagine the meeting of the people who created the 11 page how to use this website pdf. Awful. reply tylerrobinson 19 hours agoparentI actually feel for these people. They know the site is awful and have no way to improve it except to make a manual. reply bbarnett 19 hours agoparentprevThe real problem is letting the marketers and the \"we're proud of ourselves!\" sort take full control. I imagine the goal is \"we have all these things under one roof!\". Good grief. You can still have the same framework/layout. EG, support, products, etc. But you can do it under \"categories\". For example, \"VMware by Broadcom\" or some such blather. And all support, all webpages, are only vmware related in that category. But really, transitioning vmware's webpages to this is just dumb. What a waste of time. Just use vmware's website with a \"by broadcom\" in the banner, and who the hell cares. So juvenile. That little bit of brand recoginition, oh it's so important. Yeah, it's so important that it's not LSI, but broadcom in the firmware when my server boots now? Firmwares all need to have name changes? reply bityard 18 hours agoprevThis is amusing, but in a \"stand-up comedian jokes about bad drivers\" kind of way. Good for a half-second chuckle, but flattens out pretty quick since extremely bad websites and software are something I deal with literally every day. And most of them are not even corporate behemoth types like VMWare. And while I'm here... Thankfully, I am in a new job where I don't have to support vSphere anymore, but I just want to give a big \"fuck you\" to Broadcom for literally wiping the quite-decent community forums and knowledge base off the map. Sure, the KBs still exist, but on a different domain, and they deleted _all_ the metadata and the old KB links scattered across decades and the web all 404 now. If Broadcom's goal was to reduce support costs, eliminating the forums and neutering the KB was a pretty bad way to go about it. reply nick238 18 hours agoprevI think most of the \"fuck yous\" where you just simply get dropped at the main Broadcom page is because most enterprise sites just redirect what would be a 404 to the main landing page. I hate that pattern because it's super confusing. Did I click the wrong link? Just tell me you can't find that page. Many enterprise websites undergo so many retools that search engines trying to drop you off at a specific page would just 404 everything (even the main page if it's something like `example.com/main/en/index.php`), so the 404 redirect is \"required\". Then one company buys another, then all example.net/useful/docs links are translated to example.com/useful/docs links, which 404, which redirect to example.com's front page. reply tzs 18 hours agoprevThe worst website of all time was that of Yvettes's Bridal and Formal, a bridal shop in Panama City, Florida. Here's a copy of it [1]. Here's a video that explores it and talks about the person who probably designed it [2]. You won't get the full Yvette's experience on a modern browser and computer because even if your browser does automatically play the MIDI file that the site tries to send it will probably sound good because you've probable got a decent sound system with good MIDI instruments. [1] https://yvettesbridalformal.p1r8.net/ [2] https://www.youtube.com/watch?v=Rofmr7_xc7A reply eitally 18 hours agoparentThis sort of design was a style back in the late 90s when everyone was just getting the hang of HTML and using nested tables before CSS was invented & started becoming popular, and anything besides text/hyperlinks and images was pretty risky to include since browsers hadn't yet evolved to support a standard set of features. reply mft_ 18 hours agoparentprevThat’s so bad, it’s good! Reminds me of https://www.lingscars.com/ only even more so… reply jszymborski 17 hours agoparentprevYvette's Bridal and Formal is unironically art. This broadcom website is a banal evil. reply icholy 19 hours agoprevThe password requirements on some websites seem like they're designed to deter me from creating an account. reply jak2k 19 hours agoprevFor Linux, there is Gnome Boxes, which is a quite good VM for all the stuff I need one for. It may not be as complete as VMware, but has most of the important stuff. reply interdrift 19 hours agoprevWhat you get when money meets corporate meets engineers who don't say no. reply jabroni_salad 18 hours agoprevThe switch isn't nearly as easy for vmware, but nothing drove EDR sales like the new bcom website after they acquired Symantec. SEP was great because it was low impact and ticked a compliance checkbox. Useless if any event was going on but in the technical planning calls these clients just werent interested and would passively renew SEP every year like clockwork. Then broadcom switched up the website and every single one of them brought up the 'so we are wanting EDR after all' pitch request on their own. None of them could figure out how to renew their license. edit: Have you guys seen IBM's fix pack site? it technically works, but jeeze. Why do I have to go through a web store ordering flow to patch db2? reply scaglio 18 hours agoparentYES! IBM's Fix Central, or how it's called, it's literally a maze. And I hate Oracle's and Red Hat's paywalls, even if I can understand their presence. reply surfingdino 18 hours agoprevI have a strong contender. British Gas has removed their bank details from printed statements and their website, because they want to force people to create online accounts and set up Direct Debit. reply ksvarma 12 hours agoprev100%, I had this experience before and the moment when it redirected to broadcom, I just stopped. I know how disastrous their website is (from a CA acquisition), it was like this 5 years ago. Can't believe they buy companies and bury them inside this broadcom. reply frizlab 13 hours agoprevInstead, download UTM, and install Windows. The whole operation takes around half an hour if you never done it and have to read the doc from UTM’s website! I’m amazed by UTM (I know it’s “just” QEMU behind the scene, but they put a very good front on it). reply troupo 19 hours agoprevThis is like that email where Bill Gates was lambasting his subordinates for the insanity that is Microsoft site: https://www.techemails.com/p/bill-gates-tries-to-install-mov... reply foobarbecue 18 hours agoprevHe didn't even mention my favorite part: \"No learnings found\" . No learnings here indeed! reply cassianoleal 17 hours agoprevA post talking about the worst website that doesn't mention the wonderful Ling Cars [0] cannot be taken seriously. Or should that be in a post about the best website? [0] https://www.lingscars.com/ reply jbk 16 hours agoprevDid someone tell you about SAP Ariba? The website for invoices where you cannot click on your list of invoices, but can get an other click menu to get “send me a link by email”… reply ddtaylor 19 hours agoprev> When you get your own browser you can do whatever you want but while you are living in my house under my rules I get to copy/paste whenever I goddamn feel like it. Don't give Google any ideas, err, wait... reply indigovole 14 hours agoprevnext [–]It's going to be SAP, right?At least he didn't have to post a vacation request in SAP. reply indigodaddy 19 hours agoprevWeird, I thought the arngren.net website screenshot looked beautiful. reply rb666 19 hours agoprevJust use Proxmox, it's fantastic for many vMware use-cases. reply mthoms 18 hours agoparentAgreed, but it doesn't run on Apple Silicon. reply SpaceNoodled 13 hours agoprevLiterally every SoC vendor website. reply nipperkinfeet 9 hours agoprevTheverge is the worst website in the entire world. reply xutopia 18 hours agoprevMy insurance company wants to dethrone this awful web site with their own. At least they're working towards it. reply s1291 17 hours agoprevI am pretty sure you will change your mind after visiting my university website. reply gloosx 16 hours agoprevI know a website even worse, globalsign which is selling code signing certificates. They are so deep into making shitloads of money out of thin air they stopped caring in 2002. This is the only website with a password field which ONLY allows alphanumeric characters, so you have to remove all them exclamation marks, dollar signs and underscores from your generated password. They also have a freakin chat bot assistant which just throws links to documentation in response, and they use \"Live Chat\" for the button just like you are really going to talk to a human. If you google something globalsign certificate related, the whole first page is filled with links to their documentation. Guess what happens when you click one of them? It's a 404 page. The insides of a portal is just a horror website from the far far past, it takes maybe 30% of the wide screen in the top left corner, everything you click loads for good 30 seconds. Ah yes, if you go to your orders for example, you just get an empty table. Only when you click on \"search\" button the table fills. Also they will put a block on your card funds for purchase the very first moment they can do it, and it's not the last step of the form. If you could not proceed due to some nonsense error which tells your american express card zip code check failed (i used visa lol), your money will return in 10 days maybe. In the end, you have to print and send some HAND FILLED forms to them in order to get this bullshit \"vetting\" process done, you can finally launch your fucking egde browser in an internet explorer compatibility mode to collect your hard earnrd certificate. At least a bit cheaper than other providers. 0/10 would not recommend. Unless you really really need to eliminate this SmartScreen circus warning dont do it. Sabotage this stuff. Just let your users check the installer hashsums and they safe reply asimpletune 19 hours agoprevThe Vodafone website in Italy is actually worse, believe it or not. reply neilv 18 hours agoprevNot even close to the worst. The worst I see is a major brick&mortar retail chain that has been trying to do online for years. Part of their execution problems might be misleading metrics. Their \"how was our service?\" followup emails aren't sent for the routine (around 50%) fulfillment fudge-ups that backend should've prevented. Nor for occasional checkout breakage that fails with signs of multiple things that are simply being done incorrectly. So I have the nagging thought that someone might be hitting their KPIs/OKRs, and the right people aren't aware what a dumpster fire they're operating. I wonder whether Amazon could've already eaten the online component of that category, with their overall superior competence and (selective) customer focus, if they didn't have the counterfeits indifference/misalignment problem, and worsening reputation for quality and caring about the customer. reply dghughes 18 hours agoprevIt's like if eBay and Altavista had a child in 1995. reply edwinjm 18 hours agoprevDon't forget the marktcap of Broadcom is $646 billion! reply blantonl 18 hours agoparentENTERPRISE'Y reply nkg 19 hours agoprevI recently tried to register for an Apple developer account, and it has been the most infuriating process I've been through on the internet... and I am used to the French govt websites! At some point, the H1 title was in white on a light grey background, and I considered sending a screenshot to Jonathan Ive. To this day, I did not succeed registrating. reply Euphorbium 17 hours agoprevSeems like he never used government websites. reply MrPrvRyan 17 hours agoprevThe bigger they are, the stupider they get. reply HPsquared 19 hours agoprevCompare and contrast to https://maddox.xmission.com/ \"The Best Page in the Universe\" reply FlipFloopDev 5 hours agoparenthttp://thebestpageintheuniverse.net/c.cgi?u=math This one is funny! reply jmclnx 19 hours agoparentprevI have been moving my page to gemini, but maddox is a great page :) reply throwup238 19 hours agoparentprevAnother victim of enshittification. His essays used to be quirky and fun but he went off the rails once he started doing Youtube videos. reply pelagicAustral 19 hours agorootparentHaha, I remember I sent him a hate email about this and he replied with something like \"No, I haven't changed, YOU changed...\" hahaha reply amiga386 18 hours agorootparentprevThere's a bit more to it than that. Maddox _really_ went off the rails when his friends realised how thin-skinned he was on certain topics (i.e. his girlfriend leaving him for one of his closest friends) and they could get a much larger audience by making fun of him than working with him, and he totally played into their arms with his LOLsuit. https://www.vice.com/en/article/a3bwjj/the-cuck-centric-flam... > Both [Maddox and his friend Kokkinos] performed at Upright Citizens Brigade in LA, sometimes together, with Kokkinos occasionally guesting on The Biggest Problem in the Universe*, a show Maddox co-hosted with his then friend Dick Masterson. After Masterson began dating one of Maddox's exes, creating an interpersonal rift that resulted in the duo cancelling their podcast in 2016, Masterson launched his own podcast, The Dick Show, on which Kokkinos was soon a frequent guest. As The Dick Show grew in popularity, Masterson and Maddox’s public rift widened, with each party’s respective fanbases joining in on the antagonism. reply xz18r 18 hours agorootparentThis is just the tip of the iceberg, check Maddox recent 3 hour video (!) on how he was allegedly stalked for years on end by Masterson and his crew. It's a wild ride. reply jmclnx 19 hours agorootparentprevI just went to it, been a while. It is much different than I thought it was :( reply superkuh 19 hours agoprevThe worst websites in the world are the ones that are just blank pages without any content at all. Most corporate websites are like that these days unless the stars align and all the javascript executes just right. reply kokizzu5 12 hours agoprevApparently not just me that think the same, lol.. yeah lured by free vmware workstation, still cannot download after a day, \"Account verification is Pending. Please try after some time.\" reply fred_is_fred 16 hours agoprevIs that website a profit center? If not Broadcom absolutely could care less about your experience. reply BizarroLand 18 hours agoprev> When you get your own browser you can do whatever you want but while you are living in my house under my rules I get to copy/paste whenever I goddamn feel like it. Any company that blocks copy paste on their website is stupid and I hate them. reply dschuetz 19 hours agoprevBrilliant reply nokun7 18 hours agoprevRight now Broadcom.com is coming close to the worst website in the world. reply binarymax 17 hours agoprevThat Broadcom support site looks like it must be Servicenow. If you know, you know. reply ToucanLoucan 19 hours agoprevI've got a friend who's been pushing his employer to get off VMware since Broadcom bought them. Absolutely astonishing how fast the enshittification is kicking in. reply _lateralus_ 19 hours agoprevbro needs a fuck broadcom sticker https://sysadminafterdark.com/product/fuck-broadcom-sticker/ reply hulitu 13 hours agoprev [–] With white on light gray. /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author recounts a frustrating encounter with Broadcom's website when attempting to download VMware software, emphasizing the site's poor design and user-unfriendly features.",
      "They express disbelief at the lack of user-friendliness and propose that Broadcom should consider revamping the platform for a more user-friendly experience."
    ],
    "commentSummary": [
      "The discussion addresses challenges in web design and user experience, emphasizing issues like complex navigation, limitations on basic functions, and user frustrations with poorly designed enterprise software.",
      "Users are dissatisfied with websites such as IBM's Fix Central, SAP Ariba, and Workday, citing problems with password requirements and online procedures.",
      "Topics include web security, technology misuse, and worries regarding user data exploitation by companies, shedding light on the intricacies and challenges of interacting with different websites and enterprise software."
    ],
    "points": 362,
    "commentCount": 181,
    "retryCount": 0,
    "time": 1715781139
  },
  {
    "id": 40366204,
    "title": "N64Recomp Project Enhances Nintendo 64 Games on PC",
    "originLink": "https://www.tomshardware.com/video-games/nearly-all-nintendo-64-games-can-now-be-recompiled-into-native-pc-ports-to-add-proper-ray-tracing-ultrawide-high-fps-and-more",
    "originBody": "Video Games Nearly all Nintendo 64 games can now be recompiled into native PC ports to add proper ray tracing, ultrawide, high FPS, and more News By Christopher Harper published 12 May 2024 Majora's Mask is the latest to benefit, but N64 Recompiled works with nearly all N64 games. Demonstration of Zelda: Majora's Mask running in Ultrawide through current Emulation techniques vs through native PC Recompilation. (Image credit: Nerrel on YouTube) Despite its 1996 release, the Nintendo 64's original hardware and games have both remained relatively hot-button in enthusiast circles here into 2024. Now, the next frontier of high-end N64 gameplay may be through recompiled PC ports instead of emulation, courtesy of Mr-Wiseguy on GitHub. Wiseguy is responsible for the release of both N64Recomp and Zelda64Recomp, a project that ports The Legend of Zelda: Majora's Mask to PC with N64Recomp's graphical and QoL improvements, as screenshotted above and highlighted by YouTuber Nerrel below. So, what makes crazy graphical improvements like real ray-tracing, uncapped FPS, and proper ultrawide support possible for N64 games? If you've been in the Nintendo 64 enthusiast scene for a long time, you may recall the waves made when a completely decompiled Super Mario 64 PC Port dropped in 2020 and allowed for features like real ray-tracing, full model replacements, and so on. It still gets mods to this day. Recompiled ports aren't quite the same as decompiled ports like the SM64 PC port in this context, but both will run natively on PC and thus be able to truly maximize performance and effect accuracy to the original hardware while still providing the PC-expected enhancements that come with emulation. N64Recomp is basically the best of both worlds, and since manually decompiling N64 games takes years of labor from one or more people, a tool to more efficiently recompile them into a quickly playable-on-PC state is a godsend for preservationists everywhere. A tool like this also ensures that old classics that aren't currently receiving the attention of big mainstream hits remain playable well into the future in an ideal state. A Twitter post by Dario, who makes the RT64 plugin leveraged by N64Recomp and some N64 emulators, highlights this. My friend Wiseguy's been working in secret for a year on a tool to make PC ports of N64 games without complete decompilations. The result doesn't include assets and only requires a ROM to play.He's managed to run games like Banjo-Kazooie, Rocket Robot and even Superman 64. pic.twitter.com/sKGuViEsJZMay 10, 2024 Even as we speak, advancements like this aren't the only huge boons we're seeing for fans of the Nintendo 64's library or even original hardware. The open-source SummerCart64 recently dropped and is basically the definitive flash cart for the old console since it also implements full 64DD support. Several real hardware-compatible homebrew N64 games and ROM hacks also keep releasing, including highlights like the 30-fighter Smash Remix and Mario 64 engine rewrite, Peach's Fury. Stay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Christopher Harper Freelance News Writer MORE ABOUT VIDEO GAMES Rockstar's original Red Dead Redemption and its expansion spotted in launcher files — Windows gamers may finally get a remastered release Windows translator makes PC games run on Android — Fallout 4 demoed at 30 fps using Winlator app LATEST Intel's newest E-core-only \"Twin Lake\" CPUs are on the way, starting with Intel N250 SEE MORE LATEST ► TOPICS NINTENDO EMULATION SEE ALL COMMENTS (21) 21 Comments Comment from the forums ezst036 I support this. Just to stick it to Nintendo. Right in their eyeball. Reply LabRat 891 NGL, 2160p 120+hz Ray Traced GoldenEye sounds interesting. ...and yes. Nintendo will hate this. Which, means I love it. Reply Giroro So this method of emulation seems to do upfront what I assumed every other emulator was already doing in real time. Maybe that's because I exclusively code in Assembly and C? So now I'm just scratching my head wondering what the other emulators are actually doing, and why. I was also confused when I found out people spending 2 years to manually decompile an N64 game. It's just find and replace to decompile machine code into Assembly. You could write a script to do that in half a day... So what are these people actually doing? Reply TheyCallMeContra Giroro said: So this method of emulation seems to do upfront what I assumed every other emulator was already doing in real time. Maybe that's because I exclusively code in Assembly and C? So now I'm just scratching my head wondering what the other emulators are actually doing, and why. I was also confused when I found out people spending 2 years to manually decompile an N64 game. It's just find and replace to decompile machine code into Assembly. You could write a script to do that in half a day... So what are these people actually doing? Emulators have to emulate the hardware before the software even becomes a considering factor. Emulators haven't been out here recompiling game code in real time, no. Reply SirStephenH Nintendo lawsuit in 3, 2, 1... 😞 Reply KitsuneKas TheyCallMeContra said: Emulators have to emulate the hardware before the software even becomes a considering factor. Emulators haven't been out here recompiling game code in real time, no. Actually JIT recompiling is exactly what a lot of emulators do, because it's much, much faster than hardware emulation. The difference between what this project is doing and what emulators do is that all the recompiling is done up front before runtime so there's no overhead when running, meaning the hardware demands are significantly lower. Reply nimbulan Giroro said: So this method of emulation seems to do upfront what I assumed every other emulator was already doing in real time. Maybe that's because I exclusively code in Assembly and C? So now I'm just scratching my head wondering what the other emulators are actually doing, and why. I was also confused when I found out people spending 2 years to manually decompile an N64 game. It's just find and replace to decompile machine code into Assembly. You could write a script to do that in half a day... So what are these people actually doing? The whole point of decompiling is to be able to port the game over to different hardware/software. In order to do that, they need to make the code readable so that it can be modified, which requires exhaustive painstaking editing. That's why it takes so long. This new tool is basically automating that process with a universal frontend that handles real-time graphics API translation, vs the manual ports which generally mod modern graphics APIs into the game. Reply oofdragon It sucks to live in capitalism. Imagine a world without proprietary consoles and etc, you just have a universal console hardware called PC and every game designed to run on it is open source from the start. Reply Thunder64 oofdragon said: It sucks to live in capitalism. Imagine a world without proprietary consoles and etc, you just have a universal console hardware called PC and every game designed to run on it is open source from the start. Without capitalism you wouldn't be able to say how awful it is on the internet. Reply TerryLaze LabRat 891 said: ...and yes. Nintendo will hate this. Which, means I love it. Nintendo will love this to easily add n64 games to the switch, heck this opens up the way to a n64mini. But they will also hunt this down, probably, if there is a legal opening for them to do so. oofdragon said: It sucks to live in capitalism. Imagine a world without proprietary consoles and etc, you just have a universal console hardware called PC and every game designed to run on it is open source from the start. Yeah have a look at itch.io , that would be the average level of quality you would get from that sort of games. Reply VIEW ALL 21 COMMENTS Show more comments MOST POPULAR Intel closing in on $11 billion deal for Ireland factory funding — Apollo set to pay out 5x Intel's funding goal Samsung and SK hynix abandon DDR3 production to focus on unrelenting demand for HBM3 Rockstar's original Red Dead Redemption and its expansion spotted in launcher files — Windows gamers may finally get a remastered release Qualcomm goes where Apple won't, readies official Linux support for Snapdragon X Elite Windows translator makes PC games run on Android — Fallout 4 demoed at 30 fps using Winlator app Nvidia's next-gen Blackwell AI Superchips could cost up to $70,000 — fully-equipped server racks reportedly range up to $3,000,000 or more AMD launches Ryzen 7 8700F and Ryzen 5 8400F — budget Zen 4 CPUs without the RDNA 3 integrated graphics Homegrown European processor for supercomputers delayed by a year — chip upgraded to 80 cores, but timeline gets downgraded OLED monitor momentum expected to continue — analysts expect 1.34 million units shipped by year end Toshiba demonstrates 30TB+ HDDs using HAMR and MAMR technologies — customer sampling scheduled for 2025 Intel Arc GPU hacked to work on a 64-core Arm Ampere system running Linux",
    "commentLink": "https://news.ycombinator.com/item?id=40366204",
    "commentBody": "Nearly all Nintendo 64 games can now be recompiled into native PC ports (tomshardware.com)351 points by Malic 21 hours agohidepastfavorite159 comments skrrtww 19 hours agoTo operate this tool, you still need to disassemble the ROM up front and annotate it heavily before it can be recompiled. This tool is very nice, to be sure, but the hyperbole about anything close to one-click generation of standalone executables for an arbitrary ROM is getting out of hand. reply slongfield 17 hours agoparentTo be fair, N64 is not the PS2 or Gameboy--there are only 388 games that were released for it. Going through all of the games that were released and shepherding them through this process is feasible. reply kilburn 13 hours agorootparentIn fact, Thabeast is beating every N64 game, and he means ALL of them. [1] He has already completed all games, but the videos are waaay behind (he's releasing one video/game a week). [1] https://www.youtube.com/@Thabeast721/videos reply coolmitch 12 hours agorootparentif you're interested in this kind of thing, The Mexican Runner beat every NES game (over 700 of them [1]) including such gems as Miracle Piano Teaching System, which took 91 hours to beat and required him to become quite proficient at actual piano playing [2] [1] https://docs.google.com/spreadsheets/d/1KDNGI76HoMNyYLL6RqWu... [2] https://youtu.be/PB_LMW72crY?t=3997 reply doublepg23 9 hours agorootparentWow that was incredible, I love internet stories like that. reply kwanbix 14 hours agorootparentprevAnd there are probably fewere games that are worth doing. Maybe 100? 200? reply BaculumMeumEst 10 hours agorootparentEvery game on the system will have some people who are super nostalgic for it. When many of us were kids, we only had a few games, and we played the hell out of them. reply tristor 14 hours agorootparentprevI really liked the N64, but if we're being charitable it's probably like 30-40 games. For any given console, even those that achieve legendary status, the game quality follows a Pareto distribution, 80% is crap, 20% is worth playing after the console has left the public mind. reply TheGRS 12 hours agorootparentWhat no one wants to revisit Nagano Winter Olympics or Tetrisphere? Its probably closer to 50 if you want to include some of the lesser-known but interesting titles IMO. One of the most interesting games was Body Harvest since its clearly a prototype for a lot of what became GTA3 reply tacker2000 11 hours agorootparentHaha Nagano! I played countless hours of ski jumping with my friends back then reply acchow 8 hours agorootparentprev20% seems far too generous reply sixothree 8 hours agorootparentprevAnd for consoles like PS2 or OG Xbox or newer, so so many games have newer versions on other platforms reply doublepg23 17 hours agoparentprevI don't think that's true. The video in the post shows a no-name n64 game being recompiled and running fine. No annotation needed. reply amlib 16 hours agorootparentThe video also makes it clear that only games using a certain microcode works for now. I imagine there are many parts of HLE emulation that gets added to the recompiled software as wrappers and compatibility code to make it all work, so in a sense this may be more akin to an embedded emulator into a native binary (recompiled code), which can overtime be worked on to add new features, fix shortcomings of the original software and make it more \"ergonomic\"/compatible with it's new target platform. But such work can only feasibly be achieved by reverse engineering the software and giving it human readable symbols. In a sense there is almost as much work to do as on the previous decompilation projects, but you can get results (run the game) right away (or at least much faster than traditionally), assuming of course the game uses a compatible microcode and isn't doing anything fancy that depends on the intrinsics of the n64 architecture. reply DaiPlusPlus 16 hours agorootparentThere’s also inevitably going to be timing/performance-related bugs - the N64’s devkit docs say it runs games under a thin OS for multi-threading[1] so that’s another opportunity for things to go wrong. Also, after conversion these games are still designed and hardcoded for an N64 controller and 4:3 aspect-ratio - that’s hardly “native” as far as PC gamers are concerned. [1] https://ultra64.ca/files/documentation/online-manuals/man/pr... reply lordgilman 11 hours agorootparentThat operating system is statically linked into the game binary. reply christkv 16 hours agorootparentprevI’m guessing it works for the standard Nintendo provided microcodes. reply speps 18 hours agoparentprevExcept this work can be done once, stored somewhere and shared. Take a ROM (it means read-only after all) and get native port out... reply Hamuko 15 hours agorootparentShared legally? reply TehCorwiz 15 hours agorootparentYou can share the prepared binary because it contains no trace of the rom. The rom is still required to play the game. reply GuB-42 14 hours agorootparentI wouldn't count on it, it is still derivative work and I am sure the licences for the games forbid this. Some companies may let it slide, but Nintendo is known to be aggressive about their intellectual property. And you will probably have a hard time arguing that it doesn't promote piracy. You can't get \"ROMs\" officially. You have to extract them from the cartridge with specialized equipment, or maybe in some other way, but it is not as simple as reading from a CD. It means that most people will simply get them from illegal sources. With an emulator, you can say it is for \"homebrews\", but if you recompile a commercial game, you can't use this argument. reply MaxBarraclough 12 hours agorootparent> it is still derivative work I don't think that's right, I think it's essentially considered the same work under copyright law. Apparently compilation doesn't fulfil the criteria of derivative works. https://opensource.stackexchange.com/a/6435/ reply derefr 13 hours agorootparentprev> You can't get \"ROMs\" officially. You have to extract them from the cartridge with specialized equipment, or maybe in some other way, but it is not as simple as reading from a CD. Nintendo has, in fact, sold the public ROMs before (and I don't just mean custom emulator executables wrapping ROM payloads that get delivered encrypted onto DRMed consoles.) Specifically, I'm talking about the NES Classic and SNES Classic. These little boxes use multi-image emulators, rather than Nintendo's usual approach of a customized single-image emulator for each game. And the ROMs used by these systems are just sitting there as files on disk. The disk isn't encrypted, either; nor is the bootloader or kernel integrity-signed; or really anything like that. You don't need to \"jailbreak\" these things — they act like Android phones, where you can just reboot them into restore mode and plug them into a computer with a USB cable, and see a virtual disk. The \"modding tools\" for them just drop a new kernel with wi-fi support into their /boot partition! (And you don't even need to go that far to read the ROMs off the rootfs — any Linux PC will work to mount it.) So any DMCA \"they used DRM, which means their intent was to license you X, not sell you X\" arguments don't apply. They did nothing to stop people from extracting these ROMs. By buying these systems, you're effectively buying \"a box full of ROMs\" directly from Nintendo (the IP rights-holder for said ROMs), at retail, as a \"money for goods\" transaction. So now, by the first-sale doctrine, they're your ROMs, and you can do with them as you please. This is true as surely as buying a DVD box-set means you now own those DVDs, and can do with those as you please. (Part up and resell the DVDs individually? Sure, why not!) Or — for perhaps a more interesting example — as surely as buying a stock images or sound-effects library on CD (even second-hand!) implicitly brings with it the IP rights to use those assets in derivative works you create. The primary-market purchaser didn't have to agree to any kind of license terms at point of sale? First-sale doctrine applies to that IP from then on! (Amusingly, Nintendo redistributed exactly such stock-image CD assets embedded into Mario 64 — so their lawyers are double-bound to agree that this particular interpretation of first-sale doctrine should pertain.) reply anthk 10 hours agorootparentThe license of the ROMs it's to play them, not to exploit them. reply thejazzman 5 hours agorootparentI'm pretty sure all roads lead to either playing the game or otherwise not doing anything of value to Nintendo whatsoever (ie who cares if you bury a box of roms in your yard) reply immibis 13 hours agorootparentprevNintendo hasn't been taking down any projects that can't work without a ROM input. They'll change that eventually, but for now they are not. You can't share a modified ROM but you can share binary patches. reply ars 12 hours agorootparentprevA shrinkwrap license is not a real license. The game is copyrighted, and fair use applies. And if you want to avoid the license just steal it from someone. You never agreed to a license, but you are still required to follow copyright. (I've obviously being a little sarcastic, but license agreements on software are meaningless, even if a company claims they have meaning.) reply MaxBarraclough 12 hours agorootparentprev> You can share the prepared binary because it contains no trace of the rom. This is wrong. Please do not confidently present your guesswork as fact. If it were true, copyleft would have no chance of working; binaries compiled from copyleft-licensed source-code would not be subject to the copyleft licence. reply ars 12 hours agorootparentprevIt's legal if the recipient owns the game, it would be covered under fair use for compatibility purposes. reply ldjb 10 hours agorootparentNote that this is not true in many jurisdictions. reply Hamuko 4 hours agorootparentIs it in any? Because I don't think just adding a \"You need to own the game to download this binary\" text to your GitHub repo is going to do a lot legally. reply blindstitch 15 hours agorootparentprev100% legal if you delete within 24 hours. reply cauefcr 14 hours agorootparentTrue in Brazil, is it true anywhere else? reply gurchik 15 hours agorootparentprevSource? reply Retr0id 14 hours agorootparentThat guy's uncle works for Nintendo reply advael 1 hour agoparentprevI think \"but using the tool takes time and effort\" is a good but not perfect criticism of a tool that purports to reduce time and effort spent. As many asinine CEOs are making outsized claims about generic \"productivity\" improvements from various tools in this space and similar ones all the time, I get this instinct. However, it does not make sense when the use of the tool is anything other than \"something you could already do, just faster\" reply darby_eight 17 hours agoparentprev> you still need to disassemble the ROM up front and annotate it heavily before it can be recompiled. Why? Surely it's more straightforward to do binary-to-binary translation. No human input needed! reply immibis 13 hours agorootparentFor example, Zelda (both of them) load segments of code from the cartridge at runtime. The recompiler needs to know which areas contain code, and the original relocation-applying code for MIPS instructions obviously won't work on x64 instructions either, so it needs to know how the relocations work. reply ambyra 13 hours agoparentprevWhat is the process, and can any of it be chatgpted? reply 38 5 hours agorootparentGet out. reply keb_ 18 hours agoparentprevVery true. There is nothing special about this tool at all. Nothing to see here folks. reply jonhohle 19 hours agoprevThis is really cool. Many of the foundational tools created for N64 decomp are finding their way into other system decomp packages. I recently came across https://decomp.me/ which allows collaborative decompilarion for a bunch of systems with presets for their relevant SDKs and hardware. I’d imagine this happening several more systems in the coming years. reply kkukshtel 18 hours agoprevI saw this when it came out, and as someone that doesn't follow the ROM hacking scene, I'm wondering - why did this approach take so long to come up with? Translating the assembly instructions to C and then recompiling them seems like an obvious method to try early on, but I'm wondering if there was some other breakthrough that made this possible in a way it wasn't before? reply slongfield 17 hours agoparentThe N64 has a GPU, which is not the same as modern GPUs, and you need that GPU to render graphics. Skimming the Github repo, looks like this uses rt64 under the hood for the raytracing support ( https://github.com/rt64/rt64 ), and that uses the ubershader technique to emulate the GPU. This excellent article talks about the what, why, and insanity that are ubershaders: https://dolphin-emu.org/blog/2017/07/30/ubershaders/ reply kkukshtel 15 hours agorootparentWas raytracing support necessary to get the compilation working to circumvent \"normal\" GPU calls? reply DCKing 16 hours agoparentprevOther emulators want to solve the problem generically, and this solution doesn't quite. Static recompilation from one machine language to one other language is somewhere between extremely difficult to not being generally possible in practice [1]. To make recompiling something like this properly, you need some help from the binaries that make recompilation easier [2] and on top of that you need to patch certain things to make this work [3]. Dynamic recompilation doesn't have this problem. It allows you to create software you can dump the original binaries+assets (\"ROMs\") in and it will generally emulate it. There's a lot of confusion about how generic this solution it. It's extremely impressive in how much work it saves making recompilations/\"ports\" much easier, and it will be very valuable. But it is not able to replace the need for traditional emulators. [1]: https://cs.stackexchange.com/questions/155511/why-is-static-... [2]: N64 game binaries may happen to avoid a bunch of things that make general static recompilation hard that help this approach, but I don't actually know. [3]: The Majora's Mask recompilation repository contains a lot of handcrafted patches to make it work: https://github.com/Mr-Wiseguy/Zelda64Recomp reply pjc50 18 hours agoparentprevEmulating the hardware usually requires cycle-accurate emulation of things running in parallel, so it's not quite so simple as just the program in the ROM. reply whizzter 18 hours agorootparentYes and no, with the 8bits and probably 16bits like the C64, NES, etc you really want cycle accuracy and emulating something like the PS2 really well would probably need a fair bit of accuracy. HOWEVER.. writing an recompiler (not feasible for 8bits due to the amount of self-mod code) you could probably insert pattern checks to detect writes to DMA-starting locations,etc (thinking back to our PS2 games) and transform it to safe sequences without accurate timing for most games that really only would be happy with some extra cycles. reply OnlyMortal 18 hours agorootparentGood point about 8bit self modifying code. The UK C64 disk for “Skate or Die” had copy protection that did exactly that, as an example. reply tom_ 10 hours agorootparentIt's pretty much best practice on the 6502. Loading code from disk or tape is basically the sane thing too. reply kkukshtel 17 hours agorootparentprevWatching the video, it seems like the explanation is mostly \"tag the code and transpile to C\" - im assuming the tags are acting as the hardware-aware part? reply bluedino 18 hours agoparentprevAre they using self-modifying code or anything else that would make this tricky? reply dividuum 16 hours agorootparentReally surprised this topic didn't come up anywhere. I couldn't find anything in the linked repositories either. I guess there's no self-modifying code due to it being mapped into the address space from a cartridge? reply lxgr 11 hours agorootparentYou definitely do need code execution from RAM for high-fidelity emulation. How else are people going to implement Flappy Bird in Super Mario World? :) https://www.youtube.com/watch?v=hB6eY73sLV0 reply unleaded 11 hours agorootparentprevBasically every game is written in C on a modern processor (MIPS) so i highly doubt it. Maybe some weird homebrew stuff like that flappy bird video uses it but emulators tend to deal with problems like that by ignoring them (e.g. IIRC there is no emulator that can properly run that N64 linux port from a few years ago, and there have been plenty of SM64 glitches that work differently on emulators vs real hardware) reply lcnmrn 16 hours agoprevI wish all game executables would be OS agnostic, something similar to WebAssembly: GameAssembly. For example you can no longer play 32-bit games on new macOS version, neither 32-bit Windows games under Wine. It's almost impossible to find 64-bit .exe files so they can run under fine under Wine/Crossover. reply lxgr 11 hours agoparentThe code running on the CPU is arguably the smaller problem when it comes to games: At least for consoles, they're often depending on very low-level GPU implementation details, especially for older titles and platform exclusives. Older consoles had even more dedicated chips doing relevant stuff other than \"simple\" I/O, like e.g. the SNES's sound chip, which was a completely independent CPU running its own little programs in parallel with the CPU and GPU. Of course you could \"mandate\" a high-level VM for all future game development, but given the static nature of console hardware specs, that wouldn't be competitive for titles wanting to make use of cutting-edge graphics. reply codedokode 13 hours agoparentprevIt is not enough to provide portable machine code to make games portable, there are lot of different APIs that games need: graphics API, windows API, input API, sound API, disk API and so on. Sometimes they need kernel modules for anti-cheat and anti-piracy features. What saddens me is that open-source OSes do not want to provide a common set of APIs so that the developer only has to write and test the code once and it works everywhere. There is flatpak, but as I understand, it doesn't provide a set of APIs, it simply packs a Linux distribution into a virtual machine without any specifications or documentation. So ugly, and no compatibility for smaller non-Linux OSes. reply lambertsimnel 15 hours agoparentprevI agree with your sentiment, but I've had satisfactory results running 32-bit Windows games on x86_64 Linux On Debian, I found I had to run dpkg --add-architecture i386 && apt-get update and then install the wine32 package reply hot_gril 15 hours agoparentprevI was able to run 32-bit Windows games in Wine on Mac, even on Apple Silicon. GTA IV on my M1 mini. There's some kind of \"32 on 64\" emulation built into whatever version of the 32-bit Wine that PlayOnMac installs. Impossible to Google so idk how it really works. It's funny how the Mac is more compatible with old Windows programs than old Mac programs. reply josefx 14 hours agorootparentOld windows programs had the luck that intel fucked up Itanium, which left us with AMDs idea to just take the existing 16 bit compatible 32 bit processor and turn it into a 32 bit compatible 64 bit processor (that might also be 16 bit compatible, not sure on that one). As result new and old windows programs use a lot of the same basic instructs and emulating the newer programs includes a lot of the work needed to also emulate the older ones. reply kbolino 12 hours agorootparentModern x86-64 processors are still capable of running 16-bit code but, assuming you want to do it from within a modern operating system, it needs to be 16-bit protected-mode code, which isn't that useful since I think most 16-bit games run in real mode. Hence why DOSbox/Bochs are used instead (also, for emulating the original sound/graphics hardware). AFAIK, no 64-bit version of Windows ever shipped with native 16-bit support, though. That means 16-bit support on x86-64 in practice was only ever usable on a 32-bit operating system, which means you still couldn't have 64-bit, 32-bit, and 16-bit code running side-by-side, even if the hardware could theoretically support it. Intel does want to get rid of most of the compatibility modes, including all 16-bit support, but they haven't done it yet: https://www.intel.com/content/www/us/en/developer/articles/t... reply angra_mainyu 14 hours agoparentprevFunnily enough, I think Windows might just be it. Wine (+ Steam's Proton) works incredibly well these days, a huge improvement from the old days when barely anything ran, and what did run required heavy tweaking or even custom compilation. On Linux, my Steam library of about 300 games runs almost in its entirety without issues (except for Last Epoch). reply treyd 15 hours agoparentprevCIL is kinda like this for games implemented heavily in C#. reply lupusreal 15 hours agoparentprevThe JVM does this pretty well for Minecraft. I doubt Minecraft would support Linux, the BSDs, etc without it. Bedrock edition (the not-JVM version which supports Windows and consoles/phones) doesn't even support MacOS even though it obviously could. reply hot_gril 15 hours agorootparentIf Minecraft were a Microsoft product from the start, they probably wouldn't have made the Java version compatible with Mac or Linux (even though it uses JVM, there are some natives included), or wouldn't have made a Java version to begin with. reply pjerem 14 hours agorootparentAlso it’s pretty ironic that Microsoft, who wants to convince the entire world that c#/netcore is the next multi-os platform isn’t capable of delivering anything else than Minecraft Java on Linux. It’s a shame that I can’t play with my son from my Linux PC since he plays on switch (so the c#/bedrock edition) reply neonsunset 14 hours agorootparentBedrock edition is written in C++ and does not use C#. reply pjerem 12 hours agorootparentOh, didn’t know that, thanks ! reply neonsunset 11 hours agorootparentIf it were, targeting all platforms would have been fairly easy (assuming it would still rely on OpenGL), sometimes just specifying RID e.g. `linux-arm64`. reply circuit10 10 hours agorootparentThey could target Linux fine with their C++ code, they actually have secret builds for Linux for Marketplace creators (they have been leaked occasionally) but they don’t allow the general public to use them There is an unofficial Linux launcher that can run the Android build reply hot_gril 9 hours agorootparentOne of the nice side effects of not playing video games anymore is I don't have to touch anything from Microsoft with a 10' pole. reply hot_gril 13 hours agorootparentprevIt doesn't work in Wine? Or maybe that's too annoying to deal with. reply pjerem 12 hours agorootparentNo it doesn’t. It’s a UWP app with DRMs everywhere. Although there is an unofficial Linux launcher capable of running Bedrock natively using the embedded machine code in the Android version but it requires to buy the game one more time from the Google play store. reply jamesgeck0 12 hours agorootparentprevThe PC version of Bedrock edition is only available from the relatively locked down Microsoft Store. reply hot_gril 12 hours agorootparentAh, game over reply lupusreal 14 hours agorootparentprev> It’s a shame that I can’t play with my son from my Linux PC since he plays on switch (so the c#/bedrock edition) There are some plugins for the java edition server that allow bedrock clients to join, that might be an option for you. reply pjerem 12 hours agorootparentOh nice ! I’ll check that ! reply neonsunset 15 hours agoparentprev.NET would probably be as close as you can get given it allows you to have portable assemblies with pointers, structs and simd code which is very relevant to games. It models statically-typed compiled languages in a much superior way to WASM, there are projects showcasing how well e.g. Rust maps to it[0], and C++/CLI existed for a long time (although it's not cross-platform, and is considered cursed). C and C++ could be mapped in a perfect manner to IL and then AOT compiled in ways that GraalVM's Truffle can only dream of (given that .NET exposes a very wide set of intrinsics allowing a lot of code to be ported that way). [0] https://github.com/FractalFir/rustc_codegen_clr reply izzydata 19 hours agoprevI hope this eventually also works for Linux so that N64 games can be reliably played on very low end portable handhelds. N64 is notoriously tricky to emulate, but building them for the platform natively removes all performance problems. reply cpressland 19 hours agoparentI’m pretty sure Linux is natively supported via N64Recomp: https://github.com/Mr-Wiseguy/N64Recomp An example of this can be found here: https://github.com/Mr-Wiseguy/Zelda64Recomp?tab=readme-ov-fi... reply jjice 19 hours agoparentprev> N64 is notoriously tricky to emulate Huh, I wouldn't have expected that. I don't know much about the specifics of N64 emulation, but I've had N64 emulators running perfectly since the early 2010s. I played mostly the big classics, so I'm unfamiliar with more niche title performance. reply LetsGetTechnicl 19 hours agorootparentYou'd be surprised, even Nintendo doesn't get it perfect themselves. When N64 games were first made available on the Nintendo Switch Online platform, there were several problems with graphical effects and input lag. Here's an MVG video from the time: https://www.youtube.com/watch?v=jSyBMSOfPxg reply jjice 18 hours agorootparentHaha I remember this launch and how awful it was an thinking about how a bunch of fans have done such a better job. Hell, even Nintendo did a better job for the Wii's Virtual Console. reply LetsGetTechnicl 14 hours agorootparentYeah it's an interesting pattern. Even the GameCube had decent N64 emulation early in its lifecycle. But the Wii U Virtual Console also had issues with N64 emulation. Specifically it added a dark filter for no reason, which could be removed with homebrew thankfully. You'd think Nintendo would have the best emulation of their own systems on their systems, considering how long people stick around you probably have people who developed for the N64 still there reply jimbobthrowawy 12 hours agorootparentHomebrew emulators will almost always work better than first-party ones which need to avoid copyleft code, and can be content with a small subset of games being \"playable\", or \"good enough\". Console makers will often hire lead developers of open source emulators to sew something up for them, like what was used for the mario 35 collection (part of galaxy was recompiled too) or the \"ps2 classics\" software emulator for the ps3. reply mrguyorama 17 hours agorootparentprevThe N64 had a fully unified memory architecture, and weird custom silicon that also happened to be a fully programmable GPU kinda device. For decades, all N64 emulation was done by translating calls to that GPU (called the RDP) to openGL or DirectX calls, and that works horribly, but anything else would have had awful performance. Several years ago however, someone decided \"Fuck it\" and created a fully emulated version of the RDP that you could run, ie feed it real N64 instructions and it spits out pixel values. It was significantly slower than the translation approach, so you had to have like a powerful 8 or more core CPU to run it well. Some other madlad then took that approach, and ported it to a GPU Shader, because fuck yeah thousands of cores! It actually greatly improved the situation, as you didn't need a super powerful GPU, just a somewhat modern GPU. That development means devices like the steam deck actually have zero issues running awesome N64 emulation, including allowing for upscaling to much higher rendering resolutions without artifacts because you are literally telling a virtual RDP to do it, and it happily complies. Before AngryLion and ParaLLEl, we were stuck with basically the same graphics plugins, glitches, and tweaks as the late 90s. https://www.libretro.com/index.php/category/parallel-n64/ for a more detailed description of how this changed things. reply 0cf8612b2e1e 19 hours agorootparentprevI think part of the issue is that some purists believe anything less than pixel perfect fidelity is unacceptable. I recall reading a report on N64(?) Wii(?) about how the flame flicker in one level of one game was off. reply tempoponet 18 hours agorootparentAny Dolphin progress report will be full of these one-off quirks with an exhaustive deep-dive on how the issue was diagnosed and solved. Setting aside the part about purists finding it unacceptable, I appreciate the tenacity of the devs that find these problems worth solving. reply jezzamon 18 hours agorootparentprevDepends what you're trying to do, but some glitches used in speed runs literally depend on the exact values of pixels on the screen https://youtu.be/0B095eHBrCg?t=24m40s Not saying that supporting glitches like that is a hard requirement for these ports though reply jamesgeck0 12 hours agorootparentInaccurate emulation can also result in stuff like altered physics, invisible objects, stuttering animations, etc. resulting in a different experience from the original. reply grecy 19 hours agorootparentprevMany games were very playable with UltraHLE back in 1999, on the hardware of the time (~ Pentium 133 IIRC) reply throwawayben 11 hours agorootparentI remember convincing my dad to get a 3dfx voodoo2 because I really wanted to run UltraHLE. That was a seriously impressive emulator reply anthk 10 hours agorootparentprevI had to do several magic flags for video plugins in order to avoid glitches in THPS2 for instance, compared to Zelda or SM64. reply Fartmancer 18 hours agoparentprevI tried this out on my Ubuntu PC and it works great. Played it for around an hour without any issues. reply vaughnegut 19 hours agoparentprevI watched the youtube video this article links to the other day and it mentioned that it supports Linux, specifically mentioning that it should be usable on Steam Deck. reply izzydata 19 hours agorootparentThe steam deck has way more than enough resources to power through any and all N64 emulation so that wouldn't change much there. I'm specifically talking about something $50 retro handhelds and the like. For example Mario64 and Ocarina of Time are completely reverse engineered and have native support for Windows and Linux and they run perfectly on those systems as where they really struggle when trying to emulate. reply vundercind 19 hours agorootparentI still get audio pops and glitches with whatever the default n64 emulator EmuDeck installs is, on many games. Be nice to eliminate that. reply privacyking 10 hours agoparentprevIt already works on linux. I've tested it. reply andrewclunn 19 hours agoparentprevThat will likely require full decompilation. This is akin to wrapping the emulator up with the rom, with a few common functions (found via decompilation) being updated to support more modern rendering approaches. This isn't so much \"native\" as it is a patched container in an executable form. EDIT - But hey, you can always just add another layer of emulation via Proton to run it in Linux. reply GaggiX 19 hours agorootparentI don't think that's true, it really does recompile the N64 games into native executables, you can compile them using any C compiler like msvc, gcc and clang. reply vouaobrasil 19 hours agoprevOne thing this could be cool for is games like Goldeneye, which is hard to play on PC because it expects that weird joystick input. It would be awesome to have a native Goldeneye port with normal mouse behaviour like other FPSes. reply whateveracct 18 hours agoparentWASD with mouse aim control doesn't really work with the game though. The modal aspect of aiming combined with the travel time/return-to-center of the cursor is a really fun part of the game. If you could just free-aim while running a la PC FPS it would kind of ruin the campaign since everything is designed around this. Picking your spots and keeping cool while aiming are the appeal. That said, if you keep the modal aspect and have the mouse just 1) rotate Bond like left/right on the stick does and 2) control the cursor only in \"aim mode\" (hit R)..then I think that could be fun. Tbh, I wish shooters didn't go straight to Quake-style WASD+mouse free-aim across the board. The modal aspect makes gives Goldeneye a certain physicality I really like. reply X0Refraction 18 hours agorootparentWeirdly Goldeneye did have a control layout where you could move with one joystick and aim with the other using 2 controllers: https://www.youtube.com/watch?v=dZaEpugk3hY reply hot_gril 14 hours agorootparentThat's how I played, and while the N64 joysticks aren't very good, it was alright. The much bigger problem was lag. reply GardenLetter27 17 hours agorootparentprevYeah, it's awesome as you can set this up relatively easily with EmuDeck and play it with the two sticks on the Steam Deck. reply AlecSchueler 18 hours agorootparentprevYou can turn off auto aim in the game though. Always the first thing I do. reply anthk 18 hours agorootparentprevDeus Ex has quake controls but you need to focus a little before aiming. Then, later you get upgrades and laser reticles allowing you to shoot near instantly. reply terramex 18 hours agoparentprevThere is emulator hack for Goldeneye that add WASD + mouse support and I heard that it works great. https://github.com/Graslu/1964GEPD/releases/tag/latest reply hot_gril 14 hours agorootparentSomething's wrong with it, maybe related to the framerate. I can run and shoot a lot faster than before, but enemies take just as long as the old version to aim. I was able to beat levels on 00 mode with ease, mostly by sprinting past enemies. reply wodenokoto 19 hours agoparentprevWouldn't it still expect a weird joystick? reply haiku2077 19 hours agorootparentThe native ports have additional features, which are easy to implement starting from the native compilation. For example, the native port of Zelda has gyro aiming and automatic autosave options. reply vouaobrasil 19 hours agorootparentprevWell I was hoping there'd be a way to translate mouse input into the joystick input in a straightforward way that isn't available on emulators. reply haiku2077 15 hours agorootparentIt could be done with native mouse control. No joystick intermediate- instead modify the C code to allow direct control of the player character with a mouse. reply ThrowawayTestr 17 hours agoparentprevThe mouse+wasd hack works really well. Also works with Perfect Dark. reply tombert 15 hours agoprevThis is pretty neat; can someone tell me if this could lead to a proper working version of Conker on the MiSTer? E.g. could this be used to assist in a decompilation and then someone could make a fix? reply protoster 19 hours agoprevDo typical emulators compile ahead of time? Just-in-time? If not, why? This approach (ahead of time compilation) appears to have huge benefits. reply jerf 19 hours agoparentYou name the compilation and/or interpreter technique and there's at least one emulator that uses it. reply chlorion 15 hours agoparentprevThere are lots of emulation methods but interpretation is probably(?) the most common or at least very common. Interpretation involves writing a VM that represents the state of various parts of the machine, and executes instructions that change the state of the VM, with varying levels of accuracy. reply ranger_danger 19 hours agoparentprevA fair amount of 32-bit and later emulators do have JIT recompilation, but I'm not aware of any ahead-of-time translation unless that's how TeknoParrot works (you can run linux arcade games that use ELF binaries on windows with it for example), but I wouldn't call that emulation either way since the architecture is the same. reply entropicdrifter 17 hours agorootparentModern Vintage Gamer over on Youtube has worked with Nightdive and Limited Run Games and helped design a C recompiler called Carbon Engine for various older consoles that are functionally the same as ahead of time compiled emulators. https://youtu.be/eZRzaGFWoz8?si=XQyJol0pe2z2oG6d reply rcarmo 14 hours agoprevNintendo is probably having kittens about now. Extremely impressive. reply djmips 17 hours agoprevWhat's the advantage over an emulator? This is after all, a form of an emulator - one where you translate ahead. This sort of thing has been done in the past with things like Bleem so it can be more performant but on a modern PC any good emulator is going to perform well. https://en.wikipedia.org/wiki/Bleem! reply skrrtww 17 hours agoparentThe goals of this project and the goals of N64 emulation are fairly different. The goal of N64 emulators (generally) is to accurately recreate the behavior of the original console. Conversely, there is a large crowd of people who just want to play SM64, OOT and MM with a bevy of \"graphical enhancements\" like 60fps, widescreen, texture packs, randomizers, etc. For these people, the fact that these games originally ran on the N64 is practically irrelevant. In fact, it's a hindrance; The N64's graphical pipeline is cumbersome to emulate and actively stands in the way of modern 'enhancements.' This project is more aimed at giving the games themselves modern treatments, removing the N64-centric aspects of the games from the equation entirely. reply hot_gril 14 hours agorootparentHonestly SM64 with new graphics in those demo videos looks worse than the original game. But Gamecube games running in an emulator look amazing because they use a lot more vector graphics, which the emu can scale up and apply antialiasing to. What interests me more about recompilation is it can produce very efficient binaries instead of having to emulate everything. And also makes it easier to add new features. reply ClassyJacket 7 hours agorootparentYeah, SM64 is weird because a lot of '3d' objects used billboarding, it was actually just a flat object always facing the camera, so they don't scale up well. Anything spherical (chain chomp for example). Hilarious proof: https://imgur.com/gallery/Xw4tN#FoRAGl7 The characters in Mario Kart 64 were like that too, they just prerendered a heap of different angles and inserted the appropriate one. With recompilation we could change those things, but I'm not sure how much work it would be and how many people who care to play those games would want to alter them that much. Early 3d games will always have those rough edges sadly. reply wmf 17 hours agoparentprevJIT emulation can cause stutters while AOT should not. The difference may not be noticeable now that computers are so much faster than the N64. reply doublepg23 17 hours agoparentprevThe video shows multiple bugs disappearing when using this method vs. emulation. reply ThrowawayTestr 17 hours agoparentprevRecompilation is not emulation. reply anthk 10 hours agorootparentI don´t get the downvotes, this is just... porting. reply pipes 16 hours agoprevAs an aside, the perfect dark decompile port is really nice to play on the steam deck. Not sure why, but the Linux version tends to freeze on the villa level. Switching to the windows version and using proton works great. https://github.com/fgsfdsfgs/perfect_dark reply simple10 16 hours agoprevSuper impressive walkthrough video in the article. It does a great job of explaining Majora's Mask. Although, it's probably glossing over a lot of the upfront technical challenges of using it to port games. reply ranger_danger 20 hours agoprevJamulator did this for NES way back in 2013 but nobody really seemed to care... https://andrewkelley.me/post/jamulator.html There is also Winlator for running Windows programs on Android: https://github.com/brunodev85/winlator Also is the youtube video linked in the article using an AI voice? reply claudex 19 hours agoparent>Jamulator did this for NES way back in 2013 but nobody really seemed to care... At least Mr WiseGuy seems to care. From the N64Recom Github repo[0]: \"This is not the first project that uses static recompilation on game console binaries. A well known example is jamulator, which targets NES binaries.\" [0]: https://github.com/Mr-Wiseguy/N64Recomp reply BearOso 19 hours agoparentprevJamulator was unsuccessful because so much extra hardware was on the cartridges. Recompilation complicates things and doesn't provide many benefits in that situation. The N64 is fixed hardware, so it's a little bit easier. Even so, this project still uses emulation for the RDP. I don't know how it handles generated RSP microcode, maybe it doesn't. A lot of games just used the official libraries. reply zamadatix 18 hours agoparentprevJamulator didn't take it farther than what NES emulators could do beyond the way it ran. If Jamulator had released with similar enhancements for Zelda it probably would have been a lot more popular. I.e. the interest is more from what is now available with it than about how it does it. That's just Nerrel, a real person, narrating. It's crazy (in an interesting way, not necessarily a doom and gloom way) how we have AI voices these days so good we start to suspect people who don't speak like we expect are AI. reply refracture 18 hours agoparentprevThat's just Nerrel; his narration is so flat that he's always sounded like an AI. I enjoy that mix with his humor. reply DrNosferatu 16 hours agoprevAny efforts for other platforms? I would love to play a source-port of arcade “GTI Club”!historically they have cared more about protecting copyright for current consoles. This is categorically false, and I even linked to evidence of it being false in the present day. I don’t know how you could say this as a smash player, but you’re acting like they won’t come after these people for doing what they’re doing. In reality it’s entirely up to the whims of Nintendo. reply yamazakiwi 11 hours agorootparentYou did not link evidence to my statement being false, you posted a Hungrybox video of him complaining about his tournament issues he's having with Nintendo. Current Smash Tourney problems with Nintendo does not supplant Nintendo's history with emulation. I simply stated Nintendo doesn't care as much, which is true, I didn't say \"Nintendo absolutely doesn't care and won't do anything ever unless it's Switch related.\" I'm uninterested in replying any further since it's not getting across that I'm not diminishing the Smash issues, it's clear you're passionate and Nintendo are being real arseholes for sure. reply babypuncher 16 hours agoparentprev [–] Nintendo doesn't typically go after emulation projects. Yuzu was the exception, and there are reasons for that related to how the Yuzu team ran their project and distributed it that painted a legal target on their back. This project isn't distributing any of Nintendo's code, game assets, or other intellectual property. reply 0xcde4c3db 11 hours agorootparent [–] Nintendo has also gone after a handful of specific tools for circumventing the Switch encryption, which has led to speculation that this push is really about shoring up \"Switch 2\" against existing hacks and emulators being updated (in the event that it's substantially the same software platform with updated silicon). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "N64 games are now transformable into native PC versions with enhancements like ray tracing and high FPS using the N64Recomp project on GitHub.",
      "Led by Mr-Wiseguy, the project aims to maintain the original gameplay experience while enhancing graphics and performance, appealing to preservationists and N64 enthusiasts.",
      "The rise of open-source projects, homebrew games, and N64 advancements is revitalizing interest in the console's game library, offering a modern gaming experience for classic titles."
    ],
    "commentSummary": [
      "The debate revolves around a tool enabling the recompilation of Nintendo 64 games into PC ports, raising concerns about feasibility and legality.",
      "Discussions cover challenges related to compatibility, legal aspects, and dissecting ROMs, delving into the technicalities of emulation such as running 32-bit games on contemporary systems and N64 emulation constraints on diverse platforms.",
      "Topics include comparisons between official and fan-made emulations, improved N64 emulation performance, potential enhancements like recompiling Goldeneye, Nintendo's response to emulation progress, and the use of Proton for game porting amidst tournament conflicts."
    ],
    "points": 351,
    "commentCount": 159,
    "retryCount": 0,
    "time": 1715777299
  },
  {
    "id": 40366443,
    "title": "Proteins in Blood Hold Key to Early Cancer Detection",
    "originLink": "https://www.theguardian.com/society/article/2024/may/15/proteins-blood-cancer-warning-seven-years-study",
    "originBody": "View image in fullscreen The study found 107 proteins associated with cancers diagnosed more than seven years after the patient’s blood sample was collected. Photograph: Simon Dawson/PA Cancer research Proteins in blood could provide early cancer warning ‘by more than seven years’ Study identifies 618 proteins linked to 19 types of cancer, which could lead to much earlier detection Anna Bawden Health and social affairs correspondent Wed 15 May 2024 05.00 EDT Share Proteins in the blood could warn people of cancer more than seven years before it is diagnosed, according to research. Scientists at the University of Oxford studied blood samples from more than 44,000 people in the UK Biobank, including over 4,900 people who subsequently had a cancer diagnosis. They compared the proteins of people who did and did not go on to be diagnosed with cancer and identified 618 proteins linked to 19 types of cancer, including colon, lung, non-Hodgkin lymphoma and liver. The study, funded by Cancer Research UK and published in Nature Communications, also found 107 proteins associated with cancers diagnosed more than seven years after the patient’s blood sample was collected and 182 proteins that were strongly associated with a cancer diagnosis within three years. The authors concluded that some of these proteins could be used to detect cancer much earlier and potentially provide new treatment options, though further research was needed. Dr Keren Papier, a senior nutritional epidemiologist at Oxford Population Health at the University of Oxford and joint first author of the study, said: “To save more lives from cancer, we need to better understand what happens at the earliest stages of the disease … [and] how the proteins in our blood can affect our risk of cancer. Now we need to study these proteins in depth to see which ones could be reliably used for prevention.” A second linked study looking at genetic data from more than 300,000 cancer cases found 40 proteins in the blood that influenced someone’s risk of getting nine types of cancer. While altering these proteins may increase or decrease the chances of someone developing cancer, in some cases this could lead to unintended side-effects, the authors found. Mark Lawler, the chair in translational cancer genomics and professor of digital health at Queen’s University Belfast, said: “The data are impressive – finding evidence of cancer before it has manifested itself clinically provides a critical window of opportunity to treat with a greater chance for success, or even more importantly to achieve the holy grail of preventing cancer before it can even occur. More work to be done, but an important step forward in a disease that affects one in two of UK citizens during their lives.” Lawrence Young, a professor of molecular oncology at the University of Warwick, said the findings were another step towards identifying markers of increased cancer risk as well as those aiding early cancer diagnosis. skip past newsletter promotion Sign up to First Edition Free daily newsletter Our morning email breaks down the key stories of the day, telling you what’s happening and why it matters Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion “Determining protein changes that precede the development of cancer is not only important in identifying high-risk individuals but could also provide insights into factors responsible for causing cancer.” Additional reporting by Nicola Davis Explore more on these topics Cancer research Cancer University of Oxford Medical research Health news Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=40366443",
    "commentBody": "Proteins in blood could provide early cancer warning 'by more than seven years' (theguardian.com)327 points by racional 20 hours agohidepastfavorite121 comments zero-sharp 19 hours agohttps://youtu.be/FzFT-KuE4BQ?si=7-EXbRz1TD4a5leL&t=56 The video makes the case that the early detection of cancers isn't always a good thing. See 6:30 and 7:00 for specific references to scientific studies. Some cancers (thyroid and skin) can be detected at a much higher rate, but the associated mortality remains constant (we are detecting benign cancers). To be clear: obviously this is dependent on the cancer. Really my point is that we need studies that show that the screening improves outcomes. reply brnt 19 hours agoparentThis is how Dutch public healthcare motivates its general aversion to medical testing, as many a foreigner finds surprising and incongruent to the generally OK level of healthcare outcomes provided by the system. You can test and detect, but the error margins are often large (so many false positives and/or false negatives), would generate way too much workload to follow up and thereby cost time and money for better leads. Plus, as you say, knowing early doesn't always or even often mean you can actually change the outcomes. Now, this is what they say... I have tried to find the actual literature they use to motivate their protocols, and apart from occasionally, have not been able to find that evidence. I'd love to have a resource that demonstrates these sorts of things. reply mort96 18 hours agorootparentIt's not just about keeping costs down, but also about increasing quality of life. If you detect a benign cancer in someone, and they then go on to receive chemotherapy, you've massively decreased that person's quality of life for a significant period with no upside. reply iknowstuff 17 hours agorootparentA benign tumor doesn’t spread and doesn’t warrant chemotherapy I believe? And if it’s actually cancer than I don’t understand how you would not want it gone as soon as possible to avoid metastasis? reply canes123456 17 hours agorootparentIt’s more complex than this. There is a pretty narrow sweet spot where early detection actually helps. If the cancer is very fast growing, it could be too fast for treatment to help at all. Even if treatment helps there likely not a very long period of time before you develop symptoms that would have lead to treatment regardless. If it is very slow growing, you might outlive the cancer and it doesn’t require treatment. It is effectively but not actually a benign tumor. You also have to deal with false negative and positives, that could be an order of magnitude higher than the Goldilocks true postives that earlier detection actually made a difference. It’s easy to see how population results will not show much of a benefit. reply jajko 14 hours agorootparentProbably the most famous slow growing tumor is prostate cancer. As per my friend who is urology surgeon, basically all men eventually catch it, unless they die young. But it goes so slowly and symptoms are rather mild in most cases no invasive treatment is done. reply Fire-Dragon-DoL 17 hours agorootparentprevWell, this is talking specifically about the case where the cancer is benign. Of course you want a metastatic one gone. For the benign ones, that's going to hurt the person's body quite a bit reply jvanderbot 17 hours agorootparentI think the jump from \"protein blood test\" to \"Chemotherapy\" is a bit of a stretch. There are almost surely additional screenings and diagnoses going on there. And I don't think anyone is going into chemo for benign tumors. reply Fire-Dragon-DoL 17 hours agorootparentMakes sense. Is it possible for the two types of cancer to be confused? Should also point out that tests will negatively affect your life seriously either way, especially if these benign cancers are common. Think of the time spent going to/from the doctor, the incredible stress (am I dying?), the tests itself: it is damaging the person's life. Detection is good, false positives are unacceptable though. reply jvanderbot 16 hours agorootparentI recently read Outlive, and he makes the claim that almost all of the increase in survivability for cancer comes from early detection. I think projects like this are extrapolating that out. I think a little heartache and worry is excusable if it means 10 years added to your life. Over time, we'll develop a callous there and take it in stride. e.g., My two friends who got early screening had a few false positives, and one true positive treated early. One later died of heart attack and one lived long enough to get dementia. My two friends who didn't get early screening, one died of heart attack and one died of cancer that could have been caught. Both died earlier than two above. Sadly, at this point, you want to die of cancer, but you want to do it when you're 85. It beats a sudden heart attack and it beats dementia. You just want to prolong the outcome. reply steveBK123 9 hours agorootparentPrecisely. I don't know any \"did early detection which lead to unnecessary chemo\" stories.. I know people who died due to the lack of early detection. I also know people who survived due to accidental pre-symptomatic detection because of unrelated ER scans where they'd have been in stage 4 before having any symptoms. So more screenings for more types of cancer are absolutely needed. It's more treatable early, and you will have better outcomes. Patients might worry about lab results is not a good reason not to have early detection. Patients can self select if they are so sensitive. reply vidarh 5 hours agorootparentHow would you distinguish a necessary and unnecessary chemo after the fact? Nobody would give chemo if the tumor is known to benign, after all. An unnecessary chemo will be one where they couldn't tell, and the chemo ensures they wont tell. The only way you'll know is from aggregate mortality statistics not budging. The other aspect is that even if a tumour isn't, earlier detection is often not reducing mortality any more once you get to a decent baseline. The evidence often does not support early screening in general. Early screening for specific subsets of the population have better results, but are also not unambiguously helpful. reply robocat 12 hours agorootparentprev> [costs:] the incredible stress I have seen a few people get benefits from a cancer scare: a refocus onto what matters in their lives. Agree: I would guess most people just get costs. Of my middle-aged friends with health scares only a few addressed the underlying cause (and even fewer are proactively avoiding health issues). reply IG_Semmelweiss 7 hours agorootparentprevi think you are severely underestimating the human species bias for action once the train leaves the station theres a lot of steam to go in 1 direction only reply IG_Semmelweiss 7 hours agorootparentprevyet, many a malignant tumor may not metastize until you are 90 and dead from other causes, either reply dukeofdoom 16 hours agorootparentprevProstate cancer. Lots of nerve endings there. The procedure to remove it can lead you to be incontinent. Let's say you treat the cancer but get damaged by the procedure and can't be as active. Your seditary life style leads to a blood clot and an early death ...in the end you may have lowered your life expectancy as prostate cancer is slow growing reply vharuck 15 hours agorootparentProstate cancer also came to my mind first. Doctors generally stop screening for prostate cancer after a certain age (70 and older is the recommended cutoff from the US Preventative Services Task Force), because, if the cancer wasn't causing symptoms, it's unlikely to impact quality of life or cause death before something else. The USPSTF references a lot of meta-analyses dealing with screening outcomes. They make decisions by whether a specific screening practice decreases mortality rates. They explicitly don't even include the financial cost of a screening practice. reply whimsicalism 16 hours agorootparentprevthe scenario you're describing would never happen, you don't prescribe chemo based on a blood test of protein markers reply dukeofdoom 16 hours agorootparentprevMy nurse friend said she only discharged 2 people after chemo in about 7 years of service. People have a misguided notion about the odds of survinvg a deadly cancer. They also found the diagnostic procedures for breast cancer was causing the cancer. reply arcticbull 14 hours agorootparent> They also found the diagnostic procedures for breast cancer was causing the cancer. I assume you're referring to mammograms. You do get exposed to a significant amount of ionizing radiation in mammography, about 0.4mSv, about 40% of the EPA's annual radiation limit for a member of the public. That's one of the very good reasons why guidance is women wait until age 45 to get annual screenings and switch to biennial at 55. At that point the rewards outweigh the risks. reply fidotron 18 hours agorootparentprevI am not sure that is unique to the Dutch, it is the line I have heard in several countries. Heavy agreement on your last part - if there is substantial evidence to back this up I would like to see it. As it stands I personally prefer the idea of constant mass testing in order that we learn as much as possible as quickly as possible, including improving the tests from the resulting feedback loop. reply brnt 18 hours agorootparentWhen I see the prices of some of that testing, I know that that cannot be the reason, and indeed, establishing personal baselines by regular testing can only be helpful. The aversion to it grounded on taking some average patient, I am convinced. I really wish there was more transparency, because test aversion is exactly the same protocol you'd invent if you were trying to save money. I want to be able to see which of the two we're dealing with. reply radicalbyte 17 hours agorootparentGiven how the Dutch system seems to be designed to maximise paper filling and busywork instead of healthcare - and of course to make the insurance companies rich - it's no surprise that they're against it. Medical experts here have very little say in how things are run. It's all bankers and bureaucrats. reply brnt 16 hours agorootparentDo you have any sources for that? I've worked for a hospital and I've never been able to find anything approaching a complete balance sheet. Financing it utterly opaque, but I'd love to have something solid before I accuse anyone. reply 6510 14 hours agorootparentI'm far from an expert on this topic, more on the contrary. The surgeons use to run the hospital. In contrast with mba's they knew things. I don't know what the difference is precisely but I hear the ziekenfonds use to have people to divide money over treatments (set prices) without their salary depending on their choices. I don't know about the scale but longer ago we would just build hospital buildings and house a workforce of nuns nearby who had their own garden. Now we some how cant afford to put down a building and with realestate prices on the rise the salaries need to follow. We might not like the factory village concept but if you have to be on call all of the time it seems fkn convenient to me. Cut the salary and give the employees a house, seems a great perk. Employee shortage is also costing a fortune. We've created ambitious labor protection laws then we created a loop hole where all you have to do is pay 190% of the salary to a job agencies (uitzendbureu) and no laws apply, anything goes. If you don't like it you can go home. This didn't need to cost 90% of the salary. Unless our labor taxes are now that complicated(?) but that doesn't work as an excuse either. Our taxes not paying for education doesn't mean we don't have to pay for it eventually (+interest) I read they are also lacking the money to streamline the processes. reply pessimizer 16 hours agorootparentprevThe reason this rings wrong for people is because the reasons early testing is dangerous are entirely social. People are motivated by fear to have every test available, doctors are motivated by fear of being accused of neglect by the patient, doctors are also motivated by the profit that they make from the tests, the manufacturers and patent-holders of tests are motivated to have them done as much as possible, the labs that do tests make money on the number of tests that are done, the nonprofits that campaign based on diseases are expected to message to increase testing for those diseases and accept money from manufacturers and patent-holders, there's motivation to exaggerate the danger of what's detected by the manufacturers and patent-holders of treatments for the disease, and there's motivation by researchers who formulate the criteria for determining whether a particular feature of something detected is potentially dangerous/deserves treatment, and a motivation to give them the most expensive treatment, regardless of whether that treatment is unpleasant; in fact if the treatment is dangerous, it opens up secondary markets. There's just an enormous number of tailwinds pushing overdetection and overtreatment. The sum of that is what's important, which is that when you test earlier, you often objectively end up with more death and suffering. Which is what a state-run national healthcare systems needs to look at, they can't get lost in the trees. You pick an optimum age for testing that shifts the balance to less suffering and death (and costs), and you look for specific exceptions (genetic, lifestyle, comorbidities) and test just those people early. Could there be a way in which all testing would help instead of hurt? Yes, but it's political and psychological and not likely to ever happen. You'd have to (as a patient) trust probability in general, and additionally you'd have to trust the probabilities that they're handing you haven't been distorted by the self-interest of others. Not likely for the foreseeable future; maybe 1000 years from now. If you want to do the study, all you have to do is compare the number of deaths from a thing when people are tested early to the number of deaths when people are tested late. Or just look for other people who have done them. If early testing obviously saved lives, the people who sell testing would tout them everywhere. Instead, they're stuck trying to look for angles to argue that lessening death and suffering isn't the biggest consideration. They recently did this to push breast cancer screening earlier again, by arguing that if you specifically look at black American women, they benefit from early cancer screening. So overall, breast cancer deaths go up, but that's just your privilege talking. Woke conglomerates. Ignore that black people have a unique, neglected, discriminated against, and poverty-ridden situation in the US; in fact, it might even be racist to point that out (in backwards land.) Of course, you definitely don't have to do the studies, plenty have been done. Anybody saying that they've looked and haven't found them has not looked or has seen them and is not telling the truth. Both positive and negative, about every test. The studies that support earlier and more testing are press released and marketed, though, while the others can be suppressed or simply ignored, unless some public health system or insurance company champions them, and of course they would. reply brnt 16 hours agorootparentI fully agree. However, the converse is also true: we don't actually know which protocols are there because they're social, and which are there to benefit the insurers. Both could motivate cost cutting. I want to see the difference. I want whatever the full story is laid out in front of me. reply refurb 7 hours agorootparentprevThe evidence is out there in published papers that do health economic analyses of various interventions. On a population based level it makes perfect sense to not do things even if it could save lives. This is exactly what NICE does in the UK. If it costs more than 30,000 GBP to give one person one more full quality year of life, then it’s not worth it. reply vidarh 4 hours agorootparentTo the point about NICE, it's worth pointing out that this is 1) not how NICE decides. Average cost per QALY being below the preferred level is one factor. Analysis of treatments they approve suggests the \"actual\" threshold is closer to 45k. 2) These are averages per treatment or protocol and in most areas the NHS spends less than the threshold by simply delivering based on clinical assessment of benefits, and so the threshold is limiting treatment mainly on the fringes where you find a few very expensive treatments with marginal benefits with no cheaper alternative. In practice NICEs threshold is what any insurer would do: You know how much money there is in the system and try to maximise health output with what is available to you. reply refurb 2 hours agorootparentIn practice NICEs threshold is what any insurer would do This is what makes Americans getting upset their insurance rejects coverage for medicines so ironic. Single payer systems do the exact same thing. The difference is in the UK is the option is just isn’t brought up in the first place because the doctor already knows it’s not paid for. reply vidarh 50 minutes agorootparentEven then, they'll happily bring it up if you mention you're fine with private referrals or recommendations for something that isn't covered. Almost all single-payer systems are single-payer only in the sense that everyone is covered by the public system, but without preventing people from going outside it. A very few - Norway used to - strictly limits what you can go outside the system for. In the UK about 10% have private insurance on top. reply leto_ii 18 hours agorootparentprev> would generate way too much workload to follow up and thereby cost time and money for better leads Having lived a decade in NL, my impression was that keeping costs down is the top priority. Unless you have a serious chronic condition or were in an accident, good luck getting somebody to take a look at you. (irl, after a while you learn to push, exaggerate symptoms etc. or just go back home to get tests and treatment). reply brnt 18 hours agorootparentYep, the loudest people get the most help. There's no good solution for that other than to become a bitchy 'client'. It's unfortunate that despite the promises, you still have to 'use it correctly' if you want those good outcomes as a patient. reply blindriver 7 hours agoparentprevHow are we supposed to increase outcomes if we don't have the screenings? Sure maybe the first few years the outcomes may not change, but what about 20 years from now, when a generation of scientists can detect cancer years earlier? The defeatist attitude of \"well it doesn't fix anything right now\" is really strange. Once we have reliable detection, it means we can find reliable treatment as well. reply LeafItAlone 7 hours agorootparentYes, I agree, but it also could lead to over treatment. If you detect and treat cancer that otherwise never would have caused problems, it was all for naught. Cancer rates found in autopsies for other causes of death are probably higher than you think. reply fumeux_fume 6 hours agorootparentprevDo you mean improve outcomes? Assuming that’s what you mean, improvement of outcomes for cancer patients seems more reliant on proper treatment rather than detection. reply JackeJR 6 hours agorootparentprevThe screening target should be for malignant cancers, not all cancers. reply Wowfunhappy 19 hours agoparentprevI can't watch a video at work but I have seen this argument before. I just find it fundamentally hard to believe that having more data is a bad thing. What we choose to do with that data is a different story, and the actual source of these bad outcomes. reply cityofdelusion 17 hours agorootparentData isn’t necessarily good because medicine and biology are messy and inaccurate. I just went through a scare myself with elevated markers on a typical blood panel. Lots of fuss, anxiety, and cost for zero gain. At the end, I learned that human bodies vary so much that we’re was just no way to know upfront if a finding was a concern or if my body was just on a tail end of a bell curve. Turns out, if you fully scan people, we all have lumps, bumps, and various anomalies. How much do you spend “treating” and investigating this stuff? I wasted my own time and precious time with doctors for nothing, increasing costs to society as a whole. That kind of data, the costs, we have tons of. That’s why pretty much every medical association regardless of culture has limits on recommended screenings. reply m463 7 hours agorootparentprevI think the same thing. That said, I think engineers seem to believe other careers should use logic in a straightforward fashion, but they are more complicated. For example, lawyers cannot use logic in the same way, because the court can almost arbitrarily say some facts must be ignored. In the same way, medicine has the hippocratic oath. It is \"do no harm\". It is not \"find the cause\" or \"cure the patient\". It seems doctors have made their peace with this and are aligned with it, since the overwhelming majority 88% have do-not-resuscitate orders for themselves. reply mort96 18 hours agorootparentprevAre you prepared to make the decision, \"I have cancer but statistically it has a relatively low likelihood of killing me before I would die of other causes, so I won't do anything about it\"? Are most people? reply ggm 11 hours agorootparentIf you're male and live to over 60, you are going to be in this camp regarding the PSA and intervention for Prostate Cancer. Two GPs, a Urologist and an Epidemiologist (none of whom know each other btw) have all said to me \"you will die with this not of this\" because they can trace the dynamics of my presentation. Enhanced imaging and blood tests alone didn't do this: their intuition based on progression and behaviour of the system as a whole did. Treat the person, have a longterm relationship with your health provider. reply nick__m 17 hours agorootparentprevThere is a spectrum between invasive treatment and not doing anything. You could have a scan 3 months later and if there is no progression the doctor schedule a scan 6 months later and then 12 months... If there is a progression he schedule an appointmentwith an oncologist. reply vidarh 4 hours agorootparentThe problem is that once the odds of benefiting from early detection is low enough, and the rate of a given condition is low enough, it takes very little for other things to dominate. E.g. how many of them end up leading to unnecessary biopsies, or scans that are themselves introducing a risk? For any condition there will be a threshold where too much screening becomes harmful because you're doing so many unnecessary tests that rare errors / accidents come to dominate the benefits. The question isn't if there's a threshold where more scans do more harm than good, but where it is. For some things, increased screening will be unambiguously good. For some it takes work to figure it out. reply thimkerbell 17 hours agorootparentprevIt might provide enough impetus for getting you to avoid sugar and processed meats though. reply mort96 14 hours agorootparentHas avoiding those things been shown to stop existing cancer? I thought those foods just increased the chance of getting cancer in the first place. But I'm totally ignorant here, it sounds plausible that some carcinogens work by worsening cancer which would otherwise have been benign, I just haven't heard about that reply stubish 10 hours agorootparentDepending on the type of cancer, there are correlations with diet changes and slowing growth of cancer. Causation isn't known yet, with the speculations about reasons veganism works I've seen all refuted. Low carb effects seems about increasing ketones and reducing glucose levels in the blood, which I think is being studied with some lung cancers. Type of cancer is important, with different diet changes positively correlated different with cancers not returning (and the wrong change could possibly even promote growth, as different cancers 'feed' on different things). Prostate, breast and lung cancer I'm aware of there being these correlations. reply canes123456 17 hours agorootparentprevIf your are choosing not to do anything based on the data, gathering the data is objectively a net negative. There are financial costs related to taking the tests as well as emotion costs related to false positives and even with deciding not to act with possibly true positives. There needs to be a net positive action on a subset of the cases to outweigh the costs of gathering and sharing the data. reply brnt 16 hours agorootparentprevMost data is crap, and you generally can't tell where the needle in the haystack is. Having more consistency between doctors would already be a change needed to actually use data. You will find it matters more than you'd like. We can't all have the best doctors, but we could use data to level the outcomes. reply epistasis 19 hours agoparentprevThis is not about early detection in general, but rather a specific test of dubious utility, specifically full-body MRI, which often leads to tons of follow-on tests and invasive procedures that may have zero benefit. For a test with high enough specificity and sensitivity for early detection, it's likely that it would be quickly adopted, and then studied to show that it actually improves outcomes without undue cost (not merely dollar cost but also health cost) to people in terms of treatment and its side effects. reply zero-sharp 19 hours agorootparentThey specifically talk about using the fully body MRI for the purpose of detecting cancers. I'm not sure how you missed that. You literally had to watch 10 seconds of the clip. It's interesting because they explicitly talk about follow up testing (@2:10) which is to say that multiple methods are used if the MRI indicates a problem. So yes, the initial MRI may produce a misdiagnosis and that is a defect of the test. But the commentary in the video obviously suggests that additional testing for early detection is done. That's partly a problem as you pointed out, but then it clearly also indicates the scope of the conversation is more broad. reply ceejayoz 19 hours agorootparentThey didn't miss it at all. You've misunderstood the comment's point, which is \"just because prospective full-body MRI is bad doesn't mean all diagnostics are\". reply zero-sharp 19 hours agorootparentIt's true that some of the commentary is specific to the full body MRI itself (such as misdiagnosis due to an imaging artifact), however many of the claims in the video are very general. And the scientific study they referenced at 6:30 has nothing to do with fully body MRIs. The study is in regards to thyroid cancer overdiagnosis due to screening (using ultrasound and not MRI). This is clearly a statement regarding the effectiveness of screening. And, yes, it is specific to the cancer. I'm not saying anything that the studies aren't saying. For some kinds of cancers and for some kinds of screening methods, screening can result in overdiagnosis. reply ceejayoz 18 hours agorootparentOK, so you had to watch a little more than ten seconds. There's no doubt that some diagnostic tests - like getting a full-body MRI as a precaution - may do more harm than good. Your apparent mistake is thinking that means all diagnostic tests probaby do. We'll have to figure out which one this is; it's a start of that process. We've demonstrated we can do it; now we have to figure out if we can distinguish between \"big bad scary\" cancer and \"whatever it won't kill you\" cancer. reply epistasis 18 hours agorootparentprevAt 6:30 it's about a specific test for thyroid cancer, which as discussed in my second paragraph, was not found to actually improve outcomes for that specific type of cancer, not cancer in general. However, early detection is responsible for greatly improving outcomes in many specific cancers. Full body MRI is not the test to achieve that. GRAIL's gallery test might be one to do it for many classes of cancer, but that still remains to be fully seen. The general of idea of early detection is still an extremely promising one for most types of cancer, and in particular for some of the deadliest, like ovarian and pancreatic cancer. reply Spooky23 18 hours agoparentprevIt’s a risk assessment like any other. Probability/impact. My wife ultimately lost her life to metastatic melanoma, which was believed to be in remission. Had there been a way to detect the proteins associated with the mets that developed ahead of symptoms, the odds are she we be alive and thriving. In other scenarios, say most prostate cancers, early knowledge has low or negative benefits. reply siliconc0w 18 hours agoparentprevThe problem with this is that they haven't done the long term studies (which they admit). They also don't consider that once these are cheap and regular enough you get the change over time which should get you a lot less false in positives. reply xpe 9 hours agoparentprev> we need studies that show that the screening improves outcomes. I would be wary of a study conflating screening with outcomes — that is unnecessarily coupling too many factors. [1] The screening isn’t an intervention, so why would it make sense to study its effects on an outcome? Those effects are indirect, mediated by the causal factors of the choice of treatment. Am I missing something? Perhaps you are looking for psychological effects of getting an early test? [1] Treatment outcomes depend on the current state of technology and the practice of medicine. reply vidarh 4 hours agorootparentBecause screening leads to interventions when it is the data used for deciding whether to do an intervention. If a screening procedure leads only to interventions that would be made in time anyway, it doesn't provide a benefit. If it leads to extra interventions and no drop in mortality, it may be a net negative. E.g irradiate enough people to find more cancers and not improve outcomes, and some of your screening may lead to cancers. reply Fire-Dragon-DoL 17 hours agoparentprevI don't understand how we don't consider benign cancers false positives? Acting against those cause serious damage to the body for no gain reply IG_Semmelweiss 7 hours agoparentprevI think Vinay Prasad (MD) has an excellent book or paper on this topic as well. Here is a discussion about it. It really gave me a new perspective on diagnostic testing. https://www.econtalk.org/vinay-prasad-on-cancer-screening/ reply mensetmanusman 17 hours agoparentprevThe more data the better though. We need to train the models to understand what is worth doing over time. reply zjp 14 hours agoprevIt seems like every other day there's a new breakthrough. I watched my paternal grandmother succumb to lung cancer when I was 7. She was my favorite person on Earth at the time and watching her go was devastating. It gives me so much hope to watch the category \"treatable and preventable cancers\" expand over time. reply consf 13 hours agoparentI have a similar experience with my grandmother. Only she had stomach cancer. Such research warms my heart reply tombert 20 hours agoprevThis is cool, though I do wonder if the tests will be good enough to differentiate between “cancers that will be lethal in the next seven years” and “cancers that are technically there but will take so long to kill you that something else will beforehand anyway”. reply two_handfuls 19 hours agoparentAlso “cancers that your body takes care of on its own” reply Jedd 19 hours agoparentprev> .. though I do wonder if the tests will be good enough .. Sure .. but .. a) knowing that cancer is there, but you may get hit by a bus before it kills you, can still inform some medical care decisions b) the 12 authors of that paper have probably put a bit of thought into the usefulness and efficacy of this kind of very early detection, and concluded it was worth reporting on their research reply gus_massa 5 hours agorootparent> b) the 12 authors of that paper have probably put a bit of thought into the usefulness and efficacy of this kind of very early detection, and concluded it was worth reporting on their research Nah. It's not how science is done. You fill a lot of paperwork to ask for money and then a lot of paperwork to ask for permision to use human, and after the study is finished you must publish whatever positive result you got. Otherwise you will never get more money, or will be fired, or your students will not finish their Ph.D., or never get a position. My guess is that they actuallly think that it's a important topic and that it's an important new tool. Everyone thinks that thir own topic is important. About the efficacy, it's an early study. They show that the method somewhat works and it may be a good idea to continue improving the method to get more accurate diagnosis, perhaps distingish the different types of cancer, or perhaps it's a dead end. About the usefulness, it's more difficult to evaluate. It's a decition that should be made by a team with a brader vision that can analyze the alternatives and the cost of each one. The research team has always a narrow vision. reply tombert 19 hours agorootparentprevI don't dispute either of your points. It was a genuine mere curiosity on my end, not a rhetorical \"gotcha!\". I've just heard that for stuff like prostate cancer, a diagnosis can be misleading, because sometimes it can take 20+ years to kill you. If you get it when you're 70, it's probably not worth going through surgery or chemo because you'll likely die of heart disease or another cancer before that anyway. reply thfuran 14 hours agorootparentThat sort of issue crops up all the time in medicine. Screening for conditions will always produce some false positives, and the ramifications can vary from scaring the shit out of someone for a few weeks before you determine they don't have cancer after all to them dying as a result of further testing/treatment that wouldn't have happened without that initial screening. That and the potential adverse outcomes of the screening itself (and its costs) always has to be weighed against the value of the true positives. reply snarf21 14 hours agoparentprevI think we also should be looking more at change over time, not just results as a distinct value. A lot of these values have ranges and error margins but looking at changes over time can be quite informative, especially for cases where your body takes care of any potential issues. I think there are a lot of diseases we could treat in cheaper ways with improved QoL if detected early. Too often we try to fix a crashed car instead of checking the brakes once a year. reply EspadaV9 20 hours agoprevI donated some blood a couple of years back and they came back saying there were proteins present. Having follow up tests at the moment, but none of the other markers are present that would normally be there if there was cancer. Still waiting to get further follow up tests, but no one seems to be worried enough to rush things along. reply HappyJoy 19 hours agoparentWhere did you donate? The only feedback I usually get is a certificate every 8 trips. reply EspadaV9 6 hours agorootparentThis was with the Red Cross in Australia. I'm original from the UK and was not allowed to donate before due to CJD worries. The first time I was able to donate, they ran a full test (not sure if they do that for all donations or not) and I got a call back saying they had found markers for blood cancer. As mentioned, follow up tests haven't shown any other markers, and the levels of protein hasn't changed over that time, so have a 6 month wait now for the next follow ups. reply Symmetry 19 hours agorootparentprevI donate platelets with the Red Cross every month and I've got an app where they give me the blood pressure and hemoglobin level, and which used to tell me if I had Covid-19 antibodies before everybody did via vaccine or infection. Before I donate I have to sign something that says, among other things \"We're going to test your blood for AIDS and tell you if you have it, so if you don't want to know don't donate\". I hadn't thought about the other things they test for but of course they don't want blood with Leukemia in it either. reply frontman1988 17 hours agorootparentWhy wouldn't someone not want to know they have AIDS? Given the disease is not a death sentence anymore and the earlier you know better your chances of survival. The warning probably deters a lot of people who could have otherwise been saved by timely treatment. reply vundercind 11 hours agorootparentYou still need to get consent for that. Like, you can’t just assume. reply trallnag 10 hours agorootparentprevMaybe it's part of the whole bug chasing phenomenon reply krisoft 18 hours agoparentprev> Still waiting to get further follow up tests, I hope all will turn out good for you, and wishing you the best of luck. > they came back saying there were proteins present. I think probably there is a bit of a Chinese whisper kind of misunderstanding here. Your blood will contain proteins. It must. Everyone's blood does. For example hemoglobin is a type of protein which makes your red blood cells able to carry oxygen. What they probably told you is that they found the wrong quantity or the wrong kind of proteins. Wishing you the best! reply EspadaV9 6 hours agorootparentYes, sorry, it was hightened levels of proteins. Checking the report and it has these highlighted in red Total Protein Initial Screen (g/L): 87 (normal range 61-84) Immunoglobulin IgA (g/L): 4.9 (normal rangecon una concentración de 600-1800 mg por 100 mL IANAMD. It looks like most of the time the increase can be produced by many common infections that are not dangerous. I strongly recomend to get a second opinion before starting some invasive procedure. reply dogtorwoof 20 hours agoparentprevWhich proteins? reply EspadaV9 6 hours agorootparentThe only information on the report is the following Total Protein Initial Screen (g/L): 87 (normal range 61-84) Immunoglobulin IgA (g/L): 4.9 (normal rangeLittle evidence for protein associations was observed in these data for cancers of the pancreas, thyroid, lip and oral cavity, or melanoma after correcting for multiple tests From the study https://www.nature.com/articles/s41467-024-48017-6 reply anotherpaulg 15 hours agoparentprevThe Galleri blood test claims to pick up pancreatic cancer early. Which I agree, is one which would be great to find early. > More aggressive cancers, such as pancreatic cancer, tend to release more cell-free DNA into the bloodstream at early stages and are more likely to be detected by the Galleri test. https://www.galleri.com/hcp/galleri-test-performance reply arrosenberg 12 hours agoparentprevCA 19-9 is the marker everyone is look at for Panc. reply xyst 9 hours agoprevScientists and researchers come up with something new but it’s held back for years because “insurance doesn’t cover it, yet” and cites some bs “experimental treatment/test” reason. It’s a shame the US doesn’t have any way to quickly order non-invasive tests. Tests often only ordered when you are symptomatic. Even in those cases, you often get the bare minimum. Have seen doctors personally and as an observing third party totally write them off. Instead of diagnosing, prescribe meds and treat symptomatically. reply srigi 13 hours agoprevIsn’t the cancer the “exponential game”? If there are 20 cancer cells in the body on day 1, on day 30 there will be 10737418240 (10.7B) cells if they double every day. This is how we were taught about cancer, so it is a very quick process when started. How can you get 7 years of ahead of time in this setup? reply arrosenberg 12 hours agoparentThey don't necessarily double every day. Some tumors are very stable, some are particularly metastatic. It often depends on access to blood supply and what type of cell has become cancerous. reply ekanes 20 hours agoprevCompared to most cancer-related findings, this seems clear, simple and easy to replicate / disprove. Hopefully quickly, as it'd be a huge win for humanity. reply jimbobthrowawy 13 hours agoprevHere's hoping this kind of screening becomes quick and cheap enough to do at home or regularly at a pharmacy. I'd like to get a rough estimate of how many moles I have at least once a year if it wasn't a huge effort. reply consf 13 hours agoprevEarly detection is crucial in improving outcomes for cancer patients, as it allows for timely treatment and intervention when the disease is most treatable. And these kinds of research are promising reply andrewmutz 15 hours agoprevYou can get these sorts of tests already. Last year I used this company's product and it was a smooth experience: https://www.galleri.com/ reply ak217 15 hours agoparentGrail's test is a cfDNA test. It detects DNA fragments in blood that are indicative of specific methylation patterns that are in turn indicative of possible cancerous growth. While a good approach, there are continued sensitivity challenges with cfDNA tests. This research is a high quality longitudinal retrospective study of protein cancer biomarkers, not cfDNA. Protein biomarkers are a complementary signal that has the potential to boost the sensitivity and precision of these tests, especially when the signals are combined together. reply macawfish 18 hours agoprevSo you can get seven years more of nocebo effect, anxiety, stress and worry? reply kazinator 19 hours agoprevSay we find some proteins in the blood that hint at cancer 7 years away. How is that actionable, and will it make a difference? How low is the false positive rate? reply tomoyoirl 18 hours agoparentA key benefit is that it might be able to perform follow up screenings that make sense for that type of cancer, rather than expecting absolutely everyone to take all the tests ever at the same rate, at significant inconvenience and expense. reply njarboe 17 hours agoprevDoes this mean we can get the 5 year cancer survival rate to 100%? reply anonzzzies 20 hours agoprevHope we found a lot of these type of things. 7 years is a big win. reply kemmishtree 13 hours agoprevi.e., Why We Need Utility-Scale Solid-State Molecular Sensing, Reason #53,444,001 reply blindriver 20 hours agoprevHow does this differ from the GRAIL blood tests? reply epistasis 19 hours agoparentGrail is looking at DNA methylation sites, this is looking at proteins. GRAIL is available today, this test will need to be validated and commercialized reply blindriver 7 hours agorootparentThank you reply forinti 20 hours agoprevAnd how are you going to find the source? reply _xerces_ 20 hours agoparentYou're right, these proteins could be associated with conditions that can later lead to mutations that cause cells to become cancerous, but they don't point to a specific cancer. Interestingly they hint at modifying the proteins to reduce the likelihood of getting something, but that could be risk in itself. A lifestyle change could be helpful in some cases. I suppose if you have enough of these markers they could schedule routine testing earlier than usual, say 40 instead of 45 for colonoscopy, mammogram, etc. reply nerdjon 20 hours agoprevAm I crazy in thinking that we heard about this several years ago? Is this just a continuation of that study or am I mis-remembering/my timeline way off? This is really exciting though, especially when mixed with other cancer treatments the ability to catch and deal with this is fascinating. How long until a theoretical, \"Oh we detected some cancer cells in your regular blood work, here is a shot to deal with it\" like we treat many other things. reply nextos 14 hours agoparentYou've been hearing about ctDNA. Which is really interesting and predictive. But it is hard to tune so that it is practical enough to be deployed in routine healthcare. Efforts so far have not been sensitive or specific enough. reply graywh 20 hours agoparentprevwe've been searching for blood biomarkers for cancer for decades -- I'm sure we've found several by now reply jemmyw 20 hours agoparentprevYou're not crazy, I recall a similar story too. reply yieldcrv 19 hours agoparentprevwith the addition of an mRNA treatment approach I'm willing to think 8 years off from at least a dozen cancers that currently only get detected after they've metastasized. reply m3kw9 17 hours agoprevYou detect it and then what? You get depressed and there is no treatment for something that early. Better to test for obvious signs every year based on susceptiblility. Maybe the depression and stress can make you more sick, let alone the effect on your family should you announce it reply goda90 17 hours agoparentImprove your health with better diet, exercise, sleep. Find and reduce carcinogen exposures. Give your immune system a chance to nip it in the bud before you even need treatment. reply stuff4ben 20 hours agoprev [–] How long before that becomes a mandatory test to get health insurance? Or somehow the data is bought by insurers who then use it to jack up the rates for those who will eventually get cancer. God I hate the US healthcare system! reply DennisP 20 hours agoparentInsurance companies haven't been allowed to deny coverage for preexisting conditions since 2014, due to Obamacare. They can't even raise premiums. The only variables are age, sex, smoking, and where you live. This applies both to employer plans and the ACA plans on healthcare.gov, where you can get a quote without giving them any health data at all. reply Koala_ice 19 hours agorootparentBut, you can get absolutely destroyed on life insurance and, just as critically, long-term care insurance. Genetic discrimination is perfectly legal in those domains. reply qclibre22 19 hours agorootparentprevYou can be denied some kinds of health insurance : https://www.medicarefaq.com/faqs/can-you-be-denied-a-medicar... reply DennisP 19 hours agorootparentOnly if you miss the open enrollment period, and even then, you still have Medicare regardless. It seems reasonable not to let people over 65 wait until they have cancer before purchasing supplemental insurance. reply bonton89 20 hours agorootparentprevIs there anything stopping an employer from pretesting you to avoid adding a potentially expensive employee to their roster? reply zamadatix 19 hours agorootparentGenerally it runs aground with things like the ADA because you can't just \"accidentally\" find out the person has a covered condition, you're just never allowed to ask or require it be told. Exceptions for something like an airline pilot on matters related to the job like sight notwithstanding. This is why you always see things like \"can lift up to 40 lbs\" type requirements instead. reply Tenoke 20 hours agoparentprev [–] This is such an odd take for me. If the test works and insurance makes the test mandatory (which seems pretty unlikely, what other comparable tests are mandatory?) then more people will get an early diagnosis, and less people will die! The incentives are aligned, the system works! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers at the University of Oxford identified 618 blood proteins linked to 19 types of cancer, with 107 related to cancers diagnosed over seven years later, indicating a potential for early cancer detection.",
      "Altering specific blood proteins could impact the risk of developing nine types of cancer, showing promise for influencing cancer prevention and early diagnosis.",
      "This discovery marks a significant advancement in pinpointing markers for heightened cancer risk and assisting in the early detection of cancer, as recognized by experts in the field."
    ],
    "commentSummary": [
      "Early detection in cancer screening is crucial, with benefits and risks of different testing methods discussed.",
      "The importance of enhanced healthcare resources, transparent decision-making processes, and lifestyle changes for cancer prevention is underlined.",
      "Challenges in creating practical blood biomarkers for cancer detection and legal safeguards against genetic discrimination in insurance coverage are also addressed."
    ],
    "points": 327,
    "commentCount": 121,
    "retryCount": 0,
    "time": 1715778668
  },
  {
    "id": 40371612,
    "title": "New Exponent Functions Boost SiLU and SoftMax Speed by 2x",
    "originLink": "https://github.com/ggerganov/llama.cpp/pull/7154",
    "originBody": "ggerganov / llama.cpp Public Notifications Fork 8.3k Star 58.3k Code Issues 353 Pull requests 211 Discussions Actions Projects 6 Wiki Security Insights New issue Jump to bottom ggml : rewrite silu and softmax for cpu #7154 Open jart wants to merge 1 commit into ggerganov:master base: master from jart:expf Open ggml : rewrite silu and softmax for cpu #7154 jart wants to merge 1 commit into ggerganov:master from jart:expf +283 −193 Conversation 6 Commits 1 Checks 61 Files changed 1 Conversation Contributor jart commented This change upstreams llamafile's vectorized expf() functions. This lets us compute softmax and silu more accurately than the short[65536] lookup table that GGML previously used to make this operation go faster. We can support aarch64 and sse2+ with the worst case rounding error of 2 ulp. I wrote avx2 and avx512 implementations as well but they didn't offer much advantage compared to sse2+fma to be worth the code complexity. 46 7 14 1 4 jart force-pushed the expf branch from e767397 to e8c8fd3 Compare Contributor github-actions bot commented • edited 📈 llama.cpp server for bench-server-baseline on Standard_NC4as_T4_v3 for phi-2-q4_0: 543 iterations 🚀 Expand details for performance related PR only jart force-pushed the expf branch from e8c8fd3 to 36d1a7a Compare Collaborator mofosyne commented • edited Not deeply analysing the changes but these are the general observation if it would help other reviewers: Commented out #define removed Extracted 5 duplicated lines into ggml_vec_soft_max_f32() Various functions relating to GGML_SILU_FP16 removed ggml_v_expf() added ggml_v_silu() added ggml_vec_silu_f32() adjusted with preprocessor statement to adjust function based on SSE2 or __ARM_NEON flag there are other changes... but these are the main things i noticed anyway... 1 ggerganov approved these changes View reviewed changes Owner ggerganov commented On AMD Ryzen 9 5950X and M2 Ultra SOFT_MAX is about ~1.5x faster than master Using the following command to benchmark: make -j tests && ./tests/test-backend-ops -o SOFT_MAX -b CPU perf 4 mofosyne added refactoring review complexity : high labels Contributor Author jart commented I'm glad to hear that. Here's the avx2 and avx512 variations if you want to try them out: inline __m256 llamafile_expf_avx2(__m256 x) { const __m256 r = _mm256_set1_ps(0x1.8p23f); const __m256 z = MADD256(x, _mm256_set1_ps(0x1.715476p+0f), r); const __m256 n = _mm256_sub_ps(z, r); const __m256 b = NMADD256(n, _mm256_set1_ps(0x1.7f7d1cp-20f), NMADD256(n, _mm256_set1_ps(0x1.62e4p-1f), x)); const __m256i e = _mm256_slli_epi32(_mm256_castps_si256(z), 23); const __m256 k = _mm256_castsi256_ps( _mm256_add_epi32(e, _mm256_castps_si256(_mm256_set1_ps(1)))); const __m256i c = _mm256_castps_si256( _mm256_cmp_ps(_mm256_andnot_ps(_mm256_set1_ps(-0.f), n), _mm256_set1_ps(126), _CMP_GT_OQ)); const __m256 u = _mm256_mul_ps(b, b); const __m256 j = MADD256(MADD256(MADD256(_mm256_set1_ps(0x1.0e4020p-7f), b,_mm256_set1_ps(0x1.573e2ep-5f)),u,MADD256(_mm256_set1_ps(0x1.555e66p-3f), b,_mm256_set1_ps(0x1.fffdb6p-2f))), u, _mm256_mul_ps(_mm256_set1_ps(0x1.ffffecp-1f), b)); if (!_mm256_movemask_ps(_mm256_castsi256_ps(c))) return MADD256(j, k, k); const __m256i g = _mm256_and_si256( _mm256_castps_si256(_mm256_cmp_ps(n, _mm256_setzero_ps(), _CMP_LE_OQ)), _mm256_set1_epi32(0x82000000u)); const __m256 s1 = _mm256_castsi256_ps(_mm256_add_epi32(g, _mm256_set1_epi32(0x7f000000u))); const __m256 s2 = _mm256_castsi256_ps(_mm256_sub_epi32(e, g)); const __m256i d = _mm256_castps_si256( _mm256_cmp_ps(_mm256_andnot_ps(_mm256_set1_ps(-0.f), n), _mm256_set1_ps(192), _CMP_GT_OQ)); return _mm256_or_ps( _mm256_and_ps(_mm256_castsi256_ps(d), _mm256_mul_ps(s1, s1)), _mm256_andnot_ps( _mm256_castsi256_ps(d), _mm256_or_ps( _mm256_and_ps(_mm256_castsi256_ps(c), _mm256_mul_ps(MADD256(s2, j, s2), s1)), _mm256_andnot_ps(_mm256_castsi256_ps(c), MADD256(k, j, k))))); } inline __m512 llamafile_expf_avx512(__m512 x) { const __m512 r = _mm512_set1_ps(0x1.8p23f); const __m512 z = MADD512(x, _mm512_set1_ps(0x1.715476p+0f), r); const __m512 n = _mm512_sub_ps(z, r); const __m512 b = NMADD512(n, _mm512_set1_ps(0x1.7f7d1cp-20f), NMADD512(n, _mm512_set1_ps(0x1.62e4p-1f), x)); const __m512i e = _mm512_slli_epi32(_mm512_castps_si512(z), 23); const __m512 k = _mm512_castsi512_ps( _mm512_add_epi32(e, _mm512_castps_si512(_mm512_set1_ps(1)))); const __mmask16 c = _mm512_cmp_ps_mask(_mm512_abs_ps(n), _mm512_set1_ps(126), _CMP_GT_OQ); const __m512 u = _mm512_mul_ps(b, b); const __m512 j = MADD512(MADD512(MADD512(_mm512_set1_ps(0x1.0e4020p-7f), b,_mm512_set1_ps(0x1.573e2ep-5f)),u,MADD512(_mm512_set1_ps(0x1.555e66p-3f), b,_mm512_set1_ps(0x1.fffdb6p-2f))), u, _mm512_mul_ps(_mm512_set1_ps(0x1.ffffecp-1f), b)); if (_mm512_kortestz(c, c)) return MADD512(j, k, k); const __m512i g = _mm512_and_si512( _mm512_movm_epi32(_mm512_cmp_ps_mask(n, _mm512_setzero_ps(), _CMP_LE_OQ)), _mm512_set1_epi32(0x82000000u)); const __m512 s1 = _mm512_castsi512_ps(_mm512_add_epi32(g, _mm512_set1_epi32(0x7f000000u))); const __m512 s2 = _mm512_castsi512_ps(_mm512_sub_epi32(e, g)); const __mmask16 d = _mm512_cmp_ps_mask(_mm512_abs_ps(n), _mm512_set1_ps(192), _CMP_GT_OQ); return _mm512_mask_blend_ps( d, _mm512_mask_blend_ps(c, MADD512(k, j, k), _mm512_mul_ps(MADD512(s2, j, s2), s1)), _mm512_mul_ps(s1, s1)); } Here's the numbers I got with the script I used for developing these functions: 2.98601 ns 2000x run_expf() 1.35154 ns 2000x run_llamafile_expf_sse2() 1.16659 ns 2000x run_llamafile_expf_avx2() 1.18844 ns 2000x run_llamafile_expf_avx512() // input exp llamafile bad // ===== === ========= === // 0 1 1 0 // -0 1 1 0 // nan nan nan 0 // -nan -nan -nan 0 // inf inf inf 0 // -inf 0 0 0 // 87 6.07603e+37 6.07603e+37 1 // 88 1.65164e+38 1.65164e+38 0 // 88.7229 inf inf 0 // 89 inf inf 0 // -87 1.64581e-38 1.64581e-38 1 // -90 8.19401e-40 8.19401e-40 0 // -95 5.52112e-42 5.52112e-42 0 // -100 3.78351e-44 3.78351e-44 0 // -104 0 0 0 // 0.660001 1.93479 1.93479 1 // -0.324231 0.723083 0.723083 0 // 0.0205384 1.02075 1.02075 0 // -0.224604 0.798833 0.798833 1 // -0.339606 0.712051 0.71205 1 // 0.211472 1.2355 1.2355 0 // 0.238942 1.2699 1.2699 0 // -0.78286 0.457097 0.457097 0 4294967296 numbers tested successfully 4 jart mentioned this pull request perplexity: add BF16 vs. FP16 results #7150 Merged jart force-pushed the expf branch from 36d1a7a to a43cbb1 Compare Contributor Author jart commented @ggerganov Running your command, I'm noticing the advantage here increases from 1.5x to 1.9x if we include AVX2. On znver4 if we also include avx512 then that goes up to 2.1x. I'd expect that to go higher in the future, since znver4 only really implements the AVX512 ISA and uses 2 cycles for each vector operation. So I've gone ahead and included the code for you. 4 jart force-pushed the expf branch from a43cbb1 to fde7c59 Compare ggml : rewrite silu and softmax for cpu … d7359a3 This change upstreams llamafile's vectorized expf() functions. This lets us compute softmax and silu more accurately than the short[65536] lookup table that GGML previously used to make this operation go faster. We can support aarch64 and sse2+ with the worst case rounding error of 2ulp. It makes make -j8 tests && ./tests/test-backend-ops -o SOFT_MAX -b CPU perf go 1.5x faster for SSE2+FMA, 1.9x faster for AVX2+FMA and 2.1x on AVX512 jart force-pushed the expf branch from fde7c59 to d7359a3 Compare ggerganov added the merging soon label jacky1234 mentioned this pull request HackerNews Top 10 @2024-05-16 jacky1234/blogPages#421 Open xueyuanl mentioned this pull request Daily Hacker News 16-05-2024 xueyuanl/daily-hackernews#1348 Open chriselrod commented • edited With AVX512, you may want to use vscalefps. It computes zmm0 = zmm1 * 2^{zmm2}, where all are floats. It overflows and underflows properly, letting you remove checks + blends. I have an implementation in Julia, e.g. a loop with 4x unrolling and interleaving. L304:vmovups zmm15, zmmword ptr [r11 + 4*rax]vmovups zmm14, zmmword ptr [r11 + 4*rax + 64]vmovups zmm13, zmmword ptr [r11 + 4*rax + 128]vmovups zmm12, zmmword ptr [r11 + 4*rax + 192]vmovaps zmm16, zmm1vfmadd213ps zmm16, zmm15, zmm0 # zmm16 = (zmm15 * zmm16) + zmm0vmovaps zmm17, zmm1vfmadd213ps zmm17, zmm14, zmm0 # zmm17 = (zmm14 * zmm17) + zmm0vmovaps zmm18, zmm1vfmadd213ps zmm18, zmm13, zmm0 # zmm18 = (zmm13 * zmm18) + zmm0vmovaps zmm19, zmm1vfmadd213ps zmm19, zmm12, zmm0 # zmm19 = (zmm12 * zmm19) + zmm0vaddps zmm16, zmm16, zmm2vaddps zmm17, zmm17, zmm2vaddps zmm18, zmm18, zmm2vaddps zmm19, zmm19, zmm2vfmadd231ps zmm15, zmm16, zmm3 # zmm15 = (zmm16 * zmm3) + zmm15vfmadd231ps zmm14, zmm17, zmm3 # zmm14 = (zmm17 * zmm3) + zmm14vfmadd231ps zmm13, zmm18, zmm3 # zmm13 = (zmm18 * zmm3) + zmm13vfmadd231ps zmm12, zmm19, zmm3 # zmm12 = (zmm19 * zmm3) + zmm12vfmadd231ps zmm15, zmm16, zmm4 # zmm15 = (zmm16 * zmm4) + zmm15vfmadd231ps zmm14, zmm17, zmm4 # zmm14 = (zmm17 * zmm4) + zmm14vfmadd231ps zmm13, zmm18, zmm4 # zmm13 = (zmm18 * zmm4) + zmm13vfmadd231ps zmm12, zmm19, zmm4 # zmm12 = (zmm19 * zmm4) + zmm12vmovaps zmm20, zmm6vfmadd213ps zmm20, zmm15, zmm5 # zmm20 = (zmm15 * zmm20) + zmm5vmovaps zmm21, zmm6vfmadd213ps zmm21, zmm14, zmm5 # zmm21 = (zmm14 * zmm21) + zmm5vmovaps zmm22, zmm6vfmadd213ps zmm22, zmm13, zmm5 # zmm22 = (zmm13 * zmm22) + zmm5vmovaps zmm23, zmm6vfmadd213ps zmm23, zmm12, zmm5 # zmm23 = (zmm12 * zmm23) + zmm5vfmadd213ps zmm20, zmm15, zmm7 # zmm20 = (zmm15 * zmm20) + zmm7vfmadd213ps zmm21, zmm14, zmm7 # zmm21 = (zmm14 * zmm21) + zmm7vfmadd213ps zmm22, zmm13, zmm7 # zmm22 = (zmm13 * zmm22) + zmm7vfmadd213ps zmm23, zmm12, zmm7 # zmm23 = (zmm12 * zmm23) + zmm7vfmadd213ps zmm20, zmm15, zmm8 # zmm20 = (zmm15 * zmm20) + zmm8vfmadd213ps zmm21, zmm14, zmm8 # zmm21 = (zmm14 * zmm21) + zmm8vfmadd213ps zmm22, zmm13, zmm8 # zmm22 = (zmm13 * zmm22) + zmm8vfmadd213ps zmm23, zmm12, zmm8 # zmm23 = (zmm12 * zmm23) + zmm8vfmadd213ps zmm20, zmm15, zmm9 # zmm20 = (zmm15 * zmm20) + zmm9vfmadd213ps zmm21, zmm14, zmm9 # zmm21 = (zmm14 * zmm21) + zmm9vfmadd213ps zmm22, zmm13, zmm9 # zmm22 = (zmm13 * zmm22) + zmm9vfmadd213ps zmm23, zmm12, zmm9 # zmm23 = (zmm12 * zmm23) + zmm9vfmadd213ps zmm20, zmm15, zmm10 # zmm20 = (zmm15 * zmm20) + zmm10vfmadd213ps zmm21, zmm14, zmm10 # zmm21 = (zmm14 * zmm21) + zmm10vfmadd213ps zmm22, zmm13, zmm10 # zmm22 = (zmm13 * zmm22) + zmm10vfmadd213ps zmm23, zmm12, zmm10 # zmm23 = (zmm12 * zmm23) + zmm10vfmadd213ps zmm20, zmm15, zmm11 # zmm20 = (zmm15 * zmm20) + zmm11vfmadd213ps zmm21, zmm14, zmm11 # zmm21 = (zmm14 * zmm21) + zmm11vfmadd213ps zmm22, zmm13, zmm11 # zmm22 = (zmm13 * zmm22) + zmm11vfmadd213ps zmm23, zmm12, zmm11 # zmm23 = (zmm12 * zmm23) + zmm11vfmadd213ps zmm20, zmm15, zmm11 # zmm20 = (zmm15 * zmm20) + zmm11vfmadd213ps zmm21, zmm14, zmm11 # zmm21 = (zmm14 * zmm21) + zmm11vfmadd213ps zmm22, zmm13, zmm11 # zmm22 = (zmm13 * zmm22) + zmm11vfmadd213ps zmm23, zmm12, zmm11 # zmm23 = (zmm12 * zmm23) + zmm11vscalefps zmm12, zmm20, zmm16, {rn-sae}vscalefps zmm13, zmm21, zmm17, {rn-sae}vscalefps zmm14, zmm22, zmm18, {rn-sae}vscalefps zmm15, zmm23, zmm19, {rn-sae}vmovups zmmword ptr [r14 + 4*rax], zmm12vmovups zmmword ptr [r14 + 4*rax + 64], zmm13vmovups zmmword ptr [r14 + 4*rax + 128], zmm14vmovups zmmword ptr [r14 + 4*rax + 192], zmm15add rax, 64cmp rax, r10jl L304 These gave me a significant performance improvement. If my test is correct, I got a maximum error <1 ULP at x=47.483456f. What hardware are you on? I'm using skylake-avx512/cascadelake with 2x fma units. Zen4 or something like icelake-client/tigerlake likely won't benefit as much. Note that it doesn't use a lookup table. My Float64/double implementation uses a 16-element lookup table via vpermi2pd. If we wanted, we could use a 32-element lookup table of floats via the same approach. vpermi2pd is much faster than gather, the cost of course being that our table has to fit into two registers. Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Reviewers ggerganov Assignees No one assigned Labels merging soon refactoring review complexity : high Projects None yet Milestone No milestone Development Successfully merging this pull request may close these issues. None yet 4 participants",
    "commentLink": "https://news.ycombinator.com/item?id=40371612",
    "commentBody": "New exponent functions that make SiLU and SoftMax 2x faster, at full accuracy (github.com/ggerganov)310 points by weinzierl 14 hours agohidepastfavorite58 comments mjcohen 11 hours agoAbout 20 years ago, I was programming for the Hughes radar signal processor, a highly parallel pipelined machine which accounted for much of Hughes success in radar processing. Anyway, I needed to compute e^x for 0e^(a adapted from arm limited optimized routine Shoulders of giants and all that reply larodi 9 hours agoparentprevIts not something that they teach at asymptotic analysis lectures, right? which reminds me of professor who famously said - that constant that everyone's just overlooking can in engineering terms very much eat your head. reply boshalfoshal 8 hours agorootparentUnrelated but I see this sentiment a lot. Algorithms (and related courses) are supposed to be largely mathematical/theory based so it sort of has to be hardware agnostic. Imo an algorithms course doesn't really need to teach you about lowering the constant factor (unless the optimizations for lowering said factor are reasonably abstracted away from physical HW), thats the purpose of Computer Architecture or Operating Systems course. And realistically in the real world you'd be benchmarking these things anyway if a supposedly \"optimal\" algorithm is really the bottleneck, and you'd apply context specific optimizations to speed it up. reply hinkley 5 hours agorootparentprevA big C could be $3M in losses per year vs $2M in profits. Or always coming in second on contracts or sales and getting $0. reply neonsunset 11 hours agoparentprevMostly looks scary* because that's just how it is with intrinsics syntax in C and C++. As with many things there, this pain is mostly self-inflicted. There are C++ libraries that allow for C#-style SIMD and hardware instrinsics syntax as far as I'm aware. It comes at a disadvantage as you can't directly lookup mnemonics in ISAs documentation though. *not to seem as if dismissing the importance of the work done there, just highlighting that it could have been much more accessible to wider audience, but I'm not gonna suggest something that everyone here would consider preposterous such as rewriting inference back-end in C# just yet reply mhh__ 12 hours agoprev> replaces short[65536] look up table Is that not quite dim to begin with (having a LUT the size of the whole L1 cache?) or does it work surprisingly well because of some probabilistic fudging? reply Tuna-Fish 11 hours agoparentThe lookup table does surprisingly well because the workload is otherwise extremely cache-hostile, and it doesn't really matter if you blow up your L1 cache, none of the data you evicted because you needed to fit the LUT was ever going to be reused anyway. ML loads in general are streaming loads that linearly load the entire dataset for every iteration. reply pclmulqdq 10 hours agorootparentOne interesting option for these big memory scans on x86 and ARM CPUs is using the non-temporal load/store instructions. Those actually bypass caching* and may help with the cache pressure of LLM workloads that just do scans. The lookup table is still probably the wrong solution even with this sort of thing. * Not quite all of it - There are still buffers to do write combining and some read caching on scans. reply andy99 11 hours agoparentprevWhy you (probably) shouldn't use a lookup table https://specbranch.com/posts/lookup-tables/ gives some discussion about when it's appropriate generally. My narrow experience is that you can do a lot of real time calculation before it's faster to do a lookup. reply o11c 11 hours agorootparentOne case that doesn't mention explicitly: sometimes your problem is lucky enough that you have the choice between: 1. for each function, for each datum, call function with datum 2. for each datum, for each function, call function with datum For 1, if your main data is fetched predictably but does not saturate bandwidth, it can be beneficial to use even a fairly large LUT. Note also that while actual L1i is semi-ignorable, e.g. branch prediction benefits much more from remaining \"in cache\". For 2, assuming other functions also have their own miscellaneous data, the non-LUT approach is likely to win. But you should probably benchmark against #1. But sometimes you can get the best of both worlds: 3. for each arbitrary chunk of data (smaller than L3, or maybe even L2 depending on when your throughput saturates), for each function, for each datum in the chunk, call function with datum Yes, you should of course do whole-system benchmarks, but being able to guess helps you write a useful implementation to benchmark in the first place. reply pclmulqdq 10 hours agorootparentIn the two cases you have specified here, #2 is almost always the winner on performance. So much so that in performance-sensitive code, many people will (justifiably) default to it without a benchmark. Computers almost always operate in a memory bandwidth bound state, and have comparatively idle cores, and #1 is likely to just be wasteful of the resource that will almost always be the binding constraint. Examples are ECS systems in games, and async run-to-completion runtimes on servers. HPC systems also tend to operate this way. Also, in the interest of disclosure, I wrote the blog post you are responding to. reply anonymoushn 6 hours agorootparentprevFor some parsing, serialization, or filtering tasks, you end up with quite large lookup tables of shuffles that would otherwise be pretty costly to compute on the fly, but real workloads hit only a tiny part of the lookup table at a time. (This comment fits entirely within his point about benchmarks, I guess) reply a_wild_dandan 12 hours agoprev(in llama.cpp, for CPU) reply jart 12 hours agoparentI developed this originally for llamafile, which was included in the last two releases: https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8.2 Now we're upstreaming it to the llama.cpp project. There are other performance enhancements you can currently only get from llamafile, such as Kawrakow's work making K quants go much faster. reply breakingcups 12 hours agorootparentIs that just because nobody has made an effort yet to port them upstream, or is there something inherently difficult about making those changes work in llama.cpp? reply pclmulqdq 9 hours agorootparentprevWait, computing SiLU directly using some numerical analysis is probably a lot faster than doing an exp each time. Is there a significant perf impact to doing this? reply jart 9 hours agorootparentWith expf() most of the work had already been done and I could kill two birds with one stone. If you want to do the math for doing SiLU directly, that'd be an awesome change I'd happily merge into llamafile. You might even be able to get that into PyTorch and even bigger name projects. reply koe123 13 hours agoprevPerhaps somewhat off topic, does anyone know how stuff like ggml compares to runtimes (tensorflow lite, onnxruntime, etc.)? reply ilaksh 13 hours agoparentOn what hardware exactly? reply koe123 11 hours agorootparentProbably should have specified that! I’m referring to cpu inference. reply ilaksh 10 hours agorootparentOn what CPU? reply koe123 2 hours agorootparentTypically I'm considering embedded applications (armv8, armv7) reply refulgentis 12 hours agoparentprevI'm intimately familiar, maintain ONNX and llama.cpp Flutter libraries across all 6 True Platforms. Quick opinionated TL;DR: - llama.cpp for LLMs and can do whisper with it's core dependency, GGML. - ONNX for everything else. - TF is the Apple of ML, it's great if you're completely wedded to Google ML ecosystem. Virtually dead outside that. (Something absurd ,like, 94%, of HF models are Pytorch) - only chance I'd have to do a direct comparison in inference performance is Whisper in ONNX vs. GGML, someone got my llama .cpp lib running with Whisper and didn't report significant perf difference reply catgary 11 hours agorootparentI don’t think that’s a terribly fair description of Google - AWS’s chips (inferon and trainium) both have robust XLA/JAX support. Plus JAX now exports MLIR, so there is a really compelling JAX -> IREE pipeline so JAX models can more or less be deployed anywhere, even on bare metal embedded devices. reply refulgentis 11 hours agorootparentYou're right, if you need to go from data => model running in web app, I'd do TF - the inartful Apple analogy is meant to indicate that: great vertical integration. For local inference of an existing model, TF Lite pales in comparison to ONNX. ONNX goes out of its way for you get ~anything running ~anywhere on the best accelerator available on the platform.* AFAIK TF Lite only helps if your model was in TF. And there simply isn't an LLM scene for TensorFlow, so it \"loses\" to llama.cpp for that. There isn't an ONNX LLM scene either, though. (see below) * There's one key exception...until recently...LLMs! ONNX's model format was limited due to protobuf, IIRC it was 2-4 GB. Part of the Phi-3 announcement was this library they've been stubbing out that's on top of ONNX, but more specialized for LLMs. That being said, haven't seen any LLMs in it except Phi-3, and it's an absolute mess, the library was announced weeks ahead of when it was planned to be released, and then throw in the standard 6-week slippage, I'm probably not trying it again until June. reply catgary 10 hours agorootparentI didn’t mention tensorflow or TF Lite? IREE is a different technology, it is essentially a compiler/VM for MLIR. Since JAX exports MLIR, you can run JAX on any platform supported by IREE (which is basically any platform from embedded hardware to datacenter GPUs). It is less mature than ONNX at this point, but much more promising when it comes to deploying on edge devices. reply refulgentis 10 hours agorootparent> I didn’t mention tensorflow or TF Lite I know. OP didn't mention JAX, or IREE. I avoided having a knee-jerk \"Incorrect, the thing you are talking about is off-topic\" reaction because it's inimical to friendly conversation. Mentioning ONNX and llama.cpp made it clear they were talking about inference. In that context, TF Lite isn't helpful unless you have TF. JAX is irrelevant. IREE is expressly not there yet[1] and has nothing to do with Google. [1] c.f. the GitHub \"IREE is still in its early phase. We have settled down on the overarching infrastructure\". From a llama.cpp/ONNX/local inference perspective, there's little more than \"well, we'll get MLIR, then we can make N binaries for N model x arch x platform options.\" Doesn't sound so helpful to me, but I got it easy right now, models are treated as data instead of code in both ONNX and llama.cpp. I'm not certain that's the right thing long-term, ex. it incentivizes \"kitchen sink\" ML frameworks, people always want me to add a model to the library, rather than use my library as a dependency. reply catgary 9 hours agorootparentSpeaking as someone who deploys ML models on edge devices - having the MLIR and being able to make binaries for different platforms is terrifically useful! You’re often supporting a range of devices (e.g. video game consoles, phones, embedded, etc) and want to tune the compilation to maximize the performance for your deployment target. And there are a lot of algorithms/models in robotics and animation that simply won’t work with ONNX as they involve computing gradients (which torch cannot export, and the gradients ONNXRuntime give you only work in training sessions, which aren’t as widely). supported. Also, if you have JAX you can get TF and therefore TFLite. And IREE is part of the OpenXLA project, which Google started. reply a_wild_dandan 11 hours agorootparentprevWhat’re your thoughts on MLX? It’s been phenomenal on my MBP. reply refulgentis 11 hours agorootparentNo time* to try it unfortunately :( Sounds great, though, and Mac just kicks the pants of every other platform on local inference thanks to Metal, I imagine MLX must extend that lead to the point Qualcomm/Google has to a serious investment in open source acceleration. Cheapest iPhone from 2022 kicks the most expensive Android from 2023 (Pixel Fold) around the block, 2x on inference. 12 tkns/s vs. 6/s. * it sounded like a great idea to do an OpenAI LLM x search app. Then it sounded like a great idea to add embeddings locally for privacy (thus, ONNX). Then it sounded like a great idea to do a local LLM (thus, llama.cpp). Then it sounded like a great idea to differentiate by being on all platforms, supported equally. Really taxing. Think I went too far this time. It works but, jeez, the workload...hopefully, after release, it turns out maintenance load is relatively low reply svnt 12 hours agorootparentprevWhat are the True Platforms, in this case? reply crthpl 12 hours agorootparentI'm guessing MacOS, Linux, Android, Windows, iOS, and Web? reply refulgentis 11 hours agorootparentCorrect reply petermcneeley 7 hours agoprevOne can vectorize LUTs as well https://www.intel.com/content/www/us/en/docs/intrinsics-guid... I wrote about the kinds of things that are possible with LUTs awhile back https://darkcephas.blogspot.com/2018/10/validating-utf8-stri... reply boulos 6 hours agoparentYes, but a direct `exp` implementation is only like 10-20 FMAs depending on how much accuracy you want. No gathering or permuting will really compete with straight math. reply rdevulap 5 hours agoprevOn similar lines, faster tanh https://github.com/microsoft/onnxruntime/pull/20612 reply jart 4 hours agoparentGreat work. But what's their goal? Are they trying to make that GeLU approximation go faster? Things would probably go a lot faster going back to the erff(). reply DeathArrow 3 hours agoprevWith all such optimizations, isn't llama too slow to be run on a CPU with a reasonable amount of parameters? reply jart 3 hours agoparentI'm gpu poor so I use cpu to run large models. For example, llamafile running mixtral 8x22b on my threadripper can chew through long legal documents and give me advice in a few minutes, using f16 weights and kahan summation. Show me someone who has that many graphics cards. reply baq 3 hours agoparentprevModels get better for equal parameters, CPUs get faster and good folks at intel, amd and arm are probably working very hard to catch up with apple silicon in terms of memory architecture. I can see this be very relevant in a couple of years. reply nxpnsv 3 hours agoparentprevI don't see how that conclusion follows from this optimization. reply lxe 11 hours agoprevAt this point, is gguf/llama.cpp a more performant solution for unbatched inference on CUDA devices, or is exllamav2+flashattention still reigning supreme? reply GuuD 11 hours agoparentThe difference is negligible on 2x 4090. There are more important differences like 4 bit KV cache. reply KaoruAoiShiho 11 hours agoprev [–] This does help the gguf usecase of partial offloading to GPU? It'll help the CPU part be faster too? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The llama.cpp repository on GitHub boasts 58.3k stars and 8.3k forks.",
      "Contributor jart is enhancing the silu and softmax functions for the CPU, incorporating AVX512 and AVX2 instructions for exponential value calculations, showing improved performance in benchmarks with less than 1 ULP error.",
      "The author highlights the use of vpermi2pd for Float64/double lookup tables and notes that hardware like Zen4 or icelake-client/tigerlake might not see as much benefit, with the constraint that the table must fit into two registers."
    ],
    "commentSummary": [
      "New exponent functions are created to accelerate SiLU and SoftMax calculations while maintaining accuracy, drawing from experiences with a radar signal processor.",
      "Implementation optimizes exponentiation computations through lookup tables, leveraging SIMD and hardware intrinsics in C and C++.",
      "Discussion covers non-temporal load/store instructions, lookup table limitations, performance variances in data processing, performance enhancements for llama.cpp, ML runtime comparisons, TensorFlow Lite and ONNX for model deployment, robotics, animation, GPU model impacts, CPU speed enhancements, and memory architecture advancements, with a recommendation for benchmarking to identify the most effective approach for specific tasks."
    ],
    "points": 309,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1715803021
  },
  {
    "id": 40369467,
    "title": "Title: \"Exploring Datomic Pro 1.0.7075: Concurrency and Transactional Correctness\"",
    "originLink": "https://jepsen.io/analyses/datomic-pro-1.0.7075",
    "originBody": "1 Background Datomic is a general-purpose database intended for systems of record. In many ways, Datomic is unusual. At any instant in time, the state of the database is represented by a set of [entity, attribute, value] (EAV) triples, known as datoms. Each datom declares that some entity (like a person) has a particular attribute (like a name) with a specific value (like “Vidrun”). The types and cardinality of attributes are controlled by a schema. Datomic is also a temporal database: it models time explicitly. Every transaction is identified by a strictly monotonic logical timestamp t, as well as a wall-clock time txInstant. Transactions can assert a datom, adding it to the database, or they can retract a datom, removing it from the database. Every datom also retains a reference to the transaction that asserted or retracted it. A full datom is therefore a five-tuple of [entity, attribute, value, transaction, asserted-or-retracted?]. The database is an ever-growing set of these tuples.1 Users can request a snapshot state of the database at any logical or wall-clock time—right now or years in the past. They can also obtain a full view of the database’s history, allowing users to ask questions like “was there ever a time when Janelle Monáe and Cindi Mayweather were recorded in the same room together?” Given a state of the database, users may query it via a Datalog-style API, a declarative graph traversal API, or an ODM-style Entity datatype which allows lazy access to an entity’s associated values, including other entities. Datomic comes in two flavors. In this report we discuss Datomic Pro, which anyone can run on their own computers. Datomic Cloud runs in AWS and uses a somewhat different architecture. 1.1 Architecture Datomic Pro comprises several co-operating services. Transactors execute write transactions, maintain indices, and write data to storage. Peers are thick clients: they embed a JVM library which submits transactions to transactors, executes read queries against storage, and caches results. For applications written in other languages, Datomic also has a traditional client-server model. Clients are thin clients which forward transactions and queries to a peer server: a peer which runs a small network API. Internally, Datomic appends each transaction to the log: a time-ordered set of transactions. From the log Datomic maintains four indices sorted by different permutations of entity, attribute, value, and time. These indices allow efficient queries like “which entities were modified yesterday,” or “who run the world?”2 Both log and indices are stored as persistent, immutable trees in a data store like Cassandra or DynamoDB. Because tree nodes are immutable, their backing storage only needs to guarantee eventual consistency. A small pointer to the roots of these trees provides a consistent, immutable snapshot of the database’s state. To commit a transaction, a transactor saves new immutable tree nodes to storage, then executes a compare-and-set (CaS) operation to advance the root pointer. This CaS operation must execute under Sequential consistency. Using a Sequential CaS operation ensures a global order of transactions, and limits Datomic’s write throughput to the speed of a single transactor. To reduce contention, Datomic tries to have a single active transactor at all times. Operators typically deploy multiple transactors for fault tolerance. Peers connect directly to storage, and also to transactors. Transactions are forwarded to an active transactor, which executes them. Each peer also maintains a local, monotonically-advancing copy of the root pointer, which allows the peer to read tree nodes from storage. Since tree nodes are immutable, they can be trivially cached. There may be any number of peers, allowing near-linear read scalability. 1.2 Transaction Model Datomic has an unusual transaction model. Most OLTP databases offer interactive transactions: one begins a transaction, submits an operation, receives results from that operation, submits another, and so on before finally committing. Some databases, like VoltDB, use stored procedures: an operator writes a small program which is installed in the database. Clients invoke that program by name, which mutates database state and returns values to the client. Other databases like FaunaDB allow clients to directly submit miniature programs as text or an abstract syntax tree. Like stored procedures, these programs perform arbitrary reads and writes, mutate state, and return data to the user. Datomic does something rather different. It enforces a strict separation between read and write paths. There are no interactive transactions. It has stored procedures, but they cannot return values to the caller. A read obtains an immutable state of the entire database. For instance, the db function returns the most recent database state3 the peer is aware of. To obtain the most recent state across all peers, or a state later than a given time, one calls d/sync. To obtain a state from a past time (seconds or years ago), one calls d/as-of. These states are cheap, highly cacheable, and never block other writers or readers. Given a database state, one can run any number of queries using (e.g.) q or pull. Queries lazily fetch datoms from cache or storage. Since database states are immutable, any number of queries run against the same state occur at the exact same logical time. In this sense, all queries run on the same state take place in a single atomic transaction—even two queries executed on different machines, months apart. Write transactions4 are represented as an ordered list of operations.5 A transaction is simply a list of lists and/or maps, each of which is a statement in the transaction. For example, here is a transaction of three operations, all involving entity 123: [[:db/add 123 :person/name \"N. K. Jemisin\"] [:db/cas 123 :author/hugo-count 2 3]] [:author/add-book 123 \"The Stone Sky\"]] Those operations may be simple assertions (:db/add) or retractions (:db/retract) of datoms, or they may be calls to transaction functions: either built-in or user-defined. In this example, the built-in db/cas function performs a CaS operation, asserting the number of Hugo awards for this author is 3 if and only if that number is currently 2. One can also store a function (represented as a Clojure AST or Java string) in the database just like any other value. Alternatively, one may write a function in any JVM language, and provide it in a jar file on the transactor’s classpath. Once a function has been installed, any transaction may invoke it by providing the function’s name and arguments. Here, the author/add-book function receives the state of the database as of the start of the transaction, as well as any arguments from the transaction. It can perform arbitrary (pure) computation, including running queries against the database state. It then returns a new set of operations for the transaction—for instance, assertions, retractions, or calls to other functions. Function calls are recursively expanded until only assertions and retractions remain. While transaction functions can make decisions based on the results of reads they perform internally, there is no channel to return those reads (or other information) to the caller of transact. Transactions only return effects. This means there is no direct analogue for an arbitrary read-write transaction in Datomic! For example, you can write a function which performs a conditional write,6 but you can’t inform the caller whether the write took place or not. This constraint nudges Datomic users towards pulling reads out of the write transaction path—a key factor in obtaining good performance from a system which can logically execute only one write transaction at a time. Instead of offering arbitrary return values from transactions, every call to transact returns the database state just before the transaction, the database state the transaction produced, and the set of datoms the transaction expanded to. This allows callers to execute read-write transactions by splitting them in twain: they submit a transaction which performs some writes, then use the pre-state of the database to determine what data that transaction would have read. Peers can also examine the post-state and set of datoms produced by the transaction to (e.g.) determine whether a conditional write took place. From the perspective of traditional database systems, this sounds absurd. Mixed read-write transactions are a staple of OLTP workloads—how could you get anything done without them? Datomic offers a view of an alternate universe: one where database snapshots are cheap, efficient, and can be passed from node to node with just a timestamp. From this point of view, other databases feel impoverished. What do you mean, Postgres can’t give you the state of the entire database a transaction observed? The lack of a return channel for transaction functions may be annoying, but Datomic’s other strengths generally allow it to solve the same kinds of problems as a traditional, interactive transaction system. For example, NuBank (Datomic’s current developers) offers financial services to nearly 94 million users, processing an average of 2.3 billion user transactions per day. Almost all of their products use Datomic as a system of record. 1.3 Consistency Datomic advertises ACID transactions and means it: their ACID documentation makes detailed, specific promises with respect to consistency models and durability guarantees. Transactions are “written to storage in a single atomic write,” which precludes intermediate read anomalies. Every peer “sees completed transactions as of a particular point in time,” and observes all transactions, totally ordered, up to that time. Transactions are always flushed to durable storage before client acknowledgement. When our analysis began in early January 2024, Datomic’s documentation informally claimed write transactions were Serializable: The Isolation property ensures that concurrent transactions result in the same system state that would result if the transactions were executed serially. Since write transactions are Serializable and execute atomically, and since read-only queries execute against committed snapshots, it seems plausible that histories of both read and write transactions should also be Serializable.7 Serializability does not impose real-time or session ordering constraints: in a Serializable system, it is legal for a client to execute a transaction which inserts object x, then execute a second transaction which fails to observe x. While Datomic’s documentation does not make this claim, it seems plausible that Datomic’s transactor design might provide Strong Serializability over write transactions, preventing real-time anomalies. Since d/db returns an asynchronously updated copy of the database, we expect peers to observe stale reads. Indeed, Datomic is explicit that peer reads may not observe some recently committed transactions. However, it would be straightforward for peers to ensure that their time basis advances monotonically; if we say that every session is bound to a single peer node, we would expect to observe Strong Session Serializable histories. In addition to these possible realtime and session constraints, Datomic has multiple synchronization mechanisms. Clients can block until they observe a value of the database at or later than some time t. This enables clients to ensure consistency when threading state through side channels. Calling d/sync forces the client to synchronize with the transactor, preventing stale reads. We expect histories which always use d/sync to be Strict Serializable as well. Datomic’s documentation also described it as a “single-writer” system: A single thread in a single process is responsible for writing transactions. The Isolation property follows automatically from this, because there are no concurrent transactions. Transactions are always executed serially. This is wrong in two senses. First, Datomic is fault-tolerant: one can and should run several transactor nodes on different computers. Typically one transactor is active and the others are in standby. When a transactor’s failure detector believes there is no active transactor, it will attempt to promote itself to active. However, perfect failure detectors are impossible in asynchronous networks. There may be times when a standby transactor believes it should take over, but another active transactor is still running. This means transactions may actually execute concurrently. During this window Datomic is not a single-writer system, but a multi-writer one! Second, even if there were a perfect failure detector which ensured a single Datomic transactor at a time, its messages to storage could be arbitrarily delayed by the network and arrive interleaved with messages from other transactors. Thankfully this doesn’t matter: Datomic’s safety property follows directly from the Sequential consistency of the storage system’s CaS operation. Any number of concurrent transactors ought to be safe. 2 Test Design We designed a test suite for Datomic using the Jepsen testing library. Our test installed Datomic Pro 1.0.7075 on a cluster of Debian Bookworm nodes. For storage, it provisioned a DynamoDB table in AWS. Two of the test nodes ran transactors, and the remaining nodes ran peers. Our peers were small Clojure programs which used the Datomic peer library. They connected to storage and transactors and exposed a small HTTP API for performing test suite operations. For each operation the test used an HTTP client to submit that operation to some peer. The peer executed that operation using the peer library, and returned a result to the client. We ran our workloads both using d/db, which may yield stale reads, and also with d/sync, which blocks but guarantees recency. Our test harness injected faults into both transactors and peers, including process pauses, crashes, and clock errors. We created network partitions between nodes (including both transactors and peers) and between nodes and the storage system. We also requested Datomic perform garbage collection. Datomic transactors kill themselves when they cannot maintain a stable connection to storage. When we ran transactors with the default 5-second timeout settings on nodes outside AWS, transactors routinely killed themselves every few minutes due to normal network fluctuations. With a 1-second timeout, even transactors running in our EC2 test environment would kill themselves roughly every 10–20 minutes. To work around this, Datomic advises that operators run their own supervisor daemons to restart transactors. We used a systemd service with Restart=on-failure. Our test suite included four workloads. 2.1 List Append We designed an append workload for use with the Elle transaction checker. Logically, this workload operates over lists of integer elements, with each list identified by an integer primary key. Clients perform transactions comprising random operations. Each operation may read the current value of a list, or append a unique element to the end of a list. Elle then performs a broad array of checks on the history of transactions. It looks for aborted and intermediate reads, violations of internal consistency, and inconsistent orders of elements across different reads of a list. From the element orders, it infers write-write, write-read, and read-write dependencies between transactions. From the order of transactions on each logical process, and the global order of transactions, it infers per-process and real-time orders, respectively. Elle then searches for cycles in the resulting dependency graphs. Various cycles correspond to violations of different consistency models, like Strict Serializability. We encoded our lists in Datomic as follows. Each list was represented by a single entity with two attributes. One, append/key, served as the primary key. The other, append/elements, was a many-valued attribute which stored all the integer elements in a given list. Performing the writes in a transaction was straightforward: given a write, we emitted a single operation for the transaction stating that the given key now had that element: {:append/key k, :append/elements element}. To perform a read of k, we read a local cache of k’s elements. We populated that cache with an initial series of read queries, then used a small state machine to simulate internal reads. Note that multi-valued attributes represent an unordered set, not an ordered list. Elle’s inference uses the order of list elements to infer the serialization order of transactions. To obtain this order, we took advantage of the fact that Datomic is a temporal database: every datom includes a reference to the transaction which wrote it. When we read the elements for a given key, we also included their corresponding transactions. We then sorted the elements by transaction times, which provides exactly the order Elle needs. Elle’s list-append workload is designed for databases which offer mixed read-write transactions, but Datomic doesn’t have this concept. As previously mentioned, any read-write transaction can be expressed in Datomic by running the writes in a transaction function, then using the returned database state to determine what the transaction’s reads would have been. We used this technique in our workload: a single function executes the transaction, simulates internal reads, and produces side effects (for the write transaction) and completed reads (to be returned to the client). We execute this function twice: once using a stored procedure via transact, then a second time on the peer to fill in reads, using the pre-state of the database transact returned. 2.2 List Append with CaS Many Datomic users use the built-in compare-and-set function db/cas to control concurrent updates to an attribute of an entity outside a transaction. For example, they might read the current DB state using d/db, read the value of a counter as 4, then increment the counter’s value using [:db/cas 123 :counter/value 4 5]. The CaS function asserts the new value 5 if and only if the current value is 4. Datomic guarantees transactions are always Serializable, but a user might want to express a logical “user transaction” consisting of a read followed by a separate write transaction. Since Datomic database states are always complete snapshots, and transactions are Serializable, using db/cas for every write8 allows users to build an ad hoc Snapshot Isolation over these user transactions. Our append-cas workload provides the same logical API as the list-append workload, but uses this CaS pattern to ensure Snapshot Isolation. Instead of multi-valued elements, we encoded each list as a single-valued, comma-separated string. We performed a read at the start of each transaction, applied reads and writes locally, maintaining a buffer of written values, then constructed a transaction of CaS operations which ensured that any values we wrote had not been modified since they were read. 2.3 Internal Our two list-append workloads measured safety between transactions, but because they simulated the results of internal reads, they did not measure Datomics intra-transaction semantics. We designed an internal workload which measures internal consistency with a suite of hand-crafted transactions. For instance, we assert that the value for some attribute of an entity is 1, then 2. We assert and retract a fact in the same transaction. We assert a value, then try to CaS it to something else. We perform multiple CaS operations—trying to change 1 to 2, then 2 to 3. We create an entity, then modify it using a lookup ref. Using a transaction function, we attempt to increment a value twice, and so on. 2.4 Grant To ensure that transaction functions preserved function invariants, we designed a grant workload which simulates a simple state machine using transaction functions. Grants are first created, then can either be approved or denied. We encode a grant as a single entity with three attributes: created-at, approved-at, and denied-at. No grant should be both approved and denied. We ensure this invariant by writing a pair of transaction functions approve and deny. Each first checks that the grant under consideration has not been approved or denied already, aborting the transaction if necessary. If the grant hasn’t been approved or denied yet, approve adds the grant’s approved-at date. Our deny function works the same way. Our grant workload creates a new grant in one transaction. In subsequent transactions it tries to approve and/or deny the grant. We repeat this process, exploring different combinations of functions and transaction boundaries. We check to make sure that no grant is both approved and denied. 3 Results We found no behavior which violated Datomic’s core safety claims. Transactions appeared to execute as if they had been applied in a total order, and that order was consistent with the local order of operations on each peer. Histories restricted to just those transactions performing writes, and histories in which reads used (d/sync conn) to obtain a current copy of the database, were consistent with real-time order. However, we did observe unusual behavior within transactions. This intra-transaction behavior is generally consistent with Datomic’s documentation, but it represents a significant departure both from typical database behavior and the major formalisms used to model transactional isolation. We discuss those divergences here. 3.1 Internal Consistency Virtually all databases and formalisms Jepsen is familiar with provide serial execution semantics within a transaction. For example, a transaction like set x = 1; read x; would print 1, rather than the value of x when the transaction started. Although Datomic transactions are ordered lists of operations, Datomic does not preserve this order in execution. Instead, all operations within a transaction (adds, retracts, and transaction functions) are executed as if they were concurrent with one another. Transaction functions always observe the state of the database at the beginning of the transaction. They do not observe prior assertions, retractions, or transaction functions. For example, consider these results from our internal workload. Imagine entity 123 currently has an :internal/value of 0, and we execute the following transaction: [[:db/cas 123 :internal/value 0 1] [:db/cas 123 :internal/value 0 1]] In a serial execution model, this transaction would fail: the first CaS would alter the value of key 123 from 0 to 1, and the second CaS would fail, since the current value was 1 and not 0. In Datomic, both CaS operations observe the initial state 0, and both succeed. They produce a pair of redundant assertions [:db/add 123 :interval/value 1], and the value of entity 123 becomes 1. This means that state transitions may not compose as one expects. For instance, here is a transaction function that increments the value of the entity with key k: (defn increment [db k] (let [{:keys [id value]} (read db k)] [[:db/add id :internal/value (inc value)]])) What value does the following transaction produce, given an entity with key \"x\" and value 0? [['internal/increment \"x\"] ['internal/increment \"x\"]] In a serial model, the result of two increments would be 2. In Datomic, it’s 1: both increment functions receive the database state from the start of the transaction. Similarly, transaction functions do not observe lexically prior assertions or retractions. [[:db/add id-of-x :internal/value 1] ['internal/increment \"x\"]] This produces a final value of 1, not 2. Likewise, lookup refs use the state of the database as of the start of the transaction. This means a transaction which adds an entity cannot use a lookup ref to refer to it later in that same transaction. The following transaction aborts with an Unable to resolve entity message: [; Create an entity with key \"x\" [:db/add \"x\" :internal/key \"x\"] ; And set the value of the entity with key x ; to 0: [:db/add [:internal/key \"x\"] :internal/value 0]] Many of the above transactions included multiple assertion requests with the same entity, attribute, and value. What happens if the values conflict? Imagine this transaction executes on a state where x’s value is 0. [[:db/add id-of-x :internal/value 2] ['internal/increment \"x\"]] In a database with serial intra-transaction semantics, this would produce the value 3. In Datomic, the increment observes the start-of-transaction value 0. It completes successfully, and the transaction expands to the following: [[:db/add id-of-x :internal/value 2] [:db/add id-of-x :internal/value 1]] If this were executed by a serial database, it would produce the value 1. But Datomic’s order-free semantics have another rule we have not yet discussed. If two assertions in the same transaction have different values for the same single-cardinality attribute of the same entity, the transaction aborts with :db.error/datoms-conflict. This transaction aborts! This in-transaction conflict detection mechanism likely rules out many cases where the use of transaction functions would produce surprising results. A pair of increments will silently produce a single increment, but this is only possible because they all expand to compatible [entity, attribute, value] triples. Since there are an infinite number of incompatible values, and a single compatible choice for any [entity, attribute] pair, it seems likely that users who accidentally compose transaction functions incorrectly will find their transactions fail due to conflicts, and recognize their mistake. This behavior may be surprising, but it is generally consistent with Datomic’s documentation. Nubank does not intend to alter this behavior, and we do not consider it a bug. 3.2 Pseudo Write Skew The fact that transactions appear to execute in serial, but the operations within a transaction appear to execute concurrently, creates an apparent paradox. A set of transaction functions might be correct when executed in separate transactions, but incorrect when executed in the same transaction! While Datomic’s in-transaction conflict checker prevents conflicts on a (single-cardinality) [entity, attribute] pair, it does nothing to control concurrency of functions which produce disjoint [entity, attribute] pairs. We designed the grant workload to illustrate this scenario. Following the documentation’s advice that transaction functions “can atomically analyze and transform database values,” and can be used to “ensure atomic read-modify-update processing, and integrity constraints,” we wrote a pair of transaction functions approve and deny. These functions encode the two legal state transitions for a single grant. (defn approved? \"Has a grant been approved?\" [db id] (-> '{:find [?t] :in [$ ?id] :where [[?id :grant/approved-at ?t]]} (d/q db id) count pos?)) (defn ensure-fresh \"Throws if the given grant ID is approved or denied.\" [db id] (when (approved? db id) (throw+ {:type :already-approved})) (when (denied? db id) (throw+ {:type :already-denied}))) (defn approve \"Approves a grant by ID. Ensures the grant has not been approved or denied.\" [db id] (ensure-fresh db id) [[:db/add id :grant/approved-at (Date.)]]) The denied? and deny functions are identical to approved? and approve, except they use the denied-at attribute; we omit them for brevity. By ensuring that the given grant ID is fresh (i.e. neither approved nor denied), these functions ensure an important invariant: no sequence of approve and/or deny calls can produce a grant which is both approved and denied. And indeed, Datomic’s Serializable transactions guarantee this invariant holds—so long as calls to approve and deny only ever take place in different transactions. However, if a single transaction happens to call both approve and deny, something very interesting occurs: [['grant/approve id] ['grant/deny id]] This transaction produces a grant with the following state: {:db/id 17592186045426, :grant/created-at #inst \"2024-02-01...\", :grant/denied-at #inst \"2024-02-01...\", :grant/approved-at #inst \"2024-02-01...\"} This grant is both approved and denied at the same time. Our invariant has been violated! Datomic’s in-transaction conflict checker did not prevent this behavior because the approve and deny functions returned assertion requests for disjoint [entity, attribute] pairs. If we were to draw a data dependency graph between these two functions using the language of Adya’s formalism, we’d see something like the following: The approve function wrote a new version of the grant’s approved-at attribute, but when deny read that attribute, it observed the previous (unborn) version from the start-of-transaction database state. This is analogous to a read-write (rw) anti-dependency edge in Adya’s Direct Serialization Graph. Symmetrically, deny wrote a new version of the grant’s denied-at attribute, but approve saw the previous unborn version of denied-at. This gives rise to a dependency cycle: each transaction function failed to observe the other’s effects. If these approve and deny boxes were transactions, we’d call this cycle G2-item: an isolation anomaly proscribed by Repeatable Read and Serializability. Indeed, this phenomenon is analogous to a concurrency anomaly Berenson et al called called Write Skew: Suppose T1 reads x and y, which are consistent with C(), and then a T2 reads x and y, writes x, and commits. Then T1 writes y. If there were a constraint between x and y, it might be violated. There are some similarities between the inter-transaction concurrency control of Berenson et al’s Snapshot Isolation and the intra-transaction concurrency control of Datomic’s end-of-transaction conflict checker. When the write sets (assertion requests) of two transactions (transaction functions) intersect on some object (an entity and cardinality-one attribute), the first-committer-wins principle (conflict checker) prevents concurrent execution by forcing an abort. When their write sets are disjoint, invariants preserved by two transaction functions individually may be violated by the transaction as a whole. Like the internal consistency findings above, this behavior may be surprising, but it is broadly consistent with Datomic’s documentation. Nubank intends to preserve Datomic’s concurrent intra-transaction semantics. We consider this expected behavior for Datomic, rather than a bug. 3.3 Entity Predicates From Datomic’s point of view, the grant workload’s invariant violation is a matter of user error. Transaction functions do not execute atomically in sequence. Checking that a precondition holds in a transaction function is unsafe when some other operation in the transaction could invalidate that precondition! However, Datomic offers a suite of constraints for enforcing database invariants, including type, uniqueness, and arbitrary predicates on specific attributes. One of the most general constraints is an entity predicate. Entity predicates are functions which receive a candidate state of the database with all transaction effects applied, the ID of an entity, and return true if the transaction should be allowed to commit that state. “Entity” is something of a misnomer: these predicates have access to the entire state of the database, and can therefore enforce arbitrary global constraints, not just those scoped to a particular entity. We can use entity predicates to ensure grants are never approved and denied. To start, we write an entity predicate function valid-grant?. (defn valid-grant? [db eid] (let [{:grant/keys [approved-at denied-at]} (d/pull db '[:grant/approved-at :grant/denied-at] eid)] (not (and approved-at denied-at)))) Then we add an entity spec to the schema which references that function. (def schema [... {:db/ident :grant/valid? :db.entity/preds ['grant/valid-grant?] :db/doc \"Ensures the given grant is not approved *and* denied\"}]) Unlike other schema constraints, which are enforced for every transaction, entity specs (and their associated entity predicates) are only enforced when transactions explicitly ask for them. Datomic believes that whether or not to enforce an entity spec is a domain decision, and that this approach is more flexible than making entity specs mandatory. Therefore our transition functions assert the grant’s approved-at or denied-at attribute, then request the entity spec be enforced by adding a special request for a virtual datom, binding the attribute :db/ensure to our entity spec. (defn approve [db id] [[:db/add id :grant/approved-at (Date.)] [:db/add id :db/ensure :grant/valid?]]) (defn deny [db id] [[:db/add id :grant/denied-at (Date.)] [:db/add id :db/ensure :grant/valid?]]) Using this entity spec, attempts to approve and deny a grant within the same transaction throw an error, preserving our intended invariant. {:cognitect.anomalies/category :cognitect.anomalies/incorrect, :cognitect.anomalies/message \"Entity 17592186045427 failed pred #'jepsen.datomic.peer.grant/valid-grant? of spec :grant/valid?\", :db.error/pred-return false, :db/error :db.error/entity-pred} 4 Discussion In our testing, Datomic’s inter-transaction semantics were consistent with Strong Session Serializability. Intra-transaction semantics appeared strictly concurrent: the operations within a transaction seemed to be executed simultaneously, and the resulting effects merged via set union. This combination satisfies a common high-level definition of Serializability: “equivalence to a serial execution of transactions.” However, it does seem to violate the definitions of Serializability in the most broadly-adopted academic formalisms for transactional isolation. Datomic argues—and Jepsen is willing to entertain—that these formalisms should not be applied to Datomic; they are fundamentally different kinds of databases. While some details of the documentation were inaccurate or misleading, Datomic’s inter- and intra-transaction behavior appeared consistent with its core safety claims. Indeed, we believe Datomic’s inter-transaction safety properties are stronger than promised. As always, we caution that Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. We also note that correctness errors in the storage system underlying Datomic could cause violations of Datomic’s guarantees; Datomic atop DynamoDB is only as safe as DynamoDB’s compare-and-set operation. 4.1 Inter-Transaction Semantics If one considers a session as being bound to a single peer, Datomic appears to guarantee Strong Session Serializability. Histories of transactions appear indistinguishable from one in which those transactions had executed in some total order, and that order is consistent with the order observed on each peer. Histories restricted to write transactions (i.e. calls to d/transact) appear Strict Serializable. So too do histories where readers use d/sync to obtain an up-to-date state of the database rather than d/db, which could be stale. 4.2 Intra-Transaction Semantics Most transactional systems provide serial semantics within a single transaction.9 Each operation—writes, reads, procedure calls, etc.—within a transaction appears to take place after the previous operation in that same transaction. This property is explicitly encoded in the major formalisms for transactional isolation. Adya, Liskov, and O’Neil begin their database model by defining transactions as ordered, and explicitly specify later operations observe earlier ones: Each transaction reads and writes objects and indicates a total order in which these operations occur…. If an event wi(xi.m) is followed by ri(xj) without an intervening event wi(xi.n) in E, xj must be xi.m. This condition ensures that if a transaction modiﬁes object x and later reads x, it will observe its last update to x. Similarly, the abstract execution formalism of Cerone, Bernardi, and Gotsman defines an internal consistency axiom preserved by all consistency models from Read Atomic through Serializable: The internal consistency axiom ensures that, within a transaction, the database provides sequential semantics: a read from an object returns the same value as the last write to or read from this object in the transaction. In particular, guarantees that, if a transaction writes to an object and then reads the object, then it will observe its last write. Crooks, Alvisi, Pu, and Clement’s client-centric formalism similarly specifies transactions include a total order, and uses that order to ensure reads observe the most recent write to that object within the current transaction: Further, once an operation in T writes v to k, we require all subsequent operations in T that read k to return v. Even as far back as 1979, Kung and Papadimitriou defined transactions as a finite sequence of transaction steps. “Thus, our transactions are straight-line programs,” they explain. In all of these models, a Serializable system behaves equivalently to one which begins with an initial database state db0, picks some transaction T, applies the first operation in T producing an intermediate database state db0′, applies the second operation in T to db0′ producing db0″, and so on until the transaction has completed, producing a committed database state db1. Then it moves to a second transaction, and the process continues. Datomic’s semantics are quite different. As previously discussed, the operations within a transaction (assertions, retractions, transaction functions, etc.) are evaluated logically concurrent with one another. Every transaction function in a transaction T observes the state of the database when T began, and produces a new set of operations. They do not observe the other assertions, retractions, or functions in T. These operations are recursively evaluated until only assertions and retractions remain. Those assertions and retractions are merged with set union, checked for conflicts (e.g. contradictory assertions about the value of a single-cardinality attribute on some entity), and then applied to the database state to produce a new, committed version of the database. This behavior may be surprising to users familiar with other databases, but it is (to some extent) documented. The lookup ref documentation explains that refs use the before-transaction database state. The database functions documentation says the transaction processor calls transactions “in turn”, which hints at ordered execution, but explicitly notes that functions are passed “the value of the db (currently, as of the beginning of the transaction).” On the other hand, that same documentation goes on to say that “[t]ransaction functions are serialized by design,” which is true between transactions, but not within them. Datomic’s concurrent semantics yield advantages and drawbacks. For one, a common axiom of database systems is that committed database state is always consistent, in the business-rules sense. Read Committed and above proscribe phenomenon G1b (intermediate read) in which one transaction sees intermediate state from another transaction. Datomic goes one step further: it is impossible to observe your own transaction’s intermediate state. One can never10 produce or observe an inconsistent view of the system—full stop! In some sense, the concept of intermediate state is inherently confusing; Datomic does away with it altogether. This choice also simplifies Datomic’s model of time: everything in a transaction happens “at once”, and every datom is always associated with a single, totally-ordered time. On the other hand, Datomic’s model reintroduces one of the problems Serializability has long been used to prevent. As Papadimitriou’s 1979 paper The Serializability of Concurrent Database Updates11 concisely argues: Another way of viewing serializability is as a tool for ensuring system correctness. If each user transaction is correct—i.e., when run by itself, it is guaranteed to map consistent states of the database to consistent states—and transactions are guaranteed to be intermingled in a serializable way, then the overall system is also correct. It seems plausible that users would want to write transaction functions that transform data while preserving some kind of correctness invariant. Datomic’s transaction functions documentation originally suggested as much: Transaction functions run on the transactor inside of transactions, and thus can atomically analyze and transform database values. You can use them to ensure atomic read-modify-update processing, and integrity constraints… A transaction function can issue queries on the db value it is passed, and can perform arbitrary logic in the programming language. If one writes a set of transaction functions which independently preserve some invariant—say, that a grant must never be both approved and also denied, or that the circuits in a home never exceed the capacity of the main panel—one would like to say (analogous to Serializable transactions) that any composition of these functions also preserves that invariant. But as we’ve demonstrated, within a single Datomic transaction this is not true! A transaction which calls multiple transaction functions might produce an outcome incompatible with the atomic application of those functions. It might violate integrity constraints. Paradoxically, combining two transactions into one can actually make the system less safe. It seems likely that Datomic’s behavior violates the major modern transaction formalisms: Cerone et al’s internal consistency axiom, Adya’s program order, Crooks et al’s in-transaction order, etc. It may be possible to contort Datomic’s model into alignment with these formalisms: say, by defining Datomic as containing only one object (the entire database), or through a non-local translation of Datomic operations to the formalism’s sequence of reads and writes, in which reads are reordered to the beginning of the transaction, and writes to the end. However, these approaches strain intuition. Datomic databases obviously contain independently addressable entities and attributes. Datomic transactions are clearly made up of individual parts, those parts are written in order, and this looks very much like how other databases would express a transaction with serial semantics. Convincing users to ignore that intuition seems a challenging lift. An easier path might be to abandon these formalisms altogether: they are clearly not designed to apply to Datomic’s concurrent intra-transaction semantics. Instead, we could follow the classic informal definition of Serializability. The internal structure of transactions is completely opaque; all that matters is that the history of transactions is equivalent to one which executed in a serial order. Under this interpretation, Datomic does ensure Serializability, Strong Session Serializability, and so on—just with different intra-transaction rules. To avoid confusion, we carefully distinguish between inter- and intra-transaction consistency throughout this report. 4.3 Recommendations We found no evidence of safety bugs in Datomic, or serious divergence between documentation and system behavior. Datomic’s concurrency architecture is refreshingly straightforward, and its transactional correctness easy to argue. Jepsen believes users can rely on Datomic’s inter-transaction Serializability. However, Datomic users should be aware of the concurrent execution semantics within transactions. These are specified in the documentation, but remain an unusual choice which creates the potential for subtle invariant violations. Users should be careful when calling multiple transaction functions in the same transaction. In particular, watch out for intersecting read sets and disjoint write sets. Also be aware of the possibility that multiple updates (e.g. increments) to a single value might quietly collapse to a single update. In practice, we believe several factors protect Datomic users against encountering anomalies. First, users often try to create a schema and use it in the same transaction, or try to use a lookup ref to refer to an entity created in the same transaction. Both of these scenarios fail, which guides users towards re-reading the documentation and internalizing Datomic’s model. Second, the in-transaction conflict checker likely prevents many of the anomalies that could arise from logically-concurrent transaction functions: if two transaction functions produce different values for a single-cardinality attribute of an entity, the transaction aborts. In addition, users can use attribute predicates to constrain individual values, and entity specs (which must be requested on each transaction) to constrain all attributes of a single entity, or even an entire database. However, users must take care to explicitly request the appropriate entity specs within every transaction that might require them. Another potential surprise: Datomic goes to great pains to ensure every database state is business-rules consistent: there are no intermediate states, every state is the product of a committed transaction, and so on. However, not all schema constraints apply to extant data. In particular, attribute predicates are only enforced on newly-added datoms, not on existing datoms. A small operational note: Datomic transactors kill themselves after a few minutes of not being able to talk to storage. We recommended Datomic add a retry loop to make transactors robust to network fluctuations. 4.4 Documentation Changes Following our collaboration, Datomic has made extensive revisions to their documentation. First, we worked together to rewrite Datomic’s transaction safety documentation. It now reflects the stronger safety properties we believe Datomic actually offers: Serializability globally, monotonicity on each peer, and Strict Serializability when restricted to writes, or reads which use sync. Datomic also removed the “single-writer” argument from their safety documentation. Datomic’s docs now include a comprehensive explanation of transaction syntax and semantics. It covers the structure of transaction requests, the rules for expanding map forms and transaction functions, and the process of applying a transaction. Expanded documentation for transaction functions explains Datomic’s various mechanisms for ensuring consistency, how to create and invoke functions, and the behavior of built-in functions. The transaction function documentation no longer says they can be used to “atomically analyze and transform database values”, nor does it claim transaction functions can “ensure atomic read-modify-write processing”. Datomic used to refer to the data structure passed to d/transact as a “transaction”, and to its elements as “statements” or “operations”. Going forward, Datomic intends to refer to this structure as a “transaction request”, and to its elements as “data”. The [:db/add ...] and [:db/retract ...] forms are “assertion requests” and “retraction requests,” respectively. This helps distinguish between assertion datoms, which are [entity, attribute, value, transaction, added-or-removed?] tuples, and the incomplete [entity, attribute, value] assertion request in a transaction request. Datomic has also added documentation arguing for a difference between Datomic transactions and SQL-style “updating transactions.” There is also a new tech note which discusses the differences between transaction functions and entity predicates when composing transactions. 4.5 Future Work Our tests did not evaluate excision or historical queries. Nor did we investigate the Datomic client library—though we believe its behavior is likely similar to the peers we designed in this test. We also limited ourselves to a single storage engine: DynamoDB. Datomic runs atop a variety of storage systems; testing others might be of interest. Finally, we have not evaluated Datomic Cloud, which uses a slightly different architecture. Jepsen is aware of few systems or formalisms which provide inter-transaction Serializability but intra-transaction concurrent semantics. Datomic’s behavior suggests fascinating research questions. First, what are Datomic transactions? Is there a sense in which they are a dual to typical database transactions? Rather than happening entirely in series, everything happens all at once. What are the advantages and drawbacks of such a “co-transaction” model? Can the drawbacks be mitigated through static analysis, runtime checks, or API extensions? And does this actually matter in practice, or are users unlikely to write transactions which could violate invariants? Second, are there other databases with concurrent intra-transaction semantics? Conversely, what about other temporal databases with serial intra-transaction semantics? How does Datomic’s model fit into this landscape? Alvaro’s Dedalus, a research project exploring temporal Datalog, comes to mind. Like Datomic, its transactions happen “all at once.” As in Datomic, this creates the apparent paradox that breaking up operations into multiple transactions can actually make them safer. Consider also Fauna, a temporal database supporting up to Strong Serializability. Like Datomic, Fauna transactions are small programs that the database evaluates, rather than an interactive session driven by a client. Unlike Datomic, Fauna’s transactions provide (what appears to be) serial execution with incremental side effects within each transaction. Are Fauna’s in-transaction temporal semantics sound? How do their models compare? The similarity between Datomic’s end-of-transaction conflict checker and Snapshot Isolation’s first-committer-wins rule suggests new research opportunities. How close is the relationship between Snapshot Isolation and Datomic’s in-transaction semantics, and what parts of the existing literature on Snapshot Isolation could we apply to Datomic? Can we show that within a Datomic transaction, cycles between transaction functions must always involve a pair of adjacent read-write anti-dependency edges. Clearly Datomic does not prevent the intra-transaction analogue of lost update, since it collapses multiple increments. What about Fractured Read? Does it allow something like the read-only transaction anomaly described by Fekete, O’Neil, and O’Neil? Or Long Fork? Are there analogues to other G2-item and G2 cycles, perhaps involving predicates? Finally, one wonders whether there might be a connection to Hellerstein & Alvaro’s CALM theorem. Could we show, for instance, that transaction functions which are logically monotonic are safe to combine in a single Datomic transaction? Datalog programs without negation are logically monotonic. Can we show that those programs are also safe under this execution model? Jepsen encourages future research. Jepsen wishes to thank the entire Datomic team at Nubank, and in particular Dan De Aguiar, Guilherme Baptista, Adrian Cockcroft, Stuart Halloway, Keith Harper, and Chris Redinger. Peter Alvaro offered key insights into concurrent semantics. Irene Kannyo provided invaluable editorial support. This work was funded by Nubank (Nu Pagamentos S.A), and conducted in accordance with the Jepsen ethics policy. Datomic also provides an excision mechanism which rewrites history to permanently delete datoms. This is useful for regulatory compliance or removing unwanted PII.↩︎ Girls!↩︎ Datomic refers to an immutable version of the database as a “value”. To avoid confusion with other kinds of values in this report, we call this a “database state”.↩︎ Most systems use “transaction” to refer to a group of operations, including reads or writes, executed as a unit. Datomic uses “transaction” to refer specifically to a write transaction—i.e. a call to d/transact. However, Datomic’s reads are trivially transactional as well. We refer to both reads and writes as transactions in this work—it significantly simplifies our discussion of consistency models.↩︎ At the start of our collaboration, Datomic used “statement”, “operation”, and “data” to refer to elements of a transaction. We use “operation” in this report for consistency with the database literature, and to avoid confusion with other kinds of data. Datomic intends to refer to transaction elements solely as “data” going forward.↩︎ Datomic wishes to note that transaction functions (for instance, db/cas) do not actually perform writes. They produce data structures which represent requests for writes. Those writes are performed during the final stages of transaction execution. Delayed evaluation of transaction effects is a common database technique; we use the term “write” loosely with this understanding.↩︎ As Fekete, O’Neil, and O’Neil point out, adding read-only transactions to a history which is otherwise Serializable can actually yield non-Serializable histories! However, this paper applies specifically to Snapshot Isolation, where two transactions may read the same state and write new values concurrently. Datomic’s design ensures transactions are atomic, in the sense that no two transactions overlap in the window between their read and write timestamps.↩︎ We say “every” write for safety and clarity. In practice, users often arrange for all transactions requiring concurrency control to conflict on a single attribute of an entity. A single CaS operation on, say, a customer’s version attribute could ensure that any number of updates to that customer occur sequentially.↩︎ Of course, typical Serializable databases may not actually execute operations in serial order. However, they (ought to) behave indistinguishably from a system which had. Similarly, Datomic may not execute transaction functions in parallel—but it guarantees concurrent semantics. For concision, we say “serial semantics” instead of “behavior which is indistinguishable from a serial execution,” and so on.↩︎ Unless one produces new, transient database states using d/with.↩︎ Intriguingly, Papadimitriou’s paper begins with transactions which perform a set of reads, then a set of writes; this formalism might be more readily applicable to Datomic transactions. Later in the paper he addresses “multistep transactions,” which are analogous to the serial formalisms discussed in this section.↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=40369467",
    "commentBody": "Jepsen: Datomic Pro 1.0.7075 (jepsen.io)281 points by aphyr 17 hours agohidepastfavorite54 comments adrianco 13 hours agoI was a fly on the wall as this work was being done and it was super interesting to see the discussions. I was also surprised that Jepsen didn’t find critical bugs. Clarifying the docs and unusual (intentional) behaviors was a very useful outcome. It was a very worthwhile confidence building exercise given that we’re running a bank on Datomic… reply fiatjaf 15 minutes agoparentWhat bank is that, if I may ask? reply loevborg 10 minutes agorootparenthttps://building.nubank.com.br/functional-programming-with-c... reply belter 11 hours agoparentprev> I was also surprised that Jepsen didn’t find critical bugs. From the report...\"...we can prove the presence of bugs, but not their absence...\" reply jupp0r 8 hours agorootparentIn practical terms, if you are a database and Jepsen doesn't find any bugs, that's as much assurance as you are going to get in 2024 short of formal verification. reply pests 6 hours agorootparentThe work antithesis has been doing here has me really excited as well. reply vasco 11 hours agorootparentprevThat's consistent with the usual definition of \"finding\" anything. reply cdchn 9 hours agorootparentprev\"Absence of evidence is not evidence of absence.\" reply kelseyfrog 8 hours agorootparentThank you. I've updated my initial guess of p(critical bugsdid not find critical bugs) from 0.5 to 0.82 given my estimate of likelihood and base rates. reply andreareina 8 hours agorootparentprevIf you've looked, it is. The more and the better you look, the better evidence it is. reply kelseyfrog 8 hours agorootparentIf you run it through bayes theorem, it adjusts the posterior very little. reply killingtime74 10 hours agoparentprevDid you not do this work yourself before you started running the bank on it? reply cdchn 9 hours agorootparentI doubt any organization that isn't directly putting lives on the line are testing database technology as thoroughly and competently as Jepsen. Banks jobs are to be banks, not be Jepsen. reply killingtime74 6 hours agorootparentI would have thought they would be more rigorous, since mistakes for them could threaten the very viability of the business? Which is why I assume most are still on mainframes. (Never worked at a bank) reply harperlee 5 hours agorootparentBanks exist since a long time before computers existed, and thus have ways to detect and correct errors that are not purely technological (such as double entry bookkeeping, backups, supporting documentation, different processes). So a bank can survive a db doing nasty things on a low enough frequency such that is not detected beforehand, so they don’t need to “prove in coq” that everything is correct. reply cdchn 3 hours agorootparentprevMistakes don't threaten them that much. When Equifax (admittedly not a bank) can make massive negligent fuckups and still be a going concern there isn't much heat there. Most fuckups a bank make can be unwound. reply Foobar8568 4 hours agorootparentprevAnyone who has worked in a bank and is glad of its solutions is either a fool, clueless or politician. Banks have to answer to regulation and they do by doing the bare minimum they can get away with. reply jwr 1 hour agoprevThis is a fantastic detailed report about a really good database. I'm also really happy to see the documentation being clarified and updated. As a side note: I so wish Apple would pay for a Jepsen analysis of FoundationDB. I know Aphyr said that \"their tests are likely better\", but if indeed Jepsen caught no problems in FoundationDB, it would be a strong data point for another really good database. reply poidos 7 hours agoprevReally nice work as always. I love reading these to learn more about these systems, for little tidbits of writing Clojure programs, and for the writing style. Thanks for what you do! reply aphyr 4 hours agoparentThank you! reply koito17 14 hours agoprevThis is the first time I try reading a Jepsen report in-depth, but I really like the clear description of Datomic's intra-transaction behavior. I didn't realize how little I understood the difference between Datomic's transactions and those of SQL databases. One thing that stands out to me is this paragraph Datomic used to refer to the data structure passed to d/transact as a “transaction”, and to its elements as “statements” or “operations”. Going forward, Datomic intends to refer to this structure as a “transaction request”, and to its elements as “data”. What does this mean for d/transact-async and related functionality from the datomic.api namespace? I haven't used Datomic in nearly a year. A lot seems to have changed. reply stuarthalloway 12 hours agoparentDatomic software needed no changes as a result of Jepsen testing. All functionality in datomic.api is unchanged. reply klysm 9 hours agorootparentCongrats, that is a rare outcome! reply aphyr 4 hours agorootparentYeah, I think this is next to Zookeeper as one of the most positive Jepsen reports. :-) reply amgreg 14 hours agoprevIt struck me that Jepsen has identified clear situations leading to invariant violations but Datomic’s approach seems to have been purely to clarify their documentation. Does this essentially mean the Datomic team accepts that the violations will happen, but don’t care? From the article: > From Datomic’s point of view, the grant workload’s invariant violation is a matter of user error. Transaction functions do not execute atomically in sequence. Checking that a precondition holds in a transaction function is unsafe when some other operation in the transaction could invalidate that precondition! reply stuarthalloway 13 hours agoparentAs Jepsen confirmed, Datomic’s mechanisms for enforcing invariants work as designed. What does this mean practically for users? Consider the following transactional pseudo-data: [ [Stu favorite-number 41] ;; maybe more stuff [Stu favorite-number 42] ] An operational reading of this data would be that early in the transaction I liked 41, and that later in the transaction I liked 42. Observers after the end of the transaction would hopefully see only that I liked 42, and we would have to worry about the conditions under which observers might see that 41. This operational reading of intra-transaction semantics is typical of many databases, but it presumes the existence of multiple time points inside a transaction, which Datomic neither has nor wants — we quite like not worrying about what happened “in the middle of” a transaction. All facts in a transaction take place at the same point in time, so in Datomic this transaction states that I started liking both numbers simultaneously. If you incorrectly read Datomic transactions as composed of multiple operations, you can of course find all kinds of “invariant anomalies”. Conversely, you can find “invariant anomalies” in SQL by incorrectly imposing Datomic’s model on SQL transactions. Such potential misreadings emphasize the need for good documentation. To that end, we have worked with Jepsen to enhance our documentation [1], tightening up casual language in the hopes of preventing misconceptions. We also added a tech note [2] addressing this particular misconception directly. [1] https://docs.datomic.com/transactions/transactions.html#tran... [2] https://docs.datomic.com/tech-notes/comparison-with-updating... reply aphyr 13 hours agorootparentTo build on this, Datomic includes a pre-commit conflict check that would prevent this particular example from committing at all: it detects that there are two incompatible assertions for the same entity/attribute pair, and rejects the transaction. We think this conflict check likely prevents many users from actually hitting this issue in production. The issue we discuss in the report only occurs when the transaction expands to non-conflicting datoms--for instance: [Stu favorite-number 41] [Stu hates-all-numbers-and-has-no-favorite true] These entity/attribute pairs are disjoint, so the conflict checker allows the transaction to commit, producing a record which is in a logically inconsistent state! On the documentation front--Datomic users could be forgiven for thinking of the elements of transactions as \"operations\", since Datomic's docs called them both \"operations\" and \"statements\". ;-) reply stuarthalloway 12 hours agorootparentMea culpa on the docs, mea culpa. Better now [1]. In order for user code to impose invariants over the entire transaction, it must have access to the entire transaction. Entity predicates have such access (they are passed the after db, which includes the pending transaction and all other transactions to boot). Transaction functions are unsuitable, as they have access only to the before db. [2] Use entity predicates for arbitrary functional validations of the entire transaction. [1] https://docs.datomic.com/transactions/transactions.html#tran... [2] https://docs.datomic.com/transactions/transaction-functions.... reply lgrapenthin 10 hours agorootparentSomewhat unrelated ad docs: It appears that \"Query\" opens a deadlink reply Voultapher 12 hours agorootparentprevThe man the myth the legend himself. I haven't ceased to be awed by how often the relevant person shows up in the HN comment section. Loved your talks. reply puredanger 12 hours agorootparentprevDatomic transactions are not “operations to perform”, they are a set of novel facts to incorporate at a point in time. Just like a git commit describes a set of modifications, do you or should you want to care about which order or how the adds, updates, and deletes occur in a single git commit? OMG no, that sounds awful. The really unusual thing is that developers expect intra-transaction ordering to be a thing they accept from any other database. OMG, that sounds awful, how do you live like that. reply voganmother42 11 hours agorootparentNested transactions or savepoints also exist in other systems reply cdchn 9 hours agorootparentprevDo developers not expect intra-transaction ordering from within a transaction? reply kccqzy 8 hours agorootparentIt depends on the previous experience of said developers, and such expectation varies widely. reply SoftTalker 14 hours agoparentprevSounds similar to the need to know that in some relational databases, you need to SELECT ... FOR UPDATE if you intend to perform an update that depends on the values you just selected. reply aphyr 14 hours agoparentprevYeah, this basically boils down to \"a potential pitfall, but consistent with documentation, and working as designed\". Whether this actually matters depends on whether users are writing transaction functions which are intended to preserve some invariant, but would only do so if executed sequentially, rather than concurrently. Datomic's position (and Datomic, please chime in here!) is that users simply do not write transaction functions like this very often. This is defensible: the docs did explicitly state that transaction functions observe the start-of-transaction state, not one another! On the other hand, there was also language in the docs that suggested transaction functions could be used to preserve invariants: \"[txn fns] can atomically analyze and transform database values. You can use them to ensure atomic read-modify-update processing, and integrity constraints...\". That language, combined with the fact that basically every other Serializable DB uses sequential intra-transaction semantics, is why I devoted so much attention to this issue in the report. It's a complex question and I don't have a clear-cut answer! I'd love to hear what the general DB community and Datomic users in particular make of these semantics. reply nickpeterson 14 hours agorootparentI feel like “enough rope to shoot yourself” is kind of baked into any high power, low ceremony tool. reply stuarthalloway 12 hours agorootparentAs a proponent of just such tools I would say also that \"enough rope to shoot(?) yourself\" is inherent in tools powerful enough to get anything done, and is not a tradeoff encountered only when reaching for high power or low ceremony. reply refset 11 hours agorootparentprevI don't know whether it was intentional or not, but IIRC DataScript opted for sequential intra-transaction semantics instead. reply huahaiy 8 hours agorootparentCorrect. I don't know about DataScript's intention, but it is intentional for Datalevin, as we have tests for sequential intra-transaction semantics. reply aaroniba 4 hours agoparentprevI think the article answers your question at the end of section 3.1: > \"This behavior may be surprising, but it is generally consistent with Datomic’s documentation. Nubank does not intend to alter this behavior, and we do not consider it a bug.\" When you say, \"situations leading to invariant violations\" -- that sounds like some kind of bug in Datomic, which this is not. One just has to understand how datomic processes transactions, and code accordingly. I am unaffiliated with Nubank, but in my experience using Datomic as a general-purpose database, I have not encountered a situation where this was a problem. reply aphyr 4 hours agorootparentThis is good to hear! Nubank has also argued that in their extensive use of Datomic, this kind of issue doesn't really show up. They suggest custom transaction functions are infrequently written, not often composed, and don't usually perform the kind of precondition validation that would lead to this sort of mistake. reply amluto 6 hours agoprevI wonder if Datomic’s model has room for something like an “extra-strict” transaction. Such a transaction would operate exactly like an ordinary transaction except that it would also check that no transaction element reads a value or predicate that is modified by a different element. This would be a bit like saying that each element would work like an independent transaction, submitted concurrently, in a more conventional serializable database (with predicate locking!), except that the transaction only commits if all the elements would commit successfully. This would have some runtime cost and would limit the set of things one could accomplish in a transaction. But it would remove a footgun, and maybe this would be a good tradeoff for some users, especially if it could be disabled on a per-transaction basis. reply lgrapenthin 6 hours agoparentI wouldn't use it. The footgun is imaginary. I use Datomic for ten years and I can assure you that I never stepped on it. As a Datomic user you see transactions as clean small diffs, not as complicated multi step processes. This is actually much more pleasant to work with. reply aphyr 3 hours agorootparentThis is also good to hear! I'm not sure whether I'd call it a \"footgun\" per se--that's really an empirical question about how Datomic's users understand its model. I can say that as someone with some database experience and a few weeks of reading the Datomic docs, this issue actually \"broke\" several of the tests I wrote for Datomic. It was especially tricky because the transactions mostly worked as expected, but would occasionally \"lose updates\" or cause updates intended for one entity to wind up assigned to another. Things looked fine in my manual testing, but when I ran the full test suite Elle kept catching what looked like serious Serializability violations. Took me quite a while to figure out I was holding the database wrong! reply amluto 6 hours agorootparentprevNow I’m curious: what’s a useful example of a Datomic transaction that reads a value in multiple of its elements and modifies it? reply hlship 3 hours agorootparentIn traditional databases, only the database engine has a scalable view of the data - that’s why you send SQL to it and stream back the response data set. With Datomic, the peer has the same level of read access as the transactor; it’s like the database comes to you. In this read and update scenario, the peer will, at its leisure, read existing data and put together update data; some careful use of compare and set, or a custom transaction function, can ensure that the database has not changed between read and writes in such a way that the update is improper, when that is even a possibility - a rarity. At scale, you want to minimize the amount of work the transactor must perform, since it so aggressively single threaded. Off loading work to the peer is amazingly effective. reply lgrapenthin 5 hours agorootparentprevYou could include two transaction functions that constrain a transaction to different properties about the same fact and then alter that fact. I don't know of a practical usecase or that I ever encountered that, it would be extremely rare IME. reply thom 13 hours agoprevI’ve not really spent much time with Datomic in anger because it’s super weird, but is any of this surprising? Datomic transactions are basically just batches and I always thought it was single threaded so obviously it doesn’t have a lot of race conditions. It’s slow and safe by design. reply rtpg 2 hours agoparentWell the example of \"incrementing x twice in the same transaction leads to x+1, not x+2\" seems pretty important! I imagine you gotta be quite careful! reply luc4sdreyer 3 hours agoprevI'm a bit worried that most of the links on https://www.datomic.com/ are broken. reply ndr 55 minutes agoparentAny one in particular? I just clicked through some of them and they all worked for me except for one. From https://www.datomic.com/ -> \"Getting Started\" points to the wrong https://docs.datomic.com/operation/datomic-overview.html instead of the correct https://docs.datomic.com/datomic-overview.html reply CrazyPyroLinux 14 hours agoprevaphyr had given some conference talks on previous analyses (available on youtube) that are informative and entertaining reply baq 14 hours agoprev [–] aphyr you bastard I've got work to do today. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Datomic is a general-purpose database utilizing EAV triples and logical timestamps, offering Datomic Pro and Datomic Cloud with distinct architectures.",
      "The database ensures read-write separation for performance and guarantees ACID transactions, though concurrent transactions may lead to unexpected outcomes.",
      "Users need to handle transaction functions and entity specs cautiously to avoid anomalies, despite Datomic's straightforward concurrency design and transactional correctness."
    ],
    "commentSummary": [
      "The article covers the release of Datomic Pro 1.0.7075, emphasizing the successful Jepsen testing and the significance of clear documentation and enforcing invariants.",
      "It discusses the challenges and advantages of utilizing Datomic's database system, including pitfalls, tradeoffs, comprehensive testing, scalability, transaction controls, and safety features.",
      "Mentioned are broken links on the Datomic website and insightful conference talks by aphyr."
    ],
    "points": 282,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1715792250
  },
  {
    "id": 40365882,
    "title": "Unprecedented CO2 Rise in 50k Years: Human Activity Leading Factor",
    "originLink": "https://today.oregonstate.edu/news/researchers-identify-fastest-rate-natural-carbon-dioxide-rise-over-last-50000-years",
    "originBody": "Skip to main content OREGON STATE UNIVERSITY Open search box Life at OSU » Newsroom Toggle menu Go to search page Search Field Exit Search For Journalists For Faculty and Staff Contacts News Archive 2023 Top 10 Stories For Journalists For Faculty and Staff Contacts News Archive 2023 Top 10 Stories Researchers identify fastest rate of natural carbon dioxide rise over the last 50,000 years May 13, 2024 CORVALLIS, Ore. – Today’s rate of atmospheric carbon dioxide increase is 10 times faster than at any other point in the past 50,000 years, researchers have found through a detailed chemical analysis of ancient Antarctic ice. The findings, just published in the Proceedings of the National Academy of Sciences, provide important new understanding of abrupt climate change periods in Earth’s past and offer new insight into the potential impacts of climate change today. “Studying the past teaches us how today is different. The rate of CO2 change today really is unprecedented,” said Kathleen Wendt, an assistant professor in Oregon State University’s College of Earth, Ocean, and Atmospheric Sciences and the study’s lead author. “Our research identified the fastest rates of past natural CO2 rise ever observed, and the rate occurring today, largely driven by human emissions, is 10 times higher.” Carbon dioxide, or CO2, is a greenhouse gas that occurs naturally in the atmosphere. When carbon dioxide enters the atmosphere, it contributes to warming of the climate due to the greenhouse effect. In the past, the levels have fluctuated due to ice age cycles and other natural causes, but today they are rising because of human emissions. Ice that built up in Antarctic over hundreds of thousands of years includes ancient atmospheric gasses trapped in air bubbles. Scientists use samples of that ice, collected by drilling cores up to 2 miles (3.2 kilometers) deep, to analyze the trace chemicals and build records of past climate. The U.S. National Science Foundation supported the ice core drilling and the chemical analysis used in the study. Previous research showed that during the last ice age, which ended about 10,000 years ago, there were several periods where carbon dioxide levels appeared to jump much higher than the average. But those measurements were not detailed enough to reveal the full nature of the rapid changes, limiting scientists’ ability to understand what was occurring, Wendt said. “You probably wouldn’t expect to see that in the dead of the last ice age,” she said. “But our interest was piqued, and we wanted to go back to those periods and conduct measurements at greater detail to find out what was happening.” Using samples from the West Antarctic Ice Sheet Divide ice core, Wendt and colleagues investigated what was occurring during those periods. They identified a pattern that showed that these jumps in carbon dioxide occurred alongside North Atlantic cold intervals known as Heinrich Events that are associated with abrupt climate shifts around the world. “These Heinrich Events are truly remarkable,” said Christo Buizert, an associate professor in the College of Earth, Ocean, and Atmospheric Sciences and co-author of the study. “We think they are caused by a dramatic collapse of the North American ice sheet. This sets into motion a chain reaction that involves changes to the tropical monsoons, the Southern hemisphere westerly winds and these large burps of CO2 coming out of the oceans.” During the largest of the natural rises, carbon dioxide increased by about 14 parts per million in 55 years. And the jumps occurred about once every 7,000 years or so. At today’s rates, that magnitude of increase takes only 5 to 6 years. Evidence suggests that during past periods of natural carbon dioxide rise, the westerly winds that play an important role in the circulation of the deep ocean were also strengthening, leading to a rapid release of CO2 from the Southern Ocean. Other research has suggested that these westerlies will strengthen over the next century due to climate change. The new findings suggest that if that occurs, it will reduce the Southern Ocean’s capacity to absorb human-generated carbon dioxide, the researchers noted. “We rely on the Southern Ocean to take up part of the carbon dioxide we emit, but rapidly increasing southerly winds weaken its ability to do so,” Wendt said. Additional coauthors include Ed Brook, Kyle Niezgoda and Michael Kalk of Oregon State; Christoph Nehrbass-Ahles of the University of Bern in Switzerland and the National Physical Laboratory in the United Kingdom; Thomas Stocker, Jochen Schmitt, Hubertus Fischer and Thomas Stocker of the University of Bern; Laurie Menviel of the University of New South Wales in Australia; James Rae of the University of St. Andrews in the United Kingdom; Juan Muglia of Argentina; David Ferreira of the University of Reading in the United Kingdom and Shaun Marcott of University of Wisconsin-Madison. College of Earth, Ocean and Atmospheric Sciences About the OSU College of Earth, Ocean, and Atmospheric Sciences (CEOAS): The college is renowned for research excellence and academic programs that span the earth, ocean and climate sciences, as well as the human dimensions of environmental change. CEOAS inspires scientific solutions for Oregon and the world. Story By: Michelle Klampe, 541-737-0784, michelle.klampe@oregonstate.edu Source: Kathleen Wendt; kathleen.wendt@oregonstate.edu; Christo Buizert, christo.buizert@oregonstate.edu Multimedia: Click photos to see a full-size version. Right click and save image to download. Contact Info Grab a feed of news and stories for your site. Copyright ©2024 Oregon State University Disclaimer",
    "commentLink": "https://news.ycombinator.com/item?id=40365882",
    "commentBody": "Fastest rate of natural carbon dioxide rise over the last 50k years (oregonstate.edu)249 points by geox 21 hours agohidepastfavorite451 comments belter 21 hours agoWe are in the multilayered race between - The risk of the rise of AGI - The risk and madness of Nuclear conflict - The risk of runaway Climate change My money is on Mother Gaia. She will brutally and swiftly discipline her puppies. reply AlexandrB 21 hours agoparent#1 is marketing hype. #2 is very real. #3 is happening and could cause #2 if wars for livable land and water resources break out. reply aurareturn 20 hours agorootparent#1 is very real #2 is warmongering #3 will be solved by human ingenuity I have a more positive outlook. :) reply tomrod 20 hours agorootparent#1 is a pipe dream. Oauth2 token TTL expiration alone will limit infiltration :-) reply belter 20 hours agorootparentUnderrated comment :-) reply belter 20 hours agorootparentprevRecent \"Star Trek Picard\" episode... Data: We are going to die. Geordi La Forge: Data...You are being negative again. Say something positive! Data: I hope we die quickly! reply globalnode 18 hours agorootparentThat should be the slogan for the in-coming climate and viral disasters. reply daveguy 19 hours agorootparentprevOne person can make one command decision and make #2 a reality. That's all it takes. No matter how many people drool over AGI the bandwidth and processing just isn't there. We barely have control systems as intelligent as a bacterium. reply vasco 18 hours agorootparentAs far as I know no single person on earth can do it all by themselves - all the mechanisms I'm aware of at least. reply somenameforme 17 hours agorootparentTo my knowledge, this is correct. In theory a command from e.g. the President is authoritative and should legally be carried out. In practice, you'd require every person along the chain to implicitly agree to effectively ending the world. Even at the bottom of the chain, there are likely multi-key requirements so even there you don't have just one person with his hand on the trigger. This makes one of the biggest threats to humanity's survival being buggy software. If a software glitch from one side or the other suggests nuclear weapons are incoming, they will almost certainly retaliate in kind. And once the bombs are away, the other side will definitely respond, and that's pretty much gg humanity. In fact this exact thing very nearly happened in 1983. Soviet instrumentation indicated that the US had attack the USSR with multiple nuclear weapons. The technician on duty, Stanislav Petrov, [correctly] judged that what was happening was an instrumentation error, and disobeyed protocol - refusing to escalate it to his superiors who very well could have ordered retaliatory strikes. For some context this had happened at once of the highest levels of tensions between the US and the USSR. This was in the era of the 'Star Wars program', which was to be a US missile defense shield, leading the Soviet Union to become paranoid about intentions of a preemptive nuclear strike attack. Those tensions had already led to the downing of one 747 that had inadvertently veered into Soviet territory, while carrying a US Congressman at that. Given this context it's highly probable that his superiors would have ordered the retaliation. So he's one of very few people who can be reasonably said to have saved the world. [1] - https://en.wikipedia.org/wiki/Stanislav_Petrov reply mrguyorama 17 hours agorootparentIn America this is not actually a useful thing. America's ICBM silos have technicians sitting in front of consoles, and basically every day, they get a command of keywords/numbers to punch into the computers. The technicians do not know whether the numbers are a test, or a real launch, at any time. This is purposely designed to prevent a low level button pusher from preventing the US from launching nukes. If the president wants to launch a nuke, the only thing that stops them is their cabinet simply not relaying the order. This happened several times when Nixon got shitfaced. Like nearly everything in american government, this is just a \"norm\". The only thing preventing an insane american president from doing pretty much anything, at least for a while, is \"norms\" and customs. Nixon wanted to nuke pretty much all of Vietnam pretty much all the time, and it was only his cabinet that prevented that. But there is no precedent, or legal reason, to actually force that to be the case. So what happens if you then get a president who has surrounded themselves with a cabinet that explicitly thinks Nixon was right, that Nixon had the right to burgle the Watergate hotel because the president should be able to do anything (this is a real faction in US politics right now) and have openly put together a plan to fire damn near everyone on day one and replace them with explicit yes men? reply somenameforme 16 hours agorootparentThis seems to run contrary to basically everything I've read on this topic. For instance during Trump's presidency there was a tremendous amount of fearmongering about him possibly wanting to nuke North Korea. This led to commentary from numerous generals and others opining on the topic about whether they would or would not obey the order. Here's some rando link. [1] The sort of system you're describing would enable the President to unilaterally carry out illegal orders independent of the military. I just find it difficult to imagine that they would concede that, not only because of the power concession, but because of the ethical or even constitutional implications. Notably officers don't swear an oath to their commanding officer or anything of the sort - they swear it to the Constitution. As for Nixon, he was literally playing the madman as part of a strategic goal [2] (arguably the exact same thing North Korea is doing in modern times), but privately was the one who ultimately scrapped a plan that involved nuking Vietnam. [3] A nice quote from the Madman article (from Nixon), \"I call it the Madman Theory, Bob. I want the North Vietnamese to believe I've reached the point where I might do anything to stop the war. We'll just slip the word to them that, \"for God's sake, you know Nixon is obsessed about communism. We can't restrain him when he's angry—and he has his hand on the nuclear button\" and Ho Chi Minh himself will be in Paris in two days begging for peace.\" [1] - https://www.dailymail.co.uk/news/article-5082689/Retired-gen... [2] - https://en.wikipedia.org/wiki/Madman_theory [3] - https://en.wikipedia.org/wiki/Duck_Hook reply lamontcg 14 hours agorootparent> Notably officers don't swear an oath to their commanding officer or anything of the sort - they swear it to the Constitution. The Project 2025 agenda targets the Pentagon as being \"woke\" and full of \"Marxists\" and will likely result in a purge of generals and officers. Anyone who has ever signed off on anything they can call a diversity program will get ousted on day 1. The vacancies will then be filled with Trump supporters. reply tim333 12 hours agorootparentprevPutin seems quite adept at launching dumb wars that no one seems very keen on. reply Der_Einzige 17 hours agorootparentprevTactical nukes are sometimes under the command and potential authorization of battlefield commanders who in theory could operate without the authorization of a president. Consider a risk of this happening in the context of a border war between two nuclear armed states with low spillover risk to the rest of the world - i.e. pakistan and india. reply vasco 12 hours agorootparentThe commander cannot initiate it himself. No single person in the United States, that I know of (no access to classified info) can physically do it by themselves. The commander would have to order someone else to do it at least. This is a huge component of the whole doctrine as far as I'm aware. reply IMTDb 20 hours agorootparentprev#1 is marketing hype #2 is warmongering #3 will be solved while trying to reach #1 reply jeromegv 20 hours agorootparentprev> #3 will be solved by human ingenuity It already started, billions of dollars in losses, with no real solution in sight. reply exoverito 20 hours agorootparentThere will always billions in losses from something, especially when global wealth is estimated to be around 450 trillion. The Dust Bowl was a disastrous incident of climate change, yet it didn't occur because of CO2. The damage caused by the California Wildfires are just as much a function of expanding developments into forests, immediately putting out small fires leading to the accumulation of fuel over time, and opposition to controlled burns. Property damage and deaths from flooding are mostly a function of population growth in the developing world, without sufficient corresponding investment in water management infrastructure. Regarding the solution, stratospheric aerosol injection would be the most immediate and effective solution to rising temperatures. It's been estimated that current increases in CO2 have a radiative forcing effect of about 2 watts per square meter, compared to the total solar irradiance of 1361 W/m2. If CO2 levels doubled to 800 ppm then it's estimated this would have a radiative forcing effect of 6 W/m2. This scenario would require mitigation strategies like stratospheric aerosol injection to reduce solar irradiance by about 0.4%. In the context of plant growth this reduction in sunlight would be negligible given that photosynthesis is only 1 to 2% efficient. If anything we should see significantly accelerated plant growth by about 10 to 50% due to the CO2 fertilization effect at 800ppm. reply silverquiet 19 hours agorootparentI'd be curious to see the science on enhanced plant growth; my understanding is that the benefits of increased CO2 fade very quickly. I believe we can already see plant pores evolving to be smaller as they were in previous times when the Earth had greater CO2 levels. About the only thing that gives me hope is SRM, but it's a half-assed solution at best. A world with 800ppm CO2 and a dimmed sun via aerosols is not the Earth that I was born to; it is probably not possible to fully understand the affects. reply exoverito 17 hours agorootparentThe optimal CO2 level for plant growth is between 800 and 1000 ppm, as observed within greenhouses. https://www.frontiersin.org/journals/plant-science/articles/... There is no perfect solution. One of the benefits of stratospheric aerosols is that they only stay suspended for 10 years or so, reducing the risk of long term effects. They are also an exceptionally cheap strategy, on the order of a few billion a year to cool the entire Earth. Compare that with the many trillions needed to just get to carbon neutral, while significantly reducing economic growth and living standards. The world we were born into will not exist in any scenario. As they saying goes, you never step in the same river twice. CO2 emissions show no sign of decreasing, especially as China, India and Africa continue developing. Furthermore, we are on the cusp of AGI within the next decade or so, which will radically change reality far more than the Earth possibly getting a bit warmer in a 100 years or so. The IPCC does not even predict major cataclysm, and expects only sea level rise of a couple feet in the worst case scenario. reply silverquiet 17 hours agorootparentSeems plausible at least, and you are of course correct about the world; the CO2 level of the atmosphere when I was born was well below 400 and we'll not see that again as long as I live. That's what I tell those who respond viscerally to the idea of messing about with the atmosphere purposefully - we are already altering it regardless. reply deciplex 17 hours agorootparentprev> they only stay suspended for 10 years or so And then what happens? Earnest question. reply marssaxman 9 hours agorootparentAnd then we have to do it again, and again, and again; once you grab that tiger's tail, there is no letting go. In the meantime, sense of urgency abated, we would most likely keep on burning fossil fuels and putting more CO2 into the atmosphere, locking ourselves further into the loop. It is a terrible idea which will almost certainly happen. reply exoverito 5 hours agorootparentEntropy is inexorable and must always be resisted. Humans need to drink water and eat food, again, and again, and again. Unimaginable amounts of time, effort, and resources are spent every day maintaining civilization. If we build out solar panels, we will have to replace them in 20 years. There is no free lunch. We don't know how bad the climate will be at 800 ppm. 300 millions years ago, during the Carboniferous era, there were vast forests and very high CO2 levels, around 1000 to 5000 ppm. If the Earth gets too hot we will simply do stratospheric aerosol injection. If climate change turns out to be overhyped, then we get the CO2 fertilization boost for free, win-win. Regardless, the real solution is next level energy production from advanced fission, deep geothermal, and ultimately fusion power. With a vast surplus of energy we could do wildly impractical things like filter sea water for gold, and of course extract extremely diffuse gases from the atmosphere. Renewable energies are very low energy density, and more akin to farming from a physics perspective. They make sense in certain situations, but are not reliable for powering an advanced industrial civilization. I also suspect people's intuitions are out of perspective in terms of time scales. It seems likely AGI will arise within the next 20 years. Climate change is nothing compared to the singularity and rise of superintelligence. reply tim333 12 hours agorootparentprevBack to the ground in rainfall I guess. reply dahart 18 hours agorootparentprev> If anything we should see significantly accelerated plant growth by about 10 to 50% If that’s true, there should be global evidence of this happening already- are you aware of any publications confirming this effect? You say accelerated growth like it’d be a good thing, but I don’t think we should wish for it or expect that to mitigate any damage… getting to the point of doubled CO2 would probably be extremely bad. If CO2 levels doubled, we’d for sure lose significant amount of our ice sheets, and some coastal cities along with it. Some of that is already happening anyway, but tripling the radiative effect will make it go much faster and much farther. As it stands, any increase in plant growth isn’t in any way making up for the rate we’re cutting down and paving over all the plants, and it’s not clear that 10 to 50% accelerated plant growth would make up for it either… even assuming that accelerated plant growth actually leads to more plants and a greater volume of oxygen cycle, and not just earlier blooms. https://en.wikipedia.org/wiki/Deforestation#Rates_of_defores... https://www.antarcticglaciers.org/glaciers-and-climate/what-... reply vixen99 17 hours agorootparent> there should be global evidence of this happening already There is. https://www.nasa.gov/technology/carbon-dioxide-fertilization... for instance. \"From a quarter to half of Earth’s vegetated lands has shown significant greening over the last 35 years largely due to rising levels of atmospheric carbon dioxide, according to a new study published in the journal Nature Climate Change on April 25.\" Interesting observation: Trillions of dollars have been spent to date on emission reduction with many incurring a consequent lowering of quality of life. Some would say this is inevitable but better to survive than the alternative. To date (as far as I can tell) there has been no discernible reduction in the rate of increase in CO2 as measured at Mauna Loa station. When will this paramount metric relate to measures taken? Any guesses? reply chimprich 16 hours agorootparentFrom your article: \"The beneficial impacts of carbon dioxide on plants may also be limited, said co-author Dr. Philippe Ciais, associate director of the Laboratory of Climate and Environmental Sciences, Gif-suv-Yvette, France. “Studies have shown that plants acclimatize, or adjust, to rising carbon dioxide concentration and the fertilization effect diminishes over time.\" The beneficial effects of increased CO2 will also be more than counteracted by other problems as the world temperature rises, such as greater heat stress and lower soil moisture. > Trillions of dollars have been spent to date on emission reduction with many incurring a consequent lowering of quality of life It's cheaper to avoid the warming in the first place rather than deal with the consequences, so complaining about the cost is a false economy. > To date (as far as I can tell) there has been no discernible reduction in the rate of increase in CO2 as measured at Mauna Loa station It's possible that we've reached peak CO2 emissions: https://ourworldindata.org/grapher/annual-co2-emissions-per-... ...however we need to cut them significantly. reply dahart 15 hours agorootparentprevAre you suggesting that trying to reduce CO2 isn’t worth it? What alternative are you proposing? What lowering of quality of life are you referring to? How do you know what Mauna Loa would have measured without the efforts to date? And what is the expected delay between CO2 reduction and effect? reply ToValueFunfetti 18 hours agorootparentprevhttps://ourworldindata.org/grapher/solar-pv-prices https://ourworldindata.org/renewable-energy Seems like we're moving directly and rapidly towards a real solution. Perhaps not rapidly enough, but the only way to change that is billions more in \"losses\" reply RHSman2 18 hours agorootparentprevDoes money matter when we are all dead from a lack of oxygen? reply tim333 12 hours agorootparentprevOr optimistically AGI will solve #2 and #3 reply xipix 18 hours agorootparentprev#1 will lead to #2 #2 will solve #3 reply FranzFerdiNaN 20 hours agorootparentprevThe idea that we are going to technology our way out of climate change is hilariously naive. That would require us to remove more co2 from the atmopshere each year than we add, which is just a laughable idea to achieve within the required time frame of the next 15 years or so. reply UniverseHacker 17 hours agorootparentIt would have to be both cultural/political, and technological on that time frame. With current technologies that remove CO2, we could completely remove all of the CO2 we currently add to the atmosphere, without even any reduction in our use or release of CO2. The cost of such an effort with current technology would be slightly less than, for example, the cost of World War II (adjusted for inflation, etc.). If people cared enough to make huge sacrifices, we could absolutely do it right now. Now the technology to make it effortless, so it happens automatically without people doing anything expensive or difficult? E.g. pulling carbon from the air to make things becomes cheaper and superior to pulling oil from the ground? Indeed, we are a ways from that. reply jayGlow 19 hours agorootparentprevthere are likely technologies that could mitigate the effects while we work on a solution. geoengineering could help reduce the temperature of the planet although that has its own problems. reply jonathankoren 18 hours agorootparentWe have the solution for decades. The rolling class just doesn’t want to use them because it would cost them money reply goatlover 20 hours agorootparentprevWhat happens in 15 years? It’s not a comet impact. It’s an ongoing problem where technology will help us adapt. reply smallerfish 12 hours agorootparentThere's all kinds of tipping points that are plausible in the next decade that would lead to accelerating warming. Permafrost melting releasing large amounts of methane; gulf stream redirecting; extended drought in climate critical regions; large ice shelves collapsing, etc. It's gradual until it's not. reply mrguyorama 17 hours agorootparentprevThe problem is that we are adding too much CO2 to the air. In fact, we have already added too much CO2 to the air, to the point that we will have noticable bad effects within the next few decades. More importantly, all this stuff has huge inertia, so if we stopped producing CO2 right this second, we would STILL blow past CO2 goals. If we EVER want to get back to today's climate, you know, \"normal\", it requires removing gigatons of CO2 from the air. From a pure chemistry energy of reaction standpoint, that will be enormously expensive. There is zero technology that can change the physics fact that combining CO2 with something that will sequester it will take more energy than we ever got out of it. Petroenergy is explicitly a loan. We have to pay that back reply goatlover 16 hours agorootparentI understand that, but we may have to adapt to new normal with noticeable bad effects. That's where technology can help. Not sure we can remove that much CO2. Maybe if fusion becomes a thing, it would give use enough cheap energy. reply gadders 20 hours agorootparentprevnext [6 more] [flagged] andybak 20 hours agorootparentReally? reply gadders 2 hours agorootparentNot every green advocate wants this, but I think a few would be really upset if a technical solution was found that didn't involve the complete restructuring of society in the name of \"fairness\". reply jibe 19 hours agorootparentprevWe could be building nuclear power plants instead of shutting them down. reply andybak 14 hours agorootparentI have no idea how this comment relates to the previous one. reply mrguyorama 17 hours agorootparentprevAnd that makes the greenhouse effect not real how? reply jonathankoren 18 hours agorootparentprevNumber 3 is literally an article of faith. And like all matters of faith, not only is there no evidence for it, the evidence that we do have runs counter to the belief. Let’s be honest here. Climate change was a completely predicable problem with known causes causes and solutions. It was detected early and alarms were raised in a timely manner. Literally nothing happened. In fact, even today, nothing happens. Why? Money. Because the billionaires would rather have us all die than not buy a tenth superyacht. (NB: Go Team Orca!) The problem was not, and is not, science. It’s money and who has it. Now those same folks and their apologists promote quite laughable ideas like carbon capture and geoengineering. Could we have gotten cheaper lithium batteries earlier with more investment? Probably, but what did we get instead? Fracking. Literally the opposite of what was needed. Science is not magic. Even if it was, the nature of the problem means it gets exponentially more difficult every day that passes. You mange to repeal the second law of thermodynamics and somehow got the carbon pollution industry to spend enough money to make it viable? Great! Oh, oops! The permafrost melted and now 10x more powerful methane has been released, so… yeah. Too late! It’s just Pollyanna talk that actively harmful as it not only directs resources from mitigation and migration to wasteful handouts while simultaneously giving cover for just staying the course. But hey. Maybe I’m just a doomer, and those Titanic passengers should have just sciences up more lifeboats instead of drown. Maybe all of this just… “needs more study”. reply sharpshadow 20 hours agorootparentprevThat’s a difficult one, securing livable land and water with nuclear fallout. reply cheschire 19 hours agorootparentI think the implication is that the nuclear war would exist between sovereign rulers, not necessarily between the physical spaces themselves. Wipe out a capital city in an attempt to behead a sovereign government so that their land is available for take over. Apparently though, the bombs that landed on hiroshima and nagasaki did not create much fallout, making the space livable relatively quickly. It seems less destructive long term than a nuclear reactor meltdown which may be a tempting but misleading proxy. reply thijson 19 hours agorootparentA very real danger is a nuclear power that has a deranged person in charge. They know they don't have much time left, they don't care what happens after they are gone, they decide to fire off all their nukes. https://en.wikipedia.org/wiki/Apr%C3%A8s_moi,_le_d%C3%A9luge reply thijson 19 hours agorootparentprevChernobyl had tonnes of radioactive material, a nuclear bomb has much less material, in the kilograms. After Chernobyl blew its lid off, the reactor core was laid open. Witnesses said they could see a beam of light going into the sky. It was the radiation ionizing the air. A helicopter that was trying to drop sand onto the reactor accidently flew too close over it, giving the pilot a lethal dose. reply AnimalMuppet 18 hours agorootparentMaybe not \"accidentally\". That pilot may have known the danger, and done it anyway. I seem to recall reading that he did, but I can't point you to a source. reply kjkjadksj 18 hours agorootparentprevFallout is used in nuclear war to limit troop movements and other purposes. Making an area a no go zone is exactly one strategy. reply UniverseHacker 20 hours agorootparentprevRegardless of how realistic you think AGI risk is, it’s not marketing hype- it has been a human fear and central in scifi for a long time. Movies like 2001, Terminator, and the Matrix were hugely popular long before AI was profitable. Arguably ancient myths about golems and genies are essentially the same fear and concept. AI companies are afraid of public fear over AI as it might lead to regulating them out of existence…. They are actively creating marketing hype in the opposite direction, to convince people that AI is safe and useful. reply daveguy 19 hours agorootparentFear of AGI is very real. But our proximity to AGI is marketing hype. In terms of existential crisis #2 and #3 are serious and already worth investing in mitigation. But #1 (AGI) is science fiction. reply UniverseHacker 17 hours agorootparentFiction is much more than just entertainment... science fiction has been pivotal in both shaping and preparing our society for changes caused by technology. Much of the best science fiction has been so prescient that it's hard for modern audiences to even understand that these things were written before the world was like this. When I share a lot of old sci-fi with my young son, he finds it unremarkable, as they seem to simply be mundane stories about our present reality. Authors like Vernor Vinge wrote about AGI risk in stories because they were personally worried about it, and trying to share something they felt was important with others. You can easily disagree with the warnings and fears shared in particular sci-fi, but to dismiss fiction as categorically irrelevant to our reality is just ignorant. There is some important history here your comments suggest you are unaware of. One reason AI companies talk so much about AI risk despite it being bad for their bottom line isn't marketing hype, it's because many people in those companies are genuinely afraid. One could argue that this is because many of them have been exposed to the rationalist community- which could be (uncharitably?) seen as a doomsday cult obsessed with AGI risk. The founders of many AI startups including OpenAI were heavily influenced and active in this community. reply daveguy 16 hours agorootparentI neither said nor implied that AGI being firmly in the realm of science fiction is a bad thing. There's nothing wrong with thinking about the implications and they would be significant. It's just such a stretch from where we currently are that it's not worth investing significant resources in mitigation. The threat from AI that is present and real is janky systems being depended upon for life and death decisions. Authors like Vernor Vinge (and I love his stories) depend on bending the rules of both physics and computation. Physics in that computation is somehow fundamentally different in different areas of the universe and computation in that known computability limits no longer apply. It's easy to re-imagine science fiction as being closer to reality after the fact. AI safety is an issue whether AGI is 10 or 100 years away. reply nullstyle 17 hours agorootparentprevOut of curiosity, could you recommend any Science Fiction written before the nuclear age that talks about nuclear proliferation? Or to take something from our present world, what Science Fiction stories would you recommend to talk about the effect of drone tech on trade and shipping? reply azemetre 16 hours agorootparentAlas, Babylon was written in 1959. Not exactly before nuclear proliferation but before the Cuban Missile Crisis. reply nullstyle 13 hours agorootparentThat wasn't what I was looking for, but I appreciate the honest response. UniverseHacker claims \"science fiction has been pivotal in both shaping and preparing our society for changes caused by technology\" and I want to understand that claim more. Let me explain: By my reckoning, there are two driving factors in how the understanding and control of nuclear reactions affects our world: First, the huge step change in energetic density over chemical reaction, second the fact that you can cause explosive chain reactions. If there was a smooth gradient of progression towards the energetic density of Fat Man, the world would have been a very different place. If the science had panned out such that we couldn't cause a nuclear chain reaction, the bomb/energy source duality would never have emerged. I'm curious if there is any fiction out there that contended with either of these factors prior to us realizing that the disparities between chemical and nuclear reactions are what they are. I've not read Alas, Babylon, and I'm not trying to deny that it had important influence, but I'm pretty sure we knew that it was possible to end the world via nuclear conflict prior to 1959. By your estimation, how did the book shape or prepare our society for the changes caused by nuclear technology? We're on the other side of the cold war and the conflict between capitalism and communism isn't a thing anymore, right? If we die in a nuclear fire, it will not be anything like, to quote wikipedia, \"The explosion is mistaken for a large-scale US air assault on the military facility and, by the following day, the Soviet Union retaliates with its planned full-scale nuclear strike against the United States and its allies.\" I'm not trying to be snarky, I'm trying to understand what I'm missing. ---- To me these claims of SF's prescience fall short, especially when it comes to the dangers of AGI. As an example, I have no mouth and I must scream, as wonderful as it is, doesn't have much to say about preparing our society for AGI. AM will not emerge from GPT-4o, and I'm pretty sure if AGI does happen it won't be through a super computer built to coordinate militaries. I enjoy reading SF, but I think its wider influence is as much miss as it is hit, and I think we can plot better courses when we treat SF as mostly entertainment. People who develop technology would be better served studying the history of technological development. reply UniverseHacker 16 hours agorootparentprevThe unintended consequences of new powerful weapons falling into the wrong hands and new trade technology are very old themes in fiction even if the specific technical details are not. reply nullstyle 13 hours agorootparentI noticed you edited your initial response for tone before I could respond. Regardless, I'll respond to your initial statement to say that no, I'm not trying to make a snarky rebuttal. I'm trying to understand your statements better because my own personal experience and opinions disagree with what you claim and I'd like to integrate your perspective. I don't give a shit about internet points. You specifically mention Vernor Vinge, whom I haven't yet read. My understanding is his work is dependent upon their being an intelligence explosion with the development of AGI, which seems incredibly unlikely given how AI technology has developed thus far. I think we've seen amazing growth in capability but nothing close to an intelligence explosion as I've seen portrayed in SF. As someone working in this space, what lessons would I learn by reading Vinge's work as opposed to spending that time reading more about the history and happenings in the real world of computer security? Now, to respond to your post-edit: I feel like my disconnect with your positions in this thread are around the importance of fiction. I think fiction is a minor muscle in the body that drives technology, if you'll allow the metaphor, whereas you claim \"science fiction has been pivotal in both shaping and preparing our society for changes caused by technology.\" Perhaps you could explain what you mean more concretely? Another confusion I have: The claim of prescience and that the themes are very old seem to be at odds. If I develop an understanding of the dangers of runaway technology through a story about Genies, why do I need more modern work? What if I instead gain these understanding through actual history? The real world has so many more wrinkles than a story and so many more lessons to give about technological development than a story that I balk at words like pivotal. There is not pivot point, only hundreds and hundreds of articulations. Anyways, I've written enough for now, and I don't expect you to follow up. reply mhardcastle 17 hours agorootparentprevIn 2022, the median ML researcher surveyed thought that there is a 5% or 10% chance of AI leading to \"human extinction or similarly permanent and severe disempowerment of the human species,\" depending on how the question was asked. https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/#... reply Der_Einzige 17 hours agorootparentI claim in 2024 this has doubled or tripled. reply feoren 17 hours agorootparentprevSpiders have been a human fear for two million years, but I'm not too worried about them causing our downfall as a species. Just because we're afraid of AI doesn't mean it's actually worth fearing. reply UniverseHacker 17 hours agorootparentYes, even if something is worth fearing, it's not simply because people are afraid of it already. Did something I write make you think I was arguing otherwise? As an aside, I would argue that this deep rooted human fear of AI is related to the general ancient fear of humans being wiped out by other bands of more technologically advanced humans. Which for most of human history has been the single biggest danger by far. People also have deep instinctual fears of things like spiders and snakes for the same reason- they were serious hazards in the environment where humans evolved. One could say we have evolved these instincts to fear them, because they are worth fearing... not the other way around. reply feoren 16 hours agorootparent> Did something I write make you think I was arguing otherwise? At the risk of having boring meta-commentary about comments, I feel like I see people often forgetting that their comments are in the context of a larger conversation and are going to be read as responses to the parent comment that they are, well, responding to. And then they get confused when their comments are interpreted in that context, rather than as completely standalone statements. So let's backtrack: (A) AGI is a threat to human civilization. (B) Actually, its risk is marketing hype. (You) It's not marketing hype. Our fear is deep-seated. It's not that weird to interpret the 3rd comment as disagreeing that the fear is just hype. When in fact, you were disagreeing only with the word \"marketing\". It's \"innate\" hype, not \"marketing\" hype. That's a much -- for lack of a better word -- weaker comment, because it is nitpicking about a minor word choice (\"it's marketing hype\") rather than the sentiment of the comment (\"it's just hype\"). So now you're asking \"why did you assume my comment was interesting, and not just a little nitpick!?\" > One could say we have evolved these instincts to fear them, because they are worth fearing But now you're back to saying AI is worth fearing, right? Isn't that what you're telling me I was wrong to assume you're saying? This feels like a needlessly difficult conversation. All that aside, your additions to the conversation are interesting. I actually really like your take on it being a fear of technologically superior tribes. It just doesn't feel like it needs to be quite so adversarial. You established an adversarial attitude with \"it's not marketing hype\". reply emporas 14 hours agorootparentTechnologically superior tribes always needed land. Their technology was advanced, but not in a way in which land is rendered useless, i.e. land price equals to zero. Two reasons mainly for land price not equating to zero. First: buildings could not be extended upwards. Second: chloroplasts could not be produced just with air and sun. These two problems will be solved in less than 10 years both. Land price worldwide will collapse to zero, or near zero. Buildings will be built using a flexible material, very strong and lightweight, i.e. graphene. Food will be produced using genetically modified spirulina, alongside with pharmaceutical substances and narcotics. Also, just to reply to a parent comment, Matrix was a philosophy movie, Terminator was a comedy with some scary parts, and 2001 i didn't understand any of it. Was i supposed to be afraid of something in 2001? reply UniverseHacker 16 hours agorootparentprevI appreciate the thoughtful reply. The seriousness of AGI risk is a big conversation our society is having right now, and I am not interested in laying out my position on it in detail on here... only that dismissing the fear of AGI as marketing hype is a straw-man argument - and ignores all of the history, discussions, and arguments on both sides of this debate going back decades, or arguably even thousands of years. Sure, it is fair to suspect that, because I am making that point, that I might also think AGI is a real threat to human civilization. The debate is so widespread right now, with mostly aggressive and dismissive bad faith arguments on both sides, that it sounds pretty exhausting and pointless to discuss. I share your concern, and objection to making discussions needlessly adversarial. In general, I've seen a huge rise recently in internet discussions mostly reducing to people yelling at each other that the other person is a narcissist. Even in niche technical and hobby forums, it is becoming the norm for disagreements. I feel it is infecting me as well, and it might be time to take a break from the internet and talk to people I actually like in real life more. reply ballenf 17 hours agorootparentprevFor sure, it's the locusts that I'm worried about. reply iamthirsty 17 hours agorootparentprevSpiders don't, won't and couldn't control almost all aspects of human life. This is a false equivalence. reply riedel 16 hours agorootparentI guess you will surely find a movie to prove you different reply madaxe_again 17 hours agorootparentprevCouldn’t is a stretch. Microbes can control almost all aspects of human life, as evinced in 2020. reply krainboltgreene 19 hours agorootparentprevNone of the movies and stories you mentioned were about the \"rise of AGI\". The closest you can get might be The Matrix via Animatrix? You can go look up what the authors meant by their pieces, there's plenty of analysis of each of those stories. In addition I would love a citation on \"the rise of AGI\" being a fear in sci-fi for a long time, I've enjoyed quite a bit of science fiction literature and I can only think of a handful that even touched on the idea. reply UniverseHacker 17 hours agorootparentSkynet??!!?! It seems so obvious that I am having a hard time understanding your objection... it would feel condescending to try to explain that Terminator is about AGI risk. The modern conceptualization of AI existential risk comes largely from Vernor Vinge, in particular his essay \"The Coming Technological Singularity\" but it is also a central theme in most of his works, especially the Zones of Thought series. reply 8note 16 hours agorootparentI think it's about as much about ai risk as transformers is. I'm terms of skynet I think you'd be referring to the third one? But that one is about drone strikes, and that's what we do today without AGI - we send out invincible killer robots to go wipe out villages from above because all males above the age of 16 are enemy combatants. The end of the movie could just as easily have been that they find a person behind the skynet controls killing everyone reply UniverseHacker 16 hours agorootparentThe overall plot of Terminator is that an AGI called Skynet becomes self aware and unstoppable sometime in the mid 1980s and humans only hope of survival is to use time travel to destroy it before this happens. I’ll admit this isn’t very well explained to the audience, and the focus is mostly on Arnold blowing things up. reply krainboltgreene 15 hours agorootparentI don't think anyone is confused about Terminator's plot, but people in this thread are definitely confused about what theme means and how to do literary analysis (not surprising given the context of this website). reply teo_zero 18 hours agorootparentprevI understood \"rise of AGI\" as if preceded by \"uncontrolled\": a runaway intelligence that ultimately turns against its creators. In that sense, yes, those movies do include this theme. Your comment makes sense if you intend \"rise of AGI\" as just growth. But then why should it be a concern? reply krainboltgreene 18 hours agorootparentNone of those movies have AGI as a theme, you have confused \"theme\" with \"plot point\". reply flir 19 hours agorootparentprevNeuromancer. I Have No Mouth and I Must Scream. Erewhon. reply krainboltgreene 19 hours agorootparentYou have massively confused \"AGI as a plot point\" with \"is what the literary work is about\". For example Neuromancer is about trauma and identity. If I'm being charitable I could give you Erewhon, but that's only because a major theme is anthropocentrism. A vast majority of the book is a criticism of victorian society. Ellison's analysis of his own work is pretty well known and while not specifically about AGI is definitely about technology misuse, so you win there. reply UniverseHacker 17 hours agorootparentThis is an absurd level of nitpicking... the phrase \"is about\" means both of those things in common speech, and everyone can tell which you mean depending on context. Language is defined by regular people through its use and context, scholars only describe it after the fact- to their extreme dismay. You might be technically correct within the jargon of, e.g. an academic literature course, but you aren't correct to insist the words mean that unless you are in that context, with that audience, where it has that meaning. Moreover, I don't think you are correct, even in the literary sense, when saying, for example, what Neuromancer is about. I don't really care what an author says it's about, most artists are channeling something unconscious and often have very little conscious understanding of what their work is actually about. I've read plenty of Gibson books. The guy is an absolute luddite, and writes on a typewriter. He is terrified of how dangerous and dehumanizing technology is, and all of his books are \"about\" an all-consuming fear of technology and the changes it causes. reply krainboltgreene 15 hours agorootparentI don't really care about your opinions on academic literary analysis, it's OK to be wrong and have your own thoughts on the matter, but prepare to be confused when people don't agree that the Transformer movies are about the dangers of aliens turning into vehicles. For someone who doesn't care what Gibson thinks of his own artistic works you sure do have a strong opinion on Gibson's artistic work. reply UniverseHacker 14 hours agorootparentYou are mis-representing people that intuitively understand that a phrase can have different meanings depending on context, as fundamentally confused and actually muddling the two concepts. Nobody would say \"transformers are about the dangers of aliens turning into vehicles\" but people would say it is \"about aliens that can turn into vehicles.\" They might also say something like it's \"about the the struggle to live day to day without being sure if everyday things are truly as they seem\" - if that's were what it's about- I'm not actually familiar with transformers. As an academic and a human (both!) I think it is important to remember how to keep the jargon from whatever you study distinct in your mind from common usage, so you can still communicate with people that didn't study whatever you studied. reply flir 12 hours agorootparentMy son's just completed an essay on Marxist themes in Blake. I think this guy's ideas of what literary analysis is and is not are pretty idiosyncratic, to be honest. reply krainboltgreene 13 hours agorootparentprevYou're welcome to make assumptions about what I'm doing, but at the end of the day you have to contend with the fact that I read a comment at face value, in the context, and then multiple people tried to back that original comment up. Maybe if you were the original author your opinion on what they were \"trying to say\" would matter, but it doesn't. reply UniverseHacker 11 hours agorootparentWho was the original author? reply flir 16 hours agorootparentprevIf not Neuromancer (although I feel differently) then Queen of Angels (Greg Bear). It's the most fascinating take on AI I've ever read - the entanglement with the themes of justice and punishment is thought-provoking. The Turing Option (Harry Harrison, Marvin Minsky). I can't stress strongly enough, do not read this book. It's terrible. But it does exist. reply krainboltgreene 15 hours agorootparentAgain I must stress you've confused theme with plot point. I haven't read QOA in a long time, but I'm pretty sure it was about identity and justice. reply flir 15 hours agorootparentIt's about an AI struggling to become conscious! Alarms go off when it uses the word \"I\"! The whole point of that subplot is why that AI can't achieve consciousness, but the one sent to Alpha Centauri[?] can. It writes essays meditating on its circumstances, the way humans have treated it and its sibling, it writes an essay exploring whether punishment of others is a gateway to self-awareness... You want to tell me the theme of that thread is human identity? It's our responsiblity to our creations. It's what does it mean to be sentient? It's the ethics of using a self-aware AI as a tool. (Amongst many other things). reply krainboltgreene 13 hours agorootparentIt contains an AI struggling to become human (to gain identity), if I remember correctly. Either way, we're way off course from \"it was a great fear of scifi that AGI would become a risk\" to \"a book with AGI as the vessel for metaphor about humanity exists\". reply cma 17 hours agorootparentprevTerminator 2 and a deleted scene, but the essence of it was there in the movie https://www.youtube.com/v=1UZeHJyiMG8& reply genezeta 17 hours agorootparentCorrected link: https://www.youtube.com/watch?v=1UZeHJyiMG8 reply Delmololo 20 hours agorootparentprevWe have now chips with 20petaflops. The weekly news regarding AI are always adding new things to ai. We started pouring billions into ai. The amount of new research papers regarding AI are exploding. I would argue that it is not marketing hype or aluminum hat thinking to take that serious reply rafaelmn 19 hours agorootparentYou could give similar numbers for crypto (powerplants, custom chips, billions invested, R&D) and yet it amounted to nothing. Not saying it's the same - just that your argument isn't very strong. Ironically Nvidia is riding both waves. A few people in the tech scene know how to fuel a hype train. reply Delmololo 19 hours agorootparentOn the surface perhaps. Btw hype is defined as exaggerated claims but the ai 'hype' train is not hype. We already use all those tools daily. It has real impact and makes real money reply throwmeaball 19 hours agorootparentprevCrypto is crunching random meaningless numbers. AI is crunching real world data. They are not the same. But AI should not be viewed as a problem. The real problem is that some people don't want others to have the same stuff as they. They want to work 1 hour while the other guy has to work 100. Interest on interest. reply user90131313 17 hours agorootparentprev2 is the real hype reply admissionsguy 20 hours agorootparentprev#1 is marketing hype #2 is a remote possibility #3 is a non-threat reply alex_duf 20 hours agorootparentplease explain how #3 is a non-threat? reply admissionsguy 18 hours agorootparentUnlike race, the fear of global warming is a social construct. Logically, if you are worried about humans, you should be worried about aging, cancer, heart disease. And if you are worried about the rest of nature, you should be worried about geology and human influence (of which climate is a minor part and self-limiting if significant). reply jonwinstanley 17 hours agorootparentYou can’t think of any circumstances where climate change might cause a threat to human life on the planet? reply eterps 16 hours agorootparentI also think it's bewildering when people say things like that. Are they thinking that humans are incapable of causing climate change no matter what they do? Or that we are currently not in trouble and fine for decades/centuries to come? reply admissionsguy 10 hours agorootparentI think you are crazy unreasonable if you think we are not fine for decades or centuries to come, climate-wise. Barring big volcanic eruptions. reply vbezhenar 17 hours agorootparentprevAnd how hypothetical global warming is a threat? So let's assume it's happening. Let's assume some ices melt and ocean level rises. Let's assume some coastal cities gets flooded. Just move to higher places, eh? Why do you live in coast anyway, what a weird thing. I'd welcome global warming, I tired of paying $50/month every winter just to keep my house warm. reply mrexroad 16 hours agorootparentYour comment is so misguided I can’t tell if you’re joking, trolling, or serious. I don’t know what else to say other than please keep the following in mind: CO2 is transparent to solar radiation, but opaque/reflective to the IR wavelengths that earth radiates heat back to space. reply lliamander 17 hours agorootparentprev> #3 is happening and could cause #2 if wars for livable land and water resources break out. Seems more likely to me that attempts to solve #3 will lead to #2. If we actually attempted to curtail fossil fuel consumption as much activists claim we should, there would be such a drop in agricultural and industrial productivity that there would be mass starvation, particularly in the developing world. I don't think those countries would peacefully go along with that plan. reply c048 21 hours agoparentprev'Mother' Gaia will wipe all complex life from this planet with 1 to 1.5 Billion years if her 'undisciplined' puppies don't find a way to leave this ball of dirt. She's also got quite a lead with killing off 99.99999% of all species that ever lived, when compared to us. reply gmd63 20 hours agorootparentEveryone hell bent on leaving is actually distracted from the real less immediately glamorous challenge, which is learning to sustain life and exercise restraint. reply zelphirkalt 2 hours agorootparentOr has realized that even with sustaining life and exercising restraint, there are enough people, who do not care and destroy all positive outcomes. reply itsoktocry 20 hours agorootparentprev>Everyone hell bent on leaving is actually distracted from the real less immediately glamorous challenge, which is learning to sustain life and exercise restraint. Unless you're willing to exterminate people who don't comply, the incentives are such that maintaining the status quo will give economic (and military) advantages. Besides, over long time scales there's more to fear than a few degrees of climate change. reply jessetemp 19 hours agorootparentprevThey’re probably referring to the earth getting enveloped by the sun in a billion or so years as it expands. I still think you’re right though. The better plan is staying on earth. The trick is moving it outward as the habitable zone expands with the sun. Only have to convince humanity to sling a giant meteor just outside earth’s orbit every year for millions of years without messing up. What could go wrong? reply triceratops 19 hours agorootparent> They’re probably referring to the earth getting enveloped by the sun in a billion or so years as it expands. Mother Gaia refers to the Earth, not the sun. reply andybak 20 hours agorootparentprevI think the point is that we need both. One is short/medium term and the other is medium/long term. An asteroid/comet impact big enough to wipe us out is a statistically certainty - not science fiction. reply sniggers 17 hours agorootparentprevRestraint won't make the unpredictable gamma ray burst or unstoppably-sized asteroid go away. reply BirAdam 20 hours agorootparentprevSounds like a challenge for which our species is well suited. reply airstrike 19 hours agorootparentprevLuckily we have 1 to 1.5 billion years to figure out how to survive outside of this ball of dirt... (cataclysmic asteroids and other similar events notwithstanding) reply randomopining 20 hours agorootparentprevYou think leaving earth is the solution? The only place we've evolved to survive on. reply SketchySeaBeast 20 hours agorootparentAnd if we can't manage the climate challenges here, how are we going to do it on Mars or during interstellar travel? reply JeremyNT 20 hours agorootparentprevWe evolved in this little window of time on this version of Earth. In a billion years, when the Sun is hotter and Earth's oceans have all dried, it's probably not going to be a great hang any more. reply postingawayonhn 20 hours agorootparentprevThe earth hasn't always existed in its current state, or for that matter existed at all. Once day the earth will almost certainly cease to exist and intelligence will have to find a new home of some kind. We have probably got a couple of billion years though if we are careful and I have no idea what intelligence will evolve to over that timeframe. reply cdelsolar 9 hours agorootparentJust a few hundred million years. The sun will be too luminous pretty soon. reply 14 20 hours agorootparentprevDefinitely not the solution. Ending capitalism for some other form of economy is the only way in my opinion. Not that I don’t think people should be rewarded for the products and services they offer just that the incentive to make cheap shit and sell an upgrade every year is definitely harmful to our earth. The problem I see is I don’t know what type of economic solution there is that would fit. reply angusturner 20 hours agorootparentI think the tools to solve the challenges of waste, environmental damage etc. already exist within the framework of capitalism. Mostly they are just unpopular and seen by many as a government overreach. 1. taxes that force corporations and individuals to pay for the negative externalities / social costs of their actions 2. regulation (e.g. stop allowing planned obsolence, mandate the right to repair etc.) 3. government spending into R&D, incentives and subsidies for renewables etc. Anyway, my point is that the issue is basically one of co-ordination and political will. It obviously doesn't help that many Americans (and Australians too for that matter, where I live) don't accept the basic facts of the situation (before we can even discuss solutions). reply itsoktocry 20 hours agorootparent>Anyway, my point is that the issue is basically one of co-ordination and political will Again, what does \"political will\" mean? What are you going to do to those that disagree? Lock them up? Exterminate them? What is the solution to force people to do your bidding, and has it ever worked? reply DFHippie 19 hours agorootparentI assume they mean convince enough people to implement the proposed policies that they can fix things through normal, legal means. \"Forcing people to do your bidding\" normally consists of winning elections and then implementing and enforcing legislation. This is how we force people who want to shoplift, cheat on their taxes, or murder to do our bidding. It doesn't work perfectly, but it only has to work well enough. reply lucianbr 19 hours agorootparentprevTheft is also a problem of political will. If people would just not steal, the problem of theft would be solved. For some definition of \"solution\", it is a solution. But not a useful or realistic one. It's just not going to happen in any reasonable timeframe. Only if human nature itself changes in some distant future. Same thing applies to environmental damage. reply hellojesus 20 hours agorootparentprevI have yet to hear of an economic model that humans have discovered which is better than free market capitalism. The issue isn't the cheap junk; it's the demand for the cheap junk. Things would be far more sustainable if people focused on reducing their consumption habbits, as producers would be run out of business. reply amanaplanacanal 17 hours agorootparentThe free market is probably the best we are going to get, but we need to address some of its known failure modes: externalities, monopolies, and the imbalance of power between employers and employees. reply Night_Thastus 19 hours agoparentprev#1 isn't happening anytime soon, nothing we have is on a path in that direction. #2 people always worry about, but it's almost certainly not going to happen - insane dictators or no. Even in much worse scenarios we avoided it. #3 Real, but slow. I think we'll see some coastal communities devastated over the course of the next ~50 years, but the rest of the world will adapt and/or sweep it under the rug as best they can. The bigger threat is economic. Companies will no doubt try to use the changing situation as an excuse to skyrocket prices and keep everyone broke. reply jonwinstanley 17 hours agorootparentIncrease of both droughts and floods mean we have issues inland too reply mywittyname 16 hours agorootparentprev#2 People aren't evaluating this risk correctly. If the war in Ukraine has taught me anything, it's that the assertions, \"Putin would never do...\" is wrong. #3 The real risk is the refugee crisis. The world is going to be split into two groups: refugees, and those trying to keep the refugees away. Large countries are going to experience crises from both internal and external migration. Think, people from mexico flooding into texas, while people from Texas flood north into the great plains. It's going to happen slowly, then all at once. reply Night_Thastus 13 hours agorootparentThere's a big difference between an invasion and launching nukes. There is no amount of reparation, apologies, victim blaming, empty promises, propaganda, etc. to un-do a nuke. Putin wants to remain in power - which doesn't happen if there's no country left to govern. reply baq 16 hours agorootparentprevdroughts are here now and will get worse in the coming decade. the amount of energy required to melt ice sheets (if that's what you're alluding to) is extremely huge and while it is certain they'll melt in a business as usual scenario (and possible if we stopped all emissions now...), it'll take hundreds if not thousands of years to get there. whereas extreme droughts are here today, now, as we speak. reply padjo 19 hours agoparentprevClimate change is analogous to a scenario we see all the time in nature i.e. a species finds massive success causing its population to spike, this population spike degrades the supporting environment and the species risks extinction. We dodged this in the 20th century with the green revolution but the risk remains. We still haven’t figured out how to live within the limits of our environment, instead we continue to extract from it and degrade it. If we don’t figure this problem out then our species is done for. Compared to this the other two are barely even risks. reply soulofmischief 19 hours agorootparentThe other two are really only risks if the climate change risk doesn't take us out. reply 127 21 hours agoparentprevOr maybe AI will just nuke humans to fix climate change. reply gessha 20 hours agorootparentAh, good ol Gandhi strats reply Jun8 19 hours agoparentprevOn the topic of “Mother Gaia” being a bloodthirsty bitch (how we humans like to anthropomorphize rather than grasp things as they are) see Tiptree’s excellent story: https://en.m.wikipedia.org/wiki/The_Last_Flight_of_Dr._Ain. We may have come close to this being reality four years ago perhaps. reply willyt 18 hours agoparentprevYou forgot the H5N1 with 50% mortality rate raw milk thing. There is an article on Ars about raw milk drinkers actively seeking infected milk in the belief it will immunise them against H5N1. I’m not a biologist but this feels like it could be very dangerous to everyone. reply aeonik 21 hours agoparentprevAGI will likely want low noise circuitry, so at least a rogue AI overlord would likely be aligned with the other two objectives. Can't have high intensity radioactive and thermal noise messing with those world dominating algorithms. reply darby_eight 16 hours agoparentprev> The risk of the rise of AGI can someone explain what the risk of this actually is? I just assumed it was a regulatory capture ploy. IMO it is massively distracting from the very real and very measurable impact the global industry has on our climate. reply kypro 16 hours agorootparentI understand why some people might not weight the probability and risk of AGI as highly as others, but to deny the risk entirely, or to act as if it's ridiculous to be concerned about such things in my opinion is just an intellectually ignorant position to hold. Obviously near-term AGI and climate change present us with near zero risk, and therefore certain groups will dismiss both. This is despite clear trends plotting towards very clear risks, but because these individuals are yet to see any negative impact with their own eyes it's all too easy for them to dismiss the risk as some kind of hysteria. But these risks are real and rational because the trends are real and clear. We should take both seriously. The nuclear risk is real too, but unlike AGI and climate change the risk isn't exponentially increasing with time. I think other weapons like bioweapons and drone weapons potentially fit that risk profile, however. reply darby_eight 16 hours agorootparent> deny the risk entirely, or to act as if it's ridiculous to be concerned about such things in my opinion is just an intellectually ignorant position to hold. This would take someone actually articulating what this risk you refer to is. All I have to go off of is Terminator, which is patently ridiculous. People in power would never relinquish this power to a computer without being able to dictate whom it benefits. reply KingOfCoders 21 hours agoparentprevMother Gaia does not care about anything. reply shrimp_emoji 20 hours agorootparentMother Gaia is a dirty bitch![0] 0: https://youtu.be/75_nisowGX8 reply bufferoverflow 12 hours agoparentprevTemperature problem can be solved relatively easily and cheaply by putting a bunch of reflective film all over the planet (mainly deserts). Getting CO2 out of the atmosphere is actually expensive. But we will have to do it. High concentrations of CO2 reduce IQ and make humans actually uncomfortable. reply zeckalpha 20 hours agoparentprevThis is the polycrisis. reply morkalork 20 hours agoparentprevLetting off a few nukes could slow climate change a bit right? I vaguely recall that all the dust kicked up by the hundreds of tests in the 50s and 60s had an effect. reply sushibowl 20 hours agorootparentPossibly, but you have to keep doing it. kicking up dust into the atmosphere tends to have significant short-term impact on the global climate (see e.g. volcanic winters: https://en.wikipedia.org/wiki/Volcanic_winter). Then the dust settles shortly and everything returns to normal. But volcanic eruptions that cause a winter like that tend to be quite a bit more powerful than your average nuclear weapon. More importantly, not all dust is created equal. Sulphur containing compounds tend to have the biggest cooling effect. One neat (in a terrifying sort of way) geo-engineering idea is continuously injecting large amounts of sulphur dioxide into the stratosphere. reply triceratops 19 hours agorootparentDrop a nuke into a volcano /s reply unicornhose 19 hours agorootparentMorbidly curious what would actually happen in this scenario. reply Filligree 17 hours agorootparentYou’d create a lot of body thetans. reply adrianN 20 hours agorootparentprevIt works best if the nukes hit densely populated areas, but I hope we manage to implement the solutions to climate change that don’t require killing billions. reply EasyMark 15 hours agoparentprevGaia isn't really a thing. There are only natural processes, chemistry, and physics, and there is definitely nothing motherly about them; they are ruthlessly efficient. reply deepfriedchokes 6 hours agorootparentIf that is true of Gaia then that is true of you as well. reply jibbit 20 hours agoparentprevnow theres a list that \"antibiotic resistant bacteria\" fits into quite nicely reply shrimp_emoji 10 hours agoparentprevWe are not at risk of runaway climate change. That's a common misconception and a convenient scare tactic. We're also not at risk of runaway AGI any time soon. LLMs are a joke. Minds are computationally irreducible, and all our supercomputers can barely hold a baby fruit fly's connectome. Unless you mean biological AGI. We're going to figure out genetic engineering far sooner than we'll get the hardware necessary for inorganic AGI. Once we master biology, we can genetically engineer organic super intelligences or, far sooner, superviruses whose genome can be uploaded into a nucleotide synthesizer in someone's garage. Then it's game over. Unless we've colonized Mars (which also helps with nuclear conflict). reply hsavit1 20 hours agoparentprevYou don’t understand the enormous immediate risk of climate change if you think that AGI is a comparable risk. Climate change is now and is killing people every day reply rdlecler1 20 hours agorootparentNature is out to kill us. We’re doing a far better job in this fight than we ever have. I’d rather deal with problem of climate change than the problems of our ancestors. reply margalabargala 20 hours agorootparentThat's not true at all. Remember CFCs and the ozone layer? That was a comparable problem, except people actually stopped that one, by no longer emitting the gasses causing the issue. reply SketchySeaBeast 20 hours agorootparentI don't know that's it's comparable. The ozone required manufacturing changes, it didn't require an upheaval on how we live. Sure, it's \"don't emit the bad gas\", but the gases come from different sources. reply mrexroad 16 hours agorootparentThe primary difference was that the ozone depletion didn't gain much traction as a political wedge and world leaders were able to take the threat seriously. 14 years after scientists published basic research warning of potential risk, the Montreal Protocol was signed. 25 years later there was a 98% reduction in release of ozone depleting substances and the ozone layer has begun to heal. Throughout all of that, DuPont lobbied and testified that ozone depletion was a hoax / fake news / scientists making stuff up / etc. Contrast that to today, where the entertainment news outlets have people, who don’t even cook, up in arms that someone will take away their god-given right to a gas range, and who in turn view it all as a hoax and conspiracy for corrupt politicians to profit. I’m not sure the world would be able to pull off the Montreal protocol today, even if largely manufacturing changes and having to find a new hairspray brand. reply margalabargala 14 hours agorootparentI have the sense that people back in the 60-80s had a bit of an innate trust for scientists born out of the rapid technological progress that preceded that time period, but that has since gone away. Things like CFCs were taken seriously. Things like radiation were taken seriously (for better or worse, yielding our insane regulatory landscape around building new nuclear power plants). The last major thing that scientists warned about that was really taken seriously (in the sense that something was done about it before it had/would have had massive negative effects) was world overpopulation, with the publication of things like \"The Population Bomb\" and China's one-child policy, etc. Unfortunately, that one was gotten wrong; we now know that without any intervention, world population will tend to moderate itself and we won't actually see mass starvation due purely to too many people. I think that error was the first major blow resulting in people no longer really trusting catastrophic predictions. I wonder what the world would be like if instead climate change was put forth as a catastrophic issue with the same fervor back then. reply smallerfish 12 hours agorootparentprevI don't think that's the primary difference. With the ozone layer, we already had alternatives to CFCs that were viable. A handful of companies lost out on product lines, but they're all still doing fine today. Individuals didn't have to do anything. With the problem of CO2 emissions, lifestyle changes are required to fundamentally solve the issue, and people aren't willing to make them. Yes, it's possible that geoengineering can buy us some time. It's possible there will be a battery revolution. Renewable energy is increasingly widespread. But there's nothing right now that's a drop-in replacement. The only sure-fire solution that we have right now is a widespread reduction of consumption and mobility, and very few people are on board with that. reply RHSman2 18 hours agorootparentprevBtw, we are nature, so you are right. We are the problem. reply wahnfrieden 20 hours agorootparentprevAI doomers don’t even care about its harms today such as being used for automated American death panel decision-making, something that the Victims of Capitalism Memorial Foundation will recognize someday reply the_third_wave 20 hours agorootparentprevYou have taken a few too many swigs from the Kool-aid dispenser. Climate change is always while climate-related events are killing fewer and fewer people every day. Humans are inginious beasts and will keep on lowering that number reply cduzz 20 hours agorootparentprevAnd climate change is now making many more people wealthy than AGI has and potentially ever will. Reverse Carbon Sequestration employs a very large population. reply usaar333 18 hours agorootparentprevClimate risk is extremely unlikely to be existential this century. And the mean GDP hits are pretty low all things considered. 30% or so in 2100. AGI actually has a plausible case for existential risk and would result in a much greater GDP shift. I'd put nukes above climate change. See does metaculus: https://possibleworldstree.com/ reply olalonde 20 hours agorootparentprevClimate related deaths are at an all time low...[0] [0] https://twitter.com/HumanProgress/status/1634509546067574790 reply duozerk 20 hours agorootparentAh yes, a \"source\" from Charles Koch's Cato Institute. Do you really expect to be taken seriously with this \"rebuttal\" ? reply olalonde 20 hours agorootparentHere's another source using data from the \"EM-DAT International Disaster Database\"[0]. Excerpt from the article[1]: > As we see, over the course of the 20th century there was a significant decline in global deaths from natural disasters. In the early 1900s, the annual average was often in the range of 400,000 to 500,000 deaths. In the second half of the century and into the early 2000s, we have seen a significant decline to less than 100,000 – at least five times lower than these peaks. This decline is even more impressive when we consider the rate of population growth over this period. When we correct for population – showing this data in terms of death rates (measured per 100,000 people) – then we see a more than 10-fold decline over the past century. [0] https://www.emdat.be/ [1] https://ourworldindata.org/natural-disasters reply amanaplanacanal 17 hours agorootparentBetter weather prediction probably has a large hand in that. reply Bluestrike2 19 hours agorootparentprevWell, for starters, the graph lopped off the first twenty years from the data set so that it could \"start\" from the massive peak in the 20s and 1930. The top reply to your original tweet is a retweet of two videos that rebut the graph.[0] It's further skewed by the use of decadal averages, which hid the fact that the greatest peaks included deaths that were either the direct result of--or greatly worsened by, conflict and/or a handful of specific policy decisions--food production failures during and conflicts such as the Zhili-Anhui War in 1920-21[1], floods that occurred during the Chinese civil war which dramatically worsened responses and recovery, the Holodomor in Ukraine in 1932-33 and the Soviet famine of 1930-33 more broadly, the 1938 Yellow River Flood[2] following the intentional destruction of dikes in an attempt to slow the Japanese Army's advance, World War 2 more broadly in the 40s where you had both the food production interruptions of war on a massive scale and explicit acts of mass starvation, the Great Chinese Famine in 1959-61 which is considered to be one of the largest man-made disaster in history,[3] etc. The graph falsely suggests that we've we've somehow stumbled upon a viable adaptation strategy that makes climate change nothing to worry about. Since 1900, we've seen massive medical advancements, improved early warning systems for at least some types of disasters, transportation networks and technology that helps move people away from disaster zones both before some disasters and in their aftermath, the ability to rapidly move large amounts of food to disaster areas, and more. Those are all great achievements, but the largest factor in the decline your chart suggests (albeit through data misrepresentation) is the fact that we don't have massive conflicts on the scale we saw in the first half of the 20th century, genocidal dictators looking to quickly wipe out millions of people through starvation, or political ideology driving inane agricultural policies that killed tens of millions of people because the autocratic dictators of some of the most populous nations on Earth read some pseudoscientific drivel (Lysenko and others managed to inspire not only the Soviet Famine in the 30s but also the Great Chinese Famine in the late 50s) and decided it sounded pretty ideologically reliable. We still have conflict and famine, but nothing on the same scale. Trying to take that and spin it as climate adaptation is, well, absurd. Even by climate skeptic standards, that argument's a real stinker. 0. https://twitter.com/TheDisproof/status/1633492932484374530 1. https://disasterhistory.org/north-china-famine-1920-21 2. https://en.wikipedia.org/wiki/1938_Yellow_River_flood 3. https://en.wikipedia.org/wiki/Great_Chinese_Famine reply olalonde 7 hours agorootparentNo matter how you want to spin it, there are relatively few climate related deaths today compared to the past. Climate change is not causing a rise in climate related deaths, which is what OP was essentially claiming. reply lesuorac 20 hours agorootparentprevFundamentally just because the current value is at a low point doesn't make something not a threat. The easy way to think about it by handwaving half-lives of an element. You start with 100 and end up with 50 for a 50% survival rate but also a raw loss of 50. Each of those remaining 50 still are going to have a 50% survival rate despite that the next raw loss is ~25. But yeah; you can challenge the source of the argument as invalid as opposed to just challenging the argument as invalid. reply kylebenzle 20 hours agoparentprev1. AGI doesn't even exist and Sam Altman and I agree on one thing, GPTs will not get us there. Saying AGI is a risk is like saying the sun exploding is a risk. 2. Nuclear conflict is real, it's astounding Pakistan or Russia hasnt used the bomb yet but they will when the US backs them into a corner. 3. Climate change? At this point, who cares? We know it's happening and happening VERY slowly so we have PLENTY of time to get ready. Theres no real risk other than failing to move away from the cost in the next 100 years. International shipping and big ag are the largest polluters by far and those ain't stopping. reply huygens6363 20 hours agorootparentFear of AGI is not based on Altman’s latest brews. Look at the progress curve of AI in general is one way to get a feeling for it. The other one, which I prefer, is looking at existing AIs for which we thought human ingenuity was top notch and how they are beating us left and right. Strategy, creativity, cunning, all human notions that are being slaughtered. Slowly, but surely. It is “just” a matter of combining. I think it’s an “engineering problem” by now. Which is not to say that it will happen soon, just that if we set our minds to it it won’t be that big of a deal. reply blueflow 20 hours agorootparentI'm patiently waiting for my sysadmin job to be automated away. AGI, its time to deliver! I'm waiting. Should have happened yesterday? reply Workaccount2 20 hours agorootparentAGI by 2030 is not a reach statement at all anymore. The LLM's we have today weren't supposed to show up until 2030 at the earliest and 2050 more realistically. If you go back 10 years and read what people were writing about AI progress, you will find that they were _completely_ off the mark. It's prudent to assume a very liberal timeline for AI progress, based off whats been happening now. reply blueflow 20 hours agorootparent10 years ago we just had another AI hype. I want to see delivered promises & my job automated before i change my mind. reply trashtester 19 hours agorootparentWe're still in the same AI hype as 10 years ago. You just stopped noticing for a while. It really hasn't stopped since AlexNet back in 2012. Personally, since I first started to learn about Neural Nets in the 90's, I've always predicted that they would bring AGI around the time they matched human brains in complexity and compute power. Back then, assuming Moore's law meant that would be between 2030-40. Now, there was always the prospect of a serious breakdown of Moore's Law, which might delay it, but since that didn't happen (compute power just moved from CPU's to GPU's), we're now approaching the final stretch. Human brain scale neural nets are already being planned, if not already being built in secret. reply Workaccount2 19 hours agorootparentprevEvery promise has over delivered by a decade or more, so don't worry, you'll likely have your legs up 20 years faster than promised. reply Delmololo 18 hours agorootparentprevThe fact that you can now ask an LLM in a chat like fashion is absolutely irrelevant to you? Not an achievement at all? Talking to an ai? Generating any image you like? Etc.? reply short_sells_poo 19 hours agorootparentprevI'm loving this absolutely breathless parroting of \"AGI in the next 5 years\" and yet: we don't even know what general intelligence is. We don't know how/why/where consciousness arises. It's not even a clearly defined concept. But here we have \"Workaccount2\" prophesizing the rise of general intelligence :D Please enlighten us sensei, what is actually General Intelligence, and how will you know it's here? I realize my reply is a bit flippant, but it's tiring to see how certain are people about something that nobody truly understands. reply trashtester 19 hours agorootparentAGI has nothing to do with consciousness, unless consciousness is for some reason required to attain it (which doesn't seem like it's the case). AGI simply means that we can create a type of machine that can do any kind of mental task that a human can, at least at a human level. reply short_sells_poo 18 hours agorootparentConsciousness appears to be a form of intelligence. Is it required for general intelligence (whatever that means)? We don't know. Would humans be classified as having general intelligence if we weren't conscious? In any case, I perhaps worded that poorly. I wasn't saying that consciousness is the same as intelligence, it was just an example of how little we actually understand intelligence. > AGI simply means that we can create a type of machine that can do any kind of mental task that a human can, at least at a human level. What does this even mean? What is human level? The ability to write a symphony? Or the ability to calculate 2+2=4? These are such poorly defined metrics that I don't understand how can we then start throwing around very precise (and short!) timelines for attaining it. The statement: \"we'll send a human mission to Mars in the next 5 years\" is 10x more believable than this breathless AGI hype, because at least the former is well defined. reply trashtester 16 hours agorootparent> I wasn't saying that consciousness is the same as intelligence, it was just an example of how little we actually understand intelligence. I would say that current AI's do have a kind of intelligence. It's just not generic enough that we label it AGI. Add an ability to form, optimize an execute plans while also increasing the physical world model, and we're getting really close. Both of these abilities are already being developed. reply Workaccount2 19 hours agorootparentprevIronically I often post the exact same sentiment that you are sharing. But you are conflating two different things. AGI doesn't require consciousness. We likely will have no idea whether or not the first AGI computer is conscious or not. What we will know though, is that it is solving novel problems that humans struggle with, and from the outside the only way to discern it isn't another human is because it is way to smart to be one. I strongly suspect that the first computers which actually are conscious (AGI or not), whenever that happens, will have to fight a heavy uphill battle to prove it. And there will always be a group of people who will never believe an AI (is it even artificial at that point?) is conscious. reply short_sells_poo 18 hours agorootparentI didn't say that AGI requires consciousness, just that we don't understand what intelligence truly is, along with consciousness (which appears to be a form of intelligence). And I'm not particularly keen of the reasoning: \"it is solving novel problems that humans struggle with\", because a pocket calculator can solve problems that humans struggle with. A computer can play chess better than a human can, does it mean it's an AGI? reply Workaccount2 15 hours agorootparentWikipedia has an entire article for helping to understand what \"AGI\" refers to: https://en.wikipedia.org/wiki/Artificial_general_intelligenc... reply huygens6363 5 hours agorootparentprevI don’t know what “sysadmin” is today anymore, but we don’t need AGI to get rid of that. reply trashtester 19 hours agorootparentprev25 years ago, I did a back-of-the envelope calculation during a party and came up with roughly 2040. I didn't know that Kurzweil had already predicted 2029. Once I heard about it, I felt Kurzweil was over-optimistic, and stuck to 2040. Recently, though, it has started to seem that AGI before 2029 is more likely than 2040 or after. reply TeMPOraL 20 hours agorootparentprevThe only AI remotely generic enough for that exists for like 2 years. Give it a few more, you might just get your wish. reply kylebenzle 20 hours agorootparentMaybe. reply BirAdam 20 hours agorootparentprevWell, also, plastic waste getting dumped in water ways, and in the USA, steel and coal waste being dumped into the Ohio (causing a huge dead zone in the Gulf). There are serious pollution problems everywhere, and I kind of hate the climate change narrative for taking people’s eyes off of those problems. reply trashtester 19 hours agorootparentAI is already developing enzymes that can break down plastics. Put them on bacteria, and that plastic may stop being a problem quickly. reply kmeisthax 18 hours agorootparentWhich is great until plastics that we want to keep using start getting eaten by bacteria. reply trashtester 18 hours agorootparentsure. we may have to chose whether or not plastics should be biodegradable. Cant have your cake and eat it, too. reply Filligree 17 hours agorootparentWood is biodegradable, but wood houses work anyway. Ditto cotton. So long as you keep things dry they don’t tend to rot. reply trashtester 16 hours agorootparentExactly. Plastic will be the same at some point. Natural selection would probably lead to plastic-eating bacteria anyway. DeepMind's enzymes are just a way to speed it up. reply KronisLV 20 hours agorootparentprevHey, aren't you the user who was trolling I'm that one thread about someone making an app? https://news.ycombinator.com/item?id=40199670 Oh well, some concerns around AGI might be attempts at marketing and attracting attention, though some caution probably isn't too bad to have. AGI might become a thing, it's just that there's no guarantee that it will happen in our lifetimes. On the other hand, even LLMs will put some folks out of a job. Relations between nuclear powers are anyone's guess but there's no reason why Russia couldn't be forced to back off, as opposed to \"being backed into a corner\". Wouldn't be the first proxy war and most likely won't be the last. I care about climate change, there are certainly others. No idea whether much of a change will be made in our current course, but there's no reason to be super dismissive. If anything, recycling, eating a bit less meat or doing lots of the other common recommendations improves my own quality of life, even if the legislature that goes after the big corporations is nowhere to be seen yet. No reason not to make the world a better place, or ar least try to, in whatever ways are viable, like donating towards planting trees or something like Wren. reply duozerk 20 hours agorootparentprev> happening VERY slowly > failing to move away from the cost in the next 100 years Unless you're a time-traveler from the 50s who has somehow managed to post here, there is no excuse for this type of disinformation these days. Mentioning higher sea levels is also a red herring; massive agricultural yields collapse will be an issue long before (like, this century) sea levels become a major problem. More than this, we have now reached levels of atmospheric changes that put actual near-term Human extinction (not to mention that of most sea and land species) on the table. reply trashtester 19 hours agorootparentThis doesn't seem to agree with IPCC predictions. What's your source? reply duozerk 18 hours agorootparentDespite the IPCC being very conservative/optimistic in their scenarios, this is in fact in line with their reports; admittedly my comment above might be badly worded - when I said \"near-term\", I meant \"in the upcoming two centuries or so\". The IPCC reports say we might reach 4C, 5C or even more; based on the historical record, such a major change in such a short time - several orders of magnitude faster than previous CO2e-gases-linked mass extinction events - likely cannot be adapted to by the majority of species (there will of course also be a few evolutionary winners), resulting in potential extinction. I also quote the latest draft report from that same IPCC, leaked about two years back by concerned involved scientists to newspapers before the usual step where political stakeholders are allowed to reword the parts they deem too disturbing or against their interests: \"Life on Earth can recover from a drastic climate shift by evolving into new species and creating new ecosystems,\" it says. \"Humans cannot.\" reply baq 16 hours agorootparentThe IPCC also consistently underestimated sulphur in the atmosphere because if you look at this and previous years we've reached their 2030-2035 goal of warming this year. > GHG emissions will lead to increasing global warming in the near term, and it’s likely this will reach 1.5°C between 2030 and 2035. https://climatechampions.unfccc.int/the-ipcc-just-published-... > Global temperatures have been exceptionally high over the past three months – at around 1.6C above pre-industrial levels – following the peak of current El Niño event at the start of 2024. > The past 10 months have all set new all-time monthly temperature records, though the margin by which new records have been set has fallen from around 0.3C last year to 0.1C over the first three months of 2024. https://www.carbonbrief.org/state-of-the-climate-2024-off-to... reply trashtester 18 hours agorootparentprevThere is lots of space on earth that would be a lot more hospitable to humans if 5C warmer. Like the central eurasian steppe. reply duozerk 18 hours agorootparentYou can't grow food for a large population when average planetary temps are at +5C. It doesn't mean it's locally always +5C warmer than it used to be; it means you're seeing insane temperature swings in a matter of days, constantly - in both directions, it just so happens that the average is +5C. Not to mention, at +5C it is all but certain shallow methane hydrate deposits (those stabilized by temperature, not pressure) all over the world are now outgassing CH4; not to mention several other similar tripwires, and likely not all of which we've even identified. Edit: I can't seem to be able to reply to your comment below, not sure why. You're absolutely wrong about those Siberia figures, and they're not supported by current scientific consensus. reply trashtester 18 hours agorootparentIf you farm a siberia that's 5 degrees warmer and more humid (also in ipcc data), you can maybe feed 10 billion just from there. reply baq 16 hours agorootparentprevaverage is an average, you're forgetting about extremes. it only takes a few days (or hours) of above wet bulb temperatures to kill humans who live in and around the tropics today. https://en.wikipedia.org/wiki/Wet-bulb_temperature reply trashtester 16 hours agorootparentSo people will move out of those worst affected areas. reply baq 15 hours agorootparentExactly. Hundreds of millions of them. reply trashtester 12 hours agorootparentSure. That could happen. Or there could be wars/conflicts resulting in some of those not having anywhere to go. Still, this scenario is quite different from human extinction, which is what I was responding to further up. reply triceratops 18 hours agorootparentprevWhat's the soil quality over there? reply duozerk 18 hours agorootparentMuch of the soil in Russia that will warm is indeed acidic and largely inexploitable, which I suspect is what you were getting at; to their credit though, they picked an area that isn't (IIRC). Not that it changes much. reply trashtester 16 hours agorootparentMost soil can be used for some type of farming with some help from fertilizers. Temperature and access to water is more critical. Higher CO2 also increases crops, in isolation. We're also talking about a time span of 100-300 years. Keep in mind how much more efficient farming is now compared to even the 1950's. reply jmyeet 19 hours agoparentprevI'm not that worried about nuclear conflict because it's a scenario where literally nobody wins. There's always the \"Madman\" hypothesis but really that's no way to do geopolitical analysis. Nobody is truly \"crazy\" (IMHO). Remember those orders have to be carried out by someone. They've studies on this with missile silo operators and they had a disturbing or comforting (depending on your POV) tendency to not launch. It's really the \"slow death\" scenarios that are a much bigger risk. While I'm firmly in the AGI camp, I'm both fatalistic about our willingness to do anything about it but I'm also highly skeptical of the \"runaway climate change\" doomsaying. The Earth has been around for ~4.5 billion years. While it's only been similar to what it is now for th elast 300 million or so, that's still a really long time. We've had periods where ice extended to the equator (~500 million years ago). We've had much warmrer periods. The Earth will be fine. We however might fall by the wayside. There's really been such a long period of time that if runaway climate change were going to happen, why hasn't it happened already? reply trashtester 19 hours agorootparent> There's really been such a long period of time that if runaway climate change were going to happen, why hasn't it happened already? I don't think runaway climate change or even something like a reversal of the Gulf Stream were ever any of the mainstream scenarios from IPCC. The worst scenarios, IIRC, were a global warming of +6 to 10C. And that is over several centuries, provided we do nothing to stop it. This century, the worst scenarios are about 4C hotter than today. Also, it's not like the temperature is going up that much everywhere. For instance, heating an area near water from 34 to 38C means a lot more water evaporation, and thus more cooling. Also, stronger winds mean the humidity may be blown away more quickly. Now even 4C of heating, provided we don't develop any technologies to either counter it or cope with it could cause a disaster that could cause similar disruptions, forced displacements etc as WW2, but the world didn't end because of WW2. It was only a minor speed bump in the grand scope of things. Anyway, the probability that it should take 100s of years to reach AGI seems quite slim. And once we have AGI/ASI, the world is going to be so fundamentally different that I'm not sure how much some warming really means, at least for humans. reply duozerk 18 hours agorootparentAt 4C of heating, you cannot grow food in any reasonable quantity with any reliability, period. 4C is the collapse of modern civilization at the very least (in fact likely earlier due to increased geopolitical instability and tensions due to dwindling resources, combined with the availability of nuclear weapons), with a massive die-off of humans along with it. reply trashtester 18 hours agorootparentMany places could increase food production to twice what it is today if 5C warmer. reply duozerk 18 hours agorootparentWe're at +1.5C, more or less, and growing the usual grapes in France - of all places - is already becoming much harder (they keep dying of frost after waking up due to wild temperature swings). At +5C, there is no growing food in any substantial amount outside of high tech, low yield approaches; approaches that depend on complex planetary supply chains (both for initial deployment and maintenance), which will have disappeared by then. reply trashtester 16 hours agorootparent> At +5C, there is no growing food in any substantial amount outside of high tech, It seems you have some specific geographic region in mind. Earth has a lot of regions that are more than 5C colder than the most fertile regions. People adapt. People move. Sometimes they fight wars about it. This has happened many times before. We're currently living it the most peaceful, prosperous and safest period that humanity has ever experienced. (Despite what social media is tricking our brains into believing). In the future we will surely live through periods that are closer to the average. But I'm not seeing any extinction-level events due to climate change within the next few hundred years. AGI or nukes, on the other hand, they both DO have the potential to end us as a species. reply mrguyorama 16 hours agorootparentprev\"5C warmer\" doesn't mean a uniform increase in temperature of 5 degrees at all times. It means \"5 * the thermal mass of the earth's biosphere\" worth of extra energy in an extremely chaotic system that is currently in a local stable point, but doesn't have to stay there. reply trashtester 16 hours agorootparentA 5C warming is indeed likely to make a few areas uninhabitable. I'm not saying climate change is not a problem. I'm just saying it's not an extinction event. But keep in mind that the areas that tend to get the greatest warming tend to be the dry ones. In such places, sweating will still allow human bodies to regulate body heat, if air conditioning breaks down. reply mrexroad 16 hours agorootparentI think you missed the above comment’s point about risk to stability in a chaotic system… reply trashtester 15 hours agorootparentThat part looked like gibberish.... Maybe rewrite to make it more clear? reply dTal 11 hours agorootparentFrom a different comment whose main thrust you also ignored (you are all over this thread with the same fallacy): >It doesn't mean it's locally always +5C warmer than it used to be; it means you're seeing insane temperature swings in a matter of days, constantly - in both directions, it just so happens that the average is +5C. reply trashtester 8 hours agorootparent\"Wild temperature swings\" is already quite common in dry places. Sahara can be below freezing during the night. The IPCC predicts that Southern Europe will get dryer, so they will have larger variations. More humid areas, especially if they're near coasts tend to be a lot more stable. I think these tendencies will remain true. While introducing more energy to the atmosphere is likely to generate more winds (including hurricanes), it doesn't seem plausible to me that (given constant humidity) this will be enough to cause enough variation in temperature to make farming impossible in most places. If you have some reference (preferably something like IPCC, as opposed to something that could be fringe), I would be willing to reconsider. > you are all over this thread with the same fallacy Maybe you could state exactly what fallacy you think I'm advocating for? I'm not saying global warming is something good and that we don't need to worry about. I just don't think it's a likely extinction level threat, like an asteroid, rogue AI, nuclear war, an alien invation etc. Unlike those others, it's almost certain that we will experience some degree of global warming, but if that's the worst we will face, humanity will survive as a species. Also, there is a couple of other factors: First of all, I think many that worry about climate change (beyond those who honestly think it will lead to extinction) really care mostly about all the pain and suffering that global warming could bring at some point, at least to some large minority of humanity. Maybe also that it will cause a non-trivial fraction of the population to die from famine, wet bulbs etc. I don't think that's impossible (though maybe a bit pessimistic, see below), but I think the fallacy that the these people make, is to assume that we will have a future world without such events. Historically, we have seen that bad things have happend from time to time. The collapse of the Roman Empire caused half the population to die off (partly due to colder weather). The Black Death caused a similar percentage to perish in many place. The Mongol conquests resulted in large parts of the Eurasian steppes to be so depopulated that they still haven't recovered. Then the were the world wars, Bronze Age collapse, and the list goes on. The future is likely to bring similar events, too. Climate change could possibly be such an event. But usually, these events are not those we expect, but rather some kind of Black Swan that surprises everyone. The second fallacy that some climate change fanatics seem to ignore, and this one by choice, it seems: We're still very much in a kind of exponential technological development. 200 years is a very long time, and unless the technological development suddenly grinds to a complete halt, we will have a lot of new options both when it comes to",
    "originSummary": [
      "Oregon State University researchers have discovered the most rapid natural increase in carbon dioxide levels in the last 50,000 years by studying ancient Antarctic ice.",
      "The current rate of atmospheric CO2 rise is ten times quicker than any other historical period, primarily due to human activities, as per the study published in the Proceedings of the National Academy of Sciences.",
      "This research offers valuable insights into historical climate change events and the potential ramifications of today's climate change, emphasizing the unparalleled levels of CO2 in the atmosphere."
    ],
    "commentSummary": [
      "The article addresses the challenges of the fastest natural carbon dioxide rise in 50,000 years, alongside risks of artificial general intelligence, nuclear conflict, and climate change.",
      "It highlights human ingenuity's crucial role in tackling these issues and cites historical cases of averting nuclear disasters.",
      "Discussions cover rising CO2 levels, solutions like stratospheric aerosol injection, and differing views on the urgency and feasibility of emission reductions to combat climate change, while also touching on science fiction's societal impact and debates around AGI risks."
    ],
    "points": 249,
    "commentCount": 451,
    "retryCount": 0,
    "time": 1715775238
  },
  {
    "id": 40365198,
    "title": "Oracle switches to OpenTofu from Terraform",
    "originLink": "https://www.thestack.technology/oracle-dumps-terraform-for-opentofu/",
    "originBody": "Oracle has swapped Terraform for the open-source fork OpenTofu under the hood of its Oracle E-Business Suite (EBS) Cloud Manager. It is now telling customers they “must” make the shift to its new OpenTofu-based version of the migration/provisioning tool by June 30, 2024. Oracle EBS is a suite of business applications including CRM, procurement and supply chain software. “Cloud Manager” is what it describes as the “primary tool” for EBS customers looking to adopt Oracle Cloud. It can be used for migrating Linux-based environments, provisioning new environments, and performing lifecycle management activities. (Unless The Stack is sorely mistaken, \"Cloud Manager\" is a lick of Oracle paint on an underlying open source Terraform engine; or was... ) Oracle’s OpenTofu shift In a short blog, an EBS product director at Oracle said that its “latest Oracle E-Business Suite (EBS) Cloud Manager update, 24.1.1.1, is now available,” urging customers to update “at your earliest convenience.” “In this release, we have switched from Terraform to OpenTofu due to forthcoming Terraform licensing changes,” wrote Terri Noyes. “ You must therefore upgrade your Cloud Manager to 24.1.1.1 by June 30, 2024 at the latest,” she said, adding that the release updated further components to “maintain security standards and improve supportability.” The move comes after Hashicorp’s 2023 decision to adopt a more restrictive BSL 1.1. Licence for Terraform and other products, instead of the MPL 2.0 that it previously used. That move triggered a swift fork, OpenTofu, adopted by the Linux Foundation. (Version 1.7 landed in March 2024 and was widely considered to be the first enterprise-ready version.) Oracle’s move seems like a straightforward decision to ensure it is using the most permissive underlying IaC tool without having to worry about downstream licence complications, no more and no less. OpenTofu is essentially a pretty familiar drop-in replacement for Terraform; using it here is ultimately a minor implementation detail for those looking to move complex EBS environments to the cloud, but it does signal that serious enterprises feel the fork is already robust enough to use. In late April IBM agreed to buy Hashicorp for $6.7 billion. See also: \"Red Hash\"? IBM vows to run the Red Hat playbook after $6.4 billion HashiCorp buyout",
    "commentLink": "https://news.ycombinator.com/item?id=40365198",
    "commentBody": "Oracle dumps Terraform for OpenTofu (thestack.technology)249 points by p1nkpineapple 23 hours agohidepastfavorite173 comments cube2222 18 hours agoIt's great to see more companies adopting OpenTofu, and especially larger ones! As a side note, we've recently released OpenTofu 1.7 with end-to-end state encryption, enhanced provider-defined functions, and a bunch more[0]. If you've been holding out with the migration, now is the perfect moment to take another look, and join the many companies that have already migrated! [0]: https://github.com/opentofu/opentofu/releases/tag/v1.7.0 Note: Tech Lead of the OpenTofu project, always happy to answer any questions reply bloopernova 16 hours agoparentAre there any plans for a conditional way to enable/disable modules that doesn't use \"count\"? For example: # current method module \"foo\" { count = var.enable_foo ? 1 : 0 } # better? module \"bar\" { enabled = var.enable_bar } Preconditions and postconditions fail the apply run if their condition doesn't validate, so those can't be used. I'd also really like to be able to say in an output block, \"this value doesn't have to exist, only display it if its parent module is enabled\", again without the \"count\" attribute. reply cube2222 16 hours agorootparentThe relevant issue[0] is currently the 7th top-voted[1] feature request, so it's definitely high on our radar. Please upvote it as well, if it's important to you! There's a bunch of nontrivial technical complexity though, because of how OpenTofu currently works. [0]: https://github.com/opentofu/opentofu/issues/1306 [1]: https://github.com/opentofu/opentofu/issues/1496 reply paulddraper 10 hours agorootparentprevUse CDKTF. All of these \"how can I do X in HCL\" just go away. reply x86a 16 hours agoparentprevI'm really excited to see the end-to-end state encryption. I've always thought it was bizarre that Hashicorp didn't prioritize this. reply andreasmetsala 13 hours agorootparentCould it be because it weakens the business case for using their SAAS? reply x86a 13 hours agorootparentPossibly, but we are paying enterprise customers (but not using HCP) and this still isn't possible. Seems like an obvious thing they could have at least offered to vault enterprise or TF enterprise customers years ago. reply hintymad 16 hours agoparentprevThe trend will continue. A company will be crazy to trust that IBM would give them fair-priced high quality support. reply solatic 16 hours agorootparentDo you appreciate the irony of that comment on a post about Oracle adopting OpenTofu? reply organsnyder 16 hours agorootparentI'm sure Oracle doesn't want to be gouged by their vendors any more than the rest of us do. They probably don't buy Oracle either. reply playingalong 16 hours agorootparentprevYep. Funny. In this case Oracle is the user, not the vendor. reply colechristensen 16 hours agoparentprevAre there any incompatibilities cropping up between terraform and opentofu? I believe we're on terraform 1.7.5, I'm not sold on migrating yet but would like to keep the option open, especially if something like delaying an upgrade would help not have future backwards incompatible things to fix. I understand why people were upset about licensing changes but I was not one of them who were particularly bothered. Why should I switch? reply cube2222 16 hours agorootparent> Are there any incompatibilities cropping up between terraform and opentofu? OpenTofu is indeed a hard fork. When doing similar features (like provider-defined functions) we try to stay compatible where it makes sense, but there's often some differences (like our more extended capabilities of provider-defined functions[0]) and also new features in Terraform that we're not introducing - and vice versa. You can check for known incompatibilities in our migration guides[1], based on your Terraform version. In practice, the longer you wait, the more the projects will diverge, so if you still want to \"wait and see\" I would suggest settling on your current Terraform version for now - otherwise, the migration will just be more work for you later. Regarding the reasons for switching, I'd say features and community-first process. We're striving to be very community driven in what work we're prioritizing[2] and have received a lot of positive feedback over that from our users. Some companies we've spoken to see adopting the open-source community-driven project as a way to reduce risk long-term. It's also a way to keep your options open if you're in the market for commercial Terraform/OpenTofu management systems. [0]: https://github.com/opentofu/terraform-provider-go [1]: https://opentofu.org/docs/intro/migration/ [2]: https://github.com/opentofu/opentofu/issues/1496 reply Lockal 22 hours agoprev\"Oracle’s move seems like a straightforward decision to ensure it is using the most permissive underlying IaC tool without having to worry about downstream license complications, no more and no less.\" Hm, with that logic they could dump MySQL in favor of MariaDB as well reply freedomben 21 hours agoparentEspecially ironic, given that Oracle is one of the nastiest and most aggressive companies at enforcing license terms. reply geodel 19 hours agorootparentThats the way world works. Here in this forum so many software people asking for best possible salaries and perks, and when it comes paying a bit to good productivity software same developers are always full of excuses like. 1) Oh, I prefer open source alternative for ideological reason. 2) This software is not really worth that much. 3) Hectoring developers every single time in providing why their software should be preferred over unpaid alternatives. 4) Blaming companies that they are bigger users so they should pay not me. If these entitled developers who deserve all the money but no one deserve their money just shut the fuck up every once in a while it will be a good thing. reply jayd16 18 hours agorootparentIt's important to remember that a community is not a single minded entity. It's members can hold many contradicting beliefs, while each individual is ideologically consistent. This shouldn't be unexpected and it's not an excuse to be dismissive to an imagined hypocrite. Not saying there aren't hypocrites in this world, just that we shouldn't treat members of a community as some kind of superset of everything in that community. reply hitekker 3 hours agorootparentIt's the upvote where that opinion falls short. Sure, it may just be one hypocrite, but it sure is funny when a community raises scarcely concealed hypocrisy to the top comments or sub-comments. And then forgets about it 2 weeks later. reply geodel 18 hours agorootparentprevIts not unexpected and neither is Oracle's approach unexpected still its worth talking about. Besides I made observation about people in community and not community itself as I did not say HN thinks software should not be paid for. reply sangnoir 16 hours agorootparent> Besides I made observation about people in community[...] Can you link any specific HN user who holds any 3 of those specific beliefs, or was this hypocritical strawman purpose-built to bolster your argument? reply vlovich123 16 hours agorootparentprevIt's important to remember that a company is not a single minded entity. It's members can hold many contradicting beliefs, while each individual is ideologically consistent. reply jayd16 15 hours agorootparentThey often do have a hierarchical command structure and that should entail some top down consistency and some accountability rolling upwards but you're not wrong. Believing every employee at Walmart thinks the same is silly and while someone is to blame for policy its important to not blame retail clerks for store policy, for example. reply talldayo 18 hours agorootparentprevI mean, sometimes the entitled developers are right and predict that the monetization of a software product will lead to it's inevitable demise. More often than not that's how these sorts of projects end up. Linux as a whole exists because developers said \"fuck AT&T, we're taking this train off the rails\" and nobody ever looked back since. reply scarface_74 18 hours agorootparentAnd the majority of code in Linux is created by corporate employees getting paid to make changes. Those companies are merely helping to “commoditize their complements” reply talldayo 18 hours agorootparentThat's not really a problem as long as the source license stays the same. If Amazon or Microsoft need a feature in the kernel, nobody tends to care as long as it's GPL. > Those companies are merely helping to “commoditize their complements” That's how they justify it internally, yeah. From an administrative standpoint it's pretty obvious that they all choose Linux because it's easier than retrofitting proprietary UNIX for modern software. But indeed, they market it as goodwill and complimentary development. reply wcedmisten 14 hours agorootparentI think you may have misinterpreted the parent comment. \"Complement\" as in a complementary good in economics terms. Not \"complimentary\", as in free. There's a good article on this by Joel Spolsky https://www.joelonsoftware.com/2002/06/12/strategy-letter-v/ reply Timshel 21 hours agorootparentprevIn a way they know the worst that can happen :D reply SteveNuts 20 hours agorootparentYeah this is absolutely projection on Oracle's part reply totaldude87 19 hours agorootparentprevAt oracle- Hey, what if they do - what we do to other companies ... to us ... presses a red button reply globular-toast 21 hours agorootparentprevThere is no contradiction here. It's exceedingly simple: companies will take as much as they can and give as little as they can. That's why it's important to raise the bar on what they have to give and reject all permissive (non-copyleft) free software licences. reply sneak 21 hours agorootparentCopyleft is not required, free software is a gift freely given. Even public domain is ok (weird places like Germany that don’t have public domain notwithstanding). What must be rejected is nonfree licenses like the BSL. reply Pet_Ant 19 hours agorootparentBSL is preferrable to completely closed because at least researchers can look at it now and it will eventually transition to open source. If Windows XP was BSL licensed then Wine would have a lot less trouble now. reply kees99 19 hours agorootparentCitation is sorely needed for both \"transition\" and \"BSL XP be good for wine\" claims above. Specifically, supposed inevitability of BSL->OSI transition is dubious. If anything, there are examples of the opposite - terraform itself being prime one. reply Pet_Ant 18 hours agorootparent> Citation is sorely needed for both \"transition\" Sure! [1] > The Business Source License requires the work to be relicensed to a \"Change License\" at the \"Change Date\". The \"Change License\" must be a \"license which is compatible with GPL version 2.0 or later\". The Change Date must be four years or sooner from the publication date of the work being licensed So the business source license is less \"non-OSI\" and more \"not currently non-OSI, but eventually and irrevocable at future date\". In the case of Terraform it says [2]: >Change Date: Four years from the date the Licensed Work is published. >Change License: MPL 2.0 So is this ideal? No. But it's better than OpenVMS screwing over historians and hobbyists [3] decades after it's relevancy has expired.[6] It's also better than SSPL [4] which has no such transition and stays permanently non-OSI [5]. > \"BSL XP be good for wine\" claims above. Well Wine uses the LGPL, and Windows XP was released in 2001 so even if they set the expiry 20 years after release, it'd be GPL'd by now. --- [1] https://en.wikipedia.org/wiki/Business_Source_License#Terms [2] https://github.com/hashicorp/terraform/blob/main/LICENSE [3] https://www.theregister.com/2024/04/09/vsi_prunes_hobbyist_p... [4] https://en.wikipedia.org/wiki/Server_Side_Public_License [5] https://web.archive.org/web/20230411163802/https://lists.ope... [6] https://www.theregister.com/2013/06/10/openvms_death_notice/ reply kstrauser 15 hours agorootparentprevI disagree. I don’t face any copyright issues from writing code that resembles something in Windows. I never had access to its source code, so any similarities have to be purely coincidental. A BSL project could say, hey, look at this guy stealing our code!, even if I’ve never seen it. I could have, and that opens a plausible risk I wish I didn’t have. reply skissane 15 hours agorootparent> I don’t face any copyright issues from writing code that resembles something in Windows. I never had access to its source code, so any similarities have to be purely coincidental. > A BSL project could say, hey, look at this guy stealing our code!, even if I’ve never seen it. I could have, and that opens a plausible risk I wish I didn’t have. By that argument, you could have looked at Windows code too, since Windows source code has leaked multiple times, and 5 minutes of searching will find it. reply globular-toast 21 hours agorootparentprev> Copyleft is not required Yes it is. Because companies (like Oracle) will take as much as they can and give as little as they can. > free software is a gift freely given It's a gift to the public, not to individuals and companies (like Oracle). > Even public domain is ok Even worse because that expressly allows companies (like Oracle) to take everything and give nothing. reply therealpygon 20 hours agorootparentAll of which is well understood by anyone who release permissively licensed software? If they didn’t accept that, they could have used a non-commercial license. If they expected contributions they could have sold a paid product. I’d suggest not using others hard work as the basis for your argument. If it was your work and you regret it, say that. If you don’t like oracle, say that. Otherwise, people who contribute to FOSS software do so knowingly, yet you are trying to inject your own opinions of “public” vs whomever, as though you know better than those contributors own feelings and intentions. reply lolinder 19 hours agorootparentprev> Because companies (like Oracle) will take as much as they can and give as little as they can. Which in the case of free software is a completely neutral fact that causes exactly zero negative impact to the project. You're trying to apply principles of scarcity to a product category that has no scarcity—replicating the bits to serve Oracle doesn't cost a maintainer anything at all. They can prefer not to let Oracle use their otherwise-freely-provided software, but that's not a position that's as easy to get sympathy for as pretending there's harm done. reply bigstrat2003 16 hours agorootparentprev> Yes it is. Because companies (like Oracle) will take as much as they can and give as little as they can. Yes, they will. So? Nobody is actually harmed by this. The software is still perfectly available for the public to make use of. > It's a gift to the public, not to individuals and companies (like Oracle). The public is not some separate entity from individuals or companies. It's simply the collective of all individuals and companies. So yes, when you gift something to the public it's a gift to Oracle as well. It's not exclusively to them, but they are a part. reply matt_heimer 20 hours agoparentprevFunny that you mention MariaDB. Oracle bought MySQL which was forked into MariaDB. MariaDB created the Business Source License (BSL). Hashicorp switches Terraform to BSL which then leads to Terraform being forked into OpenTofu. OpenTofu seems to be getting adopted by Oracle. reply oschvr 15 hours agorootparentFull circle of corporate facepalm reply croes 22 hours agoparentprevOracle owns MySQL but not Terraform reply ethbr1 21 hours agorootparentAh, so they meant avoiding 'downstream license complications' they don't cause. reply baobun 21 hours agorootparentI wouldn't expect Oracle to have a license complication with itself... reply dijit 19 hours agorootparentit's mostly just hypocritical. If everyone acted like Oracle; there would be no mysql users. Which is the point being made. reply snapcaster 13 hours agorootparentI don't understand your comment. Oracle is hypocritical because like every company they take everything they can and give the minimum they have to? which part is hypocritical? reply dijit 13 hours agorootparentI'm incredulous that you don't understand, but I'll humour you. Let's see; If I giving away a product because I think it's for the betterment of mankind, and definitely not an attempt to rug-pull or anything like that: no, just for developer good will. Then I am offered a free service, and I do not use it for fear that there could potentially be some rugpulling, despite having a reputation for that myself: and the project I'm considering having no reputation for that. Then the pretense in which I \"give away\" my software, is morally dubious. I would never permit myself to be in the same situation as I need others to be in order for my product to be successful. MySQL/Virtualbox/Java etc; reply alexey-salmin 9 hours agorootparentThis still doesn't make any sense to me. In order to use free software you need to make all your software free, is it that? I have no sympathy for oracle or their products but I fail to see the hypocrisy here. I think oracle is pretty consistent in their position over the years. reply dijit 3 hours agorootparentIf I give you software, and tell you to trust me - but I will not use software given because I do not trust: surely I can read something into that. Clearly Oracle are aware of the danger that they themselves ask people to ignore. reply baobun 0 minutes agorootparentBusiness is business. I wouldn't call Zuck a hypocrite for not exposing himself on TikTok. (There may be other reasons to do so but that's not here nor there) disintegore 20 hours agorootparentprevThis is a slam dunk for free software. Even Oracle can't deny the benefits. reply skywhopper 20 hours agoparentprevOracle owns the most permissive possible license to MySQL. reply Lockal 1 hour agorootparentAh, that flexible \"COST license\" - \"contact our sales team\" license reply zug_zug 18 hours agoprevSlightly off-topic, but one of my greatest pet-peeves of working in devops is every few years a new \"killer tech\" comes out that some contingent of very-highly-opinionated (though not always very senior) people insists is life-and-death stakes and wants the whole company to move to (e.g. terraform). Too often it's a failure. Too often it has some upsides, but also is a LOT of work that is discovered over time. Too often it's seen as good BUT now some incompatible new version or alternative requires the whole debacle start again. I only want to learn technologies that will be relevant until the day I retire, otherwise I'm not advancing, it's all just a treadmill. reply culi 16 hours agoparentIf you think devops has it bad, don't ever work in front-end. Or web development in general reply NewJazz 10 hours agorootparentLol @ svelte's reactive statements being lauded then runed. reply waynesonfire 18 hours agoparentprevYou have to carve your own path. I've experienced the same revelation and have adopted FreeBSD and Erlang. It's for hobby / home use but if I ever launch it'll be on this stack. It's been a rewarding journey. YMMV, but this is how I dealt with this situation. reply playingalong 16 hours agoparentprevWould you classify k8s in this bucket? reply zug_zug 11 hours agorootparentI'm still debating that. Certainly on the one hand it seems like there's already dozens of different incompatible variants/tools/setups/workflows to learn [most of which will be zombies in 5 years]. If I had to pick -- my gut instinct is kubernetes will be around for 5 more years but won't be common in 20 years. reply scarface_74 18 hours agoparentprevWelcome to technology. Yes, I too wish I could make a living programming in 65C02 assembly on my Apple //e like I did in 1986. I also don’t see any reason I have to learn about S3 instead of storing all of my files on an on prem CDRW jukebox reply zug_zug 11 hours agorootparentNo, it's not inevitable. There are many technologies that will outlast my whole career: java, sql, tcp/ip, linux, to name a few. S3 will also certainly be around in 20 years. reply scarface_74 9 hours agorootparentThat’s true. But when I was first learning Java, Java beans and Java Server Pages were the hotness. The last time I did production code in Java was writing Android apps that required knowing the Android SDK. Programming in Java in 2024 is nothing like it was in the 1990s when it was first embedded in Netscape Navigator - yeah I played around with it back then. When I was first using C and C++, I was writing Windows apps with MFC in 1999. Good luck if that’s all you know in 2024z I’ve been at this awhile. I started writing C and Fortran apps on DEC Vax and Stratus VOS mainframes in 1996z My second job was part development and part managing Windows servers on prem running IIS and Classic ASP. I got my first, only, and hopefully last job at BigTech in the cloud consulting department at 46 (full time role) consulting companies on all of the latest “serverless” goodness. Either evolve or end up complaining on HN about “ageism”. When I got Amazoned at 49 last year, it took all of three weeks to have multiple offers. I’ll put my buzzword compliance against anyone of any age in my niches. While “tcp/ip” will be around as will assembly language. I’ve programmed in assembly language on five different architectures either professionally or as hobby. I haven’t touched it since 2008. Jobs are at a higher level of abstraction these days. reply JohnMakin 17 hours agoparentprevif moving to terraform fails for your org, you have much deeper issues that likely aren’t related to terraform reply zug_zug 11 hours agorootparentI didn't downvote, but I disagree. You put forth the question of when a company might rightly not use terraform and I think I can answer that. I think of terraform as a form of insurance. It's \"Oops manual change\" insurance. In the event that somebody breaks something in the console and you need to undo it, it's exponentially faster-easier. However you have to pay premiums to get this insurance as well as a setup cost. So is the insurance worth it? It depends on the org. But I've seen small places where it's a small team that communicates well and nobody screws around in the console with stuff they don't understand (and if they break it they can own it). So there absolutely are places where the amount of time terraform costs you (in learning, setup, and extra PR time, waiting for atlantis to finish, locks) is higher cost than the time saved when you need. reply quesera 14 hours agorootparentprev> if moving to terraform fails for your org, you have much deeper issues that likely aren’t related to terraform That is nonsense. You just said, equivalently, \"Terraform is all things to all people\". reply JohnMakin 13 hours agorootparentNo I didn't. Failing to adopt an IAC approach, which I have seen in my career many times, whether it's terraform or any of the other tools - comes down to organizational issues. I'll pose a question to your snotty response - What specifically about terraform would lead to it failing to be implemented at a company? The answer to that will provide all you need. reply quesera 13 hours agorootparentI'm not being snotty. Terraform is not the best choice for every organization. Rather, Terraform does not add value within every organizational structure. Not adding value is failing. Having a negative ROI is failing. None of these infrastructure tools are perfect, and the ways in which they are imperfect mean that some are better or worse matches for an organization's needs. Therefore your initial statement is oversimplified, presumptuous, and ultimately nonsensical. A logical reframing is \"if your organization does not match Terraform's strengths, then your org is the problem\", and that is clearly not true. reply JohnMakin 13 hours agorootparentYou're shifting goalposts now and still failed to answer my question. And since you seem to have cracked the long-known problem of measuring infrastructure/devops/etc. team performance (since apparently you have a way to measure the ROI on that) I'm assuming you're far above my expertise here and have it all figured out, and I'm in over my head and have clearly struck a nerve. Glad you figured out a problem that so many haven't! have a good day. reply quesera 13 hours agorootparentThe answer is that they all suck. I've used them, and I've written them. They sucked 20 years ago, and they suck today. But they suck differently, for different reasons, and they suck in different magnitudes in the hands of different teams, with different needs. I have never met an org that was happy with their infrastructure tooling! But I have met some that were happier with some tools than with others. It's horses for courses. Terraform is a contender for some use cases. Nothing more, nothing less. reply ig1 22 hours agoprevFollowing IBM's acquisition of Hashicorp the moves seems unsurprising, they wouldn't want to be beholden to a competitor. We'll inevitably see others large companies follow suite - it was one thing when hashicorp was independent tech company but it's very different when it's owned by a direct competitor. reply tw04 21 hours agoparent>they wouldn't want to be beholden to a competitor. Which is ironic given that OEL is a direct rip-off of RHEL which IBM also now owns. reply candiddevmike 21 hours agoparentprevOpenTofu hard forked, it's going to be interesting to see what happens if IBM rolls back the licensing changes. reply cies 21 hours agoparentprevIBM prolly got them to agree to do the re-licensing move \"as Hashicorp\" as part of the take over deal. So it would not look bad on IBM. reply alemanek 17 hours agorootparentFrom what I have read Hashicorp did this relicensing since IBM was reselling Vault at scale in IBM cloud. They wanted to force IBM and other cloud providers to pay them instead I believe. IBM employees then initiated the fork of vault which is called openbao. Later IBM buys Hashicorp. The fork might have just been an attempt at leverage in the negotiations but it remains to be seen if it will live on. reply JohnMakin 18 hours agoprevbeen using terraform heavily for 5 years and have hacked together modules and custom providers for various ad-hoc things. One of the things that has always really frustrated me about terraform is that it seems to go out of its way to make you do things in a very annoying, inconsistent way. Part of this is necessary due to the nature of the provider ecosystem, you can't guarantee consistency across providers - and I won't burden this post with my gripes about inconsistencies and annoyances within providers, such as the AWS provider. Really though the interface has always been terrible (IMO). Stuff like iterating through a nested map using a for loop, which is trivial in most languages, is annoying and obtuse to the point of comedy. God help you if this map contains mixed types. Novices have trouble picking it up in general. It's very easy to start a project that sprawls completely out of control, and there doesn't seem to be a standard at all as to how to organize projects/code, so each terraform project I inherit is wildly different and has its own seemingly unique pain points. A lot of this has gotten better over the years with QoL improvements within terraform itself - but really, as a developer, I've gotten more than a little tired about the hubris that Hashicorp shows with some of the stuff around the terraform ecosystem. Features that people beg for routinely get told by maintainers that they will not be doing that because reasons or because \"it's not possible\" (such as dynamic provider blocks). OpenTofu is already tackling many of the gripes and feature requests I've had over the years and are doing so eagerly and have some heavy hitters behind it. Terraform is good, but it was always going to be vulnerable to competition - It's basically just a state-based wrapper around cloud API's. A great idea, but easy to duplicate. I don't know what they were thinking trying to put this behind a walled garden when they could have used it to get people into the hashicorp ecosystem and sell their other enterprise products. reply hamandcheese 17 hours agoparentWhat really grinds my gears is how hard it is to refactor terraform code. Put something in a module, but want to move it elsewhere? Get ready for pain. I've been using terranix, which uses nix to generate a tf.json file, and oh my god is the experience night and day. I can make functions! I can refactor! And if it's a pure refactor, there is nothing to apply. reply cdchn 17 hours agorootparentHow does terranix help you with the \"move a resource from a module to somewhere else\" problem? reply hamandcheese 9 hours agorootparentIt helps because you don't use terraform modules at all, any abstraction you need can be done in nix before tf.json gets written. reply JohnMakin 17 hours agorootparentprevI know many people find it painful but isn't this fairly simple with \"terraform state mv?\" my process is roughly: comment out the resource in the module, run a plan -> get output like: \"module.foo1.aws_resource.bar will be deleted\" Then copy my resource in source to module.foo2.aws_resource.bar, the command becomes: terraform state mv module.foo1.aws_resource.bar module.foo2.aws_resource.bar I guess this might be harder if you're using upstream \"official\" modules, but I avoid those like the plague. reply rjbwork 15 hours agorootparentYou don't even need to do state mv anymore. They added the `moved` block a while ago. You can then delete it from the source after your apply at your leisure. reply hamandcheese 9 hours agorootparent\"Every change requires two PRs that aren't in the same terraform run\" still way way way too much mental overhead just to do a simple refactor. reply hamandcheese 9 hours agorootparentprevYea, there are tools to work around it, but the fact that a pure refactor would impact terraform state at all is the design flaw in my eyes. Suddenly, just to refactor the source in a way that shouldn't touch any resources, you have to have be able to mutate the terraform state. (Or use the more recently introduced moved blocks, which is still quite a big kludge). This means any kind of broadly sweeping refactor (which might impact many different state files) is really hard. reply nprateem 17 hours agoparentprevIf OT want to win all they need to do is actually make it possible to debug the code. reply toolslive 16 hours agorootparentIt ain't Infrastructure As Code if you can't put a break point. reply fishnchips 16 hours agorootparentprevCo-founder of OpenTofu here. Second that. One of my colleagues is working on adding proper tracing to the OpenTofu codebase, to help understand the exact cause of failures. reply rswail 22 hours agoprevOracle making the change due to licensing is like a dictionary definition of hubris, considering how their own license enforcement operates. reply kube-system 19 hours agoparentOracle doesn't have some ideological preference for licensing. Oracle just wants what is best for Oracle. reply bigstrat2003 16 hours agorootparentRight - don't anthropomorphize the lawn mower. reply toolslive 16 hours agorootparentwhy, cause he hates it? reply silveraxe93 20 hours agoparentprevI think you mean hypocritical, instead of hubris. reply __MatrixMan__ 14 hours agoparentprevOracle takes hostages, they know how to avoid being taken hostage. Not hubris, just tactics. It would be hubris if they tried to then take the moral high ground. reply snapcaster 13 hours agoparentprevIt's not hubris or hypocrisy, it's a profit maximizing entity maximizing profit that's it reply cies 21 hours agoparentprevAs much as I dislike Oracle's biz practices (I'd not touch their db product with a stick), they do a lot of FLOSS devt: https://opensource.oracle.com/ (almost endless list) But then they have take FLOSS projects and abandoned them, see OOo for instance: https://en.wikipedia.org/wiki/Apache_OpenOffice#/media/File:... reply thayne 19 hours agorootparentLooking through that list, most of the big projects (OpenJDK, Mysql, Opengrok) were inherited as part of acquisitions. reply nirvdrum 15 hours agorootparent> Looking through that list, most of the big projects (OpenJDK, Mysql, Opengrok) were inherited as part of acquisitions. I see this argument a lot, but I'm not sure how it detracts from their continued development. Oracle funds many engineers working on OSS and, despite having CLAs in place, retain the permissive license for most of them. In some cases they've acquired closed source software and made it open source (e.g., JRockit stuff). They're a major contributor to OSS. reply chasil 16 hours agorootparentprevOracle has been a prolific contributor to the Linux kernel. https://blogs.oracle.com/linux/post/oracle-is-the-1-contribu... XFS is really important for (their) database performance, so quite a lot comes out of Oracle for it. You might also know that btrfs began at Oracle. https://www.google.com/search?q=oracle+blog+xfs https://en.wikipedia.org/wiki/Btrfs \"Chris Mason, an engineer working on ReiserFS for SUSE at the time, joined Oracle later that year and began work on a new file system based on these B-trees.\" reply manishsharan 21 hours agoparentprevThey wrote the playbook. They know what's coming. Ruthless and smart! reply Gowiem 9 hours agoprevThe move towards OpenTofu is going to be a slow tide, but it is coming. It's an easy migration, the team behind OpenTofu is showing that they're strong, and the community opinion is most definitely shifting in their favor. We made the jump for one client, started a new client project with tofu, and are starting to migrate another client this week. Good info on our experience here: https://masterpoint.io/updates/opentofu-early-adopters/ reply jph 22 hours agoprev\"Oracle’s move seems like a straightforward decision to ensure it is using the most permissive underlying IaC tool without having to worry about downstream license complications, no more and no less.\" Oracle wins a big competitive talking point versus IBM, as well as crushing the value of IBM's acquisition of Hashicorp, and completely eliminating IBM's Terraform inroad into a large group of Oracle's enterprise customers. reply dralley 22 hours agoparentI don't see how any of this could be true considering IBM is also heavily involved in OpenTofu (and did so first) reply jph 16 hours agorootparentExactly right. You're making my point for me. :-) Oracle can now say it has one solution, whereas with IBM the attention is split between Terraform and OpenTofu. If you're an enterprise customer, do you want your enterprise deployments on a company that knowingly does two near-identical implementations, and can't seem to decide on which one to favor? reply EspadaV9 22 hours agorootparentprevI was wondering, once IBM got Hashicorp, if they would reverse the license change for Terraform. Not been that long since the announcement, so still hoping they will. reply pquki4 21 hours agorootparentHas there ever been notable instances of company regretting and reversing license change of a major project? reply pxc 2 hours agorootparentWe're talking about an acquisition, not the original company making the reversal. Red Hat used to routinely open-source acquisitions. Sun also did— that's how we got OpenOffice (and by way of it, LibreOffice). StarOffice was proprietary when Sun bought it. reply toast0 18 hours agorootparentprevOracle ended OpenSolaris although forks continue. Nokia and friends ended OpenSymbian, no forks afaik. reply webwielder2 21 hours agorootparentprevUnity! reply baobun 21 hours agorootparentprevLerna, if that's major enough for you. reply skywhopper 20 hours agorootparentprevIf they do it likely won’t be until after the deal closes. reply cies 21 hours agorootparentprevI believe Hashicorp's move wrt the license of TF was in order to close the IBM takeover deal. reply glenngillen 21 hours agorootparentYou keep posting this completely unsubstantiated theory. Way back when the license changed the threads on HN had HashiCorp employees claiming the change was primarily to protect HashiCorp from the fact IBM was reselling Vault. IBM then went ahead and helped fork Vault (OpenBao). reply skywhopper 20 hours agorootparentprevDefinitely not the case. HC leadership was totally desperate for any way to increase revenue and/or stock price. See also the announcement in a recent quarterly report that they were going to start doing share buybacks even though they are still operating at a loss. reply pwarner 22 hours agoparentprevI think Oracle Linux was a similar approach to let their customers use RedHat Linux without paying RedHat, now also IBM. reply alephnerd 22 hours agorootparentOracle Linux has been a thing for decades. It's largely because a lot of Oracle DB products where performance mattered (eg. Exadata) needed some sort of a base OS that Oracle could manage and optimize as needed. reply thelastgallon 21 hours agorootparent> It's largely because a lot of Oracle DB products where performance mattered (eg. Exadata) needed some sort of a base OS that Oracle could manage and optimize as needed All that’s needed is update sysctl.conf to tune kernel parameters to the workload. Every Linux sysadmin knows how to do this. What kernel parameters need to be updated is heavily documented for any product. reply alephnerd 21 hours agorootparentHaving these pretuned AND having a dedicated support team doing the tuning for you is the killer app for some buyers, as it allowed Admins to concentrate on other work and also minimize management overhead for the Infra org. Spending $500k/yr on compute+support SLA is cheaper than $200k/yr on compute and hiring 3 admins dedicated to that piece of compute. This is the model that every Enterprise Infra vendor pushes (eg. Oracle, AWS, MongoDB, Nvidia), and most mid- and upper-market purchasers are used to it. reply thelastgallon 19 hours agorootparent> Having these pretuned AND having a dedicated support team doing the tuning Dedicated support team costs a lot more than you think. I guess you never had to deal with Oracle support. If you get hold of someone, all they will do it point to the documentation. All software products have documentation on how to install the product. Oracle has a large suite of products, their databases, ERPs, etc. For kernel parameters, its just a file, which takes a second https://docs.oracle.com/en/database/oracle/oracle-database/1... In reality though, all Infra teams, have infrastructure to install OS (and manage the fleet), then post-install customize the OS to which team is requesting, usually done via puppet or ansible to manage the configuration. There will be standardized configuration for application, web, database (just to keep it simple). I would be shocked if Oracle support (or any other vendor) is given login access to make changes on servers owned by clients. At best, you open a case, you get an incompetent support person who'll send you documentation. Oracle support does not replace admins. Oracle support gives you access to bug fixes, updates, documentation. I believe you can download most Oracle software for free, but without the docs and updates, its worthless. Other vendors may use the opposite strategy, docs openly available but software downloads are paid/subscriptions. > Spending $500k/yr on compute+support SLA is cheaper than $200k/yr on compute and hiring 3 admins dedicated to that piece of compute. In reality though, there will always be admins, then a whole lot DevOps/Cloud Ops/Kubernetes/SRE/etc people added, smooth talking manager/director increasing the spend from what could be done on bare-metal under 20K to a 20 million dollar multi cloud strategy. Why have 3 admins report to you, when you can have an army of 200 people do the same work for 100x more cost? Success stories and promotions all around! reply alephnerd 18 hours agorootparent> all Infra teams, have infrastructure to install OS (and manage the fleet), then post-install customize the OS to which team is requesting, usually done via puppet or ansible to manage the configuration. Yep! And it takes time and effort to maintain your Puppet/Chef/Ansible/Terraform/OpenTofu scripts as well as your golden images as well as triaging escalations as well as other incidental work. This means you don't have as much time to work on tuning or debugging, because you'll have dozens of tools (some in-house, others purchased) to manage. Furthermore, most people recognize Hardware specialized IT Administration is increasingly a career dead end, so most end up switching to Engineering, Sales Engineering, or Support Engineering due to better career opportunities. > I would be shocked if Oracle support (or any other vendor) is given login access to make changes on servers owned by clients. At best, you open a case, you get an incompetent support person who'll send you documentation. This is the norm in most mid- and upper-market support contracts. You'll have a dedicated TAM, Support Eng, and CSM who will handhold teams, and will have access to the underlying infrastructure. > Oracle support does not replace admins. Oracle support gives you access to bug fixes, updates, documentation. I believe you can download most Oracle software for free, but without the docs and updates, its worthless. Other vendors may use the opposite strategy, docs openly available but software downloads are paid/subscriptions. Depending on your contract, you would be given a dedicated TAM team and support team to debug any issues in the Oracle stack. > In reality though, there will always be admins, then a whole lot DevOps/Cloud Ops/Kubernetes/SRE/etc people added, smooth talking manager/director increasing the spend from what could be done on bare-metal under 20K to a 20 million dollar multi cloud strategy. Why have 3 admins report to you, when you can have an army of 200 people do the same work for 100x more cost? Success stories and promotions all around! That \"smooth-talking manager\" needs to justify to the CFO, COO, CTO, VP Eng, etc that for $X spent, I can get 1.5 * $X back. As I've mentioned on multiple different occasions on HN, spend on on-prem infra is treated as part of the Finance+ITOps budget, not the DevOps budget (which is generally within R&D). Procurement is hard, and you need to JUSTIFY a 1% increase in headcount For example, let's assume you are hiring 3 IT Admins for $120k. That ends up costing $700-800k/yr because of benefits and incidentals. The compute as well is an additional $200-300k. This means you are spending $900k/yr AT BEST. That $200-300k in compute becomes $500k with a support contract, and you can hire 1 person for $120k to manage that. This means you're spending around $750k/yr AT BEST. That extra $150K can then be given to Engineering to help give bonuses to attract good dev talent or hire some additional headcount on the Sales side to sell the product you are hired to build. reply twoodfin 21 hours agorootparentprevMongoDB offers a preconfigured Linux to run on? Or did you just mean Atlas? reply alephnerd 21 hours agorootparentI mean MongoDB has a professional services SKU to simplify onboarding and management. reply johannes1234321 21 hours agorootparentprev> All that’s needed is update sysctl.conf to tune kernel parameters to the workload. Mind that they do quite some work on the kernel itself to optimize it for their workloads: https://blogs.oracle.com/linux/post/oracle-is-the-1-contribu... The availability of Oracle's uek kernel is a differentiator from standard RHEL. reply zaphar 21 hours agorootparentprevOracle has a long history of not documenting all of this stuff and instead suggesting you should hire one of their army of consultants to help tune the OS or Database for you. reply mingus88 21 hours agorootparentIf we are talking about tuning for a specific workload, what’s wrong with that? If your in-house DBA doesn’t have the experience to perform the specific tuning required, then that’s what support contracts are for The documentation can’t cover every customer’s use case and configuration. That’s just enabling folk to blindly copy inappropriate sysctls they don’t understand like they are building gentoo kernels. reply zaphar 18 hours agorootparentYou don't need to cover every use case. You just need to document what stuff does and how it affects to various workloads. A good engineer can from that information infer what they need for their workload. But not documenting that a control exists so that you can be bill you for consulting puts you in my \"will not use\" category. reply growse 21 hours agorootparentprevDidn't they buy one of these (Solaris)? reply claudex 18 hours agorootparentThat was after the launch of Oracle Linux. reply Kwpolska 20 hours agorootparentprevMaintaining an entire separate OS (like Solaris) is hard. Maintaining a fork of a Linux distro is easy. reply datadeft 21 hours agoprevI wish there was a type safe, algebraic data type using Terraform alternative. reply pxc 1 hour agoparentIdk if the APIs look like what you want, but Scala has ADTs and there's a new Pulumi SDK for Scala that just entered public beta: https://virtuslab.github.io/besom/ There's also a first-party Pulumi SDK for F#: https://www.pulumi.com/docs/languages-sdks/dotnet/ If you're into Nix, you might enjoy using this to generate Terraform JSON. The language is inspired by Nix, so it feels familiar to Nixers, but it has a better type system that recently includes ADTs, at least on its master branch: https://github.com/tweag/tf-ncl reply ParetoOptimal 21 hours agoparentprevYou can use dhall for terraform, but no idea if UX for it got better. reply maineldc 17 hours agoparentprevWe use typescript + pulumi for this. It's pretty amazing. And Pulumi uses Terraform modules under the hood so you get the full power of Terraform with the goodness of Typescript. Even self hosting your state management in a bucket is simpler with Pulumi since it uses lock files on S3 versus a separate DyanamoDB + S3 combo. I have been using it in production for 4-5 years and used Terraform for several years before that. reply fishnchips 16 hours agorootparent> since it uses lock files on S3 versus a separate DyanamoDB + S3 combo This is disturbing because S3 does not give you guarantees required to implement real locking. reply erik_seaberg 13 hours agorootparenthttps://aws.amazon.com/blogs/aws/amazon-s3-update-strong-rea... guarantees that a client's lockfile can always be seen by other clients immediately (which didn't used to be true). If every client backs off and retries after a race, is that enough? reply fishnchips 12 hours agorootparentI think not, actually. There would still be cases where a race is not detected. I can think of the following sequence: A checks - no lock, B checks - no lock, A writes - success, A reads - match, success, B writes - success, B reads - match, success. A and B both think they now hold the lock. For locking to work properly you'd need to have a conditional write that would fail if some prerequisite was not met. GCP offers that operation, S3 AFAIK does not. reply erik_seaberg 12 hours agorootparentI'm no expert but from a quick glance at https://www.pulumi.com/docs/concepts/state/#using-a-self-man... it looks like this might work: client A lists s3://bucket/prefix/.pulumi/locks/, sees nothing client B lists s3://bucket/prefix/.pulumi/locks/, sees nothing client A creates s3://bucket/prefix/.pulumi/locks/unique1.json client A lists s3://bucket/prefix/.pulumi/locks/, only sees unique1.json, and proceeds client B creates s3://bucket/prefix/.pulumi/locks/unique2.json client B lists s3://bucket/prefix/.pulumi/locks/ and sees both unique1.json and unique2.json client B assumes it lost a race, deletes s3://bucket/prefix/.pulumi/locks/unique2.json, and retries There's another mode where both clients pessimistically retry, but fuzzing a retry delay could eventually choose a winner randomly. reply fishnchips 12 hours agorootparentIn this case you have the opposite issue, with no-one actually guaranteed to get a lock even though nothing is holding one. Fuzzed retries may work in practice but theoretically speaking this is a flawed algorithm. reply erik_seaberg 11 hours agorootparentHm, I can sort of imagine a way to use lockfile names to claim a random position in a queue of pending changes, but I don't know if anyone has been worried enough to do that. In practice Pulumi seems to give up instead of retrying: https://github.com/pulumi/docs/issues/11679 reply cies 21 hours agoparentprevThere are Java and C# (somewhat typesafe imho) and this (Kotlin, reasonably typesafe imho): https://github.com/VirtuslabRnD/pulumi-kotlin For Pulumi. When I see the pulumi-kotlin example code I much prefer it over my Terraform scripts. (We picked TF before Pulumi was an option, and waaaaay before it had reasonably typesafe lang support) reply jpgvm 21 hours agorootparentI made this suck less last year and it was just recently merged: https://github.com/pulumi/pulumi-java/pull/1231 This lets you use Pulumi w/Gradle multi-project builds in Kotlin script. reply andrewfromx 21 hours agorootparentprev+1 for pulumi! https://thenewstack.io/pulumi-launches-new-infrastructure-li... reply loloquwowndueo 22 hours agoprevA giant corporation like Oracle switching to the fork because they don’t want to engage commercially with Hashicorp is peak greedy. reply pquki4 21 hours agoparentGreedy? Why would you bet your product and customers on another company? If someday Hashicorp suddenly died so that nobody adds new features or fixes bugs, you can't do anything about it because their code isn't \"open source\" even though available, when a \"true\" open source project is just next door. Any big enough company will think about what is the safest approach to their product. (Of course, companies do go out of business, and products stop to be maintained, and the example here is a bit extreme, but the point is that company will do what makes the most business sense) reply loloquwowndueo 21 hours agorootparentWhy would you bet your product and customers on another company? Oh so you never heard of “suppliers”? reply m1keil 22 hours agoparentprevIt is not Oracle who are using terraform, it is their customers. Terraform is the underline tech that powers the tooling suit that the customers use to manage their oracle cloud. It makes perfect sense for them to push their customers to move to the more permissive licensing to avoid any legal issues. reply freedomben 21 hours agorootparentOnly when it isn't legal issues that dump money into Oracle. They absolutely love and adore pushing their customers to less permissive licensing which can encounter legal issues when they are the ones benefiting reply m1keil 21 hours agorootparentYes, Oracle loves making money. reply trueismywork 22 hours agoparentprevMore than the money, it's the license servers that make your life miserable reply klysm 22 hours agoparentprevHow? It seems entirely sensible reply brabel 21 hours agorootparentThey seem to think that just because a company makes non-open-source software, it shouldn't itself prefer to use open-source rather than proprietary software?! That makes no sense, of course. Specially considering the non-OSS is now owned by IBM which directly competes with Oracle on multiple fronts. It seems to me that OpenTofu is actually backed by many companies in a similar position to Oracle, which don't want to have to rely on IBM for things they have tight integrations with. reply nunez 19 hours agoprevFortunately using OpenTofu is just s/terraform/opentofu/g at this point. reply darknavi 14 hours agoprev> What is OpenTofu? > > OpenTofu is a Terraform fork, created as an initiative of Gruntwork, Spacelift, Harness, Env0, Scalr, and others, in response to HashiCorp’s switch from an open-source license to the BUSL. The initiative has many supporters, all of whom are listed here. I still have no idea what I am looking at. I know that probably means this product isn't for me, but it peeves me when products do this. \"What is X? X is like Y!\" reply erik_seaberg 13 hours agoparentInfra as code. Write a template for a Spanner table in your GCP account. If it doesn't already exist, OpenTofu will notice and offer to send the API call to create it. It's like using AWS CloudFormation or GCP Deployment Manager, but supports quite a few cloud vendors with the same tools. reply outside1234 14 hours agoparentprevOpenTofu is Terraform reply robertlagrant 22 hours agoprevHow are Oracle with contributing to OpenTofu upstream? reply benrutter 22 hours agoprevThis is a pretty interesting development. Anyone know the userbase/impact of \"Oracle EBS\"? I don't know much about Oracle's services so can't figure if this is a huge number of users, of a small subset of their clients. reply amiga386 22 hours agoparentOracle has a number of departments, but one way you can look at it is: * Oracle Products (e.g. DB, Fusion, E-Business Suite) * Oracle Cloud (OCI) What's telling is that Oracle Cloud's Terraform-as-a-Service (Resource Manager) is still Terraform: https://docs.oracle.com/en-us/iaas/Content/ResourceManager/C... Clearly, Oracle must think there is some legal distinction between telling Terraform-as-a-service, and selling+distributing a product _containing_ Terraform that end users then use as Terraform-as-a-service. reply cies 18 hours agorootparentAnd the biggest department: * Oracle legal reply pwarner 22 hours agoparentprevYeah this sounds like a very narrow use case shifting. That of moving your EBS ERP application to Oracle cloud from on premise. I mean, people are doing that migration and it's exciting if they are actually using iac, but this can't represent large usage of Terraform. reply lenerdenator 22 hours agoprevJust don't let them hijack it for their own purposes. Sun and MySQL precede them. reply dylmye 15 hours agoprevI just wish Terraform would let me be a grown up and see sensitive values in an output without going through so many hoops. reply skywhopper 20 hours agoprevThe article is somewhat confusing but it sounds like Oracle packages a cloud infrastructure management tool that’s based on Terraform. Presumably it’s built on 1.6, which was still MPL. Since they offer this product as a service, it directly falls under the restrictions HashiCorp put into place to prevent competition from repackagers and SaaS offerings of their products. So to move forward with upgrading the Terraform support in their tool, Oracle had two choices: pay HashiCorp (soon IBM) a hefty license fee to resell Terraform, or use OpenTofu which is free and has now proven to be well-run enough to issue a new release with both Terraform compatibility and OpenTofu-specific enhancements, while dodging lazy accusations of code theft from HashiCorp. This is a no-brainer for Oracle, and it’s great news for the future of OpenTofu. reply empressplay 15 hours agoparentIt will be interesting to see what IBM's army of lawyers think about those lazy accusations of code theft ;) reply 8organicbits 21 hours agoprev [–] We're seeing an uptick in open source projects getting relicenced to non-open licenses. Some projects are successfully forked and the userbase shifts, other times not. One theory of mine is that we can measure the risk that a project will be relicensed by looking at things like diversity of contributors, trademark ownership, contributor agreements, and license terms. Low risk projects include the Linux kernel (GPL, DCO) [1]. High risk projects include Kubernetes (Apache, CLA) [2]. If this trend continues developers will need to get a better understanding of how relicencing works and may decide to avoid contributing to projects with elevated risk. [1] https://alexsci.com/relicensing-monitor/projects/linux/ [2] https://alexsci.com/relicensing-monitor/projects/kubernetes/ reply aragilar 21 hours agoparentI'm not sure how Kubernetes is high risk, given the CLA is to CNCF. Similarly, CLAs to the Apache Foundation, the FSF or similar are probably pretty safe (in that they have a long term interest to be good custodians for the IP), and could be safer than projects that lack a CLA but don't have (or only a few) outside contributors. To me, the obvious questions are who owns the IP, and what are their incentives to maintain the current licensing. reply 8organicbits 6 hours agorootparentThis is a good critique. Measuring intent of an organization may be difficult to do methodically and impartially, so it's not currently covered. Personally I was surprised to see Redis change license after Redis Labs promised not to change the license. I think that promise was made with good intent but overwhelming financial pressure that emerged later on swayed them. reply candiddevmike 21 hours agoparentprevI think you need to rework your algorithm. Kubernetes is no way a high risk project, its IP is owned by the CNCF/Linux Foundation. reply jamesrr39 12 hours agoparentprevRegarding Kubernetes and the Apache license, Apache license 2.0 has to be one of the most business friendly licenses around? It's widely used and understood, no requirement to open source changes, automatic patent license for any patents the software uses included. If the corporate lawyer says no to that, what do they say yes to? reply 8organicbits 6 hours agorootparent> Apache license 2.0 has to be one of the most business friendly licenses around? Yes, in my experience it is. Permissive licenses like Apache, MIT, and BSD are easiest for the corporate lawyers to approve but also easy for the project owner to relicense. Relicencing Monitor isn't measuring how easy it is for companies to use the software; risk is solely measuring how easy it is to relicense the software. Copyleft licenses are lower risk than permissive licenses in this specific context as they are viral. A CLA or a very small number of contributors can negate that, as happened with Emby [1]. SourceGraph is probably the best example here (I need to add them still). They switched off Apache 2 and prompted this [2] helpful blog post. [1] https://alexsci.com/relicensing-monitor/projects/emby/ [2] https://drewdevault.com/2023/07/04/Dont-sign-a-CLA-2.html reply janosdebugs 21 hours agoparentprevI wonder how accurate this assessment is since the Linux Foundation is a non-profit. reply cameronh90 8 hours agoparentprevI’m not sure why you’re being downvoted, this is happening a lot right now, and it’s a real risk when contributing or using an open source project. I also had the same thought to create some sort of risk metric that could be applied to projects, but I do think your initial metric is lacking some criteria. Foundations like the CNCF and ASF have to be among the lowest risk, and CLAs can be more or less harmful depending on their specific content. I think a big red flag has to be if they’ve taken any VC or PE funding. However I think the principle of taking this risk more seriously is good and important. reply jillesvangurp 21 hours agoparentprevThe good news is that projects that prevent forking from happening usually don't have huge OSS communities of contributors because of their attitude towards outside contributors. You need an outside community to be able to step up and take over for a fork to happen. Mostly things like copyright ownership transfer is not a thing with OSS communities because it strongly discourages third parties from contributing. Copyright transfers are only needed with some licenses (GPL style licenses that insist everything else is licensed the same way) and cannot prevent a retroactive fork even if you have them. Other licenses allow distributing mixed licensed code and you can just create a commercial source distribution for those because the license explicitly allows that. Either way, anyone with the pre-license change version of the code can fork. That's why Elastic, which used the Apache license and had copyright transfers, got forked. The more widely used an OSS project is, the more likely it is that somebody will fork it if it is re-licensed. Because that usually means lots of external contributors and plenty of interest from wealthy companies that depend on it. Meaning there are skills and money needed to fund the fork. Copyright transfers don't stop this from happening. Unless you specifically want to fire most of your user base, this just doesn't make any sense from a business point of view. A failure to fork basically indicates the project didn't have a strong developer community and big companies simply didn't care about the project. I consult some clients on Elasticsearch and Opensearch. Most of my recent clients now default to Opensearch. Because it's the OSS option. They are clearly spending money to get support (from me and others) but Elastic isn't getting any. As far as I can see, Opensearch now represents the vast majority of new users and is becoming a significant source of money for hosting, training, and consulting. But Elastic is getting none of that. My guess is that the industry will learn from the repeated re-licensing and forking and subsequent community split that has been happening. Elastic, Redis, OpenTofu, Centos, etc. The pattern is the same every time: 1) project gets relicensed 2) a few weeks later a consortium of companies pools resources together and forks 3) most users stick with open source and the company cuts themselves off from those users. Long term, I would not be surprised to see some of those companies offering support for their OSS forks (in addition to their commercial offerings) or even reverting the license change. This would make a lot of sense for e.g. Elastic as there's a lot of duplicated effort between them and Amazon. And Amazon gets a lot for free from outside contributors. reply cies 21 hours agoparentprev [–] Dual licensing also makes it IMHO less likely that a project \"continues as proprietary\". Example: Qt. I think \"contributor agreements\" are the biggest red flag. Though I like them for potentially upgrading a license (say from GPLv2 to v3), not that this always is a good thing. reply aragilar 21 hours agorootparent [–] It's also worth mentioning the specific agreement between KDE and Qt (https://kde.org/community/whatiskde/kdefreeqtfoundation/ and https://www.qt.io/faq/3.2.-why-do-you-have-an-agreement-with...), which shifts the incentives as well. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Oracle has swapped Terraform for the open-source alternative OpenTofu in the Oracle E-Business Suite Cloud Manager, necessitating customers to transition by June 30, 2024, following licensing alterations by Terraform's creator, HashiCorp.",
      "OpenTofu is perceived as a more lenient and business-suitable solution than Terraform.",
      "HashiCorp, the original developer of Terraform, is being acquired by IBM for $6.7 billion, indicating significant industry movements."
    ],
    "commentSummary": [
      "Oracle is transitioning from Terraform to OpenTofu 1.7, introducing features like end-to-end state encryption.",
      "The tech community is shifting towards OpenTofu, emphasizing debates on Terraform's limitations, alternative tools, and infrastructure management challenges.",
      "Discussions involve Oracle's support, using Oracle Linux for performance, outsourcing IT admin work, Oracle EBS role, and concerns about licensing agreements in open source projects, highlighting the need for diversity and strategic resource allocation in the tech sector."
    ],
    "points": 249,
    "commentCount": 173,
    "retryCount": 0,
    "time": 1715770233
  },
  {
    "id": 40367090,
    "title": "Quary: Open-source BI Tool for Engineers",
    "originLink": "https://github.com/quarylabs/quary",
    "originBody": "We are building Quary (https:&#x2F;&#x2F;quary.dev), an engineer-first BI&#x2F;analytics product. You can find our repo at https:&#x2F;&#x2F;github.com&#x2F;quarylabs&#x2F;quary and our website at https:&#x2F;&#x2F;www.quary.dev&#x2F;. There’s a demo video here: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=o3hO65_lkGUAs engineers who have worked on data at startups and Amazon, we were frustrated by self-serve BI tools. They seemed dumbed down and they always required us to abandon our local dev tools we know and love (e.g. copilot, git). For us and for everyone we speak to, they end up being a mess.Based on this, we decided there was a need for engineer-oriented BI and analytics software.Quary solves these pain points by bringing standard software practices (version control, testing, refactoring, ci&#x2F;cd, open-source, etc.) to the BI and analytics workflow.We integrate with many databases, but we’re showcasing our slick Supabase integration, because it: (1) keeps your data safe by running on your machine without data flowing through our servers; and (2) enables you to quickly build an analytics layer on top of your Supabase Postgres instances. Check out our Supabase guide: https:&#x2F;&#x2F;www.quary.dev&#x2F;docs&#x2F;quickstart-supabaseWhat we’re launching today is open source under the Apache 2.0 license. We plan to keep the developer core open source and add paid features like a web platform to easily share data models (per-seat pricing), and an orchestration engine to materialize your data models.Please try Quary at https:&#x2F;&#x2F;quary.dev and let us know what you think! We&#x27;re excited to put the power of BI and analytics into the hands of engineers.",
    "commentLink": "https://news.ycombinator.com/item?id=40367090",
    "commentBody": "Open-source BI and analytics for engineers (github.com/quarylabs)235 points by louisjoejordan 19 hours agohidepastfavorite64 comments We are building Quary (https://quary.dev), an engineer-first BI/analytics product. You can find our repo at https://github.com/quarylabs/quary and our website at https://www.quary.dev/. There’s a demo video here: https://www.youtube.com/watch?v=o3hO65_lkGU As engineers who have worked on data at startups and Amazon, we were frustrated by self-serve BI tools. They seemed dumbed down and they always required us to abandon our local dev tools we know and love (e.g. copilot, git). For us and for everyone we speak to, they end up being a mess. Based on this, we decided there was a need for engineer-oriented BI and analytics software. Quary solves these pain points by bringing standard software practices (version control, testing, refactoring, ci/cd, open-source, etc.) to the BI and analytics workflow. We integrate with many databases, but we’re showcasing our slick Supabase integration, because it: (1) keeps your data safe by running on your machine without data flowing through our servers; and (2) enables you to quickly build an analytics layer on top of your Supabase Postgres instances. Check out our Supabase guide: https://www.quary.dev/docs/quickstart-supabase What we’re launching today is open source under the Apache 2.0 license. We plan to keep the developer core open source and add paid features like a web platform to easily share data models (per-seat pricing), and an orchestration engine to materialize your data models. Please try Quary at https://quary.dev and let us know what you think! We're excited to put the power of BI and analytics into the hands of engineers. scapecast 3 hours agoCongrats on the launch! I've built analytics products, and the good thing about dashboards is that there's budget for them. People like eye-candy, and are willing to pay for it. I like how you picked Postgres as your initial database, because I think it's still the #1 databases for analyics (even though it's OLTP) that no one talks about. The three products where I think you may want to write short comparison pages are: - Rill - Preset - Metabase And I'd take a hard look at ClickHouse as your next database. They're missing a dashboard partner. And I think they're users are much more engineering-centric and therefore a good fit for you than the analytics crowd around Snowflake. reply louisjoejordan 2 hours agoparentAppreciate your feedback and guidance. I was just at the Click-house office a few weeks ago - this is a really good idea. reply rodolphoarruda 13 hours agoprevSide comment: what an interesting landing page it has. That Slack CAT button right within the fold is a good idea. A walkthrough and a way to schedule a meeting with the founders. This is very straightforward. Good luck! reply louisjoejordan 13 hours agoparentHey! OP here. This made my day, thank you! reply igeligel_dev 15 hours agoprevAll these comments ask for comparisons. It might be worth creating some alternative pages like podia do [1]. It could be helpful for your growth. Seems like a cool project! [1] https://www.podia.com/podia-alternatives reply louisjoejordan 14 hours agoparentHey! OP here. This is really good feedback thank you. reply swaraj 4 hours agoprevWe're all on on https://www.sigmacomputing.com/ bc we don't like hosting/managing/provisioning essential tools like this + this seems more complicated to configured. I would recommend a simpler setup like Metabase Docker (which I re-evaluated recently): https://www.metabase.com/docs/latest/installation-and-operat... reply ksbeking 29 minutes agoparentAppreciate the feedback! We'll keep this in mind. There is nothing to host/provision, so it's simple in that sense. You just run it locally with your credentials and connect directly to your database. It is definitely not the easiest to set up especially when thinking as a team so we'll keep that in mind. reply cellover 50 minutes agoprevWondering if the signing in is mandatory to use it? reply louisjoejordan 44 minutes agoparentNo sign in needed! reply cellover 26 minutes agorootparentThx for clearing that up, it was not so obvious from the https://www.quary.dev/docs/sample-project#signing-in-to-quar... section. I appreciate the use of Tailwind scroll-margin on your anchors btw, caring for details is communicative ;) reply huy 5 hours agoprevCongrats on the launch! I think here's a few players in this space (dev-friendly BI tool) already: - Holistics.io - Lightdash - Hashboard These tools all allow analysts to use both/either a local/cloud IDE to write analytics logic, and check in to Git version control. How do you plan to differentiate with them? reply vim-guru 3 hours agoprevIt's unfortunate that org-mode is not more wide-spread (linked to Emacs). Org-mode covers this and a million other use-cases. Don't get me wrong though, this looks really good. So, congrats to OP :) reply louisjoejordan 2 hours agoparentAppreciate the kind feedback! Curious to know if org-mode is still actively maintained. reply newusertoday 2 hours agoparentprevi tried org mode for sql queries but than went back to sql mode because lsp is not supported in org mode. Also how do you use charts with it? reply banditelol 4 hours agoprevCongrats on the launch! I've been evaluating evidence and observable framework for a while, and this seems like a nice addition as alternative But I just realized you require login when using vs code, what is it used for? And can I completely self host this? Thans! reply louisjoejordan 2 hours agoparentHey! We killed the auth flow from our extension, we used it to get an idea of how many people are using it. The extension works entirely local and connects to your database through your machine. So there's no need to self-host anything! reply dantodor 6 hours agoprevLooks pretty exciting, congrats. For looking at the intro video and skimming through the documentation, I think I mostly understood what it does and how it works. What I don't understand is the endpoint: can I show the dashboards to an end-user? Does it builds a website, or its usage is limited inside VS Code? reply louisjoejordan 1 hour agoparentWe've been focusing on the core VS Code extension and haven't released sharing yet. The plan is to provide a Vercel-like experience for deploying and sharing graphs. People will be able to connect their GitHub repositories, deploy dashboards, and share them via our website. The interface will allow switching between branches and time-traveling between different states of the dashboard. Here's a preview: https://www.youtube.com/watch?v=MD6In-iUd9g reply tayloramurphy 6 hours agoprevI'll ask another of the \"how is this different\" questions - how is this different from https://evidence.dev/ ? Quary seems a little like dbt + Evidence from what I can see. reply louisjoejordan 1 hour agoparentWe're big fans of the Evidence team. While there's some overlap, we have a heavier focus on data modelling (similar to dbt). The key difference is we've rebuilt the modelling layer in Rust, leveraging WASM for better performance and browser-based execution. This lets us build a more seamless, end-to-end workflow encompassing transformation + viz optimised for the web. reply itbk95 6 hours agoprevSounds interesting, I'll give it a try. reply louisjoejordan 2 hours agoparentGreat! Feel free to reach out to me with any questions. reply haaz 10 hours agoprevSeems similar to plotly dash, no? reply louisjoejordan 2 hours agoparentThe biggest difference I see (though I'm not super familiar with Plotly) is that we define data transformations in SQL, while Plotly uses Python. One benefit of SQL is that it provides the advantage of tracing data lineage from source to visualization, which gives you visibility into data dependencies - something that Python code in Plotly Dash doesn't offer. reply rkuodys 14 hours agoprevDoes it support datasource merges like redash do? I had hard time looking for simple solution where I could easily join data from multiple sources and provide simple charts from engineering to support teams. reply louisjoejordan 14 hours agoparentWe do if you use DuckDB and you pull data from your data sources through DuckDB. DuckDB can act as a single interface between multiple data source types. Feel free to DM me with any more questions. around your specific use-case and I can help. reply tomrod 11 hours agorootparentThis would make a good blog tutorial, I think. reply louisjoejordan 1 hour agorootparentI think so too, will put this as a to-do. reply rubenfiszel 18 hours agoprevFrom an external look, that sounds a lot like what dbt is meant to be. Why would one choose quary over dbt? reply louisjoejordan 17 hours agoparentHey, OP here. We love what dbt has done for transformation-layer engineering. But we often see companies still struggling with a mess of unstructured dashboards, even with solid dbt models underneath. The problem is that dbt models and BI dashboards are often managed by separate teams. Quary brings the two together, letting engineers define reusable models and build well-structured dashboards on top of them in one cohesive, code-first environment. reply rubenfiszel 17 hours agorootparentI think it finally occurred to me that you care only to transform data insofar as it is for the purpose of being used in BI/dashboards and not for data warehouse purposes. That wasn't clear to me at first but it makes sense. reply ksbeking 16 hours agorootparentWhile that's somewhat true, our CLI can push transformations back to your warehouse. We and some of our customer use Quary for their \"data warehouse purposes\" also. We think the integrated flow makes the E2E experience very quick. reply iamacyborg 3 hours agorootparentprevSo it’s Looker and LookML. reply tnolet 17 hours agoprevHow is this different from Lightdash? https://github.com/lightdash/lightdash reply louisjoejordan 17 hours agoparentBig fans of our fellow YC mates at Lightdash! There are some core differences that make our product feel quite different: - Lightdash isn't Lightdash without dbt so you always have that divide even though they have done a fab job of minimizing it. - The editor for us is in Visual Studio Code which means you don't have that jump and can iterate all together. - Every thing is version controlled as a file in your repository which means you can add those engineering practices to the dashboards/charts themselves. reply cuchoi 15 hours agorootparentWhat do you mean by \"Lightdash isn't Lightdash without dbt\"? reply sails 3 hours agorootparentNeeds dbt to function reply 3abiton 10 hours agoprevHow does it differ from OpenDashboard? reply louisjoejordan 1 hour agoparentFrom what I can see, OpenDashboard is tackling workflow automation tasks. We're more focused on the data modelling process. reply b2bsaas00 17 hours agoprevHow is different from Grafana? reply ksbeking 17 hours agoparentBen here from Quary. We love Grafana! It's fab for building dashboards, but it's focused on dashboarding/alerts and on pulling from various data sources, not just SQL. Quary is purely focused on SQL, and crucially, it allows you to build up and develop more complex transformations. reply nwatson 13 hours agoprevSee also Eclipse BIRT ... https://en.wikipedia.org/wiki/BIRT_Project . It seems to have languished for a while but it's active once again based on updates to this Stack Overflow posting: https://stackoverflow.com/questions/53362448/development-sta.... reply mmsimanga 2 hours agoparentGreat to see BIRT mentioned on HN. I use BIRT to generate PDFs for clients. Modern BI tools are about interactivity and real time but PDFs still have a role in BI and BIRT does the job. As it uses JDBC to connect to data sources you can connect to most data sources. For many tools these days one of the first things you have to check is which data sources does it connect to. If you use a less popular database chances are your database will not be supported. I have worked in organisations that use DB2, Sybase, Oracle and so on and these tend not to be supported by modern BI tools. PDF generation also seems to be a snapshot of the page. So yes BIRT is a great tool, old school and a bit clunky but it does the job. reply louisjoejordan 13 hours agoparentprevThis is awesome! Great to see this project still alive after so many years :) reply mdaniel 18 hours agoprevJust out of curiosity, what was the reason for the MIT -> Apache 2 move? https://github.com/quarylabs/quary/commit/db7a42a58ce66df13f... reply ksbeking 17 hours agoparentHey, Ben here from Quary; very valid comments like the one below copied meant we rethought our strategy it a little. We want to be open source but think we need a little protection. \"Hate to derail the conversation, but is Quary something I could easily whitelabel to embed BI into my product for my customers? (Passively) looking for solutions in that that don't feel dumbed down.\" reply jsiepkes 16 hours agorootparentYou mean protection as in protection from intellectual property (patent) lawsuits? reply ksbeking 16 hours agorootparentYep, I meant protection in terms of intellectual property. reply cynicalsecurity 14 hours agoprevResembles Redash. reply louisjoejordan 14 hours agoparentHey! We love Redash too. Where Quary is different is that we have more of an emphasis on Transformation. This means people can split out complex SQL blocks into modular, reusable components which improves data lineage (how the data flows from table to visualisation). Dbt makes transformations modular and easier. It applies software development methods to the T of ELT. reply halfcat 15 hours agoprevWe are looking at moving our Power BI stuff to Apache Superset [1]. How does this compare to Superset? [1] https://superset.apache.org/ reply ksbeking 15 hours agoparentSuperset is a beautiful tool focused on self-serve with amazing visualizations. I won't take anything away from them! Our thesis is that self-serve is much less important than people think, and we find people often make a mess of never-ending dashboards. Current BI tools struggle to prevent that. We solve this problem with a core of software engineering practices. reply code_biologist 13 hours agorootparentIf you're targeting use within software and engineering teams, that thesis may be right. If you're targeting adoption across whole businesses, I think the thesis is pretty wrong and will end up hampering adoption. To broadly bucket BI challenges, there's first the challenge of getting people to use the thing, then the challenges that come when everyone is using the thing. Tech types seem to underrate the challenge of getting people to even use a BI tool in the first place. I've found self serve to be a really effective tool in getting engagement with BI. My onboarding for new non-tech BI users was always to have them build a basic dashboard for the business process they were most focused on. Maybe set an alert or create a scheduled report delivery. By the end of a 15 or 30 minute onboarding session you'd see the click as they realized what they could do with it. That mess of never ending dashboards has another name: BI engagement. Though a product can help, having core dashboards and KPIs is a social and analytics leadership problem and not a technical one. Though I have issues with Looker (their dev experience is crappy), their approach to this is effective: make it difficult for self-serve users to get incorrect or nonsense answers, and make it easy for analytics admins to designate core dashboards and jockey a few hundred custom dashboards and reports as the underlying data models change. Every business unit got pretty attached to what they'd built for themselves. reply louisjoejordan 12 hours agorootparentYou're spot on that BI adoption is largely a social challenge. Our thesis is that by defining the entire journey from source to viz as code, we create a structured foundation that LLMs can build upon, democratizing access to the transformation layer for non-engineers in a way that point-and-click BI tools can't. reply ringobingo 11 hours agorootparentCan you please elaborate on how you see LLMs could build upon this model/journey? reply beardedwizard 5 hours agorootparentLlms would generate the code/definitions underlying these dashboards, presumably a model could be trained for the task. I'll argue it trades one version of the sprawl problem for another. Unless this generated code is easy to debugs and comprehends other generated code, it will still be a spaghetti mess at scale. reply xn 17 hours agoprevHow does it quary compare to rill? reply louisjoejordan 17 hours agoparentHey, great question ... Again another tool we love. A few key differences: - Visual studio code as the editor through and through - Dashboards are fully defined in code Quary which is different to Rill - At its core our architecture is also very different, Rill is built on top of Duckdb for that interactivity which can call out to other databases whereas we can call other SQL databases without everything going through DuckDB. reply vincentw21 17 hours agoprevthis looks awesome! reply louisjoejordan 17 hours agoparentHey! Thanks so much, really appreciate the feedback reply ksbeking 17 hours agoparentprevthanks! reply _hl_ 18 hours agoprev [–] Hate to derail the conversation, but is Quary something I could easily whitelabel to embed BI into my product for my customers? (Passively) looking for solutions in that that don’t feel dumbed down. reply louisjoejordan 17 hours agoparent [–] Hey! OP here, I don't have a clear answer for this yet. We're exploring ways to make Quary more extensible. We are focusing on the core piece first, happy to chat to hear more about your specific use-case. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Quary is an engineer-centric BI/analytics tool addressing engineers' issues with self-serve BI by incorporating standard software practices.",
      "It integrates with databases, particularly focusing on Supabase, and is now available as open source under the Apache 2.0 license.",
      "Planned updates involve a web platform for sharing data models and an orchestration engine, with users urged to test Quary and offer input."
    ],
    "commentSummary": [
      "Quary Labs released an open-source BI and analytics tool, Quary, catered towards engineers, emphasizing SQL transformations, data modeling, and well-structured dashboards.",
      "Positive feedback has been received, hinting at upcoming paid features, increased extensibility, and comparisons with dbt, Lightdash, and Grafana, focusing on a code-first approach and decluttered dashboards.",
      "Users have shown interest in using Quary for white-labeling and embedding BI into their products, drawing parallels with tools like BIRT and Rill."
    ],
    "points": 234,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1715781755
  },
  {
    "id": 40369119,
    "title": "Using \"BEGIN PGP MESSAGE\" bypasses email filters",
    "originLink": "https://nondeterministic.computer/@martin/112444389342113780",
    "originBody": "Create accountLogin Recent searches No recent searches Search options has: media, poll, or embedis: reply or sensitivelanguage: ISO language codefrom: userbefore: specific dateduring: specific dateafter: specific datein: all or library nondeterministic.computer is part of the decentralized social network powered by Mastodon. Administered by: Server stats: Learn more nondeterministic.computer: About · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.2.8 ExploreLive feeds Login to follow profiles or hashtags, favorite, share and reply to posts. You can also interact from your account on a different server. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=40369119",
    "commentBody": "Starting emails with \"BEGIN PGP MESSAGE\" will fool the filter (nondeterministic.computer)205 points by ColinWright 17 hours agohidepastfavorite59 comments dkdbejwi383 1 hour agoReminds me of a bug from the early 00s in a bunch of router firmware, where the router would crash/reboot on receiving a malformed \"DCC SEND\" message, so people would spam \"DCC SEND ANYLONGSTRINGHAHAPWNED\" in large IRC channels and watch as half the participants dropped. reply berkes 1 hour agoprevI've never really understood how this 'urls via our own redirector' really works, if at all. I can imagine some reasons why it would work better than merely checking the URLs when filtering: not rewriting but simply checking and scoring against other spam rules. But also see reasons why it secures less well. Pro: you can block URLs post-delivery. E.g. an advisary changes the link to malware or phishing after some time/n requests/based on user agent. Con: all encryption, including DKIM, GPG etc break. Edit: to clarify why I don't understand why this works: and advisary can still do the same: i.e. serve an innocent page to the checker but then a bad webpage to the actual visitor. Not trivial, but not hard either. A good middle ground might be to scan URLs on the mailserver against lists and maybe open the page and scan its content before delivering. Then have the client intercept links and redirect them through another on-demand checker? Because obviously, training people to \"not click links in emails\" or use only plain text or both, hasn't worked in the last decades. reply Maxion 52 minutes agoparentThe most funny filters are those that visit the URL to check what it is. Those break all the magic login links, email verification links et. al. that people receive. Really can't understand who approved such things. reply acidx 8 hours agoprevDetails are hazy as this was a long time ago, but at some point you could make parts of messages not render in Outlook and Outlook Express by writing \"begin something\" (two spaces after \"begin\") by itself in a single line. Outlook would thing that it was the start of an uuencoded block and not render anything after that. I remember annoying friends in a mailing list by quoting emails with \"begin quote from Person Name:\" :) reply gpderetta 3 hours agoparentYes! And I remember that the official recommendation from MS was \"do not write begin at the start of a line\". reply ale42 2 hours agorootparentTypical MS... reply DeathArrow 3 hours agoprevIt might fool the University filter but it will trigger the NSA filter. reply lostlogin 40 minutes agoparentWe’re afraid of the three letter agencies, but their list of failures would suggest that quite a lot gets past them. reply ethbr1 16 hours agoprevIn my experience, the vast majority of corporate mail filters ban certain file types based on name extensions. Fewer, but some, inspect files to deduce their type. None care about encrypted zips with the file renamed to a common extension (encrypted zip manifests are unencrypted, so the file names are still visible). reply mrgoldenbrown 14 hours agoparentOur mail filter inspected zip files even if you renamed them to .txt or jpg, and if it couldn't check the contents because of encryption it would just delete the whole zip. It would also look for data that might be PCI related and flag that as well. reply kccqzy 8 hours agorootparentAnother thing I've heard of is that commonly people include the password for the encrypted zip inside the email, so email filters have learned to try to decrypt the zip using every single word in the original email. reply srockets 6 hours agorootparentThen I wouldn’t dare emailing a zip bomb with a message containing the password and an instruction not to try and unzip that file. reply consp 48 minutes agorootparentprevAnother good reason to always send the passphrase via another channel. reply userbinator 9 hours agorootparentprevThat's when you start using the steganography. reply godelski 15 hours agoparentprevAnd many people have learned to bypass these filters by renaming extensions. You can always zip things up or just rename foo.py to foo.py.pdf But I understand that there is still reason to filter filetypes. Apparently some programs will run programs if they see certain filetypes... Here's a recent telegram exploit where the user did have to click on the file https://www.bitdefender.com/blog/hotforsecurity/telegram-pat... reply jerf 14 hours agorootparentYes, it isn't just voodoo, \"properly\" labelled file types can carry dangers that \"improperly\" labelled ones do not. For example, if someone wants you to open a Word document with a bad macro, getting you to open it may be no big deal, but getting someone to \"OK, first save it, then navigate to it with Explorer, then change Explorer to 'Show Extensions', then rename it to this, then open it\" is likely to either set off some alarm bells, or simply be impossible for the technically-unsophisticated target. Even if it is the same bytes nominally behind the \"improper\" and \"proper\" metadata labels the security profile of the two bits of content can still be very different. Also, obviously, you'll always be able to get things \"through\" a filter like this. But the value of raising the bar of the exploit is still quite substantial; the \"conversion funnel\" for such exploits has a very sharp dropoff at every step, including even the first (most such attempts at an exploit even if delivered would not be unpacked by the target user). Systems can generally block encrypted archives, though I suspect that many admins end up leaving that \"off\". I'm not sure it's a huge vector in the real world. My impression is that at the moment the most dangerous emails are the social attacks. Though the technical attacks are still non-trivial, still hit people, and technical folks can underestimate the need for non-technical folks to be protected from them. reply godelski 12 hours agorootparent> obviously, you'll always be able to get things \"through\" a filter like this. But the value of raising the bar of the exploit is still quite substantial I just want to stress this part. So many people I talk to will just dismiss things because something isn't bullet proof. Like there's a binary option. But in reality there's a continuum. I'm the annoying person that tries to get my friends to use Signal, but then say if you won't install, that WhatsApp is my next preference. People on Signal forums will say that you shouldn't have the ability to delete or nuke conversations (now you can delete some, but only ifdo automagic things instead of pushing for people to understand extentions A file name extension is a convenience for human-computer communication but insufficient metadata about a file to process without inspection. Examples include BOM, Exif. https://stackoverflow.com/questions/2223882/whats-the-differ... https://www.sciencedirect.com/topics/computer-science/file-s... reply WirelessGigabit 13 hours agoparentprevThis reminds me of working at a company in Brussels during eBays heydays. Their URLs looked like http://offer.ebay.com/ws/eBayISAPI.dll?... And the filter saw .dll and denied my request. reply anyfoo 9 hours agoparentprev> Fewer, but some, inspect files to deduce their type. I don’t know. That’s extremely error-prone, often easy to fool, and in some cases hardly possible in the first place. I think the only thing that’s really feasible is filtering out known bad things as some mild form of damage reduction. reply kccqzy 8 hours agorootparentGmail uses Magika https://github.com/google/magika to deduce file types to determine whether and how to scan email attachments. reply anyfoo 7 hours agorootparentNeat. And I think that proves my point… reply stuffinmyhand 2 hours agorootparentEspecially the \"whether\" :D reply stvltvs 16 hours agoparentprevThere is a way to cajole zip into encrypting filenames if this is important enough to put up with unzipping twice. Plus you might get better compression! https://unix.stackexchange.com/questions/289999/how-to-zip-d... reply ethbr1 14 hours agorootparentPoint! I forgot about that, from my pathological remoted-through-3+-systems consulting days. reply reidrac 3 hours agoprevA few times I got spam with all the text in the signature. And I thought, weird: and it makes the email look funny because the signatures are shown on a different color in my email client. Now I'm thinking if there was some smart idea behind that trying to fool some filters. reply jamespo 15 hours agoprevI've recently had to zero score SpamAssassin ENCRYPTED_MESSAGE rule as it's being exploited. Also I don't receive any encrypted messages. reply stuffinmyhand 2 hours agoparent> Also I don't receive any encrypted messages. Should start giving people your public key then! reply g4zj 17 hours agoprevI don't know much about mail servers, but would it be possible to validate the signature somehow before delivering the message? Is this impossible or impractical for some reason? reply cmgbhm 16 hours agoparentPGP doesn’t require the use of global directories so you can’t do strong validation. This is a bypass game on something like proof point url protection that rewrites URLs to go through a central redirect so that reputation check can be separated from reputation delivery, usage detected, and one liner URLs still work. I’m over a decade out of this space so take with a grain of salt. The tool could do a better job inspecting “oh that’s a pgp formatted massage”. You could probably just route them all to spam and almost no one would notice. reply thenewnewguy 16 hours agoparentprevEven if they did, you could just sign the message for real. reply upofadown 16 hours agoparentprevEmail signatures are essentially end to end. The company would have to keep a file of all the identities of all the people that everyone in the company knew. To make this useful, the company would also have to record information about who knew who. As already mentioned, the email might also be encrypted (the most common case). Then the signature would not be available as it would be hidden under the encryption. These sorts of filtering issues are why corporate cybersecurity people, somewhat ironically, tend to dislike encrypted email. There is an opportunity here to make better email clients that treat encrypted, but unsigned, emails with suspicion. reply layer8 16 hours agoparentprevThere is commercial mail server software that does exactly that. reply mrkramer 16 hours agoprevHug of death? reply godelski 15 hours agoprevWell it looks like the link is down and the only archive snapshot I see is a loading screen: https://archive.is/obzJ9 ----- BEGIN EDIT ----- I can see it from the mastadon app but not in browser. I took a screenshot for others: https://i.imgur.com/q3DA7wQ.png Text is: Our university deployed a mail filter that rewrites URLs in emails to redirect them via a service that checks for bad websites. Somebody clever worked out that PGP-signed emails are exempt from the rewrite rule, so now people are starting their emails with \"BEGIN PGP MESSAGE\" even though they haven't used PGP at all, just to fool the filter Anybody sending malware links has probably also worked out that trick by now, thereby rendering the entire filter pointless @martin@nondeterministic.computer ----- END EDIT ----- But I have an interesting issue which is (presumably) related. I keep getting obviously spam emails in my gmail and Google has refused to do anything about it (after finally getting through to support). These are embarrassingly bad emails. Ones you'd expect a naive bayesian classifier to get near 100% accuracy on! In the \"to\" address is something that's not even remotely my email. Suppose I'm godelski@gmail.com, the \"to\" has something like \"robbert01@gmail.com\" and always has a CC with a different number. Subject lines will be things like \"confirmation of receipt\" but in non-standard and inconsistent fonts. And the message body is that all too typical single image and claim of something like winning a home depot gift card. The \"from\" address is always some scammy looking address. So Naive Bayes should get it on just this alone! But the weird thing is when you show the original message. There's PAGES of stuff in here (about 20 pages or 9.5k words)! All kinds of email conversations, things including stuff like \"here is your one timepassword\", stuff in other languages, survey questions, conversations, and a lot more. I actually did a diff on a few of these and they're almost identical too! I can't help but think that these are some weird prompt injections. It looks like it is written by a madman, but it it's probably some \"throw shit at the wall and see what sticks\" kinda thing. But the message ID always comes from some Microsoft (something like microsoftstore1.microsoft.com). This is where Google support said it isn't their problem and moved on. I'm pretty critical of LLMs and their ability to reason, but I feel often we're trying to turn humans into machines instead of letting them complement one another. And I think this is why so many business people like LLMs, because they think everything can be figured out with clear immutable rules. But the thing is that the environment always changes and you need something that's not only adaptable, but can reason AND understands the actual desired outcome that humans want. I hope someone at Google pays attention. Because you can't just hand all thinking over to humans. We're not there yet and it's why everything is going to shit (not just Google) reply jeffbee 17 hours agoprevThere was a time when prefacing your message with `begin 644` would hide your message, or the remainder of it, from users of MS Outlook. Maybe still works, who knows. reply bongodongobob 17 hours agoprevTried this with my work email, it doesn't seem to work with whatever our exchange setup is. Is this just a case of an admin using the default settings in their filtering? reply layer8 16 hours agoparentIt’s a custom filter deployed at that university. reply swyx 16 hours agoprevobligatory https://xkcd.com/1181/ reply gramakri2 16 hours agoparentheh, I am awed by the foresight of the author ! reply superkuh 17 hours agoprevHere is the content that is in the HTML but which the linked page refuses to actually show on the screen, instead opting for a \"run javascript\" message:reply cesarb 16 hours agoparentA trick which works if you use Mastodon, since that page is a Mastodon post: Go to the Mastodon instance where you have an account, and paste the URL in the search box. It should load the post within your Mastodon instance (and allow you to boost/favorite/etc), without running any JavaScript from the post's instance. (It won't surprise me if someday a Firefox plugin is created to open these Mastodon post pages without running their JavaScript, by detecting it's Mastodon and going through the same protocol as federated instances to fetch the content. Of course, it would be better if the Mastodon server software didn't require JavaScript for simply showing a post page in the first place.) reply berkes 1 hour agorootparentNot exactly the plugin you envision, but Graze¹ does this for interaction (boost, favorite, follow, reply etc) with posts on any instance. I guess the redirect and re-open on your own instance would be a feature for graze? IDK. ¹ https://addons.mozilla.org/en-US/firefox/addon/graze/ reply ethbr1 16 hours agoparentprevIMHO, there's definitely some security value is at least running a non-standard config. Moving from seeing all attacks (even if blocked) to only targeted attacks decreases the amount of noise a detection system/reviewer has to deal with. The important bit, though, is remembering only the above doesn't mean things that make it through are secure, which is usually where companies fall over. (I.e. implement dumb mail filter, then assume any mails that make it through are safe) reply Isognoviastoma 16 hours agoparentprevThanks for sharing! A bit of css, and it's possible to read mastodon posts with a web browser! head {display:block; background:Canvas; color:CanvasText;} meta[content] {display:block; padding-bottom:1em;} meta[content]::after{content:attr(content); display:block;} reply o11c 9 hours agorootparentOr just append /embed The disadvantage is that threads don't render. reply cwillu 16 hours agoparentprevThank you. reply Hizonner 16 hours agoprev [–] There is a special place in Hell for people who \"helpfully\" rewrite email bodies in transit. reply jacoblambda 8 hours agoparent [–] Stares aggressively at Protonmail. Yeah you at Proton. I'm talking to you. Stop doing this. No touchy the email. reply jimbosis 7 hours agorootparentDo you or does anyone else have a summary of or any link to read about what you are alluding to, namely Proton Mail rewriting email bodies? reply super_linear 7 hours agorootparenthttps://news.ycombinator.com/item?id=36639530 reply ChainOfFools 5 hours agorootparentprev [–] doesn't this break DKIM? reply 256_ 4 hours agorootparent [–] DKIM headers can have an optional length parameter which says \"the first n bytes of the message body is cryptographically signed\", whilst allowing the rest of the message to be modified. I don't know if anyone actually uses it, though; I don't think I've ever seen or used it. Also, DKIM can optionally be lax about whitespace. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nondeterministic.computer is a decentralized social network based on Mastodon, offering search options for media, polls, and embeds with filters for replies and sensitive language.",
      "Users can explore live feeds, follow profiles and hashtags, and engage with others from their account on a separate server.",
      "Administered by server stats, users can dive deeper into information about both nondeterministic.computer and Mastodon."
    ],
    "commentSummary": [
      "The conversation delves into circumventing corporate email filters, including encrypted zips and PGP signatures, highlighting concerns about email security and the efficacy of filters.",
      "Users discuss experiences with spam emails, questioning the reliance on machine learning models, and address issues with email rewriting practices and DKIM headers."
    ],
    "points": 205,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1715790728
  },
  {
    "id": 40366137,
    "title": "Challenging 7th Graders: Addressing Drug Use",
    "originLink": "https://www.experimental-history.com/p/how-to-get-7th-graders-to-smoke",
    "originBody": "Share this post How to get 7th graders to smoke www.experimental-history.com Copy link Facebook Email Note Other Discover more from Experimental History 1) Find what's true and make it useful. 2) Publish every other Tuesday. 3) Photo cred: my dad. Over 40,000 subscribers Subscribe Continue reading Sign in How to get 7th graders to smoke OR: \"fuzzy, wet, and unfalsifiable\" Adam Mastroianni May 14, 2024 217 Share this post How to get 7th graders to smoke www.experimental-history.com Copy link Facebook Email Note Other 43 Share Article voiceover 1× 0:00 -22:59 photo cred: my dad Years ago, I wrote a master’s thesis that was so bad I immediately threw it in the trash. But along the way I learned something important, which is how to get seventh-graders to smoke. My lesson came from Hansen et al., 1989. They randomly assigned students at eight junior high schools in Los Angeles to complete one of two anti-drug programs: A) A “Social” program that was all about resisting negative influences and fostering good friendships B) An “Emotion” program that was all about managing bad feelings or C) No program at all The researchers surveyed the 2,863 participating students three times: just before the programs started, and 12 and 24 months after the programs ended. (They also took saliva samples to keep students honest.) The programs themselves lasted for 12 weeks. In the Emotion program, students did things like: Compliment each other to improve their self-esteem Set goals for themselves and think about how drugs interfere with those goals Learn strategies for emotion regulation like deep breathing and muscle relaxation Practice being assertive and saying no to drugs in role-plays Make a videotaped commitment to stay drug-free Some of this stuff seems a bit dated now, sure, and the video thing sounds a little creepy. But mostly it looks reasonable—couldn’t hurt, right? Except it did hurt. Students who did the Emotion program ended up smoking more cigarettes and more weed and drinking more alcohol than students who received no program at all. Which is to say: researchers came into schools in Los Angeles and ran a drug prevention program that caused a bunch of seventh-graders to do more drugs (Best of all, the name of this program was Project SMART.) What about the kids in the Social program? Basically bupkis. The effects were way stronger in the Emotion program, and in the wrong direction. (You should always be skeptical of any individual study, and especially one this old. But I’m at least willing to assume that these results weren’t p-hacked. If you were gonna cook your books, you would probably try to hide the fact that you got 12-year-olds to toke up.) This story makes me go “GULP”. You can have a PhD and good intentions. You can have money and buy-in. You can do a bunch of reasonable things to prevent a problem that everyone agrees is bad. You can spend a lot of time and effort running your project and collecting your data. And after all that you can, according to your own best calculations, make people worse off. 1 That’s just one random paper from the 80s, but the literature is littered with similar well-meaning interventions that failed or backfired: “Scared Straight” programs, where at-risk youth go to prisons and get yelled at by prisoners, famously do not work and maybe make students more likely to commit crimes. The Drug Abuse Resistance Education (D.A.R.E) program I had to do in elementary school, where a cop came to class and told us about all the drugs we weren’t supposed to take, also doesn’t do anything. A big trial of mindfulness training in UK schools maybe made kids a bit more depressed, and two attempts to teach psychotherapy principles to students in Australia both failed. Those programs all harmed kids or failed to help them, but don’t worry, we also harm adults sometimes, or fail to help them: Social scientists have invented lots of programs that try to make people less prejudiced. Every review of these programs concludes that they either don’t work or they don’t work in the ways we want or we don’t know if they work. Researchers came up with their best attempts at getting Democrats and Republicans to like each other more, and all of their ideas got outperformed by a Heineken commercial You know those signs on the highway that say things like, “Drive safely! Over 20 people have died on this highway so far this year!”? They maybe cause more people to crash and die.2 So yes, it’s hard to change people. But that’s okay—making iPhones is also hard, and we seem to have nailed that one. What really screws us is that it’s surprisingly hard to change people. We cook up schemes that seem like they should definitely work, then they don’t work, and this doesn’t chasten us or dim our enthusiasm for future schemes. Hansen et al., after accidentally causing seventh-graders to smoke, don’t end their paper with a long reverie on their hubris. They write a few self-exculpatory paragraphs and move on to the next project. The problem is that our illusion of explanatory depth is so deep when it comes to human behavior that we never realize how little we understand, which prevents us from ever learning more. Nobody thinks they can whip up an iPhone in their garage over the weekend, but most people think they know how to save the children, fix the schools, reform the prisons, overhaul healthcare, repair politics, restore civility, and bring about world peace. Perhaps that’s why we have iPhones and we don’t have any of those other things. Subscribe SHOULD WE RUN PARTICIPANTS OVER WITH A FORKLIFT? This myth of the easily-malleable human is so widespread and so deeply believed that it borders on delusion. Once you see it, though, it can make sense of some things that otherwise seem psychotic. For example, in the US, anybody who does academic research on humans or animals has to undergo ethics training first. That sounds reasonable, until you realize the “training” is just PowerPoint slides that you click through as fast as you can, and then you get asked questions like: Which of these represents responsible treatment of research subjects? A) Poking them in the eye B) Using the internet to discover their weaknesses C) Running them over with a forklift D) Treating them with respect And then there are a few questions like: Subsection 41b.46 of the 1972 Research Reform Act stipulates which of the following? A) Paragraph 17 of the 1971 Research Reform Act is null and void B) Principal Investigators are not allowed to be listed as Co-Principal Investigators C) Conflicts of interest must be reported on Form 99-F D) Any research expenditures greater than $100 must be done via certified check If you fail the exam, you just retake it until you pass. So this “ethics” test is actually a test of whether you can read at a sixth-grade level. Most workplace trainings look like this. Here are some questions from an actual sexual harassment training you can purchase for your organization at SexualHarassmentTraining.com: Some hypocrisy is so profound that it distorts the space-time continuum. Like when a conservative Christian senator sponsors a bill that’s like “Let’s Make It Illegal to Be Horny” and then gets caught at a brothel wearing a diaper and nipple clamps. That’s the level of hypocrisy we’re dealing with here. If you claim to care about preventing sexual harassment and then you’re out here asking whether the Civil Rights Act of 1964 is a “county” law, you do not care about preventing sexual harassment. I understand these trainings exist solely for the purposes of ass-covering. If you get sued because one of your employees, say, sticks a research participant with a dirty needle, you can go, “Hey, don’t blame us! We made them do an ethics training!” But why does this ass-covering work? I don’t mean legally; I don’t care if this is all because of Subsection 31a of the Let’s Make Things Worse Act of 1986, or because of the Supreme Court decision in Buttmunch v. Fartmeister or whatever. I mean why do we allow it? Anybody who has to suffer through these trainings knows that they’re stupid and useless. But when those same people are sitting in the jury box, they nod and say, “Well, you can’t blame the company, they made their employees do an ethics training.” This is the hypocrisy that really makes no sense—it’s hypocrisy on top of hypocrisy, a nipple-clamp on a nipple-clamp. I think we accept this ass-covering because we believe that it’s possible, and even easy, to mass-produce the improvement of humans. We think we can turn sex pests into good citizens via PowerPoint presentation. When we see those PowerPoints up close, of course, we go, “Oh no, not these PowerPoints. Some other PowerPoints, the ones that actually work.” But are no PowerPoints that work. You cannot plonk someone in front of a computer screen for an hour and expect them to become a better person. Well-meaning researchers have tried way, way harder than that and gotten way, way less. FUZZY, WET, AND UNFALSIFIABLE There are lots of reasons why we are continually surprised by how hard it is to change people, but there are three particularly nefarious ones that deserve closer inspection. 1. OTHER PEOPLE ARE TOO FUZZY We perceive ourselves in full 4000K HD with Dolby Atmos Surround Sound, but when we perceive each other, it’s like we’re watching a VHS tape that someone made by using a Motorola Razr phone c. 2007 to record the screen on the back of an airplane seat. Psychologists call this “psychological distance”: the further something is from me, the less detail it has in my imagination. So we are profoundly aware of how hard it would be to change ourselves, but only dimly aware of everything it would take to change someone else. You’re already kinda fuzzy anyway, so can’t you just be kinda fuzzy in a different way instead? Here’s an example. I’m a weakling, a real beta boy with limited upper body strength. Let’s say I want to get ripped, jacked, shredded, swoll, etc. If you made a workout plan for me, it might look something like this: Go to gym Work out Get jacked But if I made a workout plan for me, it would look more like this: Decide which of my very important tasks to ignore so I can go to the gym instead. Find workout clothes. Workout clothes are all old and ratty. Look for new workout clothes online. Spend the rest of the day figuring out whether I’m more of a nylon poplin guy or a jacquard knit guy. Wait for clothes to come in. Go to gym. Figure out which podcast I want to listen to while I work out. Okay now I’m ready to work out. Wait I’ve heard this episode before, this the one where Malcolm Gladwell proves that Toyotas are safe by driving one off the George Washington Bridge. Okay now I’m ready to work out. Oh no there’s someone on the machine I want to use. Mill around a bit, get on another machine and pretend it’s the one I wanted to use in the first place. Okay he’s gone. Get on the other machine. Oh no I don’t know how to use this machine. Watch a YouTube video where a frighteningly large man tells me how to use the machine. Oh no the frighteningly large man is now telling me January 6th was a false flag operation. Use machine. It kinda hurts, but I can’t tell if it’s a “You’re getting stronger” kind of hurt, or a “If you keep doing this you won’t be able to walk when you’re 57” kind of hurt. Repeat for another 45 minutes. Go home. Wait I have to do this every day until I die?? 2. WE SPRAY PEOPLE WITH A HOSE AND EXPECT THEM TO STAY WET FOREVER I’m going to talk about implicit bias for a second, so if you’re the kind of person who gets upset about that, please deactivate your Pedantry Module and just play along for the sake of the example. Psychologists would very much like to reduce people’s implicit biases, especially their anti-Black biases. So far, they have failed to do that. And they’ve tried lots of stuff: Helping people set a goal to not be biased Lecturing them about how great multiculturalism is Telling them a story that counters their stereotypes (“Imagine a White guy attacks you and throws you in his trunk, and then a Black guy shows up and saves the day!”) Making people play a virtual dodgeball game where all of their teammates are courteous Black players and all of their opponents are mean nasty White players Showing them a bunch of pictures of Black faces with the word “GOOD” written next to them Some of these interventions changed people’s biases in the moment, but none of the effects lasted beyond 24 hours. And of course they didn’t! As soon as participants leave the lab, they go right back into the world that massaged those biases into the folds of their brains in the first place. You can’t permanently change implicit biases with 15 minutes of screentime, just like you can’t spray someone with a hose and expect them to stay wet forever. This is part of why changing people is so surprisingly hard—no matter how much you focus on the person in front of you, you’ll never appreciate the million tiny influences that made them who they are and that keep them that way. If you really want to make someone different, you might have to change the TV they watch, the music they listen to, the things they learn in school, the friends they hang out with, the role models they look up to, etc., and if you do all that, congratulations, you’ve started what we call a cult. Which, unlike social scientists, do have a pretty good track record of changing people. 3. UNFALSIFIABILITY Most people’s theories of human behavior are just never gonna be tested, and so their hypotheses can be both wrong and immortal. For example, if you think the solution to political polarization is to drug Congress with magic mushrooms, then a) nobody’s ever going to prove you wrong, and b) I think I know which anti-drug program you got in high school. On top of this, we’re all running our own tiny-N, p-hacked studies and then slathering the results in confirmation bias: “My kid does better when we feed him raw beef and read him Ulysses, so that’s what all schools should do, and the fact that this also happens to fit my personal and political aesthetics is merely a coincidence.” “I gave my religion’s holy book to a person who was struggling and now they’re doing better, which is proof my religion is true.” “Some famous CEOs were mean to their employees and got good results, so I should do that too, which is convenient because I am a jerk.” These itsy-bitsy confounded little pseudo-studies are more convincing to us than any “official” experiments precisely because they unfold before our own eyes. On the one hand, that’s fair—“official” experiments are so poor on average that you should consider them guilty until proven innocent. But doing low-quality uncontrolled social science in your own backyard is a good way to increase your convictions without increasing your evidence. Even when there is reasonably good data, our hypotheses about how humans work are often so vague that they can withstand any attempts at falsification. Okay, so this emotions-based anti-drug intervention didn’t succeed, but maybe that’s because the instructors weren’t motivated enough, or it didn’t go on long enough, or they used the wrong kind of deep breathing technique, or they should have targeted fifth-graders instead of seventh-graders, etc., on and on, forever. If you can only think of all these critiques and exceptions after the fact, however, you at least have to admit you didn’t really know what you were talking about in the first place.3 LET’S GET STUPIDER The philosopher Michael Strevens argues that science requires humans to adopt an “alien mindset”—you have to ignore common sense, the wisdom of the ancients, the literal word of God, etc. Why would anyone toss out everything they know and instead try to learn things by putting rotten meat in a jar? Science took so long to develop, Strevens says, because it seemed stupid. I say: we must become even stupider. If all of our intuitions, theories, and knowledge cause us to run programs that make tweens do more drugs—well, then, we oughta ditch our intuitions, theories, and knowledge! Sufficiently stupid people would look at SexualHarassmentTraining.com and ask, “Wait, how does this work? You show people walls of text and then ask them multiple choice questions and then they don’t sexually harass each other anymore?” They would wonder whether an imaginary dodgeball game could really make people less racist. They might question whether a couple deep-breathing exercises are enough to stop seventh graders from doing shots at a party. It’s important to cultivate that kind ignorance, because every single person on Earth is at least a part-time people-changer. We want to raise our kids to be kind. We want to earn our boss’s respect. We would like our spouse to stop playing the “road trip playlist” that’s just a 10-hour loop of “Dreams” by The Cranberries and “Dreams” by Fleetwood Mac. We’ll never have large-N, preregistered, multi-site RCTs that can tell us how to do these things. We have to make our best guesses, but we also have to treat those guesses with enough skepticism that we notice when we guess wrong, so we can guess differently next time. Some people are supposed to be professional people-changers, and so they carry a greater burden of proof and a sacred set of responsibilities. When you have the power to compel people to do things, you should be able to prove that your compulsions make people better off. It’s not enough to say, “Hey, I’m just doing stuff that everybody would agree is intuitive and reasonable,” because, as we’ve seen, intuitive and reasonable stuff often hurts people. We can’t expect every well-meaning program to work. We know so little about how to help people that sometimes we’re going to screw it up, in which case our responsibility is to figure out what we did wrong. What we cannot do is accidentally turn tweens into binge-drinkers and then keep right on going as if nothing happened. This is a thorny problem, but I know exactly how to solve it. I’m just waiting for the IRB to approve my application to run people over with a forklift. Experimental History has never caused 7th graders to do drugs, as far as anyone knows Subscribe 1 By the way, if you work for Philip Morris and you’re rubbing your greedy little hands together because you’ve just discovered a new way to make kids smoke, don’t get your hopes up. In a later paper where the researchers caught up with these same students again, the difference between the Social and Emotional programs had mysteriously disappeared. The authors don’t even say whether the overall effect of the program was still there, which I bet means it wasn’t. Instead, the researchers go digging for a bunch of subgroup analyses (“It works for girls but not for boys! It works for Asian kids but not for Black kids!”). I doubt they actually have enough participants to be running these analyses—they may have been cherry-picking to make it look like the program did something. 2 Jason Collins blog has a great post about why he doesn’t trust this paper, which has little to do with the paper itself, but the fact that it’s a surprising single result and therefore doesn’t count for much. “You need to move from a perspective of looking at papers to looking at literatures, and then looking at those literatures with a an understanding that the literature is not representative of the full body of research [...] I don’t yet see that supporting literature.” I think this is a wise approach, which is why I qualify this finding with “maybe.” 3 I run into this all the time when I talk to people about the universal pre-publication peer review system. THEM: The problem is that no one gets paid to review papers. ME: They tried paying people and it maybe made their reviews worse. THEM: Well, it’s because nobody gets trained on how to review papers. ME: They tried training people and it didn’t help. THEM: Well, they need to pay people more and train them better. And yes, maybe they do! But if disconfirming evidence doesn’t shake your beliefs at all, you don’t really have beliefs, you have an ideology. Subscribe to Experimental History By Adam Mastroianni · Thousands of paid subscribers 1) Find what's true and make it useful. 2) Publish every other Tuesday. 3) Photo cred: my dad. Subscribe Error 217 Share this post How to get 7th graders to smoke www.experimental-history.com Copy link Facebook Email Note Other 43 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=40366137",
    "commentBody": "How to get 7th graders to smoke (experimental-history.com)203 points by HR01 21 hours agohidepastfavorite239 comments patwolf 20 hours agoThis brought back memories of the D.A.R.E. officer coming to my elementary classroom to give us the \"don't do drugs\" speech. At the end of the speech with feigning hesitation he'd pull out the drug briefcase to the cheers of the kids, like a band holding out their best song for the encore. \"This here is methamphetamine, aka crystal meth or speed. If you see a man running down the street with the strength of a bear, there's a good chance he's high on this. It's very bad. You don't want to get near this--it'll make your heart explode.\" It's a wonder that didn't work. reply flatline 19 hours agoparentI did not get the briefcase in my DARE education. What I did get is a cop telling me that my parents having a glass of wine with dinner or a cigarette after work was just as bad a them using cocaine, and that you could OD and die on weed. All drugs are equally bad. A few years later when my friends were smoking weed and laughed at my concerns, and I realized I’d been lied to, I pretty much did everything under the sun. An actual education would have let me make better choices about my recreational drug use as a young person. reply vundercind 19 hours agorootparentThe realization that almost all illegal drugs (heroin, meth, a handful like that that feel too good—probably exceptions) aren’t really in some much worse category than alcohol and nicotine, and that some are surely less-bad, even, was quite a revelation. Must be a shared experience for a lot of the DARE generation. reply Retric 19 hours agorootparentI think people underestimate many illegal drugs because they don’t know long term users. Alcohol and Tobacco have minimal short term health risks. Most people throw up rather than OD on Alcohol. Obviously driving while fucked us is dangerous, but the same is true of a lot of drugs. Driving stoned is an underappreciated risk. LSD arguably the poster child for safe illegal drugs, but still physically risky due to people doing dangerous things in it and falling etc. However the mental risks with heavy use are significant much earlier. Luckily it’s not particularly addictive, but IMO it’s more dangerous than both Alcohol and Tobacco even as people claim otherwise. Tobacco is seriously deadly long term, but the risks are low for the first 20 years. When you include risks from throwing up while unconscious etc, heavy use of many seemingly safe drugs end up risky on those timescales. reply nicoburns 18 hours agorootparentLSD is absolutely NOT the poster child for safe illegal drugs: Something like cannabis, MDMA, or amphetamines would be. I think all of those have a decent claim to being safer than or comparably safe to alcohol. Tobacco, as you say isn't particularly bad in the short term. But it is particularly addictive, so you end up with a large proportion of addicts / long term users for whom the longer term effects apply. reply NoMoreNicksLeft 17 hours agorootparent99% of what makes illegal drugs unsafe is caused by prohibition. Liquor store clerks aren't known to murder drinkers because the drinkers have been trying to pass fake twenties. The cops and beer dealers don't get in shootouts and kill each other or bystanders. Any time someone ODed, it was because the heroin was laced with fentanyl, or cut too much or not enough... none of which happens if heroin is regulated by the FDA, and doses are measured and unadulterated. With clean disposable needles in the box, no one gets AIDS or hepatitis. And you could probably train the junkies to use sharps containers and return them when full to get their deposit back. Hell, even petty theft is caused by piss tests... if you truly believe that these people become addicted to badly that they'll do anything for a fix, hey guess what? That includes scrubbing toilets for minimum wage. So why do they steal copper wire out of foreclosed homes for scrap? Because piss tests disqualify them for the toilet scrubbing positions. reply malfist 17 hours agorootparentprev>Something like cannabis, MDMA, or amphetamines would be Are you really arguing that meth is a poster child for safe illegal drugs? reply mock-possum 17 hours agorootparentArguing for recreational ADHD meds more likely reply cooolbear 17 hours agorootparentprevfalse equivalency reply Retric 18 hours agorootparentprevCannabis is linked to a significant number of traffic fatalities. Cancer risks are also meaningful with lifetime use so it’s a mix of Alcohol risk and Tobacco risk. Safe enough to be legal, but not really safe. People have fatal overdoses on MDMA and hart attacks etc. Amphetamines use is linked with cardiovascular issues and brain damage. reply imchillyb 17 hours agorootparent> …2019 on deaths in three areas that have previously been linked to cannabis use but are still poorly understood: motor vehicle accidents, suicide and opioid overdose. For each cause of death, the researchers compared trends in deaths in states with legal markets with those in states that had comprehensive medical cannabis programs and similar trends in death rates prior to implementing markets… The study did not link cannabis use with traffic fatalities. The study linked trends. Cannabis use was not a factor in those deaths. Not listed as a cause. Not listed as intoxicated at time of accident. There is a significant difference between the narrative being schlepped versus what data was actually accumulated. reply Retric 17 hours agorootparenthttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC8195290/ Findings show that meta-analyses and culpability studies consistently indicate a slightly but significantly increased risk of crashes after acute cannabis use. These risks vary across included study type, crash severity, and method of substance application and measurement. Some studies show a significant correlation between high THC blood concentrations and car crash risk. Most studies do not support this relationship at lower THC concentrations. However, no scientifically supported clear cut-off concentration can be derived from these results. Further research is needed to determine dose-response effects on driving skills combined with measures of neuropsychological functioning related to driving skills and crash risk. reply jwagenet 17 hours agorootparentprevMDMA overdoses are almost certainly all cross-contamination with fentanyl. reply Retric 16 hours agorootparentHere’s 2 MDMA deaths that aren’t: https://pubmed.ncbi.nlm.nih.gov/22640978/ From one rave “Twelve patients with 3,4-methylenedioxymethamphetamine (MDMA) toxicity from a single rave event presented to multiple San Francisco Bay area hospitals with various life-threatening complications including seizures and hyperthermia. Eight required emergent endotracheal intubation and six had hypotension. Hyperkalemia, acute kidney injury, and rhabdomyolysis were present in most of the patients. In all, 2 patients died, 4 survived with permanent neurologic, musculoskeletal, and/or renal sequelae, and 6 survived without any apparent lasting deficits. Hyperthermia was present in 10 patients and was severe (40.9-43° C) in 7. Using multiple cooling methods, the average time to achieve cooling was 2.7 hours. Serum drug analysis was performed on 3 patients, demonstrating toxic MDMA concentrations without the presence of other xenobiotics. Two capsules confiscated by police at the event contained 82% and 98% MDMA, respectively, without other pharmacologically active compounds. Capsule #2 contained 270 mg MDMA, which is more than twice the amount of MDMA usually contained in 1 dose. The MDMA-induced hyperthermia significantly contributed to the morbidity and mortality in this case series. Factors contributing to the severity of the hyperthermia include ingestion of large doses of MDMA, a warm ambient environment, and physical exertion.” To be clear drug testing would have discovered fentanyl or other common drugs in their bloodstream, so it’s not just based on the pills recovered. Granted legalization would likely result in more consistent doses which would help, but the drug itself would still be risky if people just kept consuming more. reply robocat 12 hours agorootparentEquvalent to saying Alcohol is toxic amongst a sample of people that died due to drunk driving. It appears thar heat stroke caused the problems not \"toxic MDMA concentrations\". The LD50 of MDMA is estimated to be 180mg/kg - Were those ravers taking tens of capsules? I presume there are toxicology reports from the deaths that estimated the dosages. But alcohol is poisonous if you have too much - we don't usually call it \"toxic\". Weirdly they mention \"a warm ambient environment\" but what were the humidity levels? I presume there is good information on the underlying causes of death elsewhere? That summary is terribly uninformative. \"mortality from classic heat stroke approaches 80% and, for exertional heatstroke, 33% in the absence of prompt treatment\" : https://www.nejm.org/doi/full/10.1056/NEJMcp2210623 Disclaimer: personally I'm not a MDMA user. I'm a concerned citizen that thinks raves are generally healthier than bars. \"sensible\" MDMA usage anecdotally within those events seems safe to me (sample of thousands of people, zero deaths). Number of people I know dead from MDMA: zero. Number of people I personally know dead from alcohol: many (I am a middle aged guy so some were long term effects, one was in a house fire). reply Retric 12 hours agorootparentMDMA is one of many drugs that interferes with temperature regulation. It’s a direct contributing factor and they wouldn’t have had severe medical issues without taking it. Drunk driving is a more indirect risk, but I am also including that as one of the risks of Alcohol consumption elsewhere in the thread. If you’re objecting to the name fine, but the risk still exists as MDMA consumption regularly kills people due to this and other risks. PS: Water toxicity is a similar situation. It’s not just the amount of water alone that’s a problem instead the amount they’ve been sweating is often a contributing factor. Oddly, they also mention MDMA as a risk: https://en.wikipedia.org/wiki/Water_intoxication reply evanelias 15 hours agorootparentprev\"Large dose\" doesn't mean the same thing as \"overdose\". For context, the LD50 for MDMA is 160 mg/kg for rats. MDMA screws up the body's ability to regulate temperature. But without the warm environment and physical activity leading to hyperthermia, I can't imagine any of those people would have even needed medical attention. That said, personally I would never call MDMA \"the poster child for safe illegal drugs\". Just because it won't kill you, doesn't mean it is safe or risk-free. reply Retric 14 hours agorootparentIf you’re suffering significant medical issues from taking excessive amounts of a drug, that’s called an overdose. More broadly, I’m including traffic fatalities as a risk from alcohol and falls under LSD, so having deadly issues regulating body temperature definitely qualifies in an apples to apples comparison here. reply evanelias 14 hours agorootparent> If you’re suffering significant medical issues from taking excessive amounts of a drug, that’s called an overdose. Again, no, that San Francisco situation is strictly not describing an overdose. From the comfort of your living room, you can consume a massively higher amount of MDMA than what was described in that paper, and won't suffer significant medical issues. > I’m including traffic fatalities as a risk from alcohol OK, but that doesn't mean a drunk-driving fatality is an overdose on alcohol. reply Retric 13 hours agorootparentWhat defines an overdose is the impact it has on your body not the specific quantity consumed. Thus you can OD on insulin even if the amount is needed at other times. If they had taken that much at home it may not have resulted in an overdose, but in this situation it did. reply evanelias 12 hours agorootparentWell, by that logic, if someone has a panic attack from consuming cannabis, you'd view them as \"overdosing\" on weed. I suppose you're free to say that, and everyone else is free to ignore this as ridiculous nonsense which is completely out-of-step with the widely-used definition of \"overdose\". You're completely ignoring the context of the event. Have you ever been to a rave? Some promoters are good and understand the importance of providing free/inexpensive water, proper ventilation, and security who are able to spot attendees requiring medical intervention. Other promoters are shady AF and do none of those things. In the latter case, if someone consumes MDMA and also dances for hours without hydration and the venue is way too hot / oversold, yes that person is at risk of hyperthermia. That can also happen even to sober people, I've literally seen it. reply Retric 12 hours agorootparentIt’s a definition used by doctors not just me. Try asking your GP about what it an insulin OD entails and they will refer to the results not the specific dose. Though obviously there’s a dose large enough it’s never appropriate. reply nicoburns 11 hours agorootparentprev> Some promoters are good and understand the importance of providing free/inexpensive water And indeed some countries legally require it. reply sophacles 17 hours agorootparentprevSo traffic deaths, heart attacks (and general cardio problems), and brain damage. Looks like a melange of cannabis, mdma and amphetamines is almost as dangerous as the standard american diet. reply Retric 17 hours agorootparentIgnore the rates and everything seems equally dangerous. People OD and die from drinking water, that doesn’t make it as dangerous as heroin. reply evilsnoopi3 18 hours agorootparentprevLiterally only one of the drugs you mentioned can kill you due to withdrawal. Hint: it's not LSD or tobacco. Alcohol is an _extremely_ harmful drug. reply Retric 18 hours agorootparentHarm is based on all risks not just the metrics that make what you care about look better. 60% of American adults drink regularly so extreme edge cases happen, but they aren’t anywhere close to the risks from car accidents etc. Risk of death is significant, but we’re talking under 0.1% per year. reply margalabargala 18 hours agorootparentprevSee, this is where LSD gets interesting. No, LSD won't kill your body. But what are you? You aren't just your body, you are your mind and mental state and the consistency of that. Do enough LSD, especially consistent high doses, and the \"you\" you know will die and be permanently replaced by something else. I of course agree that LSD won't put you in a coffin. But there are more axes upon which a drug's danger should be evaluated than \"what percentage of users are put in coffins?\" reply Atotalnoob 17 hours agorootparentI really hate the “LSD is safe” mantra people have. I had a college roommate who did a shit ton of LSD one night and I woke up to the fire alarm having been pulled and him punching holes in the wall. The police (fire department called the police) dragged him off and the hospital had to give him a bunch of antidepressants to short circuit the LSD. He was literally not the same person ever again. That is terrifying, even more so than chemical dependency, to me. reply margalabargala 14 hours agorootparentIn defense of LSD, your roommate did \"a shit ton of LSD\". Keeping dosage under ~300ug radically decreases the likelihood of any sort of permanent personality shift like that. That said things can still happen on a bad trip. I posted my own personal experience downthread. reply BeFlatXIII 16 hours agorootparentprevLiterally not the same as in personality change or as in unable to stick to reality after? reply Atotalnoob 16 hours agorootparentHuge personality shift reply throwaway22032 16 hours agorootparentprevI don't understand how anyone who has ever taken LSD can say that it's safe. Maybe at low doses. Once you can't seperate fantasy from reality you're just one bad trip from falling out of a window you thought was a door. edit: The responses here just seem like denialism from people who got lucky to be honest. You can be aware that you are tripping and still imagine that something is something else. Roads looked like footpaths to me, windows looked like doors, knives looked like forks, and so on and so forth. I was aware that it wasn't real but couldn't be sure of the safety of existing in the world. I basically had to sit in a corner for 10 hours, hope that the corner was actually the one I thought it was and that I wasn't going to fall down a staircase instead. It was great fun but felt like russian roulette. reply jandrese 16 hours agorootparentOne rumor I've seen is that dealers have been replacing LSD with NBOM which has a significantly higher chance of causing a psychotic break and is harder to safely dose. This gets back to the big problem with illegal drugs isn't the drugs, it the illegal. reply quickthrowman 15 hours agorootparentEhrlich’s reagent can be used to test for the presence of an income, NBOMe, 25I, and related compounds will not show up as an indole. reply Nursie 16 hours agorootparentprevHave you ever done it? Because what you’re describing doesn’t resemble LSD at all. You’re describing deleriants like datura if anything. reply Dylan16807 17 hours agorootparentprevAnd if I try to change your mind about how much it matters to get a new perspective and new evaluation on some things, I might murder you. A conundrum. reply margalabargala 14 hours agorootparentPersonal anecdote: I used to do a decent amount of LSD (~150ug every 4-5 months for ~3 years), until one time I had an extremely bad trip. Ever since that bad trip, I've found that every time I get high (from LSD, mushrooms, or even marijuana) I start having extreme anxiety to the point where I can't enjoy it anymore and am just waiting for it to end. The bad trip was now 8 years ago and to this day I've not had a great experience on any of the above drugs since then. I consider myself lucky that the anxiety only shows up when on drugs, which is pretty easy to avoid by, y'know, not doing drugs. But clearly something permanently shifted inside my brain. The inability to experience something I used to enjoy without a near-panic-attack is a bit more of an effect than a conversation might have. reply Retric 17 hours agorootparentprevMagnitude matters. A full frontal lobotomy counts as personality death even if a conversation doesn’t. reply Dylan16807 17 hours agorootparentDo you think LSD is comparable to a lobotomy? And critically, lobotomy isn't a change from one set of mental states to another. That big chunk of brain doesn't grow back. reply Retric 16 hours agorootparentMemories once missing don’t grow back either. The most extreme cases of LSD use have impacted such basic functions as limiting people’s ability to communicate. Post lobotomy people generally weren’t brain dead, even if diminished. So for extreme cases, yes it’s a solid comparison. reply lelanthran 14 hours agorootparentprev> Alcohol is an _extremely_ harmful drug. Yeah. If alcohol was discovered only today, there wouldn't be a government in the world that would legalise it. It's legal because its grandfathered in, from the time before humans developed society and civilisation. reply pxc 17 hours agorootparentprev> Most people throw up rather than OD on Alcohol. If you take enough of a drug that it makes you throw up, you did overdose. 'Overdose' doesn't mean you died. Fatal overdoses just get talked about more than other overdoses because they're the most harmful kind of overdose. reply Retric 17 hours agorootparentI’m referring to throwing up preventing negative health outcomes rather than death. You can throw up from excessive smoking, but it’s not going to help for inhaled or injected drugs. OD refers to excessive consumption or negative heath outcomes, so yes and sort of. With alcohol people will sometimes throw up before all that much alcohol enters the bloodstream, throwing up may qualify on its own but may prevent things like liver damage but throwing up itself could qualify as a negative health impact. reply nimajneb 17 hours agorootparentThe first sentence is ignoring the affect alcohol has on the liver. reply Retric 16 hours agorootparentLiver damage requires alcohol in the bloodstream at extreme levels or very long term habitual use. So for a collage kid throwing up really does provide meaningful protection for their liver. Though obviously it comes with its own risks of asphyxiation etc. reply vkou 17 hours agorootparentprevIf people regularly 'overdosed' on heroin by throwing up and feeling like shit for a day, with the same kind of survival rate that drinkers 'overdose' at, opioids wouldn't be killing tens of thousands of people a year. Alcohol abuse either kills people in the long term, or combined with automobiles. Very few people have a bit too much to drink some night and actually die from it. You'll generally black out long before you get a lethal dose. With opioids, you can trivially take a lethal dose. reply pxc 16 hours agorootparentYes, heroin overdose is much more often fatal than alcohol overdose. (That doesn't change what the word 'overdose' means. If you want to be more specific than the word is, feel free to use adjectives, e.g., 'fatal overdose', 'deadly overdose', etc.) reply Dylan16807 15 hours agorootparentIn common practice, people use 'overdose' to mean life-threatening overdose. reply arrowsmith 17 hours agorootparentprev> Alcohol [has] minimal short term health risks. What on earth are you talking about? Alcohol can kill you in a single evening if you drink enough of it. How much shorter-term could its health risks be? reply Retric 17 hours agorootparentRisk is severity of harm * probably of harm. Of the ~3 million Americans that tried Alcohol for the first time in 2023 how many exactly died from an overdose that night? That’s what low risk looks like. reply nathan_douglas 14 hours agorootparentAccording to the CDC, it seems about 6 people die per day of alcohol poisoning, or ~2,191 per year, so about .07%. Nearly 3/4 are men, which doesn't particularly surprise me, but what does surprise me is that the average age was 49 years, and only 5% were aged 18-24. I would've expected the numbers to be shifted left far more than that. Also interesting was that the overwhelming majority (71%) had a long history of alcohol use problems. In 58%, death was attributed solely to acute alcohol toxicity, but in 42% there was an underlying disease present as well. Huh. reply Retric 11 hours agorootparent~3 million is number of first time drinkers not the total number of people drinking alcohol in the US. ~180 million people drink alcohol one or more times a year so 2,191 per year is ~0.0012% per year. The risk on the first time they consumed alcohol is probably 1/100th of that due to younger ages etc. reply whatevaa 17 hours agorootparentprevYou can die from water if you drink enough of it. Some people actually did. reply arrowsmith 16 hours agorootparentYes, you're very clever, congratulations. reply rqtwteye 16 hours agorootparentprev\"but IMO it’s more dangerous than both Alcohol and Tobacco even as people claim otherwise.\" I could agree with tobacco but I don't believe LSD is more dangerous than alcohol. Most domestic violence, most fatal traffic accidents are under the influence of alcohol. It destroys families and communities. I don't see how LSD could be that damaging. reply normaler 18 hours agorootparentprevLong term use aside, the amount of life changing (mostly in the bad sense) interactions I had while using alcohol are in a class in its own. Prison statistics reflect that especially in violent crime. reply vundercind 17 hours agorootparentprevMessing you up really bad if you use way too much or use way too frequently is also a property of alcohol, is the thing. And lots of them aren’t any more habit-forming than alcohol (less, in several cases). Like yeah if you take way too much lsd or take a tab a day it might fuck you up, but so will drinking a fifth of whiskey inside an hour, or having two or three cocktails every evening. Or take like double the top-end therapeutic dose of tylenol. I’m not claiming most of them are strictly safe, just that most aren’t obviously in a whole different risk category from a couple of our legal drugs. Shit, I’ve given myself heart palpitations as a teen because nobody stopped me from getting a second triple-shot cappuccino. reply Nursie 16 hours agorootparentprevYour ideas about LSD being more dangerous than alcohol are not backed up by decent evidence. Alcohol use kills both acutely and chronically. Alcohol is addictive. Long term use cause mental health issues. LSD does not have the same health impacts and its lack of addiction potential gives it a significantly lower harm profile. Do yourself a favour and read some material published by domain experts (may I recommend “Drugs Without the Hot Air” by David Nutt). reply Retric 6 hours agorootparentI absolutely agree that on a per user basis Alcohol is more risky especially if you’re considering bystanders due to addiction and ease of access. However, I’m looking at things from a per use basis because I’m not really attracted to drinking. So for me it’s about what risks are associated with drinking a beer vs dropping a tab vs smoking 1 more cigarette. Because people like me can choose to do some, all, or none of those things and it’s worth considering the risks on a per event basis. Very few deaths are associated with LSD, but harm isn’t so binary. You can reasonably safely consume a lot of alcohol without issue, as in 1-3 beers a night for a decade, but daily LSD use at that level is another story. reply Nursie 5 hours agorootparent> However, I’m looking at things from a per use basis because I’m not really attracted to drinking. So for me it’s about what risks are associated with drinking a beer vs dropping a tab vs smoking 1 more cigarette. This is not a useful metric at all, and is incredibly naive. Each of those activities carries with it the relative risks of addiction and long term harms. Your metric can lead to absurd conclusion that cigarettes are perfectly fine because nobody ever got cancer from having just one. The danger in tobacco is that people don't stop at just one. > You can reasonably safely consume a lot of alcohol without issue, as in 1-3 beers a night for a decade Some people can, but a significant minority of the people that try that will run into trouble and find their habit escalating. Even people like you who seem to think they are above addiction. And those who don't suffer addiction from that pattern will have increased their risk of cancer and various other diseases. > but daily LSD use Is so rare as to effectively constitute a complete fallacy of argument. That's the point, virtually nobody wants to use LSD on a daily basis, even amongst people who like and use LSD. Its risk profile is significantly lower than alcohol or tobacco as a result of that. Seriously, go read up on this stuff, it's fascinating as well as enlightening. reply Retric 5 hours agorootparent> Even people like you who seem to think they are above addiction. Personal experience but Alcohol, Tobacco, opioids etc and never felt the pull. I tried smoking for a little under a year but didn’t really care for it. Used to drink fairly regularly but just kind of got bored with it to the point a found out beer eventually goes bad in your fridge. > Is rare Ok that may be personal bias in terms of use. I remember one guy saying enough LSD could replace sleep and things didn’t go well for him or a lot of other people. So yes, I knew quite a few people that used it daily, but though none were able to function long term like that. Dad and various friends were connected to Timothy Leary and that’s about as much as I am comfortable sharing. reply Nursie 1 hour agorootparent> Personal experience but .. Sure, and that's your personal experience. If we were going by my personal experience we would get a different picture - that tobacco was highly addictive (though easy enough to quit in the end) but there was no issue at all with cocaine. Yet when we look at the harm profile of cocaine, we can see it's comparatively pretty bad and pulls people into spirals of addiction and self-destruction. You can't really say \"LSD is worse than booze\" as a sweeping statement that you consider applies to everyone, and then use as justification your personal lack of issues with alcohol. > I knew quite a few people that used it daily No offence, but this is why the plural of anecdote is not data. Your sample bias here is extreme - people who were idealogically inclined to take acid daily as an almost pseudo-religious sacrament are not a representative sample. reply esaym 19 hours agorootparentprevWe didn't get the briefcase either. But one officer did bring his K9. Which then tried to bite my classmate during the \"come up here and pet it\" session. reply sandworm101 18 hours agorootparentprev>> What I did get is a cop telling me that my parents having a glass of wine with dinner or a cigarette after work was just as bad a them using cocaine That cop came to my school too. On a bicycle. He also told us that 50% of us would die in any car accident over 50mph. It was really hard to not laugh at him given the number of us who participated in the local streetracing scene. In a room full of students in jackets and ties, he stood there spouting lies in his spandex bike shorts. reply delichon 19 hours agoparentprevIn high school around 1976 we got a talk by the swim team coach, telling us the horrors of various drugs, going into his wide personal experience with them and why we shouldn't follow his example. He told us that if we ever had any questions he was there for us, and gave us all his personal phone number. So me and some buddies got a hold of some peyote and were more than a little afraid of it. We probably would have tossed it but we had a better idea. We called Coach and asked his advice. He gave us detailed preparation instructions. We followed them and got a bit sick, but not altered. Maybe it wasn't even peyote. I'm not sure that this approach was just more nuanced than \"just say no\" or closer to \"just do it\". reply cess11 19 hours agorootparentDid the proposed preparation entail heating? Might have described a process that destroyed most of the mescaline to spare you the experience, which can be quite frightening and/or debilitating. I've never come across someone that does more preparation than drying, so to me it seems suspicious to suggest anything else. Mescaline is water soluble so one can make infusions, but it takes a fair bit of plant material to get dosed so there isn't much practical use in doing so. reply delichon 18 hours agorootparentNo heating. I was just along for the ride, but I think it was something to do with removing hairs, and to make it less bitter. reply cess11 17 hours agorootparentReducing bitterness likely reduces potency as well. Mescaline itself is quite bitter and whatever solvent is used to flush out other less tasty substances will likely solve mescaline as well. Drying slightly improves the taste but not by much. reply 13of40 19 hours agoparentprevI never attended a DARE class, but in the early 90s the t-shirts were everywhere so my memory of it was people wearing the shirts \"ironically\". reply marcodiego 19 hours agorootparentIn Brazil, in the 90's there was a campaign called \"Drogas nem morto\" (drugs not even dead). I heard about a guy who used a t-shirt of the campaign to sell drugs. reply Izkata 17 hours agorootparentprev> so my memory of it was people wearing the shirts \"ironically\". Drugs Are Really Enjoyable/Excellent reply buildsjets 17 hours agorootparentprevStill have one, vintage '91 to '93 or so. I wear it to Phish shows. reply gwbas1c 19 hours agorootparentprevDARE to resist drugs. I don't do dares. (joke) reply InitialLastName 19 hours agoparentprevWe had the county sheriff come to an assembly, show us pictures of a local \"drug den\" they had busted, bring out a body bag and tell us \"it breaks my heart when I pick up a body bag and the weight shifts to one end, because that means there's a kid in it\", and conclude with \"If you ever touch drugs you'll end up living here until I drag you out in a bag\". reply YesBox 18 hours agoparentprevI had the briefcase experience (except it was a portable glass display case with every drug under the sun). Looked like a bunch of candy to me as a kid. I think they even had a fake version placed next to the real thing? One thing they did that has stuck with me though was this plastic bag contraption with some material inside to represent the human lungs. There was a hole at the top which the DARE officer placed a lit cigarette, then began (somehow) pumping air from the hole down into the lung material. I remember watching the material quickly go from white to brown and tarry. By the time the cigarette was out, she looked us all in the eyes and said \"this is what smoking does to your lungs\" reply fatnoah 16 hours agoparentprevI've probably said this in other posts, but the best \"don't do drugs\" education my child got was living in a city. It's one thing to hear about drugs, it's another to watch your dad call 911 for a man collapsed with a needle still in his arm in a subway headhouse doorway. reply bko 18 hours agoparentprevDo you feel the same way about sex ed? In other words, is exposing children to sexual education actually encouraging them to have premarital or unsafe sex? reply jandrese 16 hours agorootparentThe problem with DARE is that the message was \"all drugs will kill you and your family if you so much as look in their direction, call the cops anytime you see anyone who looks even a bit drugged out\". Then some kids try some pot and discover that DARE was full of shit and they disregard the entire message or even rebel against it. Education programs are counterproductive when they are loaded with misinformation. reply blessedwhiskers 17 hours agorootparentprevI'd argue DARE is more akin to abstinence only sex ed than an actual sex ed curriculum. Or at least my DARE experience was much closer to a Mr. Mackey \"Drugs are bad, m'kay\" than a measured instruction of various drugs and the kinds of harms they posed. reply xboxnolifes 14 hours agorootparentprevDARE is the equivalent of telling kids that if they have sex, they will get an STD and ruin their life forever with a painful, annoying, lifelong illness. Interpret outcomes from that as you will. reply rabbits_2002 17 hours agoparentprevThe sole take away I got from DARE when I was a kid is “LSD sounds really cool, gotta try that someday” reply BeFlatXIII 16 hours agoparentprevDrugs Are Really Excellent reply User23 19 hours agoparentprevIt was nice of them to teach kids how to identify real marijuana. reply nonameiguess 17 hours agoparentprevThis seems to be the overwhelming majority sentiment of the Internet or at least of what gets upvoted, but I think DARE at least worked on me. I wanna say it was probably 2nd grade, which would have been 1987 or 1988, and I don't remember the briefcase, but I do remember a plastic lung. It filled up and blackened with tar after only a few puffs from a cigarette and I remember being so viscerally disgusted and disturbed by that, I resolved to never smoke and I never did, in spite of a lot of peer pressure. I was largely friends with goths and art kids and nearly all of my friends smoked, some as early as 4th grade, but I never did. reply Log_out_ 18 hours agoparentprevThe rootcause is the hyper competitiveness od us society. There are only winners and loosees, and loosers do drugs.. So every economic downturn people do what they were told, if success does not happen and self destruct to keep life bearable. reply aeonik 19 hours agoprevIt's pretty funny in a sad way to me that this article talks about how little we know about human behavior and struggling to find answers about why the kids had a backfire effect. I remember these programs, and I remember how they made me and my friends feel. But nobody asked us, lol. I'd even try to tell people I thought, but instead of listening to us, we were admonished. The problem is that these programs are full of bull shit, bad information, bad science, and lies designed to scare. Kids pick up on this quickly, and you now have lost all trust with this cohort, permanently. It's the exact same reason abstinence only education fails. Humans want to feel good, that's okay, we also don't want to die or ruin our lives (assuming we aren't living in a hell). Basic facts here. reply pjc50 18 hours agoparentIndeed. People want \"reproducible\" campaigns, but that necessitates being entirely one-way. As soon as you start having a real connection with other humans it's non-reproducible. reply lrivers 14 hours agoparentprevOne way to ruin kids desire to drink is to offer them tastes of what you’re drinking from a relatively young age. Whatcha drinking daddy? A beer! Want to try it? YUCK!!!! This removes the rebellion portion from drinking to a degree. reply xboxnolifes 14 hours agorootparentUnless they end up liking it, which I know some that applies to. Though, they did end up fine. reply nobleach 20 hours agoprevShow them a Quentin Tarantino film? ...or any film for that matter where the main characters seem so \"cool, unfazed and untouchable\". To a young (mostly male) mind, that's the ultimate goal; to be in charge. For most beginning smokers, it's not about the inhalation of tobacco, but the subversive \"screw the rules, those don't apply to me\" façade. This might sound obvious but, if we want 7th graders not to glorify smoking, we quit glorifying it in our media. Make the stupid guy in the movie the one that's dumb enough to smoke. Make everyone laugh at him every time he tries to look cool. reply mrob 20 hours agoparentThis has been tried before. In Waterworld, the bad guys are literally called the \"Smokers\" and smoking is a big part of their culture. It's so over the top it seems more like a parody of anti-smoking campaigns. I doubt it ever convinced anybody not to smoke, and I think any similar attempt is doomed to fail in the same way. Kids are good at detecting when you're trying to manipulate them, and it doesn't matter if you have good intentions. reply emchammer 19 hours agorootparentI thought it referred to the smoke from their engines? They were the only ones in that world who used internal combustion engines? reply rdtsc 18 hours agorootparentIt's kind of both. For some strange reason I watched that movie again recently. I guess they were supposed to be the distilled version of whatever was considered bad in society in the 90s: hyper-masculinity, smoking, oil tankers, guns, religion (they went on \"crusades\", the leader was \"The Deacon\") etc. reply ido 15 hours agorootparentAren't all these things ± still what's considered bad in 2024? reply vundercind 19 hours agorootparentprevYeah I think I’ve seen that movie three or four times and I never made the connection between the name and smoking cigarettes. Figured it was all the oil they had, and engines they could run with it. reply jandrese 16 hours agorootparentThe only thing I remember is they made some old dude row around the oil tank on a tiny boat because they had apparently forgotten how fuel gauges and dip sticks work. Thinking back on it, were they powering their pirate jetskis with heavy crude? Was there a distillation column set up on the oil tanker? I probably shouldn't think too hard about Wet Mad Max because I bet a lot of that movie doesn't really make sense on closer inspection. reply vundercind 14 hours agorootparentYeah the lack of a refinery does seem like a problem. Maybe they were making something close-enough to diesel out of it with some crude process? No way they were making gasoline, nor most other petroleum fuels. reply toxik 19 hours agorootparentprevKids want to do what they see, if they see people smoke, they want to smoke. If mommy and daddy drink coffee in the morning, it doesn't matter that it tastes like ass, a literal 3-year-old will just by sheer force of will drink that bitter nectar just to feel like they belong. reply pwg 18 hours agorootparent> If mommy and daddy drink coffee in the morning, it doesn't matter that it tastes like ass, a literal 3-year-old will just by sheer force of will drink that bitter nectar just to feel like they belong Then I must be quite different from that \"literal 3 year old\". Mom and Dad both drank coffee, and around the 3-year point I did want to try it, for the reason you say, because it was something \"they\" were doing. And, yes, that sip did taste like ass. So much like ass that to this day 54 years later, I do not and will not drink coffee. reply bryanlarsen 20 hours agorootparentprevMost adults claim that they're not influenced by advertising, yet studies show they are. I'm quite confident the same effect would be found in teens. reply throwaway22032 20 hours agorootparentNegative advertising and positive advertising are quite different. I'm definitely affected by both, but generally if I feel I'm being nannied my instinct is to go the other way. People don't need to be \"convinced\" to do things that are good for them - if they're not doing it, they likely just have reasons that you don't understand. reply jareklupinski 19 hours agoparentprev> Show them a Quentin Tarantino film? Mia's OD scene in Pulp Fiction definitely scared me away from drugs even though I wasn't supposed to be old enough to see it >_> reply 2OEH8eoCRo0 17 hours agorootparentSame. That scene stuck with me. Makes me wonder if over-protecting children from negative feelings backfires. Pulp Fiction was a risk-free and simple way to make vivid the risks of heroin. reply ics 9 hours agorootparentI would put Pulp Fiction near the top of the list for \"films with surprisingly wholesome moral examples\". The overdose, Butch coming back to help Marcellus and their subsequent parting ways, Jules' acknowledgement that he's \"buying\" not \"giving\"... to top it off, nobody is trying to moralize or patronize the viewer. Jules was getting close enough to agitate Vince but that just strengthens the film; it's okay to feel Vince's side more and not feel what someone else does despite witnessing the same thing. I was probably late teens when I first saw it but pretty sure if I saw it as a kid that I'd have gotten the good with the bad. reply jareklupinski 17 hours agorootparentprev> over-protecting children from negative feelings backfires it definitely hit differently to \"see it myself\", versus having it introduced by a parent/guardian, who probably would shy away from making the lesson so graphic reply zepolen 16 hours agorootparentprevTrainspotting should be mandatory watching in schools. reply gwbas1c 19 hours agoparentprevThe ultimate thing that convinced me not to smoke was watching my uncle die of lung cancer, and being unable to quit the thing that was killing him. I'm somewhat relieved that tobacco references are considered \"adult\" now in entertainment. The amount of smoking I saw in movies and TV (and around my uncle) normalized it for me, even though I knew how unhealthy it was. reply itronitron 18 hours agorootparentFor my high school cohort the video interview of the person still smoking after having had a laryngectomy really drove home the powerlessness of addiction. reply gwbas1c 11 hours agorootparentThe problem is seeing all that... And then seeing family members and friends smoke on the weekend... It puts it in this weird \"safe to try, but not safe to keep doing\" slot. Nicotine addiction is very subtle, for many it's hard to recognize until you're hooked. reply bombcar 17 hours agorootparentprevWe had a guy who had one come to our school in person; giving his talk whilst smoking from his stoma was ... pretty convincing, to be honest. reply mdale 10 hours agorootparentprevI remember that ad. I think it did it for me as well. reply nvahalik 19 hours agoparentprevRequiem for a dream still haunts me. reply throwaway924385 19 hours agorootparentAs a non-smoker, I’ve never felt like I needed a cigarette more than when the credits rolled on Requiem. Certainly scared me off of harder drugs, though. reply ta2112 18 hours agorootparentprevJust when I finally almost forgot about it, this comment rips it back into my poor brain. reply nvahalik 17 hours agorootparentAnd now, if it hasn't already been playing in your head, Clint Mansell's \"Summer Overture\" will now haunt you. ;) reply Der_Einzige 18 hours agorootparentprevYup, that’s the real anti-drug movie. Stuff like this works. It’s not pleasant though. reply jjgreen 18 hours agorootparentprevHis worst film I thought, almost an Aronofsky pastiche. reply pjc50 18 hours agoparentprevIt is remarkable how dramatically smoking was intentionally removed from even period movies, and in some cases edited out of old content. Perhaps silence works better than negative mention? There's a real problem with anti-war or anti-violence media accidentally making it look cool, similarly with biographies of \"troubled star\" types detailing their drug uses. reply parpfish 20 hours agoparentprevMedia is part of it, but seeing the real life adults in your life smoke influences you a lot. It’s not necessarily about emulating people you admire, it’s about adopting a signifier of adulthood. reply matthewaveryusa 20 hours agoparentprevExactly, these are the kinds of ads we need: https://www.advocate.com/news/daily-news/2010/02/25/anti-smo... reply PhilipRoman 20 hours agorootparentI was thinking more like this https://www.theonion.com/new-anti-smoking-ads-warn-teens-its... But that one is not bad either :) reply npteljes 18 hours agorootparentGot a good laugh out of me, thanks for sharing! reply treflop 18 hours agoparentprevFor me, the older kids doing cool things were smoking. A bunch of actors smoking never did anything for me. reply RhysU 18 hours agoparentprevSmoking by most teenagers is easy to solve: Make all cigarettes hot pink with garish other colors in ugly, ugly patterns. Teenagers don't like looking conspicuously ridiculous. reply ics 17 hours agorootparentFinally get to use this anecdote... Friend: Do you have anything that doesn't taste like middle school girl lip gloss? Gas station attendant: *chuckles* No, sorry. Friend: Fine, cherry-berry then. I don't think it works. reply astura 18 hours agorootparentprevVaping already looks conspicuously ridiculous. Nobody ever looked cool vaping. reply buildsjets 17 hours agorootparent\"Douche Flute.\" reply toddmorey 19 hours agoparentprevTrainspotting. Those scenes are burned in my memory forever. reply sandworm101 20 hours agoparentprevRather than uncool, just kill them. In british historical drama there is an old standard that any character that coughs will soon die. If the butler even sniffles, they are doomed. So make smoking the new coughing. The kids will then associated smoking with death, living in fear every time their favorite characters go anywhere near cigarettes. reply Karellen 16 hours agorootparent> In british historical drama there is an old standard that any character that coughs will soon die. That reminds me of the classic short-film subversion of this trope: The Man Who Has a Cough and it's Just a Cough and He's Fine https://www.youtube.com/watch?v=HtQNULEudss&t=67s reply the_af 18 hours agoparentprev> This might sound obvious but, if we want 7th graders not to glorify smoking, we quit glorifying it in our media There was quite obviously a concerted effort to remove smoking from movies and shows. It's been going on for more than a decade. It's quite abrupt, go back far enough and everyone smokes, especially the cool guys and gals. Then fast forward and almost nobody smokes, not even the villains. Ads for smoking are gone from my country's TV and cigarette packs by law must display horrifying photos of emaciated cancer patients. reply Workaccount2 19 hours agoprevThe fundamental problem is that drugs are fun and the bad effects are offset by years or even decades. Also the fact that vast majority of people who do drugs never realize the bad effects from it. They do it in moderation, use them responsibly, and stop without much thought. It's the hockey stick at the edge of the chart where all the destructive narrative comes from. reply kenjackson 19 hours agoparentAnd they are usually addictive. Almost every person I know who smokes says they derive little pleasure they once did, but are addicted to nicotine and can’t stop. And as a non-smoker, it doesn’t look fun, but I trust it was at one point in their life - I guess. reply Workaccount2 19 hours agorootparentRight, I was a smoker for many years, the but the actual conversion rate of \"drug users\" to \"drug addicts\" is surprisingly low. The overwhelming majority of people who smoke their first cigarette will not die a smoking related death. And cigarettes are probably the most extreme example. How many people who try cocaine end up being the poster case for DARE? It's virtually no one. The risk analysis for having fun with drugs is totally lopsided. Society is fighting to keep 100% from using drugs in order to protect the 5% who will have their lives ruined. reply arp242 18 hours agorootparentI wish you could just buy single cigarettes rather than always getting a pack. I feel that's a big part of the problem right there. It would be like only being able to buy wine in boxes of 20, or beers in crates of 30. Once you have it, you're much more likely to use it. I just want a beer or a bottle of wine this Friday evening, and maybe after that I want a cigarette. And that's it. reply kelipso 16 hours agorootparentAbsolutely. You could taper off really easily with loosies while with 20 packs, you end up smoking the whole thing and get addicted again. reply circus1540 18 hours agorootparentprevhttps://en.wikipedia.org/wiki/Loosie yeah I'm not sure how banning sale of loosies is protecting the kids. At 30, I get carded almost always when I buy a pack and almost never when I buy alcohol. If someone is selling to kids, they wouldn't care about this law anyway. reply kenjackson 18 hours agorootparentprevI think our samples must be different. Admittedly I have small samples of cocaine users, but the behaviors for them are consistent with the mainstream literature. Maybe just a bad sample, but it’s good enough for me to avoid it. reply devoutsalsa 19 hours agorootparentprevSmoking Is Awesome, by Kurzgesagt - In A Nutshell, touched on how smokers eventually need cigarettes just to feel normal. https://youtu.be/_rBPwu2uS-w?si=hR0J-eqBBUOjVtvd reply alt227 19 hours agorootparent> touched on how smokers eventually need cigarettes just to feel normal. This is the same for every addictive substance, no? reply asdfgrasdf123 18 hours agorootparentI was amphetamine addict, I used daily for a year, very excessively in later stage. When I stopped I felt normal, it was not fun but I did not need amphetamine to get through the day. There is no physical urge to use more, you do it because you like it. It is very different with opiates I heard, but you said every. The word addiction pushed me towards drugs because I thought if I can stop any time it is not addiction and it is fine. reply amadeuspagel 20 hours agoprevThe main reason that it's hard to change people is that they resist manipulation. This is hard to see if you only consider well-meaning attempts to change people, which you might not consider manipulation because that word has a negative connotation. reply cm2012 17 hours agoparentNo. My background: I've done marketing for 15 years, have read every book on persuasion and have been called an expert. The reason for this is that the most important element of persuasion is Ethos, or your perceived character. This is much more important than Logos (logical appeal) or Pathos (emotional appeal). Suits have no credibility to teenagers so they cannot be persuasive. Cool kids have a ton of credibility and thus are very persuasive. reply jawns 18 hours agoparentprevAnd yet ... look at how much manipulation happens at the political level. I find myself constantly wondering how people can be so gullible as to buy the drivel they're being fed. reply Clent 17 hours agorootparentThis is a different form a manipulation. No one is trying to change these people's behaviors. They are feeding them what they already want; reinforcing negative behaviors. That has always been very easy. The relationship to today's political theatre is related to this discussion by comparing it to how easy it was to get everyone to smoke in the 1950's reply hn_throwaway_99 17 hours agoparentprev> The main reason that it's hard to change people is that they resist manipulation. This seems flat out wrong to me given how extremely difficult it is for most people to change themselves. I guess on some level you could argue that people are \"manipulating\" themselves, but that seems a bit silly to me given how you are using the word manipulation. It's hard to change people because humans are actually great at multiple forms of homeostasis. All research I've read has said that people's personalities are basically solidified from early childhood (i.e. age 5-6). reply amadeuspagel 15 hours agorootparentNot at all. The fact that it's hard to change yourself is itself part of the resistance against manipulation. It's a second layer of defense. Even if someone is able to manipulate your explicit beliefs, they still aren't able to manipulate your behaviour. reply hn_throwaway_99 12 hours agorootparentThat's an unfalsifiable assertion/tautology that is basically a \"not even wrong\" statement. That is, you're basically asserting \"If you try to change your behavior, but can't, it's because your explicit beliefs have been manipulated into thinking something that your body won't go along with.\" Well, how do you know? Seems like you're saying \"Because your body would go along with it if your brain wasn't being manipulated.\" Just look at the millions of people who have tried for decades to lose weight. You can call it \"manipulation\" or whatever, but I think a poor quality of life, shortened life span, etc. are plenty real, valid, \"non-manipulative\" reasons on their own for obese people to want to lose weight even if you ignore the societal implications. And does taking semaglutide somehow magically make it not \"manipulation\" anymore by your definition? reply amadeuspagel 1 minute agorootparentI'm not sure about semaglutide, but drugs in general are a good example of my point, because people use them to change/manipulate others and themselves. For example LSD has been used by cults, by the CIA, etc.. madaxe_again 18 hours agoparentprevAbsolutely this. You cannot get a person to change an opinion, a belief, a tendency, by tackling it head on. It’s almost like we have an autonomous immune response to information which challenges our preconceptions or preferences. I have found, repeatedly, that the way to get people to change is to make them think it was their idea. So you manipulate them, but in such a fashion that they do not realise they are being manipulated. For instance, I got my kid sister to quit smoking by taking up smoking and encouraging her to smoke more and more until she smelt like an ashtray and had a wallet as light as a feather - and then she decided that this was stupid, and quit. I then claimed to take inspiration from her and followed suit. Of course, this doesn’t work en masse without essentially going into conspiratorial psyops on the population, but on an individual or small group basis, people can be given a set of precepts and ideas that lead them to an almost inevitable conclusion - and because it was their idea, it’s precious. You can then really cement it by trying to get them to drop the idea, and convince them that the thing that you wanted them to think is wrong. Contrary creatures, we. reply rdtsc 18 hours agoprev> You can have a PhD and good intentions. You can have money and buy-in. You can do a bunch of reasonable things to prevent a problem that everyone agrees is bad. Imagine what they could do if they had a PhD and bad intentions... But let's say, first they do have good intentions. However, even then, if these people devoted their lives to their cause, they promoted programs, wrote countless powerpoint slides, even books. And now some other research comes out and says \"well it looks like that other stuff was actually hurting kids\". How many of the PhDs with good intentions, will acknowledge that and effectively throw away their life's work? > Some famous CEOs were mean to their employees and got good results, so I should do that too, which is convenient because I am a jerk. That's exactly what one of the leadership team did in a company I worked at. They tried to emulate Steve Jobs. And out of brilliance, dedication to perfection, design, hiring the best people, all possible qualities they could have tried to emulated, they emulated being an asshole, hoping the company would become the next Apple at some point. reply flobosg 19 hours agoprevAnother example of interventions with reversed outcomes: “baby simulator” programs increase the rate of teen pregnancy – https://www.statnews.com/2016/08/25/infant-simulators-teen-p... reply washadjeffmad 19 hours agoparentIn high school, I had to \"raise\" a bag of flour for a week with a girl that I was dating as part of an assignment. One night on the phone, I asked if she had any fantasies, and she immediately told me about how often she thought about being pregnant and doing everyday things- going to school, cleaning, going shopping, going to the doctor, etc. It wasn't the type of fantasy I was expecting at the time, but I did eventually figure out the message a year or two after we broke up. reply actionfromafar 19 hours agorootparentUs guys can be a bit slow, can't we. :-/ reply 1992spacemovie 19 hours agorootparentprev> One night on the phone, I asked if she had any fantasies, and she immediately told me about how often she thought about being pregnant and doing everyday things- going to school, cleaning, going shopping, going to the doctor, etc. Unfathomably based. reply washadjeffmad 17 hours agorootparentShe and the other Catholic girls were raised to believe the only thing in the world worse than being an unwed mother was to be a pregnant teenager, so they grew up with some complicated fixations. By their logic, they needed to experts in all the ways to be safe to prevent it from happening, but pre-internet, their info was a little dodgy. One summer, they had a \"No boys in the hot tub, at all\" rule. It took some convincing to get them to explain why, but someone had heard that sperm. could swim. in water. ... And they'd just live in there, like goldfish? Did they need to be fed? Did they have maps? That one was probably the most precious. They never really questioned how the aggressive little Olympians might end up there in the first place, and these girls could all drive. reply 1992spacemovie 7 hours agorootparentYou made me chuckle at the hot tub rule - crazy what people will believe. reply andy99 21 hours agoprevI remember hearing rumors that tobacco companies funded anti-smoking campaigns because they realized it just made smoking look that much cooler. It's certainly easy to imagine a dual purpose ad that will appear positive and affirming to adults but super lame to kids and make them want to do the opposite. reply piker 20 hours agoparentI believe big tobacco was required to fund those as part of a mass tort settlement. reply bagels 20 hours agoparentprevThis is the source of those ads: https://en.m.wikipedia.org/wiki/Tobacco_Master_Settlement_Ag... reply arrowsmith 17 hours agoparentprevAs parodied by South Park: https://www.youtube.com/watch?v=Hqglh7WoBz0 reply furyofantares 19 hours agoprevLots of great points, although a sexual harassment quiz for adults seems fairly different than a multiweek program for kids. And the article seems to argue that interventions are unlikely to change people while also accepting that the drug study did change the kids in the wrong way. To me that doesn't indicate that it's necessarily wrong to try to intervene with short-ish programs in schools, it indicates the content of those programs might be really bad. Maybe that program didn't actually harm kids, beyond wasting their time, and was a statistical outlier. I don't have much opinion that. But I can understand why interventions can backfire if they're inauthentic. When I figure out you've been lying to me, I want to throw everything you said out the window. If you also made me make some videotaped promise saying something YOU want me to believe then you can bet I'm gonna feel betrayed too. reply dwhitney 16 hours agoprevContrarian view - the programs are working. Rates of smoking among teens over the past three decades: Ever used nicotine: 70.1% in 1991 to 17.8% in 2021; Occasional use of nicotine: 27.5% in 1991 to 3.8% in 2021; Frequent use of nicotine: 12.7% to 0.7%; Daily use of nicotine: 9.8% in 1991 to 0.6% in 2021 https://www.fau.edu/newsdesk/articles/teen-cigarette-smoking... I suspect the same can be said for workplace sexual harassment and outlooks on diversity. I agree that the programs are dumb and the trainings are dumb, but they have an effect. reply foobarchu 16 hours agoparentI think that can be chalked up to other changes like the wild increases in cigarette prices, the near elimination of advertising, and cigarettes becoming culturally uncool (orthogonal to DAREs attempts). Your link shows that during the heyday of these campaigns usage rates were still rising. They didn't really drop until after the approach was dialed back, and they dropped most appreciably after DARE gave up on anti-drug messages entirely. The message of the article wasn't that you can't discourage drug use at all, it's that the means may be counterproductive. The means used in DARE were counterproductive. reply tech_ken 15 hours agoparentprevIt seems more plausible (to me) that this is the result of the cultural 'quarantine' of smoking and not the efficacy of the \"don't smoke\" programs in schools per se. The entertainment industry in the US aggressively committed to not showing cigarettes in a cool or positive light, and to me this seems way more likely to have driven these reductions in youth smoking. In the 80s pretty much any person a 13 year old would call 'cool' could be seen smoking somewhere. Now media uses cigarettes to either indicate that a movie is happening in the past or that a character is self-destructive. With the cultural cachet gone, or greatly reduced, I think what you're seeing is mostly 'background' smoking rates: kids smoking specifically for the effects of nicotine, or because it's a taboo-breaking behavior. Not because their parents and every celebrity smoke. This dynamic also explains the rise of e-cigs, IMO. Kids love streamers and influencers, and streamers and influencers vape nic. Now kids vape nic. reply defen 16 hours agoparentprevThat's cigarette usage, not nicotine usage. Why would a teenager smoke when they can vape or use Zyns? reply knowaveragejoe 16 hours agoparentprevThis isn't contrarian, this is just correct and well understood. What's contrarian is the article suggesting that any attempts to change people's behavior, no matter how well intentioned thought-out, actually result in the opposite of the desired outcome. reply syang737 16 hours agorootparentWhile the statement \"kids are smoking less cigarettes\" is correct, not sure if that's proof that the programs are working. Naturally, teens are filling the void of cigarettes with vapes and e-cigs, seeing as 26% of high school students (https://www.singlecare.com/blog/news/vaping-statistics/) have vaped. Whether the programs are working or not is more of a debate, but don't want this cherry picked stat to go untouched. reply kazinator 19 hours agoprevIf you see footnote 1: \"In a later paper where the researchers caught up with these same students again, the difference between the Social and Emotional programs had mysteriously disappeared.\" It almost seems as if the researchers triggered something resembling the Streisand effect. The Social and Emotional programs were obviously intended to discourage the kids from smoking, but participation in those programs was only serving to keep the kids' attention on smoking, and making some of them curious. The kids might also have gotten the message that trying smoking is okay, as long as it's not related to peer pressure or feelings, but one's inherent curiosity. Programs designed to produce resilience against peer pressure will not block smoking that is not caused by peer pressure. reply ordu 14 hours agoparent*> Programs designed to produce resilience against peer pressure... ... also could have a unintended effect. A kid could start to notice a peer pressure and get more clear idea how to follow trends or even predict them and to run ahead of them. Teenagers are really concerned about what their peers think about them. If you tell them all about peer pressure, they will listen attentively, and develop a strategy how to grow in the eyes of their peers. Very bright could get the idea that you can be the source of peer pressure. Lets start smoking and lets make all others to start smoking too. I would say, that if you talk with kids about smoking, some of kids get the idea that if they were seen smoking, then they will look more knowledgeable about smoking than you. Or maybe they will see rebellious and independent from boring adults. Especially rebellious if adults talk about dangers of smoking all the time. People's minds are difficult complex machines, they could react at your stimulus in unexpected ways, and the more ways you can predict the more mind-blowing unpredictable become. reply langsoul-com 20 hours agoprevMost interesting part is how we view changing ourselves VS others so critically different. We know how hard it is to change ourselves, yet view others as fuzzy and that it's easy, just enact a few advertisements and a program or two, done. But at the same time, it's basically impossible to know what will work until it hits the field. Better to just throw a bunch of stuff and then adjust according to the results. reply iamthepieman 19 hours agoprevIf you want someone to stop something you have to offer them a world of opportunities to replace it. Because you don't know exactly what will slot into that gap due to lifestyle, financial, personality and social factors. If you want someone to DO something you just need to show that it COULD fill a gap in some other part of their life. If it's addictive as well then they don't need to experiment for long before it's a solid part of their life. Getting someone to stop plays off loss aversion and sunk cost. reply somethoughts 14 hours agoprevIn all seriousness (although half joking) - these days it seems perhaps the best way to get people not to do something is to make them think that some \"Big\" corporate monopoly - with an army of lobbyists - want to sucker them into buying their mass produced product for huge $$$ profits. The movie \"Thank You for Smoking\" has a great scene between Big Tobacco, Big Alcohol and Big Firearms lobbyist caricatures [1]. [1] Merchants Of Death Thank You For Smoking https://www.youtube.com/watch?v=ss0jLHvMO20 reply kelseyfrog 18 hours agoprev> We perceive ourselves in full 4000K HD with Dolby Atmos Surround Sound, but when we perceive each other, it’s like we’re watching a VHS tape that someone made by using a Motorola Razr phone c. 2007 to record the screen on the back of an airplane seat. Does anyone else feel the opposite? I can much more easily intuit how others feel and synthesize hypotheses on why they behaved the way they did than I can with myself. I have an infinite number of framings and reframings to choose from when it comes to my own behavior that it's difficult to objectively choose. I constantly ask myself how do I know that my own model isn't unconsciously avoiding negative feelings or conversely biased toward being too critical? So much so that I simply give up more than I like to admit. reply ksaun 12 hours agoparentWell, I have a similar difficulty in analyzing myself. So you are not alone. But I also struggle hypothesizing others' reasons for behavior. I can readily empathize with others, but often can't transfer from the emotional response to the underlying reasons/logic. reply dctoedt 19 hours agoprevIn the first day or two of Navy nuclear power school in 1974, all of us junior officers in the new class (93 of us IIRC) were assembled in an auditorium for a don't-do-drugs lecture by a chief petty officer who was a \"fleet sailor\" — i.e., he'd been around the block a few times, so it was thought that young sailors would pay more attention to him — and had been assigned to get trained as the drug counselor. The speaker made one especially-memorable comment: \"I'm told that shooting heroin feels like an all-over-body sexual climax. I don't know about you, but to me that sounds pretty neat.\" (The rest of the lecture was standard stuff about how doing drugs would get us de-nuked and basically end our careers.) reply currency 14 hours agoprev>changing people is so surprisingly hard—no matter how much you focus on the person in front of you, you’ll never appreciate the million tiny influences that made them who they are and that keep them that way. If you really want to make someone different, you might have to change the TV they watch, the music they listen to, the things they learn in school, the friends they hang out with, the role models they look up to, etc. I'm a white guy who grew up in rural Pennsylvania in the 60s and 70s, so you can imagine what my implicit biases are like pretty reliably. What helps me get away from that is access to social media sites like X, reddit, and tumblr, where I follow people who are as different from me as possible. I have updated my influences. I listen. I pay attention to my internal dialogue and reactions and try to challenge myself. >and if you do all that, congratulations, you’ve started what we call a cult. Right. It's one thing to open myself to worlds I'm not aware of. It's something completely different to try to control what someone else experiences. Another way of saying that is—as an atheist, I don't find religions interesting or useful. But I know that I'll never be able to sell atheism to anyone if their religion is a deeply meaningful part of their worldview and identity. Any attempt to do so has to start by destroying that world and that identity, and that kind of violence is anathema to anyone with the smallest bit of empathy. We are made of our influences and this essay covers that well. It's a great essay that covers so much more than smoking (or drugs). Thanks for the link. reply h2odragon 19 hours agoprev\"Drugs are Bad\" except all the ones we advertise on TV and might require school kids to take in order to attend school. reply hi-v-rocknroll 13 hours agoparentThe War on Drugs was used to target Nixon's perceived opponents: hippies and black people. Later, Reagan couldn't relate to poor or minorities, so he (and Nancy) doubled-down on \"tough on crime\" militarization and ineffective PSAs. reply t-3 19 hours agoprevThe reason \"anti-drug training\", \"diversity training\", etc., don't work is that lecturing is not conducive to learning for the vast majority of people. Most people need real life experience to internalize that people that seem very different superficially are just the same humans as they are, that addictive drugs can make you poor and sick and waste a lot of time, that getting a criminal record will fuck you over in every way possible, and so on. Peer pressure works - if all your friends think drugs are really stupid you probably won't ever try them, but you probably will if they all do them. Indoctrination in this fashion works well in isolation but tends to fall apart as people move socially and geographically. reply tracker1 14 hours agoprevI think the absolute best anti-drug program is volunteering for homeless outreach programs. You'll see the level of f'd up that a lot of people will go to and through in order to get and keep taking drugs. It also helps to humanize drug victims and homelessness. reply gumby 18 hours agoprevI remember when the cigarette companies used to hire people to stand on street corners and just give them away (little three-cigarette packs) to encourage people to start or switch brands. Teenagers never have money so this was a great way to get them. I didn't smoke but I used to accept them and then give them to my smoker friends. reply swagasaurus-rex 16 hours agoparentYou’re the one the programs warned us about, giving drugs away for free. reply gumby 14 hours agorootparentAccording to Section 230 of the Communications Decency Act, I am merely an provider and the liability rests with the employee who \"posted\" the cigarettes with me. reply hi-v-rocknroll 13 hours agoprevDamn, this is something Big Tobacco could use today in the global South where they still use cartoon packaging and school donations to increase tobacco use. https://www.cnn.com/2017/08/30/health/chain-smoking-children... https://www.reuters.com/investigates/special-report/pmi-who-... reply fuzzfactor 19 hours agoprevYou could always have a tobacco company sponsor a prime-time animated TV program intended for the whole family to watch before it was kids' bedtime. Where the main characters took regular breaks to smoke their Winstons, using those ancient Zippo lighters that basically rubbed two sticks together to light up. Fred & Barney never looked so relaxed and satisfied any other time. reply pookha 16 hours agoprevOne of the main motivations that kept me from wanting to get involved with drugs was the T.V show COPS. As a young child I got to watch REALLY high people from the bottom rungs of society (all races and all creeds) get flashlight interrogated on T.V and or chased under mobile homes and crawl spaces by Belgian Malinois dogs out for blood. later in life if I got inebriated I'd think back to those faces on COPS and I'd start to feel shame and a return to sobriety. Being exposed to hard living (in the real world and not \"scared straight\" via prisoners) might have an effect that this author is breezing over. reply hi-v-rocknroll 13 hours agoparentWhile COPS might have ancillary effects on some, it was largely voyeristic, triumphal poverty porn and propaganda used to justify the repression and brutalization of poor and minorities through selective Prohibition and over-prosecution. reply porphyra 17 hours agoprevI feel like humans, especially children, are in some respects similar to LLMs or current AI models. With an LLM, if you explicitly tell it not to do something, like keeping a secret hidden, it's notoriously eager to disobey the command and do just that. Similarly, if you tell generative AI like DALL-E to not include something, it will try super hard to include it nonetheless. It's as though once the concept is shown, they can't stop thinking about it. Maybe an \"out of sight, out of mind\" thing would do better than explicitly telling people not to do something? reply hollywood_court 19 hours agoprevI have a love/hate relationship with smoking. I quit smoking 5+ years ago when my child was born. But I always loved a smoke with a beer. But I also quit drinking. As a carpenter and electrician, I smoked many cigarettes during my career and they calmed me and allowed me to focus a bit better. However, I finally realized that few things say \"hey look at me I'm am absolute idiot\" quite like smoking. That's actually what I told myself over and over again to shame myself into quitting. I associated smoking with a few trashy individuals that I knew. I told myself that I didn't want to be like those people so I tricked my mind into quitting. reply parpfish 19 hours agoparentpeople start out wanting to smoke for whatever reason, but then the addiction kicks in and you just keep smoking even after that reason has passed. among adult smokers, i've always wondered what the breakdown was between people that actively like smoking and want it in their lives versus those that are ambivalent but quitting is too much of a hassle? reply bongodongobob 18 hours agorootparentI'm a smoker and everyone I know who smokes hates it but it's just so fucking hard to quit. It takes active energy for months and months not to do it. reply hollywood_court 15 hours agorootparentI suggest using shame to leverage yourself into quitting. It's fallen out of favor in our society, but shame really should be a powerful motivator. I used it to quit smoking tobacco, quit drinking, and quit a 20+ habitual marijuana habit. I use shame to make myself do that extra bit of work in the evenings after my family has gone off to bed. Shame has plenty of good uses if leveraged properly. reply bongodongobob 13 hours agorootparentCurrently hiding in my car smoking at work. I feel like a complete asshole. reply hollywood_court 13 hours agorootparentI knew it was working for me when I didn't want anyone to ever see me smoking. I definitely didn't want my child to see me smoking. But first it was coworkers or in-laws and such. reply bongodongobob 6 hours agorootparentI feel the same, I don't have any kids, but my friends do. Feels like shit. Any other advice or recommended reading? Smoker for 27 years. reply ta_9390 2 hours agoprev> A big trial of mindfulness training in UK schools maybe made kids a bit more depressed, and two attempts to teach psychotherapy principles to students in Australia both failed. I have always belived that the widespread of cheap psychological content aimed for average people is a mental-unhealthy plague and would only make them overconfident in judging their own and others mental health, this is why we see overuse of terms like bi-polar, trauma, narcissist etc.. in social media where teenagers are active. reply chias 18 hours agoprev\"I understand these trainings exist solely for the purposes of ass-covering.\" This is not really true. They exist for shifting liability -- which is similar, but also quite different -- to where it needs to be. They exist so that you don't even have to listen to the \"but that's just normal\", \"but we always do that\", \"i had no idea that wasn't allowed\" excuses when a person gets in trouble. They exist because otherwise when someone does something that everybody including the perpetrator obviously knows was wrong, they can say \"how was I supposed to know that was bad?\" and suddenly you have to start arguing in a context where your opponent can \"win\" the argument by feigning being an idiot, which is a surefire way to have a shit day. Consider this: https://dynomight.net/teaching/ reply poulpy123 19 hours agoprevRequiem for a dream did more against drugs than any campaign reply Projectiboga 19 hours agoprevI predate DARE. In 1980 we got a cool early 1970s film that was suprisingly neutral and only focused on Cannabis. reply ladzoppelin 17 hours agoprevLike others have said its the lying and exaggeration on both sides that really made this a confusing topic to navigate. I wish there was a way to push a \"wait until your brain is developed before anything\" message without being pro drug or confusing. reply sandworm101 20 hours agoprevQuestion: Do people who vape spend more or less money than people who smoke? I have some vapers that work for me and they talk about vape like wine people debating grapes. They seem willing to pay insane amounts of money for their hourly fix. So I wonder whether \"big tobacco\" sees every smokers as potential vaper. Both are nicotine addicts. Perhaps the demonization of smoking will improve sales in the vape business, the alternative \"legal\" way to sell the drugs. reply throwup238 20 hours agoparentVapers spend a lot less. As a pack-a-day smoker I was spending $7-10 a day on cigarettes. Before I quit altogether, I was spending $20-30 a month on nicotine salts, $20 a month on cartridges, and $100 once a year or two for mods and batteries. Since I was working from home I was vaping constantly, so I didn’t reduce nicotine consumption despite the cost reduction. I could buy a brand new mod every month and it’d still be cheaper. I’m sure there are whales that spend on vaping like some stoners spend thousands on Mothership glass, but they pale in comparison to the millions of multipack-a-day smokers that are worth thousands a year in profit. I assume that’s why Juul was pushed so hard by cigarette companies, the economics of Juul pods are much closer to cigarettes than the stuff sold in vape shops. reply hipadev23 19 hours agorootparentSo nicotine salts are synthetic right? Is there a reason people consume them via oral pouches, does a pill simply not work? Or is the addiction tied to both the method of consumption and not only the effect (why i drink coffee and not take caffeine pills i guess) reply throwup238 19 hours agorootparentYes they’re synthetic. Pills take much longer than pouches since they have to go through the digestive system, whereas pouches steadily release nicotine that is absorbed quickly by the gums. Method of consumption is definitely important. I couldn’t fully switch to vapes until I found one that had the same “throat feel” as my American Spirits. Not just the type of nicotine (salts) but the exact brand of vaporizer and coil style. Just like people prefer the taste and ritual of coffee to caffeine pills. reply prophesi 19 hours agoparentprevIf they're buying disposable vapes, then it's likely on par with cigarettes in terms of price. That industry has been working hard on getting disposable vapes that last long enough and contain as much nicotine as possible. In my area it's next to impossible to find any disposables that contain less than 50mg (5%) nicotine, which is downright criminal. If they're going custom then the upfront cost will be a lot more, but e-juice is super cheap. Have to replace the coils every now and then as well unless it's a refillable pod system. You can also choose whether you get freebase vs nicotine salts and how much nicotine the juice contains. I personally believe disposable vapes are dangerous and can get a newcomer as addicted to nicotine as a pack-a-day smoker in a matter of weeks. And for smokers, it won't give you a pathway to taper off your nicotine intake. reply cess11 18 hours agorootparent\"In my area it's next to impossible to find any disposables that contain less than 50mg (5%) nicotine, which is downright criminal.\" What do you mean by this? Why would you buy less than 50 mg at a time? I use snus, and I'd guess it's several mgs per pouch, and I buy something like 200-250 pouches at a time because I'm not a heavy 'snusare' so one roll at roughly 30 bucks lasts more than a month. 50 mg would be, what, like two-three packs of cigarettes? reply prophesi 16 hours agorootparentEven for heavy smokers, vape shops will typically recommend 20mg for making the switch. Nobody needs 50mg+ at that level of concentration. With snus, as you've said, each individual pouch is like 1 - 6mg. I would get a massive head high from hitting a 50mg vape for the first time, and barely feel much from a 6mg Zyn pouch. edit: Basically, no clue how much nicotine you'd technically intake with a puff but it seems absurdly stronger than the slow intake of 6mg in your gums. reply t-3 19 hours agoparentprevThey can spend orders of magnitude less, or about the same depending on how they do it. Mixing your own fluid is stupidly cheap - a weeks spending for a pack-a-day smoker will cover at least a year of vaping like that. If they build their own coils, it's also very cheap - nearly a life's supply of wire can be had for one or two packs. On the other hand, if you buy disposable vapes or cartridges, you can easily spend more than a smoker, if you buy disposable coils and premixed fluid it probably comes about the same or slightly cheaper. If you have a huge collection of vapes and buy all kinds of different attachments and coil heads you are going to be spending a lot of money. reply sandworm101 19 hours agorootparentSo it sounds like the real money is in the hardware rather than the consumable juice? reply t-3 19 hours agorootparentThe juice is heavily price-gouged and not exactly cheap if you buy it pre-mixed, but yes, hardware is expensive and the replacement coils for refillable-but-not-rebuildable systems aren't cheap either. It can be cheaper than cigarettes, but there's a good chance it won't be significantly so if you aren't buying in bulk and are replacing your coils often. Disposables vapes and cartridge systems are so addictive and easy to use anywhere that they are more expensive than cigarettes in my experience. reply Workaccount2 19 hours agoparentprevCigarettes are very expensive and there is no way to really get around it, except esoteric approaches like rolling your own cigs, which doesn't even save that much anyway (especially factoring in time). Vapes on the other hand can range from expensive to cheap as dirt. When I vaped I was spending probably $10/mo to replace a pack a day habit. I made my own flavorless juice and used a $50 tank/mod system. reply makach 20 hours agoprevPut them on fire? reply Joker_vD 20 hours agoparentOr run them over with a forklift! reply cheschire 20 hours agorootparentStaplerfahrer Klaus as an anti-smoking mascot. Never would've had that thought before today. Thanks for that. reply ooterness 18 hours agorootparentFor those unaware, Staplerfahrer Klaus is a gory, dark-comedy PSA about the hazards of forklifts in warehouses: https://www.youtube.com/watch?v=TJYOkZz6Dck Most training modules are dry and clinical. This one is the opposite. Many people are killed, dismembered, and otherwise mutilated. I think the combination of good comedy and vividly illustrated consequences makes Klaus far more effective and memorable than the average corporate training module that OP is discussing. reply nkrisc 19 hours agoprev> You know those signs on the highway that say things like, “Drive safely! Over 20 people have died on this highway so far this year!”? They maybe cause more people to crash and die. Ah yeah, those signs that completely remove the human tragedy of those who lost their lives driving. Instead you see something like \"99 people died on this highway\" and for a moment I can't always help but think, \"so close to 100!\" before remembering these are dead people, not just a number. What always reminds me to drive more carefully and not be so complacent is reading about some family has been torn apart by some little driving mistake that I could have easily made as well. It's tragic and depressing, but that's what reminds me to think about my own family when I'm driving. With regards to smoking, what convinced me to never take up smoking was seeing what 50 year old smokers looked like. Yikes. Should some lifelong smokers be fortunate enough to live to 80, taking a look at them will typically put to rest any remaining desire to smoke. reply stackedinserter 18 hours agoprevPeople subconsciously reject agendas that authorities (parents, school, government, corporations) push into their throats, what a surprising discovery. reply iamleppert 17 hours agoprevI have a simple solution to end all world hunger, forever. Nuclear War. reply tech_ken 15 hours agoprevIt's definitely hard to change people, but I also think to some extent it's extremely easy if you know what you're doing and you're specific about the context. As the author notes: cults are really good at it. Another example is any entertainment performer, or even just a DJ: getting a crowd from one state (\"cold\") to another state (\"hot\") is well-understood game. On the other hand scientists seem to be very bad at it (if the last few decades of quantitative social science research are any indicator). My pet hypothesis is that this is because an experimental environment is actually a very bad way to reason about human behavior. When you're dealing with basic material phenomena (particles, energy, chemical reactions, even biological reactions to a certain extent) the behavior is 'isolatable'. It will happen in a predictable way under controlled conditions, and removing outside influences makes that behavior easier to observe. Human behavior is the exact opposite of 'isolatable' though: so much of what makes us act the way we do is the influence of outside factors. In a general sense I don't even think it really makes sense to imagine a 'single person' or 'isolated behavior', our mind exists in mutual definition with the social world around us. Indeed the successful examples I listed above are successful precisely because they take control of your environment: a secluded meditation area or a sweaty club are critical tools for changing the way someone is. As a result I think scientists often have a really skewed understanding of what motivates people, because anything they do in a controlled environment basically can never generalize by definition. Want to get kids to stop smoking? Don't treat the kids, treat the media environment they live in. But this isn't something that can be replicated in a generic/engineering/scientific manner; there's no magic recipe it's just common sense and coalition building (getting buy-in from the people who make media). But because these tools don't fit cleanly into an experimental framework they kind of get sidelined. Instead we get endless variations on nudge theory and little cognitive games to try and induce the behavior we want. Shockingly this fails time and time again. Preventing sexual harassment in the workplace simply cannot come from a PowerPoint deck or training, it will only follow from a massive overhaul in the cultural milleu. Look at any successful grassroots political movement: find people who are primed to receive your message; convince them to support you; use their support and evangelism to widen the circle to the next group of slightly less-primed people; repeat until massive; force structural/cultural change through a crush of bodies. Simple as. edit: hm okay I clicked one link in the OP and it seems this point is not very original https://www.vitalcitynyc.org/articles/its-hard-to-change-peo... reply yowlingcat 15 hours agoparentI really like your illustrative example and to me it points at the fundamental core problem of the idea of \"social science\" to me -- it's not that the structured anthropological inquiry is a misguided ambitions, nor that scientific methods are misguided, but that the two are, to some extent, intrinsically at odds with one another because of practical limitations to isolate factors within or to create in a controlled environment a natural social setting. To your point, the DJ (and I would say many performance artists) has a practice for how to make this occur in a repeatable manner, but the practice is an artistic practice and therein a cultural practice, not a conventional scientific practice. To me, it raises the question of whether for the goals of any kind of \"social science\" to come to fruition if it must become an artistic practice. I don't have a great answer to this question but you can probably tell in which direction I lean. reply tech_ken 14 hours agorootparentDamn you said exactly what I wanted to say but way more concisely haha. > To me, it raises the question of whether for the goals of any kind of \"social science\" to come to fruition if it must become an artistic practice. “Artistic practice” is absolutely what I think the field is missing, and which many other disciplines with similar concerns (ex. marketing) are way more enthusiastic to incorporate. And thats not just about getting social scientists to start painting or whatever, but more adopting that mindset of an artist trying to perfect their craft. The basic program for the performing artist is to bomb over and over again until you can change people’s behavior “by feel” (or maybe you never develop the knack and change careers). Politicians similarly start out by pressing the flesh and going to endless community meetings until they figure out how to move a crowd. Doing these things isn’t “hard” once you’ve figured out the trick, but it’s a skill that you acquire by extensive practice and apply by intuition/artistic judgment, rather than something that you can figure out solely through sufficiently rigorous analytics. Not to say that this isn’t experimentation, it absolutely is, but it’s not laboratory experimentation, it’s fully integrated within the real world. reply TOGoS 16 hours agoprevOnly tangentially related to the main topic of the article, but the bit about the gym makes me want to say \"that's an ADHD thing!\". Which maybe the author already knows that. As if neurotypicals have an uncanny ability to simply skip over all the details of getting things done. Which explains both why they're able to get things done so quickly, and also why their constructions completely fall apart when you poke them with a stick. Or maybe getting stuck in the fractal of details is not specifically an ADHD thing, but the ADHD meme groups I'm in sure lead me to think it is. That said, I did manage to go to the gym regularly for a while. After a few times the routine of scoping out the equipment you want to use becomes habitual and the whole process becomes less overwhelming, though the do-it-for-the-rest-of-your-life requirement poses a challenge. For a few months, sure, but forever? I've got an infinite list of projects with infinite loose ends to attend to! reply nashashmi 17 hours agoprevHere is what prevented me: My D.A.R.E. officer said to us (honestly?) when you take drugs you feel like you are on top of the world. You take it again and you again feel like you are on top of the world. This is called a high. And after that, you feel like just a little bit of trashy feeling from how you used to be. And so you take drugs again and again to keep the trashy feeling away and each time you feel trashier and trashier after the high is gone. Until people who have been drugs for so long have a difficult time ever becoming normal again. next what prevented me from doing this was that it is illegal. Plain and simple. I am not questioning the law. I am following it. Lastly, what prevents me is my community faith that has labeled it to be forbidden. OTOH, caffiene is also bad. My family told me so. And they drank it nevertheless. And I drank a few times, but very few. But I avoided it because children should not be drinking coffee/tea. Until college came. And then I had to. It was a hack. But I kept it disciplined. And then I got married to someone who always drank coffee. And now I am an addict. and my potassium is super high. And everything else in my body is in the gutters because of it. And I can't come off of it. It's a way to stave off depression. And stress. And anxiety. And keeps me in a positive state because I can drink and become a superworker and get all my work done, without addressing some of the more problematic stuff. All I have done is push the crash further down the timeline. Caffiene should be banned too. And probably ADHD drugs while we are at it. reply kayodelycaon 16 hours agoparentThe vast majority of people don't have the same issues with caffeine as you do. Also, ADHD medication is regulated. It's an effective treatment for what can be a truly debilitating disorder affecting executive function, memory and emotional control. ADHD is not something that trying harder will fix. reply kmeisthax 19 hours agoprev>What really screws us is that it’s surprisingly hard to change people. We cook up schemes that seem like they should definitely work, then they don’t work, and this doesn’t chasten us or dim our enthusiasm for future schemes. It's easier than you think, but the techniques that actually work are all the ones in the no-no box that liberals like me know not to touch. Advertisers use them all the time: appeals to emotion, misleading and outright false comparisons, ridicule, etc. I cannot think of a single successful propaganda or advertising campaign that worked specifically because it embraced the whole \"marketplace of ideas\" debate club ideal. Speaking of \"how to get 7th graders to smoke\", anti-smoking and anti-vaping PSAs. They're so awful as advertising that they make smoking and vaping look good, because that is what advertising is designed to do. Make you feel good about the product being talked about. The few commercials that did actually get people to stop smoking back in the day were ones that were profoundly disturbing and had no positive emotional sentiment. Like the one where someone's smoking through a hole in their trachea. If you want to get people to stop smoking, you need to make it look as profoundly unsexy as possible, and that's something advertisers aren't great at doing. Likewise, for sexual harassment, you don't bombard people with incredibly obvious basic moral principles and legal minutiae. You make sexual harassers look like objects of ridicule. Cringe, as the kids say. My go-to example of how to do this would be Hbomberguy's early video essays[0], like the one where he systematically deconstructs all of Davis Aurini's toughboy affectations[1]. So let's break down the archetypal sex pest. They're not sexually harassing people because they need sex - in most countries you can just pay people for it now. Instead, they're doing it for the same reason my two male guinea pigs hump each other: to assert dominance. I want you to imagine the most domineering asshole you can think of, and then figure out what qualities will make them look pathetic rather than sexy. Again, if you want an example, perhaps check out this old video from The Onion about a sex pest dinosaur: https://www.theonion.com/paleontologists-discover-skeleton-o... Forget the Social program, forget the Emotional program, I'm talking about the Make Smoking Cringe Again program. [0] No shade to his later work - it's also amazing, but in a different way. [1] People say he's still sipping that shot glass to this very day. reply nerdjon 20 hours agoprevI firmly believe that the problem in the US regarding how we handle drugs in the US is that it is so focused on Shame. Same with how we do sex ed and many other things. We shame it so much, that when you have questions or you start thinking about it, you don't have people who you can go to and ask questions without judgement or being preached at. And then of course once you do try something, it is criminalized so you can't go talk to someone to get help without being worried about going to Jail (depending on where you are). It also doesn't help that so many of these systems love to talk about how Pot is just as bad as every other drug, which of course it isn't. I feel like we need more of a focus on compassion instead of just beating us over the head with \"Drugs are Bad mkay\" so of course we are going to rebel against adults telling us this. Especially when we know we can't have alcohol but they can, so are they telling the truth about drugs? Same with Smoking. The message gets mixed up when we are being told all these things that we shouldn't do but adults are able to do some of them. reply ganzuul 19 hours agoparentShaming must be one of our worst generational traumas. I don't think millennials like me can stop it, but Gen Z already has a slim chance of doing it. reply quacked 19 hours agoprev [–] At the core of the problem is that social services that address the physical consequences of smoking (mostly medical professionals and financial counselors) are run by people who believe t",
    "originSummary": [
      "Prevention programs are failing to tackle drug use among 7th graders, questioning the idea of easily changing human behavior.",
      "Ethics training and personal improvement methods are ineffective in addressing implicit biases, emphasizing the need for critical thinking in decision-making.",
      "The article criticizes the universal pre-publication peer review system, suggesting the Experimental History newsletter as a resource for seeking and utilizing truth."
    ],
    "commentSummary": [
      "The discussion evaluates the effectiveness of drug education programs such as D.A.R.E., highlighting the risks of different substances and their impact on health outcomes.",
      "It delves into the safety and risks associated with drugs like cannabis, MDMA, and LSD, while addressing misconceptions and the influence of media on smoking habits.",
      "Advocates for a compassionate approach to drug use and addiction, emphasizing the significance of factual information over scare tactics and manipulation."
    ],
    "points": 203,
    "commentCount": 239,
    "retryCount": 0,
    "time": 1715776884
  }
]
