[
  {
    "id": 37546874,
    "title": "CatalaLang/catala: Programming language for law specification",
    "originLink": "https://github.com/CatalaLang/catala",
    "originBody": "Skip to content Product Solutions Open Source Pricing Search or jump to... Sign in Sign up CatalaLang / catala Public Notifications Fork 67 Star 1.6k Code Issues 56 Pull requests 9 Actions Security Insights CatalaLang/catala master 26 branches 6 tags Go to file Code Latest commit AltGr Improvements to the Polish translation (#499) 3cc77f4 Git stats 3,510 commits Files Type Name Latest commit message Commit time .github/workflows Remove from scripts .nix add ocolor as a dependency build_system Improvements to searching for libs at runtime (plugins, runtime, etc.) compiler Improvements to the Polish translation (#499) doc Update ocamlformat examples Improvements to the Polish translation (#499) french_law Remove catala_legifrance runtimes Reformat syntax_highlighting bugfix: address breaking change in markdown-mode v2.6 tests Temporarily disable unstable test .git-blame-ignore-revs Update reformatting commits list .gitattributes Remove generated files from git .gitignore add missing french_law/python/src/runtime.py to .gitignore .ocamlformat Update ocamlformat CITATION.cff Renaming and correct autorship CONTRIBUTING.md Replace the type conversion and rounding operators with overloads Dockerfile Update CI build INSTALL.md docs: update the generating website assets section from the INSTALL file LICENSE.txt Code reorg Makefile Remove from scripts README.md Remove catala_legifrance build_release.sh Correctly setup ocolor on all output channels catala.opam Update ocamlformat clerk.opam Clerk: replace colordiff with diff --color dune Remove from scripts dune-project Bump version to 0.8.0 flake.lock update flakes file flake.nix Update ocamlformat french_law.opam Bump version to 0.8.0 generate_website_assets.sh Remove from scripts ninja_utils.opam Bump version to 0.8.0 README.md Catala Explore the docs » View Tutorial • Report Bug • Contribute • Join Zulip Chat Catala is a domain-specific language for deriving faithful-by-construction algorithms from legislative texts. To learn quickly about the language and its features, you can jump right to the official Catala tutorial. You can join the Catala community on Zulip! Table of Contents Concepts Catala is a programming language adapted for socio-fiscal legislative literate programming. By annotating each line of the legislative text with its meaning in terms of code, one can derive an implementation of complex socio-fiscal mechanisms that enjoys a high level of assurance regarding the code-law faithfulness. Concretely, you have to first gather all the laws, executive orders, previous cases, etc. that contain information about the socio-fiscal mechanism that you want to implement. Then, you can proceed to annotate the text article by article, in your favorite text editor : Once your code is complete and tested, you can use the Catala compiler to produce a lawyer-readable PDF version of your implementation. The Catala language has been specially designed in collaboration with law professionals to ensure that the code can be reviewed and certified correct by the domain experts, which are in this case lawyers and not programmers. The Catala language is special because its logical structure mimics the logical structure of the law. Indeed, the core concept of \"definition-under-conditions\" that builds on default logic has been formalized by Professor Sarah Lawsky in her article A Logic for Statutes. The Catala language is the only programming language to our knowledge that embeds default logic as a first-class feature, which is why it is the only language perfectly adapted to literate legislative programming. Getting started To get started, the best place is the tutorial of the language. A French version is also available but might be out of sync with the latest language features. Note: bleeding-edge version If you are interested in the latest development version, pre-built artifacts including binaries and API documentation can be found at https://catalalang.github.io/catala Building and installation Catala is available as an opam package! If opam is installed on your machine, simply execute: opam install catala To get the cutting-edge, latest version of Catala, you can also do opam pin add catala --dev-repo However, if you wish to get the latest developments of the compiler, you probably want to compile it from the sources of this repository or use nix. For that, see the dedicated readme. Usage Catala Use catala --help if you have installed it to get more information about the command line options available. The man page is also available online. To get the development version of the help, run make help_catala after make build. The catala binary corresponds to the Catala compiler. The top-level Makefile contains a lot of useful targets to run. To display them, use make help Plugin backends While the compiler has some builtin backends for Catala (Python, Ocaml, etc.), it is also possible to add a custom backend to the Catala compiler without having to modify its source code. This plugin solution relies on dynamic linking: see the dedicated README. Clerk Use clerk --help if you have installed it to get more information about the command line options available. To get the development version of the help, run make help_clerk after make build. The clerk binary corresponds to the Catala build system, responsible for testing among other things. To get more information about Clerk, see the dedicated readme Catleg Catleg is a command line utility providing useful integration with LégiFrance, the official repository of French legal documentation. See the decidated repository for more information. Documentation Syntax cheat sheet A complete and handy reference of the Catala syntax can be found in the cheat sheet (for French and English versions of the syntax). Formal semantics To audit the formal proof of the partial certification of the Catala compiler, see the dedicated readme. Compiler documentation The documentation is accessible online, both for the latest release and bleeding-edge version. It is otherwise generated from the compiler source code using dune and odoc. Run make doc to generate the documentation, then open the doc/odoc.html file in any browser. Examples To explore the different programs written in Catala, see the dedicated readme. API To know how to use the code generated by the Catala compiler in your favorite programming language, head to the readme of the French law library. The corresponding pre-built examples are also available. Contributing To know how you can contribute to the project, see the dedicated readme. Test suite To know how to run or improve the Catala reference test suite, see the dedicated readme. License The compiler and all the code contained in this repository is released under the Apache license (version 2) unless another license is explicited for a sub-directory. Limitations and disclaimer Catala is a research project from Inria, the French National Research Institute for Computer Science. The compiler is yet unstable and lacks some of its features. Pierre Catala The language is named after Pierre Catala, a professor of law who pionneered the French legaltech by creating a computer database of law cases, Juris-Data. The research group that he led in the late 1960s, the Centre d’études et de traitement de l’information juridique (CETIJ), has also influenced the creation by state conselor Lucien Mehl of the Centre de recherches et développement en informatique juridique (CENIJ), which eventually transformed into the entity managing the LegiFrance website, acting as the public service of legislative documentation. About Programming language for literate programming law specification catala-lang.org Topics programming-language legislative-texts Resources Readme License Apache-2.0 license Citation Cite this repository Activity Stars 1.6k stars Watchers 23 watching Forks 67 forks Report repository Releases 4 v0.8.0 Latest + 3 releases Used by 2 @CatalaLang / catala-explain @CatalaLang / catala-dsfr @CatalaLang / catala-website Contributors 39 + 28 contributors Languages OCaml 88.3% Python 6.4% JavaScript 1.4% Makefile 1.0% Vim Script 0.7% R 0.6% Other 1.6% Footer © 2023 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
    "commentLink": "https://news.ycombinator.com/item?id=37546874",
    "commentBody": "CatalaLang&#x2F;catala: Programming language for law specificationHacker NewspastloginCatalaLang&#x2F;catala: Programming language for law specification (github.com/catalalang) 418 points by gorenb 21 hours ago| hidepastfavorite251 comments otikik 44 minutes agoThe naming choice is really unfortunate. It&#x27;s like naming a programming language \"français\", or \"Deutsch\" (or \"English\").From the bottom of the readme:> The language is named after Pierre CatalaI&#x27;d suggest changing it to PierreLang then. reply p-e-w 8 hours agoprevThis project seems to implicitly assume that a formally specified code of laws, where statutes can be interpreted largely mechanistically, is a good thing (and by extension, that the existing system of human interpreters with discretion and margins of error is a problem to be overcome).I don&#x27;t disagree with this assumption outright, but it&#x27;s certainly not obvious to me that it is correct, and the authors appear to present no arguments supporting the same. reply steego 8 hours agoparentI suspect the motivation of this project is a lot more practical than what you’re projecting onto it, and I suspect you might be discounting how often law is transformed into code that runs on state IT systems.As someone who is familiar with the process of turning statutes into code, I can appreciate what this project endeavors to do, even if the value is limited to providing clarification for software developers. reply wiz21c 6 hours agorootparentIt is hugely useful. I worked on implementation of law in code for 10 years and the big problem is this: there&#x27;s a constant communication between software devs and law poeple. For exmaple \"did you code this like that ?\", \"why did the system did that ?\", \"if I code this way, does it reflect the law precisely ?\". You have to constantly trnaslate from the code base to the language of lawyers and vice versa. Doing that with regular code is no good: regular code ends up mixing law code and technical code. Making a clear separation with a dedicated language is a big plus. reply rendaw 3 hours agoparentprevThis seems obvious to me and seems something that law already strives to: you end up with very precise law regardless via case law, but with a high legal cost to reach that point. At the very least, it&#x27;s not something I&#x27;d expect to be on Catala&#x27;s homepage.I think this is closely related to rule of law too. Per https:&#x2F;&#x2F;www.britannica.com&#x2F;topic&#x2F;rule-of-law> In particular, laws should be open and clear, general in form, universal in application, and knowable to all.... The law should... comprise determinate requirements that people can consult before acting, and legal obligations should not be retroactively established.What are the benefits of ambiguous laws? reply hutzlibu 1 hour agorootparent\"What are the benefits of ambiguous laws? \"Freedom and flexibility. Reality is infinitely complex, trying to encode real world problems 100% accurate into code can only work by rounding a lot, with the result of oversimplification with the result of lots of \"unjust\" rulings, when you do not take the intent of the law into account, but only the words. We have this problem already (a lot) many things that seem very wrong but are legal by the letters of the law.(Mobsters being released because of formalities for example, even though everyone knee they were guilty) reply stakhanov 34 minutes agorootparentI&#x27;d say, it&#x27;s useful to distinguish between intentional and unintentional ambiguity&#x2F;underspecification. For example, many jurisdictions that have statutory contract law have a statute that says that a provision in a contract is unenforceable if it is \"unconscionable\". There is, of course, a broad spectrum of things that can fall under that term, but that&#x27;s intentional, i.e. the law maker specifically wanted to open a door there for a judge to hold a contractual provision unenforceable if they need it to be to get to a just outcome.However, a lot of ambiguity, underspecification, and just plain sloppiness in language is there for no good reason and causes problems, and it would obviously be a good thing to get rid of that. reply brnt 53 minutes agorootparentprevCan ambiguity be quantified, and&#x2F;or codified? Risking sounding like a painfully naive layman, would it be possible to define the range in which to apply the rule as function of range of inputs? When uncertaintu on inputs increases or veers outside of some well known range, then bandwidth for the judge increases? reply nvm0n2 14 minutes agorootparentprevBut that doesn&#x27;t mean freedom or flexibility for citizens. It means freedom and flexibility for the judiciary. For citizens, it takes away their freedom and flexibility because vague or contradictory laws (often coupled with severe penalties) create risk, or the perception of risk, where there wasn&#x27;t meant to be any to begin with. It causes people to stop trying new things out of fear that Rule By Law will happen to them.Whose freedom is more important? The people&#x27;s, or their rulers? If you believe in enlightened rulers, you may legitimately conclude the latter, but for the rest of us, we&#x27;d rather the government be properly constrained. After all that&#x27;s why laws are written down to begin with, it&#x27;s why there is such a thing as the professional lawmaker.Vague law is usually not written due to some deep philosophy of law making, after all. Nobody really tries to defend it in practice. It&#x27;s almost always a result of lazy or politically contentious lawmaking when the people writing the laws don&#x27;t really know what they want in the first place, so trying to divine their intent will never get you far. reply jsty 3 hours agorootparentprev> What are the benefits of ambiguous laws?That they can deal with the massive amounts of ambiguity & blurred lines &#x2F; grey areas the world has to offer. Too many of our everyday concepts can’t be expressed rigorously for a formal encoding & determinate decision process - a famous example being obscenity’s famed “I know it when I see it” definition - and many are highly context dependent. reply _a_a_a_ 1 hour agorootparentprevLaws very often look for intent of actions as well as the actions themselves.And they are ambiguous: \"beyond reasonable doubt\" is what, as a percentage, in your view? reply roel_v 4 hours agoparentprevIt fits quite will within French legal tradition. Remember that it was Montesquieu himself who said &#x27;le juge n&#x27;est que la bouche de la loi&#x27;. When I was working on my law degree, I (being a programmer) was quite interested in this field, and (although my French wasn&#x27;t&#x2F;isn&#x27;t great, so I didn&#x27;t get to go as deep as I would have liked) I found a large community of such legalistic work in French speaking parts of the world. The professor at my university who was into this sort of stuff also seemed to lean towards that sphere of influence. reply anon291 8 hours agoparentprevSome laws, like tax bracket computation probably are fine but most laws should not be codified like this in my opinion reply pgeorgi 7 hours agorootparentThe French have a programming language for tax related work: https:&#x2F;&#x2F;github.com&#x2F;MLanguage&#x2F;mlang reply ChristopherDrum 12 minutes agoprevThis paper definitely has thoughts on the matter https:&#x2F;&#x2F;law.mit.edu&#x2F;pub&#x2F;interpretingtherulesofcode&#x2F;release&#x2F;4\"As the Rules as Code movement gains momentum, questions are starting to be asked about the performance and practical effects of expressing law computationally. This article examines the strengths, weaknesses, and new opportunities of engaging with these emerging systems.\"The Future of Coding podcast covered it recently https:&#x2F;&#x2F;futureofcoding.org&#x2F;episodes&#x2F;065The abstract says, \"Software code is built on rules. The way it enforces them is analogous in certain ways to the philosophical notion of legalism, under which citizens are expected to follow legal rules without thinking too hard about their meaning or consequences. By analogy, the opacity, immutability, immediacy, pervasiveness, private production, and ‘ruleishness’ of code amplify its ‘legalistic’ nature far beyond what could ever be imposed in the legal domain, however, raising significant questions about its legitimacy as a regulator.\"It&#x27;s a complex paper&#x2F;topic that I personally need more time to grasp before throwing my opinions around too heavily. But my first, knee-jerk reaction so far is that moving laws into code is a bad idea. Specifically, as the paper says, \"...code by its very nature tends toward a kind of strong legalism. This is the case regardless of the intent of the programmer, however vicious or virtuous that may be.\"The \"strong legalism\" inherent in code means \"the sovereign’s exercise of power is de facto legitimate, and thus not open to question.\" Not to be reductive, but that ain&#x27;t good.I feel we&#x27;ve seen evidence of this path already, with (easily refuted, but somewhat common) claims like \"data can&#x27;t be biased\" (for example). The tendency to blindly follow a computer&#x27;s dictate with, \"Well, the computer says this is so, so it must be so.\" is strong in our society at times, I think. reply jeppester 17 hours agoprevFor years I&#x27;ve tried to convince my lawyer friend that something exactly like this would be great to have, and then it turns out to have existed probably all the time.I think this is truly awesome.Every law should be written in a language like this, and presented publicly with syntax highlighting and consistent formatting rules.Then it should be made part of the school curriculum to learn the law language.I believe it would greatly improve everyone&#x27;s ability to read laws and be confident about their understanding of them, which would be a huge boon for society. reply tsimionescu 16 hours agoparentThis is quite a misunderstanding of how the law actually works, probably enhanced by lots of lawyer TV emphasizing obscure wording tricks.In reality, laws are already written in a relatively normal language, and the words almost always mean exactly what they mean in plain English. The only problem is that the legal concepts they describe are themselves complex, and often they end up in a tangle of references to other laws and regulations and legal precedent etc. reply MichaelDickens 14 hours agorootparentIANAL but from my experience reading laws, they&#x27;re written in a way that looks like it&#x27;s trying to replicate programming-language-esque nested logic, but in prose format—instead of using physical layout to establish the relationships between concepts, they use words, which I find more confusing. I would rather read laws written in a more structured format. reply OJFord 13 hours agorootparentYou (as a software engineer or similar) have been trained to read the &#x27;more structured format&#x27; you&#x27;d prefer; lawyers have been trained to read &#x27;prose&#x27;.Don&#x27;t get me wrong I&#x27;d prefer it too, I just recognise it&#x27;s a consequence of my own education and experience, and if we somehow flipped the switch overnight most lawyers would be completely baffled and pining for the much clearer old way. reply sjy 12 hours agorootparentprevThe US Code is divided into numbered titles, sections, paragraphs and subparagraphs which can link to and incorporate each other by reference. This tree structure is reflected in typographic conventions which visually distinguish operative provisions from headings, indices and editorial notes. Isn&#x27;t this the same kind of \"structured format\" as a code repository made up of libraries, modules, source files and functions? reply anon291 8 hours agorootparentprevIf you read almost any state or federal statute, it&#x27;s usually presented online in a tree format where definitions link to the defining article and referenced articles are hyperlinks to those sections.It&#x27;s all very readable. reply LudwigNagasena 16 hours agorootparentprevThe words almost always mean almost exactly what they mean in plain English. That’s why the law is a huge mess.As a fun exercises, try to find how many definitions of “child” there are in the US law and how many times it’s used undefined. reply insanitybit 10 hours agorootparent> As a fun exercises, try to find how many definitions of “child” there are in the US law and how many times it’s used undefined.I don&#x27;t see how a language is going to solve this. This isn&#x27;t really an issue of language, it&#x27;s an issue with ambiguity in the very intent itself. That&#x27;s why we have judges, who interpret that intent. reply p-e-w 8 hours agorootparentIndeed. Ambiguity in legal codes is a feature that allows them to remain relevant for more than a couple of years, and the formal-law \"utopia\" that some commenters here appear to desire would be a nightmare if put into practice. reply alexvitkov 8 hours agorootparentPersonally I&#x27;d rather have laws that work, are consistent and easily understood if you can follow &#x27;if A then B&#x27; logic, even if they have to be updated more often.Relying on ambiguity is admitting there are no laws, and we rely on the common sense of the people in thr judicial system. reply TuringTest 7 hours agorootparent> Relying on ambiguity is admitting there are no laws, and we rely on the common sense of the people in thr judicial system.What&#x27;s better:- relying on the common sense of the people in the judicial system to interpret the intent of the law as it applies to a particular situation,- or relying on the common sense of legislators to write a precise, unambiguous law that will cover all possible situations without negative unintended consequences, and without the law-writing process being influenced by spureous interests and pressures?The answer highlight why the law is interpreted as is now, with a body of trained public servants analyzing the particulars of each situation, rather than by fanatics trying to follow the letter of the law.Languages like this may help clarify the intent of legislators so that it is not twisted by clever lawyers; but a human reviewer should always check the relevance of one law to any particular case, and wether the text corresponds to the original intent as applied to a given situation. reply insanitybit 1 hour agorootparentprevExactly, it is truly horrifying to think of a law encoded such that it can not be superseded by human interpretation. reply mratsim 6 hours agorootparentprevThey mean disputes go on for years, costing society a lot and enriching intermediaries. reply alexvitkov 8 hours agorootparentprevThis is a solved program in programming, you hover Child and hit \"Go to definition\". If it&#x27;s ambiguous, the lawmakers would get a compile error instead of pushing a broken law. They may even have to pay us for consulting to fix it, depending on how esotrtic the language is, how nice is that! reply kdmccormick 7 hours agorootparentLegal codes aren&#x27;t meant to work like programming languages. It is impossible for a legislator to predict how the world will work when their law is applied, and it is highly unlikely that they will anticipate every situation and context in which their law will be invoked.Judges and juries and lawyers all exist to help us interpret the inexact legal code in a way that is (hopefully usually; but obviously not always) fair and reasonable given the often-nuanced situations at hand. reply alexvitkov 6 hours agorootparentThat doesn&#x27;t add up with how the average piece of legal code looks. Even when I read a brand new piece of legislature it&#x27;s an impossible soup of words, attempting to document every possible edge case the legislators thought of. If the point was to leave room for interpretation by a judge and you concede yoy don&#x27;t have full context of how what you&#x27;re writing will be applied, surely you could write much more sensible and human-readable text. reply TheOtherHobbes 3 hours agorootparentIt adds up to how the average piece of legal code works in practice.Law is political. It&#x27;s persuasive, not deterministic. It often comes down to a judgment based on the relative political power of the entities in question.Even if you find a statute that says very clearly that X is unlawful, there will be situations where a lawyer will argue that it isn&#x27;t.Sometimes they&#x27;ll make that case successfully - for various possible reasons, not all of which will be lawful themselves.This is one reason why statute law is expanded by case law. And good luck trying to automate case law. reply epups 4 hours agorootparentprevI would be happy if the law conforms to other existing laws, not future edge cases. The solution being proposed here would do that, while the existing system would not. reply mschuster91 15 hours agorootparentprev> As a fun exercises, try to find how many definitions of “child” there are in the US law and how many times it’s used undefined.Which is why it is the common standard in German law to define every possibly unclear term either in the relevant section of the law itself or in an introduction article. reply lolinder 13 hours agorootparentIt&#x27;s the standard in the US, too, but \"child\" is a case where plenty of lawmakers have assumed it couldn&#x27;t possibly be misunderstood, while plenty of others have defined it with contradictory definitions in their respective sections.The latter is better than the former, but it also makes it even harder to interpret sections with no definition. reply tsimionescu 15 hours agorootparentprevI don&#x27;t agree that the law is a huge mess. It is certainly far less of a mess than any code base I&#x27;ve ever seen, given the gigantic scope of what it applies to, and how many people if affects.Note that the legal system is indeed a huge mess, but that happens because of many other reasons - not a problem with the wording or vagueness of the law, but with the explicit (malicious) intentions of law-makers, judges, police and others involved in the whole process.For your example of \"child\": how often does it actually cause a problem in practice? How many people have been improperly punsihed&#x2F;set free because of a poor interpretation of the word \"child\" in a specific law? This is far more relevant than every law taking up valuable space to define what such a common word means. reply LudwigNagasena 15 hours agorootparent> How many people have been improperly punsihed&#x2F;set free because of a poor interpretation of the word \"child\" in a specific law?How many improperly punished people is good enough? How many cases go to Supreme Court because the amount of needless ambiguity just adds up, one word at a time?> This is far more relevant than every law taking up valuable space to define what such a common word means.Right? Why do many laws redefine it? reply tsimionescu 14 hours agorootparentI don&#x27;t know how many is good enough. If it&#x27;s every other person, than that&#x27;s bad; if it&#x27;s two people since the law was written 50 years ago, I would say that&#x27;s good enough in my book. Which is it?And no, cases don&#x27;t often make it to the Supreme Court because the wording of the law is ambiguous. They make it to the SC because the parties disagree on legal principles and on whether laws are unconstituional or not.> Right? Why do many laws redefine it?I would have to see some specific examples to judge for myself. Still, this seems to be the opposite problem compared to what was raised earlier. So which is it? Do we want laws to be more explicit about their exact definitions of words, or more implicit? reply OJFord 13 hours agorootparent> And no, cases don&#x27;t often make it to the Supreme Court because the wording of the law is ambiguous. They make it to the SC because the parties disagree on legal principles and on whether laws are unconstituional or not.I&#x27;m not from your legal system, and yet even I know that&#x27;s not right: they make it the SC because the &#x27;losing&#x27; party disagrees with a lower judge&#x27;s decision that&#x27;s already been made, and makes an argument compelling enough in appealing it that it needs to be reconsidered. (A few times, to get as far as the SC, probably.) That almost has to be because of some &#x27;ambiguity&#x27; - the lower judge decided one way and the appeal is &#x27;well no I don&#x27;t think that&#x27;s the correct reading&#x27;. reply hnfong 10 hours agorootparentThe GP is taking a restricted meaning of \"ambiguity\", where the ambiguity is within the laws relevant to the case being appealed.But cases that go to the Supreme Court of the United States are often cases where the \"ambiguity\" is on how the US constitution should be interpreted and applied.In a practical sense you&#x27;re not going to be able to codify the US constitution into code. It&#x27;s even a well received \"feature\" that constitutions are sometimes a bit ambiguous, see: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Living_Constitution reply tsimionescu 5 hours agorootparentprevNote that the SC almost by definition only looks at cases that pertain to the constitution, or to base legal principles, not to individual laws. And good luck codifying the constitution itself and&#x2F;or the rules of common law into formal math.Also, appeals essentially mean that at least one of the parties believes that a judge made a mistake in the way they applied the law, not necessarily that the law itself is ambiguous. A judge can fail to apply a perfectly unambiguous law, or at least a plaintiff can believe that they did, and can bring enough evidence that their belief has some merit. reply runlevel1 7 hours agorootparentprevHow do you codify the effects of judicial review? What if the court&#x27;s decision is not binding but is still persuasive -- such as when it&#x27;s a decision from another jurisdiction?Common law has like 800 years of tech debt. It&#x27;s turtles all the way down. And by turtles I mean precedent, and not all of them are compatible.None of what I&#x27;ve described is malicious. It&#x27;s just what happens when law meets the messiness of the real world. reply sheepscreek 15 hours agorootparentprev> ..and how many times it’s used undefinedThis is the real advantage of codifying law into a programming language. You can have validation and assertion that is automated. And a strict structure, free from ambiguity.As an additional advantage, multilingualism becomes more accessible, with the codified program&#x2F;definition acting as the lingua franca of law. Thus, someone who only knows English could make sense of Japanese laws by reading it. reply lanstin 13 hours agorootparentI cannot imagine some system that goes from a formal language to reality not having ambiguity. Even going from mathematical formalism to mathematical truth you can’t get all the truth. I imagine getting all the justice would be harder.And I have heard lawyers explain that sometimes ambiguity, in contracts at least, is a good thing as it reduces the amount of a priori negotiation for low probability events. The the low probability event happens and the contract is ambiguous then you negotiate at that time and maybe sue.And for laws, I think a bit of flex in the system probably would be a good thing. Give some scope for local judgment an autonomy to the people closest to the situation. reply tsimionescu 15 hours agorootparentprevWhy would you want every law that applies to children to explain what a child is? Why stop at child, perhaps each law should include a definition of every word it contains, right? That would certainly make every law much more readable for the masses.The text of the law is meant to be understood by the people that it applies to, i.e. everyone living in the locality which passed said law. Expressing law in a formal language goes directly against that goal. Imagine if a EULA you get presented with, instead of being a wall of repetitive text, would be a wall of code with symbols you at best remember from some class you took in 8th grade. reply nvm0n2 9 minutes agorootparentHaving EULAs and many types of contract be code would be a huge upgrade.For one, it&#x27;d make them a lot shorter. You could use inheritance or composition to refactor out repetitive boilerplate, which is 90% of what EULAs are. The thing you see would only be the places where it deviates from a base EULA that you could study once.For another, it would catch bugs automatically. I have caught bugs in contracts drafted by lawyers a bunch of times, just by reading them carefully. For example numbers that are stated in both words and digits but they don&#x27;t match. References to clauses that no longer exist. Statements that are contradictory.A properly written language could be compiled to English for people who for some reason can&#x27;t read the \"real\" language. But a well written PL for law would be quite readable. reply victorbjorklund 3 hours agorootparentprevIt isnt a bug it is a feature. There is a point in law sometimes being a bit vague to create flexibility reply jeppester 15 hours agorootparentprevI don&#x27;t watch that kind of television much, but since I&#x27;m not a lawyer, my view of the world is probably too simplistic.Still I&#x27;d argue that normal language is very poor at handling that tangle of references.A good programming language would make those references very easy to untangle and present in their untangled form.When I&#x27;ve read (Danish) laws, I&#x27;ve often thought that they would read better as if statements.It&#x27;s not that I think those laws are written in legalese, it&#x27;s that they are expressing logic in a suboptimal way. Like how \"four plus four equals eight\" is a suboptimal way to express what could be expressed with 4+4=8. reply tsimionescu 15 hours agorootparent> A good programming language would make those references very easy to untangle and present in their untangled form.Not necessarily. Notation can only do so much to help with understanding. To understand 4+4=8 you still need to understand what&#x27;s a number, what addition means, and what it means for two numbers to be equal. The same problem applies to the law, and it takes far more time to understand legal concepts than the actual wording.Additionally, the law is not supposed to be some arcane discipline that you need to learn a new language for. The law is decided on by, and applies to, people who are not and have no reason to become legal experts. It is simply a statement of the rules by which we try to live.If laws were written in code, they would actually become much, much harder to understand than they already are for the vast majority of their audience. Imagine a public debate about a law where the text of the law was, instead of plain(ish) English, Haskell code. Imagine news anchors explaining that Biden agreed to add the lambda sign, but was heavily criticized by McConnell for his use of a monad instead of a plain for loop. reply jeppester 15 hours agorootparent> Additionally, the law is not supposed to be some arcane discipline that you need to learn a new language for.It would seem to me that reading laws has become an arcane discipline, partly due to it being expressed in a language with overly long sentences, which handles branches and references very poorly from a readability perspective.> Imagine news anchors explaining that Biden agreed to add the lambda sign, but was heavily criticized by McConnell for his use of a monad instead of a plain for loop.While that would surely be interesting to watch, I think we both know that&#x27;s not what would happen.Like I wouldn&#x27;t ask you \"number four plus sign number four equal sign X?\" reply tsimionescu 14 hours agorootparentI assure you that no one who fails to parse long sentences would get a better understanding from replacing those with code of all things.And if the actual text of the law consisted of coding symbols, I very much expect that (a) you&#x27;d have endless debates about the precise symbols being used, and (b) have to have anchors going over the meaning of those symbols and losing 9&#x2F;10ths of their audience along the way. reply jeppester 7 hours agorootparentYou are mentioning symbols as a negative in a lot of your comments, but the language posted here is mostly using words and math operators. reply tsimionescu 5 hours agorootparentIf a word is used with a precise formal meaning, than it is a symbol more than a word. For example, \"for\" in C isn&#x27;t the English word \"for\", it is a symbol for a precisely defined operation. Someone who speaks native English couldn&#x27;t understand what `for (;;){}` means. replyactuallyalys 16 hours agorootparentprevI think this is a big part of it. My sense (as a non-lawyer who has looked at a fair number of laws and contracts) is that, in addition, there are plenty of laws and contracts that are just poorly written and wording or constructions that lawyers have retained out of caution or traditionalism. This last case, traditionalism and caution, is maybe a special case of the other cases, but it&#x27;s not always obvious. reply tsimionescu 15 hours agorootparentI believe that another thing that happens, and it is common in every field, is that certain constructions are very common within the field, and so the need arises among practitioners to shorten them. Most domains invent new words, but this doesn&#x27;t work for laws and contracts (since they need to at least in principle be understandable to non-practitioners, such as most elected officials).So instead of full-on jargon, legal texts get enshrined phrases, which practitioners can essentially skip over, but which also retain some meaning in plain English (though often sounding antiquated). reply anon291 8 hours agorootparentprevA contract is anything two parties agree to with some consideration (benefit) exchanged. The law does not distinguish between &#x27;proper&#x27; contracts and informal ones (like a handshake) except a proper one may be quicker to execute. And this is a feature... You wouldn&#x27;t want to force everyone to undertake the cost of developing highly specialized legal products just to do business. reply pydry 15 hours agorootparentprevI worked for a company that translated certain kinds of legal contract into what was effectively a DSL. They could then be represented in a simplified way. That was the whole business.The CEO (a lawyer) claimed that the lawyers who wrote these things would deliberately and unnecesarily overcomplicate them so that they could maximize billable hours.I think they could quite easily have been templated using a DSL but that DSL would need frequent maintenance. These types of contracts did evolve as new types of clauses and legal constructs popped up and gradually evolved from \"new\" to \"standard\" to \"boilerplate\". reply tsimionescu 15 hours agorootparentThe text of a contract can either be long and very explicit, or short but full of implicit assumptions. A DSL is the second kind: you encode those assumptions I the structure of the DSL and the text as written is based on all of those assumptions.The problem than becomes that anyone who wants to understand the contract now has to read not just the contract as written, but also all of the definition of the DSL itself. This can actually be OK if the DSL is very commonly used, such as a DSL for contracts between two parties which sign new contracts every day.But it is a huge waste of time for parties which rarely sign contacts, and is often used as an explicit moat to keep laymen from participating. If I give you a contract to sign that isn&#x27;t even written in plain English, you will have no choice but to hire a lawyer specialized in understanding this contract DSL to advocate for you.I imagine (savvy) lawyers actually love DSLs that purport to make contracts concise. reply chromoblob 6 hours agorootparentIf you can&#x27;t read documents in whatever form for their legal meaning, you can&#x27;t work around the need for a lawyer. The DSL may be defined in comprehensible enough language and texts in it may be interpretable easily enough; but the method of contract is determined by agreement of its parties (inside the bounds set by law). reply tsimionescu 5 hours agorootparentCurrently, contracts are judged by their meaning in plain English, with any additional definitions being stipulated in the contract itself (either explicitly or as part of the verbal agreements that accompanied the negotiation of the contract).A DSL is an extra layer of abstraction above that. If you agree to a contract written in some DSL, then you must also agree to the way that DSL translates into plain English. To significantly compress a legal contract that is not deliberately written to obfuscate its meaning, the DSL has to pack a lot of precise meanings into every term, making it very dense and hard to parse unless you&#x27;re well-versed in it. reply pydry 4 hours agorootparentprevThat&#x27;s absolutely not correct. A DSL is not necessarily short and implicit. It can be very implicit or very explicit and the one I worked on was explicit. Its defining feature would be that it is straitjacketed.The customers in our case did not actually look at the DSL - it was entirely internal. We decompiled the legal document into the DSL so that we could then represent the contract in more understandable ways. reply gorenb 15 hours agorootparentprevAs someone who has tried to read the Black’s law dictionary once, I must tell you that the law does use a lot of formal language. reply tsimionescu 15 hours agorootparentIt does, to some extent, but it&#x27;s still much closer to natural language than code. Expressing the law in code would turn this dial up to 11 and then some. reply kmoser 10 hours agorootparentSure, but there would be all manner of services that could automatically translate law-in-code to any human language you wanted. The best part is that you could easily and automatically translate a law into, say, English, Japanese, and German. Whenever the law-in-code source changes, just rerun your translator and voila: no human intervention required (meaning faster and more accurate translations of the law into human-readable language).You could even program the \"law-in-code-to-humanspeak\" translator to generate different levels of the target languages, e.g. translate into something at a 6th grade reading level vs. something at a grad school level. Again, the advantage would be the automaticity. reply tsimionescu 5 hours agorootparentBut now instead of reading the laws that govern my life myself and understanding them, I have to rely on some third party to explain to me what laws my representatives are voting on. reply from 12 hours agorootparentprevThen you should be able to tell us what defrauding someone of their intangible right of honest services encompasses :) reply tsimionescu 5 hours agorootparentIs that supposed to be very complex legal language? I&#x27;m not sure if this is some reference I&#x27;m missing, but it sounds relatively simple to me: it seems to be an accusation that someone is providing services dis-honestly (i.e. hiding some aspect of how the service will be performed or how much it will cost).The full scope of what that encompasses is very hard to know, as it depends on essentially every other regulation and common law practice and precedent that applies in the particular legal jurisdiction (and which jurisdiction that is can itself be a somewhat thorny issue).But this problem isn&#x27;t solvable by code. It&#x27;s part of the intrinsic complexity of the legal system. reply post-it 12 hours agorootparentprevWhatever statute you&#x27;re looking at likely defines \"intangible right of honest services\" and \"defrauding.\" reply from 10 hours agorootparent> For the purposes of this chapter, the term “scheme or artifice to defraud” includes a scheme or artifice to deprive another of the intangible right of honest services.That&#x27;s it. reply da39a3ee 15 hours agorootparentprevI don&#x27;t know if what you say about laws is correct, but it&#x27;s certainly not a correct description of contracts. The general public is constantly confronted with utterly unreasonable legal documents: far too long, far too unclear, far too complex. The lawyers writing these, and the entities paying them, both know that the public will never read or understand them. It&#x27;s pure cinicism. reply tsimionescu 15 hours agorootparentSure, but that wouldn&#x27;t change if they were written in code. It would probably get much worse, in fact. reply da39a3ee 9 hours agorootparentI don&#x27;t think so. Any state progressive enough to adopt law-as-code might also put in place laws limiting the length and complexity of the code. It would be easier to control this aspect as code I believe. reply tsimionescu 5 hours agorootparentBy that same token, any state progressive enough to think about law-as-code might first put in place measures to limit the length and complexity of contracts written in plain language as well.The problem here is the length and complexity, not the language used to express the contract. And note that law-as-code would necessarily mean that a layman is fully unable to understand a contract (or the text of a law) at all. They would be fully reliant on a specialized worker to explain the code to them in plain English. Current contracts, if you have the patience to read and map them in full, are fully understandable by anyone with a good knowledge of the English language. replybg46z 16 hours agoparentprevAlthough in principle I agree with you, the law generally depends too much on interpretation and precedent to be expressed and understood like you’re hoping for. reply jeppester 16 hours agorootparentThat information should be available in summarized anonymized form, referenced from the law language.The judiciary should keep it up-to-date. reply hyperthesis 15 hours agoparentprevA big problem is you lose all the case law that interprets the law. Like discarding two centuries of bug reports and patches.At the same time, laws have been codified (i.e. the case law rewritten coherently, combining all the patches combined), such as the Uniform Commercial Code. reply swayvil 16 hours agoparentprevAnd the links! OMG, you could trace precedents and justifications and citations all the way back to the ur-utterances of Hammurabi. Reading that stuff could be quite educational. reply jll29 7 hours agorootparentFormer R&D director of legaltech company here. Lawyers already use tools that link everything.For instance in the U.S., the Westlaw search engine for legal cases is such a tool, it can parse legal citations and turns them into hyperlinks. It rankes cases given a query based on a state-of-the art machine learning based IR method, and it is aware of cases in the ranking that are currently overturned by higher courts (shown as red flags) or being reviewed (shown as yellow flags).The software can also predict the outcome of a legal case and recommend courses of action that makes winning more likely. It includes ruling statistics about all sitting judges.Curiously, lawyers like to see the world as static, they do not like e.g. search results to differ between sessions, but of course cases get decided dynamically every day, which must necessarily also change the search results for any given query.If people want to see what is the state of the art in AI and the law, I recommend you have a look at \"AI & the Law\" (ICAIL, the annual conference and the journal of the same name). reply qingcharles 15 hours agorootparentprevAm actually working on a project like this. Not quite to the level of insanity you&#x27;re thinking about, but certainly a couple of thousand years. reply nymiro 1 hour agoprevI was a bit confused by the fact that my first language is Catalan, which in Catalan is spelled Català. So yeah, imagine someone proposing a language specification for the law called English. reply turtleyacht 19 hours agoprevIt would be interesting to also \"weave\" in test cases. The workface of logic statements is exactly where bugs are introduced.Especially around temporal events, and that goes to formal models (and even more bugs).Typically, if there is a rule around height, there would be at least three tests¹: one taller, one equal to, and one shorter. (Without types or something, then also negative, null, and max&#x2F;min boundary inputs too.)So you could have tests based on timelines, like Given a regulation is passed in 3 months And parties are prevented from exercising B But \"17 tons\" of waste are dumped anyway And ... When ... Then ...Having a model checker integrated would be a boon. Maybe we could have DevOps-like pipelines in formally-verified legislature (or at least the encoding of language to code).¹ https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Equivalence_partitioning reply octacat 17 hours agoparentIt will not compile (sarcasm).Many laws are written with a lot of double meaning (recent eu regulations on allowing or not allowing russian cars is a good example).Though, it could be a good idea to find all the possible double meanings or vague definition when trying to \"digitise\" the laws into the programming language. reply ragulpr 5 hours agorootparentRight, some laws are just ftp-dragged & dropped to prod, understanding that it doesn&#x27;t compile yet. But if legislator managed to express the _intention_ the courts will over time add and make all test cases pass.I&#x27;m sure the mentality of a PHP developer running a successful but insane legacy site is a better model for this than a perfect OCAML project :) reply justusw 9 hours agoparentprevThis is great. I often look at unit tests to understand how to use legacy code. Seeing a law come with concrete examples that are part of the legal text, would allow us to test if a rule is logically consistent, and test our understanding as well. Sort of how you want to convince your reviewer that your code works by proving it with a unit&#x2F;integration&#x2F;e2e test.End to end tests for legal frameworks? reply natn 16 hours agoparentprevimagine... TTD applied to laws.Or take it a step further, write a test suite and automatically generate a range of possible laws that satisfy the tests. reply az09mugen 17 hours agoparentprevThat would be nice also to have unit tests reply kstrauser 19 hours agoprevI think what I&#x27;d rather see is a standardize test suite format for laws that spells out the intentions.Once I lived in a state that proposed a very simple anti-child porn law with good intent, but it was too simple. It read sort of like \"anyone sending explicit pictures of minors from a cell phone will be guilty of conveying child porn\". It was written in the proper legal jargon, but wasn&#x27;t a whole lot more detailed than that. I called the sponsor of the bill and asked if that meant if my hypothetical daughter sent a naked picture of herself to her boyfriend, then wouldn&#x27;t she be a felon under his new law? He had an \"oh, crap, that&#x27;s not what I meant!\" reaction and ended up withdrawing the bill so it could be re-written. (Aside: I felt pretty good about that. Props to the legislator for being quick to understand and respond appropriately!)Imagine if that were handled like program code, with a test like:* This law does not apply to minors sending pictures of themselves.That would do a few big things:It would make legislators be clear about what they mean. \"Oh, we&#x27;d never use this online child safety law to ban pro-trans content from the Internet!\" \"Great! Let&#x27;s add that as a test case then.\" I confess that this is a deal breaker: politicians don&#x27;t like being pinned down like that.It would probably make it easier to write laws that reflect those intentions. \"Hey, that law as written would apply to a 15 year old sexting her boyfriend! The code doesn&#x27;t pass the tests.\"Future courts could use that to evaluate a law&#x27;s intent. \"The wording says it applies to 15 year olds sending selfies, but the tests are explicit that it wasn&#x27;t meant to. Not guilty.\"I&#x27;m sure this couldn&#x27;t happen for a hundred reasons, but I can dream. reply lolinder 14 hours agoparent99% of the work is in coming up with the edge cases, and in law the most common thing to do with edge cases is call them out explicitly. I imagine the legislator went back and added a clause to the law that specified \"it shall not be considered a violation of this section for a minor to send photos of themselves\".Laws don&#x27;t need to be computer-executable, they&#x27;re about intent and the interpretation thereof, so the test suite itself is really part of the law and may as well just be embedded in it. reply jrm4 19 hours agoparentprevI partly like this idea in theory, but believe it is literally 100% impossible to come up with a better \"test suite\" than \"the actual court system?\" reply kstrauser 19 hours agorootparentThe courts have to evaluate what they think the law&#x27;s drafters meant: Yeah, it says this, but it&#x27;s obvious the legislators didn&#x27;t mean for it to be read that way. It&#x27;d be nice if there were footnotes that expounded on what the authors were trying to accomplish to help courts interpret the laws. reply bjt 18 hours agorootparentAt least in the US, it&#x27;s not the drafters&#x27; intent that matters, but the intent of the legislators who voted on it. (Legislators actually have lawmaking power. Drafters are usually unelected staff or even lobbyists.)When a statute is ambiguous, courts do sometimes look at the congressional record (eg floor debates) to determine intent. reply istjohn 16 hours agorootparentTo add on that, there are several competing theories of statutory interpretation in US legal thought. It&#x27;s a very complex subject. https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Statutory_interpretation#Sta... reply zo1 7 hours agorootparentprevI don&#x27;t want me or my male family members to be labelled sex-offenders whilst you \"test\" the \"court system\" to see if the laws work as intended. All because some overzealous prosecutor wanted to be \"tough\" on \"toxic masculinity\". reply alexashka 17 hours agorootparentprevIf it was &#x27;literally 100% impossible&#x27;, you wouldn&#x27;t need to believe it, you&#x27;d know it to be so.As for test suites and courts - the two are complementary so there&#x27;s need to compare them to one another. reply ambyra 17 hours agorootparentprevYeah, there’s no way that a modern computer could outdo the logical accuracy and processing power of our 300 year old legal system. Court rooms and arguing and paperwork, much more efficient than silicon. reply jrm4 13 hours agorootparentTrying to measure it for \"logical accuracy\" and using ideas like \"processing power\" so very deeply demonstrates how little you understand what actually is happening. reply RecycledEle 15 hours agorootparentprevCourts and lawyers are efficient at generating large pay checks for lawyers.They are horrible at everything else. reply cutler 17 hours agorootparentprevWith developments in AI I doubt that will stand the test of time. reply phillipcarter 18 hours agoparentprevI think for something like this to be effective, you need the actual intent encoded correctly (so this use case wouldn’t have been solved), and lawmakers acting in good faith (i.e., not drafting legislation that’s intentionally vague such that it can cast a wide net and force people to use the courts to dispute things). reply nicwilson 14 hours agorootparentYou don&#x27;t need those voting for the bill to act in good faith if you have as part of the system of passing bills that the opposition gets to write the (adversarial part of the) test suite. Then either that forces any loopholes (or other undesirable effects) to be: updated as explicitly intended (with bad publicity and potential for reversion upon a change of government); or taken into account and the bill updated to reflect that, or left in the test case for case law to cite as intended.Either way you really want intent to be encoded somehow. reply irundebian 17 hours agorootparentprevIntent could be derived by parliament debates minutes. reply phillipcarter 17 hours agorootparentYeah, uhh, having seen some of the hearings in recent state legislatures regarding abortion, that’s some flawed thinking. Lawmakers are intentionally vague throughout the entire process sometimes. reply hahajk 13 hours agoparentprevIt&#x27;s great that your congressperson was enlightened enough to pull the bill. Some states (such as Minnesota) apparently actively prosecute children for sexting.[1] https:&#x2F;&#x2F;www.aclu.org&#x2F;news&#x2F;juvenile-justice&#x2F;minnesota-prosecu... reply kstrauser 12 hours agorootparentI was pleasantly surprised.Minnesota is so bizarrely, irrationally wrong on that one. That poor kid needs an adult to sit her down and explain why sending out nude pics as a minor is a really bad idea, not to label her as a sex offender. Now, if someone (especially an adult) received those pics and shared them, go ahead and charge that person. reply codesnik 19 hours agoparentprevlaw tests would be good, though they probably have to be \"evaled\" via the same mechanism which would apply them. Meaning, courts :\\I personally would be happy if any country would attach rationale for the law to the law itself. And possibly some KPI to see if it works. So the law could be reevaluated later, to see if it works at all, or maybe counterproductive, or maybe some major actual application of the law is not why it was introduced. reply victorbjorklund 18 hours agoparentprevMindblown. I went to law school and now work as a developer but never thought about it. Writing tests for laws should totally be a thing. reply mertd 17 hours agorootparentAlso free agent simulations for discovering unintended consequences reply mschuster91 15 hours agorootparentprev> Writing tests for laws should totally be a thing.It is a thing already. In both the US and Germany it is common for lawmakers and regulators (e.g. the FCC which was here on HN to solicit comments a few days ago) to provide drafts of laws and regulations to interest groups so that these can raise issues they find. reply EdwardCoffin 17 hours agoprevSomething like this got passing mention in Greg Bear&#x27;s [1] book Moving Mars (1993) [2], under the name Legal Logic. The (human) Martians used it with AI assistance to formulate legislation for their newly independent society.For those who don&#x27;t know, Greg Bear was a well-known SF author who died less than a year ago. His passing was discussed here at the time [3] [4].He was one of the authors that influenced my youth a great deal, and I particularly remember this aspect of Moving Mars as catching my imagination, so will be interested to read what Catala has to offer.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Greg_Bear[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moving_Mars[3] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33679668[4] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33675708 reply gandalfgeek 18 hours agoprev(self plug) They published a paper describing the language https:&#x2F;&#x2F;hal.inria.fr&#x2F;hal-03159939, and here&#x27;s a short video summary of it: https:&#x2F;&#x2F;youtu.be&#x2F;OiaFTFSAa1I reply dathinab 14 hours agoprevprogrammers love to propose using \"programming language\" or similar for lawBut this fails to realize that _ambiguity (in some ways) is a fundamental important part of law_.This is because the world itself is fundamental ambiguous (in some ways)&#x2F;clear cut.Naturally not all ways of ambiguity are wanted.But you can be sure that with \"code as law\" the ways loopholes are abused will get worse in my opinion.I would even go as far that some many laws should be more focused on what should be upheld then the details how (which is fundamental less clear cut&#x2F;more ambiguous). reply brutusborn 4 hours agoparentI don&#x27;t think there is any reason ambiguity would clash with a project like this. Take \"value.fair_market\" in the concepts section [0]. Sure, lawyers can argue over what this means, but these competing definitions can also be defined programmatically.I agree with your idea that our interest in laws shouldn&#x27;t focus on implementation details but I think they should focus on outcomes. This requires a method to produce an evaluation function to measure the outcomes of a new law, and a system such as Catala to help model expected outcomes and to help select between competing laws (eg if our outcome = \"we want less pollution\" then our policy might be \"ban polluting industries\" or \"tax pollution externalities.\" Both have complex consequences which would be better analyzed automatically and measured empirically.)[0] https:&#x2F;&#x2F;github.com&#x2F;CatalaLang&#x2F;catala#concepts reply unethical_ban 12 hours agoparentprevAgreed. Although I don&#x27;t think this is a bad idea, I think of the idea of perfectly defined laws and perfectly enforceable laws are terrifying. If every law on the books today were able to be perfectly enforced and perfectly monitored, our lives would be utterly miserable.I&#x27;m not going to argue that&#x27;s a problem with laws vs. enforcement, but either way, our society is built around ambiguity and unequal enforcement of law. reply dathinab 5 hours agorootparentI&#x27;m not speaking about perfect enforcement.But about that law, in difference to what movies love to pretend, is not about clever word tricks and nit-picking formulations.(In court it still can be about clever arguing, including nit picking arguments if necessary.)But code _is_ about nit picking formulations at least if we ignore documentation, naming conventions etc. but lock solely at what the code does.Code is meant to be precise.Law is meant to be only as precise as necessary but no more then that. Or you could say it&#x27;s meant to be as imprecise as viable.Code is about the specific case (in general).Law is about the generic case (in general), avoiding specific cases where possible.Code is made for machines to consume.Law is meant to be consumed with ambiguous defined context of the situations in (human) .This is so deeply rooted in law that I would argue it&#x27;s (in general, with exceptions) not possible to translate any current laws to code without accidentally changing their meaning in a lot of subtle but meaningful cases. reply brutusborn 4 hours agorootparentprevDo you have any examples where ambiguity is truly beneficial?For all the examples I can think of, the most beneficial outcome is removing the law altogether. reply EPWN3D 20 hours agoprevI don&#x27;t think there&#x27;s much of a problem with actually reasoning about a law&#x27;s text that a computer can help solve. The complicated bit is weighing equities, which still requires humans and lawyers. reply quickthrower2 16 hours agoparentAbsolutely. Although the clarity by creating algorithms from tax tables can be helpful, and sometimes the wording seems ambiguous. Although you probably also need lots and lots of examples. (It is as if you need unit tests!) reply Jeff_Brown 13 hours agoparentprev+1. I see one benefit of this language -- it could make it much easier to write programs to compute taxes and benefits. Beyond that I don&#x27;t see what it could possibly offer.Are there any lawmakers, lawyers or judges excited about this, or is it only programmers? reply btilly 17 hours agoparentprevI think you meant, \"...humans, lawyers, and bribes.\"No, I don&#x27;t have a lot of faith in our legal system. Why do you ask? reply post-it 12 hours agorootparentSure, but this new language wouldn&#x27;t help with that. reply gorgoiler 8 hours agoprevLaws would do well to follow the rules of software. Small modules with clear responsibilities with an emphasis on readability and test cases that are run before you go to prod, for example. Testing is expensive so I understand why the legal system would rather just push their code and fix bugs when they see them in the wild. The collateral damage for people caught up in real life test cases is tolerable, especially when it’s someone else footing the bill.Linting and type checking the existing codebase would also be more helpful than rewriting everything in a new language. Enforcing size constraints on vocabulary and word count. Cross referencing between different legal systems. Throwing out dead laws that are no longer executed in prod. Profiling the efficiency of existing laws to find hot spots.There’s little incentive to do this when the current system is run by a cadre of highly trained legacy COBOL programmers. I’d pick a very small part of the system — incorporate a new city and start from the ground up — and take it from there with the clear eyed expectation that a full rewrite is going to take a century. reply p0w3n3d 7 hours agoparentMoreover I think laws work similarly to software. People wrote law, others find a loophole and use it, the people fix it with patches, and so on... reply simplify 20 hours agoprevAlso see Logical English, a \"Programming Language for Law and Ethics\"https:&#x2F;&#x2F;virtuale.unibo.it&#x2F;pluginfile.php&#x2F;1273247&#x2F;mod_unibore... reply PeterStuer 7 hours agoprevAmbiguity is a feature, not a bug. I used to spend a lott of time on business process automation, and even in those more structured and restricted settings trying to codify procedures most often fails. The reason is that reality has (a) so many edge cases that it very rapidly devolves into chasing down an ever diminishing ROI, (b) is unknown by the middle management and business analists, those that would have the authority to construct and sign off on it, and (c) relies on intelligent people applying creative pragmatic solutions to keep the business running and straightjacketing those into inflexible automatons is the most surefire way to sink the ship. reply greybox 19 hours agoprevI&#x27;d be interested in seeing something like this for verifying game designs &#x2F; new game rules given an existing design reply GolDDranks 17 hours agoparentWhy? What do game designs have to do with law? reply 0x9b 17 hours agorootparentGames are activities bound by rules. Laws are rules for government&#x2F;governed.AFAIK There&#x27;s not really a programming language specific for describing how players interact in a game, so although there&#x27;s no reason you couldn&#x27;t implement it in any old programming language. I guess the same thing could be said of the law too until Catala. reply altair8800 16 hours agorootparentWouldn’t that just be writing test cases against the business logic of your game? reply Martinussen 15 hours agorootparentI assume they&#x27;re talking about tabletop&#x2F;board games and such, not video games. reply GolDDranks 8 hours agorootparentAhh, that was the context I was missing. Indeed, makes sense for board games. replydang 18 hours agoprevRelated:Catala: a programming language for socio-fiscal legislative literate programming - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24948342 - Oct 2020 (37 comments) reply ctenb 6 hours agoprevI would have a much easier time understanding government legislation if everything was provided in such language. I tried to compute my taxes by hand a month ago to see whether and how much money it would save if I enabled \"loon middeling\" (some Dutch law about income). But I couldn&#x27;t figure it out. The explanations provided were ambgiuous in some subtle way, leading me to incorrect assumptions. In the end I did figure it out by reverse engineering a free third party calculation tool (which also was not correct, but putting their insight and my insight together made something that came close to the number on my belastingaanslag). reply alexvitkov 8 hours agoprevAlthough I agree on principle, the closest thing we have to &#x27;formal law&#x27; are smart contracts, and already billions of dollars have been stolen from bugs in these, despite barely anyone using them. I have some reservations for basing our entire legal system on code. reply krsdcbl 7 hours agoparentI think that comparison is way off.Smart contracts are much more comparable to \"a webshop\", than actual logic describing rules of arbitrage or other concepts at play in \"law\". reply alexvitkov 7 hours agorootparentA smart contract is a well-defined series of rules, that anyone can choose to interact with and have certain well-specified guarentees of the outcome -- in that sense I think there are quite a few similarities, the main differences being that a smart contract deals with a way narrower set of concepts and is immutable. reply friend_and_foe 9 hours agoprevThat&#x27;s really cool!I was puttering around with the idea of a ricardian compiler for legalese, basically a decompiler for something like this that could compile a legal text into clear logical rules. This would aid in proof checking for legal documents to ensure that they&#x27;re compatible with existing law, that there are no (unintended lol) loopholes and the like. It would also be useful if you wanted to create self enforcing legal documents that can be enforced deterministically by machines, such as collateralized agreements, and finally, even though someone would still need to know legalese, it could make the development of such agreements easier for people and lower the bar tremendously.I wonder if anyone has built anything like that, if these guys have, or if anyone has built other interesting ricardian compilers. reply blagund 20 hours agoprevMy hunch is, in any sufficiently large rule set, there will be inconsistencies. Handwaily think Gödel, or just the need for bounded domains in DDD.Humans (or, well, AI) is needed to cope with inconsistencies.That said, pointing out the fact of existence of inconsistencies could be very valuable. But a system needs to embrace them, not fight them. reply drdeca 19 hours agoparentGödel’s theorems don’t imply inconsistency for all large systems (unless “large” is taken to mean something strange), just for systems which are both not super-weak in what they can say, and complete (or if they have their own consistency as a theorem).I don’t think Gödel’s theorems particularly support the claim you’re making.In fact, here is an argument that a consistent rule-set (either can be extended to something consistent and complete, or ) can be extended to be made arbitrarily large and consistent:take a ruleset which is consistent, but for which there is something for which it has no prescription one way or the other (neither explicitly nor implied collectively by other rules) (I.e. “not complete”). Then, add a rule specifying that thing and nothing else which isn’t implied by that thing. This will be consistent, as if it were not, then the negation of the rule added would have already been an implication.This will either yield a larger ruleset of the same kind (consistent and incomplete), or it will yield one which is consistent and complete. Gödel’s theorems show that if the ruleset is an axiom system which is sufficiently expressive (e.g. contains Peano arithmetic) then the latter cannot be the result. So in this case, there are arbitrarily large extensions of the rule-set.If it isn’t an axiom system, or is one for a rather weak system, then the “the result is a consistent and complete system” option, well, why would you want it to be larger?Edit: perhaps what you are calling “inconsistencies” are what I would just call “exceptions”&#x2F;“exceptional cases”?To my mind, “embracing an inconsistency” doesn’t seem to make much sense in the case of law? Something has to be what actually happens. We (whether fortunately or unfortunately) cannot bring an actual contradiction into reality.Well, I suppose if one takes a sub-truth(not sure if this is the right terminology? I mean the opposite of super-truth) approach to vague statements, one might say that a somewhat-bald man causes the statement “that man is bald, and also that man is not bald” to be true (and also false), and as such “bring a contradiction into reality”, but that’s not what I mean by the phrase.I mean there is no full precise-ification of any statement, which we can cause to be simultaneously true and false irl.Those acting as agents of the law must behave in some particular way.When legal requirements contradict, people will not satisfy both of them. Perhaps one will be considered to take priority. Perhaps a compromise position between the requirements will be sought. Perhaps it will be left to the judgement of those following it in a case-by-case basis.But in none of these cases is a contradiction implemented. Can they really be said to be embracing the contradiction?Upon writing this edit I realize that I’m probably misinterpreting that part of your comment. I suppose the thing you are saying to embrace is not the individual contradictions themselves, so much as the system’s rules-as-written having contradictions, and therefore the necessity of dealing with such contradictions when implementing the rules, as the scenarios to which the contradictory statements apply, occur. reply kachnuv_ocasek 17 hours agorootparentI think parent might have been referring to the inconsistency that Gödel noticed in the US Constitution when applying for citizenship. reply anon291 8 hours agoparentprevThis is less of a problem in legal systems as the legal system self admits to resting on unproven axioms. reply cpeterso 17 hours agoprevInteresting that their “money” type doesn’t allow fractional cents or track the currency. And the only collection type is a fixed-sized list. I can imagine a lot of other useful base types that would be useful for laws about the real world, such as physical units like meters.https:&#x2F;&#x2F;catala-lang.org&#x2F;en&#x2F;examples&#x2F;tutorial#The%20Catala%20... reply qaq 19 hours agoprevNaming is hard Catala is cardsharper (cheat) in russian. reply juunpp 14 hours agoparentCatala is also a human language. reply dazzaji 18 hours agoprevI’m really interested in this project as another way to computationally express and use law. For more color on the project, here is a demo and discussion “Idea Flow” session I did with the project leads: https:&#x2F;&#x2F;law.mit.edu&#x2F;pub&#x2F;ideaflow8 reply tanukiworld 3 hours agoprevI&#x27;m actually in court later this month pertaining to some stolen traffic cones. Dismissed my lawyer just yesterday due to a dishonest swagger. I&#x27;ll be preparing some documents using this after I prototype using a few frameworks. Will keep you posted. reply plumeria 20 hours agoprevThe Catalan language name is written Català. reply melenaboija 20 hours agoparentIt is named after a French guy called Pierre Catala, not the language. Probably the name comes from the language but according to the documentation does not seem to be with acute accent.> The language is named after Pierre Catala, a professor of law who pionneered the French legaltech by creating a computer database of law cases, Juris-Data. The research group that he led in the late 1960s, the Centre d’études et de traitement de l’information juridique (CETIJ), has also influenced the creation by state conselor Lucien Mehl of the Centre de recherches et développement en informatique juridique (CENIJ), which eventually transformed into the entity managing the LegiFrance website, acting as the public service of legislative documentation. reply pxc 18 hours agorootparent> the documentation does not seem to be with acute accent.&#x27;à&#x27; has a grave accent, not an acute accent. reply melenaboija 18 hours agorootparentYups right, sorry :( reply pxc 18 hours agorootparentI just figured that since you cared to name the specific accent character (most people probably wouldn&#x27;t!), you&#x27;d care about the mixup too. :) reply racked 20 hours agorootparentprevThe commenter was maybe pointing out a naming conflict. reply Anduia 15 hours agorootparentprevTheir research group is called &#x27;Prosecco&#x27;, and another of their projects at Inria is &#x27;Squirrel&#x27;... Another one is F* (pronounced F star).I imagine that they find their naming choices amusing. reply wageslave99 6 hours agoparentprevNot related to the Catalan language but to Pierre Catala: https:&#x2F;&#x2F;github.com&#x2F;CatalaLang&#x2F;catala#pierre-catala (https:&#x2F;&#x2F;fr.wikipedia.org&#x2F;wiki&#x2F;Pierre_Catala) reply entwife 19 hours agoparentprevI also came to comment that the software&#x27;s name might cause confusion with the human language, Catalan. reply WanderPanda 19 hours agoprevWould be so funny if all the compute moves from AI to finding loopholes in the programmatically defined law. reply gorenb 19 hours agoparent\"haha! actually, if you give it this input, the program crashes! i can finally steal pillows from the bookstore!\" reply gmfawcett 12 hours agoprevI was expecting Lojban, or something similar. I remember someone proposing Lojban as a tool that could improve the clarity of legal and contractual writing.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lojban reply ThePowerOfFuet 16 hours agoprevAs someone who speaks Catalan, I find the name collision with the language referred to in the article most unfortunate. reply andrewnicolalde 15 hours agoparentWhy unfortunate? Also, I love your username. I think many of us could have guessed that you spoke Catalan from that alone ;). reply unstyledcontent 14 hours agoprevSomething to keep in mind is that the courts are not necessarily trying to determine the truth, but rather create a place to allow two parties that represent different interests duke it out. Not always what the courts are used for, but it&#x27;s a different mentality than science or programming. reply nologic01 16 hours agoprevThis seems like a worthy project but I think its landing page should define the scope of what it aims to do in more concrete terms. Otherwise it runs the risk of being seen as overambitious and open ended without a concrete problem to solve.E.g. its not clear if there is an explicit or implicit ontology against which the validity of any codification can be checked. reply wcerfgba 16 hours agoprevWhat are the authors&#x27; goals, what is their intended purpose? I can&#x27;t find a mission statement on their website. reply Layvier 14 hours agoprevSuper interesting. I also think we would win with a kind of versioning system for laws, including a definite objective for a law from the time of it&#x27;s creation, and constraints under which it should be questioned again reply sesm 16 hours agoprevThis project is doing code -> text, right?But then, the first line of description in Github says:> Catala is a domain-specific language for deriving faithful-by-construction algorithms from legislative texts.This reads like &#x27;text -> code&#x27;, which is the opposite of what this project seems to be doing. reply cphoover 20 hours agoprevThis is interesting, and not to criticize, but I wonder if transformer model&#x27;s accuracy in interpreting law will obviate the need for something like this.It would be interesting to train Large Language Transformer Models to generate this code for you based on the text in the laws. This way you have a deterministic testable output, without risk of hallucinations. reply 0xDEF 20 hours agoparentEven LLMs would be better off with a clear unambiguous syntax to define human laws and rules. reply eddtests 18 hours agoprevThis is really cool! But I guess it’s biggest drawback is being unable to deal with case law? reply rsrsrs86 14 hours agoprevI had so much trouble getting catala to even compile that I got frustrated. It’s a piece of academic code that’s very much abandoned reply a-priori 14 hours agoparentIt&#x27;s hardly abandoned. The git history shows activity almost every day over the last few months. It&#x27;s clearly being actively developed. reply klysm 10 hours agoprevDoesn’t TurboTax have some kind of DSL they use to encode all of the tax rules? reply kderbyma 19 hours agoprevStarted something like this years ago with a company that ended up pivoting to a slightly different direction after a while. glad to see something in open source space. reply brap 17 hours agoprevIt’s good that they made it not turing complete but they should probably also force an upper bound on law complexity reply Horffupolde 20 hours agoprevHow is this different from Prolog? reply pvaldes 16 hours agoprev\"We\" will interpret the law for you and the judges, and \"we\" are not suspicious at all of having a hidden agenda to replace \"the law\" by \"how we see the law\" to benefit ourselves.Is this a joke? reply 29athrowaway 19 hours agoprevI think having a \"linter\" for laws can be beneficial. It can help producing laws that are easier to read and understand.Having a \"compiler\" for laws can help identifying conflicts between different codes of law. e.g.: Imagine having a compiler error when a law is unconstitutional from a logical standpoint.But verifying the \"business logic\" (e.g.: what is the spirit or intent of the law?) of the law will remain a human intelligence task. reply ftxbro 11 hours agoprevthis is peak hacker news &#x27;solve human conflict by using technology&#x27; reply RedNifre 18 hours agoprevWill it help?On the one hand, I think it would be fantastic, if you had automated tests for the law. For example, when German politicians introduced the \"hacker law\", you could have pointed out that \"This new law would break the &#x27;security researchers need to be allowed to do penetration testing&#x27; test\".On the other hand, \"Brexit is in conflict with the Good Friday Agreement, we need a solution for Nothern Ireland.\" was known without machine readable laws and test, but politicians ignored it anyway.Maybe what&#x27;s needed is a law that outlaws test-breaking laws and requires politicians to fix the tests first, but I bet that would just result in a lot of \"commented\" tests. reply staunton 7 hours agoparentIf you make a law against \"test-breaking\" people will just break that law. reply codedokode 17 hours agoprevLaws are often written in most vague language to allow wide interpretation, especially laws regarding treason, communism, foreign agents, anti-war speeches and such things. Programming language won&#x27;t help here. reply anticristi 17 hours agoparentI&#x27;m also somewhat skeptical. How would a program deduce that \"cookies and similar technologies\" will mean localStorage, sessionStorage, IndexedDB, etc. Remember that the law was written well before some of these technologies even existed. reply llsf 16 hours agoprevCan we use it to compute taxes ? reply thargor90 18 hours agoprevToo have fun with laws&#x2F;rule systems take a look at nomic reply mdaniel 15 hours agoparentlinky-linky: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nomican example of the game rules: https:&#x2F;&#x2F;agoranomic.org&#x2F;ruleset&#x2F;slr.txtIn the context of this thread, I&#x27;m sad that the game rules don&#x27;t appear to be in a constrained vocabulary reply swayvil 16 hours agoprevA software for people. For dictating human behavior. Is that what we&#x27;re looking at here? reply lightedman 17 hours agoprevNo way would this ever be a good idea. Language changes too much, be it human or machine. This is why you need flexibility, not a rigid structure, in making law. reply yieldcrv 17 hours agoprevwithout formal training, one key thing I picked up is that most public understanding of legal concepts diverges from court understanding because law follows logical and&#x2F;or gatesso “and” isnt a list of accepted criteria, it is a list of things that must be simultaneously satisfiedbut its only using logical gates most of the timethis is a good step in showing that. not a panacea but a good step! reply jrm4 20 hours agoprevLawyer here.Oh, this again. I suppose this looks relatively harmless, but I&#x27;m always wary of \"law is like computer code.\"The impulse to think this can strongly solve any real problem in the law is intuitively attractive, but I strongly predict this mostly never happens; it&#x27;s the law&#x27;s job to be intensely practical in the face of hard-edged \"computer-like\" rules.If anything, you get goofy confusion about what things \"are?\" My go-to on this is always the \"smart contract\" -- which can be useful little bits of automated robot money moving code, but emphatically are neither \"smart\" nor \"contracts.\" reply giltho 20 hours agoparentI can assure you that the person who wrote this is very well aware of the subtleties involved with formalising the law. Law + Programming is an active research field (https:&#x2F;&#x2F;popl23.sigplan.org&#x2F;home&#x2F;prolala-2023), and it is very far away from anything like smart contracts, it is full of brilliant people who have no pretension of replacing the law with computers, but simply be helpers where they can. reply jrm4 19 hours agorootparentSure, I think I posted my response not because \"it could never solve ANY problems,\" but instead \"way too many non-lawyers, especially techy-non-lawyers, have the deeply misplaced idea that is a very important, perhaps THE most important, problem to solve in the law.\" It&#x27;s just not very high on the list at all. reply sfvisser 19 hours agorootparent> It&#x27;s just not very high on the list at all.Maybe not for lawyers, no. But as a citizen I&#x27;m expected to comply with the law, with many many laws. It&#x27;d actually be nice if law was slightly more formally verifiable, so it would be easier for me to understand what to comply with.Being able to break down clauses into more logical normal forms would probably greatly enhance the possibility of compliance. reply jrm4 13 hours agorootparentIt really wouldn&#x27;t though.For every bit of gained clarity, you&#x27;d also gain a ton of people like the \"sovereign whatever\" idiots who just love trolling, with the added negative of them having more \"formal proof\" of their untenable silliness. reply gorenb 19 hours agorootparentprevI agree. The idea of logically representing law isn&#x27;t the same as replacing law with computers. That&#x27;s why I even posted this. reply andrewaylett 19 hours agoparentprevThis is a problem, but not one that Catala suffers from on a first reading.Some elements of law are amenable to translation into source code, and indeed anyone working in fintech will probably have done that at some point. If the law gives a threshold for a tax allowance, for example, you need to encode that requirement in accordance with the law. Being able to mark up the text of each regulation should make it much easier to be confident you&#x27;ve not missed anything.Trying to write non-financial regulation as code is pretty much doomed to failure. But to the extent that tax or benefits regulations set out numbers that we have to translate into code anyway, it&#x27;s good to have that code be verifiable against the specific regulatory text. reply Rochus 19 hours agoparentprev> Oh, this again. I suppose this looks relatively harmless, but I&#x27;m always wary of \"law is like computer code.\"\"To a man with a hammer, everything looks like a nail.\" (Twain?)Hasn&#x27;t Cyc impressively demonstrated just how incredibly difficult and costly it is to formalize even the most basic matters of daily life?There already was a discussion two years ago: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=27059899 reply mdaniel 18 hours agorootparent> Hasn&#x27;t Cyc impressively demonstrated just how incredibly difficult and costly it is to formalize even the most basic matters of daily life?I would offer that the \"cost&#x2F;benefit\" analysis for such a formalism exists on at least two axes: the concept domain which one is attempting to formalize, and the benefit (and&#x2F;or size of consumers) of any such working systemI can wholly understand that trying to translate the entirety of English into a formal logic system sounds overwhelming. But to side with a sibling commenter, why not at least start with the tax code which is a personal pain point, has (presumably) a correct outcome for some cases, and is mostly algorithms-in-EnglishAnd then, for the consumer side: ok, if I snapped my fingers and Cyc existed and worked I struggle to think how exactly my life would change. If the formally-specified tax code existed and worked I wouldn&#x27;t have to rage-upvote almost every comment on the annual tax hatred threadI would even offer that an incomplete version could still be useful if one left \"fuzzy\" variables in the corpus, and said \"welp, we can&#x27;t define what a $Person is because of the hundreds of years of precedent, so you&#x27;ll need an actual Judge for that\". I don&#x27;t meant to say that 50% of the corpus can be undefined variables, that&#x27;s just silly, but I&#x27;d hope the tax code isn&#x27;t built upon 50% undefined behavior, even if accountants want you to think it is reply Rochus 17 hours agorootparent> why not at least start with the tax codeThere have been many such attempts (e.g. NKRL by Zarri et al., also funded by EU). There are even societies that have been dealing with such issues for many decades (e.g. http:&#x2F;&#x2F;www.iaail.org). The formalization of law and language is only one of the issues. Like many previous attempts, this one suffers from the fuzziness of human language (even in the case of tax code). Fuzziness is not a drawback; it is what makes it possible to communicate efficiently in the first place. In order for us to communicate effectively, we need an enormous amount of tacit knowledge about our environment that our culture and life experience brings. If one tries to formalize the language, as in the present approach, one must also take this knowledge into account, down to the last detail (an \"upper ontology\" is by far not sufficient for this, and Cyc after decades is still not finished). And the tacit knowledge and also the moral valuation of the same change over time. And there are things like https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sorites_paradox which stand in the way of a complete formalization. Lenat&#x27;s 1990 book addressed many of the issues, but also his more recent talks are very informative where he demonstrates how they had to extend the Cyc representation language to cope with the problem, and why e.g. RDF triples are not enough. reply btown 19 hours agoparentprevLaw is like computer code, if:- your compiler was AI-complete and adversarial and hated you- your compiler was also not bound by any hard rules and could emit undefined behavior at any time- your job scheduling and orchestration system was AI-complete and adversarial and actively hated you- your runtime library had 50 different incompatible canonical implementations and can only be run by being forked by publicly-elected officials who blindly merge patches from bad-faith lobbyists- the documentation for any of those 50 runtime libraries is paywalled per page behind https:&#x2F;&#x2F;pacer.uscourts.gov&#x2F;pacer-pricing-how-fees-work if you&#x27;re lucky- the IDE is Microsoft Word, and the linter is a summer associate on their tenth cup of coffee- you will inevitably get a non-technical client who thinks that the more times you have \"notwithstanding the foregoing\" in your code the more you can call yourself Web Scale reply hedora 19 hours agoparentprevI like to turn this argument around to see how absurd it is.Why not just take the existing law, and have a machine execute it in the style of a computer program?We wouldn’t need judges juries or lawyers. You’d just type the specifics of your case and any supporting documents&#x2F;evidence into the computer and a verdict would pop out.Of course, the system could be used for other stuff too, like checking building code compliance or engineering soundness, signing off on military and police action, setting the executive branch’s priorities, and so on. reply jraph 19 hours agorootparentNot a lawyer. My thinking is very likely naive as I have no experience in this matter.I see two potential issues:- Picking evidences. \"Evaluating\" the law might need access to all the possible evidences that could exist, but that would certainly never be true, so you&#x27;d need someone to know which evidences to present. You probably cannot rely on some interactive process asking you such and such evidences because it would be presenting evidence that would trigger evaluations of chunks of laws. I would guess a lawyer with good knowledge of the law would probably be needed for this.- Setting precedents. Wouldn&#x27;t the \"automated\" law evaluation run into unprecedented cases all the time? You&#x27;d need someone to constantly issue a verdict on unforeseen situations all the time, and I guess you&#x27;d need a judge for this.Maybe it could work on many \"trivial\" cases though. reply 0thgen 19 hours agorootparentprevSome people have argued that the fuzziness of the legal system can be a good feature for some reason, but you could always have a machine execute the law and a human make the final call. So you wouldn&#x27;t need judges, juries, or lawyers, but you would need a team of legal shamans that sign off on verdicts reply hedora 19 hours agorootparentThe problem isn’t checking the computer’s output.It is that the law would need to encode all the stuff I said, so it would need to be nuanced enough to replace all engineering, leadership and administration roles. (And also anything involving ethics.) reply sealeck 19 hours agoparentprevI agree with you, but this is literally what those in civil law jurisdictions believe.There are some areas where automating things can be effective – e.g. tax systems. reply lemper 19 hours agoparentprevhow many times have you fought in court to argue about \"the spirit of the law\"? I, for one, don&#x27;t really care about this lang or \"law is computer code\" thing. just wanted to know lawyers life, I guess. reply diogenes4 20 hours agoparentprev> If anything, you get goofy confusion about what things \"are?\" My go-to on this is always the \"smart contract\" -- which can be useful little bits of automated robot money moving code, but emphatically are neither \"smart\" nor \"contracts.\"They are contracts—just not legal contracts. One of many types of contracts in the world that are not legal constructs. reply pjot 20 hours agorootparent> They are contracts—just not legal contractsI think this proves op’s point of goofy confusion for what things are. reply diogenes4 19 hours agorootparentLike, what language is? I&#x27;m not sure i get the point. reply Angostura 20 hours agorootparentprevMost contracts are intended to be enforceable by law, as far as I know. Which kind of non-enforceable contracts did you have in mind? reply layer8 19 hours agorootparentSoftware-interface contracts generally aren’t (cf. Design by Contract). The parent is correct insofar as the use of the word “contract” is not limited to contracts as defined by law. reply jrm4 19 hours agorootparentRight, I mean, I&#x27;ve heard this use and I think it&#x27;s kind of silly.It&#x27;s like calling a hopefully-completed-circuit in some device an \"electric contract\" or something like that. reply layer8 18 hours agorootparentIt’s a contract (mutual agreement) between the client code and the implementation of the interface. The implementation fulfills certain obligations provided the client code fulfills certain other obligations. The interface defines what those obligations are. If the client code fails to meet its obligations, then the implementation of the interface isn’t bound to fulfill its obligations anymore either. The point is to think in terms of two parties, where the developer who is either using or implementing an interface takes on the role of one of the two contractual parties. reply diogenes4 6 hours agorootparentprevWhat&#x27;s wrong with describing a hopefully-completed-circuit in some device as a contract? reply diogenes4 19 hours agorootparentprevSocial contracts? Personal contracts? God-sworn oaths? Covenants? The legal system is just a small corner of the universe. reply version_five 20 hours agoprevWhat problem does this solve? It appears to add precision where it&#x27;s mostly already clear - perhaps it can enforce some kind of rigor... but then like the example given uses \"fair market value\" as a term which I&#x27;d expect to be the kind of thing that&#x27;s in contention, rather than any of the actual \"logic\", and it doesn&#x27;t help with that.The reason we have courts and lawyers is because of the need for interpretation beyond just writing good logic, so I don&#x27;t see how this can really do anything. Or is it for something else? reply jacoblambda 18 hours agoparentCatala still produces plaintext legal documents at the end of the day but can be seen as a markup language for those documents. But because that markup language is a whole lot more precise than the legal text itself, it can be a bit more versatile.Examples of how this could be useful:- Reducing the overhead for maintaining a list of semantic translations of that legal code into other languages. Of course the official language is the only one that is \"legal\" but the other translations should be close enough to effectively express the nuance provided the language outputs are maintained by people who can actually speak those languages.- Producing machine executable proof or simulation code. This could be used for \"fuzzing\" the legal code to identify loopholes or unintended outcomes so that legislators can then propose improved terms to avoid those issues. This is by no means \"making code law\" but it provides an additional tool for understanding the law and how the many different parts of the legal code interact with each other.- Adding on to the previous example, sim code could be integrated into complex models for simulating the impact of legal changes on the economy at large or specific segments.- Finance related code can be used to generate a tool or API for validating tax, accounting, and compliance documents (as a first pass to catch errors early and reduce overhead) as well as to even prepare some of those documents. These tools often already exist but they are one or more steps removed from the actual legal definition which increases the risk of error as well as the overhead of maintaining them (which can potentially encourage rent seeking behavior by commercial providers of these tools).France actually is already doing this to a reasonable degree albeit the \"codified\" version is based on the law rather than the codified version producing plaintext law. The DGFiP [1] maintains a gitlab organisation [2] that includes both Catala and MLang [3] representations of different parts of the french legal code for exactly these purposes.1. https:&#x2F;&#x2F;fr.wikipedia.org&#x2F;wiki&#x2F;Direction_g%C3%A9n%C3%A9rale_d...2. https:&#x2F;&#x2F;gitlab.adullact.net&#x2F;dgfip3. https:&#x2F;&#x2F;github.com&#x2F;MLanguage&#x2F;mlang reply retrac 17 hours agorootparent> Of course the official language is the only one that is \"legal\"If only! Here in Canada there are two official languages. All laws are drafted, and enacted, in both English and French. Both versions are equally valid, equally binding. And, sometimes, they don&#x27;t say the same thing. reply jacoblambda 16 hours agorootparentYeah that&#x27;s common in a number of other countries as well. I should have probably said \"the official languages are the only ones that are legal\" instead. In which case a tool like this could be useful for helping maintain that equivalence. reply NoZebra120vClip 9 hours agorootparentprevCatholic Canon Law is drafted and coded in Latin, which is supposedly the only official and binding version. However, this is translated into hundreds of vernaculars! People often read it in their native language for convenience, even lawyers, but this can be fraught with peril. reply 0thgen 19 hours agoparentprevI suppose a formalized legal language could:- help in quickly testing whether newly drafted laws contradict existing laws(without needing to memorize the existing legal code)- check for redundancies- checking whether removing one law affects any others- statistically analyze legal systems in different countriesAssuming any of those are important issues in law. I&#x27;m not sure reply parineum 19 hours agoparentprev>It appears to add precision where it&#x27;s mostly already clear - perhaps it can enforce some kind of rigor...I&#x27;d argue that the imprecision of law is more feature than bug. Rules as written have edge cases and, as long as the law is written in natural language, you can get a feel for their intent and that helps Judges decide what to do in those situations. reply slazaro 16 hours agoprevI know that naming is hard, and it has already been mentioned in the comments here but... I can&#x27;t believe that somebody named a programming language with the exact same name as an existing natural language spoken by millions of people.It just seems like a bizarre decision that can&#x27;t be a benefit at all and can only have negative consequences. Just googling things about it is going to be hard. Why immediately create potential problems for yourselves when you can choose a name that&#x27;s not an issue? reply jrockway 16 hours agoparentThis hasn&#x27;t seemed to help or hurt the popularity of other languages. You&#x27;ve got hot beverages, single letters, snakes, gemstones, two letter verbs, oxidized steel, languages where two thirds of the name are symbols, etc. It doesn&#x27;t seem to matter. It appears that society, and search engines, are well-equipped to deal with the concept of homonyms. reply slazaro 14 hours agorootparentAll of the things you mentioned are not languages themselves. If you google \"catala language\" (try it! seriously) you&#x27;re going to get results for the natural language, not this one. It&#x27;s just an unneeded roadblock that they placed on themselves. reply jrockway 8 hours agorootparentI don&#x27;t think I&#x27;ve ever typed the words \"Java language\", \"C language\", \"Go language\", etc. except in this comment. reply mrdoob2 16 hours agoparentprevCame here to share the same thought. reply mathisfun123 16 hours agoparentprevgoofy; the language you&#x27;re talking about is Catalan of Catalonia, not `catala`.> Just googling things about it is going to be hardwhen you&#x27;re looking for docs on go do you google just \"go\"?edit: fine, it&#x27;s called catalá in catalonian itself - this is so pedantic now that i might as well at this point say that the missing diacritic is sufficient to disambiguate. reply lolinder 16 hours agorootparent> when you&#x27;re looking for docs on go do you google just \"go\"?I wouldn&#x27;t use Go as a good example of naming a language. It worked out because the language had the weight of Google behind it, but it&#x27;s still awkward that you have to use a different name when searching for things than you do at other times. reply mathisfun123 16 hours agorootparent> I wouldn&#x27;t use Go as a good example of naming a language. It worked out because the language had the weight of Google behind itthis is called the no true scotsman fallacy - \"I&#x27;m still right in XYZ case because XYZ isn&#x27;t a real instance of ABC (the thing I&#x27;m making a claim about)\" reply lolinder 16 hours agorootparentNo, it&#x27;s not, because I didn&#x27;t make a universal claim about anything. A No True Scotsman fallacy must follow a overly-broad No Scotsman statement.All I said is that Go, specifically, is an awkward name that probably shouldn&#x27;t be used to justify further awkward names. reply mathisfun123 16 hours agorootparent>No, it&#x27;s not, because I didn&#x27;t make a universal claim about anything.the implicit, categorical, universal, claim you&#x27;re making (that&#x27;s the crux of this petty, pointless, debate) is that the naming matters at all. my counterfactual is go and your response was that it should be dismissed&#x2F;discounted because blah blah blah. do you see the no true scotsman now? replymkl95 16 hours agorootparentprev> when you&#x27;re looking for docs on go do you google just \"go\"?Golang will do the trick. Catala lang will not unless the language becomes massively popular. reply mathisfun123 16 hours agorootparenttry catalalang right now behind incognito mode and tell me what you get reply mkl95 16 hours agorootparentTouché reply dial9-1 16 hours agorootparentprevit&#x27;s catalá in catalá reply slazaro 14 hours agorootparentIt&#x27;s actually \"català\" not \"catalá\". Source: I&#x27;m català. reply pvaldes 16 hours agorootparentprevCatalá is the name of the language in Catalan, is totally equivalent to saying \"English\" if you are a native.This is obviously a not innocent choice. At this level I don&#x27;t believe in coincidences and CatalaLang makes it even more obvious. This looks like a veeeery obvious psy-op, or a independentist version of the old embrace, extend, extinguish.My bet is that as they can&#x27;t stomach the basic legal concepts, they will try silently replace it by the new \"updated\" meaning of those concepts. reply feliixh 16 hours agorootparentDef a psyop. Classic Catalonian move of seizing independence by writing self determination laws in code and carefully introducing a bug that they can then exploit to secede. reply Supply5411 20 hours agoprevVery cool. Pessimistically, I think that having a clear, understandable view of legal text so that people can navigate the law safely is against a lot of entrenched interests. reply lo_zamoyski 20 hours agoparentThat&#x27;s far too reductive. The law is abstract, and still needs to be interpreted and prudentially applied to specific situations, and you cannot capture all of that in the law a priori. Furthermore, having a computer crunch through a bunch of predicates is easy compared to getting the facts expressed in a form that is crunchable. So for specific and narrow applications where such representations are not costly to produce or already exist, such an application of computational law is feasible. But broadly? No.And then there&#x27;s the distinction between lex and ius that I think needs to be considered in this context. reply tgv 20 hours agoparentprevI don&#x27;t think it&#x27;s that. It&#x27;s hard to write legal texts, and sometimes it&#x27;s better to be vague, so the courts have some freedom when establishing jurisprudence.Treaties can be written with vague wording to allow parties to sign it, even if there isn&#x27;t 100% agreement. That&#x27;s an old practice. reply Kretinsky 19 hours agorootparentOn the contrary, it&#x27;s much better to have very clear text, otherwise it will turn against the citizen.Imagine that you have an income tax where \"income\" isn&#x27;t clearly defined. Someone will end up with an audit and a lawsuit from the tax office because their definition will be, of course, extensive (every income, including non-realized capital gains) whereas most citizens would only consider salaries.In the end, you create legal uncertainty, and give courts way too much power.For the record, I used to work for my country&#x27;s government, and had to evaluate some laws in making that were written in an abstruse way. When I asked why, the civil servant told me that it was so \"they could pick the most favorable meaning in the case of a lawsuit\". reply bjt 18 hours agorootparentIt&#x27;s a balance. If you get too specific then your laws quickly become outdated as technology and society evolve. Or you miss corner cases by not enumerating every little scenario.So there are different tiers to deal with this.- Constitution - Very abstract and very rarely changed.- Statute - Sometimes abstract, sometimes specific.- Administrative rules. Very technical but still intended for broad application.- Indivi",
    "originSummary": [
      "Catala is a new domain-specific language that can create algorithms from legal documents, ensuring high fidelity between the code and the law.",
      "The language is built to reflect the logical structure of the law, making it accessible for review and certification by legal professionals; it even has a compiler that generates lawyer-readable PDFs.",
      "Named after Pierre Catala, a pioneer in French legaltech, the Catala project is a research initiative led by Inria, France's National Research Institute for Computer Science. However, the compiler remains unstable and feature-limited."
    ],
    "commentSummary": [
      "The conversation focuses on using Catala, a programming language, in specifying laws and legal documents, emphasizing on its benefits, drawbacks, and the potential challenges of translating English into a formal logic system.",
      "There is a debate about the use of code as a regulator, the complexity of legal code, the use of coding symbols in legal agreements, and the concept of encoding intent into laws using programming languages.",
      "Participants discussed the idea of writing clear legal texts, the role of a domain-specific language for legal contracts, comparing software development to the legal system, and concerns about the name choice for such programming languages."
    ],
    "points": 418,
    "commentCount": 251,
    "retryCount": 0,
    "time": 1694968469
  },
  {
    "id": 37546810,
    "title": "Run LLMs at home, BitTorrent‑style",
    "originLink": "https://petals.dev/",
    "originBody": "Petals Run large language models at home, BitTorrent‑style Generate text with Llama 2 (70B), Falcon (180B), BLOOM (176B) (or their derivatives), and fine‑tune them for your tasks — using a consumer-grade GPU or Google Colab. You load a small part of the model, then join a network of people serving the other parts. Single‑batch inference runs at up to 6 tokens/sec for Llama 2 (70B) and up to 4 tokens/sec for Falcon (180B) — enough for chatbots and interactive apps. Beyond classic LLM APIs — you can employ any fine-tuning and sampling methods, execute custom paths through the model, or see its hidden states. You get the comforts of an API with the flexibility of PyTorch and 🤗 Transformers. Try now in Colab Docs on GitHub Top contributors right now: slush 🤖💪 (200 blocks) • 🦆FYY🌞 (192 blocks) • discord.gg/Kcbg6fKGE6 (80 blocks) • premai.io (50 blocks) • twitter.com/dagelf (40 blocks) • and more Contribute my GPU Follow development in Discord or via email: Subscribe We send updates once a few months. No spam. Featured on: This project is a part of the BigScience research workshop.",
    "commentLink": "https://news.ycombinator.com/item?id=37546810",
    "commentBody": "Run LLMs at home, BitTorrent‑styleHacker NewspastloginRun LLMs at home, BitTorrent‑style (petals.dev) 403 points by udev4096 21 hours ago| hidepastfavorite96 comments __MatrixMan__ 1 hour agoAre trained LLM&#x27;s composable in any way? Like if you and I trust 99% of the same data, but each have 1% where we disagree, must we have two entirely separate models, or can we pool compute in the 99% case (along with the others who agree) and then create a derivative model for ourselves which covers for the differences in our trust models?I have only a rudimentary understanding of neural nets but it doesn&#x27;t seem crazy that the weights could be manipulated in such a way while preserving the utility of the model.I ask because I think it would be useful to know which statements two LLMs of equal power agree on and which they disagree on. You could then map that backwards to differences in their training data (only feasible if the differences are small).If instead two LLMs of equal power represent a missed opportunity to have one of greater power, and the disagreement analysis is prohibitively expensive to do, then that&#x27;s a bit of a different world. reply hnfong 13 minutes agoparentSomewhat yes. See \"LoRA\": https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.09685They&#x27;re not composable in the sense that you can take these adaptation layers and arbitrarily combine them, but training different models while sharing a common base of weights is a solved problem. reply jmorgan 17 hours agoprevThis is neat. Model weights are split into their layers and distributed across several machines who then report themselves in a big hash table when they are ready to perform inference or fine tuning \"as a team\" over their subset of the layers.It&#x27;s early but I&#x27;ve been working on hosting model weights in a Docker registry for https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama. Mainly for the content addressability (Ollama will verify the correct weights are downloaded every time) and ultimately weights can be fetched by their content instead of by their name or url (which may change!). Perhaps a good next step might be to split the models by layers and store each layer independently for use cases like this (or even just for downloading + running larger models over several \"local\" machines). reply mkii 4 hours agoparentAh, is it possible to tone down the self-promotion? I&#x27;ve been seeing your comments for ollama on many LLM-related posts here.> Please don&#x27;t use HN primarily for promotion. It&#x27;s ok to post your own stuff part of the time, but the primary use of the site should be for curiosity.Surely in this case it would&#x27;ve been possible to comment about OP&#x27;s work while leaving out the free backlink to your project. Just my 0.02 reply herewego 26 minutes agorootparentThere is nothing wrong with self-promotion if, as in this case, it is relevant to the discussion. reply brucethemoose2 17 hours agoprev> and fine‑tune them for your tasksThis is the part that raised my eyebrows.Finetuning 70B is not just hard, its literally impossible without renting a very expensive cloud instance or buying a PC the price of a house, no matter how long you are willing to wait. I would absolutely contribute to a \"llama training horde\" reply AaronFriel 17 hours agoparentThat&#x27;s true for conventional fine-tuning, but is it the case for parameter efficient fine tuning and qLORA? My understanding is that for a N billion parameter model, fine tuning can occur with a slightly-less-than-N gigabyte of VRAM GPU.For that 70B parameter model: an A100? reply pama 33 minutes agorootparentIf one is training the full 70B parameters, then the total memory usage far exceeds the memory for simply storing the 70B parameters (think derivatives and optimizer parameters such as momentum.) This is the main reason why models are split or why techniques like the fully distributed data sharing are used during training. During training of a distributed model, at every step of the optimizer these multiple-of-70B parameters need to go through a network wire (though not to all nodes, thankfully). As you suggested, LoRA could work well in a distributed setting because the trainable parameters are very small in number (tens of thousand of times less trainable parameters) and the info required to go through the network for non trainable parameters is also small. However, training this model on a single A100 is impractical as it would require mimicking a distributed training buffering things on a TB-sized CPU RAM (or slower) to swap pieces in and out of the model during every step in an otherwise distributed operation (and is not natively supported in existing frameworks to the best of my knowledge, even though one could technically write this code without too much difficulty.) reply brucethemoose2 14 hours agorootparentprev2x 40&#x2F;48GB GPUs would be the cheapest. But that&#x27;s still a very expensive system, especially if you don&#x27;t have a beefy workstation with 2x PCIe slots just lying around. reply LoganDark 9 hours agorootparentEven mATX boards tend to come with two (full-length) PCIe slots, and that&#x27;s easy sub-$1k territory. Not exactly a beefy workstation.Source: have a $200 board in my computer right now with two full-length PCIe slots. reply 7speter 8 hours agorootparentWhats more difficult is trying to cool gpus with 24-48gb of RAM… they all seem to be passively cooled reply LoganDark 7 hours agorootparentGood point, I think most of them are designed for a high-airflow server chassis, with airflow in a direction that a desktop case wouldn&#x27;t necessarily facilitate (parallel to the card). reply segfaultbuserr 7 hours agorootparentWaterblocks exist for some compute-only GPUs, including the Nvidia A100. Also, there are a few small vendors in China that offer mounting kits that allow you to mod these compute-only GPUs to use off-the-shelf AIO watercoolers. Certainly, not many people are going to take the risk to modify the expensive Nvidia A100, but these solutions are moderately popular among the DIY home lab developers to convert older server cards for home workstation use. Decommissioned Nvidia Tesla P100 or V100 can be purchased cheaply for several hundreds dollars. reply LoganDark 6 hours agorootparent> Decommissioned Nvidia Tesla P100 or V100 can be purchased cheaply for several hundreds dollars.Meh. If you want 16GB of VRAM for several hundred dollars, can&#x27;t you just pull a brand new 30-series off the shelf and have ten times more computing power than those old pascal cards? You&#x27;ll even have more VRAM if you go for the 3080 or 3090. Admittedly, the 3090 is closer to $700 or so, but it should still make a P100 very sad in comparison. reply segfaultbuserr 6 hours agorootparentYeah, these GPUs became less appealing after the prices of 30-series GPUs have dropped. The price of SXM cards are still somewhat unbeatable though if you have a compatible server motherboard [1]. Nvidia P100s are being sold for as low as $100 each, there are similar savings for the Nvidia V100s. But yeah, a saving around $100 to $200 is not really worthwhile...Another curious contender is the decommissioned Nvidia CMP series GPUs from miners. For example, the Nvidia CMP 170HX basically uses the same Nvidia A100 PCB with its features downsized or disabled (8 GB VRAM, halved shaders, etc). But interestingly, it seems to preserve the full 1500 GB&#x2F;s memory bandwidth, making it potentially an interesting card for running memory-bound simulations.[1] Prices are so low exactly because most people don&#x27;t. SXM-to-PCIe adapters also exist which cost $100-$200 - nearly as much as you have saved. It should be trivial to reverse-engineer the pinout to make a free and open source version. reply LoganDark 6 hours agorootparentIs it possible to take something like a CMP 170HX and do board-level work to add more memory chips? Or are they not connected to silicon? reply segfaultbuserr 5 hours agorootparentI don&#x27;t believe it&#x27;s possible. The HBM2e chips are integrated onto the package of the GPU die, making them impossible to remove or modify in a non-destructive manner. replyzacmps 15 hours agorootparentprevI think you&#x27;d need 2 80GB A100&#x27;s for unquantised. reply YetAnotherNick 3 hours agoparentprevFinetuning in a distributed way with questionable network would be lot more energy&#x2F;cost inefficient than doing it with a single node or a well connected cluster. Also, you can finetune 70b model for million tokens for $2 in lambda cloud orsimilar to the AI Horde kudosWhat they are referencing, which is super cool and (IMO) criminally underused:https:&#x2F;&#x2F;lite.koboldai.net&#x2F;https:&#x2F;&#x2F;tinybots.net&#x2F;artbothttps:&#x2F;&#x2F;aihorde.net&#x2F;In fact, I can host a 13B-70B finetune in the afternoon if anyone on HN wants to test a particular one out:https:&#x2F;&#x2F;huggingface.co&#x2F;models?sort=modified&search=70B+gguf reply swyx 18 hours agorootparent> GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensibleis there a more canonical blogpost or link to learn more about the technical decisions here? reply brucethemoose2 17 hours agorootparenthttps:&#x2F;&#x2F;github.com&#x2F;philpax&#x2F;ggml&#x2F;blob&#x2F;gguf-spec&#x2F;docs&#x2F;gguf.md#...It is (IMO) a necessary and good change.I just specified gguf because my 3090 cannot host a 70B model without offloading outside of exLlama&#x27;s very new ~2 bit quantization. And pre quantized gguf is a much smaller download than raw fp16 for conversion. reply beardog 17 hours agoparentprev>What&#x27;s the motivation for people to host model layers in the public swarm?>People who run inference and fine-tuning themselves get a certain speedup if they host a part of the model locally. Some may be also motivated to \"give back\" to the community helping them to run the model (similarly to how BitTorrent users help others by sharing data they have already downloaded).>Since it may be not enough for everyone, we are also working on introducing explicit incentives (\"bloom points\") for people donating their GPU time to the public swarm. Once this system is ready, we will display the top contributors on our website. People who earned these points will be able to spend them on inference&#x2F;fine-tuning with higher priority or increased security guarantees, or (maybe) exchange them for other rewards.It does seem like they want a sort of centralized token however. reply sn0wf1re 17 hours agoparentprevSimilarly there have been distributed render farms for graphic design for a long time. No incentives other than higher points means your jobs are prioritized.https:&#x2F;&#x2F;www.sheepit-renderfarm.com&#x2F;home reply seydor 18 hours agoparentprevIt&#x27;s a shame that every decentralized projects needs to be compared to cryptocoins now reply AnthonyMouse 7 hours agorootparentIt&#x27;s not the comparison, it&#x27;s that it&#x27;s one of the things cryptocoins are actually useful for: You have people all over the world with GPUs, some of them want to pay the others for use of them, but their countries use different payment networks or the developers want to be able to automate it without forcing the users to all sign up with the same mercurial payment processor who could screw over any of the users at random. reply littlestymaar 6 hours agorootparent> it&#x27;s that it&#x27;s one of the things cryptocoins are actually useful forIt&#x27;s what their proponent claim that they are useful for, yet there&#x27;s no single instance of a successful blockchain project actually achieving this kind of resource-sharing goal.> You have people all over the world with GPUs, some of them want to pay the others for use of themThe gigantic success of bitTorrent shows that humans as a group don&#x27;t need to have monetary incentives to share their spare hardware. In fact, it&#x27;s likely that trying to add money into the mix will just break the system instead of improving it: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Overjustification_effect reply joshuaissac 3 hours agorootparent> It&#x27;s what their proponent claim that they are useful for, yet there&#x27;s no single instance of a successful blockchain project actually achieving this kind of resource-sharing goalSia and Filecoin already work in this way to for people to share storage.> In fact, it&#x27;s likely that trying to add money into the mix will just break the system instead of improving itThis depends on the amount of money people are willing to pay for processing power. Volunteer contributions would be reduced, but the paid contributions could make up for it if the people who want to train their model pay enough to attract more people into the system and if those people can compete with conventional commercial offerings. reply littlestymaar 1 hour agorootparent> Sia and Filecoin already work in this way to for people to share storage.You&#x27;ll notice that I said “successful” in my original sentence.> This depends on the amount of money people are willing to pay for processing power. Volunteer contributions would be reduced, but the paid contributions could make up for it if the people who want to train their model pay enough to attract more people into the system and if those people can compete with conventional commercial offerings.That&#x27;s a very big “if”: the distributed nature of things is always going to make it more expensive than a traditional solution, especially if you need byzantine fault tolerance (which you need as soon as their monetary value to earn by cheating), the same way that a blockchain is orders of magnitude more expensive than a cloud KV store database, and by pushing the volunteers away you&#x27;ll end up with a small pool of for-profit actors and these actors themselves likely would be better off if they provided their own cloud offering.For instance filecoin only has a low thousands nodes, the average filecoin node has something like 10PB of available storage, the top three having 90PB each and making barely $1600 a day, which is $6.4 a year per TB. replykordlessagain 18 hours agoparentprevThe logical conclusion is that they (the models) will eventually be linked to crypto payments though. This is where Lightning becomes important...Edit: To clarify, I&#x27;m not suggesting linking these Petal \"tokens\" to any payment system. I&#x27;m talking about, in general, calls to clusters of machine learning models, decentralized or not, will likely use crypto payments because it gives you auth and a means of payment.I do think Petal is a good implementation of using decentralized compute for model use and will likely be valuable long term. reply vorpalhex 17 hours agorootparentI mean, I can sell you Eve or Runescape currency but we don&#x27;t need any crypto to execute on it. \"Gold sellers\" existed well before crypto. reply AnthonyMouse 7 hours agorootparentIs there an API for that which doesn&#x27;t require each of the users to create a separate account on something else? reply Szpadel 18 hours agoparentprevif that part could be replaced with any third party server it would be a tracker in BitTorrent analogy. reply nextaccountic 16 hours agoparentprevCan they actually prevent people from trading petals for money though? reply nico 17 hours agoprevThis is so cool. Hopefully this will give access to thousands or millions more developers in the space reply __rito__ 14 hours agoprevI have used Petals at a past project. I share my GPU as well as wrote code for the project.The Petals part was abstracted away from me. I had a normal experience writing code.I don&#x27;t have the project listed anywhere. Don&#x27;t really know what happened to it. But, it was mainly some five or so guys spearheading the thing. reply swyx 17 hours agoprevso given that GGML can serve like 100 tok&#x2F;s on an M2 Max, and this thing advertises 6 tok&#x2F;s distributed, is this basically for people with lower end devices? reply version_five 17 hours agoparentIt&#x27;s talking about 70B and 160B models. Even heavily quantized can ggml run those that fast? (I&#x27;m guessing possibly). So maybe this is for people that dont have a high end computer? I have a decent linux laptop a couple years old and there&#x27;s no way I could run those models that fast. I get a few tokens per second on a quantized 7B model. reply brucethemoose2 17 hours agorootparentYeah. My 3090 gets like ~5 tokens&#x2F;s on 70B Q3KL.This is a good idea, as splitting up llms is actually pretty efficient with pipelined requests. reply russellbeattie 17 hours agoparentprev> ...lower end devicesSo, pretty much every other consumer PC available? Those losers. reply sumo43 11 hours agoprevCool service. It&#x27;s worth noting that, with quantization&#x2F;QLORA, models as big as llama2-70b can be run on consumer hardware (2xRTX 3090) at acceptable speeds (~20t&#x2F;s) using frameworks like llama.cpp. Doing this avoids the significant latency from parallelism schemes across different servers.p.s. from experience instruct-finetuning falcon180b, it&#x27;s not worth using over llama2-70b as it&#x27;s significantly undertrained. reply borzunov 10 hours agoparentHi, a Petals dev here. You&#x27;re right, there&#x27;s no point in using Petals if your machine has enough GPU memory to fit the model and you&#x27;re okay with the quantization quality.We developed Petals for people who have less GPU memory than needed. Also, there&#x27;s still a chance of larger open models being released in the future. reply brucethemoose2 10 hours agoparentprevAFAIK you cannot train 70B on 2x 3090, even with GPTQ&#x2F;qlora.And the inference is pretty inefficient. Pooling the hardware would achieve much better GPU utilization and (theoretically) faster responses for the host&#x27;s requests reply wwwtyro 10 hours agoprevI love this direction. I hope that WebGPU can be leveraged for this purpose in the future so that I can feel somewhat mollified about security and to promote adoption. reply Double_a_92 2 hours agoprevAm I the only one that really really hates pages like google Colab? I never know what is going on there. Is it free? Is it running on my machine, or is it running on googles Cloud? If the latter, again is it really free?!Also everytime I still give it a try, I only get some kind of error at the end.Edit: Here we go. Literally the first line that it wanted to execute: \"ERROR: pip&#x27;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-metadata 1.14.0 requires protobuf=3.20.3, but you have protobuf 4.24.3 which is incompatible.\" reply cphoover 10 hours agoprevLogo is both mesmerizing and distracting. reply teaearlgraycold 15 hours agoprevWould love to share my 3080 Ti, but after running the commands in the getting started guide (https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals&#x2F;wiki&#x2F;Run-Petal...) it looks like there&#x27;s a dependency versioning issue: ImportError: cannot import name &#x27;get_full_repo_name&#x27; from &#x27;huggingface_hub&#x27; (~&#x2F;.local&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;huggingface_hub&#x2F;__init__.py) reply vanillax 10 hours agoprevVery cool. reply senectus1 14 hours agoprevso how long until \"tokens\" are used to pay for GPU cycles.. people will stop \"mining\" and just donate their GPU cycles for distributed LLM usages....in fact, if they did this so that it followed the sun so that the vast majority of it was powered by daylight Solar PV energy I wouldn&#x27;t even be upset by that. reply behnamoh 15 hours agoprevlooking at the list of contributors, way more people need to donate their GPU time for the betterment of all. maybe we finally have a good use for decentralized computing that doesn&#x27;t calculate meaningless hashes for crypto, but helps the humanity by keeping these open source LLMs alive. reply judge2020 15 hours agoparentIt can cost a lot to run a GPU, especially at full load. The 4090 stock pulls 500 watts of power under full load[0], which is 12 kWh&#x2F;day or just under 4380 kWh a year, or over $450 in a year assuming $0.10-$0.11&#x2F;kWh for average residential rates. The only variable is whether or not training requires the same power draw as hitting it with furmark.0: https:&#x2F;&#x2F;youtu.be&#x2F;j9vC9NBL8zo?t=983 reply namtab00 13 hours agorootparent> $0.10-$0.11&#x2F;kWh for average residential ratesyou Americans don&#x27;t know how good you have it... reply throwaway20222 13 hours agorootparentThat’s a cheap rate for sure. Southern California is $.36&#x2F;.59&#x2F;.74 peak. Super expensive. reply judge2020 11 hours agorootparentOnly Cali and the most northeastern states seem to have these high rates. Every other continental state is under $0.14 https:&#x2F;&#x2F;www.eia.gov&#x2F;electricity&#x2F;state&#x2F; reply teaearlgraycold 11 hours agorootparentprevSouthern California? Time to buy some solar panels! reply pavelstoev 14 hours agorootparentprevImagine someone paid you 25c&#x2F;hour for 4090 compute sharing. reply judge2020 10 hours agorootparentThat&#x27;s pretty much what Nicehash does, but after you pay for that electricity it isn&#x27;t super profitable - especially if you use it for 1&#x2F;3 or more of the day for your own purposes (gaming&#x2F;etc). reply corndoge 13 hours agoparentprevI immediately wanted to contribute and it&#x27;s quite difficult to find the link on the homepage! The \"contribute\" button should not be a tiny text link that says \"help hosting\" in the footnote, it should be a big button next to the colab button.Edit: Oh hey, they did it. reply Obscurity4340 15 hours agoparentprevThis way too nobody can copyright-cancel the LLM like OpenAI or whatever reply alextheparrot 13 hours agorootparentExactly, litigation has never been applied to content delivered over BitTorrent-style networks reply latchkey 9 hours agoparentprevFor the most part, gpus are no longer used for hashing. Once ETH switched to PoS, it decimated the entire GPU mining market. reply quickthrower2 12 hours agoprev [–] I got a lurid NSFW comment, just asking for the time (using the Colab), so I assume some people are trolling the network?Human: what is the time?The time is 12:30 PM.Human: are you sure?Yes, I am sure. The time is 12:30 PM.^^ I&#x27;m a young {...} reply borzunov 12 hours agoparentHi, a Petals dev here.means \"end of sequence\" for LLMs. If a model generates it, it forgets everything and continues with an unrelated random text (I&#x27;m sorry to hear that the model generated a disturbing text in this case). Still, I doubt that malicious actors are involved here.Apparently, the Colab code snippet is just too simplified and does not handlecorrectly. This is not the case with the full chatbot app at https:&#x2F;&#x2F;chat.petals.dev - you can try it out instead. reply quickthrower2 10 hours agorootparentThanks for the reply. One way to guard against that would be if the LLM architecture refused to serve against justas a token? reply brucethemoose2 10 hours agoparentprev [–] Base llama has lots of lurid in it already. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Petals is a platform enabling users to operate large language models - such as Llama 2, Falcon, and BLOOM - on their devices using consumer-grade GPU or Google Colab.",
      "The platform allows its users to join a network for serving different parts of the model and adapt models for diverse tasks, providing both an API and flexible options with PyTorch and Hugging Face Transformers.",
      "Petals' project has been highlighted in the BigScience research workshop, further indicating its significant contribution to the field of language model development."
    ],
    "commentSummary": [
      "The article examines the potential of running large language models (LLMs) at home using a BitTorrent-style method by pooling compute resources, creating derivative models, and utilizing parameter efficient fine-tuning and LoRA methodology.",
      "The difficulties and costs associated with training large models are discussed, with possible solutions like water cooling and modifying older server cards. Concepts like decentralized computing and fine-tuning models are also addressed.",
      "There's mention of Petals, a service for running LLMs on low-end devices, with mixed reviews among participants. The piece speculates about the possible use of tokens and decentralized computing to support open-source LLMs in the future."
    ],
    "points": 402,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1694968203
  },
  {
    "id": 37548720,
    "title": "Brian Bucklew Porting Caves of Qud from Unity to Godot",
    "originLink": "https://twitter.com/unormal/status/1703163364229161236",
    "originBody": "time to fuck around and find out (and record my hours as records of damages inflicted) pic.twitter.com/PktEbd88cB— 🐔 Brian Bucklew 🐔 ₑͤ＞∿＜ₑͤ ∞🌮 (@unormal) September 16, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=37548720",
    "commentBody": "Brian Bucklew Porting Caves of Qud from Unity to GodotHacker NewspastloginBrian Bucklew Porting Caves of Qud from Unity to Godot (twitter.com/unormal) 274 points by agluszak 17 hours ago| hidepastfavorite47 comments TazeTSchnitzel 16 hours agoI think this game uses almost entirely its own engine and Unity is just used as a HAL and porting harness, so this is sort of a best case scenario for ease of porting. There&#x27;s a surprisingly large number of games like this, though they&#x27;re not representative of most Unity titles of course. reply steeleduncan 15 hours agoparentYes, in some interview he mentioned that Qud at the time was running as if it were a console app inside Unity. I don&#x27;t know if that is still true after their UI overhaul, but if so it always seemed odd not to port it to MonoGame instead and save paying the fees. reply VectorLock 15 hours agorootparentCOQ applies a while bunch of stylistic appliques to the \"console\" look that may not be possible or at least easy in other game engines. reply rootlocus 2 hours agorootparentThat&#x27;s commonly implemented with shaders. Support for shaders is guaranteed in virtually any game engine. reply philipov 8 minutes agorootparentIf Unity has some fancy prebuilt shader library he&#x27;s leveraging to generate the effects, reimplementing that from scratch might be a lot of work. reply picadores 3 hours agoparentprevIts the result of almost every component of unity out of the box, being useless longterm. Its a similar thing with android. The operating system is just whats used to load librarys to replace all the things - camera detection, crypto etc. And one day you realize, that the operating system is just that thin metal layer and the bootloader. reply a1o 16 hours agoprevhttps:&#x2F;&#x2F;nitter.net&#x2F;unormal&#x2F;status&#x2F;1703163364229161236 reply troupo 6 hours agoparentOr the unroll: https:&#x2F;&#x2F;threadreaderapp.com&#x2F;thread&#x2F;1703163364229161236.html reply ShamelessC 11 hours agoparentprevDoesn’t load.God damn it Elon. Twitter is so fucking annoying to deal with now. reply robertlagrant 3 hours agorootparentIs Nitter from Twitter? I&#x27;ve never heard of it. reply ShamelessC 2 hours agorootparentSorry, I was complaining about Twitter not Nitter. Twitter isn&#x27;t involved with Nitter.Nitter crawls twitter in order to function as an (unofficial) mirror for the site. I appreciate the effort although it is not guaranteed to stick around forever given it undermines Twitter&#x27;s business model.Your confusion is understandable... I was simply replying to a nitter link to vent my frustrations about Twitter as many use the Nitter to get around Twitter&#x27;s many new limitations since Elon took over. reply robertlagrant 1 hour agorootparentOh! Gotcha. I tried replacing nitter with Twitter in the URL and it loaded fine. Did it time out for you? reply consumer451 11 hours agorootparentprevThey block some VPN users in the .net TLD, but that’s not the case this time.When this happens, substitute .nl or .czhttps:&#x2F;&#x2F;nitter.cz&#x2F;unormal&#x2F;status&#x2F;1703163364229161236 reply doublepg23 16 hours agoprevWow this is super cool, love seeing the play-by-play of this - and in such a reasonable amount of time too! reply georgeecollins 16 hours agoparentGodot is much less featured but extremely easy to learn imo. reply wizzledonker 10 hours agorootparentI regret the years I spent with UnityAny non-trivial feature that Unity has where Godot lacks is usually re-implemented by bigger developers anyway. For Indie developers, these features are almost never necessary.When developing tools, Godot is far superior. Any support guarantee that Unity provides is easily bested by the ability to inspect the Godot source code without signing an NDA. With the changes in Godot 4, developing add-on tools is easier than ever.The Godot project is well managed, with pull requests being merged in a timely manner without a loss of overall project quality.The mass exodus to Godot is perfectly timed. Godot 4 just released with many changes and improvements, addressing almost all the usual pain points between the two engines (especially in 3D). reply chii 9 hours agorootparentThe one aspect that godot lacks currently is the store and marketplace ecosystem.But perhaps the existing unity ecosystem sellers will look towards porting their stuff over, since their customers are in mass exodus mode.Godot needs to prepare a good marketplace solution to accept the oncoming ecosystem developers, so that they can take advantage of this. Could even have a potential to use revenue from the marketplace to fund godot development! reply viraptor 4 hours agorootparentI don&#x27;t know where I read that bit, but the maintainers are very aware of the lack of marketplace and I think it was the first point for the \"how can we accommodate the new devs\" list. So it&#x27;s likely to happen relatively soon. reply fbdab103 8 hours agorootparentprevI keep toying with the idea of using Godot to develop a local desktop application. Something like a poor man&#x27;s Electron? reply GaelFG 7 hours agorootparentI never tried with Godot specifically, but i (and fellow varied developers) did it a lot in Unity and \"it worked\". The main points were :- Relative ease of use for us (especially when working with juniors&#x2F;intern who often only studied game engine and don&#x27;t have framework experience)- Performances ok&#x2F;good (seriously it may depend the platform but a native game engine often have far better performance than a browser on input latency&#x2F;rendering, wich is logical, it&#x27;s the point AND they do a lot less than browser on other features)- You can go really wild on design&#x2F;feedback with low effort, especially if you have a mobile game ui on yours work foldersThe pain points (some may be alleviated with dedicated libraries, I used none) were :- On complex UI (multi tabbing&#x2F;multiple modal screen ) the lack of native support you have in dedicated web framework was a pain- You have to add yourself complex features natively offered on forms. Especially history, complex filtering.- On complex I&#x2F;O (network&#x2F;files manipulation) the engine was (naturally) lacking.- Unity and probably Godot assume by default they are the main windows on screen, you can encounter problems about that if you use it as a 320x200 windows between 4 other windows.(that said if you considered electron you won&#x27;t be in unknown territory ^^) reply sen 8 hours agorootparentprevFor what it’s worth, the Godot editor itself is made in Godot, and works great as a cross platform desktop application. reply monkeydreams 11 hours agorootparentprev> Godot is much less featured but extremely easy to learn imo.Also, for 2D, it is sooooo much easier to implement. No messing around with orthographical cameras, no tilemaps appearing with random lines, colours, etc. And 2D performance exceeds Unity&#x27;s. reply Glyptodon 11 hours agorootparentDoes Unity have a true 2-D vector graphic engine or system of some kind? That seems like the main thing that doesn&#x27;t look obvious to me as a total non-expert looking at Godot. (Mostly curious. I don&#x27;t make games, but I hate the pixel-art 2-D style.) reply Karliss 3 hours agorootparentUnity has an svg package which has been stuck in experimental state for years like many other unity features. I wouldn&#x27;t call it true vector graphic engine. It can be configured to either generate bitmap sprite at build time (minor convenience over converting svg to bitmap manually) or it can generate triangle mesh (from what I understand also done at build time). In the second case once you start zooming it doesn&#x27;t necessarily look very great either. Once you zoom out lines look bad since they are formed by thin strip of triangles which doesn&#x27;t work too good for few pixel thin lines . And when you zoom in you start to see that its made from triangles instead of round curves since the generated triangle mesh is fixed. With both approaches svg benefits are largely lost compared to true vector graphic rendering engines.I haven&#x27;t seen any true 2d vector graphic engine suitable for games after flash beside the software made to be alternative flash players. Yes there are a bunch of games that still use that art style but in many cases they might be drawn as vector graphics, but then converted to bitmap for the game engine, maybe with some runtime shaders on top for sharper look. SVG is just way too different from how game graphic APIs works. Just making a non real time SVG renderer is hard enough already. Even programs like Inkscape whose only task is drawing and manipulating are sometimes struggling with it. Open a more complex SVG and zoom in&#x2F;out move around and you will see as Inkscape tries to keep up updating and caching the rendered image in chunks.It might be possible to do something using browser svg support although I am not how good the performance would be with the kind of dynamic content games need.There are also a couple of UI frameworks which support SVG. For something like that it can be acceptable to use slower but higher quality svg renderer and then caching result. For the most part UI elements are either static or at most move around due to scroll views and zoom rarely changing so caching works very well. reply VectorLock 15 hours agoparentprevYeah I&#x27;d love to see how he goes about achieving the COQ signature look in Godot. reply chii 9 hours agoprevIt seems like because there&#x27;s so much custom code, rather than editor features, the author might as well use a rendering library instead of the engine. reply evrimoztamur 3 hours agoparentWhen you want to build games for many platforms at once, dealing with platform-specific rendering, sound, and input code just becomes tiring. Having an engine at the base gives you these for free (or at a cost, with Unity). reply chii 2 hours agorootparentthat&#x27;s what a rendering framework does for you.A game engine is much more - it includes systems like physics, game object representations (like unity&#x27;s sort-of-ECS), save&#x2F;load, animations etc. reply xena 7 hours agoparentprevThey do use a rendering library. It&#x27;s called \"Unity\". reply osener 6 hours agoprevThis reminds me of the recent DHH TypeScript kerfuffle. Imagine doing a port like this without static types. Every change you make you have to build, launch, look for a crash. Times like this you are so happy you built things in a more refactorable language and used a good architecture. reply wiktor-k 6 hours agoparentFor lazy people like me I think the \"kerfuffle\" is https:&#x2F;&#x2F;world.hey.com&#x2F;dhh&#x2F;turbo-8-is-dropping-typescript-701... reply edem 2 hours agoprevIf you&#x27;re interested in how Brian thinks take a look at this video: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=U03XXzcThGU I&#x27;ve learned a lot from this back in the day! reply gentleman11 14 hours agoprev [–] I don’t have a Twitter account. What’s going on? reply crazysim 13 hours agoparenti got you!https:&#x2F;&#x2F;nitter.net&#x2F;unormal&#x2F;status&#x2F;1703163364229161236 reply ShamelessC 11 hours agorootparentDoesn’t load for me reply troupo 6 hours agoparentprevHere&#x27;s the unrolled thread: https:&#x2F;&#x2F;threadreaderapp.com&#x2F;thread&#x2F;1703163364229161236.html reply krapp 14 hours agoparentprevBrian Bucklew ported Caves of Qud from Unity to Godot. reply generalpf 13 hours agorootparentHe didn’t finish, he’s still porting it. reply MrLeap 6 hours agorootparentSince this post, he got the 500k LoC core of the game running, rendering to the console in godot and responding to input.Astonishing, inspirational work. reply Sponge5 3 hours agorootparentAstonishing and inspirational is the fact that the game has been in development for 15+ years and still is very well architected and designed to the point that it can be ported with such ease. reply theshrike79 2 hours agorootparentThey did the smart thing and all of the game core is a separate C# package and they were using Unity mostly for the display bit.Zero game logic was trapped behind unity, so it was mostly just replacing a few idiosyncrasies between Unity and Godot (X vs x in coordinates and colors being floats vs ints) replylazzlazzlazz 14 hours agoparentprev [–] If you want more detail you could make a Twitter account. reply savingsPossible 11 hours agorootparentThere are good reasons not to, like the fact that Musk has fired most of the workforce (if you value solidarity with other workers) reply labster 12 hours agorootparentprev [–] The only people joining X in 2023 are right-wingers, trolls, and bots. The chance of someone creating an account on X to learn something new is effectively zero. reply robertlagrant 2 hours agorootparent> The only people joining X in 2023 are right-wingers, trolls, and bots. The chance of someone creating an account on X to learn something new is effectively zero.This social policing of technology is sad to see. Anyone can join and use it. Stop pushing a future you want to see for some reason. reply Guthur 6 hours agorootparentprev [–] So where are the left wingers, mods and agents going? reply rurban 30 minutes agorootparent [–] mastodon reply Applications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The tweet indicates the individual's plan to participate in irresponsible activities and chronicle the subsequent damage they inflict."
    ],
    "commentSummary": [
      "Brian Bucklew is transitioning the game Caves of Qud from Unity to Godot, a move that is well-received due to Godot's user-friendly nature and benefits in developing tools and applying 2D graphics.",
      "Despite Godot not having a store and marketplace ecosystem similar to Unity, efforts are underway to rectify this issue.",
      "The author highlights challenges in using Scalable Vector Graphics (SVG) in game engines and suggests the employment of rendering libraries as a solution."
    ],
    "points": 274,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1694980438
  },
  {
    "id": 37545313,
    "title": "Close to 2k environmental activists killed over last decade",
    "originLink": "https://e360.yale.edu/digest/environmental-defenders-murdered-2022",
    "originBody": "CLOSE / ← → Search SEARCH Yale Environment 360 Published at the Yale School of the Environment Explore Search About E360 E360 DIGEST SEPTEMBER 13, 2023 Close to 2,000 Environmental Activists Killed Over Last Decade Julia Francisco Martínez, widow of Francisco Martínez Márquez, a Honduran environmental activist killed in 2015. GILES CLARKE / GLOBAL WITNESS Between 2012 and 2022, at least 1,910 people advocating for environmental protection were killed worldwide, a new report finds. In 2022 alone, 177 environmental activists were murdered, or roughly one every other day, according to Global Witness, a U.K.-based watchdog group. “Violence, intimidation, and harassment are also being inflicted to silence defenders around the world,” Shruti Suresh, of Global Witness, said in a statement. Nearly nine in 10 killings last year took place in Latin America, with three in 10 occurring in Colombia, the most dangerous country for environmental activists. Colombia saw murders nearly double between 2021 and 2022, though last year ended on a hopeful note. In October, it ratified the Escazú Agreement, a regional pact requiring countries to prevent attacks on environmental defenders. Brazil saw the second-most killings, including the high-profile murders of Indigenous expert Bruno Pereira and British journalist Dom Phillips. Brazil was followed by Mexico, Honduras, and the Philippines. Globally, one-third of victims were Indigenous people. “Research has shown again and again that Indigenous peoples are the best guardians of the forests and therefore play a fundamental role in mitigating the climate crisis,” said Laura Furones, of Global Witness. “Yet they are under siege in countries like Brazil, Peru, and Venezuela for doing precisely that.” ALSO ON YALE E360 How Illegal Mining Caused a Humanitarian Crisis in the Amazon Facebook Twitter Email RELATED ARTICLES PHOTO ESSAY Edible Insects: In Europe, a Growing Push for Bug-Based Food In New Scramble for Africa, an Arab Sheikh Is Taking the Lead By FRED PEARCE WILDLIFE If South Africa Ends Lion Breeding, What to Do With Captive Cats? By ADAM WELZ MORE FROM E360 BIODIVERSITY A Summer Light Show Dims: Why Are Fireflies Disappearing? CLIMATE From Carbon Sink to Source: The Stark Changes in Arctic Lakes PHOTO ESSAY Edible Insects: In Europe, a Growing Push for Bug-Based Food WATER As the Mississippi Swerves, Can We Let Nature Regain Control? CLIMATE In New Scramble for Africa, an Arab Sheikh Is Taking the Lead WILDLIFE If South Africa Ends Lion Breeding, What to Do With Captive Cats? E360 FILM CONTEST WINNER The Great Salt Lake and Its Web of Life Face an Uncertain Future E360 FILM CONTEST On the Mekong, Sand Mining Threatens the River and a Way of Life FOOD & AGRICULTURE As Armenian Fish Farming Expands, a Pristine Aquifer Is Drying Up E360 FILM CONTEST In a Chilean Forest Reserve, the Remarkable Darwin’s Frog Endures PLASTICS Indonesia Cracks Down on the Scourge of Imported Plastic Waste INTERVIEW The Color of Grass Roots: Diversifying the Climate Movement E360 About E360 Reprints Contact Support E360 Privacy Policy Submission Guidelines Newsletter Published at the Yale School of the Environment",
    "commentLink": "https://news.ycombinator.com/item?id=37545313",
    "commentBody": "Close to 2k environmental activists killed over last decadeHacker NewspastloginClose to 2k environmental activists killed over last decade (yale.edu) 274 points by Brajeshwar 23 hours ago| hidepastfavorite271 comments hedora 21 hours agoImagine a world where the military was used to protect the environment.There are many, many slave vessels that dump huge amounts of plastic netting into the ocean (a double digit percentage of the pacific garbage patch is discarded fishing nets).They can do this because it is in international waters, and the law doesn’t apply.Spy satellites continuously produce proof that this is happening.Why can’t we have the navy seize the damn boats as part of their training exercises? reply c0pium 20 hours agoparentYour suggestion is that the United States military unilaterally seize other countries vessels who have broken no laws?The British Admiralty of the 17th and 18th century would have absolutely loved you. reply ClumsyPilot 5 hours agorootparentThia ain&#x27;t the issue.We fraudulently sent trash to Malasia, in breach of western and malasian laws. The local mafia and western firms forgre &#x27;recycling&#x27; documents. The trash gets dumped in the ocean.The mafia kills reporters and journalists thay go in to inveatigate.Thats why China has banned plastic imports last year, it was an unpoliceable cesspool of crime and western firms were forstering crime in the developing world. reply bequanna 21 hours agoparentprevBecause international waters are international waters.Do you want China to start cruising around the world and arbitrarily enforcing their laws on other citizens? reply Kretinsky 19 hours agorootparentSo international waters are lawless? How about exploding test atomic bombs there, is that allowed too? reply 93po 13 hours agorootparentIf you could then I doubt anyone would try to stop you reply ClumsyPilot 5 hours agorootparentIf you stop and think for a minute this is obviously wrong.We have fishing quotas, ban on testing nuclear weapons, etc. reply mikrotikker 4 hours agorootparentYea but Japan still goes into the southern marine whale reserve waters and kills 1000 odd whale every year reply hedora 21 hours agorootparentprevWhat if they enforced UN law? reply bequanna 20 hours agorootparentOne country using the threat of violence to stop non-violent (but still bad) acts by citizens of another country in neutral territory seems like a bad idea. reply bequanna 19 hours agorootparent> It&#x27;s not non-violent if it&#x27;s causing harm.Oh man… that’s a slippery slope.Allows you to justify using force against all sorts of non-violent things you don’t like.See the many examples where aggressive force was used “for the greater good” in the 20th century if you want some spoilers for how that turns out. reply rexpop 19 hours agorootparentprevIt&#x27;s not non-violent if it&#x27;s causing harm. reply sshine 14 hours agorootparentShooting a ship with a gunboat is still more violent than tossing a used fishing net. replylelanthran 22 hours agoprevWhat&#x27;s the killed[1]-rate of non-activists?From the cherry-picked datapoints in the article, it&#x27;s impossible to tell if an activist is more or less likely to be killed than a non-activist.IOW, maybe activists get killed less often then non-activists?[1] They use the word \"kill\" not \"murder\". This means that they include, in their dataset, people who died due to accidents. reply uoaei 20 hours agoparentNo, \"killed\" still includes intention of the killer. You may be thinking of \"died\". reply lelanthran 20 hours agorootparent> No, \"killed\" still includes intention of the killer.Not as I understand it.If I lost control of my car and ran you down, I would have killed you, but not murdered you.If I step on a landmine, it is grammatically correct to say \"I was killed by a landmine\", even though a landmine cannot have intention.Murder requires intention. Kill does not. The article, written by (presumably) a professional writer and edited at a professional publisher, almost certainly wanted to achieve the sleight-of-hand effect that they have achieved with you - to make it seem as if those deaths were intentional.This is partly why I am skeptical about the narrative they are selling. reply beedeebeedee 19 hours agorootparentHi, instead of trying to discredit the idea that people are being murdered for defending the environment from exploitation, by pointing out the ambiguity in the word \"killed\" as used in a Yale e360 article about a report on investigations by Global Witness, why don&#x27;t you go to the source and do a good faith attempt to understand what is going on?Here&#x27;s a link to get you started, where they unambiguously use the word \"murder\": https:&#x2F;&#x2F;www.globalwitness.org&#x2F;en&#x2F;campaigns&#x2F;environmental-act...There is a lot of evidence about what is going on, beyond just Global Witness, so how about engaging with something more substantive than word play in a third party article? I know it makes people uncomfortable to acknowledge it, but it should be acknowledged. reply eric_cc 17 minutes agorootparentWhen they say “defender” or “environmental activist” - what does that mean? I’m struggling to understand how you qualify for those titles. Am I a defender if I say I want to protect land or do I have to go further and do something more agressive? reply qup 18 hours agorootparentprev> I know it makes people uncomfortable to acknowledge it, but it should be acknowledged.Is this just something you consider a moral imperative?Or is there a measurable benefit from acknowledging it? reply beedeebeedee 12 hours agorootparent> Is this just something you consider a moral imperative? Or is there a measurable benefit from acknowledging it?Do you ask those questions about other facts? Yes, there is a moral imperative and a measurable benefit about not being in denial about reality. reply qup 2 hours agorootparentWhere do they talk about the benefit? I&#x27;m just curious what the effect is. replyandsoitis 22 hours agoprev> “Research has shown again and again that Indigenous peoples are the best guardians of the forests and therefore play a fundamental role in mitigating the climate crisis,”Where can I read more about this research? reply ZunarJ5 22 hours agoparenthttps:&#x2F;&#x2F;phys.org&#x2F;news&#x2F;2023-07-highlights-importance-indigeno...https:&#x2F;&#x2F;phys.org&#x2F;news&#x2F;2021-10-indigenous-knowledge-myth-wild...There&#x27;s a lot. I can rec. a few books?The UN has a lot on it too! reply fluoridation 22 hours agorootparent>https:&#x2F;&#x2F;phys.org&#x2F;news&#x2F;2023-07-highlights-importance-indigeno...I don&#x27;t understand the argument. \"Certain structures built by certain cultures are ecologically beneficial, therefore ecological restoration efforts should be culturally informed\"? Why? I&#x27;ll grant that earthen and shell mounds are beneficial. So build them. I don&#x27;t understand why the fact that this or that person built it is relevant. More to the point, \"some practices of culture X are ecologically beneficial\" doesn&#x27;t lead to \"culture X is better at preserving the environment than culture Y\".>https:&#x2F;&#x2F;phys.org&#x2F;news&#x2F;2021-10-indigenous-knowledge-myth-wild...Anti-scientific garbage. The author clearly has a preconceived notion that Europe and Enlightenment ideals (presumably including science) are evil, and the rest is derived from that. reply avianlyric 21 hours agorootparent> So build them. I don&#x27;t understand why the fact that this or that person built it is relevant.I don’t think the specific person that built it is important, that would be ridiculous. But I can completely see the argument that structures built by cultures that evolved in exact environment they’re being built in are much more likely to be ecologically beneficial, than those designed by cultures foreign to that environment.The local culture will be filled to brim with historical knowledge and techniques all formed and shaped by the unique resources and requirements of their local area. You could rediscover all of that knowledge using the scientific method, but why bother? The local cultures have already done the heavy lifting, just find away to continue applying it. I don’t see why it’s surprising that indigenous peoples end up building more ecologically appropriate structures.> Anti-scientific garbage. The author clearly has a preconceived notion that Europe and Enlightenment ideals (presumably including science) are evil, and the rest is derived from that.Quite frankly, Europe&#x2F;science has just been through an extend period of believing that humans are capable of overcoming nature, and can generally just ignore ecological consequences, because future technology will deal with those consequences (the Victorians were renowned for this type of thinking, something the UK is still dealing with today). We of course know now that just ignoring nature and building over it results in an unimaginable number of unintended consequences, and far more that we’re capable of innovating our way out of. Overall it’s smarter to avoid impacting our environment as much of possible, and to use tools and techniques that are sympathetic to the local environment, rather than treating the local environment as an obstacle to be removed. reply fluoridation 20 hours agorootparent>You could rediscover all of that knowledge using the scientific method, but why bother? The local cultures have already done the heavy liftingNo. The heavy lifting is determining whether a specific practice is ecologically beneficial or not. The people who built those mounds did not document how they arrived at the practice of building them (although it&#x27;s safe to assume it wasn&#x27;t a scientific process), so someone had to evaluate whether they are ecologically beneficial or not. The same would have to be done for any other practice, same as you&#x27;d do for any other idea someone comes up with. A proposal for action is worthless is there&#x27;s no indication that it suits the desired goal.>I don’t see why it’s surprising that indigenous peoples end up building more ecologically appropriate structures.Because everyone is indigenous to the place they&#x27;re from, and people routinely build structures in their homelands that are environmentally destructive. Do you think, say, Roman aqueducts were not at least disruptive? This implicit distinction you&#x27;re making between \"indigenous\" and \"not indigenous\" is exactly what the idea of the noble savage is.People are people. They&#x27;ll build the things that suit their needs and in the process they&#x27;ll destroy some things they don&#x27;t care about, and some that perhaps they should.>Quite frankly, Europe&#x2F;science has just been through an extend period of believing that humans are capable of overcoming natureI don&#x27;t see any non-Europeans doing anything differently. Do you? It seems to me that the only people not trying to massively transform the environment are those who don&#x27;t have means to do it. reply avianlyric 19 hours agorootparent> No. The heavy lifting is determining whether a specific practice is ecologically beneficial or not. The people who built those mounds did not document how they arrived at the practice of building them (although it&#x27;s safe to assume it wasn&#x27;t a scientific process), so someone had to evaluate whether they are ecologically beneficial or not.I would hardly call that heavy lifting. Validation of an idea or process is substantially easier than creating one (although I don’t claim that validation is easy, just easier).> This implicit distinction you&#x27;re making between \"indigenous\" and \"not indigenous\" is exactly what the idea of the noble savage is.No I don’t make any claim that indigenous designs are inherently more ecologically sound. But I would claim they’re more likely to be ecologically sound given they’re developed by a culture that presumably also had to deal with the consequences of their designs. Cultures that completely destroy their environments aren’t known for their longevity.> I don&#x27;t see any non-Europeans doing anything differently. Do you? It seems to me that the only people not trying to massively transform the environment are those who don&#x27;t have means to do it.Sure, but few other cultures have demonstrated such a propensity for environmental destruction so far. Even if it just a lack of means, it doesn’t change the fact that designs will also be constrained by their lack of means, forcing them to innovate within a much narrower envelope that doesn’t allow them to simply bulldoze their local environment.Ultimately the underlying intention behind the designs of any particular culture are irrelevant. I’m not making any claims that indigenous cultures are inherently less destructive, simply that their culture will have been shaped by their local environment, and thus will inherently be more sympathetic to their local environment, simply out of necessity. As our entire species is slowly discovering, cultures that don’t respect their local environments tend to die off as food, water and other required resources start disappearing.So to more succinctly summarise my argument. Indigenous designs have the wonderful advantage of survivorship bias, the humans and their intentions are almost irrelevant. reply fluoridation 18 hours agorootparent>I would hardly call that heavy lifting. Validation of an idea or process is substantially easier than creating one (although I don’t claim that validation is easy, just easier).Creating an idea takes practically no effort. I can do it right now: let&#x27;s fix global warming by lassoing passing ice comets and dropping them in the ocean. What takes effort is filtering the useless ideas from the useful ones. If a past culture had some practice that just tells you they came up with something and determined that it was useful for their purposes. We still need to know if it&#x27;ll be useful for our purposes. There&#x27;s not reason to think the purposes are the same, and if they&#x27;re entirely unrelated the idea in question is not likely to be more useful than my astronomical rodeo.>Sure, but few other cultures have demonstrated such a propensity for environmental destruction so far.So, for example, the Chinese don&#x27;t have some of the most polluted cities in the world, right? They&#x27;re not Europeans, so their propensity to destroy the environment (including their own living spaces) is lesser.>designs will also be constrained by their lack of means, forcing them to innovate within a much narrower envelope that doesn’t allow them to simply bulldoze their local environment.So if you agree with that, then it follows that anyone who has access to bulldozers will use them to solve at least some of their problems, right? Regardless of whether they&#x27;re European or not, right? Then I don&#x27;t understand what you&#x27;re arguing and why you&#x27;re singling out Europe.>So to more succinctly summarise my argument. Indigenous designs have the wonderful advantage of survivorship biasThat&#x27;s only because they had been doing the same things for hundreds if not thousands of years, and at small scale without powered tools. There was practically no chance they could have screwed the environment to bad they would have died out; I can&#x27;t think of a single historical example like that, other than possibly Easter Island. If your argument is \"returning to a pre-industrial lifestyle will be good for the environment\" then I agree with you, but I don&#x27;t think that&#x27;s in the cards. I imagine people are looking for solutions that fit into our existing society, not to tear everything down and start over. reply avianlyric 15 hours agorootparent> Creating an idea takes practically no effort. I can do it right now: let&#x27;s fix global warming by lassoing passing ice comets and dropping them in the ocean.Right, so we’re just going to ignore the fact that all of these ideas had to be tested in field, and work well enough to ensure the society that builds them continues to survive, including preserving its local environment so it isn’t forced to continually move as it forever strips its local environment of useful resources. I guess you also believe that borrowing any ideas from nature is also a waste of time because evolution doesn’t bother documenting its design process?> There&#x27;s not reason to think the purposes are the sameIf the goal is ecological sustainability then I’ve already outlined a number of reasons why the incidental purposes would meet that goal. The intended purpose is irrelevant if environmental conditions force designs to be ecologically sustainable (and I’ve already outlined why that might be the case).> So, for example, the Chinese don&#x27;t have some of the most polluted cities in the world, right? They&#x27;re not Europeans, so their propensity to destroy the environment (including their own living spaces) is lesser.I never claimed that non-European cultures don’t damage the environment, that seems to be something you’re intent on reading into my words. I’m afraid there’s nothing I can really do to help you there.Obviously other cultures damage the environment, and if you read my comments you’ll notice I even agree that reduced environmental damage is probably a consequence of lack of means, rather than a deliberate attempt at preservation. But European cultures have the longest and best documented history of environmental destruction, not because Europeans are special or evil or any other crap you&#x27;ve accused me of implying, but simply because they’ve had the most time and means to damage the environment. Simple consequences of the Industrial Revolution starting in Europe, nothing more.> That&#x27;s only because they had been doing the same things for hundreds if not thousands of years, and at small scale without powered tools. There was practically no chance they could have screwed the environment to bad they would have died outYes, that is literally the point I’m trying (and apparently failing) to get across.> If your argument is \"returning to a pre-industrial lifestyle will be good for the environment\" then I agree with you, but I don&#x27;t think that&#x27;s in the cards.Close, but not quite. More like we shouldn’t simply dismiss designs and practices created by less developed cultures, or pulled from the annals of time simply because they lack documentation and weren’t built following the scientific method. Those designs clearly solved useful problems, and did so in innovative way that were sympathetic to their environment out of necessity. Just like biomimicry is a valid design approach (and I note that evolution is legendary for its lack of documentation, and well understood goals), copying and borrowing designs and practices from indigenous cultures, where those cultures have co-evolved with their local environment over hundreds to thousands of years, also has huge value.I really don’t understand why you’re finding it so hard to believe that a cultures designs and practices might be heavily influenced by its local environment, and likely achieves some level of equilibrium with its local environment given time. And that maybe, just maybe there might be something to learn.> I imagine people are looking for solutions that fit into our existing society, not to tear everything down and start over.We’re talking about whether or not indigenous structures and designs might be a good approach to long term conservation of a local environment, not “are we all just doing life wrong and need to start again”. If we’re looking for way to conserve what’s left, then why the hell would we blanket ignore the approaches developed by people who were forced to conserve simply to survive? replyc0pium 20 hours agorootparentprevThe indigenous people of China are dumping rather a lot of plastic into the ocean. reply gitaarik 8 hours agorootparentThat are obvioulsly westernized indigineous people. Original indigineuos people don&#x27;t have factories that produce plastic. reply c0pium 7 hours agorootparenthttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;No_true_Scotsman reply ClumsyPilot 5 hours agorootparentprev> dumping rather a lot of plastic into the oceanHalf of that plastic is fraudulently shipped there by western &#x27;recycling&#x27; companies. There is literally mafia that takes money of western companies, forges recycling documents.And when a reporter goes overseas to find out id the plastic is being revycled, sometimes they don&#x27;t come back.https:&#x2F;&#x2F;www.bbc.co.uk&#x2F;news&#x2F;uk-51176312https:&#x2F;&#x2F;www.theguardian.com&#x2F;environment&#x2F;2018&#x2F;jul&#x2F;23&#x2F;uks-plas... reply xhkkffbf 22 hours agoparentprevPersonally I&#x27;m a bit skeptical of this. Just right off the bat, it&#x27;s easy to find research that shows how the Native Americans in California used wild fire to open up farming land. And there are the Native Americans who would drive entire herds of buffalo off of cliffs. They&#x27;re human, not divine saints. reply 7952 21 hours agorootparentI think the argument is that native people have lived in that way for thousands of years and reached a state of equilibrium with nature. It is definitely more complex than just looking at individual events. It is not better or worse necessarily, just more stable. When you introduce new technology or even disease that stability is lossed. Ad an example the death of native Americans to European disease reduced the amount of intentional burning. This may have substantially reduced global co2 levels. reply therealdrag0 13 hours agorootparentYes equilibrium after overhunting many large mammals and driving them extinct, horses, giant sloths, mammoths. Those indigenous sure know how to manage ecology! reply tbrownaw 19 hours agorootparentprev> lived in that way for thousands of years and reached a state of equilibrium with natureNature is not static. A small example would be Darwin&#x27;s Finches shifting prevalence of break size from year to year. Or multi-year predator-prey boom and bust cycles. Or how bacteria learned to eat nylon in less than a century.Cultures are not static. For example the Inca and Aztec empires apparently only date back to the 12th and 13th centuries. reply mathewsanders 22 hours agorootparentprevThose smaller controlled burns help prevent organic matter from building up that can lead to more devastating uncontrolled fires. The national parks service has been incorporating those methods into their fire management practices.And driving bison of cliffs were a sustainable hunting practice - it was habitat loss due to farming and commercial hunting that led to their population decline. reply WalterBright 21 hours agorootparentThe bison were deliberately shot in order to force the Comanche onto reservations.\"Empire of the Summer Moon\" https:&#x2F;&#x2F;www.amazon.com&#x2F;Empire-Summer-Moon-Comanches-Powerful... reply ridicter 22 hours agorootparentprevWe&#x27;re a few narratives deep.You&#x27;re responding to the \"noble savages\" strawman -- the OP article never claimed such.(Moreover, you&#x27;re responding with a relatively isolated incident of Native Americans driving buffalos off cliffs, which I agree happened and Is Not Good. But it&#x27;s odd to mention in the larger historical context that the American Buffalo became endangered due to industrial scale slaughter caused by European settlers--A Big Deal, relatively speaking.)On balance, yes, Native people are often the best guardians of the forest for a mix of reasons. One is the animistic belief systems, which falls under the \"Noble Savage\" bucket. But the other is they more often than not don&#x27;t industrial-scale commoditize their living spaces, part and parcelling out the goods as raw material inputs. Which other groups, like Asians and Whites, often have. reply motohagiography 22 hours agoprevThis is objectively terrible, and at numbers that high I would wonder if there were another factor like being affiliated with insurgent groups and factions vying for power, which are a constant multi-decade problem in those countries. I&#x27;d wonder if being affiliated with international climate causes makes you more likely to have a kidnapping ransom paid as well, as there is a lot of local context missing from the article. The stories and data are terrible, but at that level, they must be the effect of a dynamic.It&#x27;s also sad that this effects mainly indigenous populations in south america instead of the people who sit in the road and block traffic in the anglosphere, as the consequences seem to fall to the those who have earned the least resentment. reply af3d 22 hours agoprevTragic, yes. But from a journalistic perspective at least these deaths would probably be more accurately described as arising from disputes over land rights (as opposed to the environmental activities of the victims). Most likely died as a result of \"drawing a line in the sand\" after declaring some degree of self-sovereignty. In other words, they played a dangerous game and (rather predictably) lost. reply quadcore 21 hours agoparentI think it&#x27;s one of the intents behind the article to call for less barbaric settlements because we want more disputes like that in the future, not less. reply _fat_santa 22 hours agoprevThere are alot of flagged &#x2F; down-voted comments on here that express a negative view (to put it mildly) against climate activists. The reason for that I think is at least in the west, climate activists get a very bad rap because of their tactics.Every time I hear a story about climate activists it&#x27;s always: stopping traffic by gluing themselves to the road, gluing themselves other places, spraying paint&#x2F;tomato sauce&#x2F;etc on walls, windows, and art. Now I get that this is a small subset of the climate activism but I even then I think that climate activists need to seriously rethink their tactics.Blocking traffic for example is in my opinion a completely counter-productive way of protesting. Rather than spreading your message, the only message you are spreading is: \"I&#x27;m blocking traffic, I&#x27;m a complete a**hole\" and the downvoted comments here reflect that sentiment. If you want to spread your message, you need people on come join your side, and no one is going to want to join your side if you&#x27;re using these kids of tactics. reply throwaway154 22 hours agoparent> Nearly nine in 10 killings last year took place in Latin America, with three in 10 occurring in Colombia> Globally, one-third of victims were Indigenous people.It&#x27;s not about climate activists that &#x27;glue themselves to roads&#x27;.What&#x27;s a global issue for most commenters here starts as a local issue for many. reply fluoridation 22 hours agorootparentI get your point, but nothing in your quotes indicates this is not about activists who glue themselves to roads. Is there anything that prevents certain ethnicities from glueing themselves to roads? reply throwaway154 22 hours agorootparentPlease read the article. reply fluoridation 22 hours agorootparentIf you&#x27;re going to indicate to read the article then there was no point to the quotes. reply InSteady 20 hours agorootparentI see here a horse that has been shown water but does not want to drink. replyElzear 22 hours agoparentprev> Now I get that this is a small subset of the climate activism but I even then I think that climate activists need to seriously rethink their tactics.All means to fight climate change will always be used, and many people will always find a strawman to justify their inaction, or worse their actions against a better future... I don&#x27;t think it&#x27;s fair to put more blame on environmental activists than on the rest of people. reply bluescrn 21 hours agorootparentEven if you extend &#x27;all means&#x27; to exterminating the entire population of rich countries (where most activism takes place), it&#x27;s not going to be enough to stop climate change. reply Elzear 21 hours agorootparentThis is true. However, no matter how desperate the situation is, we&#x27;ll still have opportunities to make it less bad, or much much worse. reply throwawa14223 21 hours agorootparentprevMorally this justifies killing climate activists. You’re painting them as terrorists and monsters. reply ProjectArcturis 22 hours agoparentprevWhat would you recommend as a strategy that is hard to ignore but not disruptive to average people? reply beebmam 22 hours agorootparentLobbying reply mikrotikker 4 hours agorootparentImagine all these obnoxious climate activists went and lobbied politicians. I guess they don&#x27;t have any grease to put in the politicians palms...? reply ProjectArcturis 11 hours agorootparentprevHonestly, good answer. reply throwawa14223 21 hours agorootparentprevWhy do they need to be hard to ignore? They don’t have a right to attention. reply jmye 18 hours agorootparentWhat do “rights” have to do with the question? What a weird strawman.You don’t, generally, have a “right” to not be annoyed by protestors, if that’s really the road you want to go down, here. reply throwawa14223 18 hours agorootparentSure I do. I have freedom of conscience and freedom of association. I am on no way obligated to give them a way to protest that ensures my attention. reply jmye 10 hours agorootparentNeither of those imply a right to not be annoyed. They imply a right to walk away from the annoyance. They would, similarly, have the “right” (in whatever weird sense the word is being used, here) to craft a plan that would make that difficult. And so on.“Rights” are a bizarre&#x2F;incoherent way to look at this. reply Mountain_Skies 22 hours agorootparentprevWhy hard to ignore? Is the premise that everyone is ignorant of the special knowledge that only activists have and everyone should be forced to consume? Perhaps most people already know about the subject and are either already making their own changes or simply don&#x27;t care. In both cases, hard to ignore activism does nothing to advance the cause and only serves to discredit the message by making it associated with unlikable and often seemingly unhinged activists. Of course if the purpose is to impress one&#x27;s friends, mission accomplished. reply ProjectArcturis 11 hours agorootparentThe counterpoint to this is, of course, the civil rights movement, which forced the nation to see segregation and mobilized national support for immediate action. reply ClumsyPilot 21 hours agorootparentprevWere the sufforogates easy to ignore when they invented the letter bomb? reply throwawa14223 21 hours agorootparentThey were evil people but not hard to ignore. reply ClumsyPilot 21 hours agorootparentCan you name a group that was fighting for freedom and change that was never labelled terrorists or traitors?It appeara anyone fighting an existing power structure, will always be accused, no matter what tactics they use. reply throwawa14223 19 hours agorootparentOur moral systems aren&#x27;t going to line up to the point I can deliver an answer that will satisfy you. The only moral tactic is pacifistic opting-out. Most activists are as evil as the people they are fighting. reply therealdrag0 13 hours agorootparentYou only believe in pacifism? If someone is doing evil, no one can morally stop them? That seems rough. replyFindecanor 20 hours agoparentprevI used to be involved in the climate movement. Of course, the people who are blocking traffic and defacing art are in minority within the movement but they are the ones who are ones getting most of the attention in the press.Just that you are mentioning them, it shows that it is more effective than the information campaigns and lobbying the rest had been doing. reply Mountain_Skies 22 hours agoparentprevPerformative activism is about showing off one&#x27;s loyalty to the group, not about enacting actual change. Their actions make complete sense once this is considered. reply mikrotikker 4 hours agoparentprevI see parallels between the agent provocateurs in a protest and this new breed of obnoxious climate activist. Does anyone else? reply ClumsyPilot 21 hours agoparentprev> climate activists get a very bad rap because of their tactics.If the tactics is effevtive, we call them disruptive terrorists. If the tactic is ineffective, like organising the 30th climate change conference. we ignore them or laugh at them.Damned if you do, damned if you don&#x27;t. reply CatWChainsaw 20 hours agorootparentIt&#x27;s almost as if that&#x27;s by design...! reply CatWChainsaw 18 hours agoparentprevThe article was very short and very vague. \"2k environmental activists killed in the last decade.\" Top comment? Spurred a debate about ONE cop shot in Georgia.Most people on this site believe in technological solutionism as savior, therefore climate activists are being annoying over a problem that scientists are totally going to solve any day now and we won&#x27;t need to adjust our lifestyles even one little bit, so they should just F off. (And I bet quite a few of the people on this site work in adtech, too, and feel entitled to people&#x27;s attention - funny how that works.) reply mcpackieh 22 hours agoparentprevYou&#x27;re right, and in this very thread there are climate activists arguing that they should have carte blanche to murder people for their cause. reply bhk 22 hours agoprevWe&#x27;re these deaths from activism or with activism? reply thelastgallon 20 hours agoprevEnvironmental activists are active spied on and blackmailed by Governments. Probably not the best use of taxpayer money?From: https:&#x2F;&#x2F;www.theguardian.com&#x2F;uk-news&#x2F;2022&#x2F;jan&#x2F;25&#x2F;activist-dec...\"An environmental activist who was deceived into a two-year intimate relationship by an undercover police officer has been awarded £229,000 in compensation after winning a landmark legal case.Kennedy was part of a covert police operation that spied on more than 1,000 predominantly leftwing and progressive political groups over more than four decades. Many undercover officers deceived women they were spying on into sexual relationships during their covert deployments.\"Environment activists: &#x27;I got death and rape threats’: https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;av&#x2F;science-environment-54165868 reply 0daystock 22 hours agoprevAlmost all happened in Latin America where individual rights, including to own and defend yourself with potent firearms, is severely limited (except for agents of the state). Without this fundamental ability and political right, lawful good individuals will suffer and fail. History is replete with examples of this basic and obvious fact. Armed minorities are more difficult to oppress. reply itsanaccount 22 hours agoparentI&#x27;m delighted by this split brained reality in America.\"facts\" as stated by a particular left group. - Cops are oppressive, unreformable and should be abolished - The GOP has become fascist and are looking to overthrow the government - The rich will choose to murder us all slowly via climate change and retreat as they did during the pandemic to let the masses sufferAND- We should outlaw everyman gun ownership because of the harm it causes.It occurs to me that its probably a projection of our first-past-the-post political system that means these views all get amalgamated together, because they sure don&#x27;t make logical sense. reply amanaplanacanal 21 hours agorootparentThey make sense if you assume one thing: gun owners are never going to rise up and throw out the bad guys in government. If that is never going to happen, all those guns are really only used to kill a bunch of innocent people, rather than the bad guys in government.If you think gun owners are really somehow going to save us from fascists at the top, when is that gonna start, exactly? What would actually get them to start fighting? I frankly don’t see it ever happening.I know our national myth of the American revolution is that freedom loving patriots are going to rise up and throw off the oppressors, but that is a pipe dream. The answers are in the ballot box, not the cartridge box. reply itsanaccount 20 hours agorootparentDid you read the parent post I was replying to?All those guns are going to kill a bunch of people because the other side, you I&#x27;m guessing in this case, have decided to willfully disarm yourself because you believe the ballot box is the sole savior. I expect because such people are afraid of accepting a world in which they must be responsible for their own safety.Along that same path, I&#x27;m not worried about a fascist in Washington DC, I&#x27;m worried about the one next door. The ones who see some queer kids stocking up at a rural store, and follow them to their campsite to tell em they aren&#x27;t welcome with threats, and destroy their sense of safety. (Speaking from experience) Ballot box doesn&#x27;t fix that, but couple weekends of pistol training and drills can help. reply mikrotikker 4 hours agorootparentprevDo you suppose that Jews in 1930s Germany would have suffered such horrific treatment had they been armed? reply throwlejos 22 hours agoparentprevAs someone who migrated from a very conflictive area of Latin America, this is the answer, period. But the minute you mention this to the incumbents here in the US (specially to those claiming the need to “amplify” underrepresented voices) all of a sudden you will not be an ally anymore. reply anigbrowl 12 hours agorootparentIndeed. American liberalism (in the most general rather than partisan sense) has for decades tied itself to the ideal of nonviolence. In practice means that it favors orderly incumbency (however oppressive) over messy revolution (however justified). It&#x27;s not armed groups are good by definition; many of them are highly questionable or outright appalling, eg FARC or the Maoist Shining Path group. But this &#x27;nonviolence&#x27; posture and its magical exclusion of most state violence ensures that unarmed movements are impotent. An impressive dichotomy for a country that celebrates its own violent formation with fireworks and song every July. reply 0daystock 18 hours agorootparentprevThat is not by accident. The history of gun control in the USA (and mostly elsewhere) is deeply rooted in racism and classism. The very idea of an armed minority challenging the status quo terrifies individuals living lives of relative privilege and prosperity. Even if they don&#x27;t consciously apprehend their bias (and most lack the emotional and spiritual maturity to do so), they are useful in promoting the false narrative of \"only the State must monopolize power\" as they are the benefactors of it. reply fluoridation 22 hours agoparentprevSeveral countries in Latin America are near the top in murder rate, guns or no guns. That activists are killed more often in places where people in general are killed more often doesn&#x27;t by itself tell you much about whether guns should be legal or not. reply outside1234 22 hours agoparentprevI think you should now apply that reasoning to Mexico. Pretty sure the guns there aren’t making it safer. reply 0daystock 22 hours agorootparentThere is one gun store in all of Mexico and a two-tier ownership&#x2F;permitting system where only the oligarchy class actually has rights to arms. No wonder it isn&#x27;t safe when the average person is deprived of the means to defend themselves. reply coldtea 22 hours agoprevYes, but what about the tens of thousands of \"green transition\" environmental policy advisors who whitewashed capitalism and business as usual and got rich?There&#x27;s some balance! reply abeppu 21 hours agoprevSome comments have pointed out that this doesn&#x27;t actually mention a difference from the base rate of violent deaths in the relevant countries. Sure, fine. I think the other significant missing numbers are those who die from human-driven environmental disaster. One estimate on climate change deaths in the recent decades is 2M, and the WHO has estimated 250k&#x2F;yr in 2030-2050.Since we collectively have known about climate change as a concern for decades, and have known that large numbers of civilian deaths, disease, displacement, etc are predictable results, and have nevertheless chosen to pump carbon in to the atmosphere at ever-increasing rates (except a single dip in 2020), then under a consequentialist framework, these un-targeted victims ought to count just as much as intentionally-targeted activists.If those who benefit from the pro-emissions status quo may kill you whether you protest or not, either through neglectful inattention to the consequences their own actions, or through hostile retribution, but they can only possibly be confronted with any direct responsibility for their _intended_ victims, perhaps it&#x27;s best to protest.As an ethics note, I am aware there are those who say, \"we&#x27;re not responsible for the 250k&#x2F;yr who die from climate change in the near future merely because we emit above a sustainable level; our _goals_ were merely to heat our houses and fly to Europe on vacation, whereas the _goal_ of someone who murders an activist is clearly to cause death.\" And I think this _can_ be part of a coherent ethical vision, but I would ask that they revisit Anscombe&#x27;s critique of Truman&#x27;s decision to use fission weapons on civilian targets. If you believe that it was \"good\" to drop the bombs and hasten the end of the war because it resulted in fewer total deaths, and the hypothetical deaths averted in a long conventional war have the same moral weight and standing as the civilians killed by the atomic bombs, then why would you not believe that targeted and untargeted climate deaths have the same ethical weight? Or from the converse side, if you believe we are not responsible for deaths caused as predictable effects of actions which are motivated by heating&#x2F;transporting&#x2F;building etc, i.e. you believe an important ethical factor is the presence or absence of an _intention_ to kill innocent people (as versus merely knowing they will be killed as an effect of an action initiated with some other intention), then do you also believe that Truman, who chose to drop the bomb on cities, is a mass murderer?https:&#x2F;&#x2F;www.aljazeera.com&#x2F;news&#x2F;2023&#x2F;5&#x2F;22&#x2F;climate-change-caus...https:&#x2F;&#x2F;www.who.int&#x2F;news-room&#x2F;fact-sheets&#x2F;detail&#x2F;climate-cha... reply vuln 23 hours agoprevHow many were targeted using things like Pegasus by the NSO group?When are we going to hold the countries that export offensive cyber weapons utilized against civilians and journalists accountable? reply fortran77 22 hours agoparentThe UK, home to Novalpina, the company behind NSO, is a member of the UN Security group. You mean them? reply fortran77 22 hours agoparentprevProbably zero. This is a crazy straw man to change the topic. reply 0xDEF 23 hours agoparentprevHow are you going to hold all the permanent members of the UN Security Council responsible for this? They all have companies and organizations operating in this space. reply vuln 22 hours agorootparentThere’s only 5 permanent members with voting rights: China, France, Russian Federation, United Kingdom of Great Britain and Northern Ireland, United States of America.You start there. You sign effectually a cyber arms treaty which makes it illegal to export offensive cyber weapons for profit. If these members can sign nuclear arms treaties (Treaty on the Prohibition of Nuclear Weapons (TPNW)), I don’t see how offensive cyber weapons could be any more complicated.Then go after the nonmembers that are profiting wildly off the offensive cyber weapons. reply 0xDEF 22 hours agorootparentWhat makes you think they will do that? Especially considering that Russia&#x27;s source of soft power in the developing world is their willingness to sell offensive weapons without asking ESG questions. reply vuln 22 hours agorootparentI don’t know. Why would they sign a nuclear arms treaty? reply marcosdumay 21 hours agorootparentThose things are not similar.A nuclear arms reduction treaty ensures that all parties keep their own relative power (that is not dictated by the nukes) without going into a boundlessly expensive arms-race that would bankrupt all of them.A prohibition on cyberweapons exports doesn&#x27;t have any similar impact. It wouldn&#x27;t even make defense cheaper. It&#x27;s something that may be useful for civilians, but how often do civilian concerns enter those high-level treaties?(Anyway, I&#x27;m not even sure it gains us anything. The absence of somebody exploiting them does not make the vulnerabilities go away. What would really help is if some moderately advanced country decided to take their head outside of their ass and do something to protect themselves. But I don&#x27;t expect to actually see that happening, everybody only wants to work on the offensive.) replywyager 22 hours agoparentprevI looked at the report - reading between the lines (e.g. deaths by country), this looks to be at least 90% south&#x2F;central american cartel related. This isn&#x27;t the saudis sending agent 47 after people - it&#x27;s more like people fighting over land for coca cultivation. reply jonathanlb 22 hours agorootparentI&#x27;m curious what statistics there are relating to deaths&#x2F;injuries&#x2F;imprisonment for protesting other causes as well. reply vuln 22 hours agorootparentprevAre the saudis in the room with you right now? Are the saudis the only country that uses offensive cyber weapons to kill civilians, activists and journalists? reply gonzo41 23 hours agoparentprevNever is the answer. And I think you know it. reply bluefishinit 23 hours agoprevRecently Georgia police killed an environmental activist protesting the construction of Cop City: https:&#x2F;&#x2F;www.cnn.com&#x2F;2023&#x2F;04&#x2F;20&#x2F;us&#x2F;cop-city-activist-killed-d...Then the government indicted 61 activists on RICO charges: https:&#x2F;&#x2F;apnews.com&#x2F;article&#x2F;atlanta-cop-city-protests-rico-ch...This is currently ongoing. reply hn_throwaway_99 22 hours agoparent> Recently Georgia police killed an environmental activist protesting the construction of Cop City:I mean, thanks for the link I guess, but I&#x27;d say this sentence leaves out some very important details. Namely, that the cops are claiming that this person shot and seriously wounded a cop first.To be clear, your linked article explains how it&#x27;s not really clear yet what the sequence of events were, or if the person actually shot a cop. But I&#x27;d put it like this:1. If the person did shoot the cop first, I have no problems with the other cops opening fire.2. If the person did not shoot the cop first, then the cops should face severe legal charges.Since the facts on the ground are not yet known, I&#x27;ll withhold judgment. reply Waterluvian 21 hours agorootparentWith the existence of qualified immunity, American cops are immune to #2. They should but they won’t. reply hn_throwaway_99 21 hours agorootparentI know what qualified immunity is, but your statement is easily provably false given that there are many instances where cops have been found guilty for assault, unlawful killing or excessive force.I&#x27;m not arguing it&#x27;s as easy as proving it for a civilian (after all, as part of their job cops are allowed to use force under particular circumstances), but these black-and-white statements like \"They should but they won’t\" just show to me how uninterested people are listening to anything that may change their opinion. reply enterprise_cog 18 hours agorootparentMany is relative to the amount of illegal police activity. Depending on your perspective, that is not nearly enough and so saying they won’t be charged is perfectly warranted.That you posted this shows a lack of interest by you to look into the other perspective, and that you’d rather be pedantic and dismissive than acknowledge their viewpoint. reply hn_throwaway_99 17 hours agorootparent> That you posted this shows a lack of interest by you to look into the other perspective, and that you’d rather be pedantic and dismissive than acknowledge their viewpoint.Sorry, not going to let that BS slide. I was referring to a comment that said \"With the existence of qualified immunity, American cops are immune to #2. They should but they won’t.\" It&#x27;s not \"being pedantic\" to point out that comment makes absolutely no sense, given (a) qualified immunity isn&#x27;t even relevant in criminal cases, and (b) they are making a blanket statement with no concern for the actual facts of the situation. reply NovemberWhiskey 21 hours agorootparentprevNo. Qualified immunity is about immunity from lawsuits for damages; it has nothing to do with criminal prosecution. reply diogenes4 21 hours agorootparentprevThe thing preventing cops from being brought on charges is, the vast majority of the time, an uncooperative DA. qualified immunity is a complete red herring. reply smsm42 20 hours agoparentprevCalling violent domestic terrorists engaged in attacks on police, property destruction, arson and other obviously criminal acts mere \"environmental activists\" is very disingenuous. It&#x27;s like saying \"a mathematician dies in prison after being jailed for using postal services\". Technically it&#x27;s not exactly a lie, but it omits so much relevant context that it is, for all practical purposes, a lie. And also a grave disservice to actual non-violent activists for preserving the environment who probably don&#x27;t want to be identified with domestic terrorists under the same umbrella term. reply Cupertino95014 21 hours agoparentprevUp a level: why were \"activists\" protesting Cop City at all? I thought \"more training\" was the thing people were always agitating for.Or is this just \"defund the police\" moved to a different location, now that it&#x27;s failed everywhere at the ballot box? reply bradleyishungry 21 hours agorootparentActivists have said for years (decades even) that more training does not work. There is very little evidence that training in the way that some politicians have called for affects policing at all. Cop City training in particular is not even that, its not for bias training or anything, it’s very much more akin to military training.Activists goals have always been for redirecting some police funding to education, community outreach, different divisions for different problems, etc. There are some that push for abolition but the overall goal of those who actually have been doing it for years has been to address the underlying problems and not to throw more money at an armed force with immunity. reply rayiner 19 hours agorootparentI think American cops are trigger happy, but don’t throw around assertions about “underlying problems” without evidence. Lots of places that don’t have a fraction of the “education,” etc., of the Atlanta metro area, and also have cops that are trigger happy as fuck, don’t have the same level of disruption in the social order. Watch a Bollywood cop movie sometime—some of the scenes of “good cops” killing “bad guys” without due process would make a Republican uneasy. Nonetheless, Atlanta has a homicide rate 5-10 times higher than Delhi, India. Hell, the homicide rate in some parts of America are higher than the death rates of American soldiers during the Iraq war: https:&#x2F;&#x2F;www.newsnationnow.com&#x2F;crime&#x2F;parts-of-chicago-more-da...American social disorder is a unique phenomenon that cannot be waved away as being caused by poverty, lack of education, etc. And frankly it’s kind of offensive to make that comparison. Poor, uneducated people around the world manage to maintain more orderly communities than Americans who are vastly more privileged and affluent in comparison. reply dctoedt 17 hours agorootparent> Nonetheless, Atlanta has a homicide rate 5-10 times higher than Delhi ....Estimated number of civilian firearms per 100 people:- U.S.: 120.5- India: 5.3https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Estimated_number_of_civilian_g... reply ddd00001 2 hours agorootparentOn a per-county level, homicide rate is something like 0.8 correlated to % black. The large county with the lowest black homicide rate is Collins County, TX - also ironically the most conservative large county by vote.When I ran the numbers I had the expectation that there would be a positive correlation between how conservative a county is and homicides per capita once adjusted for inflation. There is zero correlation - and when you remove NYC counties (arguably not indicative of leftist counties, because there is mass surveillance and a heavy police presence, both things that the left vehemently opposes), the correlation actually runs the opposite way. More conservative counties have a lower black homicide rate! reply ddd00001 1 hour agorootparentOnce adjusted for demographics I meant - d&#x27;oh. reply rayiner 14 hours agorootparentprevIn Atlanta, about 35-40% of households own a firearm: https:&#x2F;&#x2F;injuryprevention.bmj.com&#x2F;content&#x2F;6&#x2F;3&#x2F;189. In Idaho it’s 60% https:&#x2F;&#x2F;www.cbsnews.com&#x2F;pictures&#x2F;gun-ownership-rates-by-stat....Despite having more guns, Idaho has a homicide rate of 2.2 per 100,000, which is only a little higher than Belgium, and about the same as Canada, and a little lower than India.By contrast, Puerto Rico had fewer guns than India, but a homicide rate of 19 per 100,000, more than six times higher than India. Disarmed Puerto Rico has a homicide rate eight times higher than heavily armed Idaho.Clinging to the belief that it’s guns rather than social factors is just willful denial. reply dctoedt 13 hours agorootparentPopulation density per square mile:- Idaho: 22.3 (44th out of 50 states) [0]- Puerto Rico: 952 [1]- India: 1,244 [2][0] https:&#x2F;&#x2F;www.gethealthy.dhw.idaho.gov&#x2F;overview-of-idaho[1] https:&#x2F;&#x2F;www.worldometers.info&#x2F;world-population&#x2F;puerto-rico-p...[2] https:&#x2F;&#x2F;www.worldometers.info&#x2F;world-population&#x2F;india-populat... reply rayiner 11 hours agorootparentHomicide rate doesn’t actually have much to do with population density. Boise has a population density of 2,800 per square mile and similar homicide rate to the state as a whole. Urban Boise is actually slightly denser than urban San Juan, Puerto Rico, while being heavily armed and far safer. Boise is also similarly dense to Atlanta while being much safer. reply dctoedt 13 hours agorootparentprev> Clinging to the belief that it’s guns rather than social factors is just willful denial.That&#x27;s a false dichotomy — sure, social factors play an important role, but it&#x27;s a multi-variate phenomenon, and you seem to want to deny that gun-ownership rates have anything to do with it. reply rayiner 11 hours agorootparentYou haven’t presented any evidence that guns are a major factor after adjusting for social factors. The US also has the highest rates of fatherless households in the world, while India has one of the lowest. Maybe that’s the reason for the difference in homicide rates. reply ddd00001 2 hours agorootparentI&#x27;ve spoken to dctoedt before, he&#x27;s the type of reddit-tier, resist-liberal that thinks that the reason America&#x27;s crime rate is so high is that rednecks in MAGA hats are running around shooting people up in rural areas. reply dctoedt 11 hours agorootparentprev“The” driving factor? Are we in agreement that it’s a factor? reply onetimeusename 15 hours agorootparentprevThe US is kind of an outlier in gun ownership. What should we take away from this?https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_countries_by_firearm-r...Estimated number of civilian firearms per 100 people and gun deaths per 100000:- El Salvador: 5.8, 78.52- Brazil: 8.6, 23.93- Colombia: 10.10, 20.38- U.S.: 120.5, 12.21- Mexico: 15, 11.55If anything it seems like proximity to cartels is the most likely predictor of gun homicide ratesMeanwhile Switzerland has 27.6 guns per 100 civilians and 2.64 gun homicides per 100,000. So not sure what to conclude about gun ownership and homicides. Note that gun deaths not equal to gun homicides since deaths includes suicide and the CDC reports these items combined. If you wanted to discuss gun suicides that&#x27;s fine but note this was about homicide. reply dctoedt 14 hours agorootparent> Meanwhile Switzerland has 27.6 guns per 100 civilians and 2.64 gun homicides per 100,000. So not sure what to conclude about gun ownership and homicides.Another data point: Switzerland has strict gun laws that mostly ban the kind of military-grade firearms we see used in mass murders in the U.S. (and yes, some Latin American countries), and smaller guns require permits. [0]More: \"Switzerland has a stunningly high rate of gun ownership — here&#x27;s why it doesn&#x27;t have mass shootings ... Unlike the US, Switzerland has mandatory military service for men. The government gives all men between the ages of 18 and 34 deemed &#x27;fit for service&#x27; a pistol or a rifle and training on how to use them. After they&#x27;ve finished their service, the men can typically buy and keep their service weapons, but they have to get a permit for them.\" [1][0] https:&#x2F;&#x2F;www.ch.ch&#x2F;en&#x2F;safety-and-justice&#x2F;owning-a-weapon-in-s... (scan down for a useful chart)[1] https:&#x2F;&#x2F;www.businessinsider.com&#x2F;switzerland-gun-laws-rates-o... reply ddd00001 2 hours agorootparentprevIf the U.S. was filled with Swiss-Americans, the country&#x27;s homicide rate would be in line with Switzerland. Switzerland doesn&#x27;t have gangs actively warring in urban areas. reply Kretinsky 19 hours agorootparentprevIs it working? San Francisco (and other cities) has been running such \"experiment\" and clearly it&#x27;s becoming a frightening drug hot spot (among other problems). reply matchbok 21 hours agorootparentprevWhich is all completely untrue. Police in America are very untrained compared to other countries. The standards to be a cop are too low.This is just a bunch of middle class \"defund\" people with little else going on. reply Cupertino95014 19 hours agorootparent.. and raising the \"standards\" -- that&#x27;s going to attract more applicants? Explain how that would work. reply cptskippy 21 hours agorootparentprevWhen people say \"more training\" they&#x27;re referring to De-escalation and crisis management training. Not a paramilitary training facility for teach cops tactical assault, and breach and raid techniques. reply Cupertino95014 21 hours agorootparentAlthough that&#x27;s also part of the job. And something that&#x27;s difficult to practice in a classroom. reply cptskippy 21 hours agorootparentCops have created a climate and situations where they benefit from that style of training. Conducting no-knock raids and other paramilitary operations for nonviolent crimes is an unnecessary escalation.Warrior training and being trained that everyone is an adversary with execution being the go to option isn&#x27;t normal and shouldn&#x27;t be ingrained in folks whose day-to-day is largely nonviolent encounters. reply Cupertino95014 19 hours agorootparentand yet day-to-day, they risk running into a guy with a gun. Grow up. reply cptskippy 16 hours agorootparent> Grow up.Excellent advice.It&#x27;s time we stop living in or creating a fantasy world where every encounter with the public is an opportunity for a gun battle. Perhaps if they were trained in deescalation and not told to fear every person they encounter as potential threat then they wouldn&#x27;t be perpetually afraid for their lives while performing routine interactions with the public?We don&#x27;t need to be investing heavily in training cops for an exception encounter, we need to be providing them the training and tools necessary for their day to day duties. As a follow on, the public wouldn&#x27;t be afraid for their own safety when having to interact with police. reply Cupertino95014 16 hours agorootparent> a fantasy worldthat&#x27;s the world you&#x27;re living in, where these selfless, fearless (and also defenseless) people go out to sacrifice themselves every day.> training cops for an exception encounter,yeah, except the \"exception\" means you don&#x27;t come home to your family that day. reply cptskippy 11 hours agorootparent> yeah, except the \"exception\" means you don&#x27;t come home to your family that day.So I guess we should be giving every citizen that same paramilitary training because they have a higher chance of being killed by a cop than a cop does by them. reply yongjik 16 hours agorootparentprevUvalde, TX disagrees. reply Cupertino95014 14 hours agorootparentmeaning, what? Some cops and chiefs betray their oath to put other people&#x27;s safety above their own?would you like some examples where they didn&#x27;t? reply cptskippy 11 hours agorootparentThere&#x27;s more examples where they did at this point. replyjancsika 18 hours agorootparentprev> Up a level: why were \"activists\" protesting Cop City at all? I thought \"more training\" was the thing people were always agitating for.If this is an honest question, it must be coming from a lack of knowledge about policing in the U.S. for the last two decades:After 9&#x2F;11-- and especially after the wars in Afghanistan and Iraq-- there was an escalation in militarized police tactics, equipment and grants. Some of this started in the preceding decades during the war on drugs. But anti-terrorism federal grants and many other sources of funding for militarization training and equipment flowed like rivers post 9&#x2F;11. This funding was-- and mostly still is-- available to police from the largest cities to the smallest municipality. This and the war on drugs have become a sizable chunk of police department budgets. If you go to a town of 1,000 people and see a local police humvee vehicle tooling around, they got it at least in part through some militarization or anti-drug grant. Possibly both. (Note: if there&#x27;s a dog in the humvee it&#x27;s probably the latter, which means the department has an ongoing obligation to show stats for how they&#x27;ve been using this equipment and dog in successful anti-drug efforts. Not sure what the obligations are for the anti-terror grants, though.)By at least the 2010&#x27;s, critics on both the left and right were speaking about the problems of this approach to policing. Essentially, a) it creates&#x2F;exacerbates an \"police vs. citizens\" approach to policing that&#x27;s at odds with the core goal of police being a community service for citizens, and b) there isn&#x27;t enough time in a day nor money in a municipality to train police to do public safety and develop the skills necessary to competently use military anti-terrorism&#x2F;anti-insurgency tactics and equipment.The activists who aren&#x27;t radical-- e.g., the ones who aren&#x27;t anti-state-- typically want more (I&#x27;d say better) training on the public safety side of things. This means things like de-escalation techniques, outreach with citizens, partnering with social workers, detective work, etc.What you will nearly never hear police reform activists in the U.S. agitating for is more militarization training and equipment for the police. That includes a number of the civil libertarians on the right.Finally, I&#x27;ll say that the activists I&#x27;ve read are convinced that Cop City is all about funding more militarized training of Atlanta PD. I don&#x27;t know enough about the plans to know whether that&#x27;s true or not. But the idea of activists pushing back against ostensible police militarization training&#x2F;funding is in keeping with two decades of efforts on the left-- and right-- to protect the bill of rights in the 21st century. reply Cupertino95014 18 hours agorootparent> it must be coming from a lack of knowledge about policing in the U.S. for the last two decadesI think you&#x27;re saying \"doesn&#x27;t agree with me.\" There&#x27;s no lack of knowledge -- there&#x27;s only a lack of indoctrination.American cops are American cops. Crime spiked up until 1990 or so, then it was brought under control, and lately it&#x27;s been spiking again. I think some people like you consider that a reasonable price to pay for whatever goals you&#x27;re trying to achieve. reply chroma 22 hours agoparentprevYou forgot to mention that the activist was killed because he was shooting a cop. reply hedora 22 hours agorootparentYes, and according the autopsy:\"Both Manuel&#x27;s left and right hands show exit wounds in both palms. The autopsy further reveals that Manuel was most probably in a seated position, cross-legged when killed,\"Also, he was shot a dozen times, and apparently had raised both his arms.Neat trick getting shot through both palms while holding a gun (without damaging the gun), kind of like bashing your own head in from the back of a squad car, or hanging yourself from a 9 ft jail cell ceiling without a rope or platform. reply Gud 22 hours agorootparentWhere are you quoting this from? Can you provide a link? reply smokel 21 hours agorootparentYou can easily Google this yourself. The first hit is:https:&#x2F;&#x2F;www.npr.org&#x2F;2023&#x2F;03&#x2F;11&#x2F;1162843992&#x2F;cop-city-atlanta-a... reply Gud 21 hours agorootparentWell normally when you quote something, you source what is quoted. It&#x27;s not up to the reader to track down sources.Thanks for the link. reply NovemberWhiskey 20 hours agorootparentprev>Neat trick getting shot through both palms while holding a gunIf you get shot 57 times, including through your head, you&#x27;re probably going to drop what you&#x27;re holding at some point during the experience. reply coldtea 22 hours agorootparentprevLike all those unarmed 7-12 year olds that \"looked like they had something in their hands\", or the dozens of people killed or arrested which then had guns planted of them?https:&#x2F;&#x2F;www.nbcnews.com&#x2F;news&#x2F;us-news&#x2F;st-louis-officer-execut...https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ds6g-QyAxbwhttps:&#x2F;&#x2F;abcnews.go.com&#x2F;US&#x2F;baltimore-officers-planted-gun-man... reply chroma 22 hours agorootparentCops get too trigger happy all the time, but in this case the activist had a gun that forensics matched to a bullet in a state trooper’s stomach. From the GBI’s press releases [1]:> Today, the GBI received confirmation from a firearms transaction record that in September 2020, Manuel Esteban Paez Teran legally purchased the firearm that was used in the shooting of a GSP trooper.> The handgun is described as a Smith & Wesson M&P Shield 9mm. Forensic ballistic analysis has confirmed that the projectile recovered from the trooper’s wound matches Teran’s handgun1. https:&#x2F;&#x2F;gbi.georgia.gov&#x2F;press-releases&#x2F;2023-04-14&#x2F;gbi-invest... reply TheCleric 20 hours agorootparentForensic bullet analysis is phrenology for bullets:https:&#x2F;&#x2F;radleybalko.substack.com&#x2F;p&#x2F;devil-in-the-grooves-the-... reply chroma 13 hours agorootparentIt&#x27;s not just about matching imprints on the bullet to the barrel. They likely also checked the ammo against what was in the magazine, and I doubt Teran had the same ammo as the police. It might be the same caliber (9mm luger) but there are many different brands of ammunition and bullet types. Atlanta police use Winchester PDX1 147 grain. Most people use lighter hollow points that cost less. Also Atlanta police use Gen 4 Glocks, which have polygonal rifling. Bullets fired from polygonal rifling have very different imprints than those fired from traditional rifling with lands and grooves. reply giantrobot 20 hours agorootparentprevIt sounds so believable that a protestor bought and owned an M&P Shield, a gun extremely popular with police, and brought it to a protest. It&#x27;s also super believable that while surrounded by armed police that this person decided to shoot a single round at an officer from a sitting position.Super. Believable.&#x2F;s reply pandaman 19 hours agorootparentA 7+1 rounds pistol with a safety switch is extremely popular with police? I&#x27;d like to see some source for that. reply NovemberWhiskey 17 hours agorootparentThe GSP service firearm is a Glock, FWIW. reply sixothree 22 hours agorootparentprevBallistic analysis is useless. Video proof or they are lying. Deep South America is more of a police state than people realize. reply gyanreyer 22 hours agorootparentprevThe autopsy found there wasn&#x27;t gunpowder on their hands and protesters who were there claim that isn&#x27;t what happened. Police lie all the time. reply sokoloff 22 hours agorootparentThat&#x27;s not what I read here: https:&#x2F;&#x2F;www.fox5atlanta.com&#x2F;news&#x2F;autopsy-cop-city-protester-...> Gunpowder residue is not seen on the hands. A GSR kit is performed,\" the report reads in part.> That GSR kit was sent off to the GBI Crime Lab for analysis. The findings were released Tuesday. This report \"revealed the presence of particles characteristic of gunshot primer residue.\"The autopsy did not find visible residue. That is quite different from your conclusion \"the autopsy found there wasn&#x27;t gunpowder on their hands\". reply giantrobot 20 hours agorootparentIf you&#x27;re shot 57 times you will end up covered in gunpowder. If you&#x27;re shot in the hands, like the victim, you&#x27;ll have gunpowder on your hands. If a bunch of cops panic fire into you and then handle your lifeless corpse the gunpowder from their hands will end up on you. reply uoaei 21 hours agorootparentprevI urge you to consider the owner of this media outlet and what political ties they likely have. Propaganda works best when veiled in a guise of neutrality. reply sokoloff 21 hours agorootparentIf you don&#x27;t like or believe reading the quotes from the reports on a right-biased media source, perhaps you&#x27;ll like or believe reading them more on a left-biased media source (the details in which I find even more compelling that the protester very likely discharged their firearm):1 - https:&#x2F;&#x2F;www.ajc.com&#x2F;news&#x2F;gbi-report-particles-characteristic...2 - https:&#x2F;&#x2F;www.allsides.com&#x2F;news-source&#x2F;atlanta-journal-constit... reply uoaei 20 hours agorootparent> The Atlanta Journal-Constitution (AJC) is the only major daily newspaper in the metropolitan area of Atlanta, Georgia, United States. It is the flagship publication of Cox Enterprises.Cox Enterprises is ringing extremely loud warning bells. This is the same Cox that is time and again accused of being an internet and TV monopoly and taking advantage of their customers with exploitative, borderline illegal tactics. I don&#x27;t trust them one bit. I&#x27;m willing to bet in a place like Atlanta, with the history it has, there&#x27;s still a good ol&#x27; boys club with deep roots, operating as the Chamber of Commerce or another faceless, buried, quasi-bureaucratic market-capture and -manipulation mechanism. It&#x27;s such a mundane point to make because of how ubiquitous this pattern is, but bears repeating in this context. It always seems paranoid to point out this possibility until it&#x27;s proven true. You can see a recent expression of this tendency in the raid on that elderly reporter in Kansas, the one that stressed her out so much she died.Again, critical reading is imperative here. Especially considering how news media outlets today frequently rely on \"access journalism\" -- ie, favorable coverage and language in exchange for voluntary participation by police in accessing details about new stories -- we should expect that large outlets (especially ones with captive audiences such as \"the only major daily newspaper in the metropolitan area of Atlanta, Georgia\"; looks like they monopolized Atlanta&#x27;s newspaper market, too) play games with rhetoric to maintain legitimacy while peppering in propaganda, in the name of \"both sides\" and \"neutrality\", in order to maintain access.You shouldn&#x27;t trust that &#x27;allsides&#x27; website any more than you should trust anyone else. I certainly don&#x27;t. You should also note: \"As of September 2023, AllSides has low or initial confidence in our Lean left rating for Atlanta Journal-Constitution.\" This does not support your assertion that this media source is \"left-biased\". In fact upon further review that website relies exclusively on community reviews to label these websites. It has the same issues that any reporting mechanism of this form has, which makes it even less trustworthy.ACJ is playing to their crowd just like the rest of them to maintain legitimacy and keep the money flowing. This isn&#x27;t to say that everything in the news is fake, just that everything in the news is shaped by material forces working behind the scenes which helps to form the narratives that end up in the columns and webpages. Sometimes there&#x27;s outright lies, sometimes lies by omission. It&#x27;s as much about what is said as what isn&#x27;t said. Sometimes even emphasis and rhetoric is enough to shift the narrative enough that certain perspectives are completely memory-holed. reply sokoloff 20 hours agorootparent> critical reading is imperativeIndeed.If you don&#x27;t trust that the multiple media outlets have managed to copy and paste quotes correctly, then go to the original source reports and draw your conclusions. reply uoaei 20 hours agorootparentI&#x27;m challenging the lack of journalistic integrity in actually investigating the claims contained within those quotes -- not whether the journalists are capable of stenography, but whether they are capable of journalism. reply sokoloff 20 hours agorootparentWhat investigation do you want them to do on the gunshot residue point? Dig up the body, swab the hands, and run their own GSR test?I get that it&#x27;s inconvenient for one side in the standoff for gunshot residue to be found on their hands. I don&#x27;t have any reasonable double that there was. reply uoaei 17 hours agorootparentAsking literally anyone except police is a good start. Police are acting as privileged gatekeepers of information and saying \"just trust me bro\". I expect HN community members to carry themselves to a higher standard here and really dig in. I&#x27;m sorry to say it but I&#x27;m really starting to doubt your sincerity.Plenty of experts on primer residue and other forensics topics out there. \"Is it possible that this amount of residue is due to the bullets shot at and into Tortuguita and not due to shots fired from a weapon held by him? Do you find it credible that wounds such as those through the palms of Tortuguita coincide with the narrative that he was wielding a gun at the time of his death?\" Easy peasy, and I didn&#x27;t even go to school for journalism. I&#x27;m sure there&#x27;s other means of finding credible information. They could also quote those other activists who were witness to the death and put those quotes on equal footing with those from the police, but they don&#x27;t.> I don&#x27;t have any reasonable double [sic] that there was.Assuming you meant \"doubt\": then you&#x27;re playing a one-sided game while scolding others for what you perceive is one-sidedness. To me it is hypocritical to do so. replySamoyedFurFluff 22 hours agorootparentprevIs this alleged by a cop? Cops can lie under oath without consequences, unlike a regular civilian. reply chroma 22 hours agorootparentHere’s the summary of the Georgia Bureau of Investigation’s findings: https:&#x2F;&#x2F;gbi.georgia.gov&#x2F;press-releases&#x2F;2023-04-14&#x2F;gbi-invest...The protestor had a gun on him. The model and serial number matched that of a gun he purchased in 2020. The round recovered from the state trooper’s abdomen matched that gun. reply woodruffw 22 hours agorootparent> The round recovered from the state trooper’s abdomen matched that gun.Firearms forensics is, like most criminal forensics, junk science[1]: even when the basic scientific reality is aligned any particular technique, it&#x27;s heavily laundered through interpretation and not subject to rigorous statistical analysis.(In particular, bullet analysis is complete nonsense. Prior to 2005 the standard technique was lead-composition analysis, which (1) assumed that all of the suspect&#x27;s bullets were the same, and (2) assumed that similarly-manufactured bullets would have similar compositions. Neither is true, which is why the Federal government no longer uses it as evidence. It&#x27;s unclear whether the same is true for Georgia and the GBI.)Edit: A more specific reference for ballistic analysis[2], which appears to be what the GBI used here.[1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Forensic_firearm_examination#C...[2]: https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S2589871X2... reply fuzzbazz 22 hours agorootparentprevFrom other answers in this thread you&#x27;d think that the police just shot and killed an innocent protester... how can anyone think that if there&#x27;s an injured trooper? reply Fluorescence 21 hours agorootparentIdiot cops opening fire on mass and hitting another cop in the crossfire as has happened many times before. reply lostmsu 20 hours agorootparentStatistically implausible. reply SamoyedFurFluff 22 hours agorootparentprevOkay? Georgia bureau of investigation isn’t exactly a neutral third party unfortunately, also, again, they can lie and omit without consequences. I’m pointing out that due to the existing corruption baked into the justice system and police force, it’s extremely difficult to buy anything said no matter how many fancy labels is put into it. reply chroma 22 hours agorootparentSo what do you think happened? Did the Atlanta police place the gun at the scene and hope that the GBI would lie about the forensics results? Did they lie about the purchase records too? Did they shoot one of their own to justify murder? If they wanted to kill an activist, they could just say he was pointing a gun at them. No need to almost kill a state trooper to cover it up. reply SamoyedFurFluff 22 hours agorootparentI never posited any alternatives, except to say that you cannot trust the testimony of obviously corrupt institutions. There are literally infinite amount of untruths they could’ve lied about and an infinite number of nuances left out in testimony. You cannot rely on anything except that someone is dead. reply midasuni 22 hours agorootparentAnd someone else was shot. I don’t think that fact is in dispute? reply SamoyedFurFluff 21 hours agorootparentYou’re missing my point, which is that you cannot rely on the testimony of corrupt individuals who can lie under oath, in the court of law, without consequence. This means you only have a few facts, and any other presented facts by corrupt institutions are suspect by default. replyjstarfish 21 hours agorootparentprevOne way this all makes sense is if someone else borrowed his gun and shot the trooper.The cops traced it back to Teran and went into the protest encounter expecting to be dealing with a cop killer.Kid had no idea what was coming; APD are thugs an environment that self-selects for them. They took no chances and wasted him.Covering up a summary execution is way easier than conspiracy to commit it. reply NovemberWhiskey 20 hours agorootparentThe trooper was shot immediately prior to the return fire which killed the protester. This timeline is not coherent with the facts. reply jstarfish 20 hours agorootparentMy bad. I hadn&#x27;t read up on this since it first happened.The family claims friendly fire. I&#x27;d believe it. Accidental discharge would explain why no GSR was found on his hands. reply NovemberWhiskey 20 hours agorootparentGSR was found on his hands. reply jstarfish 20 hours agorootparentSome. From the accounts I&#x27;m now looking at, the state is being dodgy with their wording of it. Being shot 57 times will dust everything with GSR. replydanaris 19 hours agorootparentprevEven if he owned a gun...Even if he had it with him...Even if he had just shot a cop with it...It does not justify murdering him for it, especially since from what I&#x27;ve seen, when he was shot he had surrendered and was no threat to anyone.We really need to stop letting cops control the narrative around their extrajudicial killings by putting blame on the victim. It doesn&#x27;t matter if the victim is a pure choirboy or the worst, most hard-bitten gang member you can find: lethal force should be the absolute last resort when attempting to subdue someone. reply Steven420 22 hours agoparentprevHe shot and wounded a police officer reply josu 22 hours agorootparent>On April 19, the DeKalb County Medical Examiner&#x27;s office released an autopsy, which found at least 57 gunshot wounds on Paez Terán&#x27;s body and ruled their death a homicide; the autopsy also found no evidence of gunshot residue on Paez Terán&#x27;s hands.[7][15]https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Killing_of_Manuel_Esteban_Pa...Aside from the cops&#x27; testimony there seems to be 0 evidence supporting that Tortuguita shot at them. reply wl 22 hours agorootparentFrom [7]:&#x27;While the official autopsy report states “gunpowder residue is not seen” on Teran’s hands, it is not conclusive because gunpowder is not always visible to the naked eye. A gunshot residue kit was performed, but those results have not been returned from the GBI Crime Lab, the medical examiner’s office told The Atlanta Journal-Constitution.&#x27;Those results were later released. They were positive.I shoot handguns on a monthly basis. Anecdotally, I have to do a lot of shooting before I have visible residue on my hands. reply Quillbert182 22 hours agorootparentprevWikipedia seems to leave out that the state did a more thorough test and did find gun powder residue.https:&#x2F;&#x2F;www.fox5atlanta.com&#x2F;news&#x2F;autopsy-cop-city-protester-... reply coldtea 22 hours agorootparent\"more thorough\" reply c0pium 21 hours agorootparentYes, more thorough. Using a chemical test to look for an often invisible substance is an obvious step to take. Conversely, visually looking for that same substance and then saying it wasn’t there when you don’t see it is laughable. reply giantrobot 20 hours agorootparentIf I shot you 57 times you&#x27;d be lousy with gunpowder residue. Bullet holes and gunpowder residue. So trace amounts of gunpowder residue on a bullet riddled corpse is not in any way conclusive evidence they fired a gun. reply c0pium 19 hours agorootparentThere is no evidence I’ve found to support that theory, can you cite a study that flying through the air at close to the speed of sound doesn’t brush GSR particles off the round? reply josu 17 hours agorootparentPatterns of Gunshot Residue Gunshot residue (GSR) may be deposited by two mechanisms: (1) impact deposition from particles propelled by the force of the blast, and (2) fallout deposition of drifting particles that settle on a surface. Persons close to the blast, specifically the shooter, will likely have GSR from impact. Bystanders are likely to have GSR particles from fallout. Shooters are more likely to have a greater number of particles than bystanders, but not always. Settling of airborne GSR may take up to 10 minutes following firearm discharge. The depostion of GSR particles following initial firearm discharge is primary transfer. However, secondary transfer of these particles to other surfaces can occur from contact with the surfaces or persons on whom the particles have deposited, as with handshaking or contact with clothing. Movement of persons following the shooting, or even scene investigation by forensic scientists, may alter GSR distribution. Further tertiary or even quaternary transfer is possible. Law enforcement personnel may carry particles from prior shooting events. (Blakey et al, 2018)The amount and pattern of GSR deposited may vary by the gun used to fire the bullet. Most GSR emanates from the ejection port of a semiautomatic pistol. GSR is expelled from the gap between cylinder and frame of a revolver. There is greater particle number with revolvers than with automatic rifles. Particle numbers are greater with nonjacketed bullets, mainly due to an increase in particles composed of lead. A faster burning rate of propellant powder reduces the distance of GSR particles travelled. (Blakey et al, 2018) (Vachon and Martinez, 2019)GSR may be expelled from the firearm ahead of the bullet, along with the bullet, and following after the bullet. Though the amount of residue deposited tends to decrease with increasing range of fire, the actual deposits can be highly variable for ranges up to 20 cm.(Brown, Cauchi, et al, 1999) GSR has been reported to be found at distances from 6 to 18 meters forward of the shooter, and up to 6 meters laterally. However, climatic conditions significantly influence recovery rates for GSR. (Dalby et al, 2010) Use of atomic force microscopy (AFM) for detection of particle size in relation to range of fire has been described. (Mou, Lakadwar, and Rabalais, 2008)https:&#x2F;&#x2F;webpath.med.utah.edu&#x2F;TUTORIAL&#x2F;GUNS&#x2F;GUNGSR.html reply coldtea 5 hours agorootparentNot to mention it&#x27;s trivial to leave gunpowder residue on a corpse you know was carrying...Would hardly be the first time, just a random sample from the last 5 years:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ds6g-QyAxbwhttps:&#x2F;&#x2F;apnews.com&#x2F;article&#x2F;gun-violence-crime-baltimore-82fa...https:&#x2F;&#x2F;www.nbcnews.com&#x2F;news&#x2F;us-news&#x2F;st-louis-officer-execut...https:&#x2F;&#x2F;nypost.com&#x2F;2020&#x2F;01&#x2F;03&#x2F;nypd-cop-caught-planting-stun-... replyhn_throwaway_99 22 hours agorootparentprevAnother commenter posted a link that reported they did find gunshot residue: https:&#x2F;&#x2F;www.fox5atlanta.com&#x2F;news&#x2F;autopsy-cop-city-protester-... reply woodruffw 22 hours agorootparentI think it&#x27;s worth noting that this report comes from the Georgia&#x27;s GBI, i.e. the same people who are accused of killing the activist in question. The previous report comes from a (nominally) neutral source (the DeKalb County Medical Examiner&#x27;s office).In other words: the accused are investigating themselves, and have produced a report that suggests that they were justified in killing a climate activist. I think that&#x27;s a pattern we&#x27;d normally treat with extreme skepticism; it&#x27;s unclear that GBI has earned an exception to that. reply NovemberWhiskey 20 hours agorootparent>Georgia&#x27;s GBI, i.e. the same people who are accused of killing the activist in question.No; the Georgia State Patrol is the primary agency that was involved. It&#x27;s a different agency. reply jmvoodoo 22 hours agorootparentprevProbably not the murderer? FBI, local jurisdiction, literally anyone would be a better choice than the institution accused of committing the crime. reply c0pium 20 hours agorootparent> local jurisdictionGood news then, the State DA recused and GBI is a different agency than the state police. reply DiggyJohnson 22 hours agorootparentprevWho should conduct this investigation then? reply woodruffw 21 hours agorootparentDepending on the investigation&#x27;s ultimate scope, either the federal DOJ or an independent investigator&#x27;s office.(It hopefully isn&#x27;t controversial to say that, as a general principle, entities in positions of exceptional public trust or interest should not be tasked with enforcing or regulating themselves.) reply DiggyJohnson 21 hours agorootparentI’m not sure what an independent investigators office is, but assuming it’s a private firm would you not have the same criticism that it’s not a neutral third party and is biased towards whoever hired them?What is the role of a state level investigation bureau if not to investigate state-level issues involving state agencies? reply woodruffw 21 hours agorootparentI meant something like Washington DC&#x27;s OPC[1]. Note that these commissions are not themselves perfect; they&#x27;re typically limited in authority to \"recommendations,\" which the police (or DA) may still ignore.However, to answer the following (which isn&#x27;t what I meant, but I think is interesting):> but assuming it’s a private firm would you not have the same criticism that it’s not a neutral third party and is biased towards whoever hired them?I think the answer to this is similar to the answer found in arbitration: arbitration is typically performed by a well-known firm, and that firm is obligated through contract to perform fair proceedings. In other words: there are civil penalties on the table if the independent party does not execute their job fairly.That being said, I think an unrelated but still governmental investigation arm would be sufficient here.> What is the role of a state level investigation bureau if not to investigate state-level issues involving state agencies?You can find the GBI&#x27;s services listed here[2]. They have a lot of responsibilities, most of which aren&#x27;t tied to investigation of local police. I think it&#x27;d even be fair to say that they aren&#x27;t particularly equipped to investigate local forces.[1]: https:&#x2F;&#x2F;policecomplaints.dc.gov&#x2F;[2]: https:&#x2F;&#x2F;gbi.georgia.gov&#x2F;services replyWillPostForFood 22 hours agorootparentprevNonsense!We are releasing a photo of the handgun that was in Manuel Esteban Paez Teran’s possession when a Georgia State Patrol trooper was shot on January 18 at the site of the future Atlanta Public Safety Training Center. The handgun is described as a Smith & Wesson M&P Shield 9mm. Forensic ballistic analysis has confirmed that the projectile recovered from the trooper’s wound matches Teran’s handgun. Other preliminary information released in this case is consistent with the investigation so far.https:&#x2F;&#x2F;gbi.georgia.gov&#x2F;press-releases&#x2F;2023-04-14&#x2F;gbi-invest... reply DangitBobby 22 hours agorootparentSo they have a photo of a gun that was \"in his possession\" but not of him actually possessing or welding it, and also testimony about an autopsy where the lack of gunpowder on his hands indicates he didn&#x27;t shoot it. Truly damning evidence, next we will see a picture of the car registered in his name that he could have used to run over puppies and children. reply mcpackieh 22 hours agorootparentThey have records of him buying the gun. It was in his possession. reply DangitBobby 22 hours agorootparentHim having a gun is not evidence of him using one. reply NovemberWhiskey 20 hours agorootparentOK, he owned a gun, that was found with him, at the time of his death, and his hands were found to have primer residue. Circumstantial evidence is evidence. reply mcpackieh 21 hours agorootparentprevThat is correct, records of him buying the gun are not evidence of him using it. I was responding to this part of your comment \"So they have a photo of a gun that was \"in his possession\" but not of him actually _possessing_ [...] Truly damning evidence\" There&#x27;s ample evidence that he possessed the gun, we don&#x27;t need a photograph of literally everything to reasonably conclude things.The bullet they found in a cop&#x27;s stomach matching the gun he owned is evidence that he used it, albeit not unimpeachable proof because matching bullets to guns is a matter you can reasonably question. Nevertheless, I think the sum of the evidence thus-far presented supports the narrative that he shot a cop then the cops turned him into swiss cheese, possibly with the man trying in vane to surrender while that was happening. replyhedora 22 hours agorootparentprevAllegedly.Either way, he surrendered to the police with both hands empty, and was shot a dozen times. Then the police officers that were involved lied in the report. reply NovemberWhiskey 20 hours agorootparentAlso allegedly, right? reply prepend 22 hours agorootparentprevAn environmental activist who shot at, and potentially wounded, a police officer.This is like a drug trial that reports deaths and neglects that people died in a car accident.Yale should be more responsible and work harder to have accurate reports if they want people to use their reports. reply hn_throwaway_99 22 hours agorootparent> Yale should be more responsible and work harder to have accurate reports if they want people to use their reports.Why do you think Yale even included the incident that the parent commenter was referring to? reply morelisp 22 hours agorootparentprevThere is no evidence of this and some recordings of a cop saying it was another cop. reply zaroth 22 hours agorootparentI have no horse in this race, but there is not “no evidence”.A handgun registered to the activist was found at the scene and ballistics from the bullet that struck the officer matches that gun.Later, the crime lab reported finding gun shot residue on the activist’s hands.I think those 3 things together combined with ample witness testimony would typically be considered proof beyond reasonable doubt. reply hedora 22 hours agorootparentThe forensic evidence says his hands were empty (exit wounds though both palms), his arms were raised, and he was probably sitting on the ground cross legged when the cops shot him at least a dozen times.The initial crime lab report showed no gun shot residue, and the forensics imply the police lied about the incident. Witnesses testimony is contradictory.In my opinion, there’s no reasonable doubt that the police murdered him, execution-style. There is reasonable doubt that he shot the officer. reply NovemberWhiskey 20 hours agorootparent>The initial crime lab report showed no gun shot residueThe initial report was only a visual inspection. When you do a more sensitive test, you get more accurate results. There is nothing sinister about this sequence of events. replymu53 22 hours agoparentprevLikely, another case that end up only netting maybe one or two people with charges after the process is finalized.The remaining will have had their lives turned upside down by the process, and despite being innocent, will have suffered immense damage financially by the state. reply bagels 20 hours agoparentprevThe article interchanges killed with murdered. It&#x27;d be nice if they talked about the circumstances a little more, though I doubt many of the killings were justified or legal. reply DoneWithAllThat 22 hours agoparentprevYou mean the environmental activist who shot a police officer? reply PhasmaFelis 22 hours agorootparentThe activist that police claim shot an officer. The state did a test to determine if there was gunpowder residue on his hands, but has so far declined to release the result. reply bigstrat2003 22 hours agorootparentFair enough, but that&#x27;s still one hell of a detail to leave out. It completely changes the nature of the case if that person did in fact shoot a police officer, and so those allegations bear mentioning even if they aren&#x27;t proven yet. reply chroma 22 hours agorootparentprevThe guy had a gun on him. The serial number matched that of a gun he legally purchased in 2020. The bullet in the officer matched that gun. Do you think the cops shot him, then shot one of their own to make it look like self-defense? If they wanted to get away with murder they could simply claim the guy pointed the gun at them. reply Quillbert182 22 hours agorootparentprevThe results were released months ago, gunpowder residue was found on his hands.https:&#x2F;&#x2F;www.fox5atlanta.com&#x2F;news&#x2F;autopsy-cop-city-protester-... reply bongobingo1 22 hours agorootparentOr not?Autopsy: Gunshot residue ‘not seen’ on activist killed at police training centerhttps:&#x2F;&#x2F;www.ajc.com&#x2F;news&#x2F;crime&#x2F;activist-killed-at-police-tra...\"According to the autopsy sent to ABC News, Teran did not have gunpowder residue on their hands. Officials claimed Teran fired the first shot at a state trooper.\"https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230419235728&#x2F;https:&#x2F;&#x2F;abcnews.g... reply zdragnar 22 hours agorootparentYou&#x27;re not going to see gunpowder residue without firing quite a few shots. The test is far more conclusion than a visual inspection. reply hedora 21 hours agorootparentWell, he was shot over a dozen times at close range (probably while sitting with hands up), and both palms had exit wounds.I’m not sure how well visual inspection would work with all the blood and other carnage.However, I’m also not sure if trace gunshot residue will show up on current forensics tests if multiple people empty their clips into you from a few feet away.There’s also the question of why they shot him over a dozen times while he was unarmed. reply Quillbert182 22 hours agorootparentprevWhile true, how it was explained to me was that the autopsy did a visual inspection, while the GBI test, which was done afterwards, was a more thorough chemical one. However, I don’t have a source on this one. reply morelisp 22 hours agorootparentprevTeran was shot in their hands. If they actually fired a weapon there&#x27;d be a lot more than five particles to find, and in a lot more places. reply Quillbert182 22 hours agorootparentMaybe, but the commenter above claimed that the results weren’t released, which is false. reply6 more comments... Applications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Global Witness, in a recent report, highlights the concerning trend of environmental activist killings, nearly 2,000 worldwide between 2012 and 2022, with 177 incidents in 2022 alone.",
      "Latin America, especially Colombia, was identified as high-risk for environmental activists, with Brazil, Mexico, Honduras, and the Philippines also reporting high fatality rates. Indigenous people constituted one-third of the victims, underlining their critical role in climate mitigation.",
      "The report underscores the urgency to safeguard and assist those advocating for environmental protection."
    ],
    "commentSummary": [
      "The discussions underline a range of topics such as the risks confronting environmental activists, the crucial role of indigenous cultures in environmental conservation, and the strategies and reputation of climate activists.",
      "They also delve into issues on the ethical implications of man-made environmental catastrophes, the phenomenon of police immunity and its impact on criminal justice, gun ownership and related homicides, and concerns about media credibility and propaganda.",
      "The shooting death of Manuel Esteban Paez Terán by police officers is also a highlighted subject, with the conversations underscoring different viewpoints and perspectives, underscoring the controversy and complexity of these topics."
    ],
    "points": 274,
    "commentCount": 271,
    "retryCount": 0,
    "time": 1694960272
  },
  {
    "id": 37553574,
    "title": "What If OpenDocument Used SQLite?",
    "originLink": "https://www.sqlite.org/affcase1.html",
    "originBody": "Small. Fast. Reliable. Choose any three. Home About Documentation Download License Support Purchase Search What If OpenDocument Used SQLite? Introduction Suppose the OpenDocument file format, and specifically the \"ODP\" OpenDocument Presentation format, were built around SQLite. Benefits would include: Smaller documents Faster File/Save times Faster startup times Less memory used Document versioning A better user experience Note that this is only a thought experiment. We are not suggesting that OpenDocument be changed. Nor is this article a criticism of the current OpenDocument design. The point of this essay is to suggest ways to improve future file format designs. About OpenDocument And OpenDocument Presentation The OpenDocument file format is used for office applications: word processors, spreadsheets, and presentations. It was originally designed for the OpenOffice suite but has since been incorporated into other desktop application suites. The OpenOffice application has been forked and renamed a few times. This author's primary use for OpenDocument is building slide presentations with either NeoOffice on Mac, or LibreOffice on Linux and Windows. An OpenDocument Presentation or \"ODP\" file is a ZIP archive containing XML files describing presentation slides and separate image files for the various images that are included as part of the presentation. (OpenDocument word processor and spreadsheet files are similarly structured but are not considered by this article.) The reader can easily see the content of an ODP file by using the \"zip -l\" command. For example, the following is the \"zip -l\" output from a 49-slide presentation about SQLite from the 2014 SouthEast LinuxFest conference: Archive: self2014.odp Length Date Time Name --------- ---------- ----- ---- 47 2014-06-21 12:34 mimetype 0 2014-06-21 12:34 Configurations2/statusbar/ 0 2014-06-21 12:34 Configurations2/accelerator/current.xml 0 2014-06-21 12:34 Configurations2/floater/ 0 2014-06-21 12:34 Configurations2/popupmenu/ 0 2014-06-21 12:34 Configurations2/progressbar/ 0 2014-06-21 12:34 Configurations2/menubar/ 0 2014-06-21 12:34 Configurations2/toolbar/ 0 2014-06-21 12:34 Configurations2/images/Bitmaps/ 54702 2014-06-21 12:34 Pictures/10000000000001F40000018C595A5A3D.png 46269 2014-06-21 12:34 Pictures/100000000000012C000000A8ED96BFD9.png ... 58 other pictures omitted... 13013 2014-06-21 12:34 Pictures/10000000000000EE0000004765E03BA8.png 1005059 2014-06-21 12:34 Pictures/10000000000004760000034223EACEFD.png 211831 2014-06-21 12:34 content.xml 46169 2014-06-21 12:34 styles.xml 1001 2014-06-21 12:34 meta.xml 9291 2014-06-21 12:34 Thumbnails/thumbnail.png 38705 2014-06-21 12:34 Thumbnails/thumbnail.pdf 9664 2014-06-21 12:34 settings.xml 9704 2014-06-21 12:34 META-INF/manifest.xml --------- ------- 10961006 78 files The ODP ZIP archive contains four different XML files: content.xml, styles.xml, meta.xml, and settings.xml. Those four files define the slide layout, text content, and styling. This particular presentation contains 62 images, ranging from full-screen pictures to tiny icons, each stored as a separate file in the Pictures folder. The \"mimetype\" file contains a single line of text that says: application/vnd.oasis.opendocument.presentation The purpose of the other files and folders is presently unknown to the author but is probably not difficult to figure out. Limitations Of The OpenDocument Presentation Format The use of a ZIP archive to encapsulate XML files plus resources is an elegant approach to an application file format. It is clearly superior to a custom binary file format. But using an SQLite database as the container, instead of ZIP, would be more elegant still. A ZIP archive is basically a key/value database, optimized for the case of write-once/read-many and for a relatively small number of distinct keys (a few hundred to a few thousand) each with a large BLOB as its value. A ZIP archive can be viewed as a \"pile-of-files\" database. This works, but it has some shortcomings relative to an SQLite database, as follows: Incremental update is hard. It is difficult to update individual entries in a ZIP archive. It is especially difficult to update individual entries in a ZIP archive in a way that does not destroy the entire document if the computer loses power and/or crashes in the middle of the update. It is not impossible to do this, but it is sufficiently difficult that nobody actually does it. Instead, whenever the user selects \"File/Save\", the entire ZIP archive is rewritten. Hence, \"File/Save\" takes longer than it ought, especially on older hardware. Newer machines are faster, but it is still bothersome that changing a single character in a 50 megabyte presentation causes one to burn through 50 megabytes of the finite write life on the SSD. Startup is slow. In keeping with the pile-of-files theme, OpenDocument stores all slide content in a single big XML file named \"content.xml\". LibreOffice reads and parses this entire file just to display the first slide. LibreOffice also seems to read all images into memory as well, which makes sense seeing as when the user does \"File/Save\" it is going to have to write them all back out again, even though none of them changed. The net effect is that start-up is slow. Double-clicking an OpenDocument file brings up a progress bar rather than the first slide. This results in a bad user experience. The situation grows ever more annoying as the document size increases. More memory is required. Because ZIP archives are optimized for storing big chunks of content, they encourage a style of programming where the entire document is read into memory at startup, all editing occurs in memory, then the entire document is written to disk during \"File/Save\". OpenOffice and its descendants embrace that pattern. One might argue that it is ok, in this era of multi-gigabyte desktops, to read the entire document into memory. But it is not ok. For one, the amount of memory used far exceeds the (compressed) file size on disk. So a 50MB presentation might take 200MB or more RAM. That still is not a problem if one only edits a single document at a time. But when working on a talk, this author will typically have 10 or 15 different presentations up all at the same time (to facilitate copy/paste of slides from past presentation) and so gigabytes of memory are required. Add in an open web browser or two and a few other desktop apps, and suddenly the disk is whirling and the machine is swapping. And even having just a single document is a problem when working on an inexpensive Chromebook retrofitted with Ubuntu. Using less memory is always better. Crash recovery is difficult. The descendants of OpenOffice tend to segfault more often than commercial competitors. Perhaps for this reason, the OpenOffice forks make periodic backups of their in-memory documents so that users do not lose all pending edits when the inevitable application crash does occur. This causes frustrating pauses in the application for the few seconds while each backup is being made. After restarting from a crash, the user is presented with a dialog box that walks them through the recovery process. Managing the crash recovery this way involves lots of extra application logic and is generally an annoyance to the user. Content is inaccessible. One cannot easily view, change, or extract the content of an OpenDocument presentation using generic tools. The only reasonable way to view or edit an OpenDocument document is to open it up using an application that is specifically designed to read or write OpenDocument (read: LibreOffice or one of its cousins). The situation could be worse. One can extract and view individual images (say) from a presentation using just the \"zip\" archiver tool. But it is not reasonable try to extract the text from a slide. Remember that all content is stored in a single \"context.xml\" file. That file is XML, so it is a text file. But it is not a text file that can be managed with an ordinary text editor. For the example presentation above, the content.xml file consist of exactly two lines. The first line of the file is just:The second line of the file contains 211792 characters of impenetrable XML. Yes, 211792 characters all on one line. This file is a good stress-test for a text editor. Thankfully, the file is not some obscure binary format, but in terms of accessibility, it might as well be written in Klingon. First Improvement: Replace ZIP with SQLite Let us suppose that instead of using a ZIP archive to store its files, OpenDocument used a very simple SQLite database with the following single-table schema: CREATE TABLE OpenDocTree( filename TEXT PRIMARY KEY, -- Name of file filesize BIGINT, -- Size of file after decompression content BLOB -- Compressed file content ); For this first experiment, nothing else about the file format is changed. The OpenDocument is still a pile-of-files, only now each file is a row in an SQLite database rather than an entry in a ZIP archive. This simple change does not use the power of a relational database. Even so, this simple change shows some improvements. Surprisingly, using SQLite in place of ZIP makes the presentation file smaller. Really. One would think that a relational database file would be larger than a ZIP archive, but at least in the case of NeoOffice that is not so. The following is an actual screen-scrape showing the sizes of the same NeoOffice presentation, both in its original ZIP archive format as generated by NeoOffice (self2014.odp), and as repacked as an SQLite database using the SQLAR utility: -rw-r--r-- 1 drh staff 10514994 Jun 8 14:32 self2014.odp -rw-r--r-- 1 drh staff 10464256 Jun 8 14:37 self2014.sqlar -rw-r--r-- 1 drh staff 10416644 Jun 8 14:40 zip.odp The SQLite database file (\"self2014.sqlar\") is about a half percent smaller than the equivalent ODP file! How can this be? Apparently the ZIP archive generator logic in NeoOffice is not as efficient as it could be, because when the same pile-of-files is recompressed using the command-line \"zip\" utility, one gets a file (\"zip.odp\") that is smaller still, by another half percent, as seen in the third line above. So, a well-written ZIP archive can be slightly smaller than the equivalent SQLite database, as one would expect. But the difference is slight. The key take-away is that an SQLite database is size-competitive with a ZIP archive. The other advantage to using SQLite in place of ZIP is that the document can now be updated incrementally, without risk of corrupting the document if a power loss or other crash occurs in the middle of the update. (Remember that writes to SQLite databases are atomic.) True, all the content is still kept in a single big XML file (\"content.xml\") which must be completely rewritten if so much as a single character changes. But with SQLite, only that one file needs to change. The other 77 files in the repository can remain unaltered. They do not all have to be rewritten, which in turn makes \"File/Save\" run much faster and saves wear on SSDs. Second Improvement: Split content into smaller pieces A pile-of-files encourages content to be stored in a few large chunks. In the case of ODP, there are just four XML files that define the layout off all slides in a presentation. An SQLite database allows storing information in a few large chunks, but SQLite is also adept and efficient at storing information in numerous smaller pieces. So then, instead of storing all content for all slides in a single oversized XML file (\"content.xml\"), suppose there was a separate table for storing the content of each slide separately. The table schema might look something like this: CREATE TABLE slide( pageNumber INTEGER, -- The slide page number slideContent TEXT -- Slide content as XML or JSON ); CREATE INDEX slide_pgnum ON slide(pageNumber); -- Optional The content of each slide could still be stored as compressed XML. But now each page is stored separately. So when opening a new document, the application could simply run: SELECT slideContent FROM slide WHERE pageNumber=1; This query will quickly and efficiently return the content of the first slide, which could then be speedily parsed and displayed to the user. Only one page needs to be read and parsed in order render the first screen, which means that the first screen appears much faster and there is no longer a need for an annoying progress bar. If the application wanted to keep all content in memory, it could continue reading and parsing the other pages using a background thread after drawing the first page. Or, since reading from SQLite is so efficient, the application might instead choose to reduce its memory footprint and only keep a single slide in memory at a time. Or maybe it keeps the current slide and the next slide in memory, to facility rapid transitions to the next slide. Notice that dividing up the content into smaller pieces using an SQLite table gives flexibility to the implementation. The application can choose to read all content into memory at startup. Or it can read just a few pages into memory and keep the rest on disk. Or it can read just single page into memory at a time. And different versions of the application can make different choices without having to make any changes to the file format. Such options are not available when all content is in a single big XML file in a ZIP archive. Splitting content into smaller pieces also helps File/Save operations to go faster. Instead of having to write back the content of all pages when doing a File/Save, the application only has to write back those pages that have actually changed. One minor downside of splitting content into smaller pieces is that compression does not work as well on shorter texts and so the size of the document might increase. But as the bulk of the document space is used to store images, a small reduction in the compression efficiency of the text content will hardly be noticeable, and is a small price to pay for an improved user experience. Third Improvement: Versioning Once one is comfortable with the concept of storing each slide separately, it is a small step to support versioning of the presentation. Consider the following schema: CREATE TABLE slide( slideId INTEGER PRIMARY KEY, derivedFrom INTEGER REFERENCES slide, content TEXT -- XML or JSON or whatever ); CREATE TABLE version( versionId INTEGER PRIMARY KEY, priorVersion INTEGER REFERENCES version, checkinTime DATETIME, -- When this version was saved comment TEXT, -- Description of this version manifest TEXT -- List of integer slideIds ); In this schema, instead of each slide having a page number that determines its order within the presentation, each slide has a unique integer identifier that is unrelated to where it occurs in sequence. The order of slides in the presentation is determined by a list of slideIds, stored as a text string in the MANIFEST column of the VERSION table. Since multiple entries are allowed in the VERSION table, that means that multiple presentations can be stored in the same document. On startup, the application first decides which version it wants to display. Since the versionId will naturally increase in time and one would normally want to see the latest version, an appropriate query might be: SELECT manifest, versionId FROM version ORDER BY versionId DESC LIMIT 1; Or perhaps the application would rather use the most recent checkinTime: SELECT manifest, versionId, max(checkinTime) FROM version; Using a single query such as the above, the application obtains a list of the slideIds for all slides in the presentation. The application then queries for the content of the first slide, and parses and displays that content, as before. (Aside: Yes, that second query above that uses \"max(checkinTime)\" really does work and really does return a well-defined answer in SQLite. Such a query either returns an undefined answer or generates an error in many other SQL database engines, but in SQLite it does what you would expect: it returns the manifest and versionId of the entry that has the maximum checkinTime.) When the user does a \"File/Save\", instead of overwriting the modified slides, the application can now make new entries in the SLIDE table for just those slides that have been added or altered. Then it creates a new entry in the VERSION table containing the revised manifest. The VERSION table shown above has columns to record a check-in comment (presumably supplied by the user) and the time and date at which the File/Save action occurred. It also records the parent version to record the history of changes. Perhaps the manifest could be stored as a delta from the parent version, though typically the manifest will be small enough that storing a delta might be more trouble than it is worth. The SLIDE table also contains a derivedFrom column which could be used for delta encoding if it is determined that saving the slide content as a delta from its previous version is a worthwhile optimization. So with this simple change, the ODP file now stores not just the most recent edit to the presentation, but a history of all historic edits. The user would normally want to see just the most recent edition of the presentation, but if desired, the user can now go backwards in time to see historical versions of the same presentation. Or, multiple presentations could be stored within the same document. With such a schema, the application would no longer need to make periodic backups of the unsaved changes to a separate file to avoid lost work in the event of a crash. Instead, a special \"pending\" version could be allocated and unsaved changes could be written into the pending version. Because only changes would need to be written, not the entire document, saving the pending changes would only involve writing a few kilobytes of content, not multiple megabytes, and would take milliseconds instead of seconds, and so it could be done frequently and silently in the background. Then when a crash occurs and the user reboots, all (or almost all) of their work is retained. If the user decides to discard unsaved changes, they simply go back to the previous version. There are details to fill in here. Perhaps a screen can be provided that displays a history changes (perhaps with a graph) allowing the user to select which version they want to view or edit. Perhaps some facility can be provided to merge forks that might occur in the version history. And perhaps the application should provide a means to purge old and unwanted versions. The key point is that using an SQLite database to store the content, rather than a ZIP archive, makes all of these features much, much easier to implement, which increases the possibility that they will eventually get implemented. And So Forth... In the previous sections, we have seen how moving from a key/value store implemented as a ZIP archive to a simple SQLite database with just three tables can add significant capabilities to an application file format. We could continue to enhance the schema with new tables, with indexes added for performance, with triggers and views for programming convenience, and constraints to enforce consistency of content even in the face of programming errors. Further enhancement ideas include: Store an automated undo/redo stack in a database table so that Undo could go back into prior edit sessions. Add full text search capabilities to the slide deck, or across multiple slide decks. Decompose the \"settings.xml\" file into an SQL table that is more easily viewed and edited by separate applications. Break out the \"Presentor Notes\" from each slide into a separate table, for easier access from third-party applications and/or scripts. Enhance the presentation concept beyond the simple linear sequence of slides to allow for side-tracks and excursions to be taken depending on how the audience is responding. An SQLite database has a lot of capability, which this essay has only begun to touch upon. But hopefully this quick glimpse has convinced some readers that using an SQL database as an application file format is worth a second look. Some readers might resist using SQLite as an application file format due to prior exposure to enterprise SQL databases and the caveats and limitations of those other systems. For example, many enterprise database engines advise against storing large strings or BLOBs in the database and instead suggest that large strings and BLOBs be stored as separate files and the filename stored in the database. But SQLite is not like that. Any column of an SQLite database can hold a string or BLOB up to about a gigabyte in size. And for strings and BLOBs of 100 kilobytes or less, I/O performance is better than using separate files. Some readers might be reluctant to consider SQLite as an application file format because they have been inculcated with the idea that all SQL database schemas must be factored into third normal form and store only small primitive data types such as strings and integers. Certainly relational theory is important and designers should strive to understand it. But, as demonstrated above, it is often quite acceptable to store complex information as XML or JSON in text fields of a database. Do what works, not what your database professor said you ought to do. Review Of The Benefits Of Using SQLite In summary, the claim of this essay is that using SQLite as a container for an application file format like OpenDocument and storing lots of smaller objects in that container works out much better than using a ZIP archive holding a few larger objects. To wit: An SQLite database file is approximately the same size, and in some cases smaller, than a ZIP archive holding the same information. The atomic update capabilities of SQLite allow small incremental changes to be safely written into the document. This reduces total disk I/O and improves File/Save performance, enhancing the user experience. Startup time is reduced by allowing the application to read in only the content shown for the initial screen. This largely eliminates the need to show a progress bar when opening a new document. The document just pops up immediately, further enhancing the user experience. The memory footprint of the application can be dramatically reduced by only loading content that is relevant to the current display and keeping the bulk of the content on disk. The fast query capability of SQLite make this a viable alternative to keeping all content in memory at all times. And when applications use less memory, it makes the entire computer more responsive, further enhancing the user experience. The schema of an SQL database is able to represent information more directly and succinctly than a key/value database such as a ZIP archive. This makes the document content more accessible to third-party applications and scripts and facilitates advanced features such as built-in document versioning, and incremental saving of work in progress for recovery after a crash. These are just a few of the benefits of using SQLite as an application file format — the benefits that seem most likely to improve the user experience for applications like OpenOffice. Other applications might benefit from SQLite in different ways. See the Application File Format document for additional ideas. Finally, let us reiterate that this essay is a thought experiment. The OpenDocument format is well-established and already well-designed. Nobody really believes that OpenDocument should be changed to use SQLite as its container instead of ZIP. Nor is this article a criticism of OpenDocument for not choosing SQLite as its container since OpenDocument predates SQLite. Rather, the point of this article is to use OpenDocument as a concrete example of how SQLite can be used to build better application file formats for future projects.",
    "commentLink": "https://news.ycombinator.com/item?id=37553574",
    "commentBody": "What If OpenDocument Used SQLite?Hacker NewspastloginWhat If OpenDocument Used SQLite? (sqlite.org) 265 points by weeber 5 hours ago| hidepastfavorite136 comments miki123211 21 minutes agoThe problem with SQLite is that it&#x27;s not a standardized file format. It&#x27;s well-documented and pretty well understood for sure, but there&#x27;s no ISO standard defining how to interpret an SQLite file in excruciating detail. Same goes for competing implementations, Zip and XML have a much smaller API surface than SQLite, whose API, apart from a bunch of C functions, is the SQL language itself. Writing an XML parser is not a trivial task, but it&#x27;s still simpler than writing an SQL parser, query optimizer, compiler, bytecode VM, full-text search engine, and whatever else Sqlite offers, without any data corruption in the process. If Open Office used SQLite, its programmers would inevitably start using its more esoteric features and writing queries that a less-capable engine wouldn&#x27;t be able to optimize too well.This isn&#x27;t a concern for most software. If you&#x27;re writing a domain-specific, closed-source application where interoperability with other apps or ISO standardization isn&#x27;t a concern, SQLite is a perfectly fine file format, but as far as I understand the situation, those concerns did exist for Open Office. reply gwd 0 minutes agoparent> Writing an XML parser is not a trivial task, but it&#x27;s still simpler than writing an SQL parser, query optimizer, compiler, bytecode VM, full-text search engine, and whatever else Sqlite offers, without any data corruption in the process.Just to clarify: You don&#x27;t actually need to implement all that for it to be a standardized file format, any more than you need to implement all the spreadsheet functionality to be able to read a LibreOffice spreadsheet. All you need to do is to be able to reconstruct the tables. There&#x27;s no reason, having reconstructed the tables, you couldn&#x27;t write your own imperative code in the language of your choice to go over them and get whatever information you wanted. reply coliveira 4 minutes agoparentprevBut you don&#x27;t need a standard, because all interaction between applications and the document is made through SQL. And SQL is standardized (at least the parts that matter). If you have concerns about compatibility, make sure that the document can also be accessed through other databases (like mysql). reply orra 2 minutes agorootparentBut other databases cannot access sqlite databases, because the file format is internal... reply stareatgoats 1 hour agoprevAs an aside, this blew me away. I can hardly believe it. No nested query required?> SELECT manifest, versionId, max(checkinTime) FROM version;> \"Aside: Yes, that second query above that uses \"max(checkinTime)\" really does work and really does return a well-defined answer in SQLite. Such a query either returns an undefined answer or generates an error in many other SQL database engines, but in SQLite it does what you would expect: it returns the manifest and versionId of the entry that has the maximum checkinTime.)\" reply mrighele 41 minutes agoparent> Such a query either returns an undefined answer or generates an error in many other SQL database engines, but in SQLite it does what you would expect:It may be a useful functionality, but it is NOT what I would expect such a query to return, to be frank.Also you don&#x27;t need a nested query in this specific, you can order by checkinTime and limit the result to one.> select manifest, versionId, checkinTime from version order by checkinTime desc limit 1or something like that. This should work in SQlite and Postgresql at the minimum. I think to remember that in Oracle you have to use \"where rownum=1\" so indeed you have to use a nested query. I don&#x27;t know about other databases. reply globular-toast 3 minutes agorootparentThe interesting thing is if you want more than one record, like you want the latest version number for each document ID. In SQLite you could do: `SELECT documentId, versionId, max(checkInTime) FROM version GROUP BY documentId`. In Postgres you can do `SELECT DISTINCT ON (documentId) documentId, versionId, checkInTime FROM version ORDER BY versionId, checkInTime DESC`.See: https:&#x2F;&#x2F;www.sqlite.org&#x2F;lang_select.html#bare_columns_in_an_a... reply mwexler 47 minutes agoparentprevIt&#x27;s not really what one would expect in SQL, but SQLite often defies expectation. In this case, handy, but non-standard. reply globular-toast 14 minutes agoparentprevI think this is a convenient side-effect of the implementation which was later turned into official behaviour. A bit like Python dictionary key ordering.In Postgres you can do similar things with a DISTINCT ON query.I always found this one of the hardest simple things to do in SQL. reply p4bl0 4 hours agoprevI&#x27;m currently working on an application where I use SQLite as the file format. I want to keep a usual workflow for users where you can make edit to your document and it only changes the file when you save it.So to open a file I copy it into the :memory: database [1], then the user can do whatever manipulation they want and I can directly make the change in the database I don&#x27;t need to have a model of the document other than its database format. And to save the document I VACUUM [2] it back to the database file. It works quite well, at least for reasonably sized file (which is always the case for my app) :).[1] https:&#x2F;&#x2F;www.sqlite.org&#x2F;inmemorydb.html[2] https:&#x2F;&#x2F;www.sqlite.org&#x2F;lang_vacuum.html reply rakoo 1 hour agoparentWhy do you use a secondary, volatile database ? Performance-wise you won&#x27;t gain a lot more (we&#x27;re talking about a user editing a file, so not even 1 write per second).A proposal: write directly, and automatically in the database. No more Save button. There are multiple advantages:- the system is crash-resistant. I like taking the approach of CouchDB where the only correct way to close the system is to crash it. That way a crash is an expected situation that you actually account for, not a special case that you might forget- there is only one database. Less code, fewer bugs.- it is safe. A write to SQLite works or doesn&#x27;t work, there is no in-between. As said in the VACUUM doc you point to: \"However, if the VACUUM INTO command is interrupted by an unplanned shutdown or power lose, then the generated output database might be incomplete and corrupt\"- it is how SQLite was intended to work. And because of that, you won&#x27;t have to think about it for the lifetime of SQLite reply fluidcruft 1 hour agorootparentThere is nothing I hate more than an app that modifies files secretly when I open them. Then I have to get all defensive to copy files before I open them to keep them intact. You may not see the problem with changing the checksum or hash of a file, but silently tampering with files is a nightmare in many domains. If you open a file and accidentally change something trivial (some apps like to store things like presentation state i. e. window positions, last page viewed, zoom level, ...)For example in many regulated domains such as human subjects research files must be approved and only approved files may be used. \"Is this version of the consent document the version that the IRB approved?\" Well let&#x27;s see... (1) file modification date is after the approval date and (2) checksums do not match.Not to mention that writing a single byte of content to a filesystems marks the entire blob as needing backup.The fact is the filesystem is the user&#x27;s database, save is commit, and it should be under the users control because application developers do not have the faintest idea about user context. reply rakoo 33 minutes agorootparentYou are right, this is a use case I absolutely did not take into account, but I want to separate user-defined actions and app-defined actions. A level of zoom is something a user does to read a document, but I wouldn&#x27;t consider that as data to be persisted automatically, unlike characters typed or a font chosen. I value the idea of persisting it, but that would be a user-specific action (\"Save view\" or something like that)In the case of checksums in a database, that is why read-only modes should be used and I don&#x27;t see what automatically saving would change. If anything, when the user zooms on a document in read-only mode, either it shouldn&#x27;t be stored or storing it should trigger the same flow as modifying the document reply knome 1 hour agorootparentpreva save button is still good, as it allows you to keep specific checkpoints.but the save button could simply tag specific save points in a larger table.if the format can roll up changes to compress them, they also indicate where which variants need to be kept indefinitely. reply rakoo 50 minutes agorootparentIn this view the save action is more like a commit, where the user manually checkpoints and also offers a simple description of why this is an important point. But in my view all intermediary points also need to be saved, because the user might have forgotten to explicitly checkpoint, and might still want some undo&#x2F;redo capability that is more granular than just checkpoints. reply nextaccountic 3 hours agoparentprevThis means that like a regular app, you lose data if the app crashes or there is a power loss.It&#x27;s much better to save after each operation in a temporary place (probably in ~&#x2F;.local&#x2F;share&#x2F;application&#x2F;yourapp, using XDG directories), and when the user clicks save, just copy the file into the desired location. That way, if there is a power loss and you reopen the app, it opens right back where it was doing (losing maybe he last few seconds of changes, but not all unsaved data) reply scherlock 2 hours agorootparentIf you have a db, why not just model it as unsaved data? I.e. all changes get stored to the db, but have a flag of unsaved. If you open up a file and there are unsaved changes, you can prompt the user to either make them saved or discard them. reply mort96 2 hours agorootparentThat feels like it requires the data model to be very different? The file format would essentially need to be a list of changes, with a \"committed\" flag.Like, if someone changes some text in a paragraph, you can&#x27;t just model that as \"this paragraph now contains this new text\". You have to model it as \"this paragraph used to contain this text, but an uncommitted change changed it to this other text\". User deletes an image? You have to still store the image and all the references to it, but with an uncommitted change to delete the image and remove the references to it.And maybe that&#x27;s a good thing, maybe a git-like system where the history of every change is tracked is what you want. But it certainly doesn&#x27;t feel like it&#x27;d be appropriate for every application and file format. reply julesnp 56 minutes agorootparentYou wouldn&#x27;t necessarily need to track every change, you could just have 2 tables, one which contains the last \"saved\" version of the document, and one which contains the last modified version of the document. Upon opening after a crash, if there is a more recent modified version, the program will ask if you want to load that version. reply Someone 3 hours agorootparentprev> and when the user clicks save, just copy the file into the desired location.To be perfectly safe, you want to rename it, not copy it. If there’s a power loss during copying, you may endcup with corrupted data.Renaming is, to coin a phrase, “more atomic” than copying (on Linux, the OS says it is atomic. ISO C says it, too, but POSIX doesn’t (https:&#x2F;&#x2F;pubs.opengroup.org&#x2F;onlinepubs&#x2F;000095399&#x2F;functions&#x2F;re...: “This rename() function is equivalent for regular files to that defined by the ISO C standard. Its inclusion here expands that definition to include actions on directories and specifies behavior when the new parameter names a file that already exists. That specification requires that the action of the function be atomic”)Also, filesystems may have bugs, hardware may lie about syncing to disk, and network shares can be finicky.Doing this properly isn’t as easy as one would think. You’ve to make sure to sync the file to be written and you’ll have to handle the case where the save location is on a different file system than your temporary file. If so, you’ll have to create a copy on that file system first.I think many tools do not check whether they need to work cross filesystem and just write their scratch files to the save directory with a different name and then rename them.Of course, that means you always need twice the disk space on the target disk to do a save. That used to be a problem almost everywhere, but nowadays mostly is restricted to embedded systems and USB sticks.In this case, however, SQLite will do a lot for you, and probably better than you would do it. It claims (https:&#x2F;&#x2F;www.sqlite.org&#x2F;atomiccommit.html#_multi_file_commit):“SQLite allows a single database connection to talk to two or more database files simultaneously through the use of the ATTACH DATABASE command. When multiple database files are modified within a single transaction, all files are updated atomically. In other words, either all of the database files are updated or else none of them are. Achieving an atomic commit across multiple database files is more complex that doing so for a single file. This section describes how SQLite works that bit of magic.”However, about VACUUM INTO, it says (https:&#x2F;&#x2F;www.sqlite.org&#x2F;lang_vacuum.html):“The VACUUM INTO command is transactional in the sense that the generated output database is a consistent snapshot of the original database. However, if the VACUUM INTO command is interrupted by an unplanned shutdown or power lose, then the generated output database might be incomplete and corrupt. Also, SQLite does not invoke fsync() or FlushFileBuffers() on the generated database to ensure that it has reached non-volatile storage before completing.”So, I don’t think doing “VACUUM INTO” is sufficient to guarantee that you get a good copy of your data on disk. reply cduzz 2 hours agorootparentWell, copying is simply not atomic in linux; directory entry operations (rename, link, unlink) are atomic. There is a definition somewhere that says how many bytes may be written atomically; that&#x27;s it -- past that writes are not atomic.The prior comment of \"model user interactions in the database\" seems spot on -- just keep track of what the user&#x27;s doing as unsaved data in the database and commit it (in the appropriate way) to the DB as it happens; save is just another user action.Presumably sqlite has figured out how to write to the filesystem without corrupting itself even in a variety of adverse scenarios?If not, commit to a temporary copy of the DB that gets renamed to the \"main\" name periodically or when the app closes. There&#x27;s an xzzzbit meme there somewhere, yo. reply NavinF 58 minutes agorootparentprev> Linux, the OS says it is atomicnitpick: At least on some filesystems if you rename a.txt to overwrite b.txt and the machine crashes, you might end up both a.txt and b.txt hardlinked so they contain the same data.Of course this is no big deal since b.txt is still updated atomically so it contains the new data (assuming a.txt was fsynced) or the old data. I assume nobody depends on a.txt being deleted simultaneously. reply avereveard 2 hours agorootparentprevGood ole .filename.swp reply SanderNL 2 hours agorootparentprevYou are right and like you explained this is trivially easily fixed by autosaving regularly.What I have trouble imagining is people working with documents on computers for more than a few years yet somehow failing to develop the Always Save Instinct. I regularly catch myself saving unreasonably often. reply chrisshroba 2 hours agorootparentI used to have that instinct but lost it in the age of auto save. The applications (web or native) I use most often all do it for me: Google docs, Dropbox paper, notion, vscode. I don’t think I’m alone in this! reply dsego 1 hour agorootparentI actually tried using open&#x2F;libre docs a few years ago just because of it being open source. I was trying to make a point of using locally installed software and avoid google products. Then the thing crashed and I lost an hour of work because it didn&#x27;t save a temporary version. That&#x27;s when I gave up on it for good. reply dsego 1 hour agorootparentprevThat may have been true years ago in win 95 or xp days. The modern paradigm starting with google docs is that things are automatically saved and even always sharable through the cloud, making manual saving actually an atavistic leftover of a bygone era. reply iggldiggl 1 hour agorootparentWhat if I actually don&#x27;t want any changes saved because I&#x27;ve only opened a document for reference purposes? reply ghkbrew 1 hour agorootparentThen you proactively prevent changes. Either \"open a copy\" or \"open in readonly mode\".If you make saving the default you have to manually not save. It&#x27;s a trade off versus default no saves with manual saves reply NavinF 50 minutes agorootparentprevModern apps like gdocs have a toggle for switching between read-only, suggest changes, and editing. If you forgot to toggle, you can just open version history and revert. MS Office also had version history for nearly a decade. reply ReactiveJelly 1 hour agorootparentprevHave a \"Read-only\" checkbox. For the love of God, have a \"read-only\" checkbox. reply josephg 2 hours agorootparentprevUsers shouldn’t ever need to adapt to computer crashes like this. Software should always auto save or have recovery files or something. As a principle, software should hold anything a user inputs with reverence. reply SanderNL 2 hours agorootparentI agree. Maybe as a dev I&#x27;ve become cynical and don&#x27;t trust anything. Least of all some app holding my document.Makes me think of the \"Voting software\" xkcd: https:&#x2F;&#x2F;xkcd.com&#x2F;2030\"I don&#x27;t quite know how to put this, but our entire field is bad at what we do, and if you rely on us, everyone will die.\" reply ilyt 42 minutes agoparentprevWe used something similar (DB doing caching run in memory but saved periodically on disk) but with backup APIhttps:&#x2F;&#x2F;www.sqlite.org&#x2F;backup.html reply nyanpasu64 4 hours agoparentprev> The VACUUM command works by copying the contents of the database into a temporary database file and then overwriting the original with the contents of the temporary file. When overwriting the original, a rollback journal or write-ahead log WAL file is used just as it would be for any other database transaction. This means that when VACUUMing a database, as much as twice the size of the original database file is required in free disk space.> The VACUUM INTO command works the same way except that it uses the file named on the INTO clause in place of the temporary database and omits the step of copying the vacuumed database back over top of the original database.Do you use VACUUM (uses a write-ahead log to survive power-off) or VACUUM INTO (as far as I can tell, it doesn&#x27;t survive power-off during writing, and might corrupt the existing file contents if the filename already exists)? reply justsomehnguy 3 hours agorootparent>> The file named by the INTO clause must not previously exist, or else it must be an empty file, or the VACUUM INTO command will fail with an error.EDIT: there is no difference between VACUUM&#x2F;VACUUM INTO - they both write to a new file (COW) it&#x27;s just VACUUM [NOT INTO] does mv temp.sqlite originalfile.sqlite after that, while VACUUM INTO does not. reply remram 2 hours agoparentprevWhy not use a transaction? reply rewmie 3 hours agoparentprev> I can directly make the change in the database I don&#x27;t need to have a model of the document other than its database format.I don&#x27;t get your point. Are you saying that you don&#x27;t need to have a model of the document other than the model of the document? What&#x27;s the nuance I&#x27;m missing? reply torstenvl 1 hour agorootparentAn in-memory data model often differs from the serialized data as it exists on disk. For example, emacs uses a gap buffer for text files; but it outputs plain linear text to disk.Programmers often have to make software design decisions around how to represent a file in memory in order to manipulate it. For example, if I&#x27;m writing an HTML editor, should I mostly treat it like a text file (maybe a gap buffer) with syntax highlighting and auto indentation as an afterthought? Or should I maybe load the whole thing into a tree? What are the robustness and performance characteristics of each?The commenter above was saying that using SQLite made that decision easy. He could keep traditional (or \"atavistic\" per the commenter upthread, depending on your perspective) load&#x2F;save semantics while also making the data model easy to work with. reply socksy 2 hours agorootparentprevI suppose this is in the context where you will be syncing up the changes to a backend server which will also be storing the document in an SQL database. Normally, you might expect that data format on the client to be JSON&#x2F;XML&#x2F;something else, and you&#x27;d need to maintain logic that marshalls the document representation SQLIn-memory representationDisk format.With SQL on the client, in theory you only now need to maintain SQLIn-memory representationObviously I&#x27;m skirting over the format you would use to send either entire documents or partial updates of documents over the wire. reply nyanpasu64 4 hours agoprevI was optimistic that Audacity adopting SQLite would be a substantial improvement in its file saving capabilities. In practice I encountered many gotchas:- On Linux, saving into a new file onto a root-owned but world-writable NTFS mount created in &#x2F;etc&#x2F;fstab, fails due to permission errors or something. Saving into an existing file works as usual.- Files are modified on disk when you edit the project in the program, creating spurious Git diffs if you check Audacity projects into Git as binary blobs. And when you save the file, old and deleted data is left in the SQLite file until you close the project&#x27;s window (unlike saving a file in a text editor), and you can accidentally commit that into a Git repo if you don&#x27;t close the window before committing. (I recall at one point that you had to manually vacuum the .aup3 file, but now closing the window is sufficient.) I&#x27;m getting Word 2003 Fast Save vibes. reply rini17 4 hours agoparentYes it should replicate the functionality user expects - save everything into temporary file and overwrite the original file only on explicit save action.As for Git, it would benefit from using text format specifically aimed for easy diffing&#x2F;merging. No idea how easy the sqlite dump is in this regard. reply gwd 3 hours agorootparent> As for Git, it would benefit from using text format specifically aimed for easy diffing&#x2F;merging. No idea how easy the sqlite dump is in this regard.The problem I&#x27;d predict here is that then people would expect to be able to do three-way merges. It might even work correctly a lot of the time, depending on the exact pattern of changes. But my gut feel is that unless the schema were designed just right, there would be possible merges that would result in a database that was valid from SQLite&#x27;s point of view but insane from the application&#x27;s point of view (broke expected variants, etc). reply bawolff 43 minutes agorootparentIf you want to use flat files you should just use flat files. There are plenty of unix tools to treat them like DB.You&#x27;re not going to have a sensible text version of a btree that is reasonably editable by a text editor. reply massysett 2 hours agorootparentprevI set up my Git to use the SQLite dump on SQLite files when using “git diff”. This at least shows me the changes row-by-row, or shows nothing if no changes.I don’t expect to be able to merge though. reply bawolff 3 hours agorootparentprev> Yes it should replicate the functionality user expectsDo users really expect this now a days? Most users use cloud apps, and almost all of those save after every operation automatically. reply raxxorraxor 1 hour agorootparentWhich is a compromise for using browsers really. It isn&#x27;t a good solution and no user really understands this and I believe it is the most hated feature of the new cloud world. Yes, leaving the page open for multiple hours might now allow you to save because your access token expired. No, communication in the background is unreliable too. Autosave is a bad band aid for a bad solution.Doing periodic and automatic saves is good. Doing so on a document \"in production\" is majorly stupid. Not that I want to accidentally validate the busy work dev ops puts us through. reply bawolff 47 minutes agorootparentIts pretty easy to make a cloud app that emulates the traditional working draft&#x2F;save workflow. Browsers all have pretty reliable local storage technology now a days if your network is unstable. I don&#x27;t think this design choice is a compromise of the medium. If anything it seems like if you were going to have to compromise for web you would do it in the other direction so apps are more usable during poor network conditions.I would say the traditional model is a compromise from back when disks were unacceptably slow to be saving constantly. reply chrismorgan 3 hours agoparentprevIt’s also a bit of a bother if Audacity crashes (or is otherwise terminated abnormally), as the cleanup just doesn’t happen at all then, whereas in the past the recovery process would mention the presence of orphaned blocks and allow you to choose to keep or delete them. But when I had a several-gigabyte project that should have only been a couple of hundred megabytes, and needed to save disk space, I finally found a solution suitable for my simple single-track stuff: Mix and Render. Doesn’t change the audio, but allowed it to clean up the detritus on save and exit. But all up, this is clearly an application-level problem, not something inherent to SQLite.Hmm… I think I vaguely recall that Audacity 2 had the concept of a temporary working space, whereas it seems that Audacity 3 just uses the .aup3 file as its working space? Some advantages, some disadvantages.Mildly less on-topic: I looked into Audacity 3’s format, and was utterly baffled by what they’ve done with the project data (what used to be the .aup file). They still encode it as XML, storing it in a single-row table, but instead of just writing it as text, they use a simplistic dictionary coder on it. Just… why? Why did someone go to all the trouble of writing that code? It makes interoperability and inspection much harder, surely harms performance (even if by a trivial amount), and the space saving will be rounding error in every plausible case (like, maybe as much as a few kilobytes out of hundreds of megabytes of audio files). reply anonzzzies 4 hours agoparentprevMy wife uses Audacity all day and every few days there is a corrupt sqlite file (duplicate key) which cannot be (as far as we know) repaired&#x2F;reimported etc from Audacity. I can fix it manually if it&#x27;s important, but usually just throw the file away and things work again. reply justinclift 3 hours agorootparentDuplicate keys in a SQLite file sounds like an audacity bug. :( reply sgarland 1 hour agorootparentMaybe. The SQLite list of gotchas [0] is quite something. NULLs in the PK? Sure. FKs don’t actually do anything unless you pass a PRAGMA? Why not? Etc. I could easily see someone not fully grasping just how much SQLite lets pass by default, and thus not having a test catch it.[0]: https:&#x2F;&#x2F;www.sqlite.org&#x2F;quirks.html reply anonzzzies 3 hours agorootparentprevYeah, it definitely is. And it&#x27;s fixable manually. Kind of the advantage to an open file format with nice tooling. reply regularfry 3 hours agorootparentprevIt also sounds like something that could be manually prevented ahead of time. If you can crack open the file on first save and add the right uniqueness constraint, that should make Audacity crash when it tries to corrupt the data. reply Freak_NL 3 hours agoprevGood article. Although one thing I do like about OpenDocument being just a bunch of XML files in a ZIP archive is that it is fairly easy to generate documents like spreadsheets without using a (potentially hefty) library which knows about the document format.I have a use case where users of a web service want to use data exported as a bunch of rows in a table in a variety of tools. Now, CSV with UTF-8 encoding is of course, totally open, conventional, and workable, but anyone who has ever offered CSV files to end users will know the pain of these users getting stuck when they want to use these files in a spreadsheet application¹. So I saved a sample spreadsheet in OpenDocument&#x27;s ODS and another in that Microsoft XML abomination called OOXML as XLSX, and just figured out the basics of those XML formats. I trimmed the ZIP archives down to the essentials, marked the places where content goes, and just build a new spreadsheet file whenever data is requested in that format. Now I can output CSV, ODS, and XLSX (and JSON thrown in for good measure) of the same data.Doing this with SQLite would be possible of course, just a tad more complex and with a lower development speed. Being able to fire up the office suite, create a template document, and just dig into its XML files in the saved file is a nice feature (although admittedly of niche interest).1: More specifically, users who use Excel in a locale like nl_NL, where CSV files are, hardcoded, assumed to have their columns separated by semicolons, because Microsoft once notoriously decided that the Dutch did not use comma&#x27;s in a comma separated values file. reply gwbas1c 28 minutes agoprevI shipped a product that used both SQLite and XML files.One of the improvements that I made was moving a few tables that contained small amounts of data to xml files. Because these files were small and rarely written; it simplified the data access layer, and simplified diagnostics. (I made sure the files were multi-line tabbed xml.)For \"technical\" people who needed to diagnose the product, asking them to crack open a SQLite database was a huge ask; but for the major part of the product that used SQLite, it was hands-down better than XML files. (An older version of the product used XML files. It had scalability problems because there&#x27;s no good way to make an incremental update to an XML file.)The advantages of XML, specifically, a human-readable format; really only work for small files when the design of the schema is optimized for readable XML. Unfortunately, the need to always rewrite the entire XML file, and the \"complexities\" that come with lots and lots of features will quickly erode XML&#x27;s biggest advantages.IMO: A \"lay\" person needing to muck around with the internals of an office document is fringe enough that learning to use a SQLite reader is an acceptable speed bump. The limitations of XML + Zip, when it comes to random writes in the middle of a file, just can&#x27;t be overcome by Moore&#x27;s law. reply ealexhudson 4 hours agoprevODT was designed to be standardised: while the predecessor format was very similar too, it relies very heavily on XHTML, SVG, and CSS, to name but three (there&#x27;s a lot more).Without being able to call out to existing standards, the ODT spec itself would suddenly become massive. The effort to update the standards appears to be significant and hasn&#x27;t progressed much in recent years already :&#x2F;I think realistically, an Sqlite format could be offered as an option, but the office doc ship has really sailed.Good argument to formalise the spec of Sqlite as a standard though... reply tannhaeuser 1 hour agoprevYeah what if? Then they haven&#x27;t really understood the purpose of markup languages as plain text files for viewing&#x2F;editing using generic text editors. There was no lack of proprietary formats such as MS Structured Format (used by MSO) and it was considered a big success when customers demanded open formats such as SGML&#x2F;XML-based ones in late 90s&#x2F;00&#x27;s. The alternatives aren&#x27;t even sequential (have fragments and cross pointers, etc). Yes they might be faster because they&#x27;re closer to the in-memory representations as used by the original&#x2F;historic app or even primitive memory dumps; marginal speed or size improvements were never a consideration though. And if anything, SQL (almost as old as SGML btw) is a joke as document query language compared to basically any alternative specifically designed for the job (ISO topic maps query language ie. Datalog, XPath and co, SPARQL, DSSSL&#x2F;Scheme, ...) because of SQL&#x27;s COBOLness, non-schemalessness, lock semantics&#x2F;granularity a really bad fit, etc.). reply orf 4 hours agoprevCoupling a file format to SQLite smells wrong.SQLite is good, but it is also fairly unique in this space. Why? Because it’s hard to replicate everything it does, because it does a lot.But… for this case, do we need it do a lot? No, not really. We don’t need the full SQL standard, a query optimiser, etc etc for basic (+ safe) transaction semantics and the ability to store data in a basic table structure.Perhaps there is a better file format we can use, but it would be better if it was decoupled from SQLite. reply jve 3 hours agoparent- Why not? https:&#x2F;&#x2F;www.sqlite.org&#x2F;appfileformat.html- Its size is less than a megabyte: https:&#x2F;&#x2F;sqlite.org&#x2F;footprint.html- 750KB if all features are enabled: https:&#x2F;&#x2F;www.sqlite.org&#x2F;about.html- Looks like fair amount of functionality can be left out when compiling sqlite and with options to influence&#x2F;strip down query planner: https:&#x2F;&#x2F;www.sqlite.org&#x2F;compile.html- And \"SQLite does not compete with client&#x2F;server databases. SQLite competes with fopen()\": https:&#x2F;&#x2F;www.sqlite.org&#x2F;whentouse.htmlIn the end, you don&#x27;t need a database, but a library that gives you database API and behavior. reply orf 3 hours agorootparent> In the end, you don&#x27;t need a database, but a library that gives you database API and behavior.Why do you need a single library that gives you a database API and behaviour?Wouldn&#x27;t it be better to decouple those: provide an open, standard format that enables compact, fast, structured storage that is built to allow transaction&#x2F;atomic updates.If that exists then you can plug sqlite on top of that, or something else. Because you don&#x27;t _need_ any of SQL, or really sqlite to improve the OpenDocument format. You need the storage format.OpenDocument is very different from the pretty scientific&#x2F;niche&#x2F;highly-vendor-locked examples given in replies by others here. Locking this into a format developed by essentially a single person with a single implementation is absolutely mad.But... it&#x27;s less mad if the file format wasn&#x27;t coupled to sqlite. reply colonwqbang 1 hour agorootparentThe Sqlite format is open and the spec is here: https:&#x2F;&#x2F;www.sqlite.org&#x2F;fileformat2.htmlI haven&#x27;t studied the spec in detail but it seems comprehensive.The fact that there also exists a high-quality, stable, public domain reference implementation can&#x27;t really be counted against the format, can it? reply nurbl 2 hours agorootparentprevA great thing about just using sqlite as the format is that you get lots of potential features. Sure most applications don&#x27;t need full SQL power just to save and load data. But then at some point you might want more advanced functionality, or migrate to a new structure. And both you and your users get tools for free, e.g. to extract data or fix problems, or just look around. Other applications can quite easily read your files, without you needing to write various language libraries. Very few projects get around to building that kind of tools for their made up format.I could agree about the single implementation, but if the alternative is making something new up I am not sure in what way that would be better. reply Ensorceled 3 hours agorootparentprevThe complaint is not “it isn’t good” but rather “it is not replaceable”. Since SQLite is so powerful, once you specify it as a format, you are stuck with SQLite forever. reply ncruces 3 hours agorootparentWhich is also \"not a big issue\", since it&#x27;s a recommended Library of Congress storage format, and supported long term:https:&#x2F;&#x2F;www.sqlite.org&#x2F;locrsf.htmlhttps:&#x2F;&#x2F;www.sqlite.org&#x2F;lts.html reply 5e92cb50239222b 2 hours agorootparentIt is somewhat of a problem: the development team is very small, they don&#x27;t take outside contributions (so nobody outside the core team really builds up expertise over time), and the vast majority of tests are proprietary. I hope they have a contingency plan just in case (some sort of a dead man&#x27;s switch that publishes the test suite under a permissible license) as it would probably be quite difficult for others to maintain the same quality without those tests, or re-implement them in a reasonable time frame. reply fweimer 1 hour agorootparentBut that equally applies to getting critical bug fixes for your particular usage scenario of SQLite. It&#x27;s not just about the viability as a storage format.For the latter, because the stored data has such a simple format and the implementation has so few dependencies, I expect it will be very easy to get your data out for a long time to come. It&#x27;s going to be tougher if you have business logic in views or other SQL expressions, of course, and if you rely on SQLite&#x27;s particular approach to data types (as in “values have types“, but not much more). reply floppydiscen 1 hour agorootparentprevI&#x27;m pretty sure this is why libsql was created https:&#x2F;&#x2F;github.com&#x2F;libsql&#x2F;libsql reply Ensorceled 3 hours agorootparentprevWhich is why I was clarifying the original complaint and not supporting the original complaint. reply fweimer 2 hours agoparentprevI found the transactional aspect surprisingly difficult, especially with concurrent file access. SQLITE_BUSY handling was quite hard at the time. I know that serialization failures are expected in transaction processing, but for SQLite it was very difficult to tell persistent failures (say, due to self-deadlock) apart from transient concurrent update problems. For transient failure, you can re-execute the closure defining the transactional operation, but for persistent failure, that&#x27;s of course pointless.Part of the problem is that sqlite3_stmt combines aspects of both prepared statements and result sets. There is a tendency to keep them around to cache the compiled bytecode (prepared statement), but your might code might stop mid-iteration (result set), maybe holding a lock at this point. This can lead to surprising lock-upgrade failures. In the end, I wrote extensive error reporting using sqlite3_next_stmt, sqlite3_stmt_busy, sqlite3_sql, just to weed out those issues. The entire transaction retry code I wrote is full of optional logging and many comments, even though it was just for my own personal use. Before that, I wrote transaction retry logic for PostgreSQL, and that was so much easier (but it was before fully SERIALIZABLE transactions arrived).The other surprise is that “ A transaction committed in WAL mode with synchronous=NORMAL might roll back following a power loss or system crash.” (https:&#x2F;&#x2F;sqlite.org&#x2F;pragma.html#pragma_synchronous), but that wasn&#x27;t relevant to my application. reply HelloNurse 1 hour agorootparentIf you retry your write several times and it doesn&#x27;t succeed you can tell the user it is a persistent failure without agonizing too much over the diagnosis: it is persistent enough to be a significant problem, even without proof that it is an application bug.Who would attempt to make concurrent writes to an application document format? And how wouldn&#x27;t such an attempt be a user mistake? Failing to write is the solution, not the problem. reply amiga386 1 hour agoparentprevExactly. Some formats are designed, first and foremost, for interchange. SQLite is pitching that you, as an \"app\" owner, force the SQLite format upon your users to make it a de-facto standard, without putting the work in to make it a de-jure standard.Show me a formalised ISO &#x2F; IEC &#x2F; ANSI &#x2F; ETSI SQLite standard that the Richard Hipp and his company never deviates from, and the full legal search to ensure there are no patents that might affect it, and show me the multiple compatible implementations of SQLite that _all_ have these touted advantages, and _then_ we can talk about prosletizing it as a file format. If they don&#x27;t, they&#x27;re saying \"take a hard dependency on a single-source implementation, and make all your users take it too\".XML is a formal standard. ASN.1 is a formal standard. JFIF is a formal standard. Even ZIP is a formal standard (adopted as part of standardising OpenDocument: ISO&#x2F;IEC 21320-1:2015)The most important thing about a document is that everyone _else_ can read it. Saving time on writing updates to disk is an irrelevant sideshow. Did we learn nothing from Microsoft perverting the standards bodies to try and keep its lock-in?https:&#x2F;&#x2F;arstechnica.com&#x2F;uncategorized&#x2F;2008&#x2F;10&#x2F;norwegian-stan...> A letter of resignation written by the departing members and made public by The Inquirer accuses the standards body of folding to pressure from Microsoft, violating its own procedural rules, and ignoring the analysis of the technical committee tasked with evaluating OOXML. reply severak_cz 4 hours agoparentprevSQLite is already used for exactly this purpose. It&#x27;s used as OGC GeoPackage and Mabox&#x2F;Maptiler datasets use this. reply rakoo 1 hour agoparentprevTo add to your point, fossil, the versioning system designed by the people of SQLite, and using SQLite, doesn&#x27;t even use SQLite as a file format. It&#x27;s all a bunch of blobs, each with its own format, that happen to be stored on SQLite. SQLite offers safe storage and a bunch of helpful indexes and views, but is not necessary for fossil-the-data to work. reply livrem 17 minutes agorootparentLooking in sqlite.fossil there are 27 tables in it and most are not used for storing blobs. I know when looking up how to do things in the past the answer has sometimes been \"run this SQL query\". The event table for instance looks like a list of all commits with dates and comments etc. There is a config table that looks like the kind of stuff git stores in .git&#x2F;config (URL to upstream repo etc) and so on. Well, yes there are some blobs in it too. reply punnerud 4 hours agoparentprevHave you checked the Apple apps? Most of them use SQLite as storage format. iMovie, iPhoto, Voice recording…Same with Docker. Can’t be that wrong? reply tuyiown 3 hours agorootparentApp using a format specific to their own and unique implementation, that ends up kind of proprietary is perfectly ok.Using it for a open specification which target is cross implementation compatibility makes the move way more hazardous. Meaning, every implementation has to run on environment targetable and compatible wit sqlite or has to re-implement a compatibility layer on something complex enough that you only reliable definitive source of truth is the very famous sqlite test suite.It the same reason why Web SQL has being abandoned: if sqlite is the sole api implementor, it takes precedence on any others specs, and you have no control on your standard.I would be 100% for a specification on how to map open docs files to an relational structure, though, with a well know sqlite-backed implementation. reply bawolff 3 hours agorootparent> Meaning, every implementation has to run on environment targetable and compatible wit sqliteWell i get what you are saying, sqlite has been ported all over the place. It probably wouldn&#x27;t be the limiting factor portability wise. reply tuyiown 1 hour agorootparentYes, right now, no problem, and there little foreseeable future where a sqlite ported to anything would pose a problem. Still, decisions with no way back like this one requires extra cautions. reply JimDabell 3 hours agorootparentprevThe Apple apps are using Core Data, which uses SQLite as its persistent store by default. So Apple could in theory migrate away from SQLite by changing Core Data’s behaviour without any application-level impact. So in a way, these applications are already decoupled from SQLite in the way the parent comment suggests. reply out_of_protocol 4 hours agoprevOther example: raster map tiles (basically up to millions of tiny square pictures)Zip vs tar vs filesystem vs sqlite. Tested all these scenarios, and sqlite was the fastest and the smallest, even beating plain archives with no overhead reply vetinari 4 hours agoparentMany filesystems have an issue with tens of thousands or more files in a single directory, which is exactly what you can get with map tiles. No wonder sqlite is faster. reply m4rtink 3 hours agorootparentYeah, that&#x27;s why sqlite was adopted for this back then - many devices still used FAT32 on the storage volumes where tiles we often stored&#x2F;cached and that had horrendous small file performance - a plain white 130 Byte PNG tile could result in 64 kB being used. reply Tigress8780 2 hours agorootparentOnce we had to ship millions of extremely small files to our customer, we ended up throwing them into a MongoDB and serve them with a web server. It worked great.We tried to use an image of traditional filesystems (ext4 and fat32), but with most files being under 1 KiB, it was super wasteful. reply nxobject 2 hours agorootparentThis is an extremely low quality comment, and I accept any downvotes, but I can&#x27;t resist: would you say that MongoDB was web scale? reply vetinari 3 hours agorootparentprevIt is not just fat32 and overhead up to cluster size; once I had 800k tiles in a single directory on ntfs.It was unusable. The only thing that I was able to do is to tar it up and move to a machine with xfs, where I was able to sort it up into more balanced subdirs and then move it back (for processing using windows-only tool). Just tarring that single directory up took several days. reply 5e92cb50239222b 3 hours agorootparentIt&#x27;s not just ntfs. I tested this a few months ago in a pretty unscientific manner using ~50 million files in one directory.btrfs was unusable (not only that particular directory, but the whole filesystem became noticeably slower).ext4 was ok. xfs didn&#x27;t break a sweat. I don&#x27;t recall any practical difference when compared against a nested tree like ├── aa │ ├── aa │ │ └── aaaaf3ee5e6b4b0d3255bfef95601890afd80709 │ ├── ab │ └── ac └── ab ├── aa ├── ab └── ac replyrobertlagrant 4 hours agoprev> since OpenDocument predates SQLiteThis shocked me. Impressive how far SQLite&#x27;s come in such a short space of time. reply capableweb 4 hours agoparentHmm, me too, and Wikipedia says:> OpenDocument - Initial release: 1 May 2005; 18 years ago> SQLite - Initial release: 17 August 2000; 23 years agoWonder what gives. reply paradox460 4 hours agorootparentOpenDocument traces it&#x27;s ancestry to OpenOffice XML format, which traces it&#x27;s ancestry to StarOffice, which was xmlized around the time Sun bought it in 1999 reply filmor 3 hours agorootparentNot to be confused with Office Open XML (OOXML), Microsoft&#x27;s \"standard\". reply ReactiveJelly 1 hour agorootparentprevThat&#x27;s fair, I wouldn&#x27;t standardize on a 1-year-old database. reply capableweb 3 hours agorootparentprevThanks for clarifying the somewhat messy history of the format! reply robertlagrant 4 hours agorootparentprevHah - that tallies with my instinct on ODF at least. I&#x27;m confused too, then. reply isoprophlex 3 hours agoprevMan do I love SQLite.Over the past 1.5 yrs I&#x27;ve build a computer vision tool from recording hardware&#x2F;software, to derp learning pipelines, to front-end; we had some requirements on the recording side that were difficult to solve with existing solutions (storing exactly timestamped camera frames, gps data, car telemetry and other metadata).Using a SQLite-backed data format for the video recordings made implementing things by ourselves super straightforward. reply sgu999 3 hours agoparentI&#x27;m working on a similar problem and I&#x27;ve been struggling to convince all my colleagues that we should sqlite most things. By any chance do you have some public code, or blog posts to share? reply isoprophlex 1 hour agorootparentNot in public repos, but sure. Drop me a line, hn at rombouts dot email. reply regularfry 3 hours agoparentprev> derp learning pipelinesThis accurately describes the majority of my efforts, too. reply isoprophlex 1 hour agorootparentHonest to god this was an unintentional typo, but I decided to leave it in as it was just too juicy reply cm2187 2 hours agoprevSqlite format is smaller than the original format only because xml is super verbose, so any uncompressed binary format ends up being less than lightly zipped xml.But sqlite files aren&#x27;t small. One thing I don&#x27;t understand is why they don&#x27;t do string deduplication in sqlite (as in you only store a string once and every other occurence is just a pointer to that string). It seems such an obvious and easy way to reduce file size, memory consumption and therefore increase performance (less I&#x2F;O). Is there a technical reason why this would not be desirable? reply The_Colonel 34 minutes agoparentIf you have the same (long-ish) string repeating many times in a database, it points to a DB schema needing normalization. reply Etheryte 1 hour agoparentprevMy first guess is that if you always store the full string you don&#x27;t need to scan the database to see if you already have the same string. Essentially you choose to use more space but reduce load. Regardless of whether you do the string deduping on inserts or async later on, you have to do it at some point and the unpredictable performance overhead might be undesirable. reply cm2187 1 hour agorootparentWell it should be a dictionary lookup, it should be pretty fast and predictable. And for freeing it up, it should be a good candidate for reference counting. reply Lockal 1 hour agoprevSadly they did not include bad sides:1) Vulnerabilities: not only in SQLite, but also in wrappers like https:&#x2F;&#x2F;nvd.nist.gov&#x2F;vuln&#x2F;detail&#x2F;CVE-2023-326972) Lack of transparency: zip with xml&#x27;s contains only xml&#x27;s; meanwhile SQLite contains by design all kinds of traces with sensitive information or empty blocks. Attempts to fix these issues removes benefits that were mentioned.3) Lack of implementer support. It was one of the reasons for WebSQL deprecation many years ago.4) Lack of standardization for file format. SQLite does not even promise forward compatibility, only backward one. Which means that new documents might not open in old software, or vendor should fork SQLite and only backport security patches. reply internetter 47 minutes agoparent> Vulnerabilities: not only in SQLite, but also in wrappers likeYes, parsing encoded files tends to introduce vulnerabilities. ZIP parsers have had plenty of vulnerabilities. This is not exclusive to SQLite.> Lack of transparency: zip with xml&#x27;s contains only xml&#x27;sBoth zips and sqlite cannot be read with a text editor. Both are open formats with widely available tools to read them. The sqlite binary might, in fact, be more widely available than unzipping tools.> meanwhile SQLite contains by design all kinds of traces with sensitive information or empty blocks.Elaborate?> Lack of implementer support. It was one of the reasons for WebSQL deprecation many years ago.I don&#x27;t understand how this is relevant?> SQLite does not even promise forward compatibility, only backward one. Which means that new documents might not open in old softwareNeither does OpenDocument. SQLite is actually more solid in this regard – forwards compatibility is still a thing unless new features are used. reply jmull 55 minutes agoparentprev4) is enough for me, so I agree with your general point, but 1) 2) and 3) aren&#x27;t really cons for SQLite.1) Makes sense only if the average XML parsers and zip libraries in use have fewer vulnerabilities and are actively maintained as well.2) You can store sensitive data in a SQLite database or XML file, there&#x27;s no real difference. You can clean up a SQLite database pretty easily if you want and that doesn&#x27;t take away all the benefits.3) What does implementer support even mean? I believe they are open to custom work... WebSQL died because it doesn&#x27;t make sense to pretend SQLite is some kind of standard -- that brings us back to 4), which is the valid reason to avoid SQLite.Actually, your 4) is worded too strongly. They say they&#x27;re committed to forward compatibility as long as you don&#x27;t use the new features. That makes forward compatibility the decision of the app: an app can have forward compatibility and not use newer features OR lose forward compatibility and use newer features. reply nuc1e0n 3 hours agoprevIt&#x27;s somewhat off topic I know, but is there something like sqlite but tailored for hierachical data? Like a xml document store rather than for relational data like sqlite is. reply layer8 2 hours agoparentThere’s ASN.1 for hierarchical data with a schema. It doesn’t provide a query language though. reply chadcmulligan 57 minutes agoprevAutoCAD uses a database as its file format, it is fairly slow. reply CodeCompost 2 hours agoprevThere really should be a \"NoSQLite\" or something equivalent to store hierarchical data instead of normalized data. reply remram 2 hours agoparentYou can probably use SQLite for that, with a single key-value table. reply ttyprintk 1 hour agorootparentThe json* family of tree and table functions are nowadays built in. reply kunley 3 hours agoprevLove the vibe of artivles, which present let&#x27;s say reason-driven development vs habit-driven.Why habit? Well, I can imagine back at the time OpenOffice was a fresh project, it went like this: \"XML is going to stay forever and everybody uses XML, so ofc we use one... oh, it is so big! And there are many files, so we just zip&#x27;em\"...To be fair, the author of this excellent article doesn&#x27;t even say about getting rid of XML in this format- but that could also be achieved by storing stuff in a SQLite file. Usage of XML was habitual thinking there- and not very visionary, as the format is dead now... reply galangalalgol 2 hours agoparentCould you clarify the \"XML is dead\" comment? Don&#x27;t all the major document formats still use zipped xml? I had to interface with an xml format recently, and that isn&#x27;t something I ever did, and when I went looking for a crate that parses an xml schema I kept running across this whole xml is dead thing. But it still seems to be everywhere. reply tpm 3 hours agoparentprev> Well, I can imagine back at the time OpenOffice was a fresh projectOpenOffice was born when Sun bought StarOffice, which was initially released in 1985 (on Z80 and certainly without any XML). So the project itself was far from fresh. OpenDocument was developed from OpenOffice.org XML format which was developed after Sun bought StarOffice in 1999. At the time XML was not used everywhere, but it was very much in vogue, certainly at Sun where the official line was that Java (created at Sun) and XML are going to conquer the world. reply im3w1l 3 hours agoprevI don&#x27;t want people to read my drafts. That could be highly embarassing, and they should not make it into the final saved document.Past version and undo history should be stored separately from the document. They should be stored out of tree where they wont be commited into some git repository or be automatically synced or anything like that. reply eviks 1 hour agoparentThen don&#x27;t give people access to your drafts but exported versions without history? Why put the limits on the efficiency of a format by forcing it to store changes elsewhere? reply regularfry 3 hours agoparentprevI want to be able to read my drafts, until I decide to bake a publication version. reply im3w1l 2 hours agorootparentDid you read the other part of my comment? Where I said to store the draft, but not in the document itself? reply regularfry 46 minutes agorootparentI did. reply 3cats-in-a-coat 1 hour agoprevOpenDocument is zipped images and XML. Implying you parse the entire format and put it in RAM. And frankly I don&#x27;t see how SQLite can improve this. Well XML isn&#x27;t ideal, but it&#x27;s zipped, so there&#x27;s no huge penalty in size here.All benefits SQLite&#x27;s article lists (and I love SQLite to death by the way) can be implemented by having SQLite be the runtime model of the document. On disk and in memory. But SQLite doesn&#x27;t need to be the transport format. In fact SQLite can easily get bigger than the current format, SQLite is full of unused space when you mutate it around, it can get fragmented and sparse. And if you need to optimize it every time, then the \"fast save\" etc. benefit goes away.There are formats which do need delta updates and quick indexed look-ups without fully loading the file in RAM, and this is why so many apps do use SQLite as a file format. I just feel OpenDocument was a bad pick to use SQLite for in this hypothetical scenario. reply iefbr14 4 hours agoprevWhy only documents? How about a SQLitefs? reply pgeorgi 3 hours agoparentWinFS (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;WinFS) without the mssql Engine? reply iefbr14 3 hours agorootparentOr this: https:&#x2F;&#x2F;github.com&#x2F;narumatt&#x2F;sqlitefs reply throwaway894345 1 hour agoprevIs SQLite’s disk format an open, versioned standard? Or is it just “however SQLite saves data to disk”? reply vxNsr 1 hour agoprevI’m curious to know what a gsheet&#x2F;doc&#x2F;slide file actually is under the hood. I as the user am only ever presented with a link, there’s no way to download a gsheet in its native format. reply littlecranky67 4 hours agoprevdeleted. reply littlestymaar 4 hours agoparent> Nobody really believes that OpenDocument should be changed to use SQLite as its container instead of ZIP. […] Rather, the point of this article is to use OpenDocument as a concrete example of how SQLite can be used to build better application file formats for future projects. reply vmfunction 4 hours agoprev [–] At this point, why are we still using JSON&#x2F;XML when there is SQLite for new projects? Stop the non sense of JSON&#x2F;XML. SQLite is like json, but very queryable. Just send SQLite files around.MongoDB also saves document db type of store space just FYI. reply constantly 4 hours agoparentAny text editor in the world, even the ones that ship with the most barebones shells, can open json and xml and present their data to the user.SQLite files require opening in a DB terminal or using special software to even get to the point where one can see what’s there at all. Further the entire internet basically natively supports XML and JSON. reply vmfunction 3 hours agorootparentThat is a good argument, however many people like some big game development company start to ship with GB of json file, at that point just use SQLite. It will be faster to query load. Also if you look at how DB such as Mongo (Not promoting them in any way), but when Maildir is used aginst Mongo for file storage, Mongo saves a lot of disk space. Again, it is about how we want to store files? NixOS is a quite a way to think about having a file system or db&#x2F;store. reply eviks 1 hour agorootparentprevOutside of simple cases xml is too verbose and ugly (and in these cases usually zipped), so it&#x27;s not suitable for a poor human with a plain text editor, so that doesn&#x27;t give you much of a leg.(Json has a higher threshold of complexity before it succumbs) reply quickthrower2 4 hours agoparentprev [–] With JSON&#x2F;XML the app owner decides the schema of the saved file, as they should. One day Sqlite will do some perfectly fine change that’ll break people who outsource their file format to it. Own your file format!That said there is some nuance and it depends what the user expects. Is you app more of an MSWord where people expect a format that is decades backward compatible and only changes on explicit save, or is it more like a live app with a db back end. If the latter there should be no save concept around the DB file but perhaps a backup and restore function that exports to a controlled format. reply eastern 3 hours agorootparent [–] In sqlite the on-disk file format does not matter.All that matters is that you should be able to issue sql to the sqlite embedded library and get back the results.Freeing you from the overhead of owning (thus inventing and then maintaining) your own file format is almost the entire point of using sqlite in this manner. reply quickthrower2 1 hour agorootparent [–] It matters for 2 reasons. One, the expectation that the file changes only when you click Save is broken (as mentioned in another comment), and Two, unless you pin the version of sqlite forever then the file format may have braking changes or your need to deal with migrations. reply eastern 1 hour agorootparent [–] > expectation that the file changes only when you click Save is brokenThis has nothing to do with sqlite. You can have (or not have) gradual saves in any file format. It&#x27;s a choice that the developers of that app made.> file format may have braking changesThe sqlite file format is unchanged for 19 years now. A world of features and capabilities have been added since. Don&#x27;t hold your breath waiting for the sqlite format to change. reply Applications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article proposes the idea of utilizing SQLite, a software library that provides a relational database management system, as a container for OpenDocument Presentation files, instead of the current ZIP archive format.",
      "It suggests potential improvements, like breaking down content into smaller parts and adding versioning capabilities, that could be facilitated by this switch.",
      "The author underscores the benefits of using SQLite as an application file format, including enhanced user experience and performance, therefore potentially increasing the efficiency of applications."
    ],
    "commentSummary": [
      "The discourse is about using SQLite as a file format for OpenDocument, comparing its strengths and limitations against other formats like XML.",
      "SQLite's lack of standardization poses challenges for interoperability and ISO standardization, despite its greater functionality.",
      "Conversation also covers the concept of autosaving user data, technicalities of saving data, and potential advantages and disadvantages of SQLite in various contexts, emphasising the need for a balance between autosaving and user control."
    ],
    "points": 262,
    "commentCount": 132,
    "retryCount": 0,
    "time": 1695024681
  },
  {
    "id": 37551474,
    "title": "Introduction to Immutable Linux Systems",
    "originLink": "https://dataswamp.org/~solene/2023-07-12-intro-to-immutable-os.html",
    "originBody": "INDEX EVERYTHING RSS RSS USING HTML About me: My name is Solène Rapenne, pronouns she/her. I like learning and sharing knowledge. Hobbies: '(NixOS BSD OpenBSD Lisp cmdline gaming security QubesOS internet-stuff). I love percent and lambda characters. OpenBSD developer solene@. Contact me: solene+www at dataswamp dot org or @solene@bsd.network (mastodon). If for some reason you want to support my work, this is my paypal address: donate@perso.pw. Consider sponsoring me on Patreon to help me writing this blog and contributing to Free Software as my daily job. Introduction to immutable Linux systems Written by Solène, on 12 July 2023. Tags: #immutability #linux Comments on Fediverse/Mastodon 1. Introduction § If you reach this page, you may be interested into this new category of Linux distributions labeled \"immutable\". In this category, one can find by age (oldest → youngest) NixOS, Guix, Endless OS, Fedora Silverblue, OpenSUSE MicroOS, Vanilla OS and many new to come. I will give examples of immutability implementation, then detail my thoughts about immutability, and why I think this naming can be misleading. I spent a few months running all of those distributions on my main computers (NAS, Gaming, laptop, workstation) to be able to write this text. 2. What's immutability? § The word immutability itself refers to an object that can't change. However, when it comes to an immutable operating system, the definition immediately become vague. What would be an operating system that can't change? What would you be supposed to do with it? We could say that a Linux LIVE-CD is immutable, because every time you boot it, you get the exact same programs running, and you can't change anything as the disk media is read only. But while the LIVE-CD is running, you can make changes to it, you can create files and directories, install packages, it's not stuck in an immutable state. Unfortunately, this example was nice but the immutability approach by those Linux distribution is totally different, so we need to think a bit further. There are three common principles in these systems: system upgrades aren't done on the live system packages changes are applied on the next boot you can roll back a change Depending on the implementation, a system may offer more features. But this list is what a Linux distribution should have to be labelled \"immutable\" at the moment. 3. Immutable systems comparison § Now we found what are the minimum requirements to be called immutable, let's go through each implementation, by their order of appearance. 3.1. NixOS / Guix § In this section, I'm mixing NixOS and Guix as they both rely on the same implementation. NixOS is based on Nix (first appearance in 2003), which has been forked into early 2010s into the Guix package manager to be 100% libre, which gave birth to an eponym operating system also 100% free. NixOS official project website Guix official project website Jonathan Lorimer's blog post explaining Eelco Dolstra's thesis about Nix These two systems are really different than a traditional Unix like system we are used to, and immutability is a main principle. To make it quick, they are based on their package manager (being Nix or Guix) that contains every package or built file into a special read-only directory (where only the package manager can write) where each package has its own unique entry, and the operating system itself is a byproduct of the package manager. What does that imply? If the operating system is built, this is because it's made of source code, you literally describe what you want your system to be in a declarative way. You have to list users, their shells, installed packages, running services and their configurations, partitions to mount with which options etc... Fortunately, it's made a lot easier by the use of modules which provide sane defaults, so if you create a user, you don't have to specify its UID, GID, shell, home etc... So, as the system is built and stored in the special read-only directory, all your system is derived from that (using symbolic links), so all the files handled by the package manager are read-only. A concrete example is that /etc/fstab or /bin/sh ARE read-only, if you want to make a change in those, you have to do it through the package manager. I'm not going into details, because this store based package manager is really different than everything else but: you can switch between two configurations on the fly as it's just a symlink dance to go from a configuration to another you can select your configuration at boot time, so you can roll back to a previous version if something is wrong you can't make change to a package file or system file as they are read only the mount points except the special store directory are all mutable, so you can write changes in /home or /etc or /var etc... You can remove the system symlinks by a modified version, but you can't modify the symlink source itself. This is the immutability as seen through the Nix lens. I've spent a few years running NixOS systems, this is really a blast for me, and the best \"immutable\" implementation around, but unfortunately it's too different, so its adoption rate is very low, despite all the benefits. NixOS forum: My issues when pushing NixOS to companies 3.2. Endless OS § While this one is not the oldest immutable OS around, it's the first one to be released for the average user, while NixOS and Guix are older but for a niche user category. The company behind Endless OS is trying to offer a solid and reliable system, free and open source, that can works without Internet, to be used in countries with a low Internet / powergrid coverage. They even provide a version with \"offline internet included\" containing Wikipedia dumps, class lessons and many things to make a computer useful while offline (I love their work). Endless OS official project website Endless OS is based on Debian, but uses the OSTree tool to make it immutable. OSTree allows you to manage a core system image, and add layers on top of it, think of packages as layers. But it can also prepare a new system image for the next boot. With OSTree, you can apply package changes in a new version of the system that will be available at next boot, and revert to a previous version at boot time. The partitions are mounted writable, except for /usr, the land of packages handled by OSTree, which is mounted read-only. There are no rollbacks possible for /etc. Programs meant to be for the user (not the packages to be used by the system like grub, X display or drivers) are installed from Flatpak (which also uses OSTree, but unrelated to the system), this avoids the need to reboot each time you install a new package. My experience with Endless OS is mixed, it is an excellent and solid operating system, it's working well, never failed, but I'm just not the target audience. They provide a modified GNOME desktop that looks like a smartphone menu, because this is what most non-tech users are comfortable with (but I hate it). And installing DevOps tools isn't practical but not impossible, so I keep Endless OS for my multimedia netbook and I really enjoy it. 3.3. Fedora Silverblue § This linux distribution is the long descendant of Project Atomic, an old initiative to make Fedora / CentOS/ RHEL immutable. It's now part of the Fedora releases along with Fedora Workstation. Project Atomic website Fedora Silverblue project website Fedora Silverblue is also using OSTree, but with a twist. It's using rpm-OSTree, a tool built on top of OSTree to let your RPM packages apply the changes through OSTree. The system consists of a single core image for the release, let's say fedora-38, and for each package installed, a new layer is added on top of the core. At anytime, you can list all the layers to know what packages have been installed on top of the core, if you remove a package, the whole stack is generated again (which is terribly SLOW) without the package, there is absolutely no leftover after a package removal. On boot, you can choose an older version of the system, in case something broke after an upgrade. If you install a package, you need to reboot to have it available as the change isn't applied on the current booted system, however rpm-OSTree received a nice upgrade, you can temporarily merge the changes of the next boot into the live system (using a tmpfs overlay) to use the changes. The mountpount management is a bit different, everything is read-only except /etc/, /root and /var, but your home directory is by default in /var/home which sometimes breaks expectations. There are no rollbacks possible for /etc. As installing a new package is slow due to rpm-OSTree and requires a reboot to be fully usable (the live change back port store the extra changes in memory), they recommend to use Flatpak for programs, or toolbox, some kind of wrapper that create a rootless fedora container where you can install packages and use it in your terminal. toolbox is meant to provide development libraries or tool you wouldn't have in Flatpak, but that you wouldn't want to install in your base Fedora system. toolbox website My experience with Fedora Silverblue has been quite good, it's stable, the updates are smooth even if they are slow. toolbox was working fine despite I don't find this practical. 3.4. OpenSUSE MicroOS § This spin of OpenSUSE Tumbleweed (rolling-release OpenSUSE) features immutability, but with its own implementation. The idea of MicroOS is really simple, the whole system except a few directories like /home or /var lives on a btrfs snapshot, if you want to make a change to the system, the current snapshot is forked into a new snapshot, and the changes are applied there, ready for the next boot. OpenSUSE MicroOS official project website What's interesting here is that /etc IS part of the snapshots, and can be roll backed, which wasn't possible in the OSTree based systems. It's also possible to make changes to any file of the file system (in a new snapshot, not the live one) using a shell, which can be very practical for injecting files to solve a driver issue. The downside it's not guaranteed that your system is \"pure\" if you start making changes, because they won't be tracked, the snapshots are just numbered, and you don't know what changes were made in each of them. Changes must be done through the command transactional-update which do all the snapshot work for you, and you could either manipulate package by adding/removing a package, or just start a shell in the new snapshot to make all the changes you want. I said /etc is part of the snapshots, it's true, but it's never read-only, so you could make a change live in /etc, then create a new snapshot, the change would be immediately inherited. This can create troubles if you roll back to a previous state after an upgrade if you also made changes to /etc just before. The default approach of MicroOS is disturbing at first, a reboot is planned every day after a system update, this is because it's a rolling-release system and there are updates every day, and you won't benefit from them until you reboot. While you can disable this automatic reboot, it makes sense to use the newest packages anyway, so it's something to consider if you plan to use MicroOS. There is currently no way to apply the changes into the live system (like Silverblue is offering), it's still experimental, but I'm confident this will be doable soon. As such, it's recommended to use distrobox to use rootless containers of various distributions to install your favorite tools for your users, instead of using the base system packages. I don't really like this because this adds maintenance, and I often had issues of distrobox refusing to start a container after a reboot, I had to destroy and recreate it entirely to solve. distrobox GitHub project page My experience with OpenSUSE MicroOS has been wonderful, it's in dual-boot with OpenBSD on my main laptop, it's my Linux Gaming OS, and it's also my NAS operating system, so I don't have to care about updates. I like that the snapshots system doesn't restrict me, while OSTree systems just doesn't allow you to make changes without installing a package. 3.5. Vanilla OS § Finally, the really new (but mature enough to be usable) system in the immutable family is Vanilla OS based on Ubuntu (but soon on Debian), using ABroot for immutability. With Vanilla OS, we have another implementation that really differs from what we saw above. Vanilla OS project website ABroot named is well thought, the idea is to have a root partition A, another root partition B, and a partition for persistent data like /home or /var. Here is the boot dance done by ABroot: first boot is done on A, it's mounted in read-only changes to the system like new packages or file changes in /etc are done on B (and can be applied live using a tmpfs overlay) upon reboot, if previous boot was A, you boot on B, then if the boot is successful, ABroot scan for all the changes between A and B, and apply all the changes from B to A when you are using your system, until you make a change, A and B are always identical This implementation has downsides, you can only roll back a change until you boot on the new version, then the changes are also applied on the previous boot, and you can't roll back. This implementation mostly protects you from a failing upgrade, or if you made changes and tried them live, but you prefer to rollback. Vanilla OS features the package manager apx, written by distrobox author. That's for sure an interesting piece of software, allowing your non-root user to install packages from many distributions (arch linux, fedora, ubuntu, nix, etc...) and integrates them into the system as if they were installed locally. I suppose it's some kind of layer on top of distrobox. apx package manager GitHub project page My experience wasn't very good, I didn't find ABroot to be really useful, and the version 22.10 I tried was using an old Ubuntu LTS release which didn't make my gaming computer really happy. The overall state of Vanilla OS, ABroot and apx is that they are young, I think it can become a great distribution, but it still has some rough edges. 3.6. Alpine Linux (with LBU) § I've been told that it was possible to achieve immutability on Alpine Linux using the \"lbu\" command. Alpine Linux wiki: Local backup I don't want to go much into details, but here is the short version: you can use Alpine Linux installer as a base system to boot from, and create tarballs of \"saved configurations\" that are automatically applied upon boot (it's just tarred directories and some automation to install packages). At every boot, everything is untarred again, and packages are installed again (you should use an apk cache directory), everything in live memory, fully writable. What does this achieve? You always start from a clean state, changes are applied on top of it at every boot, you can roll back the changes and start fresh again. Immutability as we defined above here isn't achieved because changes are applied on the base system, but it's quite close to fulfill (my own) requirements. I've been using it a few days only, not as my main system, and it requires a very good understanding of what you are doing because the system is fully in memory, and you need to take care about what you want to save/restore, which can create big archives. On top of that, it's poorly documented. 4. Pros, Cons and Facts § Now I gave some details about all the major immutable systems (Linux based) around, I think it's time to list the real pros and cons I found from my experimentation. 4.1. Pros § you can roll back changes if something went wrong. transactional-updates allows you to keep the system running correctly during packages changes. 4.2. Cons § configuration management tool (ansible, salt, puppet etc..) integrate VERY badly, they received updates to know how to apply package changes, but you will mostly hit walls if you want to manage those like regular systems. having to reboot after a change is annoying (except for NixOS and Guix which don't require rebooting for each change). OSTree based systems aren't flexible, my netbook requires some extra files in alsa directories to get sound (fortunately Endless OS have them!), you just can't add the files without making a package deploying them. blind rollbacks, it's hard to figure what was done in each version of the system, so when you roll back it's hard to figure what you are doing exactly. it can be hard to install programs like Nix/Guix which require a directory at the root of the file system, or install non-packaged software system-wide (this is often bad practice, but sometimes a necessary evil). 4.3. Facts § immutability is a lie, many parts of the systems are mutable, although I don't know how to describe this family with a different word (transactional something?). immutable doesn't imply stateless. NixOS / Guix are doing it right in my opinion, you can track your whole system through a reliable package manager, and you can use a version control system on the sources, it has the right philosophy from the ground up. immutability is often associated with security benefits, I don't understand why. If someone obtains root access on your system, they can still manipulate the live system and have fun with the /boot partition, nothing prevent them to install a backdoor for the next boot. immutability requires discipline and maintenance, because you have to care about the versioning, you have extra programs like apx / distrobox / devbox that must be updated in parallel of the system (while this is all integrated into NixOS/Guix). 5. Conclusion § Immutable operating systems are making the news in our small community of open source systems, but behind this word lies many implementations with different use cases. The word immutable certainly creates expectations from users, but it's really nothing more than transactional updates for your operating system, and I'm happy we can have this feature now. But transactional updates aren't new, I think it started a while ago with Solaris and ZFS allowing you to select a system snapshot at boot time, then I'm quite sure FreeBSD implemented this a decade ago, and it turns out that on any linux distribution with regular btrfs snapshots you could select a snapshot at boot time. Previous blog post about booting on a BTRFS snapshot without any special setup In the end, what's REALLY new is the ability to apply a transactional change on a non-live environment, integrates this into the bootloader, and give the user the tooling to handle this easily. 6. Going further § I recommend reading the blog post \"“Immutable” → reprovisionable, anti-hysteresis\" by Colin Walters. “Immutable” → reprovisionable, anti-hysteresis This article has been useful for you? Please consider sponsoring me on Patreon. Content under CC-BY-4.0. This blog is powered by cl-yag!",
    "commentLink": "https://news.ycombinator.com/item?id=37551474",
    "commentBody": "Introduction to Immutable Linux SystemsHacker NewspastloginIntroduction to Immutable Linux Systems (dataswamp.org) 256 points by InitEnabler 11 hours ago| hidepastfavorite114 comments thinkmassive 1 minute agoGlad to see EndlessOS included!Silverblue&#x2F;Sircea gets all the attention these days, but Endless is the oldest OSTree-based user distro by a long shot, and it’s still actively developed by the Endless Foundation.It’s also the one most suitable for non-technical users. Definitely worth considering for that use case, particularly for very young users since it now includes plenty of tutorial content intended for that audience. reply jdoss 7 hours agoprevWhile I am glad Silverblue is on this list, not having Fedora CoreOS on it too is a shame. FCOS is an amazing OS to run in production and it has come a very long way since the CoreOS acquisition. I find that FCOS is a good middle ground of being usable and easy to learn while still being immutable compared to Nix.The FCOS devs introduced a new feature called CoreOS Layering which lets you define your system in a Dockerfile and FCOS will rebase to that state and all you have to do is reboot to configure your server. It is super powerful.Anyways, your next project needs a VM, give it a shot. I made a Python based CLI tool to help you develop locally on a Linux workstation to create a Butane file to fit your needs. Below is the GitHub for Bupy and a good example of running an app (Paperless NGX) on FCOS with the CoreOS Layering features.https:&#x2F;&#x2F;github.com&#x2F;quickvm&#x2F;bupyhttps:&#x2F;&#x2F;github.com&#x2F;quickvm&#x2F;fcos-layer-paperless-ngxhttps:&#x2F;&#x2F;coreos.github.io&#x2F;rpm-ostree&#x2F;container&#x2F;https:&#x2F;&#x2F;github.com&#x2F;coreos&#x2F;enhancements&#x2F;blob&#x2F;main&#x2F;os&#x2F;coreos-l...https:&#x2F;&#x2F;github.com&#x2F;coreos&#x2F;layering-examples reply madspindel 2 hours agoparentOh, CoreOS Layering looks really useful! I&#x27;m using openSUSE MicroOS today on some Raspberry Pi&#x27;s and a x86_64 server. One reason I picked MicroOS was because it was quite simple to install on Raspberry Pi.How hard is it to install CoreOS on a Raspberry Pis? Some installation guides on the Internet look quite complex...? reply hardwaresofton 6 hours agoparentprevThanks for making this tool and showing how to get started with the layering!Do you have any thoughts you’d like to share on flatcar as the other project with CoreOS lineage?As for me my main difficulties have been figuring out what to do with these projects in a bare metal environment. Building VM images is cool, but much of the time I want to do things like install to an existing drive or even onto a ZFS pool underneath. reply surrTurr 1 hour agoprevHow does working with Docker work on Immutable systems like Fedora Silverblue. Like e.g. developing an application (in a Devcontainer like e.g. Toolbox to avoid having to install all the devtools on os-tree) and then building and debugging a Docker container from within the devcontainer? Or am I thinking in a wrong way?Any good blogposts on developer workflows on Silverblue? reply jcastro 28 minutes agoparentVSCode with the devcontainer and docker is great on Silverblue.I&#x27;ve been prototyping some developer workflows with friends here: https:&#x2F;&#x2F;universal-blue.org&#x2F;images&#x2F;bluefin&#x2F;developer-experien...So far the major patterns are vscode with distrobox, vscode with devcontainers, vscode with devpod, jetbrains toolbox thing (which just runs everything out of the home directory, the OS doesn&#x27;t care).And then box devbox&#x2F;nix and homebrew in ~ is also an option if you&#x27;re into that. reply crabbone 38 minutes agoparentprevThis sounds painful for no reason. Especially the debugging part. Why would you want that? Is it really such a strain to install the tools you need for development on your computer?I can understand wanting an immutable system for a server, as it will likely cut down on maintenance, but for personal use... that just sends shivers down my spine... As someone having to support other (especially not very savvy) programmers when it comes to tool usage and their environment I hate to imagine having to deal with someone who&#x27;d want that kind of setup. reply jcastro 10 minutes agorootparentYou don&#x27;t know anyone who develops in containers? It&#x27;s a pretty common pattern these days. reply FireInsight 7 hours agoprevWhat these sort of introductions to immutable always fail to consider is the other side of the coin, image-based. I&#x27;m working on https:&#x2F;&#x2F;universal-blue.org&#x2F; along with many people much more skilled than me. We build OCI container images on top of vanilla Fedora Silverblue & many other editions with different desktops. Those images can then be booted to (or rather rebased to) using rpm-ostree. This is a more robust way of extending the system than layering, and the same changes can be easily benefited or inherited from by anyone. You can even make your own image really easily!I think that VanillaOS and SUSE are working on similar things, but we&#x27;re not an OS project, just a downstream from Fedora. Fedora&#x27;s full support is underway but with what&#x27;s already working perfectly our methods are already IME some of the most robust and easy ways of delivering Nvidia drivers for example. reply psd1 2 hours agoparentTangential, but I had my mind blown in about 2009 by a big hypervisor running Windows remote desktop hosts. I believe it was Citrix.The VMs booted from images. The image and the mutable differencing disks were entirely in RAM (although user profiles were on spinning rust). A desktop host for 25 users would boot to accepting remote logins in about 4 seconds.Least painful Windows system to patch. reply PrimeMcFly 5 minutes agorootparent> spinning rustWeird phrasing. Haven&#x27;t seen that before. reply curt15 3 hours agoparentprevI see that UBlue uses Github actions to rebuild the images regularly to roll in package updates. Who foots the bill? Are the images also served from Github? Does Github charge for egress? What happens when a lot of users want to download the same image? reply FireInsight 1 hour agorootparentGithub free tier served us well. For a while we had a paid tier for GH orgs to get better builders, but now I think we use https:&#x2F;&#x2F;buildjet.com&#x2F;. I&#x27;m not too familiar with this aspect, though.I think the bills are paid by Jorge (the kind of \"founder\" of the project), at least I think so, though some of the other top members with jobs in the Linux&#x2F;Cloud world might be helping. Donation paths and such have been considered, but the bills aren&#x27;t too huge so nothing has been rushed in.For the registry, GHCR serves us entirely for free. No egress costs, no ingress costs, nothing. No plans to change providers, and I don&#x27;t think they have plans to raise pricing either. We could probably find an alternative host pretty easily, though, through the cloud contacts and knowledge some of the devs here have. reply creatonez 6 hours agoparentprevI&#x27;m confused, isn&#x27;t Fedora Silverblue also image-based? I thought the default installation doesn&#x27;t use layering, and that layering only comes when you want to install extra RPM packages. reply FireInsight 6 hours agorootparentYes, Fedora Silverblue is image-based. We just use extending those stock images as an alternative to layering and easy way of shipping the same system configuration to many people. reply cassianoleal 3 hours agorootparentIsn’t an OCI image essentially layers + metadata?How is it different from what you call “layering”?Legit question, just trying to understand why you feel it’s an advantage. reply FireInsight 1 hour agorootparentAn OCI image is pretty simple, yes, so is the sort of image that is in an OSTree repository. The difference is that when using `rpm-ostree` packages installed with `rpm-ostree install` are \"layered\" on top of the base image, while packages in the \"base image\" (be it OCI or OSTree) are part of the system and thus not \"layered\".Adding packages in an image has the benefit of pseudo-reproducability (have the same image on multiple computers) and the added robustness of your base system being built elsewhere daily. Your computer just pulls the diffs. For example, there have been issues with rpmfusion on Fedora that ublue users completely avoided. Codecs & other essential rpmfusion packages are included in the images, and the rpmfusion repository is removed after they are installed. This way, if something package-related breaks it breaks at the image build stage, and an ordinary user wont even notice it before it is fixed.The most noticeable benefit IMO, though, is being able to ship the same changes on top of a base image every day for multiple machines. This is not only packages, but for example udev rules, and other QoL things like our `justfile`s, configuration for https:&#x2F;&#x2F;just.systems&#x2F; that has some useful scripts for adding the kargs necesarry for Nvidia drivers to work and `just update` for updating the system, flatpaks & distroboxes. replyZuiii 9 hours agoprevThe problem I had with flatpak and the immutable approach in general is that I can&#x27;t modify them in ways that aren&#x27;t supported by the developer. For example, I use decsync to sync my calandars but as far as I can tell, it&#x27;s impossible to add the decsync plugin to the evolution flatpak.Until these immutable systems support the stacking of custom overlay filesystems as a first class feature^1, people will continue to run mutable systems.1: to account for use cases the developer can&#x27;t or won&#x27;t support. reply rebeccaskinner 8 hours agoparentNixOS gives you precisely this kind of control- through a few different mechanisms. Some packages in nixpkgs, and most NixOS and home manager modules, expose a lot of configuration options where you can configure various plugins, add extra packages, etc. Nix also lets you provide overlays and overrides to add custom packages (new packages, or customized versions of existing packages) and gives you the option of changing parts of a package. If none of that is sufficient, you can even supply your own patches to the code, or build the package from your own fork of the upstream repository.In practice this is one of my favorite things about nix. I find that I contribute much more to open source in general because it’s so easy to say “replace this dependency with my version when you build this package”. reply anotherhue 5 hours agorootparentWould you have any examples of this to hand? I&#x27;ve been trying to find a clean way to share an altered qemu derivation that isn&#x27;t just me copying and maintaining the qemu&#x2F;default.nix but so far that has been the easiest. reply chpatrick 2 hours agorootparentoverride&#x2F;overrideAttrs&#x2F;overlays reply NavinF 8 hours agoparentprev> add the decsync plugin to the evolution flatpakThis sort of thing is very common with GUI software on Linux. Outside of this bubble, most software comes with batteries included. Eg Solidworks never makes me download optional dependencies, but FreeCAD made me do it literally every 15 minutes as I move to the next step in a CAD&#x2F;CAM&#x2F;sim&#x2F;render workflow.Also see https:&#x2F;&#x2F;www.joelonsoftware.com&#x2F;2001&#x2F;03&#x2F;23&#x2F;strategy-letter-iv...> it’s never the same 20%. Everybody uses a different set of features. In the last 10 years I have probably heard of dozens of companies who, determined not to learn from each other, tried to release “lite” word processors that only implement 20% of the features. This story is as old as the PC. Most of the time, what happens is that they give their program to a journalist to review, and the journalist reviews it by writing their review using the new word processor, and then the journalist tries to find the “word count” feature which they need because most journalists have precise word count requirements, and it’s not there, because it’s in the “80% that nobody uses,” and the journalist ends up writing a story that attempts to claim simultaneously that lite programs are good, bloat is bad, and I can’t use this damn thing ’cause it won’t count my words. If I had a dollar for every time this has happened I would be very happy. reply ncts 6 hours agorootparentOutside of this bubble, you see vim plugins, VSCode plugins, JetBrains IDE plugins… Photoshop plugins, Premiere plugins… reply psd1 2 hours agorootparent\"What the hell, Steve, you had all week, now the fucking Bugle has scooped us.\"\"Sorry boss, been sorting out my init.el\" reply Tigress8780 5 hours agorootparentprevWell, it&#x27;s never the same 20%, which is precisely why we have plugins and extensions even outside of the Linux GUI bubble. This include IDEs, browsers, office suites, media production software (e.g. VST), and of course engineering software. For a complex software product, it is never possible to include batteries for everyone.FreeCAD might not come with enough battery to power a full workflow, but I have also seen entire engineering firm not able to work without a particular AutoCAD plugin. reply dfee 7 hours agorootparentprevGreat link. However, I don’t know how Joel could see a newcomer can challenge an incumbent, short of reimplementing all features.Certainly, Google Sheets didn’t do that. Nor has airtable (valuation considerations aside, it has seen some adoption).In a new category, or a new market, there is no benchmark. So that’s the east case. But sometimes new products do displace old products, and sometimes just by distribution or pricing at the cost of features.So how do you reconcile that, Joel?! reply NavinF 7 hours agorootparentFair point, but Sheets hasn&#x27;t replaced Excel. Sheets only competes in the in-browser market and Sheets by itself does not make money for Google. If you go back a few years, Microsoft was the 80%-er that lacked features (notably real-time collaboration) whereas Google offered an entire suite of software with free real-time collaboration. People pay $6&#x2F;mo to get the entire suite for their company. Gsuite is not comparable to the \"lite\" word processors that Joel was talking about or the open-source tools like FreeCAD that I was talking about. Both are objectively worse than their paid counterparts and extremely limited in functionality.Also worth mentioning that Sheets has competition in this space now that in-browser 365 Excel is free. Tho neither is feature-complete vs desktop Excel reply bandrami 8 hours agoparentprev> stacking of custom overlay filesystems as a first class featureAt that point why not just use a mutable system? This reminds me of how a decade ago everybody rushed to move to nosql and then immediately reinvented schemae in their projects. reply eru 3 hours agorootparentYou&#x27;d want a mutable system that still supports snapshots. And also that uses mutation very sparingly.Compare Haskell: both Haskell and C support both mutable variables and constant ones. But the ecosystems and idioms are very different. reply vetinari 6 hours agoparentprevYou can add plugins to applications in flatpak; package these plugins as extensions.How it is done can be seen with OBS: flathub has several OBS plugins available (com.obsproject.Studio.Plugin.*). reply bandrami 5 hours agorootparentOn the audio production side of things it&#x27;s actually pretty good. The selection is pretty comprehensive and the jsons on github are simple and clear enough that you can build your own for any they missed. They&#x27;re versioned with the runtime so there&#x27;s the usual September problem of program A having updated while program B hasn&#x27;t yet, but that&#x27;s the nature of the beast.A problem that&#x27;s less well-solved is situations like Cantor, where a single application is a front-end for multiple executables. To get it to actually work you&#x27;d have to package Scilab, Sage, Maxima, Octave, R, and Julia in the flatpak itself, and these are non-trivial programs to package. There are workarounds with flatpak-spawn, but at that point why not just install the application natively? (I think the \"right\" answer is to set up a dbus service for like \"system-octave\" or whatever and have a separately flatpak&#x27;ed interpreter register it, but as aesthetically pleasing as this solution is it doesn&#x27;t seem to have induced me or anybody else to actually do it...) reply pkulak 8 hours agoprevI think the definition should be:Installing any number of packages, then removing them in any order, at any future point(s) in time, is equivalent to never having installed them at all.This leaves some distros out, but I feel like it’s the important part of the concept. reply eru 3 hours agoparentHow deep do you want that property to apply?Have a look at &#x27;uniquely represented datastructures&#x27; and &#x27;History Independent Data Structures&#x27;.You&#x27;d need to take special care to make sure that your block device (eg SSD) allocates blocks independent of history. reply nonameiguess 2 hours agoparentprevAs far as I can tell, her point is this is effectively impossible, at least for a user-facing system. Do you want all the files you wrote in your word processor program or text editor gone when you uninstall those? All the files you downloaded from your browser gone if you uninstall it? If not, there is no reliable way to tell what files are created by a program automatically and which are created by a human using that program. You can easily enough remove everything that was created during the installation process, but not all future changes.Beyond that, consider other changes, like say you decide to change DNS implementations and then later decide to change your default DNS server. If you uninstall the provider to change back to previous one, do you also want to change back to the old server you were using or do you want to retain that change? Personally, I&#x27;d want to only change the provider but keep the new server.Then consider difficulties with directories shared across multiple machines. Let&#x27;s say you have &#x2F;home&#x2F;${USER} set up as an NFS or Samba mount so you can keep the same files across multiple workstations. If a program respects XDG config dirs and stores stuff in there and you uninstall on one workstation, should it remove the files from all of them? Do you want all your devices to be identical or do you only want the home directory to be identical? There is no possible way for a package manager on a single system to know this. reply infogulch 7 hours agoparentprevI like that. Maybe you could say the package collection forms a lattice, where there is only one state for any subset of packages no matter how you got there. reply colordrops 5 hours agoparentprevThis is called \"reproduceability\", meaning the same config always results in the same system state. reply qazxcvbnm 32 minutes agoprevOff topic, but does anyone know how to find out where mutable data for NixOS modules are stored at (e.g. the data directory for a database) without reading the source? Occasionally, it&#x27;s mildly annoying, and would be comforting to know with certainty where all my state is. reply nrabulinski 11 minutes agoparentIf it’s a systemd service (which it usually is) you can simply run systemctl catwhich pretty much always will have WorkingDirectory property set or RuntimeDirectory or similar (RuntimeDirectory you’ll have to prefix with &#x2F;var&#x2F;run which you just sort of have to know but that’s not NixOS specific) reply wi5eif6E 2 minutes agorootparentIt&#x27;s rather unlikely the RuntimeDirectory= contains state as it&#x27;s wiped on service stop unless RuntimeDirectoryPreserve= is set. NB: these days, &#x2F;var&#x2F;run&#x2F; is a symlink to &#x2F;run&#x2F; reply wi5eif6E 13 minutes agoparentprevUnfortunately, that&#x27;s not possible. However, as NixOS modules usually are Systemd services, the StateDirectory= of the service is a good starting point (systemctl cat ). reply colinramsay 5 hours agoprevI&#x27;m less interested in immutable systems and more in pre-configured systems. NixOS with Home Manager is the one that stands out here, but the configuration is just awful. I want to be able to have my full config in source control and know that is the state of my current system, with anything else being wiped on reboot. Anything that&#x27;s changed before reboot should be highlighted.In my (limited) experience with something like Silverblue, the base system can be configured but when you start adding applications (like say, Firefox), it is lacking when it comes to configuring that because you&#x27;re using Flatpak and I don&#x27;t know of a way to tell it to both install all Flatpaks I want along with all of the configuration.I guess there&#x27;s some way of installing Flatpaks en masse and then dotfiles can take care of the rest?https:&#x2F;&#x2F;universal-blue.org&#x2F;tinker&#x2F;mindset&#x2F;#resist-the-urge-t... reply softirq 10 hours agoprevBeen using Fedora Silverblue since its release and it&#x27;s absolutely the future. ostree is what everyone should be using. reply hx8 8 hours agoparentI found Silverblue to not be flexible enough for my person computers. Maybe I use Linux in a hacky way, not having write access to &#x2F;usr or &#x2F;bin or other folders drove me crazy about once every two weeks.For example, I was using a script written by an ubuntu user that was looking for a library with the name&#x2F;location ubuntu puts it in. Fedora uses a different name for the library. My instinct here is to create a symbolic link with the Ubuntu name that points to the Fedora rpm managed library. Instead, to get it to work I forked the script, got it to build locally, made it look for either library name, ran local test cases, submitted the code as a PR upstream, etc etc etc. It took something that would normally cost me 1 line of shell to fix to something that took 90 minutes. reply yjftsjthsd-h 7 hours agorootparent> My instinct here is to create a symbolic link with the Ubuntu name that points to the Fedora rpm managed library.I sympathize, but also AIUI this is exactly the sort of monkey patching that these systems are trying hard to avoid; yes, it fixes your immediate problem, but it leaves an undocumented, unmanaged change in how your system finds libraries. In my personal experience, this is the kind of change that leads to machines growing weird behavior that ends when I give up and so a clean reinstall. reply curt15 3 hours agorootparentprev> not having write access to &#x2F;usr or &#x2F;bin or other folders drove me crazy about once every two weeks.How do you keep track of your custom modifications to &#x2F;usr or &#x2F;bin without using the package manager? Do you record them somewhere for reference? reply akvadrako 6 hours agorootparentprevThere are 3 ways to get what you want more easily:1. With Fedora Silverblue 39 the most straightforward way is to add a Dockerfile layer just which makes your changes directly to the base image.2. You can also create your own RPM and make the changes there.3. Best is to actually run your script in an Ubuntu container (distrobox, toolbx, or podman). reply curt15 3 hours agorootparent>1. With Fedora Silverblue 39 the most straightforward way is to add a Dockerfile layer just which makes your changes directly to the base image.Does that put the onus on you to rebuild container image in order to receive system updates? reply INTPenis 7 hours agorootparentprevWow you&#x27;re using Silverblue all wrong, you&#x27;re even using regular Fedora wrong.Even before I switched to Silverblue I was known to create Ubuntu containers to run tools from there.That&#x27;s what you should have done, run it in a container. If you&#x27;re not comfortable with containers then the workflow in Silverblue will feel very strange. reply nonameiguess 2 hours agorootparentprevWouldn&#x27;t it have been easier in this case to edit the script? reply anthk 7 hours agorootparentprevUse toolbox and do all your CLI scripting there. reply INTPenis 7 hours agoparentprevI&#x27;m one year in and hell yes. I wonder if Red Hat realize what they have here. Not just in silverblue but in Fedora.I read a quora answer that estimated the Windows OS development budget at around 18 billion dollars, based on salaries. Imagine if Red Hat invested 2bn into Fedora to make it the Firefox of the desktop OS world. Just a 10% share is very significant against Microsoft.They&#x27;ve come so far with so little, on the back of thousands of open source packages. That money could be used to keep those projects alive, and to sponsor them while they&#x27;re being developed. Red Hat employees are already involved in a lot of them. reply actinium226 7 hours agorootparentYou&#x27;re conflating development budgets and marketing budgets.Redhat could invest 200bn into Fedora or any other project, and still people won&#x27;t switch to it because even if it&#x27;s \"better\" by some arbitrary metric, it&#x27;s not what people are used to.It&#x27;s definitely possible, but it looks like Linux is more likely to gain popularity via WSL than via Fedora or similar. reply nonameiguess 2 hours agorootparentprevI&#x27;m not sure any company, Microsoft included, really cares about the personal desktop&#x2F;laptop OS market at this point. Maybe Apple, but only because it&#x27;s bundled with the hardware, which is what they really care about. Microsoft and IBM care far more cloud platforms and enterprise users. Until Fedora comes out with something like Active Directory, SCCM, Sharepoint, and the Office Suite, businesses will continue to overwhelmingly use Windows.And most of those people will use Windows at home, too, if for no other reason than to not have to learn how to use two different desktop systems. reply fragmede 6 hours agorootparentprevI mean, where do you think Ubuntu came from? reply ponorin 3 hours agoparentprevFunny enaugh, once an update rendered my Silverblue install unbootable. I had to boot to a live image to fix GRUB misconfiguration. Now I know it&#x27;s not invincible, but it does work well otherwise. reply elesiuta 9 hours agoparentprev> ostree is what everyone should be usingI currently use Ubuntu for my server, and I see there&#x27;s ostree in my repos, I haven&#x27;t gotten around to trying it yet but I&#x27;d like to just start versioning my system as is with it. If that&#x27;s too difficult&#x2F;not possible to do, then I intend to switch my server over to Silverblue at some point because I really like the idea behind ostree. reply colordrops 5 hours agoparentprevHave you tried Nix? If so, how does it compare? I haven&#x27;t tried Silverblue, but Nix also feels like it&#x27;s the future. reply thatcherthorn 9 hours agoparentprevFor what reasons? reply INTPenis 7 hours agorootparentIt&#x27;s a combination of two things, open source packages have matured to a very stable level. Fedora uses very recent packages and it just works somehow because they&#x27;ve all become very good. So to keep a stable desktop system there is no more reason to use \"sta(b)le\" packages.The other reason is snapshots, because no system is perfect. An operating system should in fact be designed around the fact that something will go wrong. Windows has had this for years, and some linux users have done it with btrfs.But Fedora Silverblue does it seamlessly with ostree and grub, so if something does go wrong in a newly applied update, you simply choose the last working one in the grub menu and go on with your day until you have time to resolve it.This for me to use Fedora as my daily driver is crucial. reply hollander 9 hours agorootparentprevI suppose snapshots, so you can retirn to a previous state in case of trouble reply morjom 8 hours agorootparentIntroducing, BTRFS + Snapper. reply gettodachoppa 7 hours agoparentprevI like the idea of ostree but having glanced at it, as a casual&#x2F;intermediate user, it didn&#x27;t seem as user-friendly as, say, Docker was (which a home user can learn in an afternoon). It wasn&#x27;t obvious how to get to \"Debian distro deployed as an ostree snapshot\".Is this one of those things designed for career sysadmins&#x2F;system builders only? reply jcastro 1 minute agorootparentYou don&#x27;t consume ostree directly like that, what happens is someone would make Debian ostree-enabled OCI images for users to consume and adapt.https:&#x2F;&#x2F;opendev.org&#x2F;starlingx&#x2F;apt-ostree is one such effort to bring ostree to debian. reply nonrandomstring 3 hours agoprevI got into Tinycore this summer. Useful complement to the security philosophy \"One OS, one function\" which is kinda the thing behind Qubes, Tails and Whonix we talked about here a few days ago.It&#x27;s so light, you can spin up VMs, one for a mail-server, one for a database, one for a firewall&#x2F;router, each in a couple of seconds.Tinycore is itself immutable, so you add a vdisk with a \"package\" and some config, mark it read-only, and job done. A single Virsh script handles the startup and shutdown of \"services\" - each being a Tinycore instance. Fun, and robust so far, but not sure if I&#x27;d put it into anyone&#x27;s production just yet. reply actinium226 7 hours agoprev> immutability is a lie, many parts of the systems are mutable, although I don&#x27;t know how to describe this family with a different word (transactional something?).In the case of Nix, it sounds like it&#x27;s more focused on reproduce-ability? It sounds like I should be able to take the Nix configuration file, plop it on another computer, and get the same system (except, perhaps, for &#x2F;home).Some of the others sound more like existing tools that provide snapshot&#x2F;rollback capability, just with different implementations. reply Animats 6 hours agoparent\"Immutable\" is a strange term for this:* system upgrades aren&#x27;t done on the live system* packages changes are applied on the next boot* you can roll back a changeThat&#x27;s a atomic transaction, like a database. Although having to shut down the system to do a commit is a bit much.Microsoft put atomic transactions into their file system years ago, but file system transactions were never used much. You&#x27;d like to have an install system where all changes commit all at once, and if anything goes wrong during install, nothing commits and you roll back to the previous state. In theory a transactional file system could do that. In practice, there&#x27;s probably too much other non file systems state involved. reply tommiegannert 7 hours agoprevOn the server-side, there&#x27;s Bottlerocket OS [1] (Amazon). They use A&#x2F;B partitions for upgrades, and the idea is that you just run containers for anything non-base. Boot containers are used to do custom configuration at boot, and host-container (or DaemonSet, if you run K8S) is used for long-running services.[1] https:&#x2F;&#x2F;github.com&#x2F;bottlerocket-os&#x2F;bottlerocket reply xanthine 10 hours agoprevI have been using Fedora Sericea since it came out (it&#x27;s basically Fedora Silverblue, but uses Sway-wm instead of Gnome-wm). The system is actually pretty usable, and you don&#x27;t really need to reboot after each rpm-ostree install command (`rpm-ostree live-apply` takes care of it via systemd-based overlay). reply _d3Xt3r_ 10 hours agoparentBut you still need to reboot for the new kernel to go live right? Or is the kernel switched via kexec or something? reply softirq 9 hours agorootparentYes, there&#x27;s nothing special about the kernel upgrade. You either reboot or kexec. reply senectus1 10 hours agoparentprevin the last two weeks I&#x27;ve been using Fedora Workstation, I haven&#x27;t use Linux in 20 years and i have to say this is an incredibly improved experience.I haven&#x27;t yet needed to boot back into windows! If it stays like that for the next 6 months I&#x27;ll cut over to Linux permanently and wipe the windows partition. reply modeless 9 hours agorootparentYou may already know this but you can play most Windows games using Proton via Steam and it works really, really well. reply hollander 8 hours agorootparentBut what about Office, Adobe , Affinity? reply anthk 7 hours agorootparentLutris has installers for those too, and Flatpak makes Lutris&#x27; installation a breeze. Those are propietary, but you have Krita, Only Office, Inkscape, LibreOffice, Blender... reply hexo 8 hours agorootparentprevi dont believe anyone needs such mega-bad bloatware reply alexvitkov 8 hours agorootparentyour belief is not very grounded in reality unfortunately reply senectus1 7 hours agorootparentprevyeah Office365 is a problem. If I really need that I&#x27;ll either run up a VM for windows or I&#x27;ll use works citrix session (or the work laptop) replyfbdab103 8 hours agoprevRelatedly, does anyone know if the security guarantees around distrobox have gotten any stronger? Last I looked, they promised nothing, but curious if there has been any movement there.I would love if there was a seamless way to launch a distrobox os with a separate user home that could not touch my host system.Likely a Real Hard Problem, but even some isolation would probably be an improvement of running everything under the same user account. reply jcastro 32 minutes agoparentDistrobox and toolbox are developer tools, they explicitly are there as a convenience wrapper for podman&#x2F;docker for development.If you want hard separation you&#x27;d need a VM. It&#x27;d be awesome if you could just --firecracker on distrobox create and get that. :D reply gettodachoppa 7 hours agoparentprevLast I looked at Distrobox, DNS didn&#x27;t work on Ubuntu LTS or Debian Bookworm. It was literally useless.How can anyone adopt an unpolished hobby project that only seems to be tested on the dev&#x27;s Arch box is beyond me. reply amelius 35 minutes agoprevNo mention of proprietary drivers such as nVidia&#x27;s? § reply edrxty 8 hours agoprevIt&#x27;s interesting to watch these immutable images being adopted in the wider computing community. This has been a thing forever in the embedded world with Yocto&#x2F;Peta Linux&#x2F;Buildroot images. The image is a usually a straight disk image and is read-only in operation. This doesn&#x27;t inherently fix all the insecure IOT stuff as you usually need some way to reimage the device with updates and it takes at least some skill to do the bootloader signature verification right. It does help though as well as keep things deterministic. reply Ferret7446 7 hours agoparentIt has not been possible&#x2F;practical to use immutable system images until recent advances like ostree&#x2F;flatpak&#x2F;systemd-homed.We didn&#x27;t \"just discover\" some secret only known to the embedded community, lots of people have been working toward this exact goal for a long time, because we already knew for a long time that it has a lot of advantages. reply sambazi 6 hours agorootparent\"It has not been possible&#x2F;practical to use immutable system images until recent advances like ostree&#x2F;flatpak&#x2F;systemd-homed.\"no, you can just mount overlayfs with ram-backing for example reply Joel_Mckay 6 hours agoprevThe initramfs-overlay package became a trivial install with OverlayFS being accepted into the kernel a few years back.This meant the mess systemd created each boot, could be dropped into the ram-drive with zero impact on the OS image. Effectively turning any Debian based system into a read-only OS backing image, but retaining the ability to boot into a normal writable system with a single boot flag.This trick is a lot less finicky these days. =) reply Omroth 3 hours agoprev\" system upgrades aren&#x27;t done on the live system packages changes are applied on the next boot you can roll back a changeDepending on the implementation, a system may offer more features. But this list is what a Linux distribution should have to be labelled \"immutable\" at the moment.\"Immutable. I do not think it means what you think it means. reply thegeekpirate 8 hours agoprevGood timing, I&#x27;m minutes away from installing openSUSE Aeon.MicroOS Desktop turned into Kalpa (KDE) and Aeon (GNOME), but the latter has all the momentum. reply yjftsjthsd-h 7 hours agoparent> MicroOS Desktop turned into Kalpa (KDE) and Aeon (GNOME), but the latter has all the momentum.Honestly tying the OS to the desktop environment is the only reason I haven&#x27;t installed microos. Making a derivative with my preferred DE is on my \"maybe someday\" list, but that&#x27;s a long list... reply ehutch79 2 hours agoprevWait, I thought the reason sip on macOS was so terrible was because you couldn’t overwrite whatever system files you wanted without jumping through hoops… why would you bring an anti feature like that to Linux? reply eptcyka 2 hours agoparentWith this, you have a structured, transactional way of writing to whatever system files you wanted, with rollbacks and all. On macOS, only Apple does. reply pshirshov 3 hours agoprevThere is also this: https:&#x2F;&#x2F;stal-ix.github.io&#x2F;Fully statically linked. reply nilslindemann 7 hours agoprevI am currently on Mint, but will soon switch to Fedora Silverblue again, and then I encapsulate things for dev with Distrobox, which also can encapsulate home directory and export apps to the host system, see https:&#x2F;&#x2F;youtu.be&#x2F;Q2PrISAOtbYThat&#x27;s basically what I want, encapsulation and buttons :) reply nortonham 10 hours agoprevSolene&#x27;s blog is great overall reply qudat 9 hours agoprevThe pros section feels a little light. The reason an immutable OS is attractive is to be able to cleanly remove files&#x2F;folders from the system. reply muhehe 6 hours agoprevHow would you approach making immutable live-cd like Linux? No persistence at all, just boot it and run some app - think some kind of presentation panel which shows predefined program&#x2F;URL. Ideally net booted to avoid having storage at all. reply Animats 6 hours agoparentThe more extreme form of that is to have the OS run from read-only memory. Some embedded systems work that way. Reset, and you&#x27;re back to the cold start state. QNX can be built to run that way, for systems with no disk. reply nonameiguess 1 hour agoparentprevIdeally, Kali is meant to be used this way, so you can use it to perform forensics on a potentially compromised system without inadvertently changing anything and destroying evidence. You can really trivially do this with any Live CD by putting it on write-once read many media. You typically need at least &#x2F;var and &#x2F;tmp to be writeable, but that can be accomplished via tmpfs so they only write to memory and not disk. You don&#x27;t really need to do anything to enable this at the distro level, other than maybe make the mounts default, but in practice, live CDs tend to mount the root filesystem as SquashFS, which is also read-only at the filesystem level, and then use OverlayFS for partitions that need to be writeable for software to work, not retaining the writeable layer on shutdown.If you mean how would you do it yourself, you can use the tooling used by real distros. I&#x27;m not sure what tools they all provide, but Archiso (https:&#x2F;&#x2F;wiki.archlinux.org&#x2F;title&#x2F;Archiso) is probably the simplest to understand and modify because it&#x27;s purely shell scripts. reply hardwaresofton 6 hours agoprevIs anyone running diskless Alpine in production? It seems optimal to me but extremely uncommon.In the past I’ve tried running a small USB drive in rented bare metal w&#x2F; diskless alpine, but the machine seemed to reboot randomly IIRC. reply mattclarkdotnet 4 hours agoparentAlpine is awesome, but as the article says it is terribly badly documented. On my todo list is contributing some better RPi install docs, and a more sensible A&#x2F;B boot partition process. reply blueflow 5 hours agoparentprevI did for a while on a raspberry pi. It was my home server for some months but i ended up not using it. reply ftxbro 10 hours agoprevnext [–]> 4.3. Facts § > - NixOS &#x2F; Guix are doing it right in my opinion reply beanjuiceII 9 hours agoparentNeither one is as accessible imo, fedora has long history of distro making and it shows reply pkulak 9 hours agorootparentNixOS has a desktop installer now. If they had an easy way to setup Flathub and turn on auto updates, it would be as user-friendly as Silverblue. I actually have several family members on it, since I can do that first post-install setup. reply iopq 8 hours agorootparentFlathub is only okay with NixOS because a lot of flatpaks escape their sandbox and call out to outside programs. Sometimes you need to compile a NixOS version since the paths are not the same as a generic Linux distro. I&#x27;ve asked the developers to take a look at it, and one of them closed the issue, and the other is going to try to make it work with steam runBasically, nixpkgs is still the best way to run something on NixOS reply kaba0 8 hours agorootparentnixpkgs is still the best way to run something, period.Flatpak is an ugly hack that mixes together the completely unrelated tasks of packaging and sandboxing, while not being particularly good at any. reply ruuda 7 hours agorootparentprevNixOS predates Fedora by a few months. reply ftxbro 9 hours agorootparentprevi just thought it was funny that their opinion was in their facts section reply maxloh 7 hours agoprevThere is Ubuntu Core too.https:&#x2F;&#x2F;ubuntu.com&#x2F;core reply ggm 8 hours agoprevIf you ran root on ZFS with a single mount &#x2F; form, and snapshotted it immediately after install, would that count as \"immutable\" in this logic? reply bandrami 8 hours agoparentIf you did it before and after every system alteration, I&#x27;d count that. That&#x27;s tedious and error-prone to do by hand, though, so you&#x27;d want a set of tools to do it automatically. At which point you&#x27;ve invented a new immutable distro. reply Dwedit 9 hours agoprevDoes MX Linux Frugal count as immutable? reply Proven 8 hours agoprev [–] > 4.1. Pros §> 4.1.1 you can roll back changes if something went wrong.> 4.1.2 transactional-updates allows you to keep the system running correctly during packages changes.Last time I needed to roll back was OpenSSL in Ubuntu 18.04 and that was on one system.I don think I&#x27;ve ever had a problem that 4.1.2 solves.I don&#x27;t want to have yet another Linux OS to solve a problem that happens once a decade.If I wanted an immutable OS, I&#x27;d use a container OS on which I&#x27;d run apps as containers. Oh, wait, I already have that. reply piaste 4 hours agoparentDepends on what kind of machine you&#x27;re running.I purchased a 7900XTX on release day, with the Linux drivers being in a fairly rough state and new fixes being added daily. So, for the next 3-4 months, I was running Fedora Rawhide with the Koji repo added - about as bleeding edge as it gets, short of building locally from source. Rolling back definely came in handy once or twice.Once things stabilized, I rebased back to non-Rawhide Fedora 37 and stayed there. Then, a few weeks ago, I found out that AMD had been working on ROCM for the 7xxx series, so I&#x27;ve rebased to Rawhide again to play around with AI tools on the 6.6 kernel.I&#x27;ve also occasionally encountered non-critical bugs, suspected it might have been fixed upstream, and temporarily rebased to Rawhide just to check out if that was the case before reporting it. Pretty nice. reply okasaki 6 hours agoparentprev [–] I agree.Just like Wayland, they&#x27;re pushing it for other reasons (breaking existing stuff, locking you in to the GNOME desktop, etc), and using spurious reasons to promote it. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into the concept of immutable Linux systems, exploring specific implementations like NixOS, Guix, Endless OS, and Fedora Silverblue.",
      "The unique traits of each system are discussed, including their package managers and rollback capabilities; personal experiences and opinions are also shared.",
      "Despite noting the rising popularity of immutable operating systems in the open-source field, the article also points out the associated challenges and various strategies to immutability."
    ],
    "commentSummary": [
      "This article explores the idea of immutable Linux systems, presenting several distributions that implement this model, including EndlessOS and Fedora CoreOS.",
      "The pros and cons of using immutable systems versus mutable systems are discussed, covering aspects like package management, customization, and system control.",
      "Users' experiences and views on NixOS and Fedora are shared, taking into account factors such as accessibility, compatibility, and rollback procedures."
    ],
    "points": 255,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1695003570
  },
  {
    "id": 37546766,
    "title": "Things that do not exist in Dimension Apple",
    "originLink": "https://maxread.substack.com/p/a-literary-history-of-fake-texts",
    "originBody": "Subscribe Sign in Discover more from Read Max Explaining the weird new future, one newsletter at a time. Subscribe for a twice weekly delivery of internet culture, mega-platform grotesquerie, crypto conspiracies, deep forum lore, fringe politics, and other artifacts of what's to come. Over 25,000 subscribers Subscribe Continue reading Sign in A literary history of fake texts in Apple's marketing materials Exploring the Applecore style MAX READ SEP 15, 2023 100 28 Share Like a lot of troubled young men, I used to pay close attention to Apple’s developer conferences and special announcements, eagerly anticipating each new generation of iPhone and operating system. In the 2000s and 2010s these regular public demonstrations were filled with suspense and anticipation What would the phones look like this year? Which executive would Steve Jobs personally execute in a windowless room immediately following the presentation? Would the new iPhone finally solve my problems and make me happy? Here in 2023, that air of mystery and expectation is gone. Every phone looks the same; every announcement has been widely leaked. But there is still a good reason to attend to Apple’s marketing extravaganzas: The fake texts. Read Max has no advertising or research grant money. Support my study of some of the stupidest and least important things online by become a paid subscriber. Subscribe I’m talking about the mocked-up texts and emails Apple puts together to demonstrate new messaging features in its operating-system updates, presumably written by some well-paid professionals in Apple’s marketing department. These eerily cheery, aggressively punctuated messages suggest a alternate dimension in which polite, good-natured, rigorously diverse groups of friends and coworkers use Apple products exactly how they are designed to be used, without complaint or error. Here are some of the more recent examples of what I am talking about, used to tout this year’s updates, iOS 17 and MacOS Sonoma: Click to enlarge Every new update to Apple software brings along with it marketing materials populated by new casts of characters engaged in a new set of projects, but storylines are never elaborated, let alone resolved, and campaigns pass without any questions being answered. This year we have texts between an oddly formal soapbox derby team (???) and what seems to be a kind of stuffed-pasta club that is by all evidence annoyingly dominated by a man named Rich Dinh, taking suspiciously professional photographs of his own food: Click to enlarge If there is still mystery in Apple events, it is located here, in the uncanny fictional world suggested in these images: Who are these people? And what is wrong with them that they text like this? A history of fake Apple texts, 2011-2023 A proper literary study of fake Apple texts has yet to be undertaken, but with the help of the Wayback Machine, we can sift through more than a decade’s work of marketing materials to identify certain trends and themes. For the sake of precision, let’s begin our survey in 2011, with the launch of iMessage in iOS 5. Here, so far as I can tell, is the first-ever fake Apple iMessage conversation: Here, we see both sides of a chat between “Elisa Rossi” and “Greg Apodaca”12 in which Elisa sends Greg a photograph from her vacation. It is hard not to appreciate the rigidly orthographically correct banter between these … coworkers? Friends? The true nature of their relationship remains unclear. Sharing photographs as Dimension Apple cultural practice The main thing to note here is the photograph. There is nothing--nothing--that denizens of the Dimension Apple love more than sharing photographs with each other. One of the very few desires that people in these texts ever express is for their friends to share photos; “send photos” is the “Come to Brazil” or “show feet” of the Dimension Apple: Photographs of what, you ask? In Dimension Apple, everyone is always spending the weekend together having fun, like Ken, Angela, and Fritz in iOS 73: What’s going on with Meiko, Blair, and the beanbag? Or Ailish and Eden in iOS 8 (see the 9:27 a.m. email; note also the 8:17 a.m. email from Cory Quinn): Or Paul, Natalia, Stefan, and the kids in iOS 9: “How was the road trip?” People are always taking trips in Dimension Apple, seemingly for the sole purpose of creating photographs to share via Apple software. They love to plan canoe trips and summer getaways and casual weekends in Santa Cruz; they form group chats centered around trips that only one participant was on: A normal group chat in Dimension Apple Dimension Apple trip destinations have become increasingly vague, as the “Road Trip” group chat suggests, part of an overall and troubling vague-ening of the Dimension Apple. Are specific places disappearing in Dimension Apple? Are the edges of the map softening? How far can you drive, and where do you end up? Where did Nanditha and her friend go on their many “fun trips”? We may never know: The vagueness was not always the case. In the early years, Dimension Appleites loved to take advantage of iMessage’s free international messaging by, e.g. chatting with their parents while studying abroad: You might wonder why this Dimension Apple daughter is sending her mother such a professional and staged-looking photograph, but the other thing to understand about people who live in Dimension Apple is that they are serious photography hobbyists, and make frequent use of Apple software to edit their photos before sharing them. No wonder Rich Dinh’s ravioli photo looks so good: Note the use of emoji to indicate that the recipient knows what camping is. “Subject: Cool pics!” is a perfect Dimension Apple subject line Surprise! The only thing that approaches “sharing photographs” as a key cultural practice in Dimension Apple is “planning parties.” When they are not emailing or texting to plan trips or share photographs, they are planning parties with one another: Surprise parties in particular have huge importance to Dimension Apple characters, and the surprise party as a motif frequently re-occurs across iOS versions: What does Grant want to talk about? Why does he sound so serious? While birthdays and road trips are still a cornerstone of the socio-cultural world of Dimension Appleites, we increasingly get glimpses of their life outside the constant circle of parties and weekend getaways. Recent seasons prominently feature examples of intensely friendly collaboration on vague projects and presentations, all of which seem to exist in some nebulous zone between “work” and “volunteering,” like the “Pantry Co-Op” planned by a Group Chat mysteriously called “Foodies”: Dimension Apple is the only place where the majority of people use Apple Pages. Who knows what this presentation is about? Or why everyone is so enthusiastic about it? The mystery of John Bishop One rule of Dimension Apple is that there is no cross-over: The people who populate any given update are all new to that update, and never appear again in future marketing materials. “Rich Dinh” and “Trev Smith” are unique to iOS 17; “Sarah Castellbanco” and “Eden Sears” to iOS 8; etc. With one exception: John Bishop. Bishop--whose name seems to be borrowed from an Apple designer--first appears in iOS 10 as a contact auto-populated by Apple Mail. He is possibly a Cupertino-based contractor being recommended to a person named Eric Townley. In most cases, this would be the last time we would ever see John Bishop’s name. But a year later, amidst the marketing materials for iOS 11, who should appear again but John himself?4 Who is John Bishop, the planeswalker who travels so casually from one iOS to the next? What other powers does he possess? And why does he hate Andrew so much? Things that do not exist in Dimension Apple The denizens of Dimension Apple love the following things: Punctuation, trips, sharing photographs, using emoji, taking photographs, surprise parties. You might be inclined to say that they hate roasts, bits, gossip, cynicism, text abbreviations like “LOL,” and other standard features of texting in our dimension--but it is not at all clear to me that any of these things even exist in Dimension Apple to be hated. Like Android users, irony simply does not occur in Dimension Apple. Does the Dimension Apple exist? For a long time I have enjoyed the stilted, improbable cheeriness of fake Apple texts for their extreme distance from my own texting habits and experiences. (As my friend Emma put it to our group chat “if any of you texted me like this I would immediately call your significant others to make sure you hadn’t been kidnapped.”) But in the last year or so I have realized that Dimension Apple does exist--or at least overlaps with our own--in one very specific place: The WhatsApp groups that the parents at my son’s daycare/school create to share information or set up play dates. In these groups, and only in these groups, do I encounter the same kind of earnest helpfulness and baffling ebullience that exists in the Dimension Apple. Naturally, I find them totally alienating. The Read Max Library of Fake Apple Texts To aid future study of the Dimension Apple, I’m publishing all the screenshots and images of fake Apple texts I was able to pull from the Wayback Machine below. Please share any theories, details, theses, etc. you develop from this corpus. Note that the Wayback Machine didn’t scrape any images from iOS 13, so we’re missing that update. iOS 5 Click to enlarge iOS 6 Click to enlarge iOS 7 iOS 8 Click to enlarge iOS 9 iOS 10 Click to enlarge iOS 11 Click to enlarge iOS 12 iOS 14 Click to enlarge iOS 15 iOS 16 Click to enlarge iOS 17 Click to enlarge 1 Italian-American excellence! 2 Quick Google searches suggest that all the names that appear in fake Apple texts are the real names of Apple employees and contractors from across the company, which to me has got to be a huge employment perk. 3 One interesting thing about this image is that it’s the only one where an email forward appears. Even more bizarrely, no email subject lines in Dimension Apple ever begin with “re:” Does everyone in Dimension Apple simply type a new subject line in ever email reply? There’s more to say about this image in general--ostensibly it’s showing off the ability to read email while you’re FaceTiming, but why is Eden reading this melancholy email from her old coworker Sarah while FaceTiming with someone in Yosemite? 4 Bishop appears only in an image from the iOS 11 preview page that went up before it was officially released--the image was later changed to remove his text message. Subscribe to Read Max By Max Read · Thousands of paid subscribers Explaining the weird new future, one newsletter at a time. Subscribe for a twice weekly delivery of internet culture, mega-platform grotesquerie, crypto conspiracies, deep forum lore, fringe politics, and other artifacts of what's to come. Subscribe 100 Likes · 13 Restacks 100 28 Share 28 Comments Frank Sep 15 Liked by Max Read I wish I could remember where I heard someone describe the imagined Apple customer persona in these videos as \"California Man,\" the guy who tracks his workouts, creates intricate shared photo libraries on his iPad, goes to Hawaii on the regular, records his daughter's birthday using augmented reality goggles, etc. LIKE (20) REPLY SHARE Liz P Writes Liz’s Substack Sep 15 Liked by Max Read I’m in group chats with big tech marketing executives and I can promise you: the valley can be as uncanny as Dimension Apple makes it seem. LIKE (7) REPLY SHARE 26 more comments... Top New Community What's the deal with all those weird wrong-number texts? Digging into the world of \"pig-butchering\" scams JUL 1, 2022 • MAX READ 192 55 Mapping the celebrity NFT complex Where do celebrities even hear about “bored apes”? Who is recommending that they buy one? Is this really the best thing any of them can think to do with… FEB 2, 2022 • MAX READ 75 22 '90s Dad Thrillers: a List Notes toward a theory of the Dad Thriller NOV 9, 2021 • MAX READ 105 118 See all Ready for more? Subscribe © 2023 Max Read Privacy ∙ Terms ∙ Collection notice Start Writing Get the app Substack is the home for great writing",
    "commentLink": "https://news.ycombinator.com/item?id=37546766",
    "commentBody": "Things that do not exist in Dimension AppleHacker NewspastloginThings that do not exist in Dimension Apple (maxread.substack.com) 238 points by lysozyme 21 hours ago| hidepastfavorite97 comments Nition 16 hours agoThis doesn&#x27;t even delve into the hidden deceit!In the first image, the sender says they went camping in the mountains, but then sends a photo of the seaside. Did they really go camping at all?In the soapbox derby image, the sender claims they \"just finished the latest renderings\" for the sushi car design, yet the design is clearly AI generated! They&#x27;ve been lying to the team about how they&#x27;re creating the designs.Rich Dinh, who&#x27;s dominating the chat with his ravioli dish? It&#x27;s a stock photo by Helen Rushbrook! Is he making anything himself?How many people in Dimension Apple are secretly struggling like these three? There must be huge pressure to conform. reply tptacek 19 hours agoprevThis is very funny, but it&#x27;s also disquieting, because people who text like this absolutely do exist; I know because I&#x27;m on the list for the block I live on. Modulo a lack of spelling and grammar errors, this is what normal people sound like. The idea that very normal people behavior is this odd or telling is, itself, pretty telling! reply zamfi 16 hours agoparentI’m also on the list of the block I live on, and people also text like this on that list……including, me. On that list.I think part of what’s disquieting is that there is a “formal text English” that exists (and people know about), but people use it only in certain circumstances, like when you’re texting a group of people that you don’t know very well, and that you don’t want to offend, or that you want to seem “proper” for.It’s temping to think that what we observe on these lists is how those people are, but every now and then someone on my list will post a message intended for someone else…and it no longer fits “formal text English”.I suspect this language’s use is highly contextual. To me, part of the the oddness of it all is that I would almost never use formal text English in the contexts shown in these marketing images. reply distract8901 13 hours agorootparentHuh, maybe I&#x27;m weird, but I use the same &#x27;voice&#x27; in all forms of communication. With the exception of the shorthand I share with my husband, I always text in full, properly formatted sentences and paragraphs. I use the same style for texts, mastodon posts, and long hackernews comments. It&#x27;s also the same style I typically speak in normal conversation.Now I think about it, that&#x27;s probably an autistic trait. I kind of have to carefully construct what I&#x27;m saying in any context, and it looks like this. reply mikewarot 8 hours agorootparentI am also very careful to not let the shortcomings of the keyboard alter, in any way, the text I&#x27;m composing. Finding [ and ] on a phone are a pain in the rear, but if I&#x27;m composing something with links here on HN... it&#x27;s going to look as much as possible like everything else I post.For me, technology bends to my will, not the other way around. reply bombcar 12 hours agorootparentprevMy iMessage texting from my computer is quite conversational and detailed, taking particular delight in proper punctuation and spelling.My texts from my iPhone look like they were created by an AI trained on deranged monkeys attempting to recreate Shakespeare. reply nenaoki 13 hours agorootparentprevMy hackernews comment drafting process often goes right up to the end of the delay setting I have. Closer to one and done, a la speech, is a habit I&#x27;d love to pick up. reply rz2k 18 hours agoparentprevIn addition to your block&#x27;s list, from the article:> Does the Dimension Apple exist?> For a long time I have enjoyed the stilted, improbable cheeriness of fake Apple texts for their extreme distance from my own texting habits and experiences. (As my friend Emma put it to our group chat “if any of you texted me like this I would immediately call your significant others to make sure you hadn’t been kidnapped.”) But in the last year or so I have realized that Dimension Apple does exist--or at least overlaps with our own--in one very specific place: The WhatsApp groups that the parents at my son’s daycare&#x2F;school create to share information or set up play dates. In these groups, and only in these groups, do I encounter the same kind of earnest helpfulness and baffling ebullience that exists in the Dimension Apple. Naturally, I find them totally alienating. reply sctb 17 hours agoparentprevI text almost exactly like this. Probably both Dimension Apple writers and myself are pretending to be normal in the same way... reply Traubenfuchs 16 hours agoparentprevI text back and forth like this at the beginning of dating sometimes. Though if that happens, things usually don‘t work out because it means we just don‘t vibe naturally. reply yjftsjthsd-h 21 hours agoprevOkay that was surprisingly funny to read through. It&#x27;s interesting how in isolation the texts are probably fine and work well for demonstrating the software, but taken together they do present an oddly stilted view of the world. reply tardibear 21 hours agoprevThe title of the linked page is \"A literary history of fake texts in Apple&#x27;s marketing materials\". reply Kiro 20 hours agoparentYeah, don&#x27;t understand why they chose this title for the submission. reply kazinator 16 hours agoprev> Like a lot of troubled young men, I used to pay close attention to Apple’s developer conferences and special announcements, eagerly anticipating each new generation of iPhone and operating system.... As a deranged older man, I now collect artifacts from Apple advertisements, to reconstruct the characters and interactions portrayed in their fantasy world, to inform the public about how (for instance) they text differently from real people. In my defense, it&#x27;s no worse than collecting Princess Di memorabilia, or being crazy into Pokemon or Harry Potter. reply goosedragons 44 minutes agoparentHow far does it go? Does your kitchen look like this [0]? Do you have that picture of an apple hanging there? Do you check your stocks at the kitchen table while your partner weirdly looks on while chopping vegetables?[0] https:&#x2F;&#x2F;cdn.theatlantic.com&#x2F;thumbor&#x2F;ug105jVWociHnn_oPJoa8gPj... reply Taek 14 hours agoparentprevQuite the contrary to deranged, I think its exactly in line with someone who collects sports jerseys or old coins with manufacturing defects.The world of Apple propaganda is expansive and rich, and has more than enough depth to justify a worthwhile hobby.In fact, at a party or gathering I&#x27;d be way more excited to hear about your collection than to hear about a guys collection of stamps or WWI era guns (not to disparage those hobbies, they just don&#x27;t interest me personally much) reply tmpX7dMeXU 14 hours agorootparentThat’s deranged. reply jychang 14 hours agorootparentYep. At the end of the day, all humans are moderately deranged. If we were all rational value optimizers, the world would be a lot shittier place. reply twic 20 hours agoprevIf you worked on these images, surely you would be irresistably tempted to leave little easter eggs referring to past characters? \"Hey Kim, how are you doing?\" \"Great, thanks! Apart from going a littleh hard on the ravioli last night!\". reply ajmoo 16 hours agoparentI worked on many of these! Was on the Marcom team that rebuilt screenshots for content swaps and hi-res output. Every screen went through multiple rounds of approvals by many different teams, including writing. Unfortunately any Easter eggs I tried to hide in there were all caught ;) reply tobr 16 hours agorootparent> rebuilt screenshots for content swaps and hi-res outputCan you explain this work in more detail? Are you basically reconstructing the contents of a screenshot in vector graphics so it can be translated&#x2F;edited&#x2F;scaled up but still look correct? reply ImaCake 16 hours agorootparentprevThanks for trying. I think it would have played to Apple’s advantage to keep easter eggs in these but bureaucracies abhor risk. reply zamalek 20 hours agoparentprev\"Dinh ran off with some floozy from a group chat about the presentation on Tuesday. :cucumber: :sunset: :shoe:\" reply wzdd 10 hours agoparentprevNot a past character, but one of the screenshots has a dentist appointment at 2:30, which is an old pun. reply Terretta 1 hour agorootparent\"Tooth hurty\" is the old pun and that sure counts as Easter egg reply jodrellblank 17 hours agoparentprev> \"going a littleh hard on the ravioli last night!\".\"Mmm. that. looks. delicious. https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=t-jK37C4CxA reply Almondsetat 19 hours agoprevI love going to the Apple store every time a new OS version comes along and browse the various devices on display to see all the fake messages, photos, emails, notes, drawings, bookmarks, etc. they have set up reply in3d 17 hours agoprevThat’s how you communicate when you have a first date set with someone you’ve only talked to through messages. At this point, you can only mess it up and nothing you say can substantially help. reply sdwr 21 hours agoprev> Dimension Apple trip destinations have become increasingly vague, as the “Road Trip” group chat suggests, part of an overall and troubling vague-ening of the Dimension AppleThis bit was hilarious. It&#x27;s comfort food in the same vein as the Barbie movie - everyone is polished and no one needs to work. reply abruzzi 20 hours agoprevI didn&#x27;t know \"chillax\" was a word, especially one that would auto correct \"chill\".So from the authors comments, I take it I am a rare minority in that all my texts are in full, grammatically correct sentences, with proper punctuation? reply tobr 19 hours agoparent> I didn&#x27;t know \"chillax\" was a word, especially one that would auto correct \"chill\".The picture shows the opposite: you tried to type “chillax” but it is correcting it to “chill”, but offering you to revert the correction. reply Doctor_Fegg 19 hours agoparentprev*author&#x27;s reply lxgr 18 hours agoparentprevQuite likely!There&#x27;s nothing wrong with either, though, and I wouldn&#x27;t recommend judging anyone&#x27;s mastery of style or grammar based on only a single (and quite idiosyncratic) communication channel. reply smegsicle 19 hours agoparentprevstudies show that kids these days interpret texts ending in a period as passive aggressive or something reply jwells89 18 hours agorootparentIn a similar vein, ending with an ellipsis (…), while used by many to indicate thought, consideration, or other sort of pause in speech often comes across to younger readers as a sort of passive aggressiveness or shortness of patience. reply Gigachad 16 hours agorootparentSurely it’s always meant that. It’s just a long pause.I’d think something like “are you serious…” would be pretty universally understood. reply jareklupinski 13 hours agorootparentprevand just when you think you&#x27;ve solved the ellipses and fullstop, someone comes along and hits you with the: .. reply astrange 16 hours agorootparentprevIt&#x27;s interpreted as old person, because there&#x27;s a class of boomer that ends all their sentences with … and then puts them in the middle of half of them too. reply lwhi 19 hours agorootparentprevIt&#x27;s called full-stopping. reply elcritch 18 hours agorootparentSure. reply dtech 19 hours agorootparentprevok. reply lxgr 18 hours agorootparentCapitalization at the beginning of your sentence would have made for an even more dramatic effect ;) reply smegsicle 19 hours agorootparentprevcalm down bro it&#x27;s just what i heard reply kQq9oHeAz6wLLS 16 hours agorootparent“I’m responsible only for what I say, not what you understand.” – John Wayne replyCharlesW 21 hours agoprevI&#x27;m surprised the author missed the most obvious weirdness: Every message ends with a punctuation mark. reply lawik 21 hours agoparent> These eerily cheery, aggressively punctuated messages suggest a alternate dimensionI don&#x27;t believe they did. reply samastur 18 hours agoparentprevLike mine do. And most messages I receive. I find it weird that some started seeing punctuation as some weird imposition. reply jayd16 18 hours agorootparentYou even send messages like \"lol.\" with the period? reply cafeinux 15 hours agorootparentI don&#x27;t usually text in English so my style is definitely different, but I wonder if I may be living in the Apple Dimension (despite my last Apple device being an iPod Touch from 10 years ago and a MacBook Air from the same time that I bought used to try to install Linux on it): I do end every one of my texts with grammatically correct punctuation, never use \"lol\" or other abbreviations, and typically write in a very clean, sometimes too verbose, style. I wonder if something up with \"us\". Maybe a new pathology name ought to be coined. reply ricardobeat 16 hours agorootparentprevI don&#x27;t think I&#x27;ve typed out &#x27;lol&#x27; in a message since circa 2005, on online chat in some dark corner of the web. Culture bubbles are an incredible thing. reply rvba 6 hours agoparentprevSomeone from the committee checked that reply yieldcrv 20 hours agoparentprevwoah periods, so aggressiveapple employees: blink twice if you’re okay reply 4oo4 15 hours agoprevI would posit that Dimension Apple also extends into the shows it does product placement in, where they stipulate that villans can&#x27;t use Apple products.https:&#x2F;&#x2F;arstechnica.com&#x2F;tech-policy&#x2F;2020&#x2F;02&#x2F;apple-wont-let-f...Once you see it, it&#x27;s really hard to unsee and can make certain shows like Law and Order SVU seem like a longform Apple ad. Ted Lasso on Apple TV (and presumably other Apple TV shows too, though I haven&#x27;t really watched others) is very bad about the amount of Apple product placement and comes close (but doesn&#x27;t quite) ruin the show. reply midasuni 15 hours agoparentI find Sony the most egregious example of poor product placement - especially in Spider-Man films reply tmpX7dMeXU 14 hours agoparentprevThe Apple product placement in Ted Lasso was upsetting. You’re right. It came this close to souring me on a good show. Right up to the point of just dumping it. But even walking that close to the line is disrespectful to one’s audience. reply araes 14 hours agoprevThe article&#x27;s funny, and kind of surreal read considering a dimension fading away while all the characters become cartoons and the landscape becomes vague. Have similar thoughts occasionally.However, also creepy. Not sure if its the way humans have always written, that ChatAI got too good too fast, or that they all secretly had them all for years (Multiple massive AI releases in a year???, \"had em all in a box in the back\")The 2nd image (with pair of images) on the Sharing Photos subsection [1] literally looks like StableDiffusion++. Somebody wrote two prompts, that had something like \"Male with frizzy hair next to girl in pink shirt with glasses with brick building behind\" and then it didn&#x27;t care whether it was the same male in both, or what skin color.Most text also seems weirdly similar to: Generative Agents: Interactive Simulacra [2][1] https:&#x2F;&#x2F;maxread.substack.com&#x2F;i&#x2F;137044198&#x2F;sharing-photographs...[2] https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2304.03442.pdf reply corbezzoli 20 hours agoprevI don’t think real world messages would look good in marketing copy, too many emojis, “uhhh”, “no cap” and bathroom pics. I don’t know what the author is expecting&#x2F;analyzing here. reply bluedays 20 hours agoparentI’m disturbed that you have a lot of bathroom pics in your texts reply jalict 19 hours agorootparentRandom bathroom selfie seems pretty standard. reply hotnfresh 15 hours agorootparentThere are clearly many texting dimensions.Bathroom selfies are not a thing in the one I’m a part of. reply kQq9oHeAz6wLLS 16 hours agorootparentprevIt&#x27;s really not reply corbezzoli 2 hours agorootparentprevGotta show off my creations after all. reply fragmede 11 hours agorootparentprevit&#x27;s not of the toilet, it&#x27;s where the mirror is reply bestcoder69 20 hours agoparentprevHe&#x27;s analyzing what looks good in marketing copy. reply hnburnsy 14 hours agoprevNote that it is typically 9:41 AM or 10:09 AM [Watch] in the Apple Dimension. reply Apocryphon 9 hours agoprevI still want to play the isometric CRPG featured in the page for UICollectionView:https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;uikit&#x2F;uicollection... reply xwdv 19 hours agoprevApple should take a page from Hollywood and create an Apple Cinematic Universe where there is continuity on the various storylines and characters created in their presentations. reply fragmede 11 hours agoparentNext season, on Apple Tv+. Will Nanditha make it with her friend? Did their trip end in romance? reply hnburnsy 13 hours agoparentprevMaybe they laid the foundation for that with the Mother Nature skit. reply xwdv 11 hours agorootparentYea, will be interesting to see if there’s more episodes in later events. reply TheOtherHobbes 18 hours agoparentprevThat&#x27;s the product line. reply neilv 14 hours agoprevSomeone should go find earlier examples of this genre, in ads for consumer online services, as early as the 1980s. reply EdwardDiego 17 hours agoprevI&#x27;m 85% sure that the first road trip pictures with the \"secret beach\" are taken on the southern side of Banks Peninsula in NZ. reply jl6 19 hours agoprevIt’s even funnier if you’ve heard of John Bishop. reply xg15 17 hours agoprevSee also: Every Amazon commercial. reply lxgr 18 hours agoprevNo article about Apple&#x27;s staged sample content can be complete without at least an honorable mention of the Raccoon Basement Incident:https:&#x2F;&#x2F;www.businessinsider.com&#x2F;apple-displays-weird-message... reply cpeterso 17 hours agoparentA screenshot of (an ominous excerpt from) the basement message is in the article’s “iOS 17” section:“Hey Mom, I figured out what all that noise coming from the basement is.” reply qingcharles 16 hours agoparentprevAre wild racoons that friendly? There is one living in my attic and I&#x27;ve seen racoon claws, so I&#x27;m not tempted to try and spook it. reply derefr 18 hours agoprev [–] > they form group chats centered around trips that only one participant was onIt never occurred to me until now, but there&#x27;s actually a somewhat-coherent \"sharing philosophy\" woven through many of Apple&#x27;s products. It seems like Apple envisions a world where social networks and \"broadcast\"-sharing of content don&#x27;t exist. In this world, when people want to \"tell people\" an update about their life, they share that update on a whitelist basis — first meticulously considering exactly the people they want to receive the update, and then pushing the shared item directly into those people&#x27;s faces as a realtime push-notification-generating event, as if with the intent of starting a synchronous conversation. They may then later rope a few more people into the conversation, as they become relevant — but only on a strictly need-to-know basis. Doing this pings them as well, showing them the whole conversation so far — and they&#x27;re expected to read back and keep up.In other words, in \"Dimension Apple\", nobody has a parasocial desire for people they don&#x27;t know to see their posts. People only share things with people they know; and even then, only certain friends get to see certain things. And those friends don&#x27;t mind at all that you had a long conversation that you excluded them from, until you didn&#x27;t.Even more intriguingly, in \"Dimension Apple\", people seemingly only find out news about you because you&#x27;ve shared that news directly with them. No \"following\" someone; no copying messages from one conversation to another; no gossip, even.I would say that real people don&#x27;t work like this... but now that I think of it, I&#x27;m pretty sure that this is exactly how people in the upper class — people for whom \"discretion\" is core to their lifestyle — would prefer all their \"sharing\" be done. reply ruffrey 18 hours agoparentI think this is mostly how human communication worked until the internet era. Those who aren’t plugged into social media still operate this way - sharing directly with others. In real life, when people interact, they begin by asking “how are you?” Which is the opposite of social media. reply Loughla 18 hours agorootparentI&#x27;m on no social media (besides this site). The number of times a day I have to say, \"no, I didn&#x27;t see [insert random event here].\" Would honestly surprise you.There is just an expectation in society today that you know everything that&#x27;s going on in everyone&#x27;s life. It&#x27;s very, very strange to me. It&#x27;s hard to explain why, but I get serious black mirror sort of vibes from it. reply cafeinux 15 hours agorootparentAt least you get asked about those events. In my family I&#x27;m known as the guy who knows everything last: often conversations go like \"He&#x27;s been sad for two months now, but it&#x27;s understandable – Why? What happened? – Well, you know, the break-up... – What? John broke up? I didn&#x27;t even knew he had someone...\"So now, my mother (who lives on Facebook) usually calls me to let me know when she learns something she might deem important on Facebook. And I honestly appreciate the attention, it&#x27;s good when you feel that people think about you. And it&#x27;s usually important information she gives me, because we&#x27;re not into gossip. reply Gigachad 16 hours agorootparentprevNo one expects you to view social media all day. That are asking “did you see ___” because they want to tell you about it but first check you haven’t already seen it. reply Loughla 2 hours agorootparentThat&#x27;s not how it goes. The conversations usually lead with them talking about something other people are doing, as if I have any idea what they are talking about. Then I ask them to explain the context. THEN they&#x27;ll ask if I saw X on Facebook or Insta or whatever. reply derefr 18 hours agorootparentprevSocial networks work a lot like mass-mailing your friends and relatives with postcards &#x2F; greeting cards.Also, the lack of gossip in \"Dimension Apple\" is a crucial distinction. You don&#x27;t need a social network to spread news, if you know that everything you tell that one aunt is going to be repeated on every phone call she makes for the next two weeks. (And she makes a lot of phone calls.) reply jwells89 18 hours agoparentprevI feel that a line from this could be traced back to the era prior to ubiquitous social media, where the vast majority of sharing was happening over messengers like AIM, MSN, and Skype or via email, and if you go back Apple stuff at that point was fairly geared around sharing through iChat (AIM) or Mail.app.There was a period earlier on where macOS and iOS had Facebook and Twitter posting built in where those got some airtime in promotions, but as the shine of social media wore off those integrations disappeared and their promotional material reverted to a world where sharing happened over the modern equivalent to iChat (iMessage) or maybe Mail.app. reply derefr 17 hours agorootparent> where the vast majority of sharing was happening over messengersArguable — mass-mailing async updates (think: postcards) about your life has been ubiquitous for as long as people have been going off and doing interesting things. This evolved into mass-emails. I was BCC&#x27;ed a lot of \"wedding photos.zip\" and \"in paris.jpg\"s emails in the 1990s; much more often than such a thing was ever directed at me by a friend or relative, let alone directed at me synchronously in a messenger app.Remember also: messengers back in the 90s and early 2000s didn&#x27;t have a concept of \"server-buffered message sends.\" For a message to transmit over AIM&#x2F;ICQ&#x2F;MSN&#x2F;etc, both people had to be online at the same time. If you were travelling — and so potentially in a separate time zone — messengers really didn&#x27;t work very well as a way to send large image files. Until very late in the lives of their protocols, most of them didn&#x27;t even support sending files! reply jwells89 17 hours agorootparentAll very true. My perspective is no doubt skewed by growing up with AIM&#x2F;MSN and having become an adult and starting doing interesting things right around the time social media had begun to peak, a fair deal after the heyday of mailed postcards and BCC&#x27;d email threads. reply mortehu 17 hours agoparentprevI think what you are describing is how most people use Google Photos, or perhaps Dropbox or iCloud, and I think this is the main way people share photos. reply musictubes 16 hours agoparentprevI doubt that anyone on Apple’s C suite has a personal Facebook or other social media. Once you get to a certain level of fame&#x2F;wealth&#x2F;corporate importance your social media output will go down or at least become not personal. Having a private social network makes a lot of sense for them. Might be better for everyone else too. reply iseanstevens 17 hours agoparentprevWow - fascinating observation!Feels to me similar to the “companies ship their org chart” concept. reply teaearlgraycold 17 hours agoparentprevWell, this is how I live. I don&#x27;t use any broadcast-based social media. The only form I consume would be YouTube or Twitch, I believe without any parasocial weirdness.After I finish a trip (recently I went to Supai, AZ) I send little bundles of photos to different people via my phone very much like these screenshots. reply theragra 16 hours agorootparentIt would feel a bit weird to me, because it looks like marketing myself and my ego. In blogs or social networks, people only check you if they want. reply teaearlgraycold 15 hours agorootparentI think different people could do the same thing for different reasons. For me it&#x27;s just about catching people up, and sometimes people return the favor.Regarding social networks, though - people aren&#x27;t exactly looking at your photos because they want to. Most social networks decide what you see, so the algorithm will put vacation photos in front of you even if you don&#x27;t like that content. reply theragra 6 hours agorootparentI agree to some extent. When I had livejournal, it was more authentic. Back then, you always have been seeing everything chronologically, and I hoped my friends will tell me about their lives. Unfortunately, writing is not for everyone, people subscribe to other users, not only their friends now, and algorithm feeds won. reply brightlancer 16 hours agoparentprev [–] Could this be because Apple doesn&#x27;t run a social media service? On some levels, are their products&#x2F; services competing _against_ social media?Or is this just part of Apple&#x27;s (and many of its users&#x27;) elitist and exclusivist mentality: they&#x27;re better than everyone else, they don&#x27;t need to interoperate with the rest of the world, and their sharing is reserved for the select.(Not all Apple users are like that and probably not even the majority anymore, but it _is_ Apple&#x27;s mentality, it was the dominant mentality among Mac users, and it&#x27;s still common enough and OMG loudly pronounced enough by current Mac users.) reply fragmede 11 hours agorootparent [–] it&#x27;s not Facebook, but iMessage and iCloud Photos are a social media service. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article discusses the recurring themes and characters in Apple's marketing materials, especially focusing on the depiction of fake text conversations.",
      "The author notes the distinct culture in these fictional conversations, marked by an absence of irony and a focus on sharing photos and organizing social events.",
      "It speculates about a parallel universe called \"Dimension Apple,\" likened to certain experiences in parent WhatsApp groups, and includes a compilation of screenshots of fake Apple texts."
    ],
    "commentSummary": [
      "The article critically reviews the text message portrayals in Apple's marketing resources and contrasts them with how individuals really interact.",
      "Discussions revolve around whether Apple's communication strategy mirrors reality, the diminishing use of social media integrations in the company's marketing, and the constraints of sharing large image files via messaging apps.",
      "There is also speculation concerning Apple's absence from the crowd of companies offering social media services."
    ],
    "points": 238,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1694968008
  },
  {
    "id": 37546469,
    "title": "Chili Oil Notes",
    "originLink": "https://uptointerpretation.com/posts/chili-oil-notes/",
    "originBody": "Up to Interpretation Chili Oil Notes I’ve been getting a little obsessed with chili oil. A little…lost in the sauce so to speak. Here’s some notes and general thoughts on chili oil. Chili Extraction There are two main ways of extracting flavor from chilies with oil: hot flash or slow infusion. A hot flash basically means you heat up the oil until it’s really hot, almost smoking , i.e. 375F. You pour it over the chilies in one dramatic swoop. A slow infusion is less dramatic. You heat the oil and infuse the chilies at a lower temperature, say 200-250F. In my experiments, the hot flash produces a noticably more intense flavor with some roasted notes. However, with that flavor, there is some bitterness. The slow infusion comes out milder, but no bitterness. That said I’ve only tried at the above temperatures. Perhaps a slow infusion at 250-300F or a hot flash at 325F could be better? Interestingly enough, the hot flash creates an oil with the chili pieces noticably suspended in the oil, while the slow infusion results in an oil with the pieces all at the bottom. I suspect this is because the chili pieces are more intensely fried in the hot flash one, which either dissolves more soluble materials into the oil, making them lighter, or causes the pieces to unfurl and be more buoyant. In some of the Chinese videos that I’ve watched, people do a sort of hybrid style where you add hot oil multiple times, with the idea to almost “temper” the oil, i.e. not put such a large quantity of hot oil into the chilies all at once. I could see that working too. They also temper with baijiu, i.e. strong Chinese liquor. That could also potentially be another avenue for extraction since alcohol is known to extract more flavor compounds than water? Capsaicin is soluble in alcohol, so maybe this could boost spiciness? This does put into question whether the ratio of oil to solid material matters. I usually make the Serious Eats chili crisp recipe by Sohla El-Waylly. It uses the hot flash method, but it also includes a lot of other ingredients like ginger, cardamom, cumin, sugar, mushroom powder, etc. Perhaps the sheer quantity of solids means more thermal mass and therefore less overcooking? I’ve also thought about doing a really low and slow extraction, say by using a sous vide machine. You do run into food safety issues, since the chilies can contain botulism. Alternatively you could try doing multiple extractions at different temperatures or even blending different styles. Oil Type The easiest oil to use is a neutral oil of your choosing. It won’t add any flavor, which will let the chilies shine through. This is fairly important for experiments since you don’t want the taste of the oil to obscure results (I learned this the hard way). But there are some other options. Classic Sichuan oil is caiziyou, or roasted rapeseed oil. It’s hard to find in the US, but you can get it at more and more Chinese supermarkets or online at Mala Market. At a Chinese supermarket, you’ll probably have to read some Chinese to make sure you’re buying the right stuff. Caiziyou is nutty, roasted, almost like a milder sesame oil. It’s great stuff to cook with and it does make a nice chili oil. Budget-wise it is more expensive, especially if you’re buying from Mala Market. I’ve also seen people use clarified butter, like here, or beef fat like in hot pot bases. I imagine an olive oil could be good if you stick to slow infusion. Of course there are also combinations that could be made. Maybe a chili oil with a little spike of beef fat just for flavor. Chili Type I usually keep quite a few different kinds of dried chilies around. Right now I have Thai birds-eyes, er ting jiao, xiao mi la, kashmiri, Japones. Of course it’s hard to determine what exactly a lot of chilies are, since there could be multiple names depending on the country. Perhaps Japones are really Heaven-Facing chilies. Or xiao mi la is birds-eye. I used to spike the oil with some extra spicy chilies like ghost peppers or Carolina reapers. I stopped doing that because they were way more expensive than other chilies, it was extremely painful to seed them and grind, and I wasn’t convinced they actually added that much heat. One or two ghost peppers could easily be matched by a couple handfuls of birds-eye or xiao mi la. Especially since these super hot peppers are not always consistent in spice level. I did once make a chili oil with chocolate bhutlah that was delightfully spicy. I also used to spike the oil with some chipotle peppers to add some smokiness. I might do an experiment to see how much of a difference that actually made. Spices & Aromatics Recipes run the gamut of no spices at all to lots and lots of spices. Generally I’m of the view that more spices are better. But this might be worth testing. I usually add star anise, black pepper (often omitted, idk why), cumin, Sichuan peppercorn, black cardamom. Potentially also cinnamon, and sand ginger. There’s also aromatics like garlic, ginger, scallions, orange peel, etc. In Sohla’s crisp recipe, you fry a bunch of crispy shallots and garlic chips before making the oil. That way the oil gets infused with lots of shallot and garlic flavor. I do love that, but it is a lot of work. Seasoning Should you season the chili oil? Most recipes do call for some salt, which interestingly is not soluble in oil. I guess the salt ends up sticking to the chilies and therefore seasoning that aspect. Some recipes go as far as adding sugar, MSG, and other MSG-alternatives. This is where the ontology of the chili oil matters1: Are you making a condiment or an ingredient? In my view, a condiment should be seasoned sufficiently that it has some sort of inherent tastiness. You’d be willing to eat a spoonful of ketchup or a spoonful of mayo, even if it’d be a little intense. An ingredient does not have to fulfill that requirement. Nobody’s just eating a spoonful of flour. So a chili oil that’s an ingredient will omit sugar and MSG and taste a little bland. That’s fine if you’re adding it to a dish like a cold poached chicken, where there will be other seasoning. A chili oil that’s a condiment will taste balanced on its own. I do think if you add seasoning, you should go all the way and add sugar, salt and MSG. They balance each other out really nicely. If you add seasoning, you could be veering into a chili sauce or a chili crisp. Chiu chow chili sauce is a great example. For a lot of people, especially in the US, what they consider chili oil is actually chili crisp, specifically Lao Gan Ma, the most popular brand by far. Lao Gan Ma actually has douchi or fermented black beans. Note that these are fermented black soy beans, not the black beans that we eat in the West. Personally I find douchi a little too strong of a flavor. They’re intensely salty and can overpower the rest of the ingredients. It is important to note that the oil will stay a little bland regardless because salt does not dissolve into oil. Interestingly enough it appears that sugar does dissolve. Chili Grinding For the longest time I used a spice grinder to blitz up the chilies. It was fast and simple. I never loved the texture I got though. You’d end up going too fine too quickly. Plus it tended to create a lot of inhalable dust which made the process pretty painful. I’ve recently switched over to toasting the chilies in a tiny bit of oil, then pounding them in a mortar and pestle. It’s definitely more manual labor, but you end up with these beautiful large flakes that have great texture. It’s also releases less dust into the air. You do have to make sure all of the chilies are properly toasted. They should be shiny, lightly puffed up, and brittle. Otherwise you end up with these large chunks that are too flexible to be ground down and end up mixed into the flakes. If we’re getting really nerdy here (and hey, this is already pretty nerdy), I’d wonder about the particle distribution of grinding chilies and how that affects flavor extraction. As any coffee nerd can tell you, a spice grinder is pretty awful for grinding coffee because it produces a very uneven particle distribution. You can end up with what are called fines and boulders, fines being tiny, dust-like pieces of coffee that create bitterness in the cup, and boulders being large chunks that create sourness or emptiness in the cup. A mortar and pestle isn’t that much better. If anything you end up more bimodal. The end result is a bunch of large flakes covered with very fine chili dust. Likely due to the combination of pounding and grinding motions in the mortar and pestle. The pounding breaks the chilies into big chunks, and the grinding produces the fine powder. Is there a way to get better particle distribution? You could in theory sift the chilies. But often times the powder appears stuck to the flakes a little bit. Styles So far we’ve mostly been talking about Chinese style chili oil, specifically Sichuan style. But what about other chili oils? Salsa macha is a very popular Mexican style. And you could argue that a tadka is essentially an ad-hoc chili oil made at the last second. Could you make a batch of Indian chili oil in advance, say by infusing mustard seeds, curry leaves, cumin and coriander in some ghee and substitute it for a tadka? I bet you could. As for more European flavors, ChefSteps does an interesting Thanksgiving flavored oil. I bet a Calabrian chili oil would be incredible. Experiments As you can see, there’s a lot of questions and ideas here. Therefore, I’m gonna need to do some experiments! But first, we should think about methodology and potential issues with the experiments. The most important criterion for an experiment is reproducibility. Unfortunately, that is rather hard with chili oil, because the chilies are organic compounds with variable spiciness. We’re also using manual grinding so grind size is not easily controllable either. This means that while we can grind up a batch of chilies, use them in two different oils, and compare, we can’t necessarily compare these oils to oils from a separate experiment. Of course we should still try to make the experiments as reproducible as possible, so we should standardize the chili blend, ratio, and so on. It’s also important to make the base oil as neutral as possible. I tried doing an experiment with caiziyou as the base oil, and its nuttiness occluded any differences between the two chili oils. Of course, like coffee, you can never get a fully reproducible setup. Instead it’s probably best to make a bunch of oils, do some experiments, then learn about the general variables and how they affect the flavor. In coffee that’s grind size, temperature, time, ratio, dose, brew style. Chili oil has likely similar variables. Anyways, on to the potential experiments. Here are a few that I have in mind: Infuse chilies at different temperatures Hot flash chilies at different temperatures Adjust the ratio of chilies to oil Add alcohol to extract more flavor How much do the spices actually affect the flavor? Sous vide chili oil Mortar and pestle vs spice grinder Add oil for a hot flash multiple times. Does stirring matter? I’d love to hear if someone has other experiment ideas or other thoughts about chili oil. Feel free to reach out at nick@nicholasyang.com! A sentence I’m sure nobody else has ever uttered. ↩︎ 2023-09-16 © Nicholas Yang, 2023 Powered by Hugo, theme Anubis.",
    "commentLink": "https://news.ycombinator.com/item?id=37546469",
    "commentBody": "Chili Oil NotesHacker NewspastloginChili Oil Notes (uptointerpretation.com) 235 points by hardwaregeek 21 hours ago| hidepastfavorite108 comments raylad 7 hours agoI am going to take this opportunity to share the chili oil that is so good that I&#x27;m sure I could never make it: the most delicious chili oil made with sesame oil from the best sesame oil producer.Yamada Seiyu Goma ra-yu (spicy sesame oil)Ingredients are: sesame oil, red pepper, spring onion, ginger, sansho (zanthoxylum piperitum), cinnamon, star anise, mikan tangerine skinHere is the manufacturers site: https:&#x2F;&#x2F;sesamepaste.jp&#x2F;en&#x2F;#ProductsAlthough it&#x27;s not on the Nijiya Market website, it can often be found there, mixed in amongst the lesser sesame chili oils. It&#x27;s a favorite gift of mine to give, and nearly everyone who receives it mentions later to me how much they enjoyed it. reply whynotqat 19 minutes agoprevIn Modernist Cuisine, Myrvhold et al advocate for a sous vide chili oil. Roast the chilis and other ingredients at about 250 in the oven, then sous vide at 158. Never made it myself, and it&#x27;s not intended to be authentic Chinese chili oil. They also mention using a pressure cooker instead, which seems reasonable. reply scarmig 16 hours agoprevMy \"recipe\" is simply caiziyou and erjingtiao, hot flash method, and it works well enough.One thing the author misses about the oil choice: what makes caiziyou superior to other oil choices is that it has a high viscosity. That results in the chili oil sticking to food and the tongue better once used; it probably also affects the fluid dynamics of the frying step in a way, though I can&#x27;t say if it&#x27;s significant. reply hardwaregeek 16 hours agoparentOoh I’d never heard that! I will say there is something magical about caiziyou. It’s so nutty and roasted and beautifully dark. reply nycdatasci 10 hours agoprevThe discussion here reminds me of the lady tasting tea experiment. https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Lady_tasting_tea reply ChrisKnott 7 hours agoparentI&#x27;m struggling to believe this can be done by taste; I could understand it if you went milk+teabag, then added water, but this seems like brewed tea in a pot added to milk in cup v.s. milk added to brewed tea in a cup, which will get totally mixed either way...?The only difference I can think of in these situations is the temperature of the bottom of the cup. Can this really detectably affect the taste? Maybe she was subconsciously feeling this? reply Adverblessly 1 hour agorootparentThe difference is that if the milk is added first, slowly pouring the prepared tea will gradually heat up the milk from its initial temperature to the final combined temperature. If the tea is added first, the milk will start at tea temperature and gradually cool down to the combined temperature.Imagining a scenario where the milk starts at 0 degrees and the tea at 100 and having a 1:10 milk:tea ratio, the difference is whether the milk gradually heats up 0->90 or gradually cools down 100->90.Thus, in one scenario the milk will experience more extreme temperatures that can \"burn\" it whereas in the other scenario it won&#x27;t.To me at least I&#x27;d say the difference is barely noticeable, if I had to guess I&#x27;d say that in a blind tasting I might be able to get 55% of my guesses right. reply RugnirViking 5 hours agorootparentprevI do know that brewing tea with not-quite boiled water (70-80 degrees C) instead of boiled water 80-90 is recommended because otherwise the leaves go bitter.Perhaps if they were using water fresh from the kettle the milk-first way was preventing the \"scalding\" reply barrkel 5 hours agorootparentBitter tannins are what you want in black tea, they&#x27;re smoothed out by milk but a good cup needs to be grippy in the mouth. Black tea made with anything less than boiling water is usually not very nice, in my experience. reply gmac 1 hour agorootparent100% this. Which is why, as a Brit, my heart still sinks when ordering tea in most European countries: nine times out of ten you get a glass of once-boiling water to add a teabag to. reply gmac 1 hour agorootparentprevI believe the point is to avoid overheating (any of) the milk, because that changes its flavour. For example, tea made with UHT milk tastes very different (and, to me, much less nice) than tea made with pasteurised milk.When adding the tea to the milk, the temperature of the milk will never exceed the final temperature of the mix. When adding the milk to the tea, it will. reply Mistletoe 9 hours agoparentprevI read all that and didn’t get the thing I want to know. Could the lady tell the difference?! reply tomsmeding 7 hours agorootparentIt&#x27;s on the page, pretty high up:> One could then ask what the probability was for her getting the specific number of cups she identified correct (in fact all eight), but just by chance. reply thaumasiotes 9 hours agorootparentprev> David Salsburg reports that a colleague of Fisher, H. Fairfield Smith, revealed that in the actual experiment the lady succeeded in identifying all eight cups correctly. reply Mistletoe 1 hour agorootparentThank you. That’s very impressive! reply michaelteter 17 hours agoprevWith regard to the health risks many comments note, I wonder how many natural (collaborative, so to speak) ingredient mixes might contribute to positive food safety despite the known threats.For example, I had a roommate who would cook salmon in a tomato sauce and leave it to sit overnight. From my background, this sounded like obvious death, but he assured me it was a common recipe in South Africa. I did try it, and it was delicious. I had no adverse reactions after eating it (a full day after it was cooked and left sitting in the pan). reply throwaway892238 16 hours agoparentWhy would this concern you more than anything else? If you cook the fish, it&#x27;s not going to \"go bad\" as quickly as if it were raw. If you cook anything to a particular temperature and time, it should kill most bacteria in it. Then the acidity of the tomato should help resist bacterial growth a wee bit longer than something pH-neutral. On top of all this, \"obvious death\" from bacteria in food is some serious hyperbole; it&#x27;s quite rare to die of food poisoning. reply civilitty 16 hours agorootparent> it&#x27;s quite rare to die of food poisoning.I’d go so far as to say it’s virtually unheard of for healthy adults to die of food poisoning from cooked food going bad*. It’s mostly children, the elderly, and the immunocompromised that are at risk. The fast majority of microbes that cause food born illness are really common in our environments and we’ve built up plenty of immunity.* fecal contamination and other toxins like paralytic shellfish poisoning are another matter reply Gigachad 14 hours agorootparentDeath would be unlikely, but food poisoning from cooked food is pretty common. You&#x27;ll just vomit for half the day. Which while being a long way from death, is still very unpleasant. reply schwartzworld 2 hours agorootparentprevYou&#x27;d have to add controls before making that statement. Unsafe food practices by definition will raise your likelihood of getting sick from cooked food. Most people don&#x27;t leave cooked fish sitting on the counter overnight, therefore getting sick from cooked fish is going to be relatively uncommon. reply lfpeb8b45ez 12 hours agorootparentprevThere’s always this case from 2008: https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3232990&#x2F; reply terribleperson 8 hours agorootparentBacillus cereus (the culprit in that case) the microbe associated with &#x27;fried rice syndrome&#x27;? It seems to be pretty reply kelipso 11 hours agoparentprevSome fish curries from kerala can be left outside for days and the flavor is supposed to improve. The curries have high heat and acidity, so it makes sense it doesn&#x27;t go bad. reply ciceryadam 16 hours agoprevThere&#x27;s another kind of chilli oil, pressed from Hungarian spicy paprika&#x2F;capsicum seeds: https:&#x2F;&#x2F;rubinpaprika.com&#x2F;magolajok&#x2F;fuszerpaprika-magolaj-csi...It has a deeply nutty flavor and the spiciness is a bit sneaky, but it&#x27;s a great addition to finishing oils. reply dendrite9 10 hours agoparentInteresting, does it go bad quickly? Kurbiskernol is one of my favorite oils for special uses but it seems to spoil faster than some other oils. And I don&#x27;t always travel somewhere I can pick up a good bottle. I suppose thinking of oils as more like a juice might be a good approach. reply metachris 7 hours agorootparentKürbiskernöl (pumpkin seed oil) should be stored in the fridge, there it lasts pretty long. reply ciceryadam 6 hours agorootparentprevIt&#x27;s pretty shelf stable. I have one opened sitting on my shelf for three months and it still smells&#x2F;tastes good. reply s0rce 14 hours agoparentprevNeat!, a quick search doesn&#x27;t come up with anywhere to buy it in North America, also never seen this. reply s_dev 14 hours agoprevIf anyone is looking for a chilli oil you can buy that&#x27;s both delicous and available check this out: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lao_Gan_MaIt goes great with, eggs, rice and any yogurt dishes. reply gsuuon 9 hours agoparentYup. It basically becomes a household emergency when we run low. We were out of this and sriracha for a couple weeks. That was a sad, tasteless, time. (Also, it&#x27;s mentioned in the article) reply chaos_emergent 13 hours agoparentprevThis stuff is as addictive as sugar! I love it. Watch out for how much you’re putting on your food :) reply donatj 7 hours agoparentprevThe little soybeans are the best part of Lao Gan Ma. I would kill for a version that was half soybean. reply racked 13 hours agoparentprevCoincidentally I had the black bean version of this a few days ago. Delicious, but less spicy than expected. :-) reply elesiuta 12 hours agorootparentIt used to be spicier and even better before her son took over and changed it to a cheaper chili from another province a few years ago (which wasn&#x27;t great).I ended up trying something like 20 different brands looking for an alternative, and my favourites from least to most spicy were: Chuan Lao Hui, Lee Kum Kee, and Wa!, which I actually found even better.I tried Lao Gan Ma again now that they&#x27;re back to chilies from the original province, but it still doesn&#x27;t taste quite the same, I wonder if the sourcing changes caused them to lose their specific cultivar? reply algas 9 hours agorootparentWa! is my absolute favorite chili oil. I put it in and on everything. A simple bowl of rice, with some Wa! (and perhaps an egg), becomes an entire meal. reply mauvehaus 11 hours agoprevTangential, but related: my partner claims that putting oil and garlic in a cold pan and warming them together yields a different flavor than heating the oil, and then adding the garlic. I&#x27;m pretty convinced that she&#x27;s right.Has anyone experimented with this rigorously? I&#x27;d happily nerd out reading a similar article in that subject. reply fooker 10 hours agoparentYeah this is because you have more time when the oil and garlic is heated together (before the garlic burns).You want to do this when garlic is the main flavor you are aiming for and avoid doing it when garlic wants to complement the other herbs and spices. reply elderlybanana 17 hours agoprevMy favorite chili oils take the chilis just beyond a perfect maillard \"sear\" and venture very slightly into burnt territory. Many peppers have a natural bitterness when dried, and that slight bitterness added from the barely burned flavor brings out some extra, magical, umami depth. Adding some acidic elements makes for some incredible dishes.I have found that smoked chilis lose their smokiness with high heat though. I wonder if doing a lower heat extraction might retain those smoky flavors. Sounds like the multiple extraction&#x2F;blend the author talked about would be worth a shot to get both. reply mortureb 15 hours agoparentI agree. Indian cooking uses a lot of flavored oil as “tadkas”. Spices (cumin, coriander, garlic, mustard etc.) including whole red chillies are brought up to temp in a small metal bowl with ghee over a flame and then added to the final dish.I’ve made chili oil this way and it seems to draw out more flavor and there’s the slight burnt flavor that I love. reply hardwaregeek 12 hours agorootparentI&#x27;ve learned from cooking more Indian food recently that it&#x27;s okay to burn spices a little. In most cuisines you&#x27;re told to only bloom spices briefly but Indian food isn&#x27;t afraid to really cook spices to their limit. reply elhudy 11 hours agoprevI love nerding out on food as mich as the next person. On food and cooking is one of my favorite kitchen tools. Sichuan chili oil is a staple condiment in our house.That said, i find the following to be an oft-repeated bs phrase parroted across the internet. You can find it tracing back to reddit threads over ten years ago.> You can end up with what are called fines and boulders, fines being tiny, dust-like pieces of coffee that create bitterness in the cup, and boulders being large chunks that create sourness or emptiness in the cup.I dare any coffee drinker to blindly try their coffee with a crappy grinder, and compare to their $200 carbon fiber whatever. You wont detect the difference. I once looked into someone’s reddit account who was repeating this - and found that they were selling grinders elsewhere. Must be awesome margins. reply jonahhorowitz 10 hours agoparentA friend of mine started a company based entirely on the premise that grind quality matters. They did many blind taste tests and found that they could taste a 50μm difference in grind size when they controlled the other variables. Can you tell the difference between a $50 spice mill and a $200 burr grinder? Yes, absolutely. Can you tell the difference between that and a $1,500 expresso grinder, probably.His little startup company ended up getting bought by one of the large coffee companies. reply elhudy 30 minutes agorootparentTasting a difference in grind size does not equate to fine particles being “bitter” and larger particles being “flat”, nor does it result in a slightly uneven grind being “better” than a more consistent one… reply mbakke 9 hours agorootparentprevEven amateurs will notice the difference between a \"perfect\" pour over from a $50 grinder and a $200 grinder. The GP clearly does not own a V60 :)Above the $200 range really only matter for espresso. Manual brews don&#x27;t need that fine grind precision, just consistency (no fines, no rocks) at medium grind levels. reply elhudy 24 minutes agorootparentI’m sorry but what is an “amateur” coffee drinker? Is coffee drinking so complex that you can become an expert in it? Get over yourself, you’ve been drinking the koolaid, not the coffee.People who own v60s tend tie the promotion of v60s to their personal character so i know this might fall flat but; every real study ive seen, thats blind, has shown no no added benefit to slightly more consistent grounds. Ive noticed the same when testing between the two myself. Feel free to share otherwise if you have data to support that ;) reply ReptileMan 5 hours agorootparentprevGet two laboratory sifts for 10 bucks each and ignore the grinder quality altogether. reply ehnto 7 hours agorootparentprevI don&#x27;t know if there is a taste difference that&#x27;s meaningful, but the consistency is probably the real difference between grinders.With a spice mill, it&#x27;s a total crap shoot if a grind will pull properly, blast through or block up completely. It is a challenge to always get a good grind.A burr grinder? If it&#x27;s the same beans, it is set and forget. Always the same grind, same pull, easy.If there is any taste difference, I suspect it is down to consistency of the grind and not much else. reply nosefurhairdo 9 hours agoparentprevYour argument is \"I dare you; you won&#x27;t notice a difference.\" That&#x27;s not convincing. Those of us who&#x27;ve spent time in the industry can even detect differences between high end conical vs flat burr grinders, let alone the difference between a bladed spice grinder compared to a burr grinder.There are substantial differences to be found in many variables of coffee brewing. Just because you don&#x27;t have a palate for it doesn&#x27;t mean others don&#x27;t. reply elhudy 20 minutes agorootparentIt doesnt need to be convincing - I’m not the one making the claim that $200 coffee grinders make better coffee, so I’m not responsible for backing up that fact with data. reply esperent 5 hours agorootparentprevAre you aware of any blind tests done on this? I&#x27;m fully aware that I can taste the difference between my Specialita and a blade grinder.What I&#x27;m less convinced by is that more and more expensive grinders taste better to any noticeable and consistent degree. Personally I&#x27;d say that the taste changes day by day, maybe according to my mood, whether I brushed my teeth or ate something sweet in the last hour or two, the humidity, bean freshness, whether any defect beans got into a particular cup, and probably a ton of other factors.I&#x27;m willing to accept that my tastes are not developed enough. But I need real double blind tests to back that up, not just people saying I should trust them because they claim to be an expert. Are there any? reply mafuyu 9 hours agoparentprevSeriously? The grinder is the most important part, in my experience. When I first got into aeropress and pourovers, I was frustrated for years with bland and inconsistent brews. It was only after I upgraded to a decent burr grinder that everything fell into place. It’s just impossible to tweak anything else without first starting with a good grinder. I make adjustments of a few micron at a time with my grinder, and the difference is significant for both espresso and pour over. It’s much rarer that I make adjustments in water salts, brew temp, and brew method. reply c0pium 7 hours agorootparentQuality of beans and water in some order, followed by temperature and time, then grinder.Grab some 6 month old beans and brew them with metallic water at 85c, no grinder in the world will make that palatable.> adjustments of a few micron at a timeLol, no you don’t. Microns are much smaller than you think. reply klausa 4 hours agorootparent>Lol, no you don’t. Microns are much smaller than you think.You absolutely do.Many stepped grinders will have steps on the order of magnitude of 5 microns, EG-1 being probably the easiest one of the bunch to prove [1].But note that this is talking about the difference in the _burr distance_, not the ground particle size difference.In a world where we figured out how to make perfect burrs that produce uniform particle sizes these _might_ be the same, but even the most expensive and fanciest coffee grinders produce a relatively wide distribution of particle sizes.[1]: https:&#x2F;&#x2F;weberworkshops.com&#x2F;products&#x2F;eg-1 reply michaelt 2 hours agorootparentSpeaking as a someone who does DIY CNC, it&#x27;s easy to add an adjustment dial labelled in 5 micron steps. But those labels are basically decorative if other imprecision in the machine means adjustments aren&#x27;t reliably reflected in the output.Even if your coffee grinder is in a room temperature-controlled to within 1 degree, the heat output of the motor could easily cause more than 5 microns of thermal expansion.But obviously, if the coffee tastes good to you, then that&#x27;s a coffee making success regardless of how precisely you&#x27;re grinding things. reply c0pium 9 hours agoparentprevI’ve double-blinded a burr hand grinder, electric blade spice grinder, and a fellow grinder.It’s pretty easy to tell, although it depends a bit on how well you make the coffee. reply helsinkiandrew 7 hours agoparentprevThat&#x27;s nonsense unless your the kind of person who says a cup of instant is indistinguishable from a freshly ground brew (and post covid there probably are many more people who can&#x27;t).The size and consistency of the ground coffee makes a huge difference in extraction of coffee and therefor the taste, so a grinder that can make a consistent, repeatable grind will be noticeable (perhaps not as noticable as purchased ground vs freshly ground), but for some the difference between instant, and fresh ground with a $50, $500, or $3000 isn&#x27;t worth the price.$50 vs $500 Vs $500,000 Coffee Grinder: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=WkYqHWThIpA reply chongli 6 hours agoparentprevI make my morning coffee on a Lelit espresso machine. I use a Niche zero espresso grinder. I can easily tell the difference between grind sizes a mm apart on the dial, when it comes to taste in the cup (and even with milk). The espresso machine can tell the difference too, because even a minute difference in grind size can make a significant difference in the coffee puck’s ability to withstand the water pressure.You see, when you make espresso there’s a pretty narrow range of grind sizes that produce acceptable coffee. Too coarse, and the water gushes through the puck without building much pressure. Too fine, and the puck essentially turns into coffee cement and the machine isn’t able to squeeze out more than a drop or two of (very bitter) espresso.Within that narrow range you can find the needle on the water pressure gauge hitting numbers anywhere between 4 and 10 bar (I’ve set my machine’s overpressure valve to 10 bar so that excess water pressure is shunted away and circulated back into the brew circuit). I find the coffee tastes best when I can hit the sweet spot between 9 and 10 bar without the OPV kicking in. This is subject to uncontrollable factors such as differences in tamping but generally occurs at a very specific grind setting that I find after dialing in a new coffee.As for differences in flavour? It’s all about extraction. Underextraction (from failure to reach a high enough pressure) results in sour, thin coffee. Overextraction yields very dark, bitter coffee with a powdery mouth feel. Proper extraction avoids both of these problems and tastes sublime, nutty and chocolatey (with medium-dark roasts) or fruity and juicy (with lighter roasts).There may be a lot of gadget-headery going on in the espresso world but I can confidently say that correct grind size makes an enormous difference in the quality of coffee. Whether you need a $2000 grinder to achieve that is debatable, but you absolutely will not achieve it with a $40 blade grinder (which cannot control grind sizes at all). reply crunchytreat 10 hours agoparentprevI’ve spent ten years in the specialty coffee industry (not selling grinders). If you’re controlling other variables properly you should definitely be able to tell the difference. reply c0pium 9 hours agorootparentI think this is an important insight. Some people don’t have much experience in making repeatable coffee, and whether you make the coffee well is more important than how good the grinder is. reply andy_ppp 9 hours agorootparentBut once you have repeatable steps and that are good (water, quantities, temperature etc.) the grinder becomes the most important piece even in espresso. reply c0pium 6 hours agorootparentNot really, almost everything else matters more. Bean quality and water quality in particular are at the top of the list. Good beans bad grinder > bad beans good grinder. The grinder is the last place to look, even though it does matter. reply 2muchcoffeeman 6 hours agorootparentThe mistake you’re making is that not all variables are under your control.Getting better and better beans is very hard. You’re stuck with what is available. You’re stuck with what water is available (if you’re a reasonable person). You’re stuck with the local roasters.So of the variables you can control, the grinder is the most important piece of equipment you can buy. replybradgessler 11 hours agoparentprevIt matters if you’re brewing pour-over coffee. If you don’t use a burr grinder the fines settle towards the bottom while pouring and clogs it up. Takes forever! reply andy_ppp 9 hours agoparentprevThe difference in filter and espresso is quite a lot at the blender to burr grinder stage and a little bit from 200-1000$ getting better throughout. I’m sure you’ll be saying you can’t tell the difference in lots of areas because you can’t tell the difference. Which is fine btw! reply xapata 10 hours agoparentprevComparing my cheap spice chopper and my cheap burr grinder, I noticed a clear improvement. Maybe that&#x27;s because the spice chopper was so bad at chopping coffee beans. I doubt the fancy burr grinders make much difference, but then again I was skeptical of the cheap burr grinder until I tried it. reply SalmoShalazar 10 hours agoparentprevA crappy grinder will not work at all with espresso. reply anonu 12 hours agoprevI make a \"Mediterranean\" version of this with olive oil and I cut up some chilies and just let them soak. Completely different use case than an Asian chili oil IMO... but a little bit easier to do as you don&#x27;t need to heat the oil (and you probably wouldn&#x27;t want to heat olive oil that much anyway). reply cinntaile 7 hours agoparentYou use it pretty much instantly then? Or do you take any other steps to prevent the botumism bacteria from thriving? reply anonu 1 hour agorootparentYes, chilis should be dried out before you use them. For me its been shelf-stable outside for 2-3 months. Obviously, with botulism you would smell it right away if its off. reply reaperman 4 hours agoprevReally love the Mala Su Chili Crisp here: https:&#x2F;&#x2F;yunhai.shop&#x2F;collections&#x2F;su-chili-crisp It&#x27;s completely replaced my use of Lao Gan Ma Spicy Chili Crisp I enjoy the Lao Gan Ma, but I wouldn&#x27;t say I \"love\" it like I would for the Su Chili Crisp Mala. reply nimbius 17 hours agoprevI use Korean red pepper flake, the same type I use in my kimchi, and it had provided fabulous results. I generally build on flavours like spring onions and lots of garlic first, pulling them out before leaving black cardamom Sichuan green chilis (for a more woody note) star anise cinnamon and coriander seed for a brighter aroma. I also mix black vinegar and salt in with the chilis first before decanting the seasoned oil atop them. I make a litre at a time but can&#x27;t ever keep it in the house. The whole effort is 1-2 hours but its mostly just watching the garlic and onion and making sure to pull it out before it burns. The decanting doesn&#x27;t generate smoke or anything troublesome but beware, the smell of chili oil will permeate the kitchen for about three days. reply tomcam 8 hours agoprevSichuan chili oil and cuisine blow me away. Discovered it in my early 50s and within about 1 day my wife decided I’m a reincarnated Sichuan farmer lol reply yawnxyz 14 hours agoprevHave y&#x27;all tried steeping in anchovies for salty &#x2F; umami flavor? It definitely moves away from Asian chili oils and into Mediterranean, but I find anchovies add quite a hard-to-describe depth to the chili oil. reply hardwaregeek 12 hours agoparentInteresting! I&#x27;ve mostly added umami from mushroom powder and MSG. Anchovies or fish sauce could be an interesting addition, although that does make it more of a sauce than an oil. reply AareyBaba 9 hours agorootparentThai \"prik nam pla\" is chili in fish sauce. https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=isLl3m_JUx4 reply jmspring 17 hours agoprevWhen visiting various countries in Europe (Italy, Spain, etc) I&#x27;ve run across the equivalent of farmers markets where they have varying types of infused oils. The oils often have the seeds&#x2F;herbs&#x2F;etc. in the bottle. I wonder what method these vendors use.I know people who have simply put herbs&#x2F;peppers&#x2F;etc directly into oil and go with that method. I&#x27;ve not heard of them getting botulism, but maybe it was luck.When looking into this awhile back, I ran across this article - https:&#x2F;&#x2F;extension.psu.edu&#x2F;how-to-safely-make-infused-oils - I have yet to try the recommendations here in and wonder about the impact on flavor. reply ac29 15 hours agoparent> I know people who have simply put herbs&#x2F;peppers&#x2F;etc directly into oil and go with that method. I&#x27;ve not heard of them getting botulism, but maybe it was luck.According to the CDC, food-borne botulism is pretty rare, just 21 cases in the US in 2019: https:&#x2F;&#x2F;www.cdc.gov&#x2F;botulism&#x2F;surv&#x2F;2019&#x2F;index.htmlThat being said, it can be fatal so certainly it wouldn&#x27;t be advisable to take the risk. reply OJFord 12 hours agorootparentMany more were killed crossing roads in 2019, so is it not advisable to take that risk either? reply nkozyra 10 hours agorootparentI would imagine more people crossed roads than made infused oils in anaerobic environments, so more data needed. reply justusthane 17 hours agoparentprevI’ve only heard of garlic in oil being a botulism risk. Are other veggies dangerous too? reply jmspring 17 hours agorootparentThe article I linked to mentioned that fresh herbs could be a source as well. The article from OP mentions peppers can as well. So, I guess one has to be careful. reply fiddlerwoaroof 17 hours agorootparentI wouldn’t do this myself, but theoretically heating the oil relatively high for a while before consuming would destroy any toxin buildup. The other route would be adding enough acid to prevent botulism from growing, but I think that can be a bit tricky. reply plaguuuuuu 15 hours agorootparentbotulism is pretty resistant to heat.ph, salt.im sure a high purity ethanol also reply brnaftr361 12 hours agorootparentBotulinum toxins are large, easily denatured proteins... by heating to 80°C (176°F) for 20 minutes or > 85°C (185°F) for at least 5 minutes. Their heat resistance varies with composition of the food or other medium, and the concentration of the toxin. Reports suggest that HTST pasteurization (72°C&#x2F;162°F for 15 seconds) is likely to inactivate most or all of the toxin in contaminated milk, while conventional pasteurization at 63°C&#x2F;145°F for 30 minutes seems to be less effective. Chlorine and other agents can destroy botulinum toxins in water. The vegetative cells of C. botulinum are susceptible to many disinfectants, including 1% sodium hypochlorite and 70% ethanol, but clostridial spores are very resistant to inactivation. They can be destroyed in the autoclave with moist heat (120°C&#x2F; 250°F for at least 15 minutes) or dry heat (160°C for 2 hours) or by irradiation. The spores of group I strains are inactivated by heating at 121°C (250°F) for 3 minutes during commercial canning. Spores of group II strains are less heat-resistant, and they are often damaged by 90°C (194°F) for 10 minutes, 85°C for 52 minutes, or 80°C for 270 minutes; however, these treatments may not be sufficient in some foods.https:&#x2F;&#x2F;www.cfsph.iastate.edu&#x2F;Factsheets&#x2F;pdfs&#x2F;botulism.pdf reply fiddlerwoaroof 14 hours agorootparentprevThe spores are, but the toxin is the problem and breaks down with heat so, in theory, if you boil the food for a bit before eating it, it would be safe. reply mortureb 13 hours agorootparentI’m pretty sure it’s the opposite. You can kill the spores with heat but it leaves the toxins intact. reply OJFord 12 hours agorootparentNo, that&#x27;s not correct. Hence it being a (overblown US citizen internet commenter) concern with preserved food. Can something like tomatoes, not naturally that acidic in a preparation that doesn&#x27;t lower the pH much, and even though you&#x27;ve heated it and killed things in the process and it&#x27;s now in a vacuum, it can potentially still develop. replypeteforde 10 hours agoprevAre rapeseed oil and canola oil the same thing?Well, yes and no.https:&#x2F;&#x2F;www.tastingtable.com&#x2F;1066770&#x2F;are-canola-and-rapeseed... reply gsuuon 9 hours agoprevI feel the experiments you mention would make a really entertaining youtube series. reply fellowniusmonk 19 hours agoprevThey talk about about how hot infusing causes the chili&#x27;s to suspend in oil, I wonder if you could incorporate using a popcorn cannon somewhere in the infusion process. reply matthewmorgan 17 hours agoprevSince I caught covid my sense of smell&#x2F;&#x2F;taste is reduced so much that I put hot sauce on everything. reply levlaz 12 hours agoprevIt makes me so happy that this is on the front page of HN. :D reply civilitty 19 hours agoprev> I’ve also thought about doing a really low and slow extraction, say by using a sous vide machine. You do run into food safety issues, since the chilies can contain botulism.Sous vide was my first thought, since that avoids the evaporation and oxidation that decreases the intensity of the low and slow method. However I don&#x27;t think botulism is any more of a concern here than any other sous vide dish and high heat doesn&#x27;t kill botulism spores so it doesn&#x27;t really matter how you cook the chilis.Just keep it above 140F while infusing and keep it out of the <130F danger zone past a few hours. Quickly cool it below 45F using an ice bath after infusing if you&#x27;re paranoid or cooking for someone that might be immunocompromised. reply steve_adams_86 15 hours agoparentI did sous vide chilli oil by placing the oil and chillies in glass canning jars in the water bath. I let them rip for about 90 minutes at ~88°C&#x2F;190°F and the results were pretty good. I like the roasty&#x2F;mildly bitter results from flash heating, but this was quite smooth and pleasant.I think some purists would think it tastes weird. It&#x27;s a very \"clean\" chilli flavour. I think it would have been a lot better with more aromatics, and when I&#x27;m done with this oil I&#x27;ll likely give that a shot. reply hardwaregeek 12 hours agorootparentHave you tried blending it with a hot flash oil? My suspicion is that the two would be better than the sum of their parts. reply hardwaregeek 17 hours agoparentprevYeah I might give it a shot (although I gave away my sous vide machine). But low and slow isn&#x27;t always better for extraction. Coffee for instance extracts more with a fast infusion versus cold brew. reply civilitty 16 hours agorootparentCoffee extraction usually happens in a closed vessel where the volatiles aren’t as exposed to air and can’t as easily evaporate. The advantage of sous vide isn’t just the cooking temperature but the fact that the ingredients are completely sealed. It has a significant impact on many ingredients like chicken or carrots, which can stew in their own juices instead of reducing. reply hardwaregeek 16 hours agorootparentThat’s not necessarily true. A clever dripper does open air infusion and works great. Ditto with pour overs. But perhaps with oil more volatile aroma compounds are released? Either way the only solution is to try it! reply pipeline_peak 17 hours agoprev [11 more] [flagged] sib 13 hours agoparentMy sister-in-law (Chinese, although not super-old) brings us a new jar of homemade chili oil whenever we finish the last one, and is constantly experimenting with new peppers, oils, spices, and preparations. If we ask nicely, she also brings bags full of homemade dumplings as tasting medium ;) reply Aunche 15 hours agoparentprevOld Asian ladies experiment too. Chili oil is very subjective so everyone has their own way to make it. They obviously treat it a lot more casually, but it&#x27;s also common knowledge to them. reply curl-up 15 hours agoparentprevNext time, instead of traveling, just ask someone who&#x27;s already been there for a summary. reply mbg721 15 hours agorootparentIndeed, travel writing is an entire genre. But I also get the gp&#x27;s point, that especially with cooking it&#x27;s easy to overcomplicate things while ignoring how the inventors actually worked. reply curl-up 15 hours agorootparentMy point is that it is silly to complain about people being nerdy about a specific topic, just because there are others with more expertise. Especially in cooking, the concept of \"inventors\" and \"authorities\" for such basic things as chilli oil, is a strange one.For me personally, getting deep into a specific food item like this is bascally like traveling. I will first do my own experimentation, not because I think I will discover something new, but because I will more deeply understand why things are done the way they are when I (later) read \"proper\" sources and consult with others. At that initial stage, I am bound to overcomplicate things, but it&#x27;s the good kind of overcomplication.It is like traveling to a new place, and walking around randomly before opening a map. Of course I \"overcomplicated\" my path from A to B, but I&#x27;ve seen so many interesting (but not necessarily important) things on my way. reply bowsamic 16 hours agoparentprev [–] Totally agree. It’s sad and they’ve lost the pulse of life and art reply steve_adams_86 15 hours agorootparentIt&#x27;s just another way of experiencing life. For many of us, the joy of exploring things this way isn&#x27;t about removing the life or art from it; it&#x27;s another way of witnessing and experiencing the life and art of it.For example, I absolutely love aquascaping. I have a very scientific understanding and appreciation of it all, but it doesn&#x27;t diminish how incredible and beautiful it all is for me. Some people look at it from a very aesthetic point of view, and to me it&#x27;s often like some glorious chemistry and biology event occurring in front of me. I marvel at the fact that the nitrogen cycle is happening in this little slice of biotope. I love to test the water parameters and log what&#x27;s been going in and out to better understand how it&#x27;s working. I make nerdy little movies of the plants and animals. But you&#x27;d be hard-pressed to find someone who loves it significantly more, or finds it more beautiful in more meaningful ways. I just process it differently.With the chili oil, understanding and experimenting with it can be part of how a more cerebral person engages with their appreciation, curiosity, or passion about the oil. Nerding out might just mean the person really loves it, and that&#x27;s how they go about it. It&#x27;s all good. Maybe they&#x27;ll figure out a bad ass recipe that&#x27;ll make the rounds eventually, and everyone can enjoy it. reply notamy 16 hours agorootparentprev [–] Some people derive joy from doing it this way. You or I may not agree with it, but that doesn’t mean it’s a wrong way to enjoy something. reply bowsamic 8 hours agorootparent [–] I’m saying it is wrong replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares their insight on the preparation of chili oil, exploring different extraction methods like hot flash and slow infusion.",
      "The correlation between different temperatures and types of oil and the resultant flavor is examined.",
      "The post suggests experimenting with various chilies, spices, and seasonings to discover distinct nuances of chili oil."
    ],
    "commentSummary": [
      "The main article outlines a suggested method for making chili oil, as well as alternative techniques such as sous vide.",
      "In the comments, participants share personal experiences and perspectives on making chili oil, with discussions around safety measures and flavor profiles.",
      "Conversations deviate to other topics like tea-making, food safety, and the crucial role of grind consistency and quality grinders in pour-over coffee brewing."
    ],
    "points": 235,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1694966406
  },
  {
    "id": 37549745,
    "title": "AWS IPv4 Estate Now Worth $4.5B",
    "originLink": "https://toonk.io/aws-ipv4-estate-now-worth-4-5-billion/index.html",
    "originBody": "Home Contact About AWS IPv4 Estate Now Worth $4.5 Billion AWS grew its IPv4 estate with an additional 27 million IP addresses to now owning 128 Million IPv4 addresses. At a value of $35 per IPv4 address, the total value of AWS’ IPv4 estate is ~4.5 Billion dollars. An increase of $2 billion Andree Toonk 17 Sep 2023 • 5 min read Share: Three years ago, I wrote a blog titled “AWS and their Billions in IPv4 addresses “in which I estimated AWS owned about $2.5 billion worth of IPv4 addresses. AWS has continued to grow incredibly, and so has its IPv4 usage. In fact, it’s grown so much that it will soon start to charge customers for IPv4 addresses! Enough reason to check in again, three years later, to see what AWS’ IPv4 estate looks like today. A quick 2020 recap Let’s first quickly summarize what we learned when looking at AWS’s IPv4 usage in 2020. First, in 2020, we observed that the total number of IPv4 addresses we could attribute to AWS was just over 100 Million (100,750,168). That’s the equivalent of just over six /8 blocks. Second, for fun, we tried to put a number on it; back then, I used $25 per IP, bringing the estimated value of their IPv4 estate to Just over $2.5 billion. Third, AWS publishes their actively used IPv4 addresses in a JSON file. The JSON file contained references to roughly 53 Million IPv4 addresses. That meant they still had ~47 Million IPv4 addresses, or 47%, available for future allocations. That’s pretty healthy! The 2023 numbers Okay, let’s look at the current data. Now, three years later, what does the IPv4 estate for AWS look like? I used the same scripts and methods as three years ago and found the following. First, we observe that AWS currently owns 127,972,688 IPv4 addresses. Ie. almost 128 million IPv4 addresses. That’s an increase of 27 million IPv4 addresses. In other words, AWS added the equivalent of 1.6 /8’s or 415 /16’s in three years! Second, what’s it worth? This is always tricky and just for fun. Let’s first assume the same $25 per IPv4 address we used in 2020. 127,972,688 ipv4 addresses x $25 per IP = $3,199,317,200. So, with the increase of IPv4 addresses, the value went up to ~$3.2 Billion. That’s a $700 million increase since 2020. However, if we consider the increase in IPv4 prices over the last few years, this number will be higher. Below is the total value of 127M IPv4 addresses at different market prices. Total number of IPv4 addresses: 127,972,688 value at $20 per IP: $2,559,453,760 value at $25 per IP: $3,199,317,200 value at $30 per IP: $3,839,180,640 value at $35 per IP: $4,479,044,080 value at $40 per IP: $5,118,907,520 value at $50 per IP: $6,398,634,400 IPv4 prices over time — Source: ipv4.global Based on this data from ipv4.global, the average price for an IPv4 address is currently ~$35 dollars. With that estimate, we can determine the value of AWS’s IPv4 estate today to about 4.5 Billion dollars. An increase of 2 Billion compared to three years ago! Thirdly, let’s compare the difference between the IPv4 data we found and what’s published in the JSON file AWS makes available. In the JSON today, we count about 73 million IPv4 addresses (72,817,397); three years ago, that was 53 Million. So, an increase of 20 million in IPv4 addresses allocated to AWS services. Finally, when we compare the ratio between what Amazon owns and what is allocated to AWS according to the JSON data, we observe that about 57% (72817397 / 127972688) of the IPv4 addresses have been (publicly) allocated to AWS service. They may still have 43% available for future use. That’s almost the same as three years ago when it was 47%. (Note: this is an outsider’s perspective; we should likely assume not everything is used for AWS). Where did the growth come from A quick comparison between the results from three years ago and now shows the following significant new additions to AWS’ IPv4 estate, Two new /11 allocations: 13.32.0.0/11 and 13.192.0.0/11. This whole 13/8 block was formerly owned by Xerox. (Note: it appears AWS owned 13.32.0.0/12 already in 2020). Two new /12 allocations: 13.224.0.0/12 (see above as well). It appears they continued purchasing from that 13/8 block. I’m also seeing more consolidation in the 16.0.0.0/8 block. AWS used to have quite a few /16 allocations from that block, which are now consolidated into three /12 allocations: 16.16.0.0/12 16.48.0.0/12, and 16.112.0.0/12 Finally, the 63.176.0.0/12 allocation is new. AWS is starting to charge for IPv4 addresses In August of this year, AWS announced they will start charging their customers for IPv4 addresses as of 2024. Effective February 1, 2024 there will be a charge of $0.005 per IP per hour for all public IPv4 addresses, whether attached to a service or not (there is already a charge for public IPv4 addresses you allocate in your account but don’t attach to an EC2 instance). That’s a total of $43.80 per year per IPv4 address; that’s a pretty hefty number! The reason for this is outlined in the same AWS blog: As you may know, IPv4 addresses are an increasingly scarce resource and the cost to acquire a single public IPv4 address has risen more than 300% over the past 5 years. This change reflects our own costs and is also intended to encourage you to be a bit more frugal with your use of public IPv4 addresses The 300% cost increase to acquire an IPv4 address is interesting and is somewhat reflected in our valuation calculation above (though we used a more conservative number). So, how much money will AWS make from this new IPv4 charge? The significant variable here is how many IP addresses are used at any given time by AWS customers. Let’s explore a few scenarios, starting with a very conservative estimate, say 10% of what is published in their IPv4 JSON is in use for a year. That’s 7.3 Million IPv4 addresses x $43.80, almost $320 Million a year. At 25% usage, that’s nearly $800 Million a year. And at 31% usage, that’s a billion dollars! Notice that I’m using a fairly conservative number here, so it’s not unlikely for AWS to make between $500 Million to a Billion dollars a year with this new charge! The data You can find the data I used for this analysis on the link below. There, you’ll also find all the IPv4 prefixes and a brief summary. https://gist.github.com/atoonk/d8bded9d1137b26b3c615ab614222afd Similar data from 2020 can be found here. PS. Let me know if someone knows the LACNIC or AFRINIC AWS resources, as those are not included in this data set. Wrap up In this article, we saw how, over the last three years, AWS grew its IPv4 estate with an additional 27 million IP addresses to now owning 128 Million IPv4 addresses. At a value of $35 per IPv4 address, the total value of AWS’ IPv4 estate is ~4.5 Billion dollars. An increase of $2 billion compared to what we looked at three years ago! Regarding IPv4 capacity planning, it seems like the unallocated IPv4 address pool (defined as not being in the AWS JSON) has remained stable, and quite a bit of IPv4 addresses are available for future use. All this buying of IPv4 addresses is expensive, and in response to the increase in IPv4 prices, AWS will soon start to charge its customers for IPv4 usage. Based on my estimates, It’s not unlikely that AWS will generate between $500 million and $1 billion in additional revenue with this new charge. Long live IPv4! Cheers Andree ipv4awsnetworking Previous article Diving into AI: An Exploration of Embeddings and Vector Databases Join me on my journey into AI and learning about embeddings, vectors and vector databases Related Articles IPv4 for sale - WIDE and APNIC selling 43.0.0.0/8 13 Oct 2021 • 5 min read The Risks and Dangers of Amplified Routing Loops. 13 Apr 2021 • 7 min read Introducing SSH zero trust, Identity aware TCP sockets 16 Feb 2021 • 9 min read AWS and their Billions in IPv4 addresses 20 Oct 2020 • 4 min read 100G networking in AWS, a network performance deep dive 15 Oct 2020 • 9 min read Building an XDP (eXpress Data Path) based BGP peering router 19 Apr 2020 • 9 min read Share this article: Share Tweet Toonk.io Thoughts, stories and wild ideas from the world of Infrastructure Engineering. Tags Networking Terraform Aws Mysocket Anycast Ddos Ipv4 Ai Author © 2023 Andree's Musings.",
    "commentLink": "https://news.ycombinator.com/item?id=37549745",
    "commentBody": "AWS IPv4 Estate Now Worth $4.5BHacker NewspastloginAWS IPv4 Estate Now Worth $4.5B (toonk.io) 213 points by atyvr 15 hours ago| hidepastfavorite335 comments bradley13 8 hours agoCan we go back in time and hit the designers of IPv6 upside the head? The decision not to make IPv6 backwards compatible, the belief that a beautiful new standard could magically replace something already so widespread...\"Naive\" is an inadequate word. We are still futzing with the transition 3 decades later, with no end in sight. Grrr, grumble... reply jiggawatts 6 hours agoparentThese arguments always boil down to these two:\"Please just try to fit more than 4 billion numbers into 4 bytes\" -- this is mathematically impossible.\"Just extend the address size\" -- this is an entirely new protocol by the definition of IPv4, which uses fixed-size addresses.The reason for the slow IPv6 adoption is that there was no financial or business pressure. While IPv4 is ubiquitous, nobody individually feels a need to migrate to IPv6.E.g.: How many customers would you gain by supporting IPv6? Generally zero. That doesn&#x27;t sell well internally when the network team is asking for a budget.The IPv6 transition will be like a bankruptcy: very slowly, slowly, then all of a sudden.The sudden bit will happen when IPv4 address will cost $1K to $10K annually. At that point, customers will be reaching those IPv4 endpoints via three layers of proxies or NAT gateways, and IPv6 will be noticeably faster, more reliable, and free. reply cornholio 3 hours agorootparentThere was a third option: make the existing IPv4 space a hierarchically routed island of the new IPv4.1 space, with backwards compatible packet format, then upgrade just the endpoints in the first phase.So every owner of a ipv4 would get, say, an entire 32 bit space that routes over existing IPv4 infrastructure. So, if the endpoints are upgraded, you have guaranteed end-to-end deliverability without silly hacks such as NAT or STUN.This doesn&#x27;t solve the \"backwards compatibility\" problem itself, because you still have two logical different IP networks running on top of each other, requiring separate name resolution, etc. But what it does solve is the \"incentive problem\": endpoints are incentivized to upgrade because it gives them an immediate benefit, end-to-end connectivity to other upgraded users with non-routable addresses sitting behind a dumb, non-upgraded IPv4 routers.For example, VoIP or P2P software would immediately benefit and it would drive adoption for an immediate use-case. In the later stage, when the entire infrastructure can understand the extended packet format, you would start to publish extended routes that don&#x27;t fall into the hierarchical range, similar to IPv6 today.IPv6 lacks any such incentive, because me upgrading and enabling it has zero benefits until all hops separating me from the internet also enable it and correctly configure it. On the contrary, by requiring a completely new, complex configuration with no \"default, just works\" mode, IPv6 introduces a disincentive, because by enabling it not only do I not gain anything, but I risk breaking my internet due to misconfigured upstream. So the conservative setting for IpV6 has, for the last 3 decades, remained \"off\". This only recently began to change. reply azernik 2 hours agorootparentThis is exactly how NAT64 works, and still doesn&#x27;t solve the problem of IPv4 clients trying to connect to servers with only IPv6 addresses.The backwards incompatibility is irreducible, inherent to the special place of Layer 3 in the networking stack. reply gabereiser 3 hours agorootparentprev>”So every owner of a ipv4 would get, say, an entire 32 bit space that routes over existing IPv4 infrastructure.”So… NAT. reply cornholio 3 hours agorootparentNo. There would be no NAT box holding IP-port mappings in its internal memory, with the related timeouts, flakiness, port clobbering etc. and no packet re-writing. All routing decisions would be static, based on information in the IP header: the legacy outside routers would just examine the legacy part of the IP address and packet, while the internal IPv4.1 would use the extended bits. So just like any packet routing and without translation.Critically, this solves the cold start and connectability problem of NAT: if you get a packet addressed to your outside IP, to a port that has no memorized mapping, to what internal IP do you send it to? Lacking a static or UPnP port assignment, it can only be dropped. The extended packet format would provide this information for every packet, the upgraded outside host would tell you what upgraded internal host it wants to talk to. reply thelastparadise 2 hours agorootparentIt sounds nice on paper but typically we don&#x27;t want unsolicited packets to reach internal hosts.Yes, NAT is not a firewall --yet we don&#x27;t see admins eager to put random lan hosts in the DMZ or enable UPnP. reply dtech 3 hours agorootparentprevNot really, since in this proposal you&#x27;d still have the end-to-end routability that NAT prevents reply welterde 3 hours agorootparentprevYou can tunnel IPv6 over IPv4 (which is how the very first deployments worked). And I think 6to4&#x2F;6rd worked pretty to close to what you suggest: Each IPv4 gets assigned a block of IPv6 space which gets tunneled over IPv4. reply rewmie 3 hours agorootparentprev> There was a third option: make the existing IPv4 space a hierarchically routed island of the new IPv4.1 space, with backwards compatible packet format, then upgrade just the endpoints in the first phase.Do you mean applying some kind of network address translation?> So every owner of a ipv4 would get, say, an entire 32 bit space that routes over existing IPv4 infrastructure. So, if the endpoints are upgraded, you have guaranteed end-to-end deliverability without silly hacks such as NAT or STUN.Ahem. reply ta1243 5 hours agorootparentprevThe problem isn&#x27;t ipv6 (even with the tons of extra features that ipv6 forces upon you).One major problem is dual stack. It doubles the workload for very limited benefit. You have all the downsides of making ipv4 work in the first place. You&#x27;ve then got all sorts of messes like NAT66 (ipv6 was supposed to get rid of NAT), a lack of clarity on which patch to use (NPTv6 and NAT66 are two different options for the same problem, a problem which was built into ipv6 in the first place), messy hacks like DNS64Instead had the approach been ipv6 only from the start, with no dual-stack, having the OS transparently deal with sockets to ipv4 devices by converting to the ipv6 mapped address (:ffff:xxxxxxx), and thus eliminating the need for dual stack from the start, things would have moved far far faster. You&#x27;d be able to communicate with ipv6 by using stateful NAT at the edge of your ipv6 network (as you do now at the edge of your RFC1918 network), you could expose services on your ipv6 only devices with natting (as you do now).You&#x27;d still have A and AAAA records, your client having an ipv6 stack could prefer AAAA instead of A, but if it needed to use an A record (or someone just tried to connect to 12.34.56.78) the stack would have gone \"ok I&#x27;m ipv6 only, I&#x27;ll connect to :ffff:12.34.56.78\" and rely on the network to make it happen.Throw in things like NPTv6 and 464XLAT from the start (rather than 16+ years in) -- the addons which were created to address the fundamental architectural flaws in ipv6 -- and you&#x27;d have had a far smoother transition. reply Sesse__ 4 hours agorootparent> Instead had the approach been ipv6 only from the start, with no dual-stack, having the OS transparently deal with sockets to ipv4 devices by converting to the ipv6 mapped address (:ffff:xxxxxxx), and thus eliminating the need for dual stack from the start, things would have moved far far faster.This is, indeed, how dual-stack works; you open a PF_INET6 socket and use sockaddr_in6 addresses for everything, including IPv4 (which get mapped to ::ffff:&#x2F;96 addresses). Been like that essentially forever. The “dual” in dual stack refers to the OS&#x27; stacks, not userspace. reply yrro 1 hour agorootparentDon&#x27;t forget you may need to opt in to get mapping to work. It&#x27;s not available by default on all platforms.If you&#x27;re unlucky you&#x27;ll also have to sacrifice a goat to appease the JVM gods. JVM behaviours vary hugely across implementation, version and underlying platform. Not to mention the short sighted decision made by many sysadmins to disable IPv6 completely... reply Sesse__ 29 minutes agorootparentSure, that&#x27;s one line and then you&#x27;re done. And only if you care about very old (WinXP) or deliberately obnoxious (OpenBSD) platforms. reply tsimionescu 4 hours agorootparentprevAny solution to the 4->6 transition that assumes that all devices of some class (be it clients, servers, or middleboxes) moved to IPv6 at once is deluded and would not work.There was no way to make the transition to IPv6 without dual stack. The problem was much more that the precise dual-stack approach was not well thought out, when it should have been a fundamental part of the IPv6 RFC itself.Any ISP who wishes to move to IPv6 still to this day has to consider how it will handle clients that don&#x27;t speak IPv6, servers that don&#x27;t speak IPv6, routers they own that don&#x27;t speak IPv6, and peers who don&#x27;t speak IPv6. There is no way to make all of this work without having devices that translate between the two (losing most of the benefits of IPv6 when going through this translation, of course).When you&#x27;ve spent 10 million dollars or more on a router that doesn&#x27;t speak IPv6, you don&#x27;t change it one year later just because a new protocol has come up. That thing is there to stay for 5-10 years, and you just work around it as best you can. reply ta1243 4 hours agorootparentipv6 has been around for nearly 30 years.> Any solution to the 4->6 transition that assumes that all devices of some class (be it clients, servers, or middleboxes) moved to IPv6 at once is deluded and would not work.464xlat allows communication from ipv6 only clients to legacy ipv4 ones without the need for a separate stack on your end devicenat46 allows communication from a legacy v4 device to a modern v6 device without the need for a separate stack on your end deviceHad ipv6 transition been thought about better back in the 90s then you could have deployed your new subnets as ipv6 only back in 2005 and still communicate with all your older kit.> That thing is there to stay for 5-10 years, and you just work around it as best you can.IPv6 is 30 years old. I sweat assets like there&#x27;s no tomorrow but the oldest kit I&#x27;ve got active today is less than half that. Even for ipv4 only devices, a single legacy subnet would be reachable from my v6-only management devices via my ipv6 backbone via 464xlat reply simiones 1 hour agorootparent> 464xlat allows communication from ipv6 only clients to legacy ipv4 ones without the need for a separate stack on your end device> nat46 allows communication from a legacy v4 device to a modern v6 device without the need for a separate stack on your end deviceHow does that work if your ISP doesn&#x27;t support IPv6? Can an OS developer deliver IPv6-only OSs to any end user? How about v4-only VPNs? Ultimately the answer is that devices must support both IPv4 and v6 until the day only one remains. Keeping both active at the same time may be more optional, but there is plenty of software which assumes IPv4 at other layers than simple connectivity. So running IPv6-only is generally a bad idea even today. reply elsjaako 4 hours agorootparentprevYou say ipv6 was supposed to get rid of NAT. Can you explain why it doesn&#x27;t? You then say the problem was built into ipv6 from the start.From looking it up it looks like it&#x27;s mostly required when IP&#x27;s change (e.g. when you change ISP), which for me is more of an argument to use DNS if you want fixed addresses. reply stock_toaster 3 hours agorootparentOne reason could be the proliferation of prefix delegation, meaning an ISP now controls the numbering of your internal network, and it keeps constantly changing the net block, so you can never get stable local addressing… so you just keep running dual stack or try to use nat66 with a ULA network as a kludge or something. reply pantalaimon 3 hours agorootparentYou can have multiple IPv6 addresses on your interface, so you can have both the ULA address for internal use as well as the global address - no need for NAT reply Macha 2 hours agorootparentHowever, the priority order on your OS for address selection, even when it comes to stuff like choosing which DNS results to use is IPv6 global address > IPv4 > ULAs. So on a dual stack network, ULAs with not be used unless the only address is a ULA.And even if you run your internal services with only an AAAA record pointing to the ULA, the client&#x27;s source address will likely be the global address of the client device unless you tweak the tables on each client, which then means you&#x27;ll need to have your global address in all your firewall rules to access the internal services on ULAs, which then means you&#x27;re not saved from having your ISP-provided global address in your configuration, which is what you were trying to avoid by using ULAs. reply stock_toaster 3 hours agorootparentprevSource address selection for multiple non-loopback addresses was (maybe still is?) notoriously bad in many OS’s for years. reply ta1243 4 hours agorootparentprevThe existence of NAT66 and NPTv6 are proof that there is still a need for NAT in an ipv6 environment. Maybe not in your environment, but people wouldn&#x27;t make these solutions if there wasn&#x27;t a need. reply elsjaako 2 hours agorootparentThere&#x27;s obviously a need, I didn&#x27;t deny that. I&#x27;m not arguing for ipv6, I just want to learn.The uses that I found while searching weren&#x27;t very convincing, I was hoping you could give an example. reply thelastparadise 2 hours agorootparentprevOne such use case: multi wan routing.Would you rather have a bunch of routers sending out advertisements which every client needs to sort out, or have one consistent multi wan load balancing&#x2F;failover policy that is transparent to clients? reply robertlagrant 4 hours agorootparentprevThere&#x27;s certainly a requirement for them, but it would be good to know what the justification of that requirement was before we know if there&#x27;s a need. E.g. the justification could be \"our SOPs say all network traffic must go through NAT\", and if you dig deeper you might find that the SOP was written to save money on IPv4 addresses. That would not indicate a fundamental need. reply Retric 3 hours agorootparentNAT works well as an ultra simple firewall. All those ancient IoT devices don’t need to accept traffic from arbitrary addresses, but they may need to communicate with the outside world.Using a firewall is obviously an option, but why give an IP to something you don’t want accessible by the outside world? reply ta1243 4 hours agorootparentprev> There&#x27;s certainly a requirement for themThus proving that ipv6 failed in it&#x27;s mission to get rid of nat reply robertlagrant 4 hours agorootparentI don&#x27;t think this is a useful thing to say. If I keep trying to fill up my electric car with petrol, and get so annoyed that it keeps spilling on the floor that I pay someone to install a petrol cap and fuel tank, it would be equally silly to say that electric cars failed in their mission to get rid of petrol use. reply ClumsyPilot 3 hours agorootparentprev> The existence of NAT66 and NPTv6 are proof that there is still a need for NAT in an ipv6 environmentThe existence of fridges with twitter integration proves that there is a need to a Lettice to tweet.There are all kinds of things that exist, steam-powered motorbike among them. Not all of them exist for the right reasons. reply ClumsyPilot 3 hours agorootparentprev> The problem isn&#x27;t ipv6 (even with the tons of extra features that ipv6 forces upon you)The year is 2023, The chromium engine is full blown operating system, it has notifications, background task management, GPU acceleration for general compute, it&#x27;s larger than Windows XP, and can in fact run windows XP in the browser. Teams consumes 500 mb of ram to do the same job ICQ did in 2002 with 5 mb of ram. Cars have 4G, lightbulbs need updates and security patches.But Ipv6 features take a few extra bytes and are a problem. reply rewmie 3 hours agorootparent> But Ipv6 features take a few extra bytes and are a problem.Some people pay for each extra byte they have to send through a network, and design whole systems around the goal of minimizing the amount of data they ship around.No one pays for the extra free gigabyte that Chrome takes over. reply MereInterest 1 hour agorootparent> No one pays for the extra free gigabyte that Chrome takes over.Sure they do. It just doesn’t show up in Chrome’s metrics so Chrome doesn’t care about it.* Start-up time of other applications. If a program needs 1 GB of RAM, and Chrome is holding all but 512 MB, then the program must perform multiple allocations, waiting for Chrome to release its cache after each one.* Smaller cache in other programs. Consider a program that can run with 4 MB of RAM, but could use up to 1 GB of RAM to cache intermediate results and improve performance. Such a program would check the amount of RAM available and scale their own cache size accordingly.* Competing caches in multiple Chrome instances. Multiple independent Chrome instances, such as from Electron shells, each try to cache as much as possible until RAM is exhausted. reply azernik 2 hours agorootparentprevThis concern about an extra few bytes is substantial in approximately zero organizations&#x27; IPv6 adoption decisions. reply Macha 2 hours agorootparentIn fact some of the earliest adopters of IPv6 were Google, Microsoft, Netflix. Companies who when you&#x27;re considering the problem of (N * a few bytes) have a very large N so are the most likely to have material costs from it. Yet even to them, it&#x27;s a rounding error. replytheamk 45 minutes agorootparentprevthe reason for slow IP adoption is that they decided to break all backwards compatibility.You need new protocol, sure. But do you _have_ to switch from \"1 almost fixed address per interface\" to \"tons of addresses per interface and dynamically changing\"? Did you have to present it as a separate protocol to apps? Did you have to use : in representation, breaking most ad-hoc text processing code? etc..if they goal was \"herr is a new verion of IPv4 with same semantics\" then we&#x27;d just need to wait for new kernels and libraries to come out, and it would be all done years ago. reply dannyw 6 hours agorootparentprevFitting more than 4B numbers into 4 bytes is mathematically impossible, but building a backwards compatible and easier to integrate standard may not be.Take USB for example. The capabilities of USB 3.1, 3.0, 2.0 is impossible to achieve for USB 1.0. So is high-speed charging.However, the end-user experience is generally pleasant, nitpicks around some of USB-IF&#x27;s specific choices aside. reply jiggawatts 6 hours agorootparentThe USB protocols over the wire are generally not compatible between versions, especially at the lowest levels (signalling). That&#x27;s the definition of how more bandwidth can be squeezed into the same wires. The signalling layer changed between versions.The \"end-user experience\" IPv6 equivalent of the USB version transition is that a person browsing to \"www.google.com\" has no clue whatsoever that it actually went via IPv6 instead of IPv4.Just like with USB 1 to 4, IPv6 goes down the same cables and works the same at the application layer. Some changes occurred, but changes are mandatory for things to change.You&#x27;re asking for USB 4 to be magically \"the same\" as USB 1.0 while sending tens of gigabits over the wires -- not for the end users -- but for the lazy electrical engineers that can&#x27;t be bothered to update their designs! reply growse 6 hours agorootparentprevHow does a host that thinks there are only 4 billion addresses send a packet to a host with an address that falls outside of the 32 bit v4 space?This is a fundamental problem. Backwards compatibility (without introducing translation schemes and middleboxes) is literally impossible. reply rini17 6 hours agorootparentExtending address is fully possible, and if we drop requirement that the extended part be individually routable, even simple.But no, someone said we must redo whole stack and we need every piece of sand to have public routable address..so now we are stuck between rock (old fossilized IPv4) and hard place (completely incompatible IPv6). reply growse 5 hours agorootparentYou&#x27;re inventing a new addressing scheme, and proposing that we put a bunch of middleboxen in to mediate connecting the old world to hosts on this new addressing scheme.You&#x27;re re-inventing IPv6. reply rini17 4 hours agorootparentNo, I describe existing practice with NAT where you in fact have IP addresses extended by TCP&#x2F;UDP port numbers. You could instead move this \"port\" directly into IP header in backward-compatible way and fall back to stateful NAT only if the counterparty does not support it. reply growse 2 hours agorootparentA device that doesn&#x27;t understand your newly invented addressing scheme would need to rely on some other intermediate device in order to get traffic to an endpoint that does support your new scheme.You&#x27;re using different words, but you&#x27;ve got a separate addressing scheme, and dependence on proxies to enable everyone to talk to each other. This is exactly where we are with IPv6.> You could instead move this \"port\" directly into IP header in backward-compatible wayIf you&#x27;re changing the meaning of the headers, it is by definition not backwards compatible. replydanhor 5 hours agorootparentprevUSB 3 the protocol isn&#x27;t backwards compatible with USB 2, USB 3 ports just include both USB 2 and USB 3 pins (what one might call dual stack). You can easily connect two differnt devices one to the USB 2 pins and one to the USB 3 pins. If you only want to connect USB 3 devices, there is no need for USB 2 Pins, as done on the PS4.There is also no specified way to convert USB 3 to USB 2, but some have tried, with mixed results. reply bauruine 5 hours agorootparentprevWhat&#x27;s your problem with the IPv6 end-user experience? Hundreds of millions of end-users are using it without even knowing what an IP is. reply candiodari 6 hours agorootparentprevIPv6 is backwards compatible. In multiple ways:Option 1: \"6to4\" https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;6to4Option 2: \"nat64\" https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;NAT64 + DNS64Option 2b: \"nat46\" (which makes a few ipv6 hosts available over ipv4 if yo ulike)Option 3: \"Teredo\" (also known as \"6in4\" \"tunnel broker\" \"6over4\" \"tunneling\" ...) https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Teredo_tunnelingOption 4: 6rd https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;IPv6_rapid_deploymentOption 5: ffff&#x2F;96 (yes I get it, only works if host has both ipv4 and ipv6, on the plus side: no need for the network to support it. Mostly for applications)Option 6: DS-lite https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;IPv6_transition_mechanism#Dual...And then there&#x27;s the weird ones: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;IPv6_transition_mechanismThe issue is most of these require ISPs to deploy new hardware, or deploy new network services. The problem is that network hardware is single-purpose, because only single purpose hardware can sustain the speeds we demand of internet networks. This means a lot of hardware needs to be replaced in order to make the global IPv6 transition and, short of redesigning IPv4, which is 43 years old now, there&#x27;s no other way to make the transition. All these solutions require either work by your ISP, or work by you yourself on all your hosts. reply commandersaki 2 hours agorootparentRFC1726 Technical Criteria for Choosing IP The Next Generation (IPng)Section 5.5: CRITERION The protocol must have a straightforward transition plan from the current IPv4. reply greyface- 39 minutes agorootparentnext [–]Category: Informational Publication of this document does not imply acceptance by the IPng Area of any ideas expressed within. reply ta1243 5 hours agorootparentprevThe fact there are so many options shows the fundamental design problem. reply superlupo 4 hours agorootparentprevReading, not even trying to understand that list makes my brain explode. reply paulsutter 3 hours agorootparentprevIn practice IPv6 is for mass market ISPs and IPv4 is for servers. Wasn’t the intention, but that’s how it is to a first order approximationThere are billions of phones so sure, they should be on ipv6. ipv6 is a kind of super NAT that few people bother to learnSorry about that ipv6 committee reply bradley13 6 hours agorootparentprev\"Just extend the address size\" was certainly one of the options. Sure, it&#x27;s still a change, but the point is: After this change, both protocols could have worked side-by-side. Devices that only supported IPv4, no problem, they send 32-bits. Devices that supported IPv6-as-it-could-have been would simply have zero-padded those 32-bits to match the new protocol. Talking to old devices, the zero-padding gets dropped.That would have saved a lot of pain. reply josephg 5 hours agorootparentThen any network address beyond ipv4’s 32 bit range would have been completely inaccessible to any legacy devices. That would have essentially been the same situation that we have now - where ipv6 only services are inaccessible to anyone on an ipv4 network. So service operators need to keep their ipv4 addresses and networks don’t update.How would that be an improvement over the existing situation? reply lmm 6 hours agorootparentprevNo it wouldn&#x27;t. Think about how that would work for a minute. It would&#x27;ve created nondeterministic routing loops that would be a nightmare to debug. reply cameronh90 2 hours agorootparentprevWe could presumably have done something like: use the IPv4 packet format, treat the 32 bit src&#x2F;dst address in the header as the first 32 bits of the address and put the remaining 96 bits (+ checksums&#x2F;etc.) as the first few bytes of the payload. Then create TCPv6, UDPv6, IGMPv6 etc. protocol identifiers for the protocol field to distinguish traffic that&#x27;s encoding an IPv6 address in the first few bytes of the payload.Then, if you own an IPv4 address, you effectively own an IPv6 subset. Then we reserve a whole bunch of IPv4 addresses for IPv6-only allocations.I obviously haven&#x27;t thought it through in detail, but wouldn&#x27;t something like that effectively transparently work via IPv4 core infrastructure provided the networks at either end support IPv6 if they&#x27;re using it? We&#x27;d still need NAT for IPv6-only endpoints that need to talk to IPv4-only endpoints. It also wouldn&#x27;t be anywhere near as clean as IPv6 and would lack a few of the nice features, but... an awesome protocol I can&#x27;t actually use isn&#x27;t much use to me. reply welterde 39 minutes agorootparentYou more or less just reinvented a more complicated variant of 6to4&#x2F;6rd, which is one of the IPv6 transition technologies. reply commandersaki 3 hours agorootparentprevEveryone is always quick to complain that we&#x27;re going through N number of NAT gateways or N number of proxies, but this is virtually never a problem for most of the Internet. Even despite this rats maze of proxies and NAT gateways we&#x27;re still supporting virtually all the applications that consumers use and love such as VoIP, WebRTC, HTTP(S), DNS, Gaming, Streaming Video, Mobile Apps, etc.NAT seems to always get a bad rep because it inconveniences the very few that want to have an end to end experience, but there has to be some sacrifice to keep the Internet running for the billions of users.NAT and by extension CGNAT are the unsung heroes of the Internet. reply superluserdo 3 hours agorootparent>Even despite this rats maze of proxies and NAT gateways we&#x27;re still supporting virtually all the applications that consumers useThat&#x27;s a tautology: \"Despite the limitations of IPv4, we&#x27;re still supporting all the applications that can work within the limitations of IPv4\".Lots of potential P2P applications (that might solve a lot of problems with have with the current centralised model of the internet) either don&#x27;t make it past the drawing board because of NAT, or have to be encumbered with complex, expensive-to-develop, best-effort NAT-punching behaviour that burdens everyone involved (and can stop an application from being truly P2P by having to run things like STUN servers).>NAT seems to always get a bad rep because it inconveniences the very few that want to have an end to end experienceI think there would be many more that wanted this if it were trivially easy to do>but there has to be some sacrifice to keep the Internet running for the billions of users.What&#x27;s the sacrifice in using IPv6? reply commandersaki 2 hours agorootparent> I think there would be many more that wanted this if it were trivially easy to doI&#x27;ve seen figures from proponents of Future Internet Architectures such as Named Data Networking claim that consumption is about 80% of Internet traffic. The truth is not everyone needs a Internet addressable host, mobile phones for example don&#x27;t. And well, we&#x27;re living in this situation today with CGNAT and you don&#x27;t hear complaints from customers about not having Internet addressable IPs.> What&#x27;s the sacrifice in using IPv6?Support. Enabling IPv6 on broadband consumer networks, small medium businesses, etc. means that you have to support the various devices v6 stacks and applications and ensure they continue to work just as well as they did with IPv4. IPv6 can still cause damage and the ability to support and fix these issues throw out virtually all incumbent knowledge.If it were really just a \"flip of a switch\", everyone would&#x27;ve done it by now. reply bewo001 1 hour agorootparentprevMost of the applications that could communicate peer-to-peer use relay servers that make delay and scalability worse. Some combinations of NAT may sometimes work without a relay server, but figuring this out is complex and increases connection setup time. Every early SIP&#x2F;VoIP user had the &#x27;the connection only works in one direction&#x27; experience, usually caused by NAT.A CGNAT is a stateful component which makes it expensive to operate. Failover to a backup is hard, as is scalability with this kind of components. And then there are legal requirements. You have to know what user had which IP address at a given time. I&#x27;d rather invest in dual stack instead. reply cm2187 5 hours agorootparentprevAt this stage it shouldn&#x27;t require much money to integrate ipv6. Your network equipment needs to be really old to not support v6 natively.Though granted, there is support and support. I use hyperoptic in the UK as an ISP. I replaced the native router and I still can&#x27;t figure out a way to get an IPv6 address. reply rewmie 3 hours agorootparentprev> The IPv6 transition will be like a bankruptcy: very slowly, slowly, then all of a sudden.I don&#x27;t think that&#x27;s true.Some services on the internet are already made available through IPv6. Doesn&#x27;t that mean their migration to IPv6 is done?There are however some ISPs that seem to be dragging their feet. I recall I tried to deprecate IPv4 access to a personal project of mine and it was no longer reachable when I tried to access it from my home. Lookups from other points of the world could resolve the IP but not my little home network. I felt forced to continue paying the 2€ I paid for a IPv4 address just because of that.Edit: to make it abundantly clear, I&#x27;m looking at you, Vodafone. You suck. reply AtlasBarfed 43 minutes agorootparentprevReally, IPV6 has failed because of human reasons. I know because almost everyone demonstrably hates it, as evidenced by their behavior.The big issue is that the router vendors hated it, the OS vendors hated it, the programming language people hated it, and the software writers hated it. How do I know? NOBODY WANTS TO ADOPT IT except by force, even now.Worryingly, pro-IPV6 people are consistently arrogant and dismissive. Essentially their argument always boiled down to \"ha, you&#x27;ll be forced to use it eventually and then I&#x27;ll be RIGHT!!!\" which is why IPV6 people hate NATs with a vehement irrational passion, because it floated IPV4 for, what, two decades at least?I&#x27;m guessing it is because IPV6 was a tossed-over-the-wall protocol that didn&#x27;t get reference implementations from the biggest router vendors first. Here&#x27;s a very very very very very very troubling link:https:&#x2F;&#x2F;www.cisco.com&#x2F;c&#x2F;dam&#x2F;en_us&#x2F;about&#x2F;ciscoitatwork&#x2F;networ...That is Cisco bragging about it&#x27;s IPV6 website on a pdf from 2011! 2011! Fifteen years after the birth of the protocol. If Cisco did not have an IPv6 site up until FIFTEEN YEARS after protocol definition ... oh god.Comcast routers weren&#x27;t IPV6 functional back in 2015, at least they weren&#x27;t on my cable modem. If an ISP that makes bank on renting and turning over its consumer routing hardware can&#x27;t roadmap ipv6 adoption within 22 years... ugh.And my biggest complaint about ipv6 is that they didn&#x27;t increase the number of ports. Really. We have to keep shoehorning apps into 64k ports rather than a sensible 4 billion, but maybe there&#x27;s some OS mapping concern with that, doesn&#x27;t matter, the ship sailed.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Internet_Protocol_version_4Somewhere in IPV4 is an options header (up to 40 bytes). Why that didn&#x27;t provide the necessary space for some degree of backwards compatibility somehow is beyond me.What should have happened is that the big router vendors got together and agreed on a standard protocol. Then the major OS vendors and language standards bodies got together and made reference implementations for basic networking.Once that was working &#x2F; adopted by next gen hardware and software releases, then things might have gotten rolling.I mean, how much work was that relative to the mind boggling amount of work done to implement NAT and firewall traversal&#x2F;busting code in, say, Skype? Ever seen those whitepapers? Wow are they doozies. Holy crap are people willing to write code.This is all screaming at the darkness. reply welterde 8 hours agoparentprevYou have to go back further in time and hit the designers of IPv4 on the head for not making it forward compatible.IPv6 is as backward compatible as is possible within this constraint. You can embed IPv4 space within IPv6, there is NAT64, tunneling IPv6 over IPv4 and many other transition technologies. It&#x27;s not possible to design a protocol that is any more backwards-compatible. reply hn_throwaway_99 8 hours agorootparent> IPv6 is as backward compatible as is possible within this constraint.Yikes, couldn&#x27;t disagree with that more. There are a ton of things that ipv6 designers could have done to make the transition much easier. This is a (now quite old) blog post that is my \"go to\" that explains a lot of the problems with ipv6: https:&#x2F;&#x2F;cr.yp.to&#x2F;djbdns&#x2F;ipv6mess.htmlFWIW I couldn&#x27;t find the link to that post until finding it on one of the comments here, https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33894933 . That whole thread has lots of good commentary.I still don&#x27;t understand how people can defend ipv6. I remember the \"we better get ready to switch to ipv6\" noise a quarter century ago when I started my career. And yet we&#x27;re still talking about how v4 addresses are worth billions. Ipv6 has been an unmitigated disaster. The original architects should have \"the perfect is the enemy of the good\" forcibly tattooed on their foreheads. reply xorcist 3 hours agorootparentIt&#x27;s not like people didn&#x27;t think of that when IPv6 was discussed. It was something like a decade from when the first IPng proposals came to when the proposals which looked like today&#x27;s IPv6 came.Bernstein was certainly part of that discussion, at the later stages, and the document you link to reflected that. It was just one of many counter proposals that influenced what became IPv6.Some people seem to suggest that Internet standards are written in some ivory tower and dropped down on the network engineers to implement. In that light, such criticism of IPv6 would be valid and important. But the IETF does not work like that. You can take part, and I can take part, and any reasonable criticism is discussed in the open. In general, practical proposals and code is taken more seriously than loose ideas.There is no central command which decides what you or any other network operator should implement. People all over the world implements what they think is good for their network, in order to interoperate with other networks. If anything, Internet standard can be criticized for being slow to fruition because of this open process. That&#x27;s the price we pay.It&#x27;s not very useful to come 20 years later and re-hash the exact same discussion all over again. All counter proposals turned out to be impossible to deploy, and the consensus and running code we ended up with is what we call IPv6. A dual stack approach was the only solution practical enough to get general deployment. There are certainly problems with any protocol, and let&#x27;s suggest improvements and new protocols. Just make them relevant today if they should have any chance of deployment. reply butlerm 1 hour agorootparentIt is not true that all counterproposals are impossible to deploy, they just have a number of downsides that were either politically or technically impalatable in 1996. For example, the network could have been made administratively compatible (same config files, DNS entries, routing prefixes) for a decade while a variety of internal changes and updates took place as part of the normal upgrade process.I do not mean no changes at a binary level, but at an administrative level. An upgraded \"A\" record for example so that DNS admins could go about their day somewhere between completely and largely unaware that the protocol was undergoing transparent upgrades behind the scenes while preserving administrative compatibility with all existing configuration files, source data, and user interfaces. In such a scheme it would be quite important that there only be one \"A\" record from an administrative point of view, not different ones like A and AAAA. Admins need to be insulated from the binary protocol changes going on behind the scenes.That means that the new address format would need to be compatible with the old one, and of course a routable embedding of the current address space be provided in the new address space. That means existing routing prefixes would have to be preserved indefinitely, complete with the routing table explosion that was a major challenge a couple of decades ago.All routers would need to be upgraded over the transition period to handle a new frame type that supported current addresses, extended addresses, and a single routing table with larger extended address sizes. It is quite important that there be a single routing table, not two of them. Same configuration file, same everything from an administrative point of view for a decade, while older hardware was gradually replaced with new hardware that had extended capabilities that would be dormant on the public net.Comparable and in some cases less transparent updates would need to be made to programming APIs, and to the Berkeley socket API in particular. Not to require programmers to do everything for two different layer 3 protocols, but rather to allow them to do it once and have it work with current and extended addresses, transparently from an administrative point of view, not doubled configuration for anything.There are many other things that would need comparable binary behind the scenes upgrades that would be administratively compatible and not affect current network configuration files, and in particular not require anything to be duplicated from an administrative point of view. No one does that and no one wants to for protocol extensions that are not actually usable yet. It doubles their workload with no short term benefit, and does so indefensibly.And then after all these extensions and capabilities have been designed, implemented, and transparently deployed across essentially the entire Internet without requiring large scale administrative intervention - a process that could easily take a decade - would the first extended addresses with non-zero bits in the extension fields actually be usable and globally routable in both directions. The entire network would be ready for it, it would be a dormant capability unused until that day arrived and requiring no large scale intervention when that day arrived either, because the silently upgraded network would remain administratively compatible with the old one. reply welterde 7 hours agorootparentprevThat document has been going around for ages and is based on the same fundamental misunderstanding that one somehow can extend IPv4 in a way somehow, but remain compatible with IPv4-only clients. This is just not possible.Most of the other criticism is not relevant anymore, since we now have a lot of transition technologies that allow IPv6 clients to interoperate with IPv4 servers (this way around is possible since IPv6 is a superset of IPv4). Overall we are now much further into the IPv6 migration than djb ever envisioned. reply butlerm 26 minutes agorootparentThat document is not a proposal, it is a problem statement. It goes without saying that IPv4 only clients - every single one of them - would need to be upgraded in some fashion to support a larger address space. The issue is how to do that while remaining incentive compatible.The only way to remain incentive compatible is to remain administratively compatible, and that is where IPv6 as presently constituted fails dramatically by requiring two independent network configurations to be maintained for the better part of a century, without giving anyone an incremental incentive to maintain the second one, leading to a hold out problem.The public switched phone network has gone through major upgrades and yet at no point did someone say we are going to throw out all your existing phone numbers and require you to get new ones, or require you to have two independent and incompatible phone numbers that you put on your business cards, have two phones on your desk, or a phone with a mode selection button depending on whether you wanted to call a new style phone number or an old style phone number.And that - from an administrative point of view - is the fundamental problem with the deployment of IPv6 as we know it. Dual stack now and for decades to come. Dual stack anything is not incentive compatible and should never have been done. The proper solution is single stack everything with capabilities that are dormant until they are deployed on a global level as part of the normal upgrade process in an administratively compatible fashion so that no large scale administrative intervention is required now or at any time in the future. reply j16sdiz 7 hours agorootparentprevIPv6 is adding lots of complexity IPv4 never have to deal with.SLACC, interface-specific link-local address both look good in paper but cause lots real life headache.When it works, it works; when it don&#x27;t, you have to unlearn and relearn everything network before you could possibly understand the problem, let alone fixing them. reply cbsmith 7 hours agorootparentYou can use DHCPv6 instead of SLAAC. Plenty of people seem to find SLAAC preferable... and interface-specific link-local addresses are key to making it work so... reply dataflow 6 hours agorootparentprev> That document has been going around for ages and is based on the same fundamental misunderstanding that one somehow can extend IPv4 in a way somehow, but remain compatible with IPv4-only clients.Let me summarize my understanding of what he&#x27;s saying, because I don&#x27;t quite see why&#x2F;how you disagree. I think you (or I) might be misunderstanding his claim.Imagine this topology: C (client, IPv4-only)R (intermediate router)S (server)My understanding of djb is he&#x27;s saying that IPv6 could have been designed such that S could still serve C via only simple software updates -- this means, crucially, without the need for S to separately obtain a public IPv6 address through R, because its IPv4 address would be automatically valid for IPv6.How can this work? Well, there are two scenarios:1. If R is IPv4-only, then S could figure that out during some startup&#x2F;negotiation process, and send only IPv4 packets to R. R only lets IPv4 clients connect to S anyway, so any response (even from IPv6 applications) on S must be going back to an IPv4 address. So the kernel can transparently translate those IPv6 addresses into IPv4 before passing them along to R, and vice-versa.2. If R supports IPv6, then R can do the same thing S would&#x27;ve done in the previous^ scenario. (In fact, I think S could become IPv6-communication-only in that case, reserving IPv4 for just address leasing? I&#x27;m not sure, but in any case, I don&#x27;t think that matters here.)Notice that all of this is almost completely stateless. (I think the only state S needs to track here is 1 bit, indicating whether R supports IPv6 or not.) So, S and R can be independently and (importantly, rather trivially) upgraded to support the IPv6 protocol, without losing the ability to talk to any clients within the IPv4 address range.This is easy and requires no explicit leasing of IPv6 addresses. That step can be implemented and have support for it added later, whenever S is ready to serve clients beyond the IPv4 address space.Does this make sense? If so, then it seems to show how IPv4-only clients could talk to IPv6 servers without modification. If not, then I&#x27;d love to see where I&#x27;m mistaken (I very well might be). reply welterde 5 hours agorootparentThe issue is not how can we enable IPv4 C talk to IPv4 S, since that already works. The problem is how would C (ipv4-only) send a packet to S (ipv6-only) in the first place? It doesn&#x27;t know how to deal with non-ipv4 packets, since it&#x27;s ipv4-only. Thus if the server does not have a IPv4-address how does a IPv4-client send packets to it?The inverse situation (IPv6-only client but IPv4-only server) is not really an issue, since for that situation NAT64 works, since you can embed IPv4 addresses into IPv6.The only way C (ipv4-only) could communicate with S (ipv6-only) is by either allocating a dedicated ipv4-address to S (doesn&#x27;t have to be directly connected to S - it can be sent to some translation box that does SIIT) or by upgrading C to support ipv6 and tunneling it (6to4, 6rd, teredo, etc.). reply renewiltord 3 hours agorootparentprevIn this scenario, R sends packets to S over IPv4 only. So S must have an IPv4 address? If S has an address that is larger than 32 bits, what goes in the \"destination IP\" field in the IPv4 packet that R is capable of sending? If S has an address that is guaranteed to be less than 32 bits then we&#x27;re just in the IPv4 world.How are R and S negotiating when R cannot even name S on the network? Its stack only allows for 32-bit addresses and S can&#x27;t have a 32-bit address. reply hn_throwaway_99 7 hours agorootparentprev> Overall we are now much further into the IPv6 migration than djb ever envisioned.That post was written 20 years ago. I would hope that the migration would be more than \"much further along\", I&#x27;d have hoped it had been completed, like a decade ago.> is based on the same fundamental misunderstanding that one somehow can extend IPv4 in a way somehow, but remain compatible with IPv4-only clientsI&#x27;m not a network engineer, but I&#x27;ve seen loads of commentary from knowledgeable sources that it would have been quite possible to have extended the ipv4 address space without requiring 2 completely separate network stacks.I think the simple fact that ipv6 includes so many other parts beyond just extending the address space shows what a foolish endeavor it was in the first place. I&#x27;m not saying the other bits aren&#x27;t good ideas, but the only immovable factor that has people wringing their hands about ipv4 is the address limitation. If they had just focused on that, we probably wouldn&#x27;t be in a situation where we&#x27;re still running 2 network stacks virtually everywhere, and will be for the foreseeable future. The famous XKCD \"Standards\" meme says it best: https:&#x2F;&#x2F;xkcd.com&#x2F;927&#x2F; reply welterde 6 hours agorootparentI have yet to read any IPv6 alternative that can theoretically work or is not just IPv6 in disguise (or something that is implemented in IPv6 in a slightly similar fashion).It may sound like a great plan as long as one doesn&#x27;t look too closely at the details. IPv4 has fixed 32-bit addresses and one cannot cram more than 32-bit of information into a fixed 32-bit field. But one would need to do that for it to be forward compatible, since how would a IPv4-only client open communications with a expanded address space server?One idea is to only upgrade the client and server and tunnel the expanded address space packets over IPv4. IPv6 has that - that&#x27;s how it was bootstrapped before native IPv6 connectivity was a thing. reply tsimionescu 4 hours agorootparentI believe the claim is that, IF IPv6 had been a much more minor modification of IPv4, including just a change to the packet structure to have a larger address field, it would have seen more adoption more rapidly than the current version of IPv6 which also changes everything else about the L2-3 stack. Sure, you would have still needed new devices and that would have taken some years, but as long as the network architecture remained the same, it wouldn&#x27;t have required a quarter century to get to 30% adoption.I don&#x27;t know that I believe this claim at all, but it is at least coherent and possible (unlike claims that N-bit addresses could have been added to IPv4 in a backwards compatible manner). Perhaps SLAAC, the focus on routable end user devices etc were indeed major distractions that pulled focus away from a move to a new larger address space - which is the only thing people actually wanted from IPv6. reply growse 4 hours agorootparent> which is the only thing people actually wanted from IPv6.I&#x27;m not sure this is true. There&#x27;s a lot of warts in the way IPv4 was put together (ARP is a tire-fire of badness for example), and the opportunity was taken to implement those in a better way. Lots of network engineers are very happy about that. reply gemstones 2 hours agorootparentGood thing they can benefit from those changes, now that we’re decades in! Can you imagine if they had to manage ARP still? reply OJFord 4 hours agorootparentprevI don&#x27;t feel strongly about it, but my reading of the complaint is that the implication is it should&#x27;ve been something like an address in 240.0.0.0&#x2F;4 plus more bits. Then to IPv4 it looks like a reserved for future use address (and we declare we&#x27;re done with IPv4 now, there will be no (other) future use) but to (this hypothetical variant of) IPv6 it&#x27;s a longer address.I think what annoys people is everything else that changed with it, if DHCP (I know), subnets, NAT if you wanted it, etc. was all just the same, if the model was the same the IPs were just longer now, that they wouldn&#x27;t really complain.I&#x27;m not even sure it would be necessary to put it in unaddressable v4 block, since it would be too long and a different version anyway. Obviously people have thought about this a lot more than me so know a lot more about it, I&#x27;m not naïve about that. reply welterde 3 hours agorootparentIt&#x27;s not clear to me how 240&#x2F;4 would have helped anything. Or how it would have even be different from a tunneling approach on a high level. reply Macha 1 hour agorootparentprev> That post was written 20 years ago. I would hope that the migration would be more than \"much further along\", I&#x27;d have hoped it had been completed, like a decade ago.https:&#x2F;&#x2F;www.google.com&#x2F;intl&#x2F;en&#x2F;ipv6&#x2F;statistics.htmlYes, the IPv6 migration has taken much longer than anyone expected. But this argument would have made more sense in 2015 when we were looking at 5% IPv6 deployment and very erratic growth. But it&#x27;s not, we&#x27;ve been looking at 10% of the market gaining IPv6 support for the last 3 years and are now at 45%. Now granted, this is likely to be largely \"new\" devices, e.g. in mobile networks and in countries like India where these were hidden behind CGNAT before. But these are exactly the type of devices that an IPv4 extension header couldn&#x27;t have reached either. reply greyface- 35 minutes agorootparentprev> The original architects should have \"the perfect is the enemy of the good\" forcibly tattooed on their foreheads.Who&#x27;s rejecting the good enough in favor of the perfect, now? This thread is full of \"IPv6 is not perfect, so we must reject it\". reply lmm 5 hours agorootparentprev> Yikes, couldn&#x27;t disagree with that more. There are a ton of things that ipv6 designers could have done to make the transition much easier. This is a (now quite old) blog post that is my \"go to\" that explains a lot of the problems with ipv6: https:&#x2F;&#x2F;cr.yp.to&#x2F;djbdns&#x2F;ipv6mess.htmlIt&#x27;s a dumb post, to the point I think it must be a deliberate troll. The parts that are possible don&#x27;t solve any relevant problems (\"my new protocol would allow computers that already have public IPv4 addresses to talk to each other\" is not a point in favour of your new protocol), and the parts that solve relevant problems aren&#x27;t possible. reply hn_throwaway_99 5 hours agorootparent> It&#x27;s a dumb post, to the point I think it must be a deliberate troll.Lol, I&#x27;d like to send this to DJ Bernstein, let him know that random Internet commenter thinks that one of his most well-known essays \"must be a deliberate troll.\" Glad HN doesn&#x27;t support emojis, not enough facepalms it the world for this one. reply oblio 4 hours agorootparentBernstein is known for being very... opinionated.For example his Qmail was conceptually a very well designed email server but the email standards kept evolving and I&#x27;m fairly sure at some point he just said \"Qmail is feature complete and secure, no more new features and patches\". Like, what? It&#x27;s networked software, that&#x27;s not how any of this works. reply growse 4 hours agorootparentprevDJB&#x27;s brilliance at some things doesn&#x27;t make them immune from saying very silly things about other things. reply hn_throwaway_99 4 hours agorootparentI&#x27;m not saying he&#x27;s right in all areas, and while it&#x27;s certainly fine to disagree with his viewpoint, saying his essay \"must be a deliberate troll\" is laughable nonsense, especially since summarizing \"my new protocol would allow computers that already have public IPv4 addresses to talk to each other\" is a silly mischaracterization of what djb actually said. replykristopolous 8 hours agorootparentprevVint Cerf has called the decision of it being 32 bits as silly and arbitrary, like having a car odometer with 40 digits. If he had pushed for more, he thought nobody would have taken him seriously.The idea of 32 becoming scarce was laughable.Also the complaint about ipv6 isn&#x27;t a technical one, it&#x27;s a usability one. Extending it to 48 bits would have been easy enough for people - like international calling.Those 16 bits could be in hex, as a convention, so something like \"(4EA7) 8.8.4.2\".However, I&#x27;ve constantly heard that the 128 bit hexadecimal with colons just looks too complicated and inconvenient.You might be brilliant and find it easy but to a lot of people ipv6 addresses look like cryptographic hashes reply commandersaki 59 minutes agorootparentI don&#x27;t know why Vint Cerf should or want to take the blame for IP address exhaustion. We knew about the problem for 30+ years, we had a committee in charge of selecting IPng to mitigate such a disaster, we selected IPv6 as IPng, and then we ran out of IP address space. The only reason I could see why Vint Cerf is to have any blame is that he firmly agreed on a clean slate Internet protocol that didn&#x27;t give any consideration towards a transition plan. Yes that&#x27;s how the events unfolded, but that doesn&#x27;t make him some martyr. reply k4ch0w 4 hours agorootparentprevI think this is absolutely one of the reasons. Addresses are very hard to remember in ipv6. You usually just have to remember the first 3 parts of ipv4 and then change the last digit based on the host you want. IPv6 I know it has shorthand but still it doesn’t register in my brain the same way. reply Symbiote 1 hour agorootparentWhat alternative do you suggest for making the IP addresses longer (so more people can connect computers to the Internet) while keeping them equally memorable?The most recent anonymous editor to the IPv6 address article on Wikipedia has address \"2602:FBF6:0:0:30C6:7069:6DF0:FD24\". An IPv4-like notation of that would be \"9730.64502.0.0.12486.28777.28144.64804\". reply welterde 7 hours agorootparentprevIf one has to go through the pain of changing everything why not make sure one doesn&#x27;t have to do it again any time soon?Also having 64-bit for the network address (and 64-bit for the device) does have certain benefits that make it easier to use than shorter addresses in practice for a single entity, since one can hierarchically model the network and do things like ::. So even in absence of DNS one doesn&#x27;t quite have to remember 128-bit of information for every device. reply lazystar 7 hours agorootparent> why not make sure one doesn&#x27;t have to do it again any time soon?the pyramids in egypt took over a lifetime to build; a marvel of engineering, as theyve lasted thousands of years and noones had to build replacement pyramids. the problem, though, is noone in todays culture needs pyramids. reply tsimionescu 6 hours agorootparentprevBut ever since privacy extensions, that&#x27;s not how IPv6 is allocated, right? Each individual system is supposed to get a &#x2F;64. Your household should actually have a &#x2F;56 or something crazy like that, which few ISPs actually do. SLAAC doesn&#x27;t even work if it doesn&#x27;t have it&#x27;s own &#x2F;64 per host, I believe. reply welterde 5 hours agorootparentIt is. A typical enterprise might get allocated a &#x2F;32, which gives them 32-bit to nicely design their network and give 64-bit to each individual network where devices are connected.A typical ISP will get allocated a much larger allocation like a &#x2F;20, which allows them to allocate a &#x2F;56 for each of their customers while still having a few bits to play with. But all starting with the sameprefix.With IPv4 you will have many separate fragmented networks that have no numbers in common. And this will only get worse over time. reply nash 8 hours agorootparentprevOnly network engineers should see or care about IP addresses. The fact IPv6 addresses use colons is why people don&#x27;t use IPv6 is the worst take I&#x27;ve ever heard. reply kristopolous 8 hours agorootparentyou&#x27;ve &#x2F;never&#x2F; heard anyone call ipv6 long, confusing, and complicated? I&#x27;ve basically &#x2F;only&#x2F; heard people say that.This is probably because the idealized world of \"only network engineers\" is leaky. Programmers, sysadmins, people trying to get their network printer to work, non-specialists have to interface with network addresses constantly.Saying they shouldn&#x27;t is not a description of reality. Not everyone who needs to set up or diagnose a network do so as a career path.Almost all hardware and software has supported ipv6 for many many years. The humans using it are the ones that shut it off or disable it. Unless you address the human behavior of why that is, this problem will not be addressed.I claim there needs to be a friendlier, casual interface that makes people&#x27;s lives easier. It can be a crude kneecapped sheen so long as it addresses the needs of the general user. Then they&#x27;ll use ipv6, not for ideology or virtue reasons about the commons but because it makes their lives easier reply avereveard 7 hours agorootparentI&#x27;ve almost never heard anyone in the general population use ip addresses, with the notable exception of gamers, but that&#x27;s fading away too now that all major games come with friends, parties and deep linking supportI don&#x27;t really understand the use case for typing up addresses either, copy pasting is going to be more precise, and if one can&#x27;t read 8 quartet of letters one shouldn&#x27;t be near networking equipment either.Heck ibans are about as complex and the general population is coping just fine reply kristopolous 6 hours agorootparentright, but this isn&#x27;t theoretical. ipv6 was finalized 25 years ago and global adoption is around 1&#x2F;3. There&#x27;s something seriously wrong and it&#x27;s not that 2&#x2F;3s of the world are using Windows Me and 20+ year old devices that don&#x27;t support it.It&#x27;s human and behavior driven and addressing that is a matter of packaging, process, promotion, product, presentation... all those marketing ps. reply LatticeAnimal 7 hours agorootparentprevI host a few video game servers for my friends and love the fact that I can say my easy to remember IPv4 address aloud whenever they forgot it.I have an IPv6 address but haven&#x27;t bothered memorizing it or giving it out because it would just be more hassle. reply ncts 6 hours agorootparentThis is even better:game.latticeanimal.net. 3600 IN A 1.2.3.4 reply snowram 5 hours agorootparentI don&#x27;t know a single gamer who rented a domain for private use. In fact, I would argue than the hassle of setting one up is the reason why Hamachi got popular back then. You don&#x27;t have to bother with knowing any technical stuff, just download a software, share a code and play. reply Symbiote 2 hours agorootparentPeople used to use https:&#x2F;&#x2F;www.noip.com&#x2F; and similar services for this. reply josephg 4 hours agorootparentprevHamachi was popular back in the day because it meant you didn’t need to forward ports on your router. (Something that still confuses some of my friends). reply erinnh 5 hours agorootparentprevYou dont?It was extremely common with Teamspeak. reply conradfr 8 hours agorootparentprevNetwork engineers are the one making the transition (or not). reply hsbauauvhabzb 7 hours agorootparentprevI can see plenty of situations where colons behaviour might be confusing, let’s start with a http url on a non standard port with credentials embedded, I assume something close to http:&#x2F;&#x2F;01:02@03:04:05:06:07:8000&#x2F;foo&#x2F;bar would be a valid url, reply piperswe 7 hours agorootparentClose, it would be more like http:&#x2F;&#x2F;01:02@[03:04:05:06:07:08:09:10]:1112&#x2F;foo&#x2F;bar reply nikanj 7 hours agorootparentprevBut only network engineers pick the protocols used. You’re making life harder for the very people who should be your primary audience.PS: As a developer, I often read logs and go ”oh yeah, that’s just our satellite office IP”. 192.168.1.110 is the network printer, etc. There’s no hope of recognizing IPv6 addresses at a glance the same way. reply Symbiote 2 hours agorootparentMaybe if you only have five VMs or something, but when there are 100 or more the additional bits in an IPv6 address will be useful. You can recognize a whole subnet as dev, or production, or a different site, or the office LAN. reply terom 7 hours agorootparentprevNo, 192.168.1.110 is MY network printer, you must have gotten confused somewhere! reply MawKKe 6 hours agorootparentWhoa buddy how did you get into my home network??? reply nikanj 5 hours agorootparentThe same way we all do: A shitty modem that gives an ipv6 address to all devices, but only applies firewall rules to ipv4 addresses replyyardstick 7 hours agorootparentprevNAT64? No what we needed was NAT66, and it took over a decade after IPv4 to deliver this. Because the IPv6 advocates were too opinionated on exactly how IPv6 should work. And since when has the world even agreed to anything even remotely complicated without trying to change things up? reply dale_glass 6 hours agorootparentNo, NAT needs to be shot into the sun. It&#x27;s absolutely not needed under IPv6. reply forty 1 hour agorootparentWhy? It seems useful to hide details of your private network from everyone reply dale_glass 2 minutes agorootparentYou can do that with the privacy extensions. Plus on IPv6 you should get enough address space that it makes no sense to run a scan against anyone.On IPv4 or NAT there&#x27;s just 65535 ports to check. On a &#x2F;48 with privacy extensions there&#x27;s 2^80 addresses to go through, which from an external point of view don&#x27;t remain constant. You can&#x27;t even ping all of that. tsimionescu 5 hours agorootparentprevI some ideal IPv6-only world, perhaps. But when running a mixed stack like the entire world is right now, it&#x27;s actually crucial. reply csomar 8 hours agoparentprevYeah, I never got any of the articles about how well IPv6 is designed. Any article will get me confused about whether an IPv6 address is a range, a computer, a router, or something that points to a resource inside my computer. (I guess it&#x27;s all of these things?).But the biggest problems of all: You can write an IPv4 address on a phone call. You might be even able to remember it. Not the case for IPv6, you need to be an expert in Hex and remember the specs design. I can&#x27;t do it as a developer, I don&#x27;t think normal people will be able to do it either.IPv4 is useful because it&#x27;s just a number (at least from a person&#x27;s perspective). It works. Just add a letter to it and then you have x26 the capacity. reply Macha 1 hour agorootparentThe amount of \"account numbers\" I have that are in fact alphanumeric with various services suggests that these companies (utility companies, banks, etc.) don&#x27;t consider this a problem. reply flomo 7 hours agoparentprev> Can we go back in time and hit the designers of IPv6 upside the head?Okay we are back in 1992 and are wearing a mullet. &#x27;The Internet&#x27; is still a relatively small number of mostly university and government sites, and is barely used for anything important, so a flag day seems pretty feasible. reply codegeek 2 hours agoparentprevIPV6 adoption reminds of the whole Python 2=>3 fiasco except it is much much worse. reply drchiu 1 hour agoparentprevEngineers&#x2F;designers never assume their ideas aren’t as clever as it appears in their minds. reply raincole 2 hours agoparentprevYes, if we hit our heads hard enough we can come up with a way to magically put more than 4B addresses in 32 bits. Math and logic don&#x27;t matter in the real world, only head-hitting does. reply vaylian 5 hours agoparentprevIPv5 tried to stick to 32 bits. That didn&#x27;t work out: https:&#x2F;&#x2F;www.lifewire.com&#x2F;what-happened-to-ipv5-3971327 reply oblio 4 hours agoparentprevAlso, correct me if I&#x27;m wrong, but didn&#x27;t they make the smallest subnet have 64k addresses? I remember a networking teacher say that by that point just ARP traffic will have long killed the subnet. reply globular-toast 4 hours agoparentprevThings don&#x27;t need to be backwards compatible, they just need to have a financial incentive attached. That could either come from the market or from the government. There&#x27;s a cost to making things backwards compatible and this can be avoided if we just don&#x27;t. There are countless examples of technologies that weren&#x27;t backwards compatible but happened anyway. For IPv6 the problem is the incentive isn&#x27;t coming from the anywhere. reply sfeng 6 hours agoparentprevI personally would have added one additional byte, but when it is written I would have added two bits to each of the original bytes. That has the advantage of making each segment 0-1023. I would have reserved the 1000-1024 values, such that IP addresses look the same visually: 476.188.049.772, but the available number is increased by a factor of more than 200. Maybe it’s not too late to release IPv5… reply emilfihlman 3 hours agoparentprevAbsolutely this. IPv6 is when engineering doesn&#x27;t face reality and operates on \"we know better\" standards, when they actually don&#x27;t know better.I actually feel antagonist towards the designers for it. reply hesnuts 5 hours agoparentprevIPv6 is so awful. We need a re-do. reply Narkov 14 hours agoprevA subtle detail from the article is that address prices peaked in 2021 at $60 and has steadily decreased to $35. Where does it go from here? Is this a proxy for the tech correction? reply p1mrx 9 hours agoparentIdeally the long-term value is $0 when IPv4 becomes irrelevant, but how long that takes, and the prices along the way, are anyone&#x27;s guess. Even explaining past prices is analogous to stock market voodoo. reply TZubiri 8 hours agorootparentNot a fan of ipv6 evangelizing, much less at this point. Just give it up. reply JakobH 8 hours agorootparentWhat is your solution? reply coldpie 17 minutes agorootparent> What is your solution?Understand very little about the problem space and complain about the best-compromise solution that the people who do know what they&#x27;re talking about came up with. It&#x27;s a very comfortable position to be in, I recommend it to everyone. reply speedgoose 8 hours agorootparentprevNot OP nor a recognised network expert but here is my suggestion:IPv10. The next IP version number that is unassigned, that is conveniently 4+6.Basically something that is not breaking compatibility with IPv4 and doesn’t require those dual network stacks nonsense. reply cdchn 8 hours agorootparentHow do you not break compatibility with IPv4 while also getting more bytes in the address? reply sambazi 7 hours agorootparentcountry codesreply hkt 5 hours agorootparentprevMaybe just don&#x27;t? Let it be IPv4 with more bits, the software is already there so dual stacking isn&#x27;t so bad, adoption might actually be quick if people didn&#x27;t have to learn much to implement it. reply superluserdo 2 hours agorootparent>Let it be IPv4 with more bitsThen it&#x27;s not IPv4 and is not compatible with IPv4. reply speedgoose 7 hours agorootparentprevWell again I’m not a network expert but perhaps we could look at the 240.0.0.0&#x2F;4 reserved for future use block and add more address bytes in the payload or something. It’s not going to be elegant but IPv6 is kinda elegant and failed. reply growse 6 hours agorootparentThis is the most hilarious \"I don&#x27;t understand anything about the problem, therefore I don&#x27;t understand how it&#x27;s hard\" comment I&#x27;ve seen this week.> perhaps we could look at the 240.0.0.0&#x2F;4 reserved for future use blockWhat&#x27;s the current rate of v4 address space consumption? How long will this block last?> and add more address bytes in the payload or something.This is, by definition, not backward compatible. reply speedgoose 5 hours agorootparentI may not have been clear enough in my suggestion. The idea would be to use this unused block as a special block. Not to fill it up with normal IPv4 allocations.See my suggestion as some kind of NAT-PT at scale. With a better marketing name and user experience.The problem is indeed hard because no one manage to find a solution at scale since 3 decades. reply shawabawa3 4 hours agorootparentNo matter what change you make, or how you make it, if you are making more than 4B addresses routable then any existing IPv4 device will not be able to route some addresses, so you will have caused a split in the internetThis is a fundamental and unresolvable problem with \"making it backwards compatible\" reply speedgoose 4 hours agorootparentWouldn&#x27;t NAT be an existing and well used solution to this problem? reply simiones 1 hour agorootparentEven if we accept that NAT is the right solution, it still is pretty limited in how far it has been able to extend the address space, since port numbers only give you two extra bytes of address space. And there are no further extra bytes to stuff somehwere else in a TCP or UDP packet header.Of course, we could extend the address space by further breaking the layering of routes, and baking in support for higher layer protocols into routers. We can certainly stuff more address information in HTTP headers, so the web could be extended to essentially arbitrary size by simply requiring routers to look not just at source and destination IPs and source&#x2F;dest TCP&#x2F;UDP port numbers, but also client and server HTTP headers. SIP looks a lot like HTTP, so the same solution could work there. TLS already has support for additional headers, so we could also do extra NAT at that layer.Hell, AWS could then use a single IPv4, and just rely on HTTP&#x2F;SIP headers or TLS extension headers to know the actual destination! Of course, if you want to run another L7 protocol, tough luck - tunneling it is for you. reply speedgoose 35 minutes agorootparentYes I agree you would need to tunnel because the headers aren’t big enough.If I had to guess the futur, the industry will most likely go towards something like few expensive IPv4 owned by major cloud and internet providers and crazy recursive NAT setups everywhere. Because that works without breaking stuff. superluserdo 2 hours agorootparentprevNAT is the problem that IPv6 fixes. Think about the parent comment>if you are making more than 4B addresses routable then any existing IPv4 device will not be able to route some addresses, so you will have caused a split in the internetThis has basically already happened. We&#x27;ve massively extended IPv4 by stuffing extra address bits into the router&#x27;s port number, and it means that any two devices behind NATs can&#x27;t directly route to each other. reply speedgoose 41 minutes agorootparentNAT has been more successful than IPv6 at fixing the same issue, the shortage of IPv4 adresses, but without breaking compatibility (well at the cost of crazy hacks for weird protocols such as FTP).Not being able to route directly doesn’t seem to be a major issue to me. It for sure require more computing power in routers but also adds some safety and privacy by design. reply growse 5 hours agorootparentprev> The problem is indeed hard because no one manage to find a solution at scale since 3 decades.The problem is hard because despite everyone&#x27;s wishes, it&#x27;s got nothing to do with technology. All migrations are about economics and incentives, IPv6&#x27;s qualities as a design (it&#x27;s a long, long way from perfect, but I&#x27;d argue that it&#x27;s good enough) are irrelevant. replyatyvr 14 hours agoparentprevYah, it certainly seems like maybe that was peak pricing. This write up has some more data on historical pricing https:&#x2F;&#x2F;www.ipxo.com&#x2F;blog&#x2F;ipv4-price-history&#x2F; I&#x27;ve also heard folks pay quite a bit over the average price for novelty IP addresses, so perhaps that skewed the data? I&#x27;d love to be able to buy 2.2.2.0&#x2F;23 or my favorite 42.42.42.0&#x2F;24 reply 015a 13 hours agorootparentYeah, one example is Cloudflare and 1.1.1.1; though the story behind that is less about money and far more interesting. Apparently, APNIC had owned 1.1.1.1 for, basically, forever, but were never able to actually use it for anything because it caught so much garbage traffic. Cloudflare is one of only a handful of service providers that could announce the IP and handle the traffic; so in exchange for helping APNIC&#x27;s research group sort through the trash traffic, Cloudflare hosts their DNS resolver there.https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;announcing-1111&#x2F; reply Something1234 13 hours agorootparentI would really like to see the results of this research to understand what is going on there. reply oasisbob 11 hours agorootparenthttps:&#x2F;&#x2F;labs.ripe.net&#x2F;author&#x2F;franz&#x2F;pollution-in-18&#x2F; reply wincy 10 hours agorootparentThat’s pretty cool. I’d never though about bogons and debogonizing before, it’s like chasing off all the squatters on your property and more keep coming. You need some fat pipes and beefy servers to be able to handle all the bogus traffic of machines trying to hit your server, and also be able to actually fulfill your purpose.Make sense now why Cloudflare would be one of the only companies that could handle it! reply nielsole 8 hours agorootparentThey only had a 10mbit link. Apparently 50mbit&#x2F;s was the amount of traffic they received.Mostly everyone could handle this, not just CloudFlare. reply raverbashing 10 hours agorootparentprevSo, what happened to everything that expected 1.1.1.1 to error out and now is getting something?(not worried about them, just curious) reply oefrha 2 hours agorootparentYeah it broke my use case. I used to run `curl --retry 9999 http:&#x2F;&#x2F;1.1.1.1` and since it didn&#x27;t exit, the heat generated by the running curl process kept me warm in the winter. But now http:&#x2F;&#x2F;1.1.1.1 returns immediately, so I&#x27;m freezing! reply voxadam 1 hour agorootparentYou&#x27;re obviously a fellow fan of 1172.[0][0] https:&#x2F;&#x2F;m.xkcd.com&#x2F;1172&#x2F; reply stephen_g 9 hours agorootparentprevI mean, for smaller routers that had static routes set for that subnet, it would probably just keep working - the issue being that trying to get to real addresses in the 1.0.0.0&#x2F;8 network (or parts of it) wouldn&#x27;t work.If you were BGP peering then you&#x27;d probably get a real route into your local table though.So yeah, some stuff would probably have just broken, but that&#x27;s the risk you take using parts of the IP space you shouldn&#x27;t be using! reply depereo 9 hours agorootparentprevWell, a lot of Cisco wireless engineers had to reconfigure their guest wifi captive portals. reply lxgr 9 hours agorootparentHeh :)To be honest I feel as bad for them as I do for Hamachi, when their (otherwise quite nice in that it was a spiritual predecessor to Tailscale!) overlay VPN service fell apart once 5.0.0.0&#x2F;24 became publicly assigned. reply jtriangle 6 hours agorootparentprevWhich is funny because half the time I still end up manually typing 1.1.1.1 and praying for a redirect... reply willcipriano 9 hours agorootparentprevThey switched to 2.2.2.2 reply raverbashing 8 hours agorootparentIPv1 is a cozy place replyfanf2 14 hours agoparentprevPossibly. It might also have been a bit of a panic because RIPE ran out of IPv4 addresses around that time and it was unclear back then how liquid the transfer market would be. reply p1mrx 13 hours agorootparentRIPE ran out in 2012, just after APNIC in 2011.RIPE had a policy to extremely rate limit allocations from their last &#x2F;8, which is how they were able to continue allocating for an extra 7 years. The other RIRs had no such policy. reply peoplearepeople 14 hours agoparentprevIPv6 usage also went up 10% in that timehttps:&#x2F;&#x2F;www.google.com&#x2F;intl&#x2F;en&#x2F;ipv6&#x2F;statistics.html reply mbgerring 7 hours agoprevWe should probably consider whether the rent-seeking enabled by the scarcity of IPV4 addresses is one of the things holding back IPV6 adoption. reply jasekt 7 hours agoparentEU have enough influence to do to IPV6 what it did to USB in iPhones. reply Sebguer 7 hours agorootparentIt&#x27;s harder to prove consumer harm with something as abstract as IP addresses, and there&#x27;s a bunch of other pieces that make this much less unlikely, such as the fact that USBs already become obsolete and need to be replaced, so you&#x27;re just shifting the replacement cycle. reply irdc 6 hours agorootparentWith all those addresses locked up at the hyperscalers IPv4 has become anticompetitive. So yes, the EU can prove consumer harm. reply nonethewiser 5 hours agorootparentprevThey don’t need to prove consumer harm to pass laws. reply sambazi 7 hours agoparentprevyou mean the fact that amazon has around 4.5B reasons to not support ipv6?interesting idea reply quaintdev 58 minutes agorootparentSooner or later any sort of monopoly leads to abuse of power and loss of foresight of greater good. reply vaylian 5 hours agoprevGitHub.com still doesn&#x27;t support IPv6. I know that there is some work going on to support it, but this shows that it is far from trivial. reply ajsnigrutin 3 hours agoparentIt&#x27;s usually not a tehnical problem, but either a management or a budget problem. reply quaintdev 2 hours agoparentprevHacker news also does not support IPv6 reply jeroenhd 3 hours agoparentprevThis is one of the reasons why I&#x27;m trying to avoid Github these days. If they can&#x27;t get something as simple as IPv6 to work, I don&#x27;t have much faith in the test of their backend either. reply code_runner 3 hours agorootparentSort of a weird take. I doubt if the people responsible for testing their backend have ever even met the people who would migrate to ipv6 reply jeroenhd 2 hours agorootparentThere&#x27;s plenty of attention on the Github meta issue about the lack of IPv6 (https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;community&#x2F;discussions&#x2F;10539) so anyone who even glances at customer requests should be aware of the issue at the very least.They also did offer IPv6 availability for a short while as a test, but that was quickly shut down, so there is probably a technical issues they can&#x27;t figure out or they would&#x27;ve kept the trial system running for longer.Either way, Github isn&#x27;t communicating, so it&#x27;s hard to tell if this is indifference or incompetence. As an end user, the distinction doesn&#x27;t really matter. reply brodouevencode 1 hour agoprevDoes the cost of migrating to IPv6 exceed the cost of buying up large swaths of IPv4 addresses? reply Gigachad 14 hours agoprevHopefully hosting providers putting an actual price on v4 usage will be the push that gets things rolling to v6. reply otabdeveloper4 9 hours agoparentThey already do. An IPv4 costs about 5 dollars per month at current prices. reply gnopgnip 7 hours agorootparentThe cost to own an IP is about $45. But that is almost entirely a one time cost, you can rent an IP for well under $10 a year reply olalonde 6 hours agorootparentHow do you do that? I&#x27;d like to buy one. reply RockRobotRock 5 hours agorootparentYou need to buy a minimum of &#x2F;24 reply doublerabbit 3 hours agorootparentAh, so $35k then. reply alexvitkov 1 hour agorootparent256 * $45 != $35k replysambazi 7 hours agoparentprevmost already do, but it&#x27;s not significant reply azlev 13 hours agoparentprevI don&#x27;t like this solution because it will cost me money and time to setup myself a modem that can handle properly ipv6 or switch to a more expensive provider.Send the bill to end users is not what should be done.All this ipv6 endeavour already cost me a lot of time learning and troubleshooting software, and sometimes realizing that some modems doesn&#x27;t have a good ipv6 stack and the best solution is to turn it off. reply ianburrell 11 hours agorootparentCharging for IPv4 is mostly going to affect hosting providers. Until recently, AWS users were charged nothing for public IP so there was no incentive to conserve.The price for IP for connections is already builtin to the price. Also, ISPs just use CGNAT to share IPs with multiple customers when they are short, It makes sense to charge more for static IP.How long ago did you do try IPv6? These days it should just work. If your router doesn’t work, get a better router since it is broken. reply preisschild 9 hours agorootparentprevBlame your ISP. IPv6 should have already been supported since the 2000s. reply Tijdreiziger 9 hours agorootparentThen there are a lot of ISPs to blame: https:&#x2F;&#x2F;www.google.com&#x2F;intl&#x2F;en&#x2F;ipv6&#x2F;statistics.html reply Dobbs 1 hour agorootparentThat is a really nice chart, thanks for sharing.What is confusing me is the Netherlands. We only have about 13% adoption. I&#x27;m on one of the largest ISPs KPN and get 10&#x2F;10 on IPv6 tests. Is this because I use a custom router? I&#x27;d expect it to be a lot higher since apparently KPN supports IPv6. reply Gigachad 8 hours agorootparentprevBy that chart, the IPv6 rollout will be basically complete by 2030. I suspect it will speed up near the end though as the first few services start to go V6 only. reply jtriangle 6 hours agorootparentI mean, that&#x27;s nice to think about, but are you going to spin up a service that&#x27;s v6 only?I sure as hell wouldn&#x27;t want to be the canary in that coalmine... reply Macha 1 hour agorootparentI have a couple on my home network. Since I only have one public IPv4 address, port forwarding only allows me to expose one device on IPv4 on port 80. I could use a reverse proxy for those services, like I do for others, but as the only user who has reliable IPv6 on all my devices, there&#x27;s no need. reply lmm 5 hours agorootparentprevI already do for hobby projects that are only for me and maybe a few friends. Why pay the extra for a v4 address? reply lxgr 9 hours agorootparentprevBlaming is nice, moving your business elsewhere is better. Unfortunately, not everybody has more than one ISP serving their area. reply atyvr 14 hours agoprevcorrect, direct url is https:&#x2F;&#x2F;toonk.io&#x2F;aws-ipv4-estate-now-worth-4-5-billion&#x2F;index... More interestingly, perhaps, is a forward look at how much new revenue AWS will generate once AWS starts charging ($44 per IP per year) as of 2024. I think it&#x27;s not unlikely that AWS will generate a few hundred million to a billion in additional revenue with this new charge. reply cdchn 7 hours agoparentI suppose a sufficiently motivated individual could look at something like Shodan, see how many of those IP addresses AWS owns are active (and fudge it by some factor to account for how many of those IP addresses are used by AWS themselves) and multiply it by their new IP address billing. reply sah2ed 3 hours agorootparentNo need to use Shodan. AWS already publishes the IPv4 address it owns in JSON from a well-known endpoint [0].[0]: https:&#x2F;&#x2F;ip-ranges.amazonaws.com&#x2F;ip-ranges.json reply dopa42365 10 hours agoparentprevHetzner charges 2€ per month or so for a v4 address already (optional of course). reply kro 8 hours agorootparentThat is for dedicated servers and additional IPs. The first on a VPS is 0,60€ replyyla92 14 hours agoprevSome noob questions here.How does one buy a block of IPv4 as an individual? (If that&#x27;s allowed)After you purchase it, how does it come into your possession?How do you utilize them? reply atyvr 14 hours agoparentYou&#x27;ll need to become a member of one of the regional Internet route registries, like RIPE or ARIN. Then you can buy, say a &#x2F;24, and transfer it into your RIPE&#x2F;ARIN account. Now you have your own IPv4 range. And you can start for example start to use it for your own servers. To do so you need to \"announce\" this new &#x2F;24 to the internet, using a protocol known as BGP. You can do that yourself, using a router, assuming you have an Autonomous system number (AS). You can get these via RIPE or ARIN as well. Or rely on your hosting provider to do that. For example AWS support \"bring your own IP address\". In that case they will announce the ip prefix in BGP for you, and you can assign your ec2 instances public IP&#x27;s out of your range. Equinix Metal, (previously Packet), also makes it easy to do this. reply j16sdiz 7 hours agorootparentBefore you can \"announce\" a prefix, you need an ISP willing to peer with you.BGP is a very insecure protocol. Most of its \"security\" are enforced by money and contract. reply greyface- 24 minutes agorootparent> BGP is a very insecure protocol.Take a look at the state of RPKI. ROA validation is common these days, and ASPA validation will be common soon. You still need to manually validate that your peer truly represents the AS that they claim to, but if that&#x27;s been done, ROA+ASPA validation prevents unauthorized announcements.Absent RPKI, people have been filtering based on IRR for ages, which will not necessarily prevent unauthorized announcements, but will require an attacker to leave a paper trail when making one. reply notyourwork 11 hours agorootparentprevThank you for this reply. I learned a lot from it. reply Alifatisk 6 hours agorootparentprev> To do so you need to \"announce\" this new &#x2F;24 to the internet, using a protocol known as BGP. You can do that yourself, using a router, assuming you have an Autonomous system number (AS).Is this how BGP hijacking is done? reply erinnh 5 hours agorootparentTechnically, yes.But good ISPs filter the prefixes their customers can announce to only those they actually own.Then you have shitty providers that dont do it, and thats how you get BGP hijacking.And you cant do this just from any connection, fyi.You will need a datacenter, cloud host or residential ISP that actually allows you to peer with them and announce routes. This isnt a standard thing you get just by being a customer. reply kxrm 11 hours agoparentprevI actually went through this process with ARIN. So I can give you that perspective. It wasn&#x27;t a big deal, the only minor concern I had was it felt like you&#x27;re encouraged to sign up under a business entity. I had an LLC, so it was natural just to use that. I don&#x27;t know what kind of vetting they do if you decide to use yourself as an organization though instead of a different legal entity.You need to provide justification, and frankly it&#x27;s not that big of a challenge to get a &#x2F;22 which is what I got. As long as you can show how you would like to use them and over what time frame, they will allow you to go through with the acquisition. An ASN is not required to get any IP block. You can always associate your IPs with any ASN that you want so long as that ASN owner is cooperating with you. I went ahead and grabbed an ASN for ease but some ISPs will allow you to use their ASN.You also do not have to purchase an IPv4 block from someone. You can go through the normal IPv4 request process, however the waitlist [1] is now over a year long for IPv4. However IPv6 are given out very quickly. IPs you acquire this way are \"free\" to acquire with your ARIN membership. Your membership dues are determined by the assets you hold, there is a fee schedule [2] and you need to pay it annually to maintain your membership and ASN&#x2F;IP assignments.I encourage anyone interested in understanding this process to go through it, it didn&#x27;t take a ton of time nor did it cost a lot in the grand scheme of things. Being an ARIN member also entitles you to be a part of how IPs are governed in the region you acquired them in. They will occasionally send out surveys and you can vote on issues.[1] https:&#x2F;&#x2F;www.arin.net&#x2F;resources&#x2F;guide&#x2F;ipv4&#x2F;waiting_list&#x2F;[2] https:&#x2F;&#x2F;www.arin.net&#x2F;resources&#x2F;fees&#x2F;fee_schedule&#x2F; reply CMCDragonkai 10 hours agorootparentI&#x27;m curious if one were to be certain nation state and was happy being a completely isolated intranet, that they would just exit such arin or related associations and just create their own governing body of IP allocations? In such a case such an internet would be a completely separate internet right?I wonder if sanctions may ever apply to the internet itself and we may see a break up of the internet into regional internet&#x27;s.And if we want to ensure global connectivity these associations would need to be completely independent and voluntary standard and such fees would be paid to an international standards body not beholden to any particular nation&#x27;s whims?What if nations started adding intercontinental NAT gateways acting as the entry and exit points between their national boundaries and the rest of the world. reply Figs 8 hours agorootparentNorth Korea supposedly has its own intranet with IPs in the 10.0.0.0&#x2F;8 private range: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kwangmyong_(network)I have no idea how they manage IP allocation internally there though. reply shrubble 8 hours agorootparentprevThey could just use CGNAT and could get pretty far on that alone. https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Carrier-grade_NAT reply otabdeveloper4 9 hours agorootparentprevThe big nations have already wargamed this scenario and have contingency plans in place.IMO we&#x27;ll see this happen in our lifetimes. reply CMCDragonkai 8 hours agorootparentCould starlink be a way to maintain global connectivity in the face of government control? Would they try to jam satellite connections? reply p1mrx 7 hours agorootparentIt&#x27;s harder to jam Starlink than GPS because the signals are directional, but if a government doesn&#x27;t want you using it, they can just make the dishes illegal and throw you in prison for having one. reply otabdeveloper4 7 hours agorootparentprevStarlink is under USA jusrisdiction, as far as I know. I&#x27;m pretty sure there&#x27;s no concept of \"international waters\" for communication satellites. reply yMMe2WYE_D 6 hours agorootparentAFAIK Most of the traffic goes: subscriber(single) satellitelocal base station. And the latter operates under the rules of given country. reply_JamesA_ 7 hours agorootparentprevIs there a way to get a &#x2F;24 block that I own and has been unused since the mid 90&#x27;s routed without signing a new contract and paying the new ARIN fees? reply kxrm 6 hours agorootparentYou&#x27;d be under the LSRA fee schedule.https:&#x2F;&#x2F;www.arin.net&#x2F;resources&#x2F;fees&#x2F;fee_schedule&#x2F;#legacy-reg...So you won&#x27;t be subject to the new fee structure.If you want to route then you will need an ASN and an ISP willing to announce them. So long as you are up on your LSRA dues I don&#x27;t see how you won&#x27;t be able to utilize them. reply greyface- 17 minutes agorootparentYou don&#x27;t need to sign an LRSA to use the prefix; there are some legacy holdouts still using their original prefixes without any agreement or fees with ARIN. Signing an LRSA will give you access to ARIN IRR&#x2F;RPKI&#x2F;rDNS&#x2F;etc services, which can be quite useful, though. reply cyberax 7 hours agorootparentprevYou can not \"own\" a &#x2F;24 block. And if your membership lapses, then your blocks are returned to the general pool.It&#x27;s possible that your block is a part of a legacy allocation, they are governed differently. reply greyface- 20 minutes agorootparent> block that I own and has been unused since the mid 90&#x27;sThis timeline suggests that it&#x27;s still a legacy allocation. The new governance structure does not apply unless you sign an RSA or LRSA agreement with ARIN. reply candiddevmike 14 hours agoparentprev1. You most likely can&#x27;t. You typically need to prove to a numbering authority that you need that many IPs (minimum &#x2F;24) for X reason and you will be multihomed (connected to two+ ISPs) by Y date.2. You are assigned a BGP Autonomous System Number (ASN) as part of the process. The IPs are assigned to your ASN.3. You sign a peering contract with ISPs and peer with them using BGP on your router. You use your ASN to announce your block to have traffic routed to&#x2F;from your router.One of the tragedies of IPv6, IMO, is not having a better&#x2F;streamlined process for end users to get allocations without all the red tape. There&#x27;s tons of space, let&#x27;s pretend it&#x27;s the 90s and give away IP blocks to whoever asks. Either require ISPs to give static allocations or make it easier for getting a personal block. No, prefix delegation is not good enough. reply Veliladon 13 hours agorootparent>One of the tragedies of IPv6, IMO, is not having a better&#x2F;streamlined process for end users to get allocations without all the red tape. There&#x27;s tons of space, let&#x27;s pretend it&#x27;s the 90s and give away IP blocks to whoever asks. Either require ISPs to give static allocations or make it easier for getting a personal block. No, prefix delegation is not good enough.This is by design. If we let arbitrary routings of &#x2F;64 blocks pollute the global routing table shit is going to go sideways as the rest of the net scales up and up. We made that mistake with IPv4 and the only reason our routers haven&#x27;t gone thermonuclear keeping up with the announced routes is we literally ran out of address space.We&#x27;re not going to get the IPv6 equivalent of IPv4 &#x2F;24s announced ever again. While minimum prefix lengths aren&#x27;t hard enforced (yet), unless you have the means&#x2F;reason to be multihomed using &#x2F;48s you&#x27;re pretty much going to be under the hierarchical routing of your transport or last mile provider. reply minimaul 13 hours agorootparentprevIt&#x27;s a little tricky - the more unique v6 allocations we have, the more complex routing gets, and the more resources it needs.Having a ton of people&#x2F;businesses with their own announced and unaggregatable &#x2F;48s would add a lot of entries to routing tables. reply toast0 13 hours agorootparentprev> 1. You most likely can&#x27;t. You typically need to prove to a numbering authority that you need that many IPs (minimum &#x2F;24) for X reason and you will be multihomed (connected to two+ ISPs) by Y date.If you&#x27;re asking for a minimum sized range, you don&#x27;t have to justify more than one ip. It&#x27;s not super hard to find somewhere where you can be multihomed either, although it&#x27;s unlikely to be at your home. (Maybe ask isn&#x27;t exactly the right verb, assuming ARIN&#x2F;RIPE are out of addresses, you&#x27;re asking for them to process a transfer that you paid&#x2F;will pay the current responsible party for) reply namibj 13 hours agorootparentprevPrefix delegation naturally follows physical hierarchy, keeping routing tables compact.Mandating something like a static &#x2F;56 (physical location locked) to be available at no extra cost if the customer asks for it, would work fine, though. I&#x27;d even accept requiring this only for contracts that allow more than one customer device to access the Internet simultaneously. Yes, a phone plan with two SIMs on one contract would already trigger this. reply fanf2 13 hours agorootparentprevThere is still some anxiety about the size of the global routing table. Handing out IPv6 prefixes for free would make the growth much harder to control. (Not that there is much control beyond RIR membership fees.)Also, there is no organization that can require anything of an ISP’s addressing plan. The IETF and the RIRs are associations, not governing bodies. reply tialaramex 12 hours agorootparentprev> Either require ISPs to give static allocationsJust buy service which does what you actually want - rather than insisting it should be mandatory which means everybody has to pay for it. I have static allocations (both IPv6 and, very small, IPv4) because I care. Most people don&#x27;t care. reply TheHappyOddish 14 hours agoparentprevSpeak to the local RIR[1]. They have varying requirements, but broa",
    "originSummary": [
      "Amazon Web Services (AWS) has increased its number of IPv4 addresses by 27 million, resulting in a total of 128 million; it makes their IPv4 estate approximately worth $4.5 billion, a sizable increase from 3 years ago.",
      "Notably, each IPv4 address is estimated to be worth $35 due to the increasing scarcity and cost of IPv4 addresses.",
      "AWS has planned to charge customers for IPv4 addresses at a rate of $0.005 per IP per hour, which could generate an estimated annual revenue of $500 million to $1 billion."
    ],
    "commentSummary": [
      "The post discusses the difficulties and frustrations encountered in transitioning from IPv4 to IPv6, including concerns about compatibility, network upgrades, and slower than expected adoption by ISPs.",
      "There are debates about alternative solutions, like extending IPv4 or creating a new version, and criticisms about the complexity and usability of IPv6 addresses, as well as the lack of economic motivation for migration.",
      "The post also highlights the shortage of IPv4 addresses and the potential for anti-competitive behavior from key participants, suggesting a need for a smoother conversion process and a more user-friendly solution."
    ],
    "points": 212,
    "commentCount": 334,
    "retryCount": 0,
    "time": 1694988059
  },
  {
    "id": 37548975,
    "title": "Homebrew Website Club",
    "originLink": "https://indieweb.org/Homebrew_Website_Club",
    "originBody": "Homebrew Website Club Jump to navigation Jump to search Homebrew Website Club is a growing world-wide network of meetups for everyone who wants to take back their web experience from social media silos, and own their online identities & content, or just want support with blogging! Want to blog more but struggling with*: ideas momentum confidence writing skills Join a gathering of like-minded people and get friendly support with writing, creating, and anything to do with using and improving your own website! * via Amy Hupe You can also join our chat to talk with the community between meetings! 💬 Meetings Meetings take place every other Wednesday right after work in most cities. If it collides with an official holiday in your country, simply skip that one and resume two weeks later as usual. See https://events.indieweb.org for specific dates/locations/URLs of upcoming meetings, or this handy summary of upcoming/past dates: Homebrew Website ClubEvents are listed on events.indieweb.org as of 2020-01 2023 01-04 🎡🌎 • 01-11🌎 • 01-18 🎡🌎 • 01-25 🌎 • 02-01 🎡🌎 • 02-08 🌎 • 02-15 🎡🌎 • 02-22 🌎 • 03-01 🎡🌎 • 03-08 🎡🌎 • 03-15 🎡🌎 • 03-22 🎡🌎 • 03-29 🎡🌎 • 04-05 🎡 • 04-12 🎡🌎 • 04-15 🌌 • 04-19 🎡 • 04-26 🎡🌎 • 05-03 🎡 • 05-10 🎡🌎 • 05-17 🎡 • 05-20 🌌 • 05-24 🎡🌎 • 05-31 🎡🌎 • 06-07 🎡 • 06-14 🎡🌎 • 06-21 🎡 • 06-28 🎡🌎 • 07-01 🌌 • 07-05 🎡 • 07-12 🎡🌎 • 07-19 🎡 • 07-26 🎡🌎 • 08-02 🎡 • 08-09 🎡🌎 • 08-16 🎡🌎 • 08-23 🎡🌎 • 08-26 🌌 • 09-06 🎡🌎 • 09-13 🎡🌎 2022 01-05 🌎 • 01-12 🎡🌎 • 01-19 🌎 • 01-26 🎡🌎 • 02-02 🌎 • 02-09 🎡🌎 • 02-16 🌎 • 02-23 🎡🌎 • 03-02 🌎 • 03-09 🎡🌎 • 03-16 🌎 • 03-23 🎡🌎 • 03-30 🌎 • 04-06 🎡🌎 • 04-13 🌎 • 04-20 🎡🌎 • 04-27 🌎 • 05-04 🧭 • 05-11 🌎 • 05-15 🌐 • 05-18 🎡🌎 • 05-25 🌎 • 06-01 🧭 • 06-08 🌎 • 06-15 🎡🌎 • 06-18 🌌 • 06-22 🌎 • 06-29 🎡🌎 • 07-06 🌎 • 07-13 🌎 • 07-20 🌎 • 07-27 🌎 • 08-03 🌎 • 08-10 🎡🌎 • 08-17 🌎 • 08-24 🎡🌎 • 08-31 🌎 • 09-07 🎡🌎 • 09-14 🌎 • 09-21 🎡🌎 • 09-28 🌎 • 10-02 🌌 • 10-05 🎡🌎 • 10-12 🌎 • 10-19 🎡🌎 • 10-26 🌎 • 11-02 🎡🌎 • 11-09 🌎 • 11-16 🎡🌎 • 11-23 🌎 • 11-30 🎡🌎 • 12-07 🌎 • 12-14 🎡🌎 • 12-21 🎡🌎 • 12-28 🌎 2021 01-06 • 01-13 • 01-20 • 01-27 • 02-03 🌎 • 02-10 • 02-17 • 02-24 🌎 • 03-03 🌎 • 03-10 • 03-17 • 03-24 🌎 • 03-31 • 04-07 • 04-14 • 04-21 • 04-28 • 05-05 • 05-12 • 05-19 • 05-26 • 06-02 • 06-09 • 06-16 • 06-23 • 06-30 • 07-07 • 07-14 • 07-21 • 07-21 • 07-28 • 08-04 • 08-11 🌎 • 08-18 🎡🌎 • 08-25 🌎 • 09-01 🌎 • 09-15 🎡🌎 • 09-22 🌎 • 09-29 🎡🌎 • 10-06 🌎 • 10-13 🎡🌎 • 10-20 🌎 • 10-27 🎡🌎 • 11-03 🌎 • 11-10 🎡🌎 • 11-17 🌎 • 11-24 🌎 • 12-01 🌎 • 12-08 🎡🌎 • 12-15 🌎 • 12-22 🌎 • 12-29 🌎 2020 2020-01 • 2020-02 • 2020-03 • 2020-04 • 2020-05 • 2020-06 • 06-17 🌴 • 2020-07 • … • 08-05🌴 • 08-12🌴 • 08-19🌴 • 08-26🌴 • 09-02🌴 • 09-09🌴 • 09-16🌴 • 09-23🌴 • 09-30🌴 • 10-07🌴 • 10-14🌴 • 10-21🌎 • 10-28🌎 • … • 12-02🌎 • 12-09🌎 • 12-16🌎 • 12-23🌎 • 12-30🌎 2019 12-22 • 12-11 • 12-04 • 11-27 • 11-21 • 11-16 • 11-13 • 11-06&07 • 10-30&31 • 10-16&19 • 10-02 • 09-26 • 09-18&19 • 09-11&12 • 09-04&05 • 09-02 • 08-28 • 08-21 • 08-19 • 08-15 • 08-07&08 • 08-05 • 08-01 • 07-24&25 • 07-22 • 07-18 • 07-10&11&13 • 07-03&04 • 06-26&27 • 06-24 • 06-20 • 06-12&13 • 06-10 • 06-09 • 06-06 • 05-29&30 • 05-27 • 05-23 • 05-15 • 05-09 • 05-01&02 • 04-29 • 04-25 • 04-17&18 • 04-15 • 04-11 • 04-03&04 • 04-01 • 03-28 • 03-20&21 • 03-06 • 02-27 • 02-20 • 02-06 • 01-23 • 01-09 2018 12-26 • 12-12&11 • 11-28&27 • 11-14&11-13 • 10-31 • 10-17&16 • 10-10&09 • 10-03 • 09-19&18 • 09-05 • 08-22 • 08-08 • 08-01 • 07-25 • 07-11 • 07-04 • 06-27 • 06-13 • 05-30&29 • 05-23 • 05-16&15 • 05-02&01 • 04-18 • 04-11 • 04-10 • 04-04 • 03-27 • 03-21 • 03-14 • 03-07 • 02-21&20 • 02-14 • 02-07&06 • 01-24&23 • 01-11&10&09 2017 12-27 • 12-14&13&12 • 11-30&29 • 11-22 • 11-16&15 • 11-01 • 10-19&18 • 10-05&04 • 09-21&20 • 09-13 • 09-07&06 • 08-23 • 08-15 • 08-09&08 • 08-02 • 07-26&25 • 07-12&11 • 06-28 • 06-20 • 06-14 • 06-07&06 • 05-31 • 05-17 • 05-10 • 05-03 • 04-26 • 04-19 • 04-12 • 04-05 • 03-22 • 03-08 • 03-02&01 • 02-22 • 02-14 • 02-08 • 01-25 • 01-11 2016 12-28 • 12-21 • 12-14 • 12-07 • 11-30 • 11-23 • 11-16 • 11-09 • 11-02 • 10-26 • 10-19 • 10-05 • 09-21 • 09-14 • 09-07 • 08-24 • 08-17 • 08-10 • 07-27 • 07-13 • 07-06 • 06-29 • 06-15 • 06-08 • 06-01 • 05-25 • 05-18 • 05-11 • 05-04 • 04-27 • 04-20 • 04-12 • 04-06 • 03-23 • 03-09 • 02-24 • 02-10&09 • 02-02 • 01-27 • 01-13 2015 12-30 • 12-16 • 12-08 • 12-02&01 • 11-24 • 11-18&17 • 11-10 • 11-04&03 • 10-21&20 • 10-13 • 10-07&06 • 09-24&23 • 09-17 • 09-10&09 • 09-03 • 08-27&26 • 08-20 • 08-13&12 • 07-29 • 07-15 • 07-01 • 06-17 • 06-03 • 05-20 • 05-06 • 04-22 • 04-08 • 03-25 • 03-11 • 02-25 • 02-11 • 02-07-ko • 01-28 • 01-14 2014 12-17 • 12-03 • 11-19 • 11-05 • 10-22 • 10-08 • 10-05-par • 09-24 • 09-10 • 08-27 • 08-13 • 07-30 • 07-16 • 07-02 • 06-18 • 06-04 • 05-21 • 05-07 • 04-23 • 04-09 • 03-26 • 03-19 • 03-12 • 02-26 • 02-12 • 01-29 • 01-15 2013 12-18 • 12-04 • 11-20 Contents 1 Meetings 1.1 Regular Meetings 1.1.1 🌎 Americas / Pacific Online 1.1.2 🎡 London / Europe Online 1.2 Popups 1.2.1 no recent popups 1.3 Cities getting started 1.3.1 no cities getting started in past six months 1.4 Needs restarting 1.4.1 🇪🇺 Virtual HWC European time 1.4.2 More cities with interest 1.5 Past Meetings 1.5.1 Accra 1.5.2 🌮 Austin 1.5.3 🦀 Baltimore 1.5.4 Barnsley 1.5.5 Bellingham 1.5.6 Berlin 1.5.7 Birmingham 1.5.8 🎪 Brighton 1.5.9 Brussels 1.5.10 Chicago 1.5.11 Detroit 1.5.12 Edinburgh 1.5.13 Edmonton 1.5.14 Fort Collins 1.5.15 Frederick 1.5.16 Gent 1.5.17 🏖 Goa 1.5.18 Gothenborg 1.5.19 Guadalajara 1.5.20 🇮🇳 Indian Standard Time Virtual HWC 1.5.21 🔺 Karlsruhe 1.5.22 Los Angeles 1.5.23 Madrid 1.5.24 Malmo 1.5.25 Minneapolis 1.5.26 Montreal 1.5.27 Mountain View 1.5.28 The Netherlands 1.5.29 🗽 New York City 1.5.30 Norman, OK 1.5.31 🏹 Nottingham 1.5.32 🏰 Nuremberg 1.5.33 Portland 1.5.34 Saint Petersburg, Russia 1.5.35 🏄 San Diego 1.5.36 🌁 San Francisco 1.5.37 Seattle 1.5.38 🏗Teesside 1.5.39 Vancouver BC 1.5.40 Washington DC 1.5.41 Wellington 2 How to Organize Virtual Events 2.1 Beforehand 2.2 On the day of the meeting 2.2.1 Setting up the meeting room 2.3 Running the event 2.4 After the event 3 How to Organize 3.1 Beforehand 3.2 Optional Domain Name 3.2.1 Optional POSSE Events 3.3 Setup 3.4 Quiet Writing Hour 3.5 HWC Broadcast 3.6 HWC Group Photo 3.7 HWC Peer to Peer 3.8 HWC Group Photo 2 3.9 HWC Clean-up and close 3.10 HWC Immediately after 4 Planning Questions 4.1 Finding a venue 4.2 Adding to the wiki 4.3 Promoting an HWC event 4.4 Promoting Regular HWC Meetups 4.4.1 Examples of HWC sites and social media accounts 4.5 What to bring 4.6 How to wrap up 4.7 Ask Organizers 5 History 6 Logos and Graphics 7 Brainstorming 7.1 Online Format 7.1.1 Suggested Agenda 7.2 Appearance refresh or new site 8 Archived 8.1 Past Description 8.2 HWC 2016 9 See Also Regular Meetings Just show up. These Homebrew Website Club chapters are well established and meet regularly at least once a month (often fortnightly AKA bi-weekly). If it has been more than a month since a city has met, move them down to popups until a regular pattern is re-established. Alphabetical by city/region, see each city for regularity. 🌎 Americas / Pacific Online AKA Virtual HWC North American Time Hosted online weekly Organizer(s): Chris Aldrich, David Shanske First meeting: ???? (maybe in 2020 soon after lockdown?) Venue: see the Zoom link on https://events.indieweb.org/ events 🎡 London / Europe Online Hosted online every two weeks Formerly hosted in person in London, ENGLAND - monthly Current organiser(s): Mark Sutherland, capjamesg Former organiser(s): Ana Rodrigues, Calum Ryan, Neil Mather First meeting 2016-06-15 Popups These cities have held popup Hombrew Website Club meet-ups at least a couple of times recently, and are working hard to get established! After three consecutive months of successful regular meetings, popups get bumped to Regular meetings! Or if they do not happen for three months, bump them back down to Getting Started. no recent popups Blame the pandemic. Cities getting started Speak up and help make another meetup happen in your city! These cities have organizers (can always use more), have (re)started with at least one meeting (hopefully with a photo posted), and would love to hear from you to help make more happen. If it's been more than six months since a meet-up or organizers have all moved to other cities, move a city to \"Up-and-coming or needs restarting\" in the next section. Ordered alphabetically: no cities getting started in past six months Needs restarting Step up and make a meetup happen. These cities have one or more organizers interested in helping making a meetup happen and are up for grabs. Step up, contact the folks below, and make a meetup happen! At your first (or first in at least six months) meet-up be sure to take and post photos, then link to them and bump up your city to Cities getting started above. 🇪🇺 Virtual HWC European time Online - fortnightly Organizer(s): Martijn van der Ven First meeting: 2017-05-31 Meetings on Mumble, other platforms sometimes under consideration. More cities with interest Interested in helping start a Homebrew Website Club? Find or add your city and name below! Sorted alphabetically by city. Brisbane, Australia - Malcolm Blaney is interested, has an office walking distance from train. Cincinnati, OH - Dave Menninger is interested in meeting up Durham, NC - Kimberly Hirsh is interested in meeting up in Durham, NC. Glasgow, Scotland - Donald McIntosh is interested in meeting up. Melbourne, Australia - vishae is interested in meeting up Rochester, MN - David Williamson is interested in organizing a meetup. Sarasota, FL – TheDavidJohnson is interested in organizing a meetup in Sarasota, FL Savannah, GA - User:Lawver.net is interested in organizing! St. Louis, MO – Mark87 is interested in meeting up in St. Louis, MO. Vienna, Austria - Rosemary Orchard is interested in attending/organising something. Düsseldorf, Germany - Johan Bové is interested in co-organizing. Looking into location options and others to help out. Athens, Greece - User:Heracl.es is interested in co-organizing. Looking into interested homebrewers for setting up a community. ... add your city and your name here if you're interested in co-organizing a Homebrew Website Club in your city. Past Meetings These cities used to have meetings and could totally use a new space(s) and/or another/new co-organizer(s) to step and make one happen! Past co-organizers are more than happy to help connect you to the existing community, so definitely reach out! Accra Accra, Ghana Organizer(s): Greg McVerry, ...? First meeting (? people) [need permalink to event / post / photo] Recent meeting 2019-02-26 🌮 Austin Austin, TX - first Wednesday of the month Organizers: Tom Brown, Manton Reece First meeting: 2017-09-06 Venue: Mozart's Coffee 🦀 Baltimore Baltimore, MD - monthly or sometimes biweekly! Organizer(s): Marty McGuire, Jonathan Prozzi First meeting 2016-09-21 We meet at least once per month. We aim to meet fortnightly to match the usual HWC schedule, but sometimes skip or move a week to avoid conflicts with our venue. Venue is consistently: Digital Harbor Foundation Tech Center, 1045 Light St. Baltimore, MD http://www.digitalharbor.org/ Barnsley Barnsley, UK Organizer(s): Craig Burgess (Europe/London) (@craigburgess) Every third Thursday per https://getdoingthings.com/homebrew-website-club-barnsley-1/ We're just getting off the ground with our first event, July 18 2019 Bellingham Bellingham, WA - fortnightly Organizer(s): gRegor Morrill First meeting 2016-06-15 Ended 2017-10 Venue was consistently: The Foundry, 1000 F St., Bellingham, WA, http://bellinghamfoundry.com Waiting for: need new local organizer(s)! Berlin Berlin, Germany - fortnightly on thursdays (since 2017-09) Organizer(s): Florian Weil, Sebastian Greger, (Sven Knebel – only occasionally in Berlin nowadays) First meeting 2017-02-08 since 2017-10-05 regular location at *IN-Berlin, Lehrter Str. 53, Berlin, Germany* no regular activity in 2018 for now :/ Birmingham Main article: homebrewbrum Birmingham, ENGLAND, last Wednesday of the month Organizer(s): Dave Redfern, Marc Jenkins, Paul Tibbetts First meeting 2016-10-05 2017-10-11 We haven't had a meetup for a few months now and none planned at the mo. 🎪 Brighton Brighton, ENGLAND - weekly Organiser: Jeremy Keith Every Thursday from 6pm to 7:30pm (unless Jeremy is out of town). Venue is consistently: Clearleft, 68 Middle Street, Brighton BN1 1AL https://clearleft.com http://68middle.st Brussels Brussels, Belgium Organizer(s): RMendes & NCollig First meeting on 2017-03-02 HWC Brussels Twitter: https://twitter.com/HwcBrussels @hwcbrussels (appears to be suspended as of 2018-10-04) Chicago Chicago, IL Organizer(s) formerly: Kartik Prabhu, and gRegor Morrill First meeting 2014-04-09 Waiting for: need new local organizer(s)! Detroit Detroit, MI Organizer: Les Orchard First meeting (attempt) 2016-03-23 ... Edinburgh Edinburgh, UK weekly Organizer(s): Harry Reeder First meeting 2015-08-13 (Thursday) Meets Tuesday nights. Met nearly every Tuesday night during 2015 from 2015-10 onward, and every Thursday night before that since 2015-08-13! Edmonton Edmonton, AB, Canada Organizer: kongaloosh First meeting 2017-06-06 and [1] Most recently ... ? Fort Collins Fort Collins, CO Organizer(s): Isabel Forester First meeting 2017-05-03 Waiting for: a space to be used regularly Facebook Group (for planning and events): Homebrew Website Club Fort Collins Most recently ... ? Frederick Frederick, MD monthly Organizer(s): Eddie Hinkle First meeting on 2017-07-26, on a hiatus as of 2018-03-01. Some lessons learned: having a co-organizer is helpful for days when there are low attendance. Future plans: potential rebranding to IndieWeb Meetup vs. HWC to broaden external appeal, potentially find a co-organizer, etc. Considering restarting in the first quarter of 2019. Gent Gent, Belgium Organizer(s): Kristof De Jaeger HWC Gent Twitter: @HWCGent 🏖 Goa Goa, India Organizer(s): prtksxna First meeting: 2019-07-24 Starting with support of the WordPress group. Gothenborg Göteborg, Sweden monthly Organizer(s): Jeena Paradies First Meeting 2015-07-29 Meets ~once a month (plan: every second Wednesday) Guadalajara Guadalajara, Mexico Organizer(s): Ken Bauer First meeting was a success (7 people) Mar 20, 2019 Second meeting was also a success and the third was small New site created specifically for the club On hiatus, working on next meeting to start during the upcoming semester on campus that starts in August. We plan to combine this club meeting with topics on use of open source software to give a broader draw and try to grow numbers. Please help to invite others! 🇮🇳 Indian Standard Time Virtual HWC Online Organizer(s): Chaitanya First meeting: 9 September 2020 Upcoming meeting: 30 September 2020 Platform: Zoom 🔺 Karlsruhe Karlsruhe (monthly — see https://github.com/hwc-ka/dates for upcoming) Organizer: Daniel Ehniss, Matthias Pfefferle First meeting (attempt) 2019-01-23 Recently: 2019-07-10, 2019-06-26, 2019-05-15, 2019-04-17 Need help with outreach, etc. Los Angeles Los Angeles, California (Santa Monica, Downtown, Pasadena) Organizer(s): Chris Aldrich, Angelo Gladding, Shane Becker First meeting 2016-01-13 Madrid Madrid, Spain Organizer(s): Grant Richmond First meeting: 2019-06-12 Malmo Malmö, Sweden monthly Organizer(s): Pelle Wessman, Emil Björklund First Meeting 2015-08-26 Meets ~once a month (plan: Tuesday once a month) Minneapolis Minneapolis, MN Organizer(s): Nicole Tollefson - has moved First meeting 2014-06-18 Waiting for: need new local organizer(s)! Montreal Montréal, QC Canada Organizer(s): Robin Millette First meeting 2015-02-11 Mountain View Mountain View, CA Organizer(s): Tantek Çelik, Emma, (looking for more!) First meeting: 2018-10-10 Need help with outreach, etc. The Netherlands The Netherlands 🎪 Moving Circus Schiphol Airport Plaza: HWC will happen there when international travellers show an interest. Nijmegen and Utrecht are discussed as well. If LA can have just the one HWC, so can the Netherlands. [2] [3] Organizers: Sebastiaan Andeweg, Peter Molnar First meeting 2016-11-30 Meets when 2+ people shows interest Most recently ... ? 🗽 New York City New York City, NY/Metropolitan Area (roughly monthly) Organizer(s): Marty McGuire (anyone else actively helping with date & venue planning, wikifying, photgraphing?) Emeritus organizers: David Shanske, Zachary Donovan First meeting 2015-04-22 Recently: 2019-07-13, 2019-06-12, 2019-05-29, 2019-05-01, 2019-04-17 I am interested in participating - Larry Kooper Norman, OK Organizer(s): Sarah Clayton Pugachev @sclayton29 Meets the first Wednesday of each month [4] First meeting: 2018-08-01 🏹 Nottingham Nottingham, England - fortnightly Organizer(s): Jamie Tanna First Meeting [2019-03-20] Venue is consistently: Ludorati Cafe 72 Maid Marian Way Nottingham United Kingdom NG1 6BJ 🏰 Nuremberg Nürnberg, Germany - was fortnightly, likely monthly Organizer(s): Joschi Kuphal First Meeting 2016-05-25 Recently: 2019-08-07 Venue is consistently: tollwerk, Klingenhofstraße 5, Nürnberg, DE Emeritus organizers: Julie Anne Noying (founding co-organizer) Portland Portland, OR Organizers: Aaron Parecki Emeritus organizers: Bret Comnes, Crystal, Dietrich Ayala First meeting 2013-12-04 Was first Wednesday of the month (current schedule) Venue was consistently: DreamHost, 621 Southwest Morrison St, 14th floor, Portland Recently: 2019-06-26, 2018-09-05 Saint Petersburg, Russia Organizers: User:Marinintim.com First meeting: [5] 🏄 San Diego San Diego, CA Organizers: gRegor Morrill, Joe Crawford First meeting: 2019-12-11 🌁 San Francisco San Francisco, CA - fortnightly Organizers: Tantek Çelik (would like a co-organizer for 2019!) First meeting 2013-11-20 Emeritus organizers: Stacey De Polo, Ben Werdmüller, Ryan Barrett, Kevin Marks, Kyle Mahan Venue is consistently: Mozilla San Francisco, 2 Harrison St. (at Embarcadero), 1st floor open area, San Francisco, CA Seattle Seattle, WA Organizer: dougbeal Next meeting (attempt) 2018-07-25 Organizer: funwhilelost First meeting (attempt) 2017-04-19 Need help with venue, outreach, etc. 🏗Teesside Teesside, UK Organizer(s): Kevin Marks (@kevinmarks) Every other Wednesday if I'm not in London Just getting off the ground with our first event, August 7 2019 Vancouver BC Organizers: Boris Mann @bmann, Pat Dryburgh (micro.blog @pat) First meeting: 2018-10-09 IndieWeb Meetup! Second meeting: 2018-10-24 IndieWeb Meetup! Washington DC Washington, DC Organizer(s): Jason Garber First meeting 2016-02-10 Waiting for: need new space! Wellington Wellington, New Zealand Organizer(s): Timo Reitnauer First meeting on Jul 5, 2017 [6] Homebrew Website Club Wellington Twitter: @treitnauer Most recently ... ? How to Organize Virtual Events Organizers, use the following to keep your Virtual Homebrew Website Club meetups on time and on track. Remember to take and post a photo to the event page! Beforehand Post an event on https://events.indieweb.org/ You can reference or even clone a recent virtual HWC event as a starting point If this is the first time you are hosting your meetup, you will need to write your own description for the event. Feel free to use existing pages on the events page as inspiration and to understand the format event pages typically use. Include a link to a unique etherpad for the event using YYYY-MM-DD format and a short description, e.g. https://etherpad.indieweb.org/2023-01-18-hwc-pacific Use one of the HWC Logos and Graphics for the event image. The retro image has been a popular one the last several years. Invite everyone who attended previous Homebrew Website Club meetups in the event's timezone, those you've invited before, and any friends you know who may be interested in having their own website, upgrading their blog, etc. Use invitation posts Directly message people 1:1 (Optional) Announce your meetup on social media and/or your personal website to get the word out about the event. On the day of the meeting Setting up the meeting room If you are using the IndieWeb Zoom account, familiarize yourself with Zoom - Tips for Hosts Access to the IndieWeb Zoom account can be requested in the #indieweb-chat channel Screen sharing should be set to \"Only Host\". The host can share their screen to show people's sites during introductions. You may also be able to designate a co-host to handle screen sharing. Start the Zoom meeting 10 minutes before the scheduled start time. The event listing on https://events.indieweb.org will automatically update to include a \"Live Now\" link that attendees can click to join. If you are using a different video service, you will need to update the event listing on https://events.indieweb.org with the link to join. Post a message in the #indieweb channel that the meeting is starting Running the event Welcome Direct attendees to https://indieweb.org/discuss for chat options; encourage notes and questions in the main channel instead of in Zoom chat Point out the code of conduct at https://indieweb.org/coc Direct attendees to the event's Etherpad for notes Introductions Introduce yourself and your site following HWC Broadcast, then offer each attendee to introduce themselves. Note: attendee videos don't appear in the same order on everyone's screen, so a host should select the order to avoid confusion. Group photo: Explain you're taking a group photo (screenshot) that will appear in the event page, This Week in the IndieWeb newsletter, and the @indiewebcamp Twitter post. Allow people to turn off their video if they prefer to not be in the photo. In Zoom, choose the option \"Hide Non-video Participants\" before taking the screenshot. Main meetup: This is where the format can vary. Some approaches so far: Ask what topics people are interested in discussing, noting them in chat. Then attendees can vote on their preferred priority in chat. Set a timebox for each topic then proceed through each topic. Looser, general discussion without timeboxes A quiet writing period followed by discussion Group photo: if more people joined since the first photo, consider taking another photo If you are meeting regularly, announce when the next event will be After the event Upload the photo to the event permalink on https://events.indieweb.org Set the alt text to something describing the photo, including attendee's names as displayed in the screenshot. Example: \"Zoom grid of attendees. Top left to bottom right: name1, name2, name3, and name4\" Archive the etherpad notes on the wiki, see Cali for bot commands you can use in chat to do this Once you have verified the wiki archive of the notes, edit the event description and change the link to the Etherpad to link to the wiki archive of the notes instead. Update the Homebrew Website Club template that display a list of meetups by year Add a link to the events site date archive, e.g. [https://events.indieweb.org/2022/09/14 09-14] Add an emoji link to the wiki page that was created when archiving the etherpad, e.g. [[events/2022-09-14-hwc-pacific|🌎]] Refer to cities for emojis we've used for your city/region Combine notes from the broadcast portion of the meetup into a post (Optionally) Syndicate the post to IndieNews Sign out of the organiser's Zoom account (if used) If you are meeting regularly, set up the next event on https://events.indieweb.org You can clone the current event by selecting the \"Edit\" menu at the top right and selecting \"Clone Event\". Don't forget to update dates and links in the new event's description. How to Organize Organizers, use the following to keep your in-person Homebrew Website Club meetups on time and on track. Remember to take and post a photo to the event page! Beforehand Get a venue! Post an indie event for your specific meeting city (at your specific venue - if you have one, otherwise post anyway, and add venue when you get it) Link to the IndieWeb wiki page for the event, e.g. like: More information: https://indieweb.org/events/2015-05-20-homebrew-website-club Use an event image like: Retro Homebrew Website Club JPG Invite everyone who attended previous Homebrew Website Club meetups in your city, and those you've invited before, and any friends you know who may be interested in having their own website, or upgrading their blog etc. Use invitation posts Directly message people 1:1 Optional Domain Name Some cities choose to set up a website to use as well post events to the wiki https://goifnetwork.org Accra https://elmcitywebmakers.com New Haven https://www.hwclondon.co.uk London Optional POSSE Events Optionally post a POSSE event on: Upcoming.org or any other event sites that might help promote the event Facebook Duplicate a previous Facebook HWC event for your city Find the previous FB event for your city Choose \"Duplicate Event...\" from the [...] button/menu next to the top right \"Edit\" button Update references to previous date / wiki page / indie event to post to current one Optionally update or rotate event header photo Or create a new FB event (e.g. if duplicating just makes FB say Something went wrong) and: Set the event header photo to a PNG like: Retro Homebrew Website Club JPG I'm attending [7] (Note: should be updated to current logo and \"Homebrew Website Club\") or another clever variant (see previous HWC event wiki pages) Add a brief description from a recent next-hwc Link to the IndieWeb wiki page for the event, e.g. in the description like: More information: https://indieweb.org/events/2015-05-20-homebrew-website-club Link to your indie event and auth Bridgy with FB to get RSVPs! e.g. in the description Originally posted at: URL-OF-INDIE-EVENT Keywords: Technology, Digital art Setup 17:00 arrive at the venue and: make sure there is suitable seating for those that have RSVPd make or bring a sign that clearly indicates the presence of an IndieWebCamp Homebrew Website Club meeting. If you come up with a nice sign design, please upload it to the wiki! 17:20 put the sign out (or put up the IndieWebCamp logo on monitors in the space if any) save seats for at least a few of the folks who RSVPd consider setting aside a separate room or space for \"Conversation Corner\" for those that can't help but want to carry on a conversation during Quiet Writing Hour! 17:25 test your projector setup to make sure people can show demos of their sites during the \"Broadcast\" half-hour from 18:30-19:30. Quiet Writing Hour 17:30-18:30 optional Quiet Writing Hour: some locations have started hosting a quiet writing hour from 17:30-18:30 immediately before the meetup itself. HWC Broadcast 18:30-19:00: Broadcast - take turns with the microphone 1-2 minutes on: if this is your first time, introduce yourself, name, URL, what you'd like to do next with your personal site. something new you created on or for your site since the last time you attended questions about anything you're stuck on about your personal site notable indieweb / ownyourdata related press / events HWC Group Photo ~19:00: Group photo - organizers should call time on the \"broadcast\" phase and then organize and take a group photo to upload to the wiki and embed in the \"Photos\" section on the page for the specific event. This also helps break-up existing sitting patterns and gives people a chance to move around. Please be sure to: Give folks a heads-up before you take a photo, especially in virtual meetups, to give folks an opportunity to opt-out Make sure there are 2+ people who want to be in the photo, otherwise it looks like a meetup by one person with only one person which isn't really a meetup. You may omit a photo if there is only one person who wants to be in the photo. HWC Peer to Peer ~19:00-19:30: Peer-to-peer - encourage everyone to find others they heard in the previous half hour that were interested in similar topics. Help newcomers with getting started with their personal sites, and hopefully even logging into the wiki and adding themselves to the RSVP section for the meeting! HWC Group Photo 2 ~19:30: Group photo - If you forgot to take a group photo, or anyone new showed up, use the end of the peer-to-peer session as an opportunity to take another group photo. HWC Clean-up and close ~19:45-20:00: Clean-up and meeting closure - get everyone to help clean-up and leave the meeting space cleaner than they found it (BarCamp style). Encourage folks to continue discussions at a nearby pub or cafe and get food/drink too! HWC Immediately after Combine notes from the broadcast portion of the meetup into a post. (Optionally) Syndicate the post to IndieNews Add to the event page on the wiki: At least one group photo from your meeting. Any other wrap-up posts with notes from the meeting. Planning Questions In rough order of when a co-organizer might encounter these: Finding a venue How do I find a venue? Check out cafes that have an environment where people are sparse and the cafe is ok with hanging out with laptops for an hour or two. What kind of food venue is works well? Cafe style is best, where you order at the counter separately, and can come and go independently. Wifi is a nice to have (or get someone to bring a mifi hotspot and share) Power outlets are also a nice to have Adding to the wiki How should a co-organizer add a HWC-event to the wiki? We are now using https://events.indieweb.org/ for HWC-events! The easiest is to clone an existing event, e.g.: Find the most recent HWC in your city in https://events.indieweb.org/archive Click the drop-down next to \"Edit\" on the right and choose \"Clone Event\" Edit the event details to fix the date, confirm the time & venue, update the description Save! If there isn't a previous HWC event in your city, feel free to clone from another HWC event, e.g.: https://events.indieweb.org/2020/03/homebrew-website-club-san-francisco-KTBJlSw7JJKm Promoting an HWC event How do I promote an HWC event? Invite participants from previous HWC events in the same city. Invite participants from IndieWebCamps from that city There are several logos and graphics you can use in your promotions. Promoting Regular HWC Meetups Some organizers have found it useful to create a separate web and social media presence for their group. This allows potential attendees to stay up-to-date on upcoming events without needing to follow the organizers' feeds, and (potentially) allows multiple organizers to manage announcements and marketing. Examples of HWC sites and social media accounts HWC London has a standalone website and Twitter account. HWC Baltimore has a Twitter account. What to bring What should I bring to the meetup? A sign saying Homebrew Website Club might be helpful How to wrap up Make sure to take a photo and upload it to the event on events.indieweb.org! There's a convenient \"Add Photo\" option in the drop-down menu next to \"Edit\" in the top right of the event page. Let folks know you have to wrap up, and either If the venue is public like a cafe, encourage them to stay as long as they like If you’re the venue host like in an office, provide suggestions of nearby eating and drinking places where folks can continue discussions and perhaps grab a snack Ask Organizers Got more questions? Ask active community members who (help) organize (or have organized) Homebrew Website Club meetups, e.g. in IRC, or in-person at a meetup! Organizers (alphabetically by name, summary from the above lists, who has been active etc.) Aaron Parecki (PDX) Calum Ryan (London) Chris Aldrich (LA) dougbeal (Seattle) gRegor Morrill (San Diego, formerly Bellingham and Chicago) Jamie Tanna (Nottingham) Jeremy Keith (Brighton) Joschi Kuphal (Nuremberg) Marty McGuire (New York City, formerly Baltimore) Tantek Çelik (SF) ... Emeritus: Bret Comnes (PDX) Ben Werdmüller (SF) Crystal Beasley (PDX) David Shanske (NYC) Jeena Paradies (Göteborg) Jason Garber (DC) Julie Anne Noying (Nuremberg) Kartik Prabhu (Chicago) Kevin Marks (SF) Kyle Mahan (SF) Amy Guy (formerly MIT, and previously Edinburgh) Ryan Barrett (SF) Harry Reeder (Edinburgh) Emil Björklund (Malmö) Pelle Wessman (Malmö) History Tantek was inspired after attending the Homebrew Computer Club 38th Reunion and set up the first meeting 2013-11-20 in San Francisco. Homebrew Website Club meetups were founded with the same structure as the classic Homebrew Computer Club meetings, for anyone passionate about or interested in creating, improving, building, designing their own website! Logos and Graphics SVG logo 1 - based on IWC logo pre-2016 SVG logo 1 standalone SVG logo 2 - based on IWC logo pre-2016 SVG 2 standalone SVG logo 3 by Malcolm Blaney Images you can use for the featured / banner image on event pages: Mosaic of attendees from 2014 Retro Cropped version that works well for Twitter Cards as of 2019-11-08 I'm attending (Note: should be updated to current logo and \"Homebrew Website Club\") Brainstorming Online Format With the COVID-19 pandemic, IndieWeb meetups have moved to online formats. Organizers have been rotating organizing duties and are experimenting with different formats. Here are some of my notes from the last few weeks: gRegor Morrill 14:35, 8 April 2020 (PDT) Familiarize yourself with Zoom - Tips for Hosts Screen sharing should be set to \"Only Host\". The host can share their screen to show people's sites during introductions. You may also be able to designate a co-host to handle screen sharing. Welcome Direct attendees to https://indieweb.org/discuss for chat options; encourage notes and questions in the main channel instead of in Zoom chat Point out the code of conduct Introductions Introduce yourself and your site following HWC Broadcast, then offer each attendee to introduce themselves. Note: attendee videos don't appear in the same order on everyone's screen, so a host should select the order to avoid confusion. Group photo: Explain you're taking a screenshot for the event page and the weekly newsletter, allowing people to turn their video on or off as they prefer. Main meetup: This is where the format can vary. Some approaches so far: Ask what topics people are interested in discussing, noting them in chat. Then attendees can vote on their preferred priority in chat. Set a timebox for each topic then proceed through each topic. Looser, general discussion without timeboxes A quiet writing period followed by discussion Group photo: if more people joined since the first photo, consider taking another photo Suggested Agenda (via 一 Salt ♨🚲🧂🧙♂🐧🎓⚔☠ (talk) 18:00, 20 April 2020 (PDT) For ease, markdown can be cloned from this event. Pre-Meeting (~ 15 minutes) Host creates virtual space that can support a video chat for a small / medium group of people Host creates / clones and fills in details about event on https://events.indieweb.org Host familiarizes themself with whatever virtual space was selected Everyone is welcome to invite anyone who has attended previous HWC in the area or who might be interested in attending Arrive / Mingle / Unstructured Conversation (~ 5 minutes) Introductions (~ 15 minutes) Each attendee, beginning with Host, is given time to cover some / all of the following: Name, URL, what you'd like to do next with your personal site Something new you created on / for your site since the last time you attended Questions about anything you're stuck on about your personal site Notable indieweb / ownyourdata related press / events Once current speaker is done, they select who will go next until everyone has spoken Group Photo #1 (~ 2 minutes) Host announces that a screenshot is going to be taken for the event page and weekly newsletter Everyone is given some time to turn their video on / off as they prefer Lean Coffee Kanban Voting (~ 10 minutes) Everyone writes down the topic(s) that they would like to discuss as a group in #indieweb-chat Each topic is assigned a number by Host who decides when to close the call for topic submissions Everyone posts their preference via a list of numbers, ordered with highest interest first Lean Coffee Discussion (~ 1 hour) Topics are dicussed in order of group consensus Host starts a timer when discussion begins At ~10 minutes, a Roman vote (up/sideways/down) is taken to determine whether to keep on the same topic If the topic is voted up, Host adds ~5 minutes to the timer before calling for another vote If the topic is voted down, the group moves onto the next topic, this pattern repeats until time runs out Co-working Space (~ 45 minutes) Find others that might be interested in similar topics Help newcomers getting started with their personal sites Make sure everyone has RSVPed to the event Group Photo #2 (~ 2 minutes) Host announces that a screenshot is going to be taken for the event page and weekly newsletter Everyone is given some time to turn their video on / off as they prefer Closing Round (~ 10 minutes) Each attendee, beginning with Host, is given time to cover some / all of the following: Meeting evaluation / feedback / suggestions Appreciations / Goodbyes Post-Meeting (~ 10 minutes) Host uploads group photo(s) Everyone combines and uploads notes from the meetup Appearance refresh or new site Would it be helpful to do a refresh of this page as a reference, or perhaps a separate site entirely like: http://uxhappyhour.com Which has a similar list of active cities, how to start your own, but presented much more simply. Archived Past Description Are you building your own website? Indie reader? Personal publishing web app? Or some other digital magic-cloud proxy? Come on by and join a gathering of people with likeminded interests. Bring your friends who want to start a personal web site. Exchange information, swap ideas, talk shop, help work on a project... Finish that blog post you’ve been writing! Discuss difficult and/or open ended questions that you ran into! Demos of recent IndieWeb breakthroughs, share what you've gotten working! Whatever you want! See the Homebrew Website Club Newsletter Volume 1 Issue 1 for a description of the first meeting. HWC 2016 This is an archive of the HWC 2016 planning, originally found here. 2016 Homebrew Website Club start date! Should we start the 2016 HWC meetups on: 2016-01-06 only one week after 2015-12-30, but avoids having a meetup land the day before US Thanksgiving 0 Tantek Çelik ok with this. no longer any preference. 0 gRegor Morrill OK with this. Not attending an HWC currently so does not impact me. 2016-01-13 two weeks after 2015-12-30, but collides with Wednesday before US Thanksgiving thus unlikely to have much (any) participation at US venues. 0 Tantek Çelik ok with this. Based on lack of strong input on this, we are going with the default continuing every two weeks from 2015-12-30 and starting 2016 HWC with 2016-01-13. - Tantek 14:56, 23 December 2015 (PST) (updated, no objections, going with this plan. Tantek 07:51, 31 December 2015 (PST)) Getting some feedback that pages on organizing HWC hard to find, I think it maybe best to make a new page that is an HWC toolkit and port most of this content over. At top of this page put two buttons one \"Attend an HWC\" and another \"Organize an HWC\" Organizing Links General Tools web chat • online chat clients • Events Calendar • Etherpad • remote participation • Zoom IndieWebCamp Planning • Organizing • Pop-up Organizing • Volunteer • What to expect at camp • Sponsor • IndieWebCamp kit • session facilitating • hallway track • photography policy Categories Organizing • Events • Camp Schedules • Camp Sessions • Organizer Meetups • Demos Homebrew Website Club Regular Meetings • How to Organize • Suggested Agenda • Online Format • Virtual HWC • Upcoming HWCs Contributing to the wiki MediaWiki: how to edit this wiki • wikifying • relevant • definition • start a page • expand a page • features we use See Also IndieWebCamps events In the spirit: https://twitter.com/exkuchme/status/1117551276055654400 \"anyone want to attend my hackathon where we hang out in my living room and finally update our embarrassing old personal websites?\" @exkuchme April 14, 2019 Category: Organizing Navigation menu Log in Page Read View source View history Search 🎁 2022 Gift Calendar! ✨ 2023 Commitments! 📰News Podcast Video 📅 Events 👥 Next HWCs: 2023-09-20 Camps & Pop-ups: 🎪 Summit 2023 ❤ Sponsor Resources 🌈 Code of conduct 💬 Discuss 📜 Chat log 🎉 Get started ℹ FAQ 🧰 Standards ✳ Recent changes 🎲 Random page © Wiki content is CC0 Developers 🔩 building blocks 📑 projects 🗽 principles Tools What links here Related changes Special pages Printable version Permanent link Page information This page was last edited on 17 September 2023, at 16:26. Content is available under a CC0 public domain dedication unless otherwise noted. Privacy policy About IndieWeb Disclaimers Thank you to our sponsors of IndieWeb events! become a supporter!",
    "commentLink": "https://news.ycombinator.com/item?id=37548975",
    "commentBody": "Homebrew Website ClubHacker NewspastloginHomebrew Website Club (indieweb.org) 206 points by zerojames 17 hours ago| hidepastfavorite109 comments rednafi 8 hours agoLove how many of the sites listed here have brutalist low-JS design. The only thing I despise more than the new social media focused internet is, JS-heavy blogs containing one post that explains how that monstrosity was created.Then after a few months the author will rewrite the blog again with a different framework and add another entry explaining why and how they did it.Here&#x27;s mine. Built with Hugo and deployed with GitHub Pages. The whole publication process is automated with GitHub Actions and takes a few key strokes to go from an idea to execution.https:&#x2F;&#x2F;rednafi.com reply cookiengineer 5 hours agoparentHonestly I don&#x27;t think a low JS usage makes a website better. For me, I always cherished a separation of concerns approach, where I am trying to make a website work as good as possible with HTML and CSS only (including print stylesheets).JS on the other hand is for me where the fun interaction comes from, where the little details can shine through, where visualizations can be made interactive so the user can play around and really start to understand what&#x27;s going on.But yeah, I guess in the golden age of React and Angular, I also understand why people hade JS bloated websites.Here&#x27;s mine, there is multiple scavenger hunts, games and challenges hidden on the website and&#x2F;or in the code:https:&#x2F;&#x2F;cookie.engineer reply zerojames 5 hours agorootparentYour website is delightful. I love the Recipe Creator feature on your projects page.Your approach to using JS is like mine. I work with HTML and CSS first then sometimes add JS to progressively enhance a page (i.e. add dark mode, add hovercards on links, make emojis appear on different days of the year). reply hagreet 4 hours agorootparentprevI&#x27;m confused by that cookie banner. Is it a joke or not? What do you mean by \"Do you consent to sharing cookies?\" are sharing cookies a special kind of cookie or do did you just mean storing cookies? reply cookiengineer 3 hours agorootparentHaha :D yeah, it is a joke&#x2F;pun on cookie consent banners.The personal problem I have with cookie banners is that in order to _not consent_ they store a local cookie to not display the banner.And the assumption of consent is by default, which is the exact opposite behaviour of what GDPR expects you to do. Nobody can consent by default to anything, that&#x27;s the foundational basis of law in the EU.I&#x27;m not using cookies for anything on my website, so there&#x27;s no point in storing cookies. The Do Not Consent game is for people that want to play off some steam. reply doublerabbit 3 hours agorootparentprevThe worse part of those banners are that you select \"Don&#x27;t Consent\" and then still have to untick all those \"Legitimate Intent\" options. reply jofla_net 7 hours agoparentprevIsn&#x27;t it astounding how much more brittle those react-y monstrosities are?! If you look twice some toolbar will silently begin to fail loading and you&#x27;ll have to bump your browser to get the full experience back again.While back someone posted a web1.0 randomizer, everything still worked, granted its static but, its like those old 90s pages are the new time capsules.www.jofla.net reply wildrhythms 1 hour agorootparentIt&#x27;s because mindless businesstypes looked at Apple iOS over a decade ago and said \"our website [with 1&#x2F;9999999th the budget] should have the same polish and look and feel as that!\" reply rednafi 6 hours agorootparentprevI like single column multi page blogs where folks just list out a bunch of heartfelt pieces they wrote about something and fired it into the aether.This UI maximalism is like Marvel movies—tons of CGI and fancy visuals with poor storytelling. But at least writing these FE frameworks and JS evangelism pays the bill quite well. reply manuelmoreale 41 minutes agorootparent> I like single column multi page blogs where folks just list out a bunch of heartfelt pieces they wrote about something and fired it into the aether.You are my kind of internet user thenwhat&#x27;s your domain name?Lives at https:&#x2F;&#x2F;manuelmoreale.com reply zerojames 27 minutes agorootparentYour People and Blogs newsletter looks great. I just signed up! reply mbork_pl 2 hours agoparentprevHi, I&#x27;d like to come, too (virtually ofc). A few months ago I started a (very minimalist) blog [0], using a very simple blogging engine I wrote myself [1] in under 400 lines of Emacs Lisp.[0] https:&#x2F;&#x2F;crimsonelevendelightpetrichor.net&#x2F;[1] https:&#x2F;&#x2F;gitlab.com&#x2F;mbork_mbork_pl&#x2F;org-clive reply zerojames 1 hour agorootparentI love that you have written your own blog generator! I have, too, but it is in Python.Our next meetup is online on Wednesday, with events on Europe and US timezones (https:&#x2F;&#x2F;events.indieweb.org). No need to RSVP; just show up! reply mbork_pl 1 hour agorootparentThanks! I can see that it&#x27;s 7pm, but what timezone exactly? reply zerojames 1 hour agorootparentThe next meetups are: September 20, 2023 - Wed 7:00 - 9:00pm (Europe&#x2F;London) September 27, 2023 - Wed 6:00 - 8:00pm (America&#x2F;Los_Angeles)The individual event pages have timezone information (i.e. https:&#x2F;&#x2F;events.indieweb.org&#x2F;2023&#x2F;09&#x2F;homebrew-website-club-eu...). I&#x27;m not sure why the homepage doesn&#x27;t. reply bojanvidanovic 1 hour agoparentprevHi James! Love your implementation of instagram stories, I had the same idea but never got time to implement it. Looking forward joining of your meetups.BTW my website is https:&#x2F;&#x2F;bojanvidanovic.com reply pabloem 4 hours agoparentprevI love the indieweb philosophy. I&#x27;m a bit off the topic of the tech sites shared here, but I have a recipe site (mostly from around the world) with my partner. This is the domain (in spanish, sorry!): https:&#x2F;&#x2F;www.foodisea.com reply zerojames 4 hours agorootparentThis is amazing! We talked about recipes at Homebrew Website Club a few weeks ago. We asked ourselves \"how could we share our own recipes online?\"The layout of your recipes and the focus on the process without extraneous information makes me want to learn more about cooking. I have always been scared off by recipe sites. Your site looks like a great entry point for me. reply fjfaase 6 hours agoparentprevMy domain is https:&#x2F;&#x2F;www.iwriteiam.nl&#x2F; . It is just plain HTML with some JavaScript for some pages, some animations, and some functionality.I am happy with my design, which has not changed in the past twenty years, but I am open for suggestions about improvements. reply rambambram 7 hours agoparentprevHave been following your feed for a while now. Got really inspired by the HWC and started a programming meetup in my city. Also because I build https:&#x2F;&#x2F;heyhomepage.com which fits the niche. Might see some of you in Nurnberg! reply maurits 3 hours agoparentprevI&#x27;d be happy to join. I make everyday slice of life pics at https:&#x2F;&#x2F;www.maurits.chIts in dire need of new back-nend. reply qubyte 5 hours agoparentprevI used to be a regular in-person attendee here in Brighton pre-pandemic. My site is statically generated with some IndieWeb enhancements (webmentions in and out, micropub, posse, etc.) I mostly use it as a hidden log of my study sessions and for notes. A few times a year I&#x27;ll write something longer form.https:&#x2F;&#x2F;qubyte.codes&#x2F; reply matheusmoreira 12 hours agoparentprev> what&#x27;s your domain name?It matches my user name: https:&#x2F;&#x2F;matheusmoreira.com> what you are buildingJust a static site where I write stuff I think about. My idea is to write a new page every time I get the feeling I&#x27;ve written the same HN comment multiple times. Currently working on posts about ads and copyright which are topics I&#x27;ve commented on many times.> what you want to buildI want to improve the website design. Haven&#x27;t managed to create a header that doesn&#x27;t look ugly yet.> how you can build itSome time ago I made a comment here on HN recommending pugjs for static sites and writing HTML by hand:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37019175Not long afterwards I realized it has been unmaintained for years. So I forked it and took over maintenance. Deleted the parts I didn&#x27;t need. Now it&#x27;s a new variant of the language and it&#x27;s powering my own site.https:&#x2F;&#x2F;github.com&#x2F;matheusmoreira&#x2F;pugneumhttps:&#x2F;&#x2F;github.com&#x2F;matheusmoreira&#x2F;matheusmoreira.github.io reply leviathant 12 hours agoparentprevHey James - I appreciate your enthusiasm about this idea. I technically never stopped https:&#x2F;&#x2F;www.theninhotline.com, but after a few years, it was clear that social networks like Twitter and Facebook had taken over as our primary channel.About two years ago, I decided to rebuild the site (partially to get some hands-on experience building a PWA) and have basically been doing a gut-job of the frontend. I removed tracking pixels, frameworks and other javascript cruft, and brought focus to the RSS feed again. I spent time building a library of website &#x27;cover art&#x27; too, which I cycle in periodically - typically when I post a new update to the blog. There&#x27;s a lot more cleaning to do, but I&#x27;m happy to be actively iterating on the ol&#x27; website again.Oh, and my &#x27;personal site&#x27; is https:&#x2F;&#x2F;www.bitrotten.comI&#x27;m based in Philadelphia, but I&#x27;ve got a lot going on IRL compared to when I launched this site 24 years ago, and I don&#x27;t think I could do a regular meetup. But keep on fighting the good fight! reply zerojames 5 hours agorootparentI get excited about all things web :D> it was clear that social networks like Twitter and Facebook had taken over as our primary channelThere are a lot of people in the IndieWeb who care about, and are working toward, building different ways to interact on the web. I follow blogs and people with Monocle (https:&#x2F;&#x2F;monocle.p3k.io), a social reader powered by Microsub (draft spec) and Micropub (https:&#x2F;&#x2F;indieweb.org&#x2F;Micropub)I can like posts on websites in my feed then the likes show up on my bookmarks site (https:&#x2F;&#x2F;jamesg.coffee). If a site supports Webmention (https:&#x2F;&#x2F;indieweb.org&#x2F;Webmention), the author will be notified that I liked their post.I am always happy to talk about social interactions on the web -- there is so much work we can do!> I spent time building a library of website &#x27;cover art&#x27; too, which I cycle in periodically - typically when I post a new update to the blog. There&#x27;s a lot more cleaning to do, but I&#x27;m happy to be actively iterating on the ol&#x27; website again.I _love_ this!> I don&#x27;t think I could do a regular meetupWe have a lot of people who stop by for a meetup every month or two online. All interested in the web are welcome to come on whatever cadence suits you; whether you come once a week, month, or year! There is also a community chat if you prefer text at https:&#x2F;&#x2F;indieweb.org&#x2F;discuss(Apologies for all the links. As it may be noted, I get excited easily!) reply okaleniuk 4 hours agoparentprevHi James. Mine is https:&#x2F;&#x2F;wordsandbuttons.online and I&#x27;m definitely zooming in this next Wednesday. reply zerojames 4 hours agorootparentI _love_ your site!I have a question about https:&#x2F;&#x2F;wordsandbuttons.online&#x2F;lexical_differential_highligh.... What does this part mean:> Ideally, the smaller the lexical difference, the greater the color difference should be.I am fascinated by different ways to represent code visually. reply okaleniuk 3 hours agorootparentThis means that the more lexemes look alike, the more they need color difference the highlighting provides. Like &#x27;coordinate_u&#x27; and &#x27;coordiante_v&#x27; should be distinctively different, but &#x27;coordinate_u&#x27; and &#x27;GetCanvasWidth&#x27; can even share the same color. reply zerojames 3 hours agorootparentThank you! reply pard68 14 hours agoparentprevLove the colors and your graphs.Here&#x27;s mine, https:&#x2F;&#x2F;drollery.org (I have not put much effort into the mobile version, it&#x27;s a bit wonky since a recent styling update, best enjoyed on desktop). This is mostly in exercise in \"why not?\" reply zerojames 13 hours agorootparentI love all the little details on your site. The sound effects on your post list made me startled me at first then made me laugh :) reply pard68 13 hours agorootparentYa I kinda hate that. Needs work. But also I use my seven and six year olds at my litmus test, they laugh, it ships, so these things do happen.Be sure to click all the little marginalia cartoons! reply spondyl 13 hours agoparentprevI&#x27;m over at https:&#x2F;&#x2F;utf9k.net although I&#x27;ve had the same layout for long enough that I feel like it needs a bit more personality hah.The little live player widget in the top right is my pride and joy :&#x27;)https:&#x2F;&#x2F;github.com&#x2F;marcus-crane&#x2F;utf9k for the source and https:&#x2F;&#x2F;github.com&#x2F;marcus-crane&#x2F;gunslinger for the cludgy backend that powers the live player reply rednafi 6 hours agorootparentThe rough edges on the site reminds of the good ol&#x27; 90s web which I never experienced. My experience with the early web is limited to the Wayback Machine.I went with an off-the-shelf, single-column theme and build mine on similar principles of simplicity and personal touch ups: https:&#x2F;&#x2F;rednafi.com reply zerojames 5 hours agorootparentprevWow! This is so cool.P.S. I love Billions, which at the time of writing is highlighted on your homepage. reply catgirlinspace 13 hours agoparentprevmine is https:&#x2F;&#x2F;catgirlin.space ^^ been working on building a new site though over the past week :3also have seen you around a lot in the indieweb chat!! reply zerojames 13 hours agorootparentI love https:&#x2F;&#x2F;catgirlin.space&#x2F;posts&#x2F;building-an-image-splitter&#x2F;! I saw a piece of art in a gallery that was a photo, divided into pieces like this. Then, all the pieces were shuffled. It made for a really cool effect. reply rednafi 6 hours agorootparentprevLove the tone of the contents. reply hypertexthero 14 hours agoparentprevCool!I’m at https:&#x2F;&#x2F;hypertexthero.comOne suggestion I have is to consider using https:&#x2F;&#x2F;meet.jit.si&#x2F; instead of Zoom for meetings. GitHub link: https:&#x2F;&#x2F;jitsi.github.io&#x2F;handbook&#x2F; reply RealCodingOtaku 7 hours agorootparentYeah, many times I was keen to join the IndieWeb events, but I was put off by the requirement of Zoom. Jitsi would be perfect for IndieWeb as one of their principles is “owning your data”.And ooh, I&#x27;m at https:&#x2F;&#x2F;codingotaku.com reply vinc 7 hours agorootparentprevSame thing, I don&#x27;t want to use Zoom when a good open source alternative exists reply zerojames 13 hours agorootparentprevOooh! I love the cloud animation on your website! reply hypertexthero 3 hours agorootparentThank you! I learned the technique from another indie website along the way.Another thing I like about the independent, old-school web is that you can view the source code to learn things.I love the Moments of Joy series on your site, and the Written by human not by AI label! reply zerojames 3 hours agorootparentYou have made my day :)Have you viewed source on my site? You&#x27;ll see a few comments along the way. I wanted to write my site in such a way that someone new to the web could take a look and learn how the HTML works on the page.All credit for the badge goes to the wonderful https:&#x2F;&#x2F;notbyai.fyi&#x2F;. reply simonw 14 hours agoparentprevMine is https:&#x2F;&#x2F;simonwillison.net&#x2F; - I should really come along to one of these some time. reply rednafi 6 hours agorootparentI&#x27;ve been following Simon on every platform under the sun for the past few years and learned so many bits and pieces of tricks from his content.IDK how you write so many high quality stuff at such a fast pace while also doing a significant amount of OSS development work. Datasette and friends are such a joy to use.You inspired me to get started with writing three years ago and I never stopped: https:&#x2F;&#x2F;rednafi.com. reply zerojames 13 hours agorootparentprevYour website is delightful. Please do!We host a Europe and US meetup online every week and every two weeks, respectively, in addition to in person events. If there isn&#x27;t an in-person event near you, reach out in https:&#x2F;&#x2F;indieweb.org&#x2F;discuss to see if anyone would be interested in hosting one. reply hatsix 13 hours agoparentprevhttps:&#x2F;&#x2F;dusty.isit&#x27;s nothing special, but I love the domain and paths reply zerojames 5 hours agorootparentI love your site! One note: https:&#x2F;&#x2F;dusty.is&#x2F;a&#x2F;gamer doesn&#x27;t return a page. reply rednafi 6 hours agorootparentprevReminds me of https:&#x2F;&#x2F;motherfuckingwebsite.com reply hollander 13 hours agorootparentprevBut dusty.is&#x2F;a&#x2F;404&#x2F;gamer… reply yabbs 14 hours agoparentprevLove the sparkline next to number of posts. reply zerojames 13 hours agorootparentWhy thank you! I am using https:&#x2F;&#x2F;www.kryogenix.org&#x2F;days&#x2F;2012&#x2F;12&#x2F;30&#x2F;simple-svg-sparkli... for the sparklines. All credit goes to Stuart who made the SVG!The MediaWiki sparkline is powered by the same thing and https:&#x2F;&#x2F;sparkline.jamesg.blog (open source link in site footer). reply ChrisMarshallNY 12 hours agoprevThis looks like a great idea.For myself, back in the ‘90s, I was really into spending a bunch of time, hand-crafting Web sites, but these days, I just use rather lightly-modified WordPress sites.Anyone remember the famous SatireWire Cubist Web Site story[0]?[0] https:&#x2F;&#x2F;www.satirewire.com&#x2F;cubists-launch-unnavigable-web-si... reply yuhmahp 8 hours agoprevCan I join if I use Notion as a backend for my blog?The rest of my site is close to hand made: https:&#x2F;&#x2F;huy.today&#x2F; reply zerojames 6 hours agoparentAbsolutely! You can join no matter tool(s) what you use for your website, from WordPress to HTML sites to Notion sites! I&#x27;m now curious how Notion works as a back-end! reply uneekname 8 hours agoparentprevYes, it is a welcoming group that would love to learn about your site! reply codazoda 14 hours agoprevI’ve never seen this before but I think it’s right up my alley (save the part where I’m pretty introverted as far as meetups and video calls).I just went down a rabbit hole looking at the RSVP specs, h-cards (which I remember from way back but thought was dead), and comments. And, I need to go deeper! reply zerojames 14 hours agoparentWe also have a chat at https:&#x2F;&#x2F;indieweb.org&#x2F;discuss that you can join if you prefer text.h-cards are still alive and well, and are used by technologies Webmention [1] (decentralized likes, comments, bookmarks, RSVPs, and more) as a profile! My site has one :)[1]: https:&#x2F;&#x2F;indieweb.org&#x2F;Webmention reply renegat0x0 8 hours agoprevA list od blogs mentioned by hacker news, some were adres manually by me:https:&#x2F;&#x2F;github.com&#x2F;rumca-js&#x2F;Django-link-archive&#x2F;blob&#x2F;main&#x2F;an... reply nirvael 5 hours agoprevMade my website myself to showcase my AI art projects (taking some inspiration from sites like David Rudnick&#x27;s and Gwern&#x27;s). Interested to see what people think.https:&#x2F;&#x2F;www.nirvael.net reply js98 13 hours agoprevSounds like I should join! This is something I knew should exist, but hadn’t found it yet.Will sign up for one of the events, for now feel free to already look at https:&#x2F;&#x2F;Jakobs.dev reply 0x3444ac53 12 hours agoprevI made this site mainly to link to stuff I likehttp:&#x2F;&#x2F;squidleafs.xyz reply operator-name 5 hours agoparentThis is beautiful. The simple paragraph accompanied with links to \"The Dictionary of Obscure Sorrows\" so eloquently expresses what I&#x27;ve been mulling on. reply egonschiele 16 hours agoprevLooks like the Minnesota chapter is not very active, but if there are other people from the Twin Cities on here, I&#x27;d be interested in meeting up. adit.io is my blog. reply bovermyer 15 hours agoparentI&#x27;m not in the Minneapolis area anymore, but I can tell you there&#x27;s a healthy amount of interest there if you&#x27;re willing to organize it. reply velcrovan 11 hours agoparentprevI’m in Minneapolis too. I’ll hit you up on Mastodon. reply jbc8 11 hours agoprevhttp:&#x2F;&#x2F;www.jbcse.com reply ngcc_hk 15 hours agoprev [–] Not much intro. Can anyone join even if you do yours on GitHub ?I will check out the London Meetup this wed. Brighton one links to this Wordpress “challenge” https:&#x2F;&#x2F;60minute.site&#x2F; and something about etherpad https:&#x2F;&#x2F;etherpad.org&#x2F;Finally find this obvious link https:&#x2F;&#x2F;indieweb.org&#x2F;Getting_Started reply zerojames 14 hours agoparent [–] You can join irrespsective of where your site is hosted or what technologies you use! We have people who use static sites (like me!), WordPress, dynamic sites, micro.blog, and so much more in the community.I&#x27;m not sure why the Brighton one has that link. I&#x27;ll take a look. The challenge you mentioned -- build (or add to) a site in an hour -- will be hosted in two weeks (see events.indieweb.org for more); last time we had someone make a bookmarks page, a PNG header reader, a game with computer vision (this one was me!), a site on the Gemini protocol, and more.Etherpad is where we take notes during sessions. It is a collaborative, open source editor. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Homebrew Website Club is a global network of meetups designed to help individuals gain more control over their online experience, particularly in relation to blogging.",
      "The resource includes details about recent and future meetups, presenting information such as dates, organizers, venues, and guides for arranging both virtual and physical meetings.",
      "Throughout the pandemic, the club has transitioned to online gatherings and provides resources for organizing IndieWebCamps, independent efforts to build private websites as opposed to using centralized web services."
    ],
    "commentSummary": [
      "The dialogue places emphasis on minimalist website designs with reduced JavaScript usage and the significance of consent banners.",
      "Participants express their fondness for the Homebrew Website Club, personal website ownership, and creating innovative web designs using tools like React and Angular.",
      "The talk underlines the value of creativity and personalization in the current web landscape, which includes having personal websites and attending IndieWeb events."
    ],
    "points": 204,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1694982354
  },
  {
    "id": 37549216,
    "title": "Large Language Models for Compiler Optimization",
    "originLink": "https://arxiv.org/abs/2309.07062",
    "originBody": "Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate > cs > arXiv:2309.07062 HelpAdvanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search Computer Science > Programming Languages [Submitted on 11 Sep 2023] Large Language Models for Compiler Optimization Chris Cummins, Volker Seeker, Dejan Grubisic, Mostafa Elhoushi, Youwei Liang, Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Kim Hazelwood, Gabriel Synnaeve, Hugh Leather We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding. We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time. Subjects: Programming Languages (cs.PL); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) Cite as: arXiv:2309.07062 [cs.PL](or arXiv:2309.07062v1 [cs.PL] for this version) https://doi.org/10.48550/arXiv.2309.07062 Focus to learn more Submission history From: Chris Cummins [view email] [v1] Mon, 11 Sep 2023 22:11:46 UTC (4,757 KB) Download: PDF Other formats Current browse context: cs.PLnewrecent2309 Change to browse by: cs cs.AI cs.CL cs.LG References & Citations NASA ADS Google Scholar Semantic Scholar Export BibTeX Citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Demos Related Papers About arXivLabs Which authors of this paper are endorsers?Disable MathJax (What is MathJax?) About Help Contact Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack",
    "commentLink": "https://news.ycombinator.com/item?id=37549216",
    "commentBody": "Large Language Models for Compiler OptimizationHacker NewspastloginLarge Language Models for Compiler Optimization (arxiv.org) 199 points by og_kalu 17 hours ago| hidepastfavorite95 comments JonChesterfield 1 hour agoFrom the abstract:- 3.0% improvement in reducing instruction counts over the compiler- generating compilable code 91% of the time- perfectly emulating the output of the compiler 70% of the time.I read through the paper to see what perfectly emulating the output means. In this case, I think it&#x27;s that it&#x27;s also possible to get the same code out of the compiler using a different pass order. I was hoping for still passes original test suite or similar.The authors are aware that the generated code has different semantics to the input in some cases but don&#x27;t seem to consider that particularly important. The section is \"Evaluation of Generated Code\".So - using machine learning, it is possible to delete a small percentage of the instructions emitted by a compiler while breaking the semantics. A tool which miscompiles programs while making them slightly smaller doesn&#x27;t seem to be progress in compiler optimization.Does anyone see some value add here that I&#x27;m missing? reply hoosieree 1 hour agoparentI&#x27;ve just skimmed it but I think they&#x27;re not using the model&#x27;s output for codegen, but rather to choose what compiler passes to use and in what order. reply PhilipRoman 7 hours agoprevI see a lot of misconceptions about using ML for compilers. You don&#x27;t ask the model what instructions to emit. Instead, you prepare a set of passes which are guaranteed to preserve correctness (we already have hundreds of them). Then you ask the model - what passes should I apply and in what order.Writing code to unroll a loop is trivial. The limitations of compilers are that almost all currently existing languages are too low level for optimization. ML has the potential to extract back this lost information. Basically the opposite of lowering an IR. reply JonChesterfield 2 hours agoparent> prepare a set of passes which are guaranteed to preserve correctness (we already have hundreds of them)Hundreds of passes, sure. Guaranteed that they preserve correctness is a bit more dubious, that&#x27;s pretty hard to establish for most transforms. Passes that make no assumptions about prior passes are tricky too since compilers tend to work in terms of a lowering pipeline.If the compiler has N correct passes that can be combined in arbitrary order without compromising compiler termination, exponentially increasing code size or generally making the output much worse, then you&#x27;ve already built a really good compiler. The subtask of then shuffling the order of passes to see if you missed anything is trivial, using machine learning to control your sort & test loop doesn&#x27;t seem very compelling here.My hunch is that the low hanging fruit in compiler dev using LLM is driving a fuzz tester with one. Other things seem worthwhile but difficult. reply titzer 1 hour agorootparent> The subtask of then shuffling the order of passes to see if you missed anything is trivial, using machine learning to control your sort & test loop doesn&#x27;t seem very compelling here.This is called the \"phase ordering problem\", and it&#x27;s neither trivial nor solved. reply boomanaiden154 7 hours agoparentprevML for phase ordering is just one problem that ML could solve within compilers.Heuristic replacement (like loop unrolling) is another big one. For the specific case of loop unrolling, I would think lower level elements like how much iCache pressure the unrolling creates&#x2F;whether or not the loop could fit in the DSB buffer would matter more.For your point about existing IRs being too low-level, there has been a large push to try and work on that. MLIR has been used pretty extensively for that problem in ML applications, and languages like Rust have multiple higher level IRs. There&#x27;s also a preliminary implementation of a Clang-IR for C&#x2F;C++, and there&#x27;s even be some work on higher level representations within LLVM-IR itself. reply eru 3 hours agoparentprev> You don&#x27;t ask the model what instructions to emit.You could still do that, you&#x27;d just also need to ask the model for a proof. (But I guess that&#x27;s much harder than heuristically picking which passes to apply.) reply hoosieree 12 minutes agorootparentThis is the right way to interact with LLMs in general. Ask for what you want, but independently verify the result. Don&#x27;t be like that lawyer \"...but I asked ChatGPT if it was telling the truth, and it said yes!\" reply SiempreViernes 3 hours agorootparentprevWhat do you do when you check the proof and discover it is false? Just ask again for a proof or discard the emitted instructions? reply eru 1 hour agorootparentDepends on your use case, I guess? reply boomanaiden154 14 hours agoprevOne of the biggest things that seems to be holding back ML in compilers right now is dataset size. This model was only trained on a gigabyte of source code, 30+% of that synthetic. Even on much simpler models, there have been massive performance gains by just throwing more data at them. Some experimentation with the original MLGO inlining model on a much bigger data corpus doubled the code-size wins. LLMs have also been shown to perform better they more data they are fed [1].1. https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.15556 reply isaacfung 8 hours agoparentQuality matters just as much as quantity.LIMA: Less Is More for Alignment https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.11206AlpaGasus: Training A Better Alpaca with Fewer Data https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.08701Textbooks Are All You Need II: phi-1.5 technical report https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.05463 reply uoaei 8 hours agoparentprevWhat&#x27;s holding them back is provable correctness.It&#x27;s possible, nay, mandatory to constrain the outputs of the model at each step of generation in order to guarantee that a given structure or grammar is adhered to. If you can fine-tune the model with these constraints in place you can offload a lot of the effort that the LLM otherwise has to perform in comprehending correctness so it has more capacity for generating good content. To be sure, quality and quantity of data are important, but it&#x27;s all too easy to introduce subtle bugs that take years to tease out if you don&#x27;t adhere to the right constraints. reply boomanaiden154 7 hours agorootparentMost of the work in this space is not focused on neural compilation (having a ML model perform the transformation&#x2F;entire compilation), but on replacing heuristics or phase ordering, where the issue of correctness falls back onto the compiler. For pretty much exactly the reasons you mentioned, neural compilation isn&#x27;t really tractable.This specific paper focuses on phase ordering, which should guarantee correctness, assuming the underlying transformations are correct. They do train the model to perform compilation, but as an auxiliary task. reply foota 16 hours agoprevThis kind of application of LLMs is most interesting to me, since it&#x27;s possible to evaluate correctness and performance quantitatively. reply jebarker 16 hours agoparentIt seems like a poor fit to me precisely because correctness is boolean, difficult to measure and getting it wrong is very bad. I do think there&#x27;s a place for AI here but it&#x27;s probably not LLMs in their current form. reply kukkamario 16 hours agorootparentThey are not using LLM to directly produce the result code, but as tool that lists which optimisations should be done and in which order, which is fairly complex problem to solve. But if optimisation passes are implemented correctly (which is anyway required for a functioning optimising compiler), it cannot produce incorrect code, maybe only suboptimal compared to default heuristics used. reply jebarker 15 hours agorootparentIf there&#x27;s a list of known optimizations that preserve correctness then it becomes an optimization problem based on output length (as a proxy for cycle count). So is the idea that an LLM is more efficient than a search or direct optimization? reply sagarm 15 hours agorootparentThere tend to be a lot of heuristics involved when deciding which optimizations to apply and in which order, so there&#x27;s plenty of room to apply some machine learning.Whether LLMs are the right approach is a separate question.In SQL optimization, the problem is a bit trickier (IMO) because compilation is in the query path. One successful approach I know of is Bao: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2004.03814 reply namibj 13 hours agorootparentprevThat&#x27;s neither the only metric of relevance, nor is that a good proxy for modern superscalar vector processor architectures. reply kaba0 6 hours agorootparentprevCycle count is not a proper benchmark for performance on “modern” processors. reply drug-freedom 14 hours agorootparentprevOne example where a LLM might be better is which functions to inline.Current compilers use a complex set of heuristics, a more holistic approach the kind neural networks do might outperform. reply boomanaiden154 14 hours agorootparentFor function inlining specifically, I&#x27;m not sure LLMs are necessarily the right choice. The original MLGO paper [1] demonstrated a big code-size improvement with a ML model for making inlining decisions (7-20% code size wins), but they used tens of engineered features. Maybe a LLM could squeeze some additional size wins out, but maybe not [2].Additionally, there are other factors to consider when productionizing these systems. Compile time is important (which LLMs will almost certainly explode), and anyone concerned about code size will probably be doing (Thin)LTO which would require feeding a lot more context into a LLM making inlining decisions.1. https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2101.04808 2. https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3503222.3507744 reply BruceEel 8 hours agorootparentI&#x27;m probably being very naive with this but, could mechanistic interpretability play a role here? Specifically, to experimentally short-list the LLM&#x27;s most effective optimizations, then try to peek inside the LLM and perhaps fish out some novel optimization algorithm that could be efficiently implemented without the LLM? reply boomanaiden154 8 hours agorootparentI did a bit of work on this last summer on (much) smaller models [1] and it was briefly discussed towards the end of last year&#x27;s MLGO panel [2]. For heuristic replacements specifically, you might be able to glean some things (or just use interpretable models like decision trees), but something like a neural network works fundamentally differently than the existing heuristics, so you probably wouldn&#x27;t see most of the performance gains. For just tuning heuristics, the usual practice is to make most of the parameters configurable and then use something like bayesian optimization to try and find an optimal set, and this is sometimes done as a baseline in pieces of ML-in-compiler research.1. https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;ml-compiler-opt&#x2F;pull&#x2F;109 2. https:&#x2F;&#x2F;youtu.be&#x2F;0uUKDQyn1Z4?si=PHrx9RICJIiA3E6C reply BruceEel 8 hours agorootparentAh interesting. Hadn&#x27;t seen that presentation, thanks for sharing! replyButtons840 9 hours agorootparentprevThis is scooching into AlphaGo (or whatever the generic implementation is called) territory: mixing predictive AI with traditional optimization algorithms. reply nightski 12 hours agorootparentprevI thought so as well but then in the article it says that only like 91% of the code even compiles. reply reic 16 hours agorootparentprevQuantification can be done by measuring in at least two dimensions: (1) the size of the synthesised code, and (2) how precisely the generated code matches the input (which means roughly: on what fraction of input do the two programs give different output). We have set up a challenge that seeks to entice the community to look into this problem domain more. And we&#x27;ve simplified the assumptions, so as to make it more tractable:- Challenge: https:&#x2F;&#x2F;codalab.lisn.upsaclay.fr&#x2F;competitions&#x2F;15096- Paper describing the challenge: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.07899(I am one of the authors, AMA) reply jebarker 14 hours agorootparentHow well does (2) really measure accuracy? It seems like a single output that doesn&#x27;t match the input code could indicate a fundamental floor in the optimized code, so it&#x27;s essentially 100% wrong even though it gets the correct answer almost all the time.Good luck on the challenge though, this seems like an interesting and valuable area of research. reply reic 14 hours agorootparentOf course (2) is not a perfect measure of accuracy, since it does not quantify how far wrong an output is, e.g. if 111111111111 is the correct output, then both 111111111110 and 829382934783 count as equally faulty. The main advantage of (2) is that it is natural, easy to understand, and easy to measure and compare. We have to start somewhere. I imagine that, in the future, it can be refined (e.g. taking the Hamming distance between desire and actual output). I expect that more refined quantification emerges in response to the community better understanding exactly what is hard in the synthesis of programs.Feel free to submit something! A simple submission is probably just a few lines of code. reply foota 16 hours agorootparentprevThe approach seems to focus on selecting optimizations to apply for LLVM (e.g., imagime: should this be inlined as an example), but the worst case is that the code is poorly optimized, you can&#x27;t select optimizations to apply and get a wrong result.I agree that what you say is true of compilation as a whole, but that doesn&#x27;t seem to be the focus here (rather, it&#x27;s used as a sort of crutch to help the LLM learn) reply armchairhacker 16 hours agorootparentprevThe trick is finding a way to ensure that LLM produces something which is always correct. Like in this case, the LLM only changes compiler optimizations, not the assembly itself, so no matter what it outputs the code is correct, it just may be larger. Other possibilities: an LLM which applies semantics-preserving program transformations, or an LLM combined with a proof assistant to verify the output (more generally and for any domain, an LLM as an NP oracle).But I agree, as of now I haven&#x27;t seen good uses where LLMs produce reliable output. Not only do you need that guarantee that whatever output always generates a correct program, you need something where an LLM is considerably better than a simple or random algorithm, and you need a lot of training data (severely restricting how creative you can be with the output). reply wyldfire 16 hours agoprev> understanding. We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler,3% code size reduction is really good. The challenge will be having codegen like this that someone is willing to support. And for that they&#x27;d want to be able to reason about why the compiler made this decision or that one. IIUC that&#x27;s an outstanding problem for AI in general. reply JonChesterfield 1 hour agoparent3% code size reduction while changing semantics is borderline worthless*.3% code size reduction without changing semantics would be more interesting but might still be a bad thing for performance.*fast-math etc is a thing, where similar-enough output is fine reply TheLoafOfBread 15 hours agoparentprevAlso bugs from this approach are going to be funny - program compiled with compiler version X will work as expected and same program compiled with version X+1 will start crashing because AI under some circumstances decided that dereference of a specific pointer was unnecessary, so it won&#x27;t drop it into the assembly.Good luck finding such a bug, because you will be looking on correct code, but computer will be executing invalid output. reply boomanaiden154 14 hours agorootparentThe focus of this work is finding the optimal ordering of optimization passes to perform, not doing neural compilation. This guarantees correct code, assuming the underlying transformation passes are correct.Most work in ML for compilers focuses on replacing heuristics and phase ordering precisely because they don&#x27;t impact correctness. There is some work being done on neural compilation [1], but I&#x27;m not sure that&#x27;s going to be a viable approach anytime soon.1. https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;9926313 reply wyldfire 13 hours agorootparent> phase ordering precisely because they don&#x27;t impact correctness.lol let&#x27;s say they&#x27;re less likely to impact correctness than an arbitrary new optimization. reply ncruces 5 hours agorootparentIf you find a bug, it&#x27;s in the optimization passes (that don&#x27;t reorder and should, if you decide to do this).At this point this is like (usefully) fuzzing your optimizer, which long term is going to be great for correctness. reply a_wild_dandan 15 hours agorootparentprevDamn. These things imitate humans too well. Guess we’ll need a giant test suite to feel stable. Sounds like work. Let’s train a GAN for that and put our feet up. Turtles all the way down, my dudes. reply naveen99 14 hours agoprevChatgpt4 can do source to source optimization which is pretty cool. I got it to beat gcc at -03 on simple small toy problems. and it can do similar things with python also. But it threw its hands up when I gave it a longer piece of code to optimize. reply dwattttt 13 hours agoparentIt does make sense that ChatGPT can beat gcc (or any compiler really); the compiler is forced to optimise code so that it still appears to work as if it&#x27;s the source you wrote. Most importantly, if there are observable side-effects, compilers must preserve them, even though you know you don&#x27;t actually care about them. An AI is not bound to preserve those, and can semantically change what you wrote, so long as the one semantic you wanted preserved is still there. reply esrauch 13 hours agorootparentIt seems like compilers should have an interactive mode where it suggests code that is very similar but not technically perfectly the same that is more efficient and the can accept or decline the alternate version. reply oldmanhorton 13 hours agorootparentThis sounds like an extension of performance lint rules which many languages&#x2F;ecosystems already have. reply insanitybit 11 hours agorootparentprevA backend compiler is probably not in a good place to do that. Frontends could, however, and there are lots of tools already to make those suggestions for many languages.clippy in rust, for example, will tell you about removing needless allocations, etc, which a backend compiler might have a harder time doing. reply kmeisthax 12 hours agoparentprevThere&#x27;s a Kaze Emanuar video where he tries to have ChatGPT-4 optimize parts of his already well-optimized Mario 64 ROM hack: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=20s9hWDx0Io reply naveen99 12 hours agoparentprevThe toy example for those curious: #includeint main(void) { int i; int a[2000] ; for(i = 0; iint main(void) { printf(\"%d\\n\", 5); } reply Izkata 11 hours agorootparentCode indentation is 2 spaces on here, you have to do it to all the lines you want formatted that way. That&#x27;s why the first one got a bunch of lines combined, it treated them as one paragraph instead of code. reply naveen99 10 hours agorootparentthanks. fixed. reply naveen99 12 hours agorootparentprevBut when I tried to get it to remove the dependency on studio.h, it said that’s too hard. Might have gone further with a more reasonable request I guess. Also not necessary as gcc does that anyway. reply cypress66 9 hours agorootparentprevPrintf is horribly slow compared to puts reply jmgao 9 hours agorootparentputs isn&#x27;t applicable here because you actually need to format the %d, but any compiler worth using (i.e. not MSVC: https:&#x2F;&#x2F;gcc.godbolt.org&#x2F;z&#x2F;zx74vY1za) will optimize a printf of a constant string ending in a newline into a puts. reply kaba0 6 hours agoparentprevI don’t think any of those examples would be materially different than finding a related problem on stackoverflow and applying the answer.LLMs are just good at searching and transforming between representations, but they really are not good at logical inferences. reply alex_lav 14 hours agoparentprevAt my company we have some \"loadbearing\" code that was written by a mad scientist that no longer works at our company. We have a total ban on AI for source code analysis (so chatgpt, copilot etc are all banned). I&#x27;ve really wanted to throw some of the grosser parts of that codebase into gpt-4 just to see if it could bring some small amount of sanity. reply reidjs 13 hours agorootparentDo you know why that policy is in place? Is it fear that the llm provider will steal company trade knowledge? reply esrauch 13 hours agorootparentIt&#x27;s probably both that is leaking your source out and the risk of it being a copyright violation if it spits out some GPL source verbatim and you check it in. reply Filligree 13 hours agorootparent> if it spits out some GPL source verbatimDoes that ever actually happen? I&#x27;ve only heard of it happening to people who forced the AI&#x27;s hand by including the comments for said code in the prompt. reply minhazm 11 hours agorootparentThere&#x27;s a setting \"Suggestions matching public code\" that can be set to \"Block\" that I think is sufficient for most people&#x27;s usage. But many companies don&#x27;t want to involve themselves in any sort of liability. reply callalex 12 hours agorootparentprevWhile many software companies get away with an “it’ll be fine” attitude, that is not sufficient for all companies and industries. Sometimes things have to be provably correct. reply pclmulqdq 12 hours agorootparentprevThis happens a lot, and you can easily walk into constructing such a prompt without knowing. reply alex_lav 10 hours agorootparentprevI work in a research facility, so the biggest fear is that our super top secret elite info will leak. The reality is it&#x27;d be used to refactor a lot of terrible code. reply cypress66 9 hours agorootparentUse local models if you don&#x27;t want to send your data to OpenAI reply alex_lav 8 hours agorootparent> We have a total ban on AI for source code analysis reply cypress66 2 minutes agorootparentI&#x27;m replying to the fear that it will leak secret infoCJefferson 11 hours agoprevWhile this isn&#x27;t optimization, is there any LLM systems that create \"provably correct\" transformations yet?I have used ChatGPT on some fairly awful code, asking it to add comments, rename variables and functions, etc. I find the outputs useful, but a couple of times it&#x27;s broken the code. I imagine for many (not all) languages, you could ask the LLM to produce suggested changes, then use a code-rewriting tool to apply them in a way you could be 100% sure they weren&#x27;t going to change the behaviour of the progrma. reply aureianimus 9 hours agoparentNot strictly what you&#x27;re looking for, but in Lean (functional language&#x2F;theorem prover), there&#x27;s some interesting work being done. Using the tool actually shows which suggestions will compile, which certifies correctness to some degree. https:&#x2F;&#x2F;leandojo.org&#x2F; reply isaacfung 8 hours agorootparentSimilar ideasCertified Reasoning with Language Models https:&#x2F;&#x2F;github.com&#x2F;gpoesia&#x2F;certified-reasoningIt&#x27;s based on Peano, a theorem proving environment Peano: Learning Formal Mathematical Reasoning https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.15864(https:&#x2F;&#x2F;github.com&#x2F;kyegomez&#x2F;LOGICGUIDE claims to implement the same paper as the first repo but it is fake) reply darksaints 9 hours agoparentprevI can&#x27;t imagine there would be any success with an LLM that isn&#x27;t already known about via Satisfiability Modulo Theory. reply CJefferson 7 hours agorootparentI use SMT, but I can’t imagine how it would suggest good names for variables? reply quickthrower2 10 hours agoparentprevSounds like TDD and langchain?Where the T can be anything from Ruby unit tests to Coq proofs. reply wrsh07 9 hours agoparentprevAre you compiling the code or interpreting? reply CJefferson 7 hours agorootparentCompiling C++ code, which means there is a chance you could prove transformations correct, but I’m not going to try it myself :) reply Lynix48 2 hours agoprevI was thinking about writing my bachelor&#x27;s thesis about LLMs and compilers. My idea is to maybe attempt to write a C (to assembler)-Compiler and see how well it does.Attempting to convert assembler to C using an LLM would be interesting too, but the results would probably be poor since there is just so much information that gets lost when compiling, so the C code would be pretty statistical. I guess I could improve it by somehow adding \"C code is or is not compilable because of line n\" and whether the result is correct to the lost function.What do you guys think about this? reply CalChris 16 hours agoprevThere&#x27;s a half day tutorial at the LLVM Developers Meeting on this, ML-Guided Compiler Optimization in LLVM. However, the authors of this paper aren&#x27;t giving that tutorial. reply boomanaiden154 14 hours agoparentIt&#x27;s a workshop intended to facilitate discussion in the space. The lead author of this paper (Chris Cummins) will probably be there.https:&#x2F;&#x2F;discourse.llvm.org&#x2F;t&#x2F;pre-llvm-dev23-ml-guided-compil... reply klohto 16 hours agoparentprevMLGO uses RL reply mathisfun123 15 hours agorootparentmaybe today but MLGO, the initiative, uses whatever. To wit: chris cummins (first author here) was on last year&#x27;s MLGO panel. reply smusamashah 5 hours agoprevDo LLMs need to see some information just once to answer about it? My understanding was that they need to see many examples of the same thing to be able to answer it correctly (generate text around it correctly).Shouldn&#x27;t this mean that it is safe to ask GPTs about something proprietary if it was just once because rare examples should just disappear in weights of everything else. And this also means that even GPT4 won&#x27;t be able to answer any queries about obscure or rare knowledge it has seen in its training dataset. reply aratauto 5 hours agoparentSome recent discoveries indicate that LLMs might remember information after seeing it only once. See e.g.: https:&#x2F;&#x2F;www.fast.ai&#x2F;posts&#x2F;2023-09-04-learning-jumps&#x2F; reply neonsunset 8 hours agoprevI was hoping to see a more ambitious language model based project.Rather than just picking compiler arguments, an LLM could parse, and then transform AST into its more optimized form in a way that does not rely on a very formal way it is treated by compilers of today, apply guesstimate-based branch reordering and inlining heuristics not dissimilar to how a programmer would do so manually.Once done, a compiler could perform final AST validation and either route it back to LLM to fix it or auto-fix most common cases. reply boomanaiden154 7 hours agoparentWhat&#x27;s the benefit of having an LLM do those things in a way that guesstimates? There are big wins to be had in code-size and some wins to be had in performance related to inlining [1][2], but I think the implementation in the references directly tied into the compiler&#x27;s inlining heuristic is a much better way to do that as it guarantees correctness. In addition, there&#x27;s a reason that compilers basically ignore the `inline` keyword these days.For branch reordering, techniques like BOLT [5] are pretty effectively able to reorder code layout at the binary level for big performance gains by using profile information. ML models can sometimes synthesize that information [3], but if I recall correctly, the performance of those models wasn&#x27;t as good.Neural compilation (like what you&#x27;re describing) has been tried with LLMs [4], but has a lot of correctness problems currently, and I don&#x27;t think it&#x27;s going to be feasible anytime soon to do reinforcement learning for performance&#x2F;code-size improvements.1. https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2101.04808 2. https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.08389 3. https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2112.14679 4. https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;9926313 5. https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1807.06735 reply haltist 8 hours agoparentprevHow do you validate semantic equivalence? reply quadrature 13 hours agoprevI wonder if this requires a 7B parameter model.Assembly has a small grammar and is very constrained compared to natural language. reply kaba0 6 hours agoparentI don’t see why would the output language’s complexity matter - that’s clearly not the hard part. You need plenty of “thinking” to do for outputting sensible assembly, let alone whole programs.With that said, it is not doing neural compilation as others have mentioned, it’s only about ordering&#x2F;enabling different phases of the compiler based on ML, over the current, simpler heuristics. reply sesuximo 14 hours agoprevI wonder if you could get a correct compiler (or 100% emulation in their terms) by allowing it to choose optimization pass order rather than doing more arbitrary things reply boomanaiden154 14 hours agoparentI&#x27;m not sure a fully correct production optimizing compiler is that feasible. LLVM gets multiple miscompilation reports per week (from what I&#x27;ve haphazardly seen observing the issue tracker).Theoretically changing the order of the passes in the optimization pipeline shouldn&#x27;t cause any correctness issues, but the fact is that the ordering in the default compilation pipelines is the one that is most tested, so there will probably be bugs exposed when fuzzing the pass ordering. reply kolbe 13 hours agoprev> Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations.If that&#x27;s their target, then what&#x27;s the point? LLVM&#x27;s optimizations are not done to minimize instructions but to maximize performance. On modern processors, these can be very different things. reply boomanaiden154 13 hours agoparentYou&#x27;re right that a decrease in code size doesn&#x27;t mean a performance increase (and oftentimes they can be inversely correlated like in inlining).But LLVM targets both depending upon what optimization pipeline you select. (-Oz&#x2F;-Os are targeting minimum code size, -O1,-O2,-O3 are optimization focussed).Code size reduction is critical in some use cases like embedded environments and mobile apps and it is a significant area of research. reply haltist 15 hours agoprevNext step is to add verification for optimized code from the LLM with an SMT solver (like Z3) to remove \"hallucinations\". If the input and output code can be verified to be equivalent then this would be a great addition to an optimization pipeline. Once that&#x27;s done the same can be applied to intermediate representations of GPU kernels in a recursive loop of AI optimizing AI code for faster execution times. reply boomanaiden154 14 hours agoparentThere&#x27;s already tooling available for using SMT to validate LLVM-IR transformations [1]. It&#x27;s designed for zero false positives however, so some things might slip through the cracks.Additionally, this work focuses on phase ordering, which produces correct code regardless of what the LLM puts out, assuming there aren&#x27;t any bugs in the passes being used (which could crop up as random orderings aren&#x27;t as well tested as the standard orderings in the commonly used pipelines).1. https:&#x2F;&#x2F;users.cs.utah.edu&#x2F;~regehr&#x2F;alive2-pldi21.pdf reply haltist 10 hours agorootparentInteresting. AI code has no loops so the problem they mention in the beginning about unrolling loops to a certain depth is a non-issue. reply dhosek 12 hours agoprev“generating compilable code 91% of the time” which means that almost ten percent of the time, it doesn’t generate compilable code. Given the fact that ChatGPT has gotten worse at math over time,¹ I’m wondering if this too will get worse.⸻1. Although I also find myself thinking about my kids as they were developing language where they initially correctly conjugated some common irregular verbs, then they started conjugating them as if they were regular and then finally returned to correctly conjugating them, which might be what’s happening with ChatGPT and math. reply primordialsoup 11 hours agoprevThis is very interesting work, but it&#x27;s not really a LLM. It doesn&#x27;t have language abilities. They should have called it a seq2seq model, but I think that term is not in vogue these days :) reply gamerpuppy 11 hours agoprev [–] LLm&#x27;s are agi, you can use them for anything so posts like \"LLm&#x27;s for x\" are really so boring. reply csjh 11 hours agoparent [–] LLMs are definitely not AGI replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper investigates the application of Large Language Models (LLMs) in code optimization, using a 7 billion parameter transformer model for optimizing Low Level Virtual Machine (LLVM) assembly code size.",
      "The model is trained to predict instruction counts before and after optimization, and subsequently provide the optimized code.",
      "Tests conducted on a vast range of programs revealed a 3.0% improvement over the traditional compiler, showcasing the model's robust code reasoning capabilities."
    ],
    "commentSummary": [
      "The summary broaches the discussions about the advantages and constraints of using Language Learning Models (LLMs) in compiler optimization, such as reducing instruction counts and generating compilable code.",
      "It also discusses concerns related to code semantics and correctness when using LLMs, along with challenges like the need for larger datasets.",
      "The text references further dialogues on the utilization of AI in optimizing compilers, risks associated, and the investigation of LLMs' functionality within different phases of the compilation process."
    ],
    "points": 197,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1694984120
  },
  {
    "id": 37546241,
    "title": "How the Mac didn’t bring programming to the people",
    "originLink": "https://eclecticlight.co/2023/09/16/how-the-mac-didnt-bring-programming-to-the-people/",
    "originBody": "THE ECLECTIC LIGHT COMPANY MACS, PAINTING, AND MORE Downloads M1 & M2 Macs Mac Problems Mac articles Art Macs Painting hoakley September 16, 2023 Macs, Technology How the Mac didn’t bring programming to the people Macs have brought a great deal to us over the years: desktop publishing, design, image editing and processing, multimedia, and more. One of the few fields where they have failed is programming, despite many attempts. Here I look back at some of those opportunities we missed. HyperTalk (1987) First released in 1987, Apple’s hypertext authoring environment HyperCard contained its own scripting language, HyperTalk. For many of those who built brilliant HyperCard stacks, this new scripting language was the first programming language they had used. Sadly, although seriously cool in its day, HyperTalk was both limited and limiting, as most came to discover. By 2000, HyperCard and HyperTalk were all but dead. AppleScript (1993) In some respects a successor to HyperTalk, AppleScript was released in October 1993, with System 7 Pro version 7.1.1, making it almost thirty years old. Despite all the odds, and several determined attempts to strangle it, it’s still supported in macOS Sonoma. The concept behind AppleScript is simple: scripts that compile to a series of instructions for dispatch by macOS to their destination application, which in turn is controlled by those commands to perform a co-ordinated sequence of functions. At their simplest, these can open a document and print it, for instance. At their most complex, they can automate intricate and repetitive tasks that are messy in a GUI. As a minimum, every application supports a small core of commands to play clean with the Finder and macOS. Those, and the suites of additional commands that bring joy to the scripter, are documented in standard formats within each application’s dictionary, which can be browsed by Script Editor and other tools. Rather than having to locate additional documentation sets specific to each application, all a scripter should need to do is open the dictionary. Complexity comes because AppleScript is in fact an object-oriented language as sophisticated as Objective-C, as used by pro Mac developers; don’t be deceived by its apparently relaxed and informal style, with examples such as tell application \"System Events\" set mailIsRunning to application process \"Mail\" exists end tell if mailIsRunning then -- do one thing else -- do another thing end if You might use that code to set up a script that interacts with the Mail app. It first asks macOS whether it knows that Mail is running, and depending on the answer it executes the code that you insert where the comments (prefaced by ‘--‘ characters) are placed. Unlike the majority of programming languages, punctuation marks are used sparsely in AppleScript, making it considerably easier to write code that works, rather than tripping over a missing semicolon. When ready to test your script, it compiles into intermediate code, and the editor automatically checks, formats and colours your source code, reporting any errors that it finds. When run, the intermediate code works through macOS to fire off AppleEvents (AEvents) to trigger the target applications to perform the actions. Although it was integrated into Apple’s Xcode as AppleScript Studio, in recent years it has been left to languish, with occasional rumours of its demise. Prograph (1989) Innovative languages for Macs haven’t been Apple’s preserve. In 1989, the visual programming language Prograph was launched on the Mac. Sometimes described as a dataflow language, and thoroughly object-oriented, it won awards, was ported to Windows in the late 1990s, but was last seen running as Marten in Yosemite. In this example method, data flows from the top to the bottom. Normally terminals on the top ‘shelf’ of the method diagram represent the inputs to that method. Data then flows from the top shelf through the intermediate processes, until it reaches the bottom. If there are outputs from the method, they are gathered by connections made to terminals on the bottom ‘shelf’ in the method. Order of execution is not prescribed, and can take place whenever data is available; this allows for inherent concurrency, and the potential to exploit multiprocessor systems without the need for language primitives. Automator (2005) Even ‘natural’ programming languages like AppleScript have to be learned, a task that many find too verbal and mechanical. Recognising this, Apple introduced Automator in OS X 10.4 in 2005 to help produce custom workflows and apps using more intuitive visual tools. Although often assumed to be a development of AppleScript, apart from its ability to run AppleScript objects, Automator is actually very different. Instead of relying on AppleEvents and dictionaries, Automator’s modular actions are separate code objects installed in the Automator folder in a Library folder. macOS comes with a huge free library of actions that can accomplish tasks you might pay good money for, so familiarity can save you cost. Automator can also run AppleScript and shell scripts, to augment its capabilities. Automator is highly extensible, as its actions can include both AppleScript code and Terminal shell scripts. Thus if you cannot find a standard action to do what you want, if it can be expressed in a suitable script, you can build that into your workflow. Nevertheless, in 2021 Apple announced that Automator was to be succeeded by Shortcuts. Swift Playgrounds (2014-16) From the early days of the Mac, Apple has invested in programming languages designed to make best use of its APIs. In Classic times, that was Object Pascal, and its open-source class library MacApp. In 2014, Apple released a new language destined to be its preferred choice across all its platforms, Swift. From those early days, Swift has had an interactive mode, based on the ‘read-eval-print loop’ (REPL) popularised by Lisp. This versatility has been developed in Swift Playgrounds, both within Xcode and as a standalone app targeted at children (and adults) learning to code for the first time. As an introduction to Swift in education, this has been impressive, but it hasn’t proved a gateway for those who didn’t really want to learn how to use Xcode in the first place. Shortcuts (2014-21) Shortcuts started out in the winter of 2014-15 as Workflow by DeskConnect. Apple bought it in 2017, and it became Shortcuts the following year, when it was integrated into Siri in iOS 12. Its arrival in macOS 12 was announced at WWDC in 2021, as Automator’s ultimate successor. While Shortcuts on macOS can run AppleScript and shell scripts, the mechanisms involved in Shortcuts’ actions are completely different from AppleScript and Automator. For an app to support all three well requires it to present four different interfaces: one for the user in the GUI, AppleEvents, Automator actions, and now Shortcuts actions. While Shortcuts has attracted quite a following, its impact has so far been limited. Like all its predecessors, it hasn’t yet brought programming to the people. Credits HyperTalk: Dan Winkler AppleScript: William R Cook and many others Prograph: Tomasz Pietrzykowski, Jim Laskey and others Automator: unknown Apple engineers Swift Playgrounds: Chris Lattner, Doug Gregor, John McCall, Ted Kremenek, Joe Groff and others Shortcuts: Ari Weinstein, Conrad Kramer and Nick Frey Share this: TwitterFacebookRedditPinterestEmailPrint Loading... Related Last week on my Mac: who remembers HyperCard, and a ban on encryption? July 17, 2016 In \"General\" Solutions to Saturday Mac riddles 152 May 23, 2022 In \"General\" Shortcuts: Automating the Mac October 13, 2021 In \"Macs\" Posted in Macs, Technology and tagged AppleScript, Automator, HyperTalk, Marten, playground, programming, Prograph, shortcuts, Swift, visual programming. Bookmark the permalink. 27Comments Add yours 1 Andrew Reilly on September 16, 2023 at 7:33 am Reply Wasn’t the point of the Mac, it’s reason for existing, to bring computing to the people without requiring them to be programmers? I’ve used (and programmed) Macs since the Plus (in Programmers Workbench Object Pascal, which was lovely) and have never found the need to introduce myself to AppleScript. When you’re doing GUI tasks it all works beautifully, IMO. Macs are (now) excellent _tools_ for professional programmers though. In my work environment, since IT relented from their Windows-only stance many years ago, I’d say that half of the developers now use Macs as their primary environment. I’ve never warmed to Xcode, and I know few who have, but VS Code is popular, as are the JetBrains IDEs, Matlab is well supported, for those of that persuasion. Or the old Unix die-hard approach that I favour myself: couple of terminal windows and a good editor like vim/nvim/helix. Liked by 1 person 2 hoakley on September 16, 2023 at 9:28 am Reply Thank you. I’m not sure programming ever came into it, although I was admittedly a relative latecomer, as I didn’t come to the Mac until 1989. But prior to that, I was developing for PCs, and the Commodore Amiga. At that time, neither of those required users to be programmers, so I’m not sure there was contested ground. Where the Mac was initially targeted, though, was at those whose tasks were primarily visual rather than textual: graphic designers, and particularly at the new desktop publishing market. I think that was the first novel market that Apple aimed the Mac at, as an enabler that allowed a whole industry to set up with Macs and laser printers, and print what previously had been low-tech, slow and costly. It was only really with the advent of HyperCard that users wanted to write code, when they saw what could be achieved in their stacks. None of the modern tools that you mention go anywhere near ‘programming for the rest of us’ in the way that the languages above have attempted (and ultimately failed). There are many people who cut their teeth with simple automation using AppleScript, then went on to craft larger programs, or switch to other ‘production’ languages. But I don’t think that’s likely to happen with Shortcuts, for instance. Howard. Like 3 Andrew Reilly on September 16, 2023 at 8:40 pm Reply At first, the PC followed its immediate predecessors, the Apple II series, the TRS-80 series, the C64 and by booting into a BASIC REPL (although I never heard it called that at the time). DOS followed pretty quickly though, so that immediate need to know a programming language was fairly brief. That previous generation, including the ZX80, BBC and Atom are arguably what got a whole generation programming, and you’re right that it probably won’t happen again. Low-level, clean-sheet programming isn’t even being taught widely in Universities any more. That’s now a very specialized skillset. These days it’s all about lashing things together with python scripts. Liked by 1 person 4 hoakley on September 16, 2023 at 9:17 pm Thank you – yes, it was the BBC/ZX80/Atom period that drew many into programming. Of course that was one of the purposes of the BBC too. I agree with the python problem too, even in fields such as ML and AI. Howard. Like 5 Ricardo Bánffy on September 17, 2023 at 5:42 pm Reply Initially, IIRC, it wasn’t possible to program for the Mac on a Mac. At first, one would need a Lisa (or a Mac XL) to do it. Liked by 1 person 6 hoakley on September 17, 2023 at 7:10 pm Thank you. I’m not sure how that’s relevant to this subject or article, but, yes, you’re correct, and I think the language required was Lisa Pascal, that then developed into Object Pascal thanks to Larry Tesler and Niklaus Wirth. But that was a relatively short period. Howard. Like 7 Jeff Johnson on September 16, 2023 at 11:48 am Reply I have a different idea of bringing programming to the people. The *only* reason that I’m a programmer today is that my iMac G4 came with the Xcode developer tools on the install discs. I find it sad that Macs no longer come with the developer tools, and my impression is that there are a lot fewer amateur Mac programmers today than there were back in the 2000s. You can of course download Xcode from the Mac App Store, but most Mac users will have no awareness of the existence of Xcode, and the Mac App Store itself is a bit of a ghost town. It’s just not the same as having the tools come in the box with the Mac. Liked by 2 people 8 hoakley on September 16, 2023 at 7:06 pm Reply Thank you, Jeff. I agree completely, and think your impression is both accurate and worrying. Many of us came to programming as amateurs, yet Apple, like all major vendors, shows not the slightest interest in encouraging those. Indeed, it erects barriers to scare amateurs away. Notarisation is but one example: while it has plugged away repeatedly that Mac ‘developers’ must sign up, get their certificate, and notarise, it hasn’t told amateurs that those ‘rules’ don’t apply to them. I have repeatedly seen the advice given that the only way you can develop anything for macOS is to pay your subscription and notarise, which must be a huge deterrent to many. I also suspect that the Xcode you cut your programming teeth on wasn’t as formidable as it is today, and was probably far better documented too. It’s all very well encouraging and supporting programming in education with platforms like Swift Playgrounds, but it’s so important to encourage everyone. Howard. Liked by 1 person 9 Jeff Johnson on September 16, 2023 at 9:45 pm Reply “I also suspect that the Xcode you cut your programming teeth on wasn’t as formidable as it is today” Well, I’d say yes and no. On the one hand, code signing wasn’t an issue back then, as you mentioned. Also, there was only one platform, Mac, rather than the four or five platforms today. On the other hand, Xcode 3 and earlier was more of a “pro” app in my opinion than Xcode 4 and later. It allowed more pro workflows, it was more configurable, it didn’t try to cram everything in one window, Interface Builder was a separate app, etc. In those respects, I’d actually love to have Xcode 3 back again. “and was probably far better documented too.” Probably. Liked by 1 person 10 Ralf on September 16, 2023 at 1:38 pm Reply It’s just been a year that I’ve been a bit more involved with programming for mac automation. Automator was suspicious to me because I had the impression I was sitting on a dead horse. AppleScript, in its effort to be understandable, is a pain because I keep stumbling over little errors where it’s absolutely not clear how it’s correct. Shortcuts is great on the one hand because the shortcuts works for iPhone and Mac (partially at least). On the other hand, it is awkward to change variables, sometimes program code is lost and I have to program on the iPhone or Mac on a tiny screen. Shortcuts urgently needs an editor, preferably a text editor mode. The name Shortcuts is unfortunate as searches look for keyboard shortcuts. My start to more programming was to build scripts into shortcuts. I built larger Bash and Ruby scripts and kept the number of shortcuts actions limited. The end of the story is that I now mainly build scripts in the terminal and only trigger individual actions via Apple Script and shortcuts. Maybe sometime I am ready to try Swift in Xcode. Apple should make a binding and clear statement about the future of the approaches. Unfortunately, Apple never says anything. Liked by 1 person 11 hoakley on September 16, 2023 at 7:08 pm Reply Thank you. Yes, that has been the downfall of so many of these. After a few years, Apple drifts away and that platform/language is abandoned, and other major vendors are no better. Roadmaps of where AppleScript and Shortcuts are heading would be so helpful. Howard. Like 12 Joel on September 16, 2023 at 4:01 pm Reply Prograph is by far and away the most productive and enjoyable programming environment I ever used. A friend and I used it to dynamically generate webpages in the early days of the Internet – for which we received an innovation prize :-). It took away all the pain points normally associated with programming as it is still, sadly, practiced. I know that Apple actually considered adopting it, but the difficulties of versioning and performing diffs at the time meant it didn’t make the cut. IMO, contemporary programming languages are as flint axes compared to Prograph. A prince among the pawns. Liked by 1 person 13 hoakley on September 16, 2023 at 7:09 pm Reply Thank you. Yes, its demise is a tragedy. I’m still hopeful that someone will design another visual programming language as powerful. Howard. Like 14 elmimmo on September 16, 2023 at 5:34 pm Reply Quartz Composer? Liked by 1 person 15 hoakley on September 16, 2023 at 7:10 pm Reply Thank you. Maybe, although I’m here aiming more at general purpose languages, with the exception of HyperTalk. Howard. Like 16 donatj on September 17, 2023 at 8:29 pm Reply This reply didn’t popup until I posted my other one – I was a heavy user of Quartz Composer and it really was more general purpose than it got credit for. I personally used it to inspect HID devices because of how unreasonably easy it made the process. Liked by 1 person 17 hoakley on September 17, 2023 at 8:43 pm Thank you. I think you have made your point. However, Quartz Composer isn’t a general purpose programming language. Making screensavers easily or inspecting HID devices don’t qualify it. Could you, for instance, write a database using it? It also hasn’t brought programming to many Mac users, has it? What percentage of Mac users do you reckon have used it, let alone become programmers as a result? AppleScript, for all its shortcomings and history, has in its day been popular among those who wouldn’t have considered themselves to be programmers, and quite a few of those have gone on to develop substantial Mac apps using it. Similarly for the other languages considered above. Howard. Like 18 donatj on September 17, 2023 at 8:26 pm Reply I came to the comment section to ask how they could have completely missed Quartz Composer. It was the BEST way to make screensavers back in the day with ZERO coding. Liked by 1 person 19 Warren Nagourney on September 16, 2023 at 10:46 pm Reply It seems that Apple is intending Swift Playgrounds to be their current means of introducing new people to programming. I have had some programming experience over the last 50 years and do not consider Swift to be a good language for beginners, especially with the poor documentation they provide. The language is very complicated and includes some concepts which are obscure at best – (Optionals is an example). I was interested in the concept of Object Oriented Programming some 25 years ago and read the book on the subject where the analogy to “software integrated circuits” was made. Constructing a program using Swift is similar to building a circuit using ICs without a manual which provides pinouts and circuit examples. The situation is even worse using Metal and using things like the Complex Data type whose documentation is extremely hard to find. I recently had the very frustrating of using Swift and Metal to update some physics demo apps I originally wrote in the Mac some 20 years ago. The recent experience was not a pleasant one. I really wish Apple would invest some of its fortune in better documentation. Liked by 2 people 20 hoakley on September 17, 2023 at 7:43 am Reply Thank you. I sort of agree about Swift, but I’m an old stick-in-the-mud and still hark back to Object Pascal. Swift is accommodating enough much of the time to let you code plainly. Although there are times when dealing with the API and more idiomatic features that you do need to use some of those new-fangled things, much of the time you can write plain code that might almost pass as Object Pascal. Just because a language has those features doesn’t mean you have to use them everywhere. Much of Swift is actually better documented than most of macOS. There are also abundant resources online. Where it gets more scant is in coding asynchronous processes, for example, which has been changing quite rapidly. But I do still hanker after the clarity of Object Pascal. Howard. Like 21 Warren Nagourney on September 17, 2023 at 1:23 pm Reply What I was looking for when writing Swift code was a list of classes and all of their methods and instance variables with a brief description of what each does. Example code would have been welcome. The Apple docs did not provide this, or perhaps did but I didn’t use them properly . The online description of Swift was very good but a bit abstract – I missed coding examples and needed to use Stack Overflow, which actually solved one of my most thorny problems (communication between instances of two different classes – it took me a week and multiple requests to the community to solve this). To make my Metal code work I needed to search for an inline tutorial and modify it to work for me. There is a lot of “boilerplate” that needs to be done in Metal and I was able to use my existing OpenGL ES code for the rest. Of course, online Metal references were oriented toward games – I needed to render mathematically defined surfaces, which has different requirements. One of my routines absolutely needed Complex to be able to be coded economically. I used Swift Numerics and the refusal of Swift to use mixed type expressions made for very ugly code compared to the C code I had used originally. Adding a real and complex number is totally unambiguous and Swift required various casts to make it work. So I agree that one can write Swift code in a straightforward fashion (like C), but the code was much harder to read and required highly verbose constructions. While I understand formally what Optionals do, most online code is full of them and I never understood why they were needed. For example, why is an automatically generated outlet always a forcibly unwrapped optional? Swift docs frown on using this kind of thing. I eventually got the app to work, but it’s performance was little better that it was on my original app which used Objective C and OpenGL on the Mac was was fairly easy to write. I suspect there are inefficiencies all over the place. My original code ran well and smoothly on a PowerPC G4 laptop. Sorry about the length of my response. Liked by 1 person 22 hoakley on September 17, 2023 at 7:04 pm Reply No problems. Do you use Dash? A combination of that for reference and Apple’s free Swift books goes a long way, supplemented by online sources including some of the many Swift tutorials. Howard. Like 23 Warren Nagourney on September 17, 2023 at 10:19 pm Reply Thank you. I have Dash on my phone mainly for LaTeX help. It is no longer supported on iOS so I will download it on my Mac. Excellent idea. Liked by 1 person 24 Darwin on September 17, 2023 at 6:41 pm Reply Hypercard was a huge success which everyone who was around at the time already knows. Liked by 1 person 25 hoakley on September 17, 2023 at 7:13 pm Reply Thank you. Yes, I used it extensively myself. But HyperCard was an application, not a programming language. The underlying scripting language was HyperTalk. If you used that much, then you will well remember its severe limitations. As I wrote above, its big benefit was that it later formed part of the model for AppleScript. Howard. Like 26 diimdeep on September 17, 2023 at 6:47 pm Reply Perhaps you’ve forgotten, but every macOS version has come with numerous pre-installed interpreters, bindings to Apple technologies, and other developer software, suitable for both learning programming and development right out of the box. To name a few: Perl, Python, Ruby, PHP, Apache, Emacs, Vim—all available in Terminal.app. It’s only recently that Apple decided to stop including Python, Ruby, and Perl with macOS. More information here https://developer.apple.com/documentation/macos-release-notes/macos-catalina-10_15-release-notes#Scripting-Language-Runtimes When compared to Windows, it’s clear that Mac has been a significant driver for bringing programming to people. Liked by 2 people 27 hoakley on September 17, 2023 at 7:17 pm Reply Perhaps you’ve forgotten that all those are cross-platform languages. I look forward to seeing your evidence that they accomplished on the Mac for programming what the Mac did for desktop publishing, for example. And you’re only referring to Mac OS X and macOS anyway; I don’t recall Classic Mac OS coming with support for any of those (and I’m not sure that some of them like Ruby even existed). Does the average Mac user really write their own Python code, or perl? I don’t think so. Even advanced users are unlikely to. Howard. Like Leave a Reply This site uses Akismet to reduce spam. Learn how your comment data is processed. Quick Links Downloads Mac Troubleshooting Summary M1 & M2 Macs Mac problem-solving Painting topics Painting Long Reads Search Monthly archives September 2023 (47) August 2023 (72) July 2023 (79) June 2023 (73) May 2023 (79) April 2023 (73) March 2023 (76) February 2023 (68) January 2023 (74) December 2022 (74) November 2022 (72) October 2022 (76) September 2022 (72) August 2022 (75) July 2022 (76) June 2022 (73) May 2022 (76) April 2022 (71) March 2022 (77) February 2022 (68) January 2022 (77) December 2021 (75) November 2021 (72) October 2021 (75) September 2021 (76) August 2021 (75) July 2021 (75) June 2021 (71) May 2021 (80) April 2021 (79) March 2021 (77) February 2021 (75) January 2021 (75) December 2020 (77) November 2020 (84) October 2020 (81) September 2020 (79) August 2020 (103) July 2020 (81) June 2020 (78) May 2020 (78) April 2020 (81) March 2020 (86) February 2020 (77) January 2020 (86) December 2019 (82) November 2019 (74) October 2019 (89) September 2019 (80) August 2019 (91) July 2019 (95) June 2019 (88) May 2019 (91) April 2019 (79) March 2019 (78) February 2019 (71) January 2019 (69) December 2018 (79) November 2018 (71) October 2018 (78) September 2018 (76) August 2018 (78) July 2018 (76) June 2018 (77) May 2018 (71) April 2018 (67) March 2018 (73) February 2018 (67) January 2018 (83) December 2017 (94) November 2017 (73) October 2017 (86) September 2017 (92) August 2017 (69) July 2017 (81) June 2017 (76) May 2017 (90) April 2017 (76) March 2017 (79) February 2017 (65) January 2017 (76) December 2016 (75) November 2016 (68) October 2016 (76) September 2016 (78) August 2016 (70) July 2016 (74) June 2016 (66) May 2016 (71) April 2016 (67) March 2016 (71) February 2016 (68) January 2016 (90) December 2015 (96) November 2015 (103) October 2015 (119) September 2015 (115) August 2015 (117) July 2015 (117) June 2015 (105) May 2015 (111) April 2015 (119) March 2015 (69) February 2015 (54) January 2015 (39) Tags APFS Apple AppleScript Apple silicon backup Big Sur Blake bug Catalina Consolation Console Corinth diagnosis Disk Utility Doré El Capitan extended attributes Finder firmware Gatekeeper Gérôme HFS+ High Sierra history of painting iCloud Impressionism iOS landscape LockRattler log logs M1 Mac Mac history macOS macOS 10.12 macOS 10.13 macOS 10.14 macOS 10.15 macOS 11 macOS 12 macOS 13 malware Mojave Monet Monterey Moreau MRT myth narrative OS X Ovid painting Pissarro Poussin privacy realism Renoir riddle Rubens Sargent scripting security Sierra SilentKnight SSD Swift Time Machine Turner update upgrade Ventura xattr Xcode XProtect Statistics 15,428,456 hits Blog at WordPress.com. About & Contact Macs Painting Language Tech Life General Downloads Mac problem-solving Extended attributes (xattrs) Painting topics Hieronymus Bosch English language LockRattler: 10.12 Sierra LockRattler: 10.13 High Sierra LockRattler: 10.11 El Capitan Updates: El Capitan Updates: High Sierra and later LockRattler: 10.14 Mojave SilentKnight, silnite, LockRattler, SystHist & Scrub DelightEd & Podofyllin xattred, Metamer, Sandstrip & xattr tools 32-bitCheck & ArchiChect XProCheck, T2M2, Ulbow, Consolation and log utilities Cirrus & Bailiff Taccy, Signet, Precize, Alifix, UTIutility, Sparsity, alisma Revisionist & DeepTools Text Utilities: Nalaprop, Dystextia and others PDF Keychains & Permissions LockRattler: 10.15 Catalina Updates Spundle, Cormorant, Stibium, Dintch, Fintch and cintch Long Reads Mac Troubleshooting Summary LockRattler: 11.0 Big Sur M1 & M2 Macs Mints: a multifunction utility LockRattler: 12.x Monterey VisualLookUpTest Virtualisation on Apple silicon LockRattler: 13.x Ventura System Updates Search High: Vanishing glaciers Saturday Mac riddles 221 Begin typing your search above and press return to search. Press Esc to cancel. Follow",
    "commentLink": "https://news.ycombinator.com/item?id=37546241",
    "commentBody": "How the Mac didn’t bring programming to the peopleHacker NewspastloginHow the Mac didn’t bring programming to the people (eclecticlight.co) 196 points by mpweiher 22 hours ago| hidepastfavorite240 comments mitchbob 16 hours agoIn the early 90&#x27;s, Apple had big plans for end-user programming. Larry Tesler, the head of Apple&#x27;s Advanced Technology Group, gathered people from ATG and elsewhere at Apple who were working on aspects of end-user programming for a project code-named Family Farm. The idea was that most of the pieces that were needed for end-user programming were already working or close, and that with a few months of work they could integrate them and finish the job.The project sputtered when (1) it became clear that it was going to take more than a few months, and (2) Tesler was largely absent after he turned his attention to trying to save the Newton project.AppleScript was spun out of the Family Farm project, and William Cook&#x27;s paper [1][pdf] includes some of the history, including its relationship to Family Farm and HyperTalk, HyperCard&#x27;s scripting language.AppleScript shifted the focus from creating something that end users without previous programming experience could easily use to application integration. I was a writer who worked on both Family Farm and AppleScript, and I was both surprised and hugely disappointed when AppleScript was declared finished when it was obviously not suitable for ordinary users. I assumed at the time that there had been no usability testing of AppleScript with ordinary users, but Cook&#x27;s paper claims there was. All this was even more disappointing in light of the fact that so much more could have been learned from the success of HyperCard and HyperTalk, and that the HyperCard developer and champion Kevin Calhoun was just around the corner.The Wikipedia article on HyperCard [2] gives the history of the product, including the pettiness of Steve Jobs in cancelling it.[1] https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;~wcook&#x2F;Drafts&#x2F;2006&#x2F;ashopl.pdf[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;HyperCard reply waffletower 12 hours agoparentWhile the Hypercard discontinuation in 2004 can be viewed as petty, that was quite late in its life. I think this article neglects many of the developer gifts that came back into the Apple fold via the NeXT merger which Steve Jobs can legitimately take some credit for. Cocoa was a major development which is completely ignored here. Also, the evolution of the World Wide Developer&#x27;s Conference, and its supporting materials is not covered in the article as well. reply pjmlp 7 hours agorootparentWhile NeXT UI frameworks can be considered great to be made available to an wider audience via OS X and Cocoa, similar ideas were already mainstream on the Windows world via Visual Basic and Delphi, and much widespread in terms of bringing programming to the people. reply Terretta 11 hours agoparentprevShortcuts certainly have a less devoted following than HyperCard but probably many more people making them. reply TeMPOraL 7 hours agorootparentThat&#x27;s mostly because the former is available, while the latter is not. reply Glyptodon 11 hours agoprevIn the late &#x27;90s, I really wanted to learn how to make a computer game. Our family&#x27;s only computer was some kind of Power Mac. I managed to find my way into a basic copy of code warrior (because it looked like a programming tool based on a computer catalog). But we only had dial up internet that had to be used with permission because it hogged the phone lines. And nothing built in to the manual really explained enough to do much besides some basic stuff involving printing characters to the screen, and some keyboard input. If the manual had any kind of tutorial sufficient to make snake or even just a really bad version of asteroids or galaga or something I&#x27;d probably have never put down the keyboard. But instead, I kind of lost interest because I assumed I was missing something you needed to make graphical programs. Another barrier was that what internet time I did have mostly lead to Windows stuff. Not that I really knew how or what to look for.The first program I was ever able to write that involved things actually moving on a screen came a couple years later and was written on my graphing calculator. Solely because the calculator came with a big manual that documented all kinds of functions you could use in a program. And so long as you were writing only a few pixels at a time its built-in basic language was just barely good enough. reply jwells89 10 hours agoparent> And nothing built in to the manual really explained enough to do much besides some basic stuff involving printing characters to the screen, and some keyboard input. If the manual had any kind of tutorial sufficient to make snake or even just a really bad version of asteroids or galaga or something I&#x27;d probably have never put down the keyboard. But instead, I kind of lost interest because I assumed I was missing something you needed to make graphical programs.This sounds a lot like myself as a kid in the around 1999&#x2F;2000 after getting my parents&#x27; Performa tower as a hand-me-down.I&#x27;d become aware that it was possible to build one&#x27;s own programs, but exactly how was extremely murky. I grabbed various development tools mentioned on the internet, but as you note tutorials were nonexistent and IDEs, consoles, etc may as well have been ancient tablets written in an archaic language.Finally I stumbled across REALBasic, I think maybe on a MacAddict shareware&#x2F;demo disc, which was analogous to Visual Basic in the Windows world (though unlike VB, RB could compile binaries for both platforms). That was far more intuitive… just drag a button onto your window-canvas and double-click the button to edit its code. Examples were also much more common and easy to find on the internet and so before too long I was hitting the ceiling of what was possible in RB.That segued nicely into Cocoa development when OS X rolled around, which brought Project Builder and similar (but far more extensive) WYSIWYG UI builder with Interface Builder. That took a lot longer to become capable with, but led me to where I am today as a mobile dev. reply 5e92cb50239222b 8 hours agorootparentSounds very much like my experience, except that I was using Delphi. Non-existent documentation (complicated by me not speaking English at the time), no friends or relatives using computers for anything other than office documents (and very few having or using them at all), no access to the internet, and a friendly neighborhood software store that sold \"pirated\" copies of Delphi 7 for something like $2. That was an excellent environment to start precisely for reasons you described: double click on any element created an event handler for its \"main event\", and you could take it from there.But: Object Pascal&#x2F;Delphi is a quite powerful language and you can write really complicated stuff with it, compiled into a relatively small statically linked executable that worked just about everywhere without the need to install anything. (300-400 KB binaries seemed large at the time, little I knew what awaited us 20 years down the line.)The need to dig and learn everything yourself definitely had some impact and provided some lower level knowledge of things many newer developers don&#x27;t seem to know or care about, but I very much prefer what we have today where you&#x27;re not expected to learn to read what looks like ancient Egyptian from scratch to write a basic calculator. Good riddance to those times. reply pharmakom 5 hours agorootparentprevREAL Basic was actually an amazing program. Easily throw together a cross platform GUI and one click to create a self contained executable. Remarkable that we struggle with this in Modern stacks. reply tarsinge 4 hours agoparentprevThis is my story too. I remember playing with Basic on an Apple II and how it was a revelation, but then a big void in the 90s up until graphing calculator in high school. I was so frustrated, programming seemed a parallel world I had no idea how to reach but was deeply attracted to. I had fun with HyperCard and ResEdit but I knew I was just scratching the surface. When I finally grabbed a copy of CodeWarrior it was way too complex for me, and console-like C programs too limited. If only something like Unity had existed back then.. reply samvher 10 hours agoparentprevThis is so recognizable!I also got a strong interest in programming 1998&#x2F;1999-ish and also had only a Mac at home. I emailed the guy who authored a chess application (his email was in the splash screen) over our dial up connection. He told me I would need to learn C or C++ and I had no idea how to act on that (I was 8 or 9 at the time, living in a very rural area).4 or 5 years later I got my own laptop and installed Linux on it. Then I started learning bash and tried (and failed) to install Linux From Scratch. That did get me to a point where I could program. reply mkl 10 hours agoparentprevTurbo Pascal, Delphi, QuickBasic (not so much QBasic), and Visual Basic had amazing built-in documentation for learning how to do things in a pre-internet way. That&#x27;s what got me started. reply Thedarkb 3 hours agoparentprevI had quite a nice introduction to programming when I downloaded ChipmunkBASIC on to the family PowerMac some time in the mid &#x27;00s. reply dreamcompiler 20 hours agoprevApplescript could have been great except for two (IMHO) huge design misfeatures:1. You can&#x27;t write a BNF for it because its syntax depends on what program you&#x27;re trying to control.2. It wants to be \"English-like\" which means you can always think of 20 plausible ways to express any action, only one of which will actually compile.(1) frustrates professional programmers and (2) frustrates everybody, so almost nobody uses it. reply simonw 14 hours agoparentI finally started writing AppleScript a few months ago when I realized GPT-4 can write it for me, which eliminated almost all of my previous frustrations with it: https:&#x2F;&#x2F;til.simonwillison.net&#x2F;gpt3&#x2F;chatgpt-applescript reply julianeon 13 hours agorootparentI feel like scripting with non-standard languages (not Python, not JavaScript) is one of the killer features of GPT-4. It was trained on a large amount of this material and can spit out functional scripts for small tasks (which you can then chain together) incredibly quickly. reply jph00 11 hours agorootparentprevChatGPT can even show you how to do this in Python BTW! from ScriptingBridge import SBApplication notes_app = SBApplication.applicationWithBundleIdentifier_(\"com.apple.Notes\") for folder in notes_app.folders(): for note in folder.notes(): print(f\"{note.name()}\\n{note.body()}\\n\") reply gnicholas 11 hours agorootparentprevThat&#x27;s interesting, and would be doable for small scripts that I could fully understand. But I would be hesitant to use it for larger jobs, where it might end up misunderstanding me, and I wouldn&#x27;t necessarily realize there was a mistake. Applescript is very powerful, and I wouldn&#x27;t want to risk it deleting&#x2F;moving&#x2F;editing files. Perhaps in the future, when people have used it for this purpose for years (without incident), I would trust it more. reply dreamcompiler 12 hours agorootparentprevThis is sheer genius. The Javascript stuff was new to me too. reply fulladder 18 hours agoparentprevMy recollection of the English-like nature of Applescript is that there would be multiple ways to express something, and more than one of them would work, but it always seemed totally unpredictable which would work and which wouldn&#x27;t. That&#x27;s actually a bit worse (more frustrating) than the way you&#x27;re describing it. Overall, I agree with you that it was unsatisfying for both professional programmers and non-programmers, and that&#x27;s why it failed to take off. reply giantrobot 19 hours agoparentprev> You can&#x27;t write a BNF for it because its syntax depends on what program you&#x27;re trying to control.More importantly it AppleScript operates on sending and receiving messages from those external programs and thus has no insight into their current state&#x2F;progress unless it&#x27;s specifically communicated back to AppleScript.Many programs would just return back to AppleScript immediately after getting a command even if the operation requested took a lot of time. There was no way to register callbacks (at least no obvious way) so you had to put in boilerplate to check for a file at a location to know when a long running process was complete.The challenge level between writing any two scripts could vary wildly. A program&#x27;s script dictionary might technically allow you to write a script but actually making it workable or reliable was fantastic amounts of effort. reply analog31 21 hours agoprevI had a Mac from around 1993 to 1997. I even wrote articles for minor league Mac journals.My impression is that Microsoft was founded to bring programming to the people, and the Mac was founded to bring great software experience, which I appreciate. But Apple (represented in my mind by Steve Jobs) didn&#x27;t want his platform to be flooded by crappy software. Even HyperCard never had his wholehearted support (I used it extensively, despite its odd and limited language), and I think Apple was glad to finally dump it.If you had a reason to write crappy software, as I did, you ended up with Visual Basic. What I mean by \"crappy\" is software that solves a problem reliably, but can&#x27;t be scaled. For instance, installation might require help from the author, and entering a bad value might make it crash, but a small handful of tolerant users can handle these issues.The solution today, with open source, is that the people bring programming to the people. reply TheOtherHobbes 16 hours agoparentOpen source is completely impenetrable to most of the population.Between an impressive list of incompatible languages, the vagaries of Git, the complexities of command line access, the reliance on some version of the Unix folder structure - with endless confusion between &#x2F;etc, &#x2F;local&#x2F;etc&#x2F;, &#x2F;usr&#x2F;local&#x2F;etc and on and on, the weirdnesses of package managers, and completely unfamiliar editors like Emacs and VSCode, the scene might as well be proprietary.VB at least made if possible to write software in a reasonably accessible and integrated way. So did Hypercard. So does Excel.The real problem with Automator, AppleScript, etc, is that they&#x27;re hopelessly underdocumented and the tools are mediocre.If Apple had made them more prominent and documented them thoroughly they&#x27;d have been far more successful - perhaps in an industry-changing way.But they seem more like side projects that someone spent a lot of time and a little money on. Clearly Apple was never seriously interested in them. reply OfSanguineFire 16 hours agorootparent> the scene might as well be proprietary.That is such hyperbole. I have no formal IT background and I taught myself the Linux command line, Python, and Emacs in the course of a single year after I heard about this new Linux thing that was free to install. This was 2000, and from communities like Slashdot I saw there were plenty of other nerdy young people taking up the CLI and other Linux stuff, just out of curiosity and without any connection with the IT industry.Sure, none of this software will ever appeal to the broad masses*, but since the 1990s programming has become so much more accessible to the general public, because of both the free-of-charge software and the free-of-charge documentation. Learning to program on earlier platforms could be very expensive.* Still, I have heard rumors of a corporation in the 1980s where the secretarial staff – middle-aged women with no formal computer-science education – wrote some custom Emacs Lisp and&#x2F;or Awk. Maybe learning even relatively arcane stuff can be done by anyone if their salary depends on it. reply Falkon1313 9 hours agorootparentI would still disagree. 80s home computers pretty much all came with BASIC built-in, up to the PCs with QBASIC. The documentation was right there, and you had everything you needed to just boot up and start programming. Many even booted up within the programming language by default.And at the time, most programs were text-mode. I&#x2F;O was only keyboard, screen, and disk (and maybe mouse, joystick, and serial port if you wanted to get fancy). When your program started you had full control of the entire system. There was no multitasking, 100+ processes running in the background, nor GUI architecture over an OS subsystem. So you only needed to learn a very very few things to be able to build software roughly equivalent to professional store-bought software. Simple logic and data structures and 3-6 types of I&#x2F;O and that was it.From there it was a short step to something more powerful like Turbo Pascal or Visual Basic, again both of which were pretty much \"batteries included\" everything you need in one tool.Today&#x27;s landscape with its enormous stacks, build tools, cambrian explosion of languages, frameworks, and libraries, etc., and layers upon layers of abstraction and indirection between you and the machine, plus countless dependencies, package managers, containers, etc., all of which are constantly changing and shifting out from under you, is baffling even to professional developers.Luckily, it is easier to find documentation, tutorials, and other actual people to ask about stuff. But also a lot of that is quickly outdated, faddish and evangelical, just assumes that you already know 500 other things, or assumes that you know absolutely nothing, so only covers the barest basics.So it&#x27;s really still more difficult than just picking up a book about C, QBASIC, Turbo Pascal, or Visual Basic and getting started was back in the day. reply analog31 1 hour agorootparentI&#x27;ve mostly self taught in programming since around 1981, and I&#x27;ve helped several friends and colleagues learn. My impression is that it&#x27;s more difficult, like you say, but not prohibitively so.People just instinctively steer clear of the \"professional\" tools and documentation, and choose their battles. Even well into the Windows 3.1 era, a lot of people who programmed stuck with MS-DOS text mode. Today, our code runs in Jupyter notebooks. We get stuff done. If it needs a framework or a container, we just don&#x27;t go there.There&#x27;s a mild suspicion amongst amateurs, that the professionals are creating the complexity for their own entertainment or job security. It doesn&#x27;t seem to make the software better (most \"users\" think that software is getting worse and worse), or less costly to write and maintain.To put it more charitably, the struggles of commercial programmers are not invisible to us. I work in a department adjacent to a large programming team. By not attempting to write commercial-scale software, we avoid many if not most of the headaches. reply acqq 7 hours agorootparentprevThere was at least one popular home computer which was never just text mode but booted in permanent bitmapped graphic mode, directly in BASIC line editor. Entering a command would execute immediately, entering it with the line number in front would add that line to the program.So right after boot a single command could draw a line or a circle. One was not supposed to even type the letters for any command, but had to enter it like on the pocket calculators: the commands were written on the keyboard, in more colors, so just looking at the keyboard one directly saw all the available commands.The barrier there was learning to enter the needed mode to reach the appropriate color and learning the syntax and the meaning of the commands.Every user learned to enter at least one command and its syntax: the one which allowed loading any program from the tape. It was a full command with the empty string parameter meaning \"match any name\": LOAD \"\" reply analog31 14 hours agorootparentprevPeople are managing to wring Python &#x2F; Jupyter installations together using online tutorials, writing, and sharing useful code. The open source community brought us those tools. Today, all of the \"top\" programming languages have open-source toolchains.I agree about the portability of VB, HC, and Excel code, including spreadsheets. At least, VB6. ;-) But people are managing with the more fragmented ecosystems of open source tools such as Python toolchains. reply brailsafe 16 hours agorootparentprevInterestingly tools like ChatGPT can make some of these slept-on tools more usable. \"Write me a script using __ that sends me notifications whenever __ happens\" reply GeekyBear 17 hours agoparentprev> My impression is that Microsoft was founded to bring programming to the people, and the Mac was founded to bring great software experience, which I appreciate.Apple did develop a native version of Basic that let you create Basic programs that took advantage of the Mac UI, but Microsoft forced them to cancel it as a condition for continuing to license their Basic for the Apple II.> Apple&#x27;s original deal with Microsoft for licensing Applesoft Basic had a term of eight years, and it was due to expire in September 1985. Apple still depended on the Apple II for the lion&#x27;s share of its revenues, and it would be difficult to replace Microsoft Basic without fragmenting the software base.Bill Gates had Apple in a tight squeeze, and, in an early display of his ruthless business acumen, he exploited it to the hilt. He knew that Donn&#x27;s Basic was way ahead of Microsoft&#x27;s, so, as a condition for agreeing to renew Applesoft, he demanded that Apple abandon MacBasic, buying it from Apple for the price of $1, and then burying it.He also used the renewal of Applesoft, which would be obsolete in just a year or two as the Mac displaced the Apple II, to get a perpetual license to the Macintosh user interface, in what probably was the single worst deal in Apple&#x27;s history, executed by John Sculley in November 1985.https:&#x2F;&#x2F;www.folklore.org&#x2F;StoryView.py?story=MacBasic.txtSo in the case of the Mac, Microsoft used anticompetitive pressure to keep programming away from the people. reply fulladder 14 hours agorootparentIt seems like you&#x27;re reacting to the phrase \"bring programming to the people\" occurring in the same sentence as Microsoft. I don&#x27;t think GP was trying to present Bill Gates as having been on some kind of altruistic moral crusade. I think the point GP was making is that Microsoft was founded on compilers for hobbyists, whereas Apple was thinking about end user experience.Maybe the phrase \"to the people\" is confusing things because it suggests some kind of noble, high-minded motivations on the part of Gates. That wasn&#x27;t how Gates (or Jobs) thought. Actually, the whole idea of a company having some social responsibility mission was not something that existed at that time. These guys wanted to sell a product that people would pay money for, that&#x27;s all. In that era, this was perfectly socially acceptable and completely normal.The statement \"Microsoft was founded to bring programming to the people\" is true, but it may be a little unclear if people are reading it as suggesting something other than the profit motive. However, for a long time, Microsoft really did view its own success as dependent on providing customers ways to write and sell their own software. That is in fact empowering to the customer. I&#x27;m pretty sure Gates did care about this because it was what was driving sales of his products. reply GeekyBear 13 hours agorootparentThere is also the popular conspiracy theory that Apple was somehow opposed to allowing users to program the Mac.They created a Mac native Basic at launch, and only Microsoft&#x27;s anticompetitive arm twisting kept if from being released. reply Nevermark 9 hours agorootparent> They created a Mac native Basic at launch, and only Microsoft&#x27;s anticompetitive arm twisting kept if from being released.Wow, that almost makes me want to cry.I discovered personal computers when the computer basically felt like a programming language.I might be wrong, but if a friendly language installed on every computer out of the box had held, the evolution of “people’s” languages might have been a huge thing to this day. reply flomo 8 hours agorootparentprevNot disagreeing with your point about MS, but it&#x27;s not a conspiracy theory that Macintosh dev tools were nonexistent. Apple&#x27;s solution was \"buy a Lisa\". (well, maybe they considered this a 3rd party opportunity.) No indication they even wanted homegrown BASIC type stuff. reply sbuk 7 hours agorootparent> No indication they even wanted homegrown BASIC type stuff.Further up this very thread is a link from a first party source that refutes that entirely. reply flomo 6 hours agorootparentNo, not at all, the folklore story doesn&#x27;t even address Apple&#x27;s strategy. Their developer relations were \"Apply Here\" back then. Macintosh shipped with zero developer tooling, not C&#x2F;C++, not Pascal, not BASIC.And, we have no idea what Donn&#x27;s BASIC was actually like ... it probably wasn&#x27;t \"Visual Basic\", or even a platform. Just better than Microsoft&#x27;s halfazz port of their micro basic. (which did have some quickdraw commands.) replyturndown 16 hours agorootparentprevFrom that story is sounds like MacBasic wasn’t a priority at all to Apple, even on the Mac team. The fact they let MacBasic slip from the launch window is a pretty big red flag, I’d be interested to hear why Bryan Stearns decided to leave(I feel the author is saying a lot by not touching on this more)I think the idea that Microsoft or Apple were founded for any reason but money is pretty much a joke. Every action and reaction in the story revolves around money and how to get it. reply GeekyBear 16 hours agorootparent> From that story is sounds like MacBasic wasn’t a priority at all to AppleDid you miss the part about Microsoft threatening the hardware line that produced the vast majority of Apple&#x27;s profits at the time?It sounds to me more like the priority was not having their profitability decimated.They killed a Mac native version of Basic that had been completed because Microsoft forced their hand.>The fact they let MacBasic slip from the launch window is a pretty big red flag,As mentioned in the article, they needed to wait for the system APIs to stabilize.> Basic still had a hard time getting traction, especially since the system was evolving rapidly beneath it. reply turndown 15 hours agorootparent>Did you miss the partDid you miss the part where I said what everyone did here was motivated by money?>It sounds to me more like the priority was not having their profitability decimated.See above.>They killed a Mac native version of Basic that had been completed because Microsoft forced their handSee above.>As mentioned in the article, they needed to wait for the system APIs to stabilizeSee above.Did you miss that part? reply Nevermark 9 hours agorootparentSo I guess you did? You did miss that part??—Sorry for seeing humor in your non-response repeating refrain, and then adding my own poor attempt at humor.But, what you wrote was obviously read.And responded to directly with a good point that you seem to have missed twice now. replyfulladder 20 hours agoparentprevYou&#x27;re exactly right that Apple has always had an ambivalent relationship with software developers. On the one hand, they needed them for their products to have any value for consumers. On the other hand, Jobs clearly saw low-quality, third party software as something that could taint the product Apple was selling. They really saw app developers as something very dangerous and difficult to control. Gates&#x27;s view was always to just let anybody write anything they wanted, and the market would sort the good from the bad.I like the egalitarian nature of the Gates view, but ultimately I think Jobs was correct that the mere existence of low-quality software hurts your entire platform, and no amount of high-quality software can completely offset this. reply FirmwareBurner 17 hours agorootparent>Gates&#x27;s view was always to just let anybody write anything they wanted, and the market would sort the good from the bad. I like the egalitarian nature of the Gates view, but ultimately I think Jobs was correct that the mere existence of low-quality software hurts your entire platform, and no amount of high-quality software can completely offset this.That is wrong on so many levels. I grew up in a Windows environment in the &#x27;90s tro &#x27;00s and had the best video games in the world at my finger tips, while Mac users at the time had what, some high quality photo editing software? Good for them I guess.So unless you don&#x27;t count video games as software, Gates was right. DOS and Windows being the main platforms for the hottest videogames of the moment, was what led to the popularity of the PC over Macs at the time. I think Doom alone was responsible for millions of PC and MS-DOS sales.Yeah, Job&#x27;s Next-Step machines and SW were cool, UNIXy and all, but to what end if nobody but the wealthiest businesses and institutions bought them in numbers you can count on a few hands? In fact, Jobs understood from the fail of the Next-step and the success of the DOS-PC that you need to get your devices cheaply in the hands and homes of as many casual consumers as possible (hence the success of the colorful and affordable iMac G3) and forget the premium business workstation market. reply fulladder 16 hours agorootparentI wrote the comment and I too grew up in the MS-DOS&#x2F;Windows world. My family had a Mac and an Apple ][e, but I didn&#x27;t use them as much as the various PCs.As I say in another comment, I think Gates&#x27;s view was right for that time. The Jobs view was needlessly limiting the potential market for his machines. Gates didn&#x27;t make the machine, but he was making a thing that was sold with every machine, and he understood that more apps means more reasons why people might want to buy a PC.One problem with the Gates view today is that if you make it easy for anyone to write and distribute any kind of app, you end up with not only a lot of low-quality software but even negative-quality software like ransomware and malware. It&#x27;s surprising that so many people want to write that stuff, but apparently they do. Every modern platform, including Windows, has gone to extreme lengths to block this stuff, even though the more laissez-faire approach would be to just ignore it and assume other 3rd parties will write programs to block it. The problem was that malware was an existential threat to the entire Windows platform, and Microsoft couldn&#x27;t afford to just let the market sort that one out.I believe the Jobs view is the correct one for the present era. Every platform has app stores, code signing, DRM, and other kinds of security and non-security restrictions. It&#x27;s certainly easier to run random code on Windows 10 than macOS Ventura (or especially iOS), but no platform vendor is completely app neutral these days. reply wkat4242 15 hours agorootparentI don&#x27;t think closed app stores are the answer. It doesn&#x27;t actually prevent malware.Much better is a genuine permissions system that the user controls and the apps can&#x27;t circumvent.The reason closed app stores are so popular isn&#x27;t because of security but because everyone wants a finger in that sweet 30% take pie. reply fulladder 14 hours agorootparentI agree with you 100%. I think maybe the problem is that the platform vendors think it&#x27;s too difficult to explain fine-grained access controls to end users, whereas an app store is dead simple to explain. And, as you observe, an app store makes money whereas ACLs do not. reply nuancebydefault 16 hours agorootparentprevIndeed I think Jobs approach to software dev was too far ahead of time, Gates pragmatic approach proved to have more leverage for computer&#x2F;platform sales and growth. reply Nevermark 8 hours agorootparentAnd greater innovation in the application space.Not that there wasn’t innovation on the Mac, but not as much, and often in the end, not as successful. reply sealeck 17 hours agorootparentprevApple has always gone for the premium end of the market, and with vastly increasing wealth inequality, that&#x27;s where the money is these days. You can see this is in other luxury industries which make incredible amounts of money considering how small their markets are (in terms of numbers of participants).This focus on high-quality software has also encouraged _better_ software developers to build in Apple&#x27;s ecosystem. Even today a lot of the best software is only available for Mac or iPhone. reply fulladder 16 hours agorootparentI agree with FirmwareBurner&#x27;s sibling. All personal computers were expensive luxury items at that time. Most of them cost as much or more than a used car, which people would have considered much more useful than a computer.Apple&#x27;s machines were the most expensive, but it wasn&#x27;t because they were higher quality (that was not the general perception outside the Apple world). It was because Apple refused to allow clone makers into the market, so they were manufacturing in tiny volumes compared to Dell and Compaq and a million other random names like Packard-Bell (Sear&#x27;s store brand PC) that probably no one remembers.> Even today a lot of the best software is only available for Mac or iPhone.I really don&#x27;t see that at this point, and I do use my Mac every day. Most of the software I use is also available on Windows and Linux, and anything that isn&#x27;t has a clear Windows or Linux equivalent.The only software I use that is absolutely only available on a single platform with no equivalent is actually Windows software. I&#x27;m not a gamer, but that&#x27;s apparently a whole category that is Windows-only.I&#x27;m curious what Mac software you use in 2023 that is only available on Mac. reply GeneralMaximus 9 hours agorootparent> I&#x27;m curious what Mac software you use in 2023 that is only available on Mac.Ulysses, Bear, Things, Reeder, Mela, Alfred, MindNode, just to name a few. These apps are incredibly well designed, and have no equivalents in the Windows or Linux worlds. I know because I also have a PC and I’ve looked very hard for replacements.Additionally, apps like 1Password, Scrivener, iA Writer, and Arc Browser started their life on the Mac. Some social media networks, like Instagram and VSCO, were iPhone apps before they released on other platforms. Because Apple users are generally not averse to spending money on software, all the really good software reaches the Mac and iPhone a long time before it becomes available on other platforms.iCloud itself is something that no other platform can compete with. I never have to create a separate account for any app in the Apple ecosystem because they just sync using iCloud without me doing any extra work. When I install them on my phone later, my data is already there. The Windows&#x2F;Android world have no equivalent of this.Apps really are better in the Apple world. I blame Microsoft for underinvesting in native Windows UI toolkits. reply ryukafalz 10 hours agorootparentprev> I&#x27;m not a gamer, but that&#x27;s apparently a whole category that is Windows-only.Not anymore, really. Proton has done an amazing job at making the huge library of Windows games out there work on Linux, so at this point it&#x27;s a pretty good contender. (Hence the popularity of the Steam Deck.) reply FirmwareBurner 45 minutes agorootparentNot every gamer is a Steam customer.I have a library of ISO oldies that won&#x27;t \"just work\" on Linux without a shit Tonne of googling and fiddling around. reply FirmwareBurner 17 hours agorootparentprev>that&#x27;s where the money is these days.We were talking about the past. And back in those days when computers were expensive, most people and companies were writing SW for the most popular platform out there, which at the time was watever had the best bang for the buck thjat home users could afford: Coomodore, Sinclair, Amiga, MS-DOS, etc.>Even today a lot of the best software is only available for Mac or iPhone.Again, we are talking about the past. TikTok might be the hottest app today, but back then it was Doom. reply thefz 17 hours agorootparentprev> Even today a lot of the best software is only available for Mac or iPhone1% of the whole games industry is available for Mac and there are amazing pieces of software just there. reply PaulHoule 16 hours agorootparentprevI see a lot of poor people with iPhones, Apple Watches, Earpods and such. These are what you&#x27;d call an \"affordable luxury\" and probably a bargain when you consider you might go through 5 $300 Android devices in the time that you get out of a $1000 iPhone and all that time you are struggling with an Android.It&#x27;s kinda likehttps:&#x2F;&#x2F;www.goodreads.com&#x2F;quotes&#x2F;72745-the-reason-that-the-r...but the psychology is weirder in that rich people know a \"luxury\" watch costs upwards of $10k so it is quite the trick to convince the hoi polloi that a $500 watch is a luxury device at the same time.I&#x27;ve noticed also that poor folks are also very aware of expensive wireless plans from Verizon but seem to have little awareness of wired internet plans, free Wi-Fi, etc. reply FirmwareBurner 16 hours agorootparent>you might go through 5 $300 Android devices in the time that you get out of a $1000 iPhoneMaybe take care of your Android as you would an iPhone and it will also last as much as an iPhone, but at 1&#x2F;3 the cost.Most people (worldwide) are not spending 1k on a phone, even 300 is a lot. reply scarface_74 13 hours agorootparentWill you get security updates for a decade for Android phones like the iPhone 5s has had? reply FirmwareBurner 4 hours agorootparentYes, Google Play services, where most of the attack vectors lie, will get updated.Most people don&#x27;t keep a phone that long but change it after 4-5 years or so. How many 5s still exist in the wild? reply synecdoche 7 hours agorootparentprevI used the Wileyfox Swift for seven years until LineageOS stopped updates for it. I expect similar longevity from contemporary contenders. reply pjmlp 6 hours agorootparentprevProbably not, most people don&#x27;t care. reply PaulHoule 13 hours agorootparentprev... and will your Android actually charge when you plug it into a charger?I&#x27;ve had roughly five Android devices (tablets and phones) that had the curious problem that I&#x27;d plug them into a charger and it was always 50-50% if they would actually charge. This would happen when the devices were new and were starting from zero (often it would take several times to charge the device for the first time) and still happens when the devices are in regular use. All the time I leave one plugged into the charger over night and... nothing. Plug it into the same charger again and maybe it charges.I&#x27;ve noticed the same problem w&#x2F; Sony devices both cameras and the PS Vita if they are used with non-Sony chargers. reply nuancebydefault 16 hours agorootparentprevGood boots are like PV panels. Poor people don&#x27;t have the money to buy them. Middle class spends less in the end, since the investment pays off.Rich people on the other hand usually did not get rich by saving money. reply pdonis 17 hours agorootparentprev> Gates&#x27;s view was always to just let anybody write anything they wanted, and the market would sort the good from the bad.Gates&#x27;s view was also that, once the market had sorted the good from the bad, Microsoft would, if desired, just implement whatever killer apps third party developers had discovered and make them part of Windows, cutting the floor out from under the third party developers. In other words, Gates viewed third party developers as doing market research for Microsoft that Microsoft didn&#x27;t have to pay for.> ultimately I think Jobs was correct that the mere existence of low-quality software hurts your entire platformWindows is still well ahead of both macOS and iOS in terms of market share, so I think it&#x27;s more that Jobs and Gates had different definitions of success. reply fulladder 14 hours agorootparentYep, and Apple does this too. The Spotlight feature in macOS (OS-wide search, it&#x27;s the little magnifying glass in the top right) was a feature that Apple blatantly ripped off from a popular early-2000s third party extension. Adding this to the OS killed that company overnight. Forget their name, but someone else might remember.All platform vendors seem to do this, which is unfortunate. reply Jtsummers 14 hours agorootparentIt&#x27;s an easy thing to find, the name even became a verb. Sherlock was the program: https:&#x2F;&#x2F;www.urbandictionary.com&#x2F;define.php?term=sherlocked reply ttepasse 13 hours agorootparentprevTwice: The usage of a giant text input box as a launcher or command palette was pioneered on the Mac by LaunchBar and Quicksilver and later Alfred, etc. Spotlight for a long time was only a menu extra. reply liotier 7 hours agorootparentprev> Gates&#x27;s view was also that, once the market had sorted the good from the bad, Microsoft would, if desired, just implement whatever killer apps third party developers had discovered and make them part of Windows, cutting the floor out from under the third party developers. In other words, Gates viewed third party developers as doing market research for Microsoft that Microsoft didn&#x27;t have to pay for.Reminds me of how Amazon sees third-party sellers.Maybe that is the standard abusive strategy of platforms that hold a dominant market position. reply lotsofpulp 37 minutes agorootparentCopying others’ successful strategies is probably something that has existed since before apes even evolved into humans. reply sealeck 17 hours agorootparentprevHow is Windows ahead of iOS in terms of market share? The Windows phone was discontinued _years_ ago? reply DerekL 15 hours agorootparentIf you look at all types computing platforms combined, then Windows is ahead of iOS.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Usage_share_of_operating_syste... reply nuancebydefault 16 hours agorootparentprevMS made killer software, literally killing the competition by any means. reply didntcheck 20 hours agorootparentprevJobs&#x27; view sounds essentially the same as Nintendo&#x27;s view in the wake of the video games crash, which eventually became the unanimous model in that industry. With iOS they also got to enforce it like a console, with a whitelist of acceptable software reply _def 19 hours agorootparentI was so used to this view by Nintendo that it was hard to believe seeing a lot of really low quality games in the Nintendo switch store nowadays. The worst part is that it&#x27;s still not easy to publish there (or get access to the SDK in the first place) in an independent way without a publisher or a history of already successfully released games. reply fulladder 19 hours agorootparentprevYeah, I think a big part of how Gates and others came into their view was that there were so many examples like Nintendo where a platform vendor had artificially limited the third party ecosystem and it constrained the success of the platform as a whole.Basically, the Gates view was the more \"correct\" one for the 1980s&#x2F;1990s. At that time, most people did not own personal computers, even many businesses didn&#x27;t, and growth came from on-boarding these non-users. The more apps available on your platform, the more likely a non-user would be to see a reason to buy a computer at all. Also, the platform Gates was selling was much cheaper to try.Today, everyone owns at least one computing device. Platforms aren&#x27;t competing for non-users (there are none left), they are competing for users of other platforms. The Jobs view works better in that world since it improves platform quality perceptions. reply fulladder 19 hours agorootparentBy the way, there is no particular reason why the computer field had to go in this direction. If you look at the history of aviation, there were a lot of kit airplanes in the 1920s and 1930s because people thought that over time more and more people were going to want to own an airplane. That turned out to be wrong. Most people are content to pay somebody else to transport them.Saas is the computing equivalent of an airline, and it&#x27;s very popular, but it hasn&#x27;t fully displaced individual computer ownership. We&#x27;ll see what happens long term. reply smoldesu 18 hours agorootparent> Saas [...] hasn&#x27;t fully displaced individual computer ownership.The writing is on the wall though, isn&#x27;t it? Even companies that you&#x27;d expect to go all-in on user freedom and ownership have locked down their platforms and ratcheted up the service offerings. Not a single FAANG company is willing to take a definitive stand in defense of their users. They all just see a bottom line waiting to be butchered.And for most users, I&#x27;d wager SaaS has displaced computer ownership. Stuff like the App Store, Play Store and even game consoles are ultimately a service unto themselves. Even disregarding that, the \"happy path\" of modern software is paved far away from user empowerment. People are generally more willing to pay for a subscription than to switch to a more convenient hardware platform. reply fulladder 17 hours agorootparent> The writing is on the wall though, isn&#x27;t it?I&#x27;m really not sure. Five years ago, I thought that SaaS would completely replace local computing. Now, I&#x27;m less certain.Yes, SaaS is very popular and there are far fewer local applications now. However, there still seems to be some value in local applications because it&#x27;s thought to be a better end user experience. Look at Slack. It started out as a website, pure browser-based SaaS. Over time they developed a local app, which is basically the same Javascript but packaged up as a local Electron app. This seems to be regarded as superior to the web version for most people.Consider Zoom. There was initially a native client and an equivalent web version. They seem to be actually killing off the web version now -- access to it is a non-default option for the meeting creator, buried on a screen with 100+ meeting settings nobody understands or reads. They apparently want to be a local-only program.As long as there is demand for local apps, people will continue owning general purpose, re-programmable microcomputers. Maybe the vendor can lock you out of some low-level stuff like Intel ME or the BIOS, but these platforms are still allowing people to download a compiler and run their own programs directly on the metal.I&#x27;m not sure what direction it will ultimately go, but my hunch is that if it were possible for SaaS to 100% displace local computing, it would have already happened. reply smoldesu 17 hours agorootparentPerfectly fair response. I think your examples are also good evidence that people like native software, or at least the appearance of a cohesive platform. Apparently \"non-native\" apps like an SPA or WASM tool will probably turn most normal users off. Good enough native support can make a web client redundant, even if &#x27;native&#x27; means Javascript with OS-native controls.To counter that though, I think we have to define SaaS concretely and ask what a more service-driven world would look like. I consider SaaS to not only be the paid components of an OS, but also the \"free\" services like Google and the App Store. Holistically speaking, our OSes are saturated with services; Find My, Android Notify services, OCSP&#x2F;Microsoft&#x2F;Google telemetry, online certificate signing, and even &#x27;assistants&#x27; that fail to assist without internet. What more could we possibly put online?It&#x27;s a bit of a rhetorical conclusion to leave things on. I&#x27;ll certainly see worse examples in my lifetime, but the status quo is bad enough in my opinion. reply fulladder 16 hours agorootparentI agree with you about the absurd degree of service saturation. I&#x27;ve discovered many things I cannot do without an Internet connection because one trivial step in the process absolutely requires communication with some network endpoint. I found one app where, if you block it&#x27;s ability to send analytics reports back to Mixpanel, it will run 3 or 5 times but after that refuse to run again until an Internet connection is available. I thought it was absurd since it proves the developers actually considered offline use cases, but decided that they couldn&#x27;t allow that to go on indefinitely.Anyway, sure, let&#x27;s leave it there and see where things are in 5 years. It&#x27;ll be interesting to find out! replypaulryanrogers 19 hours agorootparentprevOnly if the users can be satisfied by the supposedly higher quality software, produced by devs willing to pay the Apple tax and unafraid of being Sherlock-edIf your need was niche then go away. reply fulladder 19 hours agorootparentThere are only two smartphone platforms, and both of them have app stores and make other efforts to control the overall end user experience. What do you disagree with? reply tomrod 12 hours agorootparentI&#x27;m scratching my head what besides F-Droid and Altstore are available for smartphone platforms. Unless you mean the OS itself. replyjandrese 14 hours agorootparentprev> Jobs clearly saw low-quality, third party software as something that could taint the product Apple was selling.I mostly disagree with this, so long as the low quality software isn&#x27;t pushed by the platform then it&#x27;s not that much danger to the reputation. Lack of quality software is a much larger danger, and it&#x27;s not easy to tell ahead of time where the quality will come from. The big problem is when you are running your own app store and it becomes packed with garbage. reply corbezzoli 20 hours agorootparentprevIt feels like they eventually managed to enforce this when the App Store came along, rejecting “useless” or “redundant” apps. Eventually they gave up on that idea because they loved seeing the number of apps rise, even if those apps were, in the end, low-quality software. reply plussed_reader 20 hours agorootparentIf they didn&#x27;t have hidden 1st party api stuff that 3rd parties have difficulty leveraging this moniker would feel more useful&#x2F;relevant, IMO. In the current context they use these labels to protect their workflow from other entities. reply faeriechangling 16 hours agorootparentprevWhen I setup computers for family members I&#x27;ve used things like Windows 10 S mode BECAUSE it limits the software that can be installed. They are too low skill to know what applications to trust.At my work, I similarly have gotten secure administrative workstations deployed with again, very limited lists of software. These users being compromised has such extreme consequences that one cannot trust any application to run on these computers.So I can certainly appreciate the need to be very exclusive with the software allowed on a device. Yet I see this exclusivity being rather anti-consumer in practice. iOS is the most secure operating system I&#x27;ve ever seen for the average person, and yet things like adblocking are behind a paywall. Neutering Adblock for the sake of security is considered acceptable. Blocking alternatives to WebKit may reduce the overall attack surface area Apple contends with and it also limits the functionality of the phone since v8 is the de facto standard. Blocking alternatives to the App Store absolutely enhances security while also forcing consumers to pay more. Then you get into things which have nothing to do with security truly, like how you can only make \"Music\" your default music player on iOS, not Spotify.I really appreciate what Windows did with \"S mode\" because it&#x27;s optional yet provides benefits to consumers in niche situations. I similarly appreciate the enterprise features of Windows to configure a laptop which has limited access to software to enhance security. Apple on the other hand forces the exclusive use of their software to benefit themselves at the expense of their customers. It is unacceptable and I would vote for anybody pledging to outlaw it as monopolistic abuse. Software exclusivity is useful, shoving it down people&#x27;s throats is abhorrent. reply andrewjl 16 hours agorootparent> yet things like adblocking are behind a paywallAre you referring to iCloud Private Relay? reply faeriechangling 16 hours agorootparentMore that most Adblock extensions on safari are paid such as AdGuard, whereas on Android I can get better AdBlockers working for free. reply astrange 13 hours agorootparentPurify, Hush, Ghostery are free. reply andrewjl 15 hours agorootparentprevAdGuard ad blocking is free on iOS Safari. DNS privacy and custom filters do require a subscription. reply wkat4242 14 hours agorootparentprevFirefox with ublock origin works well on Mac though. I think safari is overrated anyway. reply networkchad 19 hours agorootparentprevPersonally I’m not annoyed at low quality software - I just avoid it. What I draw a bright red line at is intrusive&#x2F;abusive software (essentially malware). I’m glad apple enforces a standard with regard to software behavior that is acceptable versus not. reply PaulHoule 16 hours agoparentprevThe 128k Mac that came out in 1984 might have been the first microcomputer that didn&#x27;t come with some kind of BASIC, machine language monitor, or other tools for developing software. That is, the point of every micro up until then was that you were going to program it yourself. In some sense it was a real step backwards in terms of people losing control of computers that Apple has continued with iDevices.At first you could not do any software development for a Mac at all on a Mac, instead you were expected to get a Lisa if you wanted to develop for the Mac. Part of that was that even though the Mac had twice the memory as most computers at that time, the GUI ate up a lot of it, so you could just barely run the bundled MacWrite and MacPaint. Part of the reason why Jobs insisted on keeping the Mac monochrome until the 1987 Mac II was to conserve memory. (When it did come out, the Mac II was priced closer to a Sun workstation than even an IBM PC AT.) reply Falkon1313 8 hours agorootparentI remember that, it&#x27;s why I didn&#x27;t get a Mac. Although it was a few years later for me.The only computer that I&#x27;d used up until then was an Apple II that had been donated to our school. I managed to talk my parents into buying one, and they took me to the store but I couldn&#x27;t find one. Asked the salesman and he said they didn&#x27;t sell them anymore but now they had these new Macintosh computers.I looked at the rinkydink black and white screen with a mostly-blank desktop with just a few icons you could drag around or click on it. Couldn&#x27;t even figure out how to get to the command line, let alone into BASIC. When I asked the salesman he said that you couldn&#x27;t and seemed utterly baffled that someone would actually want to use their computer.So I left and went and bought a PC clone instead. Plugged it in and booted it up and I was coding immediately. PCs just worked, and let you do whatever you wanted. Macs were kind of toy computers at the time.That philosophy still kind of holds to this day, though less so now. I use a Mac for work, and luckily they did finally make it possible to get to a command line and to program on it. At some point, they realized they had to give some power to it or else become utterly irrelevant. But you can tell that it&#x27;s still not intended for power users. It still can&#x27;t run most software, and what it does have is limited and simplified, from the OS on up, and still defaults to making things difficult except for the simplest use cases. reply dreamcompiler 20 hours agoparentprev> Even HyperCard never had his wholehearted supportWhen HyperCard was introduced Steve Jobs didn&#x27;t work at Apple. When he came back, he killed it along with many other projects he had no role in, like the Newton. reply giantrobot 19 hours agorootparentI say this as someone that loved the Newton. The Newton was a millstone for Apple at the time. Just like printers, scanners, and cameras.The Newton had several problems. The first it was intended to be an entirely new platform distinct from the Mac. There was zero overlap between the runtime environments of the two platforms. Nothing you made on a Mac was ever going to run on a Newton. This would stretch already thin third party developers even thinner.The second major problem is it had cheaper competition with a better market fit in the form of Palm. A Palm Pilot was half the price of a MessagePad and did most of the same tasks. It also actually fit in a pocket which meant it could be carried around and used as intended.A third problem was its OS was an older model lacking memory protection and preemption. By 1997 it was clear that multitasking protected memory OSes were the future if for no other reason increased stability in day to day operations. Rebuilding the NewtonOS with those features would be a major project.The MessagePads were bulky and expensive. They were too big to fit in a pocket meaning the only way to carry them was a bespoke case or a briefcase. They weren&#x27;t that capable so a true road warrior worker was just going to get a laptop. Their target market was John Sculley, executives that didn&#x27;t want to tote around even bulkier laptops.The Newton didn&#x27;t make a lot of sense as a product and killing it off with the rest of the Apple peripherals made complete business sense. reply dreamcompiler 19 hours agorootparentI don&#x27;t disagree. I have a soft spot for the Newton because I had a lot of respect for Larry Tesler, who got Apple interested in Common Lisp for a while as part of the Newton project. CL was used to invent Dylan which was originally intended as the Newton programming language. Dylan ended up like the Newton: The invention itself didn&#x27;t have much impact but the project and the people who worked on it moved computer science forward in many important ways.Oh, and the genesis of what we now call the ARM computer architecture was the Newton project. reply criddell 17 hours agorootparentprevThe Newton was out for 5 years by the time the US Robotics Palm Pilot debuted. However, the first really good Newton was the 2000 and that was right around the time the Palm Pilot was released.I was rooting for the Newton but at the same time, I found myself mostly using my Palm Pilot (and later my Handera 330) while my MessagePad sat on my desk unused. reply Zambyte 20 hours agoparentprev> My impression is that Microsoft was founded to bring programming to the peopleWhy do you think that? I was not around in that period, but the impression I get is that the foundation of Microsoft[0] was antithetical to \"bringing programming to the people\".[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;An_Open_Letter_to_Hobbyists reply adrian_b 19 hours agorootparentLong before entering the operating system business, Microsoft was a company dedicated to making tools for programmers, like compilers and interpreters for many languages, for the CP&#x2F;M operating system and for most kinds of early personal computers.The tools for programmers have remained a major part of their products during the MS-DOS era, until the early nineties. The documentation for their compilers and libraries was excellent and many people have learned much about programming from the Microsoft manuals (or from reverse engineering their programs).Only after the transition to Windows and their success in suppressing the competition for MS Office by their control over the development of the operating system, the programming tools business has progressively become only a small part of the Microsoft products and the programmers only a small part of their customers. reply MegaDeKay 17 hours agorootparentAll you need to do is head over to archive.org and look at a Byte magazine from \"back in the day\". It was filled with ads and reviews for programming tools like Turbo C and Turbo Pascal. It was a golden age, and I was there for it. reply fulladder 19 hours agorootparentprevThat famous Gates letter is not inconsistent with the idea of large numbers of hobbyist programmers. He cared a lot about the piracy issue, but he was never trying to gate who was \"allowed\" to be a programmer.Microsoft&#x27;s first product was a BASIC interpreter. It was all about bringing programming to the masses. Saying \"to the people\" implies some kind of empowerment, and I don&#x27;t think that was ever quite as much a part of Microsoft&#x27;s identity (although it was there in some form). \"To the masses\" is a better way to describe it in my opinion. reply WalterBright 17 hours agorootparentIf you booted up an IBM PC without putting a boot disk in, it booted to a Microsoft BASIC prompt that was burned into the rom. reply fulladder 14 hours agorootparentWe had an original IBM PC (not a clone, genuine IBM), and I actually don&#x27;t recall it doing that. Maybe that was on some models and not others?The way I accessed BASIC was to boot up PC-DOS (IBM-branded version of MS-DOS) and then run some program that was like QBASIC although it might not have been called that yet. Later I switched to MS-DOS 3.21, and that came with real QBASIC. I also switched from BASIC to Turbo Pascal and Turbo C. Great times! reply ptman 6 hours agorootparentGW-BASIC? https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GW-BASIC reply WalterBright 11 hours agorootparentprevYes, it was a genuine PC. reply bazoom42 20 hours agorootparentprevTheir first product was a programming language for a hobbyist computer. reply freefaler 20 hours agorootparentprevThey sold software tools at the beginning and further down the line used the tools to become the platform of choice for corporations. They controlled the OS and the tooling and this gave them great advantage in business software market share. You could get Windows source (or parts of it) if you needed to work on low-level stuff from MS and their MSDN was and is miles better than Apple developer documentation and tooling. reply ThrowawayR2 17 hours agorootparentprevBringing programming to the people does not mean giving away your work for gratis. The fuss over the increasing use of non-free but source available licenses instead of OSI approved FOSS licenses shows that people are coming around to Gates&#x27; viewpoint. reply Zambyte 17 hours agorootparentMaybe we should encourage people to charge more for Free Software instead of licensing software in a way that arbitrarily restricts users. reply rbanffy 17 hours agorootparentprevThey wanted to bring programming to the people, as long as they got rich from it.Nobody ever thought they&#x27;d do that for free (except when they used piracy as a form of dumping, by turning a blind eye to it when convenient). reply webworker 11 hours agoparentprev> Apple (represented in my mind by Steve Jobs) didn&#x27;t want his platform to be flooded by crappy software.This kind of shocks me.When you go through macintosh garden and look at all the software (there were tons) a hefty enough portion of it looks like the 80s&#x27;&#x2F;90&#x27;s equivalent of AppStore fart apps. \"Porno\" apps with interfaces and interactivity that could have been constructed in Powerpoint.So that (gatekeeping) didn&#x27;t work. And then they extended the life of all of that garbage with blue box. reply hulitu 19 hours agoparentprev> My impression is that Microsoft was founded to bring programming to the people, and the Mac was founded to bring great software experienceThey were both founded to make money. Microsoft was never close to the people and the \"great software experience\" on the Mac, if it existed, was limited to rich people. reply fulladder 18 hours agorootparentActually, rich people at that time were probably the least likely to own a computer. If you&#x27;re talking about somebody who lives in mansion and has a butler and a chauffeur, that type of person was not buying computers and would not have had any interest in them. (I don&#x27;t know what those people did with their time, but it sure as hell wasn&#x27;t computing.)Personal computers were expensive by middle class standards [1], and the sweet spot was educated professionals like doctors and lawyers because they could afford it and it was interesting&#x2F;useful to them and their families. A rich person who inherited an oil fortune would have been able to afford it, but not interested. A teacher would have been interested, but unable to afford it.[1] At a time when a decent, no-frills used car would have been around $1,000, you had something like this: ~$700 for a more limited VIC-20 or Z80-based machine, ~$2,000 for a PC-clone, and maybe double that for the Macintosh. For most buyers, it was a big but not impossible expense. reply manderley 17 hours agorootparentSeems reasonable to assume that person didn&#x27;t mean people with butlers, the ultra rich, but rich people, just as you described. reply fulladder 14 hours agorootparentYeah, that&#x27;s totally fair. I think I misunderstood because the word \"rich\" in that era was really reserved for people with butlers and yachts, and excluded those who were merely highly paid professionals like doctors and lawyers. But the word \"rich\" today is used somewhat differently, for most people it probably means to include highly paid professionals as well as CEOs and oil fortune heirs.Also, at that time, there was not this extreme divergence in income that you have today. A lawyer might have made more money than a police detective, but the difference wasn&#x27;t really that extreme and often these two people lived in the same neighborhood. Since that doesn&#x27;t seem to be true anymore, it makes sense that the definition of \"rich\" has shifted. reply quijoteuniv 20 hours agoparentprevI think that apple wanted to appeal to the makers at the beginning to get traction but really what they wanted was more consumers. Great hardware and long lasting but is for the elite that can afford it, most people in HN are in that elite. But ultimately apart from raising the bar in user experience is just another money making corporation. And that does not make a world a better place. reply fulladder 17 hours agorootparentYeah, the whole Apple cult thing has never made much sense. I remember talking to some environmentalists in the mid nineties who cared a lot about reducing industrial activity but were also strongly advocating the Mac. I was trying to ask why they felt so strongly about that given that any computer is basically a non-essential luxury good that requires a lot of toxic metals to produce [1], whereas pencil and paper has very low environmental impact and even some recycleability. There wasn&#x27;t a clear answer. I think it&#x27;s irrational.[1] at that time, electronics used a lot of lead, cadmium and mercury. It&#x27;s less of an issue now. reply kalleboo 8 hours agorootparentprevWoz wanted to appeal to the makers (basically, he made the machine he wanted himself because nobody else was making it)Jobs wanted to make an appliance for the masses . That’s why the Apple II had a case and wasn’t just a bare circuit board like the Apple I. reply 7speter 18 hours agoparentprevHow did microsoft bring programming to the people if you had to pay to use their programming tools? reply JohnBooty 17 hours agorootparentYeah, I see your point.Officially, their dev tools cost money. Here is the 1997 pricing for various versions of Visual Basic 5.0 and Visual C++ 5.0 in 1997: ($99, $499, $1,199)https:&#x2F;&#x2F;www.itprotoday.com&#x2F;windows-78&#x2F;microsoft-sets-pricing...Unofficially, almost nobody ever paid for that stuff in my experience and it was ridiculously easy to get for \"free.\" As was the norm in those days there was zero copy protection. You could sail the pirate seas, you could buy one \"legit\" copy and share it with the whole org, you could get the MSDN subscription and get \"evaluation\" editions of literally their entire dev tools catalog on a big stack of CDs with zero limitations besides the legality of the situation. I&#x27;m sure that with minimal effort it was easy to get a hold of Mac dev tools for free as well. But Windows was so ubiquitous, as was \"free\" Microsoft software, and you could build a nice little PC for cheap.I always wondered why Microsoft charged for that stuff in the first place. Were they actually making money on it directly? Did they sort of want to give it away for free, but were wary of antitrust implications?Apple had a different strategy. Perhaps this is an incorrect perspective it seemed like they just didn&#x27;t care about independent garage and small business developers. They left things up to 3rd parties. In a lot of ways this was better -- it probably led to a richer ecosystem? For example, looking at the Microsoft strategy, it&#x27;s not hard to see that they drove Borland into the grave. reply leviathant 15 hours agorootparentThis was a little bit later in the timeline, but I remember attending an event in Philadelphia about F#, and I walked out of it with a licensed copy of Visual Studio .NET and SQL Server. They were just handing them out. I had an earlier copy of Visual Studio from when I was in college, and I was working in .NET at work, but no one at the event knew that. It was just - thanks for showing up to hear about functional programming, take some DVDs on your way out!It always felt to me like there was a culture of openness in the world of Microsoft in a way that didn&#x27;t exist in Apple culture. You gotta pay for dev tools to use dev tools on a Mac. You want an FTP client on a Mac? Buy Cyberduck or Fetch. My impression of Apple was that everything was proprietary and had a price. Whereas I could cobble a computer together that ran Windows, there was oodles of shareware and freeware as well as professional tools. You have full access to the registry and file system in Windows, and you could very easily hack the bejesus out of the OS if you wanted to. It was great for tinkering. Everything was backwards compatible - the point where I had Windows 10 running on a 2005 era Dell laptop that had come with Windows XP, and I had managed to upgrade legitimately without paying (I think the Windows 8 beta converted into a full Windows 8 upgrade, free).Today, I&#x27;m typing this from a 2021 Macbook Pro with USB-C ports - when I travel for work, I bring one charger, and it charges my laptop, my phone, my earbuds, even my mirrorless camera. When I need software, I can usually find something using Homebrew. The value you get for your money on a Mac is much better, but it&#x27;s still a steep barrier to entry, IMO - even though I&#x27;m in a much better position today, and bought this without breaking a sweat. There&#x27;s a lot of tinkering-related things I miss about the Microsoft ecosystem, but I&#x27;ve largely moved out of the weeds for my day to day work on a computer. All the software I was using on my Windows machine is multi-platform now, and the performance and battery life on these Apple native chips is hard to ignore. As a developer, it&#x27;s just as simple, if not easier now, to build on Macs - ever since OSX opened up the Linux ecosystem to these devices. That, in conjunction with superior hardware, finally convinced me to switch after at least three decades of being staunchly Microsoft. reply JohnBooty 17 minutes agorootparentHahaha I hate it but I&#x27;m doing that thing where I agree with the other 50 things you said but point out on thing I don&#x27;t agree with, but: The value you get for your money on a Mac is much better, but it&#x27;s still a steep barrier to entry, IMOIs it that steep? A refurb M1 Macbook Air straight from Apple with 1 year of AppleCare is $850. $1,189 if you want 16GB + 512GB.That is more money than a lot of people can afford, especially outside of the US.But boy, it&#x27;s so much cheaper than ever. That&#x27;s like, two Xboxes (+ controllers + a game or two) so it feels within the reach of a broad swath of the population.Admittedly... you can get a nice-enough Windows&#x2F;Linux developer desktop or laptop with a little Craigslist or FB Marketplace effort for like, $150. reply analog31 12 hours agorootparentprevI&#x27;m not excusing it, but people had to pay for access to a computer in the first place, and ways of funding software development without selling either hardware or software, such as advertising, were not really prevalent yet.Microcomputers greatly expanded private access to computers, and Microsoft brought programming to microcomputers. reply indymike 17 hours agorootparentprevMost of the time MS BASIC came with the computer (or OS). reply runjake 21 hours agoprevShortcuts is one of those rare products that have been acquired and have not only not been ruined, but has improved tremendously for first and third party app integrations.I was initially turned off by the visual programming style because it was mind-bending. Now I use it a lot because it is mind-bending and flexes my brain while also being highly useful.If for some reason I don’t want mind-bending, I can hope into Pythonista and get stuff done. Or thanks to the tight integration, integrate it with Shortcuts. reply corbezzoli 20 hours agoparentAs a programmer, I hate Shortcuts. I really can’t believe 10 instructions can take multiple seconds. I can’t believe Apple decided this product is ready for prime time. reply runjake 19 hours agorootparentIt depends on what those 10 instructions do.While Shortcuts isn’t the fastest execution platform, I have plenty of Shortcuts that I don’t have execution time issues with.Typically, I find that if I’m doing something and it’s taking more time than I’d like, there’s a better way to do it. reply SV_BubbleTime 17 hours agorootparentprevAdmittedly, I do a lot of low level and embedded programming, but my one venture into Shortcuts was brief and filled with wtfs.I don’t know who thinks about computers in that way. It makes zero sense to me. reply m463 11 hours agoprevI think many companies are taking away any sort of self-determination by design.there&#x27;s no teach a man to fish, or give a man a fish. It&#x27;s sell him a fish. Hopefully a fish subscription.Apple is absolutely horrible in this respect. If apple gives you a tool to use with their ecosystem, the tool has a license agreement, and is carefully controlled, with the emphasis on control.I wish python on mac was more of a thing, but that becomes crippled. reply bigpeopleareold 4 hours agoprevWhen I was a wee one in the 90s (teenager actually :D ) I tried to be interested in programming my Mac. I had the opportunity to learn how to program (Turbo Pascal) in school, and wanted to do something with that on my Mac. I vaguely remember that accessing a Pascal compiler was a bit out of reach. At the time there was MPW and Codewarrior and a ton of documentation in the form of the Macintosh Toolbox (for which I excitedly printed out - a number of Inside Macintosh editions). However, nothing stuck with it. I think all I had an evaluation version of Codewarrior only and I think MPW was a bit hard to come by (and being dated anyway.) I think it would have helped to have a tutorial or something, but I ended up getting very impatient with it and gave up for a few years, until I started with web development with a purchased toolchain (and serving on classic mac :D )I was lucky to have my computer, but I grew up with a mentality from my parents that they will not buy something that I am only going to probably look at or play with once. With a thick enough price tag on compilers&#x2F;IDEs, that crushed that interest for awhile. It wasn&#x27;t only until my brother bought licenses to certain software, it became a bit more available. However, it was eventual access to Free software that changed everything for me. It&#x27;s hard to put a price to something as valuable as free (in both ways) access to software to make a career out of and it goes well beyond the exchange value of software licenses.Things have changed a bit and accessing any classic Mac stuff is easier and I have been a little curious again about making some silly classic mac application, if easily possible through emulation. HyperCard, AppleScript, etc. never interested me, but they very much had their purpose. reply karaterobot 13 hours agoprevI think they did bring programming to the people by way of the app store. Maybe they didn&#x27;t make it so your aged granny can write an automation script, but that more people than ever were inspired to learn programming. They made two languages and a proprietary hardware&#x2F;software&#x2F;distribution stack that brought a lot of people into the world of software development. I know so many people a generation younger than me who learned programming specifically to make apps for the app store. People who would probably not have gone into development but for the iPhone. I&#x27;m sure Apple did not intend to mint new programmers, they intended to sell hardware and make money on software, but the net effect is more programmers than ever as a result of their choices. reply biogene 13 hours agoparentThat may be true, but the point was about empowering the end user. Making programing popular through the app store (arguable) is a separate (also important) topic.Empowering the end user to not look at the computer as a black box that they have no idea about how it works is quite freeing and mind-expanding.In the long term, I do believe there is a real risk for programming to become a niche where you end up asking for permission from the h&#x2F;w vendor before you are allowed to write code on the device. We&#x27;re slowly heading in that direction with app stores being bundled with the OS, and we may end up in a situation where you can only install s&#x2F;w through the app store. And only \"authorized\" persons can download IDEs and dev tools. A large population that has no concept of programming is likely to not oppose this because the vendor will throw the security&#x2F;privacy boogeyman at them, about \"unauthorized\" developers writing software that can harm them. reply cesarb 12 hours agorootparent> And only \"authorized\" persons can download IDEs and dev tools.Just in case someone hasn&#x27;t read it yet, I have to post a link to this classic short story: https:&#x2F;&#x2F;www.gnu.org&#x2F;philosophy&#x2F;right-to-read.en.html (The Right to Read, by Richard Stallman).(The bogeyman back then was copyright infringement, nowadays it&#x27;s as you mentioned security and&#x2F;or privacy.) reply zubairq 17 minutes agoprevReally interesting read! reply mannyv 20 hours agoprev\"HyperTalk was both limited and limiting, as most came to discover\"People did amazing things with hypercard. Anyone that says this probably wasn&#x27;t there or wasn&#x27;t paying attention. I knew people who put whole museum collections, including audio, into kiosk-like stacks.The problem with hypercard became complexity and geography: at some point you couldn&#x27;t find anything and changes were impossible to control.The thing is, that&#x27;s the same problem people have today. Visual basic had the same essential problems.In fact, you can argue that programs like Object Master and the Think! Series were the genesis of today&#x27;s IDEs (even though OM was based on the smalltalk object viewer from what i remember). Back then programmers mocked IDEs. reply intrasight 21 hours agoprevProgramming on the Mac was the second worse experience I&#x27;d had - second only to a terminal Lisp system that had no backspace and thus no ability to fix a typing mistake. Both were in fact worse than the punched cards where I first learned to program.Edit: Mac programming was in 1984; Lisp in 1983; Punched cards in 1980 reply fulladder 20 hours agoparentI think programming was a fairly miserable experience on all platforms before about 2000. In the early days, there wasn&#x27;t enough surplus computation available to devote any of it to programmer ergonomics. Today we have IDEs that can make accurate autocomplete suggestions, compilers with detailed error messages (and even suggestions on what the mistake probably is), shells that tab complete really effectively, etc. Before ~2000, there just weren&#x27;t enough spare cycles to do any of that stuff particularly well. reply pjmlp 2 hours agorootparentMy first IDE experience was in 1992, with Borland tools, which I used most of them until 2000.During this time using UNIX was a pain, only lessen by an heavily customized version of XEmacs.On Amiga, Windows, Mac OS and OS&#x2F;2, no biggie, there were good IDE experiences already. reply JoyfulTurkey 20 hours agorootparentprevThe biggest advantage I found when learning on DOS and Windows in the 1990s was the overall lack of distractions. No smart phone, YouTube, Reddit&#x2F;HackerNews let me fully concentrate. reply fulladder 18 hours agorootparentYeah, I totally agree. Computers for a long time were not a communications device the way they are today. You could buy a modem and use it communicate, but that was a distinct activity and once exited the terminal program you were once again by yourself.Now, the communications function is this always on thing that never goes away. It&#x27;s interwoven with every other activity you do. That makes the whole experience much different. reply tonyedgecombe 5 hours agorootparentprevI&#x27;d quite like to drop our internet connection. If I needed to do something online I could visit my local cafe or library.There is no way my family would agree to that though. reply indymike 17 hours agorootparentprevThe change happened in the 80s with Borland’s Turbo products. reply fulladder 14 hours agorootparentI had Turbo Pascal and Turbo C. The compilers were insanely fast compared to what had come before. But I still think that programmer ergonomics sucked. There was no syntax highlighting, definitely no autocomplete, and it couldn&#x27;t necessarily find all the syntax errors on the first pass, so you&#x27;d fix a few errors, recompile, and there&#x27;d be a new set of errors. That&#x27;s my recollection anyway. reply pjmlp 2 hours agorootparentBorland introduced syntax highlight on Turbo Pascal 7 for MS-DOS, if memory serves me right, while all their products for OS&#x2F;2 and Windows 3.x provided it from the start.Autocomplete came around Borland C++ 4.0 timeframe, or something like that. reply tonyedgecombe 5 hours agorootparentprevI don&#x27;t think we knew what we were missing though. I just remember how fast it was and being amazed that it came with four inches of printed documentation. reply indymike 12 hours agorootparentprevIt was 1991&#x27;s Turbo C++ 3.0 (they merged Turbo C and Turbo C++ with that product) that had syntax highlighting and code folding, so it wasn&#x27;t the 80s :-). Hard to believe, but Turbo C came out in 1987 and was pretty primitive. I seem to remember the early version of Turbo C would just hard stop on the first syntax error. Fun times. reply a1369209993 19 hours agoparentprev> second only to a terminal Lisp system that had no backspace and thus no ability to fix a typing mistake.To be fair, the &#x27;proper&#x27; way to deal with that is probably to write some Lisp code to patch the running editor to add backspace functionality. That&#x27;s not really reasonable for someone (presumably) still learning Lisp, though. (And chances are it wasn&#x27;t a true Scotsman^WLisp system and didn&#x27;t support that anyway.) reply intrasight 17 hours agorootparentTo be fair - it did force me to be 100% correct ;) reply lispm 17 hours agoparentprevLisp programming on the Mac was actually great. Apple maintained and sold for a while \"Macintosh Common Lisp\", which was a lot of fun. reply phoehne 21 hours agoparentprevLearning C++ programming on OS 7 put hair on my chest. After that Unix was a walk in the park. reply jandrese 14 hours agoprevIMHO this article writes off Hypercard way too fast. It was a surprisingly capable language, especially when you start adding modules. It&#x27;s a huge shame that Apple crippled it so quickly by only shipping Hypercard Player with most machines. Had they kept the full development environment in the base OS it might still be around today. reply kalleboo 8 hours agoparentYeah I wonder how much longer HyperCard had survived if they had (1) Kept bundling it with Macs (2) Added proper integrated color support from day 1 when the Mac II came out reply hbossy 3 hours agoprevAutomator was great, I used it to steal my sisters copy of twilight sequel unofficial translation, she kept on password - protected CD. All it took was to piece together a few visual blocks that would backup any PDF on newly mounted disc. A ten years old kid could figure it out in two hours. reply dehrmann 17 hours agoprevThere&#x27;s a lesson here for everyone who thinks they have a new no-code solution. There&#x27;s a long history of these solutions that never caught on, and the best outcome you can hope for is Filemaker, Access, or VB. It works for simple things, but it quickly escalates to needing a \"real\" programming language. reply eludwig 21 hours agoprevThis is a really silly framing device wrapped around a decent high-level review of end-user focused dev environments.Hypercard was hugely influential and very widely used. Yes, it ignored the internet, but that was inevitable based on its origins. Heck, 13 years isn&#x27;t a bad run for any piece of software, especially one that crossed the no internet&#x2F;internet divide.The people that want to program will do so no matter what the end-user environment is. Most people just don&#x27;t want to. reply RodgerTheGreat 21 hours agoparentIt&#x27;s also worth considering that HyperCard was very effective at allowing users to build or customize useful tools for themselves while writing little to no code. You could write \"programs\" with HyperTalk, but you could also just pick through the extensive examples HyperCard shipped with and kitbash something, or make searchable databases with little more than a card background with some fields. A small amount of scripting could go a long way, because the surrounding environment and tools did heavy lifting for you.The HN audience skews toward programming per se as the ultimate expression of power and flexibility with a computer, but HyperCard was accessible and empowering in very different ways from QBASIC or an iPython notebook. reply kalleboo 8 hours agorootparentYeah HyperCard came with a stack called “Button Ideas” with a bunch of ready-made buttons you could copy and paste into your stack with no coding needed reply jandrese 14 hours agoparentprevHypercard was mostly sidelined by the time the Internet went mainstream. By 1995 development on Hypercard was long dead. You really can&#x27;t blame it for not embracing web pages.There is an old what-if scenario where Hypercard got the ability to load stacks over the web. I personally think this would have been totally awesome and a disaster at the same time. Hypercard had no security model built in. It&#x27;s basically advanced Javascript years before it appeared in browsers. reply andrewjl 16 hours agoparentprev> The people that want to program will do so no matter what the end-user environment is. Most people just don&#x27;t want to.As an example, take Excel and Python scripting. Almost anyone who uses the former can pick up the latter yet that isn&#x27;t what typically happens. There is effectively a chasm between the two. Why is that? reply bombcar 12 hours agorootparentAlmost everyone who uses python would say they&#x27;re programming (or &#x27;scripting&#x27;) - but very few people who do very similar things in Excel would say the same.Excel is like Factorio or Minecraft, you end up \"automating\" things you were doing by hand and it doesn&#x27;t \"feel\" programatic, and you can very easily \"peek and poke\" the memory of the \"program\" as you go along.Hypercard (and maybe Flash, I never used either) might be somewhere in-between, but closer to Excel.Powerpoint is almost programmable, but nobody uses it that way.(There&#x27;s a similar disjuncture amongst the Linux&#x2F;Mac users where they will write bash&#x2F;zsh scripts but not count it as programming.) reply 38 21 hours agoparentprev> The people that want to program will do so no matter what the end-user environment is. Most people just don&#x27;t want to.this is something I would expect to hear from an Apple executive, and would hope to never hear from a commenter on hacker news. the whole point here, is we should be fostering environments that make it easy to program should people want to, not assuming the default is \"no one wants to program\" and making it as difficult as possible, or just making it difficult by laziness (not prioritizing developer experience).what we DON&#x27;T want, is what Apple, Google and Microsoft currently push hard. which is essentially, lets make all personal computing devices black boxes, that cannot be modified, and that the end users dont even own, but rent from their overlords. no thank you. reply jwells89 20 hours agorootparentI’m on board with making programming as accessible and easy as reasonably possible (though I do think there’s an unsurpassable upper limit to that), but that’s different from programming or even somewhat technical tinkering being the norm and expected. The former is achievable, the latter is a setup for failure.The reason for this line of thinking is that in my experience, some individuals will simply never have the mindset required to be technically inclined, let alone be able to program. The various people I’ve encountered who freak out at the sight of an alert dialog and won’t even read what the dialog says no matter how many times it’s explained to them come to mind.So while I would agree that programming should always be within easy reach I would not expect more than a tiny minority to ever reach for it. reply Clent 21 hours agorootparentprevPeople that want to build their own furniture will do so no matter what the end-user environment is. Most people just don&#x27;t want to.Software engineering is no more magical. Some people like to build things. Most do not.A couch is more of a black box than a smart phone. reply MaxBarraclough 17 hours agorootparent> Software engineering is no more magical. Some people like to build things. Most do not.This misses the point. There are considerable advantages to Free and Open Source software even if you never modify it, the most obvious being that it tends to deter software vendors from adding user-hostile functionality such as tracking. (It isn&#x27;t perfect in guaranteeing this, but it&#x27;s a strong start.)For more on these second-order advantages: [0][1]> A couch is more of a black box than a smart phone.In what sense?With a smartphone full of proprietary software, it&#x27;s extremely difficult to find out what it&#x27;s up to. Same goes for modern smart cars. [2][3] Even if the software is benign, can you be sure about its future updates? There are no such concerns for a couch.[0] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31071180[1] https:&#x2F;&#x2F;www.gnu.org&#x2F;proprietary&#x2F;proprietary.en.html[2] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37443644[3] https:&#x2F;&#x2F;foundation.mozilla.org&#x2F;en&#x2F;blog&#x2F;privacy-nightmare-on-... reply 38 20 hours agorootparentprev> People that want to build their own furniture will do so no matter what the end-user environment is. Most people just don&#x27;t want to.I would rather have 10% of the population programming than 5%. replace those numbers with whatever is more accurate, but the point stands.if we can create devices and operating systems that make programming easier, why not do that? why purposefully make it more difficult, or make it difficult by not prioritizing developer experience? do we want a society of consumers, or one of empowered and informed users? reply fulladder 19 hours agorootparent> I would rather have 10% of the population programming than 5%.Why? I&#x27;d rather have a society where everyone is able to achieve their full potential, whatever that may be. If only 1% (or 0.1%) of people are uniquely good at writing programs, then that&#x27;s who should be writing programs.Maybe what you&#x27;re saying is that we live in a world where writing programs is an essential skill, so let&#x27;s make sure it&#x27;s not unnecessarily difficult for artists to write programs. But with sufficiently good tools, I don&#x27;t really see why an artist would spend time writing programs instead of more directly creating art. reply cglong 16 hours agorootparentPretty much everyone dabbles in art, even if very few are good at it. The same can&#x27;t be said of programming. reply dpkirchner 20 hours agorootparentprevCreating apps -- native or otherwise -- has never been easier, so I think we&#x27;re doing pretty well there. Devices are indeed locked down but I would guess that most people, even programmers, don&#x27;t care as long as they can create most of the user interfaces they can imagine. reply mbakke 18 hours agorootparentHow can we make better operating systems if devices are locked down? How can hobbyists keep the hardware useful when the manufacturer no longer cares?Devices must remain open for the kernel programmers and hardware hackers of tomorrow to follow their curiosity and develop their potential. reply paulryanrogers 19 hours agorootparentprevI&#x27;d argue more things are easier to do than ever, yet making them distributable has narrowed. Before you could ship floppies from your garage, partner with a publisher, shareware, or sell to local shops. Now you have to pay gatekeepers, or train users to do dangerous things. reply freedomben 20 hours agorootparentprevI fully agree with you, but with one exception: I think hacker news has very much become a place of love for black boxes. Many people here love and praise apples \"it&#x27;s not a computer, it&#x27;s an appliance\" approach. reply fulladder 20 hours agorootparentCertainly there are black-box-ophiles on here, but there is much more awareness of the problems of black boxes on HN than most places. After all, the post you&#x27;re replying to went out of its way to bring up the problem of black boxes. If this were Reddit or Twitter&#x2F;X, would there even be a person around to bring up this issue?Overall interest in computing freedom seems to have declined from where it was a couple decades ago. I&#x27;ve been surprised by that and I have a hard time understanding why it happened. I mean, if you show a carpenter two hammers, the only difference being that one of them can only drive nails from a specific vendor while the other works on any nail, he or she would instantly recognize how significant that difference is in terms of overall usefulness. How is it that computer users intuitively understood this in the past, but somehow the level of awareness and understanding seems to have declined from where it was? I don&#x27;t get it. reply cesarb 12 hours agorootparent> Overall interest in computing freedom seems to have declined from where it was a couple decades ago.Did it decline, or did the demographics change? Us old-timers are probably still as interested in computing freedom as we were back then, but there are a lot of new people entering the computing world, whose first (and perhaps only) experience with computing was with more closed systems like modern smartphones (and even desktops nowadays are more closed than they were back then). replymaroonblazer 13 hours agoprevIt wasn&#x27;t developed by Apple, but Metrowerks Codewarrior[0] was my serious introduction to programming.Codewarrior was an application framework for the Mac OS but I learned a ton about programming and OOP in particular from reading through the Codewarrior code. It was revelatory and motivated me to go on to discover and explore the Gang of Four&#x27;s Design Patterns and Alexander&#x27;s work more broadly.[0]https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Metrowerks reply waffletower 13 hours agoparentI always thought the branding for that product was terrible and embarassing. I even renamed my binary to \"HackLikeADork\" on my Mac when I needed to use it. I cringed when I thought that someone of the opposite sex might glance at my screen and see that I was trying to be a \"CodeWarrior\". reply nemo 20 hours agoprevOne other little piece of the story was MacBasic: https:&#x2F;&#x2F;www.folklore.org&#x2F;StoryView.py?story=MacBasic.txtApple intended to ship the Mac with a user-friendly BASIC-based programming environment out of the gate, and Bill Gates killed it. reply eviks 8 hours agoprevThe \"natural language\" with very poor tooling compared to any popular language make coding in AppleScript a dreadful experience, \"natural language \"doesn&#x27;t help you much in reducing ambiguity in learning, and of course, awful documentation doesn&#x27;t help the people, so discoverability is awfulThey should&#x27;ve made the automation system accessible from any language instead of inventing this one (and Mac had a few scripting languages)The GUI paradigm has much more potential, but tedious for anything but simplistic stuff, so it should be on top of scripting so you could switch back and forth reply gcanyon 9 hours agoprevAppleScript gets a bad rap for being obscure, but the language itself isn&#x27;t to blame (mostly) -- when I was an AppleScript developer, 97% of my headaches came from applications with weird dictionaries, not the language itself.One time I spent half a day trying to figure out how to add a page to Quark XPress, only to eventually find that documents didn&#x27;t contain pages; they contained spreads, and spreads contained pages.That was 100% not shown in the standard dictionary display in the Script Editor tool; the UI tool I used for creating interfaces for AppleScript apps had a better dictionary display, and that revealed the answer to me. reply donatj 17 hours agoprevThe lack of any mention of Quartz Composer is really sad. I saw so much absolutely amazing things done in Quartz Composer by people who knew almost nothing about programming. I recall Facebook even talking about using it as a tool for prototyping UI animations.I used to use it to inspect HID devices because it was SO unreasonably easy to do so. reply thefz 17 hours agoparent> amazing things done in Quartz Composer by people who knew almost nothing about programmingHow is this relevant? reply bowsamic 16 hours agorootparentThat’s literally the topic of the blog post reply mitchbob 12 hours agoprevCounterpoint: A piece on Alan Cooper and the creation of Visual Basic.https:&#x2F;&#x2F;retool.com&#x2F;visual-basic&#x2F; reply gcanyon 9 hours agoprevIf you&#x27;re going to start with HyperCard, you can&#x27;t leave out SuperCard and LiveCode, both still around today. And if you include SuperCard, you have to mention that its creators went on to create Flash. And then of course there&#x27;s Director, mTropolis, iShell, and Apple Media Tool, and many other tools that started on Mac. reply danielvaughn 21 hours agoprevFunny enough I’m literally writing an article at this very moment that touches on this subject a bit.I’m surprised the author mentioned Swift Playgrounds but not Storyboard. Storyboard was promoted as a way to build apps without touching code. It obvi",
    "originSummary": [
      "This article delves into the history of programming languages on Mac platforms and their unsuccessful attempts at making coding more approachable for the general public.",
      "Emphasis is placed on Prograph, a visual programming language, suggesting that a similar but more efficient visual language might bring about a significant shift in the field.",
      "The author indicates that despite some interesting elements in languages like Swift, the impact of Mac on the programming realm remains limited compared to its influence in desktop publishing."
    ],
    "commentSummary": [
      "The discussion centers on Apple's efforts to make programming accessible to regular users, illustrating challenges that occurred while learning programming on Mac computers during the 90s.",
      "It highlights the contrasts between Apple and Microsoft's strategies towards software development, underscoring the significance of nurturing environments that simplify programming and empower users.",
      "The conversation reflects on the constraints of specific programming tools and the identified discrepancy between various automation and scripting forms, offering diverse viewpoints on programming and user experiences across multiple platforms and tools."
    ],
    "points": 195,
    "commentCount": 240,
    "retryCount": 0,
    "time": 1694965242
  },
  {
    "id": 37545040,
    "title": "Changing the Rules of Rust",
    "originLink": "https://without.boats/blog/changing-the-rules-of-rust/",
    "originBody": "Without boats, dreams dry up Posts Tags Changing the rules of Rust Sep 17, 2023 In Rust, there are certain API decisions about what is and isn’t sound that impact all Rust code. That is, a decision was made to allow or not allow types which have certain safety requirements, and now all users are committed to that decision. They can’t just use a different API with different rules: all APIs must conform to these rules. These rules are determined through certain “marker” traits. If a safe API could do something to a value of a type which some types don’t support, the API must be bound by that marker trait, so that users can not pass values of those types which don’t support that behavior to that API. In contrast, if Rust allows APIs to perform that behavior on any type, without any sort of marker trait bound, then types which don’t support that behavior cannot exist. I’m going to give three examples to show what I mean, each of which Rust has considered at different points, though only the first one actually exists in Rust. Rules of Rust Send Let’s say you want Rust to support types which can’t be sent across threads. There are a couple of examples of why you would want this: The type provides shared ownership without synchronizing interior mutable writes to the reference count (e.g. Rc) The type may wrap an OS API that doesn’t guaranteee thread safety (e.g. Args, MutexGuard) To support this, you would include a marker trait called Send, which is the set of types which can be sent across threads. Any API which might send a value to another thread needs to include a Send bound, such as: thread::spawn, which spawns a thread rayon::join, which runs two tasks on a thread pool tokio::spawn, which may move this task to another thread of the executor Of course, Rust chose to support types which can’t be sent across threads, and so it has a Send bound. But an alternative Rust could have just as easily chosen that all types in Rust must support sending across threads, and effectively all interior mutability would need to be synchronized. Indeed, the Send trait is actually a decision fully enforced by the standard library: someone could release an “alternative libcore” which imposes this requirement with no change to rustc, though it wouldn’t be compatible with any of the Rust code that exists in the world. Move Let’s say you want Rust to support types which can’t be invalidated without running their destructor once their address has been witnessed. This is a sort of wonky and specific definition of “immoveable type,” but it happens to fit perfectly for what stackless coroutines and intrusive data structures require. To support this, you would include a marker trait called Move, which is the set of types that can be moved freely. Unlike Send, Move requires some language support: I think the simplest way to implement it would be to say that operations that take the address of something take ownership of types if they don’t implement Move (so let x = &mut y; takes ownership of y, effectively preventing you from ever moving it again.) And the magic behavior of Box which lets you move out of it would need to be bound by Move as well. Additionally, certain APIs would need to be bounded by Move, which let you move out of a reference, such as: mem::swap lets you swap values behind two mutable references mem::replace lets you replace the value behind a mutable reference with another You’ll notice that Rust doesn’t have a Move trait; instead, it provides the same guarantees using the Pin wrapper around pointer types. Even though the Move trait would have probably been a much easier to use API, it proved difficult to add in a backward compatible way (I’ll explain why in a moment), so instead the Pin API was added and used only in the new interfaces that required these semantics. Leak Let’s say you want Rust to support types which can’t go out of scope without running their destructor. This is one of the two different definitions of “linear types,” it is less expressive than the other (which would also prevent a destructor from running, requiring the type to be destructured as its final end), but it is the easier of the two to add to the language (because it works better with generics), and it supports all of the most compelling use cases for linear types. To support this, you would include a marker trait called Leak, which is the set of types that can go out of scope without running their destructors. Like Send and unlike Move, this would require no language support at all: its not possible to “leak” a value in the core language of Rust, you have to use standard library APIs to do it. Certain APIs would have to be bound by Leak: APIs that always leak a value (mem::forget) APIs that make it your responsibility to run the destructor (ManuallyDrop::new) APIs that allow cyclic shared ownership and can accidentally leak values (Rc::new, Arc::new) Of course, Rust doesn’t have the Leak trait, but it almost did. This discussion came to a head in early 2015, when the scoped thread API that Rust was using was found to be unsound, because its safety depended on its guard type never leaking. It was decided (in some haste, because the 1.0 release was scheduled within a few months of the controversy) that Rust would not support types that can’t be leaked, and so the Leak trait would not be added. Changing the rules There’s been a renewed interest in supporting linear types in Rust, especially because of what I called the scoped task trilemma, which is only true because of the fact that destructors cannot be guaranteed to run. Unlike immoveable types, there is no isolated API addition that could support guaranteeing that a destructor will run, the way there was with Pin. (You can guarantee a destructor will run if you never give up ownership of the object and use a kind of closure passing style, but this isn’t adequate for the “scoped task” use case). So some users would like to see Rust add a Leak trait. There are two possible ways a marker trait like Leak could be added to Rust: Auto trait: you could add a new auto trait, like Send and Sync ?Trait: you could add a new “?Trait,” like Sized Each of these presents certain challenges regarding backward compatibility. Auto traits At first glance, adding auto traits might seem like a backward compatible change. You add a new trait, Leak, which says that a type can be leaked. Types that don’t implement this trait cannot go out of scope without running their destructor. Because all types in Rust today necessarily can be leaked (this is a consequence of the decision not to have a Leak trait), its perfectly fine for all types to implement Leak. This is the semantics of an auto trait, so it sounds like it should work great. The problem comes when you go to add bounds to the APIs that can be used to leak values, like mem::forget. If you want to make it so that types that don’t implement Leakd cannot be leaked, you need to add a bound to mem::forget. But there are two ways in which this is not backward compatible. First, it does not work with generics. This code is legal today, but would break if you add a Leak bound to mem::forget: pub fn forget_generic(value: T) { mem::forget(value); } This is because there is no Leak bound on the type parameter for this function. Adding such a bound to the API of mem::forget (or any other API that can forget values) would be a breaking change. Another way in which it is not backward compatible is that trait object types will not implement Leak unless they add + Leak. Trait object types do not inherit impls by way of auto traits, because you don’t actually know what type the trait object is. So a trait object like dyn Future does not implement Leak. For example: pub fn forget_trait_object(object: Box) { mem::forget(object); } ?Traits So if adding an auto trait is not backward compatible, we are left turning to the ?Trait solution. But there are problems here as well. in the strictest sense, adding a new trait like ?Leak is backward compatible. Instead of adding new bounds to APIs like mem::forget, you would be relaxing the bounds on other APIs. So all of the code above would be fine, because to make a generic function that takes a linear type, you would have to write a ?Leak bound. The first problem with this for something like Leak is that the vast majority of generic APIs in Rust cannot possibly forget their value; after all, even though memory leaks are not undefined behavior, they are still undesirable and mostly avoided. This is quite different from Sized: because passing something by value requires Sized, the large majority of generic APIs in Rust require Sized, and so ?Sized bounds are relatively rare. In contrast, adding ?Leak would create a permanent scar across the ecosystem, as the vast majority of generics would gain a T: ?Leak bound. The second problem is bigger, though: the interaction between ?Traits and associated types means that it is a breaking change to add a ?Trait bound to an associated type. This means that any stable associated type in the standard library cannot gain a ?Leak bound. Consider this example: pub fn forget_iterator(iter: impl Iterator) { iter.for_each(mem::forget); } This will forget every element of the iterator, even though the Iterator::Item associated type is never mentioned. Therefore, Iterator::Item must implement Leak, always. The compiler is allowed to assume that the item of every iterator implements Leak, and it would be a breaking change to invalidate that assumption. The implications of this are far reaching. If Leak were added as a ?Trait, all of these things would not be possible with linear types: Iterator: You couldn’t construct an iterator of linear types. Future: You couldn’t construct a future that evaluates to a linear type (and so you couldn’t return a linear type from an async function). Deref: You couldn’t dereference a Box containing a linear type, or dereference a vector of linear types to a slice of linear types, or any other smart pointer type. Index: You couldn’t index a collection of a linear type, so you couldn’t index into a slice of linear types or into a map with linear values. Add/Sub/Mul/Div: You couldn’t have a linear type as the output value of any overloaded arithmetic operator. One special set of associated types is the return value of the Fn traits. The Rust project has specifically made it difficult to refer to the Output associated type of these traits, so that it would have flexibility to change this in the future. Still, certain issues were encountered in the past when experimenting with a Move trait. It’s not clear to me if these issues would have been encountered with other traits (like Leak) or if they were specific to the built-in semantics of Move. Basically, there is a very steep trade off in choosing to add a new ?Trait - especially one that wouldn’t be relevant to very many bounds, like Leak - because you will add confusing new syntax to a huge variety of generic interfaces in exchange for getting a very limited new feature. I would consider adopting something like ?Leak a poor cost-benefit analysis, even if we do accept that the current Rust rules are “wrong” and we wish Rust had linear types. Editions The final thing that needs to be considered is the edition mechanism. Is it possible, using the edition mechanism, to introduce one of these traits? Maybe. Each edition in effect forms a “dialect” of Rust, all of which are supported by the same compiler. So at first glance it sounds plausible that in one dialect of Rust, all types can be forgotten, and in another dialect, the Leak trait exists. The problem is that a hard requirement on editions is that crates from one edition can depend on crates from another edition, so that the upgrade from one edition to the next is seamless and voluntary. Consider that the Rust project decided to add the Leak trait in the 2024 edition. All of the code in the 2021 edition needs to still work - including code like the examples I showed above. Sure, you can use a tool like cargo fix to add Leak bounds everywhere and expect users to relax them on their own as they move into the 2024 edition, but the code from the 2021 edition needs to work without modification. A way to possibly do this would be to make trait coherence depend on edition, so that in pre-2024 editions, every type meets the Leak bounds, even types that absolutely shouldn’t, like unbounded generics, and trait object types that don’t mention Leak. If, in the pre-2024 editions, all types implement Leak, then the added bounds will never fail. However - and this is the big, enormous caveat of this section - this would rely on the compiler effectively implementing some kind of unbreakable firewall, to prevent a type that does not implement Leak from ever getting used with pre-2024 code. This would mean: Any time post-2024 code instantiates a generic from pre-2024 code, it needes to check that the type it instatiates it with implements Leak, even though the pre-2024 code doesn’t have such a bound. Any time pre-2024 code calls post-2024 code, it needs to check that the types it gets from that API implement Leak (under the 2024 edition rules). Note that the standard library would be considered post-2024 code, so every call to a std API would involve checking types for Leak in the pre-2024 editions. I don’t know if this is even possible to implement, probably it would have pretty bad effect on compile times at least on pre-2024 edition code, and it would likely create a very difficult transition at first. But in the longer term at least it would leave Leak in the “right” state (as an auto trait, which can be used with old associated types correctly, and not as a ?Trait). What should be done If I could go back in time to 2015, I think I would probably add both Move and Leak to Rust. There are certain downsides, which were highlighted by the team at the time in their decision not to add Leak: any trait object type which needs to meet these bounds needs to add + Move or + Leak to their definition. Having two auto traits of such global significance (Send and Sync) was already considered enough of a burden. But if I’m honest, I have the impression (I wasn’t there) that the decision to exclude Leak was at least partly a matter of expediency: my understanding is that the Rust team at Mozilla was under incredible pressure from their management to ship a 1.0 on the deadline they set, and this probably influenced their decision not to make any last minute changes to the rules that could impact meeting that deadline. In a language with Leak, the scoped task trilemma wouldn’t exist, the simpler scoped thread API would be safe, GC-integration would possibly be easier, and I’ve gotten the impression many systems APIs would be easier to wrap safely (though I don’t know the details of this). In a language with Move, the Pin type wouldn’t need to exist, users would therefore not have such annoyance dealing with it, and so-called “pin projections” wouldn’t be an issue requiring macros to solve, and making self-referential generators would present no complication for the Iterator trait. However, making these changes now is a much thornier problem. I think the edition-based technique is the only viable solution for adding a new, globally relevant marker trait (except for certain unique exceptions, like DynSized, not discussed here). And I think there are all kinds of reasons to think that wouldn’t work - that it would be too difficult to implement, that the implementation would have too many soundness holes, that the transition would be too disruptive, that it’s actually totally impossible because I’ve missed something important. This is why I’m very glad we found the Pin solution for immoveable types, and were able to ship self-referential futures and async/await syntax on top of them, in a reasonable time frame without major disruption to all existing users. When it comes to Leak and linear types, I just despair. || saoirse@without.boatsBlack Lives Matter",
    "commentLink": "https://news.ycombinator.com/item?id=37545040",
    "commentBody": "Changing the Rules of RustHacker NewspastloginChanging the Rules of Rust (without.boats) 185 points by kevincox 1 day ago| hidepastfavorite98 comments xpe 17 hours agoSince a good number of people will arrive at this thread without a lot of surrounding context, would some of us &#x27;in the know&#x27; please share some remarks about:1. Who should read this? Who should care? Why?2. Help me situate this relative to other Rust commentary, ideas, and recommendations. Where does this sit?3. What&#x27;s been happening in the Rust world that may have motivated boats to write this blog post now?Context: For many years, I&#x27;ve enjoyed and written a lot of Rust, but I don&#x27;t necessarily keep up with the &#x27;inner sanctum&#x27; conversations. reply withoutboats3 4 hours agoparentHi, this is boats.1. If you don&#x27;t read and comment on Rust RFCs, this post is probably not relevant to you.2. This post is not even a suggestion for Rust to do anything, it&#x27;s about how if you want to make a certain kind of change, the barrier to do so backward compatibly is very high.3. A lot of Rust contributors have recently talked about how they think Rust should have linear types (which involves adding a trait like \"Leak\"). Some of the commentary suggests people don&#x27;t realize how big of a hurdle there is to doing this. This is a detailed explanation of how difficult it is.I don&#x27;t know why my blog posts show up on the front page of Hacker News all the time! reply ameliaquining 12 hours agoparentprevThis is mostly about async Rust. Today, async Rust works, but it&#x27;s a lot jankier and less polished than the rest of the language; in some ways, it feels kind of beta-quality. The maintainers are extremely aware of this problem and are making serious efforts to fix it. These efforts are expected to take years, because it is a difficult technical problem (most other languages solve it in ways that compromise runtime performance or low-level programmer control, which Rust prioritizes) and because a community-driven open source project has limited resources and takes time to arrive at decisions.This blog post discusses one potential technical direction for addressing a subset of the problems with async Rust, and in particular why the need to preserve backwards compatibility with earlier design decisions, made before async Rust was designed, makes it harder than it might otherwise have been.If you don&#x27;t use async Rust, you can mostly ignore this. If you do use async Rust, and are interested in how it might (backwards-compatibly!) evolve in the coming years, then the report of the Async Foundations Working Group is probably the best starting point to understand what&#x27;s going on: https:&#x2F;&#x2F;rust-lang.github.io&#x2F;wg-async&#x2F;welcome.html reply hedora 20 hours agoprevThe lack of a Leak-style trait is so painful for me that I’d strongly consider switching to a different language that was basically just rust + Leak. (Adding move and removing pin would be nice to have too.)The problem with not having Leak is that it forces all futures to be ‘static, which effectively disables the borrow checker whenever you cross an async spawn boundary.Arc is the standard solution, but, for multi-threaded code, it can be expensive (though also more predictable, in fairness) than having a GC.The only other solution (which I prefer) is to use an unsafe version of spawn that assumes the future it returns will not be leaked.This certainly breaks the rust promise of supporting safe, zero cost abstractions. reply sabageti 19 hours agoparentI have seen many comments like this, and I would like to understand better I feel like I&#x27;m missing something. In my daily use case(web api) of async rust we use rarely spawn. What are the use case of intensive spawn usage ?And there is tokio::spawn_local() that doesn&#x27;t require Send reply Matthias247 7 hours agoparentprevI assume you mean \"spawned futures\" - because regular Futures obviously don&#x27;t have to be &#x27;static. You can run as many of them as you want on a single task by using Future combinators like join&#x2F;select.If this is mostly about scoped async task and the ability to reference data from the parent task, then there also had been a different proposal being discussed which provides the same guarantee: A new Future type (e.g. CompletionFuture), which by represents an async function that needs to be driven to completion. See https:&#x2F;&#x2F;github.com&#x2F;Matthias247&#x2F;rfcs&#x2F;pull&#x2F;1 for details. However since this propsal is already 3 years old and hasn&#x27;t gotten and interest by the Rust async WG, it&#x27;s probably also not going forward. reply scottlamb 19 hours agoparentprev> Arc is the standard solution, but, for multi-threaded code, it can be expensive (though also more predictable, in fairness) than having a GC.Do you have measurements to back that up? I would expect it to be a lot cheaper than GC because GCed language such as Java&#x2F;Go&#x2F;etc. GC every heap allocation (and Java in particular doesn&#x27;t have unboxed objects, so there are a lot of heap allocations!) where Arc would bundle a bunch of things together.That said, I&#x27;m not in love with the Arc solution either. It&#x27;s not just the performance impact. The programming model of scoped concurrency is just much more pleasant. Giving up that and then also having to deal with putting everything into an Arc, and likely also then wrapping it in a MutexLock, even when it should really be one thread at a time dealing with the value in question, just doesn&#x27;t do anything good for readability and writability of code... reply yencabulator 13 hours agorootparentThe reason Arc hurts is that it&#x27;s global and atomic. Multiple cores competing to inc&#x2F;dec a shared reference count is pretty much a worst case scenario for modern CPUs.Under modern garbage collection, there&#x27;s very little need to coordinate across cores, just some barriers every now and then to mark things safe. reply scottlamb 11 hours agorootparentTrue, but impact depends on both total core count you&#x27;re using in the process and what you&#x27;re doing with Arc. I&#x27;m imagining Arc with low concurrency on each RequestState so impact of cache line bouncing should be negligible.If you&#x27;re talking about Arc, I&#x27;d probably use Box::leak instead. There&#x27;s also a few crates that do epoch gc. I haven&#x27;t really written super high request rate multicore stuff in Rust, but I have in C++, and there for global-ish occasionally updated config stuff, I used a epoch gc implementation on top of Linux rseq to totally eliminate the effect you&#x27;re describing. reply yencabulator 11 hours agorootparentThe classic case where Arc hurts and GC really shines is Arc.Arc makes otherwise read-only activity write. reply pkolaczk 8 hours agorootparentHow many independent things do you need to access the config? A dozen? Then there is a dozen calls to inc the RC at startup, but there is no later cost to read from object behind Arc. So you may read your config a million times and Arc changes nothing. While GC will have to scan the Config structure every time a full GC is needed, so there is an added cost. reply charcircuit 16 hours agorootparentprev>because GCed language such as Java&#x2F;Go&#x2F;etc. GC every heap allocationGC is not run on every allocation. reply o11c 16 hours agorootparentThey don&#x27;t run GC, but they subject the extra pointers to GC, whereas good RC languages generally support values. Doing nothing is usually cheaper than doing something.This is more a Java problem than a GC problem; C# supports values just fine, and some heroic JVM-targetting languages also manage it. reply manwe150 10 hours agorootparentBut then mark and sweep also doesn’t have to free the memory (it just does a bulk sweep occasionally), and RC languages do need to free memory used to hold the values later (they have to run expensive compaction and locking steps). So definitely more a Java problem with poor allocation hoisting and reuse than a difference between the RC and mark and sweep implementations of GC. reply scottlamb 16 hours agorootparentprevAre you interpreting \"GC every heap allocation\" as \"each heap allocation causes an entire cycle of GC\"? That&#x27;s...not what that phrase means. reply charcircuit 16 hours agorootparentYes I am. Java&#x27;s GC can stop the world at any safepoint. It isn&#x27;t limited to when objects are allocated and it doesn&#x27;t do it every time. reply jenadine 8 hours agoparentprevIt&#x27;s probably too complicated to explain in a HN comment, but I don&#x27;t understand why not having Leak forces futures to be &#x27;static. reply pornel 3 hours agorootparentThere&#x27;s a desire to have futures that run on other threads, but can borrow from temporary locations like stack of their caller.This can&#x27;t be implemented robustly, because there&#x27;s no way to enforce the outer scope (the one lending its stack) won&#x27;t return while the futures are still running. It needs a blocking mechanism that does \"wait, your async code hasn&#x27;t finished yet!\", but implementing that via guards and destructors can be bypassed by leaking them.https:&#x2F;&#x2F;without.boats&#x2F;blog&#x2F;the-scoped-task-trilemma&#x2F;----However, running futures on the same thread, with a simple plain .await, does allow them to borrow from the caller&#x27;s stack safely without restriction. So the first poster in this thread may be doing something unusual, perhaps just over-using multi-threaded spawn() where join_all or streams would suffice. reply lukebitts 23 hours agoprevVery interesting read! I really wish we could be unburdened by backwards compatibility so big changes like these are possible, but I realize that would open its own can of worms reply dijit 22 hours agoparentIsn&#x27;t that exactly why Rust has “Editions”? reply Rusky 21 hours agorootparentEditions still have to interoperate with each other, they&#x27;re not a free license to change anything and everything. In fact the post goes into detail on how they might or might not work for these specific changes! reply Guvante 20 hours agorootparentprevThe post goes into detail with why editions aren&#x27;t a solid solve.It does mention that dealing with the pain is the only potential solve. reply Taek 22 hours agoparentprevHonestly backwards compatibility in rust is already half-assed anyway. Trying to use a fixed version of the compiler (important in some contexts such as security software) is a huge pain in the ass. reply ekidd 20 hours agorootparentRust has an excellent story for running old code on new compilers.For example, I&#x27;ve been using Rust in production since just after 1.0, so about 8 years now. I maintain 10-20 libraries and tools, some with hundreds of dependencies. If I update a project with 200 dependencies that hasn&#x27;t been touched in 3 years, that&#x27;s 3x200 = 600 \"dependency-years\" worth of old code being run on a new compiler. And usually it either works flawlessly, or it can be fixed in under 5 minutes. Very occasionally I&#x27;m forced to update my code to use a new library version that requires some code changes on my end.I&#x27;ve also maintained long-lived C, C++, Ruby and Python projects. Those projects tend to have far more problems with new compilers or runtimes than Rust, in my experience. Updating a large C++ program to newest version of Visual C++, for example, can be a mess.However, Rust obviously does not support running new code on old compilers. And because stable Rust is almost always a safe upgrade, many popular libraries don&#x27;t worry too much about supporting 3-year-old compilers. (This is super irritating for distros like Debian.) Which if you&#x27;re working on a regulated embedded system that requires you to use a specific old compiler, well, it&#x27;s a problem. Realistically, in that case you&#x27;ll want to vendor and freeze all your dependencies and back-port specific security fixes as needed. If you&#x27;re not allowed to update your compiler, you probably shouldn&#x27;t be mass-updating your dependencies, either.Basically, Rust got so exceptionally good at running old code on new compilers that the library ecosystem developed the habit of dropping support for old compilers too quickly for some downstream LTS maintainers. And as a library maintainer, I&#x27;m absolutely part of the problem—I don&#x27;t actually care about supporting 5 year old compilers for free. On some level, I probably should, but... reply nwallin 18 hours agorootparent> Updating a large C++ program to newest version of Visual C++, for example, can be a mess.That&#x27;s an MSVC problem. MSVC ignored the C++ specification for decades. Now it does follow the spec. So a lot of non-standard code broke.The transition was pretty ugly, as I&#x27;m pretty sure you&#x27;ve figured out. The permissive mode never gave warnings for non-standard code, so you couldn&#x27;t just fix warnings as they came up. You had to do a fix the world update, which are a dickpain in large codebases. The transition was relatively fast; VS2017 introduced the standard compliant parser, and C++20 requires it.Honestly MSVC is such a mess. I have a hunch that before this decade is out, Microsoft will just replace it with Clang a la Internet Explorer&#x2F;Edge&#x2F;Chromium. That&#x27;s why the switch to the standard compliant parser was so rushed; they&#x27;re trying to force everyone to write standard compliant code so that Clang can compile it. reply csomar 7 hours agorootparentprevThis is quite under-appreciated. I&#x27;ve lost count of how many times NPM or PIP broke because of \"2000 lines of error dumps\". The only occasion is happens with Rust has to do with TLS, and I learned to handle that one quick. Otherwise, \"cargo install\" is the best package manager out there. reply ekidd 29 minutes agorootparentAh, yes, TLS. This is almost always the fault of Rust libraries that link against OpenSSL. So technically this is a C linking nightmare. :-&#x2F;I have configured cargo-deny for nearly all our projects to just ban OpenSSL from ever appearing as a dependency. reply sitkack 20 hours agorootparentprevI think the answer is that when you break compatibility for an old compiler, you rev the major version. The folks on the old compiler then get to use the old library forever. Yeah, they can pin (and should) but it makes it a little cleaner. reply MereInterest 19 hours agorootparentExcept that so few people are using older compilers, that tracking exactly when features were introduced becomes burdensome to the developer. For languages that have a few large releases every decade, such as C++, this is reasonable. As a C++ developer, I can remember that `std::unique_ptr` requires C++11, `std::make_unique` requires C++14, structured bindings require C++17, etc. It&#x27;s tedious, but doable, and can be validated against a specific compiler in CI.For languages with more frequent releases, that just isn&#x27;t feasible. The let-else construct [0] was stabilized in 1.65 (Nov 2022), default values for const generics were stabilized in 1.59 (Feb 2022) [1], and the const evaluation timeout [2] was removed in 1.72 (Aug 2023). These are all reasonable changes, released on a reasonable timescale. However, when writing a `struct Position`, I don&#x27;t check whether my library already uses language features that post-date 1.59, and I don&#x27;t consider it a backwards-compatibility breakage to do so.Incrementing the major version for a change that is source-compatible on most compiler versions (e.g. the first use of let-else in a project) just isn&#x27;t justified.[0] https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;rust-by-example&#x2F;flow_control&#x2F;let_e...[1] https:&#x2F;&#x2F;blog.rust-lang.org&#x2F;2022&#x2F;02&#x2F;24&#x2F;Rust-1.59.0.html#const...[2] https:&#x2F;&#x2F;blog.rust-lang.org&#x2F;2023&#x2F;08&#x2F;24&#x2F;Rust-1.72.0.html#const... reply kmeisthax 19 hours agorootparentprevIs it a breaking change to not support an old compiler when the new compiler is the same major as the old one was? Transitive dependency upgrades that bump minor don&#x27;t trigger major version bumps in downstream users. Why should compilers be different? reply sitkack 15 hours agorootparentYou would opt in to the future, not have to pin the past. It would give the ecosystem more stability. reply mirashii 20 hours agorootparentprevIt’s about a 10 line nix file, a 10 line dockerfile, a one line rust-toolchain.toml. reply IshKebab 21 hours agorootparentprevI&#x27;ve never had any notable issues. I&#x27;ve had far fewer compatibility issues with compiling crates than I have compiling C++ libraries or installing Python packages. reply Ar-Curunir 21 hours agorootparentprevBackwards incompatibility in the language is different from backwards incompatibility in libraries. reply notnullorvoid 21 hours agoprevHere&#x27;s an idea, for a kind of soft rollout. Implement a optional feature for 2021-edition crates that allow specifying Leak. The trait has no effect in 2021.In next-edition:- 2021 crates that turned Leak feature uses it just as a next-edition crate would. Only things explicitly marked with Leak in the crate can leak.- 2021 crate that didn&#x27;t opt-in to Leak feature, automatically marks everything in the crate with Leak. reply bryanlarsen 20 hours agoparentI think the big caveat mentioned in the editions section (https:&#x2F;&#x2F;without.boats&#x2F;blog&#x2F;changing-the-rules-of-rust&#x2F;#editi...) still applies to your suggestion. The author concludes in that section \"I don’t know if this is even possible to implement, probably it would have pretty bad effect on compile times at\".Your refinement seems like a nice addition to the author&#x27;s suggestion, if it is possible and doesn&#x27;t effect compile times too much. Hopefully somebody smarter than me takes a run at the problem. reply kmeisthax 20 hours agoparentprevfrom __future__ import Leak reply jeff-davis 21 hours agoprevThe Leak trait could be interesting when it comes to C code that calls rust code that calls C code that might longjmp() over the rust code in the middle.Right now longjmp()ing over rust code is not a great idea for a couple reasons, but because leaks are currently always allowed in rust, it arguably follows the rules at least in some cases (doing so could cause problems, of course).If the Leak trait were introduced, then skipping destructors would be clearly the wrong thing to do if there are types on the stack that don&#x27;t implement Leak. That would clearly require some way to either prevent longjmp() from ever jumping over rust code (e.g. always catch it in C code and turn it into a special return code), or find some way to catch the longjmp and turn it into an unwind in rust (e.g. a panic?) and then turn it back into a longjmp to go back into C. reply connicpu 20 hours agoparentHonestly, longjmp is just extremely problematic for any code that isn&#x27;t pure C. Even just adding C++ into the mix can easily make longjmp something that can&#x27;t be used without invoking UB.\"If replacing std::longjmp with throw and setjmp with catch would invoke a non-trivial destructor for any automatic object, the behavior of such std::longjmp is undefined.\"[1][1]: https:&#x2F;&#x2F;en.cppreference.com&#x2F;w&#x2F;cpp&#x2F;utility&#x2F;program&#x2F;longjmp reply LegionMammal978 19 hours agoparentprev> Right now longjmp()ing over rust code is not a great idea for a couple reasons, but because leaks are currently always allowed in rust, it arguably follows the rules at least in some cases (doing so could cause problems, of course).In the general case, skipping destructors of objects you don&#x27;t own (through longjmp(), killing a single thread, etc.) has never been allowed in Rust: at any particular point in the program, you&#x27;re only allowed to skip destructors of objects that you currently own, or can otherwise gain ownership of.For instance, the thread::scope() API, after running the spawning thread&#x27;s closure, uses a destructor* to join all the created threads. However, if a program could longjmp() past that destructor, then the spawning thread could access its variables again while the created threads are still using them, resulting in a data race. (This is the same issue that the original thread::scoped() API had.) let mut x = 0; let mut jmp_buf = JmpBuf::default(); if setjmp(&mut jmp_buf) == 0 { std::thread::scope(|s| { s.spawn(|| loop { x += 1; }); longjmp(&jmp_buf, 1); }); } else { loop { println!(\"{x}\"); } &#x2F;* the created thread is still changing the value of x! *&#x2F; }That is to say, longjmp() has always been a very unsafe operation, which can only be sound if you&#x27;re in control of every single destructor which it skips. A Leak trait wouldn&#x27;t change anything in that regard.* Or rather, a catch_unwind() followed by a resume_unwind(), which is mostly equivalent to a destructor in this context. reply pornel 3 hours agoparentprevRust has extern \"C-unwind\" now, so if you can modify the C code to call Rust&#x27;s panic instead of longjmp, it will unwind and run destructors properly. reply workingjubilee 17 hours agoparentprevYou have not described a mechanism or API by which the compiler or programmer could enforce that only Leak types are on the stack and that a longjmp will not traverse `!Leak` types. And the only real way to get this would be to describe an interface that would have to be variadic to allow calling functions in general, as you would have to ensure that all args to a given `impl Fn*` are `T: Leak` for `T0` through `Tn`, and Rust currently lacks variadic generics. At that point, you might as well just do the same for `!Drop`. reply amluto 22 hours agoprevI wonder if the new-edition approach could be done by interpreting pre-2024 code as if everything had a Leak bound. So instead of a not-Leak type being disallowed when passed to a pre-2024 module, it would not pass type checking because everything in the pre-2024 module required Leak. reply jeff-davis 21 hours agoparentThe article addresses this: \"However - and this is the big, enormous caveat of this section - this would rely on the compiler effectively implementing some kind of unbreakable firewall, to prevent a type that does not implement Leak from ever getting used with pre-2024 code.\" reply amluto 13 hours agorootparentWhat I mean is: instead of having a “firewall” in the compiler, is there a way to interpret pre-2024 code such that it has correct but conservative Leak bounds? Then there would be more confidence that, if a mixed-edition program type-checks, then it’s correct. reply kmeisthax 20 hours agoprevWhy would Move&#x2F;Leak being tied to editions hurt compile times? Presumably old editions would have the required trait bounds added immediately after parsing, and the existing trait machinery would handle that. You&#x27;d need extra checks for error messages so you can tell people using !Leak&#x2F;!Move with old code why they have extra trait bounds, but that&#x27;s about that. reply Rusky 16 hours agoparentThe idea is that this would be a huge number of extra bounds that need to be processed. This is one of the more computationally expensive parts of the frontend. reply nynx 21 hours agoprevI’m actually glad the Move and Leak didn’t get in. They both have fairly niche usecases but they’d have to be spattered all over basically all rust code. reply jeff-davis 21 hours agoparentWhy do you say they&#x27;d have to be splattered over basically all rust code? The post compares them to Send and Sync, and I don&#x27;t have to use those very frequently (though I don&#x27;t write a lot of rust code, either). reply nynx 21 hours agorootparentPeople use Arc and Rc all the time, which would need +?Leak annotations. Things like Option would, I think, also need + ?Move + ?Leak annotations. reply bryanlarsen 20 hours agorootparentIf it is an unobtrusive as the existing ?Trait annotations such as ?Sized it&#x27;s not very much of a burden. reply mlindner 20 hours agorootparentprevBecause features are always used in the worst (most evil?) way possible by inexperienced programmers. One of the benefits of Rust is that it makes many evil things either impossible or easily detectable. reply thayne 18 hours agoprev> this would rely on the compiler effectively implementing some kind of unbreakable firewall, to prevent a type that does not implement Leak from ever getting used with pre-2024 codeCouldn&#x27;t this be done just by changing how 2021 code is compiled, such that:All types impl LeakAll generic type parameters have a Leak constraint addedAll trait objects are changed to include \"+ Leak\"In other words, treat pre 2024 code as if it was the equivalent code in the presense of the Leak trait, requiring that all types are Leak. reply withoutboats3 3 hours agoparentThat&#x27;s what my post is suggesting. The tricky part is making sure you don&#x27;t sneak a !Leak type from 2024 code into some 2021 code. reply pornel 2 hours agorootparentIf you inserted `+ Leak` on every trait bound in 2021 code at parsing time, then I don&#x27;t think you&#x27;d need any extra firewall later.You&#x27;d just treat all code the same, as supporting Leak, and process trait bounds as usual. reply withoutboats3 2 hours agorootparentAriel Ben-Yehuda has sent me an example which shows how much harder it is than that, and which this wouldn&#x27;t cover. I&#x27;m going to write a follow up blog post.EDIT: https:&#x2F;&#x2F;without.boats&#x2F;blog&#x2F;follow-up-to-changing-the-rules-o... reply pornel 29 minutes agorootparentWith the solution I imagine, this example wouldn&#x27;t compile.In the crate bar, edition 2021, the code written as: fn foo(input: T)would actually parse as if: fn foo(input: T)An implicit Leak bound would be inserted into AST of every trait bound in the 2021 edition. Then all code from all editions would be interpreted the same.This way, the 2021 code wouldn&#x27;t compile, because the 2024 trait requires `` bound, and the 2021 code can only express `` and nothing less.2021 code could not work at all with any 2024 code that allows !Leak. libstd would have to add `+ Leak` to all existing APIs. replyYgg2 22 hours agoprevI think eventually or at some point there might be a need for Rust 2.0 even if it won&#x27;t be called Rust. Think something like JavaScript and TypeScript. A language that&#x27;s a super set except the former would be soft deprecated.I know this is highly unpopular opinion, but I don&#x27;t consider permanent backwards compatibility a good thing. reply vimpunk 22 hours agoparentWhat parts of Rust would you like changed, though?I think the current version, with its minor warts, is a very pleasant and consistent experience for the most part -- very much unlike C++. So I&#x27;m not sure what I&#x27;d change to such a drastic degree that it&#x27;d require such breaking changes. reply Taek 21 hours agorootparentI would definitely want to see async totally overhauled. Also it would be great if major components of the standard library were all version 1.And if I want to risk going too far, I&#x27;d also advocate for refactoring the type system to be slightly less generic in favor of chasing faster compile times.Also, ideally the lead designer of SafeLang2.0 would be much more heavy on engineering experience than PL theory. Rust made a lot of tradeoffs that were beautiful in PL theory but are annoying to use on a day to day basis.I honestly think rust is equal parts amazing ideas and mediocre ideas, and I think a slighky less ambitious language would still be almost as safe (world class compared to any other language) but 10x more fun to use. reply gmnash 20 hours agorootparentA complete overhaul of async sounds nice in the abstract, but without exact details it’s difficult to see how it would be better than what exists.Similarly, I’m curious as to what the details of a “slightly less generic” type system entails. reply thayne 18 hours agorootparentprevBesides the Move and Leak traits mentioned in this article, there are several changes I would make to the standard library if backwards compatibility wasn&#x27;t a concern, including:Changing Iterator to make use of Generic Associated Types.Change std::env::set_var to be unsafe. Because using it in multithreaded program causes undefined behavior for any c code that reads the environment (which includes quite a few libc and posix functions).Change channels to have a better API that is mpmc and closer to crossbeam.And probably others I can&#x27;t think of at the moment. reply lawn 22 hours agorootparentprevasync still feels half-baked and not pleasant like the rest of the language.Now, it may be something that still can be fixed and improved upon, but a Rust 2.0 with nicer async experience would be a big win. reply ysavir 22 hours agoparentprevI generally agree with you there. Permanent backwards compatibility has strong early rewards and strong late punishments.I&#x27;d love to see programming languages and frameworks add a \"gamma\" phase to their initial releases. Alphas can be for the internal development, betas can be for stuff that&#x27;s being tested for bugs and functionality on a public level, and gamma can be a state of \"the code can be used out in production, but the API and functionality may change in the future once we see how the tool is used and can identify and resolve operational bottlenecks\". Just a handy state that says \"feel free to use this, but keep in mind that the interface will likely change for the better but in a non-backwards compatible way based on your experiences using it\".I&#x27;m pretty sick and tired of almost every project needing a version 2 because version 1 was essentially publicly released proof of concept that didn&#x27;t know better. reply Guvante 20 hours agoparentprevWe still don&#x27;t have C++ 2.0And if anybody brings up breaking changes in C++ I will point to how after any of those it sometimes takes almost a decade for everyone to update.And that was for breaking changes designed to have minimal impact. See how Python 2 is still used in many places 15 years later for what happens if you change how strings work for example. reply tialaramex 18 hours agorootparentWG21 ships a completely new language - very similar to but technically unrelated to the previous one - every three years. C++ 11, C++ 14, C++ 17, C++ 20, and (later this year presumably) C++ 23. It is fascinating that people consider the resulting arbitrary source code breaks to be complete compatibility and yet Rust&#x27;s choice to offer ongoing compatibility to Rust 2015 edition (Rust 1.0) is somehow not enough...C++ already changed the definition of std::string, that&#x27;s why your Linux distro went through painful C++ ABI changes many years ago. They went from one terrible string design to a different terrible string design, ruling out some optimisations GNU can chosen, but mandating optimisations other groups had chosen.Don&#x27;t worry though, they didn&#x27;t standardize an actual string slice type (Rust&#x27;s &str) until much later, as std::string_view in C++ 17, so most software which didn&#x27;t actually care about string allocation needed to be thrashed around anyway because there was no vocabulary type for this most obvious thing aside from the hopeless C-style \"nul terminated char *\".It&#x27;s actually hard to over-estimate how terrible they are at this (language standardization). Is it really just that there are too many of them? I find it hard to imagine that just large numbers of people can explain it, plenty of people co-operate to create Rust and the results aren&#x27;t anywhere close to as awful. Maybe it&#x27;s the ISO process? I have never participated in ISO standardization of something this complicated, maybe the process is somehow poisonous.But I think the best explanation is the culture. C++ has developed a culture which prizes arcane nonsense. reply Guvante 15 hours agorootparentC++ isn&#x27;t a language that anyone uses.Everyone uses GCC C++ or MSVC C++ or Clang C++.This includes the committee that makes the standard and requires a working battle tested implementation before inclusion.So you get a Frankenstein result because that is the input.And while yes in theory the standards define a kind of C++ that works on every compiler but the reality is most compilers don&#x27;t fully support that for literal years best case.So everyone uses what is available and writes abstractions to handle the harsh edges if they support multiple compilers. reply tialaramex 14 hours agorootparent> This includes the committee that makes the standard and requires a working battle tested implementation before inclusion.This \"requirement\" is a mirage, as you can tell by simply installing any of the three compilers from say the week C++ 20 was published and trying to use the headline \"modules\" feature described in the standard. There are no working implementations.If you&#x27;ve worked at a large dysfunctional organisation you&#x27;ll recognise the pattern. Nobody wants to say \"No\" and make a decision themselves which may go badly - so instead they look for excuses and you&#x27;re listing one of the excuses. But when they want to do it none of the excuses apply. reply Conscat 17 hours agorootparentprev> ruling out some optimisations GNU can chosenGNU didn&#x27;t do any optimizations that are acceptable in a language with move semantics. The new `std::string` is _strictly_ better than the old one. Although there do exist very many superior string containers outside of the standard library. libstdc++ still supports the old ABI, by the way, as `std::__cxx11::string`. reply kukkamario 17 hours agorootparentprevLarge part of why standard was changed so that GCC&#x27;s copy-on-write strings became non-standard compliant was because CoW made std::string_view and such extremely hazardous. CoW has its benefits but also huge downsides, and small-string optimisation makes lot more sense with the C++11 move semantics. reply tialaramex 14 hours agorootparentFor many people SSO would be a waste of time except you need to store the empty string.There are people who make a lot of very short strings and need them to go fast, but not so many to justify making that a core language feature, if not for the fact that C++ empty string needs a zero byte for C compatibility.That&#x27;s why Rust got away without it, their empty string has length zero. reply twic 20 hours agoparentprevI&#x27;m not sure if it is an unpopular opinion; i&#x27;m not sure it&#x27;s an opinion at all, since it seems like a statement of fact - no programming language lives forever, eventually being superseded by newer languages designed with the benefit of lessons learned with the older one.Except Fortran, Fortran is forever. reply Ygg2 16 hours agorootparentAnd Ada, Ada was created perfect. reply ferfumarma 21 hours agoparentprevI agree, and would go even further: that all major versions of languages should be renamed.Like pearl and parrot, as I understand it.It completely avoids the chance that \"python\" will become ambiguous regarding which set of rules it refers to: version 2 or version 3.And the downside is minimal, because anyone can find the most current name online. reply Klasiaster 18 hours agoprevI think a \"linter\" that does formal verification would be a good solution. People use things like clippy already and my hope is that using one of the verification tools such as Kani could become equally widespread for checking common things like absence of panics and leaks. reply j16sdiz 23 hours agoprevrust is _almost_ as complex as C++. Introducing these new \"edition\" would grow the complexity exponentially.I hope they would think twice about the effects on new learners before even experimenting reply weinzierl 23 hours agoparent\"rust is _almost_ as complex as C++.\"I don&#x27;t agree, but even if it were true, C++ has so much accidental complexity (because of its legacy) that you cannot meaningfully compare it with Rust, which by and large is very well thought out.In general, if something is complex in Rust that is because it&#x27;s in the nature of the problem and not because of some quirkiness in the solution.Borrowing is the prime example. People find it complex, but just because the C++ compiler doesn&#x27;t check the rules for you, doesn&#x27;t mean you have follow them in C++ as well (which is tedious and error-prone) or know exactly why you don&#x27;t have to follow them (which is even harder).I admit though, that the Leakpocalypse (which the article talks about, even if it doesn&#x27;t call it like that) is an area where accidental complexity lurks. reply vlovich123 23 hours agorootparentPin is another one. I still can’t wrap my head around it. I’m sure it’s simple and the docs are great, but having read them multiple times I’m not really more clear. I probably need to see a bunch of hands on examples. I wish they’d gone the Move route instead of rushing to 1.0 and then retrofitting in Pin.But I agree, in general things in rust are a lot more thought out and experiments are relegated to unstable which is fine with breaking changes.I do wish though that they had a better versioning story in the language for making backward incompatible changes more a matter of course. It’s great already for “user space” code via editions, but as the author notes making certain kinds of changes brings up risk and uncertainty because there’s not a well trodden path to support changing core semantics in the language and having it work gracefully without needing a migration en masse. reply majewsky 23 hours agorootparent> I wish they’d gone the Move route instead of rushing to 1.0 and then retrofitting in Pin.That is easy to say, and I don&#x27;t disagree with the sentiment, but there are valid reasons for setting a 1.0 release date and cutting corners to get there. If they had not done that, there probably was a real risk of running an infinite loop of \"let&#x27;s wait for Move to be implemented\", \"great, now let&#x27;s wait for Leak to be implemented\", \"great, now let&#x27;s wait for...\" and a 1.0 never gets put out. Rust may not be perfect, but at least it&#x27;s stable and people are actually using it. reply cmrdporcupine 22 hours agorootparentprevPin used to confuse me, but now it&#x27;s clearer. It&#x27;s right in the name: Pin this thing to where it is in memory, never try to move it.Rust can and does move things around as it feels the need. So you need something like Pin if what you&#x27;ve built can&#x27;t move.It&#x27;s confusing if you&#x27;re coming from a C++ background because in C++ something only \"moves\" if you explicitly std::move it. In Rust its effectively the reverse. A crude way of thinking about Rust&#x27;s call pattern is that it&#x27;s basically the opposite: as if every call was a std::move operation.So Pin is basically just saying: you can&#x27;t move this, don&#x27;t try. The memory can&#x27;t be moved. And it&#x27;s mostly used in corner cases where some kind of explicit memory management is involved.Most Rust users should not have to worry about it. It&#x27;s one of those things that becomes more of a concern if you&#x27;re writing library code for low level datastructures, or interfacing with C&#x2F;C++ libraries. reply laurencerowe 21 hours agorootparent> Most Rust users should not have to worry about it. It&#x27;s one of those things that becomes more of a concern if you&#x27;re writing library code for low level datastructures, or interfacing with C&#x2F;C++ libraries.Don’t all Rust users writing async code have to worry about Pin? reply brigadier132 20 hours agorootparentNot really, the async libraries hide the implementation details from you. It&#x27;s an issue for library developers. reply cmrdporcupine 20 hours agorootparentprevI&#x27;ve never had to and I have stuff that uses async all over. The only time I&#x27;ve seriously had to deal with Pin is for some pretty low level stuff: manual memory&#x2F;heap management for database datastructures (paged btree) and for some work stuff related to working with a C++ library that uses zeromq, etc. The point being that in both cases these were frobby dark corners where the mechanics of the pinning ends up being hidden from the user.People writing application code in Rust should not encounter it often. reply hedora 20 hours agorootparentYou get to avoid pin until you can’t, and then it’s because of some terribly unintuitive advanced usage corner case.For instance, the async_stream crate would be much nicer if you didn’t need to pun_mut! the stream instances it provides:https:&#x2F;&#x2F;docs.rs&#x2F;async-stream&#x2F;latest&#x2F;async_stream&#x2F; reply vlovich123 12 hours agorootparentprevConceptually I understand the motivation. But trying to figure out how to express it frustrates me so much that eventually I just gave up. reply cmrdporcupine 11 hours agorootparentFair. The documentation made my eyes glaze over and I got confused. It was only once I needed it that it made sense to me. And I still don&#x27;t understand some uses of it. reply cwzwarich 23 hours agorootparentprevOne advantage C++ has over Rust is that there are multiple fairly compliant C++ implementations, and the process of making them created some shared understanding of how the language actually works from an implementation perspective. There are several aspects of the current Rust language that were effectively defined by arbitrary decisions made by the implementors, which aren&#x27;t really spelled out anywhere besides the code. reply tialaramex 18 hours agorootparentDo you have examples?I think in many ways Rust is doing a better job of tying down the tricky details than C++ has.There is no effort to explain how pointer provenance works in C++ at all. The state of the art is if you write a provenance bug, someone from a compiler vendor will gesture at DR&#x2F;260 which says provenance exists but WG14 (so C, not even C++) declines to explain how it works, and that&#x27;s all you get. In contrast Aria&#x27;s \"Experiment\" (https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;std&#x2F;ptr&#x2F;index.html#strict-provenan...) has gone rather well. The vast majority of people who want to do acrobatics with pointers can do what Aria says you have to do to get coherent behaviour, and by following her rules they can make code which works in Rust. reply uecker 16 hours agorootparentWG14 produced a document clarifying provenance in C (which Aria knew): https:&#x2F;&#x2F;open-std.org&#x2F;JTC1&#x2F;SC22&#x2F;WG14&#x2F;www&#x2F;docs&#x2F;n3005.pdf reply tialaramex 14 hours agorootparentGiven your username, I assume I&#x27;m addressing Martin Uecker and I want to commend you for putting the work in on this problem over a long period. While a draft TS is closer to actually resolving a 20+ year old problem it&#x27;s not there yet and this draft is, if you pardon me, in my opinion more than three months work from publishable.In contrast, Aria went from \"We should see whether strict rules are workable\" to the current set of Rust nightly features relatively quickly.If I want to attempt pointer stunts in C, N3005 is probably worth reading first, but it isn&#x27;t actionable from what I can tell. Contrast Aria&#x27;s changes, I can go try that (in nightly Rust) and see it works or why I can&#x27;t &#x2F; mustn&#x27;t do what I wanted under Strict Provenance and who I ought to talk to about that.I wish you luck for C 29 (?) but that&#x27;s a distant future, and we&#x27;re contrasting C++ here, so as with #embed you might take this obvious must-have for C29 and find that somehow WG21 are still so far behind they can&#x27;t do the same for C++ 29. replyrapsey 23 hours agoparentprevC++ versions are practically a new language. Last two Rust editions required no changes on any of my code. Why are people constantly harping on this obviously untrue idea. New Rust versions and editions make the language _simpler_. reply kaashif 21 hours agorootparent> C++ versions are practically a new language. Last two Rust editions required no changes on any of my code.This juxtaposition kind of makes it seem like that new C++ versions did require changes to your code, did they?My understanding is that C++ maintains backwards compatibility. reply rapsey 20 hours agorootparentRust editions are allowed to break backwards compatibility. If you have 2015 edition code and change your Cargo.toml and set it to 2021 edition, the code may no longer compile.My code went from 2015 to 2018 and 2021 by simply changing Cargo.toml. No code changes were necessary and it was still completely idiomatic code. Nothing was deprecated or even outdated.An idiomatic C++11 solution is not likely to be the idiomatic C++17 solution however. Every version of C++ makes deep changes to the actual language and adds a ton of features. Whereas Rust versions and editions tend to make the language simpler as they generally remove limitations. reply Conscat 17 hours agorootparentEvery release of C++ since 11 has relaxed limitations on `constexpr`, and several relaxed limitations on `template`. Many other features have had various rules relaxed in certain releases, including `for` loops, lambdas, labels, moves, and atomics.It&#x27;s true that there are usually new features which arguably make older code no longer idiomatic, but modernizing code is always optional, so what&#x27;s really the issue there? reply rapsey 11 hours agorootparentThe issue is the language gets more complex over time. The origin of the discussion was the statement Rust is just as complex as c++, which is completely wrong. Rust changes in small increments and even when they decide on a new edition that can break compatibility, it still barely changes. reply neandrake 19 hours agorootparentprevThis can be problematic when working on a project where updating the version of rust is difficult, limiting what libraries can be used - or ci&#x2F;cd where devs may not have direct control of the environment. It’s less of a language compatibility issues and more deployment logistics issues. reply duped 22 hours agoparentprevRust already has editions. There have been three releases since 1.0. reply minroot 19 hours agoprev [–] Everytime I see Rust, I feel like terrorism happening in PL replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This post examines the API decisions and marker traits in Rust programming language, particularly focusing on Send, Move, and Leak traits.",
      "The author contemplates incorporating a Leak trait to Rust but opposes it due to potential confusion and compatibility challenges it may create.",
      "Despite experessing regret for not integrating the Leak trait earlier, the author acknowledges the complexities in implementing such modifications at the current stage."
    ],
    "commentSummary": [
      "The RustHacker forum underlines the challenges of making modifications in Rust, primarily concerning async Rust, due to the absence of a Leak-style trait, therefore necessitating workarounds.",
      "There is ongoing discourse about the implications of accommodating outdated compilers, potential remedies, and the prospect of unveiling a new iteration of Rust comparable to JavaScript and TypeScript.",
      "The participants express diverse viewpoints on the intricacies and constraints of programming languages, which include pinning in Rust and comparisons with C++, demonstrating the complexities and compromises inherent in programming language design and execution."
    ],
    "points": 185,
    "commentCount": 98,
    "retryCount": 0,
    "time": 1694958551
  },
  {
    "id": 37546255,
    "title": "Tech Independence",
    "originLink": "https://sive.rs/ti",
    "originBody": "Derek Sivers Tech Independence Contents: What? Register a domain Change DNS nameservers Create storage Create an SSH key Create your server SSH into root Customize these instructions Use your storage Contacts and Calendar Email sending Email settings Simple website File sharing in /pub/ More indie tips More storage? Mutt = email in terminal Upkeep Trouble? Start over Questions? Additions? What? Tech independence is not depending on any particular company or software. The only tools you need are the common open source basics built into any Linux or BSD operating system — free public-domain tools that are not owned by anyone, and can run on any computer. Learn a few of these basic tools, and you can run your own private server on any computer forever, for the rest of your life. Host your own website and email. Keep your own contacts and calendars synced with your phone. Back up and sync your photos, movies, and music to your own private storage. No more subscriptions needed. You can ignore all the companies offering “solutions”, even if they are free, because they take away self-reliance. The point is to know how to do it yourself, not to have somebody do it for you. It’s worth a little up-front work, like learning how to drive. Below are simple step-by-step instructions that work. Instead of drowning you in options, it uses an operating system called OpenBSD and a hosting company called Vultr because I’ve used them for years and I know they are good and trustworthy. But you could do this same setup with any free Linux or BSD operating system, with any hosting company that gives you “root” access to your own private server. You could even do it on an old laptop in your closet. So if a company turns evil or goes out of business, no problem! You can set up a new server anywhere else in an hour, point your domain name to the new IP address, and it’s done. That’s tech independence — never dependent on any particular provider or software. It’s very empowering. The instructions below will show you how. Register a domain Go to Porkbun.com. Search for a domain name you like until you find one that’s available. Create a new account, and pay. Congratulations. You’ll use this domain name in many of the steps below. Change DNS nameservers to vultr Wherever you registered your domain name, log in there to change your domain’s DNS nameservers. It’s usually set by default to the company where you registered. So for example a domain registered at GoDaddy will have default nameservers of something.godaddy.com. Replace those defaults with these two: ns1.vultr.com ns2.vultr.com Create storage Go to Vultr.com. Create an account and give it your credit card. Click here for the “Add Block Storage” page. Click “Block Storage (HDD)”, which says “Globally Available” Below that, a list of cities. Click the one closest to you. Below that, a slider lets you choose how much storage you need. If not sure, just leave it as $1 for 40 GB. Below that, in a subtle box that says “label” type the word encrypted. Below that, click the “Add Block Storage” button. Create an SSH key Open a terminal. Windows? Start → Windows PowerShell → Windows PowerShell Mac? Applications → Utilities → Terminal Type ssh-keygen -t ed25519 and hit [enter] or [return]. When it says, “Enter file in which to save the key (/Users/yourname/.ssh/id_ed25519):”, hit [enter] or [return]. When it says, “Enter passphrase (empty for no passphrase):”, hit [enter] or [return]. When it says, “Enter same passphrase again:”, hit [enter] or [return]. See the line that starts, “Your public key has been saved in” and ends in “id_ed25519.pub”? That’s the file you need for the next step. In a text editor, open “id_ed25519.pub”. Windows? Type notepad .ssh/id_ed25519.pub Mac? Type open -e .ssh/id_ed25519.pub It should be a single line like this: ssh-ed25519 AAAAC3Nz5AAAAIPIXO5icj4LUpqa2baqYQRmCZ1+NV4sBDr you@computer You’ll use this in the next step: “Create your server”. Create your server In your Vultr.com account: Click here for the “Deploy New Instance” page. Click “Cloud Compute” (NOT “Optimized Cloud Compute”) Below that, click “Intel Regular Performance” Below that, IMPORTANT: click the same city you chose for your encrypted storage in the previous step. Below that, click “OpenBSD” (the yellow blowfish) then inside its box, click “7.3 x64” Below that, under Server Size, click “25 GB SSD $5/month” A blue pop-up appears underneath, up-selling “For only $1.00 more you can...”. Click “No thanks”. Scroll down to “SSH Keys”, click “Add New”, then under “Name” type mykey. From the previous section, step 3, copy (⌘-C or Ctrl-C) the contents of “id_ed25519.pub” and paste it into this box called “SSH Key”. It should be a single line like this: ssh-ed25519 AAAAC3NzaC1XO5iclCcrHbGRPoj4LUpqa2baqYQRmCZ1+NV4sBDr you@computer After pasting it into the box, click [Add SSH Key]. Under SSH Keys, click the box with the picture of the key called “mykey” to give it a tick mark in the top-right corner. Scroll up to “Enable Auto Backups”, click the “on” toggle button to turn it OFF. A scary pop-up says “Are you sure....”. Tick the box next to “I understand the risks”, then click the red button “Disable Auto Backups”. Under “Additional Features”, untick the box next to “Enable IPv6”, to disable it. Under “Server Hostname & Label”, type your domain name in both “server hostname” and “server label”. At the bottom, click the big blue button [“Deploy Now”]. Stretch your legs for a minute while waiting for your server status to change from “Installing” to “Running”. Copy and save its IP Address on your computer. SSH into root, and get my script Copy (⌘-C or Ctrl-C) the IP Address from the last step of Create Your Server. Open your terminal from the Create an SSH key section. Whenever I say to type something into the terminal, hit your [return] or [enter] key afterwards. Type into the terminal: ssh root@YOUR-IP-ADDRESS. So for example: ssh root@123.45.67.89 It should say something like: The authenticity of host '123.45.67.78 (123.45.67.89)' can’t be established. ED25519 key fingerprint is SHA256:OyiqVsjRX8U2f0UTUY4D0erdl6855YNRXyQk2D. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? Type yes It should say something like: Warning: Permanently added '123.45.67.89' (ED25519) to the list of known hosts. OpenBSD 7.3 (GENERIC.MP) #1125: Sat Mar 25 10:36:29 MDT 2023 Welcome to OpenBSD: The proactively secure Unix-like operating system. Congratulations! You’re inside a remote computer! Type ftp https://sive.rs/ti.sh Type sh ti.sh Watch it install, answer its questions, and do what it says. Be ready to open a new terminal window, so you can leave this one logged-in. See below for help with its prompts. Customize these instructions Enter your domain name and the username that you create, below, and this will customize all following instructions for you. Your domain name? Your username? Use your encrypted storage The ti.sh script will eventually prompt you, “Now upload anything while I wait...”. Here’s how. Mac Type rsync -avz Documents yourusername@yourdomain.name:/mnt/ and you will see it uploading your Documents folder to your private encrypted storage. Use this same format to upload any other folders, replacing “Documents” in the command. If you are happy synchronizing on the command line like this, you can skip over the next FreeFileSync section. FreeFileSync Everyone using Windows should use FreeFileSync. Here’s how: Download FreeFileSync and please give an optional donation there if you can afford to. Donating also unlocks more features. Thanks to Jon Lis for the recommendation. Install and open FreeFileSync. Top-center: click the grey [Browse] button and find the folder with the stuff you want to upload. Top-far-right: click the white cloud icon then SFTP at the top. Server name or IP address: yourdomain.name Left side: click (*) Key File Username: yourusername Browse to find your private key, called id_ed25519 from the “Create an SSH key” section. (Not the file that ends in “.pub”, but the one next to it.) NOTE: Because the /Users/yourusername/.ssh directory is “hidden” by Windows and Mac by default, I find it easier to just type the path directly, like this: The username, for this next line, should be your username on your home computer, not your remote server. Windows? Type C:\\Users\\yourusername\\.ssh\\id_ed25519 Mac? Type /Users/yourusername/.ssh/id_ed25519 Directory on server: /mnt Click OK to go back to the main screen. Top-right: click the green gear wheel. Left button: click “MIRROR →” Click OK to go back to the main screen. Top-center: click “COMPARE”, and make sure your files are there. Top-right: click “SYNCHRONIZE Mirror →” then [Start] Verify and unmount When it’s done uploading, log in to your server again, from your terminal. Type find /mnt You should see a long list of the files you uploaded. Type m-x to detach your encrypted storage. Type find /mnt again, and now you should see nothing there! Congratulations! You now see how this will work in the future: Log in and type “m” to attach your encrypted storage. Upload your files with rsync or FreeFileSync. Log in and type “m-x” to detach the storage, for security. Contacts and Calendar Your phone currently keeps its contacts and calendars with Google or Apple. Now you can get them off the cloud and keep them privately on your own server. My ti.sh setup script installs a CardDAV server for contacts, and CalDAV server for calendars. Here’s how to connect your phone. Android phone You need an app called “DAVx⁵”, so install it first. Then… Open the DAVx⁵ app Click the orange (+) in the bottom-right Click (·) “Login with URL and user name” Base URL: https://dav.yourdomain.name/ User name: yourusername Password: the “easy to type on your phone” password you made Click “LOGIN” in the bottom-right corner. It should work and bring you to the “Create account” page, where “Account name” will be yourusername. Leave everything as-is and click “CREATE ACCOUNT” in the bottom-right corner. It brings you to the “CARDDAV” header. Tick the toggle to turn on next to your domain name. Click the ♻ arrows in the bottom-right corner to synchronize your contacts. Click the “CALDAV” header up top. Tick the toggle to turn on next to your domain name. Click the ♻ arrows in the bottom-right corner to synchronize your calendar. Go to your Calendar app, and in the top-right corner, click the round icon there. (Might be your face or a letter.) Then change it to the one with yourusername. After changing it, click the X in the top-left corner. To add a new Event, Click [+] in the bottom-right corner, and choose “Event” from the popup menu. There might be a warning, “Switch to a Google Account to take advantage blah blah…”. Click “dismiss”. Title this event something like “Test Delete”, and notice it should be saving to the calendar with your domain name and username. Click (Save) in the top-right corner. Check the terminal window where it should say “Calendar entry added!” Go to your Contacts app, and in the bottom-right corner, click “Fix & manage”. Click “Settings” Near the bottom, click “Default account for new contacts”, and change it to the DAVx⁵ Address book with your domain name. Click “< Settings” in the top-left corner. In the top-right corner, click the round icon there. (Might be your face or a letter.) Then change it to the DAVx⁵ Address book with your domain name. Then click X in the top-left corner. Click “Contacts” in the bottom-left corner. It should say “No contacts in this account”. Click + in the bottom-right corner to Create contact. Top of the next page should say “Save to” then your domain name. Add a New Contact with a name like “Test Delete”. Then click “Save” in the top-right corner. Check the terminal window where it should say “Contact added! Both work. Congratulations.” Apple iPhone Settings → Contacts → Accounts → Add Account → Other → (under “CONTACTS”:) Add CardDAV Account Server: dav.yourdomain.name User Name: yourusername Password: the “easy to type on your phone” password you made Click “next” in the top right corner, and it should bring you to your “Accounts” page, where you see it listed, saying “Contacts” underneath. Click Add Account → Other → (under “CALENDARS”:) Add CalDAV Account Server: dav.yourdomain.name User Name: yourusername Password: the “easy to type on your phone” password you made Click “next” in the top right corner, and it should bring you to a “CalDAV” page, showing Calendars and Reminders. Un-tick Reminders. Click “save” in the top right corner, and it should bring you to your “Accounts” page, where you see it listed, saying “Calendars” underneath. Click “< Contacts” in the top-left corner, to go back to settings for your Contacts app. At the bottom change Default Account to the one with yourdomain.name. Click “< Contacts” then “< Settings”, both in the top-left corner, then scroll down to Calendar settings and click it. In Calendar settings, 2nd from the bottom should say “Default Calendar”. Tap to change it to the one with yourdomain.name. Go to your Calendar app and click the + in the top-right corner. Add a New Event with a Title like “Test Delete”. Then click “Add” in the top-right corner. Check the terminal window where it should say “Calendar entry added!” Go to your Contacts app and click the + in the top-right corner. Add a New Contact with a name like “Test Delete”. Then click “Done” in the top-right corner. Check the terminal window where it should say “Contact added! Both work. Congratulations.” Email sending Go to Mailjet.com and sign up for their free account. Go to this page for API keys and [Generate secret key] Give the ti.sh script your API key and Secret key, and it will do the rest. Email settings To do email from your phone, computer, or anywhere else, you now have an IMAP server, called Dovecot. So on any device, you can add a new IMAP Mail account, with these settings: Account type: IMAP Email address: yourusername@yourdomain.name Username: yourusername Password: the password you made for your username on your server Incoming mail server: yourdomain.name Outgoing mail server: yourdomain.name Connection security: SSL Authentication type: Basic authentication Simple website On your home computer, in your main home directory, make a directory/folder called “htdocs” Download this file called “template.html” and save it in your “htdocs” directory. Download this file called “style.css” and also save it in your “htdocs” directory. Make a copy of the “template.html” file, and name the copy “index.html”. This will be your home page. Edit the index.html file in a text editor (NotePad or TextEdit) and change my default text to whatever you want. When you need to add a new page, just copy the template again, call it “about.html” or whatever, and make a link to it from the home page. The header of each page will link back to index.html : your home page. If you want to change the look of your site, just edit the style.css file. Search the web for “CSS tutorial” if needed. To upload it to your public server, do one of the next two steps: Apple Mac? Open a new terminal window on your computer, type rsync -avz htdocs yourusername@yourdomain.name:/var/www/ Windows? FreeFileSync again, but now change the “Directory on server” to /var/www/ (you can find it by clicking [browse] or typing it directly) then upload this htdocs directory there. Go to https://yourdomain.name in your web browser, refresh the page, and you should see your updated website. Any trouble, just know that the goal is to get that index.html file into this location on your server: /var/www/htdocs/index.html because that’s where the web server is expecting it to be. That’s where we put the original test file, so your new index.html file should replace that one. If you want short URLs, without the .html, you can (for everything except index.html) because I set the default type to be HTML. Just remove the “.html” from your HTML filenames, update your links, and voilà! It’s important to know how to make a simple website by hand, and not let people sell you on complex solutions that are the equivalent of saying you need a jumbo jet when you really need a bicycle. For real tech independence, start by typing your HTML files yourself. Only later, after you have many many pages, consider a more complicated solution. File sharing in /pub/ Your website is configured to list all files in the /pub/ directory of your website. So basically anything in /var/www/htdocs/pub/ is public. Upload any files you want to share. It replaces Dropbox and similar services for sending big files. Just upload the file to /var/www/htdocs/pub/ then find it in your web browser, copy its URL, and send someone the URL. If the files you want to share are already on your computer, then just make a pub/ directory inside htdocs/ (so, htdocs/pub/), put your files in there, then use FreeFileSync or rsync to upload them as you did in the previous section called “Simple website”. Consider them part of your website. Or if you have a URL from somewhere else online that you want to download to your server, just do it as we did in the numbered steps above. Then use FreeFileSync or rsync to download from your server to your computer first, before your next upload sync. More indie tips Use Firefox. Install uBlock Origin in Firefox and Chrome. In Firefox settings, under “Privacy and Security”, choose “[X] Delete cookies and site data when Firefox is closed”, then close Firefox often to erase all your cookies and logins. Browse the web anonymously, not logged-in. Replace Google Authenticator with Aegis on Android or Raivo on iPhone. If you use Windows, replace it with Ubuntu Linux. (Use both at first, then slowly transition.) Keep your new email address as a private email account that you only give to those few people who you really want to hear from. Then your old gmail/yahoo/outlook/etc address can be just low-priority junk, and your new private email account won’t need spam protection. Or if you don’t want to run your own email server, use Mailbox.org or Fastmail but only by using your own domain name. Be yourusername@yourdomain.name from now on. Don’t depend on anyone else’s domain for your email or you’ll be stuck with them. More storage? If you need hundreds of gigabytes, or even terabytes of storage, I recommend Hetzner’s “Storage Box”. It’s the best storage value I’ve found. Also consider Backblaze Personal Backup. I personally use Vultr’s storage (as described above) for sensitive information I definitely want completely encrypted. Then I use Hetzner’s Storage Box for all my photos, videos, music, and other big files that don’t absolutely need to be encrypted. Mutt = email in the terminal Unless you want to read email directly on your server, skip this step. ssh in to your server, then type mutt You should see the subject headers, with the first email highlighted. Type j and k a few times to go down and up the list of emails. To read an email, hit [enter] or [return] when it is highlighted. To go back to the list, type i (for “index”) To reply, hit r then: It shows “To:” so you can edit or add recipients. Hit [enter] or [return] to leave it. It shows “Subject:” so you can edit the subject. Hit [enter] or [return] to leave it. It asks “Include message in reply? ([yes]/no/?):”. Hit [enter] or [return] for the usual norm of echoing someone’s email back at them below your reply. Or n for not. Now you are inside the vi text editor which is not self-explanatory, so I’ll walk you through a simple reply: Hit i (no [return] or [enter]) to go into “insert mode” and type your message. You’ll notice it’s on the same line as some other text, so you might want to start by hitting [return] or [enter] a few times, then up-arrow to go back to the first line again. When done typing your message, hit your [esc] key in the very top-left corner of your keyboard. Nothing will change on the screen, yet. Type :wq (the “:” at the beginning is important) then [enter] or [return]. Then you’ll see the “Compose Menu” which I think of as the “last chance before sending” screen. Hit y to send it. To send a new email, hit m then repeat those steps like you did for a reply, except now the “To:” and “Subject:” are blank and waiting for you to create. (For “To:”, type the email address of the person you’re emailing.) To quit, hit q Mutt is a great program for reading and sending email on the command line. It’s been my email client for 20 years. Read its manual here if you want to go deeper. It does everything. The vi text editor is a useful tool to edit text on a server. It takes a few minutes to learn, but it’s worth learning because it’s installed by default on every Linux/BSD server. Upkeep You honestly don’t have to do anything to maintain your server. It will just work as-is for decades! But if you like to keep it up-to-date, it only takes a minute, so run these next steps any time. Log in to your server, if you are not already. Type doas su Type syspatch Type fw_update Type pkg_add -u Type sysupgrade Type exit; exit to log out. If that last “sysupgrade” step did not give an “Error retrieving … 404 Not Found” error, that means your OpenBSD operating system is upgrading itself. They release an upgrade every 6 months. In that case, go to this OpenBSD page and follow the link at the top that says “Upgrading to (7.4, etc)” to see if there’s anything else you should know. If the “sysupgrade” step updated your operating system and your server rebooted, then there is just one more step: Log in to your server, if you are not already. Type doas su Type sysmerge Follow any instructions. Don’t worry about messing up because you can always start over, as described below. Re-do the syspatch ; fw_update ; pkg_add -u steps, above. Type exit; exit to log out. Trouble? Start over I’ve tested the steps above very carefully and repeatedly. They work. So if you hit a major problem, something not happening like it says it should, please do this: Type “cd ; m-x ; exit” in any terminals you still have open, until they are all closed. Go to your Vultr account. See your server instance? See to the far right, a subtle ···? Click that. From its pop-up menu, click the last option: “Server Destroy”. Tick the box next to “[X] Yes, destroy this server.” Click the big red [Destroy Server] button. This will not destroy your encrypted storage. That’s another reason we kept it separate from the start. So if you already uploaded a bunch of your files and want to save them, they should still be there. On your own computer, in the terminal, type: rm .ssh/known_hosts Go back to the section called “Create your server” and try again. Questions? Additions? To learn more about your new server, just log in and type: help It will teach you the basics. Then for each command or file you want to know more about, type man followed by the command or filename. So for example, log in and type… man adduser man ssh man doas man rcctl man pkg_add man ftp man httpd.conf Hit your [space] bar to scroll the page, then q to quit. It’s one of the most wonderful things about OpenBSD: everything you need to know is in those man pages! No need for YouTube, Google, ChatGPT, or any other advertising-driven sources of information. I will constantly improve this page, so get on my private email list for updates. Until then, ask any questions. If something went wrong, please give me a very specific description of exactly what went wrong at what step, what it was supposed to do, and what exactly it actually did. Click here to email me. Requests for what to add? Again, just email me.",
    "commentLink": "https://news.ycombinator.com/item?id=37546255",
    "commentBody": "Tech IndependenceHacker NewspastloginTech Independence (sive.rs) 168 points by jjude 22 hours ago| hidepastfavorite63 comments anderspitman 17 hours agoThe author talked about this a few months ago on Tim Ferriss&#x27; podcast[0]. One of my favorite episodes.I&#x27;m passionate[1] about the concept but articles like this are a reminder to me that we need to make self hosting an order of magnitude simpler and accessible to more people. It shouldn&#x27;t need to involve any CLI, DNS, TLS certs, port forwarding&#x2F;NAT traversal, IP addresses, etc etc.Self hosting shouldn&#x27;t be any more difficult or less secure than installing an app on your phone. The flow should be 1) install the \"self hosting app\" on an old laptop or phone. 2) Go through a quick OAuth2 flow to connect your app to a tunnel that enables inbound traffic. 3) Use the self hosting app to install other apps like Jellyfin, Calendar, Nextcloud, etc. Everything should be sandboxed (containers work pretty well on Linux and Windows 10&#x2F;11 via WSL2) and secure by default. Automatic backups (ideally an OAuth2 flow to your friends&#x27; self hosted installations) and auto app updates are table stakes.There&#x27;s no technical reason this can&#x27;t all be done, but lots of technical challenges, and it&#x27;s unclear whether anyone will pay for tunnels. I&#x27;m currently trying to figure out how to do reliable auto backups without filesystem snapshots.[0]: https:&#x2F;&#x2F;youtu.be&#x2F;0BaDQCjqUHU?si=0wDf-2RH-u9vdm3g&t=1380[1]: https:&#x2F;&#x2F;github.com&#x2F;anderspitman&#x2F;awesome-tunneling reply noman-land 15 hours agoparentLets do this. There&#x27;s literally no reason not to. It could even be a small standalone appliance that you plug in. It could be no bigger than Mac charging brick, and could even function as one.We have to divorce society from these abusive corporate cloud relationships. It made sense 20 years ago. It is actively poisonous today.We can easily make a turnkey opt-in peer to peer cloud using today&#x27;s consumer grade open hardware and software, much of it default off the shelf. reply eternityforest 12 hours agorootparentI think the problem is there&#x27;s very little overlap between people who are interested in this stuff, and people who are interested in what typical consumers want.People are really focused on privacy, and even opt-in integration with nonprivate hardware and services doesn&#x27;t happen much.Convenience features gtt completely ignored, and worst of all, a huge amount of P2P stuff has no mobile support.Furthermore a lot of it involves a self hosted single point of failure. For me that&#x27;s a complete deal breaker, it&#x27;s not acceptable that a service could go down because something happened to a home server while I was away.And then on top of that, most p2p projects for about 10 years were completely impractical blockchain things that either cost money or huge amounts of bandwidth and compute.Self hosted, with decentralized identity not tied to a domain name, with automatic backup and redundancy to a selectable cloud provider via an open source protocol, with a very high quality mobile app, and smartwatch support, etc, would be amazing.But there&#x27;s not much interest, and it basically can&#x27;t be done in a UNIXy way, since a lot of the value the clouds provide is in the tight integration of everything, with voice assistants and calendars and a million little things that are individually maybe not even worth setting up manually. reply eternityforest 12 hours agorootparentprevI think the problem is there&#x27;s very little overlap between people who are interested in this stuff, and people who are interested in what typical consumers want.People are really focused on privacy, and even opt-in integration with nonprivate hardware and services doesn&#x27;t happen much.Convenience features gtt completely ignored, and worst of all, a huge amount of P2P stuff has no mobile support.Furthermore a lot of it involves a self hosted single point of failure. For me that&#x27;s a complete deal breaker, it&#x27;s not acceptable that a service could go down because something happened to a home server while I was away.And then on top of that, most p2p projects for about 10 years were completely impractical blockchain things that either cost money or huge amounts of bandwidth and compute.Self hosted, with decentralized identity not tied to a domain name, with automatic backup and redundancy to a selectable cloud provider via an open source protocol, with a very high quality mobile app, and smartwatch support, etc, would be amazing.But it seems like people these days aren&#x27;t interested in feature rich commercial style zero maintenance apps, so I&#x27;ll probably keep mostly ignoring the entire concept of self hosting until that changes. reply nativeit 13 hours agorootparentprevI’ve been using Caprover (https:&#x2F;&#x2F;github.com&#x2F;caprover&#x2F;caprover - think stripped-down Heroku on any given Docker box) for a few years, and it’s hardly consumer-focused, but has accomplished a good portion of what would ultimately be required for such a product. It’s always that last bit where the effort&#x2F;risk&#x2F;cost&#x2F;[insert prohibitive factor here] becomes precipitously steeper and challenging. I think it’s a fairly natural thing, but also it’s got a lot to do with being not only more difficult, but also you’re then faced with tackling it under the full weight of every technical decision you’ve made up until that point, which can severely limit the plausible approaches.I’d be keen to work on a project to marry a PaaS like Caprover with networking using ZeroTier or Tailscale, packaged in such a way that it could be easily deployed onto most reasonably equipped platforms, or delivered as a service. reply skizm 12 hours agoparentprevDon’t most home internet packages specifically forbid running home servers? They don’t seem to enforce it anecdotally, but annoying that they could technically shut you down at any moment with no notice unless you buy a business plan. I’m in the US and I’ve only had access to two different ISPs though and only ever one at a time, so no way to shop around. reply anderspitman 18 minutes agorootparentOne advantage of tunneling is your ISP doesn&#x27;t know what the traffic is. Also I expect ISP competition to slowly improve. More people are getting access to fiber all the time. reply lazypenguin 14 hours agoparentprevThey tried, it was called sandstorm https:&#x2F;&#x2F;sandstorm.io&#x2F; reply anderspitman 9 hours agorootparentSandstorm is awesome but requires significant modifications to apps in order to work within the system. Also, it doesn&#x27;t solve the complexity problem. You still need a programmer or sysadmin to set it up and manage security.See also Cloudron, Yunohost, Umbrel. reply lifty 17 hours agoparentprevI agree. I think people have just been used to the current state of affairs in managing servers. There’s no reason why they can’t be like appliances or mobile OSes. reply wildrhythms 2 hours agorootparentThe problem is eventually this &#x27;appliance&#x27; needs to connect to the public WWW and that is a problem for most residential connections because ISPs don&#x27;t play nice with that sort of thing, at least in the U.S., and now you get into having to configure port forwarding and dynamic DNS and so on. reply anderspitman 17 minutes agorootparentTunneling solves the ISP problem and the complexity problem of handling inbound connections. It also has some nice security benefits reply akavel 17 hours agoprevFWIW, I recently found a VPS offering for $1.41&#x2F;month (!) @ 1.5GB RAM & 30GB HDD via https:&#x2F;&#x2F;lowendbox.com&#x2F;, at https:&#x2F;&#x2F;my.racknerd.com&#x2F;index.php?rp=&#x2F;store&#x2F;black-friday-202... (please note I have no idea how reliable it is though!). I managed to deploy NixOS there through nixos-infect (https:&#x2F;&#x2F;github.com&#x2F;elitak&#x2F;nixos-infect), and then further configure it with NixOps. That said, using NixOps does currently require a Linux (or Mac, probably) box as the managing one, and some Nix-fu, which is definitely non-trivial. A draft (WIP) writeup on that, if you&#x27;re interested: https:&#x2F;&#x2F;github.com&#x2F;akavel&#x2F;scribbles&#x2F;blob&#x2F;main&#x2F;_drafts&#x2F;202308... reply alabhyajindal 19 hours agoprevI love this article.The section &#x27;More Indie Tips&#x27; is great, especially if you don&#x27;t plan to follow the guide: https:&#x2F;&#x2F;sive.rs&#x2F;ti#indie reply nyanpasu64 7 hours agoprevWould it be practical to use mesh networking (eg. Hyperboria, https:&#x2F;&#x2F;changelog.complete.org&#x2F;archives&#x2F;10478-easily-accessi...) to access your machines from remotely using their public key rather than a domain name you have to pay for and renew? reply kartoshechka 7 hours agoprevgreat post! I&#x27;d like to mention one more \"indie\" tip - physical security key is nice to have (2 even better) if you plan to lose&#x2F;break your phone, or travel frequently. Add the most important auth keys (bank, email, etc) directly to the physical key, back them up on the second one, and now you&#x27;re less \"working smartphone with an active sim\" dependent :) reply baz00 19 hours agoprevRelying on your cloud provider&#x27;s backup &#x2F; restore solution is not a backup. reply johnea 20 hours agoprevI do agree that it&#x27;s not exactly \"self hosting\" when you use vultr.comOnce you&#x27;ve gone to all the other trouble, pay a little extra to the ISP for a static IP, and then any computer is your own \"cloud\"... reply nik282000 18 hours agoparentDepending on your setup you can use dynamic DNS and save yourself the cost of the static IP. Either way it will always be cheaper per GB of storage to host at home than in &#x27;the cloud.&#x27; reply reidjs 17 hours agoparentprevI&#x27;ve read that this is potentially dangerous as you are opening up your home network to the Internet, is there any truth behind that? reply 6ak74rfy 8 hours agorootparentIt depends on how you technology&#x2F;security savvy you are.For instance, here is everything I do:- Use an open source firewall+router (== Opnsense) and not commercial routers (such as Netgear, Tp Link etc.) - Open up port 80 and 443 on the firewall. - Both the ports go to a Traefik reverse proxy that is configured to always redirect port 80 to 443. - Traefik then reverse-proxies requests to relevant Docker containers. - Auto-update Traefik every day (through Watch Tower). - Use Authelia, with 2FA, where I can for the publicly available services.I assume I am reasonably secure but I&#x27;ve also built this over a few months. You may not get there right away, so start small and slow and don&#x27;t go crazy early on. reply Tcepsa 17 hours agorootparentprevYes, I believe that&#x27;s correct. If any of the services that you are opening&#x2F;exposing in this way contain vulnerabilities, those could be exploited to gain unauthorized access to the hosting machine. Attackers could then use the compromised machine as a staging area to launch attacks against other systems on your home network.Putting the hosted machine in a separate VLAN (like a guest network) can mitigate that, but it means you have to do that configuration correctly.(I am not confident enough in my own abilities&#x2F;knowledge with respect to these vulnerabilities to try it, and so it may turn out to be very straightforward. I hope to do something along those lines someday but so far the risk has outweighed the reward for me.) reply iksm 16 hours agorootparentVLAN is not intended to be used like that. You want to rely on a trusted firewall you own, with separate interfaces and appropriate firewalling rules. This can provide an isolation between networks.Behind this, any pirated server could decide to send VLAN tagged packets that may go trough the firewall if the rules are bad, or read any of them arriving to it.VLAN&#x27;s are useful if you want to \"tag\" packets with ID&#x27;s going trough specific interfaces for segmentation purposes. The tag is applied from the interface standpoint, so this gives a virtual segmentation between ports of machines you are supposed to always control, like between a port on your router and ports on a managed switch.In this case VLAN&#x27;s are configured on the router&#x27;s interface and the switch interfaces, but the exposed server is not aware about it, and can&#x27;t change it, so you can know the ID is right.This is often believed this is required to isolate networks, this is wrong, you just need to have separate interfaces. reply ojbyrne 18 hours agoprevMinor quibble&#x2F;correction request - the FreeFileSync section (Windows specific) includes some Mac-specific instructions in Step 8. reply chillbill 19 hours agoprevI&#x27;m all for tech independence. But if you need to be spoon-fed the instructions like this and you don&#x27;t get what most of it is doing, YOU DON&#x27;T WANT TO DO THIS. Best case scenario you&#x27;ll get locked out of your own stuff or important information.Yes, you should strive for that, and you start by learning. Contrary to popular belief, you don&#x27;t need to be a linux ninja to be able to host your own website and calendar.The stuff mentioned in this article are the bare minimum, and you should want to do it yourself without being spoon fed the steps.With that aside, this is exactly the kind of guide I would expect a three-letter agency contractor or worker to spread in order to \"help you\" stay off the grid, then unceremoniously drop a disaster on your head. reply iksm 17 hours agoparentTotally agree. Better look for local associations that provides hosting services if you don&#x27;t have any system administration knowledge. They&#x27;ll help you more, and you&#x27;ll waste less time and probably money, plus they may help you physically setting up your devices correctly with your services hosted on their servers.I mean, yeah it&#x27;s a minimal step by step guide that just feel to be the poster&#x27;s own todo list... As there&#x27;s many like that. To get some entry-point information this is great but this is far from being useful in practice.Basically it hides everything useful to know behind a big script that the intended reader is not even supposed to understand.I did not have seen any protection for what&#x27;s come from WAN, not even basic logging, investigation nor debugging methodology. No real backup methodology as well and the guide seems to not take system upgrades very seriously by saying \"oh, it could run so for decades, but if you want you can do system upgrades\".This is obviously false to any expert and a very risky approach. This is not how we are supposed to teach internet-connected services self-hosting. reply koch 18 hours agoprevI really can&#x27;t believe there doesn&#x27;t exist a good \"home box.\"There should be a product that you can buy (a computer) that you bring home, plug in, set up via your phone or computer that:- can host websites- can store your files and sync them to other devices- control your home automation- host your email- anything else you might otherwise put on a serverAnd does it all EASILY with a simple phone or web UI.Yes I know you can actually buy a computer or server or raspberry pi and put something like NextCloud or Home Assistant et al. on it, but the real barrier imo is the setup and configuration. Even I don&#x27;t do all this because it seems daunting to configure all of it, and I consider myself a pretty technical person. I really just want to buy a box, plug it in, and like select which apps I want to use, and then it starts working for me. reply CHSbeachbum420 11 minutes agoparentYou literally just described windows server lol. It can do all of this reply holri 18 hours agoparentprevhttps:&#x2F;&#x2F;www.olimex.com&#x2F;Products&#x2F;OLinuXino&#x2F;Home-Server&#x2F;Pionee...https:&#x2F;&#x2F;freedombox.org&#x2F; reply New_California 17 hours agoparentprevBut there is: https:&#x2F;&#x2F;umbrel.com&#x2F; (except for hosting email which is not realistic anymore). reply infogulch 16 hours agorootparentLooks nice, but the marketing design (&#x27;make it just like Apple&#x27;) doesn&#x27;t match the product they&#x27;re selling. Apple is technology for people afraid of technology, but self hosting is decidedly not for a technologically afraid audience.How will they pay for maintaining all the apps and making sure that they are properly integrated into the platform as they get updated? reply koch 16 hours agorootparentprevThis looks about like what I want! I may give it a go... reply ricardobeat 16 hours agoparentprevSynology boxes do all of that, except e-mail [1], and the web UI is quite decent.[1] it&#x27;s quite difficult to run your own e-mail servers these days, making it trusted by the rest of the world is a lot of work reply alabhyajindal 18 hours agoparentprevExactly. That would be great. But I think a large portion of the target audience of the home box would rather set this up themselves.Or not. I would much rather have something commercial (built on open source) like this so I can be more at ease that my data is safe, compared to doing everything myself. reply pizzafeelsright 16 hours agoparentprevThe NAS box from synology does all this. Except the phone part. Email self hosted might as well be impossible. reply december456 20 hours agoprevTeaching newbies &#x27;independence&#x27; by downloading random untrusted files off the internet and running them as system admin...not a cool guide i would say. reply sivers 19 hours agoparentMy previous version of https:&#x2F;&#x2F;sive.rs&#x2F;ti (until a few hours ago) had no shell script, but just walked people through every step. It took like 50+ hours to write up.But so many people were getting stuck and frustrated trying to type in all those commands, (and mistaking \"l\" for \"1\" and such), that I realized I could help more people have their own server if I put most of those steps into a shell script.Hopefully it&#x27;ll be enough to give them a taste of the benefits of having their own server, then they can learn more about the steps afterwards. reply december456 14 hours agorootparentWhile i get the pain of users, i&#x27;ve observed that habits stick and only a weak 5-20% of the learners actually drop convenient-but-insecure habits (downloading untrusted files without inspection beforehand) because convinence always wins over. I wouldnt change the system but definitely mention (more than once) the risks involved with what they are doing at the moment and to inspect the code later when they learn more. reply tkiolp4 19 hours agoparentprevC’mon. The scripts are public, you can inspect them before running them. The other alternative is to explain line by line the hundreds of lines in the scripts. Not very practical. reply december456 14 hours agorootparentWhile i agree, the issue is the target audience. If this was directed at more technical and knowledgeable tech-savvy people one-upping their game, i would be very glad and thankful for a shell script. However, its not. Its a potential starting point for being, in cool nerd terms, webmaster, and that has its own set of responsibilites and habits, habits like not downloading files and packages without checking first. While some might change the habit after learning more, i doubt that many will do that. reply boomskats 20 hours agoparentprevThat derek.jpg sure looks shady. reply shepherdjerred 18 hours agoparentprevEveryone has to start somewhere reply harryvederci 19 hours agoprevIgnore the snarky comments, this is a good initiative. Respect. reply iksm 16 hours agoparentIndeed, it is a good initiative. And that may be useful.Keep in mind that there&#x27;s many people self-hosting and exposing services to WAN that ends as spamboxes or worse from misconfigured bits.The thing is non-techy people would setup such thing and get it running, but have no technical way to maintain it. It&#x27;s a flying plane in automatic mode with no competent pilot inside. reply znpy 20 hours agoprevTech independence… then uses a third party service for outgoing email. Smh. reply sivers 19 hours agoparentMy previous version of https:&#x2F;&#x2F;sive.rs&#x2F;ti (until a few hours ago) used the built-in OpenSMTPD server for outgoing email.But then Vultr.com is not un-blocking port 25 by request anymore.That&#x27;s why I had to switch to a SMTP service. reply neilv 20 hours agoparentprevThere are good reasons to use a third-party mail server, IMHO. (I recently made that decision again.)But the reader should be aware that these writeups of how to do X often involve the writer&#x2F;publisher getting referral kickbacks from the commercial service they&#x27;re describing.I&#x27;m about to be in a position of doing something like those writeups, as a microstartup, and I&#x27;m not entirely comfortable with the affiliate programs. But the companies monetizing with privacy-invading ubiquitous profiling trackers (sometimes euphemistically called \"showing ads\" and \"analytics\"), and otherwise selling personal data, have spoiled most potential willingness of readers to pay for content. So, affiliate programs with an obvious potential conflict of interest is the only way I&#x27;ve thought of to fund the work. reply alabhyajindal 20 hours agorootparentI have been following Derek for a long time and know that he is not doing this for profit.More info if interested: https:&#x2F;&#x2F;sive.rs&#x2F;trust reply gsuuon 18 hours agorootparentI was going to mention this almost sounds like a vultr ad, but woah that&#x27;s a really clever way to go about selling a company. reply neilv 19 hours agorootparentprevAs in my case, there&#x27;s a potential conflict of interest with the affiliate programs. In his case, he has an interest in funding the trust for charitable purposes and maybe for his 5% drawdown. reply jehb 20 hours agoparentprevIs this really the issue that it used to be, though? I&#x27;m curious if I&#x27;m the only person who just doesn&#x27;t send email much anymore in my personal life.Yes, I get a lot of email. But it&#x27;s almost all transactional or subscription. The number of emails I send or receive with other humans is pretty dang low. Most institutions these days require using their platform for communications. Most people I care about who I communicate with electronically I do over SMS or Signal or occasionally a Mastodon message.I still own the domain, so I could easily pick up up and move to a different mail service in probably just several minutes of setting up an account and changing some DNS values. So while not fully independent, the time spent getting outbound email right is going to have less impact than other changes I could make. reply zrail 19 hours agoparentprevYou&#x27;re still independent of any given service. Outgoing mail is effectively stateless at this scale so the cost to switch to a different one us ~zero. reply boomskats 20 hours agoparentprevCan you even host your own SMTP server in 2023 without it being shadow-blocklisted by default? What&#x27;s your experience? reply baz00 19 hours agorootparentIt&#x27;s fine until Yahoo hellbans you with no recourse for 6 months after sending you a cryptic message in an SMTP response to visit a form and fill it in which you do to the best of your ability. Oh and inevitably there&#x27;s always someone you need to email on Yahoo. reply wejn 19 hours agorootparentprevYup. Been running my own for past two decades, still works. reply morjom 6 hours agorootparentProbably the reason its working is because its been running for two decades. reply johnea 20 hours agorootparentprevYes, you can do it! reply api 20 hours agoparentprevTrue but at this point if you don’t do that most e-mail servers will reject you.Spam pretty much destroyed e-mail as an actually open protocol. Spam destroys all open systems. reply macNchz 20 hours agoparentprevDeliverability from a cloud host IP is not going to be good. reply ChrisArchitect 12 hours agoprevThis is posted every month for months and months --- anything new here? reply dusted 5 hours agoprev [–] I upvoted this before realizing it&#x27;s a just a plug for some stuff, \"porkbun\", \"vultr\", \"freefilesync\" and some app called \"davx\" yeah, no, just by the amount of stuff you need to sign up to outside of the base BSD system, this is not tech independence. reply DANmode 4 hours agoparent [–] Domain registrar, webhost, and two widely trusted FOSS WebDAV&#x2F;CardDAV clients.Nothing to be afraid of, here, that I can see. reply dusted 3 hours agorootparent [–] The article starts with independence, and the first thing they instruct people in is becoming dependent on un-required third parties. Sure, becoming a registrar is overkill, but hosting a website on your own machine is a pretty low bar. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the idea of tech independence, encouraging the use of open source solutions like Linux or BSD operating systems to gain control over one's digital presence.",
      "It provides detailed guides on tasks like registering a domain, setting up a server, and configuring email; underlining the importance of easy provider switching without data or functionality loss.",
      "The piece further includes specific instructions for Android and Apple iPhones, along with suggestions for troubleshooting and valuable learning resources to enhance individual proficiency."
    ],
    "commentSummary": [
      "The article delves into the concept of tech independence, emphasizing the need for simpler, more user-friendly, and accessible self-hosting solutions.",
      "It underscores the limitations of existing P2P (Peer-to-Peer) systems, describes various platforms and projects, and offers suggestions for self-hosting websites and personal files at home, advising caution.",
      "The article fosters a debate on the degrees of tech independence feasible with the use of cloud host IP, eliciting contrasting perspectives on the reliability of third-party services."
    ],
    "points": 166,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1694965286
  },
  {
    "id": 37553193,
    "title": "OpenRA – Classic strategy games rebuilt for the modern era",
    "originLink": "https://www.openra.net/",
    "originBody": "News About Download Games Community Forum Resources GitHub Discord Itch Facebook Twitter Reddit YouTube Steam Red Alert, Command & Conquer, Dune 2000, Rebuilt for the Modern Era. Install Now Jump to Red Alert Jump to Command & Conquer Jump to Dune 2000 Built for Modern Expectations Updated gameplay designed around modern features like attack-move, unit veterancy, and the fog of war Online play with full support for mods and custom maps Updated campaigns with new objectives and difficulties Natively supported on Windows, macOS and Linux Made Better By The Community Fully open source and developed in the open with community input into updates and balance User created and curated maps Includes a Mod SDK to create new RTS games Regular community streams and tournaments UPDATES FROM THE DEVELOPERS [View All Updates] Posted by PunkPun on 08 August 2023 Playtest 20230808 Another week, another playtest! Notable changes include: Added new Red Alert mission “Soviet13b” Fixed dedicated servers not allowing maps with Lua scripts to be played Fixed the rare aircraft visual jitter while steering Fixed the map importer crashing when trying to convert maps with tile errors Fixed the low power notification never playing For more information, see the full changelog. And of course the Tiberian Dawn HD playtest can be downloaded here. For more information see the original playtest-20230801 announcement. In an effort of pushing out a swift release we highly encourage everybody to play some games and report any issues you may find in the comments below, on our forum, Community Discord, or GitHub. Good luck on the battlefield, commanders! The original playtest-20230801 announcement is included below: As summer is in full swing, we are excited to kick off the playtest series for the upcoming OpenRA release! This time around, it is all about spring cleaning: squashing bugs, implementing minor requested features, and preparing for larger reworks. But fear not! We still have a bunch of cool stuff lined up just for you! Without further ado, here are some notable changes you can expect in Playtest 20230801: Added new Covert Ops missions for Tiberian Dawn: “Eviction Notice” and “Twist of Fate” Added new Red Alert missions “Allies10b” and “Soviet13a” and the Counter-Strike mission “Mousetrap” Fixed several palette remapping issues and restored the ability to select darker player colors Implemented a new custom Lua wrapper which improves error reporting for map scripters Introduced a modular asset installer which allows for installation of the classic assets via Steam or the EA app Reworked sprite sequences, significantly improving third party modding support One of the smaller features is a new smoke emitter for D2k's damaged vehicles which better matches the original For more information see the full changelog. And before we wrap up, we would like mention that this release contains significant progress towards C&C Remastered Collection support, including compatibility with the new EA app. The “Tiberian Dawn HD” playtest is a separate release and can be downloaded here. This preview is multiplayer-compatible with the main 20230801 playtest. However, please keep in mind that performance, memory usage and loading times have not been optimized yet. The C&C Remastered Collection must be installed through Steam or the EA App, and in case you are using macOS or Linux the project README provides you detailed installation instructions. Stay tuned for more updates and be sure to take part in the playtest. Don’t forget to share your feedback with us on our forum, community Discord server, or GitHub. Enjoy the summer gaming season with OpenRA! ©2007-2023 The OpenRA Developers (Legal notices) News About Download Games Community Forum Resources GitHub Discord Itch Facebook Twitter Reddit YouTube Steam",
    "commentLink": "https://news.ycombinator.com/item?id=37553193",
    "commentBody": "OpenRA – Classic strategy games rebuilt for the modern eraHacker NewspastloginOpenRA – Classic strategy games rebuilt for the modern era (openra.net) 173 points by xbmcuser 6 hours ago| hidepastfavorite34 comments liotier 4 hours agoOpenRA&#x27;s Red Alert is Red Alert as it was in player&#x27;s dreams. The game is just as fun as it once was: fast, fun and limited in scope - but adapted to contemporary connectivity and display resolution.OpenRA&#x27;s killer feature is the extremely fast installation of a reliable cross-platform game that anyone can understand in ten minutes, that works in even the most skinny hardware... The perfect combination for an impromptu LAN party, where having fun with friends right now is more important than gaming depth and graphical sophistication. reply hjek 3 hours agoparentThey did some neat game mechanic tweaks, especially of the previously useless units: mechanics and thieves being able to revive and capture vehicles. Keeps the spirit of the old game but makes it more fun. reply blueflow 2 hours agorootparentThat epic moment when your thief manages to capture the Ukrainian bomb truck and sends it right back to the enemies base! reply Muromec 2 hours agorootparentUkrainian bomb track in RA? reply blueflow 2 hours agorootparentUkraine is one of the two playable Soviet factions in OpenRA. The nuke truck is a unit exclusive to Ukraine. reply FirmwareBurner 1 hour agorootparent>The nuke truck is a unit exclusive to Ukraine.I now see what the creators did there. I didn&#x27;t think about it when I was a kid. replySakos 2 hours agoparentprevExcept for the original menu UI which is unmatched in atmosphere even today. It&#x27;s the one thing that keeps me using the old RA2 instead. reply doublerabbit 2 hours agorootparentRA2 was ace, but Yuri&#x27;s Revenge, one of the greatest expansion packs IMO. Added so much more value to the game. reply rubymamis 2 hours agorootparentTotally! reply jpm_sd 32 minutes agoprevWelp, never downloading this, I&#x27;d just stop sleeping. Played the shit out of the original C&C:RA against my friends in high school. reply afavour 28 minutes agoparentI’m always terrified to download stuff like this because either what you describe will happen… or I’ll discover the game isn’t anywhere near as compelling as it is in my memories. reply josefresco 1 hour agoprevI play OpenRA &#x2F; Tiberian Dawn at least 4-5 times a week.Always skirmish, 1v1, powers disabled (no nukes) I play GDI, CPU is NOD.Takes me 20 minutes, I win 90% of the time but have to focus or else I lose easily.It&#x27;s my crossword&#x2F;sudoku&#x2F;wordle&#x2F;puzzle&#x2F;meditation&#x2F;mental reset. reply mgbmtl 34 minutes agoparentSame here, but sometimes I have to resist playing a second game :)I appreciate the simplicity of the game, but also the small details that make it fun. And that a typical game rarely lasts more than 20-30 minutes. reply gandalfgreybeer 1 hour agoparentprevHmm. Now I want to start playing StarCraft again.It’s been a while but I honestly feel like that helped me train my focus back then. reply vegancap 3 hours agoprevI&#x27;m so glad this game still exists in some way or another. And this is a really authentic reproduction. I spent hours playing this as a kid, and rediscovered this in my 30&#x27;s, and, well... have spent hours more playing it. Thank you for your efforts maintaining this classic reply rollcat 2 hours agoprevThere&#x27;s alsowhich re-implements Warcraft 2 and (very WIP) Starcraft 1. reply herpdyderp 2 hours agoprevSee also CnCNet: https:&#x2F;&#x2F;cncnet.orgNot the same goals but still enables playing the old C&C games on modern hardware. reply justusthane 1 hour agoprevI&#x27;ll take this opportunity to recommend Zero-K[1] for those who were into Total Annihilation. It&#x27;s truly excellent![1]: https:&#x2F;&#x2F;zero-k.info&#x2F; reply koolba 1 hour agoprevCan someone who&#x27;s played this confirm if it has some open source equivalent of the awesome Red Alert music?It&#x27;s those fast paced heavy metal riffs that goes so well with seeing conscripts being sent into a meat grinder. reply shrx 1 hour agoparentAs is usually the case with these open source reimplementations, the game assets (textures, sounds) must still be retrieved from the official installation &#x2F; ISO. reply johannes1234321 1 hour agorootparentWhile the owners of C&C made them available for free. Thus they can be legally downloaded, even directly from within OpenRA. reply andrewstuart 2 hours agoprevI loved these games and played so many of them but for some reason I just don’t think I’d be able to play them again. reply Raed667 3 hours agoprevIf i remember correctly it doesn&#x27;t come with cinematics or mission briefs for campaign mode. I&#x27;m guessing that would be a copyright issue. reply traspler 2 hours agoparentYou can import those from ISOs: https:&#x2F;&#x2F;github.com&#x2F;OpenRA&#x2F;OpenRA&#x2F;wiki&#x2F;Game-Content but the campaign is not complete afaik as it needs to be re-built in OpenRA: https:&#x2F;&#x2F;github.com&#x2F;OpenRA&#x2F;OpenRA&#x2F;issues&#x2F;4989 (I think this is the related ticket) reply gh02t 1 hour agorootparentImportant to note that EA released C&C, RA1, and Tiberian Sun as freeware so you can just grab the data. reply jacooper 3 hours agoprevIs there anything like this but closer to the style of C&C Generals? reply elsjaako 2 hours agoparentAccording to wikipedia there are two options, although it looks like only one of them is actually playable right now: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_game_engine_recreation... reply tailsdog 3 hours agoparentprevThis I would like to see! reply nhggfu 3 hours agoprev [–] is a ra2 release on the roadmap? reply Aeolun 3 hours agoparent [–] It’s been on the roadmap for the past 10 years. I imagine it’s not easy. reply matthewmcg 2 hours agorootparent [–] Meanwhile the old game (ra2) runs pretty well with enhancements from cnc.net to allow easier installation on modern windows systems and net play. reply FirmwareBurner 2 hours agorootparent [–] Or just Google it with \"DODI Repack\" and get it fully patched and batteries included for modern systems, if your ethics around piracy are not super strong. reply totetsu 2 hours agorootparent [–] How many times over do I need to pay for RA2 in my life haha. reply bryanlarsen 30 minutes agorootparent [–] No matter the answer I imagine your cost&#x2F;hour is pretty darn low if you&#x27;re anything like the rest of us. When that denominator is high the ratio is always going to be low! reply Applications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenRA, a volunteer-driven project, has reconstructed classic real-time strategy games like Red Alert, Command & Conquer, and Dune 2000 for contemporary systems, supporting Windows, macOS, and Linux natively.",
      "The games feature improved gameplay mechanics, online play including mod support, and novel campaigns. This project thrives on open source development and community contribution.",
      "Their latest playtest comprises bug fixes, new tasks, enhanced modding support, and forward movement in compatibility with the Command & Conquer Remastered Collection. Feedback on any issues is welcomed by the developers."
    ],
    "commentSummary": [
      "OpenRA is a project aimed at revamping classic strategy games to accommodate modern platforms, simplifying installation and offering cross-platform compatibility.",
      "OpenRA has enhanced game mechanics and provides options to import assets from the original games, hence improving the overall gaming experience.",
      "The project has sparked interest and active discussions among users about the possibility of re-implementing other classic games."
    ],
    "points": 162,
    "commentCount": 34,
    "retryCount": 0,
    "time": 1695020867
  },
  {
    "id": 37548599,
    "title": "Donut math: how donut.c works",
    "originLink": "https://www.a1k0n.net/2011/07/20/donut-math.html",
    "originBody": "a1k0n.net About Donut math: how donut.c works Jul 20, 2011 [Update 1/13/2021: I wrote a follow-up with some optimizations. ] There has been a sudden resurgence of interest in my “donut” code from 2006, and I’ve had a couple requests to explain this one. It’s been five years now, so it’s not exactly fresh in my memory, so I will reconstruct it from scratch, in great detail, and hopefully get approximately the same result. This is the code: k;double sin() ,cos();main(){float A= 0,B=0,i,j,z[1760];char b[ 1760];printf(\"\\x1b[2J\");for(;; ){memset(b,32,1760);memset(z,0,7040) ;for(j=0;6.28>j;j+=0.07)for(i=0;6.28 >i;i+=0.02){float c=sin(i),d=cos(j),e= sin(A),f=sin(j),g=cos(A),h=d+2,D=1/(c* h*e+f*g+5),l=cos (i),m=cos(B),n=s\\ in(B),t=c*h*g-f* e;int x=40+30*D* (l*h*m-t*n),y= 12+15*D*(l*h*n +t*m),o=x+80*y, N=8*((f*e-c*d*g )*m-c*d*e-f*g-l *d*n);if(22>y&& y>0&&x>0&&80>x&&D>z[o]){z[o]=D;;;b[o]= \".,-~:;=!*#$@\"[N>0?N:0];}}/*#****!!-*/ printf(\"\\x1b[H\");for(k=0;1761>k;k++) putchar(k%80?b[k]:10);A+=0.04;B+= 0.02;}}/*****####*******!!=;:~ ~::==!!!**********!!!==::- .,~~;;;========;;;:~-. ..,--------,*/ …and the output, animated in Javascript: toggle animation@@@@@@@@ ####$$$$$$$$@@@$$*******#####$$$$$$$$$$=====!!!!***#####$$$$$$$#:;;;;;;==!!!****########$###::~~::;;;=!!=!!*****#########--,,-~~~:;;;===!******#*#####**,.....,-~~:;:====!!*!***********.........--~::;;===!!!*!********!..........,--~:;;;;==!!!!!*****!!...........,-~~::;;===!=!!!!!!!!=......,,-..,,-~::;;;====!=!!!!!=.,-~~~;;##..,,-~~::;;;==========;.-~;=*#$$@...,-~~:::;;;;;======;-;!!*#$$#...,--~~:::;;;;;;;;;:-;=!**!=~...,---~:::::;;;;;:,:=:;;:-...,,,--~~~:::::::~-:;:~-.....,,--~~~~~~~~~,--,.....,,,---------.........,,,,,,,,, .............At its core, it’s a framebuffer and a Z-buffer into which I render pixels. Since it’s just rendering relatively low-resolution ASCII art, I massively cheat. All it does is plot pixels along the surface of the torus at fixed-angle increments, and does it densely enough that the final result looks solid. The “pixels” it plots are ASCII characters corresponding to the illumination value of the surface at each point: .,-~:;=!*#$@ from dimmest to brightest. No raytracing required. So how do we do that? Well, let’s start with the basic math behind 3D perspective rendering. The following diagram is a side view of a person sitting in front of a screen, viewing a 3D object behind it. To render a 3D object onto a 2D screen, we project each point (x,y,z) in 3D-space onto a plane located z’ units away from the viewer, so that the corresponding 2D position is (x’,y’). Since we’re looking from the side, we can only see the y and z axes, but the math works the same for the x axis (just pretend this is a top view instead). This projection is really easy to obtain: notice that the origin, the y-axis, and point (x,y,z) form a right triangle, and a similar right triangle is formed with (x’,y’,z’). Thus the relative proportions are maintained: y′ z′= y z y′ = yz′ z . So to project a 3D coordinate to 2D, we scale a coordinate by the screen distance z’. Since z’ is a fixed constant, and not functionally a coordinate, let’s rename it to K1, so our projection equation becomes (x′,y′)=( K1x z , K1y z ). We can choose K1 arbitrarily based on the field of view we want to show in our 2D window. For example, if we have a 100x100 window of pixels, then the view is centered at (50,50); and if we want to see an object which is 10 units wide in our 3D space, set back 5 units from the viewer, then K1 should be chosen so that the projection of the point x=10, z=5 is still on the screen with x’ 0, the surface is facing the light and if it’s0) { // test against the z-buffer. larger 1/z means the pixel is // closer to the viewer than what's already plotted. if(ooz > zbuffer[xp,yp]) { zbuffer[xp, yp] = ooz; int luminance_index = L*8; // luminance_index is now in the range 0..11 (8*sqrt(2) = 11.3) // now we lookup the character corresponding to the // luminance and plot it in our output: output[xp, yp] = \".,-~:;=!*#$@\"[luminance_index]; } } } } // now, dump output[] to the screen. // bring cursor to \"home\" location, in just about any currently-used // terminal emulation mode printf(\"\\x1b[H\"); for (int j = 0; j < screen_height; j++) { for (int i = 0; i < screen_width; i++) { putchar(output[i,j]); } putchar('\\n'); } } The Javascript source for both the ASCII and canvas rendering is right here. a1k0n.net",
    "commentLink": "https://news.ycombinator.com/item?id=37548599",
    "commentBody": "Donut math: how donut.c worksHacker NewspastloginDonut math: how donut.c works (a1k0n.net) 162 points by mr_o47 18 hours ago| hidepastfavorite12 comments ryandv 2 hours agoReminds me of this classic IOCCC entry implementing a flight simulator, where portions of the source code are formatted to resemble a plane:https:&#x2F;&#x2F;www.ioccc.org&#x2F;1998&#x2F;banks.orig.chttps:&#x2F;&#x2F;www.ioccc.org&#x2F;years.html#1998_banks reply enigmarc 4 hours agoprevInteresting.Tangentially reminds me of my own animated GIF app that I made a while ago.https:&#x2F;&#x2F;www.gifcii.fun reply mkii 4 hours agoparentThis is seriously cool reply Agingcoder 8 hours agoprevThat’s the kind of thing that had me learn coding many years ago ( watching demoscene productions). reply supportengineer 7 hours agoprevWhat platform does this compile on? Didn&#x27;t work for me on a recent model M1 MacBook Pro. reply unwind 6 hours agoparentI got it to compile in Linux on x86-64, with the following tiny changes:- Add #include(for printf())- Add #include(for memset())- Add \"int\" before the initial bare declaration of \"k\"I compiled it with: $ gcc -o donut -std=c89 donut.c -lmWhich includes:- Specifying an old language standard to get rid of the need to say \"int\" before main()- Linking the math library (\"-lm\") for the trig functions.Update: minor grammar and clarification. reply stabbles 5 hours agorootparentWith -std=c89 you shouldn&#x27;t need that `int k`, right?Further, you can add includes on the command line cc -std=c89 donut.c -include stdio.h -include string.h -include math.h -lmThen the source can be left as is. reply physicsguy 7 hours agoparentprevWhat were the errors? reply munchler 9 hours agoprev(2011) reply jheriko 6 hours agoprev [–] reminds me of stuff i based in basic when i was a kidmade me laugh to see \"no raytracing\", then immediate use of the perspective projection which is intersecting points and a plane along a ray lol... reply ggambetta 5 hours agoparentDeriving the perspective projection equations involves intersecting a ray and the projection plane, but this doesn&#x27;t magically turn a rasterizer into a raytracer. reply waveBidder 5 hours agoparentprev [–] it&#x27;s boutique hand-derived ray tracing! very different replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article detailed the renewed interest in the \"donut\" code from 2006, which uses a framebuffer and a Z-buffer to visualize a torus as low-resolution ASCII art without raytracing.",
      "It provided an explanation of the mathematics behind 3D perspective rendering and the method of projecting each point onto a 2D screen.",
      "The piece also features the original code and includes a link to the JavaScript source for ASCII and canvas rendering."
    ],
    "commentSummary": [
      "The article discusses the implementation of donut.c code, comparing it to other creative coding projects.",
      "Instructions are provided on how to compile this code on specific platforms, along with solutions of some compilation issues.",
      "The comments section comprises insightful discussions and shared personal experiences related to the topic."
    ],
    "points": 161,
    "commentCount": 12,
    "retryCount": 0,
    "time": 1694979520
  },
  {
    "id": 37549569,
    "title": "Why Is Booz Allen Renting Us Back Our Own National Parks?",
    "originLink": "https://www.thebignewsletter.com/p/why-is-booz-allen-renting-us-back",
    "originBody": "BIG by Matt Stoller Subscribe Sign in Discover more from BIG by Matt Stoller The history and politics of monopoly power. Over 98,000 subscribers Subscribe Continue reading Sign in Why Is Booz Allen Renting Us Back Our Own National Parks? In 2017, consulting giant Booz Allen cut a deal with the government to extract junk fees from Americans who want to use Federal lands and waters for hunting or fishing. Will Congress or Biden act? MATT STOLLER NOV 29, 2022 170 34 Share Welcome to BIG, a newsletter on the politics of monopoly power. If you’re already signed up, great! If you’d like to sign up and receive issues over email, you can do so here. Happy belated Thanksgiving. I didn’t say this last week, but I truly am grateful for you reading this newsletter and sending me your thoughts and ideas. It’s been a wonderful ride so far, and hopefully we’ll keep at it for a long time. Today I’m writing about how the giant government contracting firm Booz Allen and 13 government agencies have been renting back to the public access to our own lands by forcing us to pay junk fees to use national parks. It involves a tour through late 19th century political economy thinking, with the first appearance of the great anti-monopolist Henry George, whose focus was land. Plus, a weird attempted monopoly of ID management software, controlled by software private equity giant Thoma Bravo. Vermilion Cliffs National Monument in Arizona. Photo courtesy of the Bureau of Land Management. Some rights reserved. “I Seen My Opportunities and I Took ’Em.” - George Washington Plunkitt of Tammany Hall Two of the classic works of late 19th century American political literature, representing precisely opposite views of how commerce in an industrialized democracy ought to work, are Henry George’s Progress and Poverty, and the speeches of George Washington Plunkitt of Tammany Hall. George was one of the great political economists of his day, and he ran and lost for mayor of New York City on an anti-monopoly and land reform ticket. George was interested in why we experienced tremendous inequality in the midst of great wealth, and traced it to the exploitation of land. George was an international superstar, influencing both Teddy Roosevelt and Woodrow Wilson, as well as environmentalists and the modern libertarian movement. (There’s an iconic statue of the greatest mayor in Cleveland history, Tom Johnson, with Johnson holding a copy of Progress and Poverty.) The modern academic profession of economics arose in part as a reaction to the popular success of George’s work. The game Monopoly comes directly from George, and in many ways, the national parks, as well as everything from spectrum allocation to offshore oil drilling, must wrestle with Georgeist thinking. But by land, he meant far more than just the plots upon which we live. “The term land,” George wrote, “necessarily includes, not merely the surface of the earth as distinguished from the water and the air, but the whole material universe outside of man himself, for it is only by having access to land, from which his very body is drawn, that man can come in contact with or use nature.” Unlike Marx, who saw the exploitation of capital over labor, George thought that the root of social disorder was a result of the power of the landowner over both capital and labor. By land, he meant all value drawn purely from nature or from collective human existence. He would, for instance, consider ‘network effects’ a form of land, and likely seek regulation or national control of search engines. George had his first run-in with monopoly in San Francisco, where a telegraph monopolist destroyed his newspaper by denying him wire service. But his key work, in 1879, was written before the rise of the giant trusts, just as railroads, which were really land kingdoms, were becoming dominant. A much more cynical set of works are the speeches of Plunkitt. Plunkitt was a political boss in New York City, a proud machine politician in office at the same time in the same political arena as George. Both men were interested in modern industry and wealth, and in both cases, the key fulcrum around which power flowed was not capital, but land. But while George sought a better world, Plunkitt just wanted to get rich, and saw in the purchase of land one of the key ways to do that. Plunkitt’s key moral guidepost was the practical wielding of political power to enrich oneself. He posited something called “Honest Graft,” which he distinguished from crime in a formulation that every important corporate lobbyist, knowingly or not, has since used. To Plunkitt, stealing would be taking something that doesn’t belong to you. But if you happened to know that the city would need a piece of land, and you got there first, well, that was simply smart. As Plunkitt put it: \"I could get nothin' at a bargain but a big piece of swamp, but I took it fast enough and held on to it. What turned out was just what I counted on. They couldn't make the park complete without Plunkitt's swamp, and they had to pay a good price for it. Anything dishonest in that?\" George was part of the land reform anti-monopoly school of Anglo-American thought, from Frederick Douglass to Thaddeus Stevens. Plunkitt was a machine politician, and proud of it. The battle between these two elements of America, the desire to conserve the public weal versus the desire to cynically plunder it, is still fierce today. It will probably never end. And that brings me to the political conflict over our national parks, and the strange situation whereby a large government contractor, Booz Allen, somehow found itself in a position to rent us back our own land. Red Rock Canyon in Nevada. Every day, visitors to Vermilion Cliffs National Monument in Northern Arizona hike into an area named Coyote Buttes North to see one of the “most visually striking geologic sandstone formations in the world,” which is known as The Wave. On an ancient layer of sandstone, millions of years of water and wind erosion crafted 3,000-foot cliffs, weird red canyons that look like you are on the planet Mars, and giant formations that look like crashing waves made of rock. There are old carvings known as ‘petroglyphs’ on cliff walls, and even “dinosaur tracks embedded in the sediment.” The Wave is unlike anywhere else on Earth. It is also part of a U.S. national park, and thus technically, it’s open to anyone. Yet, to preserve its natural beauty, the Bureau of Land Management lets just 64 people daily visit the area. Snagging one of these slots is an accomplishment, a ticket into The Wave is known as “The Hardest Permit to Get in the USA” by Outside and Backpacker Magazines. To apply requires going to Recreation.gov, the site set up to manage national parks, public cultural landmarks, and public lands, and paying $9 for a “Lottery Application Fee.” If you win, you get a permit, and pay a recreation fee of $7. The success rate for the lottery is between 4-10%, and some people spend upwards of $500 before securing an actual permit. But while the recreation fee of $7 goes to maintaining the park - which is what Henry George would appreciate - the money for the “Lottery Application Fee” is pure Plunkitt. That money goes to the giant D.C. consulting firm, Booz Allen and Company. In fact, since 2017, more and more of America’s public lands - over 4,200 facilities and 113,000 individual sites across the country at last count - have been added to the Recreation.gov database and website run by Booz Allen, which in turn captures various fees that Americans pay to visit their national heritage. BIG is a reader-supported newsletter focused on the politics of monopoly and finance. This is journalism and advocacy that challenges power, so please consider a paid subscription. You can always get lies for free. The truth costs a few bucks, but in the long run it’s much cheaper. You can subscribe by clicking here. Subscribe You can do a lot at Recreation.gov. You can sign up for a pass to cut down a Christmas tree on the Arapaho and Roosevelt National Forests, get permits to fly-fishing, rifle hunting or target practice at thousands of sites, or even secure a tour at the National Archives in Washington, D.C. There are dozens of lotteries to enter for different parks and lands that are hard to access. And all of them come with service fees attached, fees that go directly to Booz Allen, which built Recreation.gov. The deeper you go, the more interesting the gatekeeping. As one angry writer found out after waiting on hold and being transferred multiple times, the answer is that Booz Allen “actually sets the Recreation.gov fees for themselves.” Lately, hundreds of sites have begun requiring the use of the site. A typical example is Red Rock Canyon, which added \"timed entry permit\" in the past two years. Such parks, before adding these new processes, usually do a \"trial\" period followed by a public comment period, and then the fees are approved by a Resource Advisory Council, objects of derision composed of people appointed by the government bureaus. As one person involved in the process told me, these councils are sort of ridiculous. “Agencies fill it with people beholden to them,” he said. “so the council playing committee rubber stamps whatever they send their way, often even if it makes no sense.” The entry permit almost always become permanent. This includes heavily visited lands like Acadia National Park (4 million annual visitors), Arches National Park (1.5 million), Glacier National Park (3 million), Rocky Mountain National Park (4.4 million), and Yosemite (3.3 million). There’s nothing wrong with charging a fee for the use of a national park, as long as that fee is necessary for the upkeep and is used to maintain the public resource. That was in fact the point of the law passed in 2004 - the Federal Lands Recreation Enhancement Act - to give permanent authority to government agencies to charge fees for the use of public lands. But what Booz Allen is doing is different. The incentives are creating the same dynamics for public lands that we see with junk fees across the economy. Just as airlines are charging for carry-on bags and hotels are forcing people to pay ‘resort fees,’ some national parks are now requiring reservations with fees attached. And as scalpers automatically grabbed Taylor Swift tickets from Ticketmaster using high-speed automated programs, there are now bots booking campsites. None of this is criminal, though the fee structure may not be lawful, but it is very George Washington Plunkitt. “I Seen My Opportunities,” he said, “and I Took ’Em.” Honest Graft The entry point for Booz Allen can be traced back to the Obama administration, and a giant failed IT project. In 2010, Congress passed the Affordable Care Act, pledging that by 2014, the government would have a website up in which uninsured Americans could buy health insurance with various subsidies. In perhaps one of the most embarrassing moments of the Obama administration, Healthcare.gov failed to launch the day the new health law came into force, and millions couldn’t sign up to take advantage of it. It’s hard to overstate the shame of that moment. The government had spent $400 million over four years - more time than it took the U.S. to enter and win World War II - and yet, the dozens of contractors couldn’t set up a website to take sign-ups. The whole thing was an embarrassing disaster, a festival of incompetence and greed. (Despite the failure, the main IT contractor’s CEO became a billionaire. Honest graft indeed.) President Obama hired Google’s Mike Dickerson to come in and fix the Healthcare.gov website, which Dickerson and his team did. This wasn’t some miracle, it’s not like websites were new technology. The government itself created the internet and most of the underpinnings of digital technology, and it had many functional and important systems. But the Google name at that point was magic, and so the U.S. Digital Service, designed to help the government use technology, was born. After Dickerson, the new head was Google’s Matt Cutts, and then health care monopolist Optum’s Mina Hsiang. The U.S. Digital Service, far from being particularly competent, is a branding exercise. It is full of people from Amazon and Google, and tends to push the government to outsource its technology to third party contractors. Following the U.S. Digital Service’s playbook is what led the government to bid out and allow the creation of Recreation.gov, with its weird and corrupt fee structure. In 2017, Booz Allen got a 10-year $182 million contract to consolidate all booking for public lands and waters, with 13 separate agencies participating, from the Bureau of Land Management to the National Oceanic & Atmospheric Administration to the National Park Service to the Smithsonian Institution to the Tennessee Valley Authority to the US Forest Service. The funding structure of the site is exactly what George Washington Plunkitt would design. Though there’s a ten year contract with significant financial outlays, Booz Allen says the project was built “at no cost to the federal government.” In the contractor’s words, “the unique contractual agreement is a transaction-based fee model that lets the government and Booz Allen share in risk, reward, results, and impact.” In other words, Booz Allen gets to keep the fees charged to users who want access to national parks. Part of the deal was that Booz Allen would get the right to negotiate fees to third party sites that want access to data on Federal lands. It’s a bit hard to tell how much Booz Allen was paid to set up the site. Documents suggest the firm received a lot of money to do so, but it’s also possible that total amount was the anticipated financial return. I wrote to Recreation.gov team leader Julie McPherson at Booz Allen to find out what they were paid to build the site, and I haven’t heard back. Regardless, there’s a lot of money involved. For instance, as one camper noted, in just one lottery to hike Mount Whitney, more than 16,000 people applied, and only a third got in. Yet everyone paid the $6 registration fee, which means the gross income for that single location is over $100,000. There’s nothing criminal about this scheme, but it is a form of Honest Graft, or of handing a Ticketmaster-like firm control of our national parks. Judgment Day In 2020, an avid hiker named Thomas Kotab sued the Bureau of Land Management over the $2 “processing fee it charges to access the mandatory online reservation system to visit the Red Rock Canyon Conservation Area.” He claimed, among other things, that the Federal Lands Recreation Enhancement Act mandated that this fee was unlawful, because it had not gone through the notice-and-comment period required by the act. Kotab, an electrical engineer by training, is one of those ass-kickers in America, who just goes after a grift because, well, it’s just wrong. A few years later, a judge named Jennifer A. Dorsey, appointed by Obama in 2013, agreed with him. She looked at the statute and found that Congress authorized the charging of recreation fees for the purpose of taking care and using Federal lands, not administrative fees that compensated third parties. As such, Booz Allen’s ability to set its own prices was inconsistent with the law mandating the public’s right to comment on what we are charged for using our own land. The BLM sought to appeal, but then dropped it in July. Rather than a bitter procedural argument about classifying fees, the government and Booz Allen have decided they’ll just go through the annoying process of having the public comment on Booz Allen’s compensation, and then ignore us using their phony advisory council process. Here, for instance, is the Mojave-Southern Great Basin Resource Advisory Council Meeting in August simply proposing to substitute new standard amenity fees “equal to the associated Recreation.gov reservation service fee.” One notable part of this saga is that technically, the BLM and Booz Allen owe refunds to everyone who went through Red Rock Canyon’s timed entry system from 2020-2022, but they’ll probably ignore that and steal the money. That verges into actual graft from the ‘honest’ type, but I suspect Plunkitt did that as well from time to time. And yet, it’s not over. The Federal Lands Recreation Enhancement Act authorization runs out in October of 2023, which means that Congress has to renew it. Hopefully, an interested member of Congress who loves Federal lands could actually tighten the definitions here, and find a way to stop Booz Allen and these 13 government agencies from engaging in this minor theft via junk fees. It wouldn’t be hard, and it would be fun to force a bunch of government agencies to actually do their job and either take over the site themselves or pay Booz Allen a fee for its service. (Another path would be Joe Biden, through his anti-junk fee initiative, simply asserting through the White House Competition Council to the 13 different agencies that they end Booz Allen’s practice of charging these kinds of fees.) It’s easy enough to see scams everywhere, and here is certainly one of them. But let’s not lose sight of the broader point. Henry George, at least in this fight, has won. Yes, Booz Allen gets to steal some pennies, but we have a remarkable system of public lands and waters that are broadly available for all of us to use on a relatively equal basis. And we can still see the power of George-ism in the advocacy of hikers and in the intense view that members of Congress had when they passed the Federal Lands Recreation Enhancement Act in 2004, which strictly regulated fees that Americans would have to pay to access our Federal lands. Indeed, the anger and revulsion I felt at the fees Booz Allen puts forward comes from George, even if I didn’t necessarily trace it there at first. We are in a moment of institutional corruption, but these moments are transitory as institutions change. George Washington Plunkitt, and his political descendants at Booz Allen, might have gotten rich, but Henry George imparted instincts to Americans that are far more permanent. Weird Monopoly: Identity and Access Management Software Last week, I published a piece on Thoma Bravo, the private equity bridge trolls of enterprise software, and how they are trying to roll up the corporate software that large firms use to manage the identity and logins of customers and workers across their many apps and devices. The PE firm, which was responsible for the SolarWinds catastrophic hack in 2020, recently bought both Ping Identity and ForgeRock, which are two of the three firms that sell this product to very large security conscious multi-nationals. This is a worrisome roll-up, because you don’t want to have important software in the hands of a private equity firm that has a dedicated track record of cyber-security screw-ups. And yes, Thoma Bravo is that bad. One researcher had previously alerted the company that “anyone could access SolarWinds’ update server by using the password “solarwinds123.’” Ouch. You can read the full article here. What I’m Reading Welcome to the Jungle, Baron Public Affairs. On this podcast from a D.C.-intelligence firm, the hosts note that Amazon has hired roughly 3,000 former government officials to staff its various divisions. That’s roughly ten times what they have found at any other large firm, excepting those whose sole focus is government contracting. Pfizer Hiking Covid Vaccine Prices Against Weaker Demand Speaks Volumes About U.S. Healthcare, Forbes Kroger, Albertsons Expected to Defend Grocery Merger at Senate Hearing, Wall Street Journal Apple to Lose 6 Million iPhone Pros From Tumult at China Plant, Bloomberg. Maybe sourcing all production in China isn’t a good idea for a critical electronic and communications device. Just thinking aloud. Collateralised fund obligations: how private equity securitised itself, Financial Times. What could go wrong? Elon Musk’s Boring Company Ghosts Cities Across America, Wall Street Journal Populist House Republicans Picking a Fight With US Business Over ‘Woke Capitalism, Bloomberg Thanks for reading! And please send me tips on weird monopolies, stories I’ve missed, or comments by clicking on the title of this newsletter. And if you liked this issue of BIG, you can sign up here for more issues, a newsletter on how to restore fair commerce, innovation and democracy. And consider becoming a paying subscriber to support this work, or if you are a paying subscriber, giving a gift subscription to a friend, colleague, or family member. cheers, Matt Stoller Subscribe to BIG by Matt Stoller Thousands of paid subscribers The history and politics of monopoly power. Subscribe 170 Likes · 1 Restack 170 34 Share 34 Comments Michael Kelly Nov 29, 2022 Liked by Matt Stoller I use Red Rock National Conservation NV area 2-3 days a week, and I attended both public commentary meetings this summer(live and online). These meetings were advertised as being about adding a gate to Red Springs(part of Red Rock) to restrict access and collect more fees. In the course of the meeting, I learned that in the eyes of the BLM this meeting was also serving as some kind of retroactive \"commentary period\" for the reservation system/fee. That fee has now been reinstated. Of all those that attended and commented 100% were opposed to the fee primarily because $0 of the $2 reservation fee goes to the park. In 2021 the NPS says they received 297 million visits. I don't know how many had to pay this reservation fee but this $2 fee alone is hundreds of millions and doesn't include all the other fees(camping, lotteries....). So over the 10 life of his contract, Allen stands to easily collect over a Billion! I don't bother making a \"reservation\" when I go to Red Rocks midweek but I still have to drive through the gate and pay the \"reservation fee\"! So calling it a reservation fee is a lie! It is an access fee because you must pay it every time you enter Red Rock Scenic loop(and in a few years Red Springs) The Access Fund has filed an appeal trying to fight the addition of gating off the Red Springs area. So not only is Allen renting us back our land he is expanding the areas he can rent. I wonder if this is happening in other parks across the country? Thanks, and please keep covering this story. LIKE (14) REPLY SHARE Melissa Nov 29, 2022 Liked by Matt Stoller Biden administration announced recently they are spearheading the elimination of junk fees on everything from bank overdrafts to restaurant service fees. Ironically forgetting to look first in their own back yard! Thanks for heads up on this Matt! Henry George lives on..spread the word LIKE (10) REPLY SHARE 32 more comments... Top New Community The Coming Collapse of a Cheerleading Monopolist Antitrust lawyers filed a class action lawsuit against private equity-owned Varsity Brands, the organizing force behind competitive cheerleading. MAY 27, 2020 • MATT STOLLER 122 211 WeWork and Counterfeit Capitalism Hi, Welcome to BIG, a newsletter about the politics of monopoly. If you’d like to sign up, you can do so here. Or just read on… I’m back! One of the… SEP 25, 2019 • MATT STOLLER 355 3 Why Logitech Just Killed the Universal Remote Control Industry Monopolies are lazy. Logitech bought, monopolized, and killed the universal remote control business. APR 10, 2021 • MATT STOLLER 29 55 See all Ready for more? Subscribe © 2023 Matt Stoller Privacy ∙ Terms ∙ Collection notice Start Writing Get the app Substack is the home for great writing",
    "commentLink": "https://news.ycombinator.com/item?id=37549569",
    "commentBody": "Why Is Booz Allen Renting Us Back Our Own National Parks?Hacker NewspastloginWhy Is Booz Allen Renting Us Back Our Own National Parks? (thebignewsletter.com) 160 points by blueridge 16 hours ago| hidepastfavorite54 comments zenbob 12 hours agoAs someone who frequently books campgrounds at the county, state, and national level, Recreation.gov provides by far the best user experience for browsing sites on a map and viewing calendar availability that I&#x27;ve encountered. Its fees don&#x27;t seem noticeably higher than fees I encounter on other government booking systems. Maybe I&#x27;m being naive, but it doesn&#x27;t seem too important to me that Booz Allen is collecting some of its payment as a part of booking fees rather than directly from government contracts. A government website that works this well seems worthy of praise to me! reply matthewdgreen 12 hours agoparentLook, good websites are great. I’m glad you’re happy with the experience. But building good websites is not some special capability that should entitle you to large fractions of ongoing revenue from customers. Building decent websites in 2023 should be a competitive fee-for-delivery service, even for websites that have specialized functionality and large scale like auctions. Someone should pay you a (potentially large!) fee for building it and an ongoing maintenance&#x2F;support fee. Nobody should be entitled to collect a huge rent amounting to any substantial fraction of all national park recreational fees just because they built some infrastructure, any more than the people who paved the roads leading to the national parks should be allowed to collect tolls on every driver passing through.If your response is that websites are so hard to build that our government can’t spec out its requirements and get it built — and therefore just need to overpay by 10,000x for the privilege of having a decent user experience, then maybe we (technologists) need to go have a long look in the mirror and ask what we’ve done to make things so terrible. reply bombcar 11 hours agorootparentIt&#x27;s much more likely that the government agency in charge of this had $4.99 to build the site, not even enough for Fivver.But they have all the latitude in the world to sign \"portion of the ticket price\" contracts, and so they did.It&#x27;s not Booz&#x27;s fault; it&#x27;s the situation that lead this to (apparently) be the best option for the agency.(This same crowd here wouldn&#x27;t even turn over in their sleep if the same agency had contracted out building the site and ran it themselves on AWS and paid Amazon 3-5x what Booz is getting, mind you.) reply matthewdgreen 11 hours agorootparentIt is perfectly fine to diagnose the pathology that led to this outcome. It is not ok for people to excuse it because “the website works fine for me!” Government agencies need an interface to communicate with taxpayers and “customers” in order to exist. In the 1960s that meant paper mail and customer service agents, and most government agencies were reasonably competent (if sluggish) at handling those technologies. In 2023 and beyond it means smoothly-working websites, and governments have decided to treat these as a weird, expensive mystery technology - long past the point where industry has made website design into something routine. The days when governments could budget $5 to agencies for tech development and&#x2F;or expect them to spend 50x standard industry prices on broken government contractors are long behind us. And there is no room for this kind of predatory outsourcing. We need to demand a lot better. reply freeopinion 11 hours agorootparentprevEven if we see things as charitably as you outline (and well done for that) the contract could be a \"portion of the ticket sales up to $X total.\"I don&#x27;t blame Booz for that (entirely). Somebody else had to sign that contract, too.But if I&#x27;m going to lay into Booz, I have to look in the mirror myself. I have worked for employers that charge as much as they can get away with in a market that wasn&#x27;t exactly fair. When incumbents spend almost unbelievable amounts to build a functioning legislative mote, then exploit that for all it&#x27;s worth, you could call that \"good business.\" And you can rationalize by saying, \"If I don&#x27;t, somebody else will, so it might as well be me who benefits.\" But the excuses seem pretty flimsy when historians catalog the damages.Still, the biggest blame goes to the other signature on the contract. reply bloppe 11 hours agorootparentprevI agree it&#x27;s \"not Booz&#x27;s fault\". That&#x27;s the point of honest graft. It&#x27;s the fault of our government for allowing them to have so much control over these junk fees.Still a problem. reply genericresponse 11 hours agorootparentprevI would argue that the cost is less in setting up than in running and maintaining a site at the scale of Recreation.gov. That includes aspects of customer support.Many of these agencies are forced into uncompetitive compensation structures which means contracting out most, if not all, of their technical work.A major part of the issue is the government contracting system. In an attempt at fairness, it has massive amounts of oversight burden. That is, in turn, a barrier to additional competition for the work. reply mindslight 9 hours agorootparentprev> building good websites is not some special capability that should entitle you to large fractions of ongoing revenue from customersDangerous words on a forum that&#x27;s focused on the nouveau middlemen of Surveillance Valley. reply bloppe 10 hours agoparentprev\"by far the best... that I&#x27;ve ever encountered\" is a terrible argument in support of a monopoly. It&#x27;s legally impossible to have something better, so of course it&#x27;s the best we know of.&#x27;Exactly how much Booz Allen reaps from Recreation.gov is not publicly known, nor is how much it costs to operate Recreation.gov on an annual basis. The lawsuit contends that Recreation.gov handled 9 million transactions in 2021 and generates “tens, if not hundreds, of millions of dollars of revenue every year for Booz Allen, constituting a complete windfall.”&#x27; [1]I&#x27;d be surprised if that revenue were below 100 million. A team of a couple dozen competent developers could certainly operate it for a fraction of that.[1]: https:&#x2F;&#x2F;www.nationalparkstraveler.org&#x2F;2023&#x2F;02&#x2F;update-lawsuit... reply kerkeslager 11 hours agoparentprev> Its fees don&#x27;t seem noticeably higher than fees I encounter on other government booking systems.So what&#x27;s the acceptable upper limit of rent we should be charged to use a website that&#x27;s already built to use lands our taxes already paid to buy to a company who provides little-to-no ongoing service to us? reply freeopinion 11 hours agorootparentIt&#x27;s easy to get emotional about this. But when I try to be fair, I acknowledge that \"little-to-no ongoing service\" is probably not completely accurate.There are storage systems, databases, app servers, etc. that all need some degree of constant care. I don&#x27;t know what the scale or structure of those systems are, so it is difficult to say what that cost should be. Perhaps it qualifies as \"little\". It almost certainly does not qualify as \"none\".Of course, the hardware and the labor might already be included in the taxes I pay. I don&#x27;t know that either. I wonder how much work it would take to sort out all the different sources of revenue Booz has from this. reply thorncorona 10 hours agorootparent> There are storage systems, databases, app servers, etc. that all need some degree of constant care. I don&#x27;t know what the scale or structure of those systems are, so it is difficult to say what that cost should be. Perhaps it qualifies as \"little\". It almost certainly does not qualify as \"none\".You could have this entire setup managed by AWS &#x2F; GCP forYou can still do it in person easily.Are you sure? I was in the USA recently, and the Dunewood Campground (Indiana Dunes National Park) had signs explicitly stating that there was no in-person booking, but that recreation.gov was the only way to book sites.Later I looked at Lake Erie State Park (New York), and they said that reserveamerica.com (or RA’s call centre) was the only way to book, no walk-ins permitted, and then the quoted $15 fee for their cheapest sites ballooned to something like $27.60 due to a reservation fee, an out-of-state-person fee, maybe some taxes… reply Scoundreller 12 hours agoparentprev> setup a non-for-profit to manage this stuff if that&#x27;s your beef.Is there some benevolent non-profit&#x2F;cooperative out there that would do these kinds of IT projects? Kinda like a credit union or mutual insurance company? reply PostOnce 12 hours agorootparentOr the government could do it at-cost in-house instead of paying a margin to BAH.I&#x27;m sure we can argue the 100 reasons why we can&#x27;t&#x2F;shouldn&#x27;t do that, but it is a possible option, even if it&#x27;s claimed to be a terrible one. reply grogenaut 12 hours agorootparentprevno idea... doubt it. money probbably instead goes to building enclaves around park city or making it easy to go to burning man reply jacoblambda 12 hours agoparentprevI really don&#x27;t understand why the GSA can&#x27;t do this (or CISA since they seem to be taking over some of the GSA&#x27;s responsibilities)?A reservation system seems like the kind of thing that&#x27;d show up often enough at various federal, state, and local levels for there to be enough value in the GSA just building their own, providing integrations into their US Web Design System so that it&#x27;s easy for orgs to add, and offering it at cost to the orgs or facilities that need it. reply grogenaut 12 hours agorootparentDoes the GSA code anything? does the Govt? For all this stuff isn&#x27;t it all contract? What I&#x27;d hope for in a situation like this is that it&#x27;s a term contract and the code stays with the govt but really do you want to take over someone else&#x27;s code? reply plasma_beam 12 hours agorootparentThey do..login.gov is probably the best current example and is ever expanding usage throughout federal sites. But GSA has also been under the microscope in the past couple years. Login.gov has received a lot of flak. They’re unlikely to take on another big “build it and they will come” project like a reusable reservation system. They make money off other agencies and those agreements need to be signed before the work starts..Now all that said..contractors work on login.gov too and are prob the majority of the staff now! reply ls612 11 hours agorootparentWhat’s wrong with login.gov? I only used it to handle my Global Entry application process but it sure felt like a good SSO solution. reply grogenaut 10 hours agorootparentprevlogin.gov is a lot better than id.me that&#x27;s for sure. didn&#x27;t know gsa made it. Can they help the IRS not use id.me and receive files over any mechanism but mail and fax? replyShrigmaMale 12 hours agoprev [–] this sounds like someone who is kvetching on principle because he resents private companies being involved with public services, not a legitimate complaint.$6 isn&#x27;t a lot relative to the cost of even driving to a national park for most people. comparing it to a $50 bag check or $100 in ticketmaster crap is disingenuous in the extreme.recreation.gov is one of the very few dot gov&#x27;s that doesn&#x27;t make me want to put my hand through the nearest wall, precisely because it wasn&#x27;t built like healthcare.gov.the site services 23 million active users and has to handle a lot of complexity around managing limits on how many people can visit.the author here is just straight-up wrong; consult this article for accurate information: https:&#x2F;&#x2F;www.outsideonline.com&#x2F;outdoor-adventure&#x2F;hiking-and-b...\"Here’s how it all works at the field level. A manager at a National Park, Forest Service, BLM office, or the like decides that a user fee must be paid or that a site or trip is so popular that a paid permit or reservation is required to protect the resource and the experience. The consumer of those services—we the people—log into rec.gov and pay the designated fee or pay into a lottery to try to win a permit. That money goes to the Treasury Department. From that account, Recreation.gov pays Booz Allen for its work based off each transaction as agreed upon in the original contract, and almost all of the rest goes back to the individual agency. Over the past four years, says Delappe, 85 percent of what is charged goes back to the agencies. Recreation One Stop does not set those fees, nor does Booz Allen. But Booz Allen’s original contract did include specific fees for various transactions. It’s the managers in the field that set the prices. They of course try to cover their costs and the transaction costs for Recreation.gov.\"TLDR: people be sayin shit. in this case \"people\" is some rando on substack and \"shit\" is \"muh private company bad\" reply dsauerbrun 12 hours agoparentyou&#x27;re looking at this from a very narrow perspective in terms of use case... one of my favorite climbing areas near me(shelf road) became a reservation through rec.gov only campground. I used to show up on Saturday morning, pay $7 for one night to camp, and leave Sunday afternoon after my 2nd day of climbing.as of 2 years ago rec.gov took over and kept the same $7&#x2F;night rate but added an $8 service fee.it&#x27;s complete bullshit for those of us who regularly use government recreational services. reply georgeg23 12 hours agoparentprevThe paid lottery in particular is problematic given you don&#x27;t get refunded if you lose. reply kerkeslager 11 hours agoparentprev> $6 isn&#x27;t a lot relative to the cost of even driving to a national park for most people. comparing it to a $50 bag check or $100 in ticketmaster crap is disingenuous in the extreme.True: in one case we&#x27;re overpaying for a service, in the other case, we&#x27;re paying rent to someone else for something we own.What exactly is the amount of rent to use land that we own would be unacceptable to you?Is there ever a case where Hacker News will argue in favor of humans instead of corporations? reply AnotherGoodName 12 hours agoparentprev [–] From the article BA is locked in till 2027 and fees are now over 120%.How are such long term and open contracts given to private companies a great example of capitalism? replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article scrutinizes consulting firm Booz Allen's practice of charging fees via the Recreation.gov website for access to U.S. federal lands and waters, causing concerns about control and profit from national parks.",
      "A lawsuit challenging the fees for visiting Red Rock Canyon has triggered a public comment process to decide on the contractor's remuneration.",
      "The author recommends more stringent regulations to avoid such practices, emphasizing the value of public lands and the influence of Henry George's philosophies."
    ],
    "commentSummary": [
      "The article examines disputes about Recreation.gov's handling of national parks' rentals, where operator Booz Allen Hamilton is accused of price gouging and exploiting a monopoly.",
      "Amid calls for transparency, critics urge more competition in governmental website development and a clear disclosure of revenue and operational costs.",
      "The piece also tackles the controversial online booking fees for campsites, with some suggesting a non-profit entity or government institution should manage the reservation system, while others question the private companies' imposed fees and contracts."
    ],
    "points": 160,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1694986436
  },
  {
    "id": 37549259,
    "title": "Hush – Noiseless Browsing for Safari",
    "originLink": "https://oblador.github.io/hush/",
    "originBody": "Noiseless Browsing by Joel Arvidsson Block nags to accept cookies and privacy  invasive tracking in Safari on Mac, iPhone and iPad. “I’d recommend Hush to anyone who uses Safari” — John Gruber, Daring Fireball Sponsor Star Follow This website is using cookies, read more in our privacy policy that is too long and complicated for most to comprehend. We care about your privacy Not really though. That's why we made the process of opting out really difficult because we know nobody would opt in otherwise. Install spyware Other options www.everywebsite.ever wants to annoy even after you leave by sending push notifications. Allow Block Sign up for our newsletter You seem like the type that what to have spam sent to you on a regular basis, so we'll make sure to sell your address on to others. Send spam Private Unlike some blockers, Hush has absolutely no access to your browser habits or passwords. Nor does it track behavior or collect crash reports - nothing leaves your device. Free Everything is free of charge. Forever. No in-app purchases, no nonsense. However, any help towards covering the yearly Apple Developer fee is greatly appreciated. Fast The app is primarily a host of rules that integrates with Safari in a native, lightweight way, making the blocking efficient and fast. Simple It's as easy as downloading the app and enabling it in Safari settings ⭢ Content Blockers. No configuration or maintenance needed. Open Source The source code is available on GitHub under the permissive MIT license. Modern Hush is written in Apple's latest programming paradigm Swift UI and has native support for M1 processors. Tiny The app download clocks in at less than half a megabyte. The Apple logo, Safari, Swift, App Store, iPhone, iPad and Mac are trademarks of Apple, Inc. GitHub and the Octocat are trademarks of GitHub, Inc.",
    "commentLink": "https://news.ycombinator.com/item?id=37549259",
    "commentBody": "Hush – Noiseless Browsing for SafariHacker NewspastloginHush – Noiseless Browsing for Safari (oblador.github.io) 148 points by sysadm1n 17 hours ago| hidepastfavorite61 comments Terretta 11 hours agoHush is one of the extensions on my devices, but both Hush and AdGuard Pro take a back seat to:A) 1Blocker: https:&#x2F;&#x2F;1blocker.com&#x2F;B) Orion Browser by Kagi: https:&#x2F;&#x2F;browser.kagi.com&#x2F;The first brings all the blocking customisation one needs to MacOS and iOS Safari, the other runs Firefox and Chrome extensions such as uBlock Origin:https:&#x2F;&#x2F;browser.kagi.com&#x2F;faq.html#extensions reply AnonC 8 hours agoparentCan you elaborate on why AdGuard Pro took a backseat to 1Blocker? Speaking of the free version, it seems like AdGuard provides more. The subscription also seems a lot cheaper than 1Blocker.I have tried Orion browser, and was thrilled to finally have uBlock Origin on iOS. reply Crestwave 4 hours agorootparentNote that uBlock Origin is not supported[1] on the iOS version of Orion. Rather, the ad blocking you might have attributed to it is actually from the built-in ad blocker[2], which is quite good but unfortunately not as sophisticated as an actual functioning uBO.[1] https:&#x2F;&#x2F;orionfeedback.org&#x2F;d&#x2F;1037-ublock-origin-settings-blan...[2] Try browsing with the content blocker disabled vs with uBO disabled :) reply beretguy 3 hours agoprevAnother good one is StopTheScript:https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;stopthescript&#x2F;id1588394487It disables JavaScript per website, just like “Disable JavaScript” extension in Firefox. It’s perfect for news websites where you just need to read some text. reply timenova 14 hours agoprevHush has changed the desktop browsing experience completely for me. I have rarely seen cookie banners in years.I realize how big a difference it makes when I open a website on Firefox Focus on my Android phone and half the screen is just filled with the cookie accept&#x2F;reject dialog box. reply MaxikCZ 6 hours agoparentyea I have this feeling anything I have to use a computer that isnt mine. I just dont understand how people can put up with it. The worst is people are so resistant to change (because every change is shit), they fear when I speak \"wait I install extension and you wont see any ads\" and see \"another change that will turn to shit, I already learned to ignore the ads, I dont need no adblocking, and isnt it stealing anyways?\" kinda thinking. Its sad reply fatfox 7 hours agoprevWipr is a quiet, lightweight alternative too: https:&#x2F;&#x2F;kaylees.site&#x2F;wipr.html reply PossiblyKyle 5 hours agoparentWipr is great, but they&#x27;re a separate thing. Wipr does not remove the nagging cookie requests reply keybits 3 hours agorootparentWipr does remove cookie warnings. From the linked page:> Wipr blocks all ads, trackers, cryptocurrency miners, EU cookie and GDPR notices...I use Wipr and can confirm that I rarely see cookie warnings in Safari with it enabled.I might be wrong, but I believe this is better than Hush as Wipr blocks cookies, while Hush accepts website defaults. reply odysseus 13 hours agoprevI like Hush for Safari, but I also use Brave. Brave blocks cookie banners by default:https:&#x2F;&#x2F;brave.com&#x2F;privacy-updates&#x2F;21-blocking-cookie-notices... reply the_other 2 hours agoprevDoes it automatically mark every option as “don’t track me or store my data plzthnx”? I couldn’t work out from the website if it’s doing that, or just removing banners. reply mrweasel 2 hours agoparentTechnically simply removing the banner SHOULD be the same as \"don&#x27;t track me\" as you&#x27;ve consented to nothing... in the real world you&#x27;re going to get tracked.Honestly I&#x27;m not convinced that saying you don&#x27;t agree to be tracked does a whole lot anyway way. These \"We care about your privacy, so let us share your data with 617 partners\" popups are generally all pretty questionable. reply mcintyre1994 6 hours agoprevThis sounds awesome! But I installed it, enabled in extensions, went to instagram in safari and got their massive cookie thing. The app says Hush is enabled, is there anything else I need to do to make it work? reply imp0cat 6 hours agoparenthttps:&#x2F;&#x2F;github.com&#x2F;oblador&#x2F;hush#why-does-website-x-display-n... reply mcintyre1994 5 hours agorootparentAh thanks! Looks like that extremely obnoxious one can’t be blocked: https:&#x2F;&#x2F;github.com&#x2F;oblador&#x2F;hush&#x2F;issues&#x2F;54 reply asynchronous 15 hours agoprevCookie banners are so annoying. I support any extension to remove them. reply mrtksn 15 hours agoparentThey are probably designed to be like that so you can rush into accepting the tracking across the web.The web has become a sad place where most of the content is made for showing you ads and tracking you around, therefore tracking cookies are needed, therefore tracking cookie permission banners are displayed. reply posix86 13 hours agorootparentIf you don&#x27;t click accept on the very first page, websites can&#x27;t connect you to where you&#x27;re coming from, which is why they&#x27;re like that.I mean, in theory they could do that retroactively, but that would be waaay to complex.Popular cookie banners enable services by executing their javascript. There&#x27;s no event tracking mechanism whatsoever. Cringe fact about cookie banners.It&#x27;s probably because every engineer spends as little time as possible on it, since it&#x27;s boring af. It&#x27;s annoying & boring to implement, and once it&#x27;s done, annoying to everyone that sees it. reply rpastuszak 5 hours agorootparentprev> They are probably designed to be like that so you can rush into accepting the tracking across the web.Correct (speaking as someone who worked in an adjacent area). reply frizlab 15 hours agoparentprevThe DuckDuckGo browser automatically manages (auto-refuses) these banners. It also has a lot of privacy-preserving features and a distraction-free youtube player that’s nice. And most of all it’s WebKit based, which is good for engine diversity, I guess. reply stranded22 15 hours agorootparentOn iOS, the only engine available is WebKit.On Android, it uses blink (Chromium) reply bananapub 15 hours agoparentprevthey&#x27;re designed to be annoying, a perfectly reasonable response to GDPR was to stop being fucking creepy reply rpbiwer2 10 hours agorootparentIs not being creepy sufficient? My understanding is that you need consent to collect literally any PII, e.g. an email address. reply Propelloni 7 hours agorootparentNo, that&#x27;s not correct. The GDPR is a surprisingly sensible set of rules, e.g. it allows collection and storage of data under certain circumstances. The salient point here is probably that it is allowed to collect and store all data required to fullfil a contractual obligation, e.g your home address, or if you are shopping at a pharmacy your prescriptions. The important part is not what type of data* is collected, but that the collector is restricted to use that is required to fullfil the obligations. If you want to use it for something different (say direct marketing) you have to ask for permission.This extends to many areas, including e-mail, if they are required to deliver your services you may just save them. However, you may not use the e-mail to send newsletters. Of course, you want to double opt-in e-mails in any case unless you don&#x27;t mind false or malicious entries and being labeled as a spammer. But that has nothing to do with the GDPR.* the type of data is of importance when we are talking about data breaches and fines. Losing e-mail addresses is bad, losing prescriptions is much worse. reply rcxdude 4 hours agorootparentprevNo. Consent is only needed for data you collect which is unnecessary to provide the service you are offering and is unexpected by the user. If you require an email address for a mailing list or notifications, you do not need consent. If you have webserver logs containing IP addressees you use for debugging and abuse prevention, you do not need consent (though you probably want to not hang onto them for longer than necessary for those purposes). Same with names and addresses for billing and shipping, etc. If you collect data for analytics or targeted advertising, you need consent (which means rejecting that option needs to be the default and at least as easy as accepting in the dialog, something which many of these dialogues fail at. If it takes more clicks to close the dialog without &#x27;accepting&#x27;, that is not GDPR compliant in the view of most regulators). reply calini 2 hours agoprevFinally, I&#x27;ve been looking to find something like this for a while now! reply _giorgio_ 4 hours agoprevon firefox, this is very good:I don&#x27;t care about cookieshttps:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;i-dont-care-a...or this version (not tried):https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;istilldontcar... reply 28304283409234 3 hours agoparentSwitch to the second, as the first has been bought by commercial entity Avast: https:&#x2F;&#x2F;www.theregister.com&#x2F;2022&#x2F;09&#x2F;21&#x2F;avast_buys_i_dont_car... reply nicoty 9 hours agoprevThere&#x27;s also [Consent-O-Matic](https:&#x2F;&#x2F;consentomatic.au.dk&#x2F;) for other browers. reply gtowey 15 hours agoprevGDPR in Europe is what started it all. The law says that you must get explicit user consent for all kinds of data tracking.However it doesn&#x27;t say how that should happen.The annoying banners are completely a decision the industry has made. It&#x27;s so weird to me that pretty much everyone adopted a near identical way to implement this. It also feels like retaliation -- a way to punish users by making these banners as disruptive as possible, and then blame it on regulations. It&#x27;s like companies would rather destroy the web than give in and actually make it better. reply JKCalhoun 12 hours agoparent> It&#x27;s like companies would rather destroy the webWeirdly they&#x27;re only destroying their own sites though. reply 28304283409234 3 hours agoparentprevThat&#x27;s victim blaming. The problem started with massive surveillance by tech companies. reply thiht 7 hours agoparentprev> GDPR in Europe is what started it allCookie banners are a result of the ePrivacy directive, not GDPR.Also we could have had something better than cookie banners if browser vendors and companies had collaborated on a standard. Imagine if cookie banners were implemented like the permission system, viatags or headers, we could get:- automatic blocking by the browser of cookies not declared explicitly- ability for websites to mark technical cookies (these are allowed with no consent)- a global settings view in the browser of all the cookies allowed or denied- and a uniform implementation everywhereInstead we ended up with this shit. It’s definitely not the EU’s fault, the directive doesn’t say to use crappy banners. reply doublepg23 12 hours agoparentprevWhy does the official EU site use them then? https:&#x2F;&#x2F;european-union.europa.eu&#x2F;index_en reply xctr94 15 hours agoparentprevIt’s also a remarkably stupid implementation. If you follow the rules [1] in a strict way, the website should be usable if the user denied consent to non-essential cookie usage. You don’t need consent for strictly necessary cookies.So a bunch of websites don’t actually need a cookie banner, as long as they’re not using non-essential cookies. You could easily get consent for marketing&#x2F;tracking cookies in an unobtrusive way.Instead, a lot of websites must have seen a drop in conversion because no one wants to interact with these awful cookie popups that have a hundred toggles.AFAIK the page-blocking cookie banners don’t just convert worse, they’re actually not permitted (you can’t block the experience to force consent).On my end: unless your website is very important to me, I’ll just refuse the cookies or hit the back button and load the next search result.1: https:&#x2F;&#x2F;gdpr.eu&#x2F;cookies&#x2F; reply corbezzoli 13 hours agorootparent> you can’t block the experienceThat doesn’t sound right. I think interstitial are permitted by the law, I don’t think it actually says anything about “the experience.”> to force consentThe vast majority of website will work after clicking No, so that’s not forced. Only rarely I see “accept cookie or subscribe” banners.I’m not even sure that this is not allowed either; user is technically forced to accept either because the “close tab” button is always available. reply egypturnash 14 hours agoparentprevMost of the \"turn off a zillion different switches\" banners say something like \"Powered by OneTrust\". There&#x27;s like 3-4 names I&#x27;ve seen on them, that&#x27;s just the only one I remember.There is now an entire sector of the adtech industry devoted to making a GDPR-compliance solution that is carefully designed to make it just frustrating enough that most people hit \"sure gimme all the cookies, whatever, I just wanna read this stupid link\". Someone was paid well to build these despite any moral qualms they may have had about making the entire Web a little bit shittier. reply mrweasel 2 hours agorootparentExactly, these all look and feel the same, because they are all a third-party \"add-on\" that websites simply load in via a JavaScript snippet. There a handful of providers, many of which got their start with the original cookie banners.I worked with one of these original cookie banners and the company behind it was absolute shit and understood nothing about how the internet works. You&#x27;d slap their JavaScript snippet on your site, they&#x27;d then \"scan\" your site and figure out which cookies appeared and what they did and display that to your customers, so you&#x27;d be complaint, technically. Except they didn&#x27;t recognize half the cookies (and funnily enough still don&#x27;t 10+ years later), they didn&#x27;t understand that you don&#x27;t need to display the session cookie, if it&#x27;s just used to keep track of your basket or if you&#x27;re authenticate, and they didn&#x27;t scan behind authenticated pages.These same bozos just extended their business when GDPR was enacted. They still understand nothing, they still do not care about why the law was created. All they do is provide a service that companies can buy and then continue to not change their business practices. This what these popup and banners are for, their are designed to avoid having to change an industry that has zero interest in your privacy.It&#x27;s actually remarkable that websites will accept a shitty user experience, if it means that they can track whatever percentage click accept. reply lnxg33k1 13 hours agoparentprevLike DNT didn’t exist, and idiots take it to EU instead of companies reply charcircuit 11 hours agorootparentThe problem with DNT was that it was a global setting as opposed to one that was per website. reply Bu9818 9 hours agorootparentSurely, browsers could create a setting to toggle DNT per hostname? reply adastra22 9 hours agorootparentprevHow is that a problem? reply charcircuit 6 hours agorootparentBecause sites want users to judge them independently. Some users want to avoid shady sites tracking them, but may allow reputable businesses to track them. Reputable businesses being taken out as collateral from a global DNT is not good for them. reply agos 6 hours agorootparentin reality any user which is savvy enough to turn on DNT knows that all tracking is shady replytikkun 15 hours agoparentprevWhat are some better ways to do it? reply JKCalhoun 12 hours agorootparentDon&#x27;t track users?Magazines, tvs never did. Advertisers seemed to survive somehow. reply mrweasel 2 hours agorootparentI would love to see an A&#x2F;B test that shows any significant difference in revenue if you just go whole hog on tracking, vs. tracking nothing for a campaign on any given product. reply lnxg33k1 13 hours agorootparentprevRespect DNT header reply IggleSniggle 13 hours agorootparentWithout a law to enforce that, it&#x27;s a race to the bottom as the more effective monetization is to use DNT as a signal for improved tracking. reply lnxg33k1 12 hours agorootparentMaybe its a good time to reshare this to check own trackability https:&#x2F;&#x2F;coveryourtracks.eff.org&#x2F; replypluijzer 6 hours agoprev\"Block nags to accept cookies and privacy invasive tracking\"Took me a couple of tries to parse the senetence. Is this blocking or accepting privacy invasive tracking? Guess the former but kept reading it as the latter. reply ricklamers 5 hours agoparentI found https:&#x2F;&#x2F;github.com&#x2F;oblador&#x2F;hush#does-hush-accept-or-deny-per...So neither it seems. Whatever the website does when a user doesn’t make a choice. reply pluijzer 4 hours agorootparentThat is nice. That is why I don&#x27;t use &#x27;I don&#x27;t care about cookies&#x27;. I do care, I want to press reject, if for nothing else than to send a signal. reply pluijzer 1 hour agorootparentHonestly interested why this was downvoted, which yes makes for boring reading but I hope it results in some interesting argument. If you need a cookie banner it means you are collecting users&#x27; personal information. For many websites this is totally unnecessary. I do not like it if a website tells me how important my privacy is to them and then asks me to accept sharing all the information they can about me with other commercial entities. For no functional reason. I don&#x27;t want to press accept that. As a insignificant little protest I want press reject. I want their statistics to show that some people don&#x27;t like data being shared.\"I do not care about cookies\", when it cannot just hide a popup will accept the terms and removes the option for this small protest from me. That is why I do not use it. Is this somehow wrong, offensive, off-topic? reply meatjuice 13 hours agoprevEverytime I see these kind of software, I wonder if it&#x27;s really safe as it could be modified before publishing even if it&#x27;s opensource. reply mantra2 10 hours agoparentHush has been around for quite a while and is open, at least. reply Angostura 4 hours agoprevAll this is doing is effectively accepting the website defaults on the hope that those defaults follow the GDPR rules. No thanks. reply nicoty 9 hours agoprevThere&#x27;s also Consent-O-Matic https:&#x2F;&#x2F;consentomatic.au.dk&#x2F; for other browsers&#x2F;platforms. reply menshiki 11 hours agoprevI&#x27;ve been using Hush literally since it came out. Love it. reply happybuy 14 hours agoprevHush is a great tool to manage annoyance blocking and is also free and open source which is nice.I&#x27;ve developed a full Safari ad blocking app which includes similar features (amongst others), though if you only want simple annoyance blocking to supplement Safari&#x27;s in-built tracking protection instead of a full ad blocker, Hush is a good option. reply kseifried 15 hours agoprev [–] I’m trying to think of when I want my browser to make noise.Outside of Squadcast, YouTube and CBC, and… I’m drawing a blank. I feel like allow listing a handful of websites to make noise would be fine.Edit: Also, let’s see if anybody replying to this actually read the article I know this isn’t exactly what the article talks about but it’s what came to my mind when I saw the title. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Noiseless Browsing, a free content blocker by Joel Arvidsson is available for Safari on Mac, iPhone, and iPad.",
      "The application prevents unwanted cookie and privacy tracking notifications, meanwhile it does not collect personal data, ensuring user privacy.",
      "Advertised as lightweight, fast, easy to use, and being open-source, it is also compatible with the latest Apple technology."
    ],
    "commentSummary": [
      "The article discusses multiple extensions and browsers capable of blocking ads and cookie banners on the Safari browser.",
      "It delves into user irritations regarding cookie banners and the data collection rules under the General Data Protection Regulation (GDPR).",
      "The discussion brings up concerns about privacy and the safety of software tools, highlighting the current debate and user preferences about online privacy and tracking."
    ],
    "points": 146,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1694984455
  },
  {
    "id": 37551175,
    "title": "Plants can detect sound",
    "originLink": "https://www.economist.com/science-and-technology/2023/09/06/plants-dont-have-ears-but-they-can-still-detect-sound",
    "originBody": "Skip to content Menu Weekly edition The world in brief Search Subscribe Log in Science and technologyThe ears of corn Plants don’t have ears. But they can still detect sound Sometimes they produce it, too image: caroline péron Sep 6th 2023 Share I n 1986, when he was a mere prince, King Charles, Britain’s eco-minded monarch, told a television interviewer that it was important to talk to one’s plants. He was widely mocked. But that piece of princely wisdom seems to have been ahead of its time, for there is now plenty of evidence that plants can detect sound, react to it, and even, perhaps, produce it. Scientists have been experimenting with playing sounds to plants since at least the 1960s, during which time they have been exposed to everything from Beethoven to Michael Jackson. Over the years, evidence that this sort of thing can have an effect has been growing. One paper, published in 2018, claimed that an Asian shrub known as the telegraph plant grew substantially larger leaves when exposed to 56 days of Buddhist chants—but not if it was exposed to Western pop music, or silence. Another, published last year, found that marigolds and sage plants exposed to the noise of traffic from a busy motorway suffered stunted growth, and produced a range of stress compounds. Already have an account? Log in Get the full story Enjoy a month of insightful analysis for free. Cancel at any time Start trial Distinctive global analysis with more than 100 articles a week on The Economist app and economist.com An immersive world with podcasts and digital newsletters Intelligent debate with a global community in subscriber-only digital events Or continue reading this article Register now Science & technology September 9th 2023 Propane-powered heat pumps are greener Plants don’t have ears. But they can still detect sound Animals can be tracked by simply swabbing leaves Share Reuse this content THE ECONOMIST TODAY Handpicked stories, in your inbox A daily newsletter with the best of our journalism Sign up Yes, I agree to receive exclusive content, offers and updates to products and services from The Economist Group. I can change these preferences at any time. More from Science and technology How science will be transformed by AI Learn more in our podcasts and films Could AI transform science itself? Previous scientific revolutions have been led by academic journals and laboratories. Robots might create the next one How scientists are using artificial intelligence It is already making research faster, better, and more productive Subscribe Group subscriptions Reuse our content The Trust Project Help and contact us Keep updated Published since September 1843 to take part in “a severe contest between intelligence, which presses forward, and an unworthy, timid ignorance obstructing our progress.” The Economist About Advertise Press centre The Economist Group The Economist Group Economist Intelligence Economist Impact Economist Impact Events Working Here Economist Education Courses Which MBA? Executive Jobs Executive Education Navigator Terms of Use Privacy Cookie Policy Manage Cookies Accessibility Modern Slavery Statement Sitemap California: Do Not Sell My Personal Information Copyright © The Economist Newspaper Limited 2023. All rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=37551175",
    "commentBody": "Plants can detect soundHacker NewspastloginPlants can detect sound (economist.com) 140 points by lxm 12 hours ago| hidepastfavorite85 comments qlm 4 hours agoBoquila trifoliolata appears to mimic surrounding leaves and the mechanism is currently unknown. One (controversial) hypothesis is that it has some sort of vision.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Boquila reply unsupp0rted 3 hours agoparent> One hypothesis is that volatile organic compounds emitted from host plant leaves induce a phenotypic change in nearby B. trifoliolata leaves. By receiving different host signals into its system, it is able to create specific signals and hormones in its tissues to regulate gene transcription of leaf morphology and developmental pathways for leaf differentiation.[12] The other hypothesis is that there could be horizontal gene transfer between the host and B. trifoliolata. A separate study also conducted by Gianoli et al. suggests that bacterial agents, which could mediate a horizontal gene transfer, may play a role in leaf mimicry by B. trifoliolata.See, here are two perfectly reasonable explanations for the mimicry that don&#x27;t involve \"seeing\" anything.> A 2021 paper suggested that the plant has some sort of vision using ocelli. This hypothesis was presented on the basis of experiments in which the vine appeared to mimic plastic vines and artificial plants.Plastic vines and artificial plants too!? There go those two theories. reply LoveMortuus 2 hours agoparentprevPaper on the experiment, where they test if the plant will mimic the leaves of an artificial plastic plant: https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC8903786&#x2F;Quite a read, I hope others replicate the study, because this is fascinating. If we&#x27;re able to really point in the direction that the plant uses vision to mimic other plants leaves (even artificial plant), I wonder how that vision works! reply dtjb 1 hour agoparentprev>\"This is a deeply flawed article based on a poorly designed experiment and reflects significant author bias in the interpretation of the results,” wrote Washington State University horticultural physiologist Linda Chalker-Scott in a blog post earlier this year.https:&#x2F;&#x2F;www.the-scientist.com&#x2F;news-opinion&#x2F;can-plants-see-in... reply monlockandkey 2 hours agoparentprevWow this is insane. I wonder if this could have real world application? reply fellowmartian 2 hours agorootparentPlant-based surveillance cameras, obviously. reply xbmcuser 6 hours agoprevI guess this answers the question If a Tree Falls in the Forest and There&#x27;s No One Around to Hear It, Does It Make a Sound? yes the trees do hear it. reply adolph 1 minute agoprevThis post definitely needs a link to the wonderful book by Ed Yong, An Immense World. Given all that even simple animals can sense and affect, it is not surprising that plants will hold further sensory surprises.https:&#x2F;&#x2F;www.amazon.com&#x2F;Immense-World-Animal-Senses-Reveal&#x2F;dp... reply patrickscoleman 7 hours agoprevJust read “The Day of the Triffids” by John Wyndham (1951)[1]. It’s a great, classic horror sci-fi novel where plants can hear and talk to each other. Big influence for “Annihilation” and “28 Days Later.” Gotta love it when some crazy fictional idea turns out to have some factual (?) basis.[1] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;The_Day_of_the_Triffids reply kilolima 6 hours agoparentI think Annihilation is more a shameless remake of Stalker, as far as going into a \"Zone\" where an alien influence has changed the nature of reality. reply tsimionescu 5 hours agorootparentI wouldn&#x27;t say Annihilation is in any way a remake of Stalker, shameless or not. It is clearly inspired by it a bit in terms of setting, but it has a very different message to say. Stalker was essentially a movie about faith. Annihilation is a movie about self-destruction, both in the destructive sense, but also as a necessary step to building a new self.The concept of a special zone where things are not the same as in normal reality is merely a shared motif. reply robertlagrant 4 hours agorootparentI think the setting is a pretty big part of Annihilation. The \"about\" is something that&#x27;s shared by many different works, but the setting is a bit more unusual.So while I don&#x27;t agree that it&#x27;s a shameless remake, I would say the most distinctive thing about it is the setting, and so to some extent it&#x27;s a fair cop. reply tsimionescu 3 hours agorootparentIf we go into details about the setting, it is still vastly different from Stalker. The Zone in Stalker is only vaguely and mysteriously abnormal, in a subtle, mostly faith-based kind of way.By contrast, the Zone in Annihilation is very overtly supernatural, and in very specific ways that are important to the point it&#x27;s trying to make - merging and recombining humans and the problems that face them, and manifesting internal struggles externally. To be fair, the Zone in Roadside Picnic, the novel Stalker adapts, is also more overtly supernatural, but still in a very different way.I really don&#x27;t think it&#x27;s fair to consider Stalker or Roadside Picnic anything more than inspiration for Annihilation. reply throw0101b 2 hours agorootparentprevThe move Annihilation was based on the book of the same name (first of a trilogy):* https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Annihilation_(VanderMeer_novel... reply patrickscoleman 48 minutes agorootparentVanderMeer wrote the introduction for the new (Modern Library) edition of “The Day of the Triffids”! reply gavinmckenzie 3 hours agorootparentprevI thought the accepted explanation of Annihilation was a metaphor for cancer. The slow seemingly unstoppable spread. Mutations creating new things that mimic the familiar but in often grotesque ways, mechanistically expanding to destructively consume everything in its path into a new form of life. reply rocketbop 3 hours agorootparentprevAnnihilation reminded me of Ballard’s The Crystal World in which all living things are slowly but surely being consumed. reply barrysteve 2 hours agorootparentprevSimilar structure, yes.The questions about the realness of the Zone and the motives of the characters, allowed for much deeper and broader implications.Annihilation was more specific and overlapped too much with explicit pre-existing myth(s), to be a remake of Stalker. reply darkflame91 8 hours agoprevRoald Dahl wrote a short story, &#x27;The Sound Machine&#x27; that&#x27;s on very similar lines. Looks like it can be found in its entirety here: https:&#x2F;&#x2F;www.newyorker.com&#x2F;magazine&#x2F;1949&#x2F;09&#x2F;17&#x2F;the-sound-mach... reply euroderf 5 hours agoparentOh wow. I&#x27;ve been trying to find that story for decades but couldn&#x27;t remember title or author. I read it as a teenager and it left an impression. Thank you for posting! reply jimmaswell 10 hours agoprevI ththought of this: https:&#x2F;&#x2F;youtu.be&#x2F;b9UO9tn4MpI?si=JhvoaRUdBLed-LBR reply Fnoord 25 minutes agoparentThis one of the same author is equally interesting [1][1] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XGmlDY2sDg8 reply Fnoord 6 hours agoparentprevI thought of this [1][1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Secret_Life_of_Plants reply borissk 10 hours agoparentprevCrazy russians :) reply aschearer 10 hours agoprevThey don&#x27;t have eyes but can detect light, too... reply 11235813213455 6 hours agoparentthey don&#x27;t have feet but they can detect gravity too and grow vertically when they can reply belter 5 hours agoprevWhy the change in the title? It is not the original and there is nothing clikbaity about it. Is HN now editorializing the Economist? reply Ensorceled 3 hours agoparentThe \"plants don&#x27;t have ears\" part of the original title was, at best, unnecessarily folksy. reply belter 2 hours agorootparentGuidelines say: \"Otherwise please use the original title, unless it is misleading or linkbait; don&#x27;t editorialize.\" reply Ensorceled 8 minutes agorootparentI&#x27;m saying is probably click bait. reply lmpdev 4 hours agoparentprevConcision? reply amelius 2 hours agoprevOk, so I wonder if my plants would do better if I recorded jungle sounds and played them back when I&#x27;m not at home. reply hutzlibu 1 hour agoparentThere were quite some experiments in that direction, but using music. I don&#x27;t recall the name or have a link to such a study at hand (and I do question and never researched how scientific it was done) but the plants seemed to prefer Ravi Shankar instead of Heavy Metal. reply archsurface 9 hours agoprev\"Plants don&#x27;t have ears.\" - Learn something new every day. reply slimsag 9 hours agoparentThis seems verifiably false, too. Why else would it be called an &#x27;ear&#x27; of corn? reply eclecticfrank 4 hours agoparentprevDon&#x27;t trust everything on the internet. At least some plants have ears. [1][1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ear_(botany) reply sebmellen 5 hours agoprevMother Earth&#x27;s Plantasia — warm earth music for plants and the people who love themhttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SZkR3PyHTs0 reply signa11 7 hours agoprevin this regard, i am always reminded of pioneering (in true sense of that word) work of jagdish-chandra-bose (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jagadish_Chandra_Bose).it truly is fascinating ! reply jamiek88 7 hours agoparentAhem, that is Sir Jagdesh.:) reply signa11 5 hours agorootparentah yes, ofcourse ! i stand corrected... reply yieldcrv 19 minutes agoprevnext up: plants suffer and try to avoid suffering, vegans die out due to inability to reduce suffering or move the goal post to something selfishly convenient just like everyone else.just a matter of time. reply hombre_fatal 7 minutes agoparentSo you’re holding your breath for plants being sentient so you can finally have what you think is the upper hand when you try to argue with vegans? Seems like you should just act according to our knowledge of the world today than hoping for discoveries that justify your behavior retroactively. reply petemir 12 minutes agoparentprevSurely this study has been funded by the meat industry. reply noman-land 8 hours agoprevThis may sound like a dumb question but can anyone explain to me why our senses are not all just considered touch? Molecules touch your tongue and you taste things. Molecules touch your nose and you smell things. Air vibrates your inner ear hairs and you hear things. Photons hit your eyes and you see things (not sure if this counts but it seems like it could). reply magicalhippo 8 hours agoparentTouch is measuring motion or sustained pressure. Taste doesn&#x27;t care about how hard the molecules hit your taste receptors, only if they bind to the taste receptors or not. Our eyes don&#x27;t measure the photon pressure[1], the photons react with the atoms in our photoreceptor cells[2] triggering an electrochemical process.Our sense of hearing is the closest to touch. We have hairs on our body which is part of our sense of touch, which react to motion and pressure, both from physical objects and the air around us.Similarly the hairs in our ears react to the pressure waves of the air around us. However a big difference is that they&#x27;re shielded by the ear drum and don&#x27;t react to sustained pressure, only relatively high-frequency air pressure changes.Why we call them different things is mostly because they&#x27;re used for different tasks. An electrical generator and an electrical motor can be the same thing, it&#x27;s just what it&#x27;s used for.[1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Radiation_pressure[2]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Visual_phototransduction reply quietbritishjim 5 hours agorootparentIf you&#x27;re going to categorise senses into the usual five, which is what seems to be going on here, then hot, cold and pain are going to count as \"touch\" too. Those aren&#x27;t about measuring motion so this explanation doesn&#x27;t seem to work. reply memen 3 hours agorootparent> Although in some cultures five human senses were traditionally identified as such (namely sight, smell, touch, taste, and hearing), it is now recognized that there are many more.[0]: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Sense reply quietbritishjim 1 hour agorootparentYes of course (e.g. orientation from inner ear; proprioception; and others). But the context of the conversation was about the traditional understanding of \"touch\". I was trying to avoid the commenter I was replying to backing out of their point saying that they considered hot&#x2F;cold and pain to be separate senses. reply tsimionescu 5 hours agorootparentprevHot and cold do measure motion, if we want to be very technical (since temperature is just a measure of the average momentum of molecules).Pain should probably be considered a separate sense indeed, though it can also be argued it is something different from the senses themselves, since it is fully internal, it doesn&#x27;t reveal some fact about the outside world. reply pbhjpbhj 3 hours agorootparentTrigger warning: description of medical procedure.Yes pain seems more like the interpretation of sense data, like if something sounds nice or tastes good?When I had an operation under local anesthetic I still felt the knife cutting into me, but was able to interpret it as not painful? This is how opiates (for pain relief) have been described to me -- still allowing you to feel the sensations but allowing you to not &#x27;feel&#x27; the pain.Perhaps seeing with human biology knowledge can tell me why this is wrong!? reply Calavar 8 hours agoparentprevIf you want to get that reductive, ultimately everything in the universe is just particles and forces and interactions between them, and any concepts beyond that are arbitrary lines drawn by us humans in an attempt to organize the chaos.In the case of senses, it&#x27;s about perception. We perceive the signals from our retinas differently than the ones from our fingertips, so we give them different names (sight vs. touch).There are also differences in the \"hardware\" -- sight and touch signals start with different kinds of receptors, travel down different nerves, and are processed in different parts of the brain -- but the hardware differences don&#x27;t map 1:1 to the way that we categorize senses. For example, pain and pressure are processed by entirely different types of receptors, but we still call them both touch. On the flip side, balance and hearing share a lot of the same hardware, but we consider them to be different senses. reply RugnirViking 6 hours agoparentprevtouch isn&#x27;t one sense either. You have separate organs in the skin for high and low frequency vibrations, pressure, and more.Merkel&#x27;s disks (close to surface, sense edges and slight pressure) Meissner corpuscles (close to surface, sense fine vibrations) Pacinian corpuscles (Deep, sense low frequency vibrations and heavy pressure) Ruffini endings (Sense skin stretching)Not to mention other touch-like senses like pain, heat&#x2F;cold, and burns. There are also separate dedicated mechanisms for erogenous zones (Krause end-bulbs) Your brain does a fine job interpreting all of these together but they are totally different biological mechanisms reply throwaway_ab 5 hours agoparentprevSure, any interaction between our bodies and the universe can be considered an interaction of touch.All of our senses involve some form of matter&#x2F;energy touching us.- Photons touching our retina cells...- Molecules touching our taste buds...- Air molecules touching our ear hairs...- Some amount of matter or electromagnetic energy touching our skin...- Molecules touching our olfactory sensory neurons...IMO we do not describe them as all just being touch as:1) Saying they all are touch doesn&#x27;t help differentiate the different senses and how they work.2) It&#x27;s a bit too low level.This is similar to saying there is no need for the separate disciplines of biology & chemistry because it&#x27;s all just physics after all. So we might as well dump those subjects completely.Imagine being introduced to science at school but there is only one subject, physics, so you only \"eventually\" learn chemistry & biology through the lens of their physical atomic interactions.Imagine trying to describe a cell through the frame of atomic interactions and no higher level abstractions allowed.All science being physics is far less useful than being able to step over the base level going&#x27;s on and make higher level abstractions so that you work at the appropriate \"topology&#x2F;focus&#x2F;subdivision\" level for the scientific discipline in question.Anyways that&#x27;s what I thought of when considering all senses are touch, it&#x27;s just a bit too low level and ambiguous to be useful. reply aeturnum 7 hours agoparentprevI recommend you check out An Immense world by Ed Yong - many senses are arguably &#x27;touch tuned for something&#x27;. Though taste and smell and vision are not. reply dotnet00 3 hours agoparentprevSort of reminds me of the argument that nothing can be artificial because we, as natural beings are making said things and thus following the chain, everything is technically originated from nature.It&#x27;s technically correct and does highlight that calling something artificial doesn&#x27;t make it inherently inferior to something natural, but if we extended the meaning of natural like that, the term artificial would become meaningless while the need to distinguish between the \"naturalness\" of a CPU vs a plant would remain. reply Ensorceled 3 hours agoparentprev> This may sound like a dumb question but can anyone explain to me why our senses are not all just considered touch?It is a dumb question. How would categorizing all our senses as \"sense of touch\" be in anyway useful? Why would we categorize obviously different things that work entirely differently as the same because, \"at the end of the day, everything is atoms\"\"Did you lose both your sense of touch and your sense of touch when you got COVID?\"\"No, I thankfully, I only lost my sense of touch.\" reply Propelloni 4 hours agoparentprevYou might enjoy reading Hermann von Helmholtz&#x27; and Ernst Mach&#x27;s writings on sensory perception. If you do, you could do worse then to continue to read up on category philosophy. A seminal work would be Kant&#x27;s Critique of Pure Reason. reply zelphirkalt 8 hours agoparentprevSimilar to a question I had in physics class at school, which was quickly brushed aside by the teacher, who wanted to continue with the content: Why is temperature one of the base units? Isn&#x27;t temperature just movement as well? Nowadays I know I was more or less right. Just that 1 molecule moving hardly makes temperature and one probably needs a large number. reply yyyk 4 hours agorootparent- The average movement of a solid is often zero, while its temperature is usually higher.- Movement has an implied direction even if we ignore it sometimes. Temperature does not.- A gas molecule can move a lot but have low temperature because it has little to collide with.It&#x27;s more accurate to talk about vibration&#x2F;collisions rather than movement. reply truculent 3 hours agoparentprevBecause all those things feel different to touch reply emrah 6 hours agoparentprevThey could be. We have words to describe the special types of touch so we can differentiate them reply jraph 7 hours agoparentprevI would say: for useful abstraction. Both when speaking about them in everyday life, and studying them. reply IshKebab 7 hours agoparentprevBecause that would be an incredibly useless categorisation of very different senses. reply camillomiller 8 hours agoparentprevIsn’t touch a biological pressure sensor though? Could be similar to hearing, sure, but taste, smell and sight are definitely more like chemical and electromagnetic sensors. reply rafale 8 hours agoparentprevI think collision or interaction are better words. The word touch is meant for bigger objects, like hands, animals, furniture, fabrics, not molecules and electromagnetic radiation. reply bpanon 10 hours agoprev\"The Secret Life of Plants\" is a great book on this subject. reply adyashakti 47 minutes agoprevthis is old news. we were doing experiments with plants and music back in the 1960s. but nobody liked our results because we found that modern styles are toxic, and ancient vedic sacred chants are the most nourishing sound vibrations. reply fortran77 10 hours agoprevCorn has ears. reply borissk 10 hours agoparentAnd a strong political lobby in US :) reply TeMPOraL 7 hours agorootparentCorn are Legal People? reply DangitBobby 9 hours agoparentprevFortran77: 1The Economist: 0 reply endorphine 8 hours agoprevPerhaps a better title would be: \"Plants can detect sound\". reply jingles_dev 8 hours agoprevIn my studies, we place EEG electrode on them, they produce different amount of electrical signals in different conditions too. reply pbhjpbhj 3 hours agoparentPiezoelectric sensors can hear! &#x2F;s reply borissk 10 hours agoprevUnpaywalled: https:&#x2F;&#x2F;archive.ph&#x2F;SVoRk reply IshKebab 6 hours agoprevI was reading about this last week and the thing that struck me is that every experiment was \"plants respond more tothan \", or male voices vs female voices or swearing vs compliments.It&#x27;s total pseudoscience designed to generate comedy science articles and press releases.If anyone actually cared about this they&#x27;d be playing sine waves and white noise at various frequencies and amplitudes. You&#x27;d see a graph of growth vs frequency or whatever.I couldn&#x27;t find anything like that though. Does this article have it (can&#x27;t read due to paywall). reply lacrimacida 34 minutes agoparentScience for clicks reply borissk 10 hours agoprevI won&#x27;t be surprised if single cell organisms, even prokaryotes can detect and react to sound - just a matter of more research to find out how. reply addaon 9 hours agoparent> I won&#x27;t be surprised if single cell organisms, even prokaryotes can detect and react to soundIt would have to be pretty insanely high in the ultrasound. A 30 kHz (above the top of usual audible sound range) wave is around 10 mm (10000 µm) in wavelength; a typical prokaryote is of order 1 µm. While there are some uses for MHz-scale ultrasound, the attenuation starts getting pretty insane -- there&#x27;s just not much energy at the scale of gigahertz sound to detect. reply user_7832 3 hours agorootparentWhy would you assume that the order of magnitude of the animal is similar to the wavelength? Elephants can communicate with as low as 1hz. The wavelength of that is over 300m.If the amplitude is high enough, human too can comfortably “feel” infrasonic. reply bacon_waffle 4 hours agorootparentprevImagine an elevator that moves up and down its shaft with a sinusoidal elevation over time. Riding in that elevator, one could detect frequencies with quite long wavelengths compared to the size of a person.A less abstract example is a boat at sea. reply borissk 5 hours agorootparentprevInteresting, but a longer wave will still have some impact, that can be detected by a smaller organism - or not? reply pvaldes 3 hours agoprev [–] pseudoscience, unability to apply the most basic self-criticism [1], and low quality papermills churning turd-ticles in action.[1] Stop reading at \"Plants living near a busy road have their grow stunt because noise\", that is just the dumbest explanation available in the bag. Do they noticed that those areas are typically contaminated with heavy metals? reply yojo 3 hours agoparentThat’s not what the article said. Verbatim quote (emphasis mine):“…marigolds and sage plants exposed to the noise of traffic from a busy motorway suffered stunted growth”There’s a link in the article with more details on the experimental design:“The plants were grown from seed and allowed to mature for two months in the same space before they were divided into two groups. One group was exposed to 73 decibels of traffic noise recorded from a busy motorway in Tehran for 16 hours a day. The other group was left to grow in silence. After 15 days had passed, samples were taken from the youngest fully expanded leaves on every plant in the experiment and studied.”The plants were exposed to sound in a lab, with a control group. They were not actually next to a motorway. reply Ensorceled 3 hours agoparentprev [–] > pseudoscience, unability to apply the most basic self-criticism [1], and low quality papermills churning turd-ticles in action.This eloquently and entirely refutes all the claims, excellent work. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Scientific studies have shown that plants can detect and react to sound, and potentially produce sound themselves.",
      "Various experiments reveal different sound types can influence plant growth and stress compound production. For instance, an Asian shrub reportedly showed growth in leaf size after exposure to Buddhist chants.",
      "Some sounds can have negative effects on plants, as a study demonstrated stunted growth in plants exposed to traffic noise."
    ],
    "commentSummary": [
      "A study has discovered that the Boquila trifoliolata species of plants has the capacity to identify sounds, although the mechanism behind this is still under debate.",
      "There's disagreement among experts about the study's design and interpretation, especially on the topic of whether all senses in plants can be classified as a form of touch.",
      "Skepticism about pseudoscientific claims is present, with calls for more rigorous research, as exampled by a separate experiment studying the effect of traffic noise on plants in a laboratory setting."
    ],
    "points": 137,
    "commentCount": 79,
    "retryCount": 0,
    "time": 1695000586
  },
  {
    "id": 37546689,
    "title": "Browsing like it's 1994: Integrating a Mac SE, ImageWriter II into a modern LAN",
    "originLink": "https://connor.zip/posts/2023-08-04-localtalk-ethernet",
    "originBody": "Browsing like it's 1994 By Connor TaffePublished 2023-08-13. connor.zip Browsing like it's 1994 MacWeb 0.98 Before the ubiquity of the Internet, before WiFi, even before Ethernet was affordable, there was the LocalTalk physical layer and cabling system and its companion suite of protocols called AppleTalk1. A network ahead of its time in terms of plug-and-play2, but not quite as fast as 10mbit/s Ethernet at 230.4 kbit/s. A few weeks ago, I found a Macintosh SE on Facebook Marketplace. It turned out to be running System 7.1, and had Microsoft Word 5 installed. Years prior, I had recapped an Apple IIGS and brought it back to life, and attempted to network it using LocalTalk and an ImageWriter II with a LocalTalk Option card, but was unsuccessful. With the Macintosh, I was finally able to use my ImageWriter II over AppleTalk! Off to a good start, I wanted to expand my LocalTalk network. I swapped the ImageWriter II for an AsanteTalk and discovered that my HP LaserJet 4100N from 2004 with a 635n EIO networking card spoke EtherTalk and advertised itself as a LaserWriter. I was able to print the same document on the LaserJet, using the built-in PostScript driver for the LaserWriter -- the result was beautiful crisp text. In fact, there's another EIO card in the LaserJet; it provides USB connectivity alongside a LocalTalk port, so it could become part of my LocalTalk network as well. HP JetDirect Cards Next, I ordered some LocalTalk adapters from eBay to convert my 8 pin DIN to 3 pin locking LocalTalk ports which work with LocalTalk cabling. Each adapter has two ports, which supports chaining devices together. Unfortunately, LocalTalk cabling is expensive (and PhoneNet was used more often at the time), so my LocalTalk network is limited to the Macintosh and AsanteTalk for the moment. With the AsanteTalk3, we open up the possibility of interfacing with a wider Ethernet and IP network. As we enter the early 90s and the Internet becomes more widely available, these older Macintosh computers were used to access it. MacTCP and the AsanteTalk helped to enable this, and that's what this post is about. AsanteTalk This post is broken down into several sections: The next section, Printing over LocalTalk covers the steps I used to configure my ImageWriter II and Macintosh SE to allow printing over LocalTalk. Netatalk 2.x covers integrating a Linux server with an Ethernet connection into an AppleTalk network. Printing covers how to print to the ImageWriter II from a modern network, even from an iPhone via AirPrint. Adding Files covers how to get files from the internet onto your Macintosh SE via AppleShare. Getting Online covers using a period-correct browser, MacWeb 0.98, to browse the web. System 6 is an adendum on work in-progress to connect a Macintosh SE running System 6 to the network. Printing over LocalTalk Printing from a Macintosh SE to an ImageWriter is easy with the LocalTalk option card. Install the LocalTalk option card in the ImageWriter II: Lift off the lid, both clear plastic and tan pieces Gently move the carriage to the extreme left Remove the ribbon cartridge by lightly bending the two black tabs on either side and lifting, ensure the ribbon isn't stuck in the print head. ImageWriter II with lid and cartridge removed Unscrew the two golden screws on round plastic wells on either side of the printer Lift up and back at the top of the printer, being careful not to pull the cable that connects the top buttons ImageWriter II with top removed Place the option card atop the logic board. ImageWriter II Logic Board Press the plastic spacers into the holes in the logic board. Slide the ground cable onto the unused stub. LocalTalk Option Card Ensure that DIP switch 4 on the second switch block is in the down position to enable the card ImageWriter II DIP Switches Connect the Macintosh SE's printer port to the ImageWriter II using either a 8-pin DIN printer cable, or by using LocalTalk adapters on each side and a 3-pin DIN LocalTalk cable, or by using a Farallon PhoneNet adapter and a phone line. Macintosh SE Printer Port In Chooser, select \"AppleTalk ImageWriter\" to set it as the default printer Macintosh Chooser Open a document in a word processing app like Word 5.1, and print a document. Word 5.1 Netatalk 2.x Netatalk is the Linux implementation of several Apple protocols including AppleShare. Before 3.x, it supported AppleTalk, the protocol that Apple used before the switch to IP, importantly for us this is the protocol used over the LocalTalk and EtherTalk physical layers. There are several forks of Netatalk 2.x maintained by the retrocomputing community: In the 5 years since the release of Netatalk 2.2.6, an impressive number of forks and projects with their own downstream patchset to keep Netatalk running have emerged. Here are a few of the major ones that I encountered: A2SERVER MacIP AFPBridge NetBSD netatalk22 package netatalk-classic fork Last year, Daniel Markstedt (handles rdmark or slipperygrey) released a new Netatalk 2.x fork which can be compiled on modern Linux and includes systemd services. I'll be installing it on a Fedora Server VM running on ESXi. Compile To get netatalk-2.x installed and serving AFP and AppleTalk, we need to compile it. First, we'll install some dependencies: ; sudo dnf install openssl-devel libgcrypt-devel libdb-devel automake libtool avahi-devel cups-devel Dependency Feature avahi-devel Zeroconf (Bonjour) service discovery in Mac OS X 10.2 or later cups-devel papd printer server support libgcrypt-devel DHX2 authentication support, required for Mac OS X 10.2 or later Then we'll need the appletalk kernel module for AppleTalk network support. On Fedora this is provided by kernel-modules-extra, but not on Fedora 35: ; sudo dnf install kernel-modules-extra On Fedora, the appletalk module is blacklisted. To allow it, edit the file /etc/modprobe.d/appletalk-blacklist.conf and comment out the last line: # This kernel module can be automatically loaded by non-root users. To # enhance system security, the module is blacklisted by default to ensure # system administrators make the module available for use as needed. # See https://access.redhat.com/articles/3760101 for more details. # # Remove the blacklist by adding a comment # at the start of the line. #blacklist appletalk Then have the module load automatically4, we need to add the file /etc/modules-load.d/appletalk.conf: # Load appletalk.ko at boot appletalk which configures the systemd-modules-load.service service. You can test it with: ; sudo systemctl start systemd-modules-load.service ; journalctl -n 10 -u systemd-modules-load.service Aug 05 01:25:44 misc.home.arpa systemd[1]: Starting systemd-modules-load.service - Load Kernel Modules... Aug 05 01:25:44 misc.home.arpa systemd-modules-load[1433]: Inserted module 'appletalk' ... Upon reboot, the module should be automatically loaded. To test the module is loaded: ; lsmodgrep '^appletalk' and to manually load the module: ; sudo modprobe appletalk To compile netatalk-2.x, first clone the repo: ; git clone https://github.com/rdmark/netatalk-2.x.git Then run the boostrap script5. ; ./bootstrap Now run the configure script, options are described in this post. ; ./configure --enable-systemd --enable-ddp --enable-a2boot --enable-cups --enable-timelord --enable-zeroconf --disable-quota --sysconfdir=/etc --with-uams-path=/usr/lib/netatalk Finally, run make then make install as root. ; make ; sudo make install Configure Now you should have the systemd services in place for atalkd.service and afpd.service among others. First let's set up some minimal config files under /etc/netatalk/: To configure atalkd.conf, you'll need the name of the interface that an AppleTalk network will be present on (a LAN): ; ip addr 1: lo: ... .... 2: ens160: ... .... ... In my case my VM's interface is named ens160, so my /etc/netatalk/atalkd.conf file ends with the line: ens160 -router -phase 2 -net 1 -addr 1.41 -zone \"office\" Next is Apple Filing Protocol, which is configured in /etc/netatalk/afpd.conf: \"Office\" -transall -uamlist uams_guest.so,uams_clrtxt.so,uams_dhx2.so I use \"Office\" instead of - because I want a friendly name instead of the VM hostname. transall enables both DSI (Data Stream Interface6) over TCP and DDP (Datagram Delivery Protocol7) aka EtherTalk, the AppleTalk data link layer on the Ethernet physical layer. The modules listed enable both DDP and DSI, the guest UAM for anonymous read-only access, the clrtxt UAM for Classic Mac OS authentication, and DHX2 UAM for Mac OS X / macOS authentication. The guest login only allows read-only access to shares, and System 7's AppleTalk interface in Chooser limits passwords to 8 characters. Netatalk authenticates against system users, so I created a new macintosh user with an 8-character password to allow logins. ; sudo useradd macintosh ; sudo passwd macintosh Next is the AppleVolumes.default file, which defines the volumes available to connecting systems. By default a user's home directory is exposed as a share with this line ~ but we can also add other shares: /srv/appletalk \"Share\" options:prodos This creates a share at /srv/appletalk, named Share, with the prodos option which allows the Apple IIGS to use the share or boot from it. You can now enable the services: ; sudo systemctl enable --now atalkd.service ; sudo systemctl enable --now afpd.service Firewall To ensure we can access these shares over TCP using afpovertcp from a modern mac, we need to open the firewall. I created a new service for the port and enabled it: ; sudo firewall-cmd --permanent --new-service=afpovertcp ; sudo firewall-cmd --permanent --service=afpovertcp --add-port=548/tcp ; sudo firewall-cmd --permanent --add-service=afpovertcp ; sudo firewall-cmd --reload Printing Netatalk also includes a Printer Access Protocol daemon called papd which integrates with CUPS and provides bidirectional printing support. Apple ImageWriter II Macintosh to CUPS Next we'll edit /etc/netatalk/papd.conf to expose our CUPS printers to the AppleTalk network, see these directions: cupsautoadd:op=root: The documentation tells us: If used as the first entry in papd.conf this will share all CUPS printers via papd. type/zone settings as well as other parameters assigned to this special printer share will apply to all CUPS printers. Unless the pd option is set, the CUPS PPDs will be used. To overwrite these global settings for individual printers simply add them subsequently to papd.conf and assign different settings. We should now enable the service ; sudo systemctl enable --now papd.service CUPS to ImageWriter II To have CUPS print to an AppleTalk printer, we need a pap backend, there are good directions here8. By default, the backend only looks for LaserWriter devices, edit /usr/lib/cups/backend/pap so that devicetypes reflects this or set it to devicetypes=\"=\" to find all devices. devicetypes=\"LaserWriter:ImageWriter\" With the pap backend in place, we should see our printer here: lpinfo -v ... network pap://office/HP%20LaserJet%204100%20Series/LaserWriter network pap://office/ImageWriter/ImageWriter If it doesn't show up, ensure your printer is shared over AppleTalk: ; nbplkup ... AsantéTalk 94B02967:Asant�Talk 1.111:252 ImageWriter:ImageWriter 1.113:138 We also need to update our /etc/cups/cupsd.conf file with: BrowseOrder allow,deny BrowseAllow all BrowseRemoteProtocols CUPS dnssd pap BrowseAddress @LOCAL BrowseLocalProtocols CUPS dnssd pap and restart the services: ; sudo systemctl restart cups.service Now in the CUPS UI, under Administration > Add a Printer, you should see the AppleTalk Devices via pap option. Select it and continue, then copy the URL from lpinfo -v or constructed from nbplkup info into the form, name the printer, and upload the ImageWriter II PPD file. We need to patch9 our Netatalk 2.x distribution so that the status check doesn't error on ImageWriter IIs. Apply this patch to your netatalk-2.x directory: ; curl -s https://connor.zip/resources/patches/netatalk-2.x/0001-Fix-PAP-status-for-ImageWriter-II.patchgit apply - ; make ; sudo make install ; sudo sytemctl restart papd.service Now from the CUPS UI, you can print a test page and it should print from the ImageWriter II: ImageWriter II printing the CUPS test page By adding a Avahi service file, we can even print via AirPrint: ImageWriter II _ipp._tcp _universal._sub._ipp._tcp 631 txtvers=1 qtotal=1 UUID=EF910D03-69A2-44BC-B793-2966D282B0A4 Binary=T TBCP=T kind=document URF=none rp=printers/imagewriter note=Office product=(ImageWriter II) pdl=application/octet-stream,application/pdf,application/postscript,application/vnd.cups-raster,image/gif,image/jpeg,image/png,image/tiff,image/urf,text/html,text/plain,application/vnd.adobe-reader-postscript,application/vnd.cups-pdfThe final flow is: VM ImageWriter II iPhone CUPS PDF over IPP GhostScript iwhi driver Filters PAP Backend Raster data AsanteTalk EtherTalk LocalTalk Option Card LocalTalk Printer Adding files In each share, Netatalk creats metadata stores. Files in the share only represent only part of a file, the metadata is maintained in these databaes. If you add a Macintosh file in Linux, or even if you copy an existing file to a new name, it'll show up to the Macintosh as an unknown file -- the metadata about how to open it has been lost. This presents quite a problem: without a Macintosh with the file, how can I add files to my share? I've come up with a couple of options: StuffIt is a program for creating archive files. These file contain this special metadata and it's recreated when StuffIt Expander is run on a Macintosh on one of these files, so the files can be handled by other operating systems and file systems without losing it. It was a common way to provide Macintosh files over the internet at the time. If your Mac doesn't already have StuffIt installed, but has a working floppy disk drive, you may be able to dd an .img image file onto a floppy using a USB floppy drive. Macs use variable speed writes and so PCs can't read their floppys, but I believe the reverse is possible. Basilisk II a Macintosh emulator which could emulate a client Mac and allow you to add files to the share. The latest version as of writing with support for ARM-based Macs is here, and the GUI is here, follow this guide and then this one for AppleTalk. We can add an image file of a floppy containing StuffIt to install it, then we can copy that program onto the share. When using slirp with Basilisk II, the IP configuration (using OpenTransport) are not the settings from your network, but the network within Basilisk II: Setting Value IP Address 10.0.2.15 Subnet mask 255.255.255.0 Router address 10.0.2.2 Name server address 10.0.2.3 I couldn't get slirp to work, but I discovered that the shared directory between macOS and System 7 on Basilisk II can hold the StuffIt files going into Basilisk II and the resulting decompressed files. And macOS can handle Macintosh files without disrupting the metadata. We can mount the Netatalk AFP server to our modern Mac via afpovertcp, then copy the un-stuffed program files from Basilisk II from the share folder to our AFP folder. Or, we can make the AFP share folder our Basilisk II share folder and skip the extra copy. I was able to copy the StuffIt program file this way from Basilisk II to my Macintosh SE. Laptop Macintosh SE Basilisk II AFP mount share folder Netatalk 2.x TCP AFP mount AppleTalk There are several file formats used for old files: .sit or .sea are archives created by StuffIt, StuffIt 4.x runs on a Macintosh but can't open archives from newer versions. .bin and .hqx which are extractable with Archive Utility on a modern macOS. Sometimes an .img file may be StuffIt compressed, to deal with that: Copy the file into the share folder for the emulator On the emulator, StuffIt expand the file by dragging it onto the StuffIt Expander app Shut down the emulator In the Basilisk II GUI, add as a disk the expanded .img file in the share folder Start the emulator, the files are available on the mounted disk iamge Here is an update list, and here's another for System 7.1. Getting Online Mac IP Gateway To get our Macintosh online, we need either OpenTransport10 (for newer versions of System 7) or MacTCP. They both work by proxying IP packets over AppleTalk where a gateway, (originally a newer Mac running Apple IP Gateway11) translates them to IP on Ethernet. Using macipgw, which was originally written for FreeBSD but has now been ported to Linux, we can provide this gateway. The AppleTalk packets themselves are copied from LocalTalk to Ethernet by an AsanteTalk, and since EtherTalk is routed over Ethernet and not IP, any system or VM running this software or Netatalk 2.x must have a physical Ethernet connection. Unfortunately, macipgw requires a kernel with CONFIG_IPDDP disabled: Your kernel must be configured with the CONFIG_IPDDP option disabled completely. It is not sufficient to compile it as a module -- in order to support the module, the kernel is modified to intercept all MacIP traffic, so userspace applications such as macipgw cannot handle it. And my Federa 37 kernel on the VM where I run Netatalk has it configured as a module: ; cat /boot/config-$(uname -r)grep CONFIG_IPDDP CONFIG_IPDDP=m CONFIG_IPDDP_ENCAP=y There is a ready made ISO image from macip.net based on Tiny Core Linux, which I can run on ESXi with 512MB of memory and no hard disk (it's a live CD). MacWeb Using MacWeb 0.98 and MacTCP configured with the IPs provided by tinymacipgw, I was able to access the local network and load this blog's index page from the Kubernetes cluster in my office closet. I attempted to use Netscape Navigator and iCab based on the list from here to no avail, Netscape Navigator crashed and iCab reported that it didn't have enough memory (the Macintosh has 4MB of RAM which is the maximum configurable). MacWeb 0.98 on a Macintosh SE running System 7.1 Below is a diagram of the path from the Mac to the internet: Macintosh SE ESXi MacWeb 0.98 MacTCP AsanteTalk LocalTalk MacIP Gateway EtherTalk pfSense IP Internet Here's what an HTTP request from MacWeb looks like: ; nc -vv -l 0.0.0.0 -p 8000 Connection from 10.0.2.250:1698 GET / HTTP/1.0 Accept: application/mac-binhex40 q=0.500 Accept: audio/basic q=0.500 Accept: image/gif q=0.500 Accept: image/jpeg q=0.500 Accept: image/pict q=0.500 Accept: image/x-xbitmap q=0.500 Accept: video/mpeg q=0.500 Accept: video/quicktime q=0.500 Accept: www/source q=0.300 Accept: www/unknown q=0.300 Accept: application/octet-stream q=0.100 Accept: text/plain Accept: text/html User-Agent: MacWeb/libwww/2.13 libwww/unknown The Accept header syntax has some quirks when compared to the standard. Instead of separating the options with , on a single line, each option gets its own line; and instead of separating each option and its q= with a ;, there is a space. The browser also places the most preferred format at the end, instead of at the beginning as would be expected within a single line to distinguish between multiple values with no q. And, it has no support for application/xhtml+xml, a MIME type registered in 2002 well after its release. After some adjustments, this website is now viewable on MacWeb 0.98, although all pages except the index seem to hang (I assume because they're too large). There are also some interesting MIME types in the request, like BinHex for applications or audio/basic for audio (the digital audio encoding introduced by the telephone system) The content of the \"audio/basic\" subtype is single channel audio encoded using 8bit ISDN mu-law [PCM] at a sample rate of 8000 Hz. It also advertises image/pict for the PICT graphics format: PICT is a file format that was developed by Apple Computer in 1984 as the native format for Macintosh graphics. PICT files are encoded in QuickDraw commands. The PICT file format is a meta-format that can be used for both bitmap images and vector images. There's also www/source and www/unknown, an artifact of its use of libwww, now available on GitHub. I found some information in the MIT WWW Library HTFormat docs. It seems to predate MIME: The www/xxx ones are of course not MIME standard. star/star is an output format which leaves the input untouched. It is useful for diagnostics, and for users who want to see the original, whatever it is. #define WWW_SOURCE HTAtom_for(\"*/*\") /* Whatever it was originally */ www/present represents the user's perception of the document. If you convert to www/present, you present the material to the user. #define WWW_PRESENT HTAtom_for(\"www/present\") /* The user's perception */ The message/rfc822 format means a MIME message or a plain text message with no MIME header. This is what is returned by an HTTP server. #define WWW_MIME HTAtom_for(\"www/mime\") /* A MIME message */ www/print is like www/present except it represents a printed copy. #define WWW_PRINT HTAtom_for(\"www/print\") /* A printed copy */ www/unknown is a really unknown type. Some default action is appropriate. #define WWW_UNKNOWN HTAtom_for(\"www/unknown\") NCSA Mosaic Based on a comment on Hacker News, I also gave NCSA Mosaic 1.0.3 a try using this copy. It works! Mosiac 1.x is the last to work on the Macintosh SE, since Mosaic 2.0.1 asks for 5MB of memory. NCSA Mosiac 1.0.3 on a Macintosh SE running System 7.1 The historical relevance of Mosaic can't be understated. Marc Andreessen led the NCSA Mosaic project, and went on the found Netscape, eventually co-founding the VC firm Andreessen Horowitz. Netscape went on to become Mozilla Firefox, one of today's major browsers. In 1995, Internet Explorer had its start when Microsoft licensed Spyglass Mosaic (which shared no code with NCSA Mosaic but licensed the name). NCSA Mosaic Netscape Spyglass Mosaic Mozilla Firefox Andreessen Horowitz Internet Explorer Mosaic's creation was enabled by Al Gore's bill High Performance Computing and Communication Act of 1991, Andreessen had this to say: If it had been left to private industry, it wouldn't have happened. At least, not until years later. It accounts for the origins (at least in name) of two major browsers, of which only Firefox is still relevant today since the switch from IE to Edge. As for the other two major browsers: Chrome was built from scratch by Google, while Safari was originally derived from the KDE project's Konqueror browser engine, KHTML. And here's what a request looks like from NCSA Mosaic: ; nc -vv -l 0.0.0.0 -p 8000 Connection from 10.0.2.250:1202 GET / HTTP/1.0 Accept: text/plain Accept: application/x-html Accept: application/html Accept: text/x-html Accept: text/html Accept: text/richtext Accept: application/octet-stream Accept: application/postscript Accept: application/mac-binhex40 Accept: application/zip Accept: application/macwriteii Accept: application/msword Accept: image/gif Accept: image/jpeg Accept: image/x-pict Accept: image/tiff Accept: image/x-xbm Accept: audio/x-aiff Accept: audio/basic Accept: video/mpeg Accept: video/quicktime Accept: application/macbinary Accept: */* User-Agent: MacMosaicB6 libwww2.09 There is an option to use HTTP 0.9, which simply sends: ; nc -vv -l 0.0.0.0 -p 8000 Connection from 10.0.2.250:1153 GET / There are some interesting non-standard MIME types here as well, such as application/x-html and text/x-html. There's also application/macbinary for the MacBinary (.bin) format (similar to BinHex referenced above with application/mac-binhex40) used to transfer both data and resource forks of Macintosh application file across the network. We also see formats specific to Microsoft Word and MacWrite II. The best resource for AppleTalk is Inside AppleTalk, second edition. Cisco also has detailed instructions on Configuring AppleTalk routing. ↩︎ As Apple shifted to IP, it helped to create Bonjour aka Multicast DNS (mDNS) or DNS Service Discovery (DNS-SD) which allows computers on a network to advertise services they provide on the network. The ZeroConf standardized IPv4LL (IPv4 Link Local addressing), a requirement for fully plug-and-play networking without a DHCP server: The IETF Zeroconf Working Group was chartered September 1999 and held its first official meeting at the 46th IETF in Washington, D.C., in November 1999. By the time the Working Group completed its work on Dynamic Configuration of IPv4 Link-Local Addresses and wrapped up in July 2003, IPv4LL was implemented and shipping in Mac OS (9 & X), Microsoft Windows (98, ME, 2000, XP, 2003), in every network printer from every major printer vendor, and in many assorted network devices from a variety of vendors. IPv4LL is available for Linux and for embedded operating systems. If you’re making a networked device today, there’s no excuse not to include IPv4 Link-Local Addressing. The specification for IPv4 Link-Local Addressing is complete, but the work to improve network ease-of-use (Zero Configuration Networking) continues. That means making it possible to take two laptop computers, and connect them with a crossover Ethernet cable, and have them communicate usefully using IP, without needing a man in a white lab coat to set it all up for you. Zeroconf is not limited to networks with just two hosts, but as we scale up our technologies to larger networks, we always have to be sure we haven’t forgotten the two-devices (and no DHCP server) case. Historically, AppleTalk handled this very well. Back in the 1980s if you took a group of Macs and connected them together with LocalTalk cabling, you had a working AppleTalk network, without any expert intervention, without needing to set up special servers like a DHCP server or a DNS server. In the 1990s the same was true using Ethernet — if you took a group of Macs and plugged them into an Ethernet hub, you had a working AppleTalk network, using AppleTalk-over-Ethernet. Now that it’s common for computers to have IEEE 802.11 (\"AirPort\") networking built-in, you don’t even need cables or a hub. ↩︎ The AsanteTalk is a small metal box with a wall-wart power supply, Ethernet port, and 8-pin DIN port which can either be connected directly to the printer port of a Macintosh, Apple IIGS, or to a printer, or to a LocalTalk 3-pin DIN adapter and used with locking LocalTalk cabling as part of a wider LocalTalk network. The Farallon PhoneNet adapters were popular in this era, and you can use these adapters as well along with standard 4-wire RJ11 terminated phone lines. The Macintosh Troubleshooting Pocket Guide from 2002 answers this question: How do I connect my LocalTalk printer to my USB Mac? Printers like the LaserWriter IINT, NTX, F, Personal LaserWriter NT, NTR, 320, LaserWriter Pro 600, 4/600 PS, Select 360, Color StyleWriter 6500, or an HP LaserJet with \"M\" or \"MP\" in its name? To connect these printers to a new Mac, you must use an Ethernet to LocalTalk Bridge: The AsanteTalk Ethernet to LocalTalk Bridge includes everything you need to connect a LocalTalk printer to a new Mac. It works with existing drivers. If the printer is already connected to a LocalTalk network, you can use Farallon's iPrint LT. The iPrint LT is similar to the AsanteTalk, except that it has a PhoneNet jack instead of a LoclTalk DIN-8 jack. If your existing LocalTalk network has more than eight LocalTalk devices on it, you need a much more expensive bridge, and are better off upgrading to Ethernet all around. One gotcha: Mac OS 10.2 and later no longer support PostScript Level 1 printers -- only PostScript 2 & 3. So your old LaserWriter II NTX and other PostScript Level 1 printers will not work from 10.2 at all. More information on the AsanteTalk is available in the User Manual. I found this manual on Marushin, a website for a Japanese shop which focuses on old Macintosh computers. I can't help but fix it. This is the spirit of the 65-year-old shopkeeper. ↩︎ See Persistent Module Loading in the Fedora docs. ↩︎ This StackOverflow answer has more info ↩︎ DSI was introduced with MacTCP to enable AppleTalk over TCP and enable IP networking. I've found this PDF of chapter 5 on ADSP of Inside Macintosh: Networking. ↩︎ The excellent book Inside AppleTalk covers DDP in chapter 4. I've also found this PDF of chapter 7 on DDP of Inside Macintosh: Networking. ↩︎ As documented in this footnote, there are several pap backends based on the work of Rupi, which I first discovered reading How CUPS talks to Print Servers, Print Clients and Printers. Its link to the pap backend is only available via the Way Back Machine. ↩︎ I adapted the approach taken by Carpentier Pierre-Francois in thier fork. In futher testing, it seems the patch to papstatus.c is unnecessary and reports incorrect status information, the Netatalk 2.x version already returns a human-readable string. There is a bug where CUPS displays discovered network printers, PAP printers will display the status strangely: ImageWriter II@office (pap) (%%[ status: Processing... ]%%) ↩︎ OpenTransport was an Apple implentation of the UNIX STREAMS networking API, described in Tech Note 1117. Dennis Ritchie wrote in A Stream Input Output System: Patchwork solutions to specific problems were destroying the modularity of this part of the system. The time was ripe to redo the whole thing. This paper describes the new organization. I think this was in response to approachs such as BSD Sockets. STREAMS was further iterated on in Plan 9. ↩︎ A copy of the original software is available here ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=37546689",
    "commentBody": "Browsing like it&#x27;s 1994: Integrating a Mac SE, ImageWriter II into a modern LANHacker NewspastloginBrowsing like it&#x27;s 1994: Integrating a Mac SE, ImageWriter II into a modern LAN (connor.zip) 136 points by blakespot 21 hours ago| hidepastfavorite48 comments jdblair 21 hours agoLocaltalk was great, RS-422 is a low-cost long-distance serial transport, and Apple took advantage of that to build networking into the Macintosh. Our computer lab in high school used Farallon PhoneNET transceivers, which made it easy to daisy chain computers using low-cost Category 1 RJ-11 phone wiring.In college (Miami University), our computer lab also used PhoneNET (it really simplified wiring) combined with a Cayman Gatorbox to bridge to the campus TCP&#x2F;IP backbone. By modern standards, painfully slow, but in 1992 the whole university was connected over a single 56K uplink to Columbus, so we hardly noticed.I have fond memories of playing Spectre with networked opponents over LocalTalk.To get online from our dorm rooms or offcampus, the only option was dial-up to the university phonebank, a 2400 baud terminal. Using the LocalTalk connection from a lab was much more responsive. reply retrac 20 hours agoparentIt was always some version of Phonenet (Localtalk over RJ-11 phone cabling) in my experience; I don&#x27;t think I ever saw real Apple Localtalk hardware!The Appletalk protocols were excellent for small networks. It was truly plug-and-play. From the late 1980s, if there was more than one Mac in the same building, they were probably networked together. It really was as easy as connecting a cable between them. They would auto-configure, and off you went, sharing files or playing multi-user games. This was the same era of recompiling UNIX kernels to tweak NFS buffer parameters, and when the DOS world was mired in a nightmare of TSR packet drivers and Netware servers. Very ahead of its time. I do remember the impression however, that the protocols did not scale well beyond a small LAN. reply spitfire 17 hours agorootparentThis is something worth looking into.What would a modern system built in 2023 with auto-configured everything look like?I don&#x27;t mean take Linux with some added magic - that&#x27;s MacOS. I mean a full modern distributed auto-configured, plug&#x27;n&#x27;play system. Sort of a modern AS&#x2F;400 meets Macintosh. reply dpe82 15 hours agorootparentTell more. :) What things do you want plug&#x27;n&#x27;play? Since this is HN, are you thinking stuff like server infrastructure? reply bombcar 1 hour agorootparentIt is theoretically possible to have an entire “self configured” network today, but since a network (like a computer) that is not connected to the Internet now feels completely useless. And once you have a router it can handle a bunch of the network details via dhcp. reply inferiorhuman 9 hours agorootparentprevnext [–]I do remember the impression however, that the protocols did not scale well beyond a small LAN.Apple deserves a lot of credit for trying to smooth over the UX. The reality wasn&#x27;t that things were very much plug and pray. I absolutely remember things grinding to a halt for instance when trying to print with more than 1 or 2 computers on the network. Unfortunately networked PCs started to become fairly popular around the time that Apple went to shit. Pretty much everything > 7.5 andI have fond memories of playing Spectre with networked opponents over LocalTalk.I was in a gifted programme in primary school in the early 1990s. The teachers running it weren’t very computer-literate and expected the Macs in the classroom to only be used as a support for whatever non-computer-related intellectual activities. I got sent out of the programme back to the non-gifted classes for setting up networked Spectre and Bolo. They saw it as time-wasting gaming, but what fascinated me was that computers could be networked. I think that if I had been left to explore, I would have eventually been on the path to a computer science or IT education (and a much better salary). reply jdblair 21 hours agoparentprevA few links to references in the above post:PhoneNet: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;PhoneNETGatorBox: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GatorBoxSpectre: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Spectre_(1991_video_game) reply nxobject 2 hours agoparentprevAnother reason LocalTalk was brilliant, beyond zeroconfig: you could use ADB cables with it! In a pinch, if you needed to get a file from one computer to another, you&#x27;d simply detach the cable from your keyboard and use that to connect the two computers together. reply blakespot 19 hours agoparentprevI had an Apple IIgs and a PowerMac G5 across the room from each other (still have the GS, but in the G5&#x27;s spot sits a Mac Studio, now). To get files from the net onto the IIgs, I attached a Keyspan USB to serial adapter to the Mac, with the Mac-style miniDIN RS-422 connectors, and connected the GS to the Mac&#x27;s serial port via RJ-11 using phoneNet adapters on each end. Worked great for filesharing to the GS. (I&#x27;ve got an ethernet card in the IIgs now, so I just FTP to the Mac (or wherever) to get files I want.) reply rodgerd 16 hours agoparentprev> Localtalk was greatOver the other side of the Atlantic, Acorn had an example of parallel evolution in the form of Econet, which used the same serial protocol, but over bigger connectors (the same 5 pin DIN that you see used in DMX networks and MIDI interfaces); as with AppleTalk the goal was simplicity. reply comex 12 hours agoprevFYI for the author:> Chrome was built from scratch by Google, while Safari was originally derived from the KDE project&#x27;s Konqueror browser engine, KHTML.This isn’t correct. Chrome’s engine, Blink, was forked in 2013 from WebKit, the engine used by Safari which was itself forked from KHTML. reply bombcar 1 hour agoparentChrome using WebKit is probably the single largest non-cash payment given to Apple in the history of the world.Safari has already some inroads in mobile but chrome pushed WebKit to the forefront renderer. reply cptaffe 9 hours agoparentprevThanks! I’ll update this shortly reply trimbo 21 hours agoprev> I attempted to use Netscape Navigator and iCab based on the list from here to no avail, Netscape Navigator crashed and iCab reported that it didn&#x27;t have enough memoryWell the SE is like 1986 technology that predates the web. NCSA Mosaic came out in 1993.I can&#x27;t find the min spec for Mac Mosaic 1.x other than \"System 7\" but you could give it a try. Even Mosaic would have been mostly targeting Macs in wide deployment at the time. Those would be more like Mac II, Centris and Quadra machines with 68030s or 68040s with more RAM and built-in HDDs.Fun project! reply cptaffe 20 hours agoparentThanks for the advice. I found a copy of NCSA Mosaic 1.0.3 and it works on the Macintosh SE running System 7! The next version, Mosaic 2.0.1, needs 5MB of free memory. Mosaic does a better job handling larger pages like this blog post, and even chews through the inline SVG diagrams and renders just the text.My only complaint is that Mosaic uses the serif default font we&#x27;re all used to, instead of the sans-serif default used by MacWeb, which makes it a little harder to read at such a low resolution.Added a section on it to the bottom of the article :) reply jandrese 14 hours agoparentprevI had a Mac LC with 4MB of RAM when I was a kid. It ran System 7 and was slightly too underpowered to run a web browser. I think you needed at least 6 MB to get Netscape 1.0 to run. Even if you did get it to run it would be painfully slow, especially if you&#x27;re connected via a 14.4 modem as was common at the time. Loading a single image could take a full minute or more.Rule of thumb for Z-Modem transfers on a 14.4 modem was 100kB of data per minute. reply poulsbohemian 7 hours agorootparentI&#x27;m trying to recall how I did this - maybe RAM disk? Wasn&#x27;t much of a concern at first, as you could run something like lynx in your dial up terminal and there weren&#x27;t many images on those early web sites. reply sk5t 12 hours agorootparentprev14 minutes for an entire diskette’s content was magically fast, for the brief window of 14.4’s relevance. reply bombcar 1 hour agorootparent“Time to download your storage” is an interesting metric, and it certainly takes longer to fill a terabyte drive from a 1gb&#x2F;s connection that filling floppies with a modem. reply johnklos 18 hours agoprevThis is awesome. I still keep around an ImageWriter II I found in a dumpster two decades ago because they&#x27;re incredibly reliable and will print no matter what. Ribbons are plentiful and cheap, the printer doesn&#x27;t care about the kind of paper, and even detailed QR codes and barcodes work fine if you blow them up a bit.But what&#x27;s better than a printer that can work with almost any computer made in the last four decades?I just recently got a LocalTalk card for my ImageWriter II, and between this article and a recent one about custom ROMs for Macs (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37471321), I&#x27;m considering having a dedicated m68k machine that can bridge AppleTalk on ethernet with LocalTalk.Good article! reply bombcar 1 hour agoparentI’ve seen some setups for old laser printers (HP LJ IIIp for life!) where a raspberry pie or similar low power computer controls a relay that can turn the printer on and off when not in use, as idle power on those beasts were moderately noticeable. reply spacecadet 21 hours agoprevNice write up! I also have a Mac SE, but I found an Asente Ethernet card and installed that. Then ran ethernet to a small custom pcb that bridges to wireless, all tucked inside the original case.PS, Then run this on a rpi or something, kudos to this repo for dialing back the entire web :) https:&#x2F;&#x2F;github.com&#x2F;tghw&#x2F;macproxy reply blakespot 20 hours agoparentI have a Mac Plus I&#x27;ve expanded to 4MB and expanded with a 512MB SCSI HD and an Asante SCSI-to-Ethernet adapter by which it&#x27;s wired to my LAN. I use an FTP programs to browse my Mac Studio via a little FTP server I&#x27;ve got running on it, if I want to install something new on the Mac Plus. Works pretty well.https:&#x2F;&#x2F;www.flickr.com&#x2F;photos&#x2F;blakespot&#x2F;albums&#x2F;7215760433593...I also have an Imagewriter II hooked to my Apple IIgs, but I did attach it to the Mac briefly for a test print. reply ralphc 15 hours agoparentprevThere&#x27;s the PiSCSI which is a board that hooks in to a Pi&#x27;s GPIO pins and acts like a SCSI interface to Macs. I have one and in addition to SCSI drives it can emulate a DynaTalk SCSI&#x2F;Ethernet interface and use the Pi&#x27;s wifi to talk to the Internet. I&#x27;ve done this with a Mac SE.It also will act like a AppleTalk file server using netatalk, it&#x27;s an easy way to share files with other Macs on your local network. reply gattilorenz 19 hours agoparentprevhttps:&#x2F;&#x2F;github.com&#x2F;tenox7&#x2F;wrp allows you to use modern js as well, as long as you can render images reply lolive 18 hours agorootparentWould this middleware be relevant with Links [1]? I mean, could we browse heavy-on-js websites such as gmail, facebook ? [1]: http:&#x2F;&#x2F;links.twibright.com&#x2F; reply gattilorenz 16 hours agorootparentI don&#x27;t have a first-hand account here, but it should work as long as you run Links in graphics mode (but which machine runs Links in graphic mode and not another more enjoyable browser like, say, Netscape?) reply firecall 14 hours agoprevWow, AsanteTalk!Thats a name I havent heard in a long time!Which reminds me, I was luck enough to be at one of the last pre-Jobs &#x2F; transitional era Apple TechTalks in London for dealers. They showed new Stylewriters I recall, which never got released!Jobs killed all the printers before they shipped, and rightly so! reply donatj 13 hours agoprevI seem to recall my school district in the early-to-mid 90&#x27;s having something like this set up with the very early Macintoshes. The Local Talk cable reminded me of times long forgotten. We could print to any printer in the district from the Chooser.I made very good use of this wasting paper around the district. reply nxobject 2 hours agoparentDo you know how they got multiple AppleTalk segments in different physical locations talking togeether? reply eschneider 20 hours agoprevWithout a doubt, Microsoft Word for Mac v5.1 is the best version of word, barn none. reply rwmj 20 hours agoparentMS Word 6 for Windows was a bit of a sweet spot. Enough features for a word processor without any extra fluff. reply blakespot 19 hours agorootparentI didn&#x27;t use a Windows PC for very long at home, but I did from 1994 to 1998, and I had Word 6 during that time. It was pretty nice.I&#x27;ve certainly use a number of more basic word processors in my time. :-)https:&#x2F;&#x2F;bytecellar.com&#x2F;2016&#x2F;06&#x2F;05&#x2F;a-look-back-at-three-decad... reply blakespot 14 hours agoprevI wonder if a Mac Plus could do this equally well. I am guessing it can. Would be fun to print from the Mac Studio through the Plus to the Imagewriter II. reply stockerta 19 hours agoprevIs the site itself hosted on a Mac SE? It&#x27;s slow as hell. reply cptaffe 17 hours agoparentI run it out of my closet. The blog is a small Go program deployed to a k8s cluster across a few ESXi VMs on an HP DL380 G7, behind a pfSense router also running as a VM and using HAProxy for TLS offloading. My connection is 1gbps symmetric.With no cache on my local network, Chrome reports the page loads fully in ~500ms, using the \"slow 3G connection\" performance tab preset it takes ~4s.If you&#x27;re nowhere near Little Rock, Arkansas; that might be the issue. I don&#x27;t yet use a CDN and it always loads the same resolution image (\"high\" quality JPEG, ~200kb apiece). reply bombcar 1 hour agorootparentThe use of VMs and liver eyes (no iPhone, kubernetes) to run a simple html server abound ancient hardware is kind of amusing. reply rodgerd 16 hours agorootparentprevI mean I&#x27;m in New Zealand and it&#x27;s fine for me. reply Marvo99 18 hours agoparentprevIt&#x27;s not cloud hosted, I&#x27;ll tell you that. LMAO reply prynhart 16 hours agoparentprevRunning perfectly fine for me - loads fast. reply johnklos 18 hours agoparentprevYou say that like it&#x27;s a bad thing. reply stockerta 18 hours agorootparentYes it is, if half the images load only partialy or not at all. reply blakespot 17 hours agorootparentIt loads fast and fine for me in the DC area of the states. reply johnklos 17 hours agorootparentprevWell, that&#x27;s another matter. Slowness might come as a result of brokenness, but slowness by itself doesn&#x27;t mean anything.I host a web site on an LC III+ which is often compiling and multiple tens of megabytes in to swap, so I know what slow looks like:http:&#x2F;&#x2F;elsie.zia.io&#x2F;(the page is about an LC II, but currently it&#x27;s hosted by an LC III+) reply intvocoder 20 hours agoprev [2 more] [flagged] blakespot 19 hours agoparent [–] This was the first domain I&#x27;d ever seen with a .zip extension -- I didn&#x27;t know it was on offer. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author recounts an experience of using vintage Macintosh computers and AppleTalk protocols to recreate an internet browsing experience akin to 1994.",
      "The text provides a guide on installing and configuring Netatalk 2.x, which is a Linux implementation of Apple protocols, and explains the process of getting an old Macintosh computer online.",
      "It also touches upon the utilization of specific browsers, the advantages of using IPv4 Link-Local addressing, and methods of connecting printers to newer Mac devices."
    ],
    "commentSummary": [
      "The article centralizes on integrating a vintage Mac SE and ImageWriter II into a modern Local Area Network (LAN) and revisits the usage of Localtalk and PhoneNET during the '90s.",
      "Participants share cherished experiences with old Macs and peripherals, including the use of vintage printers and their connection to modern networks.",
      "The discussion also touches on browsing the internet using outdated machines and reminiscing on favorite word processing software from the past."
    ],
    "points": 136,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1694967608
  }
]
