[
  {
    "id": 37831062,
    "title": "The largest DDoS attack to date, peaking above 398M rps",
    "originLink": "https://cloud.google.com/blog/products/identity-security/google-cloud-mitigated-largest-ddos-attack-peaking-above-398-million-rps/",
    "originBody": "Jump to Content Cloud Blog Solutions & technology Ecosystem Developers & Practitioners Transform with Google Cloud Contact sales Get started for free Security & Identity Google mitigated the largest DDoS attack to date, peaking above 398 million rps October 10, 2023 Emil Kiner Senior Product Manager, Cloud Armor Tim April Security Reliability Engineer The attack used a novel technique, HTTP/2 Rapid Reset, based on stream multiplexing Hear monthly from our Cloud CISO in your inbox Get the latest on security from Cloud CISO Phil Venables. Subscribe Over the last few years, Google's DDoS Response Team has observed the trend that distributed denial-of-service (DDoS) attacks are increasing exponentially in size. Last year, we blocked the largest DDoS attack recorded at the time. This August, we stopped an even larger DDoS attack — 7½ times larger — that also used new techniques to try to disrupt websites and Internet services. This new series of DDoS attacks reached a peak of 398 million requests per second (rps), and relied on a novel HTTP/2 “Rapid Reset” technique based on stream multiplexing that has affected multiple Internet infrastructure companies. By contrast, last year’s largest-recorded DDoS attack peaked at 46 million rps. For a sense of scale, this two minute attack generated more requests than the total number of article views reported by Wikipedia during the entire month of September 2023. Google mitigated a DDoS attack which peaked at 398 million requests per second The most recent wave of attacks started in late August and continue to this day, targeting major infrastructure providers including Google services, Google Cloud infrastructure, and our customers. Although these attacks are among the largest attacks Google has seen, our global load-balancing and DDoS mitigation infrastructure helped keep our services running. In order to protect Google, our customers, and the rest of the Internet, we helped lead a coordinated effort with industry partners to understand the attack mechanics and collaborate on mitigations that can be deployed in response to these attacks. Hear monthly from our Cloud CISO in your inbox Get security updates, musings, and more from Google Cloud CISO Phil Venables direct to your inbox every month. Subscribe today Generally, DDoS attacks attempt to disrupt internet-facing websites and services, making them unreachable. Attackers direct overwhelming amounts of Internet traffic to targets, which can exhaust their ability to process incoming requests. DDoS attacks can have wide-ranging impacts to victim organizations, including loss of business and unavailability of mission critical applications, which often cost victims time and money. Time to recover from DDoS attacks can stretch well beyond the end of an attack. Our investigation and response Our investigation revealed that the attack was using a novel “Rapid Reset” technique that leverages stream multiplexing, a feature of the widely-adopted HTTP/2 protocol. We provide further analysis of this new Rapid Reset technique and discuss the evolution of Layer 7 attacks in a companion blog. We observed the attack campaign continued over the course of September 2023 We were able to mitigate the attack at the edge of Google's network, leveraging our significant investment in edge capacity to ensure our services and our customers’ services remained largely unaffected. As we understood more details about the attack methodology, we developed a set of mitigations and updated our proxies and denial-of-service defense systems to efficiently mitigate this technique. Since Google Cloud’s Application Load Balancer and Cloud Armor use the same hardware and software infrastructure that Google relies on to serve its own internet-facing services, the Cloud customers who use those services have their Internet-facing web apps and services similarly protected. Industry coordination and response for CVE-2023-44487 Soon after detecting the earliest of these attacks in August, Google applied additional mitigation strategies and coordinated a cross-industry response with other cloud providers and software maintainers who implement the HTTP/2 protocol stack. We shared intelligence about the attack and mitigation methodologies in real time as the attacks were underway. This cross-industry collaboration has resulted in patches and other mitigation techniques used by many large infrastructure providers. The collaboration helped to pave the way for today’s coordinated responsible disclosure of the new attack methodology and potential susceptibility across a multitude of common open source and commercial proxies, application servers, and load balancers. The collective susceptibility to this attack is being tracked as CVE-2023-44487 and has been designated a High severity vulnerability with a CVSS score of 7.5 (out of 10). Google expresses sincere gratitude to all of the cross-industry stakeholders who have collaborated, shared information, accelerated patching of their infrastructure, and rapidly made patches available to their customers. Who is susceptible and what to do about it Any enterprise or individual that is serving an HTTP-based workload to the Internet may be at risk from this attack. Web applications, services, and APIs on a server or proxy able to communicate using the HTTP/2 protocol could be vulnerable. Organizations should verify that any servers they run that support HTTP/2 are not vulnerable, or apply vendor patches for CVE-2023-44487 to limit impact from this attack vector. If you are managing or operating your own HTTP/2-capable server (open source or commercial) you should immediately apply a patch from the relevant vendor when available. Next steps Defending against massive DDoS attacks such as those described here is difficult. With or without patches, organizations would need to make significant infrastructure investments to keep services running in the face of attacks of any moderate size and larger. Instead of bearing that expense themselves, organizations running services on Google Cloud can take advantage of our investment in capacity at global scale in our Cross-Cloud Network to deliver and protect their applications. Google Cloud customers exposing their services using the global or regional Application Load Balancer benefit from Cloud Armor always-on DDoS protection, where attacks exploiting vulnerabilities such as CVE-2023-44487 are quickly mitigated. Even though with Cloud Armor always-on DDoS protection we are able to efficiently absorb most of the hundreds of millions of requests per second at the edge of Google’s network, millions of unwelcome requests per second can still make it through. To protect against this and other layer 7 attacks, we also recommend deployment of Cloud Armor custom security policies with proactive rate limiting rules and AI-powered Adaptive Protection to more comprehensively detect, analyze, and mitigate attack traffic. We provide more technical information on this current wave of DDoS attacks here, and you can learn more about Google Cloud Armor’s DDoS protection here. Posted in Security & Identity Networking Google Cloud Related articles Security & Identity How it works: The novel HTTP/2 ‘Rapid Reset’ DDoS attack By Juho Snellman • 8-minute read Networking Deliver and secure your internet-facing application in less than an hour using Dev(Sec)Ops Toolkit By Lihi Shadmi • 4-minute read Security & Identity How Sensitive Data Protection can help secure generative AI workloads By Scott Ellis • 5-minute read Security & Identity Introducing Google Cloud Firewall Plus with intrusion prevention By Megan Yahya • 3-minute read Footer Links Follow us Google Cloud Google Cloud Products Privacy Terms Help Language ‪English‬ ‪Deutsch‬ ‪Français‬ ‪한국어‬ ‪日本語‬",
    "commentLink": "https://news.ycombinator.com/item?id=37831062",
    "commentBody": "The largest DDoS attack to date, peaking above 398M rpsHacker NewspastloginThe largest DDoS attack to date, peaking above 398M rps (cloud.google.com) 717 points by tomzur 21 hours ago| hidepastfavorite439 comments dang 16 hours agoRelated ongoing threads:The novel HTTP&#x2F;2 &#x27;Rapid Reset&#x27; DDoS attack - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37830987HTTP&#x2F;2 Zero-Day Vulnerability Results in Record-Breaking DDoS Attacks - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37830998 pythonguython 19 hours agoprevWho has an incentive to carry out these DDos attacks? Why would anyone be willing to spend large amounts of money and develop a sophisticated attack against corporate cloud infrastructure? It seems like the only reasonable answer is foreign governments. But still what is the result - you inconvenience American tech companies and their customers for a few hours? This happens all the time, so clearly someone finds it worthwhile. Can anyone help me understand? reply jedberg 18 hours agoparentI&#x27;ve been working on anti-DDOS off and on for 20 years now. The answer is sometimes government actors, but oftentimes scammers in Eastern Europe. They do these big attacks for street cred amongst the botting community.They then use their street cred to get paid by less scrupulous actors to attack their rivals. Sometimes the people paying are governments, sometimes just shady companies. For example last year there was a lot of crypto companies attacking each other&#x27;s websites.Most of the people who do this have a lot of technical skill but not a lot of opportunity to get paid for it based on where they live or the circumstances of their upbringing. reply wly_cdgr 15 hours agorootparentWow, I guess sometimes the real world is as cool as the movies reply Gud 4 hours agorootparentNot cool at all, just a waste of talent. reply hoseja 2 hours agorootparentThey should work to increase ad revenue by subliminal messaging and lootboxization instead! reply jessriedel 18 hours agorootparentprevVery useful, thanks. Do you know roughly what sort of resources, in time, money, and compromised machines, it takes to do something like this? (Order of magnitude.) reply xnx 18 hours agorootparentapprox. 20,000 machines https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37831355 reply southernplaces7 6 hours agorootparentIf this attack takes only 20,000 machines to orchestrate, how is it the \"largest DDoS attack to date\"? It was to my understanding that some botnets and organizations have placed far more than 20,000 machines under their control before or currently. Could you explain a bit better? reply xnx 5 hours agorootparentI think they&#x27;re measuring \"largest\" by the number of requests per second. reply ada1981 17 hours agorootparentprevSo a single machine can do ~ 20,000 rps? reply xnx 17 hours agorootparentDepends, but there seems to be a multiplier effect at play with this attack. A single client request may result in 100x the work for the server. More details here: https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;identity-security&#x2F;how... reply imheretolearn 15 hours agorootparent> Another advantage the attacker gains is that the explicit cancellation of requests immediately after creation means that a reverse proxy server won&#x27;t send a response to any of the requests. Canceling the requests before a response is written reduces downlink (server&#x2F;proxy to attacker) bandwidth.How is this an advantage? Can someone explain please? reply jseutter 15 hours agorootparentIt&#x27;s an advantage because you as a botnet client have made the server side do extra work. You sent two packets, one to request a new connection, and a second to immediately cancel the request. The server on the other hand sees a connection request and does some work like allocating memory and fetching the resource you requested. Once the server starts sending the response back to the client via the reverse proxy, the reverse proxy notices the request is no longer current and just drops the response on the floor. As a result, you made the server do some amount of work and you don&#x27;t have to worry about saturating your internet connection. They call this a magnification attack because for the cost of two requests you made the server do some multiple of work.You could add some smarts to the server or reverse proxy that delays starting work in case a cancellation request quickly arrives. This is probably part of the mitigation work they refer to in the article. reply xnx 15 hours agorootparentprevThe attacking system is shooting a firehose of requests at the target system, but doesn&#x27;t have to deal with handling any responses being sent back to the requesting systems. reply imheretolearn 15 hours agorootparentMakes sense, thank you! reply NoThankYouTho 15 hours agorootparentprevThis is sort of an aside based on something I read in the article but does anyone know why the RFC guidelines say that you should first send an informational GOAWAY that does not prevent opening new streams when gracefully closing a connection?They point out in the article that it&#x27;s a better practice to immediately limit stream creation when you detect abuse - not wait for a round trip to complete first. I&#x27;m sure there&#x27;s a good reason for the original guidelines; I&#x27;m just trying to get it and haven&#x27;t found anything clarifying through Google. Was it specified before the rise of modern attacks? reply Guvante 7 hours agorootparentHow do you act on a GOAWAY you haven&#x27;t received and allowing the server to unilaterally stop supporting things can lead to weird edge cases.After all you could say any client ignoring a GOAWAY is either bugged or malicious but certainly not until you get confirmation they go it. reply KomoD 16 hours agorootparentprevYep: \"a client can send a RST_STREAM frame for a single stream. This instructs the server to stop processing the request and to abort the response, which frees up server resources and avoids wasting bandwidth.\"Pretty clever reply jedberg 17 hours agorootparentprevThey compromise home internet users and&#x2F;or their IOT devices mostly with scripts and malware. So the investment for the scammer is mostly in researching exploits and seeding their malware. Most of them just use exploits created by others, but the best ones with the biggest networks are actually very capable security researchers. Given different circumstances they could probably be highly paid engineers. reply brazzledazzle 16 hours agorootparentSome of their effort goes toward maintaining an exclusive hold on their botnet too. Patching them while maintaining control or blocking the vulnerability they used from being utilized by others. reply gffrd 15 hours agorootparentprevCrypto companies attacking eachothers&#x27; websites?! Color me surprised … reply frozenport 16 hours agorootparentprev>> Most of the people who do this have a lot of technical skill but not a lot of opportunity to get paid for it based on where they live or the circumstances of their upbringing.LOL. No there are plenty of legitimate enterprises as well as opportunity to immigrate. Especially in tech. These guys are just criminals. reply xnickb 16 hours agorootparentHave you tried that yourself? Especially as someone who has the skills but doesn&#x27;t speak the language.I know people who can&#x27;t relocate because of communication issues and&#x2F;or cultural differences.No they aren&#x27;t criminals, but they are definitely underpaid compared to those who managed to relocate. reply sharkoz 15 hours agorootparentIf they operate botnets, I think it&#x27;s fair to call them criminals reply colinsane 6 hours agorootparentprobably, or if they aren’t directly criminals they’re probably facilitating criminals. but if you were very particular about it you could in theory set up in a country which doesn’t have treaties with any of the countries in which the victims operate. reply ExoticPearTree 15 hours agorootparentprevIt is only criminal if the botnets are used to steal something. DDoS-in just for fun is at most an annoyance. reply grog454 15 hours agorootparentYeah, hospitals can&#x27;t stand that kind of thing... reply ExoticPearTree 13 hours agorootparentThey should have better IT.Blaming it on the people that knock them off will not make improve the situation. reply ajcp 10 hours agorootparentSo your logic is: condemning people for criminal behavior is not useful because it de-incentivizes their victims from being vigilant against that criminal behavior. Am I getting that right? replyNikolaNovak 15 hours agorootparentprevNot to condone the DDoS activities in the least, but that&#x27;s just ignorance. Which prosperous country accepts evrn remotely as many legal immigrants as apply &#x2F; would want to move there? And a lot of people &#x2F; political parties are constantly lobbying for less immigration :-&#x2F; reply whimsicalism 18 hours agorootparentprevSeems like attacking Google would be a bad target for street cred as compared to govt websites. reply fnordpiglet 18 hours agorootparentSurely bringing down Google is a bigger technical achievement than some random government website maintained by someone who stumbled into their job after 20 years doing mid level government organizational work. reply ortusdux 17 hours agorootparentHeck, I&#x27;d imagine that making headlines and having Google benchmark your attack would bring some amazing street cred. reply antonjs 15 hours agorootparentDarknet guerilla marketing. Definitely seems to have worked.Now we need the SEO content side: \"How we hit Google with 398M RPS\".\"... you can do this manually, but our product makes it as easy as a sign up and API call. Talk to us about pricing. [Python API example]. reply whimsicalism 17 hours agorootparentprevYes, but they are clearly going to fail to bring down Google. reply jedberg 17 hours agorootparentIt doesn&#x27;t matter if you fail. The cred comes from how much bandwidth and resources you can soak up. reply fnordpiglet 17 hours agorootparentprevWell - they clearly were successful enough to get a thread on hacker news…… reply usefulcat 17 hours agorootparentprevRight, so clearly the ability to bring down Google is not the point. reply rvba 15 hours agorootparentprevAim high and go out with a bang choom reply jedberg 17 hours agorootparentprevNah it&#x27;s even better because they&#x27;re considered capable defenders so it&#x27;s harder.What I&#x27;m not sure of is why Google published this. I can&#x27;t figure out what their strategy is here. We never published about the attacks we absorbed because we didn&#x27;t want them to know our capabilities.Unless this is marketing for Google Cloud? reply vineyardmike 17 hours agorootparentThis is certainly marketing. If they sell DDOS protection, then announcing that they stopped the largest attack ever is an ad. reply frostiness 17 hours agorootparentSounds like a symbiotic relationship to me. The attackers get to advertise their capability for pulling off attacks, and Google gets to advertise their ability to stop them. reply pixl97 16 hours agorootparentAlmost all (but not all) of these attacks are based on some kind of problem that leads to amplification. Advertizing that people should fix these points of exploit help everyone on the internet. reply canes123456 15 hours agorootparentprevFalse flag? :) reply epalm 17 hours agorootparentprevMaybe Google is responsible for the attack, to be able to publish this blog post!reply ignoramous 15 hours agorootparentIf Google truly went rouge, they could turn all those Chrome installs and Android devices into one gargantuan botnet. reply FartyMcFarter 1 hour agorootparentGoogle is already partly rouge, at least in their logo. reply jsnell 15 hours agorootparentprevWhat capabilities did this post reveal the existence of? Not many, beyond it having been mitigated somehow and that it didn&#x27;t cause an outage. The attackers knew that already, because they&#x27;d obviously be able to observe the system during the attack.As for why to write about it, it&#x27;s a new type of attack that resulted in almost an order of magnitude increase in attack size. That&#x27;s interesting and newsworthy by itself, and publishing a concrete number gives people an idea of the size of the problem and the trendlines.This is also something that needed a CVE, so it was going to be very public anyway. If nothing is written about it, at a minimum Cloud customers will be flooding their support reps with questions about whether the vulnerability applies to them. reply omoikane 17 hours agorootparentprev> why Google published thisBesides publicity, there is also link to a list of advisories that may be of interest to other cloud operators and users.https:&#x2F;&#x2F;nvd.nist.gov&#x2F;vuln&#x2F;detail&#x2F;CVE-2023-44487 reply chrisan 17 hours agorootparentprev> Unless this is marketing for Google Cloud?If you read the article, there are plenty of marketing remarks in there to get you to use Google Cloud reply fomine3 7 hours agorootparentCDN is the ultimate solution for DDoS, so any report about DDoS finally become an ad for CDN reply digging 17 hours agorootparentprev> Unless this is marketing for Google Cloud?That seems likely here if they&#x27;re claiming this is the largest DDOS ever. reply ignoramous 15 hours agorootparentprev> We never published...We? Netflix or Reddit? I know for a fact that Amazon doesn&#x27;t. reply jedberg 15 hours agorootparentNowhere that I&#x27;ve ever worked published about attacks. We didn&#x27;t want to validate the attackers.At eBay&#x2F;PayPal we filed patents on our DDOS shield, since it was as far as we knew the first one to exist, but that was about the only public information on it.At reddit and Netflix we didn&#x27;t actually have to deal with it because AWS just absorbed (or mitigated) it before it ever hit us. We only had to deal with L7 attacks, which we had shields in place for. reply aa_is_op 12 hours agorootparentprevThey&#x27;re not attacking Google, per se. Just the Google Cloud platform that hosts govt sites, Discord channels, gaming servers, etc. reply anonacct37 19 hours agoparentprevPR. Attack Google or cloudflare. Wait for them to publish a blog post about the biggest attack ever seen, then tell potential customers of your botnet that you can launch a bigger attack than anyone else and point to the above blog post. reply darkwater 17 hours agorootparentAnd Google and Cloudflare also get good PR because how insanely good their are at deflecting those huge attacks. It&#x27;s a win-win situation here... oh wait &#x2F;s(the &#x2F;s is just on the \"oh wait\" part, not the whole post) reply Jeff_Brown 19 hours agorootparentprevThe botnet is probably the critical thing. Even if the PR (or \"avenge the global south\", or whatever) value might not be enormous, the cost to a bad actor of having other peoples&#x27; computers do something is almost negligible. reply endergen 19 hours agorootparentprevDoesn&#x27;t using your botnet expose your botnet IP addresses&#x2F;devices? reply mrweasel 18 hours agorootparentYes, but currently that has zero consequences. Say you infect 500.000 Windows XP machines or consumer routers, the owners of those devices isn&#x27;t going to be informed, nor is their ISPs. In many cases the manufacturer of those devices also aren&#x27;t going to provide security update, but those probably wasn&#x27;t going to be applied anyway. reply jrockway 18 hours agorootparentAre you positive that \"tell nobody\" is the mitigation strategy that Google used here? They could have easily asked router vendors to patch their devices, asked ISPs to blackhole those customers until they&#x27;re patched, etc. reply soperj 18 hours agorootparentPatch what though? They know that they&#x27;re getting hit with unprecedented traffic, not how those computers were infected. reply menscher 18 hours agorootparentIt&#x27;s mostly not infected computers, but rather poorly configured proxies that are open for anyone to bounce malicious traffic through. Convincing everyone to clean up their open proxies is a long-term, hard problem. But I plan to tackle it soon.... reply superjan 17 hours agorootparentHow? I suppose the most effective way is to have those proxies attack each other. But don’t, it’s likely illegal. reply menscher 10 hours agorootparentGet a few companies to agree that open proxies are a scourge that needs to be stopped. They each apply some action to open proxies (user-facing messaging, loss of functionality, captcha, or complete block), and the users of those proxies will get the problem fixed.The hard part (and it truly is hard!) is convincing a few companies to do this. It risks user complaints in the short term, to solve a problem that may not be very acute for the largest companies (who can simply absorb these attacks). reply soperj 17 hours agorootparentprevthe most efficient way would be to write a script that gains root on those open proxies and then fixes the issue. reply wsintra2022 15 hours agorootparentEffective or efficient? Would seem rather inefficient to spend time researching all the possible ways to gain route on x number of servers, finding an exploit, crafting some plan to execute it, keeping your prints clean etc etc reply soperj 15 hours agorootparentWhat way would be more efficient? replyExoticPearTree 15 hours agorootparentprevSo you&#x27;re saying Google and Cloudflare, just as an example, should block consumers of other ISPs because they run \"unpatched\" software or they have malware running on their devices? Lol, this is a very absurd and narrow minded view how the internet works. You deal with the traffic, you don&#x27;t randomly block eyeball networks because they&#x27;re attacking you. reply c0pium 9 hours agorootparent> you don&#x27;t randomly block eyeball networks because they&#x27;re attacking you.ISPs do this literally all the time. They sell services that do this. reply mensetmanusman 17 hours agorootparentprevGoogle should start using their ad network to silently update people’s security! reply qup 16 hours agorootparentUh, no thanks from this user.Also, sounds illegal. reply lazide 16 hours agorootparentDefinitely illegal in the US. reply KomoD 18 hours agorootparentprev> the owners of those devices isn&#x27;t going to be informed, nor is their ISPsnot necessarily true reply WelcomeShorty 17 hours agorootparentBut these ISPs that give something and inform and even isolate their infected customers are few and far between.Shout out to Dutch ISP XS4ALL who was (is?) very very strict and active in this space. reply KomoD 18 hours agorootparentprevAnyone can claim that, there&#x27;s no link to a specific actor reply v-erne 18 hours agorootparentI&#x27;m guessing you would do this in advance - \"pay attention to tech news next week - our botnet will unleash hell\" reply jekude 16 hours agorootparentprevStep 1: Put message on blockchain beforehand with exact date&#x2F;time and characteristics of DDoSStep 2: Execute DDoSStep 3: Prove to others you are responsible by using private key reply c0pium 9 hours agorootparentThere’s (as ever) no need for a blockchain, people do this with twitter and sha256 all the time. Hash the message, post the hash, wait for prediction to happen, post the full message. reply daveevad 8 hours agorootparentAny examples of major predictions verified in this way? reply ehsankia 16 hours agorootparentprevWell in this case it seems like they blew their \"0-day\" and Google worked with other providers to patch this type of attack. reply gowld 17 hours agorootparentprevWhy not attack a target that can actually be harmed? Are they afraid?It&#x27;s not obvious what&#x27;s the value of having the largest ineffective attack. reply lazide 16 hours agorootparentWhy not roll a couple defenseless grannies in the streets for pocket change, rather than throw rocks at the cops and then get away unscathed?One gets you more money in the short term. The other one gets you more street cred - which gets you more money in the long term. reply forward1 19 hours agoparentprevIf they&#x27;re unsophisticated, it&#x27;s for clout and \"street cred\" in hacking communities, no different than tagging a freeway overpass with graffiti.If they&#x27;re advanced, they are doing it to test capabilities and responses. The Taliban used to pay kids to light off firecrackers outside base to check defensive TTPs. It also had the effect of desensitizing the sound of gunfire.Really good adversaries know how to accomplish the latter while appearing as the former. reply karaterobot 17 hours agoparentprevYou can (or could, my information is old) pay botnet owners a few hundred bucks to disrupt the servers of people you don&#x27;t like. An example would be ruining a match for a competing game clan. There&#x27;s a suprising amount of this kind of petty bullshit going on in the world.With the Mirai botnet, some of the creators had a DDOS mitigation company as well: they&#x27;d sell one party the weapon, and sell another party the defense against that weapon.Sometimes it&#x27;s for the street cred, or the lulz, or just the challenge of building a botnet. reply red-iron-pine 17 hours agorootparentDisruption is part of it for sure, but often big, aggressive DDoS come hand-in-hand with other attacks.Seen it happen with big DDoS on clients. Furaffinity, one of the larger furry webistes, and a constant drama magnet, was a client at a former job. They got DDoS&#x27;d hard, and in between scripted DDoS hits they slammed the hell out of their web applications to get vulns and do credential stuffing.As in blast em, lighten it up just enough to get a ssh or nmap through a few times, blast em again, and repeat until they got in.Is also why you want out-of-band solution that doesn&#x27;t touch your infra much. reply somedude895 13 hours agorootparent> in between scripted DDoS hits they slammed the hell out of their web applicationsI&#x27;ve heard this before, but I still don&#x27;t understand what purpose the DDoS serves here. Distraction, so the actual attack drowns in the noise? reply rockinghigh 18 hours agoparentprevFor a recent and similar attack at scale, the authors of the botnet software were from an American security company who sold DDOS mitigation solutions (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mirai_(malware)). reply chatmasta 10 hours agorootparentGoogle keeps showing me a CAPTCHA and telling me I&#x27;m part of the Mirai botnet. I&#x27;m using Cloudflare WARP so I have an IP from Cloudflare. reply datadeft 18 hours agoparentprevFor example a certain group decides to short on a share. The DDOS the company and \"leak\" it to the press. The bad press negatively impacts the share price. At least this was the way some time ago when I had to deal with such attacks. reply belter 18 hours agoparentprevYou don&#x27;t need a lot of money or resources to pull one of these. Code is on Github: https:&#x2F;&#x2F;github.com&#x2F;649&#x2F;Memcrashed-DDoS-ExploitAlso the participants are sometimes innocently recruited victims for the attack. I blame app insecure defaults.The trend since 2015 is to get worst as you will see in the bottom layer of this graph: https:&#x2F;&#x2F;www.digitalattackmap.com&#x2F; reply whimsicalism 18 hours agorootparentThis is a novel ddos attack. Did you all even read the article? reply belter 17 hours agorootparentI did. I replied to OP question.And that was about DDos attacks in general not \"HTTP&#x2F;2 Rapid Reset attacks\"> Who has an incentive to carry out these DDos attacks?Did you read the comment I was replying to? reply whimsicalism 17 hours agorootparentYes, “these attacks” referring to the sophisticated novel attacks under discussion in the article. No need to be defensive, just read it next time. reply belter 17 hours agorootparentAre there DDoS attacks that are not sophisticated in form or execution? :-) reply pizzafeelsright 15 hours agorootparentping replygloosx 4 hours agoparentprevfew hours of inconvenience in corporate cloud infrastructure has a result also - large amounts of money lost. So most obvious incentive is simply ransom reply danwee 16 hours agoparentprevGoogle?If you analyze the situation from the perspective of \"Who benefits from it?\", then the answer is clearly: Google benefits from it (they are so good, they can mitigate gigantic DDoS attacks). So, I don&#x27;t think it&#x27;s that crazy to think this is all a publicity stunt . reply bimguy 5 hours agorootparentI was thinking the same thing. Especially when the the bottom of the article is basically a sales pitch for their own services. Whilst what they are saying is true and correct even if they aren&#x27;t behind it (which I really doubt that they are the culprit).\"With or without patches, organizations would need to make significant infrastructure investments to keep services running in the face of attacks of any moderate size and larger. Instead of bearing that expense themselves, organizations running services on Google Cloud can take advantage of our investment in capacity at global scale in our Cross-Cloud Network to deliver and protect their applications.\" reply jxramos 18 hours agoparentprevI&#x27;ve never contemplated the cost of a DDos attack, I guess there&#x27;s the upfront setup costs to secure the software and hardware that will execute the attack but are you speaking more about the costs on the day of the attack? Are those costs trivial, like the marginal costs I suppose it would be? reply WelcomeShorty 17 hours agorootparentTo be effective, you need to either be prepared to hide behind google, Cloudflare or AWS, OR you need some pretty expensive deal with you (large) ISP who can (quickly) filter on their edge.Sitting at the end of whatever network, you will not be able to do anything against a sufficient volume attack. reply permo-w 13 hours agoparentprevI&#x27;m just guessing here but it could easily be stock market manipulation reply GartzenDeHaes 10 hours agoparentprevDDOS protection is a billion dollar a year industry. reply CrzyLngPwd 14 hours agoparentprevWhy do you think Finland or Spain might attack USA companies? reply pythonguython 8 hours agorootparentI don’t think either of those countries would attack US companies. Obviously I would suspect adversaries instead of allies reply ExoticPearTree 15 hours agoparentprevIf I had the technical prowess to do this, I would do it just for the fun of it. I mean, why not? Anarchy is fun.I&#x27;m pretty sure someone will find a way to take down GOOG&#x2F;AWS&#x2F;Azure&#x2F;etc through a DDoS so large nothing will work for anyone. reply RektBoy 14 hours agoparentprevI mean you can ask them https:&#x2F;&#x2F;t.me&#x2F;s&#x2F;noname05716eng chat of NoName group skids doing some DDOSing, they pool their bots by community of like-minded people (Russians supporting current government, murder rape etc. you know) reply syndacks 18 hours agoparentprevfor the lolz reply xeromal 18 hours agoparentprevMy gut instinct is that this is a nation-state initiated. reply oldtownroad 21 hours agoprevAt a previous company, we were subject to semi-frequent attacks (of a much smaller scale). The operating assumption internally was that it’s a competitor trying to undermine us but it remains a mystery.Anyone involved in these type of attacks (at internet-infrastructure scale or targeting specific companies) brave&#x2F;crazy enough to create a throwaway account and tell hn about the motivations? reply OsrsNeedsf2P 21 hours agoparentWe had a similar issue and assumed it was script kiddies having fun. Turns out someone (raises hand) wrote a really bad microservice who&#x27;s inefficient queries sometimes triggered all our alerts. reply ilyt 20 hours agorootparentWe had that except it was our own frontend developers.We also had some actual attacks so we made a system that detect anomalies (like more than 50rps per IP) and raises alert....which was thwarted by frontend developers again, as they loaded few hundred tiny icons at once that triggered that alert routinely, and only thru http2 multiplexing their idiotic design patents haven&#x27;t bitten them before. reply jopsen 12 hours agorootparentprevHehehe, entirely unsurprising :) reply arein2 20 hours agoparentprevA local hosting company ddosed local bussineses that had IT infrastructure and then advertised their hosting solution with ddos protection. reply logdahl 17 hours agoparentprevThe universities in Sweden were attacked by \"Turkey\" after the big quran-burning scandal. They had some twitter account bragging about it. Was pretty evident it was Russia. reply codezero 8 hours agoparentprevI had a customer that was getting DDoSed by competitors, the competitors likely didn&#x27;t know they were doing a DDoS, as they were aggressively scraping product listings but just doing so without any delay&#x2F;rate limiting and it effectively DDoSed them. They weren&#x27;t trying to make the target site slower, but were trying to get data at a rate that made the target&#x27;s servers uneconomical to their actual paying customers.This kind of attack is nothing like the actual DDoS attacks, but it&#x27;s a lot more common in my experience, but also relatively easy to mitigate with something like Cloudflare or Akamai (which is what I&#x27;d recommend to my customers). reply elorant 20 hours agoparentprevI&#x27;ve heard stories about attacks where the target is a subsystem but in order to avoid drawing attention to it they attack the entire network. reply stepupmakeup 18 hours agoparentprevprotection rackets by companies you&#x27;d only find on places like lowendtalk reply dudeinjapan 21 hours ago[flagged]| parentprevnext [22 more] Sure, I’ll spill the beans. Some people think it’s related to Gaza or Ukraine but it’s not. We just really don’t like Google, we are trying to shut it down so we can bring back Altavista. reply falcor84 21 hours agorootparentnext [17 more] Made me wonder - if Google wasn&#x27;t there and Altavista was the incumbent, would it be any different, or was the enshittification of search inevitable? reply Frost1x 19 hours agorootparent>... or was the enshittification of search inevitable?My bet is on the latter. Enshitification is a direct product of greed. No crafts person or creator I know of goes into something they enjoy creating with the intent to make it this monstrosity of money extraction. Most creators have a drive for their creation to be shared and experienced by many.Yes, you may want to get a reward in the process and for some creators, their motives may change over time if they see an opportunity to turn their creation into a wealth machine for themselves so they can do whatever after.Enshitification I believe is a secondary effect of something that becomes successful for the owners of something and either they or others change motives towards value. Optimization is no longer about the creation, sharing, experience, humanitarian, whatever motive and shifts to money. The second that becomes the goal, enshitication is just part of the optimization journey. In my line of thinking, it&#x27;s the same reason monopolies or near monopolies tend to form, these are merely further states along optimization strategies in the monetary&#x2F;wealth extraction goal.Part of that process is that when something starts to succeed, it attracts people with these goals so the goal of something shifts pretty rapidly. reply andrei_says_ 8 hours agorootparentThis is beautifully written, thank you. As a creator, my money ambitions fit between “does this thing provide enough value to pay for itself” and “could this provide enough value to pay for my needs”.The drive to create the thing comes from sincere enthusiasm + excitement that simply don’t leave me alone.Shifting the thing’s purpose from the original one that filled me with enthusiasm to the purpose of simply extracting money, corrupts its original purpose.I call this corruption.it is my observation that one kind of people create, and another kind of people corrupt.Unfortunately the economic system we have accepted as normal, leans toward corruption and extraction. reply nordsieck 20 hours agorootparentprev> if Google wasn&#x27;t there and Altavista was the incumbent, would it be any different, or was the enshittification of search inevitable?You might not remember this, but before Google, paid search placement was par for the course. One of Google&#x27;s innovations, one of the things that really endeared it to users was clearly labeling their ads.So, yes - it was inevitable. And, in fact, Google probably staved it off at least a decade; maybe more. reply dudeinjapan 21 hours agorootparentprevnext [8 more] Was at Tokyo Disneyland today and taught my girlfriend the word “enshittification”. (i.e. making your customers pay via your stupid app to do literally anything in your park, and not even providing wi-fi.) reply resfirestar 20 hours agorootparentThat&#x27;s not enshittification, squeezing money out of you is just how theme parks operate. The term can&#x27;t really apply to Disney parks at all because there&#x27;s no two sided market. reply Grazester 20 hours agorootparentYou missed the no WiFi part. At least enable customers to send their money! reply resfirestar 17 hours agorootparentThat’s shitty, but if that’s all enshittification means then it’s ten extra letters for nothing. Disneyland is not a “platform”, it doesn’t go through the enshittification process. reply wddkcs 7 hours agorootparentArguably everything driven by capitalism is geared towards enshitification. Maximum extraction for minimal effort is a hard paradigm to beat reply seanmcdirmid 20 hours agorootparentprevI don’t remember paying for anything at Disney Sea with the app except for a few fast passes (and used to schedule the free fast passes of course). Suica card and credit card worked for everything else. reply ilyt 20 hours agorootparentprevSo you taught her wrong, that&#x27;s not what it means... reply Ylpertnodi 20 hours agorootparentprevThat&#x27;s double-dipping? reply toast0 19 hours agorootparentprevBefore Google, new search engines became crappy after 6-12 months, maybe two years tops.It&#x27;s not surprising Google search is now crap, it&#x27;s what happened to all the old search engines. It&#x27;s only surprising that it took 15-20 years (depending on perspective), and in the mean time, they&#x27;ve developed a big ecosystem of other stuff. reply wmeredith 19 hours agorootparentprevI&#x27;d say enshittification is inevitable. It isn&#x27;t a technology issue, it&#x27;s human issue. Imagination and desire are what brought us this far and also what holds us back. See also: the tragedy of the commons, the prisoner&#x27;s dilemma, the trolley problem, etc. reply lock-the-spock 19 hours agorootparentprevAs someone old enough to remember: one of the main reasons Google won was that the other engines (shedding here a tear for Lycos) simply couldnt handle the increasing amount of web spam. They were built in a trusted web environment, but suddenly things became cheap enough for less scrupulous people to start creating effectively spam sites, and the engines somehow didn&#x27;t manage to react in time. reply seanmcdirmid 21 hours agorootparentprevThere is a reason Google’s first office was right next door to DEC WRL and Alta Vista. There is so much cross contamination between the two that it’s impossible to say. reply kps 21 hours agorootparentprevAltavista started turning to shit as soon as it was no longer an Alpha demo. That was a major reason Google took off so quickly. reply kps 21 hours agorootparentprevI miss boolean search operators. reply mortallywounded 21 hours agorootparentprevSounds like something those dogpile folks would do. reply c7DJTLrn 20 hours agorootparentprevI think the plan went horribly wrong, everybody started using Bing again! reply anjel 8 hours agorootparentBing aka DDG as if... reply dduarte 21 hours agoprevSame attack on Cloudflare https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;zero-day-rapid-reset-http2-recor... reply Aissen 21 hours agoparentThe technical article (linked in the post) has more interesting details: https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;technical-breakdown-http2-rapid-... reply H8crilA 19 hours agorootparentThis should be the top comment.TL;DR: HTTP&#x2F;2 is internally concurrent, can handle multiple streams. It is possible in HTTP&#x2F;2 to send a nasty request that looks like so: - GET x1 - GET x2 - GET x3 - ... - GET x100 - Actually, cancel all of the above (uses multiple RST_STREAM frames) - GET x101 - GET x102 - (...) - GET x200 - Actually, cancel all of the above (uses multiple RST_STREAM frames) - (...)This can be repeated a lot of times. The problem is that the endpoint, which typically is a reverse proxy, might start dispatching the requests before it reads about their cancellation. And sure it will cancel them, but by the time of cancellation it will already have resulted in some resource usage downstream. Such requests are accepted because at no point the client has opened more than 100 streams, which is the typical concurrency limit. The example from the blog manages to squeze in a single packet 1000 GETs (i.e. 1000 HEADERS) correctly interleaved with RST_STREAM.Maybe it&#x27;s just me, but it&#x27;s always fun to see such creative and simple abuses of protocols&#x2F;code. reply ryanisnan 17 hours agorootparentThat’s pretty fascinating. This is a naive solution, but couldn’t the protocol have supported limits of requests per packet? I get that it is antithetical, but for most sites, this type of request pattern seems highly unusual. reply AtNightWeCode 16 hours agorootparentprevIf this is true than the design is problematic. What makes it even worse is that cancellation of requests typically does not work in cloud environments. It is a bit laughable that Azure for instance recommend the use of cancellation tokens but in reality you never get them for web requests. reply carstenhag 15 hours agorootparentLook at F5&#x27;s entry regarding this CVE. They specifically mention they have set a safer limit because they expected this to be an attack vector, haha replyglenngillen 14 hours agoprev> We noticed these attacks at the same time two other major industry players — Google and AWS — were seeing the same.Curious if there&#x27;s anyone in the HN crowd that works at this level in one of the major vendors. What happens during an attack of this scale? Are there people from Cloudflare + Google + AWS on a live videoconference call co-ordinating with each other in real-time to mitigate it? Or is each vendor mostly observing from a distance what is happening elsewhere, and solely focussed on sorting their own problems out? reply menscher 10 hours agoparentWe typically fight our own fires, but if one of us sees something interesting&#x2F;new we often ask others (after the fire is out) if anyone else saw a similar attack (which could be a new botnet, a new attack method, or whatever). In this case we realized we were all looking at the same thing (which could have huge impact on smaller targets), so collaborated on understanding the problem and coordinated the security response with all webserver vendors. reply sph 21 hours agoprevHow does DDoS mitigation work? When people say \"I put my website behind Cloudflare to mitigate DDoSes\", what does it mean exactly?Is it only about having a large enough ingress pipe that you can weather however many Gb&#x2F;s you are being bombarded with, and still having some spare capacity for legitimate traffic? reply MayeulC 21 hours agoparentIt is about that and a lot of other things, but it usually involves being able to dynamically scale up your bandwidth and compute power to cope with the incoming flood.A lot of DDoS traffic isn&#x27;t actual HTTP traffic, it can be garbage targetted at your IP address to \"fill the pipes\" (bigger pipes help, as well as having multiple server geographically distributed). Some can be TCP SYN flood, to just open TCP connections and exhaust available ports. Etc. Oftentimes, multiple simple reverse proxies can handle these malformed requests in front of your server.Then, for the most sophisticated queries that send seemingly-legitimate HTTP traffic, one has to handle them... It could be serving requests from a cache, adding captchas to slow attackers and identify legitimate traffic, enforcing rate limits, etc. Usually, you&#x27;d like to be able to tell if a request is legitimate or not before forwarding it to the actual server, and you can deploy all sorts of tools to do so. reply toast0 18 hours agorootparent> it usually involves being able to dynamically scale up your bandwidth and compute power to cope with the incoming flood.I don&#x27;t think this is right. If you have a meaningful amount of bandwidth, dynamically scaling it is getting a connection upgraded in weeks instead of months. If you don&#x27;t have a meaningful amount of bandwidth, you&#x27;re rely on your provider(s) to have enough bandwidth and again, they can&#x27;t expand quickly.> Some can be TCP SYN flood, to just open TCP connections and exhaust available ports.If you have a tcp stack from maybe 2003 or later (so excluding macos, unless they changed something in the past four years), it will have synflood protection, with syncookies. In the event of a heavy synflood, your system will send at most one syn+ack per incoming syn, and actually accept connections on the incoming ack. Yes, you miss out on detailed tcp options, but it&#x27;s not that big of a deal, unless the volume impacts your available bandwidth.Also, as a tcp server, you can&#x27;t meaningfuly run out of ports; your one listen ip:port can connect to all ip:ports, if you have the memory for it. You&#x27;ll probably run out of total accepted sockets, but there&#x27;s no real resource limit on partially accepted connections, because of syncookies. It can be much more draining when DDoS clients actually hold connections. But it&#x27;s often simply about volumetrics, and it&#x27;s easier to generate a high volume of SYN packets than to hold a connection. reply richardwhiuk 21 hours agoparentprevCloudflare, and other companies, can detect that requests are DDoS and either drop, throttle, or verify the traffic, instead of forwarding it your server.You configure your server to drop all traffic which wasn&#x27;t set by cloudflare, which is efficient. reply tazjin 20 hours agoparentprevBack when I was in Google SRE, people would joke that \"we just send DDoS traffic to Australia\".In general, Google&#x27;s internal cross-DC traffic is so much larger than anything anyone could DDoS them with that they can always find a way to deal with it. reply jart 47 minutes agoparentprevYou use algorithms like token bucket. reply detaro 21 hours agoparentpreva) Big pipesb) ability to filter the noise from real traffic as far as possible (i.e. there is little point in taking in a big pipe of DDoS traffic and then just proxying it to the thinner pipe to the real backend - but if you can identify bad traffic you can drop it and not pass it through).c) being a CDN helps as a side-effect (what the CDN serves doesn&#x27;t load the backend services, what can be served from the CDN works for users even if the backend is slow or down) reply dharmab 21 hours agoparentprevOften it means automatically recognizing DDoS requests and handling them in a way that is less costly, without impacting legitimate users.In this case, it might mean recognizing when a client rapidly resets streams, and either moving that traffic to a slow lane or filtering it entirely. reply SteveNuts 21 hours agoparentprevWhen the ddos attack is volumetric, the only way to mitigate it is to have a fat enough network to handle the traffic while you work with ISPs to start blocking the traffic upstream.Not all ddos attacks are based on volume though, some are exploiting native features of a protocol, like the slow loris attackhttps:&#x2F;&#x2F;www.cloudflare.com&#x2F;learning&#x2F;ddos&#x2F;ddos-attack-tools&#x2F;s... reply dijit 21 hours agorootparentthat&#x27;s not the only way.The way we used to do it is have \"filter boxes\" with a real anycast IP address&#x27;s which reverse connect to your origin.This helps a lot because it keeps a lot of traffic localised instead of allowing it to collect in one place. Anycast should also mean you have a failover mechanism; but if it fails then you&#x27;re only down in one section of the world where the most bots are anyway, which is usually not as bad as being down globally. reply papichulo2023 20 hours agoparentprevI always believed that they have some secret mega routers with massive computation limits that allows smart and complex tcp&#x2F;udp packages filtering. reply sschueller 20 hours agorootparentThey do have special equipment at the edges like: https:&#x2F;&#x2F;www.netscout.com&#x2F;arbor reply jruohonen 21 hours agoparentprevCDN&#x2F;SDN. reply adzm 21 hours agoprevLinked in this article is more info on the rapid reset feature of HTTP2 which was used at part of the ddos https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;identity-security&#x2F;how... reply ricardobeat 21 hours agoprevNo word on the origin of these attacks? This must require massive amounts of hardware, you’d imagine it to be easily traceable unless some kind of botnet. reply tyingq 21 hours agoparentThat&#x27;s the particularly bad news, this attack does NOT require a really huge botnet.https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;zero-day-rapid-reset-http2-recor...\"Furthermore, one crucial thing to note about the record-breaking attack is that it involved a modestly-sized botnet, consisting of roughly 20,000 machines\" reply grotorea 21 hours agorootparent20000 being modest really says a lot about the state of security on the Internet. reply arp242 19 hours agorootparentThere are 5 billion people on the internet. This is 0.0004%. Even 2 million is only 0.04%.(this assumes that 1 person = 1 device; some people share devices, most people have more than one, e.g. I have a laptop and a router, many people also have a phone, a work laptop, and whatnot – the average is probably >1, maybe even >2) reply londons_explore 20 hours agorootparentprevDistribute just one warez game with your malware embedded and you&#x27;ll have well over 20,000 hosts under your control. reply mattigames 15 hours agorootparentIs there any major popular account that distributes cracked games that has been found to do such thing? I have seen some popular accounts that create their own installers (\"repacks\") and the installation takes a suspiciously long time and a huge amount of RAM while is installing. reply superhuzza 20 hours agorootparentprevOr does it say more about the sheer number of devices connected to the internet? reply tyingq 20 hours agorootparentprevWell, 20000 to hit 201 million requests per second and give Cloudflare problems. You wouldn&#x27;t need that to make problems for many sites. reply paulddraper 20 hours agorootparentprev*the size of reply danpalmer 21 hours agoparentprevOne could imagine that, given the size, it could be politically or legally sensitive to announce the origin. reply persedes 21 hours agorootparentLooking at the scale of those that&#x27;s what I figured too, but one of the previous largest ones (mirai) was targeting a minecraft server (...). Krebs has some interesting write ups on Mirai. reply knorker 20 hours agorootparentprevCloudflare explicitly says it&#x27;s an unknown threat actor: https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;zero-day-rapid-reset-http2-recor... reply ricardobeat 21 hours agorootparentprevThe silence is actually already giving it away then, one of four options. reply blagie 21 hours agorootparentEnumerate, please. reply KolmogorovComp 21 hours agorootparentI assume China, Iran, North-Korea or Russia (in alphabetical order). reply lionkor 21 hours agorootparentprevUS enemy one, two, three and four (whoever is trendy to blame right now) reply tootie 20 hours agoparentprevThe immediate assumption is that Iran is doing it. They have done it many times before and they are allied with Hamas. I haven&#x27;t seen any proof but it&#x27;s a safe bet. reply londons_explore 20 hours agorootparentA novel attack like this done at small scale like this is probably just a script kiddie experimenting.An actual nation state would have tested it fully internally and started on the public internet at a scale bigger than 20,000 machines. reply bjacobel 15 hours agorootparentprevThis happened in late August and early September. reply dominicdoty 19 hours agoprevCouldn&#x27;t cloudflare show a page to the next handful of http requests from an IP informing the user that \"something on your network is participating in DDoS attacks\".All the big providers could do this, just inject a little turnstile like page in front of the next cloudflare site you visit.I would love to know if there&#x27;s a compromised device on my network, and I don&#x27;t have any real monitoring set up to detect it.It&#x27;s not a full solution, but at least informing users there is a problem is a good start. reply BitPirate 4 hours agoparentIn the age of CGNAT? Not a good idea. reply tuananh 20 hours agoprevCloudflare blog: https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;zero-day-rapid-reset-http2-recor... reply mihaic 21 hours agoprevThe fact that large cloud providers can handle huge DDoS attacks I think in the long run leads to a worse internet. It forces botnets to up their game and for websites the only solutions available are to pay Google, Amazon or Cloudflare a protection tax.I honestly don&#x27;t see any other options, but I&#x27;d really wish for them to come through some community coordinated list of botnet infected IPs or something. reply gchamonlive 20 hours agoparentWhat?Let&#x27;s go back to username and password. 2FA forces scammers to up their game.What about password managers? Having separate passwords to every account makes hacking into your accounts much harder and might hurt everyone in the long run.And don&#x27;t get me started on end to end encryption. Privacy, long term, will mean the fall of civilization.Sarcasm aside. I think I understand your point in which we shouldn&#x27;t just delegate to cloud providers the whole effort in preventing attacks, but just with everything production-grade, the average enterprise just isn&#x27;t ready to deal with all the upfront cost to run your entire computing solution. Because it doesn&#x27;t end with this type of mitigation and dependency. A similar argument could be made for not using proprietary chip designs made by cloud providers. Or any proprietary API solution for that matter. It really is a matter of convenience that a community solution might cover in the future, abstracting away fundamental building blocks every cloud provider must have (name resolution, network, storage and computing services) to provide such higher level functions without lock in. We are just not there yet. reply tristan9 19 hours agorootparent> just with everything production-grade, the average enterprise just isn&#x27;t ready to deal with all the upfront cost to run your entire computing solutionThat’s not a fair point.We’re not even trying to make the internet safe. There is zero (0) actions being taken to stop this madness. If you run a large website, you still regularly see attacks from routers compromised 3, 4, 5 years ago. Or how a mere few days of poking around smartly is still enough to this day to find enough open DNS resolvers to launch >500Gbps attacks with one or two computers.Why are these threats allowed to still exist?The only ones attempting something are governments shutting down booters (DDoS-as-a-service platforms). But that’s treating symptoms, not causes.We will eventually need to do something, or it will be impossible to run a website that can’t be kicked down for free by the next bored skid.Just like paying protection fees to the mafia was a status quo, this also is just that. A status quo, not an inevitability.The solution is to finally hold accountable attack origins (ISPs, mostly), so that monitoring their egress becomes something they have an incentive to do. reply Analemma_ 19 hours agorootparentI don&#x27;t think it&#x27;s true that 0 actions are being taken. When new vectors for amplification attacks are found, they get patched - you can&#x27;t do NTP amplification attacks on modern NTP servers anymore, for example. But it takes a long time for the entire world to upgrade and just a handful of open vulnerable servers to launch attacks. And in the meantime people are always looking for new amplification vectors.> The solution is to finally hold accountable attack origins (ISPs, mostly), so that monitoring their egress becomes something they have an incentive to do.Be careful what you wish for. The sort of centralized C&C infrastructure and \"list of bad actors everybody has to de-peer\" that you would need to this effectively would we a wonderful juicy target for governments to go, \"hey, add [this site we don&#x27;t like] to the list, or go to prison\". reply handoflixue 14 hours agorootparent> \"hey, add [this site we don&#x27;t like] to the list, or go to prison\".Aren&#x27;t there already a dozen or so such lists? I don&#x27;t see how one more list really increases the risk.You can make the list public - most of the bad actors are obsolete, compromised equipment for which the owner is unaware of the problem. Once the list is public, it&#x27;s pretty easy to detect anyone trying to abuse the list as a tool of censorship. reply tristan9 9 hours agorootparentprevIP reputation is already a thing. And plenty enough ASNs are well-known for willfully hosting C2 servers and spam, DoS, etc sources… reply BHSPitMonkey 18 hours agorootparentprevTraditionally, a botnet can be compromised (at least largely) of actual consumer devices unknowingly making requests on their owners&#x27; behalf. This can cover hundreds of unrelated ISPs as the \"origin\" and is effectively indistinguishable from organic traffic to a popular destination. \"Accountability\" is not simple here. reply tristan9 9 hours agorootparent> Traditionally, a botnet can be compromised (at least largely) of actual consumer devices unknowingly making requests on their owners&#x27; behalf.And I do count that in.Just because a user is the source of an attack unknowingly doesn’t make it right.What would make it right is for there to be a more generalized remote blackholing system in place.ie my site runs on an IP, is able to tell my ISP to reject traffic to it from $sources, and my ISP can send that request to the source ISP.And if it makes my site unavailable to that other ISP because of CGNAT and 0 oversight, tough luck. Guess their support is getting calls so maybe they start monitoring obviously abusive egress spikes per-destination. reply ryanisnan 17 hours agorootparentprevI like the irony of saying there are zero actions being taken in response to a blog post documenting actions taken to specific CVEs. reply tristan9 9 hours agorootparentThese blogposts document the attack. Documenting it and acting in it are different.There’s no practical action being taken besides « use our profucts cause we can tank it for you » here.The mitigations listed are better than nothing, but the fact that every skid out there can hire a botnet of a few thousands compromised machines (like here) and send you a few millions (say this protocol attack allowed a 100x higer than avg impact) rps is way enough to kill the infra of 99.99% websites. No questions asked. reply Roark66 18 hours agorootparentprev>There is zero (0) actions being taken to stop this madness. If you run a large website, you still regularly see attacks from routers compromised 3, 4, 5 years agoYes, you&#x27;re 100% correct. Back in the day when the main bot net activity was spam if you were infected and you started sending TB of spam the ISP would first block your outgoing smtp. If they kept getting complaints in a week or two they&#x27;d cut you off.I remember 30 years ago when most people were on dialup, I was fortunate enough to have 128kB SDSL. As a relatively clueless kid I decided to portscan an IP range belonging to a mobile service company. Few days later my dad got a phone call saying their IDS flagged it and \"don&#x27;t do it or we&#x27;ll cancel your service\". For a port scan of few public IPs no less!ISPs could definitely put a stop to 99% of these botnets, but until they see some ROI, why would they bother? reply jsight 20 hours agorootparentprevBut that&#x27;s exactly the problem, it shouldn&#x27;t require a enterprise grade tool just to host a simple website on the internet. We&#x27;ve lost something due to our inability to stop attacks at the source and heavy overreliance on massive cloud providers to do it for us.2FA and password managers didn&#x27;t make us heavily reliant on massive companies. reply meowface 20 hours agorootparentYes, but if these cloud providers didn&#x27;t exist eventually there&#x27;d be botnets that no site could protect against, rather than the status quo of at least some sites being able to resist them. The idea that the existence of cloud providers that can soak up a lot of traffic is making things worse by causing botnets to get more powerful just seems silly. reply dylan604 19 hours agorootparentprevyou don&#x27;t need enterprise grade tools just to host a simple website. however, if your simple site ever gains enough attraction to come under an attack, especially like this, you&#x27;ll never survive. you can either just accept that your service will not survive the attack and just shut it down until the attackers realize mission accomplished and stops. you can then hope they don&#x27;t notice when you bring it back. no simple site will be able to afford what&#x27;s required to stay up from these attacks.i&#x27;m not saying i like having to put the majority behind the services of 2 or 3 companies, but if you ever get shut down from some DDOS, you&#x27;ll understand why people think they need to. reply lazide 15 hours agorootparentIt won’t survive - until a day or so later you’ve migrated to one of the large providers who provide the protection. reply spacephysics 19 hours agorootparentprevA similar analogy can be made with the likes of westward expansion in the continental US.Back then, you got a piece of land, and really could do what you wanted with it. Build a business, farm, etc. some government taxes but nothing crazy. But you had to deal with criminals, lack of access to medical care, and lack of education.Now to do the same, you have a slew of building codes, regulations, zoning laws, and are basically forced to have municipal services. Higher Taxes to pay the roads, police force, fire fighters, education services etc.However, home owners can still just have an egg or vegetable stand at the end of their driveway. It won’t be the same as having a storefront in town, but it’s still doable without the overhead.Similarly, as the internet matures, we’re going to see more and more overhead to sustain a “basic” business.But you can still have a personal blog ran in your closet, for lower-level traffic.The analogy isn’t perfect, but unfortunately as threat-actor’s budgets increase, so too do their quality&#x2F;sophistication of their attacks. If it was cheap to defend against some of the more costly attacks, they would find a different vector.The answer, to me, is some tangential technology that is some mix of federated or decentralization. Not in a crypto bro sense, but just some tech whose fundamental design solves the inherit problem with how our web is built today.Then threat actors will find another way, rinse and repeat… reply dmd 19 hours agorootparent> home owners can still just have an egg or vegetable stand at the end of their drivewayNo you can&#x27;t. That is illegal without a \"cottage food\" license, training, and labeling in most of the US.https:&#x2F;&#x2F;www.pickyourown.org&#x2F;CottageFoodLawsByState.htm reply bombcar 19 hours agorootparentChild-run lemonade stands are technically illegal in most states (some have actually carved out exemptions for them because of overzealous policing).Garage sales often have a specific carve out, also, and limitations on numbers of time per year, etc.Most areas nobody cares at all until it becomes a nuisance somehow. reply adrianN 19 hours agorootparentSelectively enforced laws are the worst kind of law. reply bombcar 19 hours agorootparentI&#x27;ve always thought it would be interesting to allow as a defense against a violation of a law to prove that the law is regularly violated without consequence.Because selectively enforced laws are just another way of saying you have a king at some level, the person who decides to enforce or not. reply zaroth 18 hours agorootparentSelective prosecution is a defense under the Equal Protection clause of the Constitution.However, the Supreme Court has left the prescribed remedy intentionally vague since 1996, which in turn makes the claims themselves less likely to be raised, and less likely to succeed.https:&#x2F;&#x2F;wlr.law.wisc.edu&#x2F;wp-content&#x2F;uploads&#x2F;sites&#x2F;1263&#x2F;2022&#x2F;... reply jrockway 18 hours agorootparentprevYou have some control over this as an ordinary citizen. Next time you&#x27;re on a jury for a lemonade stand violation, nullify. reply not2b 18 hours agorootparentHas a lemonade stand violation ever resulted in a jury trial in the US? I&#x27;m skeptical. In places that enforce those rules, usually what happens is that the cops tell the parent it isn&#x27;t allowed, the kid shuts it down and there&#x27;s no fine. reply lazide 15 hours agorootparentOr it turns into a giant PR disaster for the cops. reply acheron 15 hours agorootparentprevDon&#x27;t tell that to the GDPR defenders. reply adrianN 3 hours agorootparentI am a gdpr defender. I would like stricter enforcement. reply glompers 19 hours agorootparentprevOkay but does that mean anything regarding the parent commentor&#x27;s analogy or the article? reply 89vision 19 hours agorootparentprev20 years ago if a blog or website ended up on slashdot&#x2F;digg&#x2F;whatever there was a good chance it was going down. Scalable websites are a commodity today reply rglullis 18 hours agorootparentThat goes both ways. What was the price then to get a botnet with 10k nodes making 1k requests &#x2F; second? What is the price today? reply ExoticPearTree 14 hours agorootparentFor the website or for the use of the botnet? reply rglullis 1 hour agorootparentFor the use of the botnet... reply gchamonlive 20 hours agorootparentprev> 2FA and password managers didn&#x27;t make us heavily reliant on massive companies.Retool: https:&#x2F;&#x2F;arstechnica.com&#x2F;security&#x2F;2023&#x2F;09&#x2F;how-google-authenti...Lastpass: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34516275 reply rglullis 19 hours agorootparentIf Google Authenticator goes away, people will still be able to use 2FA (I for one use Aegis, it&#x27;s available on F-droid and does everything I need, including encrypted backups)If Lastpass goes away, people will still be able to use keepass or any of the large number of open source password managers, some of them even with browser integrations.If I have a website that is frequently attacked by botnets and Cloudflare goes away, what can I use to replace it? reply gchamonlive 19 hours agorootparentI am sorry, but if your password manager goes away and you have no disaster recovery scenario planned you might not be able to just move to a competitor:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31652650My response was to illustrate how insidious big companies are.Of course nothing compares to the backbone of the web going down. If AWS North Virginia suffers widespread downtime to all its availability zones, much of the web will just go dark, no question about it. reply bee_rider 19 hours agorootparentprev2FA, I’m not sure.But Lastpass doesn’t represent the whole of password managers. Storing your passwords in an online service is a really silly thing to do (for passwords that matter at least). Use something local like keepass. reply gchamonlive 19 hours agorootparentHope you plan ahead for a house fire with a 3-2-1 approach for backups. Maintaining an always on off-site storage is expensive unless you resort to cloud solutions like OneDrive or Dropbox, but then you go back to the problem of having your passwords on the cloud, even if encrypted.Not using cloud is just very expensive and time consuming for the average user. reply adrianN 19 hours agorootparentPasswords are small enough that you can make physical backups easily. reply gchamonlive 19 hours agorootparentHonest question, because it is interesting and might change how I approach backing up my passwords. How would you go about maintaing that physical copy updated?What I think would make this approach hard is that you would have to ponder if a newly created account is important at creation time in order to know if you should update the off-site, physical copy of your most important passwords (I say this because if you want to backup everything and avoid the cloud entirely it is just not viable, having to update this physical backup for each new account. I am currently at over 400 logins in my pw manager, 2 years ago it was half as much).I think having your passwords encrypted with a high enough entropy master password and a quantum-resistant encryption algorithm, and having an off-site, physical backup of your cloud account credentials is enough for anyone not publicly exposed, like a politician or someone extremely wealthy, even though I would be skeptical these people go through such lengths to protect their online accounts. reply rglullis 18 hours agorootparentThe lesson is not to \"avoid\" the cloud, but to not be \"dependent\" on it. Doubly so if the service provided is one that keeps you locked in and can not be ported over.So yes, I feel comfortable with my strategy of having backups on bluray disks + S3. If AWS goes down or decides to jack up their prices to something unacceptable, I will take the physical copies and move then to the dozen others S3-compatible alternatives. I am not dependent on AWS.But I am not interested in using Google Authenticator or Lastpass because that would mean that I am at their mercy. reply lazide 15 hours agorootparentLastPass is an issue - but even LastPass would let you export&#x2F;print the passwords. So no hard dependency there*. Google Authenticator recently did something similar with QR codes.* though OTP seeds don’t print, and you can’t export&#x2F;print attachments. I don’t recommend LastPass for these and many other reasons. reply adrianN 18 hours agorootparentprevWith two usb sticks it’s not that much work to take one witha fresh backup to my mom when I visit and take the other one back and update that backup. At worst I lose one or two logins. replylazide 15 hours agorootparentprevIt doesn’t take enterprise grade tools to host a website.It does take enterprise grade tools to defend against the largest DDoS ever attempted.Those are not the same thing. And those DDoS’s often are aimed at things besides a HTTPS endpoint. reply codedokode 20 hours agorootparentprevThere should be a protocol to block traffic on the upstream provider. So if someone from 1.2.3.4 sends lots of traffic at you, you send a special packet to 1.2.3.4 and all upstream providers (including the provider that serves 1.2.0.0&#x2F;16), that see this packet block traffic from that IP address directed at you. Of course, the packet should allow blocking not only a single address, but a whole network, for example, 1.2.3.4&#x2F;16.But ISPs do not want to adopt such protocol. reply tsimionescu 20 hours agorootparentSo I can deny service to your site with a single packet, instead of having to bother with establishing a whole botnet? The current botnet customers would be the first to advocate for this new protocol! reply alexfoo 20 hours agorootparentSimple! To prevent it being abused easily you could make it so you would need to send a high number of those packets for a sustained period in order to activate the block. reply brk 20 hours agorootparentThere is already an RFC we could apply, just implement forced RFC3514 compliance and filter any packets with the evil bit set.https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;rfc3514 reply simondotau 20 hours agorootparentprevAnd there could be a short time limit on that block, perhaps one hour, but even 60 seconds would be enough to completely flip the script on a DDoS. reply codedokode 20 hours agorootparentprevYou can only block access to your IP address, so you can ban someone from sending packets to you but not to anyone else. My proposal is well-thought and doesn&#x27;t require any lists like Spamhaus that have vague policies for inclusion and charge money for removing. My proposal doesn&#x27;t have any potential for misuse. reply ComodoHacker 20 hours agorootparentHow can it protect from... botnets, where there are tens of thousands \"someones\"? reply codedokode 16 hours agorootparentYou can only ban packets coming to your IP. Botnet can only ban packets coming to its IP addresses. reply tsimionescu 19 hours agorootparentprevIt&#x27;s not very hard to send packets with a fake source IP, especially if you don&#x27;t care about the reply. reply ndriscoll 19 hours agorootparentSeems easy enough to require (i.e. regulate) end-customer ISPs to drop any traffic with a source IP that isn&#x27;t assigned to the modem it&#x27;s coming from. This would at least prevent spoofing from e.g. compromised residential IoT devices. Are they not already doing that filtering? Is there any legitimate use-case to allow that kind of traffic? reply gene91 18 hours agorootparentSomeone has to go and add the filtering. Nowadays (or maybe since ten years ago) most ISPs have the filter, but not the last 1% (or maybe 0.01%). reply codedokode 16 hours agorootparentprevThe routers can send back a confirmation token to confirm the origin address. reply plagiarist 20 hours agorootparentprevSorry, this is not well-thought and certainly has potential for abuse. This is on IP and not domain? What is the signing authority and cryptography mechanism preventing a spoofed request? reply codedokode 15 hours agorootparentWhen you send a \"reject\" packet, the imtermediate routers send back a confirmation code. You must send this code back to them to confirm that \"reject\" packet comes from your IP address. No cryptography or signing required. reply plagiarist 12 hours agorootparentI don&#x27;t think you understand how networking operates at a packet level. replycodedokode 20 hours agorootparentprevYou can deny access only from your IP, not for anyone else. reply hnlmorg 20 hours agorootparentIP addresses can be spoofed. So you’d need some kind of handshake to verify you are the owner of that IP. Which is going to be tough to complete if your network is completely saturated from the DDoS in progress.I do think your idea has merit though. But it’s still a long way from being a well thought-out solution. reply iforgotpassword 20 hours agorootparentprevHow do you verify the source address of the packet is legit? reply codedokode 15 hours agorootparentThe router can send back a confirmation code and you must send it back to confirm that request comes from your IP.Also, on a well-behaved networks that do not allow spoofing IP addresses, this check can be omitted. reply iforgotpassword 3 hours agorootparent> The router can send back a confirmation code and you must send it back to confirm that request comes from your IP.Ideally with the token packet being larger than the initial packet, so it can easily be abused for a reflection attack... ;-)> Also, on a well-behaved networks that do not allow spoofing IP addresses, this check can be omitted.This is already not true for most networks, and in your case would&#x27;ve to be true for all intermediate networks which is just impossible.In another post you suggest this should also allow blocking entire networks; how do you prevent abuse of that?Your suggestion is anything but well-thought, it&#x27;s a pipe dream for a perfect world, but if we&#x27;d live in one, we wouldn&#x27;t have ddos attacks in the first place. reply papichulo2023 20 hours agorootparentprevYeah, we should invent secure communication channels and crypto keys first... reply tsimionescu 2 hours agorootparentThis proposal only works if the packets are readable by every intermediate router. Or are you suggesting that you establish a TLS session with every router between you and the attacker? reply ilyt 20 hours agorootparentprevWhat you say already exists, hell, you can use BGP to distribute ACLsBut it costs space in the routing tables and that means replacing routers earlier. It&#x27;s no wonder, especially if you multiply it by thousand customers.\"block all traffic from outside from this IP\" is significantly easier than \"block all traffic from outside from this IP to this client\". And you need to do it per ISP client, else it is ripe for abuse.And don&#x27;t forget a lot of the traffic will come from \"cloud\" itself. reply codedokode 20 hours agorootparent> What you say already exists, hell, you can use BGP to distribute ACLsBut you should own an AS for that?> But it costs space in the routing tablesNot implementing my proposal leaves critical infrastructure unprotected from foreign attacks. Make larger routing tables. Also, instead of blocking single IPs one can block &#x2F;8 or &#x2F;16 subnets. reply CountSessine 20 hours agorootparentMake larger routing tables.Brilliant! Why didn’t we think of that?!? MOARE TCAMS!!! reply codedokode 15 hours agorootparentif Cloudflare can do this on commodity hardware (stop attacks and block thousands of IPs), then router manufacturers who have custom hardware can do much more.Also, in Russia for example, there is DPI inspection and recording of all Internet traffic and if it is possible in Russia, then West can probably do 10x more. Simply adding a blacklist on routers seems like an easy task compared to DPI inspection. reply codedokode 15 hours agorootparentprevThis can be made on a paid basis. For example, for $1&#x2F;month a customer gets a right to insert 1000 records (block up to 1000 networks or IPs) into blacklist on all Tier-1 ISPs. For $100&#x2F;mo you can withstand an attack from 100 000 IPs which is more than enough and Cloudflare goes bankrupt. reply rootlocus 20 hours agorootparentprev> Of course, the packet should allow blocking not only a single address, but a whole network, for example, 1.2.3.4&#x2F;16.So, if my neighbour is infected and one of his devices is part of a botnet, I get blocked as well? reply codedokode 15 hours agorootparentYes. Because blocking several extra users on a bad network that has several infected hosts and does nothing about it is better than being under attack. reply bee_rider 19 hours agorootparentprevBlock the whole country, then I guess you’ll see laws passed that IOT providers need to start updating at a better clip. reply rcxdude 19 hours agorootparentprevThat already effectively happens in a lot of cases. reply toast0 19 hours agorootparentprevIf the source field in a packet reliably indicated the source of the packet and a given IP was sending you a lot of unwanted traffic, you&#x27;d ask their ISP to turn them off and the problem would be solved. Maybe one day BCP38 will be fully deployed and that will work. I also dream of a day where chargen servers are only a memory. Some newer protocols are designed to limit the potential of reflected responses.Null routing is available in some situations, but of course it&#x27;s not very specific: hey upstreams (and maybe their upstreams), drop all packets to my specific IP. My understanding is null routing is often done via BGP, so all the things (nice and not) that come with that.Asking for deeper packet inspection than looking at the destination is asking for router ASICs to change their programing; it&#x27;s unlikely to happen. Anyway, the distributed nature of DDoS means you&#x27;d need hundreds of thousands of rules, and nobody will be willing to add that.Null routing is effective, but of course it takes you IP offline. Often real traffic can be encouraged to move faster than attack traffic. Otherwise, the only solution is to have more input bandwidth than the attack and suck it up. Content networks are in a great position here, because they deliver a lot of traffic over symetric connections, they have a lot of spare inbound capacity. reply eptcyka 19 hours agorootparentprevDo you know what the first D in DDoS attack stands for? reply HumblyTossed 20 hours agorootparentprevI am pretty sure that protocol would be just as abused. reply codedokode 20 hours agorootparentHow exactly? You can authenticate sender by sending a special confirmation token back. reply sophacles 18 hours agorootparentHow does one get removed from the block list?Say some IoT device that half of households own gets compromised and turned into a giant botnet. The news gets out and everyone throws away that device. Now they are still blocked over a threat that doesn&#x27;t exist anymore... doesn&#x27;t seem like a good situation for anyone.I&#x27;d imagine that the website owners that want the attack stopped will soon want to figure out how to get traffic back since they need users to pay the bills.Whats to stop someone from just making an app that participates in an attack when connected to public(ish) wifi networks and participating in attacks long enough to get those all shut off from major sites?How does this stop entire ISPs from getting shut off when the attackers have managed to cycle through all the IP pools used for natting connections? (e.g. the Comcasts of the world that use cg-nat to multiplex very large numbers of people to very small numbers of IPs)? reply codedokode 15 hours agorootparent> How does one get removed from the block list?We can add an \"accept\" packet that lifts the ban.Also, how do you remove yourself from blacklist when banned by Google or Cloudflare? I guess here you use the same method.> Say some IoT device that half of households own gets compromised and turned into a giant botnet. The news gets out and everyone throws away that device. Now they are still blocked over a threat that doesn&#x27;t exist anymore... doesn&#x27;t seem like a good situation for anyone.Not my problem. Should have thought twice before buying a vulnerable device and helping criminals. As a solution they can buy a new IP address from their ISP. reply grotorea 9 hours agorootparentprevAs much as I half-wish there was something like this, it does sound like email spam blacklists all over again. reply 6510 20 hours agorootparentprevI just imagined this: isp&#x27;s could make a isp.com?target=yourwebsite.org&#x2F;fromisp [slow] redirecting url. If you receive unusual amounts of requests from the isp you redirect it though their website.They can then ignore it until their server melts (which takes care of the problem) or take honorable action if one of their customers is compromised. The S stands for service after all. reply oefrha 8 hours agorootparentIt appears you don’t understand DDoS at all. There aren’t humans sitting behind browsers or scripts using browser automation software. No one cares about less respects your “redirect” because no one’s reading your response. Most of the time the attacks aren’t even HTTP, they are just packet floods. reply starcraft2wol 19 hours agorootparentprev> Let&#x27;s go back to username and password. 2FA forces scammers to up their game.Let&#x27;s do it. It works for the website you&#x27;re using right now. 2FA was in large part motivated by limiting bot accounts and getting customers phone number.I can&#x27;t imagine how much productivity the economy loses every day due to 2FA. reply master-lincoln 19 hours agorootparentIs this sarcasm? If not please provide some more details on why you think \"2FA was in large part motivated by limiting bot accounts and getting customers phone number\". I never used a phone number for 2fa. Mostly TOTP. Bots could do that too. I don&#x27;t see the connection.>I can&#x27;t imagine how much productivity the economy loses every day due to 2FA.Is it really that much? Every few days I have to enter a 6 digit number I generate on a device I have with me all the time. Writing this comment took me as much time as using 2fa for a handful of services for a month. reply starcraft2wol 19 hours agorootparent> ? Every few days I have to enter a 6 digit number I generate on a device I have with me all the time.I use more than one service a day, and some infrequently, so for me about every day I have a minute or two where I try to login, need to find my phone (it&#x27;s not predictable when it will ask), and then type it in. This happens to every person several times a day!I also now must carry a smart phone with me to participate in society.But the main drag is that when people lose or break their phones the response is: \"just don&#x27;t do that\" and the consequences range from losing your account to calling customer service.> Mostly TOTP. Bots could do that too. I don&#x27;t see the connection.Most people using 2FA do not use TOTP, they use a phone number.Bots could use TOTP, it&#x27;s more infrastructure, and it&#x27;s a proof of work function for them to login. reply michaelt 18 hours agorootparentprevWhile I don&#x27;t take starcraft2wol&#x27;s theory seriously, there are a bunch of services that have made phone numbers essentially mandatory. They claim this is to \"protect your account\".You sign up for a Skype account or Twitter account and decline to give your phone number, instead choosing a different form of 2FA? In my experience your account will be blocked for &#x27;suspicious activity&#x27; even if you have literally no activity. reply starcraft2wol 8 hours agorootparentAnd you still don&#x27;t take my theory seriously :) reply starcraft2wol 19 hours agorootparentprevTo add, password managers provide great coverage of almost every problem 2FA is. supposed to solve and it improves the workflow your grandma already know (writing passwords on a sheet). The only difference is Google doesn&#x27;t get any money when you run a script on your own computer. reply actualwitch 18 hours agorootparentprev> It works for the website you&#x27;re using right nowIt doesn&#x27;t, you can regularly see people getting their accounts stolen here. This wouldn&#x27;t be possible (or at least this trivial) with any competent implementation of 2fa. reply seanw444 20 hours agorootparentprev> Privacy, long term, will mean the fall of civilization.I&#x27;m curious about your rationalization for this. Lack of privacy will also mean the fall of civilization. Civilization is just doomed to fail at one point or another. All things come to an end. reply gchamonlive 20 hours agorootparentThis was me being sarcastic. Of course we need privacy, not because we have things to hide, but because individuality can only flourish without constant surveillance.Yes! All things come to an end and that is why some recent philosophers think that Plato was naive to think it could minimize or erradicate society rotting. This is where negative utilitarianism comes in, where the point of society is not to maximize happiness (and therefore prevent society from collapsing) but to minimize suffering (and therefore provide mechanisms to minimize damages from transitions between organization forms when society collapses). I have to refer you to Karl Popper&#x27;s The Open Society for this, because needless to say this answer is very reductionist. reply seanw444 18 hours agorootparentAh I just missed the sarcasm. Yeah, and when the sole goal is to minimize suffering, tyranny is introduced.\"Those who would give up essential liberty, to purchase a little temporary safety, deserve neither liberty nor safety.\" reply jorvi 20 hours agorootparentprevThis discussion is somewhat reminiscent of \"Don&#x27;t hex the water\"..https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Fzhkwyoe5vI reply lxgr 19 hours agorootparentprevThat&#x27;s not a valid comparison, since there are various effective decentralized 2FA methods available – unlike for DDoS protection. reply paganel 18 hours agorootparentprevYes, what the OP is saying is related to one of the paradoxes of security&#x2F;defence, i.e. the fact that the more one increases its defences (like Google is doing) then the more said increase of defences also pushes one&#x27;s adversary to increase its offence capabilities. Which is to say that Google playing it safer and safer actually causes their potential adversaries to become stronger and stronger.You can see those paradoxes at play throughout the corporate world and especially when it comes to actual combat&#x2F;war (to which actual combat&#x2F;war these DOSes might actually be connected). For example the fact that Israel was relatively successful in implementing its Iron Dome shield only incentivised their adversaries to get hold of even more rockets, so that the sheer number of rockets alone would be able to overwhelm said Iron Dome. That&#x27;s how Hamas got to firing ~4,000 rockets in one single day recently, that number was out of their league several years ago when Iron Dome was not yet functional. reply myth_drannon 13 hours agorootparentIt&#x27;s the opposite, the number of rocket was growing and hence the Iron Dome was developed. The Israelis saw the writing on the wall and acted accordingly. The laser system will be operational soon and then it will cost 1$ per shot. reply paganel 13 hours agorootparentUnless it&#x27;s cloudy outside. reply unethical_ban 16 hours agorootparentprevNone of your examples are valid, IMO.Procuring and operating the infrastructure to mitigate this kind of attack costs many many thousands of dollars or requires becoming part of the Cloudflare&#x2F;AWS&#x2F;Google hive.Joe Schmo can set up a TOTP server, run keepass&#x2F;bitwarden and use letsencrypt for free (or another SSL provider for cheap).The lament from parent is that running a simple blog reliably shouldn&#x27;t require being inside Cloudflare&#x27;s castle walls or building your own castle.---My personal observation is that simple websites should continue operating HTTP1! reply zelon88 19 hours agoparentprevMost of them are dynamic IPs. Some of them are infected mobile devices.What happens when you log an attack from a device that is attacking you from a school or business WiFi network? Block the whole IP forever?What if the user is on a CGNAT. Are you going to block the edge proxy for that entire ISP?What if you&#x27;re getting hit from a residential connection that gets a new rotated IP every couple of weeks? Block whoever gets that IP from now on?Your solution doesn&#x27;t stop attacks. It just stops regular users. reply tristan9 19 hours agorootparent> What happens when you log an attack from a device that is attacking you from a school or business WiFi network? Block the whole IP forever?No, but for a day perhaps.> What if the user is on a CGNAT. Are you going to block the edge proxy for that entire ISP?Maybe. If the ISP doesn’t bother doing anything about it (which is THEIR job, not mine as a website operator).If the ISP can’t be arsed to do their job, why am I supposed to care about them at all?> What if you&#x27;re getting hit from a residential connection that gets a new rotated IP every couple of weeks? Block whoever gets that IP from now on?Same as the CGNAT one. It’s the ISP’s job to handle their misbehaving customers.If they refuse to do it and get complaints from their other customers that they’re getting blocked, maybe they’ll actually get to it.> Your solution doesn&#x27;t stop attacks. It just stops regular users.No. It puts pressure on the ISPs to finally stop whining loudly when they receive an attack while closing their eyes on any attack originating from their network.This is not sustainable. reply zelon88 19 hours agorootparentTrust me when I say that you don&#x27;t want the ISP&#x27;s to inspect web traffic. That is not how to solve this. That is costly for the ISP and will drive up costs. It also makes supporting a website impossible. The ISP is assumed by all parties to be impartial. That assumption is required for the internet to be operational. Sure it might function your way, but it would be impossible to support.And maybe Facebook and Google are big enough to push around the ISP&#x27;s, but they are the only ones. Nobody will bat an eyelash if 15,000 Comcast users in Phoenix AZ can access your hokey-pokey website. Comcast doesn&#x27;t care. The users won&#x27;t blame their ISP. They will blame you, or whoever owns the hokey-pokey website. If you want traffic, you need to be equipped to handle traffic. You are the one with the internet facing infrastructure.You are the one blocking traffic. Not the ISP. That is how it should be. The ISP should be impartial. You pay for connectivity. Consider yourself connected. For better or for worse. You are responsible for what you put onto that connection. reply tristan9 9 hours agorootparent> Trust me when I say that you don&#x27;t want the ISP&#x27;s to inspect web traffic.They do already. DPI on port 53 for DNS blocks or SNI inspection are common place. So are IP blocks.> If you want traffic, you need to be equipped to handle traffic. You are the one with the internet facing infrastructure.Slightly misleading wording here. More accurately your point is: « you want to run a website? Better have the infra to support traffic spikes comparable to that of a tech giant ». 400M rps would cost an unfathomable amount of money to be able to handle even just while dropping all packets.> And maybe Facebook and Google are big enough to push around the ISP&#x27;s, but they are the only ones. Nobody will bat an eyelash if 15,000 Comcast users in Phoenix AZ can access your hokey-pokey website.Obviously yes. Too bad it’s better business for everyone to say nothing and just recommend you use their product. reply mrweasel 19 hours agorootparentprevISP needs to start taking much more responsibility, currently they do not care or choose not to care to avoid having to deal with upset customers.The fact that millions, if no more, devices can continue to access the internet regardless of how long they are compromised, is just crazy. I get that it put more responsibility upon end users to secure their devices, if they otherwise run the risk of get thrown of the internet, but I currently fail to see other options. Our device security still isn&#x27;t good enough that we can just use them with reckless abandonment.Any \"solution\" that attempts to fix the problem of increasing DDoS attacks and their damage that doesn&#x27;t address the issue of compromised devices being allowed to roam free on the internet is a band aid at best.And I can almost hear people complain that I&#x27;m arguing to throw compromised IoT, SCADA and monitoring devices of the internet, and yes I am. None of these things have any business being exposed to the public internet anyway. reply lazide 15 hours agorootparentEither the ISPs are common carriers that follow some sort of basic rules, or they try to make people happy and end up stepping all over people randomly.Currently there are zero rules (outside of a ISP ToS maybe) that forbids what you’re talking about. Pretty much anywhere I think? Unless you know of a law against having a infected or out of date computer connected to the internet?There really is no way to have both. The current situation, they generally only deal with problem cases that get reported to them. And I doubt anyone is going to bother doing so for the 20k machines in this attack. reply acedTrex 19 hours agorootparentprevIt is not an ISPs job to analyze traffic patterns and attempt to stop the bad ones. Thats like saying its the job of the road crews to stop speeders reply mrweasel 19 hours agorootparentSo who else? My proposal would be to have companies like Google, Microsoft, Amazon and hosting providers be able to report sources of DDoS attack to the ISPs who can then identify the customer and let the customer know that they have a week to fix the issue or lose connectivity. reply zelon88 17 hours agorootparentThat is terrifying.Let Google, Amazon, and Apple decide who gets to use the internet and who gets put into a list.That is way worse than giving Google the W3C. That is literally just handing them the internet and making everybody else on it subservient to Google. reply axus 18 hours agorootparentprevOr that it&#x27;s the ISP&#x27;s job to cut off accounts that are downloading copyrighted works, or hashing cryptocurrency without paying taxes, etc.It would be nice if the cell phone provider could send a text message reporting the problem. But how to distinguish it from spam? reply rplnt 15 hours agorootparentprev> > What happens when you log an attack from a device that is attacking you from a school or business WiFi network? Block the whole IP forever?> No, but for a day perhaps.Then that&#x27;s also a DDoS attack vector. reply bee_rider 19 hours agorootparentprevThe idea clearly needs some work.But, a slight defense of it—the really big providers can already sink a massive DDoS anyway. So, this is just a scheme to help little websites. It doesn’t really matter if a school, or even a cellphone network, can’t access my little website for an afternoon.You’d have to decide if you want to send the block request. If you are hosting your personal blog, you’ll probably go for it regardless. If you are providing a small service; hosting git for a couple friends or whatever, you’ll probably block with some discretion. reply AdamN 21 hours agoparentprevThe only answer is publicly-resourced protection and it&#x27;s not that weird when you think about it. My apartment has a basic lock that any locksmith can undo and I&#x27;m safe because of my community and government protection (police, mental healthcare, justice system, etc...). Seems like the same logic should apply to my website or other digital property. reply candiddevmike 20 hours agorootparentISPs will gladly quarantine&#x2F;rate limit folks for pirating stuff, why don&#x27;t they use those tools to combat botnets? Though I could see this leading to a slippery slope of remote attestation for internet access. reply KomoD 18 hours agorootparent> why don&#x27;t they use those tools to combat botnets?Because they probably don&#x27;t care. reply starcraft2wol 19 hours agorootparentprevCommunity yes. Government protection no. When was the last time you heard of police stopping a break-in or making a successful investigation ?Independent of police, in bad communities your neighbors are willing to break in. In good communities they don&#x27;t. reply 542458 20 hours agorootparentprevWhere this breaks down is that because of the nature of the internet and DDoS attacks it’s not something that can easily be solved with better policing - even identifying a perp might be near-impossible, and they might be in another country anyways. The government does try to prosecute botnets and DDoS attacks today, but it’s of limited success. Is there a practical solution here I’m missing? reply gwright 17 hours agorootparentI don&#x27;t know about a \"practical solution\", but there are research efforts to think about new ways to build internets that mitigate some of these problems.Here is one that I&#x27;m aware of: https:&#x2F;&#x2F;named-data.net reply DanielBMarkham 20 hours agoparentprevWhy don&#x27;t we just require major providers to provide a realtime list of IPs that are attacking so that we can drop them in a block list with an expiration date of a month or so.If your computer is infected, I don&#x27;t want to talk to you for a month. If it continues to be infected, I might up that to a year, or permanently ban you.It&#x27;s your problem. Go fix it. reply Prickle 20 hours agorootparentI&#x27;ve been on the receiving end of \"Your\" (dynamic) \"IP has been blocked.\"I would greatly prefer not having my semi-randomized IP blocked because someone used it maliciously a year ago. reply DanielBMarkham 20 hours agorootparentKey phrase: \"a year\"If anybody is suggesting permanent bans of IPs, it&#x27;s not me, at least not at a public level. I may very well choose privately to do that.To clarify, I, personally chooses a blacklist policy. Not some other org. I think if you offload this onto any kind of external structure, it breaks again.ADD: We make publicly-available, second-by-second, how the internet is broken and invite all comers, including me and my blocklist, to help fix it.There&#x27;s a huge commerical interest in NOT fixing the problem of random crap showing up, from dancing cats selling things to targeted inserted ads. I get it. We saw this same thing happen with adblockers. It&#x27;s now going on with \"free\" VPNs. Can&#x27;t fight that perverse incentive, so don&#x27;t fight it. reply tristan9 19 hours agorootparentprevThing is, don’t care.The problem is that ISPs whose customers are originating the attacks from don’t give a shit.If we have to give up 1% of legitimate traffic to thwart 90% of attacks, it is a good deal.If you and other customers complain to your ISP (or switch), eventually they’ll do something about it.We can’t seriously keep on accepting that « thousands of compromised devices » is a fine reality for a « small botnet ».These devices should be quarantined. reply InSteady 16 hours agorootparentSounds like a really great way to potentially destroy someone&#x27;s career if they aren&#x27;t terribly competent and you are. Infect some component in their home network that they don&#x27;t even know is smart-enabled, and keep breaching their new devices, adding them to an active and conspicuous botnet. The only recourse for average Joe is to find expert help, which isn&#x27;t really in abundant supply if you are a semi-sophisticated malicious actor.I don&#x27;t even want to think about the ramifications for small and medium sized businesses. Realistically, how much would it cost to be able to completely destroy a local competitor by paying someone to orchestrate a few events in succession. reply DanielBMarkham 12 hours agorootparentThis is an odd argument. The net is currently broken in many ways. One of the many ways is fake negative reviews. They easily destroy small businesses.As I understand your argument, because the net has solid endpoints we can identify and isolate, we should ignore that fact. Instead we should create more and more complex systems to work around bad actors?Bad actor takes control of grandma&#x27;s computer. We should do all sorts of things except stop talking to grandma&#x27;s computer? The thing, I would suspect, that most people would expect?Businesses suffer from too much transparency. Got that part. They buy things that don&#x27;t work and sometimes hurt people, even if they don&#x27;t intend to do this. So far, so good. Where is the part where new businesses models are supposed to exist because some people made bad choices and the current models don&#x27;t work? Why don&#x27;t we just publicize the bad choices and let things work themselves out?Sorry. Missing it. reply dist-epoch 19 hours agorootparentprevAmazon definitely cares if they lose 1% of sales.Guess who has more votes, you or Amazon. reply tristan9 9 hours agorootparentI’m aware. Doesn’t make it sting less being in the receiving end of attacks all the time and seeing everyone collectively shrug. reply lapcat 20 hours agorootparentprev\"Moreover, the lifespan of a given IP in a botnet is usually short so any long term mitigation is likely to do more harm t",
    "originSummary": [
      "Google has successfully defended against the largest recorded DDoS attack, which peaked at 398 million requests per second, using a novel method termed HTTP/2 Rapid Reset.",
      "The coordinated response from the industry following the attack has led to the development and implementation of patches and other mitigation techniques. Users of HTTP/2 are encouraged to apply vendor patches for CVE-2023-44487 to reduce vulnerability to such an attack.",
      "For protection against DDoS attacks, Google Cloud customers are advised to make use of Cloud Armor's DDoS protection and other features like proactive rate limiting rules and AI-powered Adaptive Protection."
    ],
    "commentSummary": [
      "The conversation covers a broad spectrum of subjects related to DDoS (Distributed Denial of Service) attacks, including the motivations behind such attacks and possible attackers.",
      "Strategies to curb these attacks are discussed, underscoring the role of cloud providers and the responsibility of Internet Service Providers (ISPs).",
      "Highlighted points include the growing threat of DDoS attacks in the digital world, increasing concerns regarding internet security, difficulties in addressing cyber security, and the impact of certain safety measures."
    ],
    "points": 717,
    "commentCount": 439,
    "retryCount": 0,
    "time": 1696939844
  },
  {
    "id": 37840503,
    "title": "Log is the \"Pro\" in iPhone 15 Pro",
    "originLink": "https://prolost.com/blog/applelog",
    "originBody": "Blog About Tutorials Prolost Store Archive Recent Featured All Where to Begin? Maschwikipedia Contact Comment Policy by Stu Maschwitz Blog About Tutorials Prolost Store Archive Recent Featured All Where to Begin? Maschwikipedia Contact Comment Policy Log is the “Pro” in iPhone 15 Pro October 10, 2023 And I've got some free LUTs for you. The iPhone 15 Pro and Pro Max feature log video recording. This is a big deal, but there’s already some confusion about it. Where consumer devices and pro video overlap, that’s where the Prolost Signal gleams brightest in the night sky. So let’s get to work. First, what exactly is log? It’s short for logarithmic encoding, which is a math thing, but what does it mean to videographers? It really boils down to two things: Log is flat, and log is known. Flat is Good, and Log is the Best Flat Standard iPhone video is designed to look good. A very specific kind of good that comes from lots of contrast, punchy, saturated colors, and ample detail in both highlights and shadows. Log video looks flat. All the dynamic range and detail are there, but gone is that candy-coated punch. To make log footage look right, you have to color manage it in some way, often with a Look-Up Table, or “LUT.” A LUT is just a color adjustment baked into a single file. Some LUTs add creative looks, others are more utilitarian, converting images from one color space to another — like from log to video. If all we’re going to do with log footage is apply a LUT to make it look normal, what’s the big deal? Why add this extra step? The power log gives us lies within that extra step, because: You pick the LUT! So log footage can look however you want. Sure, you can apply LUTs or color grading to regular iPhone footage, but the problem is it already has its own very punchy look baked in. With log you can pick the look that works for you, and even more importantly... You can color grade under the LUT. So you can adjust the color of the footage in a natural and organic way. And this workflow matches what you’re doing with your digital cinema camera already, so you can color-manage this footage right into a timeline with your Canon, Sony, Arri, or whatever else you’ve got. View fullsize Log footage from iPhone 15 Pro Max with various LUTs and looks applied. You Didn’t Explain What Log is Though But what is the log of log? What does this math concept mean to me? The essence of log (in its purest form) is that every stop of light gets the same amount of data. Ten-bit footage holds 1,024 shades of gray. To represent 12 stops of light values (or dynamic range), that means each stop gets about 85 shades of gray. This means highlights and shadows all have detail that you can access for basic color corrections like exposure and white balance. You can recover some shadow and highlight detail, and make substantial color adjustments that look natural. View fullsize In this hypothetical example, we spread 12 stops of light evenly over 1,024 shades of gray. In Log We Offset It also means that these basic color adjustments are dead simple to perform. Since each stop (in our semi-made-up example) is 85 shades of gray, adding 85 to the value of each RGB pixel is the same as increasing the exposure by one stop. We call this adding and subtracting Offset, and Davinci Resolve has a whole color wheel devoted to it. In Magic Bullet Looks, the Global color wheel in the 4-Way Color Tool does an offset adjustment in ACES log, even if your source footage isn’t log. With log footage, Offset is the most natural way to correct, tint, or re-expose your footage. These kinds of simple corrections look terrible when done after the LUT, or to video-space footage, but when you do them to the log pixels before the LUT, the results look so natural, it almost appears that they happened in-camera. This makes it much easier to achieve consistent color across shots, as they all get the same last step. This is why it’s such a common workflow to put Magic Bullet Looks, for example, in an adjustment layer over all the shots in a sequence. Sun Dog Log corrections look natural in part because the simple color-grading math lines up with how light works. They also look great because of the available dynamic range. If I try to recover some of the detail in my dog’s sun-lit fur, this iPhone 12 footage rapidly falls apart. The iPhone added contrast, and in doing so smooshed the white values into a uniform overexposed patch. Nearby colors are posterized, and the clipped highlights give the clip away as consumer video. iPhone 12 Not log, not HDR, clipped highlights Source: Kodak But with this iPhone 15 Pro Max footage shot in Apple Log, I can recover all the detail — or just let it overexpose gracefully into this ACES output transform, for a smooth, film-like look. This soft built-in highlight rolloff is called a “shoulder” in film, describing the upper part of the classic s-curve. A nice shoulder for your highlights is a big part of what makes footage look “pro” — especially when your grading happens underneath it. Because log footage uses the same amount of data for every stop, it’s not the most efficient way to store an image. So log is best when it’s at a high bit depth and data rate. Apple Log is only available when recording in ProRes, which is typically 10 bits-per-channel. Apple added ProRes with the iPhone 13, but without log it wasn’t very “pro,” and I almost never used it. The baked-in look made grading difficult, so there was no reason to take on the burden of the massive files. Heavy is the ProRes, Dangly is the Drive View fullsize Apple and Samsung, sitting in a tree. Yes, massive. 4K ProRes files are big, and recording them to your phone can create some weird workflow challenges. Apple shocked us all by addressing this head-on: The iPhone 15 line charges via USB-C instead of Lightning, and this standard USB port can do a lot. When you connect a USB-C drive, your ProRes Log video automatically gets recorded there instead of to the phone’s photo library. Recording this way also unlocks 4K 60fps recording in Apple’s Camera app, enabling some gorgeous slow motion effects when played back at 24 fps. Play 00:00 00:25 Mute Settings Enter fullscreen 00:00 Apple has gone from refusing to sell you a pro laptop with an SD card reader to making a camera that records to external media. We’ll all be looking for iPhone rigs with places to strap these little drives. View fullsize Drive 1, Drive 2 Log is Half Baked View fullsize In this unprocessed log clip, the purple light reflections appear over-saturated. With its high bit depth and dynamic range, log footage has many of the benefits of raw. But Apple Log is not raw, and not even “straight off the sensor.” It’s still heavily processed — denoised, tone-mapped, and color adjusted. You can see this big time if you shoot bright, colorful lights. Even though Apple has turned their sharpening and tone mapping way down for Apple Log, there’s still some of that telltale overdriving of super-bright, super-saturated colors. It’s also important to note that log does not mean you’ll never overexpose. It’s still an iPhone with a tiny sensor, so don’t expect the dynamic range of an Arri Alexa or a Sony Venice. Blackmagic Camera App The other way Apple doesn’t quite live up to the “pro” promise of log is by not offering much manual control in their camera app — and that’s totally fine. Because all the control you could want is found in the wonderful new Blackmagic Camera app. It’s a free download, presumably meant as a gateway drug to Blackmagic’s paid products and services. It’s a terrific app with a gorgeous UI and pro features like a live histogram, display LUTs, HDMI out via USB-C, and choices of different ProRes formats. View fullsize Note that the shutter speed is set in degrees! What a time to be alive. VFX Log The flatness of log gives you the power to make it your own in the grade. The other huge advantage of log is that it is known, meaning it’s possible to convert it to and from various color spaces accurately. This is a huge advantage for VFX artists and colorists alike. Colorists can convert Apple Log to whatever color space they like to work in, and accurately incorporate iPhone footage into any color timeline. VFX artists can convert their log footage to scene-refererred linear and create accurate composites that include color-matched 3D renders. In this example, I converted the footage to EXR and camera-mapped it onto simple geometry in Cinema 4D. In Redshift, those HDR pixel values cast light and reflections onto the 3D model, doing 90% of my lighting for me: Play 00:00 ACES Compatible? Apple’s log format being known is all it takes for compatibility with ACES, the Academy of Motion Picture Arts and Sciences color management system. But it’s important to note that Apple Log does not match any pre-existing ACES format. The log curve is Apple’s own, which they’ve documented, and it’s already available in DaVinci Resolve. In the CST node, choose Apple Log for the Input Gamma, but choose Rec. 2020 for the Input Color Space, because Apple Log uses the Rec. 2020 primaries. With these as your inputs, you can either convert to Rec. 709 video, or to an ACES format like ACEScc — which is also log, but a different log. Remember when I said log uses the same amount of data for each stop? That’s not entirely true with Apple Log. The darker stops are compressed a bit, to control noise. If you’re familiar with ACES, this makes Apple Log more similar to ACEScct than to ACEScc. Many colorists prefer working with ACEScct because of the way it handles shadows, so Apple is in good company here. ACEScc A good example of a log format that allots the same amount of date to each stop Along with documenting their unique transfer function, Apple has supplied a LUT for converting from Apple Log to Rec. 709 video. Apple’s LUT is very rich in contrast and saturation. This is useful if your goal is to match the look of non-log iPhone footage, but I found it too colorful for some applications, so I created my own LUTs that roll highlight colors off more naturally. Prolost LUT pl-AppleLog-to-Rec709_01.cube While Resolve and Final Cut Pro both offer built-in support for Apple Log, some other tools don’t yet. So I also made LUTs for converting Apple Log to ACEScc and ACEScct — the ACES log formats designed for creative color work. With these LUTs, you can incorporate Apple Log footage into an existing ACES workflow without losing any color fidelity. To some, true ACES compatibility would require Apple Log’s inclusion in widely distributed ACES OCIO configs. Until that happens, Apple Log is ACES-compatible as long as you have a color-space conversion bridge — something like either the CST node in Resolve or the Prolost Apple Log LUTs, available below as a free download. Magic Bullet Magic With this workflow, I was able to run some iPhone 15 Pro Max footage through the ACES-compatible Magic Bullet Looks, using our new Diffusion filters modeled after real-world filters from Tiffen and others. Along with some basic grading, I also added some film halation and grain. The results have a rich, cinematic look that in no way betrays that it was shot on a consumer phone. View fullsize Rebel Without an Excuse The argument about whether it makes any sense to shoot professional video on a consumer device dates back to before my book, The DV Rebel’s Guide. I love small, accessible cameras that allow us to shoot unnoticed in public places, but for years I discouraged the use of phones for this, because the ability to control and author the image just wasn’t there. Now that the iPhone can shoot log, is the best camera the one you literally always have with you, or, you know, an actual good camera? The iPhone 15 Pro Max is going to tempt you to shoot real stuff with your telephone, and for the first time ever, I’ll be right there with you, thanks to log. GET THE FREE LUTS Tags: Apple, Image Nerdery, Visual Effects, Color, Magic Bullet, Blackmagic Camera App, Maxon Prev / Next Comments (0) Oldest First Oldest First Newest First Maxon One Cinema 4D, Forger, Red Giant, Redshift, Universe, and ZBrush, all in one bundle. Slugline Screenwriting for Mac, iPad, and iPhone Prolost Store Apps, presets, and other goodies for filmmaking and photography. Featured Post TANK May 23, 2018 Recent Posts Log is the “Pro” in iPhone 15 Pro Oct 10, 2023 Jurassic Punk Dec 16, 2022 Lightroom Adds Video Color Editing, with Prolost Presets Jun 15, 2022 Mac Studio and Studio Display Mar 17, 2022 M1 Max MacBook Pro Long-term Report Mar 7, 2022 The M1 Max MacBook Pros Oct 25, 2021",
    "commentLink": "https://news.ycombinator.com/item?id=37840503",
    "commentBody": "Log is the \"Pro\" in iPhone 15 ProHacker NewspastloginLog is the \"Pro\" in iPhone 15 Pro (prolost.com) 469 points by robenkleene 6 hours ago| hidepastfavorite131 comments ta8645 6 hours agoI&#x27;ve never owned an Apple device. I don&#x27;t take photographs or video with my phone very often. But this video presentation was captivating. It was clear, concise, without any nonsense, and thoroughly interesting. reply dclowd9901 4 hours agoparentI don’t know if it’s the same thing, but capture on my Nikon D7100 always felt more “manipulable” than capture on an iPhone or the like, I suspected as a downstream effect of using RAW format with a larger image sensor. Interpreting log through this understanding, it felt pretty intuitive reading through this post. I don’t know if it’s accurate, but it feels accurate… reply matsemann 4 hours agorootparentOne reason is also that phone cameras have many limitations, so to get good images they have to \"cheat\" to work around those limitations. Additionally they often apply filters to the images so it looks good \"out of the box\", like contrast, smoothing, sharpening. Those choices done for you mean you lose information to do better yourself. reply olivermuty 4 hours agorootparentWhich is a great if you consider that 99.98% of iphone owners wont know how to do it better themselves.It will be fascinating for anthropologists a few hundred years from now to see the increase in quality in «every day photography» that came with the increasing quality of smartphone camera software. reply soultrees 3 hours agorootparentI actually think what’s happening is that it’s averaging out photography with such amazing tools in everyone’s hands, however we are seeing an outlier explosion of creative photography that we haven’t seen before.That said, the dopamine hit comes from taking your own shots and seeing the visual perfectness Apple creates, rarely if ever will those shots give the same effect to a viewer. So they don’t need to be photography-perfect, they just need to appeal enough to our monkey brains and monkey eyes to get that shot of dopamine to want us to take more pictures, therefore using the phone and resulting cloud storage more. reply baz00 1 hour agorootparentThe magic of phone cameras disappears in a moment when you get hold of a mirrorless for 5 minutes. Even a bottom end one is orders of magnitude better than the best phone camera even if it’s got a lot less megapixels. reply cyberax 0 minutes agorootparentEhh, no.My typical test is to take a photo of the full moon. It works acceptably well on an iPhone (or Android). My recent Pixel phone even adjusts the brightness automatically. Sure, the lens is pretty wide-angle, so the pictures don&#x27;t have many details.I had to fiddle around for 20 minutes with settings on my Sony Alpha camera, eventually using manual focus and manual exposure. The pictures are, of course, better because of the lens and the full-frame sensor.But the user experience is just sad. So I often just don&#x27;t bother to take my camera with me on my trips anymore.Also, a note to camera makers: USE ANDROID INSTEAD OF YOUR CRAPPY HOME-GROWN SHITWARE. Add 5G, normal WiFi, GPS, Play Store, a good touchscreen. You&#x27;ll have an instant hit. kajecounterhack 45 minutes agorootparentprevI love my mirrorless, but it certainly wasn&#x27;t 5 minutes. The first two lenses I tried (cheap ones obviously) were pretty underwhelming. Once I got a large aperture lens, I started to really get it. Even then, so many of my photos came out dark or blurry because I hadn&#x27;t learned how to pick settings or focus for different lighting conditions and subject movement speeds. Autofocus on consumer cameras is pretty trash compared to iPhone&#x2F;Pixel. EyeAF my ass.These camera companies need to invest more in their software. Superzoom, night sight, subject tracking and smart autofocus should be table stakes. Auto mode on my mirrorless should at least be on par out of the box with my phone. It&#x27;s sad that the pixel phones with very old Sony sensors can take better 10x pictures than mirrorless out of the box. They need to worry less about better lenses and sensors, and worry more about better onboard compute capabilities. reply ClikeX 1 hour agorootparentprevMy mirrorless definitely beats my iPhone. But I have to put in more time, it’s not with me at all times and I need to transfer the images.In the end, the best camera is the one I have with me. And if I can take a pretty good portrait shot of my kid while I’m at a diner then I’m happy.To me, the memory&#x2F;moment is more important than the “quality”. reply olivermuty 24 minutes agorootparentprevI bought a $1200 mirrorless which was supposedly the best in class a couple of years ago. All my photos look like they were shot on a potato compared to my iphone.Not to mention that I don&#x27;t walk around with that mirrorless camera in my pocket at almost all times. reply cooper_ganglia 1 hour agorootparentprevThe magic of phone cameras lies in their convenience. reply DonHopkins 1 hour agorootparentprevIt&#x27;s mainly zillions of photos of kids and pets and food, so it doesn&#x27;t matter if other viewers aren&#x27;t impressed, they&#x27;re impressive to the person who took them. reply cdogl 4 hours agorootparentprevI can’t help but nitpick that this isn’t really an anthropologists’ domain. reply jb1991 3 hours agorootparentWhy not? Studying a culture from hundreds of years ago and measuring its advances in various ways, technological and societal.. seems fitting for the National Geographic&#x27;s definition of anthropology as \"study of the development of human societies and cultures\". reply guappa 2 hours agorootparentThe point is moot. Apple and google cloud won&#x27;t exist and all the photos will be gone. reply gambiting 1 hour agorootparentYou do know that people still print photos, right? Some of them will definitely survive. And unless the human civilization collapses I don&#x27;t see why some digital media wouldn&#x27;t survive either. reply fragmede 17 minutes agorootparentApple and Google seem dominant now, but how much stuff do you have from Yahoo and Myspace and Flickr? reply gambiting 3 minutes agorootparentI don&#x27;t understand why that&#x27;s relevant. I never upload my photos anywhere. I&#x27;m pretty sure my photos that I have printed and&#x2F;or stored locally don&#x27;t go anywhere if Google shuts down tomorrow. DonHopkins 54 minutes agorootparentprevSpeaking of photography and the progression of technology (and not spoiling the amazing final scene of the story) I highly recommend reading this poignant prescient classic:\"The Wedding Album\" short story by David Marusek>\"Wait a minute!\" shouted Benjamin, waving his arms above his head. \"I get it now. we&#x27;re the sims!\" The guests laughed, and he laughed too. \"I guess my sims always say that, don&#x27;t they?\" The other Benjamin nodded yes and sipped his champagne. \"I just never expected to be a sim,\" Benjamin went on. This brought another round of laughter, and he said sheepishly, \"I guess my sims all say that, too.\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Wedding_Album_(short_story...>\"The Wedding Album\" is a science fiction short story by David Marusek. It was first published in Asimov&#x27;s Science Fiction in June 1999.>Synopsis: After their wedding, Anne and Ben realize that they are merely recordings of the real Anne and Ben, destined to relive the hours surrounding the wedding for all eternity.https:&#x2F;&#x2F;www.goodreads.com&#x2F;en&#x2F;book&#x2F;show&#x2F;13576562>With wedding photos and videos and mementos of all kinds, newlyweds attempt to hold on to their special day and to cherish it forever. Someday technology may enable us to record not only our appearance and voices but everything we know, feel, fear, and love at the moment the shutter clicks. Then our wedding mementos, like Anne and Ben’s in this story, take on a life of their own in a world where love may be eternal, but the world is not. Till deletion do us part . . . replyplastic3169 3 hours agorootparentprevI think just the fact that people started practicing every day photography and seeing hundreds of shots done by peers everyday will be a huge factor in raising quality as well. reply matsemann 4 hours agorootparentprevYes, nothing really wrong with this. Just pointing out that what hits the sensor is far off from what&#x27;s being saved.However, some phones now even apply AI filters to fill in detail it didn&#x27;t capture. Like adding craters to the moon.And the thing about contrast, sharpness etc is that \"more always looks better\" at a quick glance. So when people are doing comparisons between phones etc, the one destroying the picture the most might be declared the winner. replyOJFord 2 hours agoprev> Standard iPhone video is designed to look good. A very specific kind of good that comes from lots of contrast, punchy, saturated colors, and ample detail in both highlights and shadows.I remarked to my wife showing me a video recently that you could tell it was taken on an iPhone, I don&#x27;t think it&#x27;s just the &#x27;punchiness&#x27;, for me the main thing is the way it seems to attempt to smooth out motion - the &#x27;in&#x27; thing seems to be to sort of spin around showing what&#x27;s around you while selfie-vlogging and tik-tokking and what-notting, and iPhones make it look like you did it with a steadicam rig that&#x27;s not quite keeping up. reply basisword 1 hour agoparentThat’s a specific camera mode (action mode I think). Does the standard video mode also do heavy stabilisation? reply OJFord 1 hour agorootparentDoes it perhaps auto-enable when it deems it appropriate?I don&#x27;t have an iPhone, I&#x27;ve just noticed this (perhaps it&#x27;s more obvious to me because I don&#x27;t have one) in others&#x27; videos. reply mattigames 49 minutes agoparentprevSomeone used an iphone to record their desktop screen playing call of duty and the top comment on Reddit was how it made the game look Disneyesque, a spot-on assessment. reply amaterasu 3 hours agoprevIf I was a prosumer&#x2F;hobbyist video equipment company, I&#x27;d be terrified about what Apple does next. They already have significant penetration into the editing market (both with Final Cut, and codec design), they control a number of the common codecs, and they have _millions_ of devices in the field along with substantial manufacturing capability. The cinema end aren&#x27;t in trouble yet IMO, but the rest should be concerned... reply sbierwagen 1 hour agoparentCell phones already killed standalone cameras: https:&#x2F;&#x2F;d3.harvard.edu&#x2F;platform-digit&#x2F;wp-content&#x2F;uploads&#x2F;sit...This is just the mop-up operation. The only products left are going to be super-telephotos for live sports (sales: a hundred a year, if that?) and 4K+ IMAX digital cine cameras. reply vGPU 1 minute agorootparentNot even close. The pocketable point and shoot cameras? Sure. DSLR’s? Not a chance. I’ve gone the upgrade path from a canon 6D to 5D4 to R6. The R6 especially is phenomenal and there isn’t a single phone that can even try to come close to what it can accomplish even in “auto” mode. reply canbus 8 minutes agorootparentprevmaybe for the average consumer. but how many professional photographers do you see using an iPhone?sensor size matters for low-light stuff too. sure, an iPhone can do a pretty good job at taking several pictures over say a 2s. exposure, but there _will_ be artifacts in the shot as there isn&#x27;t physically enough light to form a legible image regardless of post-processing.this is just one of many reasons why digital cameras are NOT at the brink of collapse yet. reply MarkMarine 17 minutes agorootparentprevFor taking photos and sharing them in the digital only space, sure I’ll buy that for the regular consumer. Making prints will expose all the small sensor flaws that exist quite quickly. I know it gets better every year but I used my phone camera (14 pro) to capture a few important shots that I would do anything to go back and had on a full frame sensor or film for. reply ClikeX 59 minutes agorootparentprevThey killed consumer point and shoots, not professional interchangeable lens cameras. reply quenix 1 hour agorootparentprevLooks like they killed cameras with built-in lenses. Cameras with interchangeable lesnes, which would&#x27;ve been used by the pros, have kept their market share identical if not grown a bit. reply m463 46 minutes agoparentprevI think what is terrifying is that they&#x27;re better enough to kill the details.Sort of a contrived example... you&#x27;re a pro and let&#x27;s say you NEED a headphone jack, but apple just killed headphone jacks.But more indirectly, they killed the lower volume, higher margin folks with alternatives that offer a headphone jack, and maybe even XLR headphones and microphones.An analogy might be tesla giving you a 90% better car experience, except they have killed off the dashboard. (now they&#x27;ve killed control stalks like PRND and turn signals) reply drra 1 hour agoparentprevCinema is safe from the optics point of view as achieving some of the effects of large sensor + large lens is impossible with a phone size camera. But Apple has it cracked and they could easily crush that market. They have a great sensor with enough resolution per inch, great dynamic range, ability to produce lenses with super low defects and have enough processing power. Sensor wise ~100 megapixels should be enough to replicate fine grain of a good movie film and iPhone 15 sensor&#x27;s dynamic range of 12-14 stops is on par with film already. reply mrtksn 3 hours agoparentprevWell, Apple seems very cozy with blackmagic design but that doesn’t mean that they aren’t going to be sherlocked. Apple already has offerings in all these categories, it’s just that the markets are different and will stay different some time more because of workflows and laws of nature but the laws of nature don’t seem as safe anymore.Currently, the best editing software for social media appears to be CapCut as its ease of use for the power it provides is miles ahead of anything else. reply novok 2 hours agoparentprevThe market for &#x27;actual&#x27; pro & prosumer cameras and such is pretty tiny. I think they&#x27;ll be pretty safe for quite a long time.But they have pro video editing features! Yes, but it&#x27;s a subfeature of their general platform, so they can &#x27;count that low&#x27; for a hardware feature like that, that will also be useful for their entire userbase since everyone takes videos with their hardware and watches video on their devices anyway. reply HALtheWise 6 hours agoprevIt&#x27;s always surprised me that there&#x27;s not more interest in log-scale&#x2F;floating-point ADCs built directly into camera sensors. Both humans and algorithms care a lot more about a couple-bit difference in dark areas than light, and we happily use floating point numbers to represent high-range values elsewhere in CS. reply morcheeba 5 hours agoparentThere was a company that did this circa 2003 - SMaL. Their \"autobrite\" sensor is built to capture log-scale natively. They&#x27;ve switched owners twice since then, but it seems like they&#x27;re getting more traction in car vision systems than in professional video.https:&#x2F;&#x2F;www.vision-systems.com&#x2F;cameras-accessories&#x2F;article&#x2F;1... reply numpad0 4 hours agoparentprevThere is no floating point ADC, just a stereo assigned to two volume levels to be stuffed into a float.hardware accelerated HDR on cameras are commonplace these days, especially in dashcams and CCTV cameras. reply kqr 4 hours agoparentprevIs the quantisation error on a modern 14-something bit sensor really that big of a problem compared to something like the inherent shot noise of dark areas? reply amaterasu 4 hours agoparentprevSome sensors do this internally, unusual though. The rest of the high-end ones apply curves manually in software directly at the egress of the sensor. The reason they don&#x27;t in all cases is that it complicates black level correction, gamut transforms and demosaic operations (without some assumptions). reply verall 5 hours agoparentprevA lot of the processing steps expect linearity and would have to be reworked for floating point or log scale data. Most HDR sensors are using some kind of logarithmic compression for sensor readout, but I&#x27;ve never really heard of a floating point adc. Google seems to suggest tbey&#x27;re not readily available. reply raverbashing 4 hours agorootparentYeah there isn&#x27;t, it&#x27;s not that simple, even at sensor level to have that much dynamic rangeA linear ADC with enough range is usually fine, you can do the math later. But maybe for this case it needs a non-linear element before the ADC? (no idea how log recording needs anything at the HW level) reply fiedzia 4 hours agoparentprevMost recently microphones&#x2F;recorders started using it for recording sound reply _kb 3 hours agorootparentFrom my understanding, the ADC&#x27;s are still fixed point and linear. Two (or more) then run in parallel over different signal levels to produce the 32-bit float output.Encoding audio with different log-scale companding has been around for some time too (since the 1970&#x27;s) with A-law and mu-law in G.711. reply u320 1 hour agorootparentIt doesn&#x27;t really matter HOW they do it, as long as you get the advantages of float encoding (practically infinite headroom). Of course if you zoom in enough there will be something in there that uses integers, but this would be true for e.g. a floating point adder as well. reply nvy 4 hours agoparentprevI feel like professional&#x2F;prosumer photogs, aka the kind of people who buy fancy SLR cameras and serious business lenses, probably already know this stuff. I also suspect that the vast, vast majority of phone users just want subjectively good-looking photos. reply dclowd9901 4 hours agorootparentYeah, 100%. About 99% of the customer base just wants to take a good photo with the smallest effort possible. Which makes it even more remarkable the company cares enough to include this kind of functionality in a consumer product. reply pas 20 minutes agorootparentApple sells a very enticing mix of feeling and product, the good old Jobs distortion field. They always had an enormous influence on what&#x27;s cool, what&#x27;s premium, what you should want, what&#x27;s the baseline, etc.And it&#x27;s entirely possible that they got to the point that they are now saying pro stuff is cool. Even is 99.x % of users won&#x27;t really use it. reply LoganDark 4 hours agorootparentprev> Which makes it even more remarkable the company cares enough to include this kind of functionality in a consumer product.Maybe people are just starting to get sick of this argument being used against everything. reply robertlagrant 3 hours agorootparentHow is this an argument against anything? reply LoganDark 3 hours agorootparentWhenever someone wants a new feature or gets disrupted by the loss of a previous feature, people go, \"you have to think of the consumers. Consumers don&#x27;t use those features, because consumers are stupid. I can&#x27;t see why anyone would ever add&#x2F;keep a feature that&#x27;s not going to be used by those consumers.\"The comment I replied to didn&#x27;t go nearly that far, but it&#x27;s an argument I&#x27;ve seen so often that I feel compelled to point out that \"consumers\" are not the only target audience for Apple - they are trying to also market themselves to creators, as well as professionals, and they absolutely know the difference and notice when things like these become available. reply robertlagrant 2 hours agorootparent> people go, \"you have to think of the consumers. Consumers don&#x27;t use those features, because consumers are stupid. I can&#x27;t see why anyone would ever add&#x2F;keep a feature that&#x27;s not going to be used by those consumers.\"Well, I don&#x27;t think \"people\" generally do that. It&#x27;s nothing to do with stupidity; just lack of need and interest.> The comment I replied to didn&#x27;t go nearly that farNot just \"didn&#x27;t go that far\", it wasn&#x27;t anything to do with that. It was just articulating pleasant surprise.If I were to return this thread to its objective origin, I would agree that if you&#x27;re selling a mass-market device, it&#x27;s surprising to cater to a fractional percentage of that user base. I don&#x27;t see how that&#x27;s contentious. reply LoganDark 47 minutes agorootparent> if you&#x27;re selling a mass-market device, it&#x27;s surprising to cater to a fractional percentage of that user base. I don&#x27;t see how that&#x27;s contentious.I argue it shouldn&#x27;t be that surprising because a lot of people find value in this 1%, and each person wants a different 1%.But maybe it&#x27;s surprising because most companies are stupid and don&#x27;t realize this. replyayoisaiah 2 hours agoprevLog seems like a strong reason to finally switch from Android to iPhone if you&#x27;re a photography&#x2F;filmmaking enthusiast like myself. The ecosystem is so much more mature and the gap seems to be growing not shrinking.Android has Raw Video with MotionCam which also produces insanely good results¹ (even better than iPhone&#x27;s ProRes video), but everything else just sucks.[1]: https:&#x2F;&#x2F;youtu.be&#x2F;O5fnGDR4i9w?feature=shared reply wraptile 1 hour agoparent> a strong reason to finally switch from Android to iPhone if you&#x27;re a photography&#x2F;filmmaking enthusiastCorrect me if I&#x27;m wrong but there&#x27;s nothing stopping Android of supporting Log (or similar). I&#x27;m not a video engineer but it really doesn&#x27;t seem that magic that it couldn&#x27;t be supported outside of iphone 15, right? My guess that if this gains any real traction it&#x27;ll show up in the next Android flagship. reply dharma1 1 hour agoparentprevMotionCam is great! They’ve been flying under the radar of RED lawyers (patent for compressed raw video) - long may it continue reply mirsadm 1 hour agorootparentI&#x27;m the dev of MotionCam. AFAIK the app is not infringing on that patent because I use my own form of lossless compression. reply pas 16 minutes agorootparentIf RED has a patent granted with a claim on compressed RAW data streaming&#x2F;storage, then it doesn&#x27;t matter which algorithm. (Though of course one could argue it&#x27;s too broad, but it&#x27;s not cheap to make this argument.) reply piperswe 5 hours agoprevI didn&#x27;t know it could record straight to USB-C storage! That gets rid of a major reason to spend crazy money on a 1TB phone, and it&#x27;s definitely a game changer for anyone shooting 4K ProRes. reply radicality 5 hours agoparentAfaik it’s actually not even possible to record that directly to the phone, it has to be into an external usbc drive. If I had to guess it’s probably because of overheating concerns with the high write rate. reply quitit 2 hours agorootparentThe article is slightly misleading here.The 4k60 ProRes mode is not available for shooting on the 128gb model until an external drive is added, but for any larger capacity iPhone Pro, the mode is available for shooting without the external drive. This doesn&#x27;t affect the Pro Max as that isn&#x27;t available in a 128gb configuration.Notably, a similar limitation existed with the iPhone 14 Pro using 4k30FPS, at the time the reasoning was that it simply fills the device too quickly to be useful. reply kalleboo 17 minutes agorootparentI have a 512 GB 15 Pro Max and it won’t let me record 4k60 ProRes to internal storage reply skunkworker 5 hours agorootparentprevThat requirement is for 4k60 Log only. 4k30 log will write to disk but takes up around 100mb&#x2F;sec. From some videos I shot last weekend. reply karolist 5 hours agorootparentTo capture any 4K ProRes footage with 128GB 15 Pro you need an external drive, this is presumably because 128GB model has a single memory chip and data write speeds are insufficient. reply slau 3 hours agorootparentprevIs that Mb or MB? reply skunkworker 3 hours agorootparent4k30 Log is approx 100MB&#x2F;second replydannyw 4 hours agoprevDoes this also disable the excessive sharpening of iPhone&#x27;s video processing?Even &#x27;ProRAW&#x27; photos are sharpened and aggressively denoised, which ruins detail. reply dharma1 4 hours agoparentIt’s mentioned in the article - turns sharpening way down. The footage still goes through iPhones ISP - with denoising etc - just with less processing and log profile reply foldr 2 hours agoparentprevThe sharpening is just the default display setting for ProRAW files in many apps. If I edit a ProRAW file in Lightroom and turn down the sharpening and denoising then it looks roughly as you’d expect (though there is some denoising inherent to multi-shot blending). It’s a tricky one, because when you’re aligning and stacking multiple captures, you definitely do want some sharpening (as the result is significantly softer than a single capture would be). But the default is maybe a little aggressive. reply canbus 12 minutes agoprevthe way i see it is it&#x27;s the video equivalent of a RAW file for a still picture. reply kqr 4 hours agoprevAlmost all the benefits mentioned in the video are (a) lack of post-processing and (b) high dynamic range. Is that what \"log\" means in videography? reply jorlow 4 hours agoparentLog is lower contrast so it&#x27;s less likely to clip (be a fully saturated color or pure white or black). And clipping inherently limits your max dynamic range.Log also means a \"look\" is not baked into the image so, since you&#x27;re starting from scratch, it&#x27;s 1) easier to tweak the images so you can cut between two cameras from different manufacturers without distracting differences and 2) you can give the image more of your personality.As a general note, I&#x27;ve found that in the world of \"cinematography\", tech terms aren&#x27;t used very rigorously and there&#x27;s a lot of cargo cult which comes from the benefit of one tech being conflated as a benefit of something else. It&#x27;s often hard to sift through the noise when learning. reply shrx 3 hours agorootparentBut clipping occurs before the log transformation. The sensor&#x27;s ADC is still linear and has a fixed dynamic range regardless of the output encoding format. reply jorlow 2 hours agorootparentSignificant clipping happens there yes but more clipping happens when the \"look\" is applied and contrast is added. reply rebuilder 4 hours agoparentprevNo, ”log” just means some form of logarithmic response curve when encoding color data. You don’t necessarily get better dynamic range per se, but you get a more useful distribution of the light samples your sensor is taking. reply jillesvangurp 37 minutes agoparentprevA little bit. The log format is non linear. This means there are more details in the shadows relative to the really bright areas. This mimics the human eye and brain which also do not have a linear range of sensitivity.Basically, the common unit of light in cameras (a stop) is one click on the aperture wheel. E.g. going from 1&#x2F;11 to 1&#x2F;16 halves the amount of light. Some cameras of course have a few settings in between. It looks linear to us but it is effectively logarithmic. The dynamic range of the human eye is much larger than the typical camera, screen, or print medium. The human eye has a range of about 20-22 stops (between black and white). A good camera might get between 12 and 14 stops. A decent screen might get to something like 8-10 and print medium is more like 5-7. Taking photos and shooting videos involves a lot of creative choices about what looks natural to us. HDR is basically taking and combining multiple exposures in a way that still looks natural to us on a medium that has less dynamic range than our eyes (-ish, a lot of HDR photography looks a bit unnatural for this reason).Digital photo processing is about compressing and moving light around to make the most of the much more limited dynamic range of the screen or print medium you are targeting relative to the camera that you used to capture that.When you do that, most of the interesting information is going to be captured in the darker portions of the image. You typically expose for neutral grey values which is only about 18% of the light. That means half of the darker information (shadows) is in that 18% range of values. And the other half is in the brighter part. Except our eyes are much more perceptive of the darker bits. So, a linear format is not ideal to store that. A log format allocates more bits to the dark half and less to the other 82%. That&#x27;s a good thing because that allows you to do things like brighten shadows and pull out detail there.The log format does this by applying a log function to the raw sensor readings. That&#x27;s why the format looks so flat because all the values end up being relatively close to the 18% mark (neutral). You \"undo\" this by applying a suitable lut that multiplies the values suitably. You deepen the shadows to near black and brighten the bright stuff to near white. The difference is that you now have full control over this process; can move the white, grey, and black points around. And you can apply color math to the log values before you apply the lut. This is not that different from how you&#x27;d process a linear format except now your starting point is better as you are using more bits for the darker parts of the image than for the lighter parts. This gives you more of the captured dynamic range to play with in post processing.The weakness of the iphone is that while it stores log format, it&#x27;s not really capable of switching between LUTs on camera or while you are shooting. I&#x27;m guessing this just takes too much CPU&#x2F;battery. So, you have to wait until post processing to see what the end result is going to look like. Some high end cameras have a lot of in camera processing that you can tweak in post processing. reply adityapurwa 2 hours agoprevI always wondered why some of the raw vs edited video on social media shows the raw one as a very washed and unsaturated picture. I even thought they made the raw looks so bad, so that the edited one looks great.I never owned a pro-camera, only a smartphone. So, reading this article now I learned that it was washed and unsaturated for a good reason. Is this Log thing 15 pro specific or its software so that we can use it on an older iPhone? reply stephen_g 2 hours agoparentThis was a huge problem even for professionals at one point - there was a time starting a bit before 2010 where more cameras started to switch from doing all the process in camera to raw and log recording, and people didn&#x27;t understand how to work with it, properly expose it, etc.I remember for example with RED&#x27;s first cinema camera, seeing people do shootouts (camera comparisons) where they&#x27;d record and compare the low-quality, partially decoded monitor output that didn&#x27;t have a proper LUT applied, to HD cameras that did all in-camera processing. Later cameras could do all the 4K processing and apply proper LUTs in the hardware in real-time, but earlier ones didn&#x27;t have the processing power, you had to do it all in post. People just didn&#x27;t get it, and it worried people when things come out all washed out before applying any kind of LUT.Crazy some of this is trickling down into phones. reply wruza 1 hour agoprevI’m very far from photography. Can someone explain why this is revolutional or something? I thought that for “pro” photos you just get the original pixel values from the sensor and work from there. X, Y and some color format of a specific bit depth (is that RAW?). What does “Log” do basically? Save space? Limit the exposition? Or is it just a format that existed for decades and iphone decided to support it? Or is the industry in such state that integrations like this are huge? It’s not clear from the video. Thanks! reply danw1979 1 hour agoparentThe article does a pretty good job of explaining it. I went from complete ignorance to layman’s understanding in the 10 minutes it took me to read it. reply bujak300 1 hour agoparentprevAs far as I naively understand, the number of distinct colors in a digital image is limited, so the \"log\" mode is using the available color space in such way that there are more details preserved in highlights and shadows. but then \"color grading\" or \"color look up table\" needs to be applied, to recover the original colors, because the \"log\" video looks greyish and washed out. reply t0bia_s 3 hours agoprevWho is a target audience? Most Apple users wont spent time in postproduction and colour grade their footages. Pros will stay with dedicated technology made for cinematography. reply scottapotamas 2 hours agoparentPeople vlogging would probably prefer the ergonomics and weight of a phone over something more serious, so I wouldn&#x27;t be surprised if this competes with the GoPro on functionality and image for &#x27;walk and talk&#x27; people?Guerrilla film-makers will probably love it - iPhones aren&#x27;t really noticed and are allowed in plenty of places where serious cameras aren&#x27;t.I can also see it being useful for some specific commercial ad work, I&#x27;ve seen people specifically shoot on phones to get the relatable phone &#x27;look&#x27; for specific shots. reply dathinab 2 hours agoparentprevfirst some people use it as a status symbol, naturally they will buy the pro no matter if useful or notthen there are tons of semi-professional (semi wrt. photo&#x2F;video capture, not wrt their job) people on platforms like YT, TickTock and similar which do only use their phone for capturing, they probably love thislets also not forget the people which aspire success on YT, TickTock and otherssimilar a lot of hobby photographers don&#x27;t bother with dedicated technology anymore so they might like that, tooand the people which don&#x27;t carry around a laptop (e.g. on holiday trips or even business trips) but might want to send slightly improved photos from there isn&#x27;t small eitheranyway I guess the main selling point is like \"not locking poor\" for thos with confidence issues or more likely bad luck to live in less healthy social circles reply te_chris 2 hours agoparentprev“Pros” is a wide space these days. Hell, my mate has a film released on hbo max which was entirely shot in 4K on iPhones probably 6 years ago now.Think of the sheer amount of ‘content’ (used pejoratively) these days. That is not being made by traditional cinematographers. It’s videographers, maybe with a pro camera, but maybe with an iPhone, or maybe one pro camera and an iPhone or two for backup. Think of weddings or similar as well, massive demand. Apparently everything needs to be video so why not this. As an aside, aforementioned director shot our wedding as a favour to us (in 2019) on a 4K lumix and an iPhone.It’s the camera you have on you, innit. reply indeyets 2 hours agoparentprev“Enthusiasts”, YouTubers, … reply t0bia_s 1 hour agorootparentWhy? Whats the benefit of having slightly more options on postproduction when you put output to compressed, 8bit, low bitrate platform that most people consume on their tiny mobile screens with blue light filter? reply keyle 5 hours agoprevGreat video btw. Well explained and as succinct as possible. reply emsixteen 2 hours agoprevGreat video - Concise and informative, without any guff. reply expertentipp 34 minutes agoprevSpeaking of their raw formats. What&#x27;s up with the iphone HEIC photo format? I only convert all HEIC to JPEG on Macbook and have no idea what advantage this format can offer me. reply zik 29 minutes agoparentYeah, what the heic? reply justsomehnguy 4 hours agoprevThe video is so good I watched it to the end despite not even having iPhone nor having any plans to have it or shoot videos. Packed and succinct.But makes me wonder how soon we would see an... SSD iPhone cases? Because you can always duct tape the external drive to the phone but it would block the screen. *grin* Sure, you can use double sided tape, but... And slightly tangential - how short and compact USB-C cable can be? Sure there are tons of the angled on the market, but I assume they aren&#x27;t guaranteed to give you full 10gbps Gen2 speeds. reply robertlagrant 3 hours agoparentI&#x27;m thinking a hand-held mini Steadicam with a hard disk as the counterweight. reply walteweiss 3 hours agoparentprevI think that would be not a short cable, but more like Mophie juice pack (it’s a case with external battery). Also, back in the days I bought a card reader for an iPad 3, for 30 pins connector. Could be something similar, as SD-cards are quite big these days as well. reply Cockbrand 3 hours agorootparentI’d assume that SD cards don’t provide the required write speed, limited by their flash memory or even the interface specs. reply justsomehnguy 2 hours agorootparentprev> more like Mophie juice pack (it’s a case with external battery).>> SSD iPhone casesBut yeah, this is what I vaguely had in mind though I did forgot those rigid cases even existed, because my phones lasts at least two days on their own finger guns.Considering the case would be quite fat and rigid for the SSD alone, it would make sense to skip cable shenanigans and just use a &#x27;dock style&#x27; connector. reply yieldcrv 6 hours agoprevIs this essentially RAW at 24-60fps? reply pixelbath 5 hours agoparentIt is less processed, but TFA points out that it is definitely not RAW, particularly in the section titled \"Log is Half Baked.\" reply mavidser 5 hours agoparentprevThat&#x27;d be &#x27;ProRes RAW&#x27;, which I don&#x27;t think an iPhone can shoot in. Log is still processed video, just graded in a flatter profile so you can do more degrees of adjustments in different dimensions like color and exposure.RAW footage, can barely be called video. Those files don&#x27;t even have any White Balance and ISO data baked in, just raw data from the sensor, providing even more amount of control in post production, at the expense of working with extremely large files. reply isomorphic 4 hours agorootparentAlso, just to muddy these waters, Apple has the ProRAW format--which isn&#x27;t a video format at all, it&#x27;s a still-picture format.https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT211965 reply mikae1 4 hours agoparentprevNo, it&#x27;s demosaiced[1]. Not raw data.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Demosaicing reply WithinReason 2 hours agorootparentSo then why not just use the raw data? Demosaicing triples the number of pixels, so expands the data by 3x. reply tuukkah 1 hour agorootparent(Conceptually, the number of pixels remains the same but the result of demosaicing is RGB pixels so what triples is the number of channels.)I hear it&#x27;s good to perform demosaicing, denoising and super-resolution in one step, so perhaps that&#x27;s what&#x27;s happening here?EDIT: on the video (section \"Log is Half-Baked\"), they also mention the processing includes tone mapping, color adjustment and lens distortion correction. reply perbu 4 hours agoparentprevThe video is still compressed, but the colors are RAW. Sort of. reply sudosysgen 5 hours agoprevIt&#x27;s a bit of a gimmick. These phones just don&#x27;t have the noise performance to make log video work outside if very very specific conditions. I had it also on my old LG V30 and it only was remotely useful in full sunlight (and since we&#x27;re talking about very low processing, not much changed since then).This is inevitable because the noise floor is just too high to have a large usable dynamic range unless illumination is high.Combined with video compression it&#x27;s just not great. It&#x27;s not really even unique to smartphones, many DSLRs&#x2F;MILCs when they first started supporting log video had similar issues, but obviously it&#x27;s going to be much worse for a smartphone. reply mirsadm 1 hour agoparentThis is not true. We&#x27;ve compared RAW video from smartphone cameras to a couple of professional cameras and the difference is not as much as you might think. reply dharma1 4 hours agoparentprevThe sensor size is the limiting factor here re: noise performance and dynamic range, but the iPhone does better than most with some ML and other “computational photography” tricks for denoising that other small sensor cameras don’t have. And ProRes is a great codec that doesn’t really have compression artifacts at high bitrates.It’s not going to replace your Alexa or even full frame dSLR but can be useful and is a welcome evolution. reply newaccount74 2 hours agoprevSlightly off-topic, but is there a way to disable some of the photo processing on iPhones?For example, iPhones automatically brighten people once they detect a face, which is especially noticeable when taking photos against the light. It ruins the contrast and makes the photo look really bad.Is there a way to turn that stuff off? reply quitit 2 hours agoparentThe purists answer is to shoot RAW, but many just want a solution that doesn&#x27;t require any post-effort off the device, for that the answer is \"sort of\":1. Taking Live photos can be a good work around, since this mostly maintains the individuals frames which are used to build the full image. Use the built in photo editor to go to a different frame in the live photo and set this as the Key Photo - this dodges a lot of the HDR&#x2F;image adjustment process.2. Using the AE&#x2F;AF lock can prevent a fair bit of the automated adjustments from taking place. (Tapping the screen on dark areas or hot points will adjust the exposure, holding your finger for a moment will turn on the AE&#x2F;AF lock keeping it there as you move the phone around.)3. If it&#x27;s not just for one shot, but all shots you take. Go to the Camera settings and turn on preserve \"Exposure Adjustment\", in the settings which starts each camera session with the exposure settings preserved from earlier - and simply keep this on 0.0. This can also make it more straight forward (one tap) to undo much of the automatic levelling without locking AE. Similarly you may want to also disable night shooting for darker scenes, by preserving that setting too.4. On older iPhones, disable the HDR mode. reply liminalsunset 1 hour agorootparentAdding on to this, newer iPhones have a ProRAW feature. Various tech youtubers have documented ways to create a shortcut in the Shortcuts app to automatically convert them to HEIF or JPEG, which might impose slightly less processing.Similarly, ProRAW photos imported to Apple Photos on Mac can be reduced in processing by clicking Edit Photo, and then hitting Save Changes without necessarily changing any settings (or just changing something inconsequential would work too). Doing editing forces the Photos app to discard the preprocessed image embedded in the ProRAW and replaces it with a software processed version which I find slightly more natural. However, ProRAW is still processed by the phone, so you need to download a third party app and shoot classic non pro RAW (then use the same conversion shortcut maybe) on top of it. reply liminalsunset 2 hours agoparentprevThis is a slightly off topic response as well, but some Android phones are currently capable of shooting RAW video in 12MP, sometimes at 60fps (more than 4K 8.3MP). Google shot a music video [1] (unfortunately only available in 1080p) on the Pixel 7 Pro using a third party app called MotionCam Pro. The app shoots RAW video which can be imported into DaVinci Resolve or similar, or rendered to a mp4 in-app with any log profile you prefer optionally, and has no processing applied.According to the developer of something called AMVR [2], the quality obtainable is much higher than that of even the aforementioned log files from iOS.I asked whether it would be feasible to shoot DNG on iOS as video and was told that iOS lacks a camera API that is performant enough, resulting in several fps only. I haven&#x27;t personally tested it though, so maybe this could be a fun project.[1] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SyeS_xYxCLI&pp=ygUfYW15IHNoY...[2] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=_Xra4ATrWZ4 reply mirsadm 1 hour agorootparentI&#x27;m the developer of MotionCam Pro. I have no affiliation with AMVR, it bothers me that they present themselves as somehow associated with me.As a side note, whenever Apple takes steps to release a feature aimed at professionals there is a significant uptick in users trying out MotionCam. I think there is a small but very vocal group of users that have wanted something like this for years but have not been catered to. reply kouru225 2 hours agoparentprevI think you gotta buy a more professional camera app to fix stuff like that, and even that doesn’t work for some things.I’m always pissed about bad the iPhone is at taking photos of rainy days. reply dijit 2 hours agoparentprevI think blackmagics iPhone App disables a lot of automatic processing; however I haven&#x27;t tested post-processing behaviour myself.Given that it gives such fine-grained control of the sensor, it wouldn&#x27;t surprise me though. reply omnimus 2 hours agorootparentBlackmagic Camera app doesnt take pictures. Its for video. reply joking 8 minutes agorootparentwell, there are some other apps like Halide, but I doubt they are free. reply dijit 2 hours agorootparentprevQuite right, this shows how little I actually looked at it. reply ayoisaiah 2 hours agoparentprevBest thing is to shoot in Raw or ProRraw and post-process yourselfEdit: Fix typo reply olliej 2 hours agoparentprevIt might be worth trying halide? I know they used to have \"we make photography better\" with a pro-photographer angle, but I guess to an extent it depends on what the camera is providing, but I recall halide kind of implied they got raw data? reply liminalsunset 2 hours agorootparentHalide has several modes, one of which is giving you the full unadulterated RAW, and optionally doing in-app jpeg conversion with less offensive processing. These photos have more noise due to the limitations of the sensor.Halide, like the official camera app, can also shoot ProRAW but ProRAW is not totally RAW, it has been processed with the frame stacking to reduce noise, but introduces sharpening artifacts. reply ricardobayes 2 hours agoprevI don&#x27;t like this new modern color keying in videography, to me everything looks yellow and washed out.What&#x27;s wrong with contrast? reply chrismorgan 2 hours agoparentLook at the picture of the dog: some of its fur is overexposed, and you can’t get the values back. A logarithmic scale means you lose less detail at the extremes (bright and dark), so the log picture isn’t overexposed. reply user3939382 6 hours agoprevnext [3 more] [flagged] hsdropout 6 hours agoparentBlammo&#x27;s best seller! reply sen_armstrong 5 hours agoparentprevIt&#x27;s long, a schlong, a marvelous dong, everyone knows it&#x27;s dildo! https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20010404231114&#x2F;http:&#x2F;&#x2F;www.damnth...No- it&#x27;s been so long, the MIME type is wrong - try ebaumsworld.com: https:&#x2F;&#x2F;www.ebaumsworld.com&#x2F;videos&#x2F;its-dildo-song&#x2F;971556&#x2F; reply tehsauce 5 hours agoprev> Apple shocked us all by addressing this head-on: The iPhone 15 line charges via USB-C instead of Lightning, and this standard USB port can do a lot reply Insanity 5 hours agoparentEven more so than their revolutionary new design? reply xchip 1 hour agoprevLUTs are a pretty basic stuff and videogames have been using them for ages, plus HDR, tone mapping and color grading. Old stuff. reply 3x35r22m4u 5 hours agoprev [–] Could someone please confirm or deny Samsung has it better because they included 10 times larger sensors, 20x better zoom, titanium since 2017 and hired BTS for their ads?Reference: https:&#x2F;&#x2F;youtu.be&#x2F;dLHJl7mwY7M?si=e0Cm2q4bnn_u14Qf reply bux93 2 hours agoparentSince Apple sources their camera chips from Sony, the more relevant question is how Sony&#x27;s flagship (the Xperia 5 V?) stacks up with its Sony IMX888 vs the Sony IMX803 in the apple. It used to be that Sony&#x27;s flagship had a slightly better imaging sensor than the Apple flagship, but I don&#x27;t think that&#x27;s the case any more. At any rate, Android doesn&#x27;t seem to provide an API that exposes log&#x2F;raw-ish sensor data for video (although for stills, it&#x27;s been there since 2015). reply DiggyJohnson 4 hours agoparentprev [–] How could that possibly be confirmed or denied? If you’re not asking a question don’t use a question mark: just make your point. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post primarily discusses the inclusion of log video recording in the latest iPhone 15 Pro and Pro Max, a format that provides increased flexibility in color grading and editing.",
      "It explores the advantages of using log footage, including the ability to select different visual appeals and naturally grade colors, as well as its compatibility with various color spaces.",
      "The post also refers to the release of the Blackmagic Camera app for the iPhone, which offers enhanced manual controls and features, boosting the creative possibilities for video shooting and editing with these devices."
    ],
    "commentSummary": [
      "The discourse involves the comparison of smartphone cameras and traditional cameras in terms of quality and convenience.",
      "The conversation touches upon the influence of smartphones on the camera industry and the capability of capturing high dynamic range (HDR) images on smartphones.",
      "Discussions also include the application of log format in videography, post-processing, shooting options, and the camera chips featured in Apple and Sony's flagship smartphones."
    ],
    "points": 468,
    "commentCount": 130,
    "retryCount": 0,
    "time": 1696994292
  },
  {
    "id": 37835995,
    "title": "Building a 42-inch E-Ink frame for generative art",
    "originLink": "https://eliot.blog/e-ink-frame",
    "originBody": "Building a 42-inch E Ink Art Frame Recently, we wanted to find a way to display Generative AI art in our office at PhotoRoom. A large TV would have done the job, but we wanted to go for something more original. So when someone informed me that the E Ink Corporation was selling 42-inch E Ink panels, I was instantly nerd-sniped. That was the perfect project to fuel my non-traditional display addiction. After the flip-dot display salvaged from a UK bus, the quest for an even bigger build was on. The result The frame changing from one image to another. The GIF above doesn't help with quality, HD video available here Cables are missing in this picture First version. In the process of building it, I broke the top half of the display The process Sourcing components Some advanced Google-fu was used to find a supplier shipping to Europe. Minimum order quantity: 3 pieces. After a bit of back and forth, the components arrived. Can you guess it's fragile? The supplier provided the displays as well as the necessary driving board. Framing While the pink packaging certainly has its own charm, we decided to go for something more minimalist. Finding a frame shop that could handle this \"special\" project was not easy. Some straight-out didn't answer but one of them was interested in that unconventional build. So we (well, mostly he) set on to custom build a framing system that could enclose the panel without breaking it and without using glue to make repairs / adjustments easy. The first build we created didn't turn out great: the display was slipping between the layers. In the second version, we found a way to keep it sturdy. Controlling the display The display controller can be talked to using USB, so I added a Raspberry Pi on the back. It exposes a small webserver that can receive images to display. Back of the frame. Notice how the ribbon cables don't align: I think I picked the wrong controller board variant Preprocessing images The display can only display 16 levels of gray. Displaying images without preprocessing leads to a blocky pattern on gradients in images. Thankfully a few techniques exist. One of them, closer to dark magic than science to me, is called Blue Noise dithering. It consist of adding a blue noise pattern to an image and them thresholding. Simple yet impressive, see for yourself. Before/after dithering using blue noise. Notice the pattern in the sky Displaying images If you have ever used a kindle, you'll know that every few pages it refreshes itself to avoid ghosting. We had the same problem where the previous image would appear slighly faded out, mixed with the new image. The trick is to run a full black then full white image. We found that running 2 full white images did a good enough job, without disrupting the experience too much (see video) Next steps The first version of this project now sits in the office. There are a few ideas we'd like to try: Make it battery-powered: right now it still requires a power cord. It would allow putting it on a wall without cables Ask ChatGPT to generate prompts for us, turning it into our very own art curator Acknowledgements A huge thank you to Charly for the idea and support in this building process, to Nico for the framing and to Florian for the dithering code. — Eliot Andres",
    "commentLink": "https://news.ycombinator.com/item?id=37835995",
    "commentBody": "Building a 42-inch E-Ink frame for generative artHacker NewspastloginBuilding a 42-inch E-Ink frame for generative art (eliot.blog) 437 points by ea016 15 hours ago| hidepastfavorite159 comments xnx 14 hours agoWell done. There would be so many more cool applications of e-ink if it wasn&#x27;t $2,500 for a 42\" display: https:&#x2F;&#x2F;shopkits.eink.com&#x2F;en&#x2F;product&#x2F;detail&#x2F;42&#x27;&#x27;MonochromeeP... (minimum order 3)For comparison a 42\" full-color 60fps TV with remote, speakers, wifi, etc. etc. is $140.It seems like the patent holder could be making a lot more money if they dropped the price. reply FirmwareBurner 13 hours agoparent>For comparison a 42\" full-color 60fps TV with remote, speakers, wifi, etc. etc. is $140.Because it&#x27;s a lot easier and cheaper to manufacture large LCD panels at scale than E-ink film.>It seems like the patent holder could be making a lot more money if they dropped the price.I don&#x27;t know any company or shareholders that would say no to making more money so if this would actually be true then it would, but that&#x27;s not the reality.The FUD that large e-ink displays are expensive because of some patent conspiracy needs to stop.Large e-ink screens are expensive because manufacturing yields are very low and so are sales volumes.Source: worked with e-ink on products with big and small displaysEdit: So I&#x27;m saying the truth and getting down voted for it? Fine, then feel free to keep believing whatever you think is the truth. No point in discussing it further. reply TaylorAlexander 7 hours agorootparentQuestion for you. I always saw the problem as: only one company or limited companies can manufacture them. Those companies are having issues manufacturing them so the price is high. But if more companies were allowed to manufacture them, wouldn’t we see more people solving production problems and lowering prices? If there is one company with low yields and high prices, if everyone is allowed to make them then there is a monetary gain for new entrants willing to solve yield problems. But if things are heavily patent encumbered then the whole world is at the whim of one organization to succeed or fail - there is no competition.That is to say, it’s not that they’re charging high prices out of greed, but that there’s no competition forcing prices down. reply wayfinder 2 hours agorootparentNot OP but what you’re saying is absolutely true.But first someone has to invent it. Nobody’s gonna spend a few thousand or million dollars to invent something unless they get their money and more back, so that’s the point of patents.And if they patent something and don’t capitalize on it enough, that’s their loss, because they had to detail their invention in their patent application and that’s public information. Patents last only 20 years and for most industries, that’s not very long.(That said, I think software patents are a different beast. Because 20 years in software engineering is like 100 years in civil engineering, for example, since software engineering is like a few thousand years younger discipline than civil.) reply squigz 8 hours agorootparentprev> I don&#x27;t know any company or shareholders that would say no to making more money so if this would actually be true then it would, but that&#x27;s not the reality.I&#x27;ve never quite understood this sort of thinking. Companies aren&#x27;t perfect omniscient systems - the people in charge of various companies make poor decisions all the time, based on the same human biases we all have. So yes, it&#x27;s entirely possible they could make more money that way, but executives simply hear \"Lower the price\" and run away. I&#x27;m not saying that&#x27;s certainly what&#x27;s happening, but this implication that a company could possibly only do the thing that&#x27;s most beneficial to it is... strange, and very common thinking. reply solardev 9 hours agorootparentprevI didn&#x27;t vote either way, but this is news to me. As long as e-ink has been around, all I&#x27;ve ever heard were the patent issues. If you&#x27;re saying it&#x27;s actually manufacturing difficulties, can you provide any links or additional info? I believe you, but as someone not familiar with the field, I&#x27;d love to learn more both to counteract the patent narrative and just for my own curiosity. What is hard about e-ink at scale that other products don&#x27;t share? reply Qwertious 8 hours agorootparent>As long as e-ink has been around, all I&#x27;ve ever heard were the patent issues. If you&#x27;re saying it&#x27;s actually manufacturing difficulties, can you provide any links or additional info?For the record, the \"patents\" think has no links to back it up either.But to answer your question: E-ink screens are less than $100 (depending on the size), that&#x27;s already pretty cheap. They&#x27;re only expensive when compared to LCDs. LCDs are produced at a rate of billions per quarter*, with ~6 billion smartphone owners, and lots of them having multiple LCD devices e.g. laptop&#x2F;desktop screen, work computer&#x27;s screen, supermarket kiosk, tablet, smartwatch, car screen, etc.In contrast, e-ink screens have basically three applications: e-readers, e-notes (which is basically just an e-reader with a stylus), and supermarket pricetags. E-readers&#x2F;e-notes are a luxury item that you don&#x27;t need if you already have a smartphone&#x2F;iPad. There&#x27;s just no economy of scale for e-ink; not compared to LCDs.Because there&#x27;s no economy scale or obvious new markets to expand into, there&#x27;s not much budget for R&D, so the entire field moves slowly. reply tough 8 hours agorootparentSounds kinda like a problem looking for a solution, you could emagine e-ink taking off for a lot of thing sthat currently are not digital, but it would need to be also cheaper.If e-ink was x100 cheaper, we could use it in x1000 more use-cases reply wahnfrieden 9 hours agorootparentprevLow market interest reply maushu 8 hours agorootparentWith those prices I&#x27;m not surprised. reply bigfudge 13 hours agorootparentprevI think it would be interesting to hear from an insider (with examples&#x2F;evidence etc) what the mechanism is for eink to be more expensive. I&#x2F;e&#x2F; the fundamentals for why this is the case beyond economies of scale. Given this history of the patent it looks to the outsider like this probably isn&#x27;t the case and short sighted rent seeking is the problem. Would love to be wrong though, so if you do have a good explanation then here would be a great place to air it. I&#x27;ve not seen anything elsewhere... reply LapsangGuzzler 12 hours agorootparentI’m sure the ad-driven subsidy on TVs plays a huge part. TV prices have declined as “dumb” TVs have been phased out of the market. A more niche tool like a large e-ink display might not make as much sense from to price this way since ad systems need large networks of users to be relevant. reply mikewarot 10 hours agorootparentAs a check against this, I looked for 42\" computer monitors, which don&#x27;t display ads. There are $600 4k 42\" monitors in stock at CDW, for example. The same size 4k SmartTVs are about $300. So, yes... there appears to be about a 50% discount for TVs vs Computer Displays, part of which could be a subsidy, most of which I suspect is volume driving down the cost of production. reply komali2 8 hours agorootparentDouble check the refresh rate, input latency, and color gamut for your comparisons. Computer monitor user concerns are quite different than tv screen users. reply alwayslikethis 7 hours agorootparentprevTVs also spy on you. That data offsets some of the cost. They also often have lots of other compromises, from chroma subsampling to overscanning, that make them unsuitable as computer monitors. reply BoorishBears 11 hours agorootparentprevpeople underestimate how long e-ink stagnated: it seems hard to imagine an alternate timeline where e-ink had taken the place of current LED outdoor billboards for example... until you realize we were decades away from LEDs being cheap and robust enough for outdoor signage when eInk was already fundamentally similar enough the tech we have nowThe company that owned the e-ink patents couldn&#x27;t scale properly until they were acquired around 2010: in an alternate timeline where development open enough in the early 2000s, we might have ended up with eInk display modules large enough to be assembled into incrementally larger and more profit driving devicesEven now you see that with eInk store tags for example: imagine if the profit they&#x27;re driving now had arrived 20 years ago and gotten re-invested reply lostlogin 1 hour agorootparentThose outdoor LED billboards can burn in hell. They are distracting when driving and are awful when a pedestrian.If I knew an easy way to disable them, I’d be doing it. reply callalex 5 hours agorootparentprevI remember seeing e-ink price tags at Kohls (American department store) as early as 2004-2005. reply BoorishBears 5 hours agorootparentYou probably saw the LCD based price tags which have been around for ages: https:&#x2F;&#x2F;www.fixturescloseup.com&#x2F;2020&#x2F;10&#x2F;29&#x2F;kohls-sloped-digi...That said e-ink price tags aren&#x27;t new, them being stupidly cheap (https:&#x2F;&#x2F;www.aliexpress.us&#x2F;item&#x2F;3256804488587338.html) and increasingly wide spread in availability is the relatively new part. replydantheman 10 hours agorootparentprevYou say that, but patents traditionally hold back progress. If you look at the amount of horse power generated by steam engines - it spiked after the patents expired.There are probably many techniques and processes that would be tried and developed if it wasn&#x27;t locked up by patents. reply simple10 8 hours agorootparentprevSometimes patent holder enter into initial deals with manufacturers and then can&#x27;t lower the royalty price for other manufacturers. Even if they&#x27;d make more from higher volume, the initial manufacture might be serving high-end customers and doesn&#x27;t want other manufactures entering the market at a lower price. So they refuse to agree to a lower patent royalty even though it would save them money. It&#x27;s counterintuitive, but rationale under certain circumstances. This is pure conjecture for the e-ink patent on my part, but I&#x27;ve seen it first hand for other products. reply bigmattystyles 11 hours agorootparentprevOne thing I don’t understand - why are the iPhone screens that can run in display mode run at 1hz a feature - what’s hard about driving a display typically driven at 120hz(?) at 1hz to save on power difficult? Or it’s not and the it’s a differentiating feature price. reply solardev 10 hours agorootparentIs the refresh really a big power draw vs the backlight or even the pixels themselves? I think the thing about e-ink is that it&#x27;s a chemical producing&#x2F;retaining the pigment, rather than a light source. So even when it&#x27;s off, it retains its current colors. When a LED is off, it&#x27;s just... nothing.But you still have a point. Even if a LED display has \"only\" a 2 or 3 day battery life, that&#x27;s still pretty huge to mere hours.How do smart watches (like the Garmins with 1-week batteries) do it? reply pxeboot 7 hours agorootparentThe Garmin watches with 1+ week battery life have memory-in-pixel displays. Like E-Ink, this seems like another technology that could be amazing if they were available in larger sizes with lower prices. reply xnx 13 hours agorootparentprevI didn&#x27;t realize e-ink manufacturing was such a tricky process. I would think there would be a lot of applications for larger e-ink panels even if they had a number of stuck pixels. reply GuB-42 10 hours agorootparentprev> So I&#x27;m saying the truth and getting down voted for it?Don&#x27;t worry about that. Posts that go against the general opinion always get downvoted at first. But if it is insightful and not demonstrably wrong, the upvotes will win. reply BoorishBears 12 hours agorootparentprevI didn&#x27;t downvote... but if you&#x27;re being downvoted it&#x27;s probably for ignoring the flywheel effect needed to scale novel tech in the market.Yes e-ink has high wastage, and yes demand is low, but having had a strong arming expensive patent holder didn&#x27;t help the situation. The patents are actually expiring&#x2F;expired but at some point the damage is already done.A great corollarly to e-ink is 3d printers: they were very expensive, very low volume products that needed to recoup expensive development costs... but when the 200 lbs gorilla sitting on the market in the form of patents started to die down, a lot more players start to iterate, which unlocked advancements, which made things cheaper and more accessible, which improved demand, which then incentivized more advancements... and now we have $400 printers that outperform $10,000 printers from not that long ago.Most display tech starts high wastage, low demand. Patents aren&#x27;t a guaranteed death, but e-ink was just a bit too far off the path for absolutely required innovations to overcome the added resistance. In the mean time other technologies got better: sunlight performance of non-eink displays has improved dramatically for example.Now the moment has probably passed and e-ink is doomed to stay a niche sideline product as a result. reply solardev 10 hours agorootparent> sunlight performance of non-eink displays has improved dramatically for example.Can you talk more about this? My phones and laptops are still as bad as ever in direct sun. Did I miss some major development...? reply BoorishBears 9 hours agorootparentYou should upgrade your phones and laptops then: Apple just released a phone that does 2000 nits outdoors, and similarly the MBP can dip into its 1000 nit sustained HDR brightness rating for SDR content if in direct sunlightIn reality this is one of those slow burn deals where the improvements in anti-glare coatings, bonding between digitizer and displays, and brightness come slowly enough that it&#x27;s easy to take for granted just how awful old displays used to be.We&#x27;ve come a long way from the days of hazy plastic suspended millimeters above the actual display, which in turn had its own coating with a different refractive index leaving two layers of reflections to deal with reply solardev 9 hours agorootparentOh, I thought there was some revolutionary coating I missed out on...I actually have a M2 MacBook Pro and a Pixel 7, both of which get decently bright, but neither of which is really usable under direct glare. And in XDR mode, the SDR colors get super washed out. HDR is a gimmick as far as I can tell.I&#x27;d trade either for a usable transflective display or a regular matte screen from early 2000s Dell. I don&#x27;t think the answer to glare is to just make the display brighter. That&#x27;s liking shining a flashlight into your eyes so the sun doesn&#x27;t seem as bright...I tried a 3m anti glare coating, and that worked well for keeping out direct light, but it darkened the screen so much it was barely visible at max brightness.The Steam Deck had some etched glass thing that seemed a little better (maybe the iMac too?) but I still wasn&#x27;t able to use it in direct sun.The only display I&#x27;ve used so far that was actually comfortable outside in the sun are e-ink ones, with matte screens also somewhat usable in shade on a sunny day. My M2 MacBook? Useless outside and barely usable under office florescent light. I hate the Apple glare. Makes movies look nice in the dark but useless for day to day office work, IMO. reply BoorishBears 8 hours agorootparentWhat you described is EDR: where non HDR content is artificially limited to allow HDR content to contrast better. In sunlight the screen will stay in SDR and use HDR brightness levels without any tweaks, which doesn&#x27;t result in washing anything out> That&#x27;s liking shining a flashlight into your eyes so the sun doesn&#x27;t seem as bright...... that&#x27;s exactly the point of e-Ink: with minimal power you get brightness that rivals the sunlight you&#x27;re holding the device in.With infinitely efficient OLEDs, we&#x27;d just crank the brightness on the lit pixels high enough to match the brightness of a white piece of paper in sunlight and have a better e-ink: unfortunately in the real world that&#x27;d generate more heat and take more power than mobile devices can afford right now.At the end of the day we have both matte and glossy displays with reflective characteristics that rival old school matte displays because of improvements made across the stack... but nostalgia makes us assume things were very different than they were. reply solardev 7 hours agorootparent> What you described is EDR: where non HDR content is artificially limited to allow HDR content to contrast better. In sunlight the screen will stay in SDR and use HDR brightness levels without any tweaks, which doesn&#x27;t result in washing anything outWhat do you mean by this? Are you saying the Macbook has a special display mode that only activates when it detects sunlight (as in lumens? or?)?> With infinitely efficient OLEDs, we&#x27;d just crank the brightness on the lit pixels high enough to match the brightness of a white piece of paper in sunlight and have a better e-ink: unfortunately in the real world that&#x27;d generate more heat and take more power than mobile devices can afford right now.Are you talking about the contrast ratio? I meant more that high brightness with a direct light source into the eyes can be pretty uncomfortable compared to reflected diffuse sunlight. If there&#x27;s a way to get the contrast of LEDs to a similar readability of paper & e-ink without needing to blast my eyes, that&#x27;d be great. The few OLED displays I&#x27;ve used seem to have that more of that effect (still not as nice as e-ink), but I don&#x27;t have a monitor that size either.> At the end of the day we have both matte and glossy displays with reflective characteristics that rival old school matte displays because of improvements made across the stack... but nostalgia makes us assume things were very different than they were.Like what? Are you saying my current Macbook is actually more readable in sunlight than old matte displays, I just don&#x27;t realize it...? If so, hmm, I&#x27;m dubious but I will take it out again and try to compare it again without nostalgia, as much as I can. Or is there an objective measure of sunlight readability I can refer to? reply BoorishBears 6 hours agorootparent> What do you mean by this? Are you saying the Macbook has a special display mode that only activates when it detects sunlight (as in lumens? or?)?Yes, if automatic brightness is on, the screen will exceed its standard SDR range in daylight, just like the iPhone. Otherwise you need special software to manually increase the SDR brightness into that higher range, maxing out at 1600 nits.> I meant more that high brightness with a direct light source into the eyes can be pretty uncomfortable compared to reflected diffuse sunlightThe discomfort would come from a mismatch in brightness: a 2000 nit screen sounds incredibly painful to look at because in normal conditions your pupils are adjusted to an indoor room and you&#x27;re looking at a 2,000 nit screen, which is terrible.But ambient light on a clear day is something like 30,000 nits, so your pupils are already significantly narrowed: a \"mere\" 2000 nit screen will still look kind of dim if anything.e-Ink sits around 40%-50% reflectivity and still looks fine in those conditions.(Blue light, contrast, and the nature of the reflections themselves also play a role so it&#x27;s not as simple as \"more nits is better\", but you need to be in a certain ballpark of brightness to even play, which is why I&#x27;m saying old devices aren&#x27;t in the running here.)_> Like what? Are you saying my current Macbook is actually more readable in sunlight than old matte displays, I just don&#x27;t realize it...? If so, hmm, I&#x27;m dubious but I will take it out again and try to compare it again without nostalgia, as much as I can. Or is there an objective measure of sunlight readability I can refer to?It&#x27;s hard to capture in a single measure because there&#x27;s two parts to the equation:- reflections from the surface- being bright enough for your eyes to make out details at allBack in the day the brightness would be roughly equal for both a glossy and a matte option, so matte would be strictly better.Now brightness has advanced enough that a newer glossy panel + modern anti-glare coating + modern bonding techniques are. enough to overcome the difference in reflections vs the old screens.You should look at even the glossy non-Retina (which was almost universally hated) vs the glossy Retina to see how dramatic the changes in anti-glare tech were at some points in the last decade: https:&#x2F;&#x2F;cdtobie.wordpress.com&#x2F;2012&#x2F;06&#x2F;18&#x2F;reduced-reflectance...A modern matte screen would have the advantage if it had the same brightness, but afaik no one actually makes a matte screen that gets as bright as the new XDR Macbooks do: the closest are all glossy displays too. reply dontlaugh 1 hour agorootparentprevIt’s better than it used to be, but I don’t think it’s because of higher brightness. No amount of it can make blacks visible in direct sunlight. reply rnk 10 hours agorootparentprevGreat points. Plus almost everyone always gets downvoted for everything, then it comes back. I always wonder why many of my \"brilliant\" comments get downvoted, but often come back by the next day. reply mikewarot 10 hours agorootparentDownvotes are emotionally driven, and limited to -4 (I think)... upvotes on the other hand, have no apparent limit. reply mikepurvis 13 hours agoparentprevI&#x27;ve read (probably on prior HN discussions about this?) that the manufacturing process still has major yield issues— when you&#x27;re making small displays, you can slice around the bad pixels and not have to waste as much, but making a large display requires a huge sheet to all be perfect.Then again, if that was really it, surely there&#x27;d be a market for people who want a 42\" e-ink display and are willing to accept some proportion of bad pixels in exchange for a deep discount. Which really shouldn&#x27;t matter much, particularly for applications where distance-viewing is the expectation. reply diggan 13 hours agoparentprev> if it wasn&#x27;t $2,500 for a 42\" displayTry $3000 :)> Driving boards are not included in the display module package. E Ink Salt kit is designed to support this module> Salt Driving Board - $500.00 - https:&#x2F;&#x2F;shopkits.eink.com&#x2F;en&#x2F;product&#x2F;detail&#x2F;SaltDrivingBoard reply prewett 11 hours agoparentprevIf it&#x27;s a 4k TV it also has more pixels and better refresh rate. And color. But for that extra $2360 you get the feature that your image is still there when the power is off. I expect that feature would be substantially more costly regardless of the patent holder&#x27;s extra. reply nomel 11 hours agorootparent> you get the feature that your image is still there when the power is offHow is that a realistic feature? Who is losing power and being happy that they spent a few thousand more, nearly 40x, than a 36\" photo quality print, just so the image can remain while their lights are off? reply k1t 11 hours agorootparentThe point is that it only uses power when changing the image, so a small battery can run it for a long time - not that it works during a power outage.(and of course it is a lot easier to change than a printed photo) reply nomel 8 hours agorootparentFor $2,200, you could run a 35W 43” $160 TV for 29 years, if your power is $0.3&#x2F;kWh. In reality, much more, with a dim display at night.I understand being eco friendly, but that’s an absurd, entirely negligible, justification.I would claim the reflective display is the killer feature. reply callalex 5 hours agorootparentAs the person you are replying to clearly stated, it has nothing to do with drawing power from a wall socket. It’s about enabling new uses that work on battery power that can last for months&#x2F;years instead of minutes. reply johnmaguire 5 hours agorootparentprevMany of e-ink&#x27;s use cases involve low power environments. This is often due to logistics - not cost. reply diogenes4 7 hours agorootparentprevRotating the art while not actively producing light is a pretty big draw. The power seems like a red herring. reply malfist 10 hours agorootparentprevBut that&#x27;s not the only benefit of eink.EInk doesn&#x27;t have a backlight. So it&#x27;s easy to see in glaring light. It&#x27;s also not disruptive if so, you want to use it as a digital picture frame and you don&#x27;t want light from the display bothering your eyes day and night.EInk is a more peaceful display technology, and some people put a lot of value on that. reply mahdi7d1 3 hours agoparentprevI guess people only care and notice theire own prefrences. I never thought about 42\" eink display but I&#x27;m constantly wishing for the price of 10\" and 13\" modules to drop cause that&#x27;s the usecase I&#x27;m interested in. I think their demand would be much much more compared to a 42\" panel (what even is the use of such thing) so they can benefit from scale. reply graypegg 13 hours agoparentprevHmm. I wonder what the whole-sale price is? A TV with wifi is partly subsidized by the ads you see when you turn it on. This 2500$ display seems like it’s for small run signage, so they know it’s being sold for government&#x2F;business use. Probably a bit inflated.Does Rakutan&#x2F;Amazon&#x2F;Pocketbook really pay a similar cost&#x2F;size ratio for the panels on their ereaders? I hope not! reply lawlessone 13 hours agorootparent>so they know it’s being sold for government&#x2F;business use. Probably a bit inflated.You just reminded me, a few years ago i wanted to buy a transparent screen for a DIY project. The company said they couldn&#x27;t sell it to me because they&#x27;re b2b.I think it was both a volume thing and a tax thing.. and possibly a liability thing. reply tomcam 11 hours agorootparentOr they may have no procedure for selling one-offs to end users. It’s not uncommon. reply red_hare 12 hours agoparentprevMy dream is, when these finally come down in price, to build a dynamic DnD map. reply spullara 12 hours agorootparentWhy not just put any other kind of display flat and put glass over it to protect it?Edit: Someone has your dream display already :) https:&#x2F;&#x2F;www.etsy.com&#x2F;listing&#x2F;1463146123&#x2F;43-4k-dnd-tv-table reply mrd3v0 4 hours agoparentprevThe E-ink patent holder is one of the most egregious cases of patent trolls. They won&#x27;t lower the prices. reply herpderperator 1 hour agoparentprevWhere does it show the minimum order is 3? reply slg 9 hours agoparentprevWho is going to do the math and figure out how long you need to hypothetically have these screens on before the power costs of the TV actually make up the $2360 difference in price? reply layer8 14 hours agoparentprevWell, Apple’s 32” Pro Display XDR is $5000, there’s always something. reply ThrowawayTestr 12 hours agoparentprevThe first flat panel tvs started in the $x000 price range. I&#x27;m amazed an eink display this big is actually affordable (minimum quantities aside). reply bufferoverflow 11 hours agorootparentE-ink has been around for a long time. It&#x27;s not one of the first panels. reply qingcharles 11 hours agorootparentprevMy first 40\" flat panel TV ran something around $6000 IIRC. reply chromakode 13 hours agoprevAwesome project! I&#x27;d strongly recommend swapping in some antireflective glass. There&#x27;s a couple affordable options with less than 1% reflection [1][2]. Made a huge difference vs. stock acrylic on my frames that get lots of environmental light.[1] https:&#x2F;&#x2F;www.groglass.com&#x2F;product&#x2F;artglass-ar-70&#x2F;[2] https:&#x2F;&#x2F;www.framedestination.com&#x2F;prod&#x2F;sh&#x2F;ultravue-uv70-pictu... reply sho_hn 13 hours agoparentI second this recommendation! Frame manufacturers sometimes call these products \"museum glass\", which combines anti-reflective properties with UV filters, usually with price points at 70% and 92% filters.Some E-ink panels can be somewhat susceptible to UV light and perform better under a filter. You will sometimes find warnings in data sheets about refresh performance in direct sunlight, and danger of long-term permanent damage. Some of E-ink&#x27;s signage products have, I think, filter layers built-in.But also if you&#x27;re using a passepartout like in this project, and the frame will hang in sunlight and you&#x27;re not sure how the paper will perform, it&#x27;s worth springing for the UV protection to avoid yellowing over time. reply schlarpc 13 hours agoprevI have a similar display, and also use blue noise dithering. Mine is driven in the backend by a web browser, which means I was able to abuse CSS and mix-blend-mode to do the dithering for me: ha-card::after { content: \"\"; background-image: url(&#x2F;local&#x2F;visionect-dither.png); background-repeat: repeat; position: absolute; height: 100%; width: 100%; display: block; mix-blend-mode: multiply; }The dithering texture used is 128_128&#x2F;LDR_LLL1_10.png from https:&#x2F;&#x2F;github.com&#x2F;Calinou&#x2F;free-blue-noise-textures reply lawlessone 13 hours agoparentwhat are the 3d and 4d textures? reply webkike 12 hours agorootparent(I could be wrong, but here&#x27;s what I&#x27;m guessing) 3D refers to RGB, and 4D refers to RGB plus Alpha reply jalict 3 hours agorootparentprevUsually something like volumetric data, animations, or other forms of data packed into something the GPU can swallow. I am not sure why you would apply the filters to those kinds. reply xnx 14 hours agoprevI thought I&#x27;d heard of all the dithering options from https:&#x2F;&#x2F;tannerhelland.com&#x2F;2012&#x2F;12&#x2F;28&#x2F;dithering-eleven-algori..., but surprised to read there&#x27;s another one (https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;3288) that was used in this project. reply cubefox 12 hours agoparentBlue noise dithering seems to be a form of ordered dithering which is better than other forms of ordered dithering, but in terms of quality it is not as good as error diffusion dithering (look up the Wikipedia comparison on the statue of David). But blue noise dithering has the advantage that it can be implemented as a pixel shader, unlike error diffusion. So it can e.g. be used for video games. So I think for the picture frame error diffusion would have been a bit better. reply datene 3 hours agoparentprevThis excellent article on dithering also explains blue noise in a simple wayhttps:&#x2F;&#x2F;surma.dev&#x2F;things&#x2F;ditherpunk&#x2F; reply esafak 14 hours agoprevSamsung sells an LED TV specifically for this use case. Currently only at 4K.https:&#x2F;&#x2F;www.samsung.com&#x2F;us&#x2F;tvs&#x2F;the-frame&#x2F;highlights&#x2F; reply huehehue 13 hours agoparentI have one of these, and only in a very specific environment is it convincing as not-a-TV (aside from the concerns of privacy and their proprietary app).Especially at night, I find the backlight makes it painfully obvious that it&#x27;s just a TV and I&#x27;d much rather have something like e-ink which blends into the surroundings. reply bigfudge 13 hours agorootparentThey look cool. Would be great to have an oled version without a backlight. reply pama 9 hours agorootparentLED on oled stands for light emitting diodes, so they are a backlight. reply xnx 9 hours agorootparent\"An OLED display works without a backlight because it emits its own visible light.\" https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;OLED replytambourine_man 13 hours agoparentprevI’m browsing this link for a good 5min. Looks interesting. I get that it tries to simulate paper, but I have no idea how.Even the “explore technology” link has little extra detail. I don’t even know if it’s backlit. reply rjvs 8 hours agorootparentThe main tech is a very anti-reflective coating; the best I&#x27;ve seen on a mainstream TV. It is backlit but in Art Mode the backlight is turned down (you can control how much and there is an ambient light sensor that fine-tunes your choice). The combination of low emission, quite low reflection and static content is pretty compelling for me in a lit room.I have the white frame on the bezel against a light-coloured wall and when displaying art it&#x27;s much less imposing on the room than an empty black screen. Obviously it uses more power than standby but it&#x27;s also quite a bit less than a dynamic screensaver.Don&#x27;t know that I&#x27;d get another one; the bugs in Samsung&#x27;s software are annoying (I just want it to display the art, or a picture from a single HDMI input, how hard can they possibly make it?) but perhaps that&#x27;s standard for modern TVs. reply ffgjgf1 13 hours agorootparentprevIt’s just a tv with a picture frame and some additional software&#x2F;wallpapers reply tambourine_man 10 hours agorootparentThanks. The selling copy in dumb because the product is. reply ThrowawayTestr 12 hours agorootparentprevIt&#x27;s just a TV with a matte screen and a way for Samsung to sell you public domain art. reply renewiltord 13 hours agoparentprevA friend has the Samsung Frame and I think you can only interface with it via the Samsung SmartThings app. It&#x27;s quite closed. So you need an online thing for the TV and then you upload it into their app. reply mholm 12 hours agorootparentYep, it&#x27;s very locked down, and Art Mode behavior is often anti-user to promote their $10&#x2F;month Art Store subscription. There are limited matting options and I believe it intentionally crops photos incorrectly, even if you upload in the correct aspect ratio. It also takes about a minute to scroll down to &#x27;User Art&#x27; sections with how slow their UI is. reply rjvs 8 hours agorootparentTheir Art Store promotion is annoying persistent but not actually required if you put images onto a USB and have it slurp them up; all you need to do is ignore their requests for subscription when setting it up. All that&#x27;s required is to resize them to full screen with your own matte, or you can just crop the images to the right aspect ratio. Depending upon your desire for fidelity, you could also use ML image extension to get the right aspect ratio.This doesn&#x27;t get around all the other annoying bugs in the firmware of course. reply rjvs 8 hours agorootparentprevThey make it superficially closed but it&#x27;s pretty simple to get around it with a USB drive. Really all you have to do is ignore their subscription system and size your images to match the screen. reply moroark 9 hours agoprevI love it. 2 Ideas:You should upload a massively complicated where&#x27;s waldo like scene and have waldo appear in different places each hour.Have a portrait with lots of subtle details but they change slowly over time. Ex. girl goes from earing-less to small earings to big earings. Just have lots of small details subtly change but at a rate which makes them hard to detect. reply lionkor 14 hours agoprevThat&#x27;s a very neat project. The only issue I have with it is that it&#x27;s basically a passive energy waster. It produces images by burning GPU power, when it could instead curate art from an existing amount of art (of which there is more to ever go through, almost in any category). Some projects that use AI could be replaced with other tech and be much more efficient. reply xnx 14 hours agoparentThe cost (and environmental impact) of generated images is a rounding error compared to the $7,500+ project cost. As an aside, I wonder how much smaller and faster a diffusion model could be if trained (quantized?) to 4-bit grayscale images. reply 1-6 10 hours agorootparent$2,500 for a display isn’t too bad considering that real art can cost much more. reply mk_stjames 13 hours agoparentprevMy home computer batch processing prompts thru Stable Diffusion can generate and then nicely upscale images at at rate of about 1 every 5 seconds. Or 360 per half-hour. Which means a newly image on the display every day to look at for a year in just an half hour of computation.At about 300W of GPU + 120W of PC, that is 420W * 0.5hrs = 0.210kWhr. This is a rounding error on my monthly electric bill. About six cents.I&#x27;ve spent more energy than that likely just sitting and reading HackerNews this week. reply whoibrar 8 hours agorootparentI would like all my future electricity bills to be calculated in terms of \"HackerNews Read time\" reply 93po 5 hours agoparentprevHow is this any different than using a GPU to play games? They&#x27;re both something people enjoy. Should I go play with sticks in the yard instead? reply bee_rider 13 hours agoparentprevThey should render these images when there’s excess renewable energy, then store them for later. reply lawlessone 13 hours agorootparentThis is just a little too close to NFT art for me lol. reply GaggiX 14 hours agoparentprevI think it simply uses some API to generate the images. reply evrimoztamur 14 hours agorootparentHow does the API generate the images then? reply GaggiX 12 hours agorootparentIt doesn&#x27;t really matter if it&#x27;s really cheap. reply hutzlibu 14 hours agorootparentprevMagic? reply dheera 14 hours agoparentprevIt doesn&#x27;t take a lot of GPU to produce one image, and you could always just keep a single image on the wall for a longer time if you want to reduce that impact.You can also have it not produce images at night or when you are not around the house.Lots of ways to save energy. The impact of an image every few hours or whatever is nanoscule compared to your transportation and heating needs.(Also if your apartment uses electric resistive heating, fire away with your GPU, you&#x27;re just producing images in the process of producing heat instead of passing it through a resistor. It&#x27;s no less efficient.) reply semi-extrinsic 14 hours agoprevReally cool project!If you&#x27;re into looking at different dithering techniques, there are a few interesting ones compared in this old CodeGolf question (disclaimer: I&#x27;m the one with the Fortran answer)https:&#x2F;&#x2F;codegolf.stackexchange.com&#x2F;questions&#x2F;26554&#x2F;dither-a-... reply andrewstuart 13 hours agoprevI would love two wall mounted e-ink displays showing chess boards so my 14 year old son could play chess with his grandfather.I imagine the game keeping track of whose turn it is and being able to give voice commands to make moves.Then they could play games over a longer period of time, maybe make one move per day. reply jareklupinski 13 hours agoparentwhat&#x27;s your budget :)the Lilygo T5 with Touch screen + a lipo battery would have all the hardware you needjust need to port chess to the platform after that https:&#x2F;&#x2F;www.hackster.io&#x2F;Sergey_Urusov&#x2F;arduino-mega-chess-d54...figuring out NAT traversal to get them talking to each other from each other&#x27;s homes might be tricky&#x2F;fun, but there may be ways around that... reply andrewstuart 13 hours agorootparentPretty cool at $36.36 USDhttps:&#x2F;&#x2F;www.lilygo.cc&#x2F;products&#x2F;t5-4-7-inch-e-paper-v2-3It would be great to be able to buy this in a size that would mount easily into common photo frame formats.Ideally with a battery that could last months and optional power.If I was retired it&#x27;s the sort of project I&#x27;d happily waste time on. reply konschubert 12 hours agorootparentI am making these displays that come in a wooden frame:https:&#x2F;&#x2F;shop.invisible-computers.com&#x2F;products&#x2F;invisible-cale...You could render the current state of the chess game to an image and serve that image on a URL on the internet. Then you can connect the display to that image:https:&#x2F;&#x2F;www.invisible-computers.com&#x2F;invisible-calendar&#x2F;image...Admittedly that&#x27;s still a lot of work from your side, and a little applet to keep running... reply 93po 5 hours agoparentprev10 inch is probably workable and is also $2800 more affordable:https:&#x2F;&#x2F;www.crowdsupply.com&#x2F;soldered&#x2F;inkplate-10#products reply Dave_Rosenthal 10 hours agoprevI&#x27;ve looked, and it doesn&#x27;t seem to exist, but I still can&#x27;t believe there isn&#x27;t an emissive display on the market that can just emulate the look of a passive display. (i.e. match the &#x27;white&#x27; of the display to inbound light)All you need is a high quality RGB light sensor or three and an accurate way to dim a backlight. None of this seems very hard. Sure, it wouldn&#x27;t show the shape of shadows or whatever, but it would still be highly effective. Why can&#x27;t the image just go black when I turn the lights off? reply ggm 10 hours agoparentThe Samsung art series TV has fooled me in an AirBnB. It took me quite a while to realise it wasn&#x27;t an expensive framed print.It was daylight in a well lit bright room. The frame and background were just right to look like modern simple poster framing. The minute sun set, of course it stopped looking right. Thats when I realized we had a TV after all.My partner pointed out the art had changed at least two times across daylight hours. I had just blanked that out. reply dsp_person 10 hours agoprevI remember seeing those Thinkpad X230 mods with an eink display and how a challenge was availability of the specific discontinued e-ink screen of the right size.The mfg from the article has a 13.3\" screen that would be fun to try a mod with: https:&#x2F;&#x2F;shopkits.eink.com&#x2F;en&#x2F;product&#x2F;detail&#x2F;13.3&#x27;&#x27;ePaperDisp...and impressive 1600 x 1200 pixels reply nealeratzlaff 12 hours agoprevThis is really cool! I&#x27;ve wanted to buy something similar for a long time, but in RGB. I know Samsung makes the Frame, but its not hackable, and its a waste of power. I don&#x27;t ever want the thing to function as a TV, just a way to display generative art.At one time there apparently was this: https:&#x2F;&#x2F;mono.frm.fm&#x2F;en&#x2F;shop&#x2F; But the price was crazy. There&#x27;s not much out there in terms of appropriately priced digital picture frames. reply Charon77 6 hours agoprev> We found that running 2 full white images did a good enough job, without disrupting the experience too muchSome devices actually clear by outputting the invert of the image. Maybe this will be faster reply landgenoot 14 hours agoprevLove it. But way too expensive, especially because we don&#x27;t know what the quality &#x2F; price will do in the future when the patents expire.For that price, you can buy a 42\" color printer. reply sho_hn 14 hours agoprevPosh frames e-ink friends! Here&#x27;s my generative newspaper: https:&#x2F;&#x2F;i.imgur.com&#x2F;tD1t9u3.jpegMy wallet stopped at 13.3\" though :)Update for those who saw me post it before: I recently charged the battery for the first time since hanging it in March, which met my expectations.Shout-out also to the people at Halbe Rahmen, the best picture frames in the world. reply yohannparis 14 hours agoparentWhere is the information on how you built it? reply sho_hn 14 hours agorootparentThere&#x27;s some info on my Hackaday project listing: https:&#x2F;&#x2F;hackaday.io&#x2F;project&#x2F;190478-hyepaperAlso the album link in the other comment. reply yohannparis 9 hours agorootparentThis is quite an achievement. I really like the curation and the process to make it a front-page. You automated your own editorial news. Love it! reply sho_hn 7 hours agorootparentThank you! reply ancorevard 9 hours agorootparentprevWhat was the total costs of the components? reply sho_hn 7 hours agorootparentIf I recall right, about $500 for the electronics and $150-ish for the picture frame (it&#x27;s a nice one with anti-reflective UV filter glass, because at the time I wasn&#x27;t sure where I&#x27;d hang it and e-ink can be susceptible to UV light). reply lawlessone 13 hours agorootparentprevYou could really gaslight house guests with this. Just deny it&#x27;s changed. reply andrewstuart 13 hours agorootparentDo you think people are fooled by any tech these days?I suspect people expect everything to be magic now. reply sho_hn 13 hours agorootparentSo far my guests are mostly puzzled by \"Why&#x27;d you frame that?\" until I tell them it will in fact change and isn&#x27;t my most prized selection of news articles :-) replydheera 14 hours agoparentprevWondering if you have by any chance shared code and plans for the long battery life setup?Mine is still wired but I&#x27;m thinking of using ESP32 + deep sleep. reply sho_hn 14 hours agorootparentI _still_ haven&#x27;t set the GitHub repo to public :( Real-life intruded in a major way (first baby).esp32 + deep sleep + external RTC is what this is using. I posted some of my experiences with getting it as low as I could here: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37663913Here&#x27;s a few more pics: https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;NoTr8XXI found the mention of Blue Noise Dithering in this project quite interesting! I&#x27;ve not put a lot of effort into picking the nicest possible dithering method (just a basic Floyd-Steinberg) for the embedded article photos, partly because the whole newspaper look didn&#x27;t seem to warrant it, but it might be worth taking stock of what the latest on dithering is. reply robszumski 14 hours agoprevThis is great! I would love to see a sample of the color input images and the resulting image on the display. reply itishappy 14 hours agoparentIt&#x27;s monochrome. reply robszumski 14 hours agorootparentThe video has a lot of glare and the dithering seems like an important part. I wanted to contrast that process with the original. reply ea016 14 hours agorootparentOP here. Here&#x27;s a comparison with an input image vs gray levels vs dithered: https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;osE57ehWhen displayed on the frame you can barely see the dithering. Interestingly you see the jpeg compression artifacts a lot more reply tonoto 14 hours agoprevFantastic project, thanks for sharing! Prohibitive pricing for home use, but I would really love to have that as a picture frame on the wall. One can dream for the future.. reply freedomben 14 hours agoprevWhen are we going to have cheaper e-ink screens available? Anyone know if the situation has changed in the last couple of years? See: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26143779 reply grawlinson 14 hours agoparentRealistically, it’ll be when the patents expire. reply eli 14 hours agorootparentMy understanding is it&#x27;s much more about demand than patents.LED panels got cheap because we built factories that pump out a lot of them. reply lrem 14 hours agorootparentI just glanced, they still want about 500 bucks for the 13\" screen and half of that for the board to drive it. While I have no clue what making the screen at a low scale might cost, the board cannot possibly be five times more expensive than a Raspberry Pi, can it? reply sho_hn 13 hours agorootparentNo, simple driver boards aren&#x27;t super complicated. Nor are the chips on them manufactured on the fanciest nodes. They&#x27;re not super-high-volume either though and as a result somewhat expensive on a BoM.Here&#x27;s the data sheet for the ITE IT8951, a frequently-used ASIC for this purpose that supports basic partial updates as well: https:&#x2F;&#x2F;www.waveshare.net&#x2F;w&#x2F;upload&#x2F;1&#x2F;18&#x2F;IT8951_D_V0.2.4.3_20...Here&#x27;s a sourcing price: https:&#x2F;&#x2F;www.win-source.net&#x2F;products&#x2F;detail&#x2F;ite&#x2F;it8951e-64-dx...It&#x27;s basically a fairly simple SPI interface (or here a SPI wrapper around an internal Z80 protocol). The rest of the board is power supply handling and a DRAM framebuffer chip, external to the driver ASIC on SPI as well. I wrote a custom driver for this in Rust for my project, and it only took about a day despite a few leaky-abstractions oddities in how it communicates over SPI.The chip itself is basically buffer handling, image processing (features like JPEG decode, resampling and some LUT mechanisms) and a waveform generator. Others will have some IP blocks for, say, a HDMI frontend. Any decent chip company can crank this out pretty quick. There&#x27;s easily 10+ product lines on the market.For simple things you may not even need one and can drive the panel from an MCU directly.There are more expensive, more advanced drivers that implement more complicated and higher-performance (say, refresh rate) update schemes or I&#x2F;O though. reply avianlyric 12 hours agorootparentprevRealistically you’re not paying for the hardware when you buy the driver board, but rather the software baked into those boards.The waveforms, and the algorithms that create waveforms, used to drive eInk displays at reasonable speeds and produce high quality images are highly proprietary and very difficult to develop. In theory it’s easy to make an eInk display an image, but doing that in a reasonable period of time, when transitioning from potentially any starting state, and handling the crosstalk between pixels, makes creating good images on an eInk display pretty hard to get right. reply GaggiX 14 hours agorootparentprevWhich patents need to expire to have cheaper e-ink displays? reply fortran77 14 hours agorootparentThey’ll just keep repeating thus trope whenever an e-ink project comes up. (They don’t think that LCD screens have patents too?) reply bigfudge 13 hours agorootparentSurely most LCD patents have expired by now? reply fortran77 13 hours agorootparent&#x27;And there are new ones as tech improves replydheera 14 hours agoparentprevYeah I built one with 10\" screenshttps:&#x2F;&#x2F;dheera.net&#x2F;projects&#x2F;einkframe&#x2F;I really want to upgrade to at least 31\" but it&#x27;s rather expensive.Separately in the next irritation of this project I&#x27;m hoping to use an ESP32 instead of an RPi Zero to power it, and use the deep sleep mode so it can periodically update but sleep through the rest of the time, allowing a few months of battery life with no wire sticking out.Even more ideal would be putting a thin strip of solar panel on the top edge of the frame to keep it charged, and use some kind of supercapacitor to power the ESP32, though I don&#x27;t know if the parts for that exist, especially solar panel that is 1cm wide. reply ea016 14 hours agorootparentOP here. Your project is really cool, thanks for sharing. I was considering the same idea, but it means updating the drivers to work on an ESP32The best option I found so far is a timer shield that can wake up the Raspberry periodically: https:&#x2F;&#x2F;www.pishop.us&#x2F;product&#x2F;sleepy-pi-2-power-management-s... reply KomoD 11 hours agoprevWhat makes E-Ink so damn expensive? reply 1-6 10 hours agoparentGood question. It has been answered before: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26143407 reply worldmerge 10 hours agoprevYour company seems really cool! I love the maker spirit! Sick flip dot display, I have some from AlfaZeta, would love to get a bus one but they seem rare in the US. Photoroom seems like a fun place to work. reply archontes 11 hours agoprevCasey Muratori has a decent blog post on the topic of blue noise dithering.https:&#x2F;&#x2F;caseymuratori.com&#x2F;blog_0010 reply numpad0 13 hours agoprevIs the control board actually wrong, or is the cable supposed to be origami folded for adjustment? A control board that works should be really close to the right one if it weren&#x27;t. reply 1-6 10 hours agoprevE-Ink is a classic example of an artificially constrained supply chain designed to compete with far more advanced technologies. reply Aeolun 10 hours agoparentYou can hardly manufacture 1000 if only 20 will ever be sold. reply Aspos 13 hours agoprevYou have plenty of space behind the screen. Wondering if you could harness wifi signal to generate power. Given days between changes, perhaps you could harvest enough amperage to make a change.Or maybe you could hide a Qi charger into the wall?https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41467-021-23181-1 reply NKosmatos 12 hours agoprevDon’t get me wrong, I like this project and the write up, but it seems a little bit overkill. These e-ink displays are 3 thousand each and this guy is the CTO of PhotoRoom. Hey it’s not coming out of my pocket and I don’t know if it’s company or personal money, but it seems a little bit off. reply andrewstuart 13 hours agoprevTablet manufacturers would do well to consider that the picture frame is the desirable form factor for E-ink tablets, perhaps more so than the iPad style hand held tablet.Look to what hobbyists are hacking to find commercial opportunity. reply levi0214 6 hours agoprevThis is cool! reply supportengineer 13 hours agoprev>> instantly nerd-snippeder, what? reply mcphage 13 hours agoparentIt&#x27;s a term from an old XKCD: https:&#x2F;&#x2F;xkcd.com&#x2F;356&#x2F; reply supportengineer 12 hours agorootparentOh, snipe not snip. reply ea016 12 hours agorootparentThanks, fixing the typo :) reply scotty79 10 hours agoprevCool thing would be to listen to the speech in the room and use it as a prompt reply adolph 14 hours agoprevIn terms of battery powering it, I wonder how many amps it takes to power a refresh. The Salt driver is 12v, which I guess can be shut down between refreshes. reply sho_hn 14 hours agoparentIn my e-ink newspaper, I have the driver board behind a relay (because it had an annoying idle current draw even when you tell the driver chip to go to standby, and they even added an annoying power LED ...) and do just fully shut it off between refreshes.There&#x27;s some projects that forego a driver ASIC entirely and drive the waveforms directly from a MCU, although I guess this large panel has a different voltage domain. reply bazeblackwood 13 hours agoprev [–] Got excited, but then you qualified it was AI.AI art is not generative art, it is reductive \"art\". It requires samples of the art style you wish to cop, and it always produces something artificial and less than the sum of its parts.On the other hand, generative art is procedural, and represents the thought process of a human being.Better luck making meaning. reply chefandy 7 hours agoparentYou&#x27;ll get downvoted into the earth&#x27;s core here expressing the philosophical stance that asking a computer to make art from other people&#x27;s art isn&#x27;t actually making art. I wonder if they&#x27;d commission a painting with a 50 word description, and after a few rounds of alterations when it was finally delivered, cross out the painter&#x27;s signature and add their own. reply lawlessone 13 hours agoparentprev [–] I despise how some companies opted to scrape artists works without their permission.But aren&#x27;t the prompts used also representative of a thought process? The chosen network architecture and the choices of images used to train also represent thought. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author created a 42-inch E Ink art frame designed to exhibit generative AI art, utilizing a Raspberry Pi for display control and Blue Noise Dithering for image preprocessing.",
      "To tackle the 'ghosting' issue common in E Ink displays, they implemented a solution involving alternating between full black and full white images.",
      "Future improvement plans include making the frame battery-powered and using AI to generate art prompts. The project had support from Charly, Nico, and Florian."
    ],
    "commentSummary": [
      "The key focus of the discussions is the high cost and constraints of e-ink displays for generative art, attributed to difficulties in manufacturing, low demand, lack of bulk production benefits, and limited usability.",
      "Participants also discuss the strengths and weaknesses of e-ink technology, such as its low energy use, enhanced visibility outdoors, along with its limitations in cost and size.",
      "Additional topics involve the role patents play, comparisons with OLED (Organic Light Emitting Diodes) displays, and the utilization of e-ink displays in different applications like digital photo frames and Do It Yourself (DIY) projects."
    ],
    "points": 437,
    "commentCount": 159,
    "retryCount": 0,
    "time": 1696964218
  },
  {
    "id": 37836237,
    "title": "Oil sector is lobbying for inefficient hydrogen cars to delay electrification",
    "originLink": "https://www.rechargenews.com/energy-transition/liebreich-oil-sector-is-lobbying-for-inefficient-hydrogen-cars-because-it-wants-to-delay-electrification-/2-1-1033226?zephr_sso_ott=XwKh7x",
    "originBody": "News Analysis In-Depth Interviews Opinion Careers Subscribe Login ENERGY TRANSITION See all articles Alert me about Energy Transition Michael Liebreich, CEO of Liebreich Associates.Photo: Liebreich Associates Liebreich: ‘Oil sector is lobbying for inefficient hydrogen cars because it wants to delay electrification’ Influential analyst and adviser explains why powering cars with H2 is a terrible idea, no matter what the hydrocarbons industry says RELATED NEWS Senior EU official: ‘The goal is green hydrogen, but there has to some blue along the way’ Energy Transition 27 May 2021 15:49 GMT European oil majors see 'great upheaval' as investors push for greener portfolios Transition 31 August 2020 15:06 GMT Wave power will never be competitive, but tidal works: Liebreich Transition 21 June 2019 12:50 GMT 'Liebreich is very wrong — wave power is already competitive' Transition 9 July 2019 12:14 GMT 'Wind and solar have nothing to fear from price cannibalisation' Transition 19 June 2019 7:28 GMT 30 June 2021 13:55 GMT UPDATED 30 June 2021 13:58 GMT By Leigh Collins Michael Liebreich, the charismatic founder of BloombergNEF who is now an independent analyst and adviser, is extremely passionate when talking about clean hydrogen as a tool for decarbonisation — but not in the way you might think. As a firm advocate of decarbonising our planet as quickly and cost-effectively as possible, he is strongly against using clean hydrogen in certain sectors where there are cheaper and more efficient electric solutions, such as cars and domestic heating — in stark contrast to elements within the oil & gas sector that are spending tens of millions of euros on lobbying for such uses. Michael Liebreich's \"hydrogen ladder\" chart identifying the merits of use cases for clean hydrogen. Photo: Liebreich Associates In an attempt to guide governments and industry players away from the spin, he has created what he calls his Hydrogen Ladder, a simple chart (pictured) showing which use cases for H2 are uncompetitive, which are unavoidable for decarbonisation, and which sit somewhere in the middle. So at the top of his ladder, at the “unavoidable” end, lie the existing uses of highly polluting grey hydrogen produced from unabated fossil fuels — such as ammonia-based fertiliser and oil refining, which are responsible for 3-4% of all global carbon emissions. Seasonal power storage, steel, chemicals, shipping and long-haul aviation (using liquid hydrogen or synthetic fuels derived from H2) are also high up. SPECIAL REPORTIs the future role of green hydrogen in the energy mix being overstated? Read more SPECIAL REPORTIs hydrogen the best option to decarbonise land transport, shipping and aviation? Read more SPECIAL REPORTIs hydrogen the best option to decarbonise heating and heavy industry? Read more At the bottom “uncompetitive” end of the ladder are hydrogen cars and domestic heating — which he says make no sense when you have battery-powered electric vehicles (BEVs) and heat pumps. Liebreich — a pro-business supporter of the UK Conservative Party, who sits on the British government board of trade and is also an advisor to Norwegian oil giant Equinor — believes that oil & gas companies know that hydrogen is a poor choice for these two segments, but are pursuing them nonetheless. “If you're an oil and gas company, in a way, talking about hydrogen is kind of a two-way bet because if it works, then you're embedded in the hydrogen industry — but if it doesn't work, you've delayed the transition to the thing you don't make, which is electricity,” he tells Recharge. “So why wouldn't you promote hydrogen for inappropriate use? For the things that are not at the top of the ladder, that are fairly down — so local trains, local buses, cars, delivery vehicles — why not promote it? Because at worst it creates confusion, which is great [for them]. And these companies have an interest in this [electric] stuff not moving too fast, I'm afraid — for all their good words.” Liebreich gives an example of Shell being happy to spend $12bn on a floating liquefied natural gas (LNG) platform, the Prelude, which has seen a host of problems, “but they won't spend $12bn just producing blue, green, pink, or any other sort of clean hydrogen for those [existing] uses where we currently are driving 3-4% of global emissions”. He points to a report last year by lobbying watchdog Corporate Europe Observatory, which found that the “hydrogen lobby, whose main players are fossil gas companies, declared a combined annual expenditure of €58.6m trying to influence Brussels policy making”. “So what they want to do is fund people to go and lobby in Brussels for uses of hydrogen, where they basically think that because politicians like having photos taken [with hydrogen cars and trains], they will tolerate [the existing use of grey hydrogen],” says Liebreich. “I don't know whether it's disingenuity or a phony war, but we’re talking at cross purposes if we’re using dirty hydrogen [for existing uses], but we’re talking about clean hydrogen [for cars]. “If I were a [government] minister, what I would be saying is anytime anybody wants to come through the door and talk about hydrogen… and they want to start making presentations about cars and trucks and trains, I'd be, like, “wait, let me stop you. What's your plan for fertiliser? What’s your plan in your refinery, you're taking [unabated] natural gas and you're using it in fertilser or in hydrocracking. What's your plan [to switch to cleaner hydrogen], but also, what do you need?” The argument against hydrogen cars Liebreich says that people in the oil industry are smart and understand the inefficiencies and expense of hydrogen-powered fuel-cell electric vehicles (FCEVs). ‘Our plasma electrolysers will cut the cost of green hydrogen by a factor of three’ Read more China to spend billions on hydrogen vehicles despite a minimal supply of clean H2 Read more “It's so obvious that hydrogen is less efficient because it's got these chemical processes and… everybody's done the numbers that show that if you have clean electricity and you want to drive somewhere, the last thing you would do is stick it in an electrolyser and then have all these other [energy] losses and then put it in a fuel cell and have more losses. You can't break the laws of physics — this is thermodynamics, it's the microeconomics of something that's very simple versus something that's very complex. “The other argument that never gets surfaced enough is that if you look at the cars and you look at the competitive dimensions, basically, electric wins on everything… sometimes by large margins. Nought to 60 in four seconds [for a battery electric vehicle (BEV)] versus nine seconds [in an FCEV], being able to fold the seats down and put loads of luggage in versus no, sorry, there's a hydrogen cylinder in there [taking up space]. [BEVs are] three times as efficient, and by the way, hydrogen [cars] don’t win on weight, they tend to be heavier, so they don’t even have that. “What they win on is being able to connect it to the nozzle and fill it with hydrogen for 400 miles in five minutes — setting aside the fact that the nozzle then gets really cold and actually the next person has to wait for it to warm up. That's very important if you're going on a trip that's 400 miles, because under that you might as well just buy the cheaper and better electric car. “So right now, the single advantage of the hydrogen car, wherever you can buy one, is that it can drive somewhere where you can't refuel because there's no hydrogen [filling stations].” He continues: “Even if that were fixed, even if we threw enough taxpayers' money at it so that then when you drive your 350-400 miles, there was a hydrogen fuelling station, that would save you the 45 minutes that you would otherwise have to charge [a BEV] to get back. “Okay, so you've saved 45 minutes. The rest of the year, you're back at home and back to your boring commute, and you're driving your 20 miles a day. Well, every time you do 300 miles, you have to go to a hydrogen filling station [unlike a BEV that you can charge at home]. So 40 times a year, you have to waste 10 minutes and maybe more driving to a hydrogen filling station. That's 400 minutes and you've saved yourself 45 minutes on the, maybe, two to five times that you actually drive to your cottage or to the Alps, or wherever. So even on the time spent, on an annual basis, it's not a win, it's a big loss for most people.” Power systems will struggle if all cars are fully electric, claims Hydrogen Council boss Read more World’s largest green hydrogen plan to tap 45GW of wind and solar in Kazakhstan Read more Recharge pointed to its recent interview with Hydrogen Council boss Daryl Wilson, who said that hydrogen cars will be needed because the electricity infrastructure won’t be able to cope with charging millions of BEVs. “That's nonsense,” says Liebreich. “[In] 1995, [people said] ‘we'll never use the internet because there are not enough modems’. [In] 2000, ‘we'll never do online video because there isn't enough bandwidth’, then, ‘you can't do multiple streams of video because you will never get fibre to the home’. We’ve got 30 years between now and 2050 [when countries plan to reach net-zero emissions] and we will simply have more and more investment. We’ve dug up the streets for cable, phone, gas, cable, fibre, electricity. It's a thing we do. We know how to just build slowly over time. This is not rocket science. “Plus, there's smart charging. And of course, we know we're going to be doing this because we're also going to be having to add capacity because of electric heating. And so the idea that you'll say, ‘no, no, we mustn't do that extension of existing infrastructure, we must build a completely new one [for hydrogen refuelling], it's nonsense, frankly.” The argument against hydrogen for domestic heating Some elements of the hydrocarbons sector, particularly gas distribution companies, have been lobbying to convert their gas grids to run on H2 — and have had some success, with the UK, for example, spending tens of millions of pounds on domestic hydrogen heating trials. As Recharge has previously reported, converting gas networks to hydrogen — which has smaller molecules than natural gas — would require all underground metal gas pipes to be replaced by polyethylene ones, including those concealed in walls and under floorboards in people’s homes. Every gas valve and compressor in each network would also have to be replaced, plus the energy needed to pump H2 around the gas grid would be three times higher than for natural gas. And of course, heat pumps are almost six times more energy efficient than hydrogen boilers, requiring 150% less primary energy to produce the same amount of heat. Global green-hydrogen pipeline exceeds 200GW — here's the 24 largest gigawatt-scale projects Read more There are also issues around the safety of using flammable hydrogen in people’s homes, as well as air quality, because burning hydrogen produces poisonous nitrous oxides that are also powerful greenhouse gases, Liebreich points out. “Hydrogen heating is just not going to happen,” he says. The reason why gas distribution companies are lobbying for hydrogen use in domestic heating is not because they want the world to go green, but because “they want to get us locked into using their gas pipes for decarbonisation, because that's their asset”, Liebreich adds. And the idea of blending clean hydrogen into the natural gas grid to reduce carbon emissions is “stupidly inefficient”, he adds. “If you put 20% hydrogen into your gas, 20% of your volume is only 7% by energy. And if it’s green hydrogen, you’ve started with electricity. One unit of that electricity could have produced three units of heat [with a heat pump]. Instead, you've put that unit into the gas grid that you've turned into hydrogen. So you've lost 30% of [the energy]. Then you put it in the gas grid, compressed it and done all sorts of things to it. And then you put it in your boiler and got 85% efficiency out of it. So from 1kWh of electricity, you’ve got heating worth half a kilowatt-hour. “If it’s blue hydrogen [derived from natural gas with carbon capture and storage], you’re always going to come back to my previous point, which is, ‘I’m not prepared to have this discussion until you've told me how you're going to decarbonise fertiliser!’”(Copyright) Technology Interviews Hydrogen Alert me about these topics Manage My Alerts Share: Email Recharge is part of DN Media Group. To read more about DN Media Group, click here ADVERTISE TERMS ABOUT US CONTACT US FAQ PRIVACY POLICY COOKIE SETTINGS Facebook Twitter LinkedIn Privacy and cookies Recharge is a part of DN Media Group AS, which is responsible for controlling your data on this site. We are responsible for the data you register with us and the information collected while you enjoy the website. We use cookies in a variety of ways to improve your experience. These include keeping our websites secure, delivering personalised content and ads and analyzing your activity patterns while on our pages. For more information on how to manage your privacy settings, please refer to our privacy and cookie policies. Read more OK, I understand",
    "commentLink": "https://news.ycombinator.com/item?id=37836237",
    "commentBody": "Oil sector is lobbying for inefficient hydrogen cars to delay electrificationHacker NewspastloginOil sector is lobbying for inefficient hydrogen cars to delay electrification (rechargenews.com) 345 points by amadeuspagel 14 hours ago| hidepastfavorite320 comments gmane 13 hours agoAnyone who has followed hydrogen as a fuel for a while sees pretty easily that its a solution in search of a problem. It really doesn&#x27;t make sense to use renewables to make hydrogen fuel to then burn: batteries are better for storage of energy in most cases (I will grant that there are edge cases where a liquid&#x2F;gas fuel is a better technical solution than batteries, but I think that options like ethanol are probably better than hydrogen for those use cases). My gut is that the majority of hydrogen proponents are oil and gas lobbyists or funded by them. reply _fizz_buzz_ 12 hours agoparentHydrogen for cars is dead. However long haul flights, fertilizer, green steel, etc. very difficult to find a carbon free alternative that isn’t hydrogen. reply espadrine 12 hours agorootparentSure, but the same companies that lobby for hydrogen for cars & heating, claiming green benefits there one day, don’t raise a finger to decarbonate hydrogen for fertilizers, which does make it seem like they don’t actually care about reducing CO₂ emissions. The article points out the hypocrisy:> Liebreich gives an example of Shell being happy to spend $12bn on a floating liquefied natural gas (LNG) platform, the Prelude, which has seen a host of problems, “but they won&#x27;t spend $12bn just producing blue, green, pink, or any other sort of clean hydrogen for those [existing] uses where we currently are driving 3-4% of global emissions”. reply Certhas 1 hour agorootparentAbsolutely. A sane hydrogen strategy can not be left to the lobbyists. Unfortunately, these companies have a lot of money to throw around, and they have no qualms about speaking to peoples \"anti-ideological\" impulses. In Germany, we have seen this strategy employed a number of times now politically: There is a clear technological winner (EV, battery, heat pumps, etc...), however with enormous transition costs (new infrastructure, people changing habits). So it&#x27;s inconvenient. You then simply claim that you are pro-climate, but you want a \"technologically neutral\" solution, claiming that the slightly inconvenient winning technologies are driven by \"green\" moralizing and ideology. As no alternative technologies are ready to go, and their trade-offs and inconveniences (e.g. where do you build all the wind farms to produce the electricity to get your hydrogen) are not apparent in the immediate future, you get to not inconvenience your electorate, seem pragmatic&#x2F;rational (vs the \"green ideologues\"), while still talking pro-climate.It&#x27;s not exactly been a winning strategy electorally for those who employ it the loudest, but it&#x27;s been an effective strategy to prevent political wins of the parties and politicians that actually want to make the sustainability transition happen.So far, it seems to have empowered the anti-establishment populist right instead... reply _fizz_buzz_ 14 minutes agorootparentprevAbsolutely agree. reply jillesvangurp 12 hours agorootparentprevFor long haul flight, hydrogen&#x27;s main use is going to be producing fuels that can actually go that distance. Hydrogen has a volumetric problem and it just isn&#x27;t a great fuel for that reason. Even over short distances this is problematic. It just takes up too much space. It&#x27;s about 2.5 x that of lng in both liquid and gas form. In liquid form, you need to cool it to a few degrees above 0 degrees kelvin, which makes it very impractical in a plane. And of course lng even in liquid form does not compare that favorably to things like kerosene.Green steel you can do without hydrogen. Basically you need lots of heat and some carbon to produce steel. Generating heat with electricity is not that hard. You can use induction, plasma heating, resistive heating, etc. And doing that is a lot more efficient than using that same electricity to first create hydrogen and then burn it.Hydrogen has its place, mostly as a chemical half product that is used to create other stuff. It doesn&#x27;t store very well; it doesn&#x27;t travel very well. It&#x27;s mostly used close to where it is generated. Which as of yet is done almost exclusively by expending large amounts of fossil methane or coal. Green hydrogen production is minuscule and relatively inefficient and costly. Most uses of hydrogen today are neither green nor sustainable. Or carbon free. And that&#x27;s going to stay like this for a while. Even just shifting current hydrogen production to being green is going to take a massive investment and take decades. reply nwiswell 12 hours agorootparent> Hydrogen has a volumetric problem and it just isn&#x27;t a great fuel for that reason. Even over short distances this is problematic. It just takes up too much space.Blended wing body designs may solve the volume problem. It&#x27;s only intractable if you try to fit the same energy content into a modern airliner that was designed for Jet A.Weight is the usual suspect in aerospace, and fuel weight significantly favors hydrogen over Jet A (factor of ~2), so a blended wing body design with a much larger internal volume wouldn&#x27;t even have to have equal aerodynamic efficiency in order to be a suitable replacement.To be clear, this is not a simple shift and will probably take decades, but it is merely an engineering challenge rather than a fundamental constraint. reply ben_w 11 hours agorootparentprev> Green hydrogen production is minuscule and relatively inefficient and costly. Most uses of hydrogen today are neither green nor sustainable. Or carbon free. And that&#x27;s going to stay like this for a while.Your other points hold, but I don&#x27;t think this one is necessarily so (depending what you mean by \"a while\"): electrolysis is so incredibly trivial that I suspect it&#x27;ll become green as fast as the grid in general, and if current exponential trend continues, that&#x27;s going to be almost everything within a decade.If the current exponential continues. It might, but such is never assured. reply jillesvangurp 11 hours agorootparentWe&#x27;d need a lot of renewables for this to work though. Like a lot more than we currently have or that is currently planned&#x2F;under construction. Orders of magnitude more. At the cost of a few trillion $. The math just isn&#x27;t great for this.Just doing the simple math of the twh needed to produce the fuel for a major airport on a daily basis is kind of eye watering. Just a little back of the envelope math makes it really obvious that that&#x27;s not going to be a thing any time soon even if we do get some magic laws of physics defying electrolysis technology (those pesky laws of thermodynamics are hard to beat though).And even just shifting current hydrogen production, which is good for a few percent of carbon emissions today, is a huge undertaking. I agree that electrolysis improvements will actually make that cost effective to do at some point. Possibly even as soon as next decade. But right now it just isn&#x27;t and most of those electrolysis companies still have a few things to prove. Like having working products or the ability to produce those cost effectively at scale. Either way, within now and a decade, we won&#x27;t have nearly enough renewables in place to power any of this new hydrogen economy. We&#x27;d struggle to power the current one with that. As long as renewables remain scarce, hydrogen is not a great use case for wasting them. reply ben_w 11 hours agorootparent> Orders of magnitude moreYes. 30% per year compound growth does that very quickly.(Exponentials will turn into s-curves of course, no guarantees, but growth here doesn&#x27;t have to be asymptotic to the current market). reply xoa 12 hours agorootparentprev>very difficult to find a carbon free alternative that isn’t hydrogen.The basic mistake you&#x27;re making here is thinking that \"carbon free\" is key. It is not, what matters is that a given energy source is carbon NEUTRAL. Anthropogenic global warming is driven primarily by net increase in atmospheric CO2 on our timescale. When CO2 in consumed from the atmosphere, and then shortly released again (textbook example being a plant fixing CO2&#x2F;H2O into sugar and the getting eaten by an animal and the animal the metabolizing sugar back to CO2&#x2F;H2O) the short term net change is zero. The problem has come from releasing stored carbon that was fixed in geologically long term ways, ie, fossil fuels.We&#x27;re used to thinking of \"hydrocarbons\" as equivalent to \"fossil fuels\" but that&#x27;s not true. It&#x27;s perfectly possible to directly synthesize any hydrocarbon from atmospheric CO2, water, and zero carbon energy like solar. Burning those hydrocarbons later will result in net zero change to atmospheric CO2, and thus are perfectly acceptable from an AGW POV (at ground level in particular burning fuels have other pollution issues that may be worth getting away from, but not global warming). The issue is \"just\" cost, it&#x27;s much more inefficient and thus costlier to go solar -> fuel synthesis -> combustion -> useful work vs solar -> transmission -> battery -> work. But for applications where battery energy density is insufficient or other properties are required (like high performance aircraft that use fuel as coolant as well as energy) that could be worth it.So green hydrogen directly faces not just batteries, but green methane, gasoline and other hydrocarbons. Which are far, far easier to work with and have many better properties than hydrogen itself. And on top of course feed seamlessly into existing infrastructure and system. reply _fizz_buzz_ 15 minutes agorootparentSynthetic fuels are also made from hydrogen. That&#x27;s actually what i had in mind. reply bluGill 12 hours agorootparentprevThe processes to make synthetic fuels are well known have proven to scale to a countries needs for energy. Germany did it in WWII, South Africa did it for a while too.The problem is it costs a lot of energy, and so it ends up 4-5 times more expensive than oil from a well. reply xoa 11 hours agorootparent>The problem is it costs a lot of energy, and so it ends up 4-5 times more expensive than oil from a well.The real problem there though of course is that oil from the a well has always been cheating by not pricing in its externalities. To get an apples-to-apples comparison with synthetic fuel, fossil fuel would also have to be made at a minimum [0] carbon net neutral too, such as by running an atmospheric scrubber and ensuring that for every ton of carbon coming out of the ground a ton was getting captured again in an equivalently long term stable way.----0: Fossil fuel extraction has always resulted in a ton of other externalities too, not just in terms of massive non-carbon pollution but geopolitical costs. That itself may well be a driver, if a country can switch fully to renewable&#x2F;nuclear power and then run its entire economy solely off of that via grid&#x2F;batteries&#x2F;synthetic hydrocarbons the security implications alone are pretty massive. No more fossil fuel blackmail from hostile regimes, ever. reply nrr 12 hours agorootparentprevI&#x27;ve seen some presentations over the last half a decade that are trying to couple this to fission as a form of cogeneration, and the carbon capture story for the resulting hydrocarbon synthfuels is pretty compelling. reply lurgburg 9 hours agorootparentprevWasn&#x27;t Germany&#x27;s WWII \"synthetic fuel\" just coal derived gasoline? reply actionfromafar 12 hours agorootparentprevSteel production really needs hydrogen though. Hopefully the first test-production facilities will come online soon in Sweden. But I agree on for instance air travel. Don&#x27;t faff around with hydrogen in planes. Convert the hydrogen to something liquid, like the Porsche e-fuel, then use it in regular jet turbofans. reply 8note 9 hours agorootparentWhat is the hydrogen doing chemically such that it is needed? reply BenoitP 2 hours agorootparentIt replaces the massive amounts of coal that is used. It changes from (simplifying) 2 Fe2O3 + 3 C → 4 Fe + 3 CO2to Fe2O3 + 6 H2 → 2 Fe + 3 H2OAt the moment emissions are: 1.4 kg CO2 per kg of steel produced. And westernized countries will use 321 kg of steel per capita per year.So switching to hydrogen could save about 500kg of CO2 per capita. Carbon capture of the fumes could help alleviate some 30% of that. Direct air capture is just not feasible, or wayy to expensive compared to just using H2.In any case, even with H2; 321 kg of steel per capita will have to be reduced (main uses: construction, transport, industry, pipes, machines, weapons) reply dgudkov 7 hours agorootparentprevQuebec now tests a hydrogen train.[1] https:&#x2F;&#x2F;www.cbc.ca&#x2F;news&#x2F;science&#x2F;hydrogen-train-quebec-city-1... reply toss1 12 hours agorootparentprevYup. Even Toyota, a big H2 promoter is bailing and announcing high-range. solid-state, fast charging batteries [0, 1, 2]. Makes me wonder if the H2 promotion was just a ruse to misdirect the rest of the industry while it caught up, and possibly even leapfrogged (we&#x27;ll see if their announcements play out), on battery tech?[0] Toyota Teases Solid-State Batteries in 2027https:&#x2F;&#x2F;spectrum.ieee.org&#x2F;toyota-solid-state-battery[1] Toyota reveals its plan to catch up on EV battery technology Three liquid chemistries, solid state cells, and flatter battery packs.https:&#x2F;&#x2F;arstechnica.com&#x2F;cars&#x2F;2023&#x2F;09&#x2F;toyota-reveals-its-plan...[2] LGES to supply Toyota with batteries, invest $3 billion in US planthttps:&#x2F;&#x2F;www.reuters.com&#x2F;business&#x2F;autos-transportation&#x2F;toyota... reply pornel 12 hours agorootparentI think Toyota is making these pie-in-the-sky announcements to Osborne[1] other manufacturer&#x27;s BEVs, and delay electrification too, because Toyota&#x27;s actual battery tech that exist is far behind their competitors.[1]: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Osborne_effect reply mulmen 12 hours agorootparentprevI think Toyota also had their hands tied a bit because the entire country of Japan has heavily invested in hydrogen. I can understand why it makes sense strategically for a country like Japan without their own oil (or lithium?) reserves. Toyota is a Japanese industrial powerhouse so I wouldn’t be surprised if they were receiving subsidies and maybe even direction on hydrogen research. Only now that they have invested enough to prove hydrogen is a fantasy can they move on to batteries.I’m not sure it’s such a loss though. Nissan was early with the Leaf but nobody talks about them as cornering the EV market. Toyota can benefit from the battery advancements everyone else made over the last 15 years and catch up quickly.At the time it made sense to invest in hydrogen. And in hindsight it wasn’t as much of a mistake as it might seem. It was a moonshot but they didn’t risk their existence on it and if it had worked they’d look like geniuses. If hydrogen was feasible Toyota and Japan would be world leaders in energy tech or at least have secure domestic energy independence. reply pauldenton 11 hours agorootparentMaybe it just needs to work in their borders Japan had a unique response to the 1973 oil crisis. Given recent instability in Russia&#x2F;Ukraine, Afghanistan after the Withdrawal, and now Israel, having a hedge against petroleum primarily sourced in the Middle East and Russia it&#x27;s a smart hedge to have. Besides, Americans have switched from wanting big extravagant American Made cars to small fuel efficient ones from Japan before reply senectus1 10 hours agorootparentprevabsolutely.but the interesting thought experiment to have is... once you have converters all over the place for those uses... would that make it more attractive to cars? reply credit_guy 8 hours agoparentprev> batteries are better for storage of energy in most casesThat&#x27;s not true. There are two main storage cases: from day to night and from summer to winter. Because there are 365 days and nights in a year and only one summer and one winter, a battery for seasonal storage needs to be 365 times cheaper than one for daily storage to break even after the same number of years. Of course, there are no batteries that are 365 times cheaper than lithium-ion batteries. Nothing comes close.People are thinking that chemical energy storage could work for the case of seasonal storage. It doesn&#x27;t quite work right now, but it&#x27;s not off by 2 orders of magnitude, it&#x27;s off by maybe a factor of 5. It is much more likely that we&#x27;ll succeed in making green hydrogen production, transportation and storage economical than that we could make batteries cheaper by a factor of 100.Separately, there is a fairly good chance that we&#x27;ll be able to extract hydrogen from underground deposits (google \"white hydrogen\", e.g. [1]).Third, hydrogen storage economics enjoys the square-cube law: larger pressurized tanks can hold a lot of hydrogen for not a much larger cost than smaller tanks. So hydrogen could make sense for applications where very large tanks are needed, and one such application is railways. A typical train oil car has a volume of about 130 m3. At 700 bar (typical storage tank pressure), one m3 of hydrogen weighs about 42 kg, so this is about 5.5 tons. Hydrogen has about 3 times the energy density of diesel, so that would be the equivalent of about 16 tons of diesel. Train have an efficiency of at least 400 ton-miles per gallon, which is more than 125 ton-miles per kg, or 2 million ton-miles for 16 tons. In other words, such a tanker car could be enough to propel a ten thousand ton freight train for 200 miles. It would be much easier to convert diesel locomotives to burn hydrogen than to electrify thousands of miles of railways, so there&#x27;s a fairly good chance that rail could be hydrogen&#x27;s killer app.[1]https:&#x2F;&#x2F;www.theguardian.com&#x2F;environment&#x2F;2023&#x2F;aug&#x2F;12&#x2F;prospect... reply tcfhgj 7 hours agorootparentSummer to winter isn&#x27;t a real storage case.If you need more energy in the winter, you build more wind power, not PV to store it an entire year.And episodes of little wind+sun have a duration of less than a month.And there are more storage use cases, e.g. short term grid stabilization ( 95% of materials already. We have lifepo4 batteries that are stable, don&#x27;t have cobalt and be charge up to 100% without destroying them. In reality I don&#x27;t have to point to some unknown incredible improvements in the future, they work today.I see hydrogen as the oil industrial complex and diesel & ice industrial complexes wanting to maintain all their infrastructure that burns fuel for engines. It makes sense from their standpoint. reply rocqua 4 hours agorootparentFor seasonal storage, because you cycle then so rarely, and demand between charging and discharging is so different, the economics favor low cost per kwh solutions, and care much less about efficiency. reply tw04 13 hours agoparentprevI don&#x27;t blame you for being skeptical, but it isn&#x27;t just the oil and gas industry. The Japanese government went all-in on hydrogen.https:&#x2F;&#x2F;thediplomat.com&#x2F;2023&#x2F;07&#x2F;a-look-at-japans-latest-hydr... reply theshrike79 12 hours agorootparentJapan is clinging to hydrogen like a drowning man to driftwood.About 80% of Japan&#x27;s energy is imported coal and natural gas. Their electric grid is a bonkers mix of 50Hz and 60Hz single-phase.Hydrogen is the only viable solution for them and even that&#x27;s not going too well despite lofty goals. They can&#x27;t build the tech and infrastructure alone and the rest of the world just isn&#x27;t as enthusiastic on H2. reply jbm 12 hours agorootparentThe 50Hz and 60Hz is an accident of history and neatly splits the country in half. It&#x27;s not a problem. reply VBprogrammer 12 hours agorootparentTheir domestic voltage is also 100v. I&#x27;ve never thought about it before but it does seem like that might pose some challenges in electrifying their road transportation. 240v is considerably more useful when you are trying to suck down 7kw or more. reply jbm 11 hours agorootparentNow that you mention it, the electric connection in my home in Tokyo was much worse than the one I have in Calgary. They might have connections of 60 amps [1] or less where I lived (whereas my home in Calgary has a 100 amp connection which can be upgraded to 200 if I paid for it)[1] Found this here (https:&#x2F;&#x2F;www4.tepco.co.jp&#x2F;en&#x2F;customer&#x2F;guide&#x2F;moving-e.html), I don&#x27;t recall what exactly it was where I lived. reply theshrike79 12 hours agorootparentprevIt is a problem, because there&#x27;s limited capacity to move electricity between the systems. You can just plug them together, they have essentially two separate grids. reply earthboundkid 11 hours agorootparentI think it would be a problem if they had ten tiny grids, but is two decently sized grids that are in roughly the same time zone (so peak solar doesn’t need to go from A to B) a big deal? reply theshrike79 4 hours agorootparentIt is when a 1500MW nuclear power plant built in area A is essentially useless for anyone in area B.Yes, you can transform from 50Hz to 60Hz, but the capacity is limited and can&#x27;t be grown infinitely. reply fomine3 9 hours agorootparentprevIt was a huge problem when earthquake stopped many power plants in east area. replygmane 13 hours agorootparentprevSure, and it&#x27;s by and large viewed as a complete failure[0]. Is your argument that we should repeat that failure?[0] https:&#x2F;&#x2F;www.popularmechanics.com&#x2F;science&#x2F;green-tech&#x2F;a4266501... reply tw04 13 hours agorootparentNo? I&#x27;m not sure why you jumped to attack me when I was simply providing a counter-point to your claim that it&#x27;s all \"oil and gas\". I drive an EV, I&#x27;m quite happy with it.As to the article: they fail to mention that Japan hasn&#x27;t even begun to throw in the towel. The newest CEO of Toyota is doubling down on hydrogen powered vehicles.https:&#x2F;&#x2F;www.autonews.com&#x2F;mobility-report&#x2F;toyotas-bold-us-pla... reply ZeroGravitas 12 hours agorootparentJapan has little traditional fossil fuels, but they were making a lot of noise about exploiting the methane hydrate in their seas:> Methane hydrate is confirmed to exist abundantly in the sea near Japan and is expected to become one of the domestic energy resources of the futurehttps:&#x2F;&#x2F;www.japex.co.jp&#x2F;en&#x2F;technology&#x2F;research&#x2F;mh&#x2F;So in this case, they&#x27;re really just a subset of the \"oil and gass industry\" boosting hydrogen vehicles. reply fomine3 9 hours agorootparentIt has been just a dream for a long time. reply appleiigs 12 hours agorootparentprevAlso, it&#x27;s not a complete failure according to the article he mentions \"The 20-page report doesn’t argue for hydrogen’s complete removal from the energy mix. In fact, REI argues that hydrogen is vital for industries where decarbonization is particularly tricky (think: aviation, shipping, and steelmaking). However, to use hydrogen in place of electrification via other renewable sources is a mistake, REI says.\" reply elihu 12 hours agorootparentprevI think that may be due to pressure from their automotive industry, who for whatever reason are very resistant to making battery electric cars. reply alexfromapex 12 hours agorootparentIt&#x27;s because batteries are expensive to produce, maintain, and are not as environmentally friendly as going from solar directly to hydrogen via electrolysis. There&#x27;s more parity for existing fossil fuel engine use cases with hydrogen as well. reply dieselgate 12 hours agoparentprevAm not well versed on these considerations but it appears one advantage of hydrogen is energy density compared to batteries. This may be less an issue for passenger vehicles but could be more relevant for air travel - as an example. No comments or opinions otherwise re: funding etc.Found the below article useful as a primer:https:&#x2F;&#x2F;www.energy.gov&#x2F;eere&#x2F;fuelcells&#x2F;articles&#x2F;fuel-cell-and... reply spott 12 hours agorootparentWhat kind of energy density? There is energy density by volume (where it matches batteries, when you ignore all the stuff you need to keep hydrogen at 700bar safe), and energy density by weight (where it wins pretty handily against pretty much everything). reply dieselgate 12 hours agorootparentAm not historically familiar with this topic though the attached paper above shows a comparative figure of Specific Energy in terms of Wh&#x2F;Kg at both 5k and 10k psiEdit: interesting to do the conversion and realize 700bar is just over 10k psi - misunderstood the parent comment regarding \"700bar safe\"Edit2: further interesting to note how the 5k compression of the hydrogen has _higher_ Specific Energy density than 10k due to _decreased_ pressure vessel requirements reply Retric 12 hours agorootparentJust be careful with Hydrogen energy density as there’s a huge range of non obvious issues. Compressed you lose a great deal of energy converting it to kinetic energy and weight due to the containers + engines. Fuel cells also need electric motors and ideally a hybrid battery.Cryogenic fuel looks great until you need to have it sit around unused. Embrittlement, volume, cost, and a rage of safety hazards make it unappealing for aircraft or ships.So, my guess is the ideal long term solution barring “super batteries” is some sort of synthetic hydrocarbons that uses atmospheric CO2 but we are a long way from viability there. reply linsomniac 12 hours agorootparentprevHaven&#x27;t we tried hydrogen for air travel before? ;-) reply Reason077 12 hours agorootparentIt&#x27;s been used successfully for space travel, in liquid (cryogenic) form. The Space Shuttle&#x27;s main engines ran on hydrogen fuel, for example.However the latest generation of rockets seems to have moved away from hydrogen back to things like RP-1 (kerosene) and liquid methane. The high cost and difficulties of working with hydrogen is a factor in this. reply the-dude 11 hours agorootparentHindenburg reply DriverDaily 12 hours agorootparentprevAnybody who needs to run 24&#x2F;7 and&#x2F;or can&#x27;t count on an electric grid could use hydrogen power: A logistics network, an army, public transportation, utility companies, forestry or mining... reply lazide 12 hours agorootparentOr they could just use the well known https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Fischer–Tropsch_process to convert it the rest of the way to a hydrocarbon, not have the pesky raw hydrogen issues, and be able to use all their existing infrastructure to burn it? With the advantage of pulling carbon from the air in the process? reply Reason077 12 hours agorootparentprevNot really, because if you don&#x27;t have access to electricity then you very likely don&#x27;t have access to hydrogen either. Hydrogen is much more difficult to transport and store compared to natural gas, gasoline, diesel, etc. reply DriverDaily 11 hours agorootparentGetting electricity out of hydrogen works the same way as getting electricity out of all the sources you mentioned, except your generator now produces zero emissions. reply dieselgate 12 hours agorootparentprevThis is a technical requirement often overlooked by the Consumer Market compared to the Commercial Market - the heavy and long usage of equipment in remote places (generators that run for days or shipping across oceans).Hydrogen may be a solution along with other hybrid approaches (WA state ferry electrification being relevant [1])[1] https:&#x2F;&#x2F;wsdot.wa.gov&#x2F;construction-planning&#x2F;major-projects&#x2F;fe... reply cjrp 12 hours agorootparentprevJCB are heavily in to hydrogen. They’ve shown how they can convert existing diesel equipment, and setup fuel bowsers on work sites. reply jimt1234 12 hours agorootparentprevEngineering Explained occasionally discusses hydrogen energy density: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=vJjKwSF9gT8 reply Pxtl 12 hours agorootparentprevOnce you&#x27;re already committed to green hydrogen (electrolysis-made hydrogen) you may as well go whole hog and do green methane (methane made through the Sabatier process, which consumes Hydrogen and CO2).Making a methane-powered jetliner is far more practical than a hydrogen-powered one. reply bryanlarsen 12 hours agorootparentThat&#x27;s right, but there&#x27;s an even better option: making synthetic kerosene from green H2 and CO2 so it can be used in existing jetliners. reply ge96 12 hours agorootparentprevI thought it was hard to contain since the particle is so small (leaks).Liquid I suppose, temperature concern though. reply freedude 12 hours agorootparentprevThey definitely fly better than a lead balloon (batteries). reply scythe 12 hours agorootparentprevFor boats, fuel cells can make sense. But air travel not only requires a high energy density but also a high power density, and here the fuel cells are relatively weak even by car standards. The alternative is to burn hydrogen, which introduces an inefficiency that eats up most of your advantage. For a car, you can have a small auxiliary battery, but a jet engine will run at max power for several minutes continuously during take-off, and for existing jet engines the power-to-weight ratio is considered a crucial figure of merit.For hydrogen storage, the DoE targets are truly sobering [1]. That whittles down hydrogen from roughly 100 times as dense as batteries to about 5. Throw a 50% thermodynamic efficiency (jet engines are not efficient!) on top of that and hydrogen still has an advantage, but it lags quite badly behind jet fuel. There have been four decades of intensive government-sponsored research effort into hydrogen storage materials, but all existing systems in practical use rely on fiber-composite tanks at 700 bar.You get better results with an ammonia-burning jet engine. Here the tank weight is negligible and the hydrogen storage density is effectively 15% (after correcting for the enthalpy of formation). But ammonia still has just half the energy density of jet fuel, and it&#x27;s rather unpleasant to work with. Plus, the existing production process of ammonia faces its own serious inefficiencies, and extensive investigation of more efficient ammonia production has been painstaking with only one Japanese startup [3] that is behind its timeline (probably a little COVID-influenced) and other contenders at lab stage.With the direct ammonia fuel cell, the power density issue is even worse, but the energy density is very good. This technology competes well with fossil fuels for weight-sensitive applications that do not require high power (DARPA has been interested in a DAFC drone). But ammonia fuel cells are mostly at lab stage, partially because ammonia production remains disappointing (despite a theoretical energy cost of zero) despite considerable research effort.1: https:&#x2F;&#x2F;www.energy.gov&#x2F;eere&#x2F;fuelcells&#x2F;hydrogen-storage2: https:&#x2F;&#x2F;www.google.com&#x2F;search?client=firefox-b-1-lm&q=tsubam... reply malchow 13 hours agoparentprevAre you referring to hydrogen as a mobility fuel specifically? I generally agree.However, it is a critical large-scale industrial input, a replacement industrial input for several additional large industries, and an excellent large-scale stationary energy store. reply mohaine 12 hours agorootparentIs it an excellent large-scale stationary energy store though?I mean, it seems like it should be but* Hydrogen really likes to leak.* Hydrogen breaks down the container holding it over time.* Hydrogen is very low density. This means you need either a very large container or very high pressure. Combined with 1 and 2 this is going to be expensive.I&#x27;m really asking the question. I would love to know the full cost difference with current tech vs other options. reply londons_explore 12 hours agorootparent> * Hydrogen really likes to leak.Yes, but the rate of leakage is tiny enough that for any reasonable sized tank it&#x27;s going to be years till it&#x27;s all leaked away.The real risk is the leakage causing explosions if you ever put a tank in a confined space. reply pixl97 12 hours agorootparentThat&#x27;s probably only the &#x27;primary&#x27; real risk, there are some number of secondary real risks of hydrogen being a very reactive chemical. Not in the flammable boom boom, but the interacting with stuff it shouldn&#x27;t and causing degradation and corrosion. reply jfengel 12 hours agorootparentprevIt leaks a lot faster if it corrodes a hole in the container. Hydrogen is fairly reactive. reply baq 13 hours agoparentprevBatteries are heavy, expensive and we don’t have infrastructure to charge them at the required scale.I don’t believe hydrogen is the answer but methane cells could be… liquid energy storage is just so much more practical compared to alternatives. reply linkjuice4all 12 hours agorootparentWe (in the US) have very little hydrogen infrastructure but there&#x27;s high voltage power lines basically everywhere - so it seems like hydrogen has the infrastructure and scaling issues.Batteries are indeed heavy - but so are portable hydrogen storage cell that don&#x27;t easily explode or require liquid cooling.I agree with you that hydrogen isn&#x27;t the answer but methane doesn&#x27;t really seem to be either. Batteries and electric cars are already here - they just need to continue scaling the infrastructure and work on lightening up the vehicles. reply LtWorf 12 hours agorootparentThere isn&#x27;t enough infrastructure to power all the eventual electric cars. reply bryanlarsen 11 hours agorootparentYou&#x27;re right, we&#x27;ll need about 20% more electricity if we replaced all our ICE cars with electric ones. If we stop selling gas cars in 2035 then by about 2045 we&#x27;ll hit ~99% electric cars.Are you implying we can&#x27;t increase our infrastructure by 20% in 20 years? reply jvolkman 9 hours agorootparentWhat&#x27;s involved in increasing electricity infrastructure by 20%? Have you looked at the timeline of some large scale, coordinated effort public infrastructure projects?One that&#x27;s close to home for me is the Seattle area&#x27;s light rail system. Voters approved a plan that includes an extension of the line from downtown Seattle to the neighborhood I live in sometime around 2016. Service is now scheduled to start in 2039, but there have already been multiple delays. reply bryanlarsen 7 hours agorootparentInterconnect backlog queue is limiting factor, and it&#x27;s five years.There are currently enough projects in the queue to double our generating capacity.https:&#x2F;&#x2F;www.utilitydive.com&#x2F;news&#x2F;grid-interconnection-queue-... reply ben_w 11 hours agorootparentprevThis is one reason I like to suggest putting PV onto cars directly, even though normal shaped cars will only get 8-12 miles of extra range per day from this, and even though the surface of a vehicle is one of the worst places for PV cells: 8-12 miles per vehicle per day makes a big difference to the overall infrastructure requirements. reply theshrike79 4 hours agorootparentSmall scale plants are the key when the grid is either unreliable or insufficient.It&#x27;s not much of an issue to have, say, a 1000kWh battery with some small&#x2F;medium scale solar or wind power feeding it. Or even the grid if there&#x27;s extra.Then everyone around that battery uses it primarily for their energy needs and only when it runs out they tap into the larger grid.The only thing solar panels on cars are good for is keeping the AC running in the summer so the car (and battery) are cool at all times. reply natch 12 hours agorootparentprevYet. reply pie420 11 hours agorootparentprevThere wasn&#x27;t enough silicon wafer manufacturing infrastructure in 2005 to meet global silicon wafer demand in 2023 either, yet somehow humans magically got together, invested money, built plants, and wow, a decade and a half later there is silicon wafer manufacturing infrastructure to meet global demand.Imagine not understanding the ability for humans to build things. Pretty sure the comment i&#x27;m responding to is a gas industry shill. reply DiggyJohnson 9 hours agorootparentIt is both a violation of HN guidelines and reason to assume that the person you are simply disagreeing with is an industry shill rather than simply someone you disagree with. reply theshrike79 12 hours agorootparentprev_You_ don&#x27;t. We do.There are 6 over 100kW charging spots under 1km of my house. Although I don&#x27;t need any of them, because I can charge where I park at 2kW all day and night. reply baq 2 hours agorootparent6 is a number an order of magnitude too small unless you live in a desert. If you live in a city, it&#x27;s probably closer to two orders.To support 60 assuming ~50% utilization you&#x27;ll need 1MW per 1 sqkm additional power. This isn&#x27;t a small number. The grid anywhere is absolutely not ready for such a rapid increase of load. There isn&#x27;t enough refined copper produced worldwide to upgrade the grid. I mean setting up a copper mine on 16 Psyche is not a batshit insane idea here. reply wilg 12 hours agorootparentprevWell, we have a power grid and not a hydrogen grid. reply alexfromapex 7 hours agorootparentHydrogen can be created from water and electricity reply theshrike79 4 hours agorootparentIt can yes, but the efficiency is laughable. It&#x27;s only viable when you&#x27;ve got essentially free renewables AND you&#x27;ve filled every battery you can. reply vel0city 12 hours agorootparentprevAbout the infrastructure. I&#x27;ve got many ways to recharge my car available today. I&#x27;ve got zero places to fill hydrogen. I don&#x27;t even have many CNG options. reply jackmott42 12 hours agorootparentprev>we don’t have infrastructure to charge them at the required scale.Yeah we do, or to be more precise, we are having no trouble expanding the infrastructure at the same rate people are buying the cars. reply dangus 12 hours agorootparentprev“We don’t have the infrastructure” is a temporary problem. The same could have been said about gas stations and the electrical grid capacity needed for widespread air conditioning.Weight isn’t such a crazy thing either. The Tesla Model Y long range weighs 900 pounds more than a Honda CRV. It’s a lot, but the most popular car in America is the Ford F-150 which weighs about the same as a Model Y at the low end.The weight efficiency can only improve from here and more charging infrastructure and faster recharge times will mean less demand for long range variants. This is ideal for electrics because shorter frequent stops are more time efficient for EVs while for gas vehicles it’s the opposite.The great thing about electricity is that it’s even easier to transport and generate than liquid fuel storage. Generation can be centralized with a diverse set of energy sources. reply baq 2 hours agorootparentWeight is an air pollution problem - tires and brake pads release fines during normal usage. Regenerative braking helps, but only so much.https:&#x2F;&#x2F;academic.oup.com&#x2F;toxsci&#x2F;article&#x2F;145&#x2F;1&#x2F;98&#x2F;1627384Electricity is easy to transport but very hard to store and it&#x27;s the storage that&#x27;s the problem, see batteries. reply ninkendo 12 hours agorootparentprev> The weight efficiency can only improve from hereI don’t think this is a given. I can imagine plenty of future battery tech that is much more physically dense, and also more energy-dense, but with an overall mass that’s heavier than what we have now. Consumers really want more range, and they don’t really care about their car’s weight so long as it isn’t prohibitive. (They will definitely care collectively as the roads deteriorate faster and they have to pay more in tax dollars and wasted time spent in construction to repair them, but that will end up being a tragedy of the commons.) reply rootusrootus 12 hours agorootparent> They will definitely care collectively as the roads deteriorate faster and they have to pay more in tax dollars and wasted time spent in construction to repair them, but that will end up being a tragedy of the commonsThe difference in wear between a 3600 pound RAV4 and a 4400 pound Model Y is so minor that it&#x27;ll disappear in the noise. Nobody will notice. The only time it&#x27;ll even come close to mattering is on residential streets that never see any commercial trucks. And even then, nobody cares now about the 6500 pound HD pickups that are so popular, so I can&#x27;t see why that would suddenly become an issue. reply ninkendo 11 hours agorootparentI mean, if we’re picking models to support our points, I’d counter with the Hummer EV which is 9,630 lbs. Road wear goes up with the 4th power of weight, so doubling the weight increases road wear 16x… it’s not something we should just ignore. reply theshrike79 4 hours agorootparentThat Hummer also does 0-60 in 3 seconds. Imagine the amount of damage it can do just going all out from traffic lights and crashing into someone...It&#x27;s like someone took America and distilled it into an EV :D reply rad_gruchalski 10 hours agorootparentprevThat car cannot be legally driven in Europe on a B license due to its weight. Phew. reply ninkendo 10 hours agorootparentIf only we had that kind of sanity in the US… the electric hummer is an absolute monstrosity and should never have been made IMO. I am a supporter of EV’s but, I am increasingly skeptical that legacy US automakers will ever “get it” when it comes to efficiency, and it’s not surprising at all to end up with monster vehicles like the EV hummer. But I do admit the American consumer shares as much of the blame by demanding such large (in size, if not necessarily weight) vehicles in the first place. It’s like GM said “we need an SUV at least this big, and we need at least this much range. Engineers said it’ll weigh 10,000 lbs? Fuck it, nobody will notice, ship it.” reply nithril 12 hours agorootparentprevIs range really an issue if you can easily find charging spot and recharge in less than 15min? reply ninkendo 11 hours agorootparentIt is in the mind of consumers, yes (I have not offered my personal opinion and don’t want to, as it’s irrelevant to the point.) There has been a lot of consumer studies on this and range anxiety is a very real thing. reply dangus 10 hours agorootparentprevI’m already on board with the urbanist camp that believes car-focused infrastructure is economically and environmentally unsustainable. EVs don’t help the American addiction to stroads and strip malls.I just happen to also believe that EVs are no worse than the status quo and are almost certainly at least marginally better than any other alternative on the table. reply 0cf8612b2e1e 13 hours agoparentprevI still believe there is potential for hydrogen&#x2F;ammonia&#x2F;methane fuel to be a seasonal storage mechanism. Battery technology does not seem capable of offering grid scale storage for the entirety of Winter. While the solar->fuel->electricity conversion takes huge efficiency hits, for “free” renewable energy, it can still pencil out as economical. Plus, it is feasible today without unobtanium (but not currently cost competitive vs fossil fuels) reply theshrike79 12 hours agorootparentHydrogen is _really_ hard to store. It leaks from every container, transport and storage losses are just an accepted fact.It might be a good option when we have so much renewables that we can&#x27;t find any more batteries to shove it in. Then we can use it to store compressed hydrogen, that can be released as electricity from large-scale fuel cells during calm cloudy days. reply redox99 12 hours agorootparentprev> seasonal storage mechanismIs this even needed? With solar, wind, etc storing more than a few days worth of energy seems very wasteful. reply 0cf8612b2e1e 10 hours agorootparentAvailable solar energy is significantly lower in winter. Additionally, heating typically requires more energy than cooling, so it is a double whammy. Solar alone cannot generate sufficient power without ludicrous over provisioning which would be wasted the majority of the year. reply londons_explore 12 hours agoparentprevYou know what does make it make sense?If you can take natural gas out of the ground, split it into hydrogen and CO2 (releasing heat, which can be used to generate electricity), and inject the CO2 back underground to help release more oil and gas.At this point, you have hydrogen gas.Sure, you could burn that hydrogen to make more electricity, but it works out financially better if you can directly use it.The benefit of this approach is it can make use of most of the existing oil and gas infrastructure, and countries sitting on big gas reserves have a use for them.The downside is it makes minimal sense unless there is some financial disincentive from just burning the gas and releasing the CO2. reply crote 12 hours agorootparentUnfortunately it is a bit tricky in practice, and still have a very significant global warming footprint[0]. You&#x27;re never going to capture 100% of the CO4, and oil and gas wells tend to leak quite a lot of methane to the atmosphere.So-called \"blue\" hydrogen only results in about 10% less GHG emissions than traditional \"grey\" hydrogen. It&#x27;s something, but not big enough of a difference to actually be usable.[0]: https:&#x2F;&#x2F;onlinelibrary.wiley.com&#x2F;doi&#x2F;full&#x2F;10.1002&#x2F;ese3.956 reply theshrike79 12 hours agorootparentprevYou hit the nail on the head, this is why oil companies are pro-hydrogen.They can greenwash their natural gas into \"green\" hydrogen and get even more profits. H2 prices are at least 5-10x compared to natural gas. reply espadrine 12 hours agorootparentprevIsn’t that effectively what they call “blue hydrogen”[0]? All oil companies seem very slow in deploying that; there is no question that it is much more expensive than just releasing the CO₂ in the air.[0]: https:&#x2F;&#x2F;corporate.exxonmobil.com&#x2F;what-we-do&#x2F;delivering-indus... reply londons_explore 12 hours agorootparentI think oil+gas companies are currently just waiting for the right moment to deploy it.Blue hydrogen only makes sense with global carbon caps, taxes or quotas. And it only makes sense if CO2 injected underground doesn&#x27;t count towards that cap&#x2F;tax&#x2F;quota.So far, there is no political appetite for global taxes&#x2F;caps&#x2F;quotas, so blue hydrogen is only at pilot-plant scale. reply candiddevmike 12 hours agorootparentprevThis is similar to fracking, no? Doesn&#x27;t it have the same seismic&#x2F;water table problems? reply katbyte 12 hours agorootparentYep. And it won’t be 100% efficient. The energy to capture and store the co2 is likely greater then the energy you get in hydrogen reply londons_explore 11 hours agorootparentCO2 reduces the viscosity of oil, and helps get more oil out of the oil well.Even ignoring climate benefits, it&#x27;s still worth pumping CO2 down the well for financial reasons alone - especially towards the end of the lifespan of classical oil wells. reply super256 13 hours agoparentprevWhat about using hydrogen in lorries&#x2F;trucks? I see that argument thrown around often in hydrogen vs battery discussions. reply crote 12 hours agorootparentIt&#x27;s an option.But short-range trucks can operate with batteries just fine, and for long-range trucks it might end up being more viable to just construct overhead wires[0] above long-distance highways and avoid the problem altogether.[0]: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=_3P_S7pL7Yg reply theshrike79 12 hours agorootparentprevFor long-haul trucks it&#x27;s a viable option. I think there&#x27;s a large-ish test fleet in Switzerland.But for personal vehicles it&#x27;s 100% dead unless we get some huge advances in the technology. reply asdff 13 hours agoparentprevIf the case for the battery was objectively better, oil companies would pull a phillip morris and buy out their antithesis to profit on both ends. The fact that they are doubling down on hydrogen itself I think its really interesting. To me, this shows that perhaps there is a true advantage, and also a high enough barrier to entry where only these oil companies at the scale they already exist can reap the most benefit. Its hard to know where the truth actually lies due to our propaganda&#x2F;media environment, but I think these actions at least do inform of it a little. reply gmane 13 hours agorootparentLike BP? https:&#x2F;&#x2F;www.nasdaq.com&#x2F;articles&#x2F;bp-joins-forces-with-harmony...or Exxon? https:&#x2F;&#x2F;www.aa.com.tr&#x2F;en&#x2F;p&#x2F;duyurular&#x2F;1002&#x2F;exxonmobil-launche...or Chevron? https:&#x2F;&#x2F;www.msn.com&#x2F;en-us&#x2F;news&#x2F;other&#x2F;ev-charging-startup-ele... reply PaulKeeble 13 hours agorootparentprevHydrogen can be captured out of some of the oil wells they have. Its not about producing it from green energy they have a plan to keep extracting it from the ground. reply timschmidt 13 hours agorootparentprevNissan has famously owned the patent on large format NiCad batteries without producing them or offering them for sale on the open market. So you&#x27;re right. reply pixl97 12 hours agorootparentprev\"If the case for digital cameras was objectively better Kodak would have bought up chip companies and profited on both sides\"..... reply pie420 11 hours agorootparentprev\"If the case for digital video consumption was objectively better, Blockbuster would have bought Netflix to profit on both ends\" reply askvictor 12 hours agoparentprevThe problem is that oil&#x2F;gas companies see their impending demise. They are good at moving flammable things through pipes. Electricity is not that, and if it takes hold, they&#x27;re in for a _major_ pivot. So they do what they&#x27;ve always done. reply xdennis 9 hours agoparentprev> Anyone who has followed hydrogen as a fuel for a while [...]. It really doesn&#x27;t make sense to use renewables to make hydrogen fuel to then burnSorry, you say you&#x27;re familiar with hydrogen fuel cells and you don&#x27;t know it&#x27;s not burning?Hydrogen fuel cells use an electrochemical reaction to turn hydrogen and oxygen from the air into electricity and water. The byproduct is water, not fire.I&#x27;m not rich enough to own either a battery-electric or hydrogen-electric car, but it&#x27;s weird how the only people who hate hydrogen are the battery people. I don&#x27;t hear any hate from fossil-fuel guys.It seems they&#x27;re more interested in protecting their investment than saving the environment through a process (which is more inefficient, but) that doesn&#x27;t require rare earth minerals. reply numbers_guy 12 hours agoparentprevYour comment contains no substance. You are just pushing a conspiracy theory.Hydrogen based fuels make sense on multiple fronts: long term storage (batteries cannot do this), scale (adding more tanks is cheaper than whole battery packs) and energy&#x2F;power densities are still above what the best batteries can provide. reply reducesuffering 13 hours agoparentprevHydrogen was pushed by the Japanese government because BEV&#x27;s aren&#x27;t as ideally suited to the country&#x27;s economy and resources. It&#x27;s not a coincidence the only two companies making hydrogen cars were both the largest Japanese automakers, Toyota and Honda. And both of those models are economic failures atypical of their usual engineering prowess.Toyota Mirai sales have been flatlined at low ~1-2k since they started, for 7 years: https:&#x2F;&#x2F;www.goodcarbadcar.net&#x2F;toyota-mirai-sales-figures-usa...While Tesla immediately had ever-growing sales: https:&#x2F;&#x2F;www.goodcarbadcar.net&#x2F;tesla-us-sales-figures&#x2F; reply Reason077 12 hours agorootparent> \"BEV&#x27;s aren&#x27;t as ideally suited to the country&#x27;s economy and resources.\"BEVs are better suited to Japan&#x27;s economy and resources than any other fuel, the vast majority of which is imported (including natural gas).It&#x27;s just that it&#x27;s taken Japan&#x27;s government a long time to realise this. But it seems like they&#x27;re finally coming around! reply fomine3 9 hours agorootparentBEV is hit and miss for Japan. For vehicle usage, it&#x27;s great that we don&#x27;t need to import oil from mideast (that is a big reason for trade deficit) and of course it&#x27;s great for commute and non-long commercial usage. But it&#x27;s not suitable for a big personal car use case. Japanese family can&#x27;t take holidays as what they want, so they tend to do long trip in the same specific long holidays (new year, Golden week, Obon, Silver week), that needs tremendous amount of fast charging equipment at highway or need to wait multiple hours after BEVs sold well. So theoretically PHEV should be good option but manufacturers hadn&#x27;t sold it much (except Mitsubishi). It&#x27;s hard to install slow charger for mechanical parking system that is common in dense cities.For vehicle manufacturing, there&#x27;s very less advantage to make BEVs in Japan, meanwhile China is too strong. Car manufacturing is a big industry so shrinking it causes Japan economy to be dead. Toyota cares Japan economy (cared in a bad way, hydrogen for car). IMO they should had promoted PHEV more in 2012-2015. reply theshrike79 12 hours agorootparentprevThe Japanese electric grid is even worse than the one in the US.Their grid is a mix of 60Hz and 50Hz networks because of weird historical reasons. It&#x27;s also single-phase, which isn&#x27;t the best for mass EV charging.Compare that to, say, Finland where every single apartment and house has three-phase power. 3x16A or 3x32A usually. It&#x27;s completely trivial to install a 11kW home charger for example, it&#x27;s just running some cables and maybe adding a load balancer - all 100yo tech. reply Reason077 12 hours agorootparentMost Japanese houses do not have parking garages or private parking spaces, so the ability to charge from home is irrelevant.But in any case, single phase is fine for home charging and it&#x27;s the default in many parts of the world. Three phase charging is a nice luxury, but far from a necessity. reply theshrike79 12 hours agorootparentIn many Japanese cities you won&#x27;t get a permit to buy a car unless you can prove you have a place to park it. Curbside parking isn&#x27;t a thing.So in those cases it really is irrelevant whether you can charge or not =) reply jacquesm 13 hours agorootparentprevToyota is said to lose in excess of $100k per hydrogen car. reply ant6n 12 hours agoparentprevThe fdp, a small german party part of government, that holds the traffic ministry, managed to block an EU-wide phase-out of ICE vehicles to promote e-fuels. Whats e-fuels you ask? Its even more ridiculous than hydrogen cars - using renewable energy to synthesize fossil fuels. The beauty is that this makes ICE vehicles supposedly green, so then you can just keep selling them forever. And at some point down the line u just kill the e-fuel requirement. reply dclowd9901 12 hours agoparentprevBatteries go bad over a rather short period of time and are incredibly expensive too. They’re basically currently infeasible as a mode of transport without _a lot_ of trade offs. Compare that to a tank for fuel, which is comparatively minuscule in cost and basically never goes bad. I don’t think electrical is as scalable as liquid fuel and I’m not convinced it ever will be, especially if it’s continued to be made by exotic materials. reply theshrike79 12 hours agorootparentHow long is a \"rather short period of time\"? There are decade old EV batteries still in use and over 80% SOH.And fuel definitely goes bad. It doesn&#x27;t work like in the movies where you can just find a 30 year old car in the desert and grab more fuel for your awesome roadwarrior vehicle. You can use additives to make it last longer and store better, but still.As for \"exotic materials\" we&#x27;re figuring out new battery chemistries at a record pace. Some are better for vehicles, that need a good kWh&#x2F;kg ratio. Others are better for grid storage where kWh&#x2F;€ is a more relevant measurement. reply elihu 11 hours agorootparentI think the point the post you&#x27;re replying to was making is that the gas tank itself doesn&#x27;t usually go bad. It&#x27;s true you can&#x27;t just let gas sit in a tank forever, but with EVs it&#x27;s also true that you can&#x27;t usually just leave a battery to sit unused for years; eventually, it&#x27;ll self-discharge below a threshold where it becomes permanently damaged.I do agree that battery \"problems\" are exaggerated -- most will last a very long time, and batteries that don&#x27;t use cobalt and nickel are available now. Sodium-ion might be relatively common in a few years for low-end applications. reply elihu 11 hours agorootparentprev> \"Batteries go bad over a rather short period of time\"Not really. They do gradually lose capacity over a long time, but it&#x27;s a slow process. The US Government requires EV batteries to have an 8 year &#x2F; 100,000 mile warranty, and California requires 10 years and 150,000 miles. I expect most batteries will significantly exceed that.LFP batteries generally have about 3x the longevity of other lithium ion types.There are some examples of early battery failures, like the Nissan Leaf didn&#x27;t use liquid cooling; now basically everyone else learned from their example and does it. Chevy Bolts had a battery recall due to bad manufacturing. Basically, if a battery fails early someone somewhere screwed up. Most EV batteries are fine and will continue to be fine for a very long time.>> \"I’m not convinced it ever will be, especially if it’s continued to be made by exotic materials.\"Resource constraints are an issue, but there are workarounds. Nickel and cobalt can be avoided simply by using LFP cells. Permanent magnet motors are more efficient than induction, but the permanent magnets don&#x27;t need to be based on rare-earth elements -- it&#x27;s just that the non-rare-earth-magnet version would be less powerful and&#x2F;or bulkier. Similarly you could wind the motors with aluminum instead of copper, and the motor would be less powerful and&#x2F;or bulkier. But that&#x27;s fine -- electric motors are already quite a bit smaller and more powerful than they need to be anyways.Lithium is probably the hardest-to-get-around resource constraint with current technology, but lithium production is ramping up.Also worth noting that the need for large batteries would be substantially reduced if were to invest heavily into charging infrastructure and even the electrification of roads so that cars don&#x27;t even need to stop. (There are some projects in Sweden and Germany along those lines.) reply robertjpayne 12 hours agorootparentprevFuel definitely goes bad, at least once the storage is in vehicles. The petroleum industry keeps peddling all this FUD about how awful batteries are.The difference is batteries can be improved (and there&#x27;s a tooooon of $$$$ pouring into this problem) and they are improving. Petroleum is a dead end while batteries, solar, and electrical grids have a bright future and lost of room for growth. reply elsonrodriguez 11 hours agorootparentprevLiquid fuel is inherently unscalable, it is a finite resource. reply teaearlgraycold 13 hours agoparentprevMy understanding is liquid hydrogen could be a contender for a green jet fuel alternative. Batteries are too heavy. reply jacquesm 12 hours agorootparentI think we&#x27;ll see a synthetic version of jet fuel before we see a hydrogen powered plane in mass production.Far less trouble to store and use and much higher energy density, also works with existing airframes (fuel tanks!) and engines. reply konschubert 12 hours agorootparentPlus it works with the FAA and with the existing expertise in the Airline industry. reply MBCook 12 hours agorootparentprevThe big problem is that fueling with liquid hydrogen is not easy. Getting things down to that temperature is not easy. Getting hydrogen cleanly is not easy.I mean if hydrogen was just as abundant in the atmosphere as oxygen and didn’t need to be at ridiculous temperatures to be liquid, it would be a great fuel.But to keep things green you’re fighting a ton of trade-offs using electricity, it which point why don’t you just use the electricity directly? reply konschubert 12 hours agorootparentprevMy understanding of the airline industry is that it will be MUCH easier to further synthesise that green hydrogen into proper jet fuel than to build and certify a jet that runs on Hydrogen. reply idontpost 13 hours agoparentprev> My gut is that the majority of hydrogen proponents are oil and gas lobbyists or funded by themI don&#x27;t understand how any one looks at the electric car charging situation and thinks it makes sense to scale that out to 100% of vehicles.A 2 hour wait to get a charger spot isn&#x27;t a workable transport system. reply theshrike79 12 hours agorootparentHave you seen the queues for H2 stations? 2 hours is peanuts.Also: the longest wait I&#x27;ve had in the almost 3 years I&#x27;ve owned an EV has been about 15 minutes and in that case it was a couple who had clearly rented an EV and was charging it for the first time ever.The infrastructure is perfectly solvable. The US is just at a disadvantage because gasoline is practically free over there compared to the rest of the world so there isn&#x27;t that much of an incentive to improve the charging infrastructure.As a reference, currently gasoline at the station near to me is $7.47&#x2F;gallon, which actually pretty cheap. The most expensive price here is $9.24&#x2F;gallon at the moment.Compare that to electricity, which costs 0.02c&#x2F;kWh right now. That&#x27;s 0,0002€&#x2F;kWh. Looking at the market price it&#x27;ll dip down to -0.18c&#x2F;kWh in the morning hours, they&#x27;re literally paying people to use electricity =) reply jsight 12 hours agorootparentEven in the US it is mostly a solved problem for Tesla owners. There are a few places that are consistently problematic (eg, Las Vegas), but there are new stalls planned there to help.Several companies are in the early stages of national DCFC rollouts. It will get better quickly in a lot of places. reply wongarsu 12 hours agorootparentprevThe envisioned car charging situation is that you charge your car at home or at work. Cars sit unused 99% of the time, they can use that time charging without issues.What we are experiencing now is transitional, where people use infrastructure intended for road-trips for their daily needs. That phase will pass reply theshrike79 12 hours agorootparentExactly.I charge at my parking spot with around 2-2.5kW power use. With that I&#x27;ll get about 100-150km of range overnight easily. I drive less than that daily, which means my car&#x27;s battery is full practically all the time. reply marssaxman 12 hours agorootparentprevThe future is public transport. reply jryle70 7 hours agorootparentDid you know that car ownership has kept increasing in the Netherlands, at least until a few years ago? The country renown for cycling and public transportation.From 197 cars per 1000 people in 1970 we&#x27;ve grown to 662 in 2019. i.e. we now have more than three times the number of cars per person even compared with the \"bad old days\" of the car dominated 1970s ... The simple fact is that Dutch car use has grown continuously for 70 years much as it has in every other nationI love train, but let&#x27;s not pretend that public transportation will be dominant. Rather, it will be a mix of different modes of transport. Cars will continue to be important. Visit the Netherlands and see for yourself.https:&#x2F;&#x2F;www.aviewfromthecyclepath.com&#x2F;2019&#x2F;08&#x2F;the-car-free-m... reply troupe 12 hours agorootparentprevThe decline in passenger trains seems to indicate that future is headed in the other direction. reply bryanlarsen 12 hours agorootparentprev> 2 hour wait to get a charger spot isn&#x27;t a workable transport system.That rules out widespread use of hydrogen, then. Hydrogen filling stations need to periodically recompress their hydrogen. Which is never a problem because current hydrogen stations are virtually always idle. However, if they were continuously busy, you&#x27;d have to wait. reply jsight 12 hours agorootparentprevI don&#x27;t see why you&#x27;d look at a 2 hour wait somewhere and extrapolate that out as an impossible problem. Show an EV nerd a video of a 2 hour wait and they can probably tell you where it happened and maybe even when, because that is really unusual.This is a solvable problem. reply forgotusername6 12 hours agorootparentprevAlmost all charging is done at home. reply otabdeveloper4 13 hours agoparentprevPutting two tons of highly toxic and flammable lithium into a car-shaped object just to commute one person a few dozen miles is quite literally insane from all practical points of view.Our ancestors will never believe that we were nuts enough to not only allow this, but actually legally manndate it. They will think it&#x27;s an urban legend and no ancient people could have been so short-sighted and egotistical. reply oblio 12 hours agorootparent> Putting two tons of highly toxic and flammable lithium into a car-shaped object just to commute one person a few dozen miles is quite literally insane from all practical points of view.And ICEs are powered by many thousands of mini explosions per minute with a side effect of producing an odorless gas that can kill us silently.Everything can be scaremongered.And personal transportation, as much as I wish wasn&#x27;t the case, is a fact of life since 1910. reply tremon 12 hours agorootparentWhat happened in 1910? The first automobile dates from 1886, and I&#x27;m pretty sure the horse&buggy personal transportation method predates it by several centuries. reply parker_mountain 12 hours agorootparentprevFun fact: per mile driven, there are about 1&#x2F;10th to 1&#x2F;5th as many EV fires, as there are gas car fires.Putting ten gallons of highly toxic and flammable fuel into a car-shaped object just to commute one person a few dozen miles is quite literally insane from all practical points of view. Electric tech is marginally less crazy, and also a step forward. reply r87 12 hours agorootparentI’d expect that, but would have assumed it is due to there being more old gas vehicles on the road. Is that data normalized for age of the cars driven?I assume that my neighbors 30 year old gas guzzler is more likely to catch fire than my 1 year old PHEV for example (or maybe I’m more likely since I have both a lithium battery and a gas tank). reply duskwuff 12 hours agorootparentprev> Putting ten gallons of highly toxic and flammable fuel into a car-shaped object just to commute one person a few dozen miles is quite literally insane from all practical points of view.And there&#x27;s another 10-20 gallons of various oils in the engine and transmission. While they aren&#x27;t nearly as volatile as the gasoline, they&#x27;re still quite flammable. Really, it&#x27;s a wonder these things were ever allowed. :) reply giardini 11 hours agorootparentduskwuff says: >\"And there&#x27;s another 10-20 gallons of various oils in the engine and transmission. While they aren&#x27;t nearly as volatile as the gasoline, they&#x27;re still quite flammable\"\"they&#x27;re still quite flammable.\" quite literally insane from all practical points of viewErm. I don&#x27;t think that&#x27;s true. reply rootusrootus 12 hours agorootparentprev> Putting two tons of highly toxic and flammable lithiumMaybe cut back on the hyperbole a bit. Two tons is the entire car. The lithium content of the battery is 15-20 pounds. And it&#x27;s not straight lithium metal that becomes flammable in the presence of water. It&#x27;s the electrolytes that are flammable. And most EVs in the world use LFP, which doesn&#x27;t have that flammability problem. reply elihu 11 hours agorootparentLFPs are flammable too, they just don&#x27;t tend to ignite as easily. I suspect they&#x27;re probably easier to put out, but I don&#x27;t know for sure.(I&#x27;d be curious what the real-life vehicle fire statistics actually are so far for regular lithium ion versus LFP. LFP is promoted as being safer, but at this point there should be enough vehicle fire statistics to be able to quantify how much safer they are. The Tesla Model 3 might be a good model to compare because it&#x27;s available in both versions, though maybe there just aren&#x27;t enough model 3 fires to be statistically significant.) reply rootusrootus 10 hours agorootparentThe FAA has studied this. LFPs are an order of magnitude less prone to fire, and less intense when they do. There are some fun videos out there put together by fire departments testing various battery chemistries. It&#x27;s fairly easy to make a regular modern lithium ion battery erupt in flame. It&#x27;s hard to get LFP to do much more than release the magic smoke. And LTO batteries, which are another variety, don&#x27;t even smoke.EV fires are already significantly less common than ICEV fires. Factor in LFP, and maybe we should start kicking ICEVs out of garages because they&#x27;re unsafe. And lest you think ICEVs don&#x27;t spontaneously combust -- that&#x27;s very much incorrect, there have been recent recalls of millions of ICEVs for exactly that. reply elihu 7 hours agorootparentI was aware that internal combustion cars catch fire more often. There&#x27;s a bit of a compensatory difference in severity: EVs don&#x27;t catch fire as often, but it&#x27;s especially bad when they do. But hopefully we can quickly move past the era of super-flammable EV batteries even if actual EV fires are rare.I look forward to kicking fossil fuel based cars out of everywhere for reasons unrelated to safety. Or you could say reasons that have to do with the collective safety of all life on Earth rather than individual safety. reply otabdeveloper4 5 hours agorootparentThe negative externalities of the large-scale strip-mining and fossil-fuel-burning effort to convert all cars to electric will not improve the collective safety of life on Earth.Also, remember that lithium is also a fossil fuel. Electicity is not magic, you&#x27;re just replacing one fossil fuel with another. replyxtorol 12 hours agorootparentprevAny mechanism that can store enough energy to get a car from point A to point B is going to store a nontrivial amount of energy and be subject to some eyebrow-raising failure modes. This applies to gasoline, diesel fuel, batteries, and all flavors of hydrogen. reply jackvalentine 12 hours agorootparentprevThe insane bit when you look at it logically is the car shaped object part - we drive around in and store portable living rooms. reply otabdeveloper4 5 hours agorootparentYes, but also the moral crusade to replace \"shitty way to live and move around\" with \"equally shitty way to live and move around, but with a different set of externalities\" is equally insane. reply wongarsu 12 hours agorootparentprevWe currently put toxic and highly flammable gasoline into car-shaped objects. The car needs to carry its energy with it somehow. Energetic things tend to be flammable and toxic, since those are just uncontrolled ways to release energy reply _gtly 12 hours agorootparentprevA canister of hydrogen in your car isn&#x27;t exactly safe, either!! reply MattPalmer1086 12 hours agorootparentprevOr possibly our descendants ;) reply tim333 16 minutes agoprevWhile I agree using clean electricity to make hydrogen to power things seems a bad solution compared to batteries there looks to be quite a lot of potential for \"white hydrogen\" which is basically drilling for naturally occuring hydrogen in the earths crust.There seems to be a lot of it eg \"huge reserves found in France\". Potentially 46 million tonnes in Lorraine, 1 euro a kilo to produce https:&#x2F;&#x2F;www.euractiv.com&#x2F;section&#x2F;energy-environment&#x2F;news&#x2F;exc...And elsewhere in the crust. It&#x27;s just people haven&#x27;t really focused on drilling for it until recently https:&#x2F;&#x2F;archive.ph&#x2F;RbQwvIt might be a practical solution to use our existing equipment which is basically the same as drilling for natural gas, but with no or at least less CO2. reply cracrecry 12 hours agoprevThere is an Oil Sector Lobby and there is an electric sector Lobby too. All Lobbies hate competition.The electric sector wants to destroy Hydrogen because it competes with them, of course, they want to destroy the seed so no tree comes out of it. Hydrogen disappearing is not going to happen, because it going to be necessary for some things, like heavy transportation.Just because clean Hydrogen is in the early stages in development(like EV were 15 years ago) does not mean that we should not develop the technology.I personally believe that NH3 is great for storing energy and energy transportation, and extremely useful for things like growing food.It is also toxic and difficult to handle, like oil, but in lots of sectors(like heavy industries) it makes lots of sense. In other sectors, electricity and batteries make more sense. Different technologies will coexist together and compete. reply downrightmike 12 hours agoparentMost of the hydrogen made today is made from fossil fuels, so they of course love that idea. reply justapassenger 12 hours agorootparentSo is most of electricity. None of that matters for the future. reply adgjlsfhk1 12 hours agorootparentThis is technically true, but only barely. In the US in 2022, renewables were 21.5% and Nuclear was 18.2% of total generated power (yes nuclear should be counted among renewables, oh well). That means nearly half of power generated is green compared to ~1% of hydrogen. reply meindnoch 12 hours agoparentprev>It is also toxic and difficult to handle, like oilExcept oil is a liquid, while ammonia is a gas. You can put oil in an open container and it won&#x27;t hurt anyone. Now try doing the same with ammonia... reply dexwiz 11 hours agorootparentAmmonia is more comparable to natural gas or propane than oil.Also crude oil is really toxic. It off gasses all sorts of questionable things, and touching it isn&#x27;t recommended. I get what you mean by your comment, but oil isn&#x27;t some super stable, safe material. reply cracrecry 12 hours agorootparentprevOil burns and CH3 explodes easily, but we use it anyway because it makes economic sense.The same will happen with ammonia. There are risks, but the energy that you could store is immense at a very low price.Don&#x27;t get me wrong, I was working in batteries and solar panels research in the early days. I heard the same arguments against it( why we should spend money when batteries and solar panels are so expensive and work so badly? They were expensive and worked badly)Now we work on fuel cells. They are expensive and there is room for improvement. reply rasz 8 hours agoparentprevWhat are you talking about? Mazda had hydrogen concept car in 1991, Toyota started selling hydrogen cars in 1996 before EV, Honda in 1999. reply hn_throwaway_99 13 hours agoprevNot saying the oil sector is powerless by any means, and I guess maybe they&#x27;re going for regulatory capture, but this is a non-starter from an economic reality perspective so I&#x27;m not too worried about it. The train is well out of the station for electric vehicles. I believe every car maker (save 1 that I know about) is fully committed to 100% electrified vehicles in the not-too-distant future. I feel like eventually even Toyota will give up their weird H2 push. reply JumpCrisscross 12 hours agoparent> this is a non-starter from an economic reality perspective so I&#x27;m not too worried about itStill dangerous. It reminds me of Zubrin’s takedown of VASIMR [1]. An unrealistic counterproposal can torpedo a workable one by appealing to peoples’ tendency to choose the perfect over the good.[1] https:&#x2F;&#x2F;youtu.be&#x2F;myYs4DCCZts?si=Ksqw_MFLKHKIgMod reply asdff 13 hours agoparentprevAll it takes is a new pump at existing gas stations and what becomes more compelling to the consumer flips overnight. Same cultural process of fueling up at the same place you&#x27;ve always fueled up in about the same amount of time as its always taken. It seems like there&#x27;s a lot lot less inertia to get consumers over that hurdle than the EV hurdle, where charge times are long and convenient chargers not a guarantee in comparison to the fuel station network. reply rootusrootus 13 hours agorootparent> All it takes is a new pump at existing gas stations\"All it takes\" is doing very heavy lifting in that sentence. There&#x27;s a good reason that hydrogen fueling stations are basically non-existent aside from a few in California. Distribution and fueling is quite complicated compared to conventional liquid fuels. reply yread 12 hours agorootparentthere are 86 now in Germany.https:&#x2F;&#x2F;h2.live&#x2F;en&#x2F;basically most of the main highways are covered reply smileysteve 13 hours agorootparentprevLol at all; Hydrogen can&#x27;t easily be transported in current pipelines; Can&#x27;t be transported in current trucks, and can&#x27;t be stored in current gas station tanks.And if the gas station doesn&#x27;t maintain its hoses, nozzles, or seals; you don&#x27;t wind up with a slight smell, a carcinogenic irritation -- you experience frostbite to you hand immediately. reply jacquesm 13 hours agorootparent> you don&#x27;t wind up with a slight smell, a carcinogenic irritation -- you experience frostbite to you hand immediately.If you&#x27;re lucky. If you&#x27;re unlucky the first you realize there is a leak is when it ignites. Fortunately it is harder to make it explode than gasoline vapors but it also escapes far more readily than gasoline does. reply smileysteve 11 hours agorootparentprevThough the maintenance issue can be solved with a nitrogen pressure test before - the equivalent of electrical connectors verifying the connection digitally before sending the amps. reply abraae 12 hours agorootparentprev> Same cultural process of fueling up at the same place you&#x27;ve always fueled up in about the same amount of time as its always taken.I don&#x27;t know that people&#x27;s cultural attraction to fueling at the gas station is high at all.After we got an EV I realised how much I had actually disliked filling the car. Smelly, every time an in your face reminder of the fossil fuels you are burning. Having to go in an queue. Having to wait and jostle for a spot with the filler hoses on the correct side for my vehicle. Dodgy characters milling around. Unable to use cellphone and take calls. Possibility of being involved in holdups&#x2F;antisocial behaviour. reply serial_dev 12 hours agorootparentIsn&#x27;t the long charging time an issue? After a quick search, Tesla needs 30 minutes to charge to 80% with superchargers and this is good for 300 kms. Assuming 120 kmh, this means that about every 2.5 hrs, I need to take a 30 min break.With the old system, I fuel in 2 minutes and I can travel 600 kms on it or approx 6 hrs (assuming 50 litres fuel tank and 7.5 litres per 100 km, and 120 km&#x2F;h).(I don&#x27;t have a car, and I&#x27;m open minded about EVs, just honestly wondering) reply nicpottier 11 hours agorootparentThey aren&#x27;t as good at long distances, you have that part right. Your numbers could be debated but are close enough. Most people don&#x27;t drive long distances on the regular though.They are MUCH better for day to day if you can charge at your home. You always start the day with a full battery. I only use a supercharger on long trips. Note that I actually drive farther than most for various reasons but that route has a supercharger at a grocery store. I tend to come back at the end of the weekend and do my shopping at the same time, so in reality I don&#x27;t spend any time refueling.So total yearly time where you are actively \"fueling\" your vehicle in a supervised manner favors EVs for the vast majority of people. reply abraae 11 hours agorootparentprevIt&#x27;s an entirely different paradigm.With an ICE, you can only fuel up at a gas station, but it is fast (not always though, queues, etc.).With an EV you charge overnight at home, and only charge on the road as required, and it can be slow (not all that slow though, not a big problem for us). Depending on your usage and vehicle range, you may never need to charge on the road at all.I wouldn&#x27;t personally want to own an EV if we couldn&#x27;t charge at home. reply ClumsyPilot 11 hours agorootparentprevIf your main usecase is using the car in the city, which it is for my family for example, then you go for months without seeing a fuel station of any sort. reply p1mrx 12 hours agorootparentprev> Unable to use cellphone and take calls.Why are you unable to use your cellphone? reply zo1 11 hours agorootparentThey&#x27;re apparently \"banned\" or \"dangerous\" at fuel filling stations. Whether that&#x27;s actually true is a different matter, but all the stations I&#x27;ve been to have had prominent \"cellphones not allowed\" signs displayed. reply hn_throwaway_99 8 hours agorootparentWhat country is this? I&#x27;ve never seen this in the US. reply abraae 8 hours agorootparentNew Zealand. Seems to be a blanket rule here. replythrowaway5959 13 hours agorootparentprevHave you ever read up on what storing hydrogen entails? Both at the station and in the cars? The tanks are totally different beasts than gasoline. reply jacquesm 13 hours agorootparentprevNot even close. It requires a whole new delivery network, trucks, pipelines need to be re-sleeved and tons of safety measures. Switching to hydrogen from diesel or gasoline is a non-trivial exercise. And large scale storage is yet another problem. That&#x27;s hard enough with LNG, LPG, diesel and gasoline, with hydrogen it is much more complex and carries more risk. reply alerighi 12 hours agorootparentprevI have a diesel car. Every 3 weeks I have to fill my tank. This wastes me around 5&#x2F;10 minutes, depending if the gas station is full of person that want to fill their tank. And this only thanks to the fact that I&#x27;ve one of the gas station with the cheapest price of my city 50 meters from where I work. Otherwise I would need to spend time (and gas) to get to the station with the cheapest price, because paying 2 euros for a liter of gas is not something I like to do.With an electric car I can get home at evening, park my car and connect a plug, exactly as I connect my phone to the charger before going to bed. Time wasted? Probably less than a minute every couple of days. Also, I pay it in my electricity bill, the price of electricity doesn&#x27;t go as high as the price of gas. Also... it&#x27;s impossible to distinguish between the electricity used to charge a car than the one used for domestic purpose. This means that the government cannot add taxes on electricity used in vehicles, as it does with gas. Finally there is the possibility to install solar and produce the electricity myself.> All it takes is a new pump at existing gas stationsEasy? We have natural gas cars since decades. Only recently (last 10 years) you find a sufficient number of stations that serve you natural gas. Still for nonsense rules in my country you can&#x27;t fill your natural gas car by yourself but it has to be done by an operator, so good luck filling it in the weekend or at night. I imagine the same rules would apply for hydrogen.Now, after decades of natural gas cars, they are mostly usable (my parents have one). Inconvenient, since you maybe have to drive 10km to get to a station, and you have the same range than an electric car (350km more or less). Will hydrogen be that better? I don&#x27;t think so.While all of this happens, electric car chargers are starting to appear everywhere. You go to a supermarket, there is a charger, you stop to eat on the highway, there is a charger, or more of them really, in the cities there are plenty of them (plus while the car is charging you don&#x27;t pay for parking!).It&#x27;s nonsense to this day to not consider buying an electric car if buying a new car to me! I keep my diesel car since I already have it, it has more than 10 years, and works fairly well, and I don&#x27;t like changing things that works, but when I will change it, I will buy an electric for sure. reply zolbrek 12 hours agorootparent>This means that the government cannot add taxes on electricity used in vehicles, as it does with gas.You don&#x27;t think you&#x27;ll be paying a significant electricity tax once ICE cars are fully phased out?Also, I don&#x27;t know what you drive but my diesel has quite a bit more range than 350 km. Yours must be quite the guzzler. reply PaulDavisThe1st 10 hours agorootparentNo such tax can be effectively charged as part of an electricity bill, at least not for areas with enough insolation to have encouraged people to have their own PV arrays for charging.Some sort of tax will be necessary to replace the fuel tax, but I&#x27;d be very surprisedby some attempt at a general electricity tax, or for that matter, a charging-station tax (since that won&#x27;t affect the majority of EVs). reply hn_throwaway_99 11 hours agorootparentprevYeah, Texas already charges an additional $400 for EV registration and then $200 annually. reply MBCook 12 hours agoparentprevWould gasoline be anywhere near something people wanted to use if it was taxed to correctly account for its externalities?Petroleum is a huge industry and they could make a big problem by getting subsidies to make some other fuel look competitive even though it’s a disaster.Look at biodiesel. reply hn_throwaway_99 11 hours agorootparent> if it was taxed to correctly account for its externalities?I think you kind of identified the issue there, though. Externalities are notoriously difficult to tax and account for. The issue with EVs is that they are intrinsically better than hydrogen in nearly every way. Even issues with long distance driving and recharging are quickly improving with EVs, and the improvements are happening much faster than anything going on with hydrogen. reply tw04 12 hours agoparentprevIt&#x27;s not just Toyota - it does seem to be a Japanese thing. Honda is also actively producing H2 powered vehicles.https:&#x2F;&#x2F;www.caranddriver.com&#x2F;news&#x2F;a42796089&#x2F;2024-honda-cr-v-... reply ClumsyPilot 11 hours agoparentprev> The train is well out of the station for electric vehicles.British government has just postponed it&#x27;s EV vehicle transition by 5 years, British tabloids are printing articles about how EVs will never work for British uniquely harsh weather reply drewg123 12 hours agoprevThe YouTube channel Engineering Explained has an excellent video regarding why hydrogen is not a practical replacement for gasoline&#x2F;petrol&#x2F;diesel in a traditional internal combustion engine:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=vJjKwSF9gT8 reply jacquesm 13 hours agoprevI wish them much good luck, but it&#x27;s a bit like the dinosaurs complaining about those uppity mammals getting underfoot: their days are numbered. BEVs are here to stay, hydrogen is - as far as I can see - a non-starter except when it comes to heavy traffic and possibly rail or air but even there I have my doubts.Meanwhile BEVs are rapidly building momentum and coupled with solar and wind make very good sense. Even the last holdout for a passenger hydrogen vehicle (Toyota) has finally seen the light and is making BEVs now, I don&#x27;t remember ever seeing a Mirai but given how many have been produced that&#x27;s not so strange.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Toyota_Mirai reply semi-extrinsic 13 hours agoparentI&#x27;m not so sure that BEVs will reach anything close to 100% of all cars on the road. My primary reason is this: as long as you \"only\" have 10%, 20%, even 30% BEVs the electric charging infrastructure is mostly free, we already have it. But after that, BEVs will require expensive infrastructure to be built in the electricity transmission grid. Think \"we need to dig up all of the roads\". Because it&#x27;s not just cars going electric, it&#x27;s everything else as well at the same time.At that point hydrogen cars could very well see a resurgence. Especially in places with cold weather, where the fuel cell waste heat can be used for the climate control, while the BEV needs to spend energy running a heat pump, so the efficiency advantage of BEVs is mostly gone. reply pornel 11 hours agorootparentNo, this has been debunked over and over.People imagine BEVs need to be charged every evening at some super high power. The reality is they need to be charged about once a week (median US commute is 1&#x2F;10th of typical BEVs range at city speeds), and can charge overnight, or when parked at work, while using less electricity than your air conditioning unit.The grid is designed to handle peak demand (see the \"duck curve\"), and there is a lot of spare capacity outside of the peak hours, especially at noon when there&#x27;s more solar production than consumption.BEVs are very flexible with their charging time, and scheduling charging in off-peak hours is a standard feature. reply abraae 12 hours agorootparentprevThe counter to your \"think of the grid\" argument is that the fossil fuel network may start to atrophy once there are significantly less people fuelling their gas vehicles.If filling your gas car involves an unplesant trip to a peeling, cracking and unreliable gas station, then that&#x27;s one more incentive to go BEV. reply jacquesm 12 hours agorootparentprevYes, that&#x27;s a good point. But: the grid can be expanded and probably will be expanded. I don&#x27;t have a BEV but I&#x27;ve run the numbers and it would work very well for me today, it&#x27;s just that I have an old, non-connected and fairly recently rebuilt car that I&#x27;ll drive into the ground. But if I were younger and still in the market for a new vehicle I would definitely consider an electric one, though I&#x27;d make sure it had LiFePO4 or maybe even LTO in it (not yet available). Battery tech is getting better rapidly now that sufficient funds is pumped into it and I would expect the next two generations to be major improvements over what&#x27;s on the market today. reply robertjpayne 12 hours agorootparentThis 100%, we&#x27;re not done with batteries yet as a society and it&#x27;s probably one of the most heavily funded industries because the opportunity to strike \"gold\" with good battery tech is too large to ignore.Lithium wont be used forever and and downsides to batteries will slowly be eroded until we wonder why we ever used petrol in the first place.As far as the grid goes, in my eyes rooftop solar is going to provide a huge amount of the power generation required for EVs. We&#x27;re not there yet but the price of solar keeps dropping rapidly and you&#x27;ll see a shift in messaging from \"discounts and charge overnight when demand is low\" to \"discounts and charge during the day because generation is high\". reply jacquesm 11 hours agorootparentI could already supply an EV with all the power required for my trips inside the country. 7700 KWh surplus from solar this year and we&#x27;re not done yet. That&#x27;s a multiple of what an EV would use for my current mileage (which is a fraction of what it was pre-COVID, and to be fair I also use my (e-)bikes a lot more than before). replyReason077 12 hours agoprevArticle is from [2021]. But it might as well be from 2015 for all the relevance it has to today&#x27;s automotive industry. Electric already won. reply g-b-r 13 hours agoprevThey&#x27;ve been doing it for twenty years, hydrogen allows maintaining a distribution network and *most of all* to keep using fossil fuels to generate it reply ska 13 hours agoparentMuch longer than 20 years. But the reason it hasn&#x27;t taken off is that both the economics and the physics don&#x27;t work very well for consumer vehicles. reply Smoosh 12 hours agoparentprevI think that it is just as important for the oil industry to make it look like alternatives are being tried, but they just don&#x27;t work out, so that the status quo seems more acceptable. reply parineum 11 hours agoparentprevThis is true and is contrary to this headline&#x2F;quote. The oil sector is lobby for hydrogen but the attribution of motive is off. They are lobbying _for_ hydrogen, not against EVs. reply g-b-r 10 hours agorootparentEVs happen to not need their oil, I have the feeling that they have and are still lobbying the sh*t against EVs; lobbying for hydrogen is one way to lobby against EVs, and if hydrogen were (or had been) successful it&#x27;s potentially infinitely more profitable than EVs for oil companies. reply 0wis 12 hours agoprevMy view on hydrogen is that we will look at it like pressurized air in the 1900’s. Some industrial applications exists but its not the silver bullet many articles are talking about. It is inefficient and we shouldn’t spend many ressources on it. However, I am happy some people experiment with it, we may discover an innovative application that may be a true breakthrough. But I won’t bet on it. reply mem0r1 12 hours agoprevThe argumentation is flawed.1. Just imagine the required charging infrastructure if all vehicles were suddenly battery powered. Charging electric vehicles requires quite some power. Can the power grid installations sustain that (power lines, Transformer stations...) ? What resources (financial and physical (e.g. Cooper)) are required in order to adapt the infrastructure ? Just an example: A supercharger station which can charge a few cars at \"full speed\" simultaneously can draw > 1 Megawatt. Furthermore, superchargers are quite complex (and expensive) technology;2. Batteries are not a suitable large scale energy storage. However, non-dispatchable, fluctuating energy sources such as solar and wind power require huge amounts of storage in order to sustain the power demand. reply re 12 hours agoparent> The argumentation is flawed.Which argumentation are you referring to? You&#x27;re re-iterating an anti-electric talking point that is explicitly debunked in the article.> “That&#x27;s nonsense,” says Liebreich. “[In] 1995, [people said] ‘we&#x27;ll never use the internet because there are not enough modems’. [In] 2000, ‘we&#x27;ll never do online video because there isn&#x27;t enough bandwidth’, then, ‘you can&#x27;t do multiple streams of video because you will never get fibre to the home’. We’ve got 30 years between now and 2050 [when countries plan to reach net-zero emissions] and we will simply have more and more investment. We’ve dug up the streets for cable, phone, gas, cable, fibre, electricity. It&#x27;s a thing we do. We know how to just build slowly over time. This is not rocket science.> “Plus, there&#x27;s smart charging. And of course, we know we&#x27;re going to be doing this because we&#x27;re also going to be having to add capacity because of electric heating. And so the idea that you&#x27;ll say, ‘no, no, we mustn&#x27;t do that extension of existing infrastructure, we must build a completely new one [for hydrogen refuelling], it&#x27;s nonsense, frankly.” reply t3rabytes 12 hours agoparentprevRe 1: The US built 1,000,000 new homes last year without anyone saying \"how we the country cope with this increase in need for wood&#x2F;etc\". Most new homes will have multiple 50A circuits to power an electric range and&#x2F;or HVAC system. Most level 2 electric car chargers max at 50-75A, and can be configured to only charge during times of low grid power demand. Every electric car in existence won&#x27;t be DC fast charging all the time. In fact, I&#x27;d wager for most EV owners in the US today, DC fast charging is a very very low percent of their total charge allocation (between DC fast and AC level 2). reply toomuchtodo 12 hours agoparentprevhttps:&#x2F;&#x2F;advocacy.consumerreports.org&#x2F;research&#x2F;blog-can-the-g... (\"A question that frequently comes up when discussing electric vehicles (EVs) is: “Can the grid handle it?” The short answer is “yes.”\") [Blog post demonstrates the math showing the grid can handle 100% EVs]Superchargers are primarily for road trips and people who need to fast charge because they don’t have home charging (for now; infra is rolling out very fast). Most will charge at home, work, or other locations that perhaps have a level 2 charger (vs a fast DC charger). A 120V 15amp outlet is sufficient to fully charge your vehicle if left for 2-3 days or more at an airport or other longer dwell location.> Batteries are not a suitable large scale energy storage.They will get us most of the way to success. Some combination of seasonal storage, renewables overbuilding, transmission, and limited fossil generation (peakers, cogeneration, etc) will be needed to get close to 100% net zero.https:&#x2F;&#x2F;www.tesla.com&#x2F;ns_videos&#x2F;Tesla-Master-Plan-Part-3.pdfhttps:&#x2F;&#x2F;www.energy-storage.news&#x2F;nrel-rapid-growth-of-energy-...https:&#x2F;&#x2F;www.nrel.gov&#x2F;docs&#x2F;fy22osti&#x2F;81779.pdfEnough sunlight falls on the Earth every ~2 minutes to power humanity for a year. We&#x27;re just arguing shuffling the electrons around. reply jcfrei 12 hours agoparentprevRe 1: Charging mostly happens overnight where there isn&#x27;t a lot of demand. Smart meters can distribute the demand to spread it out optimally. BEVs are actually very valuable for the electrical grid when they can feed their power back into it - further balancing supply and demand.Re 2:",
    "originSummary": [
      "Michael Liebreich, a recognized analyst, suggests that the oil sector promotes hydrogen fuel-cell cars to delay the electrification of cars, arguing they are inefficient and costly compared to electric solutions.",
      "Liebreich has developed the \"Hydrogen Ladder,\" positioning cars and domestic heating at the bottom as uncompetitive use-cases for hydrogen, asserting that companies might be promoting hydrogen to slow down the shift to electrification.",
      "He opposes the need for hydrogen cars, stating that electric vehicles already excel in efficiency, performance, and convenience, and disagrees with the application of hydrogen for domestic heating due to inefficiencies and safety concerns."
    ],
    "commentSummary": [
      "The central debate revolves around the use and efficiency of hydrogen as a fuel source versus the practicality of electric vehicles (EVs). Hydrogen's backing by the oil industry is scrutinized, as are its applications beyond transportation, like long-haul flights.",
      "Broad topics include the carbon-neutral synthesis of hydrocarbons, challenges of cost and efficiency against batteries, and hydrogen's potential benefits within certain industries. Japan's investment in hydrogen as a petroleum alternative is mentioned, along with the storage abilities of batteries and hydrogen.",
      "Detailed discussions cover the energy density of hydrogen for air travel, EVs' weight and concern for road damage, the lifespan of EV batteries, and the scalability and efficiency of hydrogen as an alternative fuel. The limitations and advancements of EVs, as well as the future of transportation, are also explored."
    ],
    "points": 345,
    "commentCount": 319,
    "retryCount": 0,
    "time": 1696965437
  },
  {
    "id": 37830987,
    "title": "The novel HTTP/2 'Rapid Reset' DDoS attack",
    "originLink": "https://cloud.google.com/blog/products/identity-security/how-it-works-the-novel-http2-rapid-reset-ddos-attack",
    "originBody": "Jump to Content Cloud Blog Solutions & technology Ecosystem Developers & Practitioners Transform with Google Cloud Contact sales Get started for free Security & Identity How it works: The novel HTTP/2 ‘Rapid Reset’ DDoS attack October 10, 2023 Juho Snellman Staff Software Engineer Daniele Iamartino Staff Site Reliability Engineer Hear monthly from our Cloud CISO in your inbox Get the latest on security from Cloud CISO Phil Venables. Subscribe A number of Google services and Cloud customers have been targeted with a novel HTTP/2-based DDoS attack which peaked in August. These attacks were significantly larger than any previously-reported Layer 7 attacks, with the largest attack surpassing 398 million requests per second. The attacks were largely stopped at the edge of our network by Google's global load balancing infrastructure and did not lead to any outages. While the impact was minimal, Google's DDoS Response Team reviewed the attacks and added additional protections to further mitigate similar attacks. In addition to Google's internal response, we helped lead a coordinated disclosure process with industry partners to address the new HTTP/2 vector across the ecosystem. Hear monthly from our Cloud CISO in your inbox Get security updates, musings, and more from Google Cloud CISO Phil Venables direct to your inbox every month. Subscribe today Below, we explain the predominant methodology for Layer 7 attacks over the last few years, what changed in these new attacks to make them so much larger, and the mitigation strategies we believe are effective against this attack type. This article is written from the perspective of a reverse proxy architecture, where the HTTP request is terminated by a reverse proxy that forwards requests to other services. The same concepts apply to HTTP servers that are integrated into the application server, but with slightly different considerations which potentially lead to different mitigation strategies. A primer on HTTP/2 for DDoS Since late 2021, the majority of Layer 7 DDoS attacks we've observed across Google first-party services and Google Cloud projects protected by Cloud Armor have been based on HTTP/2, both by number of attacks and by peak request rates. A primary design goal of HTTP/2 was efficiency, and unfortunately the features that make HTTP/2 more efficient for legitimate clients can also be used to make DDoS attacks more efficient. Stream multiplexing HTTP/2 uses \"streams\", bidirectional abstractions used to transmit various messages, or \"frames\", between the endpoints. “Stream multiplexing” is the core HTTP/2 feature which allows higher utilization of each TCP connection. Streams are multiplexed in a way that can be tracked by both sides of the connection while only using one Layer 4 connection. Stream multiplexing enables clients to have multiple in-flight requests without managing multiple individual connections. One of the main constraints when mounting a Layer 7 DoS attack is the number of concurrent transport connections. Each connection carries a cost, including operating system memory for socket records and buffers, CPU time for the TLS handshake, as well as each connection needing a unique four-tuple, the IP address and port pair for each side of the connection, constraining the number of concurrent connections between two IP addresses. In HTTP/1.1, each request is processed serially. The server will read a request, process it, write a response, and only then read and process the next request. In practice, this means that the rate of requests that can be sent over a single connection is one request per round trip, where a round trip includes the network latency, proxy processing time and backend request processing time. While HTTP/1.1 pipelining is available in some clients and servers to increase a connection's throughput, it is not prevalent amongst legitimate clients. With HTTP/2, the client can open multiple concurrent streams on a single TCP connection, each stream corresponding to one HTTP request. The maximum number of concurrent open streams is, in theory, controllable by the server, but in practice clients may open 100 streams per request and the servers process these requests in parallel. It’s important to note that server limits can not be unilaterally adjusted. For example, the client can open 100 streams and send a request on each of them in a single round trip; the proxy will read and process each stream serially, but the requests to the backend servers can again be parallelized. The client can then open new streams as it receives responses to the previous ones. This gives an effective throughput for a single connection of 100 requests per round trip, with similar round trip timing constants to HTTP/1.1 requests. This will typically lead to almost 100 times higher utilization of each connection. The HTTP/2 Rapid Reset attack The HTTP/2 protocol allows clients to indicate to the server that a previous stream should be canceled by sending a RST_STREAM frame. The protocol does not require the client and server to coordinate the cancellation in any way, the client may do it unilaterally. The client may also assume that the cancellation will take effect immediately when the server receives the RST_STREAM frame, before any other data from that TCP connection is processed. This attack is called Rapid Reset because it relies on the ability for an endpoint to send a RST_STREAM frame immediately after sending a request frame, which makes the other endpoint start working and then rapidly resets the request. The request is canceled, but leaves the HTTP/2 connection open. HTTP/1.1 and HTTP/2 request and response pattern The HTTP/2 Rapid Reset attack built on this capability is simple: The client opens a large number of streams at once as in the standard HTTP/2 attack, but rather than waiting for a response to each request stream from the server or proxy, the client cancels each request immediately. The ability to reset streams immediately allows each connection to have an indefinite number of requests in flight. By explicitly canceling the requests, the attacker never exceeds the limit on the number of concurrent open streams. The number of in-flight requests is no longer dependent on the round-trip time (RTT), but only on the available network bandwidth. In a typical HTTP/2 server implementation, the server will still have to do significant amounts of work for canceled requests, such as allocating new stream data structures, parsing the query and doing header decompression, and mapping the URL to a resource. For reverse proxy implementations, the request may be proxied to the backend server before the RST_STREAM frame is processed. The client on the other hand paid almost no costs for sending the requests. This creates an exploitable cost asymmetry between the server and the client. Another advantage the attacker gains is that the explicit cancellation of requests immediately after creation means that a reverse proxy server won't send a response to any of the requests. Canceling the requests before a response is written reduces downlink (server/proxy to attacker) bandwidth. HTTP/2 Rapid Reset attack variants In the weeks after the initial DDoS attacks, we have seen some Rapid Reset attack variants. These variants are generally not as efficient as the initial version was, but might still be more efficient than standard HTTP/2 DDoS attacks. The first variant does not immediately cancel the streams, but instead opens a batch of streams at once, waits for some time, and then cancels those streams and then immediately opens another large batch of new streams. This attack may bypass mitigations that are based on just the rate of inbound RST_STREAM frames (such as allow at most 100 RST_STREAMs per second on a connection before closing it). These attacks lose the main advantage of the canceling attacks by not maximizing connection utilization, but still have some implementation efficiencies over standard HTTP/2 DDoS attacks. But this variant does mean that any mitigation based on rate-limiting stream cancellations should set fairly strict limits to be effective. The second variant does away with canceling streams entirely, and instead optimistically tries to open more concurrent streams than the server advertised. The benefit of this approach over the standard HTTP/2 DDoS attack is that the client can keep the request pipeline full at all times, and eliminate client-proxy RTT as a bottleneck. It can also eliminate the proxy-server RTT as a bottleneck if the request is to a resource that the HTTP/2 server responds to immediately. RFC 9113, the current HTTP/2 RFC, suggests that an attempt to open too many streams should invalidate only the streams that exceeded the limit, not the entire connection. We believe that most HTTP/2 servers will not process those streams, and is what enables the non-cancelling attack variant by almost immediately accepting and processing a new stream after responding to a previous stream. A multifaceted approach to mitigations We don't expect that simply blocking individual requests is a viable mitigation against this class of attacks — instead the entire TCP connection needs to be closed when abuse is detected. HTTP/2 provides built-in support for closing connections, using the GOAWAY frame type. The RFC defines a process for gracefully closing a connection that involves first sending an informational GOAWAY that does not set a limit on opening new streams, and one round trip later sending another that forbids opening additional streams. However, this graceful GOAWAY process is usually not implemented in a way which is robust against malicious clients. This form of mitigation leaves the connection vulnerable to Rapid Reset attacks for too long, and should not be used for building mitigations as it does not stop the inbound requests. Instead, the GOAWAY should be set up to limit stream creation immediately. This leaves the question of deciding which connections are abusive. The client canceling requests is not inherently abusive, the feature exists in the HTTP/2 protocol to help better manage request processing. Typical situations are when a browser no longer needs a resource it had requested due to the user navigating away from the page, or applications using a long polling approach with a client-side timeout. Mitigations for this attack vector can take multiple forms, but mostly center around tracking connection statistics and using various signals and business logic to determine how useful each connection is. For example, if a connection has more than 100 requests with more than 50% of the given requests canceled, it could be a candidate for a mitigation response. The magnitude and type of response depends on the risk to each platform, but responses can range from forceful GOAWAY frames as discussed before to closing the TCP connection immediately. To mitigate against the non-cancelling variant of this attack, we recommend that HTTP/2 servers should close connections that exceed the concurrent stream limit. This can be either immediately or after some small number of repeat offenses. Applicability to other protocols We do not believe these attack methods translate directly to HTTP/3 (QUIC) due to protocol differences, and Google does not currently see HTTP/3 used as a DDoS attack vector at scale. Despite that, our recommendation is for HTTP/3 server implementations to proactively implement mechanisms to limit the amount of work done by a single transport connection, similar to the HTTP/2 mitigations discussed above. Industry coordination Early in our DDoS Response Team's investigation and in coordination with industry partners, it was apparent that this new attack type could have a broad impact on any entity offering the HTTP/2 protocol for their services. Google helped lead a coordinated vulnerability disclosure process taking advantage of a pre-existing coordinated vulnerability disclosure group, which has been used for a number of other efforts in the past. During the disclosure process, the team focused on notifying large-scale implementers of HTTP/2 including infrastructure companies and server software providers. The goal of these prior notifications was to develop and prepare mitigations for a coordinated release. In the past, this approach has enabled widespread protections to be enabled for service providers or available via software updates for many packages and solutions. During the coordinated disclosure process, we reserved CVE-2023-44487 to track fixes to the various HTTP/2 implementations. Next steps The novel attacks discussed in this post can have significant impact on services of any scale. All providers who have HTTP/2 services should assess their exposure to this issue. Software patches and updates for common web servers and programming languages may be available to apply now or in the near future. We recommend applying those fixes as soon as possible. For our customers, we recommend patching software and enabling the Application Load Balancer and Google Cloud Armor, which has been protecting Google and existing Google Cloud Application Load Balancing users. Posted in Security & Identity Networking Google Cloud Related articles Security & Identity Google mitigated the largest DDoS attack to date, peaking above 398 million rps By Emil Kiner • 4-minute read Networking Deliver and secure your internet-facing application in less than an hour using Dev(Sec)Ops Toolkit By Lihi Shadmi • 4-minute read Security & Identity How Sensitive Data Protection can help secure generative AI workloads By Scott Ellis • 5-minute read Security & Identity Introducing Google Cloud Firewall Plus with intrusion prevention By Megan Yahya • 3-minute read Footer Links Follow us Google Cloud Google Cloud Products Privacy Terms Help Language ‪English‬ ‪Deutsch‬ ‪Français‬ ‪한국어‬ ‪日本語‬",
    "commentLink": "https://news.ycombinator.com/item?id=37830987",
    "commentBody": "The novel HTTP&#x2F;2 &#x27;Rapid Reset&#x27; DDoS attackHacker NewspastloginThe novel HTTP&#x2F;2 &#x27;Rapid Reset&#x27; DDoS attack (cloud.google.com) 336 points by jsnell 22 hours ago| hidepastfavorite98 comments dang 14 hours agoRelated ongoing threads:The largest DDoS attack to date, peaking above 398M rps - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37831062HTTP&#x2F;2 Zero-Day Vulnerability Results in Record-Breaking DDoS Attacks - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37830998 comice 18 hours agoprevNice to see that the haproxy people had spotted this kind of issue with http&#x2F;2 and apparently mitigated it back in 2018: https:&#x2F;&#x2F;www.mail-archive.com&#x2F;haproxy@formilux.org&#x2F;msg44134.h... reply jabart 15 hours agoparentNice, I was looking for this type of information for haproxy. Gives me a lot of confidence in their new QUIC feature. reply vdfs 14 hours agoparentprevIf anyone is curios, Nginx is vulnerable to thishttps:&#x2F;&#x2F;www.nginx.com&#x2F;blog&#x2F;http-2-rapid-reset-attack-impacti... reply obituary_latte 13 hours agorootparentIF configured away from the defaults:By relying on the default keepalive limit, NGINX prevents this type of attack. Creating additional connections to circumvent this limit exposes bad actors via standard layer 4 monitoring and alerting tools.However, if NGINX is configured with a keepalive that is substantially higher than the default and recommended setting, the attack may deplete system resources. reply js2 19 hours agoprev> In a typical HTTP&#x2F;2 server implementation, the server will still have to do significant amounts of work for canceled requests, such as allocating new stream data structures, parsing the query and doing header decompression, and mapping the URL to a resource. For reverse proxy implementations, the request may be proxied to the backend server before the RST_STREAM frame is processed. The client on the other hand paid almost no costs for sending the requests. This creates an exploitable cost asymmetry between the server and the client.I&#x27;m surprised this wasn&#x27;t foreseen when HTTP&#x2F;2 was designed. Amplification attacks were already well known from other protocols.I&#x27;m similarly similarly surprised it took this long for this attack to surface, but maybe HTTP&#x2F;2 wasn&#x27;t widely enough deployed to be a worthwhile target till recently? reply tptacek 19 hours agoparentIt&#x27;s not really an amplification attack. It&#x27;s just drastically more efficiently using TCP connections. reply callalex 14 hours agorootparentIsn’t any kind of attack where a little bit of effort from the attacker causes a lot of work for the victim an amplification attack? Or do you only consider it an amplification attack if it is exploiting layer 3?I tried looking it up and couldn’t find an authoritative answer. Can you recommend a resource that you like for this subject? reply mike_d 13 hours agorootparent> Isn’t any kind of attack where a little bit of effort from the attacker causes a lot of work for the victim an amplification attack?That is technically any HTTP request that requires processing to satisfy. For example if I find a page on your site that executes an expensive database query.Amplification attacks are generally defined as packets that can be sent with a spoofed source address that result in a larger number of packets being returned to the spoofed victim. reply viraptor 14 hours agorootparentprevAmplification attack usually means the first victim produces more traffic than was sent to it and can direct it at the second victim. reply rasz 7 hours agorootparentprevNo, this is Resource exhaustion attacks https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Resource_exhaustion_attack And its not the first HTTP2 rodeo https:&#x2F;&#x2F;www.akamai.com&#x2F;blog&#x2F;security&#x2F;http2-vulnerabilitiesOne of the 2019 vulns https:&#x2F;&#x2F;cve.mitre.org&#x2F;cgi-bin&#x2F;cvename.cgi?name=CVE-2019-9514 even sounds extremely similar to current attack. reply js2 17 hours agorootparentprevYou&#x27;re right. I hadn&#x27;t had my coffee yet and the asymmetric cost reminded me of amplification attacks. I&#x27;m still surprised this attack wasn&#x27;t foreseen though. It just doesn&#x27;t seem all that clever or original. reply gnfargbl 17 hours agoparentprevI was surprised too, but if you look at the timelines then RST_STREAM seems to have been present in early versions of SPDY, and SPDY seems mostly to have been designed around 2009. Attacks like Slowloris were coming out at about the same time, but they weren&#x27;t well-known.On the other hand, SYN cookies were introduced in 1996, so there&#x27;s definitely some historic precedent for attacks in the (victim pays Y, attacker pays X, X I&#x27;m similarly similarly surprised it took this long for this attack to surfaceAs with most things like this, probably many hundreds of unimportant people saw it and tried it out.Trying to do it on Google, with a serious effort, that&#x27;s the wacky part. reply sangnoir 14 hours agorootparent> Trying to do it on Google, with a serious effort, that&#x27;s the wacky partIf I were the FBI, I&#x27;d be looking at people with recently bought Google puts expiring soon. I can&#x27;t imagine anyone taking a swing at Google infra \"for the lulz\". Also in contention: nation-states doing a practice run. reply kristopolous 13 hours agorootparentThat&#x27;s because you don&#x27;t think like a 16 year oldThis is exactly the kind of things that a smart kid who&#x27;s still just a foolish highschool student would do. I wouldn&#x27;t be surprised if this attack already exists in the wild, it&#x27;s not hard to writeAlso the subsequent attacks were less effective, that&#x27;s exactly what some kid would be doing.You don&#x27;t even need an expansive botnot. A rich kid whose parents are in neighborhoods with residential fiber with a bunch of friends could probably coordinate it through a discord serverMost of us really don&#x27;t interact with teenagers regularly so we forget they&#x27;re out there (they also tend to dislike adults so they make themselves especially invisible around us). When it comes to things like this, that&#x27;s my first assumption until further evidence.Merely teenaged graffiti for the digital age. reply lazide 13 hours agorootparentprevGoogle infra is attacked with large scale DDoS type attacks literally multiple times a day. They’re usually a nothingburger. reply mike_d 13 hours agorootparentprevGoogle is rarely the target of intentional attacks. Their cloud customers are the intended victims. reply klabb3 53 minutes agoprev“Cancelation” should really be added to the “hard CS problems” list.Like the others on that list (off by one, cache invalidation etc) it isn’t actually hard-hard, but rather underestimated and overlooked.I think if we took half the time we spend on creation, constructors, initialization, and spent that design time thinking about destruction, cleanup, teardown, cancelation etc, we’d have a lot fewer bugs, in particular resource exhaustion bugs. reply the8472 19 hours agoprevSo we needed HTTP2 to deliver ads, trackers and bloated frontend frameworks faster. And now it delivers attacks faster too. reply jeroenhd 12 hours agoparentHTTP&#x2F;2 makes the browsing experience of high-latency connections a lot more tolerable. It also makes loading web pages in general faster.Luckily, HTTP&#x2F;1.1 still works. You can always enable it in your browser configuration and in your web servers if you don&#x27;t like the protocol. reply tlamponi 2 hours agorootparent> HTTP&#x2F;2 makes the browsing experience of high-latency connections a lot more tolerable. It also makes loading web pages in general faster.HTTP&#x2F;3 does that in my experience (lots of train rides with spotty onboard Wi-Fi) quite a bit better though. As HTTP&#x2F;2 is still affected by head-of-line blocking and a single packet loss can block all other streams, even if the lost packet didn&#x27;t hold data for them. reply shepherdjerred 14 hours agoparentprevAre you suggesting that we didn&#x27;t need HTTP2? What&#x27;s the real alternative here? reply the8472 12 hours agorootparentIn some alternative history there would have been a push to make http 1.1 pipelining work, trim fat from bloated websites (loading cookie consent banners from a 3rd party domain is a travesty on several levels) and maybe use websockets for tiny API requests. And the prioritization attributes on various resources. Then shoveling everything over ~2 TCP connections would have done the job? reply comex 8 hours agorootparentPersonally, as a website visitor and occasional author, I don’t want the performance to be good enough to ‘do the job’. I want it to be as fast as possible. I want it to be instant. For that we need unbloated websites and better protocols. It’s not a competition.After all, you don’t need bloat to suffer from head-of-line blocking. You just need a few images.(Though, personally I’m a much bigger fan of HTTP&#x2F;3 than HTTP&#x2F;2. With a more principled solution to head-of-line blocking and proper 0-RTT, HTTP&#x2F;3 makes a stronger case for why we need a new protocol than HTTP&#x2F;2 did. I don’t know why HTTP&#x2F;2 had to exist at all, really, when QUIC already existed by the time HTTP&#x2F;2 was being standardized. Oh well.) reply the8472 22 minutes agorootparent> It’s not a competition.But it is in the context of the 3-way tradeoff we&#x27;re talking about here. complexity of the site vs. load time vs. protocol complexity> You just need a few images.On the HTTP level those can be deferred after the html&#x2F;styles&#x2F;js. Then you already have the content. What on your site would be \"blocked\" at that point? It&#x27;s just images holding up each other.On the TCP level SACK and FRTO should resolve most instances of HOL after 1 RTT. It&#x27;s not perfect but I suspect a lot of people experience \"slowness\" not because the underlying protocols are bad but because they&#x27;re on old implementations. Or because they&#x27;re on networks with bufferbloat. Upgrade those and we don&#x27;t need those complex workarounds.As for HTTP&#x2F;3... it&#x27;s a mixed bag. The basic idea is great. The execution is another googleism. They didn&#x27;t have the patience to get it into OSes, so now every client has to implement its own network stack which multiplies the things that need patching if something goes wrong. And it runs over UDP instead of being a different transport on the IP level like SCTP. And TLS is a good default but the whole CA-thing shouldn&#x27;t have been mandatory. And header compression also seems like a cure for a disease of their own making, compare which the number of headers you need for HTTP 1.0. reply shepherdjerred 11 hours agorootparentprevWhat incentive would most businesses have to do what you&#x27;re describing?It is _much_ faster, cheaper, and easier to build a bloated website than an optimized one. Similarly, it is much easier to enable HTTP2 than it is to fix the root of the problem.I&#x27;m not saying that it&#x27;s right -- anyone without a fast connection or who cares about their privacy isn&#x27;t getting a great deal here. reply the8472 11 hours agorootparentMost businesses are not in a position to push through a new network protocol for the entire planet! So if we lived in a world with fewer monopolies then protocols might have evolved more incrementally. Though we&#x27;d presumably still have gotten something like BBR because congestion algorithms can be implemented unilaterally. reply immibis 7 hours agorootparentprevWhat incentive do most businesses have to make your checkout process smooth, have automatic doors, or provide shopping carts? Simple: customers like the easiest business to shop at. reply KingMob 4 hours agorootparentprevEven for leaner websites, HTTP&#x2F;2 was always going to be an improvement, for HTTP head-of-line blocking and better header compression, if nothing else. These are orthogonal issues for the most part.Also, they tried prioritization, but it was too unwieldy in practice, the browser vendors didn&#x27;t agree, and it was deprecated in the latest RFC 9113. reply immibis 7 hours agorootparentprevLoading cookie consent banners from a 3rd-party domain is probably a GDPR violation because it transmits user information to a 3rd party without consent. reply bsder 11 hours agorootparentprevSCTP (Stream Control Transmission Protocol) or the equivalent. HTTP is really the wrong layer for things like bonding multiple connections, congestion adjustments, etc.Unfortunately, most computers only pass TCP and UDP (Windows and middleboxes). So, protocol evolution is a dead end.Thus you have to piggyback on what computers will let through--so you&#x27;re stuck with creating an HTTP flavor of TCP. reply murderfs 6 hours agorootparentQUIC (the basis for HTTP&#x2F;3) is basically the spiritual successor to SCTP, except with TLS baked in, so compared with SCTP+DTLS, connection establishment requires significantly fewer roundtrips (0 round trips for session resumption, 1 roundtrip at worst, compared to 4 or so for DTLS). reply Etheryte 14 hours agorootparentprevNothing in their comment claims that, there&#x27;s no need to bring absurd strawmen into the discussion. reply shepherdjerred 13 hours agorootparentI&#x27;m just trying to figure out what the parent was trying to communicate, since the comment by itself didn&#x27;t add much to the discussion.It seems that they&#x27;re more upset by the state of web development today than they are by HTTP2 or anything else that this thread pertains to. reply tsimionescu 13 hours agorootparentprevThe comment lists three negative things as \"the reason we needed HTTP&#x2F;2\". I don&#x27;t even see how you could read it other than implying that HTTP&#x2F;2 was not actually necessary. reply ramranch 13 hours agorootparentprevIt does strongly imply it. HTTP2 is needed for more than just the bloatware of the modern internet. reply Borg3 18 hours agoparentprevWell said :) hehe. reply scrpl 20 hours agoprevAnother reason to keep foundational protocols small. HTTP&#x2F;2 has been around for more than a decade (including SPDY), and this is a first time this attack type surfaced. I wonder what surprises HTTP&#x2F;3 and QUIC hide... reply cmeacham98 20 hours agoparentDNS is a small protocol and is abused by DDoS actors worldwide for relay attacks. reply scrpl 19 hours agorootparentDNS is from 1983, give it some slack reply cmeacham98 19 hours agorootparentThe point I&#x27;m trying to make is that \"small\" protocols aren&#x27;t less likely to be DDoS vectors.Avoiding designing in DDoS relay&#x2F;amplication vectors requires luck or intention, not just making the protocol small. reply scrpl 18 hours agorootparentSmall, less complex protocols are inherently less likely to be insecure all things being equal, simply due to reduced attack surface.DNS was created for a different environment, at a time when security wasn&#x27;t at forefront so it&#x27;s not a good example of the opposite. reply Avamander 18 hours agorootparentThis is such a strong claim I&#x27;d really appreciate something other than \"smaller is better\"Abuse and abuse vectors vary wildly in complexity, some complexity is certainly required exactly to avoid dumb bottlenecks if not vulnerabilities. So based on what are you saying something simple will inherently resist abuse better? reply baby_souffle 16 hours agorootparentprev> Small, less complex protocols are inherently less likely to be insecure all things being equal, simply due to reduced attack surface.That feels intuitive in the \"less code is less bugs is less security issues\" sense but implies that \"secure\" and \"can&#x27;t be abused\" are the same thing.Related? Sure. Same? No.Oddly enough, we probably could have prevented the replay&#x2F;amplification dos attacks that use DNS by making DNS more complex &#x2F; adding mutual authentication so it&#x27;s not possible for A to request something that is then sent to B. reply LK5ZJwMwgBbHuVI 14 hours agorootparentWe could have prevented the replay&#x2F;amplification dos attacks that use DNS by making DNS use TCP.In practice though the only way to \"fix\" DNS that would&#x27;ve worked in the 80s would&#x27;ve probably been to require the request be padded to larger than the response... reply smallnix 12 hours agorootparentBut TCP is way more complex replyaflag 11 hours agorootparentprevI&#x27;m also from 1983 and I haven&#x27;t been DDoSed reply kiitos 13 hours agorootparentprevDNS is an enormous protocol, almost unmeasurably large. reply mcesch 10 hours agorootparentThat&#x27;s a bit overblown. There&#x27;s a lot there and some of it conflicts with itself but it&#x27;s not unmeasurably large by any means. It&#x27;s a knowable protocol (and yes, I&#x27;m aware of the camel meme[1]).1. https:&#x2F;&#x2F;powerdns.org&#x2F;dns-camel&#x2F; reply londons_explore 15 hours agoparentprevHTTP&#x2F;2 is pretty small. reply qingcharles 12 hours agoprevMicrosoft dropped its patch details here:https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;runtime&#x2F;issues&#x2F;93303 reply fefe23 11 hours agoprevI would like to remind everyone that Google invented HTTP&#x2F;2.Now they are telling us a yarn about how they are heroically saving us from the problem they created, but without mentioning the part that they created it.The nerve of these tech companies! Microsoft has been doing this for decades, too. reply gsich 10 hours agoparentThey tried to solve problems that weren&#x27;t existant. reply ta1243 19 hours agoprevSee alsohttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37830998https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37830987 reply n4te 19 hours agoprevThe blog header popping up constantly makes the page unreadable. reply cratermoon 18 hours agoparentReadability hack: https:&#x2F;&#x2F;github.com&#x2F;t-mart&#x2F;kill-sticky reply oars 15 hours agorootparentThanks for sharing Kill Sticky! reply n4te 17 hours agorootparentprevHa, it works great! I like it, thank you! reply ticulatedspline 16 hours agorootparentNot sure about chrome but in Firefox there&#x27;s a button for \"reader view\" on many sites which works great for cutting out UI crap like that. reply n4te 15 hours agorootparentGood point, that also works. For some reason I never remember to use it. replyarisudesu 20 hours agoprevCan anyone can explain what&#x27;s novel about this attack that isn&#x27;t plain old requests flood? reply jsnell 20 hours agoparentIt depends on what you think a \"request flood\" attack is.With HTTP&#x2F;1.1 you could send one request per RTT [0]. With HTTP&#x2F;2 multiplexing you could send 100 requests per RTT. With this attack you can send an indefinite number of requests per RTT.I&#x27;d hope the diagram in this article (disclaimer: I&#x27;m a co-author) shows the difference, but maybe you mean yet another form of attack than the above?[0] Modulo HTTP&#x2F;1.1 pipelining which can cut out one RTT component, but basically no real clients use HTTP&#x2F;1.1 pipelining, so its use would be a very crisp signal that it&#x27;s abusive traffic. reply tptacek 19 hours agorootparentI think for this audience a good clarification is:* HTTP&#x2F;1.1: 1 request per RTT per connection* HTTP&#x2F;2 multiplexing: 100 requests per RTT per connection* HTTP&#x2F;2 rapid reset: indefinite requests per connectionIn each case attackers are grinding down a performance limitation they had with previous generations of the attack over HTTP. It is a request flood; the thing people need to keep in mind is that HTTP made these floods annoying to generate. reply maxfurman 18 hours agorootparentWhat does RTT stand for? My gut says \"Round Trip (something)\" but I could certainly be wrong. reply hathawsh 17 hours agorootparentThe HTTP spec defines it as Round Trip Time, but in this little discussion thread, \"round-trip transaction\" would be a better fit.https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Glossary&#x2F;Round_Trip... reply ynik 9 hours agorootparentprevI wonder why exactly this attack can&#x27;t be pulled off with HTTP&#x2F;1.1 and TCP RST for cancellation. It seems that (even with SYN cookies involved) an attacker could create new connections, send HTTP request, then quickly after send a RST.Is it just that the kernel doesn&#x27;t really communicate TCP RST all that well to the application, so the HTTP server continues to count the connection against the \"open connection limit\" even though it isn&#x27;t open anymore? reply wbl 8 hours agorootparentThe problem for the attacker is they then run into resource limits on the TCP connections. The resets are essential to get the consumption not counting. reply immibis 7 hours agorootparentprevThe server kernel won&#x27;t communicate the new connection to the application until you go through SYN-SYNACK-ACK. reply altairprime 16 hours agorootparentprevIn the style of the above clarification, how would you describe HTTP&#x2F;3 in \"x requests (per y)? per connection\"? reply jsnell 19 hours agorootparentprevIndeed, that&#x27;s a crucial clarification. Thanks. reply blauditore 14 hours agorootparentprevWhat happens if you send more? Does it just get ignored by the server? reply jsnell 14 hours agorootparentFor most current HTTP&#x2F;2 implementations it&#x27;ll just be ignored, and that is a problem. We&#x27;ve seen versions of the attack doing just that, as covered in the variants section of the article.Servers should switch to closing the connection if clients exceed the stream limit too often, not just ignoring the bogus streams. reply arisudesu 16 hours agorootparentprevBy request flood I mean, request flood, as in sending insanely high number of requests per unit of time (second) to the target server to cause exhaustion of its resources.You&#x27;re right, with HTTP&#x2F;1.1 we have single request in-flight (or none in keep-alive state) at any moment. But that doesn&#x27;t limit number of simultaneous connections from a single IP address. An attacker could use the whole port space of TCP to create 65535 (theoretically) connections to the server and to send requests to them in parallel. This is a lot, too. In pre-HTTP&#x2F;2 era this could be mitigated by limiting number of connections per IP address.In HTTP&#x2F;2 however, we could have multiple parallel connections with multiple parallel requests at any moment, this is by many orders higher than possible with HTTP&#x2F;1.x. But the preceeding mitigation could be implemented by applying to the number of requests over all connections per IP address.I guess, this was overlooked in the implementations or in the protocol itself? Or rather, it is more difficult to apply restrictions because of L7 protocol multiplexing because it&#x27;s entirely in the userspace?Added: The diagram in the article (\"HTTP&#x2F;2 Rapid Reset attack\" figure) doesn&#x27;t really explain why this is an attack. In my thinking, as soon as the request is reset, the server resources are expected to be freed, thus not causing exhaustion of them. I think this should be possible in modern async servers. reply jsnell 16 hours agorootparent> But that doesn&#x27;t limit number of simultaneous connections from a single IP address.Opening new connections is relatively expensive compared to sending data on an existing connection.> In my thinking, as soon as the request is reset, the server resources are expected to be freed,You can&#x27;t claw back the CPU resources that have already been spent on processing the request before it was cancelled.> By request flood I mean, request flood, as in sending insanely high number of requests per unit of time (second) to the target server to cause exhaustion of its resources.Right. And how do you send an insanely high number of requests? What if you could send more?Imagine the largest attack you could do by \"sending an insanely high number requests\" with HTTP&#x2F;1.1 with a given set of machine and network resources. With H&#x2F;2 multiplexing you could do 100x that. With this attack, another 10x on top of that. reply toast0 12 hours agorootparentprev> An attacker could use the whole port space of TCP to create 65535 (theoretically) connections to the server and to send requests to them in parallel.This is harder for the client than it is for the server. As a server, it&#x27;s kind of not great that I&#x27;m wasting 64k of my connections on one client, but it&#x27;s harder for you to make them than it is for me to receive them, so not a huge deal with today&#x27;s servers.On this attack, I think the problem becomes if you&#x27;ve got a reverse proxy h2 frontend, and you don&#x27;t limit backend connections because you were limiting frontend requests. Sounds like HAProxy won&#x27;t start a new backend request until the pending backend requests is under the session limit; but google&#x27;s server must not have been limiting based on that. So cancel the frontend request, try to cancel the backend request, but before you confirm the backend request is canceled, start another one. (Plus what the sibling mentioned... backend may spend a lot of resources handling the requests that will be canceled immediately) reply immibis 7 hours agorootparentYou&#x27;re wrong about that. It&#x27;s hard to make 65k new connections on your average client OS, but a packet generator has no problem with it. reply bribroder 20 hours agoparentprevThe new technique described avoids the maximum limit on number of requests per second (per client) the attacker can get the server to process. By sending both requests and stream resets within the same single connection, the attacker can send more requests per connection&#x2F;client than used to be possible, so it is perhaps cheaper as an attack and&#x2F;or more difficult to stop reply arisudesu 16 hours agorootparentIs is a fundamental HTTP&#x2F;2 protocol issue or implementations issue? Could this be an issue at all, if a server has strict limits of requests per IP address, regardless of number of connections? reply lazide 13 hours agorootparentImplementation issue. Some implementations are immune. reply vlovich123 17 hours agoprevWouldn’t this same attack apply to QUIC (and HTTP&#x2F;3)? reply stonogo 17 hours agoparentIt doesn&#x27;t apply to HTTP&#x2F;3 because the receiver has to extend the stream concurrency maximum before the sender can open a new stream. This attack works because the sender doesn&#x27;t have to wait for that after sending a reset in HTTP&#x2F;2. reply londons_explore 15 hours agorootparentBut the max is still ~100 streams... And you can open 100 streams all with one UDP packet using zero-rtt connections.I can send ~1 Million UDP packets per second from one machine. So thats 100 million HTTP requests per second you have to deal with. And when I bring in my 20,000 friends, you need to deal with 2 trillion requests per second.I&#x27;d say that&#x27;s still a problem. reply stonogo 14 hours agorootparentOk, but it&#x27;s not the same problem, which was the question asked. reply vlovich123 13 hours agorootparentI’m still missing something. Can’t you close a Quic stream and open another one within the same UDP packet? reply jsnell 13 hours agorootparentYou can do it a few times, but you can&#x27;t do it 500 times. For HTTP&#x2F;3, the highest permitted stream ID is an explicit state variable communicated by the server to the client, eventually forcing a round-trip. That&#x27;s different from HTTP&#x2F;2 where the client is entitled to assume that new \"stream id window space\" (for the lack of a better term) opens up immediately after a stream is closed.(I&#x27;m fudging things a bit. You can probably build attacks that look kind of similar, but we don&#x27;t think you you could build anything that is actually scalable. But we could be wrong about that! Hence the recommendation to apply similar mitigations to HTTP&#x2F;3 as well, even if it isn&#x27;t immediately vulnerable.) reply vlovich123 11 hours agorootparentThanks for the explanation. Makes sense. replydevit 16 hours agoprevIsn&#x27;t this trivially mitigated by throttling?And the throttling seems even simple: give each IP address an initial allowance of A requests, then increase the allowance every T time up to a maximum of B. Perhaps A=B=10, T=150ms. reply o11c 16 hours agoparentThe whole point of a &#x27;D&#x27;DoS is that there are numerous compromised IP addresses, which only need to make maybe one connection each.You can&#x27;t simply blacklist weird connections entirely, since legitimate clients can use those features. reply devit 16 hours agorootparentThe whole point of this attack is to be able to make a lot of requests for each IP address.If you are making one or few requests per IP you don&#x27;t need this attack, and also aren&#x27;t likely to have any effect on a Google-sized entity. reply pests 10 hours agorootparentIt is a little more complicated because a request is few layers deep. In HTTP2 you open a connection, start a stream, then send a request over that stream.Are you tracking per connection? Per stream? Isn&#x27;t it normal for multiple requests to happen quite quickly? I load a single page with 50 external assets, those get multiplexed over the current stream - is that okay? Is that abusive? The other stream is handling a video player and its requesting (http2) frames of video data - too much? Too fast? reply 1vuio0pswjnm7 14 hours agoprevhttps:&#x2F;&#x2F;nvd.nist.gov&#x2F;vuln&#x2F;detail&#x2F;CVE-2023-44487Dont forget about nghttp2 reply unethical_ban 17 hours agoprev [–] I got out of web proxy management a while back and haven&#x27;t had to delve into HTTP2 or HTTP3.It seems HTTP2 is TCP on TCP for HTTP messages specifically. This must be why HTTP3 is over a UDP based protocol. reply rollcat 13 hours agoparent [–] HTTP2 is not TCP on TCP (that&#x27;s a very basic recipe for a complete disaster, the moment any congestion kicks in); it&#x27;s mostly just multiplexing concurrent HTTP requests over a single TCP connection.HTTP3 is using UDP for different reasons, although it effectively re-implements TCP from the application POV (it&#x27;s still HTTP under the hood after all). Basically with plain old TCP your bandwidth is limited by latency, because every transmitted frame has to be acknowledged - sequentially. Some industries&#x2F;applications (like transferring raw video files over the pond) have been using specialized, UDP-based transfer protocols for a while for this reason. You only need to re-transmit those frames you know didn&#x27;t make it, in any order it suits you. reply foobiekr 12 hours agorootparentTCP&#x27;s stream nature causes multiplexing to bump into head of line blocking, basically. reply immibis 7 hours agorootparentprev [–] HTTP on SCTP on UDP. If only protocols didn&#x27;t ossify. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In August, Google confirmed that an unprecedented DDoS attack, using the HTTP/2 protocol, targeted its services and Cloud customers, with one attack reaching 398 million requests per second.",
      "Google's global load balancing infrastructure successfully prevented any service outages by mitigating the attack on the edge of its network.",
      "Google has since implemented additional protective measures and worked with industry partners to address this new attack vector throughout the ecosystem. The article further elaborates on the attack methodology and provides mitigation strategies."
    ],
    "commentSummary": [
      "A new Rapid Reset DDoS attack has been discovered, targeting HTTP/2, leading to discussions on potential alternatives such as improvements to HTTP/1.1 and the upcoming HTTP/3.",
      "Discussions are centered around preventing DDoS attacks, including replay/amplification attacks using DNS. Suggestions include having DNS use TCP, padding requests, and exploring limitation solutions.",
      "HTTP/2's vulnerability to an attack technique that increases requests per connection is highlighted, with comparisons to possible HTTP/3 attacks. Throttling is proposed as a defensive strategy, acknowledging the difficulty posed by numerous compromised IPs during DDoS attacks."
    ],
    "points": 336,
    "commentCount": 98,
    "retryCount": 0,
    "time": 1696939249
  },
  {
    "id": 37835143,
    "title": "Engineered material can reconnect severed nerves",
    "originLink": "https://news.rice.edu/news/2023/rice-engineered-material-can-reconnect-severed-nerves",
    "originBody": "Skip to main content Body RICE UNIVERSITY News and Media Relations Office of Public Affairs Menu Main Nav Current News Featured Stories News Releases Dateline Rice National Media Relations News Pre-2020 Contact Us RICE NEWS > CURRENT NEWS > 2023 Silvia Cernea Clark 713-348-6728 silviacc@rice.edu Jeff Falk 713-348-6775 jfalk@rice.edu Silvia Cernea Clark - Oct. 10, 2023 POSTED IN: RICE NEWS > Current News > 2023 Rice-engineered material can reconnect severed nerves Magnetoelectric material is first of its kind able to directly stimulate neural tissue Researchers have long recognized the therapeutic potential of using magnetoelectrics ⎯ materials that can turn magnetic fields into electric fields ⎯ to stimulate neural tissue in a minimally invasive way and help treat neurological disorders or nerve damage. The problem, however, is that neurons have a hard time responding to the shape and frequency of the electric signal resulting from this conversion. Rice University doctoral alum Joshua Chen is lead author on a study published in Nature Materials. (Photo by Gustavo Raskosky/Rice University) Rice University neuroengineer Jacob Robinson and his team designed the first magnetoelectric material that not only solves this issue but performs the magnetic-to-electric conversion 120 times faster than similar materials. According to a study published in Nature Materials, the researchers showed the material can be used to precisely stimulate neurons remotely and to bridge the gap in a broken sciatic nerve in a rat model. The material’s qualities and performance could have a profound impact on neurostimulation treatments, making for significantly less invasive procedures, Robinson said. Instead of implanting a neurostimulation device, tiny amounts of the material could simply be injected at the desired site. Moreover, given magnetoelectrics’ range of application in computing, sensing, electronics and other fields, the research provides a framework for advanced materials design that could drive innovation more broadly. Gauri Bhave, a former research scientist in the Robinson lab, is a lead co-author on a study published in Nature Materials. (Photo courtesy of Gauri Bhave) “We asked, ‘Can we create a material that can be like dust or is so small that by placing just a sprinkle of it inside the body you’d be able to stimulate the brain or nervous system?’” said Joshua Chen, a Rice doctoral alumnus who is a lead author on the study. “With that question in mind, we thought that magnetoelectric materials were ideal candidates for use in neurostimulation. They respond to magnetic fields, which easily penetrate into the body, and convert them into electric fields ⎯ a language our nervous system already uses to relay information.” The researchers started with a magnetoelectric material made up of a piezoelectric layer of lead zirconium titanate sandwiched between two magnetorestrictive layers of metallic glass alloys, or Metglas, which can be rapidly magnetized and demagnetized. Gauri Bhave, a former researcher in the Robinson lab who now works in technology transfer for Baylor College of Medicine, explained that the magnetorestrictive element vibrates with the application of a magnetic field. Schematic of neural response for linear magnetic-to-electric conversion (top two conversions) versus nonlinear (bottom third). (Image courtesy of Josh Chen/Rice University) “This vibration means it basically changes its shape,” Bhave said. “The piezoelectric material is something that, when it changes its shape, creates electricity. So when those two are combined, the conversion that you’re getting is that the magnetic field you’re applying from the outside of the body turns into an electric field.” However, the electric signals magnetoelectrics generate are too fast and uniform for neurons to detect. The challenge was to engineer a new material that could generate an electric signal that would actually get cells to respond. “For all other magnetoelectric materials, the relationship between the electric field and the magnetic field is linear, and what we needed was a material where that relationship was nonlinear,” Robinson said. “We had to think about the kinds of materials we could deposit on this film that would create that nonlinear response.” The researchers layered platinum, hafnium oxide and zinc oxide and added the stacked materials on top of the original magnetoelectric film. One of the challenges they faced was finding fabrication techniques compatible with the materials. Magnetoelectric nonlinear metamaterials are 120 times faster at stimulating neural activity compared to previously used magnetic materials. (Image courtesy of the Robinson lab/Rice University) “A lot of work went into making this very thin layer of less than 200 nanometers that gives us the really special properties,” Robinson said. “This reduced the size of the entire device so that in the future it could be injectable,” Bhave added. As proof of concept, the researchers used the material to stimulate peripheral nerves in rats and demonstrated the material’s potential for use in neuroprosthetics by showing it could restore function in a severed nerve. “We can use this metamaterial to bridge the gap in a broken nerve and restore fast electric signal speeds,” Chen said. “Overall, we were able to rationally design a new metamaterial that overcomes many challenges in neurotechnology. And more importantly, this framework for advanced material design can be applied toward other applications like sensing and memory in electronics.” Jacob Robinson is a professor of electrical and computer engineering and bioengineering at Rice University. (Photo courtesy of the Robinson lab/Rice University) Robinson, who drew on his doctoral work in photonics for inspiration in engineering the new material, said he finds it “really exciting that we can now design devices or systems using materials that have never existed before rather than being confined to ones in nature.” “Once you discover a new material or class of materials, I think it’s really hard to anticipate all the potential uses for them,” said Robinson, a professor of electrical and computer engineering and bioengineering. “We’ve focused on bioelectronics, but I expect there may be many applications beyond this field.” Antonios Mikos, Rice’s Louis Calder Professor of Chemical Engineering, professor of bioengineering and materials science and nanoengineering and director of the Biomaterials Lab, Center for Excellence in Tissue Engineering and J.W. Cox Laboratory for Biomedical Engineering, is also an author on the study. The research was supported by the National Science Foundation (2023849) and the National Institutes of Health (U18EB029353). Peer-reviewed paper: Image downloads: Related stories: Links: About Rice: ABOUT SILVIA CERNEA CLARK Silvia Cernea Clark is a media relations specialist in Rice University's Office of Public Affairs. Body Body CURRENT NEWS FEATURED STORIES NEWS RELEASES DATELINE RICE NEWS PRE-2020 CONTACT US HOME BACK TO TOP 6100 Main St., Houston, TX 77005-1827Mailing Address: P.O. Box 1892, Houston, TX 77251-1892713-348-0000Privacy PolicyCampus Carry",
    "commentLink": "https://news.ycombinator.com/item?id=37835143",
    "commentBody": "Engineered material can reconnect severed nervesHacker NewspastloginEngineered material can reconnect severed nerves (rice.edu) 335 points by geox 16 hours ago| hidepastfavorite92 comments Geee 1 hour agoIf I understand correctly, this allows nerve-to-digital and digital-to-nerve conversions. This means that we&#x27;re soon able to replace our bodies with robotic ones.The remaining problem is to figure out how to keep a brain alive in a vat, and making the life-support devices small enough to fit inside the body. It&#x27;s probably not too difficult; you need to circulate blood, remove carbon dioxide, add oxygen, add glucose. You could probably keep the brain alive indefinitely. reply momento 35 minutes agoparent> This means that we&#x27;re soon able to replace our bodies with robotic ones.Talk about jumping to conclusions. reply lordnacho 1 hour agoparentprevWhy? Find a way to distil whatever process the brain does, emulate it on a computer. reply Geee 57 minutes agorootparentYou can do this with atom level simulation and growing the brain all the way from DNA, but it needs a lot of compute (> 50 years).However, and in the end you&#x27;d be bored, because you could get rid of all of your human flaws, which is what makes life interesting. It&#x27;d be really depressing and boring to be a superintelligent computer. reply spoiler 1 hour agorootparentprevThat&#x27;s very \"draw the rest of the owl\"! reply euroderf 1 hour agoparentprevA recurring theme in science fiction. See for example \"They Saved Hitler&#x27;s Brain\". reply rpmisms 15 hours agoprevI&#x27;m not a medical person, can anyone who knows more than me explain exactly how cool this is? Fixing spinal cords, or just repairing simple nerve severance? reply achileas 14 hours agoparentToo early to tell. Might make less invasive neurostimulation therapies, may or may not make them more effective (not all neurons can grow back or grow back to where they are supposed to be). reply markisus 15 hours agoparentprevFrom the article, the device is more like a bridge rather than something that helps the body heal. reply rpmisms 15 hours agorootparentYeah, as far as I understand, it can act like a bridge between two ends of a severed nerve. reply rbanffy 11 hours agorootparentGive me a pair of bridges and a conductor to make the nerve signals travel at electric current speeds and I’ll give you almost instantaneous reflexes. reply gumby 7 hours agorootparent> I’ll give you almost instantaneous reflexes.This is almost probably not what you want. Your nerves respond pretty quickly already and you can do things faster than you can think about them (e.g. catching a ball) so there&#x27;s more to the computation than the \"CPU\".I have read that the myelin sheath on your nerves is thicker at birth then as an adult. The theory (not mine!) is that as it thins the capacitance goes down and perhaps this helps keep the transmission time more constant as yu grow, so that what you have learned about walking, catching a ball, drinking hot coffee from a cup continues to work. Imperfectly -- kids are clumsy -- but enough that you can quickly adapt and not have to keep learning all over again. reply lifeisstillgood 2 hours agorootparentSo if as a child I learn to send a signal to my hand to move out to catch the ball, the signal takes X microseconds to travel from brain down my arm to my and and \"clasp\"But as an adult &#x2F; teen my arm is now a foot or more longer. So the signal would take x+y microseconds to get to my hand - but if the resistance of the nerves reduces it&#x27;s signal is faster and back to x microsecondsThat&#x27;s brilliant reply skykooler 8 hours agorootparentprevI wonder if you could multiplex the signals from a bunch of nerves and send them over an Ethernet link. reply soco 2 hours agorootparentNext level: link together a group of humans. I can&#x27;t really imagine the implications of that, but I think of Vernor Vinge&#x27;s \"A fire upon the deep\" and the group minds of the wolves there... reply BurningFrog 7 hours agorootparentprevA radio link would simplify this enormously. You could link nerves \"wirelessly\"from a leg to near the brain without any surgery in between.I don&#x27;t see why it&#x27;s fundamentally impossible? reply westurner 7 hours agorootparent\"Walking naturally after spinal cord injury using a brain–spine interface\" (2023) https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06094-5 :> To establish this digital bridge, we integrated two fully implanted systems that enable recording of cortical activity and stimulation of the lumbosacral spinal cord wirelessly and in real time (Fig. 1a). reply gumby 7 hours agorootparentprevAfter pressing the button on the microwave you&#x27;d be frozen until the machine beeped.Unless you had a tinfoil hat (well, suit). reply ethbr1 9 hours agorootparentprevGive me one bridge and a mechanical arm...... or just make it a plane&#x27;s control surface. reply calvinmorrison 10 hours agorootparentprevOh boy wait till this guy finds out about copper reply busterarm 15 hours agoparentprevNeuropathy affects a lot of people. Diabetics, for example. This could improve a lot of lives. reply 14 9 hours agorootparentI don’t think it will ultimately help a diabetic. More worrisome then loss of feeling is loss of circulation. With High blood sugars you’re essentially get sugar crystals that build up on the capillary walls, which prevent blood circulation. Even if you cure Neuropathy will still have the issue of poor circulation. reply maximegarcia 3 hours agorootparentBut with neuropathy, which causes loss of feeling, pain… in the feet for example, you can ignore damage that combined with a loss of blood circulation capabilities can make things deteriorate very badly (necrosis, amputation). reply Gibbon1 13 hours agoparentprevEven small nerve injuries can be life changing. Friend my motor cycle rat friends hit a bridge abutment. Severed the nerves in his right shoulder. He has no feeling in and cannot move his arm. So being able to fix that would be a big deal for people affected. reply rpmisms 7 hours agorootparentYeah, one reason I&#x27;m very very careful on my motorcycle. The injuries are amazing, very interested in the continued function of my body. reply trey-jones 14 hours agoparentprevI too am interested in the applications of this technology and also what the timeline for that might look like. reply Julesman 9 hours agoprevKnowing someone with a spinal injury I can tell you that these constant innovations that won&#x27;t reach the market for 20 years are kind of continuously devastating. reply theshackleford 2 hours agoparentI have a spinal cord injury (well rather, an incomplete spinal cord injury) and knowing that every day will be like the last until I die has taken a toll that has at times almost led me to bring that event forward in time. Look after your nerves my peeps. reply DoreenMichele 4 hours agoparentprevThere is too much focus on the \"cool\" factor and not enough on quality of life and results for impacted patients. I don&#x27;t know how to fix it.I&#x27;ve mostly given up on commenting on this on HN. I have an incurable condition and it makes zero difference that these practices directly negatively impact my life and scare the hell out of me, such observations get down voted and attacked and treated like crazy talk.I&#x27;m sorry for what your social contact is going through. reply MetalGuru 3 hours agoparentprev20 years is infinitely better than never. There are a lot of life-altering diseases whose future outlook is much bleaker (eg. little funding or research interest). But yea, having to wait so long definitely sucks for those currently suffering. reply mensetmanusman 7 hours agoparentprevBring it to market faster then? reply koheripbal 7 hours agorootparentUsually they just don&#x27;t work reply codeisawesome 13 hours agoprevWould it be possible in a sci-fi near-future to transplant a new, lab-grown vertebral column, using something like this? reply rbanffy 11 hours agoparentImagine a lab-grown vertebral column where nerve signals travel at electric current speeds. reply jmprspret 6 hours agorootparentDon&#x27;t nerve signals already travel at electric current speeds? reply adrianN 3 hours agorootparentNo they are much slower, on the order of a few dozen meters per second. reply newZWhoDis 11 hours agorootparentprevCS:GO and StarCraft would need new leagues reply dghughes 8 hours agoparentprevI think the spine is far more complex than the average person is aware of. From what I have read it has some basic reaction&#x2F;thinking capabilities which may be beyond what can be made by humans. reply jddj 2 hours agorootparentMine needs to learn to just be quiet and follow instructions reply m463 12 hours agoparentprevspeaking of scifiI wonder... especially after reading about people developing limbs or fingers they don&#x27;t have:\"Some of CTRL-Labs’ goals are mind-bendingly exotic, like training a model for controlling extra fingers.https:&#x2F;&#x2F;www.theverge.com&#x2F;2018&#x2F;6&#x2F;6&#x2F;17433516&#x2F;ctrl-labs-brain-c...I wonder if we could augment&#x2F;cross our nerves to control things we normally can&#x27;t control? what if we could release hormones on demand, like maybe release adrenaline or calm down? reply jillesvangurp 11 hours agorootparentReminds me of a few hilarious plot points in a few Ian Banks novels. One of them, the Hydrogen Sonata had this passage:“Is it true your body was covered in over a hundred penises?” “No. I think the most I ever had was about sixty, but that was slightly too many. I settled on fifty-three as the maximum. Even then it was very difficult maintaining an erection in all of them at the same time, even with four hearts.”Silly as this is, Ian Banks had a way of taking the mere hint of the possibility of a thing to it&#x27;s logical extremes. His characters change sex, regrow limps, or morph themselves into a different alien species basically in an orgy of hedonism and utopianism. Definitely science fiction&#x2F;fantasy when he wrote it but eerily close to becoming science fact as time progresses. reply ethbr1 9 hours agorootparentBanks&#x27; most apt futurism was that give the ability to do anything, only a few people would use it to do something interesting. reply tomcam 11 hours agorootparentprevBought by Facebook years ago. reply unsupp0rted 15 hours agoprevI&#x27;ve always found it incongruous that we can move a person&#x27;s body around the globe at 900 kilometers per hour for 25 hours for under $1500 but for 1000x that price and even with 1000x that time we can&#x27;t bridge the distance between a brain and a foot. reply kmeisthax 15 hours agoparentSo, I&#x27;ll put this into terms that a sysadmin can understand: imagine if all the Ethernet or fiber going to a datacenter rack was bundled up, and I cut that bundle up. Now you have to splice them back together so that every port is still connected to the same port it was before I cut the cables. Oh, and also, the cables are microscopic, incredibly fragile, and we don&#x27;t know how to actually repair them. We sort of just hope they can grow back in place. reply londons_explore 12 hours agorootparentFor scale, a human spine has coming off it a few hundred thousand nerve fibers inside 31 pairs of nerves.[1]I was actually expecting more, since that is effectively IO for the whole body, and each individual nerve fiber tends not to fire more than tens of Hz.[1]: https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC4612209&#x2F; reply rbanffy 11 hours agorootparentNot the whole body. Eyes and ears have their own high bandwidth direct attachment interfaces to the cortex. reply avg_dev 8 hours agorootparentprevVery clear mental image there. Thanks for phrasing it that way. reply p1esk 14 hours agorootparentprevThe fact that they grow back in place makes the whole thing 1000x easier, no? reply itishappy 14 hours agorootparentOnly if it grows back correctly!Imagine attempting to wire up a datacenter that is actively trying to rewire itself (and not necessarily to your plans). reply garba_dlm 14 hours agorootparenthere&#x27;s the kickerit&#x27;s all connected to an organic deep neural networkwhich means that the specific arrangement of the physical wires doesn&#x27;t matter. because there&#x27;s a training period in which the neural network literally learns to control the body anyways reply chairhairair 14 hours agorootparentIf we only ever wanted to reconnect nerves of fetuses that might be relevant. reply knome 14 hours agorootparentThere&#x27;s no reason to be glib. If patients&#x27; brains can reconfigure following trauma such as strokes [1] or having our entire visual field flipped [2] there&#x27;s no reason to assume one couldn&#x27;t reroute around having nerves hooked up differently.[1] https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;books&#x2F;NBK326735&#x2F; [2] https:&#x2F;&#x2F;www.theguardian.com&#x2F;education&#x2F;2012&#x2F;nov&#x2F;12&#x2F;improbable... reply tsavo 12 hours agorootparentCan speak from personal experience. Traumatic incident which severed the nerves to my leg in multiple places. Nerves eventually regrew and reconnected within the leg, and then again where they were severed in the foot.Motor and senory nerves reacted differently.When motor nerves reconnected, I still couldn&#x27;t contract the muscles and went through a series of steps to relearn how to use the limb. First I was trying to \"move\" the leg, but effectively the \"IP address\" for the leg was changed so my \"move\" signals were going to where my head thought the leg was instead of to the new connection. Instead, I would estim a specific muscle and \"listen\" in my head for where \"noise\" was coming from. That \"noise\" was the electric buzzing from the estim&#x27;d muscle contraction. Eventually, I learned how to concentrate to make a muscle contract, and many steps later (pun intended), I learned to walk again.Sensory nerves didn&#x27;t need a push signal, they&#x27;re like a constant inbound feed when connected. When the sensory nerves reconnected, it is something you definitely notice. Going about your day, and suddenly you feel an jolt, like being shocked, and over the next few hours to days the area that is reconnecting is burning, stinging, feels like it is being crushed by pressure, and cold all at the same time. It was much more intense than when your arm falls asleep. The sensation can be maddening but it eventually passes as your body begins to sort and acclimate the signals.All of these steps on calibrating the sensory nerves and learning how to contract and coordinate muscles is something we take for granted as people usually sort it out when they&#x27;re infants. reply azalemeth 12 hours agorootparentI&#x27;ve had exactly these experiences following a spinal injury -- fracture of a vertebra but with minimal cord damage and quite a lot of disruption to the dorsal root ganglia.You can&#x27;t put into words how weird it is to fall over because your brain thinks your foot is somewhere it isn&#x27;t. Or how suddenly you become incredibly aware of how the front of your calf feels. Or how overjoyed you are to be able to move your toes again for the first time in a year. It&#x27;s not like what you see on the movies.Wallerian degeneration -- yes, degeneration -- is part of the healing process of some grades of nerve injury. Things literally get worse before they get better, as the fragment left of the crushed axon degenerates to its root and then regrows. It&#x27;s incredibly slow -- around 1mm&#x2F;day at most -- and a matter of probabilities. What&#x27;s also worth mentioning is that there are plenty of internal nerves too, where restoring function after a trauma would be life-changing -- like the Vegas nerve, which buggers up lots of things if damaged slightly, or, in my case, some of the nerves in the fundus and neck of the bladder, meaning that my toileting is really very different than it was before.I&#x27;m glad you&#x27;re doing better, and hope you continue to do so. I&#x27;ve no idea if the device the article is talking about will ever help, but nerve injuries cause so much disability worldwide I&#x27;m glad they&#x27;re continuing to be worked on. reply ethbr1 9 hours agorootparentAmazed congratulations to you and parent. I can&#x27;t imagine going through that, but then I imagine you wouldn&#x27;t have chosen to either. Hope things continue improving.Your description also made me reflect on infants, and whether we effectively \"feel more\" in that stage, as our nervous systems are self-calibrating and adjusting gain. reply avg_dev 8 hours agorootparentprevThanks to both of you for the amazing descriptions. And continued good luck. reply oneshtein 4 hours agorootparentprevDid you use any medication to help nerves regrow, like Lion&#x27;s Mane or something like that? We have plenty of injured soldier here, in Ukraine. I&#x27;m looking for something cheap and effective to help them recover from injuries. reply dotnet00 14 hours agorootparentprevExcept the arrangement starts to matter more and more with age. reply garba_dlm 13 hours agorootparentdepends on the personmost old people actually matter less and less with age reply waste_monk 4 hours agorootparentprevImagine that the bundle of cables were all completely identical and unlabelled in any way. Even if you could surgically reconnect the nerves perfectly, you don&#x27;t know which nerves should be connected to what on the other side.As another commenter posted, there are 31 \"bundles\" of nerves carrying a total of a few hundred thousand individual fibres. The odds of re-connecting a human spine correctly are simply incredibly small. reply inglor_cz 14 hours agorootparentprevUnless they get weird and turn cancerous, or some kind of strange Ethernet-eating bug attacks and eats them etc.Regenerative medicine of living tissues is extra hard. reply 2devnull 14 hours agorootparentprevNo. reply root_axis 14 hours agoparentprevI don&#x27;t see what&#x27;s incongruous about it, those two tasks are completely different, making a comparison with respect to distance isn&#x27;t meaningful. reply grishka 12 hours agoparentprevGiven enough curiosity, anyone can understand how and why a jet plane works in as much or as little detail as they want. Planes were engineered by humans from first principles and we&#x27;re good at both documenting our inventions and understanding those first principles.Building something to interface with a biological system, though, is another matter entirely. It could as well be alien technology. It requires reverse engineering a lot of extremely complex stuff that was not designed by our civilization. So incomprehensibly complex that we only fairly recently made enough progress in other fields to be able to build tools to meaningfully poke at it. reply Nextgrid 10 hours agorootparent> that was not designed by our civilizationTo the best of our knowledge, it wasn&#x27;t designed at all. It&#x27;s more like giving a monkey a typewriter and millions of years, with a very crude feedback function (evolution&#x2F;natural selection) that makes it very hard to backtrack out of a local maximum.It&#x27;s quite hard to interface with it because it&#x27;s not designed to be interfaced with and the whole thing works pretty much by accident, and where changing any variable could throw it off. reply grishka 1 hour agorootparent> it wasn&#x27;t designed at allObviously. There is no functional separation between components (some proteins do multiple unrelated jobs), there&#x27;s no distinct hierarchy (layers just kinda flow into each other), and there&#x27;s way too much global state that works seemingly something like this: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=I5mwVv5NjhA reply havnagiggle 14 hours agoparentprevSome things turn out to be hard to do. reply onlyrealcuzzo 14 hours agorootparentYes, and a lot of those things are done.It&#x27;s just interesting the way we&#x27;ve progressed. reply renewiltord 14 hours agoparentprevIs that interesting? It seems natural. Precision has always been quite hard. Most people can kick a ball 10 m more easily than they can kick a ball 1 m but to center the hit at a 1 mm spot. I can carry a pile of sand 100 m easier than I can move 1 grain of sand precisely 1 cm without disturbing the others around it. reply HL33tibCe7 15 hours agoparentprevYou’re not thinking hard enough reply xwdv 14 hours agoparentprevThe class of problems that falls under the category of movement of matter is fairly trivial.Repairing severed nerves is more like an entropy-reversal class problem. I don’t think we could even put a sufficiently broken tea cup back together exactly as it was. reply tibbydudeza 14 hours agoparentprevEver tried splicing fiber optic cable by hand compared to say telephone wire ???.How does the bus protocol work - it is not like it is 5V&#x2F;-5V , it is insanely complex ???. reply inglor_cz 15 hours agoparentprevIndividual science disciplines advance at a very uneven speed and with different start points in history.We have always (OK, at least since Ancient Egypt) known more about mathematics than about physics.We have always (...) known more about physics than about chemistry.We have always (...) known more about chemistry than about our bodies.The available technology mirrors that. reply grishka 12 hours agorootparentAnd biology in particular is unique because it requires other fields to progress pretty far for the tools necessary for comprehensive biological research to become possible. As in, a microscope is a prerequisite for the discovery of cells and microorganisms. reply throw1234651234 13 hours agoparentprevPeople think medicine is way more advanced that it actually is. We can&#x27;t add back some crystals on a tooth. We can&#x27;t re-attach a nail to a nail bed. We can&#x27;t fix cartilage. We can&#x27;t physically repair arteries (short of donor material) or varicose veins. List goes on and on. reply carabiner 14 hours agoparentprevThese seem completely unrelated. One is about travel distance the other is about forming tissue connections. We can easily travel the distance between brain and a foot, it&#x27;s around 5 ft. reply finite_depth 15 hours agoparentprevIt might help to remember that a human body has more cells than there are people on Earth by around three orders of magnitude, and that you&#x27;re engineering things on the scale of nano- or micrometers.A typical human cell is on the order of 10 micrometers. If you need to bridge even 1 cm of that, you&#x27;re bridging ~a thousand cell-widths. If you think of a cell as the somatic equivalent of a house in a city, that&#x27;s the equivalent of an infrastructure project spanning (based on a quick count of the number of houses on each block in Oakland) the equivalent of around six miles, or roughly from downtown Oakland to El Cerrito on a map of the Bay (~4 BART stations). And you have to do that on a scale where precision manufacturing is incredibly hard, where you&#x27;re dealing with extremely difficult problems of chemical synthesis, in a living body, without provoking the body&#x27;s defense or repair mechanisms to stop you. And that&#x27;s assuming you even know what you&#x27;re trying to do, which requires an understanding of the machinery of those cells that we often don&#x27;t have. reply ethbr1 9 hours agorootparentSome neurons can be extremely long though (up to 1m).https:&#x2F;&#x2F;www.khanacademy.org&#x2F;science&#x2F;biology&#x2F;human-biology&#x2F;ne... reply einpoklum 13 hours agoprevI&#x27;m very impressed with this advance of modern bio-science - a strain of intelligent rice which can be trained in engineering. reply tired_and_awake 14 hours agoprevUniversity hosted announcements (e.g. rice.edu announces a major breakthrough) have incentives that are so out of wack it&#x27;s basically clickbait at this point.Researchers oversell their results for publishing and funding purposes. Then the university oversells those results to draw in students&#x2F;investors.Curious if others feel the same way or if I&#x27;m just too cynical at this point... reply beambot 14 hours agoparentI understand the cynicism. Academic reporting is a hard & thankless gig.The non-cynical version: Half of a researcher&#x27;s job is disseminating results -- to other researchers & the populace at large. I&#x27;d much rather have excited university announcements (even over-reaching ones) than the rage-porn news that dominates the airwaves & \"mainstream news\" homepages.Besides: Any serious researcher will just revert to the actual peer-reviewed source material. It was linked in the article. Here it is: https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41563-023-01680-4And here&#x27;s the PDF: https:&#x2F;&#x2F;www.biorxiv.org&#x2F;content&#x2F;10.1101&#x2F;2022.01.24.477527v2....As someone who has peripheral knowledge in this space (piezeoelectric & magnetostrictive devices and neural stimulation & recording): I&#x27;ve never seen a magnetostrictive material capable of developing a DC bias. Further, driving the device in a non-resonant mode for neural stimulation is even more new to me -- that&#x27;s quite fascinating, and I&#x27;d say that article lives up to the hype from my 1000-ft vantage. I.e. This was effective reporting. reply anoxor 14 hours agoparentprev30 % of physics and chemistry can’t be replicated. 70 % of soft science can’t be replicated. Most of what we know about sociology may just be fake, and given that a huge foundation of progressive thought around rase, sex, and gender is based on this, this is a big problem.at a minimum, this isn’t far off reply OnlineGladiator 14 hours agorootparent> 30 % of physics and chemistry can’t be replicated.Is this true? I left academia a long time ago, but I&#x27;d be surprised if it&#x27;s that good. I&#x27;d actually suspect it&#x27;s the inverse and only 30% can be replicated. reply bluGill 14 hours agorootparentWho knows? Most science nobody attempts to replicate. The major stuff yes, but small things are not worth the cost or time. reply Terr_ 13 hours agoparentprevThat makes me think of this comic on the news cycle: https:&#x2F;&#x2F;phdcomics.com&#x2F;comics&#x2F;archive.php?comicid=1174 reply gpderetta 15 hours agoprev [–] Neural lace when? reply ElFitz 14 hours agoparent [–] Considering how hard it is to actually fix any accidental damage to brain cells and nerves, and the far-reaching consequences of such damage, I am always amazed to see how many are eager to just plug something into it and let some arbitrary code send electric shocks through their brain.I get the unfathomable potential of the thing, and the appeal of said potential. But that’s a product I won’t be an early-adopter of. reply viraptor 13 hours agorootparentYeah, every time something like that is mentioned, I just think of small issues that could cause noise on the output. Basically if you wear some device that helps you, either passive like glasses or active like a hearing aid, you can immediately disconnect then if needed. But imagine an implant bugs out and you start receiving a maximum signal for something without being able to stop. Like a blinding bright light which doesn&#x27;t go away when you close your eyes. As much as the cyberpunk idea is fun, I&#x27;d have to really suffer without some implant to risk it. reply Terr_ 9 hours agorootparentI wonder if it&#x27;s possible to make a much simpler passive-only \"mental stress detection\" system, and use that as a kind of emergency stop (let&#x27;s not call it a \"kill switch\" in this context) for any more-complicated augmentation.That creates the opposite risk that your enhancements stop working when you _want_ them in some kind of real&#x2F;external emergency situation.I suppose it also wouldn&#x27;t work in the case that the main augmentation caused, say, unending waves of paralyzing calm, although there could be another detection-gate for physiological signals to keep such people from starving&#x2F;dehydration. reply maxerickson 9 hours agorootparentprevAs of yet, most people with brain implants have them to try to improve profound problems.Cochlear implants are controversial in the deaf community, but they also have a relatively clear purpose and effect (vs being \"arbitrary\"). reply gpderetta 13 hours agorootparentprev [–] Oh, me neither! But growing up reading scifi and cyberpunk in particular, one has to wonder. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Rice University researchers have devised a magnetoelectric material capable of stimulating neural tissue and repairing separated nerves.",
      "The new material carries out the magnetic-electric conversion 120 times faster than its counterparts, paving the way for accurate remote neuron stimulation and minimally invasive neurostimulation methods.",
      "Beyond neurostimulation applications, this cutting-edge material's design framework offers potential use in computing and sensing fields."
    ],
    "commentSummary": [
      "A new engineered material has been produced with the ambition of reconnecting severed nerves, potentially paving the way for spinal cord repairs and neuropathy treatments.",
      "There are doubts and debates regarding the viability of nerve regeneration, the body's response to such technology, and the potential exaggeration of research claims by universities.",
      "The community also identified issues like the reliability of scientific replication and the risks involved with brain implants and neural stimulation."
    ],
    "points": 334,
    "commentCount": 92,
    "retryCount": 0,
    "time": 1696960018
  },
  {
    "id": 37831263,
    "title": "Valve says Counter-Strike 2 for macOS not happening, there aren't enough players",
    "originLink": "https://www.macrumors.com/2023/10/10/valve-confirms-counter-strike-2-no-macos/",
    "originBody": "Skip to Content Got a tip for us?Let us know Front Page Roundups Guides How Tos Reviews Buyer's Guide Forums iPhone 15iOS 17iPhone 15 ProApple Vision PromacOS SonomavisionOSwatchOS 10MacBook Pro 14 & 16\"HomePodiPhone 14Apple TViPad ProApple Watch Ultra 2iPadAirPods 3Apple CarApple Pro Display XDRHomePod miniMac ProMacBook AirMacBook Pro 13\"iPhone 13AirPods MaxAirPods ProApple DealsApple Studio DisplayApple Watch SEMac StudioiMaciPad AiriPad miniiPhone SEMac miniApple Watch Series 9iPhone 16CarPlayiPadOS 17Apple Pay All > Valve Says Counter-Strike 2 for macOS Not Happening Because There Aren't Enough Players on Mac to Justify It Tuesday October 10, 2023 3:14 am PDT by Tim Hardwick Valve on Monday said it has no plans for a macOS version of the recently released game Counter-Strike 2, the follow-up title replacing the hugely popular FPS Counter-Strike: Global Offensive. Valve confirmed its decision and gave its reasons in a newly published Steam support FAQ: As technology advances, we have made the difficult decision to discontinue support for older hardware, including DirectX 9 and 32-bit operating systems. Similarly, we will no longer support macOS. Combined, these represented less than one percent of active CS:GO players. Moving forward, Counter-Strike 2 will exclusively support 64-bit Windows and Linux. Last month's release of Counter-Strike 2 forced a 26GB update for everyone with CS:GO, including Mac users, but after installation those on macOS soon discovered that the update makes the original game as well as the update unplayable because of the lack of support and no rollback option. Valve now says Mac players will be eligible for a Prime Status Upgrade refund if most of their CS:GO playtime was on macOS and they played CS:GO on a Mac between the announcement of the Counter-Strike 2 Limited Test (March 22, 2023) and the launch of Counter-Strike 2 (September 27, 2023), regardless of when they purchased their Prime Status Upgrade. Valve will offer the refunds until December 1, 2023. Valve added that those who wish to continue playing CS:GO on macOS will be able to access a legacy version or \"frozen build\" of the game, which has all the features of CS:GO except for official matchmaking. Support for this version of CS:GO will end on January 1, 2024. After this date, the game will still be available, but certain functionality that relies on compatibility with the Game Coordinator (e.g., access to inventory) may degrade and/or fail, according to Valve. Despite Valve's refund offer to Mac users, the news that a macOS version of Counter-Strike 2 is not in active development will come as a bitter blow to many players. Though CS:GO launched all the way back in 2012, it's still one of the most popular FPS games available today, with tens of millions of players logging in each month. So even if less than 1 percent of the user base is on Mac, that could still account for hundreds of thousands of players. Regardless of numbers, one of the reasons Valve is reluctant to develop CS2 for Mac is that Apple devices do not provide native support for the Vulkan API that the game is based on. Vulkan was designed to succeed OpenGL and address some of the latter's shortcomings, and while there is an open-source library called MoltenVK that provides a Vulkan implementation on top of Apple's Metal graphics API, it still lacks some of Vulkan's advanced features. Tag: Valve [ 168 comments ] Related Stories Get weekly top MacRumors stories in your inbox. Top Rated Comments apocalyarts 23 hours ago at 03:44 am I said it before and I say it again: Apple needs to pay up and explicitly fund the ports for specific marquee titles in order for mac gaming to take off. Doesn't matter which M1234 chips they have or how many game porting toolkits they develop. This train will not leave the station, if Apple doesn't throw the coal into the engine themselves. Score: 30 Votes (LikeDisagree) ivayle 1 day ago at 03:33 am Imagine being Valve making millions of dollars per month for selling skins in Counter Strike. Now imagine again being Valve creating a new version of Counter Strike (still a half baked product btw) not willing to support a brand new trend in processors and gpu power because the old statistics say only 1% of players use macOS. Let be honest the macOS version of CS:GO sucked in the past, but also the hardware it was running on. This was a perfect opportunity to gain macOS gaming momentum for Counter Strike (new hardware and new version of the game) and they decided to throw it away... to save money. Score: 21 Votes (LikeDisagree) Kraszim 23 hours ago at 03:50 am You can expect this to continue happening for as long as Apple pushes Metal and refuses to support Vulkan. People are not going to design desktop class games around API that has single platform use. Blizzard did the same thing, with dropping macOS support in their newest titles, although they for now keep the old ones working. If macOS wants to get anywhere is desktop game support it has to be a case \"why not support it\" instead of \"we need to make a Metal compatible version of our engine\". Score: 19 Votes (LikeDisagree) Zc456 1 day ago at 03:22 am Now that they have their own console, they have even less of a motivation. Score: 14 Votes (LikeDisagree) H2SO4 23 hours ago at 03:49 am Apple is now trying to get as many gamers and game developers to switch to Mac and now this? Valve not allowing Mac users to play the game because there are not enough users to support? I've played Counter-Strike back when it was just a mod of Half-Life and I was only a Windows user back then... I have stopped playing computer or mobile games for about 20 years now but there are a few titles that I would still want to play whenever I have time... Counter-Strike 1.3, Grand Theft Auto III, Grand Theft Auto Vice City, Diablo II: Lord of Destruction, Need for Speed Underground 2, Need for Speed Most Wanted (2005) just to name a few... You think that them encouraging gamers no after decades of not caring is supposed to make them all switch immediately? They will be looking at Apples track record of not caring, asking themselves if they will suddenly decide not to support a specific GPU, (ahem....), and such like and making a decision based on this. I don't blame Valve. I gave up on Mac gaming ages ago. I wanted to but was too hit and miss. Score: 10 Votes (LikeDisagree) MayaUser 1 day ago at 03:18 am Lets make a petition here...to save this...lets find 500 players Score: 9 Votes (LikeDisagree) Read All Comments Popular Stories Will Apple Release New Macs Soon? Here's What the Latest Rumors Say Monday October 9, 2023 4:12 am PDT by Tim Hardwick Apple often releases new Macs in the fall, but whether that will happen this year is far from certain. It's the second week of October now, and if Apple plans to launch Macs before the holidays, recent history suggests it will happen this month. Here's what we know so far. It's been over 120 days since Apple released a new Mac, the last being M2 versions of the Mac Pro, Mac Studio, and... Read Full Article • 131 comments Amazon Prime Big Deal Days: 15-Inch MacBook Air Drops to Record Low Prices at $250 Off Monday October 9, 2023 6:36 am PDT by Mitchel Broussard Amazon's October \"Prime Big Deal Days\" sale event has officially kicked off and one of the best deals you can get is on the 15-inch M2 MacBook Air. This notebook is $250 off and at all-time low prices right now. Note: MacRumors is an affiliate partner with some of these vendors. When you click a link and make a purchase, we may receive a small payment, which helps us keep the site running.... Read Full Article • 21 comments Apple CEO Tim Cook Talks the Future of iPhone, Driving an EV, and More in New Interview Monday October 9, 2023 8:15 am PDT by Hartley Charlton Apple CEO Tim Cook has explained why the company releases a new iPhone every year, what the device will be like in the future, what he is personally doing to reduce his carbon footprint, and more in a new interview with Brut. The brief interview, shot at an Apple data center and solar power facility in Denmark, delves mainly into Cook's opinion on various environmental issues such as... Read Full Article • 145 comments OLED iPad Mini and iPad Air Could Follow 2024's iPad Pro Monday October 9, 2023 8:40 am PDT by Hartley Charlton Apple is considering bringing OLED displays to the iPad mini and iPad Air from 2026, according to a report from technology research firm Omdia. In a detailed report forecasting Apple's iPad panel purchasing plans published earlier today, Omdia explains that 2024 will mark the beginning of a transition to OLED in its tablet lineup. The potential transition to OLED on Apple's two mid-range... Read Full Article • 49 comments Top Stories: iPhone 15 Pro Overheating Fix, Apple Watch Double Tap, and More Saturday October 7, 2023 6:00 am PDT by MacRumors Staff After a few weeks of controversy over potential overheating in the new iPhone 15 Pro models, Apple this week released a software update to help address the issue, which the company says is not the result of the new titanium casing or other thermal design problems. Apple is also continuing work on the next round of more significant software updates led by iOS 17.1, and we're keeping tabs on... Read Full Article • 27 comments Breakability Test Pits iPhone 15 Pro Max Against Galaxy Z Fold5, Pixel Fold and More Monday October 9, 2023 12:22 pm PDT by Juli Clover Device insurance provider Allstate Protection Plans today shared the results of one of its annual device drop tests, and this time around, the company broke some of the most expensive smartphones on the market, including the iPhone 15 Pro Max. The drop and dunk tests included the $1,200 iPhone 15 Pro Max, the $1,200 Samsung Galaxy S23 Ultra, the $1,799 Samsung Galaxy Z Fold5, and the $1,799... Read Full Article • 114 comments 'Green Bubbles and Blue Bubbles Want to Be Together' Says Samsung Ad Pushing Apple to Adopt RCS Monday October 9, 2023 11:32 am PDT by Juli Clover Samsung today joined Google in attempting to convince Apple to adopt the Rich Communication Services or RCS messaging standard that Google has been pushing. In a short ad that's a riff on Romeo and Juliet, Samsung features two smartphone interfaces texting each other. \"Juliet\" is the iPhone in the metaphor and her parents (aka Apple) won't allow for the adoption of RCS. \"What did green ever... Read Full Article • 468 comments Next Article Apple Vision Pro Supports Up to 100Hz Refresh Rate Guides iOS 17 Features Our comprehensive guide highlighting every major new addition in iOS 17, plus how-tos that walk you through using the new features. iPhone 15 and iPhone 15 Pro: How to Use All the New Features Get the most out your iPhone 15 with our complete guide to all the new features. 50 New macOS Sonoma Features A deep dive into new features in macOS Sonoma, big and small. macOS Sonoma Features to Check Out First New screen saver experience, desktop widgets, Safari profiles, and more. • iPhone 15 vs. iPhone 15 Pro • 22 Hidden Features in watchOS 10 • Apple Watch Series 9 vs. Apple Watch Ultra 2 • 12 Time-Saving iPhone Tips • 10 macOS Tips to Make Your Life Easier • 14\" MacBook Pro vs. 16\" MacBook Pro • Apple M2 Chip • AirPods 3 vs. AirPods Pro 2 • One AirPod Not Working? See more guides Upcoming iMac Late 2023/Early 2024? A new 24-inch iMac is in an \"advanced state of development\" and could launch as soon as late 2023. A larger model may also be in the works. Apple Vision Pro Early 2024 The Apple Vision Pro AR/VR headset is set to launch in early 2024. It features dual 4K displays, gesture tracking, an M2 chip, and a $3,499 price tag. MacBook Air Early 2024? M3 models in 13.6\" and 15.3\" sizes. 13\" MacBook Pro Early 2024? Apple's cheapest MacBook Pro should get a speed bump to a new M3 chip. • 14\" & 16\" MacBook Pro • iPad Pro See full product calendar Other Stories iOS 17.1 Coming Soon With These 8 New Features for Your iPhone 20 hours ago by Joe Rossignol iPhone 13 vs. iPhone 15 Buyer's Guide: 30+ Differences Compared 20 hours ago by Hartley Charlton How to Import and Annotate PDFs in Apple Notes 21 hours ago by Tim Hardwick How to Use Block Quotes in Apple Notes 21 hours ago by Tim Hardwick Could Apple's Rumored 2024 Budget Mac Be a 12-inch MacBook? 2 days ago by Tim Hardwick MacRumors attracts a broad audience of both consumers and professionals interested in the latest technologies and products. We also boast an active community focused on purchasing decisions and technical aspects of the iPhone, iPod, iPad, and Mac platforms. About MacRumors.com Advertise on MacRumors Our Staff Arnold Kim Editorial Director Email • Twitter Eric Slivka Editor in Chief Email • Twitter Juli Clover Managing Editor Email • Twitter Joe Rossignol Senior Reporter Email • Twitter Mitchel Broussard Deals Editor Email • Twitter Tim Hardwick Senior Editor Email • Twitter Hartley Charlton Senior Editor Email • Twitter Marianne Schultz Project Manager Email • Twitter Dan Barbera Video Content Producer Email • Twitter Ryan Barrieau Graphic Designer Email •Twitter Steve Moser Contributing Writer Email • Twitter Marko Zivkovic Contributing Writer Email • Twitter Related Links Touch Arcade ‘Stardew Valley’ Concert Tour “Festival of Seasons” Announced for 2024, Live Orchestra Performances Planned ‘Honkai Star Rail’ Version 1.4 Update Now Available on Mobile and PC Alongside Today’s PS5 Launch SwitchArcade Round-Up: Reviews Featuring ‘Bomb Rush Cyberfunk’ & ‘Disgaea 7’, Plus Today’s Releases and Sales ‘KOF ’98 ACA NEOGEO’ Review – Today’s Bout Is A Mirror Match ‘Warm Snow’, Bilibili’s Chinese Martial Arts-Inspired Roguelite, is Out Now on iOS and Android New ‘Afterplace’ Update Adds 120fps Support, New Content, Features, and More ‘BlizzCon Collection’ Digital Bundles Now Available With In-Game Content From Hearthstone, Warcraft Rumble, Diablo 4, and More To Celebrate the Return of BlizzCon ‘Wrestle Jump 2’, aka ‘2Wrestle2Jump’, Launching Next Week from ‘Soccer Physics’ Developer Otto Ojala YouTube iPhone 15 Pro Max Real World Review: After the Updates! Our Favorite Apple Accessories ft. @saradietschy (MacRumors Show S02E39) Google’s New Best Take Feature on Pixel 8 Pro! iPhone 15 Pro: Create Amazing Action Button Shortcuts! Should You Upgrade to the New Apple Watch Series 9? (The MacRumors Show S02E38) Copyright © 2000-2023 MacRumors.com, LLC. Privacy / DMCA contact / Affiliate and FTC Disclosure Accessibility Statement [ Featured On/Off ] [ Full Articles On/Off ] [ FluidFluid HD ] [ AutoLightDark ] Information from your device can be used to personalize your ad experience. Do not sell or share my personal information.",
    "commentLink": "https://news.ycombinator.com/item?id=37831263",
    "commentBody": "Valve says Counter-Strike 2 for macOS not happening, there aren&#x27;t enough playersHacker NewspastloginValve says Counter-Strike 2 for macOS not happening, there aren&#x27;t enough players (macrumors.com) 321 points by tosh 21 hours ago| hidepastfavorite660 comments highwaylights 57 minutes agoPart of this is probably related to Apple&#x27;s pushing gaming a little harder with Sonoma.The pool of players is so small because gaming has been neglected on the Mac for so long, while tech like Vulkan that might have opened the possibility of gaming on a Mac has been blocked entirely. Valve is probably spot on in saying it&#x27;s not worth the effort for the users they&#x27;ll pick up right now.That said, now that Apple is at least presenting as if they&#x27;re trying to change this, that user base could grow over time. The problem is that in the shadow of Epic&#x27;s big battle with Apple over App Store rules, it would make sense that Valve is wanting to just NOPE right out of this arrangement. Why help build gaming on the Mac when the balance of evidence suggests it&#x27;s just going to be used to drive you off the platform? It presents a risk to a chunk of Steam&#x27;s install base for little to no benefit.I&#x27;m honestly pleasantly surprised macOS still gets the support it does by Unreal engine, it would be really easy for Epic to deprecate macOS support entirely (albeit not in the consumer&#x27;s interest). reply rjst01 18 hours agoprevTangentially related - I recently made a fun discovery. Many older Steam games on mac are marked as 64-bit incompatible, but it turns out this is just a backend flag that&#x27;s not always set correctly. Steam will let you install the game in spite of the warning, and if it does actually contain 64-bit code, it&#x27;ll run just fine.The days of this being relevant are probably numbered though, because at some point we will lose Rosetta. But I&#x27;m going to enjoy Antichamber [1] on my mac while I can.1: https:&#x2F;&#x2F;store.steampowered.com&#x2F;app&#x2F;219890&#x2F;Antichamber&#x2F; reply perryizgr8 1 hour agoparentThe forced 32 bit incompatibility on Macbooks is so weird. My laptop is perfectly capable of executing 32 bit codes. Why does the OS start lying to me that it can&#x27;t after an update? It&#x27;s one thing to offer a new product without some backward compatibility, but to remove it via an update? How is it remotely acceptable? reply swsieber 14 hours agoparentprevHuge shout out for Antichamber. It had a wonderfully mind bending learning curve. reply RockRobotRock 16 hours agoparentprevNumerous source games that you could play on Mac before 64 bit was mandatory. Why can&#x27;t Valve just update the binaries?? reply IntelMiner 15 hours agorootparentSource 1 is extremely old (branching off from the \"GoldSrc\" engine that Half-Life 1 used which came from Quake 2)Valve seems to be making a pretty strong effort to distance itself from working with that engine where possible (though it&#x27;s unclear if their lagging tentpole title Team Fortress 2 will get the Source 2 treatment)It&#x27;s possible that rebuilding these games for 64-bit Macs specifically would encounter an undesirable amount of bugs reply practice9 9 hours agorootparentThere are unofficial ports for ARM. They are fully playable from start to end at 2k@120fps (Retina resolutions) on Apple Silicon Macs...Sure there are minor bugs like flashlight in HL2 not working correctly.But IMO Valve just don&#x27;t care about macOS to assign even one engineer to fix this and compile the new binaries. reply jamesfinlayson 5 hours agorootparentprevValve ported Source to 64-bit Windows in 2005 (https:&#x2F;&#x2F;vghe.html&#x2F;source-engine.html) but there were bugs, and it seems like they just let it fall by the wayside (I don&#x27;t think it&#x27;s been available since 2006, maybe 2007 at the latest). And yeah, it seems like they&#x27;ve been abandoning Source 1 for a while now. Valve recently hired Joshua Ashton who has been giving Team Fortress some love but it seems like the focus is well and truly on Source 2 now. reply jamesfinlayson 3 hours agorootparentedit: https:&#x2F;&#x2F;vghe.net&#x2F;source-engine.html reply xsmasher 15 hours agorootparentprevThey&#x27;re old codebases that may use libraries that are 32-bit only for things like sound, video playback, etc. reply jamesfinlayson 5 hours agorootparentYeah probably this - Valve experimented with a 64-bit Source engine in around 2005 but that didn&#x27;t last long, and engine licensees have since added 64-bit support (https:&#x2F;&#x2F;vghe.html&#x2F;source-engine.html). Not sure where the trickiness lies but certainly in 2005 Source was very different (no Bink video, only DirectX 9 renderer etc). reply jamesfinlayson 3 hours agorootparentedit: https:&#x2F;&#x2F;vghe.net&#x2F;source-engine.html replynickzelei 14 hours agoprevThis makes me sad.I occasionally get the itch to surf or kz and boot up CSGO on my M1 Mac to do so.It&#x27;s just more convenient for me to do this on my laptop, rather than to slug over to my desk to use my windows desktop that I rarely boot up.I tried doing the same with CSS, but because it&#x27;s 32bit, it is no longer supported on macOS.I&#x27;m assuming this is a similar feeling that Overwatch players went through when their game got replaced with something that they no longer enjoy.Obviously Valve ran the numbers and I&#x27;m clearly in the minority, but man this just doesn&#x27;t feel good. There will be more Mac players if more games support it, so this feels like a step backwards and is a negative feedback loop (just like others in this thread are also saying.) reply Yeul 14 hours agoparentDon&#x27;t blame developers Apple clearly never gave a shit about supporting a gaming friendly ecosystem. It&#x27;s their way or the highway. reply nickzelei 14 hours agorootparentSure, it might be Apple - and maybe reading between the lines that is why Valve dropped the support. However, it reads (to me) as not enough people are playing the game on the mac so they are dropping support. To me that doesn&#x27;t read as being too hard for them to support mac. If it was easier, maybe it wouldn&#x27;t be a huge deal for Valve to continue supporting it, but that isn&#x27;t clear to me.I&#x27;m not a game developer though and don&#x27;t closely follow this stuff, I&#x27;m just a person who occasionally likes to play CS on my macbook and can no longer do so.Note: I am aware of Apple not generally being very supportive of games on their platform. It&#x27;s always been that way. reply ynx 12 hours agorootparentApple doesn&#x27;t support Vulkan, obviously doesn&#x27;t support DirectX, and barely supports OpenGL. Metal is a different beast, and the adapter layers are not adequate.It takes extra effort, and fairly deep and unique knowledge of rendering, shaders, and compilation to support Metal, and even engines which do (Unity, Unreal) face some amount of difficulty in doing so well.Apple&#x27;s strategy of giving the middle finger to graphics APIs is patched over poorly by their partnership with Unity, and the fact that lower-end games can get away not needing a crazy amount of compatibility...but it&#x27;s still on Apple to do better. reply Wowfunhappy 11 hours agorootparentThere is a robust and widely-used Vulcan → Metal compatibility layer. It hurts performance, and that sucks and is absolutely Apple&#x27;s fault, but it does mean developers can port games to Mac without worrying about Metal. reply kimixa 11 hours agorootparentIt&#x27;s also a massive PITA trying to debug or profiling performance on it - no native tools work right.While \"ok\" for porting an existing game as a secondary platform, you don&#x27;t want to be using it for primary development. And that will be why it will remain (at best) a second class citizen. reply Wowfunhappy 10 hours agorootparentYes, I want to be very clear GP was in response to the argument \"Valve can&#x27;t port games to macOS because macOS doesn&#x27;t support cross-platform graphics APIs\". Well, no, there is a compatibility layer, performance isn&#x27;t stellar but if the alternative is your game doesn&#x27;t run at all, you can take the hit.Apple should not be happy about this outcome. It doesn&#x27;t help them build some kind of walled garden, it just makes their hardware perform worse. They should add Vulkan support! reply johnnyanmac 10 hours agorootparent>there is a compatibility layer, performance isn&#x27;t stellar but if the alternative is your game doesn&#x27;t run at all, you can take the hitThey don&#x27;t make many games these days, but Valve tends to be a studio that prides itself on performance. It can very much be \"this game runs well or we don&#x27;t use that platform at all\" for them, compared to other AAA studios happy to take the PR hit for a quick buck. reply Guvante 7 hours agorootparentprevReleasing a bad port as a AAA studio is a bad idea as everyone focuses on the problems.Apple refusing to update graphics drivers outside of major OS updates doesn&#x27;t help... reply duped 7 hours agorootparentprevThe only story I&#x27;ve heard of people supporting Metal is by supporting wgpu.Well and, whatever embarrassing game that Apple will trot out at their developer conferences 2-3 years after it was released and claim Macs are built for gaming. reply Shekelphile 10 hours agorootparentprevSource 2 already has rendering backends for d3d9, 11, opengl and vulkan. There&#x27;s no reason why they can&#x27;t support metal besides a refusal to task someone to it. reply littlestymaar 10 hours agorootparentLack of users means lack of budget to pay for the said “someone”.Also, there&#x27;s no reason why Apple can&#x27;t support OpenGL properly either, there “no reason besides the refusal to task someone to it”. reply Shekelphile 10 hours agorootparent> Also, there&#x27;s no reason why Apple can&#x27;t support OpenGL properly either, there “no reason besides the refusal to task someone to it”.They do. OpenGL is deprecated, but still supported. If it wasn&#x27;t supported at all it would not have worked on apple silicon macs. reply littlestymaar 9 hours agorootparentNote that I said “supported properly” above. In addition to the very high number of bugs in Apple&#x27;s implementation, the fact that it is officially deprecated sends a message to anyone wanting to do new development on MacOS “don&#x27;t invest in OpenGL now as we could pull the rug under your feet” and it&#x27;s done on purpose to push developers to Metal.So now they can only blame themselves if they&#x27;re losing games because the developer doesn&#x27;t want to invest in Metal either. reply Shekelphile 9 hours agorootparent> In addition to the very high number of bugs in Apple&#x27;s implementationMany of those &#x27;bugs&#x27; remain unfixed to maintain backwards compatibility, not because Apple doesn&#x27;t want to fix them.> the fact that it is officially deprecated sends a message to anyone wanting to do new development on MacOS “don&#x27;t invest in OpenGL now as we could pull the rug under your feet” and it&#x27;s done on purpose to push developers to Metal.As it should be.> So now they can only blame themselves if they&#x27;re losing games because the developer doesn&#x27;t want to invest in Metal either.There&#x27;s nothing to &#x27;invest&#x27; in. Metal is by far the simplest low level graphics API out of all I&#x27;ve used and it would not be hard for valve to write a source 2 rendering backend for it (few days&#x2F;weeks at most for an experienced dev). There&#x27;s plenty of AAA games being properly ported to apple silicon now and in a year or two there will be many more. Valve intentionally withdrawing themselves from the ecosystem is purely out of spite. reply phone8675309 7 hours agorootparent> it would not be hard for valve to write a source 2 rendering backend for it (few days&#x2F;weeks at most for an experienced dev).Put your money where your mouth is and write one. Just write one. If it&#x27;s that easy, do it.Which is more likely - a company that dumps tons and tons of resources into Linux, Wine, DXVK, and put out the Steam deck didn&#x27;t do this supposedly easy thing because they just don&#x27;t care or could it be that they don&#x27;t have the talent in house and wouldn&#x27;t recoup their investment by doing it? reply Shekelphile 7 hours agorootparent> Put your money where your mouth is and write one. Just write one. If it&#x27;s that easy, do it.I can&#x27;t because source 2&#x27;s source code isn&#x27;t available.> Which is more likely - a company that dumps tons and tons of resources into Linux, Wine, DXVK, and put out the Steam deck didn&#x27;t do this supposedly easy thing because they just don&#x27;t care or could it be that they don&#x27;t have the talent in house and wouldn&#x27;t recoup their investment by doing it?They don&#x27;t have the talent? That&#x27;s funny, considering Valve ported all their games to macOS many years before they ported to linux. reply littlestymaar 2 hours agorootparent> They don&#x27;t have the talent? That&#x27;s funny, considering Valve ported all their games to macOS many years before they ported to linux.Using OpenGL, not metal, thought. There&#x27;s a good chance that they don&#x27;t have a single developer knowing metal.Sure they could learn, but then there&#x27;s the next part of the sentence you&#x27;re responding to: it&#x27;s unlikely they wouldn&#x27;t recoup their investment by doing it. reply phone8675309 4 hours agorootparentprev> I can&#x27;t because source 2&#x27;s source code isn&#x27;t available.Perhaps then refrain from telling everyone how easy it as when you have no idea what’s going on in the code.> That&#x27;s funny, considering Valve ported all their games to macOS many years before they ported to linux.Are those people still at the company? Either they are not or they’re doing a piss poor job at advocating for their platform. reply Shekelphile 4 hours agorootparent> Perhaps then refrain from telling everyone how easy it as when you have no idea what’s going on in the code.I&#x27;ve been reverse engineering source engine games and modding them on and off for the last ~15 years, I almost certainly know more about it than you. reply littlestymaar 3 hours agorootparent> I&#x27;ve been reverse engineering source engine games and modding them on and off for the last ~15 years> I can&#x27;t because source 2&#x27;s source code isn&#x27;t available.No contradiction at all here… replylittlestymaar 3 hours agorootparentprev> Many of those &#x27;bugs&#x27; remain unfixed to maintain backwards compatibility, not because Apple doesn&#x27;t want to fix them.Backwards compatible kernel panics is a interesting idea for sure…Also, talking about backwards compatibility when discussing about Apple is pretty ironic as well.> As it should be.Can&#x27;t complain if Valve don&#x27;t follow them on that then.> There&#x27;s nothing to &#x27;invest&#x27; in. Metal is by far the simplest low level graphics APIThat doesn&#x27;t change the fact that porting the game engine to it is still a massive undertaking. It doesn&#x27;t depend on the graphic API itself that much, but more about the size and complexity of the rendering engine.Valve isn&#x27;t stupid, if it was profitable to do so, they&#x27;d do the investment anyway, but do not because it doesn&#x27;t make financial sense.Apple isn&#x27;t entitled a free port of popular games to their walled garden. Developers may do it if it&#x27;s profitable, but that&#x27;s it. If Apple wants to bootstrap the gaming use case, then it&#x27;s on them to make an effort.> Valve intentionally withdrawing themselves from the ecosystem is purely out of spite“Boohoo stupid Gabe does that out of pure naughtiness.”I hope you realize how childish your statement is. replykllrnohj 7 hours agorootparentprev> However, it reads (to me) as not enough people are playing the game on the mac so they are dropping support. To me that doesn&#x27;t read as being too hard for them to support mac.I think you&#x27;re missing the context here that CSGO was just replaced with CS2, and CS2 is an entirely new engine (it&#x27;s Source 2 based whereas CSGO was still Source 1)So this is a switch from a DX9 engine from ~15 years ago to a modern, Vulkan-focused actively developed engine. reply GeekyBear 13 hours agorootparentprevIs this the same Apple whose new iPhone SOC has hardware ray tracing?Apple has a gaming strategy. iPhone makes developers too much money to ignore, and since adopting in-house silicon across all product lines their handhelds, tablets, laptops and desktops all use the same APIs and CPU&#x2F;GPU cores.If you target iPhone, there is not much additional work to add a keyboard&#x2F;mouse&#x2F;gamepad UI to target the rest of their ecosystem.We&#x27;ve started seeing Console games get ported to iPhone, which I don&#x27;t think anyone was expecting a couple of years ago.> Resident Evil Village on iOS Is Legit. I Forgot I Wasn&#x27;t Playing on a Consolehttps:&#x2F;&#x2F;www.cnet.com&#x2F;tech&#x2F;gaming&#x2F;resident-evil-village-on-io... reply asdff 12 hours agorootparentIt has ray tracing with touch screen controls for the masses to play a reskinned bejeweled with banner ads. It&#x27;s not a real gaming platform. No keyboard and mouse support. No first party controller. They even took away 3d touch that would have helped a lot with the haptics of touch screen controls. reply mortenjorck 11 hours agorootparent> a reskinned bejeweled with banner adsI&#x27;m not sure this is a fair portrayal of gaming on iOS.Now, a reskinned Bejeweled with in-game currency IAPs – that&#x27;s more like it! reply lilyball 11 hours agorootparentprev> No first party controllerWhy does it matter if Apple makes their own controller? iOS&#x2F;iPadOS supports other popular controllers and the Apple Store even sells multiple controllers (including the PS5 controller). reply GeekyBear 11 hours agorootparentAdding built in support for a user&#x27;s existing controllers, (XBox, PlayStation or Nintendo) to iOS devices and Macs is another one of those things that debunks the \"Apple doesn&#x27;t care about gaming\" conspiracy theory. reply zarzavat 7 hours agorootparentApple once argued in court that they ought to be allowed to kick out all games built on Unreal engine by third party developers from their platform.Imagine you have developed a game, it is selling well, and one day you find out that the game engine you are using has been banned for disobedience and Apple wants you to rewrite your entire game in a different engine.Apple doesn’t just not care about games, they despise game developers with a burning passion.https:&#x2F;&#x2F;arstechnica.com&#x2F;gaming&#x2F;2020&#x2F;08&#x2F;microsoft-backs-epic-... reply ehnto 4 hours agorootparentIt&#x27;s all a bit moot. Gaming is an ecosystem, and MacOS is not a big part of it. Even if Apple supported all the things it needed to, developers still wouldn&#x27;t build for it, it&#x27;s too small a segment.Apple also has no buy in for the segment of gaming people wish it supported. Where SteamOS and Windows both have first party AAA game development studios. reply strken 10 hours agorootparentprevHow exactly does not caring (much) about gaming require a conspiracy? It should be the default state for the manufacturer of a computing platform whose main competitor has already cornered the gaming market. reply johnnyanmac 10 hours agorootparentBecsuse this is the gaming community and we can&#x27;t speak subtly, apparently. There&#x27;s definitely some spectrum between \"does not support any games\" and \"full on dedicated gaming machine\" and given the efforts of Google&#x2F;Apple they are certainly somewhere in between. reply fomine3 9 hours agorootparentprevIt&#x27;s proven that optional controller means there&#x27;s nothing. reply johnnyanmac 10 hours agorootparentprevI mean, IOS supports K&M and oodles of Bluetooth controllers, including console ones. Dunno why you want Apple to sell you $150 iController anyway that locks you into their walled garden3.I think we&#x27;re well past the point where phones can&#x27;t play \"real games\". If you don&#x27;t want to do that in lieu of preserving battery, that&#x27;s understandable. But at this point mobile has usurped what we used to called handheld gaming.I&#x27;d hope by now we&#x27;d dig deeper and ask questions like \"why isn&#x27;t there a steam for IOS&#x2F;android\"? The answer is obvious for IOS and I hope future regulations help to allow alternative stores, but android seemed like a very obvious void for years. reply lmm 6 hours agorootparent> I think we&#x27;re well past the point where phones can&#x27;t play \"real games\". If you don&#x27;t want to do that in lieu of preserving battery, that&#x27;s understandable. But at this point mobile has usurped what we used to called handheld gaming.Nah. Anyone gaming seriously will still take a Switch because even if their phone theoretically has more horsepower, in practice it&#x27;s just not as good a gaming platform.> I&#x27;d hope by now we&#x27;d dig deeper and ask questions like \"why isn&#x27;t there a steam for IOS&#x2F;android\"? The answer is obvious for IOS and I hope future regulations help to allow alternative stores, but android seemed like a very obvious void for years.Amazon ran an alternative app store for a while, and were pushing the gaming angle on it pretty heavily, but they got rid of it. I don&#x27;t know exactly why, but my naive explanation would be that no-one was spending money there because mobile games are bad and not worth paying for (or perhaps just that it&#x27;s a lemon market and while there are good proper games on mobile it&#x27;s impossible to distinguish them from the bad ones until you&#x27;ve bought it). reply psd1 10 hours agorootparentprevI don&#x27;t believe you can make money on F-droid, and Google is unlikely to tolerate you turning your Play Store app into a seondary app store (or if they did tolerate it, they might switch at any moment). reply johnnyanmac 9 hours agorootparentWell yea, F Droid is made for mostly open source games and apps. Huge deal breaker for games using proprietary engines (since the lions share is on Unity) and doesn&#x27;t want to make it&#x27;s assets open source.This theoretical \"steam of Android\" would work similarly to how desktop steam works. Allows sale of proprietary premium games, Bans f2p games in the beginning (inevitably opening up once it&#x27;s culture is established), does some QA to ensure certain features (e.g. Input, compatibility, no viruses, etc.), has discovery algorithms for consumers, user reviews, forums, etc. It could even allow adult games like Steam, but it may also ban those for a while to establish culture (the porn black hole is a real, scary phenomenon).>Google is unlikely to tolerate you turning your Play Store app into a seondary app store (or if they did tolerate it, they might switch at any moment).I think it&#x27;d be fine in the beginning if you banned f2p apps (which most of google&#x27;s revenue comes from), but yes. I&#x27;d wonder how big it&#x27;d get before we get the next Apple v. Epic debacle over such matters. reply jahsome 9 hours agorootparentprevI too arbitrarily define very narrow parameters for broad terms based on my personal preferences. reply GeekyBear 12 hours agorootparentprev> It has ray tracing with touch screen controlsAgain, Apple uses the same GPU cores across all it&#x27;s hardware platforms. It will have hardware ray tracing on handhelds, tablets, laptops, desktops as well as on their upconing AR&#x2F;VR platform as the SOCs are updated. reply dolni 11 hours agorootparentThat might be useful if iPhones and iPads weren&#x27;t gimped platforms.There is no Steam (let alone any other game store) on iDevices. Just Apple&#x27;s store.As a consumer, I don&#x27;t wanna pay for a game on Apple&#x27;s store just so I can have a shitty iPhone experience. Especially when nearly every other game I have is on Steam and I can play on whatever device of mine I want. reply johnnyanmac 10 hours agorootparent>Especially when nearly every other game I have is on Steam and I can play on whatever device of mine I want.Except for Mac, apparently. Not much android support either.I&#x27;d say we can&#x27;t truly reach \"whatever device I want\" until we at least cover the 3 major desktop platforms, the 2 major mobile platforms, and the 3 major console platforms. Until then we are all making compromises when buying a game. reply GeekyBear 10 hours agorootparentprevSteam is just another DRM platform. reply brnt 10 hours agorootparentIt isn&#x27;t &#x27;just another&#x27; DRM platform. It&#x27;s in a league of its own with the efforts create a virtual OS-platform to run games indefinitely. That&#x27;s neither technically nor functionally trivial. reply TheDong 9 hours agorootparent> virtual OS-platform to run games indefinitelyThey encourage windows games to bundle old and insecure dlls, which the games generally did anyway. On windows, they don&#x27;t really do anything here, it&#x27;s mostly just that windows provides quite a bit of backwards compatibility anyway.On linux, they do a bit more. They ship a hacked up copy of various libraries from ubuntu 12.04, mostly without security patches, and have the games use those, calling it the \"steam runtime\" despite it really being \"ancient ubuntu libraries\". They reduce the security of your machine, and I would only play steam games on a burner machine you don&#x27;t login to your bank accounts on. reply tapoxi 4 hours agorootparentI mean you&#x27;re not wrong, but games need a stable set of libraries to target. This comment implies they haven&#x27;t updated those in a decade, but they have releases based on Debian Buster and Bookworm https:&#x2F;&#x2F;gitlab.steamos.cloud&#x2F;steamrt&#x2F;steam-runtime-tools&#x2F;-&#x2F;b... replyyieldcrv 9 hours agorootparentprevEvery single metric on the business side supports those “not real gamers” being worth far more than the self identified gaming enthusiasts that balk at paying more than $60 for a AAA title for 30 years straight reply rowanG077 9 hours agorootparentI have never seen anyone complain about $60 for an AAA game. Well unless it turned out bad, but the price is not the issue then. reply yieldcrv 7 hours agorootparentright, they complain about anything greater in price, its heavily undervalued for the value it provides reply snickerbockers 9 hours agorootparentprev>their ecosystemand it truly is their ecosystem, because they refuse to support vulkan or OpenGL.>which I don&#x27;t think anyone was expecting a couple of years ago.for sure, it&#x27;s surprising to see a real AAA game get ported to phones instead of the usual cashgrab gacha&#x2F;social garbage. on a technical level id be more impressed if it wasn&#x27;t a game that already has to support consoles that came out in 2013. reply GeekyBear 9 hours agorootparent> id be more impressed if it wasn&#x27;t a game that already has to support consoles that came out in 2013Neither the PS5 nor Resident Evil Village came out in 2013.>Resident Evil Village on iOS is a remarkable sight to see. It&#x27;s hard to tell any difference between playing the game on a console. From the accurate textures on the characters to the lighting and shadow effects, the game looked as good as it did as when I played it on the PlayStation 5 back in 2021.https:&#x2F;&#x2F;www.cnet.com&#x2F;tech&#x2F;gaming&#x2F;resident-evil-village-on-io... reply snickerbockers 8 hours agorootparent>Neither the PS5 nor Resident Evil Village came out in 2013.I don&#x27;t understand why you&#x27;re replying to me with irrelevant facts?PlayStation 4 and Xbox One came out in 2013. Resident Evil Village came out on Playstation 4 and Xbox One on 2021. Unlike wine, video game consoles do not improve with age.>Resident Evil Village on iOS is a remarkable sight to see. It&#x27;s hard to tell any difference between playing the game on a console. From the accurate textures on the characters to the lighting and shadow effects, the game looked as good as it did as when I played it on the PlayStation 5 back in 2021.im not surprised that a layperson is unable to discern between the version of re8 he&#x27;s playing now and the version of re8 he says he played two and a half years ago. It&#x27;s also completely irrelevant to the fact that the most impressive game on iphone is a port of a game that also came out on PS4 and Xbox One. reply hotstickyballs 8 hours agorootparentprevDoes Nintendo or PlayStation support Vulkan or OpenGL? reply remexre 6 hours agorootparentThe VK_NN_vi_surface extension seems to imply that Nintendo does. reply snickerbockers 8 hours agorootparentprevi wouldnt know, i don&#x27;t have access to their SDKs.consider this: Microsoft at the height of its monopoly in the 1990s could have ditched OpenGL support to lock developers into the DirectX ecosystem but they didn&#x27;t. reply phone8675309 7 hours agorootparentprevNo, but both of them have at least an order of magnitude (maybe two) more video game players on their flagship platforms (Switch and PS5) because it&#x27;s solely for playing games, and the engines that are used have tooling to directly support the consoles.Not so for macOS. reply kllrnohj 7 hours agorootparentprev> Is this the same Apple whose new iPhone SOC has hardware ray tracing?IMG has had raytracing in powervr for like a decade, and they just did another \"most advanced ever!!!\" thing a few years ago ( https:&#x2F;&#x2F;www.imaginationtech.com&#x2F;news&#x2F;imagination-launches-th... )But it&#x27;s mobile and so it&#x27;s way too slow to be useful, so nobody cares. That&#x27;s 99% likely to be the case with Apple&#x27;s newest offering, just like it is with Qualcomm&#x27;s and ARM&#x27;s (that&#x27;s right, literally every mobile GPU offers this now - Apple is one of the masses here)Look at how hard it hits consoles and PCs, it&#x27;s utterly pointless on a 6\" display with a 5w SoC reply otikik 13 hours agorootparentprevThat’s not the gaming department, that’s the gambling-but-with-extra-steps department. reply eindiran 13 hours agorootparentThe skinnerbox whaling department; now with built-in hardware support! reply johnnyanmac 10 hours agorootparentprevIf you want a \"gaming department\" you should try to show demand for premium games when they come about. But we failed 12 years ago to do so, so I&#x27;m not surprised to see history repeat itself. reply makeitdouble 10 hours agorootparentIf the gaming support strategy is \"we&#x27;ll try once and completely give up if it isn&#x27;t immediately a wild success\", the chances to succeed are pretty small.That&#x27;s not how you get the Xbox, you&#x27;ll barely get a Stadia shaped hole if you give it a fair shake. reply johnnyanmac 9 hours agorootparentIt&#x27;s more like they tried a few times and realized they have no way to sell a game given the absurd amounts of piracy. 2010 (aroind the time of Infinity Blade) to 2013 was the golden age of premium games. By 2015 almost every game pulled out or went the f2p route.This wasn&#x27;t some 2 year flash in the pan like Stadia, I could still name a dozen premium games from this time I greatly enjoyed. But if Zenonia 2 had 70% piracy rates and the ftp Zenonia 3 (or maybe 4) made more money in a day than Zenonia 1+2 combined, what you going to do?(note these figures are made up, I don&#x27;t know the Zenonia piracy rates nor sales. But 70% is the average Android game piracy rate in 2015). reply makeitdouble 7 hours agorootparentOn the main point, yes, piracy on PC&#x2F;mac was a huge issue.But then, Windows PC were in the same boat. Looking from the sidelines, Windows gave publishers a lot more freedom in implementing anti-pirating (super invasive) features, and when push came to shove Microsoft stepped in to add OS supported anti-piracy features that the games could hook into (Microsoft using the same system for its own games).I think we can at least blame Apple for doing nothing in a situation where a whole group of developpers are stuck with an issue that could be alleviated with the platform&#x27;s help.Irrelevant to the point, but Stadia was 3 year and half (nov 2019 start, to the death announcement on sept 2022, and actual shutdown on jan 2023). So basically the same span than the 2010 ~ 2013 miracle for Apple.I forgot a lot about these early days (was absolutely not into PC gaming), for people in the same boat a nice refresher:https:&#x2F;&#x2F;www.cultofmac.com&#x2F;73192&#x2F;why-2010-was-the-year-mac-ga...The fun part being that 2010 span was triggered by Valve, so from beginning to end Apple just didn&#x27;t do anything. reply GeekyBear 8 hours agorootparentprev> 2010 (aroind the time of Infinity Blade) to 2013 was the golden age of premium games. By 2015 almost every game pulled out or went the f2p route.I remember this too.We had a period with plenty of premium pay in advance games, then a period of games with ads that you could pay to remove, then the companies decided that the most reliable monetization strategy in the face of widespread theft was microtransactions. replyrandomluck040 13 hours agorootparentprevIn case you mean the games on iPhone on the App Store, they generate revenue for Apple via in game transactions and not by game sales necessary. What people typically play on PCs are not the same games. Most of the games, as you can see with CS2, aren’t even available on iOS or macOS which is a problem. Outside of that I think you can run the games for iPhone on Mac as well but even then optimisation is necessary. reply GeekyBear 12 hours agorootparent> they generate revenue for Apple via in game transactionsOnce upon a time, there were many iOS games that you paid for up front with no micro-transactions at all, for instance, XCom was about $20 a decade ago.I have a feeling the console game ports will be pay up front, just like they are on a console. reply Clamchop 12 hours agorootparentprevI think Apple&#x27;s angle on this either should be or already is synergies. Because of AI and their nascent push into AR&#x2F;VR, it appears to me at least that promoting gaming is one worthwhile prong in the campaign.To keep investment in the platform strong, they stand to benefit from stabilizing and recovering from some of their NIH brain damage.Will the fixation on collecting their cut come to an end? Time will tell. reply arvinsim 1 hour agorootparentprevApple&#x27;s \"gaming\" strategy is a lot of microtransaction riddled games while they get 30% cut.Everything that doesn&#x27;t follow that pattern gets the boot. reply baby 13 hours agorootparentprevI&#x27;m sorry but mobile gaming is a whole different \"gaming\" world. It&#x27;s often not the same playerbase. reply johnnyanmac 10 hours agorootparentI&#x27;m assuming they are not trying to appeal to your parents playing bejeweled when showcasing Resident Evil 8.Will it work? No clue. Historically premium games had absurd piracy rates which is why most premium games pulled out and why F2P games with servers holding to value rose. But IOS always had lower piracy rates than android, so Apple has a better foothold for trying to re-appeal to console games studio than Google. reply 93po 5 hours agorootparentprevmore direct link to actual gameplay instead of a single screenshot on cnet: https:&#x2F;&#x2F;youtu.be&#x2F;7LHJPSVR4Ek?t=135 reply fomine3 9 hours agorootparentprevApple cares games, in a Silicon Valley way, like Google. I feel they don&#x27;t love games but accidentally become a big player. reply naikrovek 12 hours agorootparentprev> Is this the same Apple whose new iPhone SOC has hardware ray tracing?it&#x27;s the same apple who said \"please bring games to our system\" followed immediately by \"opengl is going away. also, fuck you.\" reply Alupis 9 hours agorootparentprevPeople severely underestimate how powerful the gaming economy is.Gaming is what made Windows dominate personal computing - not Word, Excel or Powerpoint.Gaming is what drives new hardware development, is what drives new personal computer ownership, and is what drives operating system preference.For everyone else - a 10 year old computer does the job just fine... and it doesn&#x27;t really matter what OS it happens to run.Imagine if Apple had made it a point to dominate gaming too. Suddenly the folks spending $2-5K on a top of the line gaming machine would be forking it over for the latest generation of $5k Apple Desktops. reply Mengkudulangsat 9 hours agorootparentIf it were not for clunky game support, the day of the Linux desktop would have arrived decades ago. reply phone8675309 7 hours agorootparentIt is a TON better than it was twenty years ago when I first played with no longer dual booting and running only Linux.Proton plays almost all of the games I care about, and the games it doesn&#x27;t play I can either reboot to Windows for or I can run a Windows VM with GPU passthrough.So much easier than chasing patches to Wine and bashing your head against the wall to get even simple things running. reply matteoraso 8 hours agorootparentprev>Gaming is what made Windows dominate personal computing - not Word, Excel or Powerpoint.Being the default OS is what made Windows dominate personal computing. The average person doesn&#x27;t think about what OS they use at all, even when they&#x27;re thinking about getting a Macbook. reply fransje26 2 hours agorootparentI suggest you go an read some of the blog post by Alex St. John, one of the original DirectX developers, explaining why they were task with creating DirectX in the first place..And anyway, the the Windows dominance was cemented way, way before Macbooks even existed, at a time when people were very much thinking about the OS they were using.. reply enlyth 9 hours agorootparentprevAgreed, I&#x27;m no stranger to Linux, and also use a MacBook for work, but when I come home I just want to play a bit of Diablo 4 or whatever and chill. I would have switched years and years ago.Got hooked on Microsoft as early as the DOS days by playing games as a kid. They earned a customer for life indirectly there. Maybe Adobe helped a bit too. reply sandworm101 9 hours agorootparentI gave up windows the day i learned that minecraft ran perfectly well on linux. More than a decade later, i have never felt the need to go back, but it all depends on the types of games you play. reply mr_toad 9 hours agorootparentprevIBM didn’t care about gaming, and neither did the early PC clone manufacturers. And early IBM compatible PCs were pretty shitty gaming machines compared to other PCs. And game companies used eye watering hacks to make their games work within the limits of the platform. PC dominance in gaming came after PCs became popular. reply NERD_ALERT 8 hours agorootparentI don’t see how this is relevant. The PC gaming market was niche and immature at this point in time and developers were trying to gain traction. Today the market is massive and virtually all of the users and money is in the PC platform. There is no reason at all that Valve would waste money and developer time on supporting a platform that virtually nobody is going to use now or at any point in the near future. Valve is already doing great work with Proton and Steam Deck to get Linux gaming on par with Windows. reply shpx 13 hours agorootparentprevIs it also Apple&#x27;s fault that Steam takes 34 seconds to start up on my Mac with four 3.2 GHz cores? reply jshier 8 hours agorootparentSteam and Battle.net are so annoying here. Three years on and what are essentially web browsers still haven&#x27;t be compiled for Apple Silicon. Bnet keeps getting worse and I don&#x27;t even run Steam anymore, there&#x27;s no point. reply kevingadd 12 hours agorootparentprevIt boots fast on Windows and Linux so probably? reply alpaca128 11 hours agorootparentOr Valve cares about it as much as about Counter-Strike on Mac? There is no technical reason for it to be that slow. reply littlestymaar 10 hours agorootparentprevDoes that run on ARM instruction directly or through the x86 emulation layer? reply shpx 6 hours agorootparentThe latter reply hexo 12 hours agorootparentprevyes reply olliej 4 hours agorootparentprevYou mean apple should only use x86 cpus still? or it should only use Nvidia or AMD gpus? Or maybe it should implement Direct3D? (oh wait, they&#x27;ve done that) reply MenhirMike 14 hours agoparentprevThe thing that makes this a bit more frustrating is that Apple actually kinda has a solution, the game porting toolkit (Essentially Wine&#x2F;Proton for macOS). Except that they don&#x27;t just make it a component of the system for end users, and don&#x27;t allow developers to use it as a runtime layer in their app (so Developers can&#x27;t go the proton route and actually ship a Windows binary running on top of the game porting toolkit). So it&#x27;s reasonable to assume that the toolkit will go away in a few years as well.Oh well, at least it&#x27;s not the first time that Mac gaming has shown to be a dead-end, but without BootCamp, it really feels final. Can&#x27;t even count on half-baked, outdated ports from Aspyr anymore since they got bought by Embracer and are probably trapped in their financial struggles. reply xcv123 10 hours agorootparentCrossover can run Steam on M1. It uses Wine&#x2F;Proton&#x2F;Rosetta and works quite well for many Windows games. reply mrpippy 10 hours agorootparentAnd, CrossOver now includes D3DMetal, the important part from the game porting toolkit. reply lewisedc 13 hours agoparentprevJust incase you didn’t know, cs:go is still available and a lot of surf&#x2F;kz servers are still active.If you want to know how to access cs:go you can check out this vid https:&#x2F;&#x2F;youtu.be&#x2F;A8cWDxqOG2k?si=w4xdxmRntqKLtYg0It’s really simple and I would have wrote the steps here but I’m not at my computer atm. reply arvinsim 1 hour agoparentprev- Apple blocking Nvidia drivers- Apple Silicon not supporting EGPUs- Apple not adapting Vulkan and went with Metal exclusively... reply xcv123 11 hours agoparentprevInstall Crossover then run Steam in Crossover. Works with M1. reply coolspot 11 hours agorootparentOr use Arch BTW, I meant Ashani Linux + Steam reply xcv123 10 hours agorootparentSteam does not run natively on Asahi Linux (no ARM build)You have to use an x86 emulator to run Steam on Asahi.Crossover on MacOS would be faster as it uses Rosetta. reply eikenberry 12 hours agoparentprevPlay on your laptop using Steam&#x27;s remote play, Moonlight or something similar. You run the games on that windows desktop you have stored out of the way (mine is headless as well) but play on your laptop where ever you want. reply lolinder 9 hours agorootparentThis works for some games, but for others I find the latency to be too high, and compression can get pretty wacky with games that have a lot of movement. reply AdmiralAsshat 20 hours agoprevI feel for the Mac users in spirit: Linux users have often received the same answer (\"There aren&#x27;t enough Linux buyers of this game to justify the development&#x2F;support costs of porting.\"). OTOH, Counter-Strike 2 actually has a native Linux build even without Proton, so, this particular title isn&#x27;t a problem for us. reply sevagh 15 hours agoparentMac users are voluntarily in the walled garden of a _trillion dollar company_, it&#x27;s a bit different from Linux. reply syntaxing 14 hours agorootparentI honestly don’t get this rationale though. Why is this different from Linux? It’s like blaming Linux users for using Linux and they should have used Windows instead if you really want to game. reply vntx 13 hours agorootparentHow is it the same? Apple runs a closed ecosystem and has shown it doesn’t care for gaming for a long time so its user should know that by now. Linux is an open platform and the results show. SteamDeck is running on Linux and can run most Steam games. Macs still can’t. Apple users should take responsibility on this one. reply egypturnash 13 hours agorootparentI am now having a brief fantasy of a world where Apple pulls Proton into the OS as a Windows emulation layer for games and starts pushing their changes upstream just like Valve does.I can think of many reasons why it would never happen but it sure would be nice. Not that I haven&#x27;t been voting against Mac games with my wallet for years, I&#x27;ve had a Mac to get shit done with and a rotating set of consoles to play games on since about 2000, and very occasionally bought a point-and-click adventure for the Mac. reply xcv123 10 hours agorootparentprevM1 Macs can run the Windows version of Steam with Crossoverhttps:&#x2F;&#x2F;www.codeweavers.com&#x2F;crossover reply drusepth 13 hours agorootparentprevI think the (implied) distinction is this:* Apple has billions of dollars and a hierarchy of decision-makers who could prioritize the R&D and implementation of making \"gaming on Mac\" a reality* Linux is a distributed, community project without billions of dollars or top-down decision-makers who can unilaterally prioritize making \"gaming on Linux\" a reality reply nani8ot 11 hours agorootparent> Linux is [...] without [...] top-down decision makers who can unilaterally prioritize making \"gaming on Linux\" a realityI&#x27;d say Valve is exactly that. Valve pays many developers their salary who are responsible for making almost all single player games work on Linux via proton (wine, dxvk, vkd3d). Although they do push their changes upstream. reply johnnyanmac 10 hours agorootparentKind of. But I wouldn&#x27;t call Valve a top down decision maker for Linux. Simply a very talented contributor and influencer. And you always need to keep in mind that their work is still in the interest of supporting their proprietary platform. reply johnnyanmac 10 hours agorootparentprevFor the end consumer it isn&#x27;t different. But in a pragmatic sense:1. Apple has multiple teams of well paid engineers to solve any problem that arises. There&#x27;s a lot more money in the game supporting Mac than Linux.2. Apple has been shown to be hostile towards game development and make active decisions to make it harder to port to them, whereas Linix&#x27;s most hostile issues arise from the proprietary nature of games vs. the open source nature of Linux (e.g. Package management, DRM, etc).3. Linux makes concessions on its philosophies for games while Apple makes ultimatums. You can&#x27;t ever trust that your game on Apple will work in 5 years, at least if you were developing in the 2010&#x27;s. Meanwhile there are proprietary ways to deliver your game if you want to launch on Linux (not as sure about DRM but I&#x27;ve heard of solutions that simply haven&#x27;t had mass adoption yet). reply makeitdouble 10 hours agorootparentprevApple as a company actively and consistently gives the middle finger to Nvidia and Epic. For linux it was merely Linus doing so on camera, once. reply psd1 9 hours agorootparentprevLinux has potential that Mac does not - case in point, Steam Deck. reply goosedragons 13 hours agorootparentprevApple has money to actually invest in games and the tech surrounding it. Don&#x27;t think the case is same for Linux. reply AdmiralAsshat 15 hours agorootparentprevAre you suggesting that they are more or less deserving of sympathy? reply sevagh 15 hours agorootparentMuch less, much much less. Imagine paying $4,999 USD for a device from a company with $2.8T market cap and blaming anybody except Apple for the drawbacks of your platform. reply starttoaster 14 hours agorootparentApple&#x27;s absolute top end models might go for $5,000, but then again, have you bought gaming computer components lately? I can&#x27;t imagine I spent far off of $3000 for my gaming computer all things taken into consideration (besides peripherals of course.) I only have not attempted to tally up the cost because I fear it would make me sick. Without actually listing a real drawback to using the platform besides \"software providers decided not to support the platform,\" you come off as one of those drones that mindlessly hate on Apple products only because you don&#x27;t want to pay for them. And to be clear, I&#x27;m not even your supposed opposition, in that I don&#x27;t even own any Apple products besides an iPhone SE (which are notoriously cheaper than flagship Android phones.) I just find the Apple hating bickering to be about as childish as the Playstation vs Xbox console argument.And yes, I consider your comment to be childish bickering primarily because you don&#x27;t have another argument besides \"their product is more expensive than it&#x27;s worth comparing hardware to hardware.\" Apple users understand that they&#x27;re actually paying a premium for the software (the operating system), as well as the designed and usually fetching exterior (which to be fair does look nice, and I tend to prefer Macbook charging ports over Dell&#x27;s old barrel ports.) Which, there is something to be said about an operating system that decided they don&#x27;t want to run Java anymore. I&#x27;ll grant that their software might have merit. Java web start apps are the bane of my enterprise existence. reply Panzer04 8 hours agorootparentYou can get a very competent gaming machine for 1k, especially usd. That gets you apple&#x27;s bottom end laptop, if that.It&#x27;s not comparable. reply starttoaster 7 hours agorootparentYou get a budget gaming machine for 1k these days, if that. The GPU almost steals your 1k budget by itself. Apple&#x27;s new top end Mac Mini goes for $1300 and has a 10 core M2 Pro with a 16 core GPU, 16GB memory, and a 512GB SSD. More than enough to play Counter Strike 2 if Valve decided to support the platform. It&#x27;s not comparable but not as outrageous as people say.But again, you&#x27;re still thinking in terms of money here. Where the problem isn&#x27;t with how much the computer costs. It&#x27;s completely irrelevant, actually. The problem for Mac users is that the software providers in question in this discussion don&#x27;t want to support MacOS, which is a decision totally on them. I have no strong opinions against their decision to not support them if the user base just isn&#x27;t there. There&#x27;s no doubt that there are tons of MacOS users, they just might not be users that intend to game on those computers the majority of the time. But I do feel empathy for Mac gamers, as that was historically the opinion of game studios refusing to support Linux machines. And to be honest, it&#x27;s still the case that Linux users are second class citizens when it comes to games that require an anticheat to run (many online FPS games fall into this category.) None of the big anticheat providers want to port their software to run on Linux. reply out_of_protocol 55 minutes agorootparent> The GPU almost steals your 1k budget by itselfYou can play almost any game on ancient GeForce 1080 (i do) with decent fps, except Starfield and maybe some other poorly optimized titles. On windows, that is.You can also play almost anything on SteamDeck (15W TDP max) on linux ($400 before discounts).You can&#x27;t really play 3&#x2F;4 of game catalog on MacBook Pro M1Pro or newer ($2000+) reply porridgeraisin 7 hours agorootparentprevExactly reply xcv123 9 hours agorootparentprev> Imagine paying $4,999 USD for a device from a companyA high spec Mac Mini is $1299 USD (10 core CPU, 16 core GPU, 16 GB).Anyone paying $4,999 for a Mac Studio&#x2F;Pro is doing media creation for work and doesn&#x27;t give a fuck about games or will have a separate console&#x2F;PC for that. Those are workstations, not for gaming. reply astrange 15 hours agorootparentprevThe platform is fine. It can play Windows games using WINE just like Linux can. reply Alupis 14 hours agorootparentThat&#x27;s... not fine.As great as WINE can be, it&#x27;s not native. Apple has gone out of their way to prevent you from enjoying games on their systems.And who do the users blame? Game makers - not Apple... reply charcircuit 10 hours agorootparentGames are usually not native. They usually create a window and render to the entire viewport insted of using the OS&#x27;s standard UI framework. reply xcv123 9 hours agorootparentUI framework? What about everything else such as file system, networking, audio, controller IO, threads, graphics API, etc? reply charcircuit 8 hours agorootparentWine wraps those APIs. An abstraction over the filesystem is not that different whether it is in the game engine or in wine. reply xcv123 7 hours agorootparentNo shit. That&#x27;s my point. You said \"games are usually not native\". Which games?Not sure what you mean by that. Never mind. reply charcircuit 6 hours agorootparent>Which games?As I said the vast majority of games. For example Minecraft is not a native Windows application in any way. It just creates a window and then renders the entire contents of the window itself instead of using win32 to make an interface. reply xcv123 6 hours agorootparentNo, only a tiny minority of PC games run on the JVM. The vast majority are native and not running on any kind of VM. Even with the Unity engine the games are natively compiled. reply xcv123 6 hours agorootparentprevIf the vast majority of games were on the JVM, then the vast majority of games would run on Linux or Mac without Proton or Wine, wouldn&#x27;t they?That is not the case. Minecraft is probably the only major game that runs on the JVM. reply charcircuit 5 hours agorootparentI didn&#x27;t intend to bring up the JVM. Minecraft Bedrock edition is written in C++ and renders the entire window contents and even has its own UI framework that it uses. reply xcv123 4 hours agorootparentThere&#x27;s way more to it than GUI libraries. That is a native Windows application calling Windows APIs. You can&#x27;t run that Windows executable on Linux without Wine&#x2F;Proton.Even a console application (no GUI) depends on operating system APIs. reply charcircuit 3 hours agorootparent>You can&#x27;t run that Windows executable on Linux without Wine&#x2F;Proton.That&#x27;s like saying you can&#x27;t run Firefox without freetype. Requiring a dependency doesn&#x27;t make you no longer native.>Even a console application (no GUI) depends on operating system APIs.But the same API can be handled by different operating systems or libraries. reply xcv123 3 hours agorootparent> That&#x27;s like saying you can&#x27;t run Firefox without freetype. Requiring a dependency doesn&#x27;t make you no longer native.At this point you&#x27;re just trolling.Take notepad.exe and try to run it on Linux. It wont run because its Windows native. That&#x27;s a native application. Same as 99% of games that aren&#x27;t JVM based. reply xcv123 3 hours agorootparentprevIt creates the window and writes the window contents using Windows operating system APIs. replyastrange 14 hours agorootparentprev> Apple has gone out of their way to prevent you from enjoying games on their systems.How, by having a different OS? Sorry about that I guess.(If it had native Vulkan it wouldn&#x27;t matter. The most effective strategy, the one Microsoft uses, is to buy all the game studios.)> And who do the users blame? Game makers - not Apple...Calling Valve a game maker is a stretch; as a company they&#x27;re famously unwilling to actually make games. See gaben&#x27;s allergy to the number 3. reply Alupis 14 hours agorootparent> How, by having a different OS? Sorry about that I guess.> (If it had native Vulkan it wouldn&#x27;t matter. The most effective strategy, the one Microsoft uses, is to buy all the game studios.)By making their OS easy to target by game makers. There&#x27;s no good excuse - Apple has access to the same graphics pipelines as everyone else. XPlat game engines have boiled it down to mostly a checkbox these days... so where&#x27;s OSX? Apple has a lot of work to do before that&#x27;s a reality.> Calling Valve a game maker is a stretch;You can&#x27;t be serious, are you? Valve&#x27;s titles are among the most popular games in the history of games. They may make most of their money through Steam, but to say Valve doesn&#x27;t make games is ridiculous.That wasn&#x27;t even the point - Apple users will blame the actual studios&#x2F;developers for not supporting OSX when the blame lies at Apple&#x27;s feet.Billions in annual profit, zero f&#x27;s given about gaming on their platform. It&#x27;s a choice - and one Apple users need to comprehend. Apple doesn&#x27;t care. reply astrange 13 hours agorootparent> Valve doesn&#x27;t make games is ridiculousValve is not a normal company; there&#x27;s no hierarchy and they&#x27;re only capable of doing things if someone at the company decides to pay attention to it.Do you remember what happened to TF2? It first degraded into an item trading game, then they abandoned it for years and it was full of bots. There&#x27;s no reason Overwatch and Apex should&#x27;ve replaced it except that they stopped fighting for it. reply Alupis 12 hours agorootparentTF2 was released in 2007... and has over 100k players playing right now as you read this[1].Counter-Strike is still one of the most-played games ever. CS:GO had an average of almost 1 million daily players while AAA Games like CoD Warzone hover around 200-500k.DOTA&#x2F;DOTA2 also rakes it in. They also have many very successful single-player games. Valve is a wildly successful game company - they just don&#x27;t do the \"yearly release\" dance...[1] https:&#x2F;&#x2F;steamcharts.com&#x2F;app&#x2F;440 reply astrange 11 hours agorootparent> TF2 was released in 2007... and has over 100k players playing right now as you read this[1].They fixed it again after people sassed them enough about it, but it was always a better game than Overwatch and there&#x27;s no reason people should&#x27;ve been tricked into playing that.(Though, I don&#x27;t know if the people on right now are actually playing TF2 or just trading hats.) reply hadlock 12 hours agorootparentprevTF2 today is mostly unplayable due to bots and weirdness with their player matching game. I put a lot of hours into TF2 and the experience is almost unrecognizable today. CS:GO was, I think, completely outsourced to a third party. They still produce some game-like artifacts but primarily they&#x27;re the owner and operator of the premier online games store reply Alupis 11 hours agorootparentCS:GO is complicated - it started as a console port of CS:Source by a 3rd party developer, then was taken in-house and transformed into a full stand-alone new Counter-Strike game. So, it was indeed developed by Valve.IDK anything about TF2 - but bots or not, 100k active daily players is nothing to sneeze at for a 16 year old game.They are indeed the premier online game store - yes... but saying they are not a game developer is absurd. They don&#x27;t release a new title every year, but when they do, it&#x27;s a huge hit. reply robertoandred 13 hours agorootparentprevYou think cross-platform game engines don&#x27;t support Metal? They do, and have for a decade. reply Alupis 12 hours agorootparentWho else supports Metal? Oh, that&#x27;s right - only Apple.There&#x27;s more to game support than just graphics API.Apple chooses to make game support on OSX hard - and shocker... you don&#x27;t get games supporting OSX. Who can we blame? Apple...Just like Apple chooses to make Linux kernel support hard on M1&#x2F;M2 and leaves it entirely up to volunteers to make it work. Who do you blame? The Kernel developers or Apple? reply fingerlocks 2 hours agorootparentThe Metal API is heavily documented and Apple provides a plethora of code samples in four programming languages, with literal step-by-step how-to guides on porting from OpenGL to Metal.You can complain that they don’t support third party low-level frameworks, sure. But they definitely make it easy and inviting to support their homegrown solutions reply GeekyBear 13 hours agorootparentprev> If it had native VulkanAside from a subset of Android devices, what platform has Vulkan as the default API?Windows&#x2F;XBox has DirectX. Playstation has GNMX. Macs&#x2F;iOS Devices have Metal. Nintendo uses NVN. reply Alupis 12 hours agorootparentDepends what you mean by \"default\". Windows ships with DirectX, OpenGL and Vulkan support. Call of Duty runs on Vulkan by default, for instance.Vulkan is notable as being new (doesn&#x27;t have legacy baggage OpenGL and DirectX have), is natively cross-platform, and is often more performant than other options for modern games. reply astrange 11 hours agorootparentVulkan is not cross-platform if you include cross-GPU vendor, because it&#x27;s too low level for that. You&#x27;d want to rewrite for different GPUs. reply johnnyanmac 10 hours agorootparentIt&#x27;s cross platform in that if you want, you can write implementations for other platforms. In addition to supporting multiple platforms in its current state. reply GeekyBear 12 hours agorootparentprev> Windows ships with DirectX, OpenGL and Vulkan support.Windows ships with DirectX. Your graphics card&#x27;s software package can add support for other APIs. reply Alupis 11 hours agorootparentGraphics API support is usually provided by drivers - and Windows ships with drivers that support Vulkan. reply GeekyBear 11 hours agorootparentGraphics card drivers are also provided by the Graphics card vendor, not Microsoft. reply littlestymaar 10 hours agorootparentThat&#x27;s true of DirectX as well though… reply GeekyBear 9 hours agorootparentThe difference is that Microsoft is responsible for the DirectX API on Windows but does not have anything to do with shipping OpenGl or Vulkan for Windows. reply littlestymaar 56 minutes agorootparentMicrosoft plays roughly the same role a Khronos (specifies the API, provide conformance test suite, provide an SDK, etc.) but when it comes to actually “shipping” DirectX, Microsoft doesn&#x27;t have anything to do either, it&#x27;s all on the graphic card vendor to ship DirectX drivers. As an example, for a while after its release, many people didn&#x27;t have access to DX12 at all, just because their GPU didn&#x27;t have DX12 drivers.So the situation is much less different between DirectX and Vulkan than you make it sound. replyastrange 13 hours agorootparentprevLinux and Steam Deck, ironically.Though, for Windows it matters more what the GPU vendor wants you to use. Don&#x27;t remember what that is atm. reply out_of_protocol 12 hours agorootparentprevWindows, Linux, SteamOS, Android, Switch reply astrange 11 hours agorootparentNVN is the native API on Switch. reply out_of_protocol 1 hour agorootparentNintendo does support Vulkan as first class citizen on switch, and there high chance this is the reason of so many ports (besides units sold) replyzamalek 14 hours agorootparentprevSo then it should be able to play CS2 via WINE, rendering all of this is moot? reply astrange 14 hours agorootparentYou&#x27;d have to install Steam under WINE too, which does work fine but would be confusing to have two of them. reply kcb 11 hours agorootparentprevExcept without the primary api that makes gaming on WINE viable. Vulkan with DXVK and VKD3D. reply astrange 8 hours agorootparenthttps:&#x2F;&#x2F;github.com&#x2F;KhronosGroup&#x2F;MoltenVKTranslating between rendering APIs is not really the problem. The GPU design is more different than the API is. reply kcb 7 hours agorootparentMoltenVK is not enough and it lacks features that are mandatory for acceptable performance with dxvk&#x2F;vkd3d.For example, VK_EXT_descriptor_buffer is a critical one, and there are more: https:&#x2F;&#x2F;www.khronos.org&#x2F;blog&#x2F;vk-ext-descriptor-buffer. And MoltenVK doesn&#x27;t support Vulkan 1.3. reply vetinari 14 hours agorootparentprev* As long as the games are 64-bit and while Rosetta is still available. reply astrange 14 hours agorootparentNo, Win32 works too. If Rosetta stops being available there&#x27;s always qemu. reply mschuster91 13 hours agorootparentqemu is dog slow, and it&#x27;s barely working enough to get Windows 7 x86 to boot on a M2 (not to mention that the UTM guest tools are mostly broken, so no acceleration anywhere). reply asdff 12 hours agorootparentprevmacbook air is like the same price as a comparable dell reply kelsey9876543 14 hours agorootparentprevAnyone that can afford a professional mac setup already has a gaming setup if they want to play games. reply Razengan 14 hours agorootparentprevIf you&#x27;re judging how much sympathy people \"deserve\" by what they spend their own money on, you deserve even less.Here&#x27;s the most basic thing about purchases: People spend money on what they think is worth it. So the millions of Mac users decided the drawbacks of Windows&#x2F;PC weren&#x27;t worth the money. reply lukeschlather 14 hours agorootparentI think you&#x27;re missing some nuances of the conversation here. A lot of Apple buyers celebrate the fact that Apple intentionally prevents interoperability with large swathes of software. So if someone is complaining that software doesn&#x27;t work on an Apple product, they don&#x27;t \"deserve sympathy\" because they&#x27;ve deliberately chosen a product where that is ostensibly a selling point. reply simplify 10 hours agorootparentWhat? Who is \"celebrating\" that Apple prevents interoperability? Unless you&#x27;re saying that buying a macbook is equivalent to celebrating all of Apple&#x27;s decisions, a clearly false equivalency? reply dpc_01234 14 hours agorootparentprevYes. Absolutely.We are not talking here about people trying to uplift themselves or humanity around them, and get&#x2F;give access to education, Internet, computing, etc. We&#x27;re talking about people who dropped 2 grands to buy into an luxurious anti-freedom walled garden. reply wilg 8 hours agorootparentYes the real enemy is people who have Macs reply phist_mcgee 6 hours agorootparentprevA real &#x27;reap what you sow&#x27; mindset huh? reply xmprt 15 hours agorootparentprevI sympathize with them but I also sympathize with Valve for being put in a position where they have to support an extremely close and opinionated platform that can change at any moment just because a trillion dollar company decides to. reply ankurdhama 5 hours agorootparentprevCare to explain your definition of \"walled garden\"? reply culopatin 14 hours agorootparentprevLinux users are there at gunpoint? reply HansHamster 20 hours agoparentprevIt would be weird for Valve not to support Linux, given that the Steam Deck runs it. They could of course use Proton to run it, but AFAIR (most of(?)) the previous CS versions also had native Linux support. (Not sure about Mac support in the previous releases?) reply milesvp 18 hours agorootparentIt would also be weird to me for them to not have linux ports of their engines after blogging about the performance and bug squashing benefits of doing so for previous game engines. reply Matheus_17 15 hours agorootparentI would be interested to read about that, could you post the source on it? reply milesvp 12 hours agorootparenthttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=4327908This is one of the threads I remember reading about this. The blog seems to be offline, but you might find some luck with the wayback machine.https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20190323200813&#x2F;http:&#x2F;&#x2F;blogs.valv...In any event, around this time they found a lot of optimizations porting to linux that they then back ported to the windows client. This was 10 years ago, so the zeitgeist in the company may have changed, but I&#x27;ve certainly taken to heart the idea that you should be cross compiling to at least one different target whenever you can to improve your engineering. The earlier you start the cheaper the cost of doing so as well. (This isn&#x27;t the only company doing so at the time talking about gains of cross compilation either, and not just games) reply mustacheemperor 16 hours agorootparentprevThere was a Mac version of CS:GO, so because that game has been replaced by CS2, anybody playing it last month has lost it now. reply RockRobotRock 16 hours agorootparentIsn&#x27;t that kind of messed up? CS:GO used to cost money, some users paid for something they can&#x27;t play anymore. reply Fornax96 10 hours agorootparentThis happens all the time. I used to be able to play Rocket League natively on my Linux PC. Then Epic bought it and simply pressed delete on the Linux port. reply lights0123 8 hours agorootparentAnd offered full refunds for those users, even if they had bought it five years prior. reply porkbeer 3 hours agorootparentprevSame happened with facepunches Rust reply itishappy 15 hours agorootparentprevKinda? Honestly that&#x27;s just how online games work.CS:GO is over a decade old at this point, and has been free for 5 years. What should we expect? reply hotnfresh 14 hours agorootparentYou can play CS:GO locally against bots. It&#x27;s the only way I&#x27;ve played it, in fact.And I did pay for it. reply BoorishBears 14 hours agorootparentprevExpect that you don&#x27;t lose access to a game you have?They didn&#x27;t just release a new game: they literally removed the previous game people had and replaced it with a new, incompatible game.They didn&#x27;t have to do that, they could have made a new game entry and earned their new install base by... putting out something that was better than the last entry and letting the players decide?But much easier to just remove the last entry.It&#x27;s the kind of nonsense people normally jump at developers for, but because Valve is apparently still the darling of the gaming world so people seem to be excusing it. reply porkbeer 3 hours agorootparentThe previoys game was not removed, you can go into settings and pick the old version iirc. reply smoldesu 13 hours agorootparentprevApple kinda set the tone here with their depreciation of 32-bit execution modes and OpenGL, plus their general hostility towards open or cross-platform APIs. By removing support for the old way of doing things and refusing to meet in the middle with developer frameworks, you end up with a platform that benefits nobody but the license-holders. That&#x27;s what happened with DirectX, an actually useful API that Valve invests millions of dollars in spurning anyways. Why would they kiss the Metal ring?CS:GO still launches now that CS2 exists. Never mind the fact that the source code leaked, you can still install CS:GO from Steam or run a cracked copy without VAC support. Valve supported unofficial servers from the get-go, so diehard CS:GO fans on Mac can play any version they like for as long as Apple chooses to support it.> Valve is apparently still the darling of the gaming worldDamn these alternative App Stores and their powerful competition! reply BoorishBears 12 hours agorootparentThis is an insane amount of noise to throw out for such a simple point...Valve made a new game: Instead of releasing it as a new game, they abused a loophole to force said new game onto people&#x27;s devices without them having any say in the matter.In other words, a piece of software they didn&#x27;t ask for is being installed, and the software they did ask for is now hidden behind workarounds._Apple plays no part in any of that: trying to paint not abusing your digital distribution platform... as being equivalent kissing Apple&#x27;s ass is nonsensical to the point of absurdity. reply smoldesu 10 hours agorootparentThis isn&#x27;t ass-kissing, this is me being a pedantic asshole so I can untangle your argument and refute the idea that people \"lose access\" to their game. I&#x27;m not fond of DRM when Apple or Steam does it, but the way Valve depreciated CS:GO is borderline unimpeachable.- Everyone with auto-updating enabled got the new game installed by-default. Nobody lost access to anything but official game servers.- The old game has support for third-party servers on every point release, leaving it perfectly playable today.- Bonus points: A copy of the game&#x27;s source code was leaked, enabling the community to unofficially support new systems if they want.Mac users got the short end of the stick, but I find it hard to demonize Valve over it. The root of this entire complaint is that an auto-update didn&#x27;t behave as it should.Feel free to hate on whoever you want. I&#x27;ve heard a lot of people say that CS2 is bad on it&#x27;s own merits, and that&#x27;s fine too. I just don&#x27;t think auto-updating to a broken build is some vast betrayal of confidence or anticonsumer row. It mostly feels like an example of how MacOS and it&#x27;s users are getting left behind over petty business issues. reply BoorishBears 8 hours agorootparent> I just don&#x27;t think auto-updating to a broken build is some vast betrayal of confidence or anti-consumer rowI think you showed why I&#x27;m having trouble taking you seriously fairly succinctly. replyIntelMiner 15 hours agorootparentprevValve seems to intend on just releasing the original CS:GO as an (unsupported) title with community run servers reply crooked-v 15 hours agorootparentThey could have just... kept the existing game listing as-is (but unsupported) and released CS2 as a separate game listing. reply itishappy 15 hours agorootparentWhy? Not trying to be difficult, but what would they get out of this? reply Philpax 13 hours agorootparentGame preservation, options for people who can’t play CS2, keeping a clear chronology of the game’s history.Replacing CSGO with CS2 is the oddity. reply porkbeer 3 hours agorootparentMost players can just go to betas&#x2F;versions in the steam program prefrences and download the version they prefer and play. replymehlmao 15 hours agorootparentprevYou can still download and play CS:GO. Right click > Properties > Betas. I&#x27;ve used ti for playing a mod with friends that doesn&#x27;t work in CS2 yet. Of course, there&#x27;s less players now. reply charcircuit 14 hours agorootparentprevHalf Life Alyx didn&#x27;t support Linux on release and I had to play it via proton reply surgical_fire 20 hours agoparentprevHere&#x27;s to hoping that with gaming on Linux gaining prominence (thanks in no small part to Wine&#x2F;Proton and Steam Deck), Linux overcomes Windows as the go-to OS for gaming. reply pjmlp 18 hours agorootparentNever going to happen, after all Proton emulates Windows. reply xethos 18 hours agorootparentAnd Windows 10 can emulate the Super Nintendo. But multiple consoles are not as cheap as the Windows box you already have, more annoying to setup or stow, and don&#x27;t have some little QoL addons, like freezing the game.What you&#x27;re emulating is irrelevant. What matters is which provides a better experience to you, the end user. One choice is actively being worked on, while the other (despite under-the-hood perf. improvements IIRC) seems to garner more loathing and disdain with each release. reply pjmlp 14 hours agorootparentExcept AAA game developers no longer care about Super Nintendo. reply belthesar 13 hours agorootparentI think you missed the forest for the trees there. reply pjmlp 3 hours agorootparentLinux folks swearing for \"Linux games\" that are in reality Windows games running on top of a Windows emulation layer are the ones missing the forest from the trees.OS&#x2F;2 says hello. reply porkbeer 3 hours agorootparentWe are literally in a thread where a native AAA release was on Linux day one. Still not common, but not exactly rare. reply pjmlp 2 hours agorootparentAAA games from Valve don&#x27;t count.It is the AAA games from other publishers that matter, those that even publish to Android, while ignoring GNU&#x2F;Linux. replyyesco 16 hours agorootparentprevProton is just a new desktop application API for Linux, one specialized for gaming. While the win32 API is certainly not an open standard, it is a stable one, arguably more stable than even browser APIs, and obviously more stable than the various APIs offered by Linux.Years ago I was optimistic that fixed shared library runtimes, such as the ones offered by Steam, would be enough to make native game development for Linux viable. But even with these, so many things simply break in places they should NEVER break, and can only be fixed by recompiling them (thanks glibc), so I no longer feel this way.While my heart bleeds for open source, video games are fundamentally more art than function, so the expectation that the user (or the upstream maintainer, Steam) needs to be capable of recompiling them is simply unrealistic when considering the licensing & support required for this. Video games need a stable API, perhaps even more than most other types of software. A game built 10 years ago should continue to work when run today.When considering all of this, along with the fact that Valve is the only large organization with the resources and incentive to fix this situation. The only real path forward Valve has here is to create their own custom desktop application API for Linux that goes beyond just a set of shared libraries and is fully integrated, meaning third-party game developers don&#x27;t need get involved in the often messy inter-library politics the Linux community is accustomed too.But if Valve really needs to go that far, why invent a whole new API for it? Why not just copy win32? That&#x27;s the most popular API among game developers after all. In fact there is already a Linux runtime that not only supports win32, but actually treats desktop stability as a core priority: WINE. Why not just throw resources at that instead of making something new?While I&#x27;m mostly speculating here, I suspect that this line of reasoning is what gave birth to Proton. It has also led to interesting situations, consider this: Elden Ring on release had&#x2F;has a problem with micro-stuttering, this issue was patched in Proton very quickly, within the first week of release, but remains an issue on Windows to this day. This means that right now, Linux is the best platform to play Elden Ring on if you want the best performance and graphics. Much like video drivers are updated to fix bugs for individual games, Proton now fills this role as well, but with an even faster development cycle.This highlights one of the key benefits of Proton: it scopes an entire Win32 runtime (wineprefix) to each game. A hasty hotfix for one game will not break another, each game needs to only pin the runtime that works, and no further tweaking is necessary. You simply cannot do this on Windows, Microsoft does not have this kind of flexibility.A future where Proton outlives Windows, one where Proton expands the win32 API to include more features never supported by Windows, while I can&#x27;t say it will happen, it certainly can happen, and I&#x27;m excited to see where this goes. reply keyringlight 16 hours agorootparentOne thing that gets me with proton is that as you say it effectively &#x27;crowns&#x27; win32 as the PC gaming platform, but it seems like a weird situation where MS control it and valve&#x2F;codeweavers are constantly chasing them for any new&#x2F;changed functionality so their sub-platform remains relevant. I think an opportunity has passed to divorce PC gaming (or perhaps \"consumer real time 3D\"?) from MS&#x2F;windows because Valve don&#x27;t want to take on all that responsibility, and no one else is interested enough to set up a consortium to pick up that gauntlet.This is my cynical side, but I&#x27;m sure they don&#x27;t mind the opportunity to get their store in front of people, both with the deck and by how closely knit steam is to providing gaming to non-windows PCs. In my view PC gaming is in a weird spot right now if you try defining \"what is the platform?\" Is it windows, is it steam (and all the other features it has), is it x86, how much can&#x2F;should a game be &#x27;portable&#x27; from one ecosystem or enclave of PC. There&#x27;s also been issues with games like Starfield not working with the intel Arc GPUs until a few days ago that have me wondering (from a fairly naive point of view) how closely that aspect is tied to assuming nvidia&#x2F;AMD are the only possibilities versus how well it was written to the abstraction layer, assuming intel were compliant. reply jorl17 16 hours agorootparentIf enough players play via Proton and want to stay that way, then the subset of the Win32 API that is well supported for Proton can itself become the standard. MS might add new APIs, but developers may choose to keep the old ones, therefore targeting the largest userbase.Of course, MS might also start changing the behaviour of lots of APIs, deprecating them left and right, effectively trying to kill Proton. But that means they&#x27;ll also kill their own backwards compatibility.If, again, Proton becomes \"popular enough\", I don&#x27;t see how MS can stop it short of finding some way of preventing developers from using old APIs in new applications (which would very likely be anti-competitive behaviour).I&#x27;ve been a macOS user for 11 years now, but I hope that, however unlikely, Proton wins out :) reply capitainenemo 15 hours agorootparentprevIsn&#x27;t this the whole reason Valve created steam runtimes so that they could have native linux binaries with also long patch lifetimes? AFAIK they even select Ubuntu LTS releases so they can surf off upstream patching for longer.Ubuntu&#x27;s extended support even pushes that out to 10 years, and I imagine Valve could also do its own patching (or just ignore security concerns).Nice thing w&#x2F; Valve is they allow you to use your distro&#x27;s libraries or switch to the runtime, depending on how much performance you are trying to eke out of a game.Odds are anything written a decade ago doesn&#x27;t need cutting edge performance though. reply yesco 15 hours agorootparentI briefly touched on this but steam runtimes (aka shared library runtimes) in practice aren&#x27;t as isolated as one would assume, an update to glibc is often enough to break applications targeting them. Furthermore, in practice many maintainers think little of video games and are often comfortable breaking them if it helps with the server side of things.For evidence of this, look no further than Ubuntu, the basis of the steam runtime, dropping 32bit support and breaking all software (video games) that depend on it. Sure you can still use it for older stuff, but there is no future here. reply capitainenemo 14 hours agorootparentWell. Microsoft is also dropping 32 bit support for Windows 10 in 2025. I guess that makes Proton some weird hackish abstraction layer over the continuously updated underlying linux libs. Would be funny if Proton gets pulled into Windows releases to maintain support for legacy games as Microsoft shortens the length of their support. reply Wowfunhappy 11 hours agorootparentIs Microsoft dropping support for 32bit binaries in 2025? I think it&#x27;s just dropping hardware support. reply leeter 8 hours agorootparentThey are however (by dropping the 32bit versions of Windows) dropping 16bit support. Which still exists in 32bit Windows 10 along with NTVDM (AKA DOS emulation). So I guess it&#x27;s the end of the line for MSDOS? reply capitainenemo 10 hours agorootparentprevAh yeah, you&#x27;re right. WOW64 appears to not be going anywhere anytime soon. Although. I gotta say, I don&#x27;t use windows much, but some folks I kind of helped out on the side were upgrading to Windows 8 and were completely unable to launch their 32 bit XP accounting software binary with every emulation flag I tried. (could just have been my inexperience on this, but wasn&#x27;t finding anything in the support guides).Oddly enough, it ran fine in Wine in virtualbox in a small ubuntu instance, so they ended up just using that in seamless mode.So, at least from past experience that legacy compat is not 100% and I&#x27;m guessing games might be even more finicky than accounting software. reply yesco 14 hours agorootparentprevIt would be very amusing to me if certain games on Windows could only be run via a hacky WSL -> Proton layer. Yet somehow not surprising ;) reply bobajeff 16 hours agorootparentprev>But if Valve really needs to go that far, why invent a whole new API for it? Why not just copy win32?I can think of at least two reasons:Legal - APIs might be copyrightable in the future.Control over changes - Not having to play ketchup every time Microsoft changes or updates an API. Windows APIs do still change and break applications they have compensated for this with application specific shims.That being said I can&#x27;t imagine what Valve would come up with. Maybe they could base something around Vulkan, Musl and Wayland. reply surgical_fire 15 hours agorootparentprevI would welcome a future where Proton is the target environment for game development.I never thought about it that way, but I can see this as a possibility based on how you framed it. Have an upvote. reply pjmlp 14 hours agorootparentprev\"OS&#x2F;2 runs Windows applications better than Windows\", yep. reply surgical_fire 13 hours agorootparentPerhaps ironically, I find it easier to run older games in Linux than it was to run them on Windows itself before I made the switch. reply surgical_fire 15 hours agorootparentprevYes. But that&#x27;s the thing - if more and more people play games on Linux (even through Wine), there will be more Linux machines around.Developers might be more interested in developing for it if it is more than a footnote of an OS, possibly creating a positive feedback loop. Especially with companies such as Valve invested into it.A man can at least dream, no? reply pjmlp 14 hours agorootparentWorked great for OS&#x2F;2. reply yesco 13 hours agorootparentOS&#x2F;2 was competing on the enterprise market, I don&#x27;t really see it as a good comparison to the Linux&#x2F;Proton situation, yet for some reason I see it brought up a lot in these discussions. Why would OS&#x2F;2 come to mind and not the successes of https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Embrace,_extend,_and_extinguis... or https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Windows_Services_for_UNIX ? reply pjmlp 3 hours agorootparentBecause those things were only to get DoD contracts, \"Supports POSIX\" checkbox. reply oaththrowaway 18 hours agorootparentprevWine is not an emulator reply coldpie 17 hours agorootparentI hate this reply. Wine is not an emulator, but it does emulate Windows(&#x27;s behavior). reply karolist 15 hours agorootparentTo those who might not be aware: Wine originally stood for \"Wine is not an emulator\", a recursive acronym that were popular in the geek culture a few decades ago. Same for KDE Desktop Environment and GNU&#x27;s not Unix. reply IntelMiner 15 hours agorootparentprevIt *approximates* Windows behaviour and acts as an intermediary layer of \"glue logic\"An emulator will generally try to provide an abstracted set of hardware and other features reply paulmd 14 hours agorootparenthttps:&#x2F;&#x2F;en-academic.com&#x2F;dic.nsf&#x2F;enwiki&#x2F;1318672I guess this falls under \"other features\" but it&#x27;s pretty common to emulate systems behavior via a re-implementation, or thunking calls to a copy of the original libraries, etc.Oracle uses the verb \"emulate\" to describe branded zones, which function similarly, as well. I think it&#x27;s semantically fine to call this emulation, it&#x27;s just high-level emulation.https:&#x2F;&#x2F;docs.oracle.com&#x2F;cd&#x2F;E53394_01&#x2F;html&#x2F;E54762&#x2F;gitrc.html reply pjmlp 14 hours agorootparentprevIndeed, the technical implementation doesn&#x27;t matter for the context that it helps to strenghen Windows market position among game studios. reply beebeepka 18 hours agorootparentprevProton Is Not an Emulator reply CelticBard 16 hours agorootparentPINE reply diogenes4 15 hours agorootparentprev...isn&#x27;t that just evidence that windows is actively being replaced? reply pjmlp 14 hours agorootparentRather the evidence Linux will never have native games, not even studios targeting Android NDK care about GNU&#x2F;Linux. reply AussieWog93 13 hours agoparentprevI&#x27;ve released software on Linux.The biggest problem isn&#x27;t the small market share (3% of desktop users is still a decent chunk of customers), but the lack of standards and fragmentation making good QA basically impossible. reply struant 12 hours agorootparentBecause it isn&#x27;t really 3%. It is 300 distros with 0.01% each, and the fragmentation keeps getting worse. reply tmtvl 11 hours agorootparentDon&#x27;t Ubuntu, Fedora, and Arch have like 90% of desktop GNU&#x2F;Linux users? reply porridgeraisin 8 hours agorootparentYes, but that&#x27;s a very broad brush. Regardless of the distro people use either x11 or Wayland, this compositor or that compositor or no compositor, this or that driver, ....So even if you manage to publish a deb, rpm and to the aur, QA is difficult. reply mcpackieh 3 hours agorootparentprevThis isn&#x27;t relevant to game developers. Most game developers who nominally support linux will test on a single distro and release a single zipped or tarred binary (not even an rpm or a deb). Linux users are expected to sort out any issues they might encounter on their own distro, and linux users expect to do this as well.So for example, I as a linux user download factorio_alpha_x64_1.1.91.tar.xzThis isn&#x27;t a package made specifically for my distro. Was it tested with my distro? Almost certainly not. Do I care? No, not really. Might I have some trouble with it? Probably not, but maybe! Do factorio devs get flooded with hate mail for not packaging factorio as an RPM and testing every single release with every single long-tail distro? I think they don&#x27;t. They might get a few nutjobs sending them letters but by in large the linux gaming community (as it were) has low expectations and is easily pleased with the bare minimum. reply rvz 8 hours agorootparentprevThis is the factual reality of the matter. 300+ distros to test and you can&#x27;t support them 100% of the time like you can with Windows or macOS and give a guarantee to your customers that it works on your OS.Unfortunately, the Linux fans continue to ignore the worsening fragmentation issues. reply piperswe 6 hours agorootparentSteam includes the Steam Runtime, a container-like environment that provides a single target for Linux game developers. In fact, it&#x27;s commonly used by NixOS users to run proprietary software that wasn&#x27;t built for NixOS. reply porkbeer 3 hours agorootparentprevSteam solves this by picking a distro or 2 to officially support, and the other users quickly adapt. reply dartharva 16 hours agoparentprevDoesn&#x27;t Counter Strike 2 have major performance issues on Linux? There exists a huge outcry of it not being playable on the Steam Deck. reply f1refly 15 hours agorootparentPerformance is about equal to the windows build, the problem is most likely that steam deck hardware is not sufficient to play counter strike 2 reply diogenes4 15 hours agorootparent> the problem is most likely that steam deck hardware is not sufficient to play counter strike 2Seems nuts for a game whose entire draw is being able to be played on commodity hardware at high speeds. reply wly_cdgr 15 hours agorootparentIt&#x27;ll prob run great on Steam Deck 2 reply KennyBlanken 20 hours agoparentprevThe only reason Linux now sees support is because it tied in with Valve&#x27;s desire for a handheld console and them wanting to rattle MS&#x27;s cage a bit by showing how much better games ran on Linux. It worked; MS has devoted more effort in recent years to fixing gaming issues.The number of Steam Decks pales in comparison to the number of Macs, which now represent 30% market share of desktops, while linux is still stuck around 3%. reply dinkblam 19 hours agorootparentApple does not have a market share of 30% of desktops.Even it it had, what matters here is actual number of players.Steam stats come in at Windows 96.94% Linux 1.63% macOS 1.43% [1].So when they say excluded platforms account for 1%, they are right, barring rounding errors.[1]: https:&#x2F;&#x2F;store.steampowered.com&#x2F;hwsurvey&#x2F; reply zymhan 19 hours agorootparentSteam stats won&#x27;t account for users who don&#x27;t install Steam because there are no games they can run. reply JAlexoid 15 hours agorootparentWhy? Mac users are cornered into exclusively a mobile device(MacBooks) and will more than likely purchase game consoles... or even just use remote play services(I use GeForce Now)Windows users are already on Windows and Windows machines are much more open to dual-booting Linux... if needed. reply Toutouxc 14 hours agorootparent> Mac users are cornered into exclusively a mobile device(MacBooks)Huh? iMac, Mac Mini, Mac Studio, Mac Pro. reply mcpackieh 18 hours agorootparentprevValve is more concerned with meeting games were they are, not where they hypothetically could be. In the case of Linux (more gamers on Linux than Macs, although it&#x27;s close), Valve has additional reason to care because they view Linux as insurance against anti-competitive behavior from Microsoft. Clearly they don&#x27;t feel the same way about Macs (makes sense, because their Windows customers could more readily switch to Linux than Macs). So if gamers are already acquiring Windows&#x2F;Linux machines to play games, why should Valve waste their resources trying to bring about a hypothetical reality where gamers choose to buy Macs instead?It&#x27;s not Valve&#x27;s responsibility to engage in pro-Mac activism. It would be weird if they were doing so. reply freedomben 15 hours agorootparent> Valve has additional reason to care because they view Linux as insurance against anti-competitive behavior from Microsoft. Clearly they don&#x27;t feel the same way about Macs (makes sense, because their Windows customers could more readily switch to Linux than Macs).Agreed, and I think that&#x27;s quite wise. When it comes to escaping tight-reigned dictators that can threaten your business, Apple is out of the frying pan an into the fire. Imagine if they invested heavily into macs and then Apple required all mac software to go through their store (with 30% cut of course) like they do on iOS? That would be disastrous reply KerrAvon 16 hours agorootparentprev> Valve is more concerned with meeting games were they are, not where they hypothetically could be.I have no comment on the rest of it, but this is clearly false. Valve has been trying to make gaming on Linux happen for a long time now. reply mcpackieh 16 hours agorootparentAs I said, Linux has more importance to Valve than can be accounted for by its popularity because Valve sees Linux as a hedge against anti-competitive behavior from Microsoft. reply pjmlp 18 hours agorootparentprevNo, but it has more iOS and iPad devices than Valve will ever ship SteamDecks. reply mcpackieh 18 hours agorootparentiOS isn&#x27;t relevant to Valve due to the way Apple chooses to do business, so why bring that up? reply pjmlp 14 hours agorootparentIt is relevant to Apple customers that care about playing games and don&#x27;t need to care Valve exists at all. reply mcpackieh 4 hours agorootparentIn other words, it&#x27;s not relevant to the discussion at all. reply pjmlp 3 hours agorootparentIt is, as it shows Valve&#x27;s decision doesn&#x27;t matter to Apple customers, they have enough games from other publishers. reply mcpackieh 3 hours agorootparentValve says \"99% of our players don&#x27;t use MacOS, so we&#x27;re dropping support for that.\"You say \"MacOS users play other games, so Valve leaving doesn&#x27;t matter\"Pretty much the same thing, isn&#x27;t it? If anything, it seems like you&#x27;re angrily confirming Valve&#x27;s decision. reply SirMaster 17 hours agorootparentprevWhat about after sideloading is allowed? reply Macha 16 hours agorootparentThe vast majority of mobile games are not really substitutes for PC games the way e.g. console games are, except in the super broad \"Netflix: Our biggest competitor is video games because customers only have so much time\" way. reply ok_dad 15 hours agorootparentprevRounding 3% to 1% is a hell of a rounding! reply Nodebuck 17 hours agorootparentprevValve has supported Linux for over a decade[1], way before they made any hardware. It makes sense for them to not rely on Microsoft which have threatened their position as a gaming store before with UWP games appearing exclusively on Microsoft Store and the win32 platform potentially being killed off (which all turned out to be a flop in the end).[1] https:&#x2F;&#x2F;store.steampowered.com&#x2F;oldnews&#x2F;9289 reply zamalek 14 hours agorootparentprev> The only reason Linux now sees support is because it tied in with Valve&#x27;s desire for a handheld consoleValve have been improving&#x2F;supporting Linux gaming for almost a decade[1].Apple figured out that privacy is a marketable product. They don&#x27;t do it because of altruism, they do it because it makes extreme amounts of money.Likewise, Valve figured out that doing good by the customer is a marketable product. Gamer loyalty to Valve is an insurmountable fortress that no other storefront has managed to pierce. The likes of TotalBiscuit and AngryJoe galvanized the PC gaming community into one that does not tolerate anti-consumer bullshit, and anti-consumer bullshit Apple does.> The number of Steam Decks pales in comparison to the number of Macs, which now represent 30% market share of desktops, while linux is still stuck around 3%.Which is completely irrelevant to gamers. A substantial amount choose to have a console and no desktop&#x2F;laptop machine at all[2]. Linux is trending up [3], while MacOS trending down (and in addition is installed on fewer gaming machines than Linux overall) [4].MacOS is completely and utterly irrelevant to gaming - unless you&#x27;re talking about Candy Crush and the sort. This is unlikely to change unless Apple has a radical change in direction regarding gaming; both in terms of hardware and business model. They have clearly and loudly demonstrated that they have no interest in either: you can&#x27;t use GPUs with MacOS, and their fork of WINE is neither free nor upstreamed.[1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Steam_Machine_(computer) [2]: https:&#x2F;&#x2F;venturebeat.com&#x2F;games&#x2F;pc-and-console-sales-are-down-... [3]: https:&#x2F;&#x2F;www.gamingonlinux.com&#x2F;steam-tracker&#x2F; [4]: https:&#x2F;&#x2F;store.steampowered.com&#x2F;hwsurvey&#x2F;Steam-Hardware-Softw... reply Panzer04 7 hours agorootparentIt helps that by and large gamers on PC are savvy enough to pirate if stores get too frisky. It&#x27;s amazing valve managed to establish such a moat that they can take their 30% cut. reply some-guy 16 hours agorootparentprevI&#x27;m sure the number of Steam Decks is far higher than the number of people gaming on their Mac. reply philistine 15 hours agorootparentNope. reply throitallaway 16 hours agorootparentprev> MS has devoted more effort in recent years to fixing gaming issues.Could&#x27;ve fooled me. Their first party title Halo MCC is riddled with bugs (crashes, textures not loading, etc.) and has cheaters flaunting super speed in multiplayer. reply HideousKojima 15 hours agorootparentDelta Halo on Halo 2 is practically unbeatable on Legendary mode due to crashes, unless you use the original graphics. The big was reported to 343 years ago, I think back when MCC was still an Xbox exclusive, and they still haven&#x27;t fixed it. And they don&#x27;t give you any warning about it, you only figure it out after the game crashes halfway through the level for the 5th time and you decide to start Googling it. reply dangus 19 hours agorootparentprevThere’s another problem with Mac gaming: even though Macs have a higher marketshare than they’ve ever had before, they aren’t owned by people who are expecting to use them for games.Someone who is into the PC gaming scene already owns a gaming PC by now. If they own a Mac it’s probably their productivity mac",
    "originSummary": [
      "Valve has decided against releasing a macOS version of Counter-Strike 2 due to insufficient player numbers. Counter-Strike: Global Offensive (CS:GO) players on Mac can request a refund if they played between March 22 and September 27, 2023.",
      "Future enhancements to Apple products might include OLED displays for the iPad mini and iPad Air by 2026. An important software update for the iPhone 15 Pro models was released to fix overheating issues.",
      "Samsung encourages Apple to adopt the RCS messaging standard in a recent advertisement, following damaging drop tests on luxury smartphones by Allstate Protection Plans."
    ],
    "commentSummary": [],
    "points": 320,
    "commentCount": 660,
    "retryCount": 0,
    "time": 1696941437
  },
  {
    "id": 37832319,
    "title": "Postgres: The next generation",
    "originLink": "https://redmonk.com/jgovernor/2023/10/10/postgres-the-next-generation-investing-in-the-next-generation-of-committers/",
    "originBody": "Skip to Content RedMonk the developer-focused industry analyst firm Become a client Videos Research Events About Team Services Clients Contact Search @redmonk JAMES GOVERNOR'S MONKCHIPS Postgres: the next generation. Investing in the next generation of committers. By James Governor@monkchipsOctober 10, 2023 Share via Twitter Share via Facebook Share via Linkedin Share via Reddit PostgreSQL isn’t getting any younger. Which is fine – after all, databases generally improve with age. The platform is going from strength, and is a default choice for a big chunk of modern software development. But Postgres has been around for a while – it launched in 1986 – which has an implication for the folks actually building the database. Just how long will they want to do the heavy lifting of maintaining a high profile codebase that so many folks rely on? Postgres is a close knit group and project. Robert Haas, Postgres committer and chief database scientist at EnterpriseDB writes a regular contribution post and the latest numbers are salutary – Who Contributed to PostgreSQL Development in 2022? I calculate that, in 2022, there were 192 people who were the principal author of at least one PostgreSQL commit. 66% of the new lines of code were contributed by one of 14 people, and 90% of the new lines of code were contributed by one of 40 people. The core development community is aging somewhat – the average age is probably around 50. Which is totally fine. 50 year olds are more than capable of doing a shitload of work – don’t ask me how I know. Tom Lane, who works at Crunchy Data, is 68 and he’s still the Postgres project’s fulcrum. Long may that continue. The Postgres community is amazing. Open Postgres governance is something we can and do rely on, which is refreshing in the current era of commercial open source licensing rugpulls. But as an axis to consider in terms of open source sustainability let’s assume that Postgres is still going strong in say, 20 years. Who is going to be doing the work in 2043? I had a fascinating conversation with Nikita Shamgunov, CEO of Neon recently and one of the subjects we discussed was aging in tech projects and its relationship to project sustainability. Neon is a fully managed Postgres database optimised for serverless apps, separating storage from compute – the database is just a URL. That’s the design principle. It allows for branching, with preview deployments – thus Neon’s partnership with Vercel. Make it easy, make it modern, make it a zero config API. Neon has 62 employees and has raised $108m so far. It competes with the likes of Supabase. But back to the subject at hand. According to Shamgunov: If you look at the Postgres committer crowd they’re in their 50s, 60s, 40s and maybe a few in their 30s. It takes a lot of effort to become a committer but very little to be a contributor – you just need to write good code. I think we’re doing good to the world by hiring more junior people and training them to become committers and hopefully maintainers. It’s very important that the Postgres engine continues to evolve. Neon is being intentional about investing in the next generation of contributors, committers and maintainers. The natural move for a lot of companies is to try and hire the existing top talent, rather than fostering new blood. We debated whether to just find more Postgres committers and hire them. But it’s not clear that would be spending our money in the best way. If we train new ones it’s better, and that’s how we can keep ramping the Postgres team. There are some interesting questions here. For example – consider Neon’s IP, which is currently permissively licensed, but Shamgunov is not an open source zealot. What happens if in a few years the company decides to relicense, as other database companies have – see for example Redis, MongoDB and Elastic. Neon would be perfectly within its rights to relicense under more restrictive terms, potential community blowback aside. But any code they had contributed to Postgres? That’s not going to be affected. Having core Postgres maintainers on staff is a pretty good example of enlightened self interest and should serve to keep the company honest. Whatever decisions Neon makes in future, assuming they have employees dedicated to making Postgres better, then the community and core codebase still wins. Cohort aging is certainly not a problem that’s unique to Postgres. Anyone remember the year 2000 bug? Communities and ecosystems do get older, which can be an issue when it comes to skills and staffing and rejuvenation. IBM has done a great job of bringing younger developers into the mainframe fold, for example with vocational education programs at universities – here is a post I wrote about that a while back. There are plenty of projects with literally millions of users that are run by one or two people and don’t have the level of corporate sponsorship with see with projects such as Postgres or even Kubernetes. Postgres isn’t in any sense struggling to attract new users – there are plenty of 22 year olds defaulting to it today. It’s a hugely popular platform. But yes, ensuring the ongoing maintenance of the project will require some intentionality, funding, and enlightened self interest. Disclosure: Neon is not a RedMonk client. Crunchy Data, IBM and Vercel are all RedMonk clients. This piece is published independently of any client relationships. The illustration above was created with Midjourney. No Comments Leave a Reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Save my name, email, and website in this browser for the next time I comment. Notify me of follow-up comments by email. Notify me of new posts by email. Recent Posts Postgres: the next generation. Investing in the next generation of committers. Client Profile: deepset Cloudflare as an AI play. An interview with CEO Matthew Prince. Spicy Takes, Platform Engineering, and Dynamic Configuration Management JSON/Relational Duality and developer-led tech adoption. Thoughts on Oracle and industry database directions. Subscribe to Blog via Email Enter your email address to subscribe to this blog and receive notifications of new posts by email. Join 143 other subscribers Email Address Subscribe Search for: Recent Posts Postgres: the next generation. Investing in the next generation of committers. Client Profile: deepset Cloudflare as an AI play. An interview with CEO Matthew Prince. Spicy Takes, Platform Engineering, and Dynamic Configuration Management JSON/Relational Duality and developer-led tech adoption. Thoughts on Oracle and industry database directions. Recent Comments Shashwat Gupta on Cloudflare as an AI play. An interview with CEO Matthew Prince. Shashwat Ravi Gupta on Fly Less, Write More: the future of Developer Relations, and maybe, well, everything else Mary Branscombe on Cloudflare as an AI play. An interview with CEO Matthew Prince. James Governor on Spicy Takes, Platform Engineering, and Dynamic Configuration Management Danno on Spicy Takes, Platform Engineering, and Dynamic Configuration Management Archives October 2023 September 2023 July 2023 May 2023 April 2023 February 2023 January 2023 December 2022 November 2022 September 2022 August 2022 June 2022 May 2022 April 2022 March 2022 February 2022 January 2022 December 2021 November 2021 October 2021 August 2021 June 2021 May 2021 April 2021 March 2021 January 2021 December 2020 November 2020 October 2020 July 2020 June 2020 May 2020 April 2020 February 2020 January 2020 November 2019 October 2019 August 2019 July 2019 June 2019 May 2019 April 2019 March 2019 February 2019 January 2019 November 2018 October 2018 September 2018 August 2018 July 2018 June 2018 May 2018 April 2018 March 2018 February 2018 January 2018 December 2017 November 2017 October 2017 September 2017 August 2017 July 2017 June 2017 May 2017 April 2017 March 2017 February 2017 January 2017 November 2016 October 2016 September 2016 August 2016 July 2016 June 2016 May 2016 April 2016 March 2016 February 2016 December 2015 November 2015 October 2015 September 2015 August 2015 July 2015 June 2015 May 2015 April 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 January 2014 December 2013 November 2013 October 2013 September 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 February 2013 January 2013 November 2012 October 2012 September 2012 August 2012 July 2012 June 2012 May 2012 April 2012 March 2012 February 2012 January 2012 December 2011 November 2011 October 2011 September 2011 August 2011 July 2011 June 2011 May 2011 April 2011 March 2011 February 2011 January 2011 December 2010 November 2010 October 2010 September 2010 August 2010 July 2010 June 2010 May 2010 April 2010 March 2010 February 2010 January 2010 December 2009 November 2009 October 2009 September 2009 August 2009 July 2009 June 2009 May 2009 April 2009 March 2009 February 2009 January 2009 December 2008 November 2008 October 2008 September 2008 August 2008 July 2008 June 2008 May 2008 April 2008 March 2008 February 2008 January 2008 December 2007 November 2007 October 2007 September 2007 August 2007 July 2007 June 2007 May 2007 April 2007 March 2007 February 2007 January 2007 December 2006 November 2006 October 2006 September 2006 August 2006 July 2006 June 2006 May 2006 April 2006 March 2006 February 2006 January 2006 December 2005 November 2005 October 2005 September 2005 August 2005 July 2005 June 2005 May 2005 April 2005 March 2005 February 2005 January 2005 December 2004 November 2004 October 2004 September 2004 August 2004 July 2004 June 2004 May 2004 March 2004 February 2004 October 2003 July 2003 June 2003 Categories agile analystbusiness APIs AR Bit Miles charity clo cloud Cloud Computing Cloud Native CSR data Dell developer relations developerintelligence developers EMC Google HP HTML IBM industry ecosystems Java language linux, cloud, saas, IBM, mainframe LiveCycle mainframe messaging Microsoft mobile development opensource ops programming languages salesforce.com SAP sdlc Shoreditch Social Business socialmedia sustainability testing transparency Uncategorized UX Meta Log in Entries feed Comments feed WordPress.org Clients Contact Services Briefings briefings@redmonk.com RedMonk 48 Union Wharf Portland, ME 04101 Twitter YouTube",
    "commentLink": "https://news.ycombinator.com/item?id=37832319",
    "commentBody": "Postgres: The next generationHacker NewspastloginPostgres: The next generation (redmonk.com) 274 points by mooreds 20 hours ago| hidepastfavorite80 comments pjungwir 18 hours agoI am hoping to be part of that \"Next Generation\"---although I&#x27;m already 46. I&#x27;m sure there are others a bit younger.My last talk at PGCon tried to fill some of the gaps around what you must know to hack on Postgres, especially the Executor Phase and TupleTableSlots. I&#x27;m not the most qualified, but sometimes it takes a learner to know what learners need. Just the other day I wrote a table of contents for a book about how to contribute to Postgres. I figure it would sell at least ten copies. Maybe a serial publication online would be better. Curious if anyone would be interested in either of those?For now Postgres is more like a hobby for me, but if someone is hiring to do open source Postgres contributions full time, I&#x27;m open to a chat. ;-) reply craigkerstiens 17 hours agoparentJust coming to say hi Paul! I recall you being very excited and getting the email after you submitted your first patch and loved it.I think a ton of the current set of \"next generation\" came to Postgres a good bit later. Even Tom himself will tell you he did some stuff with images for a few years which undersells himself (tiff, jpg, png - in some form involved in creation of each of those), then found this Postgres thing and started working on it. reply pjungwir 16 hours agorootparentCraig, you really made me feel welcome in the Postgres community. Maybe people have. It&#x27;s such an amazing group. Thanks for all the pg advocacy you&#x27;ve done! reply gavinray 17 hours agoparentprevIf you haven&#x27;t already, you might be interested in reaching out to PG contributor Andrey Borodin.He does a lot of content focused around how to get started contributing to Postgres and is very friendly + interested in speaking with other PG enthusiasts.He might be open to collaboration or offering some helpful advice =)For example: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rihfAnd_leMYou can reach him either via email (x4mmm@.ru) or Twitter (@x4mmmmmm) reply andatki 12 hours agorootparentThanks. This looks great! reply pjungwir 16 hours agorootparentprevThank you! reply mch82 7 hours agoparentprevI’ve met a bunch of top engineers that kept working into their 70s and beyond because they couldn’t do the cool stuff at home. I’m very excited to see if more people may begin to retire earlier & get involved with open source now that participation is more broadly accessible. reply fanf2 14 hours agoparentprevI get the impression that publishing the work in progress online can be a good way to market the book, eg https:&#x2F;&#x2F;www.cl.cam.ac.uk&#x2F;~rja14&#x2F;book.htmlSome publishers allow early-access readers to help with reporting mistakes https:&#x2F;&#x2F;nostarch.com&#x2F;early-access-program reply davidthewatson 17 hours agoparentprev+1 I&#x27;ll buy your book. What&#x27;s it called? reply pjungwir 16 hours agorootparentThank you! It&#x27;s just a table of contents. :-) And there is a book about temporal data I want to write first. . . . But it&#x27;s awesome to hear someone is interested.EDIT: to plug a couple other independent-author Postgres books on my short-term reading list, these look awesome (but not quite the same focus):- https:&#x2F;&#x2F;postgrespro.com&#x2F;community&#x2F;books&#x2F;internals- https:&#x2F;&#x2F;theartofpostgresql.com&#x2F; reply biehl 17 hours agoparentprevI would be happy to have a look at your book drafts. reply pjungwir 16 hours agorootparentThank you! May I keep you in mind once I have something? My email is in my profile, so maybe send me a note. (I don&#x27;t see yours.) reply biehl 13 hours agorootparentI sent you a note and added my email to my profile. Best of luck with the writing. reply wkoszek 17 hours agoprevMy aspirational goal is to be in a position to retire early and hack on Postgres full-time, it’s so interesting. It has networking, storage, data, algorithms etc.C is less of a problem tbh. Postgres has good code style and its pretty consistent. The complexity of the internals is, and having a small community may infer the speed in which people help you. reply teaearlgraycold 9 hours agoparentWe should have an NSF for FOSS contributions reply nikita 15 hours agoparentprevYou can work full time for someone as a Postgres committer. reply 1500100900 14 hours agorootparentUnless you&#x27;re Tom, you&#x27;ll have to work on tickets assigned to you by some manager. reply samaysharma 11 hours agorootparentThat&#x27;s not really true any more.I personally know several Postgres contributors &#x2F; committers who have a very high amount of control on what projects they work on. reply candiddevmike 17 hours agoprevI wonder if C codebases are going to have problems finding maintainers in the future. Postgres has commercial backing and inertia, but it seems like we are lacking a pipeline of (proficient) C developers. reply swatcoder 17 hours agoparentIn some future, of course, but that future is decades away. C is a living language with no shortage of active users and a learning curve that&#x27;s not very steep for systems programmers working in other language.It can feel daunting to today&#x27;s web and application developers who are used to an opaque curtain between themselves and the underlying system architecture, but systems programmers using C++, Rust, etc already work behind that curtain (but with thicker gloves). They&#x27;ve often worked with C in the past, at least in education or experiment, and can ramp up on the footgun quirks with some intentional study when taking it up professionally.There are arguments against picking C for some new systems projects, but -- outside a lack of systems programmers more broadly -- there&#x27;s no pressing concern around finding maintainers for what already exists. reply anarazel 17 hours agoparentprev(postgres hacker)Yes, I think that will be the case. Obviously not a scientifically measured, by I think we&#x27;re already seeing that average C skills for new contributors are lower than what they used to be - of course that might just be my slowly greying beard speaking. So far I think people just \"learn on the job\", but how much delta that bridge I am not sure.I think eventually we&#x27;ll have to make it easier to use some other language in parts of the system (e.g. in-core data type implementations). But realistically I think that&#x27;s still a bit off. reply koolba 17 hours agorootparent> ... I think we&#x27;re already seeing that average C skills for new contributors are lower than what they used to be - of course that might just be my slowly greying beard speaking.That&#x27;s definitely true. Just like everything else, actual experience matters and the C-share of public codebases (out of the total set) is less and less every year. So while someone may have studied the docs and overall language mechanics, he&#x27;d be less likely to have actually worked on a different C code base. Even less so for people who have worked on portable C software.> I think eventually we&#x27;ll have to make it easier to use some other language in parts of the system (e.g. in-core data type implementations). But realistically I think that&#x27;s still a bit off.There is an elegance to having the C structs directly match the data layout. But I suppose that it&#x27;s only elegant if your mind has been wired from the start to think about byte alignment and word sizes. Coming from the world of interpreted languages with dynamic objects as bags of properties, I bet it&#x27;s nowhere near as magical. reply eatonphil 15 hours agorootparent> There is an elegance to having the C structs directly match the data layout.Doesn&#x27;t this ability significantly depend on the system (and also becomes less relevant when you have variable length data)? replypetergeoghegan 17 hours agoparentprev> Postgres has commercial backing and inertia, but it seems like we are lacking a pipeline of (proficient) C developers.It&#x27;s difficult to get into Postgres development, but that has little to do with C expertise. The hard part is having the right domain expertise. Building up general familiarity with the system just takes a long time. reply wg0 16 hours agoparentprevNow this might be idiotic take but isn&#x27;t C easier and simpler to learn than Rust? Even Typescript&#x27;s type system is pretty complicated compared to C.Or I might have revealed my ignorance about how much complicated C is. reply steveklabnik 16 hours agorootparentBrainfuck is easier and simpler to learn than C. That doesn&#x27;t mean that writing an application in Brainfuck is easier than writing one in C.\"which is easier to learn\" is basically an inherently subjective concept at this point, we as a field do not have an objective way to answer this question, except for extreme cases like the one I am drawing above. For any \"real\" language, it is much, much, much less clear-cut. reply wg0 10 hours agorootparentC isn&#x27;t brainfuck. Modern tooling (linters, valgrind and friends) should remedy shortcomings if any.There aren&#x27;t classes, generics, exceptions etc but that also means there isn&#x27;t much to learn either. reply steveklabnik 9 hours agorootparent> C isn&#x27;t brainfuck.I never said it was.I specifically contrasted the two, saying that C is easier to build an application in than Brainfuck would be. Because it is. That means I think they&#x27;re different, not the same. reply fsociety 5 hours agorootparentprevModern tooling and lack of language features does not make it easier to design a program. I have seen self-proclaimed C developers struggle with managing the ownership and lifecycle of memory in a sane way. reply dajtxx 38 minutes agorootparentI started writing commercial software in c in the late 80s, did my own memory management system to work around the limited memory handles in win3, got sdcc going for a custom z80 system where I had to do the startup code, and i have no idea how const works. replyjmull 15 hours agoparentprevAre there fewer C programmers than their used to be?Are they getting generally dumber for some reason?I get that younger devs often don&#x27;t have the low-level background older devs necessarily have, but all of the useful skills are learnable (and they have less bad things to unlearn, too). All good devs, of any age, have learned to learn what they need when they need it. I don&#x27;t think anything has changed there.I think the issue is more that key positions in these more prominent well established projects are usually filled and haven&#x27;t been opening up. reply lmm 6 hours agorootparentFewer new programmers are learning or being taught C. And fewer mid-career programmers are choosing it as a language to learn.Of course any competent programmer can pick up most languages. But projects written in a language that&#x27;s in decline still have a harder time attracting developers (look at anything written in COBOL or Perl or TCL): it&#x27;s less good for their CV than a more popular language, and the level of support in tooling and the library ecosystem tends to be worse, which means you spend more time working on the scaffolding and less time doing interesting work. And frankly C is already a language where you spend a lot of time on ceremony and bookkeeping and relatively little on the essence of the problem. reply hedora 17 hours agoparentprevI&#x27;m guessing this won&#x27;t be a major problem. I don&#x27;t know any proficient Rust or C++ developers that are not also proficient C developers.I do wonder when even more C code bases are going to start seeing modules ripped out and replaced with Rust.This is already happening with Linux, curl, and with C++ stuff like Chrome, many MS products, Amazon S3, etc, etc. The biggest explicit holdout I know of is OpenBSD, and that&#x27;s because they&#x27;re trying to keep the bootstrap &#x2F; base install toolchain small. reply phamilton 5 hours agorootparent> I&#x27;m guessing this won&#x27;t be a major problem. I don&#x27;t know any proficient Rust or C++ developers that are not also proficient C developers.There&#x27;s a growing trend in high level languages like Ruby&#x2F;Elixir&#x2F;node to use Rust + FFI to optimize specific hot code paths. Likewise, pgrx is a much more approachable path towards postgres extension development.All of these development paths have always been available with C, but Rust has made them more accessible to a wider range of developers. I&#x27;d wager a non-trivial number of Rust developers don&#x27;t feel comfortable in C. reply rafaelmn 15 hours agorootparentprevI&#x27;m not proficient at C++ but I have written a decent amount of it (although not in years)While I&#x27;m sure I&#x27;d be able to pick up on the style of the project, it would take a lot of time. Large scale C looks completely different to C++. No RAII, no classes, virtual dispatch, smart pointers. Containers are completely different, templates&#x2F;generics only via preprocessor.I think C requires a lot more conventions and experience to get correct code than C++, and especially Rust. reply beautron 4 hours agorootparent> No RAII, no classes, virtual dispatch, smart pointers.I enjoy programming more when the above concepts are absent. RAII and smart pointers tend toward a fragmented and confused layout of a program&#x27;s memory—there are much simpler ways!The arena concept for managing your program&#x27;s memory is more straightforward. It&#x27;s easier to think about (not confused) and it becomes natural to have your memory laid out nice and orderly (not fragmented, which can be horrible for performance). See the recent article by Ryan Fleury:https:&#x2F;&#x2F;www.rfleury.com&#x2F;p&#x2F;untangling-lifetimes-the-arena-all...I also think life is easier without classes or virtual dispatch. I value a sort of \"mathematical elegance\" in programming languages, and prefer to create programs from a small set of fundamental language primitives. Classes and virtual dispatch don&#x27;t earn their keep in that set. reply OJFord 17 hours agoparentprevI doubt it, universities basically choose between C & Java as the main if not only teaching language. Sure the former might evolve into C++, but it somewhat inherently starts as C, and I suspect you didn&#x27;t mean &#x27;C developers&#x27; to exclude people with C++ experience anyway?An organisation worried about it can easily hire grads&#x2F;juniors with a corresponding proficiency, and ensure that they still have that by the time they&#x27;re seniors by employing them to work in C. reply packetlost 17 hours agorootparentIf people are worried about Postgres, they should be more worried about Linux maintainership. reply piaste 15 hours agorootparentThere are many more billion-dollar companies that rely on the Linux kernel than there are that rely on Postgres. reply packetlost 14 hours agorootparentI mean, fair. replycraigkerstiens 17 hours agoprevSo many thoughts on this. The community has definitely ebbed and flowed, on this for a while. A few varying pieces of insight with no intention other than to share a bit more on the PG community. And I&#x27;m sure some current and former colleagues already in comment threads are going to correct me on nuance of a lot of this.For several years there were no new committers at all. In recent years the team has tried to be a little more intentional about adding new ones and culling those no longer involved.About 15 years there was a phase of letting a lot of younger people earn their commit bit. I can recall 3 people by name that all got a commit bit before the age of 25, and they may have actually all been under 22. One of those three shortly after moved on to work outside of the Postgres community, another quietly was busy on other things for over 10 years before coming back, and the third was actively involved going forward. I suspect there was some unease of folks getting a commit bit and then sort of falling off a cliff so it slowed for a few years on adding new folks. Edit - sounds like it was less age driven but maybe still slightly related to some folks falling off that there was a slow down in new committers – tldr - you&#x27;re not getting a commit bit right out of college for Postgres.What to me would be interesting but likely hard to gather is what age to people become a committer to Postgres. It wouldn&#x27;t surprise me if the average age of getting a commit bit is closer to 45 than not. Many folks contributing come to Postgres after other systems work or just don&#x27;t consider contributing to they&#x27;re a bit more seasoned because it feels intimidating–I mean patches sent on a mailing list who does that any more? Postgres thats who. reply andrenotgiant 15 hours agoparentI have the honor of working with a Postgres ~committer~ contributor who was just over 25 when they first contributed! The story about their first commit is great:They were testing SQL behavior for Materialize and thought to check that both systems handle interval functions identically. Being thorough, they tried something like: select interval &#x27;0.5 months 2147483647 days&#x27;;You can try it yourself on dbfiddle[0] Instead of erroring, Postgres returned a bogus value `{\"days\":-2147483634}` you can read why here[1]So naturally they decided to fix it in Postgres, which is why they contributed and why it&#x27;s handled properly in 15+ [2][0] https:&#x2F;&#x2F;www.db-fiddle.com&#x2F;f&#x2F;ijT76fsmL99bHvXxhAtf7j&#x2F;0 [1] https:&#x2F;&#x2F;git.postgresql.org&#x2F;gitweb&#x2F;?p=postgresql.git;a=commit... [2] https:&#x2F;&#x2F;www.db-fiddle.com&#x2F;f&#x2F;i3KikCb72AN1EZpywErZvr&#x2F;1 reply koolba 15 hours agorootparent> I have the honor of working with a Postgres committer ...That&#x27;s not a committer, that&#x27;s someone who submitted a patch that got committed. A committer is the one who actually applies the patch and can push the branch into the mainline repo. Committers decide if something is worthy of being merged.Now that aside, yes this plus reviewing patches to get a wider feel for the codebase is how you eventually become a committer.Best way to eat an elephant is one bite at at time. reply craigkerstiens 15 hours agorootparentThis is a common source of confusion for a ton of folks. Anyone can submit a patch, but commit bits are reserved for a much smaller list. The attitude is something like you commit it, you maintain it–so if bugs come in you&#x27;ll spend your time fixing those for whatever time it takes vs. working on the next shiny feature that you&#x27;re excited about for the next release.There was sort of a fuzzy \"major\" contributors (https:&#x2F;&#x2F;www.postgresql.org&#x2F;community&#x2F;contributors&#x2F;) which were people that contributed major features and then a list of other contributors. Depending on who you talk to this is either dated or a pretty close attempt at reflection of reality but not perfect. In recent years they expanded the contributors to include others that were contributing in non-code ways though it&#x27;s still a decent place to find people contributing to major feature sets.Of course this is not to be confused with the core team–which is more like a steering committee. But not so much steering committee of code and feature sets. reply andrenotgiant 15 hours agorootparentprevAhh thanks for clarifying - now I better understand the significance of the OP&#x27;s point about the rarity of younger COMMITTER&#x27;s. reply gavinray 16 hours agoparentprevThe thing about becoming a PG contributor is that the barrier to entry is fairly high.I love Postgres so much I have a PG tattoo, but from the perspective of the two ways you can contribute:- As a random user, in your free time: There&#x27;s not a ton of \"Good first issue\" type tickets. Where you can ease your way into PG dev by working on something that doesn&#x27;t require you to have context on many parts of the PG architecture and at least a little historical knowledge on why things are written the way they are. Also, it can be a bit intimidating to have your patches reviewed by the likes of Tom or Andres.- As a developer for a paid PG company like EDB&#x2F;PG Pros&#x2F;Crunchy etc: It&#x27;s a sort of Catch-22 scenario here, where it&#x27;s difficult to get hired as a junior without having previous PG hacking experience, but the path to doing that is not the easiest thing in the world.If I was going to work somewhere that wasn&#x27;t $CURRENT_CO, it&#x27;d be somewhere doing PG work, but there&#x27;s not a lot of viable avenues&#x2F;inroads there. reply hlinnaka 14 hours agorootparentPostgreSQL isn&#x27;t that special as a codebase. Every codebase has its quirks, every project has its own processes and there&#x27;s a learning curve. When you switch to a new job as a software engineer, you pick it up. PostgreSQL is no different: you can hire an engineer to work on PostgreSQL.I&#x27;m not sure how well that path works in growing new contributors, though. In a usual company setting, the goals are better defined, and the company is in control. Once you reach the goals, mission accomplished. With an open source project it&#x27;s more nebulous. Others might have different criteria and different priorities. You are not in control. Choosing the right problems to work on is important.Other storage or database projects would be a good source of new contributors. If you have worked on another DBMS, you&#x27;re already familiar with the domain, and the usual techniques and tradeoffs. But to stick around, you need some internal desire to contribute, not just achieve some specific goals. reply harikb 16 hours agorootparentprevThe biggest hurdle I see is that it is a C project, unfortunately something we can do nothing about. It is so much harder to trust a random code not have to have serious implications for the database. It will take ages for someone to get comfortable with the pg-code-base way of handling errors, basic string manipulation, memory alloc&#x2F;free etc.I want to highlight the difference in \"making a non-core contribution\" to \"understanding database internals\". I am highlighting it is not the latter, but the former that is the first hurdle.I wanted to reuse builtin pg code to parse the printed statements from logs - I ended up writing a parser (in a non-C language) myself which was faster. reply gavinray 16 hours agorootparentCouple of points in this post, so will address a few of them: \"(Paraphrased) C is bad, and it takes forever to pick up the PG-specific C idioms\"There&#x27;s probably not a productive conversation to be had about C as a language. I will say that as of C23, the language is not quite as barebones as it used to be and incorporates a lot of modern improvements.On the topic of PG-specific C -- there are a handful of replacements for common operations that you use in PG. Things like \"palloc&#x2F;pfree\", and the built-in macros for error and warning logging, etc.I genuinely don&#x27;t think it would take a motivated party more than a day or two to pick all of these up -- there aren&#x27;t that many of them and they tend to map to things you&#x27;re already used to. \"I wanted to reuse builtin pg code to parse the printed statements from logs - I ended up writing a parser (in a non-C language) myself which was faster.\"It&#x27;s true that the core PG code isn&#x27;t written in a modular way that&#x27;s friendly to integration piecemeal in other projects (outside of libpq).For THIS PARTICULAR case, the pganalyze team has actually extracted out the parser of PG for including in your own projects:https:&#x2F;&#x2F;github.com&#x2F;pganalyze&#x2F;libpg_query reply zxexz 15 hours agorootparentlibpg_query is a godsend of a library. I spent a lot of time writing a custom parser before I found it - was very happy to replace the whole thing. A major boon was the fingerprinting ability - one of my needs was to track query versions in metadata. reply craigkerstiens 16 hours agorootparentprevI disagree on this. Yes it&#x27;s C. But I&#x27;ve heard people comment \"I don&#x27;t like writing C, but I don&#x27;t mind Postgres C\".The bigger hurdle which Peter mentioned in another thread is simply building up enough expertise with the system and having the right level of domain expertise. reply stouset 16 hours agorootparent> Yes it&#x27;s C. But I&#x27;ve heard people comment \"I don&#x27;t like writing C, but I don&#x27;t mind Postgres C\".While \"Postgres C\" might be wonderful, in practice learning the project&#x27;s unique idioms is yet another hurdle for newcomers to overcome. reply eatonphil 15 hours agorootparentEvery project has unique idioms. Let alone ones that are 30+ years old.Idioms are a baked in cost of learning to contribute to any project. reply fanf2 14 hours agorootparentprevI found that I learned a lot when trying to write a logical decoding plugin. So I guess if you are a user of Postgres and there’s some small friction you could reduce by writing a plugin, it’s a good way to get started. Scratch your own itch, you don’t have to publish the results :-) reply samaysharma 11 hours agoparentprevI don&#x27;t have the data for the average age, but I was recently in a conversation around how long does it take to become a committer since getting involved in Postgres by writing code for it.So, I wrote a couple git commands like below [1] to figure out when someone was first named in a commit message vs when they made their first commit (as a committer) for the last 10 people who became committers.The average time of involvement was ~8.9 years (just comparing month &#x2F; year), with the lowest being ~6.5 years.Obviously one could do better analysis but my goal was just to get an approximate understanding.[1] git log --grep &#x27;Name&#x27; --format=%cssorthead -1git log --author &#x27;Name&#x27; --format=%cssorthead -1 reply dist-epoch 16 hours agoparentprevHow much bigger (in lines of code) is Postgres now versus the one from 15 years ago?Maybe it was more approachable for a 22yo then, you could figure out more of it.Also, C was a standard language back then, today the kids are more likely to program in Rust than in C. reply anarazel 15 hours agorootparent> How much bigger (in lines of code) is Postgres now versus the one from 15 years ago?I was curious as well and wrote a, very crude, script to measure it: for t in $(git tag -l|grep -E &#x27;REL.*_0$|REL[67]_[0-4]$&#x27;|grep -v REL2);do echo -ne \"$t\\t\"; git ls-tree -r $t --object-only |xargs git show |grep -a -v &#x27;^\\s+$&#x27;|wc -l;done REL6_1 270033 REL6_2 320297 REL6_3 386532 REL7_0 630771 REL7_1 843219 REL7_2 986991 REL7_3 1363668 REL7_4 1492418 REL8_0_0 1649775 REL8_1_0 1702325 REL8_2_0 1806170 REL8_3_0 2017685 REL8_4_0 1924918 REL9_0_0 2011704 REL9_1_0 2225796 REL9_2_0 2290872 REL9_3_0 2405598 REL9_4_0 2487304 REL9_5_0 2527906 REL9_6_0 2632559 REL_10_0 2534653 REL_11_0 2771914 REL_12_0 2697892 REL_13_0 2822066 REL_14_0 2980221 REL_15_0 3054963 REL_16_0 3351147This is counting non-empty lines. It&#x27;s definitely not a good measure of overall code size, as it includes things like regression tests \"expected\" files. But as that&#x27;s true for all versions, it should still allow for a decent comparison.8.3.0 was released 2008-02-01, with 2M non-empty lines, we&#x27;re now at 3.4M. reply pcthrowaway 14 hours agorootparentI suspect you&#x27;d get much more useful results by checking out the version tags and running `cloc` - https:&#x2F;&#x2F;github.com&#x2F;AlDanial&#x2F;cloc reply misiek08 3 hours agoprevMy observation is: most of people coming to IT these days are just money greedy, no enthusiasts anymore. It’s very sad and it seems like many open source projects die because of that - no back contributions or any help, only “copy and paste from SO and get paycheck”. I’m not saying everyone is like that by my close observations and conversations in people from few companies I contributed in show me 19:1 ratio…Disclaimer: I work for two companies every day because had so much free time because did my job too fast for standards and only wasted time waiting for meetings. Did a lot of side jobs to do something interesting, many time for free or to just use some new hardware and experiment with. reply nijave 2 hours agoparentI think companies are partially to blame. Contracts filled with invention assignment clause and outside activities clauses raise the barrier to contribution. reply bagasme 8 hours agoprev> Postgres isn’t in any sense struggling to attract new users – there are plenty of 22 year olds defaulting to it today.Yes, I do use Postgres for a lot of my self-hosted apps. For PHP applications, though, I keep using MariaDB as their default&#x2F;only DB. reply tristan957 14 hours agoprevI am a new contributor to Postgres as of 5 months ago. I turn 27 later this month. While I haven&#x27;t contributed too much of value, I have a few commits here and there with plans to do other things, like make it easy to build Postgres extensions with Meson and hopefully drop the autotools build of Postgres ASAP. You might also catch me in the pgbouncer or pgvector repositories very soon.The story of how I came to contribute to Postgres is that I got tired of working at a software consultancy where I had worked for 3 years, 2 years during college, 1 year full-time. I had always more envisioned myself as an open-source systems software guy. I found a job at Micron working on an open-source storage engine. It kind of felt like luck to have found that job to be honest, but I felt like the job description was written for me, so I applied. I worked on that project for 2.5 years and loved it. Unfortunately, Micron laid off my entire team back at the end of February. I began searching for a job, and eventually received an offer from MongoDB to work on the C&#x2F;C++ drivers, but that was rescinded. Then, I really started to lean into my network, and one guy I knew from #mesonbuild on Libera.Chat&#x2F;Matrix worked on Postgres, so I messaged him and asked if there were any positions open in the Postgres space for someone with my background that he might be aware of. He told me that Neon was hiring, so I sent an application to work on their storage engine, but in the initial interview, my eventual manager thought that I would fit better in the new Postgres team they were forming, which would contribute upstream to Postgres. I am very appreciative to Neon for taking a chance on me.The topic of this blog post is interesting because it just came up during some discussions with other young Postgres contributors at PGConf NYC. Some points that we brought up:- It is hard to get patches looked at, even small ones. With better name recognition in the community, comes more patch reviews it seems, which is most likely the case in most projects, but still, it is a circular issue.- The organization of the Postgres mailing list is not very good. You are forced to drink from the firehose that is pgsql-hackers, whereas the LKML is organized into various subsystems. Modern code forges have value in that you can subscribe to certain tags on PRs&#x2F;issues, which isn&#x27;t the case with the current state of pgsql-hackers.- Adding things to a commitfest is a little burdensome. The only way to get a patch through the entire Postgres CI is adding it to the commitfest, and even then, you have to be proactive about checking it, or hope that a committer will tell you to look at the CI failure.- Bug reports are also sent to a mailing list (pgsql-bugs). There is no equivalent to the Linux bugzilla for Postgres for instance.- Patches are sent as attachments to emails, and not necessarily git-format-patched either, whereas the LKML uses git-send-email exclusively from what I can tell.All in all, it kind of seems like tooling in the Postgres contributor community works best for those that have been ingrained in it for 15+ years, which I guess is the case for most things. I don&#x27;t want this to turn into a \"Use GitHub&#x2F;GitLab\" post. Let it be known that I actually think email is the superior way to communicate about patches, but the tooling around the mailing list could improve. Everything seems very disjoint. SourceHut, I think, has done a good job of making mailing list development more approachable for the everyday contributor. Issues, mailing lists, CI&#x2F;CD, and repositories are all connected to each other. There aren&#x27;t separate services like they currently are for Postgres.This comment is probably worth a blog post of its own at some point, but I&#x27;ll end it here. If you are also new to contributing to Postgres, perhaps we can share experiences. Email me at tristanneon.tech or tristanpartin.io. Another Postgres contributor that I talked to thought it might be useful to have monthly meetings among non-committer contributors where we can talk about the patches we are working or have posted in order to get reviews from peers. reply andatki 5 hours agoparentThat last idea is really good Tristan and I bet you’ll have interest. You could lead an online meetup. Melanie Plageman was interested in ideas like that as well. We briefly discussed different types of office hours. reply anarazel 10 hours agoparentprev> - It is hard to get patches looked at, even small ones. With better name recognition in the community, comes more patch reviews it seems, which is most likely the case in most projects, but still, it is a circular issue.I agree that this is a significant issue. I&#x27;m less sure about the \"better name recognition\" bit, I feel there&#x27;s also a significant drop off at the other end. But that might just be biased by my level of experience.> - The organization of the Postgres mailing list is not very good. You are forced to drink from the firehose that is pgsql-hackers, whereas the LKML is organized into various subsystems. Modern code forges have value in that you can subscribe to certain tags on PRs&#x2F;issues, which isn&#x27;t the case with the current state of pgsql-hackers.Yep. And it has gotten a lot worse in the last couple years, I&#x27;d say.> - Adding things to a commitfest is a little burdensome. The only way to get a patch through the entire Postgres CI is adding it to the commitfest, and even then, you have to be proactive about checking it, or hope that a committer will tell you to look at the CI failure.My main reason to reply here was this: You can also enable CI in your repository: https:&#x2F;&#x2F;github.com&#x2F;postgres&#x2F;postgres&#x2F;blob&#x2F;master&#x2F;src&#x2F;tools&#x2F;c... - that&#x27;s the same CI that happens for commitfest entries.> - Bug reports are also sent to a mailing list (pgsql-bugs). There is no equivalent to the Linux bugzilla for Postgres for instance.Yep, I hate this. I loose track of things all the time, I&#x27;m way too easily distractable. I think the kernel bugzilla is pretty useless, but it&#x27;s not that hard to do better than that.> - Patches are sent as attachments to emails, and not necessarily git-format-patched either, whereas the LKML uses git-send-email exclusively from what I can tell.I find lkml style patch handling bad as well, particularly with every patchset revision getting its own thread. Very easy to loose track.> All in all, it kind of seems like tooling in the Postgres contributor community works best for those that have been ingrained in it for 15+ years, which I guess is the case for most things.I personally wouldn&#x27;t say it works particularly well, even after participating in development for about 15 years... I&#x27;d also say that the development process has evolved some in that time, just not as far as it&#x27;d be good. It&#x27;s a lot of hard work to get a community as grey-beardy as the PG community to evolve. Not impossible, but ...> I don&#x27;t want this to turn into a \"Use GitHub&#x2F;GitLab\" post.Personally I strongly dislike using either for nontrivial work. But: I still think we ought to accept PRs&#x2F;MRs via one of the two, just to make it easier for newer contributors. But it isn&#x27;t just my call...> Let it be known that I actually think email is the superior way to communicate about patches, but the tooling around the mailing list could improve.I suspect you&#x27;d actually have a hard time finding more than 2-3 people disagreeing with that notion. One of the problems is that many of us end up preferring to spend time hacking on postgres than on development-process tooling &#x2F; integration.... reply DrDroop 18 hours agoprevI&#x27;ve done a little bit of postgres work with the help of pgrx and I can recommend it as a platform to build data solutions. Another great resource has been the CMU channel: https:&#x2F;&#x2F;www.youtube.com&#x2F;@CMUDatabaseGroup reply gavinray 16 hours agoparentpgrx unfortunately has essentially zero documentation or samples on use outside of extensions.For instance, say you want to write a new Table Access Method handler. There are bindings to TableAM stuff in the core pg-sys SDK but no docs or examples on using it in Rust. reply zombodb 10 hours agorootparentI don’t think we even expose the TableAM APIs? They are incredibly hard to generate bindings for from the C headers — lots of inline functions and complex #define macros.We have an ambitious goal with pgrx and it’s going to take many years and countless hours of developer effort to get there.It can, however, serve as a way for newcomers to gain experience with Postgres internals.Regarding the TableAM specifically, when we are able to create a safe Rust wrapper around it, that wrapper will be documented. reply gavinray 10 hours agorootparentAh, curious what this bit does then, I think I might have misunderstood it:https:&#x2F;&#x2F;github.com&#x2F;pgcentralfoundation&#x2F;pgrx&#x2F;blob&#x2F;c2eac033856... reply zombodb 9 hours agorootparentI think maybe what you’re really looking for are the files here: https:&#x2F;&#x2F;github.com&#x2F;pgcentralfoundation&#x2F;pgrx&#x2F;tree&#x2F;c2eac033856...Those are the internals we currently expose as unsafe “sys” bindings.As we&#x2F;contributors identify more that are desired we add them.pgrx’ focus is on providing safe wrappers and general interfaces to the Postgres internals, which is the bulk of our work and is what will take many years.As unsafe bindings go, we could just expose everything, and likely eventually will. There’s just some practical management concerns around doing that without a better namespace organization —- something we’ve been working.The Postgres sources are not small. They are very complex, inconsistent in places, and often follow patterns that are specific to Postgres and not easy to generalize.If you’ve never built an extension with pgrx, give it a shot one afternoon. It’s very exciting to see your own code running in your database. replykumarvvr 8 hours agoprevAs an interested programmer, but with no experience in C&#x2F;C++, I would really love to have a series of videos explaining the code in detail, so that I can begin to contribute. reply alberth 10 hours agoprevWhat limitations does Postgres have that would cause you to use another data store?Or what big feature &#x2F; functionally is Postgres missing (if any, like replication?) reply mattashii 9 hours agoparentAssuming you mean replication (as defined in https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;warm-standby.html#ST... or https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;logical-replication....), then PostgreSQL has it (see those docs for details). If you mean something else, could you elaborate? reply nerdbaggy 10 hours agoparentprevReplication for sure reply cannonpalms 10 hours agorootparentWould you mind expanding on this? In what ways is PG&#x27;s replication support lacking versus the competition? reply choffman 14 hours agoprevArticle says Postgres launched in 1986. Wikipedia says 1996. reply tristan957 13 hours agoparent> The implementation of POSTGRES began in 1986https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;history.htmlPostgres started as a project at Berkeley. reply fanf2 14 hours agoparentprevThe POSTGRES project at Berkeley started in 1986, the open source successor became PostgreSQL in 1996. reply throwjabdb 13 hours agoprevTL;DR PostgreSQL are aging and Neon os hiring and training juniors instead of existing commiters so the developer base grows. reply dist-epoch 16 hours agoprev [–] > Who is going to be doing the work in 2043?ChatGPT. It won&#x27;t matter anyway since we won&#x27;t be around to worry about this question. reply DiggyJohnson 2 hours agoparent [–] 2043 is only 20 years away. Presumably many of us will be around and productive in that year. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article stresses the necessity for a younger generation of contributors, committers, and maintainers to support the aging PostgreSQL developer community.",
      "The importance of open source sustainability is underscored, along with the potential influence of companies like Neon that invest in enhancing Postgres and can relicense their intellectual property.",
      "For the continued success of projects like Postgres, the author recommends intentionality, funding, and enlightened self-interest."
    ],
    "commentSummary": [
      "The article sheds light on the commitment and enthusiasm by the Postgres community towards the open source database system, whilst acknowledging the challenges faced.",
      "It discusses the high entry barriers for new contributors, primarily due to the shortage of proficient C developers and the complexity involved in learning and using C.",
      "Also, it touches upon the organizational issues in the Postgres mailing list, limitations, and absent features in Postgres, indicating areas of improvement."
    ],
    "points": 273,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1696946928
  },
  {
    "id": 37832585,
    "title": "Passkeys are now enabled by default for Google users",
    "originLink": "https://blog.google/technology/safety-security/passkeys-default-google-accounts/",
    "originBody": "blog.google uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more. Hide Skip to main content The Keyword Latest stories Product updates Company news Subscribe SAFETY & SECURITY Passwordless by default: Make the switch to passkeys Oct 10, 2023 2 min read For Cybersecurity Awareness Month we’re making it even easier for users to get started with passkeys S Sriram Karra Senior Product Manager C Christiaan Brand Group Product Manager Share Earlier this year we rolled out support for passkeys, a simpler and more secure way to sign into your accounts online. We’ve received really positive feedback from our users, so today we’re making passkeys even more accessible by offering them as the default option across personal Google Accounts. This means the next time you sign in to your account, you’ll start seeing prompts to create and use passkeys, simplifying your future sign-ins. It also means you’ll see the “Skip password when possible” option toggled on in your Google Account settings. To use passkeys, you just use a fingerprint, face scan or pin to unlock your device, and they are 40% faster than passwords — and rely on a type of cryptography that makes them more secure. But while they’re a big step forward, we know that new technologies take time to catch on — so passwords may be around for a little while. That's why people will still be given the option to use a password to sign in and may opt-out of passkeys by turning off “Skip password when possible.” We’ve found that one of the most immediate benefits of passkeys is that they spare people the headache of remembering all those numbers and special characters in passwords. They’re also phishing resistant. Passkeys in more places Since launching earlier this year, people have used passkeys on their favorite apps like YouTube, Search and Maps, and we’re encouraged by the results. We’re even more excited to see the growing adoption of passkeys across industry. Recently, Uber and eBay have enabled passkeys — giving people the option to ditch passwords when signing-in on their platforms — and WhatsApp compatibility will also be coming soon. 1 2 3 The passwordless road ahead We’ll keep you updated on where else you can start using passkeys across other online accounts. In the meantime, we’ll continue encouraging the industry to make the pivot to passkeys — making passwords a rarity, and eventually obsolete. For more information on how you’re safer with Google, visit myaccount.google.com/safer. POSTED IN: Safety & Security Related stories ASK A TECHSPERT Ask a Techspert: What are passkeys? By Eben Carle Oct 10, 2023 GMAIL New Gmail protections for a safer, less spammy inbox By Neil Kumaran Oct 03, 2023 GOOGLE IN EUROPE Complying with the Digital Services Act By Laurie Richardson Jennifer Flannery O'Connor Aug 24, 2023 EDUCATION New updates to ensure safer learning at school and at home By Jennifer Holland Aug 22, 2023 ANDROID ENTERPRISE Protect your business with Zero Trust security on Android By Al Chappelle Eugene Liderman Aug 08, 2023 SEARCH New privacy tools to help you stay safe and in control online By Danielle Romain Aug 03, 2023 Follow Us Privacy Terms About Google Google Products About the Keyword Help Deutsch English English (Africa) English (Australia) English (Canada) English (India) English (MENA) Español (Latinoamérica) Français (Canada) Français (France) Italiano Português (Brasil) اللغة العربية (MENA)",
    "commentLink": "https://news.ycombinator.com/item?id=37832585",
    "commentBody": "Passkeys are now enabled by default for Google usersHacker NewspastloginPasskeys are now enabled by default for Google users (blog.google) 272 points by vdelitz 19 hours ago| hidepastfavorite565 comments jiggawatts 12 hours agoAs others have pointed out, cryptographic authentication is very hard to bootstrap if you simply loose your device.Just last month my missus cracked the glass of her iPhone. Apple repaired it under AppleCare, which is great… except… that they didn’t tell her that the “glass repair” entails them replacing the guts of the phone and wiping it in the process.Apple iPhone backups don’t contain cryptographic secrets like eSIMs!She got stuck in a loop where she couldn’t activate her eSIM because that needed her email, but her email needed MS Authenticator, which she couldn’t activate without an SMS.She had to drive to the Telco with a pile of photo ID to reissue her eSIM. Her bank account got locked in the process despite the password being correct because of some sort of phone hardware lock.This took days to fix and multiple in-person visits to various organisations. If this had happened while overseas on holiday, she would have been screwed.Times have changed.Your entire digital identity is now a smart card in your phonesThat Smart Card is either a SIM card or an onboard TPM chip, but in any event if you lose it, you may as well be dead as far as anyone else is concerned.Passkeys make this much worse. At least if you still have a physical SIM you can transfer it from any phone to any other phone.Passkeys are not cross-vendor transferable!Run away screaming. Don’t believe the hype. Wait until the vendors get their act together and come up with a solution for transfer and recovery. reply jjav 10 hours agoparent> Run away screaming. Don’t believe the hype. Wait until the vendors get their act together and come up with a solution for transfer and recovery.Very much this. Having authentication tied to hardware you don&#x27;t control is a near-certain denial of service in the future.People love to hate on passwords but the reality is that for many circumstances (threat models) they are the best compromise. You can make them more than strong enough (take 32+ bytes out of &#x2F;dev&#x2F;random and encode however you like, nobody will ever brute force that in this universe) and various passwords managers solve the problem of re-use (never reuse a password).And it comes with the benefit that you control how it is stored and can apply as much redundancy as you want to feel comfortable. reply KronisLV 1 hour agorootparent> People love to hate on passwords but the reality is that for many circumstances (threat models) they are the best compromise. You can make them more than strong enough (take 32+ bytes out of &#x2F;dev&#x2F;random and encode however you like, nobody will ever brute force that in this universe) and various passwords managers solve the problem of re-use (never reuse a password).> And it comes with the benefit that you control how it is stored and can apply as much redundancy as you want to feel comfortable.Honestly, I agree!I used KeePass back in the day (https:&#x2F;&#x2F;keepass.info&#x2F;) but now use KeePassXC (https:&#x2F;&#x2F;keepassxc.org&#x2F;) and it&#x27;s really nice - I don&#x27;t know any of my passwords because they&#x27;re all randomly generated and are pretty secure, in addition to them being unique for every account. The only one I have to remember is my main password for decrypting the safe, which I also wrote down and entrusted to someone close to me due to its complexity.It honestly works great, software to interact with the password safe is on every platform where I need it to be, in addition to it being super easy to reason about storage, because it&#x27;s basically just a file - that I can then put on self-hosted Nextcloud, or another solution like that, or USB sticks or burn to CDs for all I care.Maybe I should also migrate all of my TOTP stuff over to it and look into good Android apps at some point, then I wouldn&#x27;t quite need Google Authenticator or FreeOTP anymore, either. reply stavros 8 hours agorootparentprev> Very much this. Having authentication tied to hardware you don&#x27;t control is a near-certain denial of service in the future.Then you can tie it to hardware you do control, or to software.Obligatory \"Passkeys misconceptions\" article: https:&#x2F;&#x2F;www.stavros.io&#x2F;posts&#x2F;clearing-up-some-passkeys-misco... reply 90-00-09 7 hours agorootparentHow does this address OP&#x27;s concern? If you have a single device (e.g. an iPhone) and you store all your passkeys on it, then losing it means you lost all passkeys. Your post describes exactly that:- \"I can’t recover the keys if I lose the hardware\"- \"That is a risk you’ll need to take if you’re using hardware authenticators\"Fantasizing that with this proposed simplification of the authentication process people will introduce complexities such as a password manager or a backup hardware device is naive to say the least. reply stavros 7 hours agorootparentI haven&#x27;t heard anyone claim that passkeys are simpler than passwords, as that would be trivially false. The claim is that they&#x27;re more secure while still remaining fairly usable.Passkeys are WebAuthn credentials that are synced between devices, so they aren&#x27;t hardware keys, they&#x27;re software keys. reply jjav 4 hours agorootparent> more secure\"more secure\" is a completely meaningless statement, I wish this usage would die already (in general).You need to talk about security in the face of a very specific threat, then you can say solution A is better than solution B against threat T1, worse for T2 and about a wash for T3 and so on.Security is not a linear scale from 0-100 where you can say \"more secure\". There are many different criteria and any given solution will be better in some, worse in others. You must do a threat model for your specific use case to say if something is better or worse for those specific threats, and keep in mind other people will have very different threat models for the same solution. reply jesseendahl 2 hours agorootparentThreat #1: Credential theft from server breachesThreat #2: User creates a weak credentialThreat #3: User reuses a credential (uses same credential across multiple services)Threat #4: PhishingAttackers use huge password dumps compiled from multiple server breaches, and then try them against other services. Relying on a combination of the fruits of their labor from all four threats, attackers successfully compromise millions of accounts on the internet every year.If you want to see the data, check out the Verizon Data Breach Investigation Report that comes out every year.*These threats affect a majority of both consumers and enterprises.Passkeys address all four of these major real-world threats. Passwords address none of them.Threat #1 mitigation: with passkeys, only a public key is stored on the server. Attackers can steal all the public keys they want; it will not help them compromise any user&#x27;s account.Threat #2 mitigation: passkeys (which are WebAuthn credentials) are guaranteed to be cryptographically strong. It is not possible for a user to generate an insecure passkey. This is because the browser the the Operating System APIs take care of generating the credential.Threat #3 mitigation: passkeys (which are WebAuthn credentials) are guaranteed to be unique. It is not possible for a user to reuse the same passkey across multiple apps&#x2F;websites. This is because the browser the the Operating System APIs take care of ensuring that a new, unique credential (passkey) is generated for every new app&#x2F;website the user sign&#x27;s into.Threat #4 mitigation: passkeys (and all WebAuthn credentials) are bound to the server FQDN at the time they are created. The browser and Operating System APIs take care of ensuring the credential is only ever sent to the app&#x2F;website server it was created for. Users cannot be tricked (via phishing) into using their passkey on a malicious app&#x2F;website controlled by an attacker.* https:&#x2F;&#x2F;www.verizon.com&#x2F;business&#x2F;resources&#x2F;T249&#x2F;reports&#x2F;2023... reply sanitycheck 23 minutes agorootparentYou are correct about all of that.But personally, as a technically able user, my risk of randomly losing access to my Google (or MS, Apple, Meta, etc) account is far greater than from all those threats combined.If we had a trustworthy and accountable authority operating this stuff then it would be great. But we don&#x27;t, we have a bunch of companies who are neither of those things.It&#x27;s like mandating that everyone must use self-driving cars that are on average safer than human motorists but occasionally randomly drive off a cliff. reply keskival 2 hours agorootparentprevThe most problematic and the most probable security risk I have relating to logins is losing access. It for example took a week to restore access to my Apple account after I had forgotten to update my phone number there.Since this is the greatest security problem, I would hope all the vendors trying to improve security would focus on that. reply raxxorraxor 2 hours agorootparentprevThey are not more secure from a cryptographic standpoint. There are different attack vectors and for some passkeys are superior but in others they are certainly not.Additionally, part of the security concern is also accessibility by yourself.edit: Just tried the Google passkeys on one of my Android phones. It is a complete usability hell and it seems I cannot opt out again without logging in from my browser and deleting my \"device\". If there is another way to just do it from my device without an additional browser, please tell. reply jiggawatts 2 hours agorootparentprev“Hardware vendors helpfully suggest buying more hardware as a solution to a problem they introduced.” reply endgame 6 hours agorootparentprevWhy does this article claim that attestation is unlikely? We know Google loves the idea - see Web Environment Integrity (WEI).Also, what&#x27;s stopping us from falling into the passkey version of the world we got with OpenID, where many services force you to log in with your BigTech account? reply lcnPylGDnU4H9OF 6 hours agorootparent> Why does this article claim that attestation is unlikely?Facebook is still going to want me on their websites even if I’m running Firefox. Most websites people visit will not do any chrome WEI attestation. Likely exceptions are sites which handle any legal, financial, or health-related data. Not credit cards. I doubt most Google properties will use WEI.They still want to slurp up all my juicy Firefox usage data and I bet they think a lot of such users will drop their services like a rock if it meant otherwise dropping their browser. reply RedComet 4 hours agorootparentI&#x27;m willing to bet otherwise (w.r.t. your first statement). They probably consider the sliver of such users expendable. They probably also (rightly) assume that a significant percentage of that sliver will continue using their service (e.g. via sanctioned Chrome on sanctioned hardware) if push comes to shove.Or at least they will at some point in the near future. reply josephcsible 7 hours agorootparentprevThat article is about as misleading as it&#x27;s possible to be while still being technically right. The \"attestation\" feature of passkeys exists solely to let websites refuse to let you use passkeys tied to hardware you do control, or to software. The way this article only mentions it in passing and tries to downplay it reminds me of the joke of https:&#x2F;&#x2F;what-if.xkcd.com&#x2F;49&#x2F; - a very long article listing a bunch of upsides of the Sun going out, and only one sentence about the downside: \"We would all freeze and die.\" reply stavros 7 hours agorootparentI guess we&#x27;ll have to wait and see whether websites will force people to use specific authenticators. I don&#x27;t think they will. reply julianwachholz 4 hours agorootparentWe can already see this to some degree. On my Android device, Chrome will only let me create a synchronized passkey in a Google account UNLESS attachment is explicitly set to \"cross-plattform\" - even though not specifying the option is supposed to allow all types.You can try this out on webauthn.io and changing the attachment setting. reply RockRobotRock 7 hours agorootparentprevNon-tech people have a fundamental misunderstanding of what makes a good password, and stupid IT policies like password expiration lead to really bad habits like frequently forgetting passwords, frequently reusing passwords, and end with writing it down on a post-it note. reply EGreg 9 hours agorootparentprevJust like the rest of it, they’re going to try to lock down the open web, general-purpose computing, etc.They are going to be the gatekeepers if you and the web services let them. Oh yeah — also they’ll run all the web, email and other services anyway. Trap you in their metaverse and AI most likely, since that’s where your coworkers and friends will be you’ll have to be there too.Resist by opting out :) reply paiute 7 hours agorootparentI think it’s time for a government solution, but nowadays it’d be done to be benefit big tech and the surveillance state. reply EGreg 6 hours agorootparentHow about an open source one?https:&#x2F;&#x2F;intercoin.org&#x2F;overview.pdf reply artursapek 8 hours agorootparentprevThere are existing cryptographically secured systems which have scaled to many users, like Bitcoin wallets and Keybase accounts, and do have a recovery fallback. It&#x27;s usually called a \"seed phrase\" or \"paper key\"... which is really just a password! :D reply Shank 11 hours agoparentprev> Run away screaming. Don’t believe the hype. Wait until the vendors get their act together and come up with a solution for transfer and recovery.I believe all of the issues you&#x27;ve described, but you can usually add multiple passkeys to each service. There is nothing stopping you from adding your iPhone and a cheap android phone and having redundancy, or using 1Password and storing your passkey in there.iPhone backups do store backups of the media stored in iCloud Keychain, if you have another apple device or if you have the recovery key, you can get back in. You just need the device passcode or recovery key and you can re-bootstrap everything. eSIMs are unique because they&#x27;re carrier things and those things have and always will be a pain and tied to stores and phone calls. reply abdullahkhalids 10 hours agorootparent> I believe all of the issues you&#x27;ve described, but you can usually add multiple passkeys to each service.How does this work? Do I have to visit the website of each service from my secondary device for it to get the alternate passkey? reply bombcar 10 hours agorootparentYes. You go an add secondary passkey when logged in on another device. Or with another tubikey.But not all setups support this. Some only allow one. Obvious issues abound. reply FireBeyond 9 hours agorootparent> But not all setups support this. Some only allow one. Obvious issues abound.I&#x27;d go as far as to say \"most setups don&#x27;t support this. Most only allow one\".The services I&#x27;ve seen so far that support multiple passkeys are in the minority. reply vel0city 8 hours agorootparentThe majority of sites I&#x27;ve used that supported U2F&#x2F;passkeys&#x2F;yubikeys&#x2F;webauthn support multiple. In fact the vast majority I&#x27;ve used supported multiple, only a few outliers only supported one. reply joshuamorton 8 hours agorootparentprevPasskeys and u2f keys aren&#x27;t the same. Systems must support multiple passkeys, otherwise you could only access the service from a single device, since passkeys are usually tied to a particular piece of hardware. reply freeone3000 4 hours agorootparentNevertheless, nothing forces my banking app to accept a second PassKey other than the one linked to FaceID. When I buy a new phone, I need to re-bootstrap auth from zero. There’s no way to store two. reply joshuamorton 2 hours agorootparentIs the app the only way you interface with your bank? It doesn&#x27;t have a website?All of the things I&#x27;ve used passkeys on support both web and mobile auth, which necessitates two keys. reply artursapek 8 hours agorootparentprevI can&#x27;t stand services that only support a single U2F key (cough AWS cough) reply omgwtfbyobbq 7 hours agorootparentprevYup. I&#x27;ve inadvertently done this, and should probably add a third device. reply selcuka 11 hours agorootparentprevThere are workarounds, but that doesn&#x27;t mean that passkeys is a half-baked technology. The real, simple solution would be a way to write down the passkey, similar to an SSH private key. reply metafunctor 11 hours agorootparentA main idea of passkeys is that the private keys are bound to hardware and cannot be copied. Using the private key is subject to biometric authentication. This eliminates a whole category of issues where the private key could get stolen.So no, writing down the SSH private key is not the solution. The solution is to trust multiple private keys, each stored within tamperproof hardware.This is also why, as a service provider, I&#x27;d like to see some device attestation. I want to know that the keys being used here are not written on a fucking piece of paper. reply hypothesis 10 hours agorootparent> This is also why, as a service provider, I&#x27;d like to see some device attestation. I want to know that the keys being used here are not written on a fucking piece of paper.This is precisely why user should run away. Service provider is moving liability to end user and washing their hand away, while user gets screwed if anything happens during vacation. reply tadfisher 7 hours agorootparentEnd user also gets screwed when they are phished for their paper key. And I&#x27;m not sure about liability, unless you consider the requirement to check haveibeenpwned once a week for a breach to be no one&#x27;s responsibility. reply postalrat 8 hours agorootparentprevStill beats passwords in ever meaningful way. reply 0cf8612b2e1e 10 hours agorootparentprevIt sounds like I am up the creek if all of my devices are gone.With a bank, if I lose paperwork, they will have a process in place for me to prove my identity. BigTech will shrug if my phone-locked passkey becomes inaccessible. reply FireBeyond 9 hours agorootparent100%.The effort and hassle for Google et al to invest in robust support mechanisms (backend and people) for passkeys makes it highly unlikely.No doubt you&#x27;ll get the standard boilerplate email responses, if you are even that lucky, that just point you to an FAQ or something similarly unhelpful. reply ungamedplayer 8 hours agorootparentprevI recently watched a movie called the circle with Emma Watson where they want to tie the account with a corporation as a means of Id to register to vote.Imagine leaving identity to a corporate who simply shrugs off all but legal threats. It&#x27;s terrifying and I reckon we are in our way there reply arzig 8 hours agorootparentI strongly recommend the book. It’s considerably better and gets into the dystopia better. reply insanitybit 2 hours agorootparentprevThe initial poster described that process - you bring government IDs in person to an office.If you want to avoid that, just set up multiple devices. reply idle_zealot 10 hours agorootparentprev> as a service provider, I&#x27;d like to see some device attestationAs a user I hope you don&#x27;t get it. Having an easy way for services to require that everyone using them is doing so via the official app on an iPhone or OEM Android phone sounds like a nightmare. reply javagram 10 hours agorootparentprev> A main idea of passkeys is that the private keys are bound to hardware and cannot be copied.This seems incorrect. “ Like passwords, passkeys are encrypted and stored in your iCloud Keychain”I also just recently set up some passkeys via 1Password and they are also not hardware bound. reply brnt 10 hours agorootparentprevWhy is a piece of paper not a piece of hardware? We do this for TOTP too as a last resort, nothing wrong with that. reply joshuamorton 8 hours agorootparentTotp is phishable. Passkeys aren&#x27;t phishable. At the point where the user can access the private keys phishing is once again a concern.If I can&#x27;t access my pk, I cannot be phished. As soon as you allow me to copy my key (instead of creating many, which should be acceptable) I can be phished again. reply zappb 8 hours agorootparentprevWhat kinds of services would benefit from this level of security? I could see it being useful in corporate contexts (like locking down which machines are allowed to remotely control other machines), but not as much from a general consumer point of view. reply mr_toad 4 hours agorootparentAt least with enterprise IT, or a bank etc you can pester them until they let you back in. They’ll have to sort it out eventually. That’s not going to work with Google or most web services.Any web service that locks accounts to devices is going to be shedding customers as they lose or replace phones. reply eduction 10 hours agorootparentprev> The real, simple solution would be a way to write down the passkey, similar to an SSH private key.Except shorter for convenience. Something you could even memorize. reply hedora 7 hours agoparentprevTo save people from reading the article before running away screaming:> But while they’re a big step forward, we know that new technologies take time to catch on — so passwords may be around for a little while. That&#x27;s why people will still be given the option to use a password to sign in and may opt-out of passkeys by turning off “Skip password when possible.”So, soon passwords will be added to “Killed by Google,” along with my account. (I keep zero devices logged in.)It’s well past time to migrate off my few remaining use cases. I wonder if my employer will be able to reset my corporate account passkeys when the inevitable happens. reply jrockway 6 hours agoparentprevAs with everything, you probably want a backup. Get more than one passkey.I pretty much use 3; Yubikey in my workstation, portable Yubikey, phone. All 3 of those can bootstrap Google, which I use for email, and Apple, which I use for my phone. Then, everything else is in 1password, which are available through those mediums. Worst case, I am pretty sure in the most dire of dire emergencies, I can get my email back no matter what. Verify ID with my DNS provider, switch MX records, back in business. Even then, it&#x27;s not necessarily essential to daily life. (A colossal inconvenient to lose access? For sure. Death sentence? Probably not.) All my SMS and Signal contacts are elsewhere. I can spend money out of my bank account by writing a check. I can get into work stuff by showing up in person at an office.I do think that passkeys are probably too complicated for the ordinary user of computers; unfortunately that \"we&#x27;ll just email you a link every single time you want to sign in\" seems like the most user-friendly passwordless authentication.I also don&#x27;t feel great about my habit of putting passkeys in 1password, because I know I&#x27;m locked in forever. But, I like the service, and when I want to switch, welp, at least there&#x27;s a list of accounts I have to remake.My biggest fear is something like forgetting my phone&#x27;s passcode. One time I woke up, got distracted at just the wrong moment, and could not for the life of me remember my 6 digit passcode. (I also use the same code to unlock my workstation.) I had to distract myself and then use muscle memory to remember it. It was really crazy, truly one of those \"did I just have a stroke\" moments. I have that saved in 1password now, so if I have one unlocked device, I can refresh my memory. This happened a while ago and I don&#x27;t think I have dementia. Just a weird quirk.(Meanwhile, I can perfectly remember every 1-year-max-lifetime password I&#x27;ve ever had at any job. A lot of that good does when you can&#x27;t remember a 6 digit number!) reply qingcharles 12 hours agoparentprevAdd to that many web sites now make it a point of pride that they employ no humans in support and will not do anything to help you get back into your account if you are locked out (Google, Meta etc). reply KennyBlanken 9 hours agorootparentThey employ humans in support - behind firewalls like follower counts.If Neil deGrasse Tyson gets locked out of his Instagram, you can be damn well sure someone answers his support request.If you or I, in two to four digit follower counts, have an issue? We can get fucked.Damn near most companies do this with the social media PR teams, too. Any tags&#x2F;mentions, messages, etc are filtered through software that decides how much cloud you have and thus how worthy of attention you are. Delta loses your guitar and you&#x27;ve got 100 followers? Nobody in the social media team is likely to even see it. Someone with 5000 followers, and a post about it gets a couple hundred likes&#x2F;retweets? American is going to fall over themselves to make it right.That&#x27;s the great lie about social media - that you can use it to draw attention to a problem you&#x27;re having. Unless you&#x27;ve cultivated a large enough following, you&#x27;ll be completely ignored. reply two_handfuls 6 hours agoparentprevMy understanding is that Passkeys are transferrable, unlike earlier efforts. See for example this iOS help page: https:&#x2F;&#x2F;support.apple.com&#x2F;guide&#x2F;iphone&#x2F;passkeys-passwords-de...Unless you store the passkey in a hardware Fido key like a Yubikey. Then the way to transfer it is to physically carry the key and plug it to another device. reply d-z-m 3 hours agorootparentOP specifically mentions \"cross-vendor\" transferable. Which, to my understanding, is currently true. reply cpuguy83 9 hours agoparentprevThis is just bad and uninformed advice.Adding a passkey to an account is like adding a yubikey to an account (experience wise). You can (typically) add multiple keys to your account.It&#x27;s also not all or nothing. You can (in every service I&#x27;ve setup) still have a password and an even a TOTP. reply qudat 8 hours agorootparentThere is no way adding multiple keys to your account is more work than a password manager. Just use a password manager and retain full control over your secrets. reply davkan 5 hours agorootparentYeah I don’t really see the upside of these over just sticking with bitwarden. reply ungamedplayer 8 hours agorootparentprevCan, if the auth provider allows it. In my limited experience of 4 accounts with passkey, only one does.. kinda.The multiple passkey option kicks out my other session when i use the other passkey. reply skybrian 9 hours agoparentprevYour wife&#x27;s experience sounds very bad and the risk of getting locked out of your various accounts is serious.That doesn&#x27;t mean giving up on having good security, though. Passkeys don&#x27;t work like eSims and other users&#x27; situations might not be the same. Their failure modes will be different. They might have more than one device (like a phone and a tablet), or they might not use MS Authenticator for their email, or they might have set up different recovery methods?We need more backups and user education, which ideally would include rehearsing account recovery before it&#x27;s actually necessary. reply brundolf 11 hours agoparentprevHonestly, if they&#x27;d just give me the option to write it down (or take a picture or whatever) and manually restore it by typing it in if I need to, that would just about solve the issue reply eduction 10 hours agorootparentI like this idea of authenticating yourself by typing things in. reply raxxorraxor 26 minutes agorootparentAnd it would be completely independent of vendor or device. Should write a paper on it.Seriously, just tried passkey on an Android phone of mine with a burner account. I would not recommend this to anyone. Passkey as a tech might not be the issue, but the lacking option of just removing devices and passkeys freely from your account is just not acceptable. The vendor simply has different ideas about security than I have and it is not just restricted to serving me \"safer\" ads. reply depereo 8 hours agorootparentprevSeems like it&#x27;d be a little annoying to pick different things to type in for each service, maybe we could manage those, but still have a primary &#x27;thing to type in&#x27; to the &#x27;thing to type in&#x27; manager, which would then handle choosing and typing the various things into the various authentication boxes. reply paiute 7 hours agorootparentprevBut how can i be sure it’s really you? How can i track you that way? reply metafunctor 11 hours agorootparentprevThe point is that the private key resides on a tamperproof piece of hardware. Malware, viruses, or shoulder surfers cannot copy the key.The solution is to set up multiple pieces of secure hardware, not writing down the keys to the castle on a piece of paper. reply kahnclusions 10 hours agorootparentGreat so now people need to be rich enough to own multiple phones? Really. The solution can’t be “buy multiple devices” when the average person can barely afford to maintain one working device. reply Dlanv 10 hours agorootparentYour computer can also be a passkey. I currently use both my laptop and my computer as a passkey, and a USB drive. So I have 3 backups to my Google account.It is true that you do need to be rich enough to own a phone and ~100 USD of something else (laptop or USB), which does put redundancy out of the reach of a large portion of the world. But then they can just use regular 2fa at the expense of not being phishing-proof. reply svieira 8 hours agorootparentYes, but in order to add new items to each piece of hardware you have to be physically co-located with all the pieces of hardware you want to use as your backups. Which means they cannot be geographically distributed (or if they are that there is a period of time in which you aren&#x27;t fully backed up). Which means you&#x27;re either in a place where you can loose all your keys (e. g. a house fire or a flood) or your in a place where you can loose all the devices that have a key. reply tmpX7dMeXU 9 hours agorootparentprevYou have to be within several layers of bubbles to not see how small a percent of the general population are going to even understand any of this BS.Things being this complicated makes them a non-starter. A nerd vanity project.And this isn’t a knock on the “intelligence” of the general population. They quite rightfully won’t want to spend their limited time on God’s earth learning about all this. reply brundolf 9 hours agorootparentprevIt&#x27;s still a stronger key than virtually any password would be, and is (to my understanding) resistant to data breachesBut information sovereignty is crucial when it comes to this stuff; losing that is a regression that will cause problems reply diogenes4 9 hours agorootparentprevI&#x27;m much more concerned about my access being tied to physical hardware than I am about \"malware\". reply rjh29 9 hours agorootparentprevGoogle has backup codes for this purpose. reply RockRobotRock 7 hours agoparentprevAs an aside, I bought a hardware TOTP token that I could have access to if i got locked out Bitwarden&#x2F;Authy: https:&#x2F;&#x2F;www.amazon.com&#x2F;gp&#x2F;product&#x2F;B07RQPJNZHIt would be awesome to have an \"emergency\" server where I could type in the URL, decrypt it with my passphrase and OTP, and get access to everything I need temporarily so I can re-bootstrap all my stuff. Of course, this doesn&#x27;t solve the problem of SMS 2fa being used for everything, but it&#x27;s a good first step.I am in favor of crossplatform solutions like YubiKey. Apple and Google passkeys are lame. reply bhawks 6 hours agoparentprevThe solution to your problem is simply more passkeys.I am not being sarcastic - which ever service your authenticating to make sure you have passkeys from at least 2 different devices so you do not lock yourself out.If you don&#x27;t fit into this multi device assumption, passkeys are not going to work well for you. There will not be a standard for transfer &#x2F; recovery. reply raxxorraxor 22 minutes agorootparentThat is completely unrealistic to me. The normal users will not register different devices and will simply be locked out when the device fails or is lost.And the question they will ask is about the need to have two passwords. reply solatic 5 hours agoparentprevPasskeys follow the 3-2-1 backup rule, just like any other digital data. The main difference being that you don&#x27;t need to backup the passkey itself, just have multiple passkeys. Have 3 passkeys 2 of them on-person at any time (e.g. one on your phone TPM, one on a Yubikey) 1 of them off-site (e.g. keep a backup Yubikey at home in a fireproof safe, or use a 1Password passkey, depending on your threat model)Whenever you sign up for a new vendor&#x2F;service, register all three passkeys with your account. reply stouset 5 hours agorootparentI do follow this. Unfortunately, it leaves out one glaring flaw: you can’t register a Passkey you don’t physically have.I use four: an Apple Passkey, a YubiKey I keep on me, a YubiKey at home, and a YubiKey in the bank. When I sign up for a service, I need to register all four of them. Not only is this generally a bit of a pain in the ass, but it also means I have to remember to go fetch the one in the bank vault periodically and update the credentials.If I could save a stub locally that would let me register with a key not in my physical presence, that would go a long way to making this more usable. Even better would be the ability to register a bundle of them all in one go without having to do it four separate times.As it stands right now, it’s hard to recommend to users who don’t understand or care enough to take all of these steps. Which to be clear is entirely reasonable on their part. It’s an unacceptable amount of work and mental accounting for it to be something the average person can do without high risk of losing their entire digital identity. reply johnmaguire 5 hours agorootparentprev... or just store a password in your password manager.I thought passkeys were supposed to be convenient to use. reply Brajeshwar 6 hours agoparentprevOk. Now, I need to re-read and I’m tad worried.I moved quite a bit of logins to Passkey and I chose to stay with the Apple ecosystem as my Passkey Lord&#x2F;God. So far, it has worked and I have moved between devices (desktops, mobile, and the in-betweener).Assuming I’m going to stay for quite a while with the Apple Ecosystem, am I doing it wrong by making my Passkeys pass through my Apple ID?For instance, I change my eSim or number or replace phone, won&#x27;t accept next time I login and then verify from the laptop, desktop, iPad, watch, or, heck, the Apple Polishing Cloth? (Assuming the cloth will become a smart cloth eventually). reply sandeep_random 4 hours agoparentprevPasskeys are one of the non-phishable means for authentication. If something is easy to recover for user then its same for a malicious actor. Some platform based passkeys (apple, google) are actually sync-able across the devices. The whole Passkeys concept is under debate and discussion for what it means for different types of WebAuthn authenticators when it comes to the ability to sync the credentials. Alternatively one can use security keys which they can keep with themselves and could protect themselves by enrolling one additional security key for recovery purposes that they can keep away. Regardless the whole idea is to have more than one MFA factors enrolled so that one is not get locked out. Ease of using WebAuthn&#x2F; Passkeys overweighs typing in password, SMS, TOTP codes and has big savings for big players to avoid phishing attacks. It might not be suitable for every use case but worth using for some. reply v7p1Qbt1im 3 hours agoparentprev> Passkeys are not cross-vendor transferable!They are when using a third party password manager like 1password or dashlane. At least in they are device agnostic. Haven‘t yet tried to export a passkey to another manager. reply negative_zero 6 hours agoparentprevCompletely agree. Currently I can perform a full bootstrap using information stored in my brain (with my partner&#x27;s brain as backup). Any new \"solution to passwords\" that doesn&#x27;t allow that means an instant NO from me. I don&#x27;t care how much more theoretically secure it is. reply samcat116 7 hours agoparentprevAn eSIM isn&#x27;t a cryptographic secret you need to backup its provisioned by your carrier. reply jiggawatts 16 minutes agorootparentIt is quite literally a virtual Smart Card stored in a TPM chip. It has a private key in a hardware device, making it a cryptographic secret. reply hbt 11 hours agoparentprevwhat you are describing is why I use a virtual phone for all services.you can do it on your own with twilio, then create a phone number and have a program forward you stuff to your real phone.the twilio phone is hard to lose as it has an api and you can toss it when you want to start over.except now, you need an entire phone virtualized as your proxy instead of just a twilio phone number.they keep raising the barrier reply jesseendahl 11 hours agorootparentThis provides significantly weaker account security than using a passkey. 2FA codes delivered over SMS can be phished. reply briffle 11 hours agoparentprevthat is why its important for 3rd party tools like bitwarden and 1password to support passkeys.. reply lynx23 4 hours agoparentprevNot being able to regain access in exceptional cases is one of the big reasons why I am very weary about being forced to activate 2FA and other auth. It is so nice in theory... But the reality is that many users only use their phone to do almost everything digital in their life. My gf works for an assitive technology reseller. Since 2FA has been forced down the throats of unsuspecting users, she had to support several of their customers in regaining access to their Apple ID, noticing a few glitches in the supposed apple support path while at it. Phone hardware changes every few years. email addresses can change. And phone numbers can change. Combine all of them, and 2FA is suddenly no longer such a good idea... For reasonably sized companies, 2FA might be a good solution, because in case of you loosing access in some way, there is likely a support path that gets you back on track in reasonable amount of time, given that IRL auth is relatively simple. But for services where you are just a number, like every big provider, I believe a reasonably strong \"master\" password is still comforting to have. reply ktosobcy 12 hours agoparentprevFor that reason I don&#x27;t want passkey. Password and regular 2fa&#x2F;totp are fine... when setting 2fa I put it on my phone and my computer and another password vault on rpi... granted, everything still in same location but still somewhat better. I&#x27;m not really sold on esim neither - regular sims let you pop and swap them easily... why complicate it? reply jesseendahl 11 hours agorootparent>Password and regular 2fa&#x2F;totp are fineThis might feel true, but it&#x27;s factually not true.Both passwords and TOTP can be phished. In addition, passwords can be weak, reused, and password hashes can (and are frequently) stolen and cracked in server breaches.Passkeys are guaranteed to be strong, unique (can&#x27;t be reused), strongly phishing-resistant, and there&#x27;s nothing worth stealing from servers (just public keys).Passwords and TOTP are not fine, they&#x27;re both fundamentally broken when you look at them in the context of the modern internet attack landscape. reply dvngnt_ 2 hours agorootparentpros and cons to each approach reply jesseendahl 2 hours agorootparentWhat are the pros to using phishable passwords and 2FA? reply metafunctor 12 hours agoparentprevWhy do you believe that introducing support for passkeys inherently makes the situation worse? If you don&#x27;t trust them, you&#x27;re not forced to use them; traditional methods still exist.In any case, you should have multiple methods. It could be passkeys on multiple devices. It could be TOTP, plus recovery codes in a safe. Passkeys are just one more method.For the longest time, the gold standard for authenticating people has been tamperproof hardware with keys that cannot be copied. Except iPhones actually have credible biometrics on top of that. Much better than Yubikeys, for example. Of course you always need to have at least one backup device or other method in case your primary device is lost. Now that this is finally making it&#x27;s way to the “normal people”, it&#x27;s suddenly a “run away screaming” scenario? Come on. reply pseudalopex 10 hours agorootparent> If you don&#x27;t trust them, you&#x27;re not forced to use them; traditional methods still exist.I predict this will not be true always. reply noahtallen 9 hours agorootparentYes, the security industry is probably going to shift massively to Passkeys over the next few years. Phishing is a massive issue for enterprise security, and Passkeys basically completely fix it.IMO, this also means the problems with Passkeys will get fixed pretty quickly. And given I can already store my Passkey in 1Password and then use it on every device I currently use (including Firefox on mac&#x2F;windows and iOS Safari), it&#x27;s honestly not a huge problem.I think passwords are a much bigger problem for people. Simple&#x2F;re-used passwords are still incredibly common-place, and too many people don&#x27;t realize how big of a problem that is. Once you incorporate a password manager so that you don&#x27;t need to remember passwords... Passkeys via a password manager should be even easier to use, given you don&#x27;t have to rely on browser extensions auto-detecting input fields. reply veeti 39 minutes agorootparentIf passkeys evolve by enterprise requirements it sounds unlikely you&#x27;ll be able to ever properly export your keys. Instead, you&#x27;ll get forced attestation to make sure you&#x27;re not using Linux or some other untrustworthy platform. reply tharkun__ 9 hours agorootparentprevHow do you secure 1Password? With a passkey? See the loop?Or a password? Wait, didn&#x27;t we want to get rid of passwords? How is that any better?The kinds of people with reused passwords all over the place won&#x27;t use 1Password. And if you do use 1password to actually generate strong passwords you don&#x27;t need passkeys and it works on all kinds of services without those having to support passkeys. reply Nathanba 8 hours agorootparentit&#x27;s better than a password because good passwords pretty much require to be generated by password managers in this day and age. Which means you can&#x27;t actually remember them anyway, yet a password is still hackable or guessable Theoretically of course but not really, I&#x27;ve had some fairly long passwords of mine hacked somehow. I assume because a service stored them in plaintext and then got hacked. Make it 40 or 50 characters long, it doesn&#x27;t matter: It&#x27;s still just text and it can be stolen from you by remote, digital thievery somehow. The promise of passkeys is that this cannot happen anymore, they&#x27;d have to steal your physical device AND your way of unlocking that device. Sure you still need a master password to unlock your password manager but like I mentioned above: You now need this any way because you need a password manager no matter what. reply pseudalopex 6 hours agorootparentprev> IMO, this also means the problems with Passkeys will get fixed pretty quickly.Apple and Google do not quickly fix things when users have no alternative in my experience.> And given I can already store my Passkey in 1Password and then use it on every device I currently use (including Firefox on mac&#x2F;windows and iOS Safari), it&#x27;s honestly not a huge problem.For you. You believe the criticisms are dishonest? reply hypothesis 10 hours agorootparentprev> If you don&#x27;t trust them, you&#x27;re not forced to use them; traditional methods still exist.Still being the operative word. Consider situation with running banking apps without hardware attestation, etc reply eduction 10 hours agorootparentprevMuch easier to have spare yubikeys than a spare biometrically secure smartphone. Perfect is the enemy of good. reply rawgabbit 14 hours agoprevWhile I believe this is a step in the right direction. I have read too many horror stories of people who were locked out of their Google and iCloud accounts with no real possibility of getting back in.I don’t think I am alone in thinking I am on borrowed time. Someday, probably due to my own fault I will be locked out of Google and my digital life will be over.If a private company can offer a similar login method like login.gov and let me talk to a real person when I am locked out like the USPS, I will be screaming shut up and take my money. reply leotravis10 13 hours agoparentEspecially for normal and older folks and Google&#x27;s history of very non-existent support. Not to mention that passkeys is a flawed system as well. [1] [2][1] https:&#x2F;&#x2F;mastodon.laurenweinstein.org&#x2F;@lauren&#x2F;111103819626952... [2] https:&#x2F;&#x2F;mastodon.laurenweinstein.org&#x2F;@lauren&#x2F;111211366080459... reply gowld 12 hours agorootparentWhat is the flaw? reply gcr 12 hours agorootparentAFAICT, the flaw is that passkeys are tied to device security. If I steal a naive person’s phone at the bar, and if I can guess that their PIN is 1234, then I can get into their Google account.The criticism is based on the idea that most non-techie folks are unlikely to use a strong PIN and are unlikely to set up strong biometrics. There’s a related criticism about malware being able to steal passkeys on PC-based systems. reply true_religion 12 hours agorootparentWon’t most people be logged into their Google account anyways? So if you steal their phone, and guess their PIN then you can just use the already logged in account.What does this change? reply j-bos 9 hours agorootparentCertain account changing actions cannot be completed without the password. But if you have the phone (session, sms, passkey), you can reset the password and it&#x27;s off to the races. reply jackson1442 12 hours agorootparentprevWhat are the odds that someone with a passcode 1234 is 1&#x2F; already signed into Google on their phone or 2&#x2F; has their Google password already saved in the device password manager (since it asks you to save it every time you sign in) which is also protected by the device pin?At least in this case the thief has to steal the physical phone instead of guessing \"password123\" on the google signin prompt from the comfort of their home.Also- how many non-techy people do you know that avoid using on-device biometrics? On my end, the number is approximately 0. reply chownie 11 hours agorootparent> What are the odds that someone with a passcode 1234 is 1&#x2F; already signed into Google on their phone...very high? I don&#x27;t understand how this is unlikely, pretty much every phone owner with a google account is signed into that account on their phone. reply zerocrates 10 hours agorootparentThe point is that the odds are very high, i.e. if you&#x27;ve stolen their phone and know the PIN, you&#x27;re very likely already in their account, passkey or no passkey. reply Dlanv 10 hours agorootparentprevExactly. So they already have access to your email, passwords, and text messages regardless of the passkey reply joshspankit 12 hours agorootparentprevDon’t try to argue that on-device biometrics are a foolproof solution to this. Even at it’s best you can unlock a device from a sleeping (or drunk or naive) user which just brings us back to the same issue: already being logged in to a passkey service. reply charcircuit 12 hours agorootparentprevIf someone steals my phone and guesses my pin they already have access to my Google account because I&#x27;m signed in. To look at my email they just have to click on the gmail app. This \"flaw\" exists regrardless of password or passkeys reply kahnclusions 10 hours agorootparentprevMost people have _extremely_ weak device security. 0000, 1234, DDMM of their birthdate, etc, you probably cover the majority of people.And none of that helps you when someone robs you of your phone and says tell them your unlock code or they’ll stab you. Now they’ve got all your passkeys too. reply Dlanv 10 hours agorootparentThey also have your phone so they have text messages, email access, Google auth access, etc.So yes it is true that your phone and it&#x27;s pin&#x2F;biometrics are ultimately the most important thing for security. But passkey on your phone is no worse than the previous state. replyverytrivial 14 hours agoparentprevDisaster recovery. This is 100% my biggest worry with 2FA&#x2F;MFA. I also think this is one of the reasons stuff like PGP never took off (don&#x27;t @ me regarding perfect forward secrecy): the problem has always been managing some little, precious thing and the ramifications of what happens if it put beyond use or is used by some bad actor. reply Whatever13 9 hours agorootparentI&#x27;m from Brazil, where as is known many robberies and assaults happen on the street, and ever since the whole process of putting essential life services into smartphones started, many people are adopting a scheme of having 2 smartphones (if not 3 or 4 for other reasons! ) :1) The House smartphone → it is where you install everything truly vital, like the main bank app (started mainly because of this), 2FA apps like authy google microsoft equivalents, passwords, streaming apps (to do their 2FA), etc. This phone NEVER NEVER leaves the house, except ONCE if the bank app requires on location authentication of the phone for the bank app to function, which is common practice with traditional banks.2) The Street Smartphone → you essentially create a &#x27;&#x27;street bank account&#x27;&#x27;, deposit sufficient money for day-to-day transactions for some days or weeks, install the app, and only keep this app installed for any money use (many people also avoid even using the same bank as the main bank app, as the main bank usually is a traditional bank with physical locations to get help - and has tons of personal information stored - and the street bank usually is a fin-tech bank that people do not really trust like old banks, either economically or for security, but it has less personal data anyways). It also has the essencial social media like Whatsapp, instagram, some password manager like bitwarden or the apple-google cloud, and 2FA (the ones who actually use it) is avoided in this device.3) the thief smartphone → many people like to take some old phone around to give to a thief if the need arises, this way even the street smartphone is saved. Might not work if the thief smartphone is too old or clearly broken though.4) the work smartphone → the only mostly chill 2nd smartphone on the list, useful to keep private life separate from the professional life, and also is useful to avoid the boss sneaking into the worker&#x27;s private life and devices. There was a scandal here when a provincial government out of the blue installed a whole app in the smartphones of teachers AND students with no warning or any control whatsoever, and many people got scared that the administrative google service app being used by all (google education or some s*t) pretty much allowed the devices to be remotely controlled and viewed by the employer , be it private or public, so many people assumed any work devices is or can be done the same. reply rsync 7 hours agorootparent\"... This phone NEVER NEVER leaves the house, except ONCE if the bank app requires on location authentication of the phone for the bank app to function ...\"Can you elaborate ?What does this \"on location\" process look like ? What do they ask you to do ? reply dserodio 6 hours agorootparentNot sure what he means, but some banking apps in Brazil have a \"geofencing\" feature like \"only allow transactions when phone is inside this area\". Presumably you set your home and work addresses as trusted. reply fmobus 13 hours agorootparentprevGopass is my current solution. Easy to sync and move around (it&#x27;s just git), supports OTP generation, everything is encrypted by GPG. I have at least three devices in separate locations with it, so my DR is covered (and I exercise it frequently). reply Fogest 8 hours agorootparentI personally use Bitwarden, the self-hosted version. My phone, computer, laptop all essentially have the passwords and OTP synced from my server running it. Gopass seems to be quite a bit less user friendly so Bitwarden may be a better solution for a lot of people I think. reply brundolf 11 hours agorootparentprevI feel okay with Authy because I&#x27;ve got synced across multiple devices; one I bring out into the world and one I usually don&#x27;t. I would be pretty nervous if I didn&#x27;t have a second eligible device though reply dharmab 4 hours agorootparentprev1Password makes it pretty easy- both the OTPs and the backup codes can be synced to your account on the web and on all your devices. reply rkagerer 13 hours agorootparentprevThere are some Google Authenticator replacements that have an export function (eg. Authenticator+ on Android, although I&#x27;m not sure if it&#x27;s still maintained). You give up a bit of [theoretical] security for a whole lot of DR insurance. reply llui85 12 hours agorootparentGoogle Authenticator now has an \"Export QR code\" function that allows exporting the 2FA secrets. reply eep_social 12 hours agorootparentprevFYI the authenticator app itself has this now. reply compiler-guy 14 hours agoparentprev\"You might get locked out of your account\" is the updated version of the old \"Your hard drive will crash.\"It isn&#x27;t a matter of if, it&#x27;s just a matter of when. Backups and a thorough disaster recovery plan is absolutely mandatory for anyone who cares about their data. Some company is going to mess something up due to no fault of your own. It is inevitable.Unfortunately, there aren&#x27;t good disaster recovery options for some aspects of lost accounts, but having multiple accounts and avoiding single-points of failure help some. reply Gibbon1 13 hours agorootparentMy thought on this is to involve notaries. As in you can get a notorised account. And if something goes wrong you can get a notary in the loop and by law the providers have to fix what ever has gone wrong or they are liable for actual and statutory damages. reply j-bos 9 hours agorootparent100% agreed. It&#x27;s the analog reason notaries even exist. reply skybrian 9 hours agoparentprevImagining pessimistic scenarios is useful if it spurs action. In this case, appropriate action would be to learn about Google&#x27;s account recovery options and take advantage of them. Make sure you have multiple, independent ways of logging in. (For example, by printing out backup codes and keeping them with your important papers.)But even this can&#x27;t protect against getting locked out of your Google account due to some Google policy change, so ideally we&#x27;d rehearse how to get by without it. reply syntaxing 14 hours agoparentprev100% agreed. I’m excited about the passwordless future but one unexplained ban from them and it’s like losing your physical wallet. reply hedora 7 hours agoparentprevI haven’t heard of people getting locked out of iCloud. Losing all your devices nukes E2EE stuff, but that’s not much stuff by default. In particular, photos, device backups and messages are recoverable.I thought the Apple Store would check your driver’s license or whatever and reset your password, recovering whatever is protected by the escrow keys.I know Google loses accounts all the time, mostly thanks to “surprise 2FA” combined with zero tech support. I’d believe Apple screws this up too, but I haven’t heard any anecdotes.Care to share a link to examples? reply leotravis10 13 hours agoprevLauren Weinstein is sounding the alarm on passkeys which is flawed and that it would make a huge headache for a lot of people especilly normal folks. https:&#x2F;&#x2F;mastodon.laurenweinstein.org&#x2F;@lauren&#x2F;111103819626952... https:&#x2F;&#x2F;mastodon.laurenweinstein.org&#x2F;@lauren&#x2F;111211366080459... reply crote 13 hours agoparentYup! I&#x27;ve had similar complaints for years now.Modulo the whole privacy&#x2F;vendor lockin issue, passkeys are not a terrible alternative to people without 2FA reusing the same basic password on every single website.However, when you actually rely on it to secure things, it quickly becomes a massive nightmare - made even worse by it being treated as equivalent to password+2FA. reply leotravis10 13 hours agorootparentCoupled with Google&#x27;s very shaky support track record and you have a very dangerous combination. This will surely get ugly. reply jesseendahl 11 hours agorootparentprev> made even worse by it being treated as equivalent to password+2FA.passkeys are significantly more secure than the most widely-used&#x2F;most popular forms of 2FA, because the most popular forms of 2FA are TOTP and SMS, and both are subject to phishing attacks. A passkey alone is much more secure than the vast majority of password + 2FA combinations.The only thing stronger than a passkey standing alone is a Security Key, but Security Keys come with a lot of usability downsides that can easily bite the average user, including:- inconvenience: you have to remember to carry it around with you everywhere (and not lose it!)- recoverability: you&#x27;re completely screwed if you lose it and don&#x27;t have extras that you already previously added to your accounts. (this also means that you need to buy at least two security keys to have a decent recovery story.)- rotation (have to log in to every single service, one by one, to re-add new key if you change keys)And if you really want the extra security that a Security Key provides, you can use a Security Key as a passkey. reply jjav 9 hours agorootparent> passkeys are significantly more secureBlanket statements like this demonstrate a misunderstanding that \"security\" is just one thing in a single lineal scale.In reality you have to ask, secure against what? And to answer that meaningfully you need to a thorough threat model for the specific use case of person P and account A.The same person P will have a different threat model for every account they have.The D in STRIDE is for denial of service. Passkeys are much worse on this axis than any other solution. You need to evaluate for the specific combination (P,A) how much this matters vs. other criteria. reply jesseendahl 2 hours agorootparentI&#x27;m not sure why you didn&#x27;t include the rest of the sentence, which made it clear that I was making a very specific comparison to password + phishable forms of 2FA. I was very clearly not making a blanket statement.Here&#x27;s the full context again:passkeys are significantly more secure than the most widely-used&#x2F;most popular forms of 2FA, because the most popular forms of 2FA are TOTP and SMS, and both are subject to phishing attacks.>The D in STRIDE is for denial of service. Passkeys are much worse on this axis than any other solution. You need to evaluate for the specific combination (P,A) how much this matters vs. other criteria.How are passkeys (really, WebAuthn credentials in general) any worse in terms of denial-of-service attacks than passwords?I think you&#x27;re trying to make a point about specific passkey&#x2F;password managers, rather than the actual credentials themselves. Is that accurate? reply joshuamorton 7 hours agorootparentprevI don&#x27;t think this is correct. It&#x27;s more difficult to steal something from me than to, e.g. repeatedly force password reset emails. From a ux perspective, it may be easier to accidentally kneecap yourself with a passkey, but security wise, they&#x27;re still probably better since it&#x27;s harder for someone else to kneecap you. reply d-z-m 2 hours agoparentprevTo me, it&#x27;s unclear what the headache is. If the argument is about the consequences of passkeys for most ordinary people, most people are signed into their google account on their mobile device. In that case, your account is compromised anyway if your device authentication is breached.For Google in particular, password&#x2F;passkey isn&#x27;t a binary choice(currently). You can fall back to the password sign-in flow if your device doesn&#x27;t have a passkey. reply gowld 12 hours agoparentprevDoes he explain the flaw anywhere?He says it&#x27;s \"easy to find\" but apaprently he can&#x27;t find it. https:&#x2F;&#x2F;mastodon.laurenweinstein.org&#x2F;@lauren&#x2F;111211489395997...Why is \"weak device password\" a reason to avoid passkeys, when those users presumably have weak service passwords as well? reply panarky 12 hours agorootparentIt seems like his argument is that putting access to valuable accounts on your phone is a bad practice, because if your phone is stolen at the club after the thief watched you enter your code, then the thief can get at your banking, brokerage, crypto, password manager, etc.But that argument doesn&#x27;t address how passkeys somehow make that worse.Sure, if you don&#x27;t want your valuable stuff stolen, don&#x27;t put it on your phone. But that&#x27;s a problem whether you use passkeys or passwords or passwordless links sent to your email or SMS. reply passkeyspoor 12 hours agorootparentThe point is that the phone with a crappy 4 digit pin can be used to authenticate everything on every device the user owns that uses passkeys. It&#x27;s a one stop shop of failure. reply Dlanv 10 hours agorootparentPhones are already that way. They have text messages and email which is enough to log into almost any service. reply taeric 9 hours agorootparentThe argument is that without your phone, you likely have no recourse to stop the attack. Since your passkey on the phone is what controls your access, now. reply passkeyspoor 9 hours agorootparentprevYes, that&#x27;s also bad. They&#x27;re both bad. Passkeys are worse. reply pseudalopex 10 hours agorootparentprevThe argument for passkeys is they make it better. Not not worse. reply donmcronald 12 hours agoparentprevIt&#x27;s not about security. It&#x27;s about having a system for digital signatures that acts against the interests of the user. reply drdaeman 6 hours agorootparentThe fundamental idea of using asymmetric cryptography to authenticate is good. It is time-proven, and it works in the best interests of users, simultaneously providing improved security. SSH just works (and while it typically lacks fancy UIs for key management, it&#x27;s irrelevant to the core idea).The passkeys design, though, has a number of obvious deficiencies and limitations. It is drastically better than ye oldebut it&#x27;s not a good standard.The other alternative is SRP, but no browser vendor had bothered to do anything about this, so it remains a curiosity implemented on a couple websites (with all JS crypto gotchas, so - no good). reply LeoPanthera 14 hours agoprev1Password enabled PassKey support recently and I was \"surprised\" to learn that there is no way of exporting them out of 1Password. They&#x27;re not included in the 1PUX format export, nor in the CSV.That means that they&#x27;re literally impossible to back up. If 1Password goes down, or the company stops operating, or anything else like that, your Passkeys are just... gone. Absolutely no way to recover them. reply jxcl 13 hours agoparentCurrently, none of the big players in the passkey space support exporting or importing of passkeys, because the spec for doing this securely has not been agreed upon, and nobody wants to allow plaintext export of passkeys.See a recent post in the 1Password passkey AMA about this subject: https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;1Password&#x2F;comments&#x2F;16to6x7&#x2F;hey_redd...Re. your point about 1Password going down: Your passwords and passkeys are all stored locally when they sync to your devices. If 1Password becomes unreachable for any reason, you still have access to everything in your vaults, you just can&#x27;t sync between devices any more. reply LeoPanthera 13 hours agorootparentIt&#x27;s difficult not to see the \"it&#x27;s for your own security\" argument as a cynical lock-in ploy.Because you can export plain-text passwords just fine, and they give you exactly the same access as a PassKey does. reply darknavi 13 hours agorootparentprev> nobody wants to allow plaintext export of passkeys.While noble, why? 1Password exports a plaintext file that has all of the credentials in plaintext already. reply xg15 11 hours agorootparentI guess \"100% secure against phising\" is incompatible with \"the user can in any way access the key\" because if you knew the key, in theory some super-convincing phishing site could get you to spill it.I still think the real reason is lock-in, but I could imagine this is their official justification. reply veeti 49 minutes agorootparentprevBecause in this brave new world you aren&#x27;t supposed to own your keys, some proprietary HSM inside your device does. reply bobbylarrybobby 8 hours agorootparentprevGiven all of the horror stories (some real, some hypothetical) told in this thread, it seems that one of the major side effects of passkeys — if not the primary purpose — is to keep you locked into whatever you used to create your passkey. Plaintext export would ameliorate that. reply mplewis 12 hours agorootparentprevBecause passkeys are supposed to be a bit more secure than plaintext passwords. reply wutwutwat 12 hours agorootparentPasskeys are supposed to eliminate the need for companies to store a password so we no longer have to deal with the fallout of 40 breaches a year. In order to export passkeys it has to be in plaintext at some point, even if encrypted once again into the export file. Point is, one of the huge selling points of pushing people to use passkeys is the portability and lack of vendor lock in yet here we are with choices that are all currently vendor lock in. reply hedora 7 hours agorootparentprevComputer security is generally defined as Confidentiality, Integrity and Availability.Not “or”. Passcodes don’t provide availability, so they are not providing security.This is undergrad-level stuff. reply afiori 12 hours agorootparentprevThis sounds a bit like \"a turned off computer is the only secure computer\" reply varjolintu 6 hours agorootparentprevIf you consider KeePassXC to be one of the big players, they (will) support importing and exporting Passkeys. reply XorNot 13 hours agorootparentprevAnd what a surprise that is, the one feature necessary to ensure vendor lock in doesn&#x27;t happen was at 0 priority before they rolled it out. reply SheinhardtWigCo 13 hours agorootparentThe whole point is vendor lock-in. reply sbuk 12 hours agorootparentHow does that work if you can register multiple different keys using different devices from different vendors on an account?Edit: I took the last sentence out, it was childish on my part. reply drdaeman 6 hours agorootparentCan you, though? passkeys.io does not showcase this. The default assumption from every vendor is that you&#x27;ll use their passkeys and they don&#x27;t care about anything else. It&#x27;s a very explicit silence, no \"official\" resource from any major vendor addresses cross-platform portability.Yes, some individual implementers recognize the issue and have \"log in with another device\" (which is the best option you can have, although still quite clunky), so you can solve the chicken-and-egg problem of logging in on another platform&#x27;s device to add your another platform&#x27;s passkeys. But to best of my awareness, this is not a part of any standard or recommendation (it should&#x27;ve been).And other implementers do the contrary and artificially limit your options so you can&#x27;t add a portable authenticator with them without some hacking around. reply imran-iq 10 hours agorootparentprevWhat are the vendor options though? (I think) its Google, Apple, Microsoft, Yubico and 1password? None of which support exporting the keys as per other comments in this thread.Also (i think) none of them are open source? reply tadfisher 8 hours agorootparent1password publishes their implementation: https:&#x2F;&#x2F;github.com&#x2F;1Password&#x2F;passkey-rs replyidle_zealot 13 hours agoparentprevIsn&#x27;t that the point of Passkeys? The user isn&#x27;t allowed to interface with them directly, so social engineering can&#x27;t compromise them [1]. Rather than move your passkey between devices, you&#x27;re meant to generate a different passkey for each device, then register all of them with the relevant service, like SSH keys.1: of course, a user could still be tricked into adding an attacker&#x27;s passkey to their account or something reply LeoPanthera 13 hours agorootparentBut 1Password syncs your passkey to all your devices, so you only have one. reply efitz 13 hours agorootparentDon&#x27;t worry, if you lose your passkey all you need is access to your email to receive a password reset link.reply CharlesW 13 hours agorootparentThat&#x27;s literally the solution to \"What if I lose all the passkeys associated with my account and I&#x27;ve also forgotten my password?\" reply drdaeman 12 hours agorootparentThe major problem with passkeys is that first they were poorly designed so there&#x27;s no portability or ability to enroll an offline (or worse, physically unavailable, like stored in a safe) authenticator, then there&#x27;s this kludge to work around the limitation.It was obvious from day 0 (to anyone except for Apple and Microsoft) that people do have multiple devices and not all of them are from a single vendor. My only explanation is that they deliberately decided to ignore this aspect, because it wasn&#x27;t in corporate interests.They made it significantly easier to lose all the passkeys, because they made it very hard to add multiple passkeys (you literally have to walk&#x2F;run&#x2F;drive&#x2F;fly and grab every different device you have, get it online and register - or get properly locked in with a single vendor and pray they work for you, forever).Carrying a Yubikey does not work (you can lose it). iCloud&#x2F;Windows Hello does not work (you can be on a non-Apple&#x2F;Microsoft device). 1Password is better but still does not really work (you can lose access to your account). They&#x27;re all SPOFs, and avoiding SPOF was deliberately made hard (you can&#x27;t easily enroll a \"backup\" Yubikey that you don&#x27;t have at hand, and if you have it at hand it&#x27;s not a backup anymore).Heck, \"official\" demo at passkeys.io doesn&#x27;t even bother to showcase how multiple passkeys are going to be a thing at all, which is an obvious red flag.That is, not to mention that a growing number of vendors contributed to the crappiness by limiting what kind of authenticators and which platforms one can use (BestBuy, PayPal and so on), contributing to decreased security and increased headaches. reply afiori 12 hours agorootparentprevExcept for when it happens to your email account. replyYeul 13 hours agoparentprevI had a discussion with my mother advising her to switch: she is afraid of changing ISP because her email is tied to her provider.We fixed this on mobile years ago but email is still a goddamn mess. Moral of the story: never get locked in. reply CharlesW 13 hours agorootparentYou&#x27;re not locked in. Want to switch? Add a passkey. Lose all your passkeys? Do the \"forgot password\" thing just like you&#x27;ve done forever. reply kibwen 12 hours agorootparentThe \"forgot password\" flow involves accessing your email. And accessing your email without having access to your passkey requires a device that has previously logged in to your email. And the device that has previously logged in to your email is the same device where your passkeys are stored, which is to say, the same device that is now lost or bricked, which is the reason your passkeys are lost in the first place.And sure, you and I have multiple devices. We&#x27;re in the minority. Most people just have the one. Without another way in, they&#x27;re irrevocably fucked. reply sbuk 12 hours agorootparentYou only use your passkey when logging in to your email account if you use a web-based client exclusivley. reply rkagerer 13 hours agoparentprevDo any of the third-party, self-hosted password managers provide a compatible passkey implementation that can actually be exported and backed up in a secure manner? reply maxwellg 11 hours agoparentprev1Password&#x27;s Passkey support feels very aggressively growth-hacky to me. They intercept calls to `window.credentials` and if you want to use 1Password along side other verifiers like Yubikey, you need to go into your settings and disable their passkeys offering entirely. It&#x27;s similar to how they also intercept (and globally disable!) Google One Tap prompts in order to show their own OAuth prompt. I only use their Chrome extension so I&#x27;m not sure if the native app experience is significantly different. reply LeoPanthera 10 hours agorootparentI&#x27;m kind of mad at 1Password - but this isn&#x27;t correct. When the 1Password prompt some up, you can click the little \"USB key\" icon which ostensibly is for hardware keys, but all it does is pass control back to the OS, at which point your iCloud prompt, or whatever provider you are using, can be used. reply gumby 13 hours agoparentprevIs version 8 reasonably mac-like? On 7 it&#x27;s still a mac application that acts like a true mac application (drag&#x2F;drop works properly everywhere, expansion, properly keyboard-enabled, etc) which is well nigh impossible when running inside a chrome box.Agile Bits support kept insisting it was the same as the old native app and people kept complaining about bugs until I stopped following it. reply LeoPanthera 12 hours agorootparentIt&#x27;s as Mac-like as any other Electron app. Which is to say, it does a pretty good impression of a Mac app, but the bundle is 345M, with another 244M hiding in your Library directory. reply tomduncalf 13 hours agorootparentprevIt’s so rare that I use anything other than the 1Password Chrome extension that I couldn’t really tell you! The main app seems.. fine? But like I say, I hardly use it, so I probably wouldn’t notice details like you mention.Do you have a different workflow where you use the main app a lot? reply gumby 11 hours agorootparentI keep a lot (including images) in the main app as an ecrypted shared resource for IDs and various other secure info. If I suddenly need my insurance card I can quickly grab it out of the app rather than rummage through the (unencrypted) icloud or dropbox filesystem on ios. And I can cut&#x2F;past text out of the images. I also use it for logging into apps, dragging credentials into remote machines over ssh etc.With 1password 7 whe safari plug in is more conveniently integrated than the chrome one which is pretty clunkly by comparison, though this is true of other chrome plug ins too. But that&#x27;s not a big deal as I rarely use chrome anyway, just for google docs which don&#x27;t need 1password. reply numpad0 11 hours agoparentprevCan&#x27;t you enroll a Yubikey and keep it in a safe? reply shepherdjerred 14 hours agoparentprevIt&#x27;s a feature that came out just last month. Give them some time. reply droopyEyelids 13 hours agoparentprevThis is a quibble, but if 1password goes down, your vaults still exist on all your devices and the app will keep working, it&#x27;s only the syncing of modifications between devices that won&#x27;t work. reply frabcus 19 hours agoprevAs a user I still don&#x27;t understand this.What happens if there&#x27;s a house fire or something and all my devices where I&#x27;m logged in with Google break? How do I log into my account again? reply havnagiggle 14 hours agoparentJust happened to my in-law. She dropped her phone on the stairs, screen cracked, and became unresponsive. I gave her an older phone I had and swapped the sim fine. But she couldn&#x27;t figure out how to log in to Google account because it was so adamant telling her to use her phone. Her laptop was logged out of her email, etc. Fortunately I have backup tokens for her from a previous incident heh. I have no idea what other folks will do. reply hansvm 13 hours agorootparentA few months ago Google wouldn&#x27;t even accept backup tokens for me. I was on vacation, and that tripped enough fraud detectors to cause problems. I couldn&#x27;t log back in till I got on my home network and changed my password. reply nre 9 hours agorootparentBack in the old days with no 2FA and only username&#x2F;password access geolocation lockouts happened every time I went travelling. You could regain access by getting a code from a recovery email, but that often got locked out as well!Eventually I set up my own VPN server so that the services still thought I was using my home IP. reply hn_throwaway_99 14 hours agorootparentprevI&#x27;m don&#x27;t know the specifics of how passkeys with Google work, but don&#x27;t they usually require multiple synced devices? reply Msw242 14 hours agorootparentprevGet a lockbox at the bank reply skarra 15 hours agoparentprevPasskeys are a new technology and everyone - including users, service providers, and organizations - will take time to learn and adapt. In this interim period the recommended approach is to provide passkeys as an alternative to whatever is already offered. This is the approach that Google and many other service providers are taking.That said, you are bringing up the right questions on the general topic of account recovery that everyone should be asking even without passkeys: \"How would I login if I forget my password &#x2F; lose access to my password manager &#x2F; lose my second factor devices\" and have a plan. Introduction and adoption of passkeys do not completely eliminate the need for thinking about your account recovery situation.However, there is one special case where using passkeys is actually better for account recovery. If you create passkeys for your Google account on an Apple device with iCloud keychain, the passkeys are synched to your iCloud, so now even if you lose all your devices because your house burned down, as long as you have access to your iCloud account, you can just get all the passkeys for your Google accounts(and other websites).Now, you may ask: &#x27;what if I lose access to my Apple iCloud account\" -> that&#x27;s a fair question! Which is why I said Account Recovery concerns do not completely go away - but they can be significantly reduced with passkeys in many cases. reply drdaeman 11 hours agorootparentAll those issues were obvious from the day zero, and raised multiple times by many people. They&#x27;re deliberately ignored by the stakeholders.They strongly want to lock you in to their own authentication platforms (iCloud Keychain, Windows Hello, 1Password*), that&#x27;s why they don&#x27;t want to address this.It&#x27;s impossible they&#x27;re not aware about those issues. Anyone with a brain and some technical expertise would come up with those questions in an evening or two, and Passkeys were worked on for months. To best of my awareness, there is no official acknowledgement (support replies \"no, you can&#x27;t do this\" doesn&#x27;t count, that&#x27;s just restating facts, not acknowledging an issue).*) Ok, 1Password says they&#x27;re all about user freedoms and that it&#x27;s up to user to decide where they store their passkeys - but that&#x27;s what they say, not what they do. What they do is indistinguishable from Apple and Microsoft. reply jesseendahl 11 hours agorootparentYou can recover access to your iCloud Keychain even if you&#x27;ve lost 100% of your devices.See the section titled \"Recovery security\" in this article:https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;102195Relevant excerpt for those too lazy to click through:\"However, it&#x27;s also important that passkeys be recoverable even in the event that all associated devices are lost. Passkeys can be recovered through iCloud keychain escrow, which is also protected against brute-force attacks, even by Apple.\" reply drdaeman 10 hours agorootparentIf I understand it correctly, this only works on another Apple device, though. So you&#x27;ll need a spare iPhone or something.Also, I&#x27;m pretty sure if Apple decides to block your iCloud account, you&#x27;re most likely SOL. reply vorpalhex 7 hours agorootparentprev> To recover a keychain, a user must authenticate with their iCloud account and password and respond to an SMS sent to their registered phone number. reply skarra 5 hours agorootparentprevOn account recovery, the user is strictly no worse off with passkeys relative to passwords and arguably actually better off in many cases. This is not what I&#x27;d call deliberately ignoring concerns. reply jasonjayr 14 hours agorootparentprevHow can a user, right now, take control + ownership of backing up their own pass keys, without iCloud or Google?This is a privilege I currently enjoy right now, and one I am not really eager to give up. reply awegio 13 hours agorootparentIt depends on your web browser. Just see what happens here https:&#x2F;&#x2F;webauthn.io&#x2F;Firefox on Desktop tells me to \"touch my security key\". Not sure how that works. Firefox Android gives me a few hardware options to store my passkey to. Chrome Desktop asks me to enable Bluetooth. Chrome Android asks which Google Account to use. reply Izkata 3 hours agorootparentJust tried that with Firefox on Android and while it works, I can&#x27;t find any evidence of a stored passkey on my device, let alone a way to export it. reply blitzar 12 hours agorootparentprevI use passkeys everywhere I find them. I do not take control or ownership of backing up - instead I have alternative 2fa or hardware key authentication with all those accounts.For every account I have a hardware key for, there are 3 hardware keys associated with that account - 2 on-site, 1 off-site. reply rssoconnor 11 hours agorootparentHow do you register your off-site hardware key. Did you have to go retrieve it each time you wanted to make an account?I suppose every time one makes an account one can register the two on-site keys, and then rotate one of your on-site key to off-site and take the off-site key home with you, and then finally register it.Maybe I should get a third key... reply blitzar 11 hours agorootparentI think you answered your own question! The three key is optimum for ease of rotating (or so you can carry one on person) - but if your house burns down with your phone in it - you will lose anything set up since your last offsite rotation.Sounds paranoid &#x2F; crazy - but I have 0 anxiety about being locked out of an account that matters. reply jasonjayr 12 hours agorootparentprevWhich hardware keys are you using? And have you found any difficulty in adding multiple keys to a web site? reply blitzar 11 hours agorootparentYubikey keys - zero difficulty adding multiple - if a site doesn&#x27;t allow multiple I wouldn&#x27;t lock my account down to a single point of failure. All the big players seem to offer it, and I can not recall any that didn&#x27;t. Google in the \"advanced protection\" days forced you to have more than 2 keys for this reason.By count of sites, most sites don&#x27;t appear to take security that seriously so anything more than a password is off the cards, but the big ones - the ones that actually matter; email, cloud, etc. should all be able to be secured. reply vel0city 7 hours agorootparentprevI&#x27;ve got security keys on Yubikeys, Android devices, and Windows devices. Only one of these are Google. reply skarra 14 hours agorootparentprevPassword managers like Dashlane and 1Password have announced support for storing and synching passkeys. As passkeys becomes more popular I expect more providers to step up as well.Ecosystem lockin is not how we make a new technology like this successful. And all players in the game understand that. reply pseudalopex 10 hours agorootparent1Password does not give control and ownership.[1][1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37836783 reply jasonjayr 12 hours agorootparentprevAppreciate the response. And I wish this message was front and center. The Attestation feature is what worries me, when, say, the bank turns it on for a few &#x27;blessed&#x27; providers, or mandate a hardware implementation.Watching https:&#x2F;&#x2F;github.com&#x2F;keepassxreboot&#x2F;keepassxc&#x2F;issues&#x2F;1870 with baited breath... :) reply skarra 5 hours agorootparentYour concern around attestation (mis)use is spot on. I&#x27;d say the industry is yet to arrive at an acceptable consensus or compromise on that question. reply shepherdjerred 14 hours agorootparentprevI use 1Password [0] for syncing passkeys, and it works quite well. I would imagine other password managers are building similar features.[0]: https:&#x2F;&#x2F;support.1password.com&#x2F;save-use-passkeys&#x2F; reply pseudalopex 10 hours agorootparent1Password does not give control and ownership.[1][1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37836783 reply pseudalopex 10 hours agorootparentprevYou should disclose your employer more consistently. reply skarra 5 hours agorootparentI work on Google&#x27;s authentication team. I have mentioned this elsewhere in the thread. reply pseudalopex 5 hours agorootparentYour other disclosure is why I said more consistently. Do you believe all readers will read all comments and index mentally by user name? reply vorpalhex 14 hours agorootparentprevRearranging deck chairs on the titantic.This whole scheme depends on either users being savvy enough to do vault backups or depending on service providers being functional.Both are quite doomed.Users have a path for passwords - they can write them down on paper and keep them with their important things. This tends to work for most folks.The backup story for passkeys is horrible. There is no path for my elderly relatives who don&#x27;t use cloud services.Until that is fixed, passkeys will never replace passwords.Don&#x27;t forget password sharing! That is a whole screwed up story with passkeys too. reply skarra 13 hours agorootparentPasskeys represent the cumulative wisdom and experience (and compromises!) of the whole industry on how to keep users safe online. Appreciate your opinions that these efforts are doomed. It is safe to say, \"We&#x27;ll surely find out!\" reply vorpalhex 12 hours agorootparent\"The Industry\" also has interests like making password sharing impossible, uniquely tracking users and _doesn&#x27;t care_ if users get locked out.The industry does not put users first. It puts it&#x27;s own risk reduction first. reply skarra 11 hours agorootparentDid you know that Apple allows sharing passkeys via Airdrop? reply kiwijamo 11 hours agorootparentDoesn&#x27;t that give access to everything you&#x27;ve signed in using that passkey? Rather than e.g. Sharing the password for the family Netflix account. reply rkeene2 9 hours agorootparentNo... A passkey is specific to a context (RP), which is why they&#x27;re not stored on things like Yubikeys (which I think a lot of people in this thread are confused about -- the keying material on the Yubikey isn&#x27;t enough to create the passkey).Your Netflix passkey is not the same as your passkey to other services. It&#x27;s generated as soon as you enroll the passkey with Netflix (by calling \"navigator.credentials.create()\") and is identified by an opaque handle and also the public key (this is important, because you never get the public key again so you must keep both of these: the ID, and the Public Key, otherwise you can&#x27;t verify a challenge-response, since you&#x27;re only given an ID and a Digital Signature at that point).For a site to use a passkey it calls \"navigator.credentials.get({ publicKey: { challenge: ..., rpId: \"\" }, mediation: \"silent\" })\"Which returns the key ID and a signed version of the challenge, or an error.Everywhere you authenticate you have one or more keys, identified by these opaque handles which are stored in the User Agent and associated with some mechanism for performing digital signatures with that unique key. The User Agent, generally, has to store and distribute this information if you want to use the same passkey across multiple devices -- even if you&#x27;re using a Yubikey (because, again, it&#x27;s not storing the key being used for the digital signature, it&#x27;s storing a private key which is used in the process of generating the digital signature, but not the passkey&#x27;s actual private key -- i.e., the secret part of the public key generated earlier) reply xg15 10 hours agorootparentprevCan I print out the passkey as a QR code and scan it back in on a different device? reply vorpalhex 8 hours agorootparentprevOnly if you exchange contacts first and are ah.. in Airdrop range.Your grandmom probably isn&#x27;t gonna be airdropping a Netflix password. reply CogitoCogito 10 hours agorootparentprev> Passkeys represent the cumulative wisdom and experience (and compromises!) of the whole industry on how to keep users safe online.That is true _if_ you do not highly weigh all the concerns that have been brought up in this thread today. I do not trust Google to help if things go wrong so why would I ever consider such a system wise? Frankly, you seem to be ignoring concerns if they contradict your belief that this system is better. I&#x27;m reminded of Upton Sinclair. reply pseudalopex 10 hours agorootparentDid you see they worked for Google? Or did you guess correctly?[1][1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37833206 reply CogitoCogito 10 hours agorootparentprev> That said, you are bringing up the right questions on the general topic of account recovery that everyone should be asking even without passkeys: \"How would I login if I forget my password &#x2F; lose access to my password manager &#x2F; lose my second factor devices\" and have a plan. Introduction and adoption of passkeys do not completely eliminate the need for thinking about your account recovery situation.Talk about victim blaming. Google and other companies introduce policies that make total identity lockout both easier and more problematic. Instead of investing in customer service to deal with this issue, the customer needs to \"have a plan\". What a crazy coincidence that this policy increases Google&#x27;s profitability by decreasing support. reply segmondy 17 hours agoparentprevI don&#x27;t know how Google solved this, but it&#x27;s an old solution. Shamir secret sharing. You break apart your keys into M pieces, where you need N pieces to reconstruct the key, so let&#x27;s say 3&#x2F;8. Then you need 3 pieces out of the 8 pieces it&#x27;s broken into to recover your key. You take each of those 8 pieces and give to trusted sources. When you need to reconstruct your key, you have at least 3 of those give you the key and you recover.How does this look in implementation. When I Implemented this in multipasskey (YC demo). It would ask you to select contacts you trusted. Then it would send the sharded parts of the key in the background to them. If you need to recover, you make a request to them. It would reconstruct your device key when you got enough pieces. Once you have your device key, it would download your encrypted backup of keys from the remove server and you are back as new.I called my project multipasskey in 2017&#x2F;2018 and applied to YC with a working demo and they said nope. I&#x27;m going to assume that I sucked at selling it. ;-) reply bojo 14 hours agorootparentI had the same idea about a decade ago but never bothered to try to implement it. I felt like it would have suffered from the same problem all other technologies have in security: overly complex user interactions. The concept makes sense, but getting N other people to commit is overhead the average user probably doesn&#x27;t want to deal with. reply segmondy 13 hours agorootparentSo I preferred the idea of regular folks for backup, for security reasons. I thought of the idea of professional users like say your bank or 3rd party. The issue is that it&#x27;s far easier for the govt to subpoena those pro 3rd parties and recover your key. Whereas, they would have to know which of your friends you used for key recovery to be able to do that. The idea was to make it tough for a bad&#x2F;powerful actor to steal your key. Of course, the challenge is that a non social person would need friends or to depend on ISPs, banks (pro 3rd party providers). My goal besides security when building this project was to break the chain of 3rd party auths (Google, MS, Github, etc) :-(. They use their auth as a way to lock folks into their ecosystem and if you offend them in anyway, you could lose access to everything. Offend Google on adsense and lose your personal photos&#x2F;email. Offend Amazon on sales and lose your prime streaming&#x2F;AWS access. Hopefully as the idea picks up, the monopolistic corps can be tackled again to remove such power. reply bojo 10 hours agorootparentI wholeheartedly agree with where you were aiming your goals. Other thoughts I&#x27;ve had:- What if access is time critical but your backup people are distributed across timezones? Or they aren&#x27;t available for some reason? Could be hours to days before you could recover your account- Adding&#x2F;removing people as they enter&#x2F;exit your life could make it a challenge to maintain (PGP + trust vibes) reply compiler-guy 14 hours agorootparentprevNow there is a technically savvy solution that is a technical tour-de-force.Very very cool.But also completely unrealistic for the average person to use. reply segmondy 13 hours agorootparentHow? The usage was very easy. You select a contact and add them as your recovery contact (by selecting contact from your contact list) The system adds the key in the background. If they don&#x27;t have the app, the app asks you to tell them to install the app (viral growth?). The users didn&#x27;t need to know any thing technical. But install app, and click yes&#x2F;no like they do with a 2FA app. reply jsperx 12 hours agorootparentI think the challenge is more coordinating the 8 people who will be a trusted part of your life long-term. Also they’d have to be sure to keep their fragments of the key intact through replacing devices, etc, no? Seems like just keeping a Yubikey in a safe deposit box would be simpler. reply Tijdreiziger 10 hours agorootparentDefine ‘safe deposit box’?If it is a safe at your home, you need to have a fixed home address in the first place, and the usual advice about off-site backups also applies.If it is at friends or family, you’re back at the same problem.If it is a rented deposit box, you need to trust the company you rent it from (banks don’t usually offer such services anymore, and there are risks like in [1])[1] https:&#x2F;&#x2F;www.nytimes.com&#x2F;2019&#x2F;07&#x2F;19&#x2F;business&#x2F;safe-deposit-box... (archive: https:&#x2F;&#x2F;archive.is&#x2F;7qbkR) reply segmondy 10 hours agorootparentprevif they replaced their device, their new device would still preserve your key, just like you replacing your device keeps your key. reply doublerabbit 12 hours agorootparentprevI don&#x27;t have eight people, what then? reply drdaeman 11 hours agorootparentYou use different numbers, for example 3&#x2F;5 or 2&#x2F;3.You have to have at least 3 peers, though (IIRC, 2&#x2F;3 is the minimum split possible that would provide fault tolerance). reply wutwutwat 12 hours agorootparentprevI’m not in the crypto world to know why this is the way it is, but if you only need 3 pieces out of the 8 to reconstruct the key, why split it into 8? Is it to have a larger pool should you need it&#x2F;higher odds of being able to gather 3 should some pieces be lost? reply progmetaldev 13 hours agorootparentprevSounds like a great idea. Sometimes it&#x27;s hard to be so in tune with the technology, and also be the salesperson! reply wutwutwat 12 hours agorootparentprevAdded bonus you can’t die unless someone locates each piece and destroys them all reply frabcus 18 hours agoparentprevTo add, it is pretty poor there is no FAQ linked to from that post to answer basic non-technical questions as to how this is intended to be used.I assume as a technical person, the answer is I should have a backup device with a friend and&#x2F;or store my passkeys somehow on my Apple or Microsoft or password manager account as well.But it needs more explanation in detail from Google! reply dotancohen 14 hours agorootparentSo now that \"friend\" has access to your account? How is that more secure than my 32 random character password I store in an encrypted Keepass database that I back up offline? reply skarra 15 hours agorootparentprevYou can try this: https:&#x2F;&#x2F;support.google.com&#x2F;accounts&#x2F;answer&#x2F;13548313?hl=en, this help center page is linked to from various parts of the product experience for regular users to get a better idea about passkeys if they are interseted. reply pseudalopex 10 hours agorootparentThe page did not answer the questions they asked. reply evanjrowley 18 hours agorootparentprevYou have a valid concern, but I&#x27;m curious how many sufficiently non-technical users would be reading Google&#x27;s blog. Practically speaking, it could be a moot point. reply qingcharles 18 hours agoparentprevI had a fire. I lost every single thing I own, except my landlord grabbed my phone, bless him. Otherwise I would have been totally stuck as all my TOTP apps are on there.Also, never lose your phone number. I can&#x27;t get back into my Google account even though I have the username, password and recovery email because I can never get the SMS code. reply TheNewsIsHere 17 hours agorootparent> Also, never lose your phone number. I can&#x27;t get back into my Google account even though I have the username, password and recovery email because I can never get the SMS code.This is an excellent point. Google seems to be uninterested in addressing this transparently, but despite their push for phishing-resistant MFA and first factor sign-in options, they still consider a phone number to be golden evidence.My father changed his phone number last year and never updated his Google account. Despite having a recovery email address he could access, TOTP, and printed backup codes, it was not enough. Google wanted to “verify it really was him” after a move (and IP address change) and it doesn’t even allow a password reset to be authenticated with any other recovery option. Phone number or bust. reply rurp 12 hours agorootparentI have heard and read about a number of similar cases where people can get completely locked out of their account despite being able to authenticate correctly, because they lost access to some other required resource that Google decided is essential. I&#x27;m very skeptical about the utility of these types of security policies. I&#x27;m sure they prevent hacking in some cases, but they also greatly increase the chance of a legit user permanently losing their account which is a pretty freaking bad outcome for someone who has all of their email, messages, photos, documents and more stored in their Google account.Given the importance of these digital services I expect that refusing to provide support to users in this situation, as Google is well known to do, won&#x27;t be legally tolerated at some point in the future. Unfortunately this won&#x27;t be changing anytime soon, so t",
    "originSummary": [
      "Google is setting passkeys as the default sign-in method for personal Google Accounts with a goal to enhance security and streamline the login process.",
      "Passkeys, which use biometric data or a pin, are considered quicker and safer than traditional passwords. Google has shared that the feedback for passkeys have been largely positive.",
      "Despite the shift towards passkeys, Google would still offer the option for users to use passwords for account access."
    ],
    "commentSummary": [
      "The discussion about using passkeys for online account authentication is mixed, with some users worrying about the potential for loss of access and insufficient support.",
      "Some argue that passkeys offer greater security than traditional passwords; however, this method raises concerns about dependency on device security and lack of passkey exportability and recoverability.",
      "The debate underscores the need for improvements in passkey systems and the necessity for user education and backup options."
    ],
    "points": 272,
    "commentCount": 564,
    "retryCount": 0,
    "time": 1696948226
  },
  {
    "id": 37830980,
    "title": "A student asked how I keep us innovative – I don't",
    "originLink": "https://ntietz.com/blog/forefront-of-innovation/",
    "originBody": "technically a blog homeblog / tagssletterprojects A student asked how I keep us innovative. I don't. Monday, October 9, 2023 Last week, I did a Q&A session for a friend's security class. One of the students asked a question that I loved. They asked something like, \"As a principal engineer, how do you make sure your company stays at the forefront of innovation?\" There are two reasons I love this question. The first is that it's a good and natural one, which I had early on too. The second is that it's unintentionally leading. It assumes you should be working at the leading edge of innovative technology. And that's why my answer started with \"I don't. I make sure we don't.\" A leading question gets a snappy answer! But that's not the whole story, of course. The key is to understand why you don't want to be on the leading edge of innovation all the time, and also to understand when it's appropriate. Why we use proven technology Most of the time, the problems you run into while doing your work are mundane. The vast majority of your hot new startup is things that have been done before. For any new web app you're going to have users, logins, a frontend, a database. For each of these, you could use something hot and new. You could tie your users to some public blockchain (sorry). You could come up with a novel new way of logging in (please, please, no). The frontend can be built with that new framework you saw on HackerNews last week (or is that already out of date?). And of course, the database should be a NoSQL, graph, or vector database depending on which hype wave you caught. Each of these bring advantages, no doubt. There's a reason I spent years working on graph databases: they're dope technology that can solve some real problems. There's also a reason I've talked many people out of using them. When you adopt a new innovative technology, you're giving up a lot. Proven technologies are searchable and have robust documentation. Have a problem with PostgreSQL? Pop it into a search engine and you'll get an answer right away. But have a problem with a vector DB? Comb the GitHub Issues or Discord and hope that you find an answer 🙏. This can save you so much time when you inevitably run into problems. They often have great ecosystems around them. With proven tech, like PostgreSQL, you will usually have great packages and integrations. Your well-known DB and well-known observability provider probably get cozy and integrate well. Your favorite language has drivers for this time-tested DB. But with the new stuff? You're writing a lot of that yourself, or patching it. They use well-known concepts. Proven technologies have kind of by definition been around a while. This means you can (more) reasonably expect people to know the core concepts. Most software developers are probably familiar with relational DBs, but far fewer are familiar with graph DBs. Well-known concepts are accelerants: they let you converse more quickly, design more quickly, understand more quickly. New concepts are a tax which slow you down as you have to understand it and fit everything into that new model. There's a lot to love with the proven stuff. This isn't a new or novel opinion: there are a lot of advocates of choosing boring technology. It's a strategy that I expect technical leaders to employ, and it's a red flag if teams are eschewing tons of the boring stuff. It means they probably don't have a good technical strategy and strong leadership. That said, sometimes it is justified. When (and how) to use innovative new tech The reason we build software is to get something done, to solve some problem. That destination is what guides our adoption of technologies. With any given choice, the question is: does this technology fundamentally alter my chances of solving this problem? If the answer is \"no\", then just go with the boring choice. It doesn't make a difference, so why would you give up the benefits? If the answer is \"yes, it makes us much more likely to succeed,\" then you get to move on. Now you have to figure out why. Committing to this should be done eyes wide open, so figure out the specific reasons that this technology is necessary. Contrast using it with using the boring choice, and try to figure out the properties that it gives you that you need. Once you've found that irreducible property that greatly aids in solving the problem, and it cannot be done with boring one? That is when you reach for the shiny new thing, and you go in eyes wide open. With the bleeding edge, you are going to get cut, but sometimes that's necessary. Use the boring things until you absolutely cannot succeed with it, and you'll get a lot further a lot faster. My framework for choosing technologies Part of technical leadership is being involved in technical design and choosing what to use. Here's my general approach for doing that (at work1). First, understand the problem. This is similar to how we approach debugging, because both are a form of problem solving. If you don't have a clear understanding of the problem at hand, then you cannot solve it, and you cannot pick the right tech to use. I like to test my understanding by explaining the problem to a lay person. If I can explain it in a relatively clear way, then I understand the problem well enough to proceed. Then, prove that a solution exists. This \"existence proof\" of a solution is always my first step, because if you cannot get anything working it doesn't matter, the problem isn't getting solved. It also allows you a lot of creative freedom. The outcome is a design document showing some valid solution to the problem. In this step I'll allow myself to use whatever technology comes to mind. Can I solve this with that shiny DB and my favorite programming language? The only point is to prove that a solution can exist. Now reduce down the solution. Now that you have a proof of a solution, you can reduce it down to its essential complexity. For each component in your design, what role does it serve, why did you include it? Go deep and determine the absolute properties that each piece provides, and question if you need those properties or can achieve them another way. Then iterate on your design, cutting out unnecessary things. Refactor pieces of the design. Add new pieces, remove old pieces, play with it, make it sleek. Evaluate your design again. Now that it's reduced down, look at it again and ask a few questions: Does it still solve the problem at hand? Can this possibly be done in any simpler way? Why or why not? Can we use more well-known technologies instead? Once you have those answers, you'll either repeat the process or proceed on. Socialize the design. Hopefully you've been working as a team so far up to here, but you usually can't include everyone in the early design. Now that you've reduced it as much as possible, go find some critics and socialize the design. Find people who you think will be contrarian, and have them poke holes in the design. Especially in any new technologies or innovative things. When you have convinced your critics and yourself, you can actually move on and... wait, did we finally get to use a new piece of tech? Yes! And you know that it's for the right reasons. It serves a critical role in the solution and it cannot be replaced. So, yes. As a principal engineer, I view it as my role to keep us off the bleeding edge as much as possible. That way, when we really do need to innovate, we have the capacity to do so. And when we don't need to, we can go really freaking fast. 1 This framework is only for at work. On my personal projects, I will usually go with whatever sparks joy. My personal projects are usually just about learning and having fun, so \"optimal technology\" is evaluated very differently. If this post was enjoyable or useful for you, please share it! If you have comments, questions, or feedback, you can email my personal email. To get new posts, subscribe to the newsletter or use the RSS feed. Want to become a better programmer? Join the Recurse Center!",
    "commentLink": "https://news.ycombinator.com/item?id=37830980",
    "commentBody": "A student asked how I keep us innovative – I don&#x27;tHacker NewspastloginA student asked how I keep us innovative – I don&#x27;t (ntietz.com) 240 points by SerCe 22 hours ago| hidepastfavorite101 comments dvas 18 hours agoSocialize the design: \"Find people who you think will be contrarian, and have them poke holes in the design\"Happy to see this mentioned! By involving everyone, the experienced engineers will usually ask the harder questions and test the assumption of your design and the juniors to learn and re-evaluate assumptions they have about building software. Almost a win-win for everyone.Even though in these discussions there may be a social aspect where certain individuals are trying that much harder to find disadvantages of a proposed design (many reasons which readers are probably familiar with). However, I think this becomes a net positive to the overall engineering as everyone tries to bring their A-game to these discussions.Anyhow, my experience has been that collectively reasoning about a design (assuming the team feels comfortable with criticism) will always get results quicker. reply atoav 14 hours agoparentImportant additional requirment: do not have a manegerial class that will throw all reasoning of those on the ground out of the window and demand it to be done a certain way.I had a boss like that once. He could literally turn everything into shit. You had a good plan for an event room, he strikes everything useful out for \"minimalism\" and aesthetics reasons. Then they ended up with a room that was both expensive and unusable. No wall sockets pure concrete ceiling (\"to show the carrying structure\"), a reverb time worse than in a church.For him it was always looks above literally everything else, no matter if it was about architecture, furniture, tools or graphic design. And he had the habbit of impulsively demand changes after months of planning, as soon as things started to take shape. Horrible. reply paulryanrogers 11 hours agorootparentThe Jony Ive school of management? reply intelVISA 18 hours agoparentprevCan be a tough act to balance in reality: \"assuming the team feels comfortable with criticism\" is where this well-meaning process falls over in most shops. reply yndoendo 16 hours agorootparentOne of the best design evaluation methods I found was to teach and train others on the products. Occasionally doing onsite project installations was a good feedback loop. I wouldn&#x27;t let them know I was a principal designer. Would sit back and see how well they grasped the design and constantly take notes on how to improve it.I want criticism about the designs. Tell me where you think the product is junk and why. There is not one user but many users you are designing for in a single product. Each has an unique take and requirements. Features to sell it, those that write the checks, and those that use it daily must both be met. reply ska 17 hours agorootparentprev> is where this well-meaning process falls over in most shopsI would argue that if you have this problem, it is fundamental, and your team will never be firing on all cylinders until you fix it. Admit it is not easy if the dysfunction is well set in. reply saulpw 16 hours agorootparentGreat, but then how do you fix it, how long will that take, and how do you get any work done until then? reply RoyalHenOil 10 hours agorootparentI have a job where we all check each other&#x27;s work as a routine part of the process. Most of our work undergoes five official rounds of double-checking&#x2F;feedback, and we often do a couple more unofficial rounds of testing just to be sure. Every round of testing always finds at least a couple of issues (often many more) and so we&#x27;re all very used to getting feedback.Testing takes time, but we factor it into our schedule.When new people join the team, they make more mistakes than experienced people, which can be very demoralizing for them. There are a couple of things that I find helpful for this:1, I warn them during training that they will make a lot of mistakes, but it will get better as they get practice, and I ask them to be patient with themselves. You can only learn one thing at a time, so some of the training won&#x27;t click right away.2, I tell them that even those of us who are experienced make mistakes; that&#x27;s whole reason we have this feedback process. I ask them not to shy away from pointing out issues if they notice anything in anyone else&#x27;s work, even if that person is much more experienced than them.3, I make sure they see other people&#x27;s mistakes before they start making their own; once they are out of the training period, their first tasks are to check other people&#x27;s work, rather than do their own work. After that, they may be tasked with fixing other people&#x27;s mistakes (which also helps them understand the sorts of mistakes to watch for in their own work later). Only then do they start their own work. reply wiml 8 hours agorootparentThat sounds fascinatingly different from how most teams I know of operate. If you don&#x27;t mind talking about it a bit more, what application domain do you work in? Do you know how this work style arose in your team? reply kridsdale3 7 hours agorootparentI hope its spacecraft design. 5 reviews! reply ska 15 hours agorootparentprevIncrementally is the answer to most of your concerns, but the specifics will depend a lot on context. More concretely, this is bread and butter stuff for engineering leadership. I mean that in the broad (including experienced staff) sense, not hierarchically. reply lfowles 18 hours agorootparentprevI&#x27;ve observed it used as a great way to shut down projects that don&#x27;t immediately have demonstrable results reply Exoristos 16 hours agorootparentprevIf your shop is full of grown children, like this, then, face it, you&#x27;re doomed regardless. reply convolvatron 16 hours agorootparentso much discussion around software engineering these days is based on infantilization and making sure you get _something_ out of your fundamentally dysfunctional team. its pretty disheartening. reply mattgreenrocks 11 hours agorootparentAbsolutely. It’s a toxic, nihilist mentality that leaks into tools that greatly influence how we think.Bad programmers are an organizational failure. You cannot fix that with tools.You can make it easier for devs to fall into a pit of success, however. But, it’s fine line between facilitating that versus foisting things on users under the guise of being “opinionated.” The reality is it is often taken to be patronizing, such as views of nextauth on storing local usernames and passwords. reply anotherhue 14 hours agorootparentprevI keep waiting for the O’Reilly book “Agile: Managing Mediocrity”. reply Pannoniae 13 hours agorootparentprevIsn&#x27;t what agile&#x2F;scrum literally is? Or \"equaliser\" languages like Go? Both a trained monkey and a senior engineer will be equally mediocre in using it. Zero room for improvement. reply mattgreenrocks 11 hours agorootparentIndividual programmer skill is seen as taboo to discuss despite it having a huge impact on project outcomes. reply Jensson 8 hours agorootparentProject success? Thanks to workers, they deserve a reward! Project failed? Due to managers, they should be replaced!Or vice versa, depending on what side of the fence you are on. Acknowledging incompetence seems to be taboo everywhere, managers don&#x27;t want to acknowledge that managers are often incompetent, and neither do workers want to acknowledge workers are often incompetent. reply DiggyJohnson 3 hours agorootparentI think I disagree. As an IC, I have often experienced the problem of all the developers knowing who the critically weak developer is on the team, but nobody is willing to inform the manager of just how much of a problem the weak link is causing, and the manager doesn’t have the perspective or role to realize just how poor that teammate’s work is, despite generally understanding the hierarchy of ability &#x2F; experience &#x2F; output of the team.And I say that as someone who thinks this forum tends to dunk on management a bit too much. My point is that workers do want to acknowledge that others workers are incompetent, and likewise for managers and their colleagues, but they are structurally and culturally disinclined to do so primarily from an information gap problem. reply lmm 10 hours agorootparentprevI&#x27;d say that&#x27;s backwards. Agile is all about having a lighter and more flexible process that allows skilled developers (very little correlation with \"senior\" IME) to go faster than something more rigid like RUP or Six Sigma. Agree about Go though. reply Pannoniae 10 hours agorootparentI am not talking about actual Agile (as in the Agile Manifesto), I am talking about the actual \"business\" bastardisation of it. Otherwise, I 100% agree with you :) replykjkjadksj 17 hours agoparentprevSeems like that would invite armchair engineering like you seen in HN comments. “Couldn’t you just do the easy and obvious thing?” The answer is probably a long no and the creator had your thought already before realizing the true scope of the work. reply ar_lan 16 hours agorootparentIt&#x27;s definitely double-edged. The main thing I&#x27;ve learned regarding this is to clearly document this answer somewhere, very early on.I recently learned this because I was asked this precise question and basically had to drudge up all my two-quarters-ago research that was the answer to this question. My answer to their question was \"yes we could do the easy thing, but...\" and everything trailing the \"but\" is a very long string of small points that add up to something that tipped the scale (at least in my and several other folks opinions). But without that laid clearly out they thought I was just over-engineering for the sake of job security or something like that. reply pixl97 8 hours agorootparentMake sure people know why Chesterton&#x27;s fence was put up.Letting people know why is two fold. Not only does it keep them from relearning the 1000 edge cases you discovered in the school of hard knocks, sometimes technology changes and something that was a hard limit no longer is. reply delusional 15 hours agorootparentprevI find this pretty easy to deal with. You just don&#x27;t engage with that question. Make it clear to the people you are discussion it with what kind of things you want input on (architecture, design, product market fit), and make it very clear when you consider their input out of that scope. People will very quickly learn what sort of comments you care about.This form on \"collective design\" usually uses a confrontational form of rhetoric, but the exercise is very much collaborative. reply 8note 17 hours agorootparentprevThat&#x27;s great stuff to include in your design doc, at least on an appendix reply lizard 9 hours agorootparentprevIs that so bad?Ideally, if as others have suggested, you&#x27;ve already worked through that question and documented it, you can just refer to that documentation amd move along.If you haven&#x27;t documented it, it&#x27;s a good reminder others may have the same question and answering it will either lead the to the same conclusion or other questions you did not think of. reply reactordev 14 hours agorootparentprevWithout proper management, you&#x27;re correct. It would. I would retort, never start a sentence with a negative. Find something you do agree upon, make comments about the stuff that you do like, before going into the details of what you don&#x27;t. This way it&#x27;s clear where those boundaries are and in what context the disagreed design would need changing, if any. This also prevents brilliant jerks from completely destroying the confidence of others to even present designs to the group. Yes, it&#x27;s a little showy, using more words than are necessary, but it keeps the human aspect of agreement and cooperation in-tact. reply Jensson 13 hours agorootparent> I would retort, never start a sentence with a negative.\"Could you do the easy and obvious thing?\" means the same thing, I don&#x27;t see how it makes a difference. reply ozim 12 hours agorootparentI believe parent was more into “could you do xyz?” Where xyz is easy and obvious thing, but no one would explicitly phrase it exactly like “could you do easy and obvious thing?”.This way proposing xyz as a solution is positive not a negative. reply reactordev 13 hours agorootparentprevEasy and obvious are subjective. reply EdgeExplorer 16 hours agoparentprevUnfortunately a lot of contrarian people think \"it wasn&#x27;t my idea\" is a hole in a design even if they would never say it. reply indigochill 16 hours agorootparentNot a problem if they don&#x27;t say it since then it&#x27;s not derailing the conversation. It could even be a benefit if it motivates them to look for actual holes. reply convolvatron 15 hours agorootparentyou just have to refuse to accept input like &#x27;I don&#x27;t buy it&#x27;, &#x27;I tried something like that before and it was a failure&#x27;, &#x27;thats a code smell&#x27;, &#x27;I saw a blog post about how that is an anti pattern&#x27;, and other unsubstantiated disses on an approach. if its so bad, you should really be able to put some real words behind that. reply 6510 9 hours agorootparentYou have to exclude them from the discussion if you know they will go there. reply _jal 15 hours agoparentprev>\"Find people who you think will be contrarian, and have them poke holes in the design\"Socializing the design is good advice.Seeking out contrarians is not.It costs you much more to defend particular engineering tradeoffs than it does to raise questions about them. So unless you have a good relationship with your \"contrarian\" and they will operate in good faith, you&#x27;ll quickly end up exhausted and dispirited after the anklebiters attack.Architecture astronauts and other armchair engineers will suck up all your energy with half-baked ideas they aren&#x27;t even invested in, if you let them. reply ska 15 hours agorootparentContrarian probably isn&#x27;t the right term, but there is real value in having someone come at the problem with a different set of blinders than you have - particularly if they understand their role is not to redo the design but to stress test the existing one. I suspect this is what you mean by \"good faith\" and \"good relationship\". reply beebeepka 10 hours agorootparentI think the poster you replied to understand this. What they&#x27;re saying is that asking for input often invites malicious behaviour. It&#x27;s really painful to experience. I think this person knows what they are talking about.There are people who somehow manage to ruin even the simplest code reviews with their precious input that ultimately goes nowhere. Such people will absolutely destroy your spirit when you invite them to consider your something substantial. Those guys so everything in their power to get themselves into such position. reply charlie0 13 hours agoparentprevThe issue I see here is that being a contrarian is not enough. In order to drive change, one must also have influence. It doesn&#x27;t matter how good a design is if others are not quick to agree with it. reply reactordev 14 hours agoparentprevit&#x27;s a win-win-win. Win for the seniors to ask the harder questions and make sure there&#x27;s alignment. Win for the juniors who learn and re-evaluate assumptions and bias. Win for the business because they get the best design (at the time, with the resources on hand) for the ask.It&#x27;s important to make sure that folks who are doing the work, agree upon the work. Building a bridge only works if the engineers are building a bridge, not a suspended walkway of dubious quality because they didn&#x27;t like the design so some wood and rope is what you got. reply scubbo 17 hours agoprevA relevant slideshow (with captions): https:&#x2F;&#x2F;boringtechnology.club&#x2F;You can tell it&#x27;s getting old because it used Phony Stark as a symbol for innovation, but the underlying message is timeless. reply sublimefire 16 hours agoparentTotally, if you want to solve the problem you do not want to create more problems by solving it. Great slides and captions BTW.Nothing pisses me off more that to maintain two stacks at the same time - the good and the bad. The bad one was enough. reply garba_dlm 16 hours agorootparent> if you want to solve the problem you do not want to create more problems by solving it.but if, in contrast, what you want to create is a businessthen it&#x27;s easy to argue you actually DO want to create more problems; partially so you can \"solve\" them in \"the next version\" (or something); this way your \"customers\" (which really are looking more like victims) can have a real motivation to upgrade; to buy the next version, to keep on being \"valued customers\"this is why making tech products used to be very different from making consumable goods (e.g. food); right up until all businesses turned into subscriptions businesses and other kinds of rent-like schemes reply atomicnature 18 hours agoprevHowever, isn&#x27;t innovation usually about building something exciting from output perspective, rather than about the building materials? We want the most reliable building materials, but how you put them together to get novel functionality, or other desirable characteristics at a higher level is what makes a system innovative. reply koonsolo 2 hours agoprevAdvocates of new technology mostly only mentions the benefits. I always ask what the drawbacks are, and when the answer is \"None\", I reply \"Ah, so you don&#x27;t know the drawbacks yet?\"Everything is a tradeoff. If you want to switch to something newer, make sure you know the benefits and drawbacks of both systems, and then decide if it&#x27;s worth the effort and risk. reply ngrilly 17 hours agoprevI am a proponent of \"choosing boring technologies\", but I don&#x27;t see how it relates to building innovative products and services. You can create an innovative product or service using boring technologies under the hood. I really like the post otherwise. reply mattgreenrocks 17 hours agoparentYounger devs conflate tech choices with innovation.They’ll believe that a computer vision company is a dinosaur because they’re using C++ and a glorified CRUD startup is hip and relevant because it uses Rust. It doesn’t help that the anemic-ness of some domains doesn’t set in until 1-2 years later. reply BlackFly 14 hours agorootparentI agree that novel technical choices are often conflated with innovation over eagerly. My experience is that in learning the lesson that these things are different many people over correct and assume that novel technical choices are only ever resume padding or useless hype.There are no silver bullets and the only way to get an order of magnitude improvement is iterated marginal improvements. That something is at best a marginal improvement shouldn&#x27;t be taken for a lack of innovation. reply kritr 16 hours agorootparentprevUsing newer technologies sometimes buys development velocity, which is a good indication of innovation. reply LtWorf 15 hours agorootparentOr it makes development slower due to lack of documentation, support, and libraries for the chosen technology. reply threeseed 7 hours agorootparentThere are plenty of boring languages that have poor documentation, support and libraries.Often it&#x27;s all just out of date. reply js8 16 hours agoparentprevYeah, I favor a slightly different rule. Choose a conservative technology in places which are not a core competency of your new product. Building a new product is risky, and you should minimize the additional risk coming from a technology choice, except the parts where you&#x27;re actually trying to beat the competition. That should be the focal point of innovation. reply ngrilly 15 hours agorootparentTotally agree. That&#x27;s why I like the notion of \"innovation tokens\", also mentioned in the seminal post \"Choose Boring Technologies\" [1]. You choose boring technologies for everything, except for a few topics that will differentiate your solution, where you get to spend your \"innovation tokens\".[1] https:&#x2F;&#x2F;mcfunley.com&#x2F;choose-boring-technology reply bdcravens 16 hours agorootparentprevAs most companies aren&#x27;t pure tech, usually the core competency isn&#x27;t the software itself. Sometimes when it is, it&#x27;s hidden away (for instance, background processing of data) and the best thing you can do is keep as easy to reason about as possible. reply feoren 17 hours agoparentprevI think the article is a good response to the question the student asked, though:> \"As a principal engineer, how do you make sure your company stays at the forefront of innovation?\"This is most likely about using all the shiny new technologies in a rapidly changing \"forefront of innovation\". The article simply isn&#x27;t about building innovative products and services.Of course, choosing boring technologies is one of the ways that you can help save engineering brain-cycles for the actual innovative core product or service you&#x27;re building. reply bdcravens 16 hours agoparentprevIt also applies to trying to avoid being clever with the tech you choose. reply GrumpyYoungMan 18 hours agoprevSolid article. The one quibble I have is IMO that there should be mention that, when the choice is made to use an untried technology, it needs to be acknowledged and communicated that this carries some level of risk and an increased chance of not meeting the project goals. reply ulizzle 7 hours agoprevIt’s a leading question, likely political, trying to pose innovation as some sort of universal good when engineers intuitively ask “what” exactly they mean by that, as evidenced by this post.Any engineer knows intuitively that the answer when it comes to picking new technologies in your architecture is always: “it depends” but that’s not the point.When it comes to socializing the design, imo, it’s another ideologically motivated waste of time. In order to poke holes you must know the problem well enough to do so, but that’s not what people do on their “sprint planning”. Instead everyone has their own problems, which they must complete by the end of the sprint, or get a ding in their performance review.With no accountability in their estimates, there’s no pressure for accuracy. That’s why tests are better than code reviews. reply kubb 13 hours agoprevThis post seems to be mistaken in its understanding of innovation. It seems to take it as using new technologies. But this is a misunderstanding.Innovation means inventing novel solutions that solve relevant problems better than what has been known before. This could be creating a new tech, and using it, but it could also be applying existing tech in a novel way.So you could be innovative using old technology, and you could use new technology, but not be innovative.I guess the company where the author of the post works is neither innovative, nor do they use new technology, but there&#x27;s space on the market for companies like that. Principal engineers who work there don&#x27;t even need to know the meaning of innovation. reply mattgreenrocks 11 hours agoparentIMO, most people don’t want to be working on innovative things, despite what they say. It is brutally hard sometimes, and you have to be okay with being alone to figure things out in your own. reply datadrivenangel 18 hours agoprevThe bleeding edge is called the bleeding edge, because you risk cutting yourself on it!Innovation can be good, but you have to understand the risk&#x2F;reward profiles. reply a1o 18 hours agoprev> On my personal projects, I will usually go with whatever sparks joy.I think this is the right thing for personal projects. I think throwaway toy projects should be free to go that route too. reply sdoering 17 hours agoparentI agree. I once built a small weather application that changes the background image based on the weather being reported.I wanted to understand single page applications a bit better and just decided to do it in Vue. Had never done anything bigger than a bit of web tracking in JS before. So it was quite a fun experience. I still sometimes enjoy using it (it is still running and available, so I like to visit it like sn old friend). reply sublimefire 16 hours agoparentprevI like to think about it like R&D in a company. You do not spend all of the budget on it but just some part, like 15%. Similarly you try out new things and spend some time on it, like 15%. reply xnx 18 hours agoprevVery sound advice. It&#x27;s amazing how many projects never identify what problem they&#x27;re trying to solve, or lose track of it immediately to focus on the technology. reply mysterydip 17 hours agoparent\"our project is just like x but written in y!\" reply sublimefire 16 hours agorootparent\"it is very easy use, I wrote it in a week\" reply reactordev 14 hours agoprev>\"Now reduce down the solution\"I love this part. This is the validation after proof that you indeed, solved the correct problem. Because if you can&#x27;t shave away the complexity to the underlying problem and \"get to the heart of it\" then you didn&#x27;t solve the problem yet. You merely handled its edge-cases. reply YetAnotherNick 11 hours agoprevThis is the case where author has a fixed story and then filled rest of the details with good sounding sentences.> Have a problem with PostgreSQL? Pop it into a search engine and you&#x27;ll get an answer right away. But have a problem with a vector DB? Comb the GitHub Issues or Discord and hope that you find an answer> They often have great ecosystems around them> They use well-known conceptsPostgreSQL issues are generally significantly harder to debug than vector db in my experience. In general, a lot of times I found documentation and concepts to be significantly better for quite a bit of newer technologies like tailwind, caddy, gin, golang than say springboot or react or oracle DB or C++ or cassandra. Comparing PostgreSQL to graph DB or vector DB is very apples to oranges as one can&#x27;t replace other.Telling old is better than new in general is both wrong and harmful to software engineering. reply mplewis 9 hours agoparentBut this is empirically true, I’m not sure what your issue is with the statement? The likelihood you will run into a serious undocumented issue with Postgres is so much lower than with some new vector database. I’ve personally experienced this moving between the mature Rails ecosystem and multiple immature microservice frameworks. reply YetAnotherNick 7 hours agorootparentAs I said comparing postgres to vector db is weird example as one is not replacement of other and one field is going more technological shift. Better example would be comparing say apache to caddy or spring boot to gin or even oracle nosql to mongo. reply jackblemming 16 hours agoprevNot interested in hearing people parroting the boring technology mantra over and over. Maybe say something novel for once. Also maybe choose technology based on pragmatic metrics, not how old it is. “Choose boring technology” is like MBAs saying “we’re data driven”.And I guarantee if we look at this persons tech stack it’s probably slathered with a bunch of shiny new AWS crud that’s “web scale”. reply stouset 13 hours agoparent> “Choose boring technology” is like MBAs saying “we’re data driven”.This is so true.“Choose boring technology” is also used to post-hoc justify whatever choices you’ve already made. To many, golang is boring and Rust is unproven. To others, both are unproven and C&#x2F;C++ are boring. Who’s right? Everyone and no-one. reply tikhonj 13 hours agorootparentC is boring in the same way that weaving through traffic at full speed on a motorcycle is boring—people have been doing it forever and some of them haven&#x27;t even gotten hurt by it... reply taylorbuley 16 hours agoprevI feel this. But at the same time, there are these unproven toolchains that just feel right and I am OK adopting them. GraphQL is one today. React (and before it, Backbone.js) before. Hell, even CoffeeScript. Angular 1.0 was one thing, but I don&#x27;t regret a single one of those other implementations. reply Justsignedup 22 hours agoprevWell put, and is a great read for people to understand the fundamentals of leading an eng team. What sort of thinking goes into it. I feel like this perfectly explains how I need to think on a daily basis.Lead engs are about bringing order to a chaotic eng process. reply Joel_Mckay 17 hours agoprevThank you for being honest with the kid, as a lot of folks exploit peoples inexperience for financial reasons as policy.In general, there are a few assertions people make:A. The industry always changesB. The industry always follows the same trendBoth are incorrect, but rather the truth floats somewhere between the two.A better question is often \"what skills will offer long-term fiscal utility?\"The answer to that utility question exposes the dirty side of the tech industry:1. is the concept an orthogonal theme from purely Academic or Commercial Marketers? If Yes, than the probability it will still be around in 2 years is vanishingly small.2. is the role occupied by Senior staff over 35 at more than 6 firms? No, than the probability of redundancy increases exponentially with time, as the skill demand is under artificial supply manipulation due to wage suppression, regulatory capture, and or tax incentives refactored into a subsidy.3. what is the average ratio of staff with the skill still present at firms over 3 years? if less than 5%, than the roles still fall under churn-and-burn economics, and at some point the stock valuation bump will need ritualistic HR sacrifices. if over 90%, than a roles true value is marginal, and will be bid down in pay over time.4. An \"award for the worlds smartest sucker\" is not a certificate that will hold value. Anyone that claims they know what will happen in 5 years is a fool selling something like nonsense vendor certifications, media packages, and or political rhetoric (STEM etc.)5. Does the skill require physical presence? No, than the law of outsourcing bids down skill value over time due to irrational competitors.6. Integrity in whatever you choose to do is important regardless of the role. For example, someone using an emotionally charged subject in an 72% LLM generated article will antagonize the wrong people eventually.7. Copying what others do... often means one will land in second place... or worse.Good luck out there, =) reply __xor_eax_eax 14 hours agoprevMeh. Reading the techniques behind the giant GCloud DDoS before this, if you&#x27;re not understanding the latest and greatest technology and you&#x27;re attackers are, you&#x27;re going to be victimized.To me this sounds like a stogy old grey-hair being defensive about why its okay to be obsolete. Its not reply innagadadavida 14 hours agoprevThe tearoom is ask is “Can I hire an average software engineer with 2-5y experience who’ll will have no problems keeping this running 99.999% of the time while doing development and deployments? reply mlindner 14 hours agoprevThis is a great answer if the thing you’re building isn’t innovative. If it’s just a nodejs storefront site. However it’s a pretty garbage answer if you’re say trying to create a new thing that’s not been seen before. reply tikhonj 16 hours agoprevAnother \"use popular tech because it&#x27;s popular\" article dressed up as engineering wisdom. \"Nobody gets fired for choosing IBM\" isn&#x27;t supposed to be aspirational!The most effective organizations I&#x27;ve seen have been the ones who are the most willing to both use \"non-standard\" technologies and develop their own tools in-house. There&#x27;s a question of causation—are they more effective because they&#x27;re open-minded, or do they get away with weird choices because they&#x27;re more effective?—and, from what I&#x27;ve seen, it&#x27;s a bit of both. But I&#x27;ve absolutely seen, first-hand, that top-down standardization on enterprise \"best practices\" is absolutely not a recipe for anything beyond passable mediocrity. If anything is a sign of weak engineering leadership, it&#x27;s that.I&#x27;m not saying that a team should use different technologies for the sake of using different technologies, I&#x27;m saying that strong engineering leadership means having a clear technical vision and culture where the popularity of a tool is very low on the list of considerations when choosing what to use. reply pdonis 15 hours agoparent> \"use popular tech because it&#x27;s popular\"...is not what the article is saying. It&#x27;s saying that you should use boring tech when it is sufficient to get the job done that you need done. \"Boring\" is not the same as \"popular\"; indeed, \"popular\" is often skewed by hype about the latest shiny new toy. \"Boring\", as it is used in the article, just means \"does job X and is proven by much experience to work for that purpose\".> I&#x27;m not saying that a team should use different technologies for the sake of using different technologiesAnd this is the same point the article is making. The article is saying you should only use different technologies to the extent that you have to to get the job done that you need to get done. reply BlackFly 15 hours agorootparentThe person you responded to is using a different definition of popular than you are.Popular: 1. Widely liked or appreciated. 3. Of, representing, or carried on by the people at large.In the phrase, \"use popular tech because it is popular\" you should think of the latter definition: carried on by the people at large. Hence the comparison to \"Nobody got fired for choosing IBM.\" Instead some people decide to call this \"boring\" technology and you have a movement of people trying to make boring choices glamorous that the person you are responding to is lamenting. The article is saying to use popular technology (definition 3 above) because of the implications of \"at large\".Pragmatically, the folklore isn&#x27;t helpful. Choosing boring technology and then building an ad hoc layer on top of it compared to using a cutting edge off the shelf solution for the problem at hand shouldn&#x27;t be an easy decision based on engineering folklore. The difficulty is often in seeing that your actual problem is well adapted (never perfectly) to a particular novel technical choice and compounding that is the difficulty of knowing about the novel technology and what problem it is actually adapted to solving. Hence the need for technical vision and understanding of the problem. reply pdonis 15 hours agorootparent> The person you responded to is using a different definition of popular than you are.No, he&#x27;s conflating \"popular\" as he uses the term with \"boring\" as the article uses that term, which, as I explained, is not valid. As the GP uses \"popular\", I agree that tech should not be chosen because it is \"popular\"; see my response to tikhonj&#x27;s follow-up post. But as I noted in that response, the article is not arguing for choosing tech because it is \"popular\" in that sense.> The article is saying to use popular technology (definition 3 above) because of the implications of \"at large\".No, the article is saying that you should not even care whether tech is \"popular\" in any of your senses; you should choose tech based on whether it does the job that you need done based on much prior experience. I.e., based on technical criteria, not popularity (in any sense).The article does imply that tech that does the job you need done based on much prior experience will be \"popular\" in your sense 3, but that does not mean it is telling you to choose the tech based on popularity. reply tikhonj 15 hours agorootparentprev\"Boring\" inevitably means \"popular\" in these discussions—\"popular\" in the sense of \"mainstream\" and \"widely used\" rather than \"trendy\" or \"hyped\", but it&#x27;s still a popularity contest at the end of the day. reply pdonis 15 hours agorootparent> \"Boring\" inevitably means \"popular\" in these discussionsNot the way the article is using the term \"boring\", or the way the post I responded to was using the term \"popular\".\"Boring\" means choosing the tech on the basis of it being proven to work by much experence. That is what the article is advocating doing unless the boring tech simply won&#x27;t do the job you need it to do. The \"boring\" tech might well also be popular, but that is irrelevant to what the article is saying because the tech is not being chosen for its popularity, it&#x27;s being chosen for its technical capability.\"Popular\" means choosing the tech on the basis of its popularity. That means not even looking at whether the tool meets technical requirements. That is what the post I responded to was describing, and arguing against. But the article is not advocating for doing this, so arguing against it is not arguing against the article at all.> it&#x27;s still a popularity contest at the end of the day.Not if the criterion is technical merit. See above. reply ska 15 hours agorootparentprev> \"Boring\" inevitably means \"popular\" in these discussionsNo it really doesn&#x27;t.In this context it means something more like: we know this approach has been successfully shipped hundreds of times for similar use cases. There may be warts, but they are known, and it will work. reply jf22 15 hours agorootparentprevI have never encountered a scenario where \"boring\" meant \"popular\". reply kelsey9876543 16 hours agoparentprevThe author makes a horrible assumption themselves right in their opening argument that \"most of the things you are trying to do have been done already\". How is a business going to find value copying and pasting sql solutions from stack overflow as honestly suggested by the article author? Horrible advice, pick the correct tool for the job, sometimes that tool is brand new because you are doing something brand new. The author completely avoids the real question which is how to select what new technology to use by saying only choose old technology. reply blincoln 14 hours agorootparentIt is not a horrible assumption for the vast majority of developers.Even if the business they&#x27;re supporting is doing something genuinely new (and IMO that&#x27;s rare), most of the software written for that business is almost certainly going to do something similar to a bunch of existing software.ChatGPT&#x27;s Vision add-on is able to do things like generate code for a web dashboard from a screenshot of a mockup because nearly every web dashboard is extremely similar except for the branding and labels on the data&#x2F;elements.Most developers working for a business are writing that kind of code. It&#x27;s the dev equivalent of hiring an architect and contractor to build a house. The result may very well be unique overall in some way, but nearly all of it would be based on well-tested designs and components. Most people wouldn&#x27;t want the contractor to fabricate custom roofing panels out of recycled Russian submarine titanium, because even though it might have a \"wow\" exotic factor and some tangible benefits, having maintenance done on it would be a nightmare, and there are likely to be surprises no one considered because it was the first house to use them.Software is even worse in some ways. I&#x27;ve seen complex products built on flash-in-the-pan frameworks of the moment that ended up with problems so close to the foundation that fixing them would require a complete rewrite or forking the (now unsupported) framework and making low-level architectural changes there. At least with a titanium roof, the worst-case scenario is having to replace the entire roof.There&#x27;s some audience bias here, because people who read Hacker News are more likely to be working for startups that do something genuinely new, and maybe even writing code that does something genuinely new, but IMO it&#x27;s still not very common. reply luispauloml 8 hours agorootparentprev>The author completely avoids the real question which is how to select what new technologyThere is a whole section called \"When (and how) to use innovative new tech\" talking about this, and another one called \"My framework for choosing technologies\" describing the author&#x27;s approach. reply marcosdumay 13 hours agorootparentprevI&#x27;d bet the people programing the LHC detectors have plenty of their problems already solved in Stack Overflow.If most of the things you are trying to do were never done, you will never achieve anything, with complete certainty. Not even if you are trying to make art. reply turtlebits 14 hours agorootparentprevDisagree - everything novel has been done already. Technology at its basic form is moving&#x2F;transforming data. Too many engineers fall into the trap of chasing what&#x27;s new. reply yayitswei 10 hours agoparentprevAgree. Another way I like to think about it is to pick the simplest path to solve the problem, which might not be the most popular solution. Choosing postgres by default bakes in a ton of assumptions and often adds extra layers of complexity. Which ORM do you use? What if you want to version your changes? reply willcipriano 18 hours agoprev [–] Would you consider a different pattern, woven on a old loom a new technology?A app is not a \"new technology\". A microprocessor is a technology and the code you run on it is just another pattern on the loom. reply terminous 17 hours agoparent> Would you consider a different pattern, woven on a old loom a new technology? A app is not a \"new technology\". A microprocessor is a technology and the code you run on it is just another pattern on the loom.It&#x27;s not useful to get pedantic about the definition of a \"new technology.\" What is your point? Take the example cited in the article about deciding whether to store your data in SQL databases, vector databases, or blockchains. Are you saying that this is a purely aesthetic choice and not a choice that bakes in a lot of other patterns and decisions? reply willcipriano 17 hours agorootparentIt&#x27;s a different mindset once you start thinking in those terms. Inventing a new technology sounds impossible but writing some code isn&#x27;t difficult. You&#x27;ll be less likely to get paralyzed daydreaming about the lofty future of this new technology if it&#x27;s just some code that can be thrown away at any time for something better.When we list humanities technological inventions a century from now, I don&#x27;t think we will be listing CockroachDB or similar. reply terminous 15 hours agorootparent> it&#x27;s just some code that can be thrown away at any time for something better.That&#x27;s exactly what the author is arguing against. Decisions about foundational infrastructure (frameworks, databases, programming languages) have consequences for the organizations that decide to use them. Switching costs can be high. Lock-in of all kinds is real.> When we list humanities technological inventions a century from now, I don&#x27;t think we will be listing CockroachDB or similar.That isn&#x27;t at all what I or the author of the article was saying. reply willcipriano 14 hours agorootparentIn this use, sometimes technology will upgrade itself with new technology that uses technology to prevent the users from doing things the technology previously had the technology to do, but it no longer has the technology due to the technology that was downloaded and installed on the technology in the user&#x27;s closet. reply MattPalmer1086 17 hours agoparentprev [–] Everything is built on top of other stuff. Surely a micro processor is just a product made using chip fabrication technology? Which itself is just a commoditised production method based on other... err... technologies.You could pick another word at some arbitrary level, but I&#x27;m not sure it&#x27;s a particularly useful distinction to draw. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author emphasizes the significance of using tried-and-true technology for its robust documentation, familiarity, and established ecosystems.",
      "Innovative tech should only be opted for if it dramatically boosts the chance of problem-solving. This idea forms a part of the author's tech selection framework, which includes understanding issues, validating solutions, refining complexity, evaluating designs, and exposing ideas to critics.",
      "However, the author advises remaining clear of cutting-edge technologies to reserve capacity for innovations. This framework's approach is primarily for work projects, as personal projects are prioritized based on personal enjoyment and learning potential."
    ],
    "commentSummary": [
      "The article underscores the need to socialize the design process, engaging diverse viewpoints, and gathering feedback to encourage innovation in software development.",
      "It explores the challenges in facilitating effective team communication, advocating for open communication and constructive criticism.",
      "The piece debates the risks and rewards of selecting innovative versus established technologies in development projects, arguing against top-down standardization and stressing on the value of technical merit in technology decisions."
    ],
    "points": 239,
    "commentCount": 101,
    "retryCount": 0,
    "time": 1696939218
  },
  {
    "id": 37834946,
    "title": "US sues eBay for allowing sale of emissions defeat devices",
    "originLink": "https://www.thedrive.com/news/us-sues-ebay-for-allowing-sale-of-more-than-300000-emissions-defeat-devices",
    "originBody": "NEWS CAR REVIEWS FEATURES THE GARAGE THE WAR ZONE NEWSLETTER SIGNUP News → Car Tech → Aftermarket Ebay Could Owe $1.9 Billion in Fines for Allowing Sale of 343,000 Emissions Defeat Devices The Department of Justice alleges that eBay illegally sold 343,011 emissions defeat devices, and could face billions in penalties. BY CHRIS ROSALESPUBLISHED SEP 30, 2023 2:30 PM EDT NEWS YouTube/Dippin' Diesel SHARE CHRIS ROSALES View Chris Rosales's Articles chrishasacamera chrishasacamera The Department of Justice (DOJ) sued eBay for allowing the sale of 343,011 aftermarket emissions defeat devices and other products that violate the Clean Air Act. The online sales giant faces billions in fines, including up to $5,580 for each Clean Air Act violation, which could involve the sale of ECU tuners, aftermarket exhausts, or other parts that tamper with factory emissions systems. The DOJ’s suit comes after the Environmental Protection Agency (EPA) said that emissions defeat devices would no longer be a top priority for the agency. Despite that, the federal government has continued pursuing cases of systematic Clean Air Act violations, like tuning shops and online resellers. In August, the DOJ hit a diesel tuner with a $1 million fine for selling and installing unapproved ECU tuners that tamper with onboard emissions systems. Last year, eBay banned the sale of aftermarket parts that tamper with emissions, but has apparently not kept up with moderation. Aftermarket tuning parts that were listed on eBay. Ebay screenshot According to Reuters, the DOJ also alleges that eBay sold 23,000 unregistered or restricted-use pesticides, which violated a 2020 U.S. EPA stop-sale order, as well as distributing 5,614 paint and coating removal products that contain methylene chloride. The chemical is linked with lethal brain and liver cancer, as well as non-Hodgkin lymphoma. The complaint says that “eBay has the power, the authority, and the resources to stop the sale of illegal, harmful products on its website.” The complaint continues, “It has chosen not to; instead, it has chosen to engage in these illegal transactions.” EBay responded to the allegations, calling the lawsuit “entirely unprecedented.” The e-commerce giant says it will defend itself vigorously, and says that it maintains “a safe and trusted marketplace.” Finally, eBay says that it is removing and blocking “more than 99.9%” of the listings cited by the DOJ. The time when you could get a cheap eBay downpipe, intake, or many aftermarket parts may be gone. Despite words to the contrary, the federal government is not lifting off of the proverbial gas pedal. It’s still going full tilt against emissions violators. Got a tip? Email tips@thedrive.com AFTERMARKET CAR TECH Sign Up For Our Newsletters The chronicle of car culture, delivered to your inbox. SIGN UP RV Rentals Privacy Policy Terms & Conditions Contact Us The Drive Team Sitemap © 2023 Recurrent Ventures. All Rights Reserved. Articles may contain affiliate links which enable us to share in the revenue of any purchases made.",
    "commentLink": "https://news.ycombinator.com/item?id=37834946",
    "commentBody": "US sues eBay for allowing sale of emissions defeat devicesHacker NewspastloginUS sues eBay for allowing sale of emissions defeat devices (thedrive.com) 223 points by PaulHoule 16 hours ago| hidepastfavorite351 comments Scoundreller 15 hours agoMy experience is that eBay screws up their filters and wayyy overfilters.The Canadian government has some secret spat with Xiaomi cellphones and got eBay, newegg and Amazon to stop their sale. They weren’t approved for sale in Canada, but I suspect some other reasoning for targeting a Chinese manufacturer in particular and not all the other unapproved products.However, eBay just blocked anything with “xiaomi” in the title, so their scooters, vacuums, android TV hubs and everything were no longer available for sale.I had an old Mi Box (TV hub) listed for sale that became shadow-banned: after it didn’t sell for a while (duh, nobody could find it), I went to lower the price and it gave some vague error. I called eBay and they said it was due to gov regulation but had no further info, so I just took “xiaomi” out of the title and voila! reply Aurornis 12 hours agoparent> My experience is that eBay screws up their filters and wayyy overfilters.This is what happens when the penalties are high and users are motivated to evade keyword filtering.I doubt it&#x27;s what eBay wants to do, but if the law is unclear about how filtering must be done then companies will over-filter to stay safe. reply Guvante 9 hours agorootparentI think calling it overzealous government orders is giving eBay too much credit.If you ask \"what is the quickest way to filter X\" someone is going to do a keyword search.You could then actually put forth the effort to properly filter but generally once the fire is out no one cares anymore.Unless those other products added up to enough revenue to impact numbers of course. reply kps 14 hours agoparentprevMaybe they&#x27;re under a different name, but I don&#x27;t see any Xiaomi phones on the REL, so I&#x27;d guess the ‘secret spat’ is lack of RF certification. reply Scoundreller 14 hours agorootparentI’m still trying to get the gov to cough up the documents, but funny how the gov didn’t tell our telecoms to ban them from roaming on our cellular networks. Nor identify domestic users that imported them and put on a transition plan to get them off our networks.If they’re actually so harmful to public health.Though I wonder if our telecoms were behind this; they prefer you to buy your device from them. reply kps 13 hours agorootparentI don&#x27;t think “harmful to public health” is a consideration. Pretty much every country with a functioning government requires RF certification for electronics, but especially radio transmitters. It&#x27;s never going to be worth hunting down individual buyers or hassling tourists, but selling is another thing. reply Scoundreller 13 hours agorootparentCell phones do have emitted energy regulations based on estimated tissue absorption. And they vary country to country (some countries even further regulate them near schools & other locations).If they were causing harmful interference, I’d like to think they would be harder-headed about it. reply kps 13 hours agorootparentNo one is saying they cause harmful interference. It&#x27;s up to the vendor to get certified that they don&#x27;t. You can&#x27;t sell an uncertified radio transmitter. It&#x27;s not a conspiracy. reply Scoundreller 9 hours agorootparent> You can&#x27;t sell an uncertified radio transmitter. It&#x27;s not a conspiracy.When they pick and choose who they target for RF and somehow end up focusing on a Chinese cellphone manufacturer, it suggests something is going on beyond RF reasons. reply mirashii 7 hours agorootparentNo, it signals that that manufacturer didn’t follow the established process for certification.What evidence is there that anyone is picking and choosing? reply Scoundreller 6 hours agorootparentThe sheer amount of other cellular devices I can find on ebay in 2 seconds that clearly don&#x27;t have any certification.https:&#x2F;&#x2F;www.ebay.ca&#x2F;sch&#x2F;i.html?_from=R40&_nkw=gsm+gps+tracke...https:&#x2F;&#x2F;www.ebay.ca&#x2F;sch&#x2F;i.html?&_nkw=cell+phone+boosterI&#x27;m sure I can find wifi access points and other cell phones that aren&#x27;t certified, but I&#x27;m too lazy to check against what vendors&#x27; models bothered getting Canadian approval or not. reply kps 7 hours agorootparentprevOnce again, it&#x27;s up to the vendor. You submit your test reports showing compliance with the standards, and you get your certification. The only ‘pick and choose’ here would be by Xiaomi who apparently don&#x27;t think the Canadian market is large enough to bother with. reply arcticbull 13 hours agorootparentprevAll that RF radiation from any of these devices can do is mildly warm you up. When you get to kW it could cook you I guess.It&#x27;s not about health effects, there are none.It&#x27;s about the risk of interference. reply Scoundreller 13 hours agorootparentI agree, but governments think otherwise (and inconsistently with their acceptable limits).https:&#x2F;&#x2F;www.who.int&#x2F;data&#x2F;gho&#x2F;data&#x2F;indicators&#x2F;indicator-detai...https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Specific_absorption_rateCan be a handy tool to selectively remove vendors from the market (which may be what occurred here).If there was a problem with interference, I really wished our regulator would require the telecoms to disable them from the networks to force migration to non-interfering devices. reply arcticbull 12 hours agorootparentI know they do have limits, but I don&#x27;t believe any government or private entity has linked absorption to any kind of health impact at levels anywhere close to the published SAR limits.These are likely just out of an abundance of caution just in case, not because they actually think harm will come. Frankly that&#x27;s just fine, I wish they&#x27;d do that more - especially with new chemicals.> If there was a problem with interference, I really wished our regulator would require the telecoms to disable them from the networks to force migration to non-interfering devices.Agreed. reply pas 4 minutes agorootparentRegulation happens at the margin of functionality. If something is very useful but \"bad\" (asbestos, lead, perfluorocarbons, radioactive stuff) it gets regulated out of places where there are substitutes. But of course not with any overarching consistency. (Unsurprisingly due to classic lobbying, typical problems of governance, etc.)Then some regulatory body ossifies into some role, and then we end up with some regulations.Some useful but not doing enough (lead, fly ash from powerplants, plus basically any emissions regulations via the EPA, huge political battleground, insane lobbying, etc), some that have benefits but are doing too much and in a very wrong way (environmental reviews and zoning regulations which make construction permitting process a non-deterministic nightmare with bonus added possibility for endless legal challenges, but no outright bans of course ... you can try to apply for whatever you want, and local democracy, council, judiciary will just happen to say no. how fair, right!? ... of course, makes no sense, since persistent&#x2F;rich builders end up getting approved eventually. democracy, fairness, yey!) And again, of course, public good projects, from railways&#x2F;subways to nuclear power plants, where there are no special interests pushing for (and plenty of against), that simply don&#x27;t get built (or carry an enormous premium of wasted time and \"reviews\"). JumpCrisscross 11 hours agorootparentprev> Cell phones do have emitted energy regulations based on estimated tissue absorptionThe real reason everyone does RF certification is to make sure the radio isn&#x27;t screaming in others&#x27; bands. replydmazzoni 6 hours agoparentpreveBay posts are littered with typos, though. How well does eBay do at filtering out Xaiomi, Xoiami or Xiaoim phones? reply dheera 11 hours agoparentprevDuring COVID I reported a bajillion eBay sellers for selling masks above MSRP, they did nothing. reply tbihl 8 hours agorootparentAnti-price-gouging laws are an incredibly bad idea. reply dheera 4 hours agorootparentI disagree, people hoarding life-saving equipment people can use during a pandemic just to make a quick buck should be punished and publicly shamed. reply niij 6 hours agorootparentprevDemand is the signal to create more supply. Anti price gouging disincentivises more production. reply jmopp 5 hours agorootparentCreating more supply is hard. Jacking up the price is easier. Sure, someone will see the opportunity and undercut your inflated price, but you will have made your money by then and there will be a lot of collateral damage in the interim. reply niij 5 hours agorootparentIn what way does a higher equilibrium price cause collateral damage? Can you explain what collateral damage you&#x27;re envisioning? reply messe 4 hours agorootparentI would have thought: a lack of access to masks during a pandemic due to their in-affordability?But who am I to question the wisdom of the free market… replywredue 14 hours agoparentprevThe Canadian government “secret spat” doesn’t seem so secret. Xiaomi stuff is laced with various levels of spyware.There’s also tech relations issues due to huawei shenanigans and back doors. reply Scoundreller 14 hours agorootparentSo are iPhones and US Android phones, but I guess when it’s a “partner” country’s corporations doing the spying, that makes it okay. reply december456 11 hours agorootparentWell yes, partner country&#x27;s corporations intelligence, your intelligence. reply Eisenstein 13 hours agorootparentprev> I guess when it’s a “partner” country’s corporations doing the spying, that makes it okay.According to the government? Yes, of course. reply throwaway2990 10 hours agorootparentprevNot even slightly similar. reply Neil44 14 hours agorootparentprev*citation needed reply porkbeer 13 hours agorootparenthttps:&#x2F;&#x2F;www.knowyourmobile.com&#x2F;phones&#x2F;is-your-xiaomi-phone-s... was the first ddg result.Or just google it. Pretty common knowledge that the too cheap to be true chinese phones, amazon fires, etc all trade on your personal data to subsidize the device, either overtly with amazon, or less openly in the case of xiaomi, huwei, and dozens of smaller brands of phone, television, and other connected devices. reply sandworm101 16 hours agoprevMy understanding is that such parts are not by themselves illegal. The crime is actually using them on the road. My understanding was that they were legal for track purposes.I would never want to say that people should be able to defeat emissions, but a part of me does not like the idea that every vehicle owner is beholden to the OEM for parts in perpetuity. The \"right to repair\" people should be all over this fight. Restrict cars to only their original emissions parts and that becomes yet another lever for manufacturers to pull when they want to sell new cars&#x2F;trucks&#x2F;tractors. I say let people use the parts they want to use, then properly inspect and test the resulting vehicles once they are on the road. reply hoofhearted 12 hours agoparentThe manufacturers got sued by the EPA for not actually enforcing the “for track use” only rule.A lot of the big name exhaust companies have stopped producing catless products; and others require all of your information to be on file if you’re buying a catless race downpipe.A lot of the install shops around here in Maryland that run a legit receipt paper trail of work type of business won’t even touch catless installs anymore. reply paulddraper 11 hours agorootparent> not actually enforcing the “for track use” only ruleHow does a manufacturer enforce the use of a device in that way? reply hoofhearted 8 hours agorootparentThey will require you to fill out a waiver saying you that you will use it for offroad use only, and all liability is on you and not them. They will also require you to upload your photo id in order to purchase it. reply mint2 10 hours agorootparentprevVerify their distributors are doing some sort of due diligence? Doesn’t seem that hard to make some sort of reasonable effort. reply 13of40 8 hours agorootparentIs the rule really about \"track use\" or is it just off-road or on-private-property use? How does the distributor know I&#x27;m not just rolling coal on my uncle&#x27;s farm? reply Scoundreller 6 hours agorootparentFrom what I can find, \"track use\" isn&#x27;t in the law, but rather a matter of enforcement discretion.\"The Clean Air Act does not contemplate removing emissions controls from an EPA-certified motor vehicle in order to convert it into a competition vehicle. As a matter of enforcement discretion, the EPA’s longstanding practice has been not to take enforcement action against vehicle owners for removing or defeating the emission controls of their EPA-certified motor vehicles, so long as they can show the vehicles are used solely for competition events and no longer driven on public roads.\"https:&#x2F;&#x2F;www.autoweek.com&#x2F;news&#x2F;industry-news&#x2F;a36482793&#x2F;sema-e... reply PH95VuimJjqBqy 12 hours agorootparentprevis catless a portmanteau of catalytic converter and less? meaning, the lack of a catalytic converter? reply hoofhearted 12 hours agorootparentYes. Remove the cat for either a straight pipe, or resonator.HKS silently discontinued their “catless” downpipes in the U.S. around 2021 for mentioned reasons above. reply jojobas 10 hours agorootparentIs is still legal to saw off you cat and weld a length of pipe in? I guess it still is if you can drive 60&#x27;s cars, so what&#x27;s the point? reply hoofhearted 8 hours agorootparentNo lol.. This is the EPA&#x27;s whole argument.You can&#x27;t modify the factory emissions output to go below the required output.People like the Diesel Brothers from the Discovery Channel settled for like $3 million USD with the EPA for selling and promoting emissions defeat devices like tunes and straight pipes. reply tommoor 10 hours agorootparentprevLegal in what respect? It wouldn&#x27;t be legal to drive it reply jojobas 9 hours agorootparentSo it&#x27;s only legal to drive a catless car if it was manufactured that way 50 years ago? reply bombcar 9 hours agorootparentCorrect. Federal prohibits tampering with or disabling lavatory smoke detectors I mean emissions equipment.The loophole is that it doesn’t apply if you never take it off private property. reply talldatethrow 10 hours agorootparentprevIt has never been legal in the US to remove any emissions equipment on any vehicle that is to be driven on the street. Even if it is outside of the smog check window, it still can not be removed or tampered with. reply hallway_monitor 12 hours agorootparentprevYes reply sidewndr46 14 hours agoparentprevThis is incorrect. Federal law prohibits the removal of emissions devices on a vehicle, even if not used on public roads. reply TulliusCicero 14 hours agorootparentWhich makes sense. It&#x27;s not like pollution stays confined within the track area. reply oceanplexian 14 hours agorootparentPollution only applies to the little people.If you’re a wealthy politician with a private jet you can pollute as much as 10,000 modified diesel trucks and the EPA might even give you an award or something. reply ceejayoz 13 hours agorootparent> pollute as much as 10,000 modified diesel trucksThis is insanely hyperbolic.A Cessna Citation I carries 564 gallons of fuel, with which it can travel ~1,500 miles - 2.7mpg.A Ford F-250 carries 48 gallons of fuel, with which it can travel ~700 miles - 14mpg.It&#x27;s not even 10x. reply chrisBob 13 hours agorootparentMaybe in terms of CO2, but turbines also don&#x27;t burn as completely as most piston engine cars. 10k is probably an exaggeration , but the pollution is much worse than the fuel economy indicates.Similarly, my chain saw probably pollutes nearly as much as my new Honda truck even though it only gets a few gallons of gas per year. reply SAI_Peregrinus 12 hours agorootparentprevA Gulfstream G700 carries 22,407kg of fuel (about 6,100 gallons at 800kg&#x2F;m^3 ) and a max range of 14,353km (8,918.5mi), for 1.45mpg. Closer to 10x, by a bit. Using the 12,316km high-speed cruise range manages to bring it down to 1.24mpg, just under 10x worse. reply paulddraper 11 hours agorootparentprevPer mileWhat if the little person and the wealthy person travel a different # of miles? reply tmtvl 10 hours agorootparentThen the person who travels more miles is better off doing it with a private jet rather than a truck? reply profile53 13 hours agorootparentprevYes but an F250 does not use leaded gas like many planes do, has much more complete and efficient combustion, and likely produces far less particulate emissions. reply ceejayoz 12 hours agorootparentPrivate jets don&#x27;t use leaded gas. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jet_fuelNo amount of cherry picking is gonna get you to the 10,000x number upthread. reply sam345 12 hours agorootparentJust curious, where in the wikipedia article does it say Private jets don&#x27;t use leaded gas ?Just searching the article for \"lead\" only gets this quote: \"The possibility of environmental legislation banning the use of leaded avgas (fuel in spark-ignited internal combustion engine, which usually contains tetraethyllead (TEL), a toxic substance added to prevent engine knocking), and the lack of a replacement fuel with similar performance, has left aircraft designers and pilot&#x27;s organizations searching for alternative engines for use in small aircraft.\"Also a reference to \"Planemakers challenged to find unleaded fuel option - The Wichita Eagle\" a 2009 article which has a dead link.Curious to me why you would cite an article that says the opposite of your assertion and then repeat the definitive statement. I must be missing something. reply ska 11 hours agorootparentJets don&#x27;t use avgas (which is still available with lead), they use jet fuel.The planes you are thinking of are mostly used for hobby flying or some short haul commercial flights, e.g. float planes. These have piston engines, more like your car ICE. reply ceejayoz 12 hours agorootparentprevAvgas is not used in jets. It’s for the little four seat propeller Cessnas you see tooling around. reply paulddraper 11 hours agorootparentprev> No amount of cherry picking is gonna get you to the 10,000x numberI cherry-pick \"5 miles in a F-150\" vs \"5000 miles in a Citation.\"Cherry-pick complete. reply fallingknife 10 hours agorootparentAnd I nit pick your cherry pick on the grounds that 1000x miles * 5.2x efficiency ratio only gets you to 5200x, far short of 10K reply paulddraper 10 hours agorootparentAn F-150 has better gas mileage than a F-250. reply jojobas 10 hours agorootparentprevPlanes emit their crap at a higher altitude which somehow makes a greater impact or so I hear. reply Blackcatmaxy 12 hours agorootparentprevPrivate planes absolutely still use leaded gas https:&#x2F;&#x2F;www.nbcnews.com&#x2F;business&#x2F;business-news&#x2F;leaded-gas-wa... https:&#x2F;&#x2F;www.faa.gov&#x2F;newsroom&#x2F;leaded-aviation-fuel-and-enviro... reply vdqtp3 12 hours agorootparentIt&#x27;s funny watching you all use different words and think they mean the same thing. Turbine powered aircraft, whether jet, fan or prop (a Citation is a turbofan powered aircraft) use Jet-A which is basically diesel (OK, OK, it&#x27;s kerosene but they aren&#x27;t that different). Piston planes (of which there are plenty) mostly use 100LL - high octane, leaded gasoline. Now that we&#x27;re done with the pedantry, please continue with your argument. reply ceejayoz 12 hours agorootparentprevYes, if you change entire words in my post you can make it say anything you like. reply Blackcatmaxy 12 hours agorootparentParent talked about private planes which you changed to talk about specifically private jets which doesn&#x27;t disprove \"many planes\" using leaded gas. reply ceejayoz 12 hours agorootparentDirect quote from the parent: “ If you’re a wealthy politician with a private jet” reply missedthecue 10 hours agorootparentprevJets don&#x27;t reply rgmerk 9 hours agorootparentprevAnd furthermore, it matters where non-CO2 pollution is released. Particulates released from a jet at cruising altitude, particularly over an ocean, are less harmful to human health than a truck belching pollution next to a kindergarten.\\I&#x27;d also add that two wrongs don&#x27;t make a right, and while not strict enough there are emissions regulations for aviation. reply babypuncher 11 hours agorootparentprevBut nobody drives their Ford F-250 10,000 miles a week. It&#x27;s not about the energy efficiency of someone&#x27;s mode of transport, it&#x27;s about the total emissions from their transit habits. You know, the actually important metric when measuring an individual&#x27;s impact on the climate. reply protonbob 10 hours agorootparentprevIt’s not just about mpg. It’s about pollutants emitted. reply ceejayoz 9 hours agorootparentWhich pollutant does a private jet emit 10,000 times as much as a diesel truck? reply datavirtue 11 hours agorootparentprevLeer jets, they so thirsty. reply kortilla 11 hours agorootparentprevGood thing Cessna citations are only used to travel as far as an F-250 travels &#x2F;s reply westmeal 11 hours agorootparentprevThey&#x27;re not in a fuckin Cessna. reply paulddraper 11 hours agorootparentA Cessna Citation, not a Cessna 172. reply vwcx 11 hours agorootparentprevOf course they are. reply TulliusCicero 9 hours agorootparentprevI think you&#x27;re probably rather exaggerating the difference here, but nonetheless, sure, let&#x27;s regulate or the tax the shit out of private jets too, I&#x27;m okay with that. reply pixl97 10 hours agorootparentprevEh, you&#x27;ll look for anything to complain about.If you were one of those rich people you&#x27;d be complaining about the certification of airplane parts causing them to be so expensive.And if you&#x27;re in the US, it&#x27;s likely you could take dozens of flights per year if you wanted to, if you fit in the techy HN pay grades. reply queuebert 13 hours agorootparentprevDedicated race cars don&#x27;t have emissions controls, though. For some reason, only race cars that drive on both roads and tracks are regulated. reply SoftTalker 12 hours agorootparentDedicated race cars are too few in number to really make a difference, so they don&#x27;t worry about them. reply TulliusCicero 9 hours agorootparentprevMaybe that might be worth regulating, but there aren&#x27;t many actual race cars around. reply enragedcacti 14 hours agoparentprevMy understanding is that such parts are not by themselves illegal.The relevant text regulating the sale and manufacture merely allows the EPA to exempt products \"that you intend to be used solely for competition, where we determine that such engines&#x2F;equipment are unlikely to be used contrary to your intent.\"> but a part of me does not like the idea that every vehicle owner is beholden to the OEM for parts in perpetuity.I don&#x27;t think the regulation requires OEM parts, Dorman and MagnaFlow sell bolt-on Catalytic converters without any trouble from the EPA. I think there is a real problem when it comes to more complex parts like ECUs but right to repair laws can help solve that by requiring documentation and restricting arbitrary locks of electronic components.https:&#x2F;&#x2F;www.ecfr.gov&#x2F;current&#x2F;title-40&#x2F;chapter-I&#x2F;subchapter-U... reply Scoundreller 13 hours agorootparentI was thinking more about O2 sensors. At least a lot of Toyotas don’t like the aftermarket ones for some reason. No idea if they’re truly out of spec or what.But there’s a risk with living with a check engine light. Some vehicles disable stuff like traction control in that state, and you won’t know when a new unique code has hit that you should deal with for other reasons.(The disabling of traction control is insane to me here: https:&#x2F;&#x2F;mechanics.stackexchange.com&#x2F;questions&#x2F;25383&#x2F;why-does... ). A catalytic converter problem is zero reason to do this. reply I_Am_Nous 13 hours agorootparentMy old Suzuki SX4 (RIP) disabled cruise control when it threw a code, but so far as I could tell that was the only thing it did. Traction control still worked, automatic&#x2F;manual all wheel drive still worked...it was also a secondary catalytic converter alarm, so I just kept a code reader with me and reset it whenever it would come on.It would always come on after I got off the interstate and coasted up the on-ramp, so I am not sure if it came on after getting hot enough on the interstate or if it was just the coasting that caused it. reply rumblerock 12 hours agoparentprevI for one love that BMW built the catalytic converters into the exhaust manifolds on my car, meaning that the only CARB-viable route outside of gambling on used units is a $10k replacement. reply achates 7 hours agorootparentYou knew what you were getting into when you bought a BMW. reply superq 10 hours agorootparentprevYou forgot the &#x2F;s. reply fsckboy 15 hours agoparentprev> My understanding is that such parts are not by themselves illegal. The crime is actually using them on the road. My understanding was that they were legal for track purposes.ok, it&#x27;s just a single datapoint, but motocross racetracks in New England, already built in the woods away from civilization, started requiring mufflers about 50 years ago, early 1970&#x27;s reply MarCylinder 11 hours agorootparentMufflers do not impact emissions.In the case of gasoline cars, there are mechanisms for controlling intake air, recirculating exhaust gas, and filtering emissions directly out of the exhaust gases. All of these impact carbon emissions.These defeat devices allow drivers to modify some or all of these attributes in favor of gaining engine performance. reply datavirtue 11 hours agorootparentMuffler delete is a known cause of misfires. It manifests as a popping sound surrounded by the other shitty sounds produced by such a modified exhaust. We have all heard it.A misfire is raw fuel being expelled. reply mcronce 7 hours agorootparentA misfire is a failure to combust the air&#x2F;fuel charge in the combustion chamber during the expansion stroke, and they aren&#x27;t typically caused by muffler deletes.They can lead to the popping sound you&#x27;re describing, but they don&#x27;t always, and they&#x27;re far from the only cause, and muffler deletes aren&#x27;t necessary for those either. Unburned fuel hitting oxygen in the tip of a hot exhaust and immediately burning is the direct source of the noise, and causes for that are numerous. reply sandworm101 9 hours agorootparentprevIn my kneck of the woods we call that backfire. A misfire is when something goes wrong in the cylinder such as the spark failing. Misfire can cause backfire, but not all backfire is traced to misfire. A very rich mixture can backfire without misfire. reply Rumudiez 8 hours agorootparentA backfire is when the exhaust goes back out the air intake. An explosion in the exhaust system is called an afterfire, and is far more common. Most people just say backfire for both, unaware of the other term reply omginternets 5 hours agorootparentprevThat’s a backfire, not a misfire. reply nullc 9 hours agorootparentprevDepends on the rest of the car, a turbocharged engine is going to have plenty of backpressure even with a straight pipe after it. reply oceanplexian 14 hours agorootparentprevWe’re not taking about mufflers. For example in California “Emissions defeat system” means you simply used an off-brand air filter that wasn’t blessed by the government.The parts may even be more efficient and less polluting, the government bureaucracy everyone here is worshipping doesn’t give a rats behind. reply ryandrake 14 hours agorootparentExactly. Years back, I moved to California with an 80&#x27;s sports car that was admittedly quite hotrodded, but I made very sure that the hotrodding did not worsen its emissions because 1. I wanted to keep driving the car in CA and 2. I don&#x27;t want to pollute. First SMOG test in CA, the car blew clean as a whistle. It met emissions standards 40 years harsher than it ever had to meet.Except I had all sorts of aftermarket parts in there from years of repairs, none of them with the all-important California CARB seal of approval on them, so even though it blew clean, it still \"failed emissions\" and I had to sell it to someone out of state. I&#x27;m still bitter about it. reply datavirtue 11 hours agorootparentIn Ohio in the early 2000s all us kids were removing cats and passing the sniffer test easily. There was a visual test so we would normally just hollow out the existing cats. A lot of times cars with missing cats would pass the visual as well. reply CobaltFire 13 hours agorootparentprevThis is disingenuous; you have to do a lot more than change an air filter. You would have to change the actual piping, etc. with one that the seller declined to spend the money on certifying as compliant.Further, and most people don&#x27;t know this is a thing, you CAN tune your car as much as you want and have it certified with all of those parts as a one off for about $5,000 and a six-ish month wait. This is how you can legally import overseas vehicles and get a California title and registration, but it&#x27;s $5,000 per test, and it usually takes more than one if you aren&#x27;t super prepared. After the certification the entire engine (including all modifications) are regarded as that being the SMOG compliant form.One other major loophole is kit cars; SB100 allows for non-SMOG vehicles to be built as a kit. Not everyone&#x27;s cup of tea, but you can build things like the Superlite GT-R [0] or SL-C [1] and never SMOG it.0: http:&#x2F;&#x2F;www.superlitecars.com&#x2F;gtr&#x2F;1: http:&#x2F;&#x2F;www.superlitecars.com&#x2F;slc reply vdqtp3 12 hours agorootparent> you have to do a lot more than change an air filterYou are barely correct. A filter alone is insufficient to be an issue, but a cold air intake will mean you don&#x27;t pass smog in CA. reply mcronce 7 hours agorootparentThat would be changing the actual piping, which they mentioned. reply Rebelgecko 9 hours agorootparentprevWhich air filter? The engine air filter? I&#x27;ve never had a smog check that involves looking at that one. And AFAICT there&#x27;s no RFID or anything in it, so it&#x27;s not something they can check remotely reply valianteffort 15 hours agoparentprevNot everyone lives in California so while OEM&#x27;s might produce all cars to meet California standards, it is completely legal in states with more relaxed laws to make such modifications.At the least I think the government could argue about selling in jurisdictions where it is illegal. reply dieselgate 14 hours agorootparentTo tap into this thread - this is quite real. One can just remove a catalytic converter (or have it stolen) and they&#x27;d then be \"defeating emissions\" so this lawsuit seems a bit zealous to me.Recently saw a post on a diesel truck forum about a CA resident who had legal&#x2F;registration issues because of an engine swap (post emissions engine into a pre-emissions vehicle) [1]Am under the impression this particular vehicle would be fine in WA state by comparison.It&#x27;s frustrating to see consumer vehicles being so strongly targeted by regulatory shenanigans compared to \"private jets\" and the like[1] https:&#x2F;&#x2F;www.oilburners.net&#x2F;threads&#x2F;california-diesel-convers... reply adgjlsfhk1 4 hours agorootparentfor the non c02 pollutants it makes a ton of sense to go after consumer vehicles more than private jets. noise, N0x, and other pollutants have significant negative effects for anyone nearby. I care a lot more about the vehicles 20 ft from me than the ones 30k feet above me reply imp0cat 14 hours agorootparentprevI think the reason is (any diesel mechanics around please correct me if I&#x27;m wrong) that modern engines stripped of their emissions equipment will pollute a lot more than old engines. reply smileysteve 11 hours agorootparentModern (fuel injected with wideband) engines without emissions devices will produce much fewer hydrocarbon emissions but more Nitrous oxides reply dieselgate 14 hours agorootparentprevI understand the reasoning and am all for mitigating pollution. It may be frustrating for consumers when these systems have potential to decrease economy, performance, and reliability while increasing cost and maintenance. reply imp0cat 20 minutes agorootparentOh, absolutely. Emissions equipment in modern diesel engines is extremely complicated and expensive to repair&#x2F;replace. Multiple cats&#x2F;particulate filters that can get clogged.But, at least in Europe, diesel options in passenger cars are mostly gone, most SUVs nowadays are either gas powered (with a particulate filter, of course, so another thing that can go wrong) or some kind of gas&#x2F;electric hybrid. reply ospzfmbbzr 12 hours agorootparentprevIn the case of the DEF it absolutely reduces the lifespan of the engine. A truck engine that could last 500K miles lasts 200K max.Factor a second or third engine (whole new truck) and the energy and pollution to produce it and DEF looks not as great.Vehicles and engines that last 25 years are easy to build yet they are rare outside of commerical and military.The whole argument is disengenuous and proceeds from an unstated a priori desire by government to restrict movement of citizens and markets, rather than as a real requirement for a real benefit.It&#x27;s all so tiresome... reply pixl97 10 hours agorootparentCommercial fleet and military engines do not &#x27;last&#x27; 25 years. They have a consistent maintenance schedule to fix and replace common wear parts. You have to think of it much more like the Ship of Theseus and not the same vehicle.My last two vehicles lasted over 250k each, well, one is still going, I just don&#x27;t own it any more. I feel the engine on the Toyota thats still running may last forever. It&#x27;s replacing other parts outside the drive train that made it not worth it. Consumers are much more into toss and replace than full rebuild maintenance schedules. reply Scoundreller 6 hours agorootparentThat dead Corolla will probably get re-built in a developing country (after the cat gets harvested for its minerals of course). reply superq 10 hours agorootparentprevThat DEF is insanely toxic, too. I rented a diesel truck for moving and everyone got passing-out sick just moving things in and out of the back of it. reply Scoundreller 6 hours agorootparentUrea? It&#x27;s used on foodcrops and you pee out 20-35g&#x2F;day of it:https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC9161740&#x2F;It has a boiling point of 135C. Something else was (I gotta say it, likely) causing your sickness. reply omginternets 5 hours agorootparentThere’s a reason you pee it out. reply Eisenstein 8 hours agorootparentprev> The whole argument is disengenuous and proceeds from an unstated a priori desire by government to restrict movement of citizens and marketsWhat is their motivation for doing this? replygrecy 15 hours agoparentprev> My understanding is that such parts are not by themselves illegal. The crime is actually using them on the road. My understanding was that they were legal for track purposes.That certainly used to be the case, but it changed recently, and the EPA has been aggressively going after anyone who sells them for any purpose.If you&#x27;re in the US and sell things that can modify emissions stuff, they&#x27;re coming for you. Period. reply User23 14 hours agorootparentDoes this include chip tunes? reply LazyMans 14 hours agorootparentYes, a popular diesel tuner was the target a take down and fined big money for providing tunes that make vehicles non-compliant. https:&#x2F;&#x2F;www.thedrive.com&#x2F;news&#x2F;another-diesel-tuner-nailed-wi...Omitting wear and reliability concerns, some tunes gain their effect by not controlling for things like NoX and unburned hydrocarbons.The worst offenders? \"Coal rolling\". It would be one thing to do this to help spool a turbo or for some other performance reason, but it&#x27;s literally running the motor as rich as possible to dump unburned fuel out the tailpipe to make a visual effect. At the expense of everyone around you. reply omginternets 5 hours agorootparentThere are few things as obnoxious and antisocial as coal rolling (open headers, maybe) and I really wish the EPA would make an example out of them. reply grecy 13 hours agorootparentprevIt does, but it&#x27;s worth noting that a manufacturer of such devices can make it meet federal emissions regulations (if they want to spend the $$$$ on R&D), and they can sell devices that are 100% legal in all 50 states.Examples are Green Diesel Engineering (Who did a TON of dyno time and used to work for a major OEM on emissions stuff) and Banks (who are virtually an OEM at this point) reply datavirtue 11 hours agoparentprevI would fully suspect people to be fined for producing enhanced emissions reduction hardware and offering it for sale. The default emissions systems end up reducing the life of the engine, creating another problem for the environment, and there are opportunities for fixing those deficiencies. reply omginternets 5 hours agorootparent0W20 oil and EGR systems are going to go down in history as environmental disasters. reply WirelessGigabit 16 hours agoprevThis is wonderful.I wish they did the same about modifications done to increase the noise.I&#x27;ve got 2 people living nearby that both have straight-piped their car and I literally cover my ears when they pass by.My Watch literally tells me that a noise peak happened when they drive by. reply kjkjadksj 16 hours agoparentNot to mention good old fashioned overpowered unfocused led lighting. Its been a meme with the truck bros for years, but now these products have entered the general lighting supply. Now your landlord replaces the garage light with one thats brighter than the sun and shines into your bedroom. You will end up like Kramer in the chicken roaster episode of Seinfeld. reply extraduder_ire 13 hours agorootparentA friend of mine bought a hyundai i20, and the LED lights in it aren&#x27;t aimed low enough, either by manufacturing error or design, and even when dipped they shine at oncoming drivers. No idea if this is common, or if he can get them adjusted anywhere.Can&#x27;t tell you the amount of times I&#x27;ve seen people in the other lane flash their highbeams at him while I&#x27;ve been riding in the passenger seat, because they think he&#x27;s doing the same. reply brianwawok 13 hours agorootparentIt&#x27;s 100% adjustable by user. Very likely when replacing them they got bumped? The classic way is you go in a garage or against a wall and shine them, and measure to ensure the horizontal line is going down towards the ground (vs going up and blinding everyone). Not only is it nicer for oncoming traffic, they actually work a lot better... reply sokoloff 13 hours agorootparentI typical set the high beams on and aim those straight ahead.That gives max range for high beams and the low beam down and right (for right drive countries) is then also very close to optimal. reply brianwawok 13 hours agorootparentYah. If you are being flashed it&#x27;s likely at least 1 degree upward, which is not helping a driver see the road. reply RC_ITR 14 hours agorootparentprevSay what you will about over-engineering, but the Germans got it so so so right with their metal full black out shades. reply slaymaker1907 14 hours agorootparentI&#x27;d prefer to just have less outdoor light since blocking out all outdoor light also blocks out sunlight which can mess up your circadian rhythms. reply userabchn 14 hours agorootparentprevBut unfortunately it is the type of thing that, although obviously a superior solution, is unlikely to be adopted in the rest of the world. Where I live, as, I think, in most places, people would think you were very strange if you put those kinds of metal shutters on your windows, and I can even imagine neighbours being displeased with you as it makes it look like it is a bad neighbourhood. reply jrwoodruff 14 hours agorootparentprevI&#x27;m unfamiliar with this, but I&#x27;d love some well-done black out shades - got a link or a brand? reply progman32 13 hours agorootparentVery common in Italy as well - search Images or Youtube for \"finestra tapparelle\" for an idea. The sound of tapparelle is super ingrained in many european minds, i.e. https:&#x2F;&#x2F;italybeyondtheobvious.com&#x2F;taparelle&#x2F;They are also good for deflecting debris :) reply m4jor 9 hours agorootparentprevIn Germany they are called rolladens.The best thing ever! reply plagiarist 14 hours agorootparentprevIt is somehow every single vehicle on the road with these. People should lose their license for operating a vehicle with the blinding LEDs tilted up higher than highbeams would be. It is unsafe that approaching drivers cannot see past them as they drive by. reply kraquepype 14 hours agorootparentUgh yeah. It&#x27;s that time of year where it gets dark early and I get blinded on the way home from work by all the bro-dozer truck lights and intense LEDs. reply rad_gruchalski 10 hours agorootparentI get blinded by morons who have no idea that a normal H light bulb can be installed „the other way around”. So it points upwards.And teslas but only when they drive behind. For whatever reason tesla lights blind when seen in the mirror. reply rumblerock 12 hours agorootparentprevI&#x27;ve been looking into transparent windshield films to reduce glare for this reason. reply ChuckMcM 14 hours agoparentprevIf you seriously want to take action here, then do this;1) Look up your city ordinances on sound, there should be copies of this is the reference section of your city&#x27;s public library. Identify the ordinance and paragraph that the people nearby are violating. Record&#x2F;video one or more violations on your phone.2) Call the police, don&#x27;t \"complain about the noise\", tell them you want to \"swear out a complaint\" for violating ordinance x, paragraph y. They are obligated to take your complaint and forward it to the District Attorney[1].3) You will probably be called as a witness if the folks with the noisy cars choose a trial.[1] When a police officer hears a complaint, they can \"investigate\", and then \"cite\" the offender. It is up to the officer&#x27;s discretion. That citation is the sworn complaint from the officer of the offending behavior. When the police refuse to do their job, everyone has the right to \"step in\" to that role and swear out a complaint. The downside is that retaliating against an officer is &#x27;scary&#x27; but retaliating against a neighbor isn&#x27;t. This can result in a variety of negative externalities that may be criminal but hard to prove (car getting &#x27;keyed&#x27;, tires slashed, rocks thrown through windows, feces on your porch, Etc.)Because taking on the job of enforcement when the cops let you down incurs significant personal risk, I usually suggest people make an appointment with their representative on the city&#x27;s governing body, city manager, or district attorney and encourage them to actually do their job. Editorials in the local paper are good to, but projecting soft power as a voter in that way is less likely to blow back directly on you. I can also take longer and be less successful but sending emails to the DA with repeat infractions over and over can often convince them of the need for enforcement. reply notyourwork 14 hours agorootparent> 2) Call the police, don&#x27;t \"complain about the noise\", tell them you want to \"swear out a complaint\" for violating ordinance x, paragraph y. They are obligated to take your complaint and forward it to the District Attorney[1].Good luck getting them to pick up. At least where I live (Seattle), I don&#x27;t think its possible to get someone on the phone unless you call 911. Daytime phones automated system defers everything to online. reply RajT88 14 hours agorootparentCity council meetings are for this. They typically have a citizen comments section at the end of the meeting.Where I live, these citizen comments are beloved by wingnuts who hate their neighbors and are trying to convince the city to harass them. So - you know - try not to come off as one of those. reply ChuckMcM 9 hours agorootparentAgreed, per my comment the 1:1 meeting with your representative works well too. I don&#x27;t know how common it is but I have found my city council member always willing to sit down for 5 - 20 minutes to hear what I have to say. Want to emphasize again though about doing a bit of research at the library about just what ordinances are involved and how to identify them for law enforcement to use. This helps your rep appear to be smart and well informed when they go to the police chief or city manager to ask them to \"can you do something about this?\" kind of request.That saves waiting for your turn at the podium in the meeting and it avoids the wingnuts labeling you as the person \"out to get them.\" Both are good things IMHO. reply altec3 14 hours agorootparentprevI lived in Portland for years - and it&#x27;s the same. Even if you did get someone on the phone, I doubt anything would happen. There&#x27;s tons of cars without plates, mufflers, windshields, etc. driving around that the police don&#x27;t care about. reply brianwawok 13 hours agorootparentAnother upside to living in the flyover states I guess? I can get multiple people up to the mayor on the line. reply no_wizard 11 hours agorootparentI fear Oregon may become a flyover state. I was at a local meetup (though admittedly, all tech centric) in Portland and there was consistent joking about how Oregon is what you see when you&#x27;re traveling between Seattle and California or California and Seattle. reply Pseudocrat 14 hours agorootparentprevActually faster here to dial the non-emergency. I once called 911 because a 7 year old child approached me at a gas station asking for money. I was put on hold. When I called the non-emergency I got through right away. The cherry on top was when I received a callback from 911 dispatch over an hour later to check if I was okay. reply DiggyJohnson 9 hours agorootparentprevAs someone who has lived in a variety of south-east US cities, the idea that nobody picks up on the non-emergency police line is bananas to me. Is this really true? reply liquidpele 9 hours agorootparentprevFor good reason, have you SEEN how crazy most of the public that would be calling is? reply DiggyJohnson 9 hours agorootparentAre you unfamiliar with “non-emergency” lines? I’ve made considerably more (obviously still not a large number) of calls to the non-emergency number than 911. Most recently when my car was towed but the two company hadn’t updated their database quickly enough, for example.I thought that being able to connect to a live human operator &#x2F; police dispatcher outside of 911 was ubiquitous in America. reply pseg134 12 hours agorootparentprevI’m sure this has worked but I have gotten the same result by writing fix your exhaust on a piece of paper and tying it to a brick and throwing it through their back windshield. Some times two wrongs make a right. reply westmeal 11 hours agorootparentNot really you&#x27;re just an asshole. reply pseg134 10 hours agorootparentSure, but so is the guy with the loud car. What business of it is yours what two assholes do to each other? reply ChuckMcM 9 hours agorootparentAll about your risk tolerance right? Some places the driver will start shooting at you at that point. replywutwutwat 15 hours agoparentprevYour municipality likely has noise ordinances for anything above a certain decibel level. This can be handled by your local PD giving them tickets for their exhausts. I’ve seen it done before. If they keep getting tickets&#x2F;fines they will likely change their exhausts. You can probably call the city about it but you can also call the cops. Just remember what they say about snitches. reply genter 15 hours agorootparentDon&#x27;t waste your time. I&#x27;ve tried, my PD told me that I need to get used to the noise. reply grepfru_it 15 hours agorootparentLet me translate for you:Management (the courts) do not find it lucrative enough (political capital) to pursue weak cases (lax laws passed by the legislative branch).In order to fix you must bring awareness in your local community to the problem (lax laws) and petition change (raise political capital). The courts and byproxy, enforcement, will follow. The larger the city, the more difficult this becomes. reply genter 14 hours agorootparentNo, I believe the officer I talked to didn&#x27;t care and thought I was a pansy. reply bdcravens 14 hours agorootparentProbably a gearhead showing his personal bias. reply genter 14 hours agorootparentThe really frustrating thing is I love cars. I love driving, I love shifting though the gears, I love working on them, I&#x27;ve replaced headgaskets, rebuilt transmissions, and regeared differentials. I just hate how obnoxious noisy vehicles are. reply bdcravens 14 hours agorootparentFor sure - I love speed myself. (Ironic that my fastest car ever is also the quietest, but EVs are a different creature). I guess not everyone has the same ethos that our personal enjoyment doesn&#x27;t entitle us to encroach on others. reply lokar 14 hours agorootparentprevThis. More so for loud motorcycles, since cops tend to also own them. reply alexfoo 12 hours agorootparentIt&#x27;s just the odd love of the Harley or similar.I got pulled over for noise (in the UK) as my ZX6-R had a hole in the downpipes and it was obviously a bit too noisy. I was trying my best to keep the noise down and (genuinely, provably) on my way to drop the bike off to have it repaired.The Cop that pulled me over was happy to chat about it when I explained that it was on its way to get fixed he let me off.Then a Harley that was twice as loud went past and the cop says \"Cor, doesn&#x27;t that sound lovely!\" I asked him if he was going to pull it over for being louder than mine and he suddenly got very grumpy with me again. I didn&#x27;t push any further as I didn&#x27;t want to end up with a fine.What&#x27;s the old line?\"Harley&#x27;s are probably the most efficient machine to turn gasoline into noise without that pesky by-product of horsepower.\" reply wutwutwat 13 hours agorootparentprevThis logic is absurd. Cops are hands off with bikers not because they own bikes. Extrapolating your logic one could say:Cops don’t ticket cars because they also own a carCops don’t stop people with guns because they also own gunsCops don’t go after bikers because they factor in if the offense is worth the risk, to both themselves and the public. There’s also a whole thing with outlaw motorcycle gangs that plays into it. Most non entry level bikes can out maneuver and outrun cop cars with ease, yes, even those slow looking Harley’s. Starting a chase over loud pipes would put a lot of innocent people at risk and for what? It isn’t worth it and it’s safe to say if the biker is running modified pipes they are not on an entry level bike.Story: a buddy of mine had a gsxr 1000 and would purposely screw with cops on the freeway and as soon as they’d light up he’d be gone and doing 200mph within a few seconds, fitting between cars easily. Within 30-60 seconds a bike can be damn near a mile away from a cop car. Radioing ahead won’t help if you don’t know where they went and they were never running a plate to run… most cops know it’s pointless to chase and often immediately call it off. Public safety is more important. Those types of bikers have a way of making the issue go away all on their own eventually. You can’t temp fate that much and not eventually meet yours. reply alexfoo 12 hours agorootparent> Story: a buddy of mine had a gsxr 1000 and would purposely screw with cops on the freeway ... You can’t temp fate that much and not eventually meet yours.An idiot friend used to do similar (in the UK), then one day he did it and the Police turned up at his house 1 minute after he had stopped and put his bike away. He&#x27;d gotten away with it loads of times but this time he did it with a Police helicopter in the air less than a mile away and easily able to track a bike all the way back to his house. A top sports bike may be able to do 200mph but not everywhere, and helicopters are fast, can see for miles with very good tracking cameras, and can fly in straight lines.They then used all of the footage from the Police cars of previous incidents to tie them all to him (similar clothing, similar bespoke exhaust, etc). I think he got 8 years in prison with all of the totting up that went on (and previous offences taken into account). reply pests 8 hours agorootparentThen you have the video of the challenger hellcat outrunning a helicopter...https:&#x2F;&#x2F;youtu.be&#x2F;W9eLpH513po?si=e2vfe_ntQSkduewW reply defrost 8 hours agorootparentInteresting link, that said:A video that opens from a news helicopter and a reporter stating \"we&#x27;ve been tracking him for ten minutes\" that continues to track for another 3 or four minutes.The police helicopter pulls off towards the end for lack of fuel .. and the news helicopter says that they are having trouble keeping up (despite having done so for at least 13 minutes).I&#x27;m going to guess this is more a resources ran out thing than a car outran helicopter thing.( FWiW \"The speed of a police helicopter can vary depending on the make and model. However, most police helicopters have a top speed of around 130-145 miles per hour (209-233 kilometers per hour).\"The car was peaking at 110 mph, the helicopters may have been hitting headwinds, even so the statement was explicit that police helicopter had low fuel. )The police rig was probably easily keeping up - but was low on fuel, the news helicopter likely had less grunt and|or started to exceed budget burn raterun low on fuel itself.The follow through on that wasn&#x27;t given but there&#x27;s a fair chance the car was tracked down - being distinctive and the police capturing the plates. reply pests 7 hours agorootparentI agree with your analysis but they do state they are using a powerful lens to even get that footage and then further on....3:20ish \"We&#x27;re having trouble keeping up with this guy\"3:30 \"We are currently going 120MPG and he is losing us. We&#x27;re at the maximum speed right now.\" [news anchor after speaking to pilot]3:50 \"This is the first time the vehicle has outrun the chopper\" [studio host?]So I think you are correct in the 130-145 MPH max speed and them just hitting 120 due to wind or hardware. reply I_Am_Nous 12 hours agorootparentprev>doing 200mph within a few secondsIt&#x27;s still kind of amazing to me that race-tuned liter bikes are even legal in the US. Even a starter 250 likely has a faster 0-60 than most cars on the road. I believe you need a little extra power to spare when riding, so that you can suddenly speed up if you need to, but not so much extra power that a quarter-turn on the throttle gets you from 70 to 100 MPH in 1 second.I&#x27;ve heard big bikes described as \"ride to survive\" because even a small mistake can be fatal when you are dealing with that much performance. Full disclosure, I ride a big, fat Kawasaki touring bike (1993 ZG1200, right about 760 pounds wet) and while the engine is big, it&#x27;s geared to a max of 113 MPH. It&#x27;s big enough I have to respect it when riding, but powerful enough that if I need to I can spur it a bit :) reply wutwutwat 8 hours agorootparentI’ve had a Honda cbr and I’ve been into modified tuner cars my whole life. I have much more experience going fast in cars than on bikes. Everything becomes much bigger at higher speeds. You learn pretty quickly that when you’re doing 150 you don’t make jerky movements with the steering wheel. Tiny, tiny, fluid movements are amplified to become more. You need to anticipate understeer and be able to feel when the ass end is getting squirrelly or is breaking loose. You learn to use the engine compression as a brake assist when trying to slow that mass by 80mph in a 100 foot span. I’m guilty of doing obnoxious and deadly things on the highway in my various drift cars over the years and that’s more than enough for me. I had my bike up to 120 when I owned it and decides squiding out wasn’t something I wanted to risk. The shit I do in cars is more than enough to get the devil looking my way.Interestingly enough I passed a cop on the highway doing 150 once. He was merging on from the on ramp. I stayed in it thinking I was fucked anyway but to my surprise he took the very next exit and got right back off the highway. There’s no way he didn’t notice me blow past him. The only thing I can think of is he got a priority call or he saw my speed and said fuck this my shift is about over and noped on out of running me down. reply sib 12 hours agorootparentprevGrappling hook through the front wheel fixes that right away... reply wutwutwat 12 hours agorootparentSo we should kill people for loud exhausts? What about the innocent bystander the bike turns into jelly when it is catapulted into the air and lands on them? That energy has to go somewhere… reply pixl97 10 hours agorootparentYa know, part of the reason cops should enforce the laws on the books is to keep people from going all Batman when they get tired of lawlessness. reply wutwutwat 8 hours agorootparentYes because Americans all over are going vigilante and taking justice into their own hands. What world are you living in? The America I live in everyone is too chicken shit to handle things for themselves and when I get to that point I’m not thinking about grappling hooks and going after people with loud exhausts… replydumbfounder 14 hours agorootparentprevI am in favor of louder pipes for motorcycles for safety reasons. But there is such a thing as too loud. reply rrix2 14 hours agorootparentsafer for whom? My windows only rattle after the motorcycle has driven past, they don&#x27;t project the noise ahead of you nearly as much as you think they do. reply BobaFloutist 9 hours agorootparentprevWhich is the same reason every biker with loud pipes wears a full high-vis outfit.Or maybe they&#x27;re just assholes. reply rando_dfad 14 hours agorootparentprevnot sure the safety argument has factual support; the sound would need to project forward from the motorcycle, wouldn&#x27;t it? reply fsckboy 13 hours agorootparenthmmm, if I have to hear the noise, I would like a requirement that it blare forward! (so I can step into the crosswalk) reply genter 14 hours agorootparentprevSo should every pedestrian have a blazing siren on their head? reply charcircuit 12 hours agorootparentPedestrians should not be on the road. reply freeone3000 10 hours agorootparentNo, cars should not be on the road. reply charcircuit 10 hours agorootparentRoads were built for cars. reply rrix2 6 hours agorootparentThat&#x27;s not actually the case in many places in the US, they were forcibly taken for cars in the same way electric streetcar systems were taken from us:> Before the American city could be physically reconstructed to accommodate automobiles, its streets had to be socially reconstructed as places where cars belong [..] Until then, streets were regarded as public spaces, where practices that endangered or obstructed others (including pedestrians) were disreputable. Motorists’ claim to street space was therefore fragile, subject to restrictions that threatened to negate the advantages of car ownership.https:&#x2F;&#x2F;mitpress.mit.edu&#x2F;9780262516129&#x2F;fighting-traffic&#x2F; reply charcircuit 6 hours agorootparentComing up with a fact that roads existed in the ancient times is not a gotcha. Most of the roads today did not exist back then. 99% of the roads I have driven on did not exist before cars. reply genter 10 hours agorootparentprevCrosswalks reply charcircuit 10 hours agorootparentCrossing the road is different from what is being discussed which is riding along side cars. reply mikestew 13 hours agorootparentprevIf noise made your bike safer, you’d get an insurance discount. People say such things so that they don’t come off as sociopathic, not because it is true. Don’t parrot them. reply pseg134 12 hours agorootparentprevTo be fair to that cop I think you were too. Did you try solving the problem for yourself first? reply genter 11 hours agorootparentBeing woken up 1:00 in the morning multiple times a week by the walls of my house shaking from my neighbors vehicles makes me a pansy?But I did talk to them. Turns out he lives with his mom, and she just bitched me out. replyplagiarist 14 hours agorootparentprevI have never heard of a municipality actually enforcing their sound regulations other than live performances being shut down. reply CobaltFire 13 hours agorootparentCoronado Island (San Diego) loves giving tickets to all the Navy personnel driving through with too loud of vehicles. We try to warn people transferring there but many don&#x27;t listen and end up with tickets.To be fair (to those who think that&#x27;s excessive), they also (on two occasions) chased down and cited&#x2F;arrested people who tried to run me off the road on my bicycle there (commuted via bicycle for years), and I&#x27;ve had many other great interactions with them. reply wutwutwat 13 hours agorootparentprevGrow up with alcoholic parent who loves to max out their high wattage sound system at 3am all the time and you’ll see it enforced often. Most house parties are shutdown due to noise complaints. reply tbyehl 13 hours agorootparentprevWhere I grew up, an automotive noise citation was easy to beat in court because the law incorporated a complicated and exacting standard for measurement. reply wutwutwat 12 hours agorootparentMost citations can get off on technicalities but that involves missing work, appearing in court, mounting a defense, and the gamble of owing the fine + additional court costs if you lose, and they probably, literally, bank on the probability that most people just pay the fine. Speeding tickets can be thrown out if the radar wasn’t recently calibrated, just have to ask about it in court. Window tint is the same deal, when was it last calibrated and documented to be within spec? Etc. reply lokar 14 hours agorootparentprevLaguna beach is pretty strict on out of towners with loud pipes reply miguelazo 15 hours agorootparentprevMost municipalities, especially ones relying on cops to enforce these rules, rarely take action on these complaints. reply Scoundreller 15 hours agorootparentAt least in Canadian cities, both bylaw officers and police can enforce these things. Bylaw officers tend to have less discretion on doing their job (and uhhh, fewer other crimes to claim to be busy with)Though sometimes this double-responsibility just means finger-pointing instead of action. reply InitialLastName 14 hours agorootparentI&#x27;ve never heard of bylaw officers being a thing in the US. There are some code enforcement people in the US, but they mostly deal in very specific areas (fire code, building code, etc). reply Scoundreller 14 hours agorootparentDon’t worry, you’re not missing out. Ours are ineffective&#x2F;unwilling to enforce noise bylaws against vehicles anyway. reply pc86 15 hours agorootparentprevWho is going to enforce these rules other than cops?I mean yeah if they never get complains about a given area and they get one call about a loud car driving they&#x27;re not going to come in lights and sirens to search for them. But if they consistently get complaints it will absolutely guide their decision-making around where to patrol, how often, etc.Of course this assumes that the political leadership in your municipality hasn&#x27;t decided that cops don&#x27;t need to enforce laws (rare outside of a few large cities) and they&#x27;re not busier with much more serious crimes, underfunded and understaffed, and just not able to spend time on noise complaints (rare but a common talking point). reply RajT88 14 hours agorootparentprevThese rules are usually used as probable cause to target someone the police officer was already targeting. \"Profiling\" is the better word for it maybe. reply wutwutwat 12 hours agorootparentIf I chop the muffler off my car and run around revving the hell out of my engine I’m not going to say I was targeted or profiled after I’m pulled over in the vehicle that’s calling out to nearby police to take notice of me, regardless of what I’m ticketed for in the end. The squeaky wheel gets the grease as they say. You don’t want to be “targeted” or “profiled” (which isn’t what that would be anyway), then you need to blend. reply deepsun 13 hours agoparentprevAre there any legislative initiatives to ban loud motorcycles (like they did in some EU countries)? I&#x27;d really want to vote for it.US is all about freedom, and concern for neighbors is a necessary prerequisite to practicing freedom.PS: I ride a motorcycle as well, there&#x27;s no problem making it reasonably quiet. reply sidewndr46 14 hours agoparentprevYou would need to stop the sale of exhaust tubing to prevent this. I don&#x27;t think that is realistic reply toomuchtodo 14 hours agorootparenthttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30364669 (\"New York is using cameras with microphones to ticket loud cars\") reply akudlacek 14 hours agorootparentprevexactly reply MisterTea 14 hours agoparentprevI wish it were simply just strait pipes. The biggest buffoons are the ones running \"pops and bang\" tune where the clown car&#x27;s engine dumps fuel into the exhaust to make it pop, bang and gurgle. Makes the car sound like shit but these mental midgets think it makes their already ugly slammed trashcar somehow sound cool. The kinds of people who do this usually have very little going on upstairs. reply alistairSH 10 hours agorootparentYou can (indirectly) blame World Rally for this. Those car started doing this 15 or so years ago - the engineers found that dumping raw fuel directly into the turbo (IIRC, they had a 5th injector in the exhaust manifold) kept it spooled up during shifts and throttle lifts. reply I_Am_Nous 12 hours agorootparentprevMy dad bought a truck with aftermarket headers installed and dual aftermarket glasspack mufflers. If you were going down a hill and let off the throttle, it would \"popcorn\" like you described, but not obnoxiously. It was loud when you accelerate quickly, though. Scared a horse once when he pushed the clutch in and revved the engine.I&#x27;d personally rather have a fast, but quiet vehicle. Most vehicles people make loud don&#x27;t really have enough displacement to warrant that much sound so it&#x27;s usually loud AND slow lol reply winrid 8 hours agorootparentThat was also probably due to having a carburetor which won&#x27;t completely stop putting fuel into the engine on decel like EFI. reply bluedino 10 hours agorootparentprevhttps:&#x2F;&#x2F;youtube.com&#x2F;shorts&#x2F;m2_bfeGOzvg?si=2pjk2hD8W6Y_CKXKThis kind of thing for people who were wondering reply otteromkram 7 hours agoparentprevYou could try phoning the police about the issie. Be sure to record the call log.Then, review your town&#x27;s ordinances and see if there&#x27;s anything about passenger vehicles requiring properly maintained and baffled exhaust system.Then, write to your city coun, mayor, and police chief (all in one email) asking why this ordinance isn&#x27;t enforced? Can it be struck from the ordinances if it will never be enforced?Finally, ask them whether or not citizens should be able to enforce the code or ordinances amid willful police negligence to do so? If not, why not? If not, csn they add \"City X does not issue tickets to noisy vehicles\" to their marketing material.Ask if you have fewer rights than someone who knowingly bought or modified a vehicle to disturb tens or hundreds of people all around them at any given moment during the day without recourse?Ask them to provide at least two good points on why a passenger vehicle should be allowed to have a noisy exhaust system.If they can&#x27;t provide a solid solution, then ask them all to step down.(I&#x27;m on your side, btw. Not sure if that was clear from my post.) reply akudlacek 14 hours agoparentprevThis will do nothing to stop people from removing their mufflers. reply sschueller 14 hours agoparentprevCall the police on them. It&#x27;s illegal almost everywhere. reply benced 9 hours agoparentprevFor noise, that should be local police enforcing it. Unfortunately, American local police are somewhere between neo-Nazis and actively useless. reply DiggyJohnson 9 hours agorootparentThis is a caricature in all but a handful of American municipalities. reply tristor 14 hours agoprevI am very annoyed by the decision of the EPA to start cracking down, because it&#x27;s made it impossible to import parts for my track&#x2F;race cars, even though it&#x27;s completely legal to do so. Like many things, the market for enthusiasts supported availability and reasonable pricing for the more hardcore enthusiasts. reply LogicX 6 hours agoparentFrom TFA: “The DOJ’s suit comes after the Environmental Protection Agency (EPA) said that emissions defeat devices would no longer be a top priority for the agency.”So literally the EPA did the exact opposite of your complaint. Or you meant to say DOJ. reply kart23 8 hours agoparentprevlike what parts... I doubt epa cares about your suspension or non-emission related parts. reply rapjr9 2 hours agoprevMaybe they will also go after eBay for the sale of high power lasers, pesticides, lasers sold as laser pointers that have much higher output than 5mW, non-CE tested electrical appliances, use of lead metal in products, and lots of other dangerous stuff. Amazon too. reply jimnotgym 16 hours agoprevFinally the billion dollar online marketplaces get held to a similar standard to real mom and pop shops. reply 8jef 15 hours agoparentLet&#x27;s see where it goes before getting ahead of ourselves.. reply yieldcrv 10 hours agoparentprevdeferred prosecution agreement in 3… 2…. reply ChuckMcM 14 hours agoprevGreat, now do Amazon.It is always amusing to see the evolution of \"code names\" for things that sellers use to try to get around algorithmic detection.And bottom line, I expect businesses operating in the country of origin will likely become the sellers of choice here. reply xnx 15 hours agoprevThis is a good step, would also like to see more enforcement of emissions and noise ordnances at the local level. reply vondur 9 hours agoprevAt first I couldn’t care if someone bought this equipment to mod their vehicles. It’s readily apparent if you see someone driving a diesel like that. However here in Southern California I’ve seen countless vehicles driving the freeways belching smoke and police ignoring it. reply 101 more comments... Applications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Department of Justice (DOJ) is suing eBay due to the alleged sale of more than 343,000 emissions defeat devices and products that infringe the Clean Air Act.",
      "eBay could potentially face billions in penalties, with fines reaching up to $5,580 per violation; the company has claimed this action as \"entirely unprecedented\" and intends to defend itself firmly.",
      "The DOJ also accuses eBay of selling other illegal items, like unregistered pesticides and paint removal products containing a cancer-related chemical; federal pursuit against violations continues despite suggestions otherwise."
    ],
    "commentSummary": [
      "The comments cover a myriad of topics including: a lawsuit against eBay for selling emission defeat devices; the ban of Xiaomi cellphones by the Canadian government; debates about emission regulations and controversies around catalytic converters.",
      "The environmental impact of private jets over trucks, issues with disabling traction control and using aftermarket parts in vehicles, as well as the efficiency and requirement of emissions equipment in contemporary diesel engines, are also highlighted.",
      "The discussion also covers the concern of noise pollution from loud vehicle exhausts and the imposition of noise regulations."
    ],
    "points": 222,
    "commentCount": 350,
    "retryCount": 0,
    "time": 1696958981
  },
  {
    "id": 37830998,
    "title": "HTTP/2 zero-day vulnerability results in record-breaking DDoS attacks",
    "originLink": "https://blog.cloudflare.com/zero-day-rapid-reset-http2-record-breaking-ddos-attack/",
    "originBody": "Get Started FreeContact Sales: +1 (888) 274-3482 The Cloudflare Blog Subscribe to receive notifications of new posts: Subscribe Product News Speed & Reliability Security Serverless Zero Trust Developers Deep Dive Life @Cloudflare HTTP/2 Zero-Day Vulnerability Results in Record-Breaking DDoS Attacks 10/10/2023 Grant Bourzikas 7 min read Earlier today, Cloudflare, along with Google and Amazon AWS, disclosed the existence of a novel zero-day vulnerability dubbed the “HTTP/2 Rapid Reset” attack. This attack exploits a weakness in the HTTP/2 protocol to generate enormous, hyper-volumetric Distributed Denial of Service (DDoS) attacks. Cloudflare has mitigated a barrage of these attacks in recent months, including an attack three times larger than any previous attack we’ve observed, which exceeded 201 million requests per second (rps). Since the end of August 2023, Cloudflare has mitigated more than 1,100 other attacks with over 10 million rps — and 184 attacks that were greater than our previous DDoS record of 71 million rps. Under attack or need additional protection? Click here to get help. This zero-day provided threat actors with a critical new tool in their Swiss Army knife of vulnerabilities to exploit and attack their victims at a magnitude that has never been seen before. While at times complex and challenging to combat, these attacks allowed Cloudflare the opportunity to develop purpose-built technology to mitigate the effects of the zero-day vulnerability. If you are using Cloudflare for HTTP DDoS mitigation, you are protected. And below, we’ve included more information on this vulnerability, and resources and recommendations on what you can do to secure yourselves. Deconstructing the attack: What every CSO needs to know In late August 2023, our team at Cloudflare noticed a new zero-day vulnerability, developed by an unknown threat actor, that exploits the standard HTTP/2 protocol — a fundamental protocol that is critical to how the Internet and all websites work. This novel zero-day vulnerability attack, dubbed Rapid Reset, leverages HTTP/2’s stream cancellation feature by sending a request and immediately canceling it over and over. By automating this trivial “request, cancel, request, cancel” pattern at scale, threat actors are able to create a denial of service and take down any server or application running the standard implementation of HTTP/2. Furthermore, one crucial thing to note about the record-breaking attack is that it involved a modestly-sized botnet, consisting of roughly 20,000 machines. Cloudflare regularly detects botnets that are orders of magnitude larger than this — comprising hundreds of thousands and even millions of machines. For a relatively small botnet to output such a large volume of requests, with the potential to incapacitate nearly any server or application supporting HTTP/2, underscores how menacing this vulnerability is for unprotected networks. Threat actors used botnets in tandem with the HTTP/2 vulnerability to amplify requests at rates we have never seen before. As a result, our team at Cloudflare experienced some intermittent edge instability. While our systems were able to mitigate the overwhelming majority of incoming attacks, the volume overloaded some components in our network, impacting a small number of customers’ performance with intermittent 4xx and 5xx errors — all of which were quickly resolved. Once we successfully mitigated these issues and halted potential attacks for all customers, our team immediately kicked off a responsible disclosure process. We entered into conversations with industry peers to see how we could work together to help move our mission forward and safeguard the large percentage of the Internet that relies on our network prior to releasing this vulnerability to the general public. We cover the technical details of the attack in more detail in a separate blog post: HTTP/2 Rapid Reset: deconstructing the record-breaking attack. How is Cloudflare and the industry thwarting this attack? There is no such thing as a “perfect disclosure.” Thwarting attacks and responding to emerging incidents requires organizations and security teams to live by an assume-breach mindset — because there will always be another zero-day, new evolving threat actor groups, and never-before-seen novel attacks and techniques. This “assume-breach” mindset is a key foundation towards information sharing and ensuring in instances such as this that the Internet remains safe. While Cloudflare was experiencing and mitigating these attacks, we were also working with industry partners to guarantee that the industry at-large could withstand this attack. During the process of mitigating this attack, our Cloudflare team developed and purpose-built new technology to stop these DDoS attacks and further improve our own mitigations for this and other future attacks of massive scale. These efforts have significantly increased our overall mitigation capabilities and resiliency. If you are using Cloudflare, we are confident that you are protected. Our team also alerted web server software partners who are developing patches to ensure this vulnerability cannot be exploited — check their websites for more information. Disclosures are never one and done. The lifeblood of Cloudflare is to ensure a better Internet, which stems from instances such as these. When we have the opportunity to work with our industry partners and governments to ensure there are no widespread impacts on the Internet, we are doing our part in increasing the cyber resiliency of every organization no matter the size or vertical. To gain more of an understanding around mitigation tactics and next steps on patching, register for our webinar. What are the origins of the HTTP/2 Rapid Reset and these record-breaking attacks on Cloudflare? It may seem odd that Cloudflare was one of the first companies to witness these attacks. Why would threat actors attack a company that has some of the most robust defenses against DDoS attacks in the world? The reality is that Cloudflare often sees attacks before they are turned on more vulnerable targets. Threat actors need to develop and test their tools before they deploy them in the wild. Threat actors who possess record-shattering attack methods can have an extremely difficult time testing and understanding how large and effective they are, because they don't have the infrastructure to absorb the attacks they are launching. Because of the transparency that we share on our network performance, and the measurements of attacks they could glean from our public performance charts, this threat actor was likely targeting us to understand the capabilities of the exploit. But that testing, and the ability to see the attack early, helps us develop mitigations for the attack that benefit both our customers and industry as a whole. From CSO to CSO: What should you do? I have been a CSO for over 20 years, on the receiving end of countless disclosures and announcements like this. But whether it was Log4J, Solarwinds, EternalBlue WannaCry/NotPetya, Heartbleed, or Shellshock, all of these security incidents have a commonality. A tremendous explosion that ripples across the world and creates an opportunity to completely disrupt any of the organizations that I have led — regardless of the industry or the size. Many of these were attacks or vulnerabilities that we may have not been able to control. But regardless of whether the issue arose from something that was in my control or not, what has set any successful initiative I have led apart from those that did not lean in our favor was the ability to respond when zero-day vulnerabilities and exploits like this are identified. While I wish I could say that Rapid Reset may be different this time around, it is not. I am calling all CSOs — no matter if you’ve lived through the decades of security incidents that I have, or this is your first day on the job — this is the time to ensure you are protected and stand up your cyber incident response team. We’ve kept the information restricted until today to give as many security vendors as possible the opportunity to react. However, at some point, the responsible thing becomes to publicly disclose zero-day threats like this. Today is that day. That means that after today, threat actors will be largely aware of the HTTP/2 vulnerability; and it will inevitably become trivial to exploit and kickoff the race between defenders and attacks — first to patch vs. first to exploit. Organizations should assume that systems will be tested, and take proactive measures to ensure protection. To me, this is reminiscent of a vulnerability like Log4J, due to the many variants that are emerging daily, and will continue to come to fruition in the weeks, months, and years to come. As more researchers and threat actors experiment with the vulnerability, we may find different variants with even shorter exploit cycles that contain even more advanced bypasses. And just like Log4J, managing incidents like this isn’t as simple as “run the patch, now you’re done”. You need to turn incident management, patching, and evolving your security protections into ongoing processes — because the patches for each variant of a vulnerability reduce your risk, but they don’t eliminate it. I don’t mean to be alarmist, but I will be direct: you must take this seriously. Treat this as a full active incident to ensure nothing happens to your organization. Recommendations for a New Standard of Change While no one security event is ever identical to the next, there are lessons that can be learned. CSOs, here are my recommendations that must be implemented immediately. Not only in this instance, but for years to come: Understand your external and partner network’s external connectivity to remediate any Internet facing systems with the mitigations below. Understand your existing security protection and capabilities you have to protect, detect and respond to an attack and immediately remediate any issues you have in your network. Ensure your DDoS Protection resides outside of your data center because if the traffic gets to your datacenter, it will be difficult to mitigate the DDoS attack. Ensure you have DDoS protection for Applications (Layer 7) and ensure you have Web Application Firewalls. Additionally as a best practice, ensure you have complete DDoS protection for DNS, Network Traffic (Layer 3) and API Firewalls Ensure web server and operating system patches are deployed across all Internet Facing Web Servers. Also, ensure all automation like Terraform builds and images are fully patched so older versions of web servers are not deployed into production over the secure images by accident. As a last resort, consider turning off HTTP/2 and HTTP/3 (likely also vulnerable) to mitigate the threat. This is a last resort only, because there will be a significant performance issues if you downgrade to HTTP/1.1 Consider a secondary, cloud-based DDoS L7 provider at perimeter for resilience. Cloudflare’s mission is to help build a better Internet. If you are concerned with your current state of DDoS protection, we are more than happy to provide you with our DDoS capabilities and resilience for free to mitigate any attempts of a successful DDoS attack. We know the stress that you are facing as we have fought off these attacks for the last 30 days and made our already best in class systems, even better. If you’re interested in finding out more, we have a webinar coming up with more details on the zero-day and how to respond; you can register here. We also have more technical details of the attack in more detail in a separate blog post: HTTP/2 Rapid Reset: deconstructing the record-breaking attack. Finally, if you’re being targeted or need immediate protection, please contact your local Cloudflare representative or visit https://www.cloudflare.com/under-attack-hotline/. We protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust. Visit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer. To learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions. Discuss on Hacker News Security Vulnerabilities Attacks DDoS Follow on X Grant Bourzikas |@GrantBourzikas Cloudflare |Cloudflare RELATED POSTS October 02, 2023 2:00PM Birthday Week recap: everything we announced — plus an AI-powered opportunity for startups Need a recap or refresher on all the big Birthday Week news this week? This recap has you covered... By Dina Kozlov , Mia Wang Birthday Week , Product News , AI , Turnstile , CAPTCHA September 28, 2018 8:40PM Birthday Week Wrap-Up: Every day is launch day at Cloudflare Our customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community.... By Jake Anderson Birthday Week , Product News , Registrar , Cloudflare Workers , Cloudflare Workers KV April 25, 2017 8:45AM Ecommerce websites on Cloudflare: best practices Cloudflare provides numerous benefits to ecommerce sites, including advanced DDOS protection and an industry-leading Web Application Firewall (WAF) that helps secure your transactions and protect customers’ private data.... By Nick B eCommerce , Tips , Page Rules , Railgun , JavaScript March 17, 2022 12:59PM Clientless Web Isolation is now generally available Today, we’re excited to announce that Clientless Web Isolation is generally available... By Tim Obezuk Security Week , Remote Browser Isolation , RBI , Clientless , CASB Sales Enterprise Sales Become a Partner Contact Sales: +1 (888) 99 FLARE Getting Started Pricing Case Studies White Papers Webinars Learning Center Community Community Hub Blog Project Galileo Athenian Project Cloudflare TV Developers Developer Hub Technical Resources Cloudflare Workers Integrations Support Support Cloudflare Status Compliance GDPR Company About Cloudflare Our Team Press Analysts Careers Logo Network Map © 2023 Cloudflare, Inc.Privacy PolicyTerms of UseDo Not Sell or Share My Personal InformationTrust & SafetyTrademark",
    "commentLink": "https://news.ycombinator.com/item?id=37830998",
    "commentBody": "HTTP&#x2F;2 zero-day vulnerability results in record-breaking DDoS attacksHacker NewspastloginHTTP&#x2F;2 zero-day vulnerability results in record-breaking DDoS attacks (cloudflare.com) 193 points by kayfox 22 hours ago| hidepastfavorite68 comments dang 16 hours agoRelated ongoing threads:The novel HTTP&#x2F;2 &#x27;Rapid Reset&#x27; DDoS attack - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37830987The largest DDoS attack to date, peaking above 398M rps - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37831062 raminf 20 hours agoprevhttps:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;security&#x2F;how-aws-protects-custo...https:&#x2F;&#x2F;www.nginx.com&#x2F;blog&#x2F;http-2-rapid-reset-attack-impacti... reply nicolinox 21 hours agoprevhttps:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;identity-security&#x2F;goo... reply anshargal 20 hours agoprevSo is nginx with http2 enabled vulnerable too? Caddy? I should I not worry about this, because a small (by Cloudflare scale) botnet may DDoS a single server completely anyway? reply mholt 17 hours agoparentGo is patching it soon: https:&#x2F;&#x2F;github.com&#x2F;caddyserver&#x2F;caddy&#x2F;issues&#x2F;5877#issuecommen...(Caddy just uses Go&#x27;s HTTP&#x2F;2 implementation.) reply bwesterb 16 hours agorootparentGo patches are out. (1.21.3, 1.20.10) reply otbutz 19 hours agoparentprevnginx: https:&#x2F;&#x2F;www.nginx.com&#x2F;blog&#x2F;http-2-rapid-reset-attack-impacti...caddy: https:&#x2F;&#x2F;github.com&#x2F;caddyserver&#x2F;caddy&#x2F;issues&#x2F;5877 reply thresh 17 hours agoparentprevnginx: https:&#x2F;&#x2F;mailman.nginx.org&#x2F;pipermail&#x2F;nginx-devel&#x2F;2023-October... reply insanitybit 20 hours agoprevI&#x27;m curious to learn more. How how much work is it to establish a stream and close it? It feels like something that could be done very quickly, but it also involves setting up some state (stream buffers) that could be a problem too. reply jeffbee 19 hours agoparentThe cost comes from initiating the request to some backend which presumably starts working on it. reply dartos 20 hours agoprevDoes HTTP&#x2F;3 suffer from this kind of complexity bloat? reply drewg123 19 hours agoparentWell, it requires almost an order of magnitude more energy to serve HTTP&#x2F;3 than HTTP&#x2F;1, so maybe?Why do I say this? Because it breaks nearly every optimization that&#x27;s been made to serve content efficiently over the last 25 years (sendfile, TSO, kTLS, etc), and requires that the server&#x27;s CPU touch every byte of data multiple times (rather than never, for http&#x2F;1). Its basically the \"what if I do everything wrong\" case in my talk here: https:&#x2F;&#x2F;people.freebsd.org&#x2F;~gallatin&#x2F;talks&#x2F;euro2022.pdfGiven enough time, it may yet get close to HTTP&#x2F;1. But its still early days. reply juped 19 hours agorootparentno it&#x27;s more efficient in every way, they called it QUIC not SLO reply giantrobot 19 hours agorootparentThat&#x27;s why they don&#x27;t have data centers in San Luis Obispo. reply insanitybit 20 hours agoparentprevBit of a leading question since you assuming that this is \"complexity bloat\" and not just \"a feature that people use\", but yes, HTTP&#x2F;3 has streams and so it should be vulnerable. reply neild 18 hours agorootparentHTTP&#x2F;3 is not vulnerable to this specific attack (Rapid Reset), because there it has an extra confirmation step before the sender can create a new stream.HTTP&#x2F;2 and HTTP&#x2F;3 both have a limit on the number of simultaneous streams (requests) the sender may create. In HTTP&#x2F;2, the sender may create a new stream immediately after sending a reset for an existing one. In HTTP&#x2F;3, the receiver is responsible for extending the stream limit after a stream closes, so there is backpressure limiting how quickly the sender may create streams. reply insanitybit 16 hours agorootparentThanks. I&#x27;m curious to see how the backpressure ends up playing out in terms of \"do you need 10k boxes to DoS vs 100k vs not feasible\". reply throw0101c 20 hours agorootparentprev> assuming that this is \"complexity bloat\" and not just \"a feature that people use\"¿Por qué no los dos?:) reply insanitybit 19 hours agorootparentWell, to me \"bloat\" and \"useful + used\" are incompatible. The feature only made it into HTTP&#x2F;2 because it saw validation from gRPC, I believe. reply sophacles 19 hours agorootparentprevBloat implies that it isn&#x27;t useful - that it&#x27;s just dead weight.If a lot of people use the thing, it must provide some value to them. reply turminal 21 hours agoprevA number of people have expressed concerns about making the relatively simple protocol more and more complicated in the name of performance. This looks like it&#x27;s going to be their \"Ha, told you so!\" moment. reply notpachet 20 hours agoparentIt reminds me of Meltdown&#x2F;Spectre: you have a pipe, and instructions need to flow through it in a single file line. Let&#x27;s increase performance by allowing things to be sent&#x2F;processed out-of-order! reply insanitybit 20 hours agorootparentThat&#x27;s a good example, because it would be an incredibly bad decision to drop speculative execution since it leads to a massive performance improvement. reply gruez 19 hours agorootparentprevtechnically the issue is speculative execution, not superscalar execution (ie. \"allowing things to be sent&#x2F;processed out-of-order!\"). Most high performance processors have both, but you can have one without the other. reply notpachet 19 hours agorootparentTrue, fair point. reply hotpotamus 20 hours agoparentprevMy problem is that it often seems like significant complexity is added in order to chase marginal performance gains. I suppose performance is relatively easy to measure while complexity is not. reply dur-randir 17 hours agoprev>HTTP&#x2F;2 protocol — a fundamental protocol that is critical to how the Internet and all websites workNo, it isn&#x27;t. This whole article seems more like a marketing sales pitch than a disclosure. reply wtarreau 17 minutes agoparentActually even the diagrams are wrong because they focus on a single connection to explain the problem, carefully omitting the fact that a client can easily open many connections to do the same again. I agree it&#x27;s mostly marketing and press-releases. reply mrmuagi 17 hours agoparentprevI visited a few common sites and they seem to use HTTP&#x2F;2. I&#x27;m not sure the point of arguing it&#x27;s not fundamental, a cursory glance shows HTTP&#x2F;1 is bottlenecked by not being able to use the same TCP connection to serve multiple resources (something HTTP&#x2F;2 fixes)? Is there ire against HTTP&#x2F;2 adoption, and for what reasons? reply jakub_g 16 hours agorootparentI&#x27;m not an area expert, but common issues raised over the years:- HTTP&#x2F;2 as implemented by browsers requires HTTPS, and some people don&#x27;t like HTTPS.- HTTP&#x2F;2 was \"designed by a committee\" and has: a lot of features and complexity; most of those features were never implemented by most of the servers&#x2F;clients; most of those advanced features that were implemented were very naive \"checkbox implementations\" and&#x2F;or buggy [0]; some were implemented and then turned out to be more harmful than useful, and got dropped (HTTP&#x2F;2 push in browsers [1]) etc.[0] https:&#x2F;&#x2F;github.com&#x2F;andydavies&#x2F;http2-prioritization-issues[1] https:&#x2F;&#x2F;developer.chrome.com&#x2F;blog&#x2F;removing-push reply s3p 16 hours agorootparentprevEvery tech company uses HTTP&#x2F;2. I&#x27;m confused as to what the comment before yours is trying to say, it doesn&#x27;t seem to be supported by any facts. reply fzzzy 16 hours agorootparentprevhttp 1.1 connections can be reused, including with pipelining, and it can open multiple sockets to make requests in parallel. http 2 allows out of order responses on one socket. is it worth the complexity? http 1.1 is over 20 years old and battle tested. reply yencabulator 13 hours agorootparentClients stopped using HTTP&#x2F;1.1 pipelining because it just didn&#x27;t work well enough.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;HTTP_pipelining#Implementation... reply jeffbee 19 hours agoprevThis seems like a hyperbolic misuse of both “vulnerability” and “zero-day”. reply sophacles 18 hours agoparentHow is it not a vulnerability? reply tasuki 16 hours agorootparentWhat is the vulnerability anyway? I skimmed the linked article twice and could find no explanation of how it works, beyond \"request, cancel, request, cancel\" and that it&#x27;s called Rapid Reset. Why is HTTP&#x2F;2 in particular vulnerable? Are all protocols supporting streams vulnerable? How is it possible to vomit such a long article with so little information? reply sophacles 15 hours agorootparentThe article we&#x27;re discussing has a link to this deeper description: https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;technical-breakdown-http2-rapid-... reply jeffbee 18 hours agorootparentprevA vulnerability is a flaw in the implementation that allows an attacked to trigger some kind of unexpected result. The result in this case is defined in an RFC. It is 100% working as intended. reply baobabKoodaa 1 hour agorootparentA protocol can also have a vulnerability (the term is not constrained to implementation flaws only) reply sophacles 16 hours agorootparentprevSo your contention is that the creators of HTTP&#x2F;2 intended for all users of it to be DDoSed? reply joshuamorton 16 hours agorootparentI mean yes, much as http1 allows for people to be ddosed. replyformerly_proven 21 hours agoprevtl;dr HTTP&#x2F;2 allows clients to DDoS backends much more effectively by using the multiple-stream feature of HTTP&#x2F;2 to amplify their attack directly inside the reverse proxy (which typically translates HTTP&#x2F;2 to HTTP&#x2F;1).> When Cloudflare&#x27;s reverse proxies process incoming HTTP&#x2F;2 client traffic, they copy the data from the connection’s socket into a buffer and process that buffered data in order. As each request is read (HEADERS and DATA frames) it is dispatched to an upstream service. When RST_STREAM frames are read, the local state for the request is torn down and the upstream is notified that the request has been canceled. Rinse and repeat until the entire buffer is consumed. However this logic can be abused: when a malicious client started sending an enormous chain of requests and resets at the start of a connection, our servers would eagerly read them all and create stress on the upstream servers to the point of being unable to process any new incoming request. reply fragmede 20 hours agoparent> which typically translates HTTP&#x2F;2 to HTTP&#x2F;1Sticking with HTTP&#x2F;2, or going with grpc&#x2F;similar is also possible. It depends on which corner of the Internet you inhabit. (Cloudflare isn&#x27;t the whole Internet, yet) reply bflesch 21 hours agoparentprevhow on earth did nobody anticipate this kind of attack when designing the protocol? it&#x27;s very obvious it can be abused like this reply jsnell 20 hours agorootparentIt took 8 years for somebody to discover this. It can&#x27;t have been that obvious. reply wtarreau 25 minutes agorootparent> It took 8 years for somebody to discover this. It can&#x27;t have been that obvious.Actually that&#x27;s not true, it was already suggested here as a way to circumvent the max_concurrent_streams setting an it seemed particularly obvious: https:&#x2F;&#x2F;lists.w3.org&#x2F;Archives&#x2F;Public&#x2F;ietf-http-wg&#x2F;2019JanMar...As soon as you start to implement a proxy that supports H2 on both sides, that&#x27;s something you immediately spot, because setting too low timeouts on your first stage easily fills the second stage so you have to cover that case.I think that the reality is in fact that some big corp had several outages due to these attacks and it makes them look better to their customers to say \"it&#x27;s not our fault we had to fight zero-days\" than \"your service was running on half-baked stacks\", so let&#x27;s just go make a lot of noise about it to announce yet-another-end-of-the-net. reply carapace 18 hours agorootparentprevIt took eight years for somebody to use this. We don&#x27;t know when it was discovered (nor how many times by how many different people.) reply bombela 17 hours agorootparentI remember noticing this from the HTTP&#x2F;2 RFC, maybe 8y ago. I was studying the head of line blocking issue on a custom protocol atop TCP and was curious to compare with HTTP&#x2F;2. I think I might even have chatted with a coworker about it at the time, as he was implementing grpc (which uses HTTP&#x2F;2) in Rust.It never occured to me that it could be used nefariously! reply mgaunard 20 hours agorootparentprevNot everyone cares about Cloudflare, or even HTTP&#x2F;2.The exploit has more to do with their implementation than the protocol. reply drewg123 19 hours agorootparentGoogle was apparently DOSed by the same sort of attack: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37831062 reply insanitybit 20 hours agorootparentprev> The exploit has more to do with their implementation than the protocol.Is it? I imagine that implementations can do things like make creating&#x2F;dropping a stream faster but how would an implementation flat out mitigate this? reply mgaunard 17 hours agorootparentThere is a maximum bandwidth at which data can arrive. Simply make sure you can always process it faster than the next packet can arrive, or implement proper mitigation in cases where you cannot.It&#x27;s called programming under soft real-time constraints. reply insanitybit 16 hours agorootparentWell yeah, that&#x27;s just how DoS kinda works with these sorts of vulns. \"Be faster\" is obviously a good strategy, but is it viable ? Is setting up and canceling a stream something that can be done at GB&#x2F;s speeds? Maybe, idk. reply mgaunard 16 hours agorootparentIf you push arbitrary amount of pressure through a pipe that can only handle 1000 psi, you need a valve to release the excess pressure, or it will blow up.In the real world, pipes cannot put arbitrary pressure, so your constraint is more bounded than this. So if you receive 2000 psi but your pipes can only handle 1000, you just need a small component that can handle the 2000 to split the pressure in two, and you can handle it all without releasing any.The same applies to digital logic; it&#x27;s always possible to build something such that you can guarantee processing within a bounded amount of time by optimizing and sizing the resources correctly.As the word \"digital logic\" suggests, these sorts of guarantees are more often applied when designing hardware than software, but they can apply to either. reply immibis 7 hours agorootparentprev> Simply make sure you can always process it faster than the next packet can arriveThis is pretty much impossible unless you make the client do a proof-of-work so they can&#x27;t send requests very quickly. Okay, you could use a slow connection so that requests can&#x27;t arrive very quickly, but then the DoS is upstream. reply kiitos 14 hours agorootparentprevHTTP&#x2F;1 between a client-server pair incurs per-request overhead which is not present in HTTP&#x2F;2. You can do more RPS with less CPU if you use HTTP&#x2F;2. reply sophacles 19 hours agorootparentprevMeanwhile several different HTTP&#x2F;2 implementations are dropping fixes for this today. reply formerly_proven 20 hours agorootparentprevIt&#x27;s pretty similar to HTTP&#x2F;1 pipelining, though no reverse proxy I&#x27;m aware of supports it. reply da_chicken 19 hours agorootparentprevFrom what I can tell, people were talking about reset flooding that was dated back in July. Not as a novel thing, either. It&#x27;s just one of several known vulnerabilities, suggesting they&#x27;ve known about this kind of thing for awhile.https:&#x2F;&#x2F;pentestmag.com&#x2F;good-bad-and-the-ugly-of-http-2&#x2F;I genuinely don&#x27;t know if this is real a zero day, or if it&#x27;s a known protocol vulnerability that nobody was mitigating. reply fanf2 13 hours agorootparentprevAt least haproxy anticipated it https:&#x2F;&#x2F;www.mail-archive.com&#x2F;haproxy@formilux.org&#x2F;msg44134.h... reply sebstefan 21 hours agorootparentprevHindsight is 20&#x2F;20 reply is_true 20 hours agorootparentprevThis is the reason you need a security researcher that is actively exploiting things. reply jacquesm 19 hours agorootparentIt is but most companies see that as a cost without upside until they get compromised. reply badrabbit 18 hours agoprev [–] This sounds like an IP spoofing issue, it is an IP&#x2F;layer3 problem where ISPs don&#x27;t filter spoofed addresses from their users. There sre technical solutions but should also happen is cutting off these ISPs from the internet as a whole when there is a large scale ddos affecting global scale network performance. reply nonane 18 hours agoparentNo. This is not a ISP problem and the ISP can not solve this - it’s not even visible to the ISP for encrypted connections. This a problem with HTTP&#x2F;2 itself that web servers &#x2F; load balancers &#x2F; proxies need to account for. reply badrabbit 9 hours agorootparentYou&#x27;re right. My fault. reply charcircuit 18 hours agoparentprev [–] This attack just spams requests to a web server. The novel part of the attack is that it also spams packets to cancel those requests to bypass any concurrency limits that may be in place. reply badrabbit 9 hours agorootparent [–] Yup, I failed by not actually reading past the first few sentences. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Cloudflare, Google, and Amazon AWS have revealed a zero-day vulnerability known as the \"HTTP/2 Rapid Reset\" attack, which exploits the HTTP/2 protocol to conduct significant DDoS attacks.",
      "Cloudflare has successfully mitigated several attacks, including a massive one exceeding 201 million requests per second, and has created technology to guard against this vulnerability.",
      "For protection, Cloudflare recommends understanding your network connectivity, deploying necessary patches, and considering the backup from a secondary cloud-based DDoS provider."
    ],
    "commentSummary": [
      "A zero-day vulnerability in the HTTP/2 protocol has resulted in DDoS (Distributed Denial-of-Service) attacks, leading companies like Cloudflare and Amazon Web Services to generate patches.",
      "HTTP/3, the upcoming version of the protocol, is immune to this vulnerability. This has sparked a debate over whether the vulnerability was foreseen during the protocol's creation.",
      "The security flaw lets attackers overload servers with swift reset requests, resulting in clients discontinuing the use of HTTP/1.1 pipelining due to its inefficiency."
    ],
    "points": 193,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1696939373
  },
  {
    "id": 37833278,
    "title": "Scheme in the browser: A Hoot of a tale",
    "originLink": "https://spritely.institute/news/scheme-wireworld-in-browser.html",
    "originBody": "[Donate] [Jobs] [News] [Community] [Goblins] [Hoot] [OCapN] Scheme in the browser: A Hoot of a tale Spritely Institute -- Tue 10 October 2023 Hey there, it’s been awhile! We’re back to share some more exciting news about Guile Hoot, a WebAssembly toolchain and Scheme→WASM compiler. In our last post we demonstrated that the Guile Hoot toolchain can be used to assemble programs written in WebAssembly Text (WAT) format, which allowed us to develop for the WASM-4 fantasy console. That was pretty cool, of course, but our goal is to compile Scheme programs (of the R7RS-small dialect, for now) to WASM. We are getting ready for our first release, and we’re happy to report that we’ve made excellent progress! We can now demonstrate the Hoot compiler working end-to-end. Before we get to the demo, let’s go over what makes Hoot special. Hoot differs from most WASM compiler projects in some notable ways: Self-contained toolchain: No emscripten, binaryen, wabt, etc. GC reference type usage: The host provides the garbage collector. Small programs produce small binaries: Kilobytes, not megabytes. Full development environment: Compile and run WASM binaries without leaving Guile. Self-contained toolchain The Hoot toolchain has been written from the ground up to be self-contained with no external dependencies required (aside from Guile, of course.) This gives us a lot of flexibility and control over the entire compilation process. The toolchain includes: WAT parser: Converts WAT to WASM. Both folded and unfolded forms are supported. Assembler: Generates WASM binaries. Binary parser: Converts WASM binaries to an internal WASM data structure. Disassembler: Converts WASM to WAT. Linker: Composes WASM modules with a standard libary (for example), adding in only what is actually used. Interpreter: Validates and evaluates WASM without having to jump into the web browser. All of these tools are available as a Scheme library that could be used to automate other WASM targeted build workflows. GC reference types In WASM 1.0, if a language implementation needed a garbage collector then it had to bring its own built on top of linear memory. There are many drawbacks to this approach: Large binaries: A whole GC needs to be shipped along with the program. Stop the world: Parallelism and concurrency are limited in WASM right now, so GC performance suffers. Collectable confusion: How do you know when references to your heap objects are collectable if they’ve been passed to JavaScript or other WASM modules? It’s a heap of trouble. In other words, you had to ship a bad garbage collector when there is a battle tested, high performance, production garbage collector sitting right there in the browser. Check out Andy Wingo’s excellent BOB 2023 talk (in text or video form) for more detailed information about the problems with GC in WASM 1.0. (Andy is the lead engineer for the Hoot project.) Thankfully, WASM GC has added the ability to allocate heap values that are managed by the host. Hoot is currently one of the few dynamic language implementations that makes use of GC reference types. We hope that by sharing our work and our experience we can help other dynamic languages find their way into web browsers, too. Hoot is also taking advantage of another recent addition to WASM: tail calls. Loops are expressed as recursive function calls in Scheme, so having a tail call feature available simplifies our implementation effort. We don’t have to wait long to use these features, either. Production versions of WASM GC and tail calls are already shipping in bleeding edge versions of browsers and will become available to all users of the web soon. Small programs produce small binaries We made the choice when starting this project not to compile Guile’s C runtime to WASM using emscripten. While it would have been feasible, the resulting binaries would have been quite large, among other things. We did not want the end result of this project to be a multiple megabyte binary for “Hello, world.” Instead, we’ve taken the approach of writing a dedicated compiler backend for Guile with some handwritten WASM providing primitive functionality and calling out to imported host functions when necessary. Programs compiled with Hoot include only the parts of the standard library that they rely upon. As mentioned earlier, WASM GC obviates the need to link in an entire garbage collector. For these reasons, Hoot binaries tend to be much smaller than the output of most other compilers. Small programs produce binaries measured in kilobytes, not megabytes. Full development environment Hoot includes an embedded WASM interpreter that makes it possible to run WASM directly within the Guile process being used for development. Since the toolchain is also readily available as a set of Scheme modules, you can compile and test out programs (either Scheme programs or hand-crafted WAT) from the comfort of the Guile REPL. There is also a set of REPL tools to make inspecting and debugging WASM easier. Keep reading for a practical demonstration of the development workflow. OK, onto Wireworld! Wireworld part 1: Scheme To show off the compiler, we decided to revisit our favorite cellular automaton: Wireworld. Check out our last post to learn more about what Wireworld is and how we used Hoot to build a pure WASM version of it. This time, instead of writing several hundred lines of WAT, we wrote about 70 lines of Scheme: (let ((grid-size 40) (empty 0) (cu 1) ; copper (ehead 2) ; electron head (etail 3)) ; electron tail (define (make-grid) (make-bytevector (* grid-size grid-size) 0)) (define (grid-ref grid x y) (bytevector-u8-ref grid (+ (* y grid-size) x))) (define (grid-ref/wrap grid x y) (bytevector-u8-ref grid (+ (* (modulo y grid-size) grid-size) (modulo x grid-size)))) (define (grid-set! grid x y t) (bytevector-u8-set! grid (+ (* y grid-size) x) t)) (define (neighbors grid x y) (define (check x y) (if (= (grid-ref/wrap grid x y) ehead) 1 0)) (+ (check (- x 1) (- y 1)) (check x (- y 1)) (check (+ x 1) (- y 1)) (check (+ x 1) y) (check (+ x 1) (+ y 1)) (check x (+ y 1)) (check (- x 1) (+ y 1)) (check (- x 1) y))) (define (update from to) (do ((y 0 (+ y 1))) ((= y grid-size)) (do ((x 0 (+ x 1))) ((= x grid-size)) (let* ((t (grid-ref from x y)) (t* (cond ((= t empty) empty) ((= t cu) (if ( ,use (hoot reflect) (wasm parse) ;; Load reflection WASM module. scheme@(guile-user)> (define reflect-wasm (call-with-input-file \"js-runtime/reflect.wasm\"parse-wasm)) ;; Compile and evaluate Wireworld source. scheme@(guile-user)> (define-values (grid-size grid-ref grid-setgrid-cycle grid-update) (compile-value reflect-wasm wireworld-src)) scheme@(guile-user)> (grid-ref 0 0) ; electron tail at (0, 0) $10 = 3 scheme@(guile-user)> (grid-ref 1 0) ; electron head at (1, 0) $11 = 2 scheme@(guile-user)> (grid-ref 2 0) ; copper at (2, 0) $12 = 1 scheme@(guile-user)> (grid-update) ; step the simulation scheme@(guile-user)> (grid-ref 0 0) ; copper at (0, 0) $13 = 1 scheme@(guile-user)> (grid-ref 1 0) ; electron tail at (1, 0) $14 = 3 scheme@(guile-user)> (grid-ref 2 0) ; electron head at (2, 0) $15 = 2 The electron has moved to the right one cell, exactly as it should. With some amount of confidence that the program is working, we proceed to the web browser. Wireworld part 2: JavaScript To get this version of Wireworld running in the browser, an additional ~100 lines of JavaScript were used to glue the Scheme program to various browser APIs. Since we aren’t targeting WASM-4 this time, we needed a new rendering method. We chose HTML5 canvas to keep things simple. The code below updates the simulation at some regular interval, renders to the canvas, and handles keyboard/mouse input. Hoot’s reflect.js library allows us to call Scheme procedures from JS. async function init() { const canvas = document.getElementById(\"canvas\"); const ctx = canvas.getContext(\"2d\"); const [_gridSize, gridRef, gridSet, gridCycle, gridUpdate] = await Scheme.load_main(\"scheme-wireworld.wasm\", {}); const gridSize = Number(_gridSize), tileSize = 16; const canvasSize = gridSize * tileSize; const empty = 0, cu = 1, ehead = 2, etail = 3; let paused = true, mouseX = 0, mouseY = 0; function render() { requestAnimationFrame(() => { for (var y = 0; y{ mouseX = pixelToTile(event.offsetX); mouseY = pixelToTile(event.offsetY); if(event.buttons == 1) { // left setTile(mouseX, mouseY, cu); } else if(event.buttons == 2) { //right clearTile(mouseX, mouseY); } render(); }); canvas.addEventListener(\"mousedown\", (event) => { const x = pixelToTile(event.offsetX); const y = pixelToTile(event.offsetY); if(event.button == 0) { // left cycleTile(x, y); } else if(event.button == 2) { // right clearTile(x, y); } }); canvas.addEventListener(\"contextmenu\", (event) => event.preventDefault()); document.addEventListener(\"keydown\", (event) => { if(event.code == \"Space\") togglePause(); }); update(); render(); } window.addEventListener(\"load\", init); Note the use of BigInt in several places. This is a consequence of how JavaScript’s Number type works. JS numbers are floating point values, but Scheme has a much richer set of numeric types including exact integers. So, values of type Number are treated as Scheme floats, and the BigInt type is used to box all integer values that traverse the JS⇄Scheme bridge. As Hoot matures and we implement a foreign function interface (FFI) for calling into JS (or other hosts) from Scheme, we expect to see the amount of glue code shrink considerably. Live demo OK, now it’s time to see Scheme Wireworld in action! But first, a disclaimer: As of publishing, major web browsers are just beginning to ship WASM GC support. Starting with Firefox 120 and Chrome 119, GC is enabled by default. Neither version is stable yet, so use either Firefox Nightly or Chrome Dev if you’d like to run the demo. If you’re reading this further into the future then congratulations! The browser you are using should have everything you need. How to use: Press the left mouse button and move the cursor to draw copper. Press the right mouse button and move the cursor to erase. Left click on a cell to cycle its type (copper becomes electron head, etc.) Right click on a cell to clear it. Press the space bar to pause or resume the simulation. Now you try! If you’d like to compile and run this yourself, check out our GitLab repo for this demo. Like most of our projects, it uses Guix to setup the development environment. The environment requires an unreleased version of Guile compiled from source, so be prepared to wait awhile for it to build. git clone https://gitlab.com/spritely/scheme-wireworld.git cd scheme-wireworld guix shell # The following commands are evaluated *within* the Guix shell guile wireworld.scm guile web-server.scm Once the web server is up and running, visit http://localhost:8080 in your web browser. Feel free to modify the code and have some fun! We look forward to sharing more when we release Hoot 0.1.0! Site contents released under Creative Commons Attribution 4.0 International. Powered by Haunt.",
    "commentLink": "https://news.ycombinator.com/item?id=37833278",
    "commentBody": "Scheme in the browser: A Hoot of a taleHacker NewspastloginScheme in the browser: A Hoot of a tale (spritely.institute) 183 points by nickmain 18 hours ago| hidepastfavorite11 comments tannhaeuser 14 hours agoI&#x27;m personally not a fan, but still am hesitating to point out that Scheme was, in fact, chosen as the language for styling and transforming SGML (DSSSL, still available as OpenJade), and I believe also Brendan Eich&#x27;s first choice, presumably because of the DSSSL precedent. But then supposedly his bosses told him it has to be more like Java, at least in name.I&#x27;m not a big believer in syntax (and think LISPy language will always remain niche, which is part of its appeal), but one thing I&#x27;d imagine is that LISP&#x2F;Scheme could&#x27;ve helped to prevent the syntax excess that is CSS, simply because there were already plausible styling examples for eg. classic stateful recto&#x2F;verso print formatting, and LISP&#x27;s homoiconicity would&#x27;ve make CSS syntax look kindof gross. reply Zambyte 12 hours agoparent> and think LISPy language will always remain niche, which is part of its appealAs someone who uses Scheme regularly and follows Scheme communities fairly actively, I think most people who use Scheme and other LISP dialects would disagree. People often (I think only half jokingly) talk about the possible \"Utopia\" we missed out on if LISP has won. The word \"successful\" is often used to refer to implementations or dialects based on how much use they have.I think the fact that LISP is niche is actually a negative for most of the community, not part of the appeal. reply _a_a_a_ 11 hours agoparentprevIIRC DSSSL influenced XSLT. I personally consider XSLT an abomination.Separately, your criticism of lisp syntax is pure bikeshedding, and depressing to see it brought up again and again here (and I&#x27;m not a lisper). If you can never see below the surface you&#x27;re always going to be stuck at the shallow end. reply tmtvl 10 hours agorootparentAs someone who loves Lisp syntax I have to say that for a programming language syntax is actually very important: it&#x27;s how your mind interfaces with the fabric of the program you&#x27;re working on. A syntax that doesn&#x27;t fit your mind well will make it more awkward to translate your thoughts into code. reply moron4hire 9 hours agorootparentFitting the syntax to your mind? That sounds... limiting. Seems better to do it the other way around. reply postalrat 6 hours agorootparentHow is it a limit when you can choose the syntax? replyiainctduncan 9 hours agoprevAwesome! I am using s7 Scheme in WASM as an interpreter, and it&#x27;s a blast. Very excited to see this.For anyone interested, here&#x27;s how easy it is to use s7 (which is implemented as one big ANSI C file) in WASM. https:&#x2F;&#x2F;github.com&#x2F;iainctduncan&#x2F;s7-wasmAnd a more comprehensive example: https:&#x2F;&#x2F;github.com&#x2F;actondev&#x2F;s7-playground reply schemescape 13 hours agoprevAre there specific Guile code bases that are motivating this work? reply davexunit 13 hours agoparentYes, Spritely&#x27;s Goblins distributed programming environment. We are working on making it run in web browsers. https:&#x2F;&#x2F;spritely.institute&#x2F;goblins&#x2F; reply beepbooptheory 14 hours agoprevUnreasonably excited about this. Seems like such an incredible team with the right kind of goals, doing something novel and cool for, it seems, me personally. reply cpill 13 hours agoprev [–] this layout is terrible on a phone replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Spritely Institute has made significant progress on their Guile Hoot toolchain, which is designed to compile Scheme programs to WebAssembly (WASM).",
      "Among its features, Hoot is self-contained, incorporates garbage collection (GC) reference types, generates compact binaries, and involves a comprehensive developmental environment within the Guile process.",
      "The Institute is set to release Hoot 0.1.0 in the near future, which utilises recent WASM features like GC and tail calls, as demonstrated by a Wireworld cellular automaton program coded in Scheme and JavaScript."
    ],
    "commentSummary": [
      "The article explores the potential benefits of using the Scheme programming language in web browsers, suggesting it could help avoid the complexity of CSS with more manageable styling examples.",
      "The influence of Scheme on other languages, such as XSLT, is also highlighted, emphasizing the role of language scheme in programming.",
      "It discusses the application of Scheme in WebAssembly (WASM) and its use within the Goblins distributed programming environment, signifying its versatility and potential for broader application."
    ],
    "points": 182,
    "commentCount": 11,
    "retryCount": 0,
    "time": 1696951437
  },
  {
    "id": 37830303,
    "title": "Samsung expected to report 80% profit plunge as losses mount at chip business",
    "originLink": "https://www.cnbc.com/2023/10/10/samsung-earnings-preview-q3-2023-chip-losses-weigh-on-profit.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS CNBC TV INVESTING CLUB PRO MAKE IT SELECT USA INTL WATCH LIVE Search quotes, news & videos WATCHLIST SIGN IN CREATE FREE ACCOUNT TECH Samsung expected to report 80% profit plunge as losses mount at chip business PUBLISHED TUE, OCT 10 20236:02 AM EDTUPDATED TUE, OCT 10 20239:34 AM EDT Arjun Kharpal @ARJUNKHARPAL SHARE Share Article via Facebook Share Article via Twitter Share Article via LinkedIn Share Article via Email KEY POINTS Samsung Electronics earnings are expected to plunge nearly 80% in the third quarter, according to analyst forecasts. Analysts see its semiconductor business — typically Samsung’s cash cow — reporting a more than 3 trillion won ($2.2 billion) loss for the third quarter. Memory chip prices have fallen dramatically this year due to a glut caused by oversupply and low demand for end products like smartphones and laptops. Customers experience Samsung’s new flagship fold-screen phone Galaxy Z Fold5 at a Samsung sales store in Hangzhou, East China’s Zhejiang province, Aug. 14, 2023. CostfotoNurphotoGetty Images Samsung Electronics earnings are expected to plunge nearly 80% in the third quarter, according to analyst forecasts, as the company’s biggest profit-driving segment — semiconductors — continues to come under pressure. The South Korean technology giant will issue earnings guidance on Wednesday. Analysts polled by LSEG expect operating profit of 2.3 trillion Korean won ($1.7 billion) for the September quarter, a 78.7% year-on-year decline. Revenue is expected to come in at 67.8 trillion won, a fall of 11.6%, according to LSEG consensus forecasts. Samsung is the world’s largest maker of memory chips, used in products ranging from laptops to servers. It is also the world’s biggest smartphone player. Samsung’s semiconductor business — typically the company’s cash cow — is expected to post a more than 3 trillion won loss for the third quarter, according to analyst forecasts, as it continues to face headwinds. Memory chip prices have fallen dramatically this year due to a glut caused by oversupply and low demand for end products like smartphones and laptops. This has hit Samsung’s profits hard. In its last earnings reports in July, the company predicted a pick-up in demand for chips in the second half of the year, although this does not appear to be playing out as fast as many had hoped. The tech giant has cut production in a bid to help shore up prices, though the effect is not likely to be seen in the third-quarter results. Daiwa Capital Markets said in a note earlier this month that it expects Samsung earnings to miss consensus estimates “due to the higher cost burden from the memory production cut and ongoing soft demand” for its chip manufacturing unit, known as the foundry business. Daiwa analyst SK Kim sees operating profit for the third quarter at 1.65 trillion won, much lower than the average analyst estimate of 2.3 trillion won. There could be two potential bright spots for Samsung in the September quarter, however. Firstly, its display business could see quarter-on-quarter growth due to the release of Apple ’s iPhone 15 series; Samsung sells displays to Apple for iPhones. Secondly, Samsung’s smartphone unit could see improving margins due to the high-end foldable phones it launched in July. Recovery ahead? Investors will be looking for signs that Samsung’s core chip division will stabilize in the current quarter. Looking ahead to the fourth quarter, analysts expect operating profit of 3.8 trillion won, according to consensus estimates. That would represent an 11.5% year-on-year decline, much smaller than the profit drops recorded in the first and second quarters of this year. Revenue is seen flat, arresting the declining sales the company has seen this year so far. Daiwa’s Kim sees the inventory glut easing and memory prices rising in the fourth quarter. Meanwhile, a Citi note in August suggested that Samsung will begin supplying advanced memory chips for U.S. semiconductor giant Nvidia ’s graphics processing units, which are used for artificial intelligence. Kim suggests this will also be a boost for Samsung, adding: “We expect growing opportunities related to AI demand in 2024.” Correction: The key points of this article have been updated to reflect that 3 trillion won is equivalent to $2.2 billion. Squawk Box WATCH LIVE UP NEXTSquawk on the Street 09:00 am ET TV Squawk Box WATCH LIVE UP NEXTSquawk on the Street 09:00 am ET Listen TRENDING NOW Consumers starting to buckle for first time in a decade, former Walmart U.S. CEO Bill Simon warns Troops mass at Gaza border; Hezbollah and Israel exchange shelling How often should you wash your jeans? The Levi’s CEO settles the debate This in-demand remote job pays up to $250 an hour and doesn’t require a degree White House announces new efforts to crack down on ‘tens of billions’ in junk fees by Taboola Sponsored Links FROM THE WEB The Breathtaking 2024 BMW 530e Leaves Nothing To The Imagination Qsearch Search Now The New Electric Ariya Is A Jaw Dropper SearchTipsNow Subscribe to CNBC PRO Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH Advertise With Us PLEASE CONTACT US CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Privacy PolicyDo Not Sell My Personal InformationCA NoticeTerms of Service © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by close dialog Watch: CNBC’s full interview with legendary investor Paul Tudor Jones WATCH NOW",
    "commentLink": "https://news.ycombinator.com/item?id=37830303",
    "commentBody": "Samsung expected to report 80% profit plunge as losses mount at chip businessHacker NewspastloginSamsung expected to report 80% profit plunge as losses mount at chip business (cnbc.com) 176 points by cebert 1 day ago| hidepastfavorite93 comments thiago_fm 22 hours agoSemiconductor in general is going bust, too much supply. All semiconductors companies invested more because of Covid (and the sudden huge demand).Only CUDA&#x2F;Nvidia managed to build something profitable with it, a competitive advantage that could also go down the drain once more investment is put into CUDA alternatives.And this is counting on AI hype to continue (likely, but maybe demand for hardware might eventually stabilise).Even though semiconductors is a highly specialized industry, it&#x27;s a CAPEX heavy business, you need both fabs and research and you also have too many players in the market.People talk a lot about TSMC and the bleeding edge business(computers, phones), but that&#x27;s just one part of it, but there&#x27;s a whole industry of embarked software that will likely be much bigger.I don&#x27;t doubt that the sector will grow(in revenues), but I doubt they&#x27;ll be able to produce a profit as high as some companies in the tech sector like advertising etc, that are much more asset-light.I really fail to see what people see even in Nvidia&#x2F;Cuda, it won&#x27;t take long until others catch up, and they&#x27;ll likely move back to margins closer to the industry, which are extremely low. There&#x27;s even more money to be made in sectors like energy drinks. reply sgerenser 21 hours agoparentEnergy drinks you say? Fun fact, Monster is one of only a small number of stocks that outperformed NVDA over the past 20 years: https:&#x2F;&#x2F;www.portfoliovisualizer.com&#x2F;backtest-portfolio?s=y&s... reply 0cf8612b2e1e 19 hours agorootparentThat’s amusing. Although, Monster had explosive growth in its first few years (starting from nothing). Do it over 17 years and NVidia wins. reply eigenvalue 18 hours agorootparentMonster is a classic \"capital lite\" business model with a large and growing addressable market, and management has been extremely successful reinvesting the substantial free cash flow generated by the business into organic growth and acquisitions. There are really only a handful of businesses of that calibur in the entire public market. Of course, people have realized all of this by this point, so Monster stock trades a large multiple of earnings (nearly 40x), while prospective growth will never match what they did historically from inception, otherwise they&#x27;d end up the largest company in the world after not too long! reply joecool1029 15 hours agorootparentI mean, look at private ones like AriZona iced tea that still sells cans for $0.99 and the owners are multi-billionaires. reply DaveExeter 9 hours agorootparentWhat do think the manufacturing cost for a can of it is? Maybe ten cents?It&#x27;s a high-margin business! reply zappb 7 hours agorootparentIt’s too bad that most business owners care more about power than profits. Otherwise, we’d see far more occurrences of successful businesses like this. reply boringg 19 hours agorootparentprevThat and construction slowing down. I see a lot of construction guys downing Monster energy drinks. TBH I&#x27;m always a little surprised -- but it does give you energy when you need it. reply ComputerGuru 18 hours agorootparentAside from the caffeine boost the bigger issue is just that They burn through calories. reply thiago_fm 19 hours agorootparentprevCelsius is also growing like crazy. As I said, there&#x27;s more money to be made in energy drinks, I don&#x27;t understand the big hype. reply lotsofpulp 19 hours agorootparentMonster energy drinks net income is ~$1B. Intel&#x2F;TSMC&#x2F;etc have or have had net incomes many multiples of that.Is there that much more potential left for energy drinks?I would hope not for the sake of people’s health. reply thiago_fm 18 hours agorootparentCheck Celsius out, it contains less sugar, more caffeine, they&#x27;ve been growing +100% YoY($CELH). There&#x27;s more space than you could imagine. reply lotsofpulp 18 hours agorootparentI don’t see why a startup would be indicative of potential profits in this space, especially a miniscule business where it is expected to grow 100% YoY.Coca Cola and Pepsi each seem to have profits in the $5B to $10B range, which can be a proxy for how much profit there exists in the space because everyone else is much smaller.Maybe if these energy drinks displace all of coffee and tea, but I would be surprised. How much more caffeine will (or should) consume? reply fakedang 16 hours agorootparentprev> Is there that much more potential left for energy drinks?President Camacho has yet to come to power to authorize energy drinks for irrigation (because it&#x27;s got electrolytes), so I&#x27;d say yes, there&#x27;s still a ton of potential. reply otikik 13 hours agorootparentAnd then the become the central part of the economy. reply vkou 17 hours agorootparentprevSelling sugar water that costs pennies to make for dollars per can is a business with excellent margins. The hardest part is figuring out how to market it. reply GuB-42 10 hours agorootparent\"Liquid Death\" is even better. Dollars per can of just water, no sugar. Of course, all of it is marketing, and they are really, really good at it. reply Amezarak 19 hours agorootparentprevEnergy drinks are addictive and the inputs are cheap.I suspect that to some degree individual drinks will be boom -> bust, as newer brands cannibalize the market by having (or advertising) superior energy, more caffeine, better comedowns, etc., and with older drinks having a higher likelihood of bad publicity simply on account of having been around longer. MLMs like Herbalife are also operating in the space (all \"loaded tea\" shops are MLM), so obviously in cases like that there&#x27;s a ton of pressure producing early growth. reply thiago_fm 18 hours agorootparentSo are the inputs of GPU cheap. reply c7DJTLrn 21 hours agoparentprev>Semiconductor in general is going bust, too much supply.TSMC literally can&#x27;t make chips fast enough. Companies are competing for wafers.>you also have too many players in the marketHow? There are a handful of companies on the planet that can do 7nm process chips and below. Western Digital had an issue with its fab in 2022 and that alone was apparently enough to increase NAND flash prices by 10%. reply marcosdumay 19 hours agorootparentI strongly suggest anybody commenting about the situation of 6 months ago to play this:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Beer_distribution_gameAt 2020, a lot of people were saying that no, the executives on those companies know about the problem, of course they&#x27;ll be able to avoid it. Yet, here we are. (Anyway, the game is supposed to be played by people that were instructed about the problem. But you can try it with naive people before instructing them too. It doesn&#x27;t make a difference.)TSMC specifically may be able to avoid it. They are in a different market from most players, with less players that maybe are close enough to act to avoid it. I still wouldn&#x27;t bet on it. reply phkahler 17 hours agorootparentprev>> TSMC literally can&#x27;t make chips fast enough. Companies are competing for wafers.That&#x27;s interesting since Samsung is their primary competitor at the advanced nodes. It takes time and money to move though as fabs aren&#x27;t completely fungible. reply rowanG077 21 hours agorootparentprevI almost can&#x27;t believe this comment. TSMC literally has an entire paragraph in parents comment that addresses it. The comment is about the much larger semiconductor business, not bleeding edge. I fail to see from any perspective how you can read that comment and post this. reply jonwachob91 21 hours agorootparentParents Comment >> People talk a lot about TSMC and the bleeding edge business(computers, phones), but that&#x27;s just one part of it, but there&#x27;s a whole industry of embarked software that will likely be much bigger.Your comment >> TSMC literally has an entire paragraph in parents comment that addresses it.That looks like a single sentence, not an entire paragraph... And it&#x27;s a paragraph without supporting evidence. No shocker c7DJTLrn wasn&#x27;t convinced about parent commenters weak claim. reply rowanG077 21 hours agorootparentThe sentence is obviously delineated by two newlines. A paragraph made of a single sentence.The paragraph is so obvious that I would never expect anyone to question it. TSMC is a mere 12.5% of global semiconductor market in 2022. reply reisse 20 hours agorootparentThere is no global semiconductor market. Saying it exists is the same thing as saying that global vehicle market exists and Boeing competes with Ford in making the best selling vehicle.There is, however, \"consumer chips foundry market\", which is dominated by TSMC with 55%+ market share. And TSMC&#x27;s market share is limited only because of their production capacity. reply rowanG077 17 hours agorootparentA vehicle is a finished good. A semiconductor is a semi-manufacture. Comparing it to a finished good with an end-user purpose is nonsensical. You could compare the global semiconductor market to, for example, the global steel market. reply simne 15 hours agorootparentSemiconductor market is PARTIALLY finished good, if we consider creators market (gigs economy), where sell microcontrollers (RPI2040), and some other semiconductors.It is market of small series custom devices, prototypes (like Oculus Rift), aftermarket upgrades, like cars tuning.Yes, creators market is not too large, total number of sells just around million, but it exists.BTW it is interest question, how large part of steel market also belong to creators market, if it is larger than semiconductor creators market. replyc7DJTLrn 21 hours agorootparentprevAre you referring to this?>People talk a lot about TSMC and the bleeding edge business(computers, phones), but that&#x27;s just one part of it, but there&#x27;s a whole industry of embarked software that will likely be much bigger.I have no idea what \"embarked software\" means. So-called \"bleeding edge\" makes up a significant portion of semiconductor profits, so how can you disregard that and say semiconductor is going bust? reply azraellzanella 20 hours agorootparentMy guess: embarked -> embeddedOP&#x27;s name sounds brazillian, where embedded is called embarcado. reply thiago_fm 19 hours agorootparentYeah sorry, my bad, it&#x27;s embedded :-) reply thiago_fm 18 hours agorootparentprevSemiconductor profits !== revenue.Those semiconductor companies can make lots of revenue, but operate on thin margins, because a competitor can also build a fab. Think graphic cards, Radeon(now AMD) has consistently built worse graphic cards, with less R&D, but sold at a cheaper price point and survived throughout the years.TSMC has some extra expertise that others doesn&#x27;t, and has good profit margins because of that, but together with NVidia, they are the champions, and others are constantly catching up. As with many things in a capitalist system, prices face a race to the bottom.Related to bleeding edge fab. 14nm has suffered some shortages, but even that has changed.One example is Apple, even though they&#x27;ve managed to increase their revenues (and products sold) a lot because of Covid, that demand is down. Apple is projected to have less revenues this year than the last years.The same has happened with PC sales. What has been really driving revenues and profits up in this space is AI.I don&#x27;t believe this will continue. More competition will drive profits down, meaning it&#x27;ll be a less attractive field to invest for VCs or investors.Semiconductor business is more akin to package food products in my point of view. Some products can enjoy a nice margin because of their brand, taste or being hyped at a specific moment, but the majority of it compete on thin margins.It isn&#x27;t like Meta, a closed garden, that has plenty of data and have a product that got everybody addicted and dependant on it, either social networks or communications, that because of network effects is extremely sticky. Where only regulations and government intervention could pose a risk to it.But wall street and people&#x27;s expectations(and valuations) on semiconductors is suddenly as if they are building the next monopoly. Monopolies only exist because there&#x27;s only one, what I see is a reaaaally fragmented market, with high CAPEX, thin margins and low moat. reply HansHamster 17 hours agorootparent> but operate on thin margins, because a competitor can also build a fabBuilding new fabs, especially for the advanced nodes, costs billions, takes years, and requires a lot of know how and special equipment. There are only a handful of companies operating and building new fabs.> TSMC has some extra expertise that others doesn&#x27;t [...] but together with NVidia, they are the champions, and others are constantly catching upNot sure what you mean. Nvidia uses TSMC for the manufacturing for the 40 series (and Samsung for the 3000 series) and doesn&#x27;t have their own fabs.> Think graphic cards, Radeon(now AMD) has consistently built worse graphic cards, with less R&D, but sold at a cheaper price point and survived throughout the years.You mean ATI (now AMD)? They got rid of their fabs back in 2009 and put them into GlobalFoundries. They now also mostly use TSMC for their CPUs and GPUs (and profit from the more advances nodes compared to GF) and GF for a few things in larger nodes.Apple also (just as everyone else you have mentioned) uses TSMC for (most of) their chips and is one of TSMCs largest customers. Intel and Samsung have their own fabs, but also seem to lag behind TSMC. reply thiago_fm 16 hours agorootparentI&#x27;m not comparing TSMC with Nvidia, I said they are champions in that business, given that there are companies that design chips and also has fabs, my comment makes sense?I&#x27;m also sure here people know how fabs are expensive.I believe you understood my comment related to Radeon being part of AMD. And also you need to work on being less pedantic. replySuchAnonMuchWow 21 hours agoparentprev> I really fail to see what people see even in Nvidia&#x2F;CudaCloud providers are choosing nvidia today as the gpgpu platform on the cloud, driven by neural network needs. So lots of software will be written with the platform in mind, aka. for nvidia arch &#x2F; in cuda.This will add friction preventing users to easily change platform (see how long it took for people to switch their workloads to arm for example). It will drives more cloud providers to use nvidia, more software to be written for cuda, and the moat will build itself. reply l33tman 20 hours agorootparentBut there is really nothing that \"normal\" AI requires that is bound to CUDA. pyTorch and Tensorflow are backend agnostic (ideally...). Sure some hardcore people write some of their own cuda extensions but the vast majority don&#x27;t. If there was a competing pytorch&#x2F;tensorflow-supported backend at half the price, AI companies would quickly fix the quirks and run on that instead. It will be a race to the bottom eventually. Even more so on the cloud where you have a higher tendency to run value added services on top, that in turn are price sensitive. reply brucethemoose2 29 minutes agorootparentSome popular ML projects ship with hand optimized CUDA kernels for performance. It is, unfortunately, the standard.If you get around that, sometimes projects will use dependencies that include an odd CUDA-only library. I ran into this a lot with GANs (which often use some weird loss metric with a random hard CUDA dependency) or media libs.Of course there are plenty of solutions, and some are theoretically transparent. But the issue is getting people to use them. Academic ML researchers in particular seem incredibly impatient, looking for the absolute easiest way to prototype something (which usually means coding what they already know). Ease of installation on a wide range of HW is about their last priority. reply Roark66 18 hours agorootparentprev>But there is really nothing that \"normal\" AI requires that is bound to CUDA. pyTorch and Tensorflow are backend agnostic (ideally...).That&#x27;s true, but the current bottleneck in AI&#x2F;ML is memory size+ bandwidth. NVidia with their cuda really is the most economic game in town right now on both the Low \"hobbyist level\" end (nothing beats 2x used rtx3090 setup in value for money), up to \"high end\" (h100 is still the best generally available card). Some interesting \"unified memory\" competition appears on the horizon(Apple, AMD), but both are still far from where nvidia is. reply thiago_fm 18 hours agorootparentCost vs. benefit here is the most important factor.If somebody would come up with a product Tomorrow that isn&#x27;t as performant as H100s, but price-wise is cheaper, they could make NVidia miserable, as NVidia is selling those products with a huge margin, they&#x27;d have a hard time keeping those margins and a price race to the bottom would happen. reply godelski 15 hours agorootparentprev> there is really nothing that \"normal\" AI requires that is bound to CUDA. pyTorch and Tensorflow are backend agnostic (ideally...).There are a lot of optimizations that CUDA has that are nowhere near supported in other software or even hardware. Custom cuda kernels also aren&#x27;t as rare as one might think, they will often just be hidden unless you&#x27;re looking at libraries. Our more well known example is going to be StyleGAN[0] but it isn&#x27;t uncommon to see elsewhere, even in research code. Swin even has a cuda kernel[1]. Or find torch here[1] (which github reports that 4% of the code is cuda (and 42% C++ and 2% C)). These things are everywhere. I don&#x27;t think pytorch and tensorflow could ever be agnostic, there will always be a difference just because you have to spend resources differently (developing kernels is time resource). We can draw evidence by looking at Intel MKL, which is still better than open source libraries and has been so for a long time.I really do want AMD to compete in this space. I&#x27;d even love a third player like Intel. We really do need competition here, but it would be naive to think that there&#x27;s going to be a quick catchup here. AMD has a lot of work to do and posting a few bounties and starting a company (idk, called \"micro grad\"?) isn&#x27;t going to solve the problem anytime soon.And fwiw, I&#x27;m willing to bet that most AI companies would rather run in house servers than from cloud service providers. The truth is that right now just publishing is extremely correlated to compute infrastructure (doesn&#x27;t need to be but with all the noise we&#x27;ve just said \"fuck the poor\" because rejecting is easy) and anyone building products has costly infrastructure.[0] https:&#x2F;&#x2F;github.com&#x2F;NVlabs&#x2F;stylegan2-ada-pytorch&#x2F;blob&#x2F;d72cc7d...[1] https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;Swin-Transformer&#x2F;blob&#x2F;2cb103f2d...[2] https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;tree&#x2F;main&#x2F;aten&#x2F;src reply ckastner 22 hours agoparentprev> Semiconductor in general is going bust, too much supply. All semiconductors companies invested more because of Covid (and the sudden huge demand).Semiconductors are cyclical in general, and most sources that I trust believe that the bottom is already behind us. Many of the big players have already begun guiding revenue growth again, though not as massive as Nvidia&#x27;s windwall, obviously. reply MayeulC 21 hours agorootparentI think I&#x27;ve really picked the worst time to finish my PhD thesis and go on the semiconductor job market... Though I think it will delay me by a few months at worst. reply dboreham 21 hours agorootparentAfter a couple decades you won&#x27;t remember if you graduated during a boom or a bust. reply bigfudge 20 hours agorootparentI thought the evidence was that this did actually have a substantial impact on lifetime earnings and employment. The data I saw actually related to unemployment, but I would be surprised if it didn&#x27;t have an impact. There are more opportunities to shine when things are on the up, and that benefit will persist into the bad times later. It&#x27;s kind of like taking steroids as an athlete early on... you might stop later, but the muscle mass is still there. reply ghaff 18 hours agorootparentprevIn a bust period, a lot of people end up taking whatever job will help pay the bills and, at some point, getting into the field they studied for means competing with new grads with more current knowledge and connections.I don&#x27;t mean to imply your life is over if you graduate during a downturn but you can easily end up derailed and with a potential hit to your earnings going forward. reply jbm 18 hours agorootparentprevI graduated with my Comp Sci degree in 2003ish, and my life was awful for at least 5 years post-graduation. I know it wasn&#x27;t just me too.(This is also because of where I graduated, and the fact that I didn&#x27;t feel comfortable moving to the US post-9-11) reply lawlessone 19 hours agorootparentprevI graduated 2012. The only people hiring were institutions that kicked off the 2008 recession. reply ethbr1 22 hours agoparentprevI&#x27;d have said there was substantial unserved AI demand at less than current Nvidia prices, but hardware-via-cloud upsets that, given they can hit higher utilization.Of everyone, you&#x27;d think Microsoft would have the most incentive (and ability) to improve tooling&#x2F;stability on not-Nvidia.> I really fail to see what people see even in Nvidia&#x2F;Cuda, it won&#x27;t take long until others catch up, and they&#x27;ll likely move back to margins closer to the industry, which are extremely low.Looks like analyst predictions are that Nvidia growth will start flattening in Q1 2025.And when even Cathy Woods is selling because of over-valuation concerns, everyone should be looking for the exits.Part of Nvidia&#x27;s strategic problem is that they made too much money recently: they needed lower profits in their primary markets to disincentivize competitor investment and retain their moat.Instead, AMD and Intel saw a cash cow. And those aren&#x27;t small, ignorable competitors. reply mewpmewp2 21 hours agorootparentI don&#x27;t mean it as a commentary or opinion on Nvidia, but hearing Cathy Woods talking about Nvidia is as if she&#x27;s just doubling down on her bad decision based on her initial gut feeling which she is too stubborn to admit that was wrong.To me it just seems like she happens to pick some stocks based on gut feeling because of some random detail she liked about it and then will say whatever to justify her positions afterwards. reply webninja 7 hours agorootparentYeah Cathy Wood publicly said that she’d back off into more reliable cash flow positive businesses better PE ratios or even cash if the valuations got too high — which she didn’t do. Instead, at the peak, and the trough, she bought more Teladoc and Docusign instead! Both still have a negative PE ratio! They’re no-moat, no-profit, no-dividend, hype companies.The problem was, her ETFs are not paid and incentivized by performance. She’s paid and incentivized by AUM. So her team spends their time writing speeches and on Twitter writing hype posts instead of shutting up, being heads down buried in a laptop analyzing stocks, and performing analysis quietly. She might’ve been worried her AUM could drop if she made big changes like liquidating to cash. reply p1esk 19 hours agorootparentprevInstead, AMD and Intel saw a cash cow. And those aren&#x27;t small, ignorable competitors.AMD and Intel have been trying hard to compete with Nvidia for a while now. I mean in AI accelerator market. Little success so far. reply ethbr1 17 hours agorootparentI&#x27;d say AMD and Intel have been focused on their CPU businesses.If AMD had been really interested (from a corporate strategic level) in the GPGPU market, their driver&#x2F;library team would be a lot bigger. reply gpapilion 16 hours agorootparentAMD was but focused on hpc instead on ml. They have very large installations, but the card is tuned for fp32+ calculations not lower precisions like NVIDIA. reply ethbr1 12 hours agorootparentThe other side effect of HPC, I assume, is that compatibility and dev tooling are non-concerns.You provide a BLAS implementation, and your primary customer can take it from there.As opposed to Nvidia&#x27;s current market, which spans HPC to AWS&#x2F;Azure&#x2F;GCP to corporate datacenter to \"I followed the directions on Reddit\" personal machines. replymattnewton 19 hours agoparentprev> I really fail to see what people see even in Nvidia&#x2F;Cuda, it won&#x27;t take long until others catch up, and they&#x27;ll likely move back to margins closer to the industry, which are extremely lowI thought that would happen by now too, in 2016, so I started slowly selling much of my NVDA stake after a few years. This was wrong. It turns out they have created an ecosystem effect where everyone makes their papers and code work in CUDA and everything else is best effort. AMD hasn’t even really been seriously trying from my point of view, but once they do they’re still trying to bail water with a leaky bucket - new models come out every day where the researchers have run and tested on CUDA only. reply thiago_fm 16 hours agorootparentThere wasn&#x27;t much money on the table back then. Now there is. reply scarface_74 21 hours agoparentprevIt’s not just a matter of catching up with Nvidia hardware - and who is going to do it? One of the major cloud providers? AMD? Apple?Say they do catch up with the hardware, are they also going to convince the entire industry to switch from CUDA. reply ElectricalUnion 19 hours agorootparent> and who is going to do it?One of the big-enough generative&#x2F;LLM AI users would be my guess. OpenAI for example seems to be even going towards making it&#x27;s own chips as an alternative to Nvidia things being too expensive.> are they also going to convince the entire industry to switch from CUDA.Statistically unlikely, for example ARM definitely disrupted MIPS in several embedded networking niches uses, but didn&#x27;t convince the entire industry to switch from MIPS. reply scarface_74 18 hours agorootparentYou realize the type of expertise and money needed to design a processor? That’s not even to mention the loads of money you need to back up to TSMC to even get capacity. For cutting edge fabrication, you’re still going to be behind, Apple and Nvidia to get capacity reply jlokier 17 hours agorootparentOpenAI doesn&#x27;t need a general purpose GPU like Nvidia. It has too major tasks that could benefit from custom ASiCs that we know of: training and inference, for which efficient designs are different.Designing a specialised ASIC is much easier and cheaper than a general-purpose, public use processor for a wide range of applications. You don&#x27;t need most of the programmability or even half the compute and fancy memory and scheduling units. You also don&#x27;t need the to develop a rich API (like CUDA or Vulkan) for third parties to use.This holds even for state of the art intensive calculation engines with minimised energy consumption. Think of all the crypto-mining ASICs that were built a few years ago. They were relatively cheap to design and optimised for calculations per energy unit. reply thiago_fm 15 hours agorootparentYour comment is straight to the point, just see how fast ASICs have destroyed the idea of using graphics cards to mine crypto.I find it very likely that we&#x27;ll have in a year or two, not only cheaper but also faster hardware that can do training and inference. replyda_chicken 20 hours agoparentprev> I really fail to see what people see even in Nvidia&#x2F;CudaThey&#x27;re a nice middle ground between a GP CPU and ASICs that is well suited to the kinds of workloads that venture capitalists are, for the moment, interested in investing in. And it&#x27;s possible that there are enough applications that benefit from GPU architecture that it&#x27;s more efficient to force a GPU to pretend to be a CPU instead of having a CPU.Most likely it&#x27;s just another cycle of the RISC vs CISC pendulum. Like cloud vs desktop, physical vs virtual, serial vs parallel, monolithic vs micro, etc. We&#x27;ll stick with CUDA and use it to solve as many problems as we can with that method, and then as we discover new ways to create or organize GP CPUs, we&#x27;ll shift back to those until we find that problems get hard to solve again. reply reaperman 20 hours agoparentprev> there&#x27;s a whole industry of embarked software that will likely be much bigger.What in embarked? Genuinely curious if this is a misspelling of embedded or if it&#x27;s something else entirely. reply airstrike 19 hours agorootparentEmbedded but on a boat rather than a bed reply k7sune 18 hours agoparentprevWhen the governments see that their investments or subsidies could not break the monopoly of TSMC, I wonder if they’ll change their geopolitical strategy. Instead of protecting Taiwan for silicon supply stability, they’ll encourage an all out war between Taiwan and China. By destroying the head of the monopoly, the fabs in their countries can compete against the market again. reply wslh 21 hours agoparentprevDon&#x27;t forget about the crypto scene that generated a lot of demands for GPUs. One day Ethereum moved, finally, to PoS. reply thiago_fm 18 hours agorootparentIt&#x27;s the same story being played over again and nobody is seeing it.AI has a lot of similarities to the crypto bubble, but people keep repeating this \"ah, but now it is useful!\", sure it is. I like ChatGPT, but will people really use all those new AI products being built?Most things in the real world need an accuracy beyond what AI can offer and will likely offer in a few decades ahead, go learn about AI and any expert will show you that for example, LLMs can&#x27;t solve some basic problems.I&#x27;m not saying AI will never be as smart as human, but rather that, it will take longer until we&#x27;ve figured out stuff that people are already pricing in into markets as if it will happen in a year. reply jncfhnb 21 hours agoparentprevSo… why though? Is the market really not mature enough to anticipate the bull whip? reply pjc50 19 hours agorootparentThe business cycle in general is an unsolved problem, and I think enough companies were investing during COVID to meet the shortages that they over-invested for the demand impacts of the Russia-Ukraine war and its effect on Western inflation. reply tortoise_in 15 hours agoparentprevNividia because of bitcoin? reply mikeryan 20 hours agoprevWhat a sensational way to write this headline. Focusing on profits vs the 11% decline in revenues which is a more salient number. reply hn_throwaway_99 19 hours agoparentReporters love doing this, and I agree with you, it&#x27;s always for pure clickbait value. I mean, when a company swings from a profit to a loss (which happens all the time), you rarely see a headline \"Profits fell 240%!\" because reporters know that would be a confusing way to write it, and it would really call out how dumb it is to use change in profit in the headline. reply jackmott42 18 hours agoparentprevI read the headline and thought \"still making a profit! sweet!\" reply hospitalJail 20 hours agoparentprevProfit is more important than revenue. reply mikeryan 20 hours agorootparentNo. It’s not.The only time that’s true is if your revenues are flat year over year. reply onlyrealcuzzo 18 hours agorootparent> The only time that’s true is if your revenues are flat year over year.That&#x27;s only true in an environment that values growth dramatically over profit.With non-negative real interest rates - profits actually matter. reply marcosdumay 18 hours agorootparentprevThe only time the GP is false is when companies are reinvesting profits in a way that makes it look like costs.But relative changes in revenue are much more impactful than relative changes in profit. This one is obvious, because revenues are much larger and bounded at a minimum of 0. reply mikeryan 18 hours agorootparentI&#x27;m a bit confused These statements seem to contradict themselves?The only time the GP is false is when companies are reinvesting profits in a way that makes it look like costs. (GP said profits are more important)But relative changes in revenue are much more impactful than relative changes in profit (you said revenue is more important) reply marcosdumay 18 hours agorootparent> you said revenue is more importantNo. What I said means that relative change isn&#x27;t a good metric for any of them. But it&#x27;s much less bad for revenue than for profit. reply barelyauser 19 hours agorootparentprev>> \"The only time that’s true is if your revenues are flat year over year.\"That also makes a lot of assumptions on its own. Baseline is: profits matter. Capitalists want their cut. Profits is the extraction of the \"thing\". If capitalists can&#x27;t put the \"thing\" in their pockets then why even bother? reply Jorge1o1 19 hours agorootparentIf you&#x27;re an investor in a company, theoretically you can think of the company as a stream of future dividend payments (cash flows) discounted over time.So maybe the company has $100 in profit in 2024, and they have 5 shares, and they pay out all of their profit as $20 dividend per share. And in 2025 you expect them to make $200 in profit, with the same 5 shares, which is $40 per share.Although that&#x27;s a theoretical model, and there&#x27;s many companies that don&#x27;t pay dividends at all and still command mighty stock prices (e.g. TSLA) this is how real companies are actually valued -- on the expectation of future cash flows, maybe times some multiple.The reason why revenue is more important than profits for the vast majority of technology &#x2F; growth-stage companies is that most of these companies are focused on creating double-digit percent revenue growth every single year. Their profits will be 0 or negative because all of the money that they make from selling widgets is getting spent on marketing or hiring new engineers to build more widgets, or building a factory.But after 5-10 years, this company will have \"scaled over\" its fixed expenses like rent, they will have huge revenues, they&#x27;ll have serious market share, and then you can simply slow down &#x2F; turn off the marketing and other SG&A expenses, and then suddenly you go from being a 0 or negative profit company to a multi-billion $ profit company.But if you&#x27;re too focused on clipping coupons and scrounging pennies as a startup founder, you aren&#x27;t focused on the right aspect: revenue growth. reply dmoy 18 hours agorootparent> Although that&#x27;s a theoretical model, and there&#x27;s many companies that don&#x27;t pay dividends at all and still command mighty stock prices (e.g. TSLA) this is how real companies are actually valuedGotta look at stock buybacks too, which are effectively the same thing but more tax efficient for investors. (Though TSLA doesn&#x27;t do significant amounts of that, and many other giant companies also don&#x27;t do it relative to profits either, so your general point still stands) reply ghaff 17 hours agorootparentYes, but from the perspective of the theoretical model, the reason stock buybacks increase share price is that now the (theoretically unchanged) dividend payouts are getting divided among fewer shares outstanding.The theory is useful to understand that there&#x27;s some basis to the share value of equities although it probably doesn&#x27;t have a lot of practical application. reply tiahura 19 hours agorootparentprevOr if your job is to make a profit. reply CoastalCoder 23 hours agoprev> Analysts see its semiconductor business — typically Samsung’s cash cow — reporting a more than 3 trillion won ($2.2 trillion) loss for the third quarterIs this some kind of reporting error, or is that number accurate? reply twarge 23 hours agoparentIt should be $2.2 billion. reply barelyauser 19 hours agoparentprevNo offense but, how the hell could it be accurate? reply CoastalCoder 18 hours agorootparentIt seemed high, but (a) CNBC has a decent reputation for financial reporting, and (b) I know Samsung is big but I didn&#x27;t know exactly how big, and (c) I hadn&#x27;t had coffee yet. reply padjo 23 hours agoparentprevThe dollar conversion looks wrong for sure reply SushiHippie 8 hours agoparentprev\"Correction: The key points of this article have been updated to reflect that 3 trillion won is equivalent to $2.2 billion.\"from TFA reply vegabook 23 hours agoparentprev1USD = 1349KRW reply fomine3 7 hours agoprevThe worst situation player in this market is Kioxia. They only make NAND chips unlike Micron with DRAM or Korean conglomerates. reply simne 15 hours agoprev [–] I remember old article, on HBR, said, semiconductor fabs are now underloaded (predicted about 70% load), so very much sense to create fabless manufacturer.Looks like, AMD was right, to separate fabs and become fabless, and SEC for some reason cannot do it and suffer losses. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Samsung Electronics is projected to experience a significant drop in earnings, about 80%, in the third quarter due to troubles in its semiconductor sector.",
      "The semiconductor business is expected to record a loss of over 3 trillion won ($2.2 billion), chiefly attributed to declining memory chip prices, an outcome of oversupply and lackluster demand for products such as smartphones and laptops.",
      "Despite these losses, optimism arises from Samsung's display business and smartphone unit, which could witness potential growth in the upcoming fourth quarter."
    ],
    "commentSummary": [
      "Samsung is predicted to see a notable drop in profits due to struggles in its chip business as the semiconductor industry contends with oversupply and strong competition.",
      "The conversation includes the influence of CUDA technology on the AI market and Nvidia's prominence among researchers and developers, alongside the uncertainties about the future profitability of the semiconductor sector.",
      "Speculation regarding Samsung's reported losses in its semiconductor business and the exploration of fabless manufacturing in the industry underscore the importance of profitability, revenue, future cash flows, and stock buybacks for companies."
    ],
    "points": 176,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1696932968
  },
  {
    "id": 37832189,
    "title": "Google Kubernetes Engine incident spanning 9 days",
    "originLink": "https://status.cloud.google.com/incidents/WMmjrixdPfBGFKCohYGd#Z6wKuSfovJCG51wpbwiB",
    "originBody": "Console Google Cloud Service Health Incidents Global: Google Kubernetes Engine Nodepool Upgrade Failures Service Health This page provides status information on the services that are part of Google Cloud. Check back here to view the current status of the services listed below. If you are experiencing an issue not listed here, please contact Support. Learn more about what's posted on the dashboard in this FAQ. For additional information on these services, please visit https://cloud.google.com/. Available Service information Service disruption Service outage Incident affecting Google Kubernetes Engine Global: Google Kubernetes Engine Nodepool Upgrade Failures Incident began at 2023-10-02 11:29 (all times are US/Pacific). Currently affected location(s) Taiwan (asia-east1)Hong Kong (asia-east2)Tokyo (asia-northeast1)Osaka (asia-northeast2)Seoul (asia-northeast3)Mumbai (asia-south1)Delhi (asia-south2)Singapore (asia-southeast1)Jakarta (asia-southeast2)Sydney (australia-southeast1)Melbourne (australia-southeast2)Warsaw (europe-central2)Finland (europe-north1)Madrid (europe-southwest1)Belgium (europe-west1)Berlin (europe-west10)Turin (europe-west12)London (europe-west2)Frankfurt (europe-west3)Netherlands (europe-west4)Zurich (europe-west6)Milan (europe-west8)Paris (europe-west9)Doha (me-central1)Dammam (me-central2)Tel Aviv (me-west1)Montréal (northamerica-northeast1)Toronto (northamerica-northeast2)São Paulo (southamerica-east1)Santiago (southamerica-west1)Iowa (us-central1)South Carolina (us-east1)Northern Virginia (us-east4)Columbus (us-east5)Dallas (us-south1)Oregon (us-west1)Los Angeles (us-west2)Salt Lake City (us-west3)Las Vegas (us-west4)DATE TIME DESCRIPTION10 Oct 2023 13:27 PDTSummary: Global: Google Kubernetes Engine Nodepool Upgrade Failures Description: We believe the issue with Google Kubernetes Engine is partially resolved. We do not have an ETA for full resolution at this point. We will provide an update by Wednesday, 2023-10-11 12:00 US/Pacific with current details. Diagnosis: A small number of customers are experiencing failed nodepool upgrades. Customers experiencing this, may see \"Internal error\" in Google Cloud Console. Retrying is suggested but may not always work. Workaround: Customers can re-create nodepools at the new version instead of upgrading in-place.10 Oct 2023 11:51 PDTSummary: Global: Google Kubernetes Engine Nodepool Upgrade Failures Description: A mitigation has been rolling out and we are assessing its effectiveness. We will provide an update by Tuesday, 2023-10-10 14:00 US/Pacific with current details. Diagnosis: A small number of customers are experiencing failed nodepool upgrades. Customers experiencing this, may see \"Internal error\" in Google Cloud Console. Retrying is suggested but may not always work. Workaround: Customers can re-create nodepools at the new version instead of upgrading in-place.9 Oct 2023 12:02 PDTSummary: Global: Google Kubernetes Engine Nodepool Upgrade Failures Description: A mitigation has been rolling out and we are assessing its effectiveness. We will provide an update by Tuesday, 2023-10-10 12:00 US/Pacific with current details. Diagnosis: A small number of customers are experiencing failed nodepool upgrades. Customers experiencing this, may see \"Internal error\" in Google Cloud Console. Retrying is suggested but may not always work. Workaround: Customers can re-create nodepools at the new version instead of upgrading in-place.9 Oct 2023 10:57 PDTSummary: Global: Google Kubernetes Engine Nodepool Upgrade Failures Description: A mitigation has been rolling out and we are assessing its effectiveness. We will provide an update by Monday, 2023-10-09 16:00 US/Pacific with current details. Diagnosis: A small number of customers are experiencing failed nodepool upgrades. Customers experiencing this, may see \"Internal error\" in Google Cloud Console. Retrying is suggested but may not always work. Workaround: Customers can re-create nodepools at the new version instead of upgrading in-place.6 Oct 2023 10:28 PDTSummary: Global: Google Kubernetes Engine Nodepool Upgrade Failures Description: A mitigation has been rolling out and we are assessing its effectiveness. We will provide an update by Monday, 2023-10-09 12:00 US/Pacific with current details. Diagnosis: A small number of customers are experiencing failed nodepool upgrades. Customers experiencing this, may see \"Internal error\" in Google Cloud Console. Retrying is suggested but may not always work. Workaround: Customers can re-create nodepools at the new version instead of upgrading in-place.3 Oct 2023 11:07 PDTSummary: Global: Google Kubernetes Engine Nodepool Upgrade Failures Description: Our engineering team is working on a mitigation which will rollout over the next few days. We will provide an update by Friday, 2023-10-06 12:00 US/Pacific with current details. Diagnosis: A small number of customers are experiencing failed nodepool upgrades. Customers experiencing this, may see \"Internal error\" in Google Cloud Console. Retrying is suggested but may not always work. Workaround: Customers can re-create nodepools at the new version instead of upgrading in-place.2 Oct 2023 14:57 PDTSummary: We are experiencing an issue with Google Kubernetes Engine Description: Our engineering team continues to investigate mitigation pathways. We will provide an update by Tuesday, 2023-10-03 12:00 US/Pacific with current details. Diagnosis: A small number of customers are experiencing failed nodepool upgrades. Customers experiencing this, may see \"Internal error\" in Google Cloud Console. Retrying is suggested however may not work. Workaround: Customers can re-create nodepool at the new version instead of upgrading in-place.2 Oct 2023 14:40 PDTSummary: We are experiencing an issue with Google Kubernetes Engine Description: Our engineering team continues to investigate mitigation pathways. We will provide an update by Tuesday, 2023-10-03 12:00 US/Pacific with current details. Diagnosis: A small number of customers are experiencing failed nodepool upgrades. Customers experiencing this will see \"Internal error\" in Pantheon. In this scenario, retrying may not work. Workaround: Customers can re-create nodepool at the new version instead of upgrading in-place.2 Oct 2023 14:30 PDTSummary: We are experiencing an issue with Google Kubernetes Engine Description: We are experiencing an issue with Google Kubernetes Engine. Our engineering team continues to investigate the issue. We will provide an update by Monday, 2023-10-02 14:45 US/Pacific with current details. Diagnosis: Customers with nodepools at version 1.24 cannot upgrade them to the next minor version, 1.25. Workaround: Customers can re-create nodepool at the new version instead of upgrading in-place.2 Oct 2023 12:14 PDTSummary: We are experiencing an issue with Google Kubernetes Engine Description: We are experiencing an issue with Google Kubernetes Engine. Our engineering team continues to investigate the issue. We will provide an update by Monday, 2023-10-02 14:30 US/Pacific with current details. Diagnosis: Customers with nodepools at version 1.24 cannot upgrade them to the next minor version, 1.25. Workaround: Customers can re-create nodepool at the new version instead of upgrading in-place.2 Oct 2023 12:11 PDTSummary: We are experiencing an issue with Google Kubernetes Engine Description: We are experiencing an issue with Google Kubernetes Engine. Our engineering team continues to investigate the issue. We will provide an update by Monday, 2023-10-02 12:47 US/Pacific with current details. Diagnosis: Customers with nodepools at version 1.24 cannot upgrade them to the next minor version, 1.25. Workaround: Customers can re-create nodepool at the new version instead of upgrading in-place.2 Oct 2023 11:47 PDTSummary: We are experiencing an issue with Google Kubernetes Engine Description: We are experiencing an issue with Google Kubernetes Engine. Our engineering team continues to investigate the issue. We will provide an update by Monday, 2023-10-02 12:47 US/Pacific with current details. Diagnosis: Customers with nodepools at version 1.24.14-gke.2700 cannot upgrade them to the next minor version, 1.25.12-gke.500. Workaround: Customers can re-create nodepool at the new version instead of upgrading in-place. All times are US/Pacific Documentation RSS Feed JSON HistorySchema JSON Product CatalogSchema Send feedback Privacy policy",
    "commentLink": "https://news.ycombinator.com/item?id=37832189",
    "commentBody": "Google Kubernetes Engine incident spanning 9 daysHacker NewspastloginGoogle Kubernetes Engine incident spanning 9 days (cloud.google.com) 174 points by talonx 20 hours ago| hidepastfavorite110 comments wg0 16 hours agoMay be this a hated take but just wondering - The place that pretty much invented cluster orchestration and reinvented it as k8s is having problems upgrading it.What chance a bunch of poor sys admins stand running bunch of k8s clusters for a mid size company I wonder.Every time I think of deploying (self managed, have done full stack) it for something mission critical, this upgrade scenario simply makes me rethink it altogether.And even managed k8s has no guarantees and if managed is to be the option, nothing beats ECS in simplicity and smooth operation at certain scales.PS: Full stack K8s means ingress controllers, DNS auto registration, GitOps , logging, monitoring, CI&#x2F;CD and all the bells and wistles including a management UI behind oauth etc. reply 0xbadcafebee 15 hours agoparentThe design of K8s is ridiculous. The fact that you&#x27;re forced to upgrade every 9 months is even more absurd. It&#x27;s \"opinionated\", but the \"opinions\" are those of one of the largest companies in the world, who hire the most engineers in the world, and everyone else is supposed to operate like that now.K8s is a great example of a \"tech jobs program\": technology that is absurdly complicated to the point that you have to pay someone lots of money to make it keep working, but you can&#x27;t afford not to pay for this, because everyone has decided to be stuck on it and now there&#x27;s no alternative (and probably never will be) reply sangnoir 15 hours agorootparent> K8s is a great example of a \"tech jobs program\"No, it is not. K8s is an example of how it&#x27;s impossible to be a jack of all trades and retain simplicity. If a tool seeks to be all things to everyone, then there&#x27;s no sweeping the inherent complexity under the carpet.Full-coverage solutions to complex domains are themselves complex by necessity - see SAP, Salesforce, Oracle, Windows backwards compatibility, or try writing a parser&#x2F;stemmer for a spoken language that evolved naturally :) reply 0xbadcafebee 6 hours agorootparentThe complexity of an implementation is a different thing than the complexity of a design, and each determines things like how frequently it needs maintenance or how difficult it is to use. As usual, the devil&#x27;s in the details.You can make something that has complexity without making it complicated or expensive to maintain. Take cars for example. There are many cars today that have a barrage of complex features, yet don&#x27;t have costly maintenance bills and aren&#x27;t difficult to use. That&#x27;s because they have been engineered purposefully to reduce that complexity, and because the people that make them are masters of their craft, trying to create a premium product. Materials engineers, structural engineers, sound engineers, mechanical engineers, and more, all work toward the best design possible, to implement the most functionality with the minimum number of defects. They can tell you how likely it is for the paint to begin chipping on a new tailgate due to potential contamination in an alloy. And they&#x27;ll use that knowledge to make it less likely that you&#x27;ll have to take your car in for repair.The software world today - especially where customers are an afterthought (such as backend software) - does not make attempts to improve its design or implementation to reduce the need for repairs. Instead they just \"manage\" it for you, doing constant maintenance on the constantly breaking vehicle they drive for you, while charging you a subscription fee. (the auto industry noticed how profitable this is, and is now doing the same wherever it can)K8s is a great example of terrible design and half-assed implementation. It was not designed to be easy to install, configure, operate, upgrade, or maintain. It was designed the way a company designs software for its own internal use: start with some \"opinions\" that are more about inflated intellectual ego than practicality, throw some shit together that \"basically works\", and keep introducing breaking changes so it can never be operated continuously without requiring constant reworking and changing how it&#x27;s run.If a car were produced like this, nobody would buy it. But it&#x27;s handed out \"for free\", so we all just accept it and end up driving this janky-ass school bus that constantly needs its components changed while the bus is running. But you can&#x27;t ask a regular person to change the parts on a bus mid-cross-country-tour, so you have to hire a mechanic to sit there on the bus. It doesn&#x27;t have to be that way, it just is because the people who made that bus don&#x27;t give a shit about hiring a mechanic, they have 10,000 mechanics already. reply wg0 10 hours agorootparentprevThe biggest foot gun and most complicated part IMHO is networking.It is a blackbox with many competing implementations and each implementation even differing in implementation details within it&#x27;s own versions.In theory, it seems reasonable to say that K8s networking is an implemention detail which you shouldn&#x27;t bother about but in practice, it&#x27;s going to bite you because of some misconfigured component somewhere someday and you would have no idea what to make of thousands of iptable rules or BGP routes or even least inspectable eBPF mini programs.Paid support, consulting. reply Too 6 hours agorootparentThis.Most application developers don’t give a rats ass about how 2 nodes are connected as long as the pods can speak to each other. Throw these guys in a devops team and give them the task to setup a 2 node k8s cluster and guess how much consideration they will pay to networking. Correct, zero. Except you can’t, because you have to choose a CNI plugin or choose a distribution that has made an opinionated choice for you.To lower the barrier of entry. A default installation must become a lot more opinionated with much less options and more sane and safe default. reply OhSoHumble 4 hours agoparentprevWhat&#x27;s really unfortunate is that it feels like not choosing k8s is career suicide for SREs&#x2F;DevOps&#x2F;platform engineering.I chose Nomad for my current role and it works fantastic.I used ECS in my last role and it was also fantastic.I&#x27;m currently gently looking for a job and the market is brutal. I&#x27;m not even getting responses back from applications and I have ten years of experience. Every job posting I see requires deep k8s knowledge and hands-on experience with some ancillary k8s project like ArgoCD or Flux. I&#x27;m actually thinking about downplaying non-k8s experience on my resume and just floating the k8s that I do have straight to the top because, at the very least, it&#x27;ll get my resume looked at.If I was going to go back in time, I&#x27;d pick k8s instead of Nomad or ECS even if they weren&#x27;t the best contextual choices because, hey, I gotta&#x27; put food on the table. reply eitally 15 hours agoparentprevIt used to be straightforward when Google Cloud just offered everyone the same version of GKE essentially everywhere. It was one of not-too-many services that are vanilla and identical in pretty much every region. But, newer offerings have fractured things a bit (sovereign cloud, Google Distributed Cloud, FedRAMP, Tencent, etc) and I can see why it would become problematic to adequately test and keep everything in sync while also not breaking interfaces and&#x2F;or downstream services. Also, Google used to allow customers essentially 0 say in when upgrades to services were applied. Now, as part of many vendor security audit processes, it&#x27;s a hard stop requirement that customers are able to test new revs before applying to prod ... and GKE, like GCE, is one of those critical services. reply harikb 16 hours agoparentprevAn equally hated contrary take - may be it is our (as users of k8s) desire to do in-place upgrades without downtime..Sometimes a clean install of fresh cluster and switch over is so much simpler if we can just take a few minutes downtime to cut over.Complex requirements (sometimes) end up with complex solutions. reply xeromal 15 hours agorootparentI&#x27;ve never used K8s in my life but if you have to deal with downtime, I&#x27;d question the use of K8s when I can do it the old and busted way. reply harikb 15 hours agorootparentI am not sure what \"old and busted way\" you are referring to, but k8s does have value, particularly when we are not upgrading k8s itselfIt would be good to think of the occasional downtime during a significant upgrade (once in 6 months?) separate from the ability to automatically replace failed nodes, scale up and down, having all the k8s magic. reply PH95VuimJjqBqy 12 hours agorootparentI would imagine using a load balancer in front of multiple servers across several datacenters coupled with a stateless design that allows traffic to flow freely across either of them.there are other variations of this, but this seems the most obvious. blue&#x2F;green deploys and the like. reply wg0 10 hours agorootparentprevIt is simply not feasible if you have stateful workloads for some reason. Or at least isn&#x27;t as straight forward.Booting up a cluster is fine but the real struggle starts from that point onwards. reply llama052 15 hours agorootparentprevI think it&#x27;s definitely overlooked. You should always have the ability to spin off a new cluster and run your workloads on that. At a minimum as a disaster recovery scenario. reply talonx 6 hours agoparentprevWhat bugs me most about GKE is that incidents seem to span a whole of datacenters simultaneously (source: see zones affected by previous incidents), and you are often left with no option except wait for a resolution. Often there are issues like networking problems affecting almost the entire global GKE control plane. Whatever happened to resilience by isolation? reply moondev 15 hours agoparentprevCheck out cluster-api. Complete zero downtime cluster upgrades is as simple as bumping the version and machine image name in the cluster manifest - or ClusterClass if you run classy clusters reply markstos 15 hours agorootparentAnd why is that expected to be more reliable than Google’s service? reply Freedom2 15 hours agoparentprevWhile I see your point and agree, there&#x27;s also the view that the people who invented the tech are simply different than the kind of people who Google hires nowadays - pure algo focused LeetCoders with zero-to-little operational experience and little interest in even learning how to run and maintain software. reply alectroem 20 hours agoprevWow, I literally did a full cluster version upgrade last night without knowing about this. I would have delayed the upgrade if I had known GKE was failing for \"a small number of customers\"I wish cloud providers would just communicate outages to services I use like this to me! reply londons_explore 19 hours agoparent> communicate outages to services I useIn general, they seem bad at communicating relevant information. Just looking over at emails from Google, every single one of the last 10 emails they sent me was not relevant to me specifically.> \"[Important notice] Tax changes in Nepal\" (yet I have never made a sale in Nepal)> \"Secure your Google Admin account with these best practices\" (yet I already do all those things)> \"Preparing for the upcoming Google Ads change on October 31, 2023\" (it&#x27;s all about mediation waterfalls, and I&#x27;m 90% sure I don&#x27;t use them, and 100% sure I don&#x27;t know what they are) reply Spivak 18 hours agorootparentAWS got this very right with the per-account incident dashboard. reply FireBeyond 17 hours agorootparentExcept when their systems don&#x27;t correctly discern what services you may be using... reply ljm 18 hours agoparentprevI haven&#x27;t had that comms issue with Google but I have to say, even though I prefer GCP to AWS in terms of user friendliness, it is far too often the case that you find exactly the solution you need only to learn it&#x27;s deprecated in favour of a less useful alternative. reply talonx 19 hours agoparentprevThey do publish an RSS feed for the status page, but there is no direct way to get notified AFAIK. I used to create a Slack notifier using IFTTT. reply captn3m0 19 hours agorootparentFor infra folks, I always suggest having a Slack channel with RSS feeds from vendor incident sites.Slack let’s you subscribe directly to an RSS feed using &#x2F;feed. reply sleepybrett 17 hours agorootparentDon&#x27;t put this in a special channel just for vendor incidents. Hopefully you have a channel for each vendor tool where you have a vendor representative present. You put that bot in there. It&#x27;s much more likely to be noticed and much less likely to be a channel that everyone ignores because it&#x27;s SNR is to low. reply danielovichdk 17 hours agorootparent\"vendor representative\" ? Internal or external contact ?Sounds like a good approach nonetheless reply sleepybrett 7 hours agorootparentexternal, not every vendor will do this but if you are big enough and they are big enough it never hurts to ask. reply talonx 18 hours agorootparentprevYes, I used to do that for my teams. Most infra vendors have RSS feeds for their public status pages. reply alectroem 18 hours agorootparentprevThats a really good idea! reply andrelaszlo 19 hours agoparentprevYou can also use the (pre-GA) Service Health API to get alerts specifically for the regions and services you use. It&#x27;s pretty nice!https:&#x2F;&#x2F;cloud.google.com&#x2F;service-health&#x2F;docs&#x2F;overview#how-pe... reply heyoni 19 hours agoparentprevI will say CircleCI’s dashboard makes it impossible to NOT know there is an outage going on by putting it in the sidebar. Unless it’s collapsed you’ll be aware of everything breaking (that they report) to the point where it feels like everything is breaking all the time reply senderista 15 hours agorootparentIf you&#x27;ve ever worked at a cloud provider, then you know everything is breaking all the time. The good ones are just able to hide it most of the time&#x2F;for most customers. reply danielvaughn 18 hours agoparentprevlooks like it&#x27;s only affecting clusters on 1.24. If you upgraded it was likely to 1.27 reply Racing0461 19 hours agoparentprevThe status page became political. reply jamietanna 19 hours agorootparentSorry what do you mean by this? reply ucosty 19 hours agorootparentI think they mean that companies rarely update their status pages to reflect reality (for instance, AWS outages are rarely shown on their own status pages). This is often by design, company policy, or a desire to save face. reply deathanatos 18 hours agorootparentAnd it&#x27;s so incredibly dumb. Companies need to get it through their thick heads that this is so incredibly short-sighted.Not once has a status page that&#x27;s devoid of information or slow to update ever saved face. I am far more annoyed to have to continue to verify \"no, it is indeed that your service is down, not mine\" and then file a support ticket. I am triply annoyed if the response from support is \"ah yes that&#x27;s a known problem and we&#x27;re working on it\" — known, and you just didn&#x27;t bother to communicate.I miss the days when Github had graphs. Even if they simply hadn&#x27;t had the time to put a message on the page, you could tell from the graphs that it was Github. But even with \"more information\" that some PM might not like being put out publicly, Github felt more reliable & stable in those days.At the end of the day, no amount of political gamesmanship will save you from having to actually run a reliable service, and gamesmanship just makes it more likely I&#x27;ll ascribe false positives to your service, further lowering my perception of its reliability.It&#x27;s so watered down that the \"AWS\" emoji in our Slack instance is literally a meme of the status page. reply the-dude 18 hours agorootparentI bet as long as the status page is not updated, it is not taken into account when calculating quarterly or yearly uptime statistics.I am sure that counts. Probably tied into someone&#x27;s bonus as well. reply sambazi 2 hours agorootparent> Probably tied into someone&#x27;s bonus as well.very likely not just one.not sure how this could work, as some qa&#x2F;sre would have to be paid even more in order to effectively work against this type of falsification. there would have to be a very strong incentive for the company to do that. reply Racing0461 19 hours agorootparentprevAny metric that becomes a target ceases to be a good metric. https:&#x2F;&#x2F;health.aws.amazon.com&#x2F;health&#x2F;statusSee all those green there? Once it started becoming \"monitored\" by VPs instead of the software engineers on call, they started to become political. I bet there are several sev2s happening for several of those services even as we speak but it still shows green to the outside observer. If one has access to the actual metrics for those services, i bet we would see a different story than what is shown on the \"status\" page. reply arzig 15 hours agorootparentThere was something probably here a few days ago to the effect of &#x27;Their 9s are not your 9s&#x27;. Like yes, their status is showing an error rate of less than .00001%. However, all of those errors are being generated by your 5 instances that are completely down. reply secondcoming 18 hours agoparentprevThere is a &#x27;Google Cloud Service Health Updates&#x27; Slack app that has notifications about this incident. Here&#x27;s what it looks like on our channel: 8:18 PM APP UPDATE: Global: Google Kubernetes Engine Nodepool Upgrade Failures Incident began at 2023-10-02 11:29 (all times are US&#x2F;Pacific).Summary: Global: Google Kubernetes Engine Nodepool Upgrade Failures Description: A mitigation has been rolling out and we are assessing its effectiveness. We will provide an update byTuesday 2023-10-10 12:00 US&#x2F;Pacific with current details. Diagnosis: A small number of customers are experiencing failed nodepool upgrades. Customers experiencing this, may see \"Internal error\" in Google Cloud Console. Retrying is suggested but may...There are quite a lot of alerts about various issues. reply nailer 18 hours agoparentprevI wish I had a better answer but:When you suspect things are broken, check X (FKA Twitter).Other devs will be talking about it before there&#x27;s an official status page. reply deathanatos 18 hours agorootparent… if only the site formerly known as Twitter wasn&#x27;t so hostile to being checked these days.If we as an industry can&#x27;t think of something better (cough honest status pages cough) … can we at least transition these tweets to Mastodon. reply pixl97 18 hours agorootparentHonest status page are a really really hard problem.If your connector between your status monitor and the service breaks you&#x27;ll have some subset of users panicking and causing problems (or asking for refunds for outages) when the service was up the entire time.3rd party services are the only ones that you&#x27;ll get a \"more honest\" but not always correct view of what the actual status is. reply solardev 18 hours agorootparentEven if they just updated it manually for major incidents, it would still be useful. reply pizzafeelsright 16 hours agorootparentDefine major incident. Defines update. reply piperswe 14 hours agorootparentWhen an incident is declared, have someone tasked with determining customer impact. If the impact radius is greater than a handful of customers, declare a public incident. If customer communication is made a priority, then you can actually have a helpful status page.Where I work, just about any non-false-alarm incident ends up on the status page in a timely manner. There&#x27;s nothing stopping the likes of AWS from doing the same except for culture. reply solardev 12 hours agorootparentExactly. It&#x27;s the kind of thing where one person managing it and deciding whether to post updates is probably gonna work better than anything automated.Well, maybe not at AWS scales though, if they have thousands of everything =&#x2F; reply solardev 15 hours agorootparentprevWhy? It&#x27;s just a judgment call reply nailer 17 hours agorootparentprev> … if only the site formerly known as Twitter wasn&#x27;t so hostile to being checked these days.You can easily check it manually. Search for \"EBS\" and see a bunch of people talking about EBS timeouts or whatever. That was what I was getting at.But yeah scraping is harder now. reply deathanatos 15 hours agorootparentI&#x27;m not talking about scraping, I&#x27;m talking about manual usage.> You can easily check it manually.You cannot. Twitter&#x27;s site is plagued by redirect loops. If you work around those, these days &#x2F;search just redirs to the login page. You can view single tweets, but there won&#x27;t be any replies. (I have no idea if the site formerly known as Twitter is still rate-limiting views, or if they canned that.)It is unusable if you&#x27;re not actively logged in, and some of us have no desire to give away a phone number just to see AWS&#x27;s true status. replyinput_sh 15 hours agoprevThey also had an issue with creating and deleting persistent volumes on 1.25. It lasted for 15 days, or half a month(!) last month: https:&#x2F;&#x2F;status.cloud.google.com&#x2F;incidents&#x2F;EBxyHQgEPnbM3Syag5...I&#x27;m also incredibly annoyed at them displaying time in PDT. I genuinely don&#x27;t understand why they decided on that instead of doing something normal like UTC or detecting my timezone. Especially annoying every six months because Europe and the US don&#x27;t do Daylight Savings Time changes at the same time, so for a week or two there&#x27;s an additional hour I have to account for. reply DishyDev 19 hours agoprevNot a great week for managed Kubernetes services as Digital Ocean have been having an ongoing issue since yesterday morning on their service https:&#x2F;&#x2F;status.digitalocean.com&#x2F;incidents&#x2F;fsfsv9fj43w7 reply k8svet 15 hours agoparentTo be honest, I&#x27;m pretty disappointed that this thread doesn&#x27;t have some attempt at a root cause now that two major managed providers are having extended outtages linked to upgrades.To think, I once was so in love with it. This kind of crap, and imagining the live-site chaos makes me so glad to have noped-out when I did. With my modest lifestyle, I probably passed on life changing money but I also have 1&#x2F;10th the stress and anxiety I used to. reply endisneigh 19 hours agoprevSucks that there isn’t anything simpler than k8s that’s production grade.Maybe it’s time to yolo with a regular container that just restarts on failures, ha… reply ctvo 18 hours agoparentI use AWS ECS (sometimes with ECS Anywhere to use my own servers + ECS&#x27;s control plane) for orchestration where I can.It&#x27;s a little surprising how many folks are unaware of the non-Kubernetes orchestration options. Going full Kubernetes is rarely what organizations need. Getting CI&#x2F;CD -> containers on servers and being able to configure the resources containers need, having the placement handled, etc. gets most of these folks 90% of what they want with much much less complexity.- AWS ECS: https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AmazonECS&#x2F;latest&#x2F;developerguide&#x2F;...- ECS Anywhere: https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AmazonECS&#x2F;latest&#x2F;developerguide&#x2F;... reply willthames 9 hours agorootparentDoes ECS support mounting configuration files (without needing the configuration file to be on the host)?Being able to mount secrets and configmaps into the container file system (without having to modify the container image to provide an entrypoint) definitely seemed to be one major advantage of kubernetes over ECS a few years back. reply ldjkfkdsjnv 18 hours agorootparentprevECS Fargate is a bit priecy, but is also a no brainer. It just works reply tebbers 16 hours agorootparentprevWe moved from DigitalOcean to ECS 2-3 months ago and it has been rock solid. Highly recommended. There is some initial setup required with container registries and wrangling AWS&#x27;s vocab but once set up, push to master, GitHub Actions builds and deploys. reply sofixa 19 hours agoparentprevDisclaimer: I work at HashiCorp, but have been of this opinion since before joining (source: https:&#x2F;&#x2F;atodorov.me&#x2F;2021&#x2F;02&#x2F;27&#x2F;why-you-should-take-a-look-at...)HashiCorp Nomad is a perfectly easy and flexible orchestrator (not only containers, also VMs, random binaries, Firecracker, WASM, etc.) that is production grade. It has a number of advantages over Kubernetes (being drastically easier to deploy, use and maintain; having integrated native templating with logic; being much nimbler but also being able to scale much more, etc. etc.) and it&#x27;s a very good choice in many scenarios. Of course there&#x27;s also disadvantages, most notably ecosystem size, but often that doesn&#x27;t matter. Kubernetes is great and extremely powerful, but also very complex and straight up overkill for many organisations. reply nh2 19 hours agorootparentWe&#x27;re using Nomad, and appreciate its comparative simplicity (we launch plain binaries created with Nix), but there are still basic features missing that can create struggles. The ones I hit are:* It can&#x27;t work down batch jobs in priority order [1] -- a feature even the ultra-old university cluster schedulers had* Sometimes job starts can be in arbitrary order, in the API and the UI, making it difficult to find the most recent ones [2]* Nomad seems to store pretty much everything in its Raft state, making it extremely slow to startup and operate (including GUI) if some rogue process created 100k allocations. Cleaning that up seems to require nuking the Raft state, thus also losing all info about all jobs.I don&#x27;t know how well Kubernetes does in these areas.[1]: https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;nomad&#x2F;issues&#x2F;12792 [2]: https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;nomad&#x2F;issues&#x2F;17742 reply k8svet 15 hours agorootparentSomeone I know once worked for the company maintaining etcd at the time and... they were appalled at how it was underpinning millions of dollars of VC dreams on it, implicitly via kubernetes, with the flaws and downsides, and operational complexities it has. And now to hear you say this of Nomad and Raft.I guess, my take away at this second is to really be skeptical of hype that I don&#x27;t have years of operational knowledge and experience with. Which, to be fair, regarding my current tech du jour (well, 9+ years), I have a list of complaints that could span a short novela. reply mati365 19 hours agorootparentprevNomad solution moves complexity from tech to license ;) reply wg0 10 hours agorootparentprevI&#x27;ve run both. First k8s is free and nothing beats free but that&#x27;s not about it.Nomad&#x27;s mental model is hard to grasp. Or let&#x27;s say is not as elegant. It&#x27;s is nowhere near k8s. And k8s at least from a usage standpoint can be explained in under ten minutes. Pod, Deployment, Service, Ingress and you&#x27;re pretty much done. reply stuff4ben 19 hours agorootparentprevyeah, but ya&#x27;lls licensing tho... reply orangepurple 19 hours agorootparenthttps:&#x2F;&#x2F;www.hashicorp.com&#x2F;products&#x2F;nomad&#x2F;pricingThere is an unsupported community edition reply dharmab 18 hours agorootparentIn case you haven&#x27;t heard, the next version of Nomad is moving to a non Open Source license. reply whoknowsidont 18 hours agoparentprevErlang&#x2F;Elixir screaming in the backgroundIt&#x27;s a wonderful, observable, manageable orchestration layer that&#x27;s easily and safely customizable if you&#x27;re wise enough to treat it as such. reply datadeft 4 hours agoparentprevThere is AWS Autoscaling, ECS, EKS, Hashicorp Nomad, Fly.io and the few more.However I would ask the question (same question I have been asking since the first appearance of k8s) what do you need so much that it justifies this complexity?Implementing a simple autoscaling system or using the provided solutions like AWS ASG was and is results in more reliable infra.If you want to use cutting edge there Lamda function that most cloud vendors offer. That is also less complex option. reply speedgoose 19 hours agoparentprev\"Don&#x27;t throw the baby out with the bathwater\".It’s just a bug in something most people don’t use. reply dewey 19 hours agoparentprevMaybe https:&#x2F;&#x2F;kamal-deploy.org could be worth a try there, as 37Signals almost moved all their services from Kubernetes I&#x27;d say it can be called production grade. reply bsdnoob 19 hours agoparentprevHashicorp nomad comes to mind. reply jarym 19 hours agorootparentIt does and most people that used Nomad were very positive about it. But with the turn that Hashicorp have taken I&#x27;m not sure that it now stands much of a chance. reply dwroberts 19 hours agorootparentprevI love Nomad but having used it in two different roles and now invested time to understand k8s properly, I would absolutely not recommend it.Nomad is simple on the surface and could have been a great tool - but it is basically unusable on its own without tighter integration with Hashi’s own tools (eg Vault). Configuring and maintaining all those things is nontrivial and ends up being more annoying (for a lesser end result) than just using a fully managed kubernetes cluster like GKE reply robertlagrant 19 hours agorootparentYou&#x27;d imagine that any scheduler that&#x27;s well integrated with Vault would have a huge advantage over other ones. Surprising that it&#x27;s not like that for another product from the same company. reply throitallaway 17 hours agorootparentVault is highly integrated with K8s via the vault-agent. reply jdoss 19 hours agorootparentprevSeconded. Hashicorp Nomad has been a breath of fresh air for doing HA deployments for my workloads. Getting a small cluster setup to self host Nomad is so easier than Kubernetes and defining workloads is much easier to understand too IMO.The only negatives about Nomad is the Hashicorp license drama that has happened recently and persistent storage can be a pain in the ass. reply hughw 19 hours agorootparentI hope somebody can elaborate on the licensing problem? I see a community edition... reply jdoss 19 hours agorootparentThey changed to the BDL from the MPL https:&#x2F;&#x2F;www.hashicorp.com&#x2F;license-faq which in FOSS culture is considered a dick move. reply gabeio 19 hours agorootparentprevhttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37081306They pulled the rug out on a lot of people and people tend to hold grudges. Community edition is peanuts compared to using an open source tool you _could have_ fixed your own bugs with by creating PRs or adding features with PRs. I used to be a huge hashicorp fanboy... _used to_. reply sofixa 18 hours agorootparentJust to clarify, you can still access the code, submit PRs to add features, etc. The only difference is that during a fixed time period (4 years) you cannot use Nomad to compete with HashiCorp. If you want to do that, you can ask for a license. replySpivak 18 hours agoparentprevThere is, you create a load balancer, an autoscaling group, put an ubuntu image in it, set the userdata to install docker and docker-compose up your desired containers.* Instance refresh to deploy new images.* Every so often update your AMI to patch.* Create an RDS database and hook the instances up to that if you need such things.It&#x27;s really hard to get more boring but works than this. It can be set up in a few hundred lines of terraform. I promise the code for your entire AWS account will be less than just the k8s YAML. reply turtlebits 16 hours agorootparentWhat happens when all your services won&#x27;t fit on a single box, or you need X more of a specific service? reply Draiken 15 hours agorootparentYou create a new instance and run the same setup.If your workload is extremely variable, something like k8s with auto-scaling makes sense. But a lot of applications (maybe the majority?) have very predictable loads and you don&#x27;t always need new services.When you&#x27;re at the size of large corps with hundreds&#x2F;thousands of services, then k8s is amazing. If you&#x27;re not even close to that number, it&#x27;s very likely premature optimization. reply secondcoming 18 hours agorootparentprevVM images aren&#x27;t cool enough!We do this but create custom images that have all their requirements installed via ansible.Unfortunately there&#x27;s now a push to move to GKE instead of GCE, for no real reason. reply nailer 18 hours agoparentprevAWS ECS reply acedTrex 19 hours agoparentprevHA k3s? reply uniformlyrandom 20 hours agoprevThe incident impact (nodepool upgrade issue) seems to be matching the speed of mitigation rollout. One does not want the cure to be worse than the disease; roll forwards should be slow unless the impact is high (and even then, it should be a rollback&#x2F;freeze rather than fast roll forward). reply water-your-self 4 hours agoparentHow do you propose rolling back a kubernetes upgrade that users might depend on reply tuananh 19 hours agoparentprevsorry what mitigation is that? reply shizcakes 19 hours agoprevWe&#x27;ve been stuck in this state for all 9 days. We&#x27;ve filed tickets, etc, but no resolution has come about yet. Just re-tried yesterday, still not able to update nodepool. reply talonx 18 hours agoparentThe incident dashboard does mention creating a new node pool with the new version instead of upgrading - as a workaround. Curious to know why this was not an option for you. reply wg0 10 hours agorootparentIf there are stateful workloads. reply talonx 6 hours agorootparentEven if there are stateful workloads, one has to be prepared to migrate at some point. Nodes can crash, become unreachable, etc. - workloads have to be able to survive such events.Edit: But I see your point - if their stateful workloads are not \"migration&#x2F;K8S-incident\" ready, it&#x27;s hard to suddenly build that overnight. reply danjc 16 hours agoparentprevIs it a big deal to just build a new cluster and cut traffic over to it via lb or dns? reply dilippkumar 15 hours agoprevNoob here with some meta-questions about developer and operations complexity.From an outsider’s perspective, it looks like in a 2x2 matrix of developer simplicity&#x2F;complexity and operational simplicity&#x2F;complexity, the current patterns all seem to be heavily biased for developer simplicity&#x2F;operational complexity.1. Is this assumption correct?2. Does optimizing for another quadrant: developer complexity &#x2F; operational simplicity make sense?My intuition is that complexity in code can be managed far better than complexity in operations. Developers have abstractions, reusable libraries, unit tests&#x2F;integration tests, etc. There may also be weird efficiencies that arise from having developers deal with some of these problems right from the design stage.It seems kubernetes takes a problem and pushes it to fully to operations.Is there a solution that takes this problem and turns it into a developer problem? reply diarrhea 13 hours agoparentInfrastructure is, or at least should be, code as well. And as it is, you can write tests for it all the same!However, writing those tests is incredibly hard. It doesn’t matter if you approach it from a dev or ops angle. The system under test doesn’t only have side effects, it is side effects. You also cannot mock most things (in my opinion…), as that is either also very hard to instrument or straight up removes the test usefulness altogether. Imagine mocking the AWS management API for your integration tests. Not possible.So what Dev calls integration or e2e tests, ops calls the dev environment. Works, but differently to how devs would do it. I don’t see an alternative.Next, as much as knowledge siloes are being heralded as evil, they exist. Undoing siloes altogether isn’t possible. You’ll end up reerecting them elsewhere. Devs have their skill sets, and ops isn’t part of that. The opposite is also true. The intersection can be substantial, but never enough to have dev to it all alone. I don’t think that’s a bad thing either. reply Too 6 hours agoparentprevThe difficult part about Kubernetes for newcomers is that it tackles every problem at once. Large scale best-practices you would previously sweep under the rug are now things you can’t avoid addressing. Centralized logging, secret management, certificate rotation, rbac, liveness probes, rolling deployments, etc. A lot of people complaining about the complexity are doing all those things by hand instead - “just ssh into the server with a shared password, remove old logs and restart it lol”.It’s great if you actually need all of this and consider those practices essential complexity. If you don’t - it feels like someone is shoving accidental complexity down your throat.Turning it into a developer problem will not hide all these things or make them more manageable. It means you will reinvent k8s yourself, poorly. reply mrweasel 19 hours agoprevIn Google terminology is a \"mitigation\" the same as a solution? I read it as \"Yeah, we still have no idea how to fix this correctly, but we have applied a temporary work-around\". reply yegle 17 hours agoparentDisclaimer: Google SRE working in Cloud but not related to the GKE product.Mitigation can vary: an additional firewall rule to stop certain traffic, a rollback to known good version, temporarily redirect traffic away from impacted data center. A mitigation is part of an incident response mainly focused on stopping the pain to whoever are impacted, it may not always be a long term solution. reply peddling-brink 18 hours agoparentprevI can’t speak for google, but in common parlance, mitigation simply means the problem isn’t affecting you any more. Doubtless they will perform a full post-mortem, identify the contributing factors, and fix them over the coming days, weeks, quarters.Edit: “we are assessing its effectiveness” lol, yeah that sounds more like they they are throwing something at the wall to see if it sticks. reply denysvitali 17 hours agorootparentWhich is arguably better than just waiting for something to stick to the wall by itself reply VirusNewbie 11 hours agoparentprevIt more or less means actions have been taken to reduce or eliminate the problem, but it is not the same as the long term fix. Mitigation might be &#x27;give more CPU&#x27; to the system, despite the performance regression still being in the code etc. reply timo-e-aus-e 2 hours agoprevoh man, you don&#x27;t wanna be the engineer on-call for that. reply vinni2 20 hours agoprevI have been pulling my hair to fix this all week. reply edude03 11 hours agoprevCreating a new node group works which is super easy on GKE so it&#x27;s pretty much a non issue. Definitely frustrating but not as bad as it sounds at first brush reply akokanka 20 hours agoprev [–] This shows the astronomical complexity of k8 systems even gods of k8 fail. reply Closi 19 hours agoparent [–] To be fair, running a Kubernetes Cluster is simpler than running a Fully managed Kubernetes As-A-Service Offering for thousands of clients. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google Cloud is currently facing a service issue causing Google Kubernetes Engine nodepool upgrade failures, which is impacting a small number of customers.",
      "Affected users may encounter an \"Internal error\" message in the Google Cloud Console as a result of this disruption.",
      "Google advises affected customers to either retry the upgrade or to re-create the nodepool at the new version, while assuring that mitigation efforts are underway. Updates regarding this situation will subsequently be provided."
    ],
    "commentSummary": [
      "Google Kubernetes Engine (GKE) faced a 9-day incident that sparked discussions on Hacker News about the difficulties of upgrading Kubernetes and its complex design.",
      "Highlights of the discussion included networking challenges, limited developer options, the employment consequences of lacking Kubernetes knowledge, and suggestions for simpler orchestration methods such as AWS ECS.",
      "Participants mentioned ECS Fargate and HashiCorp Nomad as potential substitutes, commending Nomad for its simplicity but noting its lack of certain features."
    ],
    "points": 174,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1696946349
  }
]
