[
  {
    "id": 40654190,
    "title": "Founder Liquidity: The Hidden Practice Reshaping Startup Risk Dynamics",
    "originLink": "https://www.stefantheard.com/silicon-valleys-best-kept-secret-founder-liquidity/",
    "originBody": "Silicon Valley’s Best Kept Secret: Founder Liquidity Ask most venture-backed founders why they get 10x more equity than employee #1, 100x more equity than employee #5, and 1000x more equity than employee #15, and you'll get the same answer: \"I'M TAKING SO MUCH RISK, IT'S SO HARD TO START A COMPANY, I MADE A BIG MOVE!!!\" And then you'll ask, \"but why are you yelling?” The narrative of the founder's risk is a cornerstone of Silicon Valley's mythology. Founders are celebrated for leaving stable jobs and pouring their lives into an “uncertain” and “high-risk” venture. This mythos justifies the enormous equity stakes founders hold compared to early employees who take very similar risks by joining an unproven startup. However, there's a lesser-known aspect of the startup ecosystem that significantly shifts the risk landscape: founder liquidity. My Experience in Early Stage Startups Being a software engineer who has a strong preference for creativity, problem-solving, and autonomy I realized during college that very big and slow companies were not for me. I joined a startup straight out of college as employee number 8 and immediately knew I made the right choice. My skills were improving week over week, I was responsible for shipping important features and was given a lot of responsibility right out of the gate. I eventually got pretty good at choosing strong founders to join and building great products from zero to one which in turn spawned a cycle of joining a team early, finding success, the company gets too big, and then I leave to do it again elsewhere. I have been an early or first engineer at five different companies and have had three liquidity events in a 9-year career. The Reality of Founder Risk & Liquidity Founder liquidity refers to the practice where founders sell a portion of their shares during a new funding round. This allows them to \"take chips off the table,\" securing personal financial stability while continuing to build the company with a fresh influx of venture capital. This practice is often kept under wraps, discussed in closed boardrooms, and only briefly mentioned in investor updates. You would really only know this happened if you were a founder, investor, or had direct access to the cap table. Why is it a secret that founders get liquidity in many venture rounds? Because it undermines the narrative of the founder who is \"all-in.\" The story of the founder who mortgaged their house and lived on ramen noodles for years is compelling. It garners admiration and sympathy, attracting top talent willing to work for lower salaries in exchange for a piece of the pie. If it were widely known that founders could de-risk their financial position while their employees remained all-in, it might change how startups are perceived and valued. This is a graph of cash compensation over time modeled off of a real scenario that happened over 4 years. This level of founder liquidity is fairly common. The founder in this scenario was offered $400,000 of liquidity at Series A and $750,000 at Series B and encouraged to do so by their board of investors to de-risk their own life. Liquidity was not offered to any employees and the fact that this happened at all was only revealed to people on the cap table. Another more well-known and extreme example was in the case of Adam Neumann the founder of WeWork - Neumann was able to cash out over 2B in secondary meanwhile not a single WeWork employee was able to capitalize on their equity stakes. They were told internally how much their shares were worth at each raise, and the hype surrounding each raise continued as WeWork sky-rocketed in valuation. Neumann was smart to de-risk his position by selling as much secondary as possible during the ascent but only attempted to structure a tender offer for non-founding employees in 2019 nine years after WeWork was created. That tender offer with SoftBank fell through and employees were left with absolutely nothing. (source) The part about these stories that feels unfair is not that the founders are getting liquidity - it's that they are the only ones getting liquidity. There are other stories like Hopin where the founder takes tens or hundreds of millions in secondary just to later sell the company for less than the liquidation preference stack and leave the employees with a grand total of zero dollars for their equity. Right-Sizing Perception There are a lot of odd perceptions surrounding founder liquidity: Investors and founders both tend to think that if employees knew founders were getting liquidity that that would negatively impact employee morale (it wouldn’t) Founders often feel guilty that they are getting liquidity (they shouldn’t) Investors think that the liquidity could taint the perception of future investors negatively (it doesn’t) Investors, founders, and employees all believe that founders are taking more risk than early employees (this isn’t true once founders have exclusive access to liquidity) When I found out that my founders got access to liquidity during our series A my first thought was “That is awesome, they deserve it” my second thought was “I wonder why employees didn’t get access to any liquidity?” and then my third thought was “Is this a secret? It seems like a secret. That’s weird”. I was the only employee who knew about it because I had incidental access to the cap table. Once I found out, I was curious if I was reading it correctly, so I immediately went to one of the founders and asked “Did you get a bit of liquidity during the series A?”. His reaction went from surprise to confusion and then he said “Yeah I did, a little bit”. I said “Wow that is awesome, congrats! It has to be nice to be able to backfill some salary after grinding for a couple of years” and he said, “Yeah, definitely.”. I could sense relief after chatting about it with him, almost like he felt better knowing that I knew about it. I never felt negative, had low morale, or anything of the sort, I trusted in my founding team and I was happy for them. If it were the case that I was lied to about it, then I would be upset and have low morale but that would be a result of being lied to - not liquidity access. Balancing the Scales As of 4 months ago I left a very successful stealth startup (which grew to 40M in ARR in two years) to become a founder and that is when it clicked - I expected to feel stressed, pressured, and the weight of all of the risk I was taking. What actually happened is that I realized I could have been a founder 6 years ago and I would have been taking a similar amount of risk as I did then as the first employee at tackle.io. My intention now, as a founder, is to balance the risk for early employees by being transparent, more generous with equity, and only taking liquidity if I can also offer it to employees as well. Our employee option pool is 20% which is double the average We have a 3-month equity cliff which is 9 months sooner than the average. We allow employees to exercise options up to 10 years after they leave instead of 90 days. Our equity packages vest over 3 years instead of the industry standard 4-year period. These changes are great, but nowhere near enough. In my view, every internal announcement of a new round at venture-backed companies should be accompanied by education and transparency around liquidity. Without transparency, none of the misperceptions have a chance of going away. The net result is that employees have a fundamentally misguided idea of the risk landscape as it shifts beneath their feet. If you work at a venture-backed company the next time a round is announced ask if the founders took any liquidity. Do it anonymously if you have to. This question should become so common that founders and investors become transparent by default. If they say no, great - no change to risk profiles. If they say yes - great, employees are operating with the same information as the founders and investors. This levels the playing field and allows employees to assess if they are still in a lower risk bucket than the founders, or if they are now taking significantly more risk than the founders. If employees realize they are taking more risk than the founders, maybe they'll ask for more compensation, maybe they'll congratulate the founders and move on with their day, maybe they'll start yelling: \"I'M TAKING SO MUCH RISK, IT'S SO HARD TO BUILD A COMPANY, I DON'T EVEN HAVE ACCESS TO LIQUIDITY!!!\". And maybe they're right. (special thanks to anu, jessica, erika, laura, derek, and minh for reading drafts and giving feedback on this)",
    "commentLink": "https://news.ycombinator.com/item?id=40654190",
    "commentBody": "Silicon Valley's best kept secret: Founder liquidity (stefantheard.com)1397 points by mooreds 15 hours agohidepastfavorite623 comments corry 3 hours agoThree interesting part of the discussion: (1) The opportunity cost to the founder of taking early liquidity: If a founder cashes out 10% of their position for $500k @ $25M Series A valuation, that de-risks a lot of their personal life. But when the startup ends up selling for $250M, that $500k of 'early' selling would have been worth $5M (less any dilution between rounds) - hard not to regret the choice in that case even if hedging is going to be the correct choice 99% of the time. (2) Meaningful vs. not-meaningful amounts: From my prev example, the founder sells 10% of their position for $500k. Well, if all employees were allowed to sell up to 10% of their positions too, would that even matter to them? If you were an employee and had $200k total value in your options, and you could sell 10%, you're getting $20k. Not really enough to de-risk your life although still might be welcome (and employees would appreciate having the choice). (3) Sellers need buyers: In order for there to be a seller of shares, there needs to be a buyer. The founder is effectively choosing his buyer and future business partner by taking investment and choosing to give that buyer more control over the corp by selling him even more shares (his personal shares). The buyer wants to make the founder happy and de-risk their downside so they can be more aggressive or big-picture or whatever, plus is happy to own more of the company assuming it's a hot round. But what does the buyer want to achieve by purchasing the employees shares? Just to own a little bit more % of the corp? For amounts that might not even matter for the employees and may de-incentivize them? It's all very complicated and perhaps there are nuances that make every situation unique. reply neilv 2 hours agoparent> If a founder cashes out 10% of their position for $500k @ $25M Series A valuation, that de-risks a lot of their personal life. But when the startup ends up selling for $250M, that $500k of 'early' selling would have been worth $5M (less any dilution between rounds) - hard not to regret the choice in that case even if hedging is going to be the correct choice 99% of the time. IMHO, it's very easy not to regret, with those particular numbers. I'd take $500K now plus possibly $45M later -- over $0 now and possibly $50M later. I'd take that deal even if \"possibly\" were \"guaranteed\". (Who might regret that is a founder who was otherwise already wealthy.) reply corry 1 minute agorootparentRegret is perhaps too strong of a word. But $5M is $5M even if you have $45M. Sure, it won't change your life since you have the $45M, but the incremental investing / philanthropy / estate / family help etc that it allows you is real in absolute terms. The other thing I've noticed is that for people on the other side of this transaction, it's not like \"smaller numbers\" all of a sudden become immaterial. $1M is still $1M. $5M is still $5M. Again, I'm with you, I don't think it's regret exactly. But post hoc you might choose differently, even if it's the rationale choice at the time. reply brightball 1 hour agorootparentprevExactly. About 15 years ago I was offering equity in a good little startup. I didn't take it because I just wanted to go somewhere with a higher salary. When they finally sold about a decade later I ran the numbers and determined it would have been about $40,000 based on the actual sale price. There's no guarantee of a $50M exit for anybody. reply remus 1 hour agorootparentprevNot to mention that in reality there is no guarantee you'd end up selling for $250m. $500k now would look pretty damm good if the whole thing tanks and the other 90% of your shares become worthless. reply simonebrunozzi 1 hour agorootparentprevYou are not taking into account QSBS. [0] When you sell your stocks before 5 years of holding period has passed, you pay significantly higher taxes. So you don't get 500k net, you get 500k gross, or probably 300k net. Which makes the de-risking less compelling. [0]: https://www.investopedia.com/terms/q/qsbs-qualified-small-bu... reply nine_k 29 minutes agorootparentThis is when you immediately liquidate your stock position, instead of taking a loan using it as a collateral, which would likely cost you 10%-15% in interest, not 30%. reply andrewmutz 8 minutes agorootparentNo, in this example the person sold equity in order to get the 500K. They can't use the equity as collateral for the loan because they dont own it anymore reply ipsento606 2 hours agoparentprev> hard not to regret the choice in that case even if hedging is going to be the correct choice 99% of the time in the scenario you outline the founder sells the remaining 90% of their position for $45MM? I don't think many people would experience any real regret at \"only\" getting $45.5MM instead of $50MM, due to declining marginal utility of money reply gwbas1c 18 minutes agoparentprevI think the most interesting part of the discussion is that the early employees almost always get the worst end of the deal: Going in they have a lower salary than if they work for a more established company. Then, either their shares end up being worthless, or at the final exit, they make less money than if they worked for a more established company the entire time. IE: Being an early employee in a startup is a lose-lose situation. This is something founders need to understand when recruit their early employees: These are often the most critical hires for the business, and therefore it needs a high probability of upside. IMO: A series of retention bonuses, and/or guaranteed bonuses at acquisition / funding events is a good solution. It's how I've sidestepped the equity issue when I was employed during an exit event. reply PheonixPharts 1 hour agoparentprev> hard not to regret the choice If you can't handle \"regret\" in these cases, then you probably shouldn't be in a position where you're deriving the vast majority of your income/weatlh from investments (which is fundamentally what a CEO does). It's astounding how many ICs can't wrap their heads around the concept that holding onto your RSUs make absolutely no financial sense. With rare exceptions, this doesn't make sense for anyone. And yet, fear for \"regret\" keeps people holding. But it's not shocking that even in tech many ICs are not good at reasoning financially. But if you want to be a co-founder, and hold a lot of your wealth in investments it's essentially that you learn to reason, plan and accept outcomes accordingly. Otherwise you're more-or-less a professional gambler. reply molyss 28 minutes agorootparentI’m going to rebound on that and explain why it doesn’t make sense to hold on to RSUs. Disclaimer: I’m an IC myself. I worked for my 1st company for 15 years. Held to their RSUs most of the time. Then moved to another (public) company and stayed there for a year before leaving. Now in a startup with a lower salary and no immediate liquidity on my stock options. When you work at a public company, you have multiple exposures to the company’s growth: the RSUs that have already vested, the RSUs that haven’t vested yet and through your own career growth and salary increase that goes with a successful company. If you were early enough, you also get market cred for having made the company successful. If the companies goes under (or shrinks, or lays people off), all those assets are at risk. Usually, one has more in granted stocks than in vested stocks. If your company just went public, you might have a lote more sellable than in your pipeline, but even that is unusual. Usually, you’ll still have more in the pipeline than you’ve already vested. If your company has been public for a while, you should get frequent refreshes, which means you still have a significant numbers of unvested shares. Regardless, you should sell as soon as you can, because of the remaining exposure through unvested equity. Use the proceeds to place in an ETF, or in a high-yield savings account, or some more aggressive investment strategy. Or use it for the downpayment on your house, or fund your kid’s college funds, whatever floats your boat. Anyways, keep in mind that you still have a significant exposure to the growth of the company through your unvested equities. If you’re worried about short-term cap gain, don’t be. If you sell immediately, there’s no growth between cost basis and selling price, so no cap gain. Another upside to selling is that you’re not bound by the blackout periods, so your assets are much more liquid. And remember you still have exposure reply skybrian 36 minutes agorootparentprevI believe in diversification and index funds for most people, but this seems overdone. The issue here is that sometimes if you procrastinate about diversifying, it pays off very well. As a Google employee (who joined after IPO), it was by far my best investment and funded my retirement. I guess that's accidental gambling. I did have other investments. reply lurking_swe 5 minutes agorootparentYes that’s accidental gambling. Or what i like to call “at the right place at the right time”. Ask a Yahoo employee how that same plan would have worked out for them. That being said, good for you. :) reply dmitrygr 1 minute agoparentprev> when the startup ends up selling for $250M s/when/in the very unlikely case that/g reply rybosworld 1 hour agoparentprevNone of this is very good justification for founders being the only employee that have the option to sell part of their stake. > If you were an employee and had $200k total value in your options, and you could sell 10%, you're getting $20k. It obviously depends on your financial situation, but having the option vs not will certainly matter to some employees. Not to mention that the stake could well be worth $0 in the future. reply earnesti 1 hour agorootparentI don't think it needs any justification, really. The investor decides, whom to sell to and how much. If the founder doesn't want to organize a sale for employees, then he doesn't do that. He would probably have to pitch it and include it in to an already complicated funding round. I totally understand why a typical founder doesn't want to do that. If for you as an employee it is a deal breaker, then you can complain about it, change company or whatever. It is not like the founder owes anything to the employees (unless he has promised that). Everyone in the equation are adults and have to decide themselves, if the position they are in makes sense for them with the terms they have. reply rybosworld 1 hour agorootparent> I don't think it needs any justification, really From a founder's perspective sure, you can do what's best for you. That's not what this article is about. This article is highlighting that there's a tendency in SV for founders to cash out early, and secretly. And along with that, there's a tendency to paint a narrative that the founders haven't sold a share. It's hard to see that as anything other than deceptive. It's one thing to join a startup that you know may not succeed in the long run. It's another to join a startup that has a founder whose been secretly cashing out along the way. Justification does seem necessary in that second scenario, at least from a morality perspective. reply thunkshift1 2 hours agoparentprevAlso bake in the fact in your calculations that 9/10 startups will not see the kind of success you are talking about. And the authors point still stands.. the founder made some money at liquidity event at round A vs … making even more money later if he doesnt sell? reply galaxyLogic 1 hour agorootparentBird in hand is better than 3 in the bush reply WalterBright 1 hour agoparentprevFor me, it's always regret: 1. If I buy stock, and the stock goes down, I regret buying 2. If I buy a stock, and the stock goes up, I regret not buying more There's no winning :-/ reply cashsterling 2 hours agoparentprevGreat points... as to #3, investors are often happy to be buyers. They are buying shares anyways that would otherwise have to be created. Allowing founders and employees to sell shares lowers dilution vs. creation of new shares... usually this is not a large effect, but still not bad for current & future investors. reply yuliyp 1 hour agorootparentI mean it effectively means that the amount of cash going into the business is less than it otherwise would have been. The company wanted $5M of cash. With the owner selling $500k worth of shares it means they had to find $5.5M to be invested. The only reason it happens is that the founder is negotiating both on behalf of the business and a bit for themselves. reply MisterBastahrd 1 hour agoparentprev$500K right now would pay off my home and do a good job of setting me up for retirement in the future. $5M when you already have $45M doesn't move the needle much. reply fragmede 1 hour agoparentprevto put it bluntly asf, you're being poor (and I'm being insensitive). what's $500k going to do for you if you come from a rich family? you already have your rent paid for until you die, and vacations paid for. all you have to do to do is put up with your annoying family, which isn't the worst if you've been through therapy. your mom or dad's abusive? if you've been through enough family therapy, that's not a problem. if you ask you mom or dad, whichever believes in you, they have enough money to fund your dreams (if you care enough to ask) of joining or starting a startup to become an (x) CEO/salesman/builder/marketer/whatever for whatever you want to build, and that includes signing onto some startup that won't pay you a living wage until it fail-exists for $5 million and everyone goes to burning man/Berlin/ibiza on the founders dime (including rent for everyone N months). reply Matticus_Rex 35 minutes agorootparentYes, there are people who won't get the same benefit from hedging like this. But they're a small minority. Not that many people meet your description here. reply joshuamerrill 41 minutes agoprevFounder liquidity events are done in secret in startup land. There's a simple reason for that. It's wrong. Startup employees, especially early ones, take on most of the risk that founders do. They take pay cuts. They work insane hours. They sacrifice. And they have the same liquidity needs, too. It's wrong to make them wait a decade for a fraction of the liquidity that founders got in the Series B. It's wrong to force them to absorb the risk of the Series B, C, D, E, F, and IPO. All while the founders were set for life years ago. If founders are going to take money off the table, they should extend the same liquidity offer, pro-rata, to their employees. Period. reply Finbarr 14 hours agoprevSecondary at Series A is very rare. Part of the reason more early employees don't get included in secondary sales is because of the Securities Exchange Act of 1934 14e-2. If you have more than 10 sellers involved, the transaction can be considered a tender offer, which triggers additional regulatory requirements and disclosures. > As of 4 months ago I left a very successful stealth startup (which grew to 40M in ARR in two years) to become a founder and that is when it clicked - I expected to feel stressed, pressured, and the weight of all of the risk I was taking. Please let us all know how that's working out for you in 5-10 years. 4 months in and no stress? Must be easy riding from here! reply onlyrealcuzzo 5 hours agoparentEspecially 5 years down the road when you own ~30% of a $100M company - but you know there's a decent chance you'll walk away with very little, if not nothing - while your peers are all making ~$1M per year working 6 hour days at FAANG with a life partner, maybe kids, and a sizable net worth that isn't going away. Sure, you've got a decent chance to rocket past them in wealth. But they've got everything they really want. You might have foregone your shot at a partner to build a company to mostly profit someone else who did nothing but write you a check. If you do have kids, you'll be old as hell raising them. All you'll have is extra stuff hardly anyone cares about - except maybe you - if you're the type of person chasing down a decimillion net worth. I hope these people truly enjoy their boats and their third homes in Aspen! It sure is a lot of work to get them. reply ditonal 2 hours agorootparentYou’re obviously overstating the FAANG SWE lifestyle. But beyond that, it’s interesting you picked FAANG SWE and not startup SWE as the basis of your comparison. The whole premise of the article is that startup employees are often sold a bag of goods about equity and upside that’s simply a terrible deal. Not terrible in the sense that it’s highly risky, but that it doesn’t even come close to compensating for that risk premium. Its sold as FAANG is low risk medium upside but startup SWE is high risks high upside but really its extreme risk and almost no upside because VCs find dozens of ways to carve it out. And people will say startups pay “market” compensation but they almost always mean base salary only, and the equity is such a horrible deal, it’s borderline fraudulent scam on the part of founders to sell startup employees on the equity as a fair deal. As an aside, when people think SWEs don’t need unions/ professional associations, they think of teachers unions or autoworker unions where pay is standardized on seniority. Instead, we could have something where our lawyers in our camp could review equity terms and we could collectively advocate for things like liquidity deals. That will never ever happen if you only trust the deals the VCs and founders offer. reply jakjak123 2 hours agorootparentThis 100%. Really the only reason to work at a startup as an engineer is if you really want to, because everyone pays low and the tiny bit of equity is essentially worthless in 99% of cases, which gives it a very low value. reply nirvdrum 1 hour agorootparentAnd, if you exit the company -- either voluntarily or involuntarily -- you often only have 90 days to exercise your options. If you've gotten laid off, eating into your savings while searching for a job is a pretty risky proposition. If you have an appreciable amount of equity, that bill can be rather high. Then there's AMT. Many end up letting the options expire. So, taking that pay cut for equity really didn't work out -- you had less money to exercise the options and because you couldn't, the option portion of your compensation was effectively clawed back. I very much appreciate the startups pushing to extend the exercise window out to 5 - 10 years, but that's far from the norm. I've debated this with a couple of investors and their stance is if you leave the company then you're not committed enough and shouldn't receive anything. I think that's quite debatable, but that's certainly not the case when folks are laid off. And we commonly discuss people thriving in one particular phase of a company. If you're not in that phase, it's no good for either the company or the employee to continue the relationship just to defer having to exercise options. reply Finbarr 44 minutes agorootparentThe 90 day exercise window is the most obviously broken thing about startup equity in my opinion. We changed this to 5 years at Shogun. reply Finbarr 2 hours agorootparentprevLet's not forget that FAANG companies were all startups at one point. Early employees at those companies experienced significant upside. Startups can be very high risk, and in rare cases, extreme upside. reply ditonal 2 hours agorootparentThis is the “startup myth” that lets the scam perpetuate. The world has changed. Google IPOed just a few years after it founded. Now Stripe, objectively one of the most successful startups ever, still hasn’t IPOed after 15 years. Liquidity preference Dilution Even the F in FAANG had a major movie made about early employees getting shafted by dilution! FAANG is 5 companies founded a long time ago. Since then VCs have completely rewritten the rules of the game. But they’ll still point to extreme outliers in the old rules. The fairy tale of the Google masseuse has probably cost tens of thousands of engineers millions in compensation. You need to get things in writing and do the math and startups make it as difficult as possible to do that and then the math never adds up. So they resort to fairy tales. reply Finbarr 2 hours agorootparentMany huge private companies, like Stripe, have found ways to provide liquidity to their employees without going public, e.g., through tender offers. Some more recent examples of companies where early employees did very well would be AirBnB, Coinbase and DoorDash. reply ditonal 1 hour agorootparentEarly executives at those companies did very well. Early employees did well, but risk-adjusted , not really. I know people who were fairly early at those companies and they own nice SFH in the Bay Area but they're still working as Directors or whatever. Consider that if you could make 400k (including liquid stock) in compensation at FAANG but you take 180k at the startup, you're basically betting 220k a year on the company. Except unlike any other company you bet 220k on, you won't get a board seat, you won't get access to key metrics, your influence will be dominated by \"real\" investor's influence. If your NW is less than 10M, which presumably it is, anyone who's heard even heard of the words \"Kelly Criterion\" would tell you your nuts for betting 220k a year on one startup. And yet, you get treated like \"an employee\" and not like \"an investor\" for taking that insane risk. So YC has invested in 5000 companies, and you can name 3 that had top-notch outcomes, thats 0.06% success - and you had to work like a dog to realize it! And that money was locked up. Those same early employees could have taken that $220k/ year, put it on Bitcoin or Apple stock, and retired off that. And Bitcoin and Apple were much easier \"picks\" than an given startup. The math simply does not add up and the whole system runs off mystique and naivety. And I've worked at startups that gave me a hard time about asking about outstanding shares, about asking about the cap table, about asking about liquidation preference. This is _critical_ information before you invest a significant portion of your life and net worth on a company and that they're guarded about and it should raise the ultimate alarm bells that they don't fall over themselves to explain every part of it. There's a bunch of propaganda out there \"Explaining ISOs, written by a16z\" that's a smoke screen of the truth. The math does not add up. The dream startup employee is really really good at Transformer architectures and really really bad at personal finance. Fortunately for startups, a shocking amount of these people exist. But it doesn't change that if sharp financiers looked at employee equity packages at startups objectively, every single one would agree it's a scam deal. reply Finbarr 1 hour agorootparentFirst of all, we're on the same page about the risk profile of working for larger companies being better for employees. But the reality is there aren't enough of those jobs for every single startup employee out there to get one. Some people also like the startup environment - move fast and break things, etc. Your denominator (5000) is _all_ investments that YC has made. You need to look at investments of a certain vintage, e.g., 10 years or more. You also need to include all the other companies of that vintage where employees did well (way more companies in that cohort have sold or gone public). The result is 0.06% is a gross understimation of the success rate (where success is defined as successful enough for early employees to make a lot of money). reply JakeTheAndroid 1 hour agorootparentprevI don't agree it's a myth. Is it an extreme risk? Yes, of course. Do people view the risks to be way too low? Yes. But I worked at Cloudflare pre-IPO, got shares at 1.73, and at one point CF was at 200 a share. That was more or less what I was \"promised\" from the equity. Stripe is one example of a successful startup not going public, but there are tons of startups that are going public. And there are many startups that wish they could go public, but they simply don't have the finances or business to do so. I don't think VCs changed much from when Google went public until COVID. We were seeing massive overvaluations of tech companies for years. Once through 2020, VCs got scared and now the landscape is a bit different. But the AI craze has started to get VCs back out of their shells taking bets on risky projects. So, yeah, idk what I agree with this assessment. At least it's not been my experience in tech over the last 8+ years. reply whiplash451 1 hour agorootparentprevCan you please explain in what way VCs have completely rewritten the rules? Asking genuinely. reply gopher2000 1 hour agorootparentprevThe argument that a startup could be the next FAANG is anchored in lottery-like odds. reply icedchai 4 hours agorootparentprevOr, based on examples I've witnessed, 5 years down the road you own 20% of a $1M company because your forecasts were off by an order of magnitude. You've gone through a couple down rounds, where investors took at least 20% each time. You feel obligated to your investors and employees, while there is almost zero chance of walking away with anything. reply htrp 4 hours agorootparentOne thing that is underappreciated in the startup mythos is just closing up shop and trying again. reply moneywoes 3 hours agorootparentdo investors allow that reply tomrod 3 hours agorootparentThey don't control it. reply nlh 2 hours agorootparentOne of the greatest quotes I've ever heard from a founder buddy was when his startup was going through a particularly dark moment and struggling: One of the investors said to him \"Maybe you should seriously think about shutting down and giving us our money back\", to which he replied: \"It's not your money anymore.\" reply icedchai 2 hours agorootparentYeah, then the investors call a board meeting and bring in a new CEO to provide adult supervision after a 2/3rds vote. The give that guy more equity than you to keep the ship afloat. \"It's not your company anymore.\" reply Finbarr 2 hours agorootparentYour daily reminder of the importance of maintaining board control. reply flyinglizard 46 minutes agorootparentNot realistic to maintain control past A unless you built a real rocketship. The board doesn’t usually want to run your company - they have enough other companies, some evidently better than yours as they don’t require this intervention. reply Finbarr 13 minutes agorootparent> some evidently better than yours as they don’t require this intervention. I’m not sure where that’s coming from. Also plenty of companies out there have control past the A. Finbarr 3 hours agorootparentprevThat very much depends on the stage of the company and how much control has been given up at different points. Do you think the management team of a public company could just decide to shut it down? As you raise consecutive rounds, your control is eroded. reply golergka 56 minutes agorootparentprevReputation is a thing. You might need investors in your next startup. reply throaway893 3 hours agorootparentprev>5 years down the road you own 20% of a $1M What a horrible fate. They only got five years of salary plus 200k extra. I'll include them in my prayers (just kidding, I don't pray). reply icedchai 2 hours agorootparentExcept the \"$200K\" is purely paper, and has an expected value of closer to zero. Remember, common shareholders are the last ones to get paid. Investors have preferences and get paid back first (often with interest.) Also realize you were probably forced to take a pay cut and have a below average salary due to cost-cutting measures from the board. We'll ignore the non-financial problems, like tons of stress, complaining employees demanding more equity because you couldn't give them raises... No, it's not a good situation. reply wordpad25 3 hours agorootparentprevThey also probably worked many 100 hour weeks while they could've earned more and worked a lot less with less stress reply rbranson 3 hours agorootparentThat's how risk works. The FAANG employee friends have exactly a 0% chance at a 9-figure outcome. They'll easily be at top decile if they pull a nominal $10M+ post-tax in 20 years. reply Thorrez 3 hours agorootparentprev>while your peers are all making ~$1M per year working 6 hour days at FAANG I highly doubt ALL your peers are making that much. And I think the people making $1M per year at FAANG tend to work much more than 6 hour days. You have to be very productive to get $1M per year. reply smokel 1 hour agorootparentThe \"Four Yorkeshiremen\" sketch by Monty Python always lightens my mood when this kind of bragging comes up. reply boringg 4 hours agorootparentprev\"Sure, you've got a decent chance to rocket past them in wealth.\" I might rephrase that as you have a non-zero chance. Odds are not that high and certainly not decent. reply whynotminot 5 hours agorootparentprevIt’s a shame you were forced to take on this burden and not allowed to be a regular engineer like your peers. reply Finbarr 4 hours agorootparentNobody is forced to become a founder. A lot of people are naive to the sheer level of stress involved, and think it’s going to be easier than it actually is. You don’t find out just how stressful it is until you’re already super committed, have raised money, have employees, and there’s no easy way out without screwing a whole bunch of people over. Founders tend to only talk about the good things happening at their companies, and tech press tends to focus on the successes. These things contribute to more people starting companies. reply ipaddr 3 hours agorootparentIf you can't stomach screwing people over you shouldn't be a CEO. reply seansmccullough 3 hours agorootparentprev> while your peers are all making ~$1M per year working 6 hour days at FAANG Unless you are a manager, in which case you are working more like 12 hour a day. reply slashdave 1 hour agorootparentprev> there's a decent chance you'll walk away with very little Well, some founders care about their employees and their idea, and the idea of the start up failing is much more than just money. reply dasil003 4 hours agorootparentprevYeah I feel like successful founders natural ambition and optimism is sort of weaponized against them by the VC industry here. From a VC perspective it's worth playing the odds for moonshots. As a founder though, if you can create a $100M company that you own 30% of, you can probably create a $20M company you own 70% of with a much more realistic and sustainable growth targets. I can't help but feel this would be better for the founders, the employees and the customers of the company. It just doesn't make as much sense for the investors. reply foobarian 4 hours agorootparentprev> making ~$1M per year working 6 hour days at FAANG Can you say more on this? I didn't realize FAANG TCO was quite that high. Maybe it's time to swallow some pride and take the adtech money after all... reply onlyrealcuzzo 4 hours agorootparentThe average SUCCESSFUL founder is in their earlier 30s. At that point - you should be at least L4 (probably L5) at FAANG. Salaries are about ~$450k at that level and age. In 5 years, if you work even a fraction of as hard as you need to be a successful founder, you should be L7 - salaries are usually >$800k at that point. No, it is not like any average slacker straight out of college in 5 years can get to a $1M salary at FAANG. But if you're the type of person that could successfully grow a company to a multi hundred million valuation in 5 years - you can make $1M at FAANG. reply Xcelerate 1 hour agorootparent> But if you're the type of person that could successfully grow a company to a multi hundred million valuation in 5 years - you can make $1M at FAANG. Disagree. Totally different skill sets. Not saying there is no correlation at all, but probably less than one might think. reply dclowd9901 3 hours agorootparentprevBig caveats on these numbers: 1. You’ll have to be located in SF or Seattle. 2. Going from L5-L7 is _not_ trivial. It requires a somewhat miraculous combination of being on a productive team with a good boss, a lot of opportunities for showy work and your own gamesmanship around corporate politics. Is it possible? Sure. But in my short stint at Amazon, I met a lot of people who should have been higher level and were simply not due to missing one of these factors. reply onlyrealcuzzo 1 hour agorootparent> Going from L5-L7 is _not_ trivial It is trivial compared to growing a company successfully from $10M valuation to $100M valuation + an exit. reply dclowd9901 1 hour agorootparent[citation needed] Frankly, many more aspects of trying to grow your career from l5-l7 are out of your hands than they are when you're at a startup. reply onlyrealcuzzo 1 hour agorootparentThe vast majority of startup success is luck... There are literally thousands of people going from L5-L7 at the major tech companies per founder successfully exiting a >$100M company. reply kolbe 3 hours agorootparentprevThose aren't entirely overlapping skills. There are plenty of founders whose attitudes and generalist skill sets make them unhireable in the management ranks of FAANG. reply foooorsyth 2 hours agorootparentprev>you should be L7 The distribution of the ladder is logarithmic. Most never make L6. L5 is often terminal level IC without any “up or out” obligations. Lots of people spend a long time at L5 and retire. reply Thorrez 3 hours agorootparentprevlevels.fyi says Google L4 in the Bay Area is 306k total comp on average. https://www.levels.fyi/companies/google/salaries/software-en... reply kilbuz 4 hours agorootparentprevYou don't start there, but you can get there as you level up. A lot of that would be because the stock on your RSU grants goes up while you work there though. I don't think many SWE have 7 figure targeted comp (highest levels, yes). But plenty get there with refreshers and stock appreciation. reply kolbe 3 hours agorootparentExplain the math on leveling up. Each year, Meta hires more Jrs than there exist L7+'s at the entire company. reply throwaway-blaze 3 hours agorootparentMany people top out at a lower level because they don't play corporate politics games or because the L7s aren't moving on, so there's no real room for promotion among the L5s and 6s. Microsoft in the Ballmer years (early/mid 2000s) had this problem. Promising L65/L66/L67 (probably equiv to L5/6 at AMZN) would leave because the next step was full. All the \"partners\" were hanging around and not making room for the next gen of leaders. reply gopher2000 1 hour agorootparentprevSee levels.fyi. The pay levels for FAANG companies are fairly accurate. But you'd have to be something like L7/E7 level at a Meta/Google to break $1M. Also note that some comp numbers get heavily inflated by people incorporating stock value increasing between the equity was first issued and the stock actually vested. reply chucksmash 3 hours agorootparentprevs/deci/deka/ reply dclowd9901 3 hours agorootparentprevOk, please argue in good faith here. Maybe 1 or 2 people who aren’t executives are pulling in that kind of money from FAANGs. reply wordpad25 3 hours agorootparentIf you're CEO at a $100MM company, your peers ARE executives at FAANG reply rbranson 3 hours agorootparentprevIncorrect. See: https://www.levels.fyi/companies/facebook/salaries/software-... reply dclowd9901 1 hour agorootparentI was under the impression we were talking about ICs here -- your link shows that an upper-middle IC that you'd expect to be choosing between early startup or FAANG will see something around 450 a year, which tracks much closer to what I'd expect. reply gopher2000 1 hour agorootparentYou said 1 or 2 people who aren't executives. There's way more than 1-2 E7s at Meta, as an example. reply algobro 3 hours agorootparentprevSir if you live in USA and do not take a dip into the VC money swimming pool, you are stupid, because crazy people with stupid ideas routinely get to $100 Million valuations, like no other place on earth. Its like going to Disney Land and saying \"Oh i'll just sit at the coffee shop\". Some people are here for the ride. Some people like the 9 to 5. Like you, obviously. Why dont you go start corporate-drone-news.org, this board is for hackers and founders. reply alexvitkov 2 hours agorootparentI don't mind the shit take, but please don't use underscores in your domains. reply algobro 2 hours agorootparentGood catch, fixed. reply throaway893 3 hours agorootparentprevYou are completely detached from the real world. Even in super rich countries like the US there are a lot of people without savings, living paycheck to paycheck. Most/all software engineers outside the US can only dream of ever earning that much money. And yet here you are, worrying that you'll end up only slightly richer than people earning ~$1M per year. reply Thorrez 3 hours agorootparent>And yet here you are, worrying that you'll end up only slightly richer than people earning ~$1M per year. onlyrealcuzzo is comparing 2 things: * Being a founder and working mega hours and having no life (e.g. no spouse or kids) and having a decent chance of losing most of your money as the company fails. * Working at FAANG and making a lot of money while not having to work too much. onlyrealcuzzo is saying the second option is better. reply RhodesianHunter 3 hours agorootparentprevI don't think you fully understand the context in which this was written, but you probably should before passing so much judgement. reply Finbarr 12 hours agoparentprevThe bigger secret is that stock sold in secondary sales by founders and employees is usually common stock, and the purchasers will often get the right to convert this to preferred stock. This means that the company is instantly encumbered with a greater liquidation preference, without the increase in balance sheet to offset it. reply laser 10 hours agorootparentHow is that legal and not considered self-dealing and unjust enrichment? If I was a minority common stock owner in a business I assume I would have standing to sue for damages if a majority owner or officer made my position materially worse while enriching themselves in such a manner? Are you sure such a right is typically granted? I mean even the gap between 409A valuations and preferred valuations, as well as a huge amount of precedent, give a different material value to preferred and common stock. Giving that right out of thin air in a sale by an insider is effectively theft from common holders and I have trouble believing what you’re saying as I’m not sure how that could be kosher, if perhaps infrequently litigated. But is it really standard like you make it sound? That would be a very dirty secret and I expect would and should lead to litigation. reply mountainb 6 hours agorootparentWho has the cause of action? The majority shareholders. Who authorized the stock sale? The majority shareholders. Are they really likely to sue the founder for something that the shareholders authorized? Only in some states would minority shareholders have a cause of action. So there are some states in which the courts agree with you. As you might imagine, startups do not typically incorporate in those states. reply lancewiggs 10 hours agorootparentprevFlip it around - it becomes a condition of the deal happening imposed by investors, who themselves are motivated to present the best deal to founders, and to have founders less economically stressed. No secondaries - no deal, and that doesn’t help anyone. reply Finbarr 4 hours agorootparentprevIt is very common and usually a condition of closing. Investors know that preferred is way better than common. They are buying highly speculative assets and want strong downside protection. reply red-iron-pine 4 hours agorootparentprev> How is that legal and not considered self-dealing and unjust enrichment? because, ultimately, Capital writes the rules, and they chose to allow this reply KingMob 7 hours agorootparentprevIANAL, but if you only have options, and not stock, do you still have standing to sue? reply anxman 10 hours agorootparentprevI used Founders Preferred shares to get liquidity at the A (for a now defunct startup). In our case, we offered all vested employees the option of selling in the same round on the same terms. I personally don’t recall any disclosure requirements at 10 people; however, we didn’t have that many participate so perhaps it didn’t apply. In general, Founders Preferred does layer on the preference stack but also hopefully by a relatively trivial amount to the overall funding size. reply throwaway-blaze 3 hours agorootparentFounders never have preferred shares, at least not the same class of preferred (with the same preferences) as investors. reply janjongboom 3 hours agorootparentNot never. E.g. all the capital we as founders put in the business before we raised our seed round was converted into Series Seed Preferred shares at the same rights as angels / seed VC. Small portion of total equity but still. reply Finbarr 5 hours agorootparentprevYes, as I mentioned it only applies when you have 10 sellers. reply burutthrow1234 6 hours agoparentprev> Please let us all know how that's working out for you in 5-10 years. 4 months in and no stress? Must be easy riding from here! Honestly VC-funded startups seem like a cake walk compared to actually starting a small business. Your biggest challenge is walking into a room full of rich dudes and schmoozing for your pay cheque. If you fail you get acquired and get golden handcuffs. If you start a real business you can expect to take on debt, and you'll be personally guaranteeing it because nobody cares about equity in your boutique ice cream parlour. Plus a 5-year lease (which you will also personally guarantee). reply joenot443 6 hours agorootparentThe most common endgame for a startup is slowly running into the ground until the money runs out and you eventually shut the doors. Failing your way into a happy acquisition isn’t really something to expect as a contingency, I don’t think. reply tmpz22 2 hours agorootparentThe contingency isn't golden handcuffs its using one of the hundreds of C-level connections you made as a Founder doing sales and networking (and accelerator programs) to get you a cushy gig as a Product Lead, Operations Lead, or similar title with a strong paycheck and immediate authority. reply BeFlatXIII 3 hours agorootparentprevThen use those five years to enjoy the fanciest office equipment VC money can buy. Don't cry because it's over; smile because it happened. reply mrkurt 4 hours agorootparentprevNot that people with VC need defending, but: Sure, if you magic up a startup with VC funds you suddenly have it easier than a small, bootstrapped business. Startups almost never start with a round of VC though. There are almost always months or years of the same experience as a bootstrapped business (ie: extreme uncertainty, no money to pay yourself, etc). Most startups don't manage to raise VC, and most startups that raise VC fail with no acquihire. reply p_l 6 hours agorootparentprevRunning with VC funds? Oh god, that would be cakewalk compared to even just figuring out how to ensure there's food if you spend money on some necessities for prototype... reply WarOnPrivacy 5 hours agorootparentprev> Honestly VC-funded startups seem like a cake walk compared to actually starting a small business. Make this about any brick/mortar businesses and the stresses multiply by another factor. If they're in a federally regulated biz (compliance) or an insurance dominated state (rates, inspections), then multiply again. reply burutthrow1234 1 hour agorootparentThis is a comment about brick and mortar businesses, I literally talked about having to personally guarantee a multi-year lease in the post. And yes, some businesses are even harder due to regulatory requirements reply boringg 4 hours agorootparentprevI don't know why you are trying to make this a me vs them situation. Both situations are difficult in different ways and they are all real businesses. \"Your biggest challenge is walking into a room full of rich dudes and schmoozing for your pay cheque.\" - Sounds like you are trolling or alternatively incredibly naive. \"If you start a real business you can expect to take on debt\". ... Real business? Come on. No one in this thread is saying starting a business is easy - ice cream business is debt funded because you have a very definitive range of outcomes. Venture funding is completely different animal - failing to see that limits the value of your comment significantly. reply owlstuffing 3 hours agorootparent> I don't know why you are trying to make this a me vs them situation. In terms of economic disparity it _is_ very much an us vs them situation. Consider the optics over the last 20+ years. The middle class and their small businesses have been decimated while former VC funded companies hoover up their futures on Wall Street. The level of risk involved starting an average small business is much closer to home compared with a startup seeking VC funding. The former can literally lose his shirt, the latter has to settle for a high six figure salary somewhere else. Failing to see that limits the value of your comment significantly. reply boringg 3 hours agorootparentThis isn't a me vs them. Who is hoovering up the ice cream futures? Different business model, different businesses. Both are difficult. Being a founder of a VC based company is difficult and being a builder of a retail brick and mortar is difficult. It isn't zero sum and both can exist in the same economy trying to make this a Me Vs Them narrative is totally BS. Making a wedge where there isn't one is disingenuous. reply dclowd9901 3 hours agoparentprevCorrect me if I’m wrong, but my perception of most startups at series A is that they’re not usually more than ten-ish employees, and even then, you’d expect more balanced comp packages for employee number 11+, no? reply colordrops 11 hours agoparentprevI often hear about these SEC rules that explain why individual contributors get fucked, as if that's a good excuse. Either the requirements and disclosures should be fulfilled and more than 10 sellers allowed, or the rules should change, or both. reply llamaimperative 8 hours agorootparentIt sounds like they could’ve fulfilled the requirements and had more than 10 sellers but chose not to. reply Finbarr 5 hours agorootparentprevI didn’t comment on whether it was a good reason or not. My comment was just highlighting some of the complexities in what the blog author was hoping to achieve. reply sneak 10 hours agoparentprevWhere would the stress come from? You get a paycheck and there is no personal downside except opportunity cost (and perhaps reputation). You don’t lose any money if your startup fails. reply Finbarr 5 hours agorootparentHiring, firing, layoffs, making the wrong decisions with limited information and not finding out they were wrong until years later, huge shifts in the tech market around you undermining your business, competitor actions wrecking your business, pressure from investors, pressure from your family to earn more money, uncertainty about whether the business will ever succeed, and an endless list of other things. > You don’t lose any money if your startup fails. Except all the money you lost by not having a proper job along the way. Also it’s not uncommon for founders to float the company at early stage until investment is raised, and they don’t always get a refund for this. reply jboggan 2 hours agorootparentExactly. I quit Google in 2017 to work on a promising start-up idea (generative AI chatbot for coding, a tad early on that one) and ended up raising barely any money, running up massive CC debt to finance cost of living and GPUs, and taking a huge compounding opportunity cost to not continue growing as a FAANG SWE (not to mention missing out on the stock market run with the extra money I didn't have). I spent the last several years paying off that debt instead of buying a house or investing, etc. I'm massively behind in earnings and net worth compared to my colleagues who talk about their future startup idea but never struck out on their own. But I'm finally debt free and ready to risk my future yet again on another startup. reply neilv 6 hours agorootparentprevI tend to have large stress in startups, more than in established companies. I won't get into some of the occasional toxic-element sources that can happen anywhere, but some reasons that happen more in startups: 1. Caring about the mission -- the real-world positive impact -- and potentially able to make or break that. Not just taking a shot at making money, for some opportunity cost that I could evaluate quantitatively on a napkin, and walk away from as soon as an option with a higher expected dollars number came along. 2. Livelihoods and investments of time&effort by colleagues hinge to a large degree on decisions I make, ideas I have, and things I have to pull off, and not wanting to let them down. (A bit similar with money investors, but I care more about personal connections, and involvements where it's not just someone buying lots of lottery tickets.) 3. Low \"paychecks\" for my HCOLA, at that startup and earlier, so personally needing a big win financial exit, and the startup is what I decided to invest my time&energy into. If that fails, it's starting over, and a lot of wading through various startup ickiness to get another good opportunity (or doing FAANG interview BS, and then their promotion-chasing BS). reply angio 10 hours agorootparentprevA lot of people (esp people that performed extremely well in school and in corporate environment) find \"failing\" and \"losing reputation\" very stressful. reply FactKnower69 8 hours agorootparentI guess harden the fuck up? reply smeej 7 hours agorootparentExactly. Or just don't do it. I am sure enough that I would crumble under that specific kind of pressure that I don't put myself in situations where I would experience that specific kind of pressure. Works great! reply SOVIETIC-BOSS88 7 hours agorootparentprevThank God someone said this. Of course it is stressfull, there is no free lunch. If you can't take the heat just dont enter into a such top-heavy game. reply verticalscaler 7 hours agorootparentnext [2 more] [flagged] apantel 3 hours agorootparentAnd change. reply sneak 10 hours agorootparentprevLandlords and supermarkets dgaf. There is no real risk, and if they stress over it, that’s more a founder’s own psychological failing than anything else. If you care what other people think that much, you probably don’t have sufficient quantities of the oft-cited “grit” that founders supposedly require. reply zztop44 8 hours agorootparentI think if the idea of your company failing doesn’t cause you at least some stress then you probably shouldn’t be running a company? reply epolanski 6 hours agorootparentprevMaybe risky ventures aren't for them. reply throwaway98797 6 hours agorootparentprevcause if you fail you have to let people go cause if you fail you have to tell your investors you lost money cause if you fail is a thought that’s always running through your head as you live it reply LargeWu 2 hours agorootparent\"cause if you fail you have to let people go\" This isn't the founder's risk. It's the employee's risk. And it has the added bonus of, if there is a liquidity event, the employee's don't get the upside. I was like engineer #3 at a company that eventually was acquired for ~$250MM. My payout was $60,000, after 5 years of employment there. I could have made more by going and contracting at megacorp for a single year. There was never any upside for me. reply yard2010 5 hours agorootparentprevThis is not a real risk you're talking about, but small inconveniences. A risk is losing your house for example, or losing the ability to rent. Inconveniences are part of life anyway. Being the first engineer means you get all these inconveniences (tell your wife and your kids) plus real risks as above (taking a loan to buy the options and losing it) reply naravara 5 hours agorootparent“Letting people go” is taking on the risk of all of those people being let go losing the ability to rent or pay their mortgages. That seems like more than an inconvenience to me if you take one of the responsibilities of being an employer at all seriously. reply zdragnar 5 hours agorootparentNo employee should join a startup with the expectation that the company will be around forever. Compare startups to restaurants- their failure rate is absolutely massive. Working for a new company is simply always a risk for everyone involved, there's no getting around that. reply sneak 5 hours agorootparentprevIf you are working a tech job and know how to program computers and have no savings slash the loss of a job costs you your house, you have deep and fundamental problems far beyond the loss of one job and it isn’t your former employer’s fault that your life is mismanaged. reply shortrounddev2 6 hours agorootparentprevMy primary motivation as an employee of a startup is fear of personal financial ruin. That the company won't be able to make payroll and I won't be able to pay my rent, that I'll be evicted eventually or that if the company goes under I won't be able to find a new job. There is no mission or any other soft carrot that I care about. I also don't have any faith in stock options. I can't imagine caring about reputational damage with rich people unless that reputation is in service of not starving in the streets. reply sashank_1509 5 hours agorootparentPerhaps you shouldn’t be working in a startup because your lifestyle is unaffordable, or your company is paying you peanuts. I have worked in startups in Silicon Valley and have had many friends working for them. Most startups pay a base salary of around 200k$ I reckon (for new grads, perhaps 150k). This might come down to 9-10k after taxes per month. A good 2 bedroom house to rent in a location like San Jose would be 3k$ per month, which leaves you 6k for other expenses. Assuming 1k for car, you should still have 5k in savings per month, in a year of working you will have saved up 20 months of rent, maybe 12 months of living without a job. I find it hard to believe anyone in SV startups, is in risk of “personal financial ruin”, or “starving in the streets” just because they lost a few months of paychecks while searching for another job. That may be true in another country, in another market, but all tech workers in the Bay Area are living well above subsistence and acting like they are living paycheck to paycheck is a fantasy. There is a cost to working in startups, and it is an opportunity cost of not working in a big tech company and cashing out your 200k+ RSU over 4 years and instead receiving paper money stock options that can be worth 0. reply lettergram 6 hours agorootparentprevStarting a company myself, I took 6 months with no salary. After we raised, my salary was massively cut from where it was (30-40% of what I made the year prior). Then you have the fact I gave up guaranteed raises & promotions (to the tune of hundreds of thousands in RSUs). There’s a pretty large risk to family security. By year two of the startup I have made 15-20% the cash I could have made elsewhere. I have stock that I trust will be worth more in the future (so imo worth it). However, I can see liquidity events being useful if you’re tight on cash after that run reply erikerikson 3 hours agorootparentInvestors do it so that the founder can better focus on increasing the value of the company. Having financial stress on top of everything else reduces the probability of liquidity events. reply rohansingh 7 hours agorootparentprevI think for a lot of founders, there is significant financial cost or opportunity cost upfront. Especially if you are bootstrapping. reply boringg 4 hours agorootparentprevI mean if you don't care about the company mission (if it's mission based), you don't compare about your employees, your word, you don't care about your time or care that you sold people that you were going to take their money to build something... then yes there might be no \"personal\" downside. Though if you don't care about anything in the first place what are you doing trying to build a company? reply josh2600 15 hours agoprevThe best startups have a concept which is summed up thusly: “We all go to the pay window at the same time.” It’s ok for founders to take a little bit of money off of the table if they extend that to their employees as well. Asymmetry is where things get weird. I’ve seen many founders who got deep into the fundraising cycles without ever realizing they could take a cent out. VCs will constantly tell you to let it all ride, and sometimes that works out, but for most people, having a little bit of financial security while you’re trying to change the world is necessary. The best startups figure out how to manage liquidity through financing in a way that aligns incentives, keeps the goalposts at the mission, while allowing their teams to thrive. It’s about alignment. If everyone is pulling in the same direction you’re going to execute the vision. Whether you win in the startup lottery is up to the threads of fate, but alignment is the straightest path towards a result. reply kneath 14 hours agoparentI have seen a lot of companies, a lot of rounds. I have known zero founders who have turned down an option to take money off the table (and zero A raises that offered that to employees). I love the idea of your universe, though. reply bradleyjg 7 hours agorootparentAll you’re saying is that in the contemporary context it’s exceedingly foolish to be an employee at an early startup. The VCs and founders have optimized away all the incentive. Eventually the message will reach even naive 22 year olds. reply smeej 7 hours agorootparentI'd tweak this slightly: \"It's exceedingly foolish to be an employee at an early startup for the money.\" I think there are a lot of us who struggle to fit the larger corporate mold who pretty much only thrive in the startup world. I can't speak for all of them, but I've been very willing to take the balance of lower cash compensation and a fistful of lottery tickets and not having 12 layers of middle management breathing down my neck over more liquidity. I guess I'm also blessed with inexpensive tastes, which helps, but I'm still able to live somewhere I love and do all the things I care to do, so it works out. reply p1esk 6 hours agorootparentWhy does everyone thinks startups don’t pay well? I have worked for various startups all my life, most of them well funded, and competing for talent with faangs. Yes, I could probably make more at Google but I don’t feel like I’m underpaid. At the last 3 startups my base salary was above 250k. I work remotely and I rarely work more than 30 hours a week. reply bradleyjg 4 hours agorootparentEarly startup is the part you seem to be overlooking. A well funded startup with few or no runway concerns is a different calculation. reply bradlys 4 hours agorootparentprevI’d say you’re uncommon. I’ve never seen anyone who is a typical engineer making $250k/yr at a startup that’s below $1B valuation. Same for the amount of work you’re doing and that it’s remote with that compensation. It’s possible you’d be making $700k+/yr if you were at google. About triple what you are now. reply gen220 3 hours agorootparentI think one component of their point is that the marginal utility of money beyond $200k/year cash comp is quite small, especially if you (1) came to tech early in life (2) plan on staying in it for most of your working life. With that perspective, $200k/year and $700k/year both reduce to \"well-paid\". Also, a Staff title at a Seed or Series A startup can definitely ask for $250k/year, although they'd likely be trading off against equity grants. reply whiplash451 1 hour agorootparentI would revisit that calculation assuming you are drained at 45 instead of 60, including taxes and the opportunity cost of 500K x a few years at 3% rate for the next 15 years. reply gen220 43 minutes agorootparentThat's pretty much exactly the calculation I'm positing. But actually with an even earlier terminus (late 30s or 40 at most). Assume an \"effective\" average pay (i.e. \"net\" pay + retirement and other deductions, inflation-adjusted to today's dollars and averaged over the course of your career) of $120k/year. From age 22 to 40, you've earned $2.16mm in inflation-adjusted-to-today dollars as a single earner. With a not-unreasonable average savings rate of 30%, not accounting for tax-advantaged growth or any growth at all, you'll come out with $650k of inflation-adjusted-to-today capital in savings. Realistically, this should end up invested in some kind of equity (housing, stocks, bonds, whatever). If you finance the purchase of a house at 30, you're only 10 years into a traditional 30-year mortgage at this point, for reference. So you're roughly 1/3rd of your way to owning all the equity in your home. That's fairly comfortably a $1mm home (home equity being 30% of your assets at 40). Of course, if you're DCA-ing into something that yields a modest average of 5%/year in inflation-adjusted returns, that $650k is closer to $1mm inflation-adjusted-to-today capital. And you still have 25 years at that point for your retirement savings to compound. And you can work part-time in something more fulfilling until retirement to supplement your income. YMMV, but the marginal utility of money beyond $1mm in equity at 40 and $6k/month in expendable (on rental housing, food, travel, social events) income during your 20s and 30s is pretty small for most people. If you add a partner with any kind of income to the mix, it makes the marginal stress of earning more money even less appealing. Edit: the main thing you ought to avoid like the plague is lifestyle creep. Spending money on things with zero or vanishingly-small happiness ROI. Read this story every year or two, or whenever you get a raise at work. https://www.marxists.org/archive/tolstoy/1886/how-much-land-... reply p1esk 2 hours agorootparentprevWhat many people don’t seem to realize is there are a lot of early stage but already well funded (10M+) startups who are desperately looking for top quality people. Once I was approached by a founder who offered 500k base salary (wasn’t a good fit for my area of expertise). reply shortrounddev2 6 hours agorootparentprevIt's a difficult trade off I've found. Large tech companies are boring and slow and you deal with a lot of red tape and BS, and you feel utterly powerless in the security of your own job as economic tidal waves direct the momentum of layoffs and not your personal contribution. At a startup you have more autonomy and power over your personal position. I wrote 90% of the code that is generating company growth, released 2 months after a layoff. If I had taken longer to release that code or if my code didn't work the company would be in a worse financial position. But that also means a lot of personal stress. There aren't 4 layers of middle management to catch flak for you. If you fuck something up, you are directly responsible and depending on the environment that can result in some heated conversations. I also work way harder at a startup than I ever did for a big company reply smeej 49 minutes agorootparentThose are the factors that make the tradeoff easy for me. I would vastly prefer direct accountability for my own fuckups, because that means I have the agency to do something to fix it. What makes me want to put my head through a wall is when I fuck up, and four layers of people above me are the only ones allowed to fix the thing, but they don't, so I keep catching flak for my fuckup without any way to stop it and fix the thing. I have many more heated conversations with those managers, which typically leads to the door. When I fuck something up, rarely is anyone more upset about that than I am. Nobody's dumping more heat on me than I am on myself, so bring on the heat-- as long as I have the agency to fix the problem. reply tivert 2 hours agorootparentprev> All you’re saying is that in the contemporary context it’s exceedingly foolish to be an employee at an early startup. The VCs and founders have optimized away all the incentive. Eventually the message will reach even naive 22 year olds. My startup idea is a firm that uses generative AI to flood the internet with pro-startup, pro-VC, pro-founder propaganda, so that message will never reach the naive 22 year olds. Personally, I think it's like saving the environment, since naive 22 year olds are precious resource we cannot allow to be destroyed. reply ethagnawl 7 hours agorootparentprev> All you’re saying is that in the contemporary context it’s exceedingly foolish to be an employee at an early startup. As a rule, it is and always has been. For every unicorn piñata stuffed with winning lottery tickets, there are hundreds/thousands? of others whose employees walk away with nothing or less (debt, strained relationships, mental health issues, etc.) at worst or a job at AcquiHireCo at best. reply bradleyjg 6 hours agorootparentThere was always very high risk, so it was only ever for certain people. But in earlier iterations of SV it was possible to become generationally rich as an early employee. The VCs and founders have fixed the glitch. To put it another way: early employee equity was always a lotto but now the payout is like some lame scratch off instead of the powerball jackpot. reply est31 5 hours agorootparentThe startups where employees get really rich still exist. I'm pretty sure the early employees of OpenAI are generationally rich for example. It's just that these companies very often are the darlings since their inception, get constantly talked about. Everyone wants to to invest in them and everyone wants to join them. So they have the ability to pick out the best talent, in other words, it's unlikely you'll be able to join that specific startup. But even 20 years ago, try getting into early Google. From what I heard they had extremely high bars for hiring as well and only lowered them once they got so large that the pool was exhausted. I'd argue that the total comp at the established companies for engineers has increased precisely because of competition from startups: to make the startup not be the better option. Does that mean that VCs are not taking a bigger slice than they used to? Absolutely not, but I wouldn't put the blame solely on them. reply bradleyjg 4 hours agorootparentRe: openAI We’ll see when it happens. If I had to name a company most likely to have massive landmines buried in front of common stock cashing out, it would be at the top of the list. reply mbesto 6 hours agorootparentprev> All you’re saying is that in the contemporary context it’s exceedingly foolish to be an employee at an early startup. As long as naive 22 year olds think have that one friend that stuck around long enough to cash out on an IPO, then yes. On a risk-adjusted basis, this has basically always been the case - you're better off working at FAANG. reply shmel 4 hours agorootparentprevIf you only care about money, sure. I have plenty of friends working in FAANG. For some mysterious reason any time I ask them about work, they say something along the lines \"ehh... it's fiiiine. Paycheck is pretty good though\". Okay, not all, but perhaps 95%. And half of them work massive overtime on regular basis. I can get behind working weekends when you hope to change the world. They often say things like: \"yeah, I have to work 60-70h per week because I don't want my boss to yell at me\". Those who work normal hours say: \"there is not much work to do really, we literally have meetings about meetings to fill the day. I wish I had some real work to do\". I truly hope that higher TC compensates for that. reply closeparen 3 hours agorootparentThe Bay Area housing market is too competitive for this. If you’re renting a room in your early 20s then sure just have fun, any tech job should cover it. If you want to own a place to raise a family in by your 30s, and you don’t have some exogenous source of wealth, you’re going to need every dollar of liquid compensation you can possibly get. reply shmel 2 hours agorootparentOr you can just live somewhere else. The world doesn't end at Bay Area. reply closeparen 1 hour agorootparentSure but this thread is about technology startups. The jobs you can get anywhere are business IT departments. reply lupire 7 hours agorootparentprevThere's a sucker born every September. You can find your comment in the HN archives as far back as 2010. reply extragood 12 hours agorootparentprevIt happens. I was offered the option to liquidate up to 20% of my vested shares at my last company's Series A. It was restricted by tenure though (3 years), so it wasn't available to everyone. In retrospect, I should have liquidated the full amount, but it was a new concept to me at the time and I was more conservative with the amount. I more recently interviewed with a pre-series A company and they said that they'd include me in a liquidity event when I brought up compensation. reply dmurray 8 hours agorootparentDoing this by tenure seems like a fairer way to distribute the liquidity. The founders still get preferential access to it, but because they really have taken more risk (bigger stake for a longer time period), not just because they have a better individual negotiating position. reply KingMob 7 hours agorootparentTenure/cliffs/etc should already take care of that by gating access to shares/options/etc in the first place. No need to add an extra tenure complication to liquidity as well. reply hackerlight 5 hours agorootparentprev> The founders still get preferential access to it, but because they really have taken more risk It's not related to risk, at least not directly. It's related to the supply of entrepreneurship as a factor of production. Entrepreneurship is scarce, so founders have leverage in any bargaining situation against early employees, who are more numerous and therefore less valuable and less powerful. If 10x the number of people tried to become founders, then founders would hold less leverage and the equity terms would become more \"fair\" because they'd have no choice but to give generous terms if they wished to hire people. reply chollida1 7 hours agorootparentprev> I was offered the option to liquidate up to 20% of my vested shares at my last company's Series A. It was restricted by tenure though (3 years), so it wasn't available to everyone. In retrospect, I should have liquidated the full amount, but it was a new concept to me at the time and I was more conservative with the amount.f Oh wow, how many companies have a series A after 3 years? How did your company survive without any raises for 3 years and what made your company finally decide to raise money after going 3 year without doing so? reply extragood 2 hours agorootparentThat policy was actually one of the major reasons I liked that company and stuck with them for so long. Their goal early on was to avoid raising money if at all possible, and they managed that for a long time by mostly being cash-flow positive/profitable. The trade off is slower, but sustainable growth. We hit an inflection point in the early pandemic where money was cheap and we had a ton of new customers coming in, so we were able to secure very favorable terms for the Series A and used that money to expand the business. Things continued to go in the right direction for the next ~2 years and we ended up doing a Series B round, and that in retrospect was a mistake. We over-hired in 2022 and couldn't back that up with increased business. And because we had given up so much control to investors in the previous rounds, we were unable to return to the sustainable-growth strategy that had worked for us in the past, and had to adopt faster growth strategies, none of which panned out and ultimately hurt the company and led to many rounds of lay-offs. reply anonymousDan 10 hours agorootparentprevHow would you negotiate that in practice? Would it be reasonable to ask for it to be in your contract? How would you suggest wording it roughly? Sorry I'm inexperienced with this kind of thing and have no idea how I would go about negotiating for it. reply sashank_1509 14 hours agorootparentprevThe very first startup I joined after grad school allowed all employees to cash out significant chunks of their stock in the Series A round. Also Elon famously put 200 million of his own money into Tesla and SpaceX to keep it afloat, which is the opposite of cashing out early. reply mapt 13 hours agorootparentIf you have 200 million \"of your own money\" to spare, you are no longer just a person for the purposes of this conversation, you're a walking VC fund, and you're not really risking a substantial change to your quality of life going from 250M to 50M net worth. Your living expenses are already generously compensated for by the large salary that you, the VC fund pays you, the person, out of your personal bank account, and they will be paying you those expenses until the end of your natural life. This isn't \"risk\" in the same sense as somebody who jumps to supplement their $150k salary with $450k of founder liquidity because it dramatically changes the material security of their life. reply robertlagrant 6 hours agorootparent> If you have 200 million \"of your own money\" to spare, you are no longer just a person for the purposes of this conversation, you're a walking VC fund, and you're not really risking a substantial change to your quality of life going from 250M to 50M net worth. Is that what happened? I thought he had $200m, and put in $200m. reply oblio 4 hours agorootparentDo we know this story from any credible source or are we just trusting Musk's (a famous liar) word about it? reply robertlagrant 2 hours agorootparentIt's true that that's what I thought, which is my statement. And it's better caveated than the previous one, which implied uncaveated that he still had $50m, but hasn't attracted the eye of any budding skeptics. reply rmbyrro 8 hours agorootparentprevYou may dislike Elon, but it's pretty absurd to say that what he did is trivial. reply forgot-im-old 7 hours agorootparentElon had most of SpaceX money fronted by Mike Griffin: https://historycollection.jsc.nasa.gov/JSCHistoryPortal/hist.... He was the key guy in the recent nuclear war connection for SpaceX, https://www.reddit.com/r/WikiLeaks/comments/1dc0m9s/elon_mus... reply llamaimperative 8 hours agorootparentprevGP didn’t reply throwbigdata 10 hours agorootparentprevLife is very different at $50M v $250M reply Tepix 9 hours agorootparentIs it? reply fooker 5 hours agorootparent50m is the upper echelons of private chaffeur money, 250m is the lower echelons of private jet money. reply BoorishBears 9 hours agorootparentprevIt's not really compared to an average person's life, but in SV tradition never let the chance to subtly flaunt a wealth gap pass by freely (This is the part where you say \"Yes, having lived both \") reply shermantanktop 46 minutes agorootparentHas 50M: wishes they had 250M Has 250M: wishes they had 1B reply romwell 13 hours agorootparentprev> to keep it afloat Can't \"cash out\" (early or not) if your company is sinking. reply gorbachev 11 hours agorootparentPrivate equity firms do exactly this. reply phlo 10 hours agorootparentprevAdam Neumann begs to differ. reply eru 13 hours agorootparentprevYou totally can. That's what investor money is for. reply KennyBlanken 9 hours agorootparentprev...while he was getting loaned $200,000 a month for personal expenses by his billionaire buddies. https://www.cnbc.com/2017/04/27/the-crucial-decision-teslas-... Also, that may have kept tesla and spacex 'afloat' but what really saved both companies was billions upon billions of dollars in government contracts, subsidies, preferential loans, and tax breaks. Nevada alone gave nearly two billion dollars to Tesla. reply petesergeant 9 hours agorootparentThe government is expecting something in return for these breaks rather than them being some kind of gift, though. reply oblio 4 hours agorootparentThe government is not monolithic and politicians might except other things than what their constituents want. It's a bad test of the value of an investment. reply petesergeant 4 hours agorootparentFor sure, and it may well have been a terrible investment with terrible returns, but selling to the government and responding to government incentives is an entirely legitimate thing to do, rather than some kind of inherent weakness in a company’s model. A company being “saved” by a government contract is a company being saved by making sales to its largest customer. reply nick7376182 6 hours agorootparentprevAnd the government got it, in the form of a cost effective usa-based launch solution. reply chatmasta 11 hours agorootparentprevThis assumes that the founders are aware of, or offered, the option. If anything this is an argument for why founders should be represented by a banker or lawyer at the closing of every investment round. Let the founders do the negotiating, but once it comes time to sign the papers, bring in the sharks. reply _heimdall 6 hours agorootparentHonestly if a founder isn't pulling in finance or legal experts prior to signing a funding round they really have no business being in position to begin with. They have to know VCs are leaning on their own financial experts and lawyers, why would you not have your own to protect your own interests? reply newswasboring 7 hours agorootparentprevThat's not common? I was under the impression that everyone hires at least a lawyer to get through the paper work. reply imadj 14 hours agorootparentprevA: I’ve seen many founders who got deep into the fundraising cycles without ever realizing they could take a cent out. B: I have known zero founders who have turned down an option to take money off the table [...] I love the idea of your universe, though. Fortunately, our universe is massive with varied different views. Even OP implied that they have experienced both sides firsthand. reply rKarpinski 13 hours agorootparentprev> I have known zero founders who have turned down an option to take money off the table (and zero A raises that offered that to employees). Have seen companies offer this to employee's And companies that let employee's take money off the table at series A are also likely to be generous with meaningless titles; that is they will let early employee's call themselves founders. reply adastra22 13 hours agorootparentAt a Series A?!? That's insane to me. We're talking about the first priced funding round for the company, right? reply muzani 9 hours agorootparentWhy is it insane? Some founders take zero salary since the start, and part of the reason for raising funds is that they have to eat too. Anyone who is an \"early employee\" usually get lower salary than market, and some stock. It's only fair they get to cash out a little early on, or hold on if they're liquid and think it's worth a lot more. It also works well for everyone involved if they're selling their shares to the investors for Series A - investors get shares for cheaper, founders get paid based around the value of those shares, more cash & runway in the bank. reply adastra22 3 hours agorootparentIn my industry the series A occurs in the first year of operation, and before the company has really achieved anything. A founder taking money off the table then is ludicrous. reply criddell 2 hours agorootparentFounders who have no need for money in the first year or two are fortunate people who are either already wealthy or have a spouse or family supporting them. Surely those aren't the only types of people worth backing. reply owlstuffing 5 hours agorootparentprevCorrect answer reply quartesixte 13 hours agorootparentprevI have witnessed small liquidity events at Series A and Series B that allowed for some small percentage of all total equity vested (around 3-5% ish, depending on the terms of your specific options grant) to be cashed out at some multiple of the FMV price. AFAIK the founders held themselves to the same restrictions (5% total, I believe?) to keep it relatively \"fair\". Pre-Seed, Seed, and some really really early Series A employees got to cash out fairly significant chunks of equity. Not as much as a founders' 1-2 million, enough for downpayments on homes or slick new cars all cash. The founders apparently were incredibly generous to Seed stage employees. Still doesn't compare to a Founders' equity, as this article implies. reply lmeyerov 11 hours agorootparentis this zero-interest rate phenomena in action? reply meheleventyone 10 hours agorootparentNope! Although the availability of funding obviously plays a role so the wider investment environment affects it. reply fire_lake 13 hours agorootparentprevThese days there is typically an institutional seed round before that. reply adastra22 13 hours agorootparentHow long is typical for the seed round? Both my prior startups only spent a few months in the seed stage. reply rKarpinski 12 hours agorootparentprev> At a Series A?!? Yeah, at Series A. reply sponaugle 3 hours agorootparentprev\"> I have known zero founders who have turned down an option to take money off the table (and zero A raises that offered that to employees).\" Nice to meet you. Now you know one. :) reply smallnamespace 14 hours agorootparentprevVCs will go along with or sometimes even encourage founders to take a little bit out, but employees rarely don’t have the same level of bargaining power. reply nytesky 12 hours agoparentprevI’m sorry, I think the era of “change the world” motivation in tech was eclipsed by “make 42 tons of money” about a decade ago. Along that line, I would be very surprised that there are founders who don’t seek an opportunity to set aside their nest egg to “de-risk”. You say you have seen such guileless dedication to the founding first hand, can you share what industry or type of company? Perhaps I’m just exposed to the wrong crowd. reply xbmcuser 13 hours agoparentprevIt's not in the interest of the VC that the founders have financial security. Well at least the type of VC's that have come up in since the dot com boom where it was not about building viable businesses but getting sold to the highest bidder when the founder is under financial pressure to sell they can strong arm him into easily compared to a founder that is financially secure and interested in building and running a business reply wrs 13 hours agorootparentIt’s not binary. Enough financial security that they don’t care what their investors think, no. Enough that they’re thinking of how to grow the company rather than how they’re going to pay their mortgage, yes. reply zenlikethat 3 hours agorootparentprevIt’s literally the opposite to what you suggest. Someone who hasn’t eaten for days isn’t thinking about eating healthy when they walk by a McDonalds. reply TimPC 8 hours agoparentprevAssymetry makes a certain amount of sense. Employees don’t take $0 for a long time and generally aren’t having as large a pay cut as founders afterwards. Most of the founders I’ve worked with have had the seniority to justify the top salary in the company and have typically had pay at or near the bottom. Someone operating at that extreme getting to trade equity doesn’t necessarily mean that everyone should get to. reply fidotron 7 hours agoparentprevI agree with the core of your point, and would extend it to any post-IPO lock in periods. reply gadders 10 hours agoparentprev>>It’s ok for founders to take a little bit of money off of the table if they extend that to their employees as well. Asymmetry is where things get weird. Yeah, if the founders don't do this I wouldn't want to work for them (not that I'm the target demographic anyway). reply darth_avocado 14 hours agoparentprevIt’s like trading windows and blackout periods for employee RSUs, but equity selloff on a schedule for the c suite. reply hackitup7 14 hours agorootparentThat's not quite how it works. Certain people are required (or strongly encouraged) to sell on a 10b5-1 plan. These plans can trade outside of open trading windows, but they have a meaningful cooldown period before they go into effect and can only be entered into during open trading windows. So it's not necessarily \"better.\" reply jahewson 13 hours agorootparentprevThat’s really about not falling foul of insider trading laws. Regular employees are free to set up limit orders within their trading windows (eg sell if stock hits $200) if they want. Can’t subsequently cancel it though! It makes way more sense to just sell on the day of vesting and then trade shares that you’re not restricted from trading. No tax or other reason not to do this. reply throwbigdata 9 hours agorootparentI’ve never worked at a public company that allowed limit orders to survive blackout periods. reply nick7376182 6 hours agorootparentI believe they are referring to a 10b5-1 plan that includes price-based sale triggers. reply lupire 7 hours agorootparentprevRegular employees can also make scheduled trading plans. ETP. reply vonmoltke 2 hours agorootparentWe couldn't at Twitter, which is the only company I've worked at that had a blanket trading blackout policy. The closest we could do was elect to sell all RSUs as soon as they vested (even if outside an open window). reply goalonetwo 57 minutes agoprevIn my 20s I joined a couple startups as \"early engineer\" or \"founding engineer\". I quickly realized those are the absolute worst positions to be in. You take almost as much risk as the founders but almost none of the upside. One startup died, the other one sold for 100m$. Out of that I saw 400k$ as an exit. Not too bad but even with that exit I ended up making way less than if I joined a FAANG. In both cases the founders made millions (through the exit or through liquidity) Now I'm a realist. Either you create/found a startup or it's not worth joining one as an employee. You have way more upside at a FAANG/pre-IPO mid-life startup that already found a great product market fit. Essentially, founders are pushing a crazy narrative of Startups being worth it to early employees because they need them. It was sometimes fun but I wish I just joined a FAANG like most of my friends, I would have a couple millions by now if I did. reply abvdasker 31 minutes agoparentI second this. I took a large salary cut to be 1/2 of an engineering team at a seed stage startup for ~1.25%. After 2 years of pretty grueling work I left to go back to big tech for 3x the pay. I wouldn't call it a total waste of time in the sense that it made me a better engineer, but it certainly wasn't worth it financially. The company still exists and has had a relatively successful series A and \"A extension\" but I don't think my equity will ever be worth anything. I really wouldn't recommend anyone work as an engineer at an early stage startup unless you're getting ~5% or more (this would be unprecedented) because the risk is barely less than the founders and the pay is generally terrible. Series B or later (growth stage) may be a sweet spot where the salaries are decent and there is still significant equity upside without the insane hours. reply koalaman 7 hours agoprevI recently left a long career in FANG to roll the dice on an early startup. I was pretty surprised by the uneven terms between founders and early employees. From what I could tell the early employees takes more risk than the founders because they don't get that magic token dollar turning into their share of the founding equity event and have to pay the fictional valuation of the seed to convert their options. Depending on how hot your startup is that can be a lot of money. Anyway that ended in tears, but I got what I was looking for from it. A look under the covers of the hot VC backed startup roller coaster. I may be getting old and cynical, but it looked considerably more exploitative than what I saw at Google. Obviously depends on the character of the founders and leaders, but the structure seems to be setup for toxicity. reply geepeeyoudata 6 hours agoparentI worked at a Series A startup as an employee, and wont be doing that anymore. Early engineers have all the risk (lose job the second things go bad) but little upside. They would offer 500 options, or 1000 options, or 30,000 options -- but when you look at the prices, that was worth $100-$10,000. Why would anyone take all this risk, and lower base salaries for that lottery ticket?! Secondly, they wont share the cap table, so you dont know what the denominator is. 30,000 shares of What!? No one would tell you. You should run. Third, the VCs installed a buddy from SV as CEO who was creative with revenue. Great -- so they make their bonuses based on creative revenue, but the company gets saddled with VC rounds they have to dig out of w/o showing real revenue growth. Once you get SV insiders being placed into the company, often with their entourage of cousins and neighbors' kids as Director of HR or Director of Finance -- RUN FAST. The company is being strip-mined for cash, while Engineers slave away trying to code their way out of the wreckage left by locusts. The C-Suite operated in a separate tier of the company with a heads-i-win-tails-you-lose setup. You could tell -- no way you are all driving Tesla Plaid on a \"startup salary\" -- the \"startup salary\" was for suckers, engineers, and those not in the VC-back-scratch loop. My advice to everyone -- if you want risk, be a founder. Not Engineer #1 or #10. If you want balanced risk, go to a Series C or D company where you dont have the risk of fake accounting. If you want money, go to a public company with real accounting rules, visible revenue, visible liabilities, and more accountability. reply bagels 6 minutes agorootparentMy early engineer story is a lot different than this. 0.7% sold shares for ~1M at the end of 4 years. There is a bit of luck, and a bit of picking the right one to join. Don't join the ones that don't tell you what your equity share is and what the last valuation was to start with. reply zenlikethat 3 hours agorootparentprevEven if you don’t see the cap table, any company you talk to should be clear and consistent in disclosure of facts like number of shares outstanding, including viewing it in tools like Carta. You are basically describing the abusive version of a startup and then saying all startups are bad. I actually think going to a Series C or D is not the ideal play. It’s better to join an early company, with good leadership, reasonable if not mind blowing salary and cheap shares. Then, work hard, but not brutally hard. Somewhere that you enjoy the people, the work/product, and you can level up a lot. The options are cheap, and you can bail to FAANG at any time if you burn out. Realistically, that’s your shot at making 1% of $Xmm without completely hating your life. It will be a rare company so, yeah- be picky. I don’t know why all startups get lumped into one when there’s a lot out there for the discerning employee. reply randerson 3 hours agorootparentprevMy experience was similar, right down to the $10,000 worth of options. Eventually the company went public and those options would have been worth $5M if I'd had the foresight (and cash) to exercise them (which I didn't). The co-founders did not have exercise costs or AMT of course. It is an unfair system indeed. I'd encourage those seeking to be early engineers to go work at a FAANG for a few years before joining a startup so that you have the cash reserves to take the risk. reply davedx 3 hours agorootparentWait, you couldn’t find the 10k cash to exercise 5m worth of options? reply hylaride 1 hour agorootparentWhat they likely meant was that the options would eventually be worth $5m, but not when they left the company and could exercise them. reply randerson 2 hours agorootparentprevThe paper value was far lower during the exercise window & no guarantee it would ever be liquid. The AMT would also have dwarfed the 10k. reply xnx 2 hours agorootparentprevYou typically don't know what they're worth when you exercise the options. Often it turns out to be nothing. reply djbusby 5 hours agorootparentprevIf you are early and they not sharing the cap-table it's a red-flag. reply pragma_x 4 hours agorootparentprev> but when you look at the prices, that was worth $100-$10,000. Why would anyone take all this risk, and lower base salaries for that lottery ticket?! I was in a company when my options were \"purchased\" from me at the strike price, when the company itself was sold. We never made it to IPO. I've learned to not overvalue options and phantom stock, and just chalk it up to another bonus down the road. The real money is, or already has been, made elsewhere. What really steams my biscuits is when I figured out how the payout was worth less than the unpaid overtime (never more than 50 hours a week), weekend support time, and travel time spent in my years there. reply e40 3 hours agorootparentprevSpot on, and I say this as a founder of a company that didn’t fuck over the employees. 40 years and still going, and most people have been with us for more than 25 years. I didn’t get rich because I wanted to sleep at night, but people in my orbit (probably me in theirs?) advised me very differently. reply carterklein13 5 hours agoparentprevI did a similar thing to you. However, I do feel like cutting your teeth as a \"founding engineer\" at an early startup has 2 major benefits: 1. You get to see what it's like under the covers, as you said. It's not nearly as glamorous as it looks from the outside. And yes, as an early engineer, you share in a lot of the downside without nearly an equal share of the upside. 2. You get to leave. Unfortunately, the startup I joined entered a tailspin. But, my name wasn't attached to the company, and I didn't have a fiduciary obligation to our investors. I had a lot of \"stake\" myself after putting in years of 12-hour days, nights and weekends, but at a certain point I saw that my career was actively being harmed by staying. That \"founding engineer\" role on my resume got me the job I'm at now, at a level that skews higher than my YOE. Do those two points mean you should get a fraction of the equity (or rather, a fraction of the options) as the founder? Honestly... maybe. I've now seen a few founders fail. It can really be a career-killer. reply palata 5 hours agorootparent> Do those two points mean you should get a fraction of the equity (or rather, a fraction of the options) as the founder? Honestly... maybe. I've now seen a few founders fail. It can really be a career-killer. And I have seen a few founders fail and enter bigger companies at a pretty high position. Not sure I would relate that to how much money they should get in case their startup is one of the lucky ones. reply zenlikethat 3 hours agorootparentprevGetting to leave is so underrated. Nothing keeps your head above the doom and gloom like knowing you aren’t shackled to the thing, and the world’s your oyster if you need to move on. We live in a weird world if people don’t think a gig with $160K salary, 2% of the company, where you can work hard but not 24/7, and _leave any time you want_ is a bad gig. That 0.25-0.5% after one year that you get is PERMANENTLY gone for them even if you just fuck off after a year. Years later it could be worth millions. But anyway, as founding engineer you get to set the systems, culture, language etc. maybe some people don’t want the responsibility but for others it’s an opportunity to build things out in our own image and learn a lot. reply hliyan 4 hours agoparentprevThere was an oft-repeated response back in the day (but gladly rarer now) when you dig too deep into employee benefits at startups: \"If you're offered a seat on a rocket ship, don't ask what seat!\" To this, I usually reply \"Unless the seat happens to be in a stage that gets jettisoned before reaching orbit\". reply palata 6 hours agoparentprevI have been working in multiple startups, I've come to think that it's a Ponzi scheme for the founders. Generally underpaid and quickly toxic. It is an experience, but it's important to know it. reply RhodesianHunter 2 hours agorootparentIt's a Ponzi scheme for VC and other investors. Founders just get greased palms along the way if they're successful. reply koalaman 2 hours agorootparentYeah agreed to both of you. I had the same thought. reply sackfield 5 hours agoparentprevAs an engineer you really have a finite amount of good working years, and accepting startup salary vs big tech compensation is a bigger risk than founders are willing to generally admit. reply wnolens 3 hours agoparentprevI almost left for an ultra early startup, still running on seed money. They offered a typical SDE2-Senior salary + 1%. I was kind of offended. I'd be inventing their core technology (which didn't exist yet and which their CTO wasn't fit to do) and probably interviewing every engineer and growing them. Even IF they achieved a 100-300M exit, after dilution I would be compensated at best par with a FANG Senior over about 5-7y. I was pretty excited about joining and would have been all-in. So I asked for 2-3% and was denied. Looking back, I'm glad because even 3% isn't worth it. Not when the founders are taking 10x. reply indymike 1 hour agorootparent> . I'd be inventing their core technology (which didn't exist yet and which their CTO wasn't fit to do) and probably interviewing every engineer and growing them. I see this a lot in failing startups: The CTO is a pure manager who can't do any actual engineering. The result is that the shares and salary that could have been traded for getting product to market faster & better ends up being burnt on an empty chair. reply o283j5o8j 4 hours agoparentprevI did early employee several times because I didn't know any better, didn't have anyone around to tell me not to. I won't do that again. All the risk, none of the reward. 1% of $10-20M after 4+ years of 80hrs/wk is less than the difference between a startup salary and a good salary over that same time. reply bagels 9 minutes agoparentprevI completely disagree on the risk. What was the opportunity cost for you in founding? Are you taking a salary comparable to your FANG comp? Usually the early employees are getting paid a lot closer to their market rate than the founders are. reply oblio 4 hours agoparentprevYou have the current unicorns, basically anything from about the time YC started, and then you have the old school unicorns. For comparison, Microsoft IPOed in 1986: > The company's 1986 initial public offering (IPO) and subsequent rise in its share price created three billionaires and an estimated 12,000 millionaires among Microsoft employees. https://en.wikipedia.org/wiki/Microsoft I would really, really want to know if anything more recent has gotten to that level of widespread distribution of the riches. I kind of doubt it, such an event would probably be considered Communist by modern standards :-) reply kaiokendev 3 hours agorootparentFacebook, although it didn't have nearly as many employees upon its IPO reply ilamont 3 hours agorootparentprevan estimated 12,000 millionaires One of them is my neighbor, an early Microsoft employee. She basically retired in her 30s. reply jejeyyy77 4 hours agoparentprevmost i know who work as eng #1 (non founder), are new grads who couldn't get into FANG. So mainly just looking for experience/inflated job ti",
    "originSummary": [
      "Founder liquidity allows founders to sell shares during funding rounds, securing personal financial stability, which significantly alters the risk landscape compared to early employees.",
      "The practice of founder liquidity is often kept secret to maintain the image of a fully committed founder, attracting top talent willing to work for less in exchange for equity.",
      "The author advocates for transparency in founder liquidity, suggesting that every new funding round should disclose if founders took liquidity, to balance risk and compensation for early employees."
    ],
    "commentSummary": [
      "The text discusses the financial risks and potential regrets for founders and employees selling equity early in a startup, highlighting the significant difference in value if the startup succeeds later.",
      "It emphasizes the importance of understanding the financial implications, including taxes and opportunity costs, of selling equity versus holding onto it, and suggests using equity as collateral for loans as an alternative.",
      "The text critiques the startup ecosystem for often misleading employees about the value of equity, suggesting that early employees frequently get a worse deal compared to founders and that financial literacy is crucial for making informed decisions."
    ],
    "points": 1397,
    "commentCount": 623,
    "retryCount": 0,
    "time": 1718163163
  },
  {
    "id": 40650844,
    "title": "Flameshot: Versatile Open-Source Screenshot Tool with Cloud Integration and CLI Support",
    "originLink": "https://flameshot.org/",
    "originBody": "Flameshot Features Download Contribute Donate Docs Powerful, yet simple to use open-source screenshot software. Download Free & open source screenshot software Flameshot is a free and open-source, cross-platform tool to take screenshots with many built-in features to save you time. Flameshot Features Highly customizable Customize the interface color, button selection, keyboard shortcuts, how images are saved, and more with Flameshot's accessible configuration dialog. In-app screenshot editing You can choose to add an arrow mark, highlight text, blur a section (blur or pixelate an area), add a text, draw something, add a rectangular/circular shaped border, add an incrementing counter number, and add a solid color box with Flameshot's built-in editing tools. Simple & intuitive Using Flameshot is as simple as launching, dragging the selection box to cover the area you want to capture, making annotations as needed in on-screen and saving the shot to your computer, all with a very simple and straightforward interface. Upload to online platforms Flameshot allows users to simply upload their screenshots directly to the cloud in order to easily share it with others. You can upload your image directly to Imgur with a single click and share the URL with others. Command-line interface (CLI) Flameshot has several commands you can use in the terminal without launching the GUI via a command line interface. The command line interface lets you script Flameshot and use it as the subject of key binds. Download Flameshot Windows macOS Linux Get the latest Flameshot Windows Downloads 64-bit only, either installer or portable version available Download Installer Download Portable Looking for older releases? Get the latest Flameshot macOS Downloads 64-bit only, install via Homebrew or download the dmg file Install via Homebrew Download .dmg Looking for older releases? Get the latest Flameshot Linux Downloads 64-bit only, install via Appimage, your package manager, Snapcraft or Flathub Download AppImage Download Nightly-builds Binaries Looking for older releases? Install via Package Manager Arch pacman -S flameshot Ubuntu 18.04+ and Debian 10+ apt install flameshot openSUSE zypper install flameshot Void Linux xbps-install flameshot Solus eopkg it flameshot Fedora dnf install flameshot NixOs nix-env -iA nixos.flameshot Contribute to Flameshot on GitHub Contribute Sponsors Flameshot Features Download Contribute Donate Docs Copyright © 2017-2024 Flameshot contributors • Site design by Correct Syntax",
    "commentLink": "https://news.ycombinator.com/item?id=40650844",
    "commentBody": "Flameshot – Open-source screenshot software (flameshot.org)343 points by nikolay 22 hours agohidepastfavorite119 comments Gormo 7 hours agoI use Flameshot combined with Tesseract and zbarimg to quickly clip areas of the screen and either OCR them or decode barcodes, which I then map to hotkey combinations. For example, I have `bash -c 'flameshot gui -s -rtesseract - -gxmessage -title \"Decoded Data\" -fn \"Consolas 12\" -wrap -geometry 640x480 -file ` mapped to Super+O, so I can just press the key combo, select a region of the screen, and have the OCRed text immediately displayed in a dialog box from gxmessage (which accounts for most of the command line). Replace 'tesseract' with 'zbarimg' and you have a barcode scanner. reply noisy_boy 4 hours agoparentGreat idea - I ended up experimenting to improve the ocr accuracy: #!/bin/bash screenshot=$(mktemp) decoded_data=$(mktemp) processed_data=$(mktemp) cleanup() { rm \"$screenshot\" \"$decoded_data\" \"$processed_data\" } trap cleanup EXIT flameshot gui -s -r > \"$screenshot\" convert \"$screenshot\" \\ -colorspace Gray \\ -scale 1191x2000 \\ -unsharp 6.8x2.69+0 \\ -resize 500% \\ \"$screenshot\" tesseract \\ --dpi 300 \\ --oem 1 \"$screenshot\" - > \"$decoded_data\" grep -v '^\\s*$' \"$decoded_data\" > \"$processed_data\" cat \"$processed_data\"\\ xclip -selection clipboard yad --text-info --title=\"Decoded Data\" \\ --width=940 \\ --height=580 \\ --wrap \\ --fontname=\"Iosevka 14\" \\ --editable \\ --filename=\"$processed_data\" reply Gormo 4 hours agorootparentNice! Once you start getting complex, a standalone script might be a good idea. But it should be noted that your ImageMagick processing can also be inserted into the original one-liner: bash -c 'flameshot gui -s -rconvert - -colorspace Gray -scale 1191x2000 -unsharp 6.8x2.69+0 -resize 500% png:-tesseract - -gxmessage -title \"Decoded Data\" -fn \"Consolas 12\" -wrap -geometry 640x480 -file reply noisy_boy 2 hours agorootparentIndeed; I just wanted to break it into sequential steps so that in case of issues, I can conveniently add debugging steps in the middle as needed. reply pixelmonkey 17 hours agoprevThis is my go-to screenshot tool on Linux. For Linux users who also use Google Photos already, you may not realize this but the Google Photos web app accepts paste from system clipboard via Ctrl+V in the browser. Thus, my workflow if I want to save a screenshot for later is to call up flameshot in rectangular selection mode (I bind it to the PrtScn key), select my screenshot area, Ctrl+C to copy it to clipboard, navigate to GPhotos web app via address bar / bookmark bar shortcut, and Ctrl+V to upload there. The nice thing about this is that GPhotos recognizes it as a screenshot (so I can find it on my phone later, too, for example). And, GPhotos also automatically indexes any text within the screenshot, so free text search can often find it, too. If I need the screenshot as a file for some other purpose, I'll navigate to it in GPhotos and use Shift+D to download it. I can also use GPhotos to privately share the screenshot with someone via their email address, or get a tokenized link for it. Just sharing this tip because I notice a lot of people hunt around for cloud storage for desktop screenshots. But Google Photos works pretty well for this purpose already, if you use the paste-to-upload trick! reply qmarchi 16 hours agoparentLooks like there's a Google Photos API, so you could concievably make a full chain to upload screenshots automatically. https://developers.google.com/photos reply xhrpost 5 hours agorootparentMy employer uses Gmail/gdrive so I just use the gDrive sync tool to auto upload my entire screen shot folder. reply brightball 13 hours agoparentprevI just store screenshots to a Dropbox folder. It’s worked great for me for years with Flameshot. reply xpil 4 hours agoprevI'm a big fan of Greenshot. My only issue with it is that it's not available on Linux, which I use occasionally. Re Flameshot, I've tried it and it generally works well for me. My only beef is that the layout of the icons around the captured area is dynamic, changing based on the shape and size of the area, requiring me to actively search for an icon instead of finding it in a static, predictable location. reply weinzierl 3 hours agoparentI worked for an organization with more than 150000 employees. All their PCs had Greenshot pre-installed and it was part of their standard software. Greenshot was used a ton over more than a decade, maybe still is, and (observed from my limited view) loved very much. They never payed a cent to the developer - shame on them. reply canpolat 22 minutes agoprevThis is the tool I have been looking for quite some time. I don't know why my searches returned only the simplest screenshot tools that are available on Linux. I have been using ShareX on Windows and was surprised to not find a similarly powerful tool on Linux. Now, I know that it was because of my rusty duck-duck-fu (or search is basically useless nowadays). Thank you for sharing. reply smusamashah 8 hours agoprevFYI, [Win + Shift + S] is quickest way in windows to copy selected area to clipboard, if that's all you need. I use it nearly everyday for something e.g. posting snap of a code snippet or anything in slack to putting these clips in docs. EDIT: Just tried Flameshot and loved that I can draw while taking a snap, instead of opening a new window and do the drawing in that. Looks like this is going to replace Win+Shift+S for me. reply glonq 4 hours agoparent> FYI, [Win + Shift + S] is quickest way in windows to copy selected area to clipboard, if that's all you need. Nowadays (at least for me on Win11) it's also bound to the PRNTSCRN button, which is a nice way to redeem an otherwise anachronistic key. reply MaxikCZ 8 hours agoparentprevThats the reason I stopped using win+shift+S, to draw on snippet right away. Now if only I could do 2 snippets and merge them into one pastable immage in flameshot.. reply geoka9 19 hours agoprevI use the following script (activated by a system-level shortcut key) to take a screenshot, upload to S3 bucket (using the minio client[0]) and place the URL in the X selection buffer, ready to be pasted: #!/bin/bash set -e dbus-update-activation-environment DISPLAY XAUTHORITY FNAME=`cat /dev/urandomtr -cd 'a-f0-9'head -c 32`.png flameshot gui --raw > /tmp/$FNAME ~/go/bin/mc -q cp --attr x-amz-acl=public-read /tmp/$FNAME s3/your.s3.bucket/dir/$FNAME echo -n https://s3-us-west-2.amazonaws.com/your.s3.bucket/dir/$FNAMExsel [0] https://min.io/docs/minio/linux/reference/minio-mc.html reply endgame 12 hours agoparentYou may want to modernise your script because Bucket ACLs are disabled by default these days: https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-... reply wzyboy 19 hours agoparentprevI have a similar setup but with SHA256 hash of the file as the object key. reply aendruk 5 hours agorootparentIn mine the hash is encoded as z-base-32 and namespaced with an uncommon first character: $ publish example.png https://example.com/+umk3cm5cah5akbqeueq8914zimfktoih And for when it matters, the filename can optionally be attached: $ publish --named example.png https://example.com/+umk3cm5cah5akbqeueq8914zimfktoih $ http --headers 'https://example.com/+umk3cm5cah5akbqeueq8914zimfktoih'grep 'Content-Disposition' Content-Disposition: inline; filename=\"example.png\" reply sandinmyjoints 18 hours agoparentprevI forget, do you pay for bandwidth serving from S3 in this case? I have been looking for a good screenshot hosting solution to replace Cloudup, which was perfect and still usually works but I figure it might stop any day now. My only worry would be the unlikely case of a surprise high bill from a screenshot gone viral or something along those lines. reply mfkp 9 hours agorootparentMy workflow uses SFTP to upload to a cheap webhost, with cloudflare in front acting as a cdn. Easy peasy reply djbusby 17 hours agorootparentprevDoesn't R2 have no egress fee? reply byteknight 13 hours agorootparentprevImplement a cloudflare cache infront? reply AnthOlei 15 hours agoparentprevI wonder: if we set a TTL on the image, and also make it require a signed link that gets copied the same way, is it now a secure & ephemeral service? reply wackget 20 hours agoprevHaving briefly tried it I have to say it's not as clear or easy to use as ShareX (another open-source screenshot tool). The monochrome icons are really not intuitive or easy to discern at a glance. Another commenter asks why it's not possible to trigger with the PrtScn key and I would also think that is an essential feature. reply runsonrum 20 hours agoparentI found it to be the exact opposite. ShareX has a lot of features which makes it hard to quickly get your head around all its clutter, when sometimes all you want to a screenshot utility. Flameshot has key bindings just like any other screenshot program. If the shortcut is already bound by another program, then it will not let you bind it to Flameshot, I believe. I have been using Flameshot portable for years and it isn't without missing features but I keep coming back to it. I generally use the copy function, sometimes save to location. It would be good to have a built in editor that can be loaded after the action. reply Saris 19 hours agorootparentWhy not just use the built in system screenshot tool if you just want to copy or save? ShareX can pop up an editor after a screenshot which I do use a lot. reply runsonrum 17 hours agorootparentMy apologies as I was not clear. Copy or Save after crop, and arrows, numbers etc. Thank you for pointing out that ShareX has an editor. I was aware that it has that feature but I do not remember the specifics on why I went back to Flameshot. The lacking features of Flameshot is that once you move away from the editor, you can not go back and make alterations. Not that I am aware of anyway. reply ASalazarMX 41 minutes agorootparentI've used ShareX and Greenshot, and the latter is more straightforward. I don't want to upload retouched/annotated screenshots, but ShareX is a image sharing application at its core, so it had too much unused baggage for me. Greenshot hits the sweet spot for my use cases perfectly. Also, Greenshot is lighter and snappier than every other Windows screenshot application I've tried. reply rcv 20 hours agoparentprevI have the PrtScn key set up to open flameshot on my PopOS machine and it works great. I just followed the instructions here: https://flameshot.org/docs/guide/key-bindings/#on-ubuntu-and... reply SubiculumCode 18 hours agoparentprevFlameshot is the best. I'm never going back. And configuring it to a the print screen button just involves assigning it, and in the case of Ubuntu, overriding system defaults. reply billwashere 16 hours agoprevFor the last 10(ish) years I've been using Greenshot [1]. I haven't found any issues with it but it is only available on Windows and Mac. [1] https://getgreenshot.org/ reply winrid 15 hours agoparentGreenshot is great! I use it on Windows, and ksnip on everything else atm as it has similar draw arrows/annotation features. reply ASalazarMX 38 minutes agorootparentIf ksnip is cross-platform, may I ask why don't you use it everywhere, if it's so similar to Greenshot? BTW I've never heard of Ksnip before. I gotta try it. reply alan-hn 4 hours agoprevFlameshot is great, I've been using it for years. I love being able to draw and add annotations in the moment I take the screen shot reply giancarlostoro 4 hours agoparentI have wanted this for ages, didn't know Flameshot did this or I would have tried it much sooner. It's the only thing missing from any screenshot utility I use. reply SV_BubbleTime 4 hours agoparentprevI needed a screenshot software when I moved to Linux Desktop last year. Flameshot (despite being available on Windows) helped make that transition really pleasant! I mentally put it in the LINUX HAS BEEN GOOD column. reply alan-hn 4 hours agorootparentThat's actually how I found it too, the drawing and annotations were just the cherry on top reply tmcdos 6 hours agoprevFor the last 10+ years I am using portable WinSnap (it is Windows only) - just 3 Mb, does not need .NET, can conveniently snapshop the current window, all windows of the current application, whole desktop or just a selected area. Has some built-in filters (mirror, border, watermark, negative, grayscale, blur) but most importantly - allows moving around or deleting the annotations individually after their creation (unlike Flameshot). Can not recommend any other screenshoting software. reply PenguinCoder 21 hours agoprevAmazingly useful, definitely powerful and easy to use software. I know I sound like I am just repeating the title, but that's my honest, user opinion too. Does what it says it does, does it well, and stays out of your way until you want to use it. reply dang 20 hours agoprevRelated: Flameshot v11.0.0 - https://news.ycombinator.com/item?id=30071766 - Jan 2022 (30 comments) Flameshot – Simple, powerful screenshot tool for all major operating systems - https://news.ycombinator.com/item?id=26446070 - March 2021 (125 comments) Flameshot – Superb Screenshot Tool - https://news.ycombinator.com/item?id=26113753 - Feb 2021 (83 comments) reply I_am_tiberius 20 hours agoprevI find it ok as I'm on a Mac now. On Linux, I used KSnip which was incredible (doesn't work well on Mac). Nowadays I would use Spectacle on Linux. reply mergy 19 hours agoprevIt's been the best option for a while now on Linux IMHO. I was a long time SnagIt user on Windows and when I went Linux full-time, I tried all different options but the ability to snapshot and markup quickly are key. Works well in XFCE, KDE Plasma, and Cinnamon - notso in Gnome because, well, Gnome. I wish it did video too. Until it does, I am using SimpleScreenRecorder - which is okay. reply pentagrama 16 hours agoprevThe UI and features look well-polished. I use ShareX, another open-source tool but only available for Windows. However, a crucial feature for me is quick screen recording (GIF or MP4), which Flameshot seems to lack. Does it have this feature? It's not mentioned on the landing page. reply graynk 7 hours agoparentOn Linux for screen recording I use Peek and I really like the approach You just resize Peek‘s transparent window over the part of the screen that you want and hit record https://github.com/phw/peek reply deadbunny 7 hours agorootparentI make heavy use of Peek and it's great. Unfortunately it's abandoned so will likely stop working eventually. https://github.com/phw/peek/issues/1191 reply graynk 6 hours agorootparentOh, I was not aware of this. Very sad to see this reply just-tom 11 hours agoparentprevI use ShareX for mp4 and gif using CTRL+PrtSc / SHIFT+PrtSc and for image screenshot only PrtSc (currently lightshot, will aoon move to flameshot) reply Jerry2 3 hours agoprevI've been using it for years. Being able to quickly add arrows and highlights is fantastic. My only wish is for it to be able to open existing files. I wish I could just open an image, make some edits, and save it. Unfortunately, you can only make edits to screenshots. reply W3cUYxYwmXb5c 21 hours agoprevBeen using this for a few years now. First grabbed it because I wanted a linux alternative to ShareX, but now I use it on windows too. It's great! reply jayknight 20 hours agoparentCan you trigger it with the PrtScn key in windows like you can with sharex? I can't find an option for that. reply jcelerier 19 hours agoprevA great example of how to use Qt to make very neat, useful and feature-packed cross-platform software at a very low cost reply dspillett 7 hours agoprevI've been using ShareX (https://getsharex.com/) for some years, which is also open-source, and very featureful while not feeling too bloated, though Windows only. I'll have to have a look at this next time I'm on a Linux desktop, as I found the options lacking compared to ShareX last time I looked. reply Liquidor 6 hours agoparentI use ShareX for Windows and Flameshot for Linux. I wish ShareX worked for Linux. It's so good. reply SoftTalker 15 hours agoprevFor screenshots I use 'scrot' and then open the image in gimp if I need to crop or edit further. Seems very simple to me and avoids browsers, cloud storage, and other potential pitfalls. https://github.com/resurrecting-open-source-projects/scrot reply Toorkit 15 hours agoparentFlameshot is local. May have a subscription service, not sure. But flameshot is extremely fast and does 95% of what I'd do in gimp anyway, without having to open a whole image editor and saving a picture. I can directly copy it to the clipboard and paste it somewhere. reply SoftTalker 15 hours agorootparentYeah it sounds good, I just rarely do screenshots so I stick with what I know. If my workflow required a lot of screenshots I might either automate it a bit more or look at other tools. I'm actually about as likely to just pull out my phone and take a picture of the screen as to use software-based screenshots. reply cess11 9 hours agoparentprevI also use scrot, because it's simple. Almost always use 'scrot -s', so I get to do immediate cropping, and it's trivial to bind to a shortcut in i3wm. reply dano 21 hours agoprevThis is a great piece of software that I use under Linux and MacOS reply heavyset_go 21 hours agoprevWorks great on Linux using Wayland. There was a period years ago when that wasn't the case. reply thekoma 19 hours agoparentWhile it works on Wayland for me, it feels much clunkier and less snappy than what it used to be on X. reply 0x1ch 20 hours agoparentprevYears ago? Try less than 12 months ago. I still have active bugs open in their issue tracker. reply heavyset_go 20 hours agorootparentThat's been my experience. I've been using a rolling release distro and haven't had problems in years. There was a period where any screenshot/recording app didn't work at all, including Flameshot, due to limitations in Wayland implementations. reply SpaghettiCthulu 19 hours agoparentprevIt doesn't work at all for me under hyprland. reply synergy20 16 hours agoparentprevnever worked on ubuntu 22.04, I had to use xorg for that. reply betimsl 4 hours agoprevYou know what Flameshot needs? A color picker and even w/o it, it's a very good piece of software. reply bjoli 3 hours agoparentIf you use gnome you can use eyedropper reply stronglikedan 4 hours agoprevVery nice, but I gotta give Greenshot the edge here, strictly from a usability standpoint. reply tveyben 4 hours agoprevGreenshot have great features - but i need to test flamshot ad it (also) looks promising… reply programmertote 20 hours agoprevI have been using it recently and like it. It is MUCH better than Windows native snipping software. It is also open source and free, so that's an added bonus. Having said that, I wish we can select the objects (e.g., text box, arrows) we have created and move them around. Right now, we can only undo and if an arrow is drawn a few steps before and now you want to reorient its head, you are out of luck. In the past (4-5 years ago), I used to use Jing (now called TechSmith Capture) and liked it a lot: https://www.techsmith.com/jing-tool.html and liked it. But I think the company decided to remove some features and/or require some sort of account creation; on top of that, it (if I remember correctly) kind of lost its earlier simplicity, so I stopped using it. reply diogotito 20 hours agoparent> Having said that, I wish we can select the objects (e.g., text box, arrows) we have created and move them around. […] On Windows I like to use Greenshot because the editor opens up in a dedicated window and gives me full control over the objects I place (move, resize, change colors, duplicate, cut-copy-paste, reorder, save objects to file for reuse...). It's also open source, but seems unmaintained for some time now (but there is a fork implementing zoom in the editor). reply runsonrum 20 hours agoparentprevI think you can move objects around but I believe you have to de-select the tool first. Try pressing ESC first to deselect. Going off of my poor memory. reply zie 16 hours agoprevI don't understand the fascination with blur. It's terrible from a security/privacy perspective. The data is still in the image. Sure you can add randomization to the blur to make it less easy to undo(I haven't looked if Flameshot does or not, most don't). If you crop the stuff you want out of the image, there is no data there to do anything with. Every screenshot tool I've ever come across has a blur tool but no cut tool. So I just use the OS specific screenshoter and then load it up in an image editor and cut to my hearts content. reply eviks 14 hours agoparentWith blur there can also be no data (as you said -with randomization, you can even blur randomized text instead of the real one). But the fascination is easily explained - it looks better as it \"fits\" the rest of your image vs having some jarring black rectangle reply zie 1 hour agorootparent> you can even blur randomized text instead of the real one I've never ever seen that in the wild, but that would def. make it sane to use the blur tool. reply MemphisTrain 16 hours agoparentprevFlameshot has a draw-rectangle function. So you can just draw a red or black rectangle on top of things. reply zie 1 hour agorootparentAnd since it's a PNG file, I assume, It won't be layered! nice! reply __fst__ 13 hours agoparentprevI sometimes (but rarely) use blur to shift the attention focus on certain screen regions by blurring out other parts. Like an inverted highlighter. But mostly I actually use the highlighter function :-) reply LooseMarmoset 20 hours agoprevI discovered this when I was playing around with i3wm a few years ago. It's a really nice piece of software that does what you need it to do, and it stays out of your way otherwise. I mostly use it for screenshots, but it can edit and annotate, and pin images. reply Epskampie 6 hours agoprevI prefer shutter. It's way uglier, but also has some features others don't have, like a history of shots, so you can make several in a row. reply yellow_postit 17 hours agoprevOn Mac I swear by CleanShot X [1] which has more than justified its price many times over for me. [1] https://cleanshot.com/ reply shepherdjerred 4 hours agoparentI agree, CleanShot X is the best thing I've found. Open to any alternatives that someone knows about, though! reply elric 7 hours agoprevI just use ImageMagick's \"import\", and if I need to annotate the screenshot I have a shortcut to screenshot and open it in gimp. reply Ameo 19 hours agoprevThis is my go-to screenshot tool for Linux; I've been using it almost daily for years now. It's by no means as feature-rich as ShareX for Windows, but it works perfectly for what I need. It covers the essentials like simple annotations, blocking out areas of screenshots, saving local copies, etc. I made some tweaks to support my own custom image uploader API, and similar to the comment by geoka9, I have it set up to take a screenshot, upload to my app, and copy the URL into my clipboard all behind a single shortcut. reply askvictor 13 hours agoprevThis is my screencap tool - the 'pin' feature is particularly useful when debugging things. One minor annoyance is it struggles with DPI scaling across screens (i.e. multi-screen with different scaling factors on different screens). There's a long thread on github with workarounds. reply dfc 9 hours agoparentWhat does pinning do? The docs just say \"--pin: Pin the screenshot\" reply skyyler 21 hours agoprevI initially installed for an ex-windows user that needed something similar to the snipping tool. I quickly started using it for myself, it's very very convenient. reply nikolay 20 hours agoparentThe reason I love is that I can create pixel-perfect screenhots and precisely pick what I want to cut out and, if necessary, add arrows, text, etc. It also allows you to copy into the Clipboard and now Facebook and others allow me to paste images, which saves me the effort of going through the file system. reply hu3 20 hours agoprevCan this do OCR? I couldn't find in feature list so I'm guessing not yet. I use Windows native screenshot tool because it supports OCR. reply jerbear4328 18 hours agoparentIs that on Windows 11? The Windows 10 tool doesn't, though there is a PowerToys feature to select an area and copy text (not accurate enough to be useful though). reply hu3 18 hours agorootparentWindows 11. Pro version but it shoudn't make a difference. https://www.pcworld.com/article/2070861/windows-snipping-too... I press Win+Shift+S then select the region of the text. reply boomboomsubban 17 hours agoparentprevPiping a screenshot to tesseract seems easy, a quick search shows a plugin available to do so and no shortage of scripts. reply pmontra 10 hours agoprevIt's good but be sure to notice and read the help, to learn how to change colors and sizes. reply whitefang 11 hours agoprevMy default screenshot tool on Linux, Mac and Windows. It works well. Not super cool with aesthetics but it's flawless so yeah. reply phkahler 19 hours agoprevWouldn't it make more sense to separate the screenshot functionality and dump that into a separate editor - configurable which editor. Or do these utilities combine the functionality in some inseparable way? reply procarch2019 19 hours agoparentI actually rebound my windows keys to use this instead of default windows snippet tool. Sure, if I need a high level of editing I’ll bring it into some other program (still using flame shot to take a capture). 95% the built in arrows, boxes, numbers, etc do the quick attention calling I need. Bonus, this was a piece of ‘bloatware’ an admin rebuilt my computer with, but I came to love. reply sdenike 17 hours agoprevThis is great but I wish it had other storage options outside of Imgur. I use Dropshare on Mac and have it upload to my Nextcloud instance which also creates the share URL. reply hobs 20 hours agoprevIf you are on windows ShareX blows almost everything else out of the water imo, the GIF/movie capture feature is great. reply hermitcrab 11 hours agoprevSeems timely given that SnagIt has moved to a subscription model of £37 per year. reply __fst__ 20 hours agoprevI use that daily. Very simple but has exactly the features I need. reply cloudking 20 hours agoprevI'm still looking for a Windows version of https://screen.studio reply linhns 7 hours agoparentBecause of how much optimized it is for MacOS, it may take very long or even never be on Windows. Similar to Zed. reply sdenike 17 hours agoparentprevSame! I haven’t come across anything like it yet. reply julius-fx 10 hours agoprevFlameshot is awesome, works very well with Linux Mint. reply v3ss0n 11 hours agoprevSpectacle from KDE and Flameshot both really good . reply wdfx 7 hours agoparentThe annotate/draw in-place in flameshot just made me convert from spectacle. Also on i3 spectacle is sometimes buggy, it misfires or loses the capture. reply SV_BubbleTime 4 hours agoprevMy long standing request has been SMOOTHING for the hand drawn pencil tool. I want a smoothing slider so I can draw a circle and an arrow with the mouse and not have it look like I have sever palsy with a sclerosis topper. Maybe someday. reply incomingpain 4 hours agoprevFlameshot is awesome. reply anotheryou 11 hours agoprevshareX for windows is king sadly (slight learning curve, but feature rich). miss it under Linux reply arminiusreturns 20 hours agoprevLove flameshot, use it all the time! It was part of my move to try to make as much of my stack GPL as possible. reply dugmartin 18 hours agoprevI use this nearly everyday on my Linux desktop - works great. reply alexzeitler 20 hours agoprevGreat tool, using it for several years already. reply bnj 18 hours agoprevIs there any feature rich open source option for MacOS? I use and enjoy free shot on windows but I’ve struggled to find good options for the mac reply CraftThatBlock 17 hours agoparentI use Flameshot on Linux, and I've found that Shottr works great on macOS. reply elrostelperien 18 hours agoparentprevFlameshot works on macOS, too. reply vaillant 18 hours agoparentprevMacOS has pretty strong screenshot capabilities out of the box; honestly, makes me want the same for Windows. reply cmcconomy 19 hours agoprevgreat app. I use the rectangle, arrow, and numbering annotation tools frequently reply SV_BubbleTime 4 hours agoparentNumbering tool is a nice touch. Click click click, paste in email and reference the numbers… much more clear than arrows and scribbles all over the place. reply dbg31415 18 hours agoprevSkitch is still better. But this is promising. reply Pr0ject217 20 hours agoprev [–] Love it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Flameshot is a free, open-source screenshot software with customizable features, including editing, annotations, and cloud uploads for easy sharing.",
      "It supports a command-line interface for scripting and key binds, and is available on Windows, macOS, and Linux.",
      "Users can download AppImage or Nightly-builds Binaries for older releases, or install via Package Manager for various Linux distributions like Arch, Ubuntu, Debian, and Fedora."
    ],
    "commentSummary": [
      "Flameshot, an open-source screenshot tool, is praised for its features and compatibility with Linux, but lacks screen recording capabilities.",
      "Users discuss integrating Flameshot with Tesseract for OCR (Optical Character Recognition) and zbarimg for barcode decoding, enhancing its functionality.",
      "Various users compare Flameshot with other tools like ShareX, Greenshot, and Ksnip, highlighting preferences based on simplicity, feature set, and platform compatibility."
    ],
    "points": 343,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1718136237
  },
  {
    "id": 40656747,
    "title": "Elixir 1.17 Released: Set-Theoretic Types, New Duration Data Type, and OTP 27 Support",
    "originLink": "https://elixir-lang.org/blog/2024/06/12/elixir-v1-17-0-released/",
    "originBody": "Home Install Learning Docs Guides Cases Blog Elixir v1.17 released: set-theoretic types in patterns, calendar durations, and Erlang/OTP 27 support June 12, 2024 · by Andrea Leopardi · in Releases Elixir v1.17 has just been released. 🎉 This release introduces set-theoretic types into a handful of language constructs. While there are still many steps ahead of us, this important milestone already brings benefits to developers in the form of new warnings for common mistakes. This new version also adds support for Erlang/OTP 27, the latest and greatest Erlang release. You’ll also find a new calendar-related data type (Duration) and a Date.shift/2 function. Let’s dive in. Warnings from gradual set-theoretic types This release introduces gradual set-theoretic types to infer types from patterns and use them to type check programs, enabling the Elixir compiler to find faults and bugs in codebases without requiring changes to existing software. The underlying principles, theory, and roadmap of our work have been outlined in “The Design Principles of the Elixir Type System” by Giuseppe Castagna, Guillaume Duboc, José Valim. At the moment, Elixir developers will interact with set-theoretic types only through warnings found by the type system. The current implementation models all data types in the language: binary(), integer(), float(), pid(), port(), reference() - these types are indivisible. This means both 1 and 13 get the same integer() type. atom() - it represents all atoms and it is divisible. For instance, the atom :foo and :hello_world are also valid (distinct) types. map() and structs - maps can be “closed” or “open”. Closed maps only allow the specified keys, such as %{key: atom(), value: integer()}. Open maps support any other keys in addition to the ones listed and their definition starts with ..., such as %{..., key: atom(), value: integer()}. Structs are closed maps with the __struct__ key. tuple(), list(), and function() - currently they are modelled as indivisible types. The next Elixir versions will also introduce fine-grained support to them. We focused on atoms and maps on this initial release as they are respectively the simplest and the most complex types representations, so we can stress the performance of the type system and quality of error messages. Modelling these types will also provide the most immediate benefits to Elixir developers. Assuming there is a variable named user, holding a %User{} struct with a address field, Elixir v1.17 will emit the following warnings at compile-time: Pattern matching against a map or a struct that does not have the given key, such as %{adress: ...} = user (notice address vs adress). Accessing a key on a map or a struct that does not have the given key, such as user.adress. Invoking a function on non-modules, such as user.address(). Capturing a function on non-modules, such as &user.address/0. Attempting to call an anonymous function without an actual function, such as user.(). Performing structural comparisons between structs, such as my_date = string. Building and pattern matching on binaries without the relevant specifiers, such as > (this warns because by default it expects an integer, it should have been > instead). Attempting to rescue an undefined exception or a struct that is not an exception. Accessing a field that is not defined in a rescued exception. Here’s an example of how the warning for accessing a misspelled field of a struct looks like: Another example, this time it’s a warning for structural comparison across two Date structs: These warnings also work natively in text editors, as they are standard Elixir compiler warnings: These new warnings will help Elixir developers find bugs earlier and give more confidence when refactoring code, especially around maps and structs. While Elixir already emitted some of these warnings in the past, those were discovered using syntax analysis. The new warnings are more reliable, precise, and with better error messages. Keep in mind, however, that the Elixir typechecker only infers types from patterns within the same function at the moment. Analysis from guards and across function boundaries will be added in future releases. For more details, see our new reference document on gradual set-theoretic types. The type system was made possible thanks to a partnership between CNRS and Remote. The development work is currently sponsored by Fresha (they are hiring!), Starfish*, and Dashbit. Erlang/OTP support This release adds support for Erlang/OTP 27 and drops support for Erlang/OTP 24. We recommend Elixir developers to migrate to Erlang/OTP 26 or later, especially on Windows. Support for WERL (a graphical user interface for the Erlang terminal on Windows) will be removed in Elixir v1.18. You can read more about Erlang/OTP 27 in their release announcement. The bits that are particularly interesting for Elixir developers are the addition of a json module and process labels (proc_lib:set_label/1). The latter will also be available in this Elixir release as Process.set_label/1. New Duration data type and shifting functions This Elixir version introduces the Duration data type and APIs to shift dates, times, and date times by a given duration, considering different calendars and time zones. iex> Date.shift(~D[2016-01-31], month: 2) ~D[2016-03-31] We chose the name “shift” for this operation (instead of “add”) since working with durations does not obey properties such as associativity. For instance, adding one month and then one month does not give the same result as adding two months: iex> ~D[2016-01-31] |> Date.shift(month: 1) |> Date.shift(month: 1) ~D[2016-03-29] Still, durations are essential for building intervals, recurring events, and modelling scheduling complexities found in the world around us. For DateTimes, Elixir will correctly deal with time zone changes (such as Daylight Saving Time). However, provisions are also available in case you want to surface conflicts, such as shifting to a wall clock that does not exist, because the clock has been moved forward by one hour. See DateTime.shift/2 for examples. Finally, we added a new Kernel.to_timeout/1 function, which helps developers normalize durations and integers to a timeout used by many APIs—like Process, GenServer, and more. For example, to send a message after one hour, you can now write: Process.send_after(pid, :wake_up, to_timeout(hour: 1)) Learn more Here are other notable changes in this release: There are new Keyword.intersect/2,3 functions to mirror the equivalent in the Map module. A new Mix profiler was added, mix profile.tprof, which lets you use the new tprof profiler released with Erlang/OTP 27. This profiler leads to the soft-deprecation of mix profile.cprof and mix profile.eprof. We added Kernel.is_non_struct_map/1, a new guard to help with the common pitfall of matching on %{}, which also successfully matches structs (as they are maps underneath). Elixir’s Logger now formats gen_statem reports and includes Erlang/OTP 27 process labels in logger events. For a complete list of all changes, see the full release notes. Check the Install section to get Elixir installed and read our Getting Started guide to learn more. Happy learning! News: Elixir v1.17 released Blog Categories Announcements Elixir in Production Internals Releases Important links Development & Team Source code & issues tracker Watch the Elixir mini-documentary! Join the Community Hex.pm package manager @elixirlang on Twitter #elixir on irc.libera.chat Elixir Forum Elixir on Slack Elixir on Discord IDE/Editor support Meetups around the world Jobs and hiring (community wiki) Events and resources (community wiki) © 2012–2024 The Elixir Team. Elixir and the Elixir logo are registered trademarks of The Elixir Team.",
    "commentLink": "https://news.ycombinator.com/item?id=40656747",
    "commentBody": "Elixir 1.17 released: set-theoretic types in patterns, durations, OTP 27 (elixir-lang.org)264 points by clessg 7 hours agohidepastfavorite48 comments dankai 25 minutes agoI've been building my startup 100% fullstack in elixir, and it's been the most wonderful technology I've ever worked with. I'm evangelising all my serious tech friends about how great it is. Now it would be awesome if rabbitMQ and its client would run on OTP 27, would love to upgrade :( reply gregors 6 hours agoprevElixir and Erlang teams are absolutely killing it over the last few years, not to mention all the work done by the library and book authors out there. I've never been more excited about a release. I've been watching commits to both Elixir and OTP for awhile now and I feel Elixir/Erlang has really picked up steam. Thanks to everyone involved for making my life easier! reply mike1o1 1 hour agoprevI've been using Elixir as a backend for a side project (with a Remix frontend) and it's been really pleasant and productive to work with on the backend. I appreciate how productive LiveView can be, but for my specific case I needed to handle poor network connections and LiveView was (as expected) a poor experience. I wish Elixir was able to decouple itself from LiveView in a sense in the minds of developers. Even without LiveView and realtime/channels, just using Elixir as a backend for a simple API has been really fun. reply sbuttgereit 39 minutes agoparentThere are a couple comments to you so far and both of them miss the point. > decouple itself from LiveView in a sense in the __minds of developers__ This isn't a technical knowledge issue or if a given technology should be the default on install. Assuming I understand your original post correctly this is a mindset issue: too many people dismiss Elixir and the rest of the ecosystem because the first thing they think about is LiveView while ignoring the rest of the ecosystem and that's a shame, because it's much more than that... and I would agree with that point. Even Phoenix is more than LiveView. It's possible to have solidly productive, cost effective Elixir applications not involving LiveView or even Phoenix for that matter. The choice of Elixir in the backend should be more common than it is, though I understand some of the conventional wisdom and fears that motivate other choices. reply manchmalscott 1 hour agoparentprevYou can use Phoenix without LiveView by running mix phx.new with the —no-live flag (or manually pull it out of an existing project). reply mike1o1 47 minutes agorootparentYes, I totally get that (and have been doing that). My complaint/suggestion is more around the marketing and messaging. There is so much more to Elixir than LiveView! reply bnchrch 58 minutes agorootparentprevBut too the OPs point. This is not a technology that should be included by default. No matter how cool. Saying this as a die hard Elixir fan who's been using it since 2015. reply aczerepinski 4 hours agoprevFor 10 years I’ve been reading about cool Elixir stuff here. Love the language. I gave up on finding a job in Elixir many years ago though after seeing salaries consistently lower than more mainstream languages. It may be the language I’d want to use most, but salary and cool product are more important to me than tech stack so it may never happen. Still fun to follow from afar. reply ch4s3 3 hours agoparent> seeing salaries consistently lower than more mainstream languages That seems surprising to me as an Elixir developer. Are you looking in the US, or elsewhere? reply acangiano 3 hours agoparentprevSalaries are consistently above other mainstream stacks, partly because most Elixir jobs look for senior engineers. reply neillyons 6 hours agoprevNice feature in this release is the addition of `get_in/1` which works with structs. eg. `get_in(struct.foo.bar)` If `foo` returns `nil`, accessing `bar` won't raise. reply mtndew4brkfst 5 hours agoparentIt was still possible to do this in prior versions of Elixir, just syntactically noisy. Any layer which isn't a vanilla map needs Access.key, as in: get_in(struct, [Access.key(:foo), :bar]) reply pawelduda 6 hours agoprevThis is it, the final piece I wanted. Looking forward to the further stages. Apart from that, the language is 100% feature-complete as far as I am concerned. reply GiorgioG 6 hours agoprevSuper-excited for this release. I wish someone would put some resources into the Elixir IntelliJ plugin. I've tried, but I just can't enjoy using VSCode (vs IntelliJ based IDEs, Visual Studio, etc.) reply bhaney 7 hours agoprevI'm excited about the type system so far, but I'm especially excited to hear about what else the type system is going to enable in the future. I remember José describing this as a \"gradual gradual type system,\" where the gradual type system is going to be gradually added in stages, with this being the first stage. Any cool new type-system-related stuff coming up in the next few stages? I'm especially hoping for newly enabled compiler optimizations. reply neillyons 6 hours agoparentHere is the roadmap https://hexdocs.pm/elixir/main/gradual-set-theoretic-types.h... The next milestone will include a mechanism for defining typed structs. reply pawelduda 6 hours agoparentprevDoesn't gradual type system mean something else here? As in developers being able to iteratively add types while leaving untyped parts for later? At least this is how Ruby's Sorbet describes it reply pjam 6 hours agorootparentYes, but the discussion here was about it being a gradual, gradual type system, as in, the gradual type system being added gradually reply dub_gui 2 hours agorootparentprevYou’re right about what a gradual type system means! A gradual type system allows flexibility in how you approach typing a codebase. It also means that your system incorporates a dynamic type, making your types 'gradual'. Interestingly, gradual set-theoretic types are flexible enough to let you gradually implement type inference for that system. Hence, the gradual gradual type system. :-) reply mhanberg 6 hours agorootparentprevCorrect, a gradual type system is a form of type system and not the method for rolling it out. reply arrowsmith 5 hours agorootparentIt's a gradual type system that they're rolling out gradually. reply tiffanyh 5 hours agoprevHow does this compare to Gleam (strong) typing? https://gleam.run reply weatherlight 5 hours agoparentDifferent type system with different goals. Gleam uses a modified version of the Hindley-Milner Type system. (Which is a rock solid tested type system.) A Set theoretical type system is more expressive(which suits the dynamic nature or Elixir better.) You can do Union and Intersection Types, and Negation Types, among other things, that you can't do with a HM type system. but it comes at the cost of how fast the program can be typed. HM types systems have amazing type inference capabilities and do so at O(n). Im not sure what the time complexity of the algorithm they are using to do the type checking for Elixir programs, but I bet it's more expensive than that. (Which is probably why they are rolling out the type system slowly.) reply nerdponx 5 hours agorootparent> A Set theoretical type system is more expressive(which suits the dynamic nature or Elixir better.) You can do Union and Intersection Types, and Negation Types, among other things, that you can't do with a HM type system. but it comes at the cost of how fast the program can be typed. I doubt that Python's static type hints existing within a formal mathematical framework, but it's interesting that intersection types have been under consideration for a long time now: https://github.com/python/typing/issues/213 reply tiffanyh 5 hours agorootparentprevDoes Gleam check types at compile-time? And Elixir check types at run-time? (If at run-time, would that mean it would slow down your entire app as a result?) reply bo0tzz 5 hours agorootparentMy understanding is that the BEAM vm already has some (optional) runtime typechecks in the form of guards, which this new typesystem will also add for you automatically. And that these guards are actually used to speed things up, for example by JITing more optimized code. reply davydog187 5 hours agorootparentprevBoth the Elixir and Gleam type systems run at compile time (and whenever you recompile during development) There are no plans for runtime checking AFAIK reply arrowsmith 5 hours agorootparentprevElixir checks at compile time. reply throwawaymaths 5 hours agoparentprevGleam's type system is (IIUC) basically h-m bolted onto the beam. elixir's is its own reply sapiogram 6 hours agoprevDoes anyone know what they mean by \"set-theoretic types\"? I'm a PL nerd, but I've never heard this term before. reply kylecazar 6 hours agoparentThe original post Jose wrote announcing the gradual typing project has a good summary of the goal specific to Elixir (2022) https://elixir-lang.org/blog/2022/10/05/my-future-with-elixi... reply gregors 6 hours agoparentprevIf you're into watching talks, this will be well worth your time. ElixirConf 2023 - José Valim - The foundations of the Elixir type system https://www.youtube.com/watch?v=giYbq4HmfGA reply freedomben 5 hours agorootparentThanks, this is a fantastic talk. Long, but well worth it, especially if you work with Elixir reply pjam 6 hours agoparentprevThere’s a paper about it, linked from the blog post: https://arxiv.org/abs/2306.06391 reply aatd86 3 hours agoparentprevIt's the definition of types as set of values. Then you have set operations on these types that are lade available. That translates into subtyping considerations. Cf. Castagna's et.al. For semantic subtyping. reply bobwaycott 6 hours agoparentprevFrom the original paper[0]: > We present a gradual type system for Elixir, based on the framework of semantic subtyping ... [which] provides a type system centered on the use of set-theoretic types (unions, intersections, negations) that satisfy the commutativity and distributivity properties of the corresponding set-theoretic operations. The system is a polymorphic type system with local type inference, that is, functions are explicitly annotated with types that may contain type variables, but their applications do not require explicit instantiations: the system deduces the right instantiations of every type variable. It also features precise typing of pattern matching combined with type narrowing: the types of the capture variables of the pattern and of some variables of the matched expression are refined in the branches to take into account the results of pattern matching. [0]: https://www.irif.fr/_media/users/gduboc/elixir-types.pdf reply williamdclt 6 hours agorootparentsounds like what typescript does, I'm not clear if the Elixir approach is different from \"structural typing\" (as TS calls it) or if they're rediscovering the same thing. Either way, I'm happy it's the way they're going reply josevalim 5 hours agorootparentWhile both Elixir and TypeScript are structural, they are different type systems. A set-theoretic type system is not one that has unions, intersections, negations, but rather one where the foundation of the type system is represented on top of unions, intersections, and negations. Even relations such as subtyping, compatibility, etc. are expressed as set-theoretic. We also have a very different approach to dynamic/gradual typing (which is sound): https://elixir-lang.org/blog/2023/09/20/strong-arrows-gradua... I recommend reading Giuseppe Castagna's work for those who want to dig deeper: https://www.irif.fr/~gc/ reply weatherlight 5 hours agorootparentprevTypescript's type system isn't sound. The Set theoretical type system proposed for Elixir is. reply lolinder 5 hours agorootparentFor more information about what that means, see this playground [0] from the TypeScript docs. PL people often make a big deal about TypeScript's lack of soundness as though it was some kind of mistake, but it was very much an intentional choice given the trade-offs they were making at the time. If Elixir can pull off soundness without compromising expressivity that will be a huge feat, and I'm excited to see it! [0] https://www.typescriptlang.org/play/?strictFunctionTypes=fal... reply zarathustreal 5 hours agorootparentYea I never bought this assertion from the TS team, saying “given the trade-offs at the time” is the same as saying “we’re already backed into a corner by previous decisions” - the decision(s) may have been intentional at each step but the design itself probably was not. Given the choice of sound or unsound, considering that the purpose of a type system is to give certain guarantees, a type system design must always choose soundness to be considered reasonable. That being said, I don’t think it’s possible to “pull off soundness without compromising expressivity” because the expressivity in this context is self-referential types which equate to non-terminating unification logic (and thus, unsoundness). Still, I’m excited to see what they do with this type system! Reminds me a bit of Shen’s type system. reply lolinder 4 hours agorootparentIs the TypeScript team correct in explaining that soundness would exclude functions accepting subtypes as laid out there? If so, it seems like any type system that was meant to be able to type common JavaScript idioms would have to be unsound. reply zarathustreal 1 hour agorootparentSub-types can be represented with algebraic types so I’m not sure that’s necessarily true, for example an abstract class with subclasses can be represented as a sum type reply sergiotapia 11 minutes agoprevCan't say enough good things about Elixir and Phoenix. Now with types coming, it'll get even better. By the way, you hear a lot about the BEAM and it's power - but in my experience you can go a LONG LONG LONG way before you ever have to even think about that part of your stack. Phoenix does such a tremendous job abstracting that for you. You just get the gainz with no effort. Other parts of the stack also just abstract that for you. Case in point: Oban. You get powerful, flexible and easy to use background jobs for essentially free. Right there in Postgres, with your Elixir code. It's crazy. Try it. reply dimitrisnl 6 hours agoprevI'm very happy for this release. I picked up Elixir recently, and the missing a type system was my only pet peeve. reply notemaker 7 hours agoprev [–] Really impressed with how thoughtful _and_ fast they are delivering on their type system. reply widdershins 2 hours agoparent [–] I'm really curious to know if many people with large Elixir problems are finding any issues with the new type system. This is pure curiosity, I don't have a dog in the fight! reply krnsi 1 hour agorootparent [–] José Valim already caught two bugs (one in Phoenix, one in Livebook) because of the type system: https://x.com/josevalim/status/1791409843888111667 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Elixir v1.17 introduces set-theoretic types, providing new warnings for common mistakes and supporting Erlang/OTP 27, while dropping support for Erlang/OTP 24.",
      "The release includes a new Duration data type, Date.shift/2 function, and Kernel.to_timeout/1 function, enhancing date and time manipulation capabilities.",
      "New warnings and type-checking features help developers find bugs earlier, with future releases expected to expand type analysis across function boundaries."
    ],
    "commentSummary": [
      "Elixir 1.17 has been released, introducing set-theoretic types, durations, and OTP 27, which has garnered positive feedback from users.",
      "The new `get_in/1` feature allows working with structs without raising errors when accessing `nil` values, enhancing developer productivity.",
      "The release includes a gradual type system based on set-theoretic types, which improves type safety and expressivity, and is expected to significantly enhance the Elixir and Phoenix ecosystems."
    ],
    "points": 264,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1718190832
  },
  {
    "id": 40651054,
    "title": "Swift's Static Linux SDK: Build Fully Statically Linked Executables for Any Linux Distribution",
    "originLink": "https://www.swift.org/documentation/articles/static-linux-getting-started.html",
    "originBody": "Getting Started with the Static Linux SDK It’s well known that Swift can be used to build software for Apple platforms such as macOS or iOS, but Swift is also supported on other platforms, including Linux and Windows. Building for Linux is especially interesting because, historically, Linux programs written in Swift needed to ensure that a copy of the Swift runtime—and all of its dependencies—was installed on the target system. Additionally, a program built for a particular distribution, or even a particular major version of a particular distribution, would not necessarily run on any other distribution or in some cases even on a different major version of the same distribution. The Swift Static Linux SDK solves both of these problems by allowing you to build your program as a fully statically linked executable, with no external dependencies at all (not even the C library), which means that it will run on any Linux distribution as the only thing it depends on is the Linux system call interface. Additionally, the Static Linux SDK can be used from any platform supported by the Swift compiler and package manager; this means that you can develop and test your program on macOS before building and deploying it to a Linux-based server, whether running locally or somewhere in the cloud. Static vs Dynamic Linking Linking is the process of taking different pieces of a computer program and wiring up any references between those pieces. For static linking, generally speaking those pieces are object files, or static libraries (which are really just collections of object files). For dynamic linking, the pieces are executables and dynamic libraries (aka dylibs, shared objects, or DLLs). There are two key differences between dynamic and static linking: The time at which linking takes place. Static linking happens when you build your program; dynamic linking happens at runtime. The fact that a static library (or archive) is really a collection of individual object files, whereas a dynamic library is monolithic. The latter is important because traditionally, the static linker will include every object explicitly listed on its command line, but it will only include an object from a static library if doing so lets it resolve an unresolved symbolic reference. If you statically link against a library that you do not actually use, a traditional static linker will completely discard that library and not include any code from it in your final binary. In practice, things can be more complicated—the static linker may actually work on the basis of individual sections or atoms from your object files, so it may in fact be able to discard individual functions or pieces of data rather than just whole objects. Pros and Cons of Static Linking Pros of static linking: No runtime overhead. Only include code from libraries that is actually needed. No need for separately installed dynamic libraries. No versioning issues at runtime. Cons of static linking: Programs cannot share code (higher overall memory usage). No way to update dependencies without rebuilding program. Larger executables (though this can be offset by not having to install separate dynamic libraries). On Linux in particular, it’s also possible to use static linking to completely eliminate dependencies on system libraries supplied by the distribution, resulting in executables that work on any distribution and can be installed by simply copying. Installing the SDK Before you start, it’s important to note: You will need to install an Open Source toolchain from swift.org. You cannot use the toolchain provided with Xcode to build programs using the SDK. If you are using macOS, you will also need to ensure that you use the Swift compiler from this toolchain by following the instructions here. The toolchain must match the version of the Static Linux SDK that you install. The Static Linux SDK includes the corresponding Swift version in its filename to help identify the correct version of the SDK. Once that is out of the way, actually installing the Static Linux SDK is easy; at a prompt, enter $ swift sdk installgiving the URL or filename at which the SDK can be found. For instance, assuming you have installed the swift-6.0-DEVELOPMENT-SNAPSHOT-2024-06-06-a toolchain, you would need to enter $ swift sdk install https://download.swift.org/development/static-sdk/swift-DEVELOPMENT-SNAPSHOT-2024-06-06-a/swift-DEVELOPMENT-SNAPSHOT-2024-06-06-a_static-linux-0.0.1.artifactbundle.tar.gz to install the corresponding Static Linux SDK. Swift will download and install the SDK on your system. You can get a list of installed SDKs with $ swift sdk list and it’s also possible to remove them using $ swift sdk removeYour first statically linked Linux program First, create a directory to hold your code: $ mkdir hello $ cd hello Next, ask Swift to create a new program package for you: $ swift package init --type executable You can build and run this locally: $ swift build Building for debugging... [8/8] Applying hello Build complete! (15.29s) $ .build/debug/hello Hello, world! But with the Static Linux SDK installed, you can also build Linux binaries for x86-64 and ARM64 machines: $ swift build --swift-sdk x86_64-swift-linux-musl Building for debugging... [8/8] Linking hello Build complete! (2.04s) $ file .build/x86_64-swift-linux-musl/debug/hello .build/x86_64-swift-linux-musl/debug/hello: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped $ swift build --swift-sdk aarch64-swift-linux-musl Building for debugging... [8/8] Linking hello Build complete! (2.00s) $ file .build/aarch64-swift-linux-musl/debug/hello .build/aarch64-swift-linux-musl/debug/hello: ELF 64-bit LSB executable, ARM aarch64, version 1 (SYSV), statically linked, with debug_info, not stripped These can be copied to an appropriate Linux-based system and executed: $ scp .build/x86_64-swift-linux-musl/debug/hello linux:~/hello $ ssh linux ~/hello Hello, world! What about package dependencies? Swift packages that make use of Foundation or Swift NIO should just work. If you try to use a package that uses the C library, however, you may have a little work to do. Such packages often contain files with code like the following: #if os(macOS) || os(iOS) import Darwin #elseif os(Linux) import Glibc #elseif os(Windows) import ucrt #else #error(Unknown platform) #endif The Static Linux SDK does not use Glibc; instead, it is built on top of an alternative C library for Linux called Musl. We chose this approach for two reasons: Musl has excellent support for static linking. Musl is permissively licensed, which makes it easy to distribute executables statically linked with it. If you are using such a dependency, you will therefore need to adjust it to import the Musl module instead of the Glibc module: #if os(macOS) || os(iOS) import Darwin #elseif canImport(Glibc) import Glibc #elseif canImport(Musl) import Musl #elseif os(Windows) import ucrt #else #error(Unknown platform) #endif Occasionally there might be a difference between the way a C library type gets imported between Musl and Glibc; this sometimes happens if someone has added nullability annotations, or where a pointer type is using a forward-declared struct for which no actual definition is ever provided. Usually the problem will be obvious—a function argument or result will be Optional in one case and non-Optional in another, or a pointer type will be imported as OpaquePointer rather than UnsafePointer. If you do find yourself needing to make these kinds of adjustments, you can make your local copy of the package dependency editable by doing $ swift package edit SomePackage and then editing the files in the Packages directory that appears in your program’s source directory. You may wish to consider raising PRs upstream with any fixes you may have. Contributed by Alastair Houghton Alastair Houghton works on the Swift and Objective-C language runtimes at Apple.",
    "commentLink": "https://news.ycombinator.com/item?id=40651054",
    "commentBody": "Swift Static Linux SDK (swift.org)239 points by mannuch 22 hours agohidepastfavorite195 comments w10-1 21 hours agoThis static SDK is just one example of Swift's new support for user-definable platforms, timed to amplify support for embedded and WASM. Along with the move to a non-Apple GitHub organisation, it represents real progress in extending swift to other platforms. It would be interesting to see if this is used for the AI OS that they are inviting researchers to validate for security purposes. reply janice1999 20 hours agoparentWow, I had no idea people were using Swift for Embedded: https://github.com/apple/swift-embedded-examples reply favorited 19 hours agorootparentIt's relatively new. Apple put out a WWDC session video about embedded Swift today: \"Go small with Embedded Swift\"reply hugodan 20 hours agoparentprevWasm? Care to provide source for that? reply mannuch 41 minutes agorootparenthttps://github.com/apple/swift-for-wasm-examples?tab=readme-... reply andrewjl 17 hours agorootparentprevhttps://swiftwasm.org https://github.com/swiftwasm/swift https://github.com/swiftwasm/WasmKit reply stephen_g 15 hours agoprevThis is awesome, and I believe opens up what I and a bunch of people were wanting which is the ability to run Swift binaries in Alpine containers! I'd seen work on musl support going in but I didn't realise it would all be going so soon. Cross compilation is very nice too to not need your runtime platform to necessarily support the whole toolchain. I'm really glad there's finally support for vanilla Debian and from what I've seen it looks like there are initial Swift packages finally being added in the Debian testing version, which will be nice. Debian is more and more my go-to for development VMs where Ubuntu used to be, so I'm excited for official support! With all this and embedded, I'm really excited about Swift! I've done a lot of embedded in C and I'm super keen to try out Swift on my STM dev boards! reply palata 22 hours agoprev> Additionally, a program built for a particular distribution, or even a particular major version of a particular distribution, would not necessarily run on any other distribution or in some cases even on a different major version of the same distribution. Not sure I understand that. Is it something specific to Swift, or is it exactly what is expected from using shared libraries? Say my Linux distribution distributes some Swift runtime, then the corresponding Swift packages should have been built for this runtime as well. Just like when my Linux distribution distributes a libc, the corresponding packages need to be built for this libc. Right? Still, it's cool that Swift provides static linking. But like in any language, the best IMHO is when the Linux distribution can choose how it wants to distribute a package. I tend to like shared libraries, and already Rust seems to be interfering and imposing its preferences. I am happy if Swift doesn't. reply e63f67dd-065b 21 hours agoparentIt's just classic dependency issues. I'm not familiar with swift specifics, but probably a combination of ABI instability and just plain version incompatibility from one distro to the next with your target program. My opinion is the opposite: I think the old paradigm of distros managing a giant set of system libraries is a bad one, and is how we ended up in the land of docker. Go and Rust made the right decisions here: vendor all the dependencies, and distros can't mess with them. Makes it easier for upstream and the distro. Modern languages are not like C/C++: a single non-trivial rust program can easily depend on 100+ crates, and re-creating crates.io in your package manager is just a bad idea, even putting aside that there's probably major version incompatibilities the moment you go beyond a handful of programs. Look at the disaster that's python package management, that's not where you want to end up. reply jiripospisil 21 hours agorootparent> Makes it easier for upstream and the distro. Until there's a vulnerability in one of the dependencies and now you have to rebuild all of the packages which use it. Specifically for Rust, there's also the fact that most projects use a lock file and if your build process respects it, you now have to wait for the upstream to update it and release a new version (or update it yourself). And if your build process doesn't respect the lock file (or the project doesn't use it) and you just fetch the latest compatible dependencies at the time of build, you now have no idea if you're affected or not by the vulnerability because you don't have the exact resolved versions stored anywhere (https://github.com/rust-lang/rfcs/pull/2801). reply josephg 21 hours agorootparent> Until there's a vulnerability in one of the dependencies and now you have to rebuild all of the packages which use it. Packages get rebuilt all the time. This is fine. As for the rest, it would be cool if binaries shipped with a manifest of some sort naming all the versions of their statically included dependencies. A SBoM of sorts. It would make this sort of vulnerability scanning much easier to do. reply dolmen 21 hours agorootparent> As for the rest, it would be cool if binaries shipped with a manifest of some sort naming all the versions of their statically included dependencies. A SBoM of sorts. It would make this sort of vulnerability scanning much easier to do. Go binaries have that. You can apply \"go version -m\" to any recent Go binary. Also \"govulncheck\". Examples: go version -m $(which go) go install golang.org/x/vuln/cmd/govulncheck@latest go version -m $(which govulncheck) govulncheck -mode=binary $(which govulncheck) for b in $(go env GOPATH)/bin/*; do govulncheck -mode=binary \"$b\"; done reply palata 21 hours agorootparentprev> It would make this sort of vulnerability scanning much easier to do. Scanning, sure. Fixing... surely much harder than with shared libraries. reply josephg 21 hours agorootparentOnly because the build tooling is less mature. It should be pretty easy to programmatically update a lock file, run the tests, and rebuild a package. For rust crates that are compiled and packaged straight from git, you could probably automate that today. reply palata 20 hours agorootparent> It should be pretty easy to programmatically update a lock file, run the tests, and rebuild a package. You still fundamentally need to rebuild (or at least relink) and re-distribute all the packages, whereas with shared libraries... well you just update that one package. reply kibwen 18 hours agorootparentI hear the refrain \"you'll have to rebuild your packages\" a lot in these discussions, and I confess I don't see how this is a problem. Maybe it's a holdover from C and C++ veterans for whom figuring out how to build any given project is a Herculean effort, but for every other language with a first-class build system it's trivial. reply amszmidt 16 hours agorootparentDebian has about 59000 packages. Even rebuilding 1k of that is a significant pain on build infrastructure, specially in one go, if say a vulnerable library is found. You also now get a huge set of users who will now download these 1k of updated packages. Not to mention the utter fun of trying to understand why your system just requires maybe 100 packages to be updated… Compare to upgrading a single shared library. reply tracker1 4 minutes agorootparentMost USERS only run a handful of applications any given month. I think upgrading 1-2 applications is easier to reason with over even a hundred packages. kibwen 15 hours agorootparentprevThe distro repositories are being continuously rebuilt, because packages are receiving continuous updates. In the meantime, as far as I'm concerned dynamic linking is itself a security failure, because if I run a binary I want to know exactly what code is being run, without having to worry about dynamic linking and loading nonsense inserting itself into my process. reply amszmidt 14 hours agorootparentThey aren’t being rebuilt with that kind of frequency as you might think. They get rebuilt when a change in the package is made, not when a change in a dependency is made — which is a huge difference. There are plenty of packages that might not get an update for a long time (month, half year, … whatever their release cadence might be). Dynamic linking makes it _easier_ to handle security, including checking if your program is linking to a broken library — static linking does not have the same means. Statically linked programs don’t mean you know what is being run either, you can always dlopen() and all kind of shenanigans like Go and Rust do. What is a security nightmare is statically linked binaries, you have no clue what they are linked against. reply kibwen 5 hours agorootparentI'm against dlopen as well. If something makes use of LD_LIBRARY_PATH, rest assured that I want it excised from my system. Reproducibly-built, fully-static binaries are what I want, ideally with an accessible SBOM embedded. reply palata 1 hour agorootparentSo all your binaries are statically linked on your system? 100%? reply comex 15 hours agorootparentprev> The distro repositories are being continuously rebuilt, because packages are receiving continuous updates. At large timescales, yes. But not at timescales relevant for rapid security updates. reply eviks 11 hours agorootparentprevDo you know whether someone has made an estimate based on the history of actual vulnerabilities of how much rebuilding would need to be done in a non-shared library approach? reply amszmidt 1 hour agorootparentNot to my knowledge. But consider something like crypto, libssl ... which is linked to almost everything, or libxz, zlib, etc. If there is a bug in a shared library, one can replace it with a interm one very easily, and the whole operating system will use it. One can even replace it with a different implementation assuming ABI compatibility, without having to do \"much\". Enclaves like Go, Rust, Swift (things that insist on using direct sys calls, and eschewing system libraries) would need to implement their own versions, or use FFI, and then you're SOL anyway if you really insist to use static linking... And who knows what kind of bugs might crop up there that don't match whatever anything anyone else uses. Shrug :-) reply tracker1 0 minutes agorootparentFor what it's worth, Rust uses libc by default in Linux. On the other side, there are quite a few self-updating applications out there (browsers), and source control systems like Github will push notifications, and even pull requests for out of date dependencies. So pushing out a new version of a given application is pretty easy. If the application comes through Flathub or self-updates then it doesn't have to wait for the distro maintainers to get the update out. palata 6 hours agorootparentprevIt really sounds like you see it from the point of view of a developer who has never thought about how a Linux distribution works. Yes, for you as a user it's simpler to link statically and not learn about anything else. But distros are a bit more elaborate than that. reply LtWorf 14 hours agorootparentprevFor distributions, it costs a lot of money they don't have to just be rebuilding stuff all the time. For randomly downloaded binaries from the internet it means you will most likely keep using it with the vulnerabilities for years to come. reply LtWorf 21 hours agorootparentprev> Packages get rebuilt all the time. This is fine. You forgot to link the image with the dog sitting in the fire. reply palata 21 hours agorootparentprevI totally agree. Static linking is essentially easier for people who don't want to care. But ignoring security does not mean it solves it, on the contrary. Static linking comes with a lot of issues, just like dynamic linking is not perfect. I really think it depends on the use-case, and that's why I want to have the choice. reply palata 21 hours agorootparentprevAs you say, a typical Rust program can easily depend on hundreds of crates that nobody really checks. That's a security issue. The whole point of a distro is that someone distributes them, so you can choose which distro you want to trust. What I don't like about Rust and Go is that they enforce their preference. I am fine if you want to link everything statically. I just don't want you to force me. reply kibwen 18 hours agorootparent> I just don't want you to force me. Rust supports dynamic linking, and has since well before 1.0. reply palata 6 hours agorootparentIn practice, it feels like everybody assumes that Rust is installed through Rustup and dependencies are statically linked from cargo. I have tried going the dynamic linking way, it's painful when it's not impossible. reply umanwizard 4 minutes agorootparentRegardless of what \"everybody assumes\", you can in fact build rust binaries without using rustup or cargo. Distros do this. Yes, it's more painful for end users than just typing `cargo build`, but that's irrelevant to distro maintainers. akira2501 21 hours agorootparentprevYou're not forced to with Go. CGO exists and requires dynamic linking. reply newZWhoDis 19 hours agorootparentprevMore software should “enforce” their preferences, many decisions are objectively superior and this “anything goes” attitude has done nothing but hurt OSS/Linux adoption. Everything about the shared lib model is stupid, and has contributed to poor Linux market share to date. reply lmz 5 hours agorootparentDynamic linking seems to work fine for Windows or OSX. Maybe it's open source + dynamic linking that's the problem. reply palata 3 hours agorootparentHonestly a problem is that most developers just don't understand how it works. Docker / static linking solves a lack of understanding, mostly. That's why they are popular. reply palata 6 hours agorootparentprevI couldn't disagree more. > Everything about the shared lib model is stupid I don't think we can discuss if that's you're stance. But also I don't think you understand the shared lib model if you think like that. reply umanwizard 21 hours agorootparentprevSome distros do actually break out rust dependencies into separate packages (e.g. Guix does this). It's just that a lot of rust software isn't distributed primarily by distros. reply jiripospisil 21 hours agorootparentFedora and Debian do this too. That's why it sometimes takes longer for a project to be packaged - you literally have to recursively package all of the dependencies first. reply palata 21 hours agorootparentWhich is a feature, and not a bug: some of us want our distro maintainers to actually have a look at what they distribute. reply prmoustache 20 hours agorootparentYes and it doesn't prevent anyone to build software separately, being written in rust or any other language. reply NekkoDroid 19 hours agorootparentprev> Fedora and Debian do this too. IIRC Arch does as well (or at least tries to). reply jiripospisil 10 hours agorootparentArch does for quite a few ecosystems (Python, Ruby, Haskell, ...) but currently not for Rust. reply lynndotpy 21 hours agorootparentprevI feel the same. A huge part of the pleasure of Go and Rust are that I never run into dependency problems. I'm happy to pay the \"Hello World is 4MB\" tax. reply LtWorf 21 hours agorootparentAnd the \"I now have to evaluate thousands of libraries instead of 10 well established framework\" tax? I've seen plenty of bad go libraries. For example their authors will be unaware of isatty() so they will output control codes when piped. If you factor in the time to find a competently written library (and quite possibly write one yourself) it starts to be less convenient. reply OKThatWillDo 20 hours agorootparentprev\"vendor all the dependencies\" What does that mean? reply PaulDavisThe1st 17 hours agorootparentIt means \"add the source for your dependencies to your own codebase and build them with your build system\". reply PaulDavisThe1st 17 hours agorootparentprev> Modern languages are not like C/C++ I think there's a bit of a misconception here. Ardour is written in C++ and depends on 80+ other libraries. The dependency situation may be exacerbated by packaging culture that encourages the use of lots of relatively small dependencies, but it is by no means determined by language or even context. reply palata 6 hours agorootparentI started a small project both in C++ and Rust. When in C++ I had 2 direct dependencies it resulted in 6 total dependencies (direct + transitive). In Rust I had 5 direct dependencies (because some standard stuff is not in the standard lib) and it resulted in... 300 total dependencies. > but it is by no means determined by language or even context. Not sure what you mean there. My feeling is that making it super easy to pull 50 transitive dependencies without realizing it does not help. If you have to manually handle every single dependency, first it forces you to look at them (that's a good thing), and second it encourages you to minimize them. reply PaulDavisThe1st 5 hours agorootparent> My feeling is that making it super easy to pull 50 transitive dependencies without realizing it does not help What is it that makes you think that C/C++ does not do this also? [ EDIT: this also has something to do with the typical size and scope of C/C++ libraries ] reply palata 3 hours agorootparent> What is it that makes you think that C/C++ does not do this also? My experience. I haven't worked in a single C++ project that had hundreds of dependencies, but I encounter those regularly in Rust/npm. reply PaulDavisThe1st 2 hours agorootparentI would suggest that this caused more by ideas about the appropriate scope and scale of libraries in the 2020s than anything specifically connected to the language in use. reply palata 1 hour agorootparentI don't see that in Kotlin for instance. reply LtWorf 21 hours agorootparentprev> the disaster that's python package management, that's not where you want to end up The one where the only sane option is using distribution packages or conda and ignoring anything that the python community comes up with? reply j1elo 20 hours agorootparentI had my Python and pip installed from distribution packages. One day I needed to build something from source, but it needed Meson... Ok installed Meson also from my distro packages! But no, the project required a newer Meson version. Ok let's install Meson with pip! But no, it turns out pip packages themselves can require a minimum pip version!! Go figure. So I couldn't build that program without first pip-installing Meson, and I couldn't pip-install Meson without first pip-installing a more modern version of pip itself. Guess how well it worked when I upgraded pip. Spoiler: not a smooth way to discover that Python packaging is a joke. reply icedchai 20 hours agorootparentWere you using a virtualenv? Or were you pip installing into your distro / system python, like a true savage? reply j1elo 19 hours agorootparentI just knew the basics. So I was just following commands from stack overflow or similar. Silly me, thinking that it would work logically like other languages do! Later I ended up learning about all that stuff with venvs and whatnot, not to mention it's not even a unified solution but there are multiple alternatives, adding to the confusion... All this, no doubt, is a consequence of how terrible the Python packaging story is. Rust, Go, Ruby, Node, and more, I had used without remotely similar hassles. reply icedchai 16 hours agorootparentYou are right. It's not a good experience for new developers, especially given how long Python has been around. reply j1elo 9 hours agorootparentThey are so close, though! I think one lesson Python took is that it's better, even mandatory, to use venv and install stuff in isolated containment. This is exactly what Node/NPM does: \"npm instal\" creates a new dir, \"./node_modules\" and installs stuff there, and next \"npm\" commands will transparently refer to that subdir. This is, in effect, the same thing as a Python's venv! to the point that in my scripts I always name them \"python_modules\", to give other unfamiliar devs a clue of what it is. If \"pip install\" just did this transparently, without introducing the whole concept of venvs to the user, it would be a huge step forward. I cannot imagine how it would suck if NPM didn't make its node_module dir, and instead it forced you to first learn all the idiosyncrasies of module isolation. reply icedchai 20 hours agorootparentprevHave you tried \"poetry\"? reply LtWorf 20 hours agorootparentYes. Having tried it led me to write that comment. reply icedchai 16 hours agorootparentI'm curious what problems you ran into with it? reply manmal 21 hours agoparentprevA Swift program for a particular distribution will dynamically link some system libraries from the distro, and these libs might change on every distro update. They mention in the post that dynamic linking can cause versioning issues. > Say my Linux distribution distributes some Swift runtime That runtime would need to be compatible with the Swift program. Nowadays that’s not a big issue due to ABI stability (https://www.swift.org/blog/abi-stability-and-apple/), but this would close the door on new features that need runtime support (need an OS update then, or the OS must come with multiple runtimes). reply umanwizard 21 hours agoparentprevDynamic linking works fine for software that is distributed by distros, but lots of software isn't. reply PaulDavisThe1st 17 hours agorootparentFor decades, Unix-y software has been distributed with dynamic linking made possible by the use of a startup script that also sets LD_LIBRARY_PATH (which means the dynamic linker will find the libs that come with the software in preference to system-provided ones. reply palata 21 hours agorootparentprevSure. It's great to have the possibility to link statically. My beef with the \"static linking trend\" is that many people (or languages, e.g. Rust) don't want to let me link dynamically, for some reason. Just let me choose! reply tux3 21 hours agorootparentBut they do. They do let you choose. Debian's build of rust packages are linked dynanically, for instance. It's a build setting, you can turn it on. reply LtWorf 21 hours agorootparentNo, crates will be statically linked anyway in general. reply kibwen 18 hours agorootparentRust supports dynamic linking, it's just not the default, so you need to configure a given crate to use it at build time. reply LtWorf 14 hours agorootparentRust supports compiling each crate as a separate .so and then linking all of them? That's not at all how debian packages written in rust are linked. So the person I replied to is incorrect. I suspect you're incorrect as well and what you claim doesn't exist at all, but you're claiming a different thing. reply umanwizard 8 hours agorootparent> Rust supports compiling each crate as a separate .so and then linking all of them? Yes, by passing crate-type=dylib to the compiler. reply mshockwave 21 hours agoparentprev> Not sure I understand that. Is it something specific to Swift, or is it exactly what is expected from using shared libraries? More of a shared library issue I believe. > Say my Linux distribution distributes some Swift runtime, then the corresponding Swift packages should have been built for this runtime as well. Just like when my Linux distribution distributes a libc, the corresponding packages need to be built for this libc. Right? That's correct reply autoexecbat 21 hours agoprevI guess this means it can compete with golang on ease of distribution. Pushes all the complexity away from the end user reply e63f67dd-065b 21 hours agoparentYeah, I'm really glad for the trend of statically linked binaries that came with Rust/Go. No more version incompatibilities, fights with downstream packagers, weird build configs, etc, just distribute the binary and that's it. reply umanwizard 21 hours agorootparentRust binaries (at least on Linux) are not statically linked by default. They depend on libc. reply Cyph0n 18 hours agorootparentFairly easy to build against musl thanks to rustup and cargo (except if you have native deps): https://doc.rust-lang.org/rustc/platform-support.html#tier-2... reply MrDrMcCoy 18 hours agorootparentI've had a 0% success rate compiling other people's Rust apps statically. I always end up with a shared binary or a failed build, even when building with musl. reply kibwen 18 hours agorootparentWhat problems did you have? As someone who's never in his life used musl before, I just now ran `rustup target add x86_64-unknown-linux-musl` and then built ripgrep with `cargo build --release --target=x86_64-unknown-linux-musl` and it worked without a problem. ldd confirms that it's statically linked. I'm surprised it was as painless as it was, I thought I'd have to install musl separately. reply burntsushi 7 hours agorootparentI just tried this myself and I did not have a painless experience. Hah. Without the `musl` Archlinux package installed, I get this: --- stderr configure: error: in `/home/andrew/rust/ripgrep/target/x86_64-unknown-linux-musl/release/build/jemalloc-sys-b7d053053989ba56/out/build': configure: error: C compiler cannot create executables See `config.log' for more details thread 'main' panicked at /home/andrew/.cargo/registry/src/index.crates.io-6f17d22bba15001f/jemalloc-sys-0.5.4+5.3.0-patched/build.rs:351:9: command did not execute successfully: cd \"/home/andrew/rust/ripgrep/target/x86_64-unknown-linux-musl/release/build/jemalloc-sys-b7d053053989ba56/out/build\" && CC=\"musl-gcc\" CFLAGS=\"-O3 -ffunction-sections -fdata-sections -fPIC -gdwarf-4 -fno-omit-frame-pointer -m64 -static -Wall\" CPPFLAGS=\"-O3 -ffunction-sections -fdata-sections -fPIC -gdwarf-4 -fno-omit-frame-pointer -m64 -static -Wall\" LDFLAGS=\"-O3 -ffunction-sections -fdata-sections -fPIC -gdwarf-4 -fno-omit-frame-pointer -m64 -static -Wall\" \"sh\" \"/home/andrew/rust/ripgrep/target/x86_64-unknown-linux-musl/release/build/jemalloc-sys-b7d053053989ba56/out/build/configure\" \"--disable-cxx\" \"--enable-doc=no\" \"--enable-shared=no\" \"--with-jemalloc-prefix=_rjem_\" \"--with-private-namespace=_rjem_\" \"--host=x86_64-unknown-linux-musl\" \"--build=x86_64-unknown-linux-gnu\" \"--prefix=/home/andrew/rust/ripgrep/target/x86_64-unknown-linux-musl/release/build/jemalloc-sys-b7d053053989ba56/out\" expected success, got: exit status: 77 Notice, in particular, that there is `CC=musl-gcc` in the error above. But: $ which musl-gcc musl-gcc not found I think the spoiler here is the fact that ripgrep uses jemalloc when you build with musl on 64-bit: https://github.com/BurntSushi/ripgrep/blob/c9ebcbd8abe48c833... If you comment out those lines and remove the `jemallocator` dependency from `Cargo.toml`, then the build succeeds. I imagine you'll run into this same problem if you enable ripgrep's `pcre2` feature. However, after installing the `musl` Archlinux package (which provides `musl-gcc`, among other things of course), then the above command builds just fine. Including with `jemallocator` or even with the `pcre2` feature enabled. reply freedomben 17 hours agorootparentprev(not directed at parent specifically) Do you have to link againt musl? I have some C applications that I link into a static binary using the glibc-static package (in Fedora). Can the same be used with Rust? reply umanwizard 17 hours agorootparentNothing stops you from linking against glibc statically from rust. It’s recommended to link against glibc dynamically, but that’s just as true for any language. reply fuzztester 17 hours agorootparentprev>the trend of statically linked binaries that came with Rust/Go \"Trend\"? Maybe \"recent reverse trend\" is more appropriate. Static linking preceded dynamic linking by several years, for compiled languages, AFAIK. https://en.m.wikipedia.org/wiki/Static_library https://en.m.wikipedia.org/wiki/Dynamic_linker reply zshrc 21 hours agoprevCrazy that they’re still offering CentOS 7 images. PLEASE DONT USE THEM reply dzonga 20 hours agoprevSwift could've easily replaced python. but the language got complex and is now a baby C++. reply azinman2 19 hours agoparentThose are pretty different languages. One you just run as a script with no compilation and no static typing, the other is fully compiled and linked into a binary. How could they replace each other? And what’s the complexity now that even remotely approaches c++? reply dagmx 17 hours agorootparentFwiw, you can use Swift in a very Python like way. If you give a Swift file a shebang, it’ll execute as a standalone script. I still don’t agree they’re necessarily as overlapped as the other person claims but there is a decent area where Swift is easy enough to replace Python tooling reply pzo 16 hours agoparentprevI think it's too late which is sad since Swift is not a bad language even though I agree it got complex since the last 5 years. Python is mostly used not because of its language features but for cross-platform ecosystem. This is hard and slow to build especially these days (comparing to 10 years ago) since there are many languages and winner takes all or most of the cake. The only way is 1) to provide something really groundbreaking at the time (e.g. ruby on rails) 2) piggyback on other ecosystem by providing great interop (e.g. maybe mojo in the future) 3) slowly grind and hustle until something eventually catchup (e.g. maybe rust in the future) The problem with apple is they are too much focused to wall garden you instead of build ecosystem. Their IBM partnership failed to make something like Rails or Spring Framework for backend. Tensorflow for Swift failed even though it was google project. They don't have cross platform support in their DNA. You don't even get Xcode on Windows or iPad - this is dealbreaker for many people. I would bet more on kotlin (native) / java even though I'm iOS dev myself. Embedded is the exception that they still might win the game since the only competition there is pretty much: C, C++, Rust and sometimes python, js more as a glue. reply glhaynes 15 hours agoparentprev\"Swift is too complex now\" has seemed to have achieved meme status of late but I don't see it. For the average developer, writing modern Swift is generally not any more difficult than it was several years ago (much more often it's easier), it's just that the language can do a lot more now. But you only need to learn those new parts if you're using them. If you're not writing low-level or special-purpose code, you don't need to learn about more recently added things like move-only types, the embedded subset, varadic generics, or macros, etc, etc. And if you are writing things that require those features, you often couldn't have even considered Swift for that purpose before they were added. reply veidr 2 hours agorootparentI'm rooting for Swift, but OMFS (S=Science btw, join my acronym club, it's free) the complexity has doubled? tripled? since I wrote the Swift SDK for my emlpoyer's API, in the Swift 3-4 era. I think it is a lot more difficult now, because the scenario wrt the, uh FUCK, WHAT THE— CEASE ALL MOTOR FUNCT— reply msie 40 minutes agorootparentprev\"But you only need to learn those new parts if you're using them\" - people only use a subset of C++. reply guestbest 17 hours agoparentprevDoes swift still lack a built in package management system? reply newdee 9 hours agorootparentNo, this came with Swift 3 back in 2016. reply dagmx 17 hours agorootparentprevhttps://swift.org/documentation/package-manager/ reply 29athrowaway 21 hours agoprevWhy should I use Swift instead of Rust? reply singularity2001 20 hours agoparentSwift is elegant, beautiful and concise, whereas Rust is an ugly verbose sigil mess. reply manmal 21 hours agoparentprevThe concurrency story is quite good now with Swift 6, and it’s arguably easier to handle than the borrow checker, but still very safe. reply Sytten 20 hours agorootparentThe way I view it as a Rust experienced programmer: Its kinda like if everything in your program was Arc and we didnt have the mess that are the async executors. But I might be wrong. Love rust but god that sometimes I curse the choices that were made and how slow it evolves. reply manmal 10 hours agorootparentI don't have a lot of experience with Rust, but that sounds about right. reply rgreekguy 15 hours agoparentprevBecause it is not Rust. reply Vt71fcAqt7 21 hours agoprevGenuine question: is there any reason to use Swift without iOS/SwiftUI? (Outside of devs who are primarily Swift developers that want to use something they already know for a small project or similar.) reply Mandelmus 21 hours agoparentIt's a beautiful language that's a joy to write. It's safe and ergonomic, and has an extremely powerful type system. I'd say those are good reasons to use Swift. reply oddevan 21 hours agorootparentI'm really interested in exploring it for building web app backends because of this. Being able to have a drag-and-drop distribution is even better. reply dlachausse 20 hours agorootparentI haven't used it yet, but I've heard really good things about Vapor as far as server side Swift goes... https://vapor.codes reply OKThatWillDo 20 hours agorootparentThanks. What is a \"protocol server?\" reply potatolicious 20 hours agorootparentNot a super-expert but I've used Vapor on some personal projects. Do you have a link to the mention for \"protocol server\"? I'm not familiar with the concept but might be able to help. reply Redoubts 10 hours agorootparentI'm guessing they clicked onto the Built with SwiftNIO link and saw this at https://github.com/apple/swift-nio. reply e63f67dd-065b 21 hours agoparentprevThe main reason is that Apple has effectively deprecated Objective-C for ios/macos development going forward. It's not going away for probably another decade+, but all documentation is now in Swift, new APIs are written with Swift in mind, etc. Think of it this way: in 5-10 years, ObjC will be the python2 of Apple development. reply hbn 21 hours agorootparentThe GP was asking specifically about non-Apple development reply airstrike 21 hours agoparentprevNo language feels more pleasurable to write in. It's incredibly expressive and things are just dead simple 99% of the time It's not perfect, but I hope it continues to flourish because it gets a lot right reply wg0 17 hours agoparentprevHaven't written Swift ever but it seems very elegant language. In that spot, only go stands but go although simple but has very ugly corners such as error handling and a weird type system. So yes for me, I might switch to Swift which seems very elegant and readable compared to some other options. reply jimbobthrowawy 19 hours agoparentprevHas nobody made a polyfill for swiftUI on other platforms yet? Or is it too tied to apple OS-level APIs? The language itself is pretty nice, last I used it. Even if some of the method (argument?) names in most SDKs are absurdly long. reply righthand 21 hours agoparentprevNot really no, there are far better languages with better cross platform support. reply petereddy 21 hours agorootparentThere are definitely languages with better cross platform support. But far better languages? I think Swift is a pretty good language and I like to know what you think is far better than it. reply pzo 16 hours agorootparentmaybe not far better as a language but on the same league but with far better cross-platform ecosystem that would be kotlin (native) or java (this days). You have great ecosystem for e.g. backend (Spring Boot) and modern java is also looking better every year. reply josephg 21 hours agorootparentprevWhich languages do you think are both better and have better cross platform support? reply boffinAudio 8 hours agorootparentLua, I would say. Far easier to get it everywhere, and a very nice language, also. reply josephg 4 hours agorootparentLua is nice, but it’s a bit of a weird comparison for Swift. Swift is compiled and lua is interpreted. Swift is statically typed. Swift uses ref counting to Lua’s runtime GC. And Swift has non nullable types by default, classes, enums, ADTs, match expressions and so on. Lua is nice. But it’s chalk and cheese comparing the languages. I can’t think of many cases where you’re trying to decide between lua and Swift. reply criddell 21 hours agorootparentprevOut of curiosity, which language works best for you and your use case? reply righthand 4 hours agorootparentCPP reply anothername12 21 hours agoprevCan I make Gtk GUI with this like you can with cocoa on macOS? reply square_usual 21 hours agoparentYes: https://www.swift.org/blog/adwaita-swift/ reply jkelleyrtp 21 hours agoprevSwift 6 is amazing - I say this as a big Rust person. It seems like Swift is now its own entity outside the Apple bubble and has a few very interesting features: - An \"embedded\" mode that turns off reflection for kilobyte-sized binaries - An upcoming WASM target - An LSP for VSCode - Native C++ interop - Typed \"throws\" - Static linking on linux and the linux sdk - Porting a number of \"core foundation\" / foundation primitives to nix platforms - Distributed actors for concurrency that run on distant machines - Data-race free by default - non-copy RAII generic types Watch the video on Swift 6: https://developer.apple.com/videos/play/wwdc2024/10136/. IMO Swift 6 is a better Go and than Go, and a better Rust for app-dev type stuff, which seems to be what people want to use Rust for. Swift has shipped stuff that Rust has been sitting on its hands over for years: - Anonymous enums/structs in parameters/match statements - Named function arguments - A type of `Send` bound that allows Rcs to be sent between threads (not Arcs) - Swift preview hotreloading - autoclones (without GC) Swift also took a bunch of goodies from Rust *[1] - No garbage collector - Traits via \"protocol\" - ADT for Option/Result - macros - non-copy types enforcing RAII I really really wish Rust had been keeping up with Swift's ergonomics because it's looking like Swift is going to catch up to Rust's cross-platform story much faster. Swift really could end up as the \"do it all\" language for frontend/backend/app dev. [1] By \"took\" I mean if you like these things in Rust you'll see them in Swift reply coldtea 21 hours agoparent>Swift also took a bunch of goodies from Rust - No garbage collector - Traits via \"protocol\" I'd say they took them from Apple's own Objective-C additions who had those for decades before Rust (ARC and protocols). reply favorited 20 hours agorootparentYeah, Objective-C Protocols are so old that they influenced Interfaces in Java 1.0. reply Klonoar 20 hours agorootparentprevThis is correct. reply cube2222 21 hours agoparentprevAgreed that Swift looks quite amazing. There’s some cool talks on how they’re trying to move FoundationDB to Swift and their approach. I think the main problem, and this is a really big problem, is that based on a very shallow investigation I did a couple months back, the ecosystem for server-side stuff barely exists. And in my opinion, the ecosystem is one of the deciding factors when picking a language. reply jkelleyrtp 21 hours agorootparentI have a slight inkling that Apple internally is using Swift for backend stuff somewhere, especially with all the ML stuff they just shipped. Word on the street is their datacenters are full of M-series chips. I could see those servers running Swift + distributed actors + mlx. The move to make swift run on Linux could be part of that work - their sysadmins, devops, and backend folks probably use Vim/VSCode and deploy on Linux. We might see the release of a very good backend framework by apple for Swift that works on Mac/Linux. The new macros they added kinda bolt on serde into the language too - and the distributed actor stuff is basically RPC in the language. reply MBCook 20 hours agorootparentApple deploys a ton of their server stuff on Linux. Plus they appear to be doing all new code in Swift and moving old code over slowly. They’re also using embedded Swift in the Secure Enclaves and other small processors due to its safety, though I think they have an extra layer of automated security checking that they use internally that doesn’t exist in the public builds. But in short, yes. I believe they’re trying to use Swift for everything at this point and getting away from C/C++/whatever even for kernel work. reply manmal 21 hours agorootparentprevYes their “private cloud” runs on Swift, which supposedly increases security because they believe Swift has things nailed by now. reply Klonoar 20 hours agorootparentprevI believe it’s confirmed that they’re doing it with Swift, buried somewhere in the ML post IIRC. reply dagmx 21 hours agoparentprevAs another huge rust fan myself, Swift 6 has really won me over. I find myself reaching for Swift as the default for a lot of things rather than Rust, especially with the advent of C++ Interoperability and the new Swift embedded stack and ownership model. The only thing I really need is a UI story on other platforms, but it’s an area Rust is lacking great solutions too. I suspect the C++ interop might eventually make Swift have great Qt bindings to compete with PySide as an accessible way to make multi platform UIs. reply jclay 18 hours agorootparentThere’s a very minimal example using C++ interop in Qt’s test suite: https://github.com/qt/qtdeclarative/tree/dev/tests/manual/he... I’ve kicked around with the C++ interop a few times. Projects like Qt that use raw pointers extensively seem hard to expose. Ended up having to wrap Qt classes to model as a value type which was straight forward but it’s not nearly as seamless as the Objective-C++ experience reply dagmx 17 hours agorootparentAh, very cool. Thanks for the link and write up reply square_usual 21 hours agoparentprevSwift and Go are on polar ends of the language complexity spectrum. Someone who enjoys Go for what it is, i.e. a dead simple, straightforward language, would not see Swift as being \"better\". reply jkelleyrtp 21 hours agorootparentSome people want generics and ADTs in Go. If you're using go because it's 1) fast 2) expressive 3) concurrent 4) compiles natively then I think Swift is the better of the two. If you just want a very simple language, then sure, Go is there. But I don't think that's necessarily what makes Go, Go. reply square_usual 21 hours agorootparentI would say that yes, in fact, simplicity is characteristic of Go. That is clear from how the go team considers the impact of every new language feature, often choosing not to do things that would've been considered obvious in other languages. Many parts of the community still resent generics, and there is widespread backlash to the new iterators proposal. Most Go enthusiasts wouldn't be fans of Swift and vice versa. reply beanjuiceII 21 hours agorootparentprevafter using swift extensively i did not find it better of the two, in fact it is a complex keyword ridden language that seems bolted together at very many angles, each swift codebase i enter is like a new mystery to unfold because of its extremely rocky evolution reply Klonoar 20 hours agorootparentOne thing that bugs me to this day is reading Swift code without an IDE can be an exercise in “let me build up a mountain of context first”, due to the way enums and types can get shortened. It’s nice to write but a pain to review. Meanwhile if I read Rust or Go, it’s more or less what you see is what you get. I do agree in general that it’s frustrating that Rust hasn’t kept up with some of Swift’s pushes, but then again that’s the difference when you have the richest company in the world involved. For context, I’ve written many apps in Swift, deployed a backend in Vapor, and worked with teams who use the language. I always thought I’d get over it but it just never fully clicked. reply msie 34 minutes agorootparentprevYes! Reading Swift code, even tutorial code, can be difficult. reply kbolino 17 hours agorootparentprevSwift has its strengths against Go but it does not really beat Go at Go's own game. Where are channels and select? Why is async code special? Why do protocols have to be explicitly adopted? Why are errors \"thrown\" instead of returned? reply arthur-st 21 hours agorootparentprevI think it depends on the person. My anecdotal experience is that I'm using Go for hobby projects because with Python at my day job, I wanted a modern language with fancy-ish static typing. Additionally, I wanted it to be equally practical on Win/Mac/Linux, and to not run inside a VM. The obvious first candidate was Rust, but the low-level features were unnecessary overhead. A \"higher-level Rust\" is literally Swift, but the portability is not there. The search basically ended here, so I went with Go as a consolation prize - I don't get a fancy type system to play with, but the development ergonomics, especially with the v2 modules system, are the best I've experienced yet. reply manmal 21 hours agorootparentHave you checked out the static linking option announced yesterday? Portability has increased a lot. reply arthur-st 12 hours agorootparentI have, and I am indeed very hopeful about the WWDC 2024-related Swift announcements. My comment was meant to illustrate my thinking circa a year or two ago. reply christophilus 21 hours agoparentprevHow’s the compilation speed?If it’s going to replace Go, that’s gotta be sub second for a medium sized project on my clunky old laptop. reply josephg 21 hours agorootparentAssuming my experience with Swift 5 is still accurate, compilation speed is at least an order of magnitude slower than Go. It’s a much more powerful language. In my opinion slower compilation is worth it for all of its great features. (Does Go even have parametric enums?). But its definitely a trade off. reply MBCook 20 hours agorootparentThat’s the one big loss from moving off Objective-C. Because the language was so stupidly simple to parse and didn’t have a lot of “we’ll figure that out for you from what you wrote” it could be compiled incredibly quick on today’s machines. I know they’re working on improving it. I’m glad they are. But that speed is something I definitely miss. reply jkelleyrtp 21 hours agorootparentprevSupposedly faster in Swift 6 but I don't have any big swift projects to throw at it. It uses LLVM so it's not going to be as fast as Go's. In Swift 6 they: - Improved parallelism via compilation pipelining (ie top-level object files are referencing not-yet-compiled symbols but can still proceed while those compile in the background) - Parallelized and pipelined debug symbol generation (lazy debug symbols) reply coldtea 21 hours agorootparentprevWhy, would you care if it's a full second or even 5 seconds? Go hyped their compilation speed a lot in its early years, but unless it's C++ templates level slugginess it's not like it's that big of deal. reply square_usual 21 hours agorootparentCompiling fast has ripple effects throughout your workflow. Having your iteration time go from 1s to 5s is a massive difference: it can mean you break out of the flow state when you restart your server to test your code. reply manmal 20 hours agorootparentAs someone dealing with Swift everyday (and, before that, faster langs like Elixir and Typescript), I can only agree. Sometimes my incremental builds take 20-30 seconds and it feels like a huge toll. Waiting on the CI (which is a weak machine) is up to 25 minutes until the test suite has passed - a large chunk of that being compilation time. Releases take 5-10 minutes. I really like Swift, but I’d highly appreciate faster compilation times. reply iainmerrick 19 hours agorootparentprevOnce you use a language where compile times are essentially invisible, it’s hard to go back. reply plorkyeran 19 hours agorootparentprevSwift compilation is slower than C++. reply coldtea 18 hours agorootparentC++ with heavy template use or C++ as glorified C-with-classes? reply plorkyeran 16 hours agorootparentC++ with heavy template use. reply wg0 17 hours agoparentprev> IMO Swift 6 is a better Go and than Go, and a better Rust for app-dev type stuff, which seems to be what people want to use Rust for. This is a great assessment. There are some shortcomings in go that I wish weren't and I'm kind surprised that people having decades of language design experience under their belt ended up designing go. I'm looking for a way to generate server stubs from OpenAPI specs. If that, I'm all set with Vapor+OpenAPI and would retire go for my container adventures. reply glhaynes 15 hours agorootparentI'm looking for a way to generate server stubs from OpenAPI specs I don't have experience with OpenAPI, but is this what you're looking for? https://www.swift.org/blog/introducing-swift-openapi-generat... reply wg0 9 hours agorootparentYou're right. This actually is first class Apple[0] and can't get any better than that whereas with Go, I am stuck with so many third party packages. They work well but kinda... not so elegant. Decent enough given what the language can offer. [0] https://github.com/apple/swift-openapi-generator reply pcwalton 16 hours agoparentprevSwift has a garbage collector. Reference counting is a form of garbage collection. reply mdhb 21 hours agoparentprevYou almost caught up to Dart. - Built for embedded devices and already has RISC-V support - Already has WASM was a compilation target - Not just an LSP but amazing VSCode tooling - Native C, C++, Java, Rust and JavaScript interop and a .NET bridge is in the works it seems. - Typed \"throws\" - Static linking (in development) - Data-race free by default Unique features to Swift: - Distributed actors for concurrency that run on distant machines. (This is doable but not native to the language) - non-copy RAII generic types (not actually sure what this is so I assume it’s not in Dart) reply almostgotcaught 20 hours agorootparent> - Native C, C++, Java, Rust and JavaScript interop dart doesn't do C++ interop reply manmal 20 hours agorootparentprevIsn’t “data-race free” in Dart achieved by just not really sharing mutable memory between isolates? Probably because the web version needs to map isolates to web workers, right? reply mdhb 20 hours agorootparentFor now but this is in the pipeline too. https://github.com/dart-lang/language/blob/main/working/333%... reply manmal 11 hours agorootparentGreat writeup, and I love that the authors looked at a lot of other languages. The solution section on how to prevent races is a bit light still - I‘m curious how they will solve this. I think there will need to be exceptions to a strict race-free model eto not take away from dart‘s current capabilities, and without forcing people into something like a borrow checker. Swift has `@unchecked Sendable` which lets you tell the compiler \"I promise I'll take care myself to keep everything in there race-free\". What I‘ve been missing in Swift are tools that let me add safety _within_ such @unchecked Sendables. There's the Swift Atomics lib: https://www.swift.org/blog/swift-atomics/, but its quite limited in its capabilities. So for now, I find myself reaching for tools provided by the community, like pointfree's LockIsolated: https://github.com/pointfreeco/swift-concurrency-extras/blob.... reply e63f67dd-065b 21 hours agoparentprevI'm primarily a C++ developer these days, and if the C++ interop is actually good then I'm very excited for the future. For app-dev type stuff it really does seem like it's better than Rust, and I can definitely see a world where we start migrating some code into Swift. Edit: back from watching the video, and a few observations: - Swift does not suffer from Rust's refusal to add things into the standard library. gasp regex support in stdlib? Async/await runtimes? - Default testing framework is a godsend, especially when integrated into the build system - Rust but better for everything that's not low-level systems development is the impression I get reply glhaynes 21 hours agorootparentI'm not a (direct) user of the C++ interop, but everything I've heard about it has been quite positive. A few hopefully relevant resources, first a couple of videos: - \"Introducing a Memory-Safe Successor Language in Large C++ Code Bases\" (John McCall; CppNow 2023) https://www.youtube.com/watch?v=lgivCGdmFrw - \"Swift as C++ Successor in FoundationDB\" (Konrad Malawski; Strange Loop 2023) https://www.youtube.com/watch?v=ZQc9-seU-5k And then a series of blog posts by Doug Gregor: - \"Swift for C++ Practitioners\" https://www.douggregor.net/posts/ And finally: - \"Mixing Swift and C++\" (Swift.org) https://www.swift.org/documentation/cxx-interop/ reply MBCook 20 hours agorootparentprevApple is using it for some low-level stuff and they appear to be trying to improve that every year. I think they want to be able to use a limited subset of it in their kernel. Actually I think they already are but I’m not positive. Obviously it started as an application level language but I do think they’re trying to move it down the stack as well. reply jkelleyrtp 21 hours agorootparentprevThe default testing framework made my jaw drop a bit - I have a very hard time seeing Rust have the enthusiasm or momentum to implement something like that right now. reply kibwen 17 hours agorootparentBut Rust has had a default testing framework since 1.0? reply glhaynes 21 hours agorootparentprevWhy do you think they wouldn't? (I don't keep up with Rust+its community closely enough to know.) reply lll-o-lll 20 hours agoparentprev> No garbage collector No tracing gc. Ref counting is the other kind of garbage collector. reply KurtMueller 21 hours agoparentprevAlgebraic data types are not a Rust invention and aren't exclusive to that language. reply MBCook 20 hours agorootparentI don’t think they were trying to claim that, just that it was a feature that was in Rust that they liked and wanted to keep using. Just like Rust didn’t invent not using garbage collection. reply heavyset_go 21 hours agoparentprevGiven there is native C++ interop, is exporting a C ABI still a seemingly experimental/niche feature? Same question with interop between C and Swift. Recently went with using Kotlin Native over Swift on macOS for this reason, the C ABI export and C interop were painless. reply MBCook 19 hours agorootparentSwift has painless interop with Objective-C, which is just C plus small talk. C should already be pretty easy, though I haven’t really used it myself. I only use the Objective-C stuff. I know they’ve been working on C++ but don’t have any experience to comment. Someone else had a nice comment listing resources on that. reply vips7L 20 hours agoparentprev> - No garbage collector ARC is a GC, and a slower one than most traditional tracing GC's. reply MBCook 19 hours agorootparentReference counting and garbage collection are different. They’re both forms automatic memory management, but they work in very different ways. reply pcwalton 16 hours agorootparentAutomatic memory management is a synonym for garbage collection. Reference counting is a form of garbage collection. reply kibwen 17 hours agorootparentprevReference counting and garbage collection are different, yes. But in terms of classifying \"no garbage collection\" under \"things that Swift took from Rust\", Swift's pervasive reference counting is much closer to classic tracing GC than it is to Rust's system of single ownership. reply vips7L 17 hours agorootparentprevNo they're not. Reference counting is a garbage collection algorithm. Its even described as such in most literature on garbage collection. reply stux 19 hours agoparentprev> Anonymous enums/structs in parameters Can you share an example of this? It sounds really interesting but I couldn’t find any references. Do you mean the parameter “packs” features? reply JackYoustra 20 hours agoprevSwift tooling has some really! Sharp! Edges!!! If you need something quick and easier than rust or have some reason to want to take your swift code and run it elsewhere I guess this is good but I don't see why you'd use it besides that. reply akdor1154 18 hours agoparentIt does, but it's also a really, really nice language. I'm excited to see Apple work to file those edges down. (and i say this as someone who is a Golang and Linux fan and owns zero iThings) reply JackYoustra 15 hours agorootparentCompletely. The language philosophy is much easier than rust imo reply gh123man 19 hours agoparentprevI'd love to hear some specific examples. Ive built several iOS apps and a whole backend (on linux) with Swift and other than lack of OSS library support for some SaaS APIs, it's been quite nice. Sure Swift itself has some sharp edges, but not any more (or worse) than many other popular languages. reply JackYoustra 15 hours agorootparentSure! I love talking about this stuff :) here's a few: - if you want to manager your Package.swift target, you mostly have to deal with .target enum (can't figure out how to paste the URL) You can really only do a define or unsafeFlags, which leads to a couple issues... 1) There's only Debug and Release configurations! What if you want more? What if you want a nice flow where you emit and consume pgo data and re-ingest it? 2) What if you want to use a define for another language managed by SPM, such as metal? 3) What if you want to put in a portable path into a linker argument? SPM blocks use of build-time environment variables, so you can't do that either. All of these things seem contrived, but I ran into all three of them at my current job (shameless NanoFlick plug). All of these can be handled by cmake or xcodebuild (although probably better to use rules_xcodeproj). - Swift compiler feedback and performance in builders is absolutely atrocious, to the point where binary searching a view to find the actual source of a type error over ~30 second build times isn't very unusual! - It's very overly conservative with build times, and because there's only module-level import, it's very easy to accidentally have a long dependency chain which makes building one file imply building many files, further tanking your build time. There are some forum posts I can dig up on this if you're curious more on this point. - POD structs are default-Sendable at module-level visibility, but lose that if they're public, which means if you try modularizing your app into different small packages to restore some notion of compilation units to fight against the above two points, you end up having to go around and mark all of your different classes with default protocols (usually just Sendable) to reimplement! - Not a huge deal, but no way to have macro_rules! like you can in rust: macros are kinda hard to use and have to be in another package. This is just my thoughts on tooling, I have many more thoughts about the language itself that I'd be happy to share. Honestly though it's much better with vscode. I do hope that it makes progress in the right direction, Swift has a lot of things going for it! reply rescripting 19 hours agoparentprevCould you elaborate? reply JackYoustra 15 hours agorootparentYes! See my reply above reply thowdfasdfq 20 hours agoprevnext [4 more] [flagged] mardifoufs 18 hours agoparentI get rust and swift (maybe) but kotlin is basically on par with java at this point. Maybe 2.0 will create a wider gap but if Java 21 has nice things that kotlin doesn't have, and when you add those up there's no clear winner imo. reply tadfisher 17 hours agorootparentJava 21 can't have things that Kotlin doesn't have, because you can call any Java method from Kotlin. On the other hand, Kotlin can target native, JS, and WASM platforms as well as the JVM, which is something Java does not do (barring alternative VMs like Graal). reply janice1999 20 hours agoparentprevWhy though? They don't fulfill the same niches. None of those languages have a data science and ML ecosystem like Python. Rust and Swift lack mature cross platform GUI frameworks etc. Also Python is getting exciting again. 3.12 can optionally remove the GIL and add a JIT. reply 1vuio0pswjnm7 19 hours agoprev [–] This website is sending gzip _and_ ignoring the client Accept-Encoding header. Specifiying \"identity\" has no effect. Ignoring the Accept-Encoding header is common but sending gzip compressed response body and _also_ ignoring the header, thereby making it impossible for the client to disable compression using Accept-Encoding: identity, is relatively rare. Another example that comes to mind is www.amazon.com. However in that case one can disable compression by sending a Viewport-Width header. reply 1vuio0pswjnm7 16 hours agoparentThey have now fixed it after I submitted this comment. No longer sending gzip by default. Thanks for that. Someone tell Amazon they should do the same. reply cqqxo4zV46cp 19 hours agoparentprev [–] There is, in reality, nothing unusual about that. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Static Linux SDK allows Swift programs to be built for Linux without needing the Swift runtime and dependencies on the target system, creating fully statically linked executables.",
      "It supports development on macOS with deployment to Linux, eliminating runtime overhead and versioning issues but resulting in larger executables and no code sharing.",
      "The SDK uses the Musl C library for static linking, requiring adjustments for packages using the C library, and can build binaries for x86-64 and ARM64 machines."
    ],
    "commentSummary": [
      "Swift's new static SDK supports user-definable platforms, including embedded systems and WebAssembly (WASM), showcasing its expansion beyond Apple ecosystems.",
      "Swift's move to a non-Apple GitHub organization and its use in AI OS for security validation highlight its growing versatility and adoption in various tech domains.",
      "The development allows running Swift binaries in Alpine containers, cross-compilation, and support for vanilla Debian, exciting developers who prefer Debian for development virtual machines (VMs)."
    ],
    "points": 239,
    "commentCount": 195,
    "retryCount": 0,
    "time": 1718137518
  },
  {
    "id": 40651203,
    "title": "Elon Musk Withdraws Lawsuit Against OpenAI and Co-Founders",
    "originLink": "https://www.cnbc.com/2024/06/11/elon-musk-drops-suit-against-openai-and-sam-altman.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS CNBC TV MAKE IT SELECT USA INTL WATCH LIVE Search quotes, news & videos WATCHLIST SIGN IN TECH Elon Musk drops suit against OpenAI and Sam Altman PUBLISHED TUE, JUN 11 20244:36 PM EDTUPDATED MOMENTS AGO Hayden Field @HAYDENFIELD KEY POINTS Elon Musk on Tuesday withdrew his lawsuit against OpenAI and two of the company's co-founders, Sam Altman and Greg Brockman, in California state court. A hearing was scheduled for Wednesday in San Francisco, in which the judge was going to consider whether the case should be dismissed as requested by the defendants — including Altman and Brockman, the current CEO and president of OpenAI, respectively. Experts told CNBC in March that the case was built on a questionable legal foundation, because the contract at the heart of the suit was not a formal written agreement that was signed by all parties involved. In this photo illustration, the logo of 'OpenAI' is displayed on a mobile phone screen in front of a computer screen displaying the photographs of Elon Musk and Sam Altman in Ankara, Turkiye on March 14, 2024. Muhammed Selim KorkutataAnadoluGetty Images Elon Musk on Tuesday withdrew his lawsuit against OpenAI and two of the company's co-founders, Sam Altman and Greg Brockman, in California state court. Musk's decision to file to dismiss the suit came just one day after he publicly criticized OpenAI and its new partnership with Apple. The case was dismissed without prejudice, according to a court filing obtained by CNBC. In February, Musk had filed a lawsuit against OpenAI, Altman and Brockman — the current CEO and president of OpenAI, respectively — for breach of contract and fiduciary duty. A hearing was scheduled for Wednesday in San Francisco, in which the judge was going to consider whether the case should be dismissed as requested by the defendants. Experts told CNBC in March that the case was built on a questionable legal foundation, because the contract at the heart of the suit was not a formal written agreement that was signed by all parties involved. Rather, Musk had alleged that the early OpenAI team had set out to develop artificial general intelligence, or AGI, \"for the benefit of humanity,\" but that the project has been transformed into a for-profit entity that's largely controlled by principal shareholder Microsoft. Musk had used much of the 35-page complaint (plus attached exhibits) he filed in March to remind the world of his position in the creation of a company that's since become one of the hottest startups on the planet, (OpenAI ranked first on CNBC's Disruptor 50 list in 2023) thanks largely to the viral spread of ChatGPT. \"It's certainly a good advertisement for the benefit of Elon Musk,\" Kevin O'Brien, partner at Ford O'Brien Landy LLP and former assistant U.S. attorney, told CNBC at the time. \"I'm not sure about the legal part though.\" Last year, Musk debuted his own AI startup and OpenAI competitor, xAI, which last month announced a $6 billion Series B funding round. Investors included Andreessen Horowitz, Sequoia Capital and Fidelity Management & Research Company. X.AI seeks to \"understand the true nature of the universe,\" according to its website. Last year, X.AI released a chatbot called Grok, which the company says is modeled after \"The Hitchhiker's Guide to the Galaxy.\" The chatbot debuted with two months of training and has real-time knowledge of the internet, the company claims. Representatives for Musk and Altman did not immediately respond to a request for comment. —CNBC's Lora Kolodny contributed to this report. Correction: The lawsuit was filed in February. An earlier version of this story misstated the month. Subscribe to CNBC PRO Subscribe to Investing Club Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Advertise With Us PLEASE CONTACT US Privacy Policy CA Notice Terms of Service © 2024 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=40651203",
    "commentBody": "Elon Musk drops suit against OpenAI and Sam Altman (cnbc.com)201 points by hggh 22 hours agohidepastfavorite212 comments jcranmer 21 hours agoFrom the docket (https://webapps.sftc.org/ci/CaseInfo.dll?CaseNum=CGC24612746, if links work correctly), it seems that this is Musk dismissing the case without prejudice (i.e., he can litigate the claims again if he wants) on the eve of a hearing asking the case to be dismissed with prejudice (i.e., precluding him from litigating the claims again). Which makes me think this is a \"you can't fire me because I quit\" kind of action. reply rmbyrro 21 hours agoparentI think he feels bad for leaving OpenAI and is now jealous of what they accomplished. He probably just wanted to cast legal doubt over OpenAI's ability to profit from closed-source models. Which he managed to accomplish. If this hurts OpenAI's prospects to raise money is yet to be seen. I hope not... reply beambot 20 hours agorootparentI'd be pretty peeved if I backed a non-profit through its most-risky early days only for them to turn around and shun the original philanthropic mission in the spirit of profit maximization... reply tw04 20 hours agorootparentNot quite how that played out. He wanted complete control, they told him no, he pulled his money in the hopes of bankrupting them. When he reneged on the $1 billion he pledged in funding they were forced to seek funding elsewhere (Microsoft). https://www.theverge.com/2023/3/24/23654701/openai-elon-musk... reply indoordin0saur 20 hours agorootparentOpenAI went from a not-for-profit institution seeking to build in a safe and fair way that wouldn't advantage a handful of tech elite to very much the opposite. We can speculate on Musk's own motivations or call him a hypocrite but he's not wrong in pointing out OpenAI's sketchy behavior. reply qarl 16 hours agorootparentThey needed Musk's pledge of $1B to remain non-profit. When he took back that money, their only choice was to either A) shut-down or B) go profit. I think most people (aside from Elon) are happy they didn't shut-down. reply adrr 19 hours agorootparentprevHe formed x.ai. Is it non profit? reply Alupis 19 hours agorootparentThat would only be a \"gotcha\" if he claimed it was going to be a non-profit all along, then pulled a switch-a-roo a la OpenAI style... reply KerrAvon 20 hours agorootparentprevWe don't have to speculate on his motivations. He's literally bankrolling a competitor. reply llm_trw 19 hours agorootparentYou're still missing the part where a not for profit literally turned itself into a multi billion dollar for profit company and no ones been held accountable. reply throwup238 19 hours agorootparentNonprofits have to do that if they engage in commercial activity for tax reasons. You can't just group whatever activities you want under a nonprofit and make all that money tax free. OpenAI had to split it off as a for-profit once they had a product to sell to the public. The nonprofit didn't \"turn into\" a for-profit, the nonprofit owns the corporation. Mozilla has the same structure with a for-profit corporation owned by the nonprofit, which allows it to take Google's money in exchange for default search engine placement. Likewise with Wikipedia/Wikimedia, Newman's Own, and so on. reply mr_toad 17 hours agorootparent> the nonprofit owns the corporation They only own about 2% of it. The rest is split between Microsoft and other investors. reply dragonwriter 4 hours agorootparentThe nonprofit has 100% control of the for-profit entity, though it does not have 100% of equity. reply llm_trw 19 hours agorootparentprevYou absolutely can. You just can't distribute the results as dividends. reply throwup238 19 hours agorootparentNo, you absolutely can't. Nonprofits can't engage in substantial business activities that are unrelated to their mission. If they do, they can lose their non-profit status. https://www.irs.gov/pub/irs-pdf/p598.pdf https://www.thetaxadviser.com/issues/2021/jun/unrelated-busi... reply llm_trw 19 hours agorootparentI'm trying to wrap my head around your point and having a real hard time understanding what the problem is. Yes, openai can't be a butcher, a baker, or a candlestick maker. This is a feature not a bug. reply throwup238 19 hours agorootparentThe problem is that you don't understand how tax exemption works? According to the OpenAI charter, their mission is to \"ensure that artificial general intelligence benefits all of humanity.\" That's their legally binding mission - not selling cloud services by the token or licensing their models to companies like Microsoft and Apple. Any lawyer with half a brain would have advised them that those activities would almost certainly be found by the IRS and the courts to be unrelated business activities, so they spun them off as a for-profit corporation instead of taking an entirely unnecessary risk. Just because their product is vaguely AGI related doesn't mean it has anything to do with their actual mission. reply llm_trw 18 hours agorootparentI've ran a not for profit worth ~$100m. All of those things are ones which fall within the original charter of openai. It's only when you start doing completely unrelated activities, like Mozilla's women who code, or you start raiding the coffers of the organization, like Wikipedia, that you need fancy structures to hide the blatant theft. reply throwup238 18 hours agorootparent> I've ran a not for profit worth ~$100m. Based on this thread I find that incredibly hard to believe. Incredibly. > It's only when you start doing completely unrelated activities, like Mozilla's women who code Or like... Mozilla Firefox. Cause that's why they've had Mozilla Corporation since 2005. Google pays the corporation for default search engine placement and all of the full time Firefox developers I've ever known have worked for the corporation, not the nonprofit. reply llm_trw 12 hours agorootparent>Based on this thread I find that incredibly hard to believe. Incredibly. It doesn't take much more than showing up and taking responsibility when no one else does. >Or like... Mozilla Firefox. Cause that's why they've had Mozilla Corporation since 2005. Yes, because Mozilla has always wanted to do more than be a browser company. Remember Firefox OS? Not for profits who have weird structures are there to make sure they can be looted for the benefit of the people running them, or to be used for pet projects that have nothing to do with the original charter. reply agolio 9 hours agorootparent> It doesn't take much more than showing up and taking responsibility when no one else does. Your argument aside, I am interested to hear you found it straightforward to run a successful non-profit. Any tips for someone who is interested to enter that space? reply llm_trw 6 hours agorootparentTalk to people, be helpful, find what the pain points of the organization are and solve them. If you don't turn it into a personal fiefdom expect hard work, no gratitude and eventual burnout. reply jcranmer 19 hours agorootparentprevElon Musk kept giving OpenAI money after they announced they did that (even admitted that in the complaint!), which makes me suspect he's not as miffed about that decision as he claims to be. reply llm_trw 19 hours agorootparentNotice how many times Elon Musk appears in my comment. reply jcranmer 19 hours agorootparentSince we're commenting on an article about Musk's suit against OpenAI purportedly seeking the accountability the lack of which you decry, it is reasonable to assume even in the absence of a direct mention that you are expressing an opinion on the merits of Musk's suit, at least re seeking accountability of OpenAI. Especially when you comment is a direct reply to someone talking about Musk. reply llm_trw 19 hours agorootparentYou should assume less and read more. reply xcv123 20 hours agorootparentprev\"in the hopes of bankrupting them\" Where do you get that idea? How exactly were they entitled to his $1B donation? reply IgorPartola 20 hours agorootparentThe person you are replying to said nothing about being entitled to the money. You are moving goal posts. This is the equivalent of me coming to you and promising you money so that you can pull off a risky business move, then at the critical moment when I don’t like something pulling that money. Sure I might be entitled to do that as it is my money, but it’s a dick move and would certainly leave you holding the bag. reply xcv123 20 hours agorootparent\"it’s a dick move\" implies that you are somehow entitled to that money. Otherwise it's a fair move. It is extremely delusional to expect to receive $1 billion dollars unconditionally. reply bcrosby95 20 hours agorootparentAs my dad used to say: being an asshole isn't illegal. reply xcv123 19 hours agorootparentThey declined to give Elon Musk control when he offered them a billion to save the company. Instead they turned around and gave up control to Microsoft for billions. Do you honestly believe that if you were in Elon Musks shoes, that you would have given away a billion dollars without any conditions? How would you feel when you saw them turn around and sell out to Microsoft instead? Expecting $1B unconditionally seems like an asshole expectation. Delusional and childish entitlement. Do you believe that Microsoft has donated billions to OpenAI unconditionally? reply qarl 18 hours agorootparent> They declined to give Elon Musk control when he offered them a billion to save the company. On the contrary, it was the fact Elon pulled the $1B he had previously offered that put the company in jeopardy. https://www.semafor.com/article/03/24/2023/the-secret-histor... reply adventured 19 hours agorootparentprev> How would you feel when you saw them turn around and sell out to Microsoft instead? They did not give up control to Microsoft. If that were true, the Apple deal (which Microsoft opposed) would not have just happened. reply xcv123 18 hours agorootparentHaving Apple dependent on and paying for Microsoft infrastructure is a win for Microsoft. Of course the deal benefits Microsoft as much as Apple. Microsoft needs Apple as a strong competitor. Otherwise they will be declared a monopoly and broken up by regulators. Microsoft was smart to use OpenAI as a proxy for this deal. reply qarl 18 hours agorootparent> Having Apple dependent on and paying for Microsoft infrastructure is a win for Microsoft. LOL! I would love to see your evidence that Apple's new AI offering will be running on Microsoft's infrastructure. reply xcv123 17 hours agorootparentIt was a public announcement. https://openai.com/index/openai-and-apple-announce-partnersh... OpenAI models run on Azure infrastructure. Apple is just a client and will be paying Microsoft via OpenAI to run it. Apple also has their own models running on their own infrastructure. I'm not saying that Microsoft or OpenAI has anything to do with that. reply qarl 17 hours agorootparentThat's SO WEIRD. When I load that page, I see no mention of Azure. reply xcv123 16 hours agorootparentnext [6 more] [flagged] qarl 16 hours agorootparentOpenAI's current infrastructure runs on Azure. Do you have any evidence that their new Apple services will run on Azure? reply qarl 16 hours agorootparentprevIt's impossible to discuss this with you as you keep changing your comment. I never once implied it would run on Apple servers. You're putting words in my mouth. I am asking if you have any direct evidence that they'll be running on Azure. The fact that you keep dodging that question tells me you don't. reply qarl 16 hours agorootparentprev> Holy shit you are a fucking moron. I posted direct links to everything. Holy shit you are a fucking moron! None of your links say anything about Apple's new services. I think it's safe to say we're not going to find common ground here. You have a nice day. reply qarl 16 hours agorootparentprev> To answer your question below: Apple is a customer of OpenAI. They are using OpenAI as a service. The OpenAI models won't be running in Apple data centers. Don't move the goalposts. I asked you if you had evidence that it will run on Azure. reply qarl 16 hours agorootparentprevFYI - It's dishonest to keep changing your comment. reply threeseed 17 hours agorootparentprevThis comments belongs in a time capsule. Microsoft was a breakup risk back when all that existed were computers and they had 95+% market share. reply andrewflnr 19 hours agorootparentprevIt's a dick move to give people false expectations when you don't have to. It's a dick move to break your word. He didn't \"have\" to give them the money, but he didn't have to tell them he would give it to them either. reply elevatedastalt 19 hours agorootparentprevHe didn't offer them false expectations. On the contrary, he withdrew money when he saw that they had offered him false promises (of being non-profit) reply qarl 19 hours agorootparentPretty sure you have the order of events backward there. The story I heard was that Musk pulled the $1B and THEN they were forced to go profit in order to survive. https://www.semafor.com/article/03/24/2023/the-secret-histor... reply stale2002 20 hours agorootparentprev> How exactly were they entitled to his $1B donation? They weren't. But neither is Musk entitled to anything from OpenAI and the lawsuit was BS. He pulled his donation because he wasn't given total control, and now he is being a sore loser about it with a failed lawsuit. reply xcv123 20 hours agorootparentFair enough. But I would do the same if I were in his shoes just out of spite for selling out to Microsoft. Sue them for everything they are worth. reply stale2002 18 hours agorootparentThem having to sell out to microsoft is directly Elon's fault though! Like, you can't just directly cause the bad thing to happen with your actions, and then cry about it when someone goes to a different group for funding. If Elon didn't want the microsoft deal to happen then he shouldn't have reneged on his funding promise just because they refused to give up complete control to him. reply xcv123 17 hours agorootparentElon offered funding. OpenAI declined the offer then signed a deal with Microsoft. Elon did not force them to choose Microsoft. It was simply a disagreement. After they signed up with Microsoft, Elon tried to get some of his money back. Didn't work. Too bad. Now he can move on with his life. reply qarl 16 hours agorootparentWell - except Elon offered the funding without the contingency of being CEO. It was when he demanded it that they went their separate ways. > Elon did not force them to choose Microsoft. By rescinding his offer, he did force them to make drastic changes in order to get new funding. > Now he can move on with his life. Are you familiar with Elon? reply stale2002 14 hours agorootparentprev> It was simply a disagreement. Ok great. Then that means that any complaints from Elon about selling out are completely invalid, and it makes no sense for him to be mad about such a simple disagreement. > Now he can move on with his life. That would be wonderful. Unfortunately you were the person arguing that it was somehow deserved for him to sue. It is not deserved because he was the one who cause them to sell out to Microsoft. That is the point. The point is that you can't directly cause the supposedly bad action to happen, and then claim that you have been morally wronged when someone gets funding elsewhere. If you think that it was merely a disagreement, then fine. Your previous arguments are invalid where you try to claim that this is some huge sellout situation, when it is directly Elons fault. reply KerrAvon 20 hours agorootparentprevYou too may be laughed out of court, then. IANAL, but your feelings aren't the law. reply amanaplanacanal 19 hours agorootparentprevBut he isn’t. He was just stroking his own ego. It was always bullshit. reply jorts 20 hours agorootparentprevIf you read the article, in the emails published Musk acknowledged the need to start charging customers due to the high cost of service, so it’s not like the other cofounders deceived him. reply penjelly 19 hours agorootparentprevwhy do people take anything musk says at face value? He has his motives and they aren't ever as altruistic as he implies. reply whynotkeithberg 19 hours agorootparentThis is the better question... But probably easily answered by his army of sycophantic followers who will parrot anything he says no matter how contradictory the statements may be. reply indoordin0saur 20 hours agorootparentprevYeah. Musk is cringe but on this he's right on this. OpenAI went from a company with noble aspirations to just another sketchy big tech company at odds with the interests of the rest of society. reply jonathankoren 20 hours agorootparentprevYeah but the emails with Elon show that the \"open\" and \"nonprofit\" part was always a fucking a scam. reply belter 21 hours agorootparentprevOr maybe his lawyers explained to him the meaning of legal discovery... reply AstroJetson 19 hours agorootparentYea, they published some of Elon's emails around this. It wasn't a good look and I think they said \"By the way, we will see you at a deposition\" and it was over. reply truncate 21 hours agorootparentprevThe way things are going, OpenAI can experiment on puppies and that'll still not affect their ability to raise money. They are clearly leading the space right now and making lucrative deals with most valuable companies out there. reply p1esk 19 hours agorootparentThey might not need to raise any more money. If they manage to release a significantly smarter model (gpt-5), I, and millions of others, will gladly pay hundreds of dollars per month to use it. reply threeseed 21 hours agorootparentprev> Which he managed to accomplish He accomplished absolutely nothing. OpenAI has established lucrative partnerships with Apple and Microsoft and are the primary platform developers are building their applications on. They will have no problem profiting and raising money. reply lossolo 20 hours agorootparentprev> is now jealous Reading his (already deleted) tweets from yesterday about the Apple and OpenAI announcement, I think you could be right. reply 7e 16 hours agorootparentprevHe didn’t leave OpenAI. He tried to steal employees and was asked to leave. reply rand1239 20 hours agorootparentprevNice. Do you actually realise these are just your beliefs and how you view the world? A good way to test this would be to check the number of zeroes in your bank account and Musk's net worth. Are they really close? No? Probably he has a different belief system then. reply mlindner 20 hours agorootparentprevWhy do people always assume anything he says isn't why he does what he does? He's repeatedly stated why he cares. It's rather frustrating. Like assume good faith please. reply justinclift 19 hours agorootparentThe problem is that we don't know which of his statements are actually directly honest, and which are bullshit, spin, or just plain exaggeration. Musk seems quite happy to do all of the above without distinction. :( reply FireBeyond 19 hours agorootparentprev> Like assume good faith please. Which year of promising \"FSD later this year/end of year\" do we stop assuming good faith and recognize that maybe Musk has dug himself into his own hole when it comes to assuming good faith about his statements? Kinda like \"I have evidence that Vern Unsworth is a pedophile\"? Should we have assumed good faith there, too? reply mlindner 16 hours agorootparent> Which year of promising \"FSD later this year/end of year\" do we stop assuming good faith and recognize that maybe Musk has dug himself into his own hole when it comes to assuming good faith about his statements? I take it as that is what he actually thinks. He is perpetually bad, exceptionally so, at judging the time things will take to complete. He's not actively lying. He actually thinks that. Once you assume that, you start to get a better understanding of why he operates the way he does. He's naive to a fault. > Kinda like \"I have evidence that Vern Unsworth is a pedophile\"? Should we have assumed good faith there, too? (First off, that's not a direct quote, he never said that.) That's not an engineering topic so I'm not going to stake my reputation on it, but my default assumption would be the same, he actually thought that, as silly as it seems. He had a preconceived notion, unfounded or not, that any old white man who retired to that area of the world was more than likely to be that type of person. Of course the origin of that dirt seeking effort was that he was upset with the guy for doing personal attacks on him personally and also on the efforts of the SpaceX employees which he almost considers as extensions of himself. So he considered that he must be a bad person (as only a bad person would have attacked what he was trying to do), in multiple ways, so he went trying to find what other ways he was a bad person. reply throw9474 21 hours agorootparentprevElon, was just outed by his own xAI, https://grook.ai/share?id=e269e88a7b1a71eff4f176c864b30161&h... reply digging 20 hours agorootparentWhy do I keep seeing this exact comment and what the hell is grook.ai? Can you provide some context? reply throw9474 20 hours agorootparentAI connected the dots using public citations and concluded SpaceX is more about Wars than Mars. reply mewpmewp2 20 hours agorootparentnext [11 more] [flagged] doctorwho42 18 hours agorootparentIf WW3 happens anytime in the next 100 years, a colony on Mars would not survive because they would eventually stop receiving shipments of earth made tech. For example, replacement IC's or other silicon wafer based technology. Not like you are going to see a fab built on Mars in the first 20-50 years of a Mars colony. reply mewpmewp2 15 hours agorootparentIt probably won't survive, but it doesn't mean that it shouldn't be tried for all different sorts of reasons. We don't know when a danger will occur or materialize, but we should begin to make sure our eggs are spread out as soon as possible to have the highest odds of survival as species. reply wazer5 20 hours agorootparentprevAmazing Reddit discussion on this: https://www.reddit.com/r/Starlink/comments/1daovey/elons_xai... reply mewpmewp2 20 hours agorootparentWell yes, but I wouldn't be surprised if Starlink was for nuclear? Why am I being downvoted here? US, Europe, west, and democracy in general, at current times needs to be 100% at its game on the most significant power in human history. Ultimately dictatorship is not going to give up, and they want to prove democracy is wrong, and at some points nuclear weapons are going to be used for that. We are at pre-war right now. But it's a matter of time for all of it to escalate to all out war. We must be prepared for it. reply talldayo 16 hours agorootparentprev> If humanity was in harmony No, stop it. Humanity cannot attain harmony because disparity exists. People are different, we have different beliefs in how rule should be attained and held, and we are willing to fight each other to the death to prolong that way of life. The concept of \"perfect order\" is nonsense, because it would require so much concentrated bigotry that nothing would possibly ever change or be unique. There is no harmony for us to reach, besides the mutual acceptance that we exist in constant resource competition with our fellow man. That's all there is. Elon Musk isn't your savior, he's a fickle authoritarian that would destroy society like he destroyed his own image, if he got the chance. You guys need to stop obsessing over the same three popular celebrities and move on, this stuff is embarrassing (even by HN standards) and a waste of both our time. Send it to TMZ and get ignored by them, if you're desperate for attention. reply mewpmewp2 16 hours agorootparentThere is no harmony at current state, because that is how natural selection has developed us. Although it would still be better if democracy ruled the World rather than dictatorship together with democracy. Elon Musk is not the saviour, but none the less the goals he set make sense. The other part of your reply is quite ugly unbased attacks from you. reply throw9474 20 hours agorootparentprevIt's way more direct if you read what it says Elon fans downvote it though. reply mewpmewp2 20 hours agorootparentI'm not an Elon fan, but surely you agree that earth is probably going to be destroyed soon by either WW3 or any numerous climate change issues? reply andrewflnr 18 hours agorootparentNo. Climate change will be bad but won't render Earth uninhabitable. It probably won't even be as bad as any of the major mass extinctions we know about. WW3 is not assured, and all the major powers understand why nukes are a terrible idea. Even if things go hot between US/CN I bet it'll stay conventional. Furthermore if those disasters do happen, they will surely kill a martian colony, since it won't actually be independent of earth for decades, in terms of spare parts, genetically viable population, etc, even if they do manage to grow their own food and oxygen. reply mewpmewp2 20 hours agorootparentprevPlease explain to me? I may be too dumb to understand the inbetween lines here. reply mywittyname 21 hours agoparentprevCan the the defendants request that the case be heard anyway, should they want to have the case dismissed with prejudice? reply jcranmer 21 hours agorootparentThis is a bunch of legal procedure that I don't know with confidence, but my expectation is that the answer is \"theoretically yes, but it won't be granted in this case.\" That is, the \"theoretically yes\" comes from some sort of mechanism to handle plaintiffs who pull this sort of stuff abusively, but Musk's (lawyers') actions here haven't reached anywhere near that level of abusiveness, so there's no reason to grant it. reply Hamuko 21 hours agoparentprevWhy give up a good vehicle for posturing? reply Havoc 22 hours agoprevNeither of them are doing their image any favours lately. They'd do much better if they just shut up & built...something they're both clearly good at reply swat535 17 hours agoparentThe battle for AI really needs to be won with FOSS like it happened with Linux. I can't imagine a future where AI is only controlled by the powerful and wealthy. Honestly, we take many of the great things we have thanks to FOSS for granted and come to think of it, if those were to be invented today, they would be locked behind a subscription fee or entirely discontinued because VC's couldn't figure out how to make money from them. reply Havoc 10 hours agorootparent> I can't imagine a future where AI is only controlled by the powerful and wealthy. No need to imagine. It’s already here. Look at all the open weight models - all big corporate developed and then shared because it suited their corporate objectives. There is not a hint of FOSS like organic model development in sight. On the technique and research yes but not on the actual training. It’s too expensive and thus automatically limited to big tech and gov reply Zambyte 13 hours agorootparentprevIt's crazy how good open weight models are already. With a 7900 XTX I can run a roughly GPT-3.5 quality q6 model at 70 TPS. FP16 at 30 TPS. I can swap out the model to cater to specific tasks. Sure, it's nowhere near the amount of knowledge as Claude Sonnet / Opus, or the suite of GPT 4s, but when you hook it up to external sources of knowledge, it's still freaking useful. reply deepfriedchokes 22 hours agoparentprevIt would be great if both of these guys would become eccentric recluses like Howard Hughes. reply throw0101d 21 hours agorootparent> […] eccentric recluses like Howard Hughes. And not an eccentric recluse like Ted Kaczynski: * https://en.wikipedia.org/wiki/Ted_Kaczynski reply Coolbeanstoo 20 hours agorootparentA recluse like Kaczynski with the resources of someone like Elon Musk would be world or at very least American society levels of quite interesting reply Aerbil313 19 hours agorootparentprevWhat's the relation between the two? Kaczynski's ideas are pretty much correct. See the book Technological Slavery: https://ia800207.us.archive.org/16/items/tk-Technological-Sl... reply tim333 4 hours agorootparentBoth were eccentric dropouts. Hughes did big flying things, Kaczynski did AI doom. Musk tweeted \"He might not be wrong\" about Kaczynski so maybe it's in progress. https://x.com/elonmusk/status/1667627403089268739?lang=en reply sebzim4500 21 hours agorootparentprevOr we just go back to 5 years ago Elon where he tweeted about rockets instead of far right conspiracy theories. reply NickC25 21 hours agoparentprevSeriously. Musk has enough money to fuck off and go build something. His political and economic opinions are pretty bad. He should just shut up. reply nostrademons 20 hours agorootparentMy pet theory is that Musk basically unraveled when the study came out that there does not exist enough CO2 on Mars to terraform the planet: https://www.nasa.gov/news-release/mars-terraforming-not-poss... Note the date on that: it was published in Nature on July 30 2018, and it wouldn't surprise me if somebody would've sent a preprint to Musk up to a month before then. The \"pedo guy\" tweet during the Thai cave rescue was July 18 2018. Musk's \"funding secured\" Tweet that the SEC sanctioned him over was Aug 7 2018. His appearance on the Joe Rogan podcast was September 2018. Most of his children and extramarital affairs post-date 2018; at that point, he only had the 5 with Justine Musk. Pre-2018, most of his ideas were crazy but at least engineering-focused on reasonable causes. Musk has repeatedly said his ambition is to die on Mars. After a reputable scientific paper came out saying that if you step off Starship, that will be the very first thing you do, he doesn't really have anything to live for. Meanwhile he's given up so much for that goal (most notably, his first family) that it must feel pretty bitter to have invested so much in something impossible. For that matter, the psychology is likely pretty similar to the core MAGA demographic, many of whom work hard all their life to achieve the American Dream and then find that the American Dream is going to other people. reply 360macky 19 hours agorootparentI always thought Musk's turning point was in the pandemic when he was wrong about how dangerous it could be (https://www.nytimes.com/2020/05/13/business/Elon-Musk-tesla-...). And from there the decline began, but now I realize and remember that before 2020 he was already doing some crazy things... reply john_minsk 19 hours agorootparentLove his response. Makes total sense to me. This paywall is crazy. reply Geee 19 hours agorootparentprevNah, terraforming has never been Elon's immediate plan. He knows that it's difficult and very slow process and won't happen in his lifetime. Their goal is to set up a self-sufficient Mars settlement, which doesn't depend on terraforming at all. Also, impossible with current technology doesn't mean that it's impossible in the future. He actually talked about this on his latest Diablo stream, here: https://youtu.be/wzIJDU8SPMM?t=2066 He seems to think that terraforming is possible in the long term, with solar reflectors in the orbit which warm up the planet. reply tracerbulletx 19 hours agorootparentprevUsing mars as a long shot target to develop real affordable space capabilities which will help us learn how to stop asteroids and launch cheap internet satellites is cool. Being serious about living there being a near term important and realistic goal is lunacy. reply brokenmachine 13 hours agorootparentprevThese billionaire fuckwits... Why not see what we can do about Earth so it might be livable 100 years from now? Naah, lets spend all our money on a desolate frozen rock 140 million miles away. reply neilv 20 hours agorootparentprevPeople can come back from shattered dreams, and he can afford any therapist. Being a billionaire makes pivoting to executing on new dreams easier. Newfound empathy could give him a new dream of making that American Dream happen for others. reply gravescale 19 hours agorootparentRight, if this is true (possible I guess, but does sound a bit Syndrome-from-The-Incredibles), even if there's really no technical way at all to terraform Mars it's hardly the end of the world, uh, planet. Not least, terraforming would take centuries in any plausible effort. If he goes to Mars to die, he'll die in a pressurised environment whatever happens. reply KerrAvon 19 hours agorootparentprevI think you mean: many of whom work some of their life to try to get richer than they already are and then continuously watch Fox News and Newsmax for 20 years and therefore believe brown people should suffer. reply oskarkk 19 hours agorootparentprevWell, that's a crazy theory (but fun to think about). I don't think he ever believed that terraforming Mars would be achieved in his lifetime. It's not necessary for a colony, and he always talked about \"window\" in which colonization can be started. Like, there are many things that can set our civilization back, like nuclear war, pandemics, meteors etc., and technical progress isn't guaranteed (we've been on the Moon on 60's and can't do that now), so his philosophy is that we must do it as soon as we can because at any moment we can lose that chance. And starting a colony without terraforming is of course better than not doing anything at all. I think his behavior is just the consequence of his wealth and success, of being literally at the top of the world in terms of money. And especially because his money comes from SpaceX and Tesla, both of which were considered crazy and impossible enterprises by most people (and there were many people people saying that it all will fail well before he became really famous and really rich). So I think he just started believing that since he was right with SpaceX and Tesla, and others were wrong (and he became the world's wealthiest person because he was right, so he was in a certain capitalistic way more right than literally everyone), he is right about everything and can do everything. And he was always kind of crazy. Politically I think that maybe his actions are somewhat calculated, maybe he's predicting that Trump will win and be more willing to finance crazy big missions to space? Right now Tesla isn't dependent on American environmental programs as much as it was before, and SpaceX is in a position in which they'll get government contracts forever, no matter who's in power, so he doesn't need to please the current government. And I think he was always more of a libertarian, so that checks out with many things he's saying. His \"anti-war\" views can also be explained by desire to minimize the risk of closing that window in which Mars colonization is possible. reply EnigmaFlare 20 hours agorootparentprevBad means they're different from your own which are automatically good because NickC25 is the ultimate arbiter of how good political and economic opinions are? Many public figures just copy popular opinions and present them as their own precisely so that most people will like them. They haven't even tried to understand what they're talking about. At least Musk has some understanding and isn't driven by conformity. reply NickC25 5 hours agorootparent>NickC25 is the ultimate arbiter of how good political and economic opinions are? God, I fucking hope not. I'm just a single individual, just a man. No different than anyone else here. I think Musk's political/economic opinions are awful because he's so far abstracted away from the real world that his opinions are effectively worthless. I wouldn't trust a random homeless guy's opinion much like I wouldn't trust Jeff Bezos' or Elon Musk's opinions - they both represent an extreme of how thing ares, but no way would they represent how society should move forward. reply john_minsk 19 hours agorootparentprevInteresting. What makes \"political and economic opinions\" bad? reply NickC25 2 hours agorootparentHe's so far abstracted away from the world we live in due to his hundreds of billions of dollars that he really has no idea what he's talking about, but because the new-age version of \"might makes right\" which is \"obscene capital makes right\", people tend to give him more credence than he deserves. He got lucky with PayPal, and lucky with Tesla. That's it. He is not the president, he is not god, not allah, not yahweh, the flying spaghetti monster, he is just a man that got incredibly wealthy both through his own intelligence and some good fortune. That does not mean he's qualified to opine on something outside of his area of expertise. Would you trade your ability to speak publicly about things you don't really understand in depth in exchange for 100 billion dollars? I sure would. Give me 1/100th of that and I'm off to a private island to read books and grow my own food. reply GaggiX 21 hours agorootparentprevHe'll never shut up, people will learn to ignore him if they haven't already. reply CamperBob2 19 hours agorootparentLike they've learned to ignore Trump? reply GaggiX 19 hours agorootparentWhat do you mean? reply CamperBob2 19 hours agorootparentThe person who still owns one of the two major US political parties despite being manifestly unfit, and who continues to hold sway over at least 33% of voters nationwide? Say what you will about Musk, he's no Trump. At least, not yet. He hasn't alienated that many people, and even if he were to do so, it wouldn't be enough to impact Tesla's sales. reply r00fus 21 hours agorootparentprevMusk was always the money and hype guy - he has personally built nothing. Tesla was not founded by him, SpaceX is Gwynne's baby, not his. Twitter still hasn't recovered from being X'd by Musk. reply woooooo 20 hours agorootparentI think this is taking it too far. There are a lot of money and hype guys out there, none of the rest of them transformed the automotive and space industries. Most of them were shoveling money into \"disrupt laundry\" startups instead. reply wustangdan 20 hours agorootparentprevJust to be clear, you're saying that Shotwell has contributed more to SpaceX than Elon? What about, without Elon they'd have reusable rockets but without Shotwell they wouldn't? Do you believe this? I get you hate Elon but at some point these takes are just so outrageous I can't believe you are making them in good faith. reply chipdart 20 hours agorootparent> Just to be clear, you're saying that Shotwell has contributed more to SpaceX than Elon? Which contribution do you believe that Elon Musk had on the development of reusable rockets? Let's put it this way: if you kicked Musk out of SpaceX and replaced it with absolutely any random guy as CEO, do you believe reusable rockets would never see the light of day? reply Prickle 20 hours agorootparentYes, unironically. In the USA you have the SLS, which can only be described as a congressionally designed failure. Past experiments by NASA for self landing rockets had their funding denied as well. In the EU there was the Arianespace CEO who explicitly said that self landing rockets were a waste of time. In Japan, space experimentation and failures are such a public nightmare we would never have bothered. The idea of losing dozens of rockets in order to aim for reusability would have been untenable. Starship would not exist. Because the idea of a rocket with that many engines on the booster was also believed to be impractical. Elon is egomaniacal sure, but that's only magnified by his status as a CEO. His behavior, unfortunately pretty close to the average person. Doesn't change the fact that SpaceX under his leadership is the only reason we have reusable rockets, or the ridiculously ambitious Starship launches. No one could have predicted the current incredible cadence of launches by SpaceX either. reply wustangdan 20 hours agorootparentprevOkay read Walter Isaacson and Vances biography and get back to me. There is hundreds of examples in each. Or read this thread that has a few snippets from the book. https://www.reddit.com/r/SpaceXLounge/comments/k1e0ta/eviden... He is a constant technical driving force. > if you kicked Musk out of SpaceX and replaced it with absolutely any random guy as CEO, do you believe reusable rockets would never see the light of day? Would we have reusable rockets in the same timeline as SpaceX, absolutely not. The proof is all the other rocket companies that have failed to do so, including government entities. So yah obviously if Musk never founded SpaceX, we would not have reusable rockets right now. reply JumpCrisscross 20 hours agorootparentprev> if you kicked Musk out of SpaceX and replaced it with absolutely any random guy as CEO, do you believe reusable rockets would never see the light of day? Any time prior to ~2014, absolutely. reply elevatedastalt 19 hours agorootparentprev> do you believe reusable rockets would never see the light of day? Correct. You don't need a hypothetical, it's not like SpaceX is the first rocket making entity in the world. Why were all the other darlings incapable? SpaceX didn't invent a new branch of rocketry after all. And they hired from the pool of engineers who could have and did work at all the other rocket companies. How did this same pool of scientists and engineers end up with a viable reusable rocket with 300+ successful landings only when they came together at SpaceX? reply bilvar 20 hours agorootparentprevYes. Everyone was ridiculing him for believing they could do it. Including industry experts. reply elevatedastalt 19 hours agorootparentprevThere are many reasonable points of criticism one can make for Musk. 99% of Musk-hate I see on HN isn't among them though. It's more reminiscent of the kind of nonsense articles Tesla short sellers used to publish back in 2016-17. reply KerrAvon 19 hours agorootparentprevYes. He clearly had nothing to do with it. Twitter has proven he can't manage people and has blundered into every other success he's ever had. I get you love Elon, but as some point you need to look in the mirror and recognize your sycophancy for what it is. reply wustangdan 19 hours agorootparentYou are saying he \"clearly had nothing to do with\" a company he founded, funded, and has been CEO, CTO, and chief engineer. What do you think the word \"nothing\" means? reply r00fus 13 hours agorootparentYou’re talking about Tesla right? The one where he essentially did a hostile acquisition then booted one of the real co-founders? You can’t honestly say he founded that company. reply gamepsys 19 hours agorootparentprev> Twitter has proven he can't manage people and has blundered into every other success he's ever had. I think you are just as biased as the parent comment if you think one failure invalidates the merit of all previous successes. reply havefunbesafe 20 hours agorootparentprevI could be wrong, but it seems as though he built Zip2? https://en.wikipedia.org/wiki/Zip2 reply acchow 19 hours agorootparentprevIt seems the internet has decided some narrative about Elon Musk which is not true, and people go around repeating it. If you talk to actual employees, especially early and senior employees, you find that Elon Musk is absolutely pivotal to the engineering direction. reply blackeyeblitzar 20 hours agorootparentprevThe original Tesla guys recognized and agreed Musk was a founder as part of their settlement, and this is now both the formal and legal truth of Tesla. I think some others were also recognized as founders too. The two initial guys didn’t really achieve anything prior to Musk and other early people joining. Calling SpaceX Gwynne’s baby is just straight up misinformation. Talk to actual employees from SpaceX, especially early on. They’ll tell you that Musk actually does get involved in various deep aspects of the vehicles. You might not be aware, but Gwynne Shotwell was in BD not product. reply jonathankoren 20 hours agorootparentI'm sure Elon does get involved. The question is, does he get involved in a constructive way. All signs point to \"no\". We know this, because of what the Tesla and SpaceX people he brought on to Twitter in early days after the acquission said. I believe the words were, \"babysit\", \"distract\", and \"manage\". I'm sure some fanboy will mark me down, but this was discussed on this very forum when it happened. reply acchow 19 hours agorootparentTwitter is a software company. Elon doesn't seem to be an actual software guy. Instead, he has an incredible knowledge of rockets and electric cars. He probably takes his rocket knowledge and under-estimates software complexity. reply StackRanker3000 19 hours agorootparentHis two first successful business endeavors were web-based services (i.e. software), and he had no real experience with or education in cars or rockets before he joined/founded Tesla and SpaceX respectively (he’s got a bachelor’s degree in economics and one in physics). I think he’s probably a smart guy who’s worked hard to learn as much as possible about the fields he’s entered into, but I find it hard to believe that he’s a world-leading technical mind at either. The reason he’s struggling comparatively with Twitter is partly that he doesn’t take it seriously, and partly that he has other ambitions for it (“X the everything app”) than what it actually is. The main attributes behind his success are his obsessive desire to achieve certain goals, and his willingness to take on a very large amount of risk over and over again (he could easily have gone bankrupt several times, but so far things have mostly gone his way). Edit: Another reason he’s had a tough time with Twitter is that he’s acted out of spite and alienated people and organizations he should have been on good terms with, mainly advertisers (Twitter’s actual customers). reply mlindner 20 hours agorootparentprevThis is such a poor hot take. Literally every single person that has personally dealt with him disagrees this narrative. It's only popular on reddit/hacker news boards and among some journalists. Karpathy has a good discussion on it that I've heard several employees at his other companies agree with. Karpathy: https://old.reddit.com/r/singularity/comments/1bpwo0w/andrej... Tom Mueller: https://i.redd.it/89dqiz2lc2t81.jpg Or even Shotwell herself for that matter and how she has expressed how she and Elon subdivide the work. Elon Musk is not nice person, but he gets things done and he's deeply involved in the day-to-day activities of his companies. I know a low level software engineer at SpaceX and he regularly attends their team meetings and contributes. reply bawolff 19 hours agorootparent> I know a low level software engineer at SpaceX and he regularly attends their team meetings and contributes. You say that like its a good thing. Normally CEO attending random low level meetings is considered a pretty big red flag. reply elevatedastalt 19 hours agorootparentIt's a red flag in traditional companies. Musk has never been a traditional CEO or run his companies like a traditional company. If you want to run an IBM or Cisco, don't hire Musk, sure. reply mlindner 16 hours agorootparentprevTom Mueller also mentioned how he had like 20+ direct reports. The organization is _extremely_ flat with almost no middle managers. reply CamperBob2 19 hours agorootparentprev(Shrug) It seems to be working for them. reply tmpz22 22 hours agoparentprevWhats wrong with their image? They're still able to raise billions no problem. Their day to day doesn't change. Altman just did a big deal with Apple. Musk's board is still likely to cram through a $45B+ comp package for him. reply bee_rider 21 hours agorootparentAltman has a basically fine image I think, in the sense that nobody outside of tech circles knows his name, and everyone in tech circles must at least admit that he’s good at getting investors. Musk has a reputation as being too demanding, which is one thing if you are doing something revolutionary, but nowadays the market has mostly caught up to his cars. It seems all that over-working his employees has done is produced poor QA. He’s also politically alienating to liberals, who would otherwise be inclined to buy his cars. reply Nuzzerino 21 hours agorootparentnext [4 more] [flagged] NickC25 21 hours agorootparentWhat's wrong with a liberal? How are they alienating to normal people? How would you define \"normal\"? Also, would love to hear your definition of who is and who isn't \"liberal\". Remember kids, the left/right & liberal/conservative is a completely false dichotomy. reply impossiblefork 20 hours agorootparentLiberalism is free trade, property ownership, etc. Many people are excluded from that-- it's an ideology where both the rich and the poor are forbidden from sleeping under bridges. Of course it's alienating to ordinary people. This is why you got movements like social liberalism, to try to temper the madness and turn into something less socially useless, but everything moves back towards its origin, so here in Europe even once reasonably decent social liberal parties have now decayed into liberal parties, into the inherently alienating position that is liberalism's core. reply bee_rider 16 hours agorootparentThat, of course, isn’t what people mean when they say liberal in the US context, here it just means social liberal or cultural liberal. In our defense, liberal economics are the ground state in the US, we don’t think to name them for the same reason a fish doesn’t think to name water. reply Havoc 21 hours agorootparentprev>Whats wrong with their image? They're still able to raise billions no problem. Their day to day doesn't change. Not sure I'd classify either of those as \"image\". reply m463 19 hours agorootparentprev> Altman just did a big deal with Apple. did Altman do it? Musk on the other hand has built things. People can disparage him all they want, but I think every one of us is better off for tesla, neuralink, spacex and even the boring company and paypal. who cares if he opens his mouth on social media. I mean, he went to russia to launch a rocket, figured out they were a mess, came back and... several years later the world is launching rockets all the time. (It's june 11, and just starlink launched 4 rockets since the beginning of the month) The rocket stuff is like the rise of pc hardware, where things start happening faster and all of a sudden, commercial flights are putting people in space. I kind of think openai got off mission, and won't be as good for everyone, except maybe in the sense that other folks like meta have opened up models like llama. reply HWR_14 17 hours agorootparent> I think every one of us is better off for tesla, neuralink, spacex and even the boring company and paypal. None of those make me better off in any way. Some (neuralink, the boring company) have never made anyone better off except maybe in some pilot program. reply m463 17 hours agorootparentThey have second order effects, usually wrt competition. Tesla might not be your favorite car, but it has created robust competition in EVs, and made the incumbent car manufacturers have to compete. Someday if you need internet in your cabin or RV, starlink made it easier even if you don't use them. reply HWR_14 4 hours agorootparentI don't believe in either of your listed secondary effects. How does starlink existing make it more likely for other companies to build expensive infrastructure while having to price compete with it? reply RIMR 21 hours agorootparentprevThis is a strange comment. Economic success and executive pay packages don't disqualify the claim that there is an image problem. If anything, Musk getting a $45B package after a year of failures and layoffs at Tesla is something that tarnishes Musk and Tesla's image. The fact that this cash package is almost exactly what he owes creditors for the loans he took out buying Twitter, it almost looks like he's begging Tesla for an undo button for his own terrible decisions. As for Apple making a deal with OpenAI, the fact that every product integrating ChatGPT is just being ruthless enshittified as a result, this is terrible optics for Apple when their biggest competitor, Microsoft, is currently playing target practice with its feet shipping poorly advised AI integrations (see: Recall). As for OpenAI's image, the only thing they have going for them is the technical impressiveness of ChatGPT - organizationally everyone I know in the tech space assumes that OpenAI is a dumpster fire of a company. reply rayiner 21 hours agorootparent> If anything, Musk getting a $45B package after a year of failures and layoffs at Tesla is something that tarnishes Musk and Tesla's image I hate to defend executive comp, but Tesla’s market cap has gone up by $487 billion (almost 10x) since he was awarded that comp package in 2018. Half of all EVs sold in the US are Teslas. In 2023, the Model Y was the world’s bestselling car, outselling the Toyota Corolla. That’s completely insane. What more do you want? reply JumpCrisscross 21 hours agorootparent> What more do you want? I believe in honouring deals, even if it’s unsavoury ex post facto. Until recently, that meant approving the pay package. But Musk unilaterally amended the deal when he “threatened on X…to develop AI elsewhere if he doesn’t get a 25% stake in Tesla” [1]. Then he developed AI elsewhere [2]. If you promise to give me a dollar, you dither, I say I’ll burn your house down if you don’t, and then I burn your house down, I don’t believe you owe me the dollar anymore. Musk should offer to return xAI’s funding and merge it into Tesla in exchange for the vote. [1] https://apnews.com/article/tesla-elon-musk-pay-package-share... [2] https://x.ai/blog/series-b reply snowwrestler 21 hours agorootparentprevBoards are supposed to be forward-looking. Typically a board would electively reward a CEO who has delivered out-sized performance out of fear that the CEO would leave for a different job (creating a bad future for the company). This situation, though, has a different set of forward-looking concerns. Musk already has several other jobs. And the primary concern is that he wants this large pay package so that he can have more personal resources to put toward those other jobs (at the expense of Tesla). As it stands now, Musk’s personal financial position depends on leverage against his Tesla position. This binds him to Tesla’s future performance. Giving him a huge personal pay package essentially weakens that binding, giving him greater license to deprioritize his Tesla leadership in favor of SpaceX, Xai, Neuralink, etc. There is an argument to be made that Tesla’s board should be essentially antagonistic toward Musk’s other companies, given that his time and attention are finite resources, and Tesla is a public company. The board has responsibilities to investors to maximize their return, which should take precedence over whatever feelings of gratitude or connection they feel toward Musk personally. reply ProfessorLayton 21 hours agorootparentprev>...Tesla’s market cap has gone up by $487 billion (almost 10x) since he was awarded that comp package in 2018 Another way to think about this is that investors are in a position where they got that wildly ambitious growth, and are currently not obligated to pay for it, so why would they? This is to the guy that has no problems breaking contracts of his own, and gives the finger to people he owes money to. I guess we'll see when this comes to a vote, but it would be shrewd business not to. reply BlarfMcFlarf 21 hours agorootparentprevIt’s also, what, 3 years of profits by Tesla? Where does the money come from? reply acchow 19 hours agorootparentThey are stock options. The money comes from the open market if he decides to sell the shares, and the shares themselves come from share dilution if he decides to exercise the options. reply renewiltord 21 hours agorootparentprevShould've let him keep the options we granted him back in 2018. Taking away his performance-based compensation is total bullshit. I voted to give it back. His options were worthless for most of that time. He got the stock to appreciate a lot and some guy with 6 shares sued him for this crap so that the lawyers and him could split $5 b that a judge granted them. If anyone took away a software engineers options here after a company became successful because the company later decided it's too much there'd be hell to pay. Everyone rightfully complained about Zynga doing this. But here it's fine? Bullshit. I voted to pay the man his due. I voted to move the corp out of Delaware to Texas. He made me money under reasonable terms. Back then everyone said he'd just fail. reply threeseed 21 hours agorootparentpreva) Full-time, engaged CEO free of conflicts of interest e.g. X.ai. b) Stop devaluing the Tesla brand with his controversial, inflammatory comments. c) Compensation package that is more reasonable compared with other companies. reply troupo 21 hours agorootparentprev> but Tesla’s market cap has gone up by $487 billion (almost 10x) since he was awarded that comp package in 2018 The speculation on Tesla's stock raised the \"market cap\". Any stock that Tesla doesn't own brings Tesla exactly 0 dollars, no matter how hight the illusionary market cap there is. How much of the stock does Tesla actually own? How much of the stock that Tesla owns can the company safely sell to offset the $45 billion of actual money that they had to cough up for Elon? reply acchow 19 hours agorootparentThey don't have to cough up any money. The 2018 compensation package they agreed upon is in stock options with vesting in 12 tranches, tied to stock price performance https://ir.tesla.com/press-release/tesla-announces-new-long-... reply rayiner 19 hours agorootparentprevThe stock price increase directly benefits Tesla shareholders. It also reflects revenue growing by 5x since 2018 and the company becoming profitable since 2020. reply tedunangst 21 hours agorootparentprevThey're not writing Elon a check for $45 billion. reply troupo 21 hours agorootparentYeah, if it was just stock, then its value fluctuates wildly reply ToucanLoucan 21 hours agoparentprev> They'd do much better if they just shut up & built...something they're both clearly good at That would require either of them to be good at building anything, and I don't think either has ever demonstrated that. They're just money men, and Musk went out of his way to PR himself as the real life Iron Man before utterly ruining it because he can't shut his mouth when he really, really should. reply Havoc 21 hours agorootparent>I don't think either has ever demonstrated that. So what's your space company called? reply ToucanLoucan 18 hours agorootparentGive me six billion dollars and I'll also \"build\" a space company. I also don't know fuckall about rockets, and Elon and I have that in common. reply JumpCrisscross 17 hours agorootparent> Give me six billion dollars and I'll also \"build\" a space company You're off by an order of magnitude on the start-up capital SpaceX drew, and that almost exclusively from Musk. reply ToucanLoucan 4 hours agorootparentDoesn't change the fact that he didn't build shit. He brought money, which is what I said he did. He wrote some code back in the 90's and there's no evidence he's done any actual work since then. His job is to be rich, and write checks. Anyone can do that. I wrote a check this morning. Well, that and going on Joe Rogan to get high and crash the stock price of his companies. I could do that too I suppose but I'd prefer not to. reply floppiplopp 12 hours agoprevIs this what he wants to distract from by throwing screaming tantrums over apple and openai? Or is it the $46bn grab-the-money-an-run move as long as the tesla stock is still worth something? reply fs0c13ty00 17 hours agoprevTo this point I just want to have a way to block all Musk related news from my doom scrolling time. I just can't see any meaningful value in these news anymore. reply hn_throwaway_99 22 hours agoprevThis was always \"lawsuit as press release\", and despite the fact that I may agree that how OpenAI essentially morphed into a for-profit entity was dubious, Musk looked a bit ridiculous when it came out that he was essentially just butt hurt that he didn't get to take control of OpenAI under Tesla. reply robotnikman 21 hours agoparent>how OpenAI essentially morphed into a for-profit entity This is my big issue with them as well, and the fact that they still stick to 'Open'AI as their name. They might as well just sell themselves to Microsoft at this point. Of course that lawsuit probably was not going to go anywhere though, seems like it was just for publicity. reply diimdeep 15 hours agoprevKim Kardashian of tech, always in the news. reply ilikeitdark 21 hours agoprevThe big question is....whose going to play Elon Musk in the true-to-life film version, about the slow downfall of the man who wanted to be king of everything, but ended up a nothing as part of a musical duo with Kanye West, with a Reno casino resort residency. reply indoordin0saur 20 hours agoparentHis pipedream rocket company just succeeded at launching and landing the most impressive vehicle ever built by man. You could watch high-def live footage as the thing re-entered at the atmosphere at 20,000mph thanks to the space internet system that is rapidly becoming the best in the world. And in two days he's getting a $56 billion pay day granted to him by Tesla shareholders. It's a little early for schadenfreude. reply shepherdjerred 19 hours agoparentprevWhen would you say the downfall started? Do you think the recent achievements of the companies he's involved with are repeated luck? Would you say that all successful individuals were always successful or had pleasant personalities? I'm not necessarily a huge fan of Musk, but it's always interesting to see how his accomplishments are diminished. The attitudes towards Musk are remarkably similar to how others perceive Steve Jobs. reply jjk166 21 hours agoparentprevKevin Durand was born for the role. reply silisili 21 hours agorootparentI never thought about it before, but you're right. He's got such an odd presence and unidentifiable accent(to me) in most of his roles, I think he'd be about the best we could ask for. reply tibbydudeza 21 hours agoparentprevI could have been a contender :). reply pedroma 20 hours agoparentprevI'd watch Elon Musk starring Kanye West as Elon Musk. reply mullingitover 21 hours agoprevI honestly wonder if Apple should come after him for libel after yesterday's comments. Apple exhaustively demonstrated privacy-focused AI integration in the OS using local models, and the ability to dial out to the OpenAI API, but only with the user authorizing every single call. If Apple isn't going to publicly insult Elon's intelligence by accepting that he really believes his mentally bankrupt statements, he leaves them no choice but to file a libel case. His statements seemed to be carefully and maliciously prepared in a way that would damage Apple. reply brodo 21 hours agoparentMost people know that Musk lies constantly. I think the smart thing for Apple is just to ignore him. reply chipdart 20 hours agorootparentI don't think that allowing a bully to smear anyone's reputation with each tantrum is a smart thing to do, specially as Musk's tantrum consists of attacking a central piece of Apple's sales pitch: security. reply mullingitover 20 hours agorootparentMy point is that it's arguably not a 'tantrum,' it's actually a strategically worded statement designed for maximum damage to Apple's business. The man isn't actually stupid. Musk is desperately trying to carve out space for his personal brand in AI, and to do that he needs to drag down major players like Apple. I think there's a strong case to be made that that's exactly what his move was. If he gets serious blowback he can try to claim 'free speech,' but weaponizing speech for malicious purposes/personal gain has never been a protected activity. reply chipdart 6 hours agorootparent> My point is that it's arguably not a 'tantrum,' it's actually a strategically worded statement designed for maximum damage to Apple's business. I think so, too. > The man isn't actually stupid. I'm not sold on that idea. The character has a long track record of doing stupid and easily avoidable things, specially out of impulse. > Musk is desperately trying to carve out space for his personal brand in AI, and to do that he needs to drag down major players like Apple. I don't think he has anything to show regarding PR. His modus operandi is taking over other people's work and pass himself off as a pioneer. Hence his despair to hijack OpenAI,and his tantrum when he was denied that. reply stale2002 20 hours agorootparentprevWell fortunately we live in the USA, where free speech protections are extremely strong. Someone not liking your product isn't going to result in a winning defamation lawsuit. Musk is free to attack the reputation of the trillion dollar for profit company all he likes. reply chipdart 5 hours agorootparent> Someone not liking your product isn't going to result in a winning defamation lawsuit. I think you're grossly misrepresenting the issue. In the very least, you're seeing a top representative of a company resorting to libel to attack companies who consume the product of a competitor. reply loceng 21 hours agoparentprevGood luck to Apple to prove damages. Could you actually outline for us what comments of his you're saying are libel, and explain why they're libel? Rather than just seemingly piling on the ad hominem in this thread. reply mullingitover 20 hours agorootparentHis shrill statements implying that Apple will be piping all your private data to OpenAI are the exact opposite of what Apple demo'd. It's like they were designed to destroy consumer confidence in Apple's privacy protections across their entire product line, protections which Apple obviously designed at great expense and effort and are core to their business strategy. This is someone with 187 million twitter followers who knew his statements would reach headlines to influence far beyond even the massive following. I don't think it would be hard to prove a billion dollars or more in damages for the kind of legal team Apple could easily assemble. reply stale2002 20 hours agorootparentThats not how libel works. He is allowed to dislike the fact that Apple is working with OpenAI. And whether or not it is good security practice isn't an objective fact and is instead a matter of opinion. In the USA, free speech protections are very strong and you aren't going to win a lawsuit just because someone doesn't like your product. reply loceng 20 hours agorootparentprevCan you quote him directly please? You're saying implying but it's more likely you're placing that assumption or interpretation on it. Was it actually concerns he put forward of how easy it would actually be for Apple to do so, rather than claiming they will 100% do so? Re: \"I don't think it would be hard to prove a billion dollars or more in damages for the kind of legal team Apple could easily assemble.\" It's practically impossible to prove - in fact his statement and his reach you talk about could perhaps even have driven up their revenue, if we're just going to play armchair expert and put assumptions forward as reality. reply LordDragonfang 22 hours agoprevDiscussion from the initial filing: Elon Musk sues Sam Altman, Greg Brockman, and OpenAI [2024-03-01] (https://news.ycombinator.com/item?id=39559966) reply wnevets 22 hours agoprev> “It’s certainly a good advertisement for the benefit of Elon Musk,” Kevin O’Brien, partner at Ford O’Brien Landy LLP and former assistant U.S. attorney, told CNBC at the time. “I’m not sure about the legal part though.” reply Joel_Mckay 21 hours agoprevOne can clearly see E.l.o.n. and S.a.m. prototype models in the background: https://www.youtube.com/watch?v=IkismK9a_84 They should act more harmonious moving forwards... lol =3 reply minimaxir 22 hours agoprevElon likely got sufficient satisfaction weirdly complaining about Apple and OpenAI's partnership on Twitter/X yesterday. reply threeseed 21 hours agoparentI suspect he was trying to negotiate a place for Grok in Apple's LLM story. With Google, Apple and Samsung having their own locked-in strategies there is now no pathway for Grok to any relevance. reply JumpCrisscross 21 hours agorootparent> With Google, Apple and Samsung having their own locked-in strategies there is now no pathway for Grok to any relevance( Why wouldn't Apple license Grok? reply lanstin 20 hours agorootparentBecause they have a brand reputation and all the (recent) words coming out of Elon, his media platform, and his AI are shit and people don't like them and it would conflict with Apple's brand image - they don't even let movie villains use iPhones and you think they'd put the potty mouthed, anti-science pro-fascism LLM he's trying to make onto their phones? reply JumpCrisscross 20 hours agorootparentSorry, I should have said why couldn't Apple license Grok. Their deal with OpenAI isn't exclusive, after all. Musk's shenanigans are good reasons for Cupertino to keep a wide berth. reply threeseed 20 hours agorootparentprevElon's record on trust and safety is not in-line with Apple's values. And ignoring that its quality is far behind OpenAI, Gemini, Anthropic etc. reply rodgerd 20 hours agorootparentprevI dunno, why wouldn't you want an LLM trained by a company whose owner keeps inviting child pornographers and neo-Nazis to post? What could possibly go wrong? reply ClassyJacket 20 hours agorootparentPlease do show me where he explicitly invited child pornographers. Go ahead, link me to the specific place where he said that. I want to see what he said word for word. reply threeseed 19 hours agorootparentNot sure if he has said anything explicitly. But he has re-invited child pornographers back on the platform. https://www.forbes.com/sites/conormurray/2023/07/27/twitter-... reply john_minsk 18 hours agorootparentCan't make decision for myself. Article doesn't contain picture in question or a link to a tweet or anything of that nature. Given that current internet narrative is neurotic and even smallest questions are blown out of proportions I tend to think that offence was not that big, otherwise FBI would request IP address of the guy and put him in jail (much better outcome then \"ban his twitter account\") reply whalesalad 21 hours agoparentprevhttps://x.com/elonmusk/status/1800265431078551973 reply insane_dreamer 21 hours agorootparentsour grapes reply cjk2 21 hours agorootparentprevSays the guy selling data collection machines on wheels... Asshat! reply novok 21 hours agoprev [–] Ah now Elon's freakout about iOS's upcoming ChatGPT integration makes a little bit more sense now. reply warunsl 21 hours agoparentHow exactly? His tweets (at least to me) come off extremely jealous of the fact that Apple integrated with OpenAI. reply indoordin0saur 20 hours agoparentprev [–] I'm annoyed what was once an admirable mission of bringing open-source not-for-profit AI to the world was gleefully abandoned as soon as Micro$oft and now Apple threw some money their way. Seems understandable to be peeved if you were someone who put money and effort into this thing due to concerns about an AI arms race. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Elon Musk has withdrawn his lawsuit against OpenAI and its co-founders, Sam Altman and Greg Brockman, which was scheduled for a hearing in San Francisco.",
      "The lawsuit alleged that OpenAI deviated from its original mission to develop artificial general intelligence for humanity's benefit and became a for-profit entity controlled by Microsoft.",
      "Musk's decision to dismiss the suit follows his public criticism of OpenAI's new partnership with Apple and comes after launching his own AI startup, xAI, which recently raised $6 billion in Series B funding."
    ],
    "commentSummary": [
      "Elon Musk has withdrawn his lawsuit against OpenAI and Sam Altman, which some speculate was a strategic move possibly driven by jealousy over OpenAI's achievements.",
      "The discussion highlights OpenAI's shift from its original non-profit mission to a for-profit model, raising questions about accountability and tax-exempt status.",
      "The text also touches on Musk's controversial behavior and his significant influence on the tech industry, despite criticisms of his management style and public antics."
    ],
    "points": 201,
    "commentCount": 212,
    "retryCount": 0,
    "time": 1718138312
  },
  {
    "id": 40658095,
    "title": "Intel Transports 916,000-Pound 'Super Load' Across Ohio for New Chip Plant",
    "originLink": "https://www.tomshardware.com/pc-components/cpus/intel-is-trucking-a-916000-pound-super-load-across-ohio-to-its-new-fab-spawning-road-closures-over-nine-days",
    "originBody": "PC Components CPUs Intel is trucking a 916,000-pound 'Super Load' across Ohio to its new fab, spawning road closures over nine days News By Dallin Grimm published 9 hours ago Imagine the nine day long traffic jam. Comments (12) (Image credit: Ohio Department of Transportation) Ohio is seeing the effects of Intel's growth, but maybe not in the way state officials had hoped. Intel will put a 916,000-pound \"super load\" on the road in Ohio on Wednesday, for a trip that will cover approximately 150 miles in nine days and snarl traffic for over a week. The price of progress! Intel's new campus coming to New Albany, OH, is in heavy construction, and around 20 super loads are being ferried across Ohio's roads by the Ohio Department of Transportation after arriving at a port of the Ohio River via barge. Four of these loads, including the one hitting the road now, weigh around 900,000 pounds — that's 400 metric tons, or 76 elephants. The super loads were first planned for February but were delayed due to the immense planning workload. Large crowds are estimated to accumulate on the route, potentially slowing it even further. Intel's 916,000-pound shipment is a \"cold box,\" a self-standing air-processor structure that facilitates the cryogenic technology needed to fabricate semiconductors. The box is 23 feet tall, 20 feet wide, and 280 feet long, nearly the length of a football field. The immense scale of the cold box necessitates a transit process that moves at a \"parade pace\" of 5-10 miles per hour. There's a lot of moving parts to it. It's not just jump in a truck and go Matt Bruning, ODOT press secretary Intel is taking over southern Ohio's roads for the next several weeks and months as it builds its new Ohio One Campus, a $28 billion project to create a 1,000-acre campus with two chip factories and room for more. Calling it the new \"Silicon Heartland,\" the project will be the first leading-edge semiconductor fab in the American Midwest, and once operational, will get to work on the \"Angstrom era\" of Intel processes, 20A and beyond. Beyond bringing jobs to the region, Intel seeks to make nice with Ohio by investing millions into local schools and universities to provide local students with the tools to grow up to work at the foundries. The cold box and the other super loads to come after it required immense planning from ODOT. \"There's a lot of moving parts to it. It's not just jump in a truck and go from point A to point B,\" said Matt Bruning, ODOT press secretary. \"There's a lot of planning and coordination and analysis that goes with doing a move like that.\" The Department of Transportation has been planning the route for months, ensuring that bridges and roadways could handle the loads coming for them. Power lines were moved underground or extended so work crews could lift them over the loads. The Ohio Department of Transportation has shared a timetable for how long they will be dealing with the super loads. Bruning shared that other companies are piggybacking on the super load route plans now that accommodations have already been made. \"It is kind of abnormal to see this many in this close succession. Usually, you have a couple, and you may not see another load like that for years,\" he said. The summer of road closures is here for Ohio, thanks to Intel. Stay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Dallin Grimm Freelance News Writer SEE MORE CPUS NEWS MORE ABOUT CPUS AMD updates 3D V-Cache optimizer driver ahead of Ryzen 9000X3D launch Startup claims it can boost any processor's performance by 100X — Flow Computing introduces its 'CPU 2.0' architecture LATEST Elegoo's Neptune 4 Pro is now only $284 SEE MORE LATEST ► SEE ALL COMMENTS (12) 12 Comments Comment from the forums evdjj3j 280 feet is not longer than 100 yards. Reply peachpuff Four of these loads, including the one hitting the road now, weigh around 900,000 pounds, 400 metric tons, or 76 elephants. Adult or baby sized elephants? Reply rluker5 A typical semi is limited to 80,000lbs. in the US, 900,000lbs isn't a trivial task to run down the road. Lot of work being done. Can't be cheap. Reply parkerthon I can’t imagine this doesn’t cause damage to the roads or is this what they are dealing with? That much weight is intense on pavement. Hopefully some of the spectators shoot video and post on youtube so I can see this spectacle. ODOT themselves should share more. I love this sort of thing. Reply Notton They use special trailer trucks that are designed to distribute the load. AFAIK they don't exceed the axle load limit for the road they travel on, and they move slowly so the wear and tear on the road is not as high as you'd think. It's pretty fun to watch these on youtube from an aerial time lapse Cl1WHk17QVE Reply TechieTwo These very long transports have many axles/wheels to distribute the load. The real problem is the width, height and length. Reply Gillerer peachpuff said: Adult or baby sized elephants? African or Indian elephants? A number of regular fully-loaded semi trucks would have been a far more effective analogue; Not only do very few people have an instinct for how much elephants weigh, African elephants are much heavier than Indian ones (~1.6×). (Brings to mind Monty Python and the Holy Grail and the discussion on various species of swallows and their respective coconut-carrying capacities.) Reply Eximo But then again African Elephants are non-migratory. Reply Neilbob It's a power supply. Just the one. Reply Amdlova The truck willdestroy less the road than a tesla car with a fat family inside. Reply VIEW ALL 12 COMMENTS Show more comments",
    "commentLink": "https://news.ycombinator.com/item?id=40658095",
    "commentBody": "Intel is trucking a 916k-pound 'Super Load' across Ohio to its new fab (tomshardware.com)197 points by prng2021 5 hours agohidepastfavorite180 comments coldpie 5 hours agoIt doesn't happen often, but I always enjoy when I spot oversize equipment making its way across the roads. Enormous construction equipment; wind turbine blades; partial buildings. The kind of stuff that requires multiple lanes, careful route planning, and lead & follow cars. Gives a little glimpse of the immense effort we put in to building, improving, and maintaining our society. reply Loughla 2 hours agoparentDuring covid shutdown a massive piece of industrial machinery was transported down our rural highway. It had like 10 lead and chase cars and a dozen state police. The actual item had 4 semis with 2 pushing and 2 pulling. I still have no idea what it was, but it added some excitement to a very mundane April that year. I have to imagine it was a very opportunistic move, since most traffic was stopped for covid already. reply CydeWeys 2 hours agorootparentIf it wasn't obviously a wind turbine blade (or a rocket!), maybe it was a petroleum distillation column? Those are a common extra-oversized highway load. reply denimnerd42 1 hour agorootparentaccident involving one in the news recently. impatient/distracted driver crashed into the transport truck. https://www.tdtnews.com/news/business/article_c49e80c8-124b-.... reply fathyb 1 hour agorootparentLink for Europe: https://archive.ph/IHdjm reply TiredOfLife 1 hour agorootparentprevCareful with that website. I am trying to access it from EU. \"We recognize you are attempting to access this website from a country belonging to the European Economic Area (EEA) including the EU which enforces the General Data Protection Regulation (GDPR) and therefore access cannot be granted at this time. For any issues, contact webadmin@tdtnews.com or call 254-778-4444.\" reply denimnerd42 39 minutes agorootparentoh interesting. well if someone wants to learn more via a search engine could use terms distillation column crash and: > The incident occurred at about 11:20 a.m. April 27 on State Highway 36 near State Highway 317 in West Temple. reply rat87 14 minutes agorootparentprevThat's not a sign of anything malicious. GDPR is just a pain in the ass to implement properly, for some US websites it's just much easier to ban people from the EU from accessing it then risk the potentially massive fines reply nvahalik 42 minutes agorootparentprevOh yeah I remember that! My wife had to turn around and iirc that road was closed at least 2 days. reply xattt 3 hours agoparentprevPrince Edward Island is connected to the mainland with a two-lane 13.6 km bridge. There are designated times over the course of the day when the bridge is closed to traffic and oversized loads are transferred across. Several deliveries are usually staged one after the other, so you get to see a variety of loads if you are unlucky enough to have to wait for 30-40 minutes. Most of the time, it’s pre-fab buildings and nothing too exciting. reply indoordin0saur 49 minutes agorootparentBeing on an island, it seems like loading it on a barge would be more efficient. reply jamesfmilne 5 minutes agorootparentDoes it have a harbour with sufficient cranes to offload it, and a good enough road to transport it out? And if you're going to need 4 trucks to get it out of the port anyway, might as well just drive it there. Sorry, no idea why I'm debating some random person on the internet about an issue which has absolutely no impact on me whatsoever. Sport, I guess. reply Damogran6 39 minutes agorootparentprev'loading it on a barge' doing a lot of heavy liftin there. (heh) reply wishfish 1 hour agoparentprevFor anyone interested in simulating such a drive, American Truck Simulator and Euro Truck Simulator both have DLC (\"Special Transport\") with trucks pulling massive payloads. Lead & follow vehicles. Plus a police escort. Mostly fun and a little frustrating. The latter due to not-always-great AI on the lead vehicles / police escorts reply whartung 4 hours agoparentprevMy wife didn’t realize it at the time, but she got to see the Space Shuttles SRBs on the freeway as they were coming into LA for the Shuttle exhibit there. Searching for when they moved Endeavor from LAX to the museum years ago is worthwhile. That was a tight fit on LA streets. reply sib 3 hours agorootparentA friend of mine was the official photographer for the move of the Shuttle to the museum in LA. https://www.instagram.com/p/CjtJA0PPWm3/?img_index=1 reply bcrosby95 3 hours agorootparentIt's actually smaller than I expected. It always appeared so much larger in the launch pictures. reply dylan604 2 hours agorootparentThe fact it could ride piggyback on a 747 shows either how small the shuttle was or how big a 747 is. Either way, it was cool to see reply sib 2 hours agorootparentprevI agree - they've recently repositioned it vertically (as it would be launched). I was at the museum a few weeks ago and it is definitely smaller than you'd think. reply ceejayoz 3 hours agorootparentprevI wish it were feasible to occasionally take it out for a parade again. What a lovely moment that was. reply CoastalCoder 3 hours agorootparentprevOne time when I was flying out of DC, I got to see a space shuttle on the airport tarmac. (I can't remember if it was mounted on a 747 or not.) A very cool surprise. This was probably in the 2009-2013 timeframe, IIRC. reply throwup238 4 hours agorootparentprevThe California Science Center has a little projector room showing a timelapse of the Endeavor being delivered to its final resting spot, right before you get to the big room with the shuttle. reply woodrowbarlow 4 hours agoparentprevdriving the follow car seems like an interesting job. they have to predict drivers and strategically position themself to redirect the herd. a diesel sheepdog. reply hansvm 1 hour agorootparentMost jobs are probably super interesting for a month or three. Even the simplest of jobs assume you won't be productive immediately, which implies there's probably something fun to learn. I used to do a number of those odd tasks (jackhammering down bank safes in preparation for the new tenants, ordinary pizza delivery, engineering a track for people to ramp/jump their mid-90s shit-mobiles, ...). I'm in tech as an ML engineer or something now, but one of these years I think it'd be a ton of fun to sit down and intentionally experience a hundred or so jobs and get a feel for what other people do for their livelihoods -- ideally in many distinct locales (e.g., where I grew up it was common for people to rent/(buy-with-intent-to-resell) a gas-powered bandsaw mill and produce most of the raw materials for their house, partly because land was cheap, partly because wages were piss-poor, and partly because there was a strong \"make-do\" attitude where people were willing to work hard to make a nice life with whatever hand they were given -- the sorts of jobs you'll have access to there are very different than those even a few hours away, much less a few states or countries). That was a bit of a minor rant. I'm in strong agreement though; that sounds like a super interesting job. reply therouwboat 3 hours agoparentprevI remember one wind turbine blade transport in small country roads in Finland, first car was basically driving on oncoming traffic side and forced me to go as close to ditch as possible, but it was good because the truck and trailer barely fit past my car. reply jauntywundrkind 18 minutes agoparentprevIt's definitely mind-boggling to me too. I do hope Radia gets their WindRunner plane off the ground. It's designed for onshore wind renewable energy, to be a plane with colossal internal volume. But it's also designed to work on landing strips that are much shorter & crucially primitive iirc or at least much more basic, to radically expand access to where we might setup wind power. I have high confidence such a plane would most likely end up being used for other tasks too, if created. Maybe it's not as absurdly expensive as I imagine, planning & coordinating these intense logistics routes, but being able to airship things to interesting destinations seems super compelling to me, seems like it could enable a lot of infrastructure development that's simplify infeasible right now. Marching into fancifulness here, but with something as big as a fab, I could definitely picture an initial construction phase where a sizable fraction of the land is dedicated as runway to start, isn't developed, while big items are flown in. Sounds wild, but could be possible! The model Radia has is off-site construction, and it's not too too hard to imagine perhaps more than wind turbines might benefit from this model: maybe not just coolers and fabs, but perhaps even prefab building walls could benefit from this. https://www.wsj.com/business/energy-oil/how-the-worlds-bigge... https://news.ycombinator.com/item?id=39690182 Maybe it's infeasible, but I could imagine something as large as a fab being able to reply adolph 4 hours agoparentprevI wonder if: * there are oversized load spotters like those folks with a hobby of keeping track of aircraft or trains? * there is economic intelligence value in extracting and surfacing data from oversize load permits? reply elil17 4 hours agorootparentI'd guess that there's not a lot of stock trading value in tracking oversize load permits. I think most publicly traded companies announce large capital investments like new factories long before trucks hit the road. reply Arrath 2 hours agorootparentprevFor the 2nd, perhaps in identifying critical routes which may be worth an extra bit of infrastructure funding for upkeep to allow such important loads to continue to transit without issue. Then again, the need for permitting should mean that state DOT's are already aware. reply sadhorse 1 hour agoparentprevSadly you can't get a glimpse on the immense effort we put into destroying our society. But the nunbers are available if you are curious enough. reply bell-cot 4 hours agoprev> Intel's 916,000-pound shipment is a \"cold box,\" a self-standing air-processor structure that facilitates the cryogenic technology needed to fabricate semiconductors. The box is 23 feet tall, 20 feet wide, and 280 feet long... The weight gives Tom a big number for his headline - but 99% of the problem with this load is the dimensions - which are too big for normal railroads. (Yes, the weight is also large enough to rule out using a heavy-lift helicopter.) Vs. a single 1940's-era steam locomotive+tender could weigh over 1,200,000 lbs. Modern locomotives - where electronic control makes it very easy, flexible, and wage-saving to operate 'em in sets - generally weigh a bit over 400,000 lbs. per. reply giarc 2 hours agoparent\"and 280 feet long\" Is that an error? reply a1o 2 hours agorootparentThe website posted has the picture not corresponding to the actual load. Check the picture here: https://www.truckersnews.com/news/article/15677391/expect-de... It appears to indeed be 280 feet long. reply xboxnolifes 2 hours agorootparentprevDon't think so. The article emphasizes that it's nearly as long as a football field. So, if it's an error then it's with their source. reply knodi123 2 hours agorootparentHuh. I usually push against the use of american-style weirdo units to describe things, but I guess one benefit is that if you specify multiple different units, you get automatic protection against typos! reply black6 3 hours agoparentprevToo long for even the largest Schnabel car in the US (maybe worldwide.) reply bell-cot 1 hour agorootparentGiven the height and width (23' and 20'), I suspect the length doesn't matter much. Customizing a Schnabel car is plausible. Raising overpasses, widening tunnels, etc...not so plausible. reply rqtwteye 6 minutes agoprevI remember listening to a German podcast about this kind of transport. It's super interesting how much planning is needed before such a transport. Talk to different police departments, check weight limits of roads, corner radius, plan to disassemble some obstacles. Huge effort. reply jjice 4 hours agoprevTangential, but what's the state of semiconductor fabs in the US? Looking at this Wikipedia article [0], there are quite a lot. That said, all but three (assuming I counted correctly) are pre-2020. Is the push for fabs in the US specifically to have modern, sophisticated fabs? My initial understanding of needing more fabs in the US was mainly for embedded stuff like for military, automobiles, and that kind of thing. Is this push actually more so for higher end fabrication for modern non-embedded use? Whatever the case is, having some more domestic production (especially for something as valuable as microprocessors) seems like a big win for any nation. I'm looking forward to seeing how the US does with chip fabrication. I don't expect them (us?) to become the dominant player, but I am bullish on US chip production. [0] https://en.wikipedia.org/wiki/List_of_semiconductor_fabricat... reply Workaccount2 4 hours agoparentBeing able to produce state of the art semiconductors is arguably the most important manufacturing ability for a country to have. The US does not want to be dependent on Taiwan or South Korea to build what is nowadays a cornerstone economic driver (compute) and cornerstone defense tool (compute). So to put it simply; the US wants to be sure it can still make H100's even if the rest of the world goes to shit. reply krisoft 3 hours agorootparent> Being able to produce state of the art semiconductors is arguably the most important manufacturing ability for a country to have. I don't think that is quite right. It is a very important ability. I don't think it is the \"most important\" ability. If you have semiconductor fabs but not dry docks to build capital ships you will be in for a world of hurt. If you have semiconductor fabs but not agriculture to feed your people you will be in a world of hurt. If you have semiconductor fabs but not the ability to cast solid-fuelled rocket engines for your missiles you will be in a world of hurt. It is one of the many important abilities. The reason we are talking about it is not because it is the \"most important\", but because it is at danger of being lost. We don't talk about the other equally very important abilities (like dry docks for giant ships, agriculture to feed the nation, or solid fuel casting, or a myriad of other things) because nobody worries about those going away. reply ivalm 3 hours agorootparentI am not sure capital ships are as critical now. With drones/hypersonics it seems they are too vulnerable for use in any peer conflict. Pretty sure if China/US were to wage war in 5 years then all the capital ships from both sides in the conflict zone will be scrap within 24 hours. Send 100 hypersonics per ship and one will hit. reply buildbot 3 hours agorootparentThe parents point is more that for a functioning state, you need all of those things. If one of those things is at risk externally, bringing that one internal first makes sense. Carriers are still very important to US force projection and hypersonic missiles are really overblown. We also seem to be readily able to take out existing hypersonic ballistics. Also a 5 year war with the US and China that starts with a multiple thousands of missles? That’s just going to be a nuclear exchange and last not very long at all if one sides detects any launch like that. reply ethbr1 2 hours agorootparentprevWe'll see how the SinkEx on the ex-USS Tawara goes at RimPac 2024. I'd be surprised if they don't use some ASBMs. But larger ships are built to take an incredible amount of punishment. It typically takes a heavyweight torpedo to crack them (hence why ASW is a primary skill set for navies). The physics of getting a 1/4 ton+ non-nuclear warhead (torpedo class) highly maneuvering are rough. And there's a reason the Navy developed and deployed SM-6, and is now adding SEWIP Block III... https://m.youtube.com/results?search_query=rimpac+sinkex reply generic92034 2 hours agorootparentJust recently a major ship was sunk, though: https://en.wikipedia.org/wiki/Sinking_of_the_Moskva Did they make mistakes? reply ethbr1 13 minutes agorootparentStill having a conscript-based navy? Loading their capital ships decks with exposed, extremely large anti-ship missiles? And they likely still would have been able to tow it back to port, if the weather hasn't been bad. reply dylan604 1 hour agorootparentprevAre you asking if the underfunded Russian military made mistakes in manning a ship, building a ship, designing a ship? Surely, it was meant to be rhetorical reply bee_rider 1 hour agorootparentTechnically, the Russian military didn’t make any mistakes in designing or building that ship since it was inherited from the Soviet Union. Although make the mistake was having it still in operation… reply dylan604 1 hour agorootparentprev> Send 100 hypersonics per ship and one will hit. What about all of those submerged ships that will lay waste to their opponent with the weapons they carry? reply ivalm 6 minutes agorootparentSure, subs also have a place. I’m just saying that conventional warfare with high tech weapons favors resource decentralization. Right now things are easier to blow up than defend. reply Workaccount2 1 hour agorootparentprevArguably the most important, not the most important. You can make a strong argument for why it is, is all that I am saying. reply elzbardico 2 hours agorootparentprevTha vast majority of military chip requirements can be met with old processes. You don't need a 5nm chip with a 13 billion transistors in a F-35 or for the guidance module in a cruise missile. Very modern chips are more suited for intelligence work no weapons systems per se. The physics of flight, artillery and balistic missiles is pretty much well understood, we don't need machine learning for that. Some modern systems use computer vision as a terminal guidance system, but again, you don't need state of art semis for that. reply vel0city 1 hour agorootparentEven from a designing a chip for ordinance perspective, there's a lot of meat on the bone to make chips just that much more accurate and that much less susceptible to interference. And making those more and more reliable, cheaper, and smaller is also a big deal. Stuff like this wasn't realistic from systems made in the 90s or even 00's. https://youtu.be/vY9rJBL1S2Y?si=Dq1QJp90Up6UKLaQ reply yyhhsj0521 2 hours agorootparentprevI'm not a military expert by any means, but I imagine 5nm chips/ML are immensely useful in designing F-35 and cruise missiles. reply 4gotunameagain 1 hour agorootparentFor CDF (Computational Fluid Dynamics) yes, but ML not so much. This is a relatively new technology and this sector is slow to change. Add in the fact of the black box nature of ML, and it becomes a pain to anything that requires a certification. reply Workaccount2 1 hour agorootparentprevThe defense department is far larger than missiles and planes. Also nuclear weapons simulation is what keeps the DoE continuously buying new world leading super computers. reply vel0city 4 hours agoparentprevSometimes the fab location might be older but the fab itself might have gone through much retooling throughout the years. I doubt the TI fabs in Sherman and Dallas in 1965/1966 are running all the same equipment as back then. There are a lot of interesting fabs in the US making some pretty bleeding edge products, but often not digital microprocessor chips. A lot of the more bleeding edge are analog/RF kind of stuff, especially GaN and GaAs stuff. reply brewdad 1 hour agorootparentA modern fab is basically a super, super clean warehouse. The equipment and layout can easily be changed up for a new generation of products. reply briffle 2 hours agoparentprevIntel is costantly modernizing one of its fabs in PDX. From what I have heard from a neighbor working at it, Intel brings most new equipment here, uses it for a certain time, then packages it up and ships to Arizona, so they don't have to pay sales tax on it, since its now used... reply brewdad 1 hour agorootparentIntel gets a sweet break on property taxes too, basically paying a flat fee per year. Otherwise, they could never afford the taxes on $20-40 billion of equipment in a single fab. The state makes up for it by taxing 20,000 well paid employees at a 10% income tax rate. reply dfxm12 4 hours agoparentprevYou can find context for the recent push here: https://en.wikipedia.org/wiki/CHIPS_and_Science_Act reply jjice 4 hours agorootparentThank you, this is exactly what I needed! reply bee_rider 3 hours agoparentprevSomething I think I should have better understanding of: My impression is that automotive and military applications mostly use older, more mature, cheaper modes. Are the fabs for these older nodes mostly re-purposes formerly cutting edge fabs, or do they go around building brand new (higher volume?) fabs for these nodes? I guess I’m wondering if the capacity to build automotive chips in 10 years will be limited by the ability to build cutting-edge chips nowadays. Or maybe if TI (or whoever) can, like, borrow a couple near-retirement TSMC guys in a couple years and spin up some brand new old-tech fabs. reply jandrewrogers 2 hours agorootparentIt depends on the application. Some use an unconventional process because of application requirements e.g. radiation hardening. Special purpose fabs are often subsidized to some extent to keep them running. Also, the US military will upgrade the silicon of existing systems if the situation warrants it e.g. it is cheaper to use more modern silicon than to maintain a fab for the old silicon. Most weapon systems use old silicon that needs to be robust in all military operational environments. They don't benefit from having more modern silicon. Terminal guidance systems on hypersonic missiles that do kinetic intercept of other hypersonic missiles often use something like a MIPS R3000/4000 class CPU and a DSP of similar vintage. ISR systems (intelligence, surveillance, reconnaissance) benefit from state-of-the-art silicon because that data is extremely large and analysis is time sensitive. Since pervasive ISR at scale is a cornerstone of modern military operations, having the best silicon and software for this type of computation is strategic. In these cases, it is often the latest commodity silicon. reply CoastalCoder 3 hours agoparentprevSince we're talking about chip fabs as strategic military assets, I'm curious how vulnerable they are to sabotage. E.g., how difficult would it be for a nation state to damage one in a way that requires a major replacement part from ASML, with long lead time? And with or without it clearly being an intentional act of sabotage? reply unregistereddev 2 hours agoparentprevIntel has older fabs on that list - including one that was built in 2003 - that are capable of 7nm production processes. Some of the older domestic plants have been significantly updated over the years. reply burnte 2 hours agoparentprevIt's mostly about having enough capacity so if China goes nuts and takes over Taiwan the world isn't held hostage. reply notesinthefield 4 hours agoprevFor other Ohioans who were wondering about this : ODOT started talking about the shipping routes a couple weeks ago and contrary to the article, we do have a timetable here : https://www.transportation.ohio.gov/about-us/traffic-advisor... Im going between Cincinnati and Columbus several times over the next three weeks and am not looking forward to this even though it seems theyre taking a mostly out of the way route through the SE Ohio farming towns. reply jcranmer 3 hours agoparentI've put together a rough map of the route it's taking here: https://maps.app.goo.gl/MPubWurLDu4XjMQJ7 (unfortunately, the limitations of Google Maps keeps me from properly marking the final bouncing between Mink Rd and 310--it continues east past Mink Rd to go up OH-310, then returns to Mink Rd on US-40 then returns to OH-310 on OH-16 then back to Mink Rd on OH-161. Not sure why it's not sticking to one or the other way the entire last stretch.) If you're going between Cincinnati and Columbus, you should be absolutely nowhere near this route, as it's going nowhere near I-71. reply Arrath 2 hours agorootparent> Not sure why it's not sticking to one or the other way the entire last stretch. Generally, low overpasses or turns that would be problematically tight. E: Checking street view, for example: that Mink Rd overpass over I-70 is likely either too narrow or not rated for the weight. https://www.google.com/maps/@39.9506421,-82.7300744,3a,75y,1... reply dylan604 2 hours agorootparentUpon first look at the map, I thought it had to have loaded wrong to not see it starting at what I expected as a port city. From someone that grew up around rivers that can pretty much run dry during the summers, it's amazing to me that a river runs that wide/deep that far inland to a land locked area to be able to accommodate a ship of this size. reply jcranmer 1 hour agorootparentI couldn't find the exact port they're starting at, so I guessed a boat ramp just to put a pin somewhere. The only places near Manchester, OH I could find that looked like they had some kind of port facilities were what appeared to be a power plant, although per Wikipedia, the plant has been decommissioned, so it's possible that part of the site was reused for the offloading facility. The Ohio River is famously an easily navigable river; there's only one natural falls along the route from Pittsburgh (the confluence of the Allegheny and the Monongahela rivers) to Cairo (where it joins the Mississippi). It provides the plurality of the water to the Mississippi River, a little less than half of the total discharge. I don't know where you grew up, but the river is more comparable (in European terms) to the Rhine, Danube, Dnieper, Don, or Volga than something like the Thames or the Seine rivers. reply Arrath 1 hour agorootparent> so it's possible that part of the site was reused for the offloading facility. Quite likely, there's a fair chance that large items for the power plant (like the steam turbines) were brought in by the river in the first place. reply briffle 3 hours agoparentprevI guess i'm not sure why they would transport this in the middle of the day. Seems like starting at 8pm would mean less snarl, less traffic to deal with, etc. reply ethbr1 2 hours agorootparentIn my experience, that's the difference between southern and northern state DOTs. Southern states do their work from 8pm-midnight+. Northern states do it from 9am-5pm. reply dylan604 2 hours agorootparentSouthern states have temps >100° so the night time work is also just for the survival of the workers. (only a slight exaggeration) Also, night time is dark. It makes things much more difficult than it needs to be at the convenience of some drivers in a car. Natural daylight is just so much better than having to have portable lights. reply brewdad 1 hour agorootparentYes. Likely there are multiple points along this route where the vehicle will need to be aligned just so in order to safely make a turn or clear an obstacle. Far easier to do those maneuvers in daylight rather than risk compromising the cargo. It’s road construction season in Ohio anyway so most drivers are going to face a significant delay along their journey regardless of when this load gets moved. reply immibis 1 hour agorootparentprevSouthern states are the ones where it's illegal to give water to workers - I can't imagine they care very much about worker survival. reply cityofdelusion 31 minutes agorootparentThis is false, it is perfectly legal to give water to workers in the South (and in every other state). reply ethbr1 9 minutes agorootparentParent is stretching a few bits of actual news to absurdity, for rhetorical effect. Conservative state governments battling liberal city/metro governments for power. E.g. https://apnews.com/article/texas-death-star-water-breaks-con... Some rather draconian \"election reforms.\" E.g. https://www.cnn.com/2023/08/18/politics/georgia-election-law... TimMeade 3 hours agoparentprevIt's going within about 10 miles of my mothers house... Not sure she would care... But i'll tell her! reply Severian 3 hours agoparentprevOh god, 33 and Gender Road. That's not going to be pretty for traffic. reply azalemeth 1 hour agoprevA fun little fact is that the load on a road causes damage roughly proportional to the fourth power of its mass – or at least, proportional to the fourth power of the weight of each axel. The US government publication detailing this is here – https://onlinepubs.trb.org/Onlinepubs/sr/sr61g/61g.pdf – in 1962 – and it's become something of a famous approximation ever since. reply Keyframe 45 minutes agoparentThat'd mean that a twice as heavy vehicle would cause 16 times more damage than the lighter one. Wild and not really intuitive! reply neilv 4 hours agoprevThis is worth watching (2m41s): LA Times, \"Space shuttle Endeavour's trek across LA: Timelapse\" https://www.youtube.com/watch?v=JdqZyACCYZc&t=3s reply deweller 3 hours agoprev> ... and 280 feet long, stretching longer than a football field. Was this written by an AI? A football field is 360 feet long. reply ecshafer 2 hours agoparentI could see a journalist that just gets confused between yards and feet writing that. reply dylan604 1 hour agoparentprevThe unit itself if 280 feet long, but by the time it is mounted on wheels and has a truck in front of it, how long is it then? An additional 80' seems long for just the truck, but this doesn't seem like it'll be a normal cab over tractor type truck reply throwaway2037 4 hours agoprevDoes anyone know why Intel select Ohio over Oregon or Arizona? I tried to Google for it, but the sources are awful and nothing definitive. Intel has been quite tight-lipped about it. I saw this quote, which seems to be hiding something: > What exactly is meant by \"pursued us very aggressively\"? I can only guess. reply mindcrime 4 hours agoparent> What exactly is meant by \"pursued us very aggressively\"? You can pretty much bet it means \"economic development\" incentives of some sort, combined with a fair amount of personal glad-handing. Intel probably got all sorts of tax-breaks, credits, grants, and FSM-knows what-else as part of the deal to pick Ohio. And the Governor and others probably took Intel execs to plenty of nice steak-houses, strip-joints, exclusive parties, yadda, yadda, yadda. reply CoastalCoder 3 hours agorootparentI hope Pat Gelsinger has more integrity than that, but who knows. reply justin66 2 hours agorootparentYou could have a pretty bleak outlook on government-corporate graft and corruption and still recognize how ridiculous the idea of Mike DeWine and Pat Gelsinger in a strip club is. reply mindcrime 2 hours agorootparentstill recognize how ridiculous the idea of Mike DeWine and Pat Gelsinger in a strip club is. I'm not literally claiming that we can state with certainty that Mike DeWine or Pat Gelsinger went to a strip club. That's just an example to illustrate a more general point. reply justin66 2 hours agorootparentI don't think you really illustrated anything. (but don't get me wrong, the image Mike DeWine and Pat Gelsinger in a strip club is extremely funny) Ohio politicians are rather more expensive than your comment implies. If one wants to dig into how actual corruption of this sort works in Ohio during its current GOP supermajority era, they ought to start here: https://en.wikipedia.org/wiki/Ohio_nuclear_bribery_scandal reply tonetegeatinst 2 hours agorootparentprevPerhaps they were bribed using high purity sand or the governor \"gifted\" some high purity silicon ingots? Unrealistic but it would be histerical to imagine sand or ingot bribery reply istjohn 1 hour agorootparentprevWell Gov. Mike Dewine and the Ohio Republican lawmakers aren't above a little graft: > The Ohio nuclear bribery scandal (2020) is a political scandal in Ohio involving allegations that electric utility company FirstEnergy paid roughly $60 million to Generation Now, a 501(c)(4) organization purportedly controlled by Speaker of the Ohio House of Representatives Larry Householder in exchange for passing a $1.3 billion bailout for the nuclear power operator. ... > In July 2019, the House passed House Bill 6, which increased electricity rates and provided that money as a $150 million per year subsidy for the Perry and Davis–Besse nuclear plants, subsidized coal-fired power plants, and reduced subsidies for renewable energy and energy efficiency. Governor Mike DeWine signed the bill the day it passed.[0] 0. https://en.m.wikipedia.org/wiki/Ohio_nuclear_bribery_scandal reply osnium123 4 hours agoparentprevIntel is also expanding in Arizona and Oregon. The benefit of going to a third state is that you get two more Senators rooting for you and helping out with tax breaks. In addition, there are a lot of course excellent universities in the region to recruit from. Ohio also has water and is seismically stable. reply bsder 3 hours agorootparentAFRL is also nearby in Ohio. reply mikey_p 1 hour agoparentprevThis area of Ohio has been exploding lately, and within a few miles of the Intel site, are AWS, Google and Meta data centers. Microsoft just bought 200 acres within a stones throw of the new Intel campus as well, possibly for an Azure DC. Not sure how all that relates, but it sure is messing with housing prices in the area. https://www.datacenterdynamics.com/en/news/microsoft-buys-20... reply bee_rider 3 hours agoparentprevOregon has gotten a little pricy lately, right? I wonder if Ohio is in a sweet spot—low property values but still not far from the Megalopolis in the grand scheme of things. reply hindsightbias 2 hours agoparentprev\"The state government is providing incentives in three chunks: a $600 million reshoring grant that reflects the higher cost of building these factories in America; $691 million in infrastructure improvements; and $650 million over 30 years in state income tax incentives based on the number of workers Intel hires.\" Plus 15 congressmen, 2 senators are going to want their part of the Federal rain money. https://www.dispatch.com/story/business/manufacturing/2024/0... reply gostsamo 4 hours agoparentprevTax incentives, usually. Add to it government cooperation through sped up permits and the like. reply snypher 3 hours agoparentprevOregon is relatively close to the Pacific coast... reply bregma 3 hours agoparentprevWhat happens on the back 9 stays on the back 9, I'm afraid. reply soared 4 hours agoprev> Bruning shared that other companies are piggybacking on the super load route plans now that accommodations have already been made I would’ve thought the ability to do a super load or not determined a lot of your mfg/etc process. Seems surprising a company could switch over to a super load because it’s now available. reply gehwartzen 4 hours agoparentMakes sense though if the route is already getting prepped for one load. It's a huge amount of work from logistical and coordinating effort (trains getting re-routed, hazmats cleared, roads shut down, etc) to get the route setup. May as well send any other extremely large shipments along while it's cleared. Lots of companies have huge pieces of equipment sitting at factory X that they would rather have at factory Y for example. reply crote 2 hours agoparentprevThere are plenty of applications where a single load would be more convenient for one way or another, but it's possible to do it in multiple loads with a final assembly step at the destination. Nobody is going to spend months of planning and many millions of dollars on alterations to save a few grand on a one-off project. But hey, if someone else has already done the planning and alterations, why not piggyback off of that and save yourself some money? reply leetrout 5 hours agoprev> 280 feet long, stretching longer than a football field Seems unclear to phrase it that way when talking a about the dimensions of the load and then comparing the entire transport apparatus length. Unsure how they jump from 280 being greater than 360. reply adolph 4 hours agoparentGiven that the author makes reference to the overall transport apparatus, the correctness of the length analogy also depends on if they refer to the \"field of play\" or the \"entire field\" or if the word \"field\" was edited from \"pitch.\" The rectangular field of play used for American football games measures 100 yards (91.44 m) long between the goal lines . . . . The entire field is a rectangle 360 feet (110 m) long. . . [0] The pitch is rectangular in shape. . . . the longer sides are called the touchlines. . . . The two touchlines are between 100 and 130 yards (91 and 119 metres) long . . . [1] 0. https://en.wikipedia.org/wiki/American_football_field 1. https://en.wikipedia.org/wiki/Football_pitch reply kuu 3 hours agoparentprev85 meters reply JasserInicide 3 hours agoprevWith semiconductor manufacturing being overseas (where lower wages are) for much of the past decades, how will Intel/TSMC opening up locations in the US affect prices? I imagine this just means everything will just get more expensive. reply zaphod12 3 hours agoparentsemiconductor manufacturing is highly skilled - ie the folks over seas were already well paid. It was a lot more about approvals, permissions, etc, than wages with this manufacturing area. It shouldn't have a huge impact. reply bee_rider 3 hours agorootparentAlthough, in the US semiconductor companies have to compete for technical people against companies doing the apparently really economically productive stuff—coming up with ad algorithms and playing Wallstreet shell games. reply chrsw 3 hours agoprevAm I way out of line to assume TSMC will be well past the capability of this fab when in finally comes on line? Not when the current schedule says it will be ready but when the first actual production wafer comes out of the factory. reply cogman10 3 hours agoparentPerhaps, though fab advancements everywhere have slowed to a crawl. TSMC might not be significantly more advanced than what it is today when this fab comes online. I believe we are rapidly approaching the end of miniaturization. Even now, node sizes are mostly fabricated just to keep advertising smaller numbers. reply chrsw 57 minutes agorootparentThe end of miniaturization I can believe. But the end of performance, I'm not so sure. reply mperham 2 hours agoprevAny insights on why this component cannot be subdivided into smaller pieces? reply mperham 2 hours agoparenthttps://whatispiping.com/cold-box-cryogenic/ reply paulkrush 3 hours agoprevHere's an image of the huge load: https://www.truckersnews.com/news/article/15677391/expect-de... reply 01acheru 3 hours agoparentI hate Cloudflare internet... accessing a page from an iPhone... parameter1.com reply mminer237 2 hours agorootparentHere's a direct link on Ohio's website: https://www.transportation.ohio.gov/wps/wcm/connect/gov/f618... reply bombela 2 hours agorootparentI get a 404. And I also get blocked on the cloudflare link for some reason. reply jtvjan 1 hour agorootparentSame. I suspect they're all blocked in the EU to avoid having to comply with GDPR. It's archived here, though: https://archive.today/D0fQZ The picture from Ohio's website too: https://web.archive.org/web/20240612173428/https://www.trans... reply o283j5o8j 4 hours agoprev\"The box is 23 feet tall, 20 feet wide, and 280 feet long, stretching longer than a football field.\" Author doesn't understand football. reply swores 4 hours agoparentEither 280ft is longer than whatever type of football field they're thinking of, or it's not. No understanding of football needed for that. You haven't said what part of it you think is wrong - though I do believe that both American Football and Association Football (aka \"soccer\", or \"real football\" :P) do both play on pitches a bit longer than 280ft. reply david_shi 1 hour agoprevLooks like the Mad Max Fury Road caravan reply Remmy 3 hours agoprevThey've all passed through my town. They coordinated it all with our local sheriff's office and sent out notifications well in advance so people would know to take alternate routes. reply kylehotchkiss 1 hour agoprevSmall price to pay for a massive regional economic engine! reply yellow_lead 4 hours agoprevThis may seem like a weird question but I wonder how much it is worth? Presumably it's very expensive and precise equipment. I wonder if it's protected from gunfire in transit. reply teitoklien 4 hours agoparentmost definitely protected, and very expensive insurance too. > gunfire in transit Not just gunfire, they need to protect it from - Climate Activists, blocking road and sabotaging the instrument, like they destroy old paintings at museums and art galleries to bring awareness via headlines [1] - Foreign Adversary Intentionally Sabotaging Equipment to prevent America from leading in Semiconductor ( Had happened to India’s SCL silicon lab when they were catching up with market in 1980s ) [2] - And ofcourse Gun Slingers too [1](https://www.nationalgeographic.com/environment/article/famou...) [2](https://youtu.be/isBYV6QWDIo?si=_g_0uYoXqiekhh9n) reply swores 3 hours agorootparent> - Climate Activists, blocking road and sabotaging the instrument, like they destroy old paintings at museums and art galleries to bring awareness via headlines [1] Saying they \"destroyed\" any paintings is plain wrong, you've either not read past the headline or you're deliberately trying to make climate protesters look bad. While I'm aware of many art/museum related climate protests, I haven't heard of a single one that damaged any art - all of them, like the example you linked to, the protestors chose targets where they could avoid doing any real harm. In the story you linked they threw paint over a glass case that protects the painting, not over the painting itself. So it definitely wouldn't surprise me if climate protestors decided to delay the trip by temporarily blocking a road, but it would be extremely surprising if they did anything worse than splashing paint on something that does no harm other than needing a cleaning crew to get it off. reply sib 3 hours agorootparenthttps://www.nytimes.com/2023/11/07/arts/design/rokeby-venus-... reply DwnVoteHoneyPot 1 hour agorootparentprevhttps://www.bbc.com/news/uk-england-cambridgeshire-68515368 reply j_walter 3 hours agoparentprevI would guess based on their description this is not expensive or precise...at least compared to the cleanroom tooling like an EUV tool. This is probably a gas tower that is used to produce high purity gas on site (like N2). They are large and often transported from where they are built. TSMC AZ had one delivered from Texas a few years ago (or more precisely Linde had it delivered to their site next door to TSMC AZ). You can see the large towers in the pictures linked below. https://www.phxind.com/projects/linde-spectra-30 reply bob1029 4 hours agoparentprevI don't know that the entire volume is considered \"precise\". Most of it should be cryogenic equipment (compressors, pipes, etc), packed in some kind of insulation. If anti-material rounds did penetrate the load, on-site repairs might be feasible. Contrast this with something like an EUV tool, which would probably be instant scrap if this kind of attack were successful. reply dr_orpheus 4 hours agoparentprev> I wonder if it's protected from gunfire in transit I would guess that it isn't. My assumption (without any data backing this up) is that making a specific, super rugged transport container would cost more (along with the additional logistics of it) than the insurance on it. reply mhb 4 hours agoparentprevGunfire is the threat that comes to mind? Weird. reply ls65536 4 hours agorootparentIt has happened before with other kinds of large cargo (the type that you really wouldn't want to have undesired holes in): https://www.airliners.net/forum/viewtopic.php?t=276419 reply pjc50 3 hours agorootparentI'd heard about this, and wondered how America squares it with its ridiculously large and expensive War on Terror. reply dghlsakjg 3 hours agorootparentIt should be pretty clear after a two decades that it is a war on islam aligned unfriendly governments. The US largely doesn't take internal Christian terror seriously. And we certainly don't take rural whites acting recklessly with guns to be anything but the free expression of inalienable rights. reply SoftTalker 4 hours agorootparentprevHave you been thru rural Ohio? reply dylan604 1 hour agorootparentprevI've heard tales that Boeing's transporting of fuselages rarely made it to the destination without bullet holes. reply vundercind 4 hours agorootparentprevEver seen a stop sign in rural America? Quite a few folks like shooting stuff and have very poor judgement. reply bunderbunder 4 hours agorootparentprevI can totally see someone taking a potshot at it just for a lark. Heck, I can see someone doing it for the purpose of creating content for their TikTok or YouTube channel. It's not like something like that has never happened before: https://www.justice.gov/usao-cdca/pr/santa-barbara-county-ma... reply reaperman 4 hours agorootparentMost of the large petrochemical tanks on the Gulf Coast have bullet dings in them. Folks just like to take potshots at stuff. Luckily the steel of these tanks are already more than thick enough to handle most small-arms fire. They just take the damage into account when they do corrosion monitoring every few years (those impact spots tend to be thinner afterwards, of course). reply aftbit 4 hours agoparentprevAlmost certainly no. Probably not protected from meteor hits or car crashes either. reply dylan604 1 hour agorootparentThe 10mph travel speed under heavy escort seems like car crashes wouldn't be fatal. reply aftbit 36 minutes agorootparentThe Dali hit the Key Bridge at under 10 mph. The kinetic energy of a 415000 kg object going 4.5 m/s is about equivalent to a 2000 kg object going 65 m/s = 145 mph. reply dylan604 21 minutes agorootparentSomething that big in water trying to stop vs something on the ground with lots of friction contact with the ground is two entirely different things. I'm way less concerned about this thing hitting something moving at 10mph than I am someone else driving their thing into this bigger thing. It would have to be a massive thing to budge it. reply galdosdi 4 hours agoparentprevWhat? It's self protecting. How is a thief going to fence a million pounds of heavy equipment, and get away from a convoy of dozens of people? As soon as they turn onto some smaller road they're going to run into power lines. If it's so expensive for Intel/Ohio DOT to move it, it's not gonna be any easier or cheaper for a thief. This is like, what if someone stole the copper wiring from the white house Fun to think about though. Fun weird question. reply swores 3 hours agorootparentThe comment you replied to was talking about risk of people damaging it, not running off with it. And using your white house example, while I'm sure people stealing copper cabling from it isn't considered a high risk, the possibility of people using guns to shoot at the white house certainly is something they take quite seriously! reply dylan604 1 hour agorootparentprevI think you just gave Netflix Studios their next heist movie/series. reply ge96 54 minutes agoprevNew idiom unlocked reply jancsika 3 hours agoprev> 280 feet long There's no way the truck in the picture is hauling something 280 feet long! The traffic behind it is closer than that! reply mminer237 2 hours agoparentYeah, that photo is an older, separate very large load. They're trying to do a bunch of them back-to-back so they only have to figure out the logistics once. The 280' one hasn't left yet (or probably been loaded), but there's a picture of it on ODOT's site: https://www.transportation.ohio.gov/about-us/traffic-advisor... reply Almondsetat 3 hours agoprevThis seems like a super easy target for a foreign actor to sabotage and cause massive economic setback reply cogman10 3 hours agoparentNah. There are far easier targets with far larger economic impacts. This cold box certainly cost a lot of money, but it won't break the bank, just delay the fab construction. In fact, I could almost imagine that sabotaging this thing would actually have an economic boon as it would allow the roads to operate again (rather than shutting them down for days to move a 10mph mega fridge). Consider, for example, the economic damage of the camp fire wildfires. That was caused by a single downed power line. reply dylan604 1 hour agorootparent>the economic damage of the camp fire wildfires. That was caused by a single downed power line. So why are they called camp fire wildfires if a camp fire was not the start of it? reply beeskneecaps 3 hours agoparentprevAgreed, but I imagine the only setback is for the insurance company. reply lotsofpulp 1 hour agorootparentAn insurance company spreads losses, it does not make them disappear. From a country’s economic point of view, the destruction of an economically productive thing is just as much of a loss, regardless of whether a single business and group of investors/shareholders experiences it, or if it is spread amongst various insurers, and hence their insureds via increased premiums in the future. reply theandrewbailey 4 hours agoprevI grew up less than half an hour away from the fab site. Back then, if you told me that this would happen, I wouldn't believe you. reply ano-ther 1 hour agoprevWhere does it come from? reply stuff4ben 4 hours agoprevwhy would you not build that stuff onsite? reply 01acheru 4 hours agoparentIMHO because it's not something trivial to build so you would also need to build the stuff to build it. To build complex (and huge) machinery you need specialized factories with lots of other big and complex machinery, you cannot just create a factory to build n=1 objects it's not economically viable. reply bunderbunder 4 hours agorootparentThough, perhaps they meant \"assemble\". A lot of other big things, like wind turbines, are shipped in smaller pieces and assembled onsite. At least according to a family member of mine who works on them, you even get slightly different designs that are specifically tailored to the local highway transport regulations in different countries. That said, I think in general you're right that the story is that it's just cheaper to do it this way. I just think that assuming you'd need to build actual factories is a bit drastic. But perhaps even getting the equipment needed for final assembly onsite is prohibitively expensive compared to just transporting the fully-assembled equipment. reply 01acheru 3 hours agorootparentWell if they mean assemble instead of build the things change a bit, but anyway even when talking about big and complex things there are big and complex things that can be \"easily\" split into pieces and assembled at a later time and other things that cannot be split so easily so much more resources are needed to assemble the thing in its final form. Just guesses it's not something I'm into. reply ethagknight 4 hours agoparentprevThe short answer is probably that the government is throwing so much money at this, and it’s usually an uncapped direct government expense to do offsite utility work like this for these kinds of major projects. Compare to making it an “onsite expense” which will have a dollar cap. reply sneak 1 hour agoprevSo Intel is literally too big to fail now? Maybe they can share offices with Boeing. reply secondcoming 4 hours agoprev [–] 916k lbs = 415k kg reply Workaccount2 4 hours agoparent450Mg! Poor megagram never gets any love! reply Alghranokk 4 hours agoparentprevWhich to be even clearer is 415 metric tons. For context: typical max weight for a truck in the US of A is 80k lbs, or a bit over 36 metric tons. reply PaywallBuster 4 hours agoparentprev> Big rigs are limited by federal regulation to a maximum loaded weight of 80,000 pounds including cargo over 10x heavier than the maximum allowed by law reply bowsamic 4 hours agorootparentIt's split up into 18 separate loads reply npongratz 4 hours agorootparent> It's split up into 18 separate loads TFA suggests it's not split up: > Intel will put a 916,000-pound \"super load\" on the road in Ohio on Wednesday, for a trip that will cover approximately 150 miles in nine days and snarl traffic for over a week... > Four of these loads, including the one hitting the road now, weigh around 900,000 pounds — that's 400 metric tons, or 76 elephants. > Intel's 916,000-pound shipment is a \"cold box,\"... EDIT: Further information in the ODOT advisory detailing the schedule of this one shipment confirms 916k pounds is not split among multiple loads: https://www.transportation.ohio.gov/about-us/traffic-advisor... > This is the twelfth of nearly two dozen \"super loads\"... This load... measures approximately 23’ tall, 20’ wide, 280’ long, and weighs 916,000 pounds. reply swores 3 hours agorootparentprevIf that were the case the story would just be \"18 lorries are going to do a boring trip\", and there wouldn't need to be any special plans made for road use at all. reply bowsamic 3 hours agorootparentWell you can ask them why they thought it was such an interesting story to post. But yes the article discussing the schedule in detail says it is split up across 18 loads reply allannienhuis 2 hours agorootparentOne of the individual loads was at this scale. There are multiple 'super loads'. The article doesn't say that a single 'super load' was split up into 18 parts. reply Ylpertnodi 3 hours agoparentprevEddie Hall lifted 500kg. Trained for years. 415kg is a heavy beast. reply kayge 12 minutes agorootparentIt is heavy indeed, and unfortunately there weren't ~1000 Eddie Halls available for this trip so they had to settle for trucks :) reply Ylpertnodi 3 hours agoparentprev [–] Eddie Hall lifted 500kg. Trained for years. 415kg is a heavy beast. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Intel is transporting a 916,000-pound \"cold box\" across Ohio to its new $28 billion Ohio One Campus, causing road closures for nine days.",
      "The project involves building two chip factories on a 1,000-acre site, expected to create jobs and fund local schools and universities.",
      "The Ohio Department of Transportation (ODOT) is ensuring infrastructure can handle the load, including moving power lines, with other companies also using the planned routes for their large loads."
    ],
    "commentSummary": [
      "Intel is transporting a massive 916,000-pound \"Super Load\" across Ohio to its new semiconductor fabrication facility, highlighting the logistical challenges and planning required for such oversized equipment.",
      "The \"Super Load\" is a 23-foot tall, 20-foot wide, and 280-foot long air-processor structure, too large for normal railroads and heavy-lift helicopters, necessitating a complex route and multiple support vehicles.",
      "This transport underscores the importance of semiconductor fabrication facilities in the US for economic and defense reasons, aiming to reduce dependence on foreign countries like Taiwan and South Korea."
    ],
    "points": 197,
    "commentCount": 180,
    "retryCount": 0,
    "time": 1718199946
  },
  {
    "id": 40653785,
    "title": "T-Mobile Faces Backlash Over Broken Lifetime Price Lock Promise",
    "originLink": "https://arstechnica.com/tech-policy/2024/06/t-mobile-users-thought-they-had-a-lifetime-price-lock-guess-what-happened-next/",
    "originBody": "Wiggle room — T-Mobile users thought they had a lifetime price lock—guess what happened next \"T-Mobile will never change the price you pay,\" the carrier told users in 2017. Jon Brodkin - 6/11/2024, 9:28 PM Enlarge Getty ImagesNurPhoto reader comments 128 When T-Mobile announced price hikes of up to $5 per line on older smartphone plans last month, many customers were shocked because of T-Mobile's years-old promise that their price would never rise as long as they stuck with the same plan. \"New rule: Only YOU should have the power to change what you pay,\" T-Mobile said in a January 2017 announcement of its \"Un-contract\" promise for T-Mobile One plans. \"Now, T-Mobile One customers keep their price until THEY decide to change it. T-Mobile will never change the price you pay for your T-Mobile One plan.\" Unfortunately, the promise wasn't as simple as T-Mobile claimed it to be in that press release. T-Mobile also published an FAQ that answered the question, \"What happens if you do raise the price of my T-Mobile One service?\" It explained that the only guarantee is T-Mobile will pay your final month's bill if the price goes up and you decide to cancel. The FAQ stated, \"The Un-contract is our commitment that only you can change what you pay and we mean it! To show just how serious we are we have committed to pay your final month's recurring service charges if we were to raise prices and you choose to leave. Just let us know within 60 days.\" The FAQ link now just redirects to the T-Mobile home page, but the Internet Archive has a capture of the FAQ from January 2018. While we couldn't find an earlier capture of the page, a discussion on an Android Central forum shows that the text mentioned above was noticed by some customers in January 2017. The Un-contract was also previously applied to T-Mobile Simple Choice plans starting in March 2015. The 2015 announcement said the Un-contract would be enabled automatically with \"no crazy strings, no hoops to jump through, no hidden fees, no BS.\" Advertisement The recent price increases reportedly affect Simple Choice plans as well as other packages, but we haven't been able to find any language from 2015 that acknowledges an exception that lets T-Mobile raise prices on Simple Choice. We asked T-Mobile several questions today and will update this article if it provides more information. T-Mobile disputes complaint to FCC T-Mobile is pointing to the above caveat to defend itself against at least one complaint filed to the Federal Communications Commission after the recent price hike. Yesterday, a Reddit user and T-Mobile subscriber posted a letter in which T-Mobile asked the FCC to close the complaint filed by the user. T-Mobile's response to the FCC and the user who complained said: Regarding these changes, we are aware some customers have inquired about T-Mobile's Un-contract and Price Lock. With Un-contract, T-Mobile committed to its customers that if we were to increase prices and customers chose to leave as a result, T-Mobile would pay the customers' final month's recurring service charge, as long as we are notified within 60 days. Consistent with this commitment, customers who activated on an eligible rate plan between January 5, 2017 and April 27, 2022, can request to have their final month's qualifying service charge reimbursed if their rate plan increases and they choose to cancel service. Customers simply need to request reimbursement within 60 days of the price increase. The T-Mobile response goes on to describe the more recent \"Price Lock\" guarantee that was offered starting in April 2022 and discontinued in January 2024. The T-Mobile response said that customer lines covered by Price Lock are exempt from the recent price increases: As for customers with concerns about T-Mobile's Price Lock guarantee, it is important to note that customers with Price Lock are not impacted by the change. On April 28, 2022, T-Mobile began offering Price Lock on new account activations on qualifying rate plans. For customers who activated on a qualifying plan between April 28, 2022 and January 17, 2024, Price Lock guarantees that accounts activated with a qualifying rate plan, within the enrollment period, would not be subject to a price increase, so long as the account remained in good standing and the customer remained on the qualifying rate plan. T-Mobile's response to the complaint also said that customers who switch plans lose their Price Lock guarantee. Page: 1 2 Next → reader comments 128 Jon Brodkin Jon has been a reporter for Ars Technica since 2011 and covers a wide array of telecom and tech policy topics. Jon graduated from Boston University with a degree in journalism and has been a full-time journalist for over 20 years. Advertisement Channel Ars Technica ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=40653785",
    "commentBody": "T-Mobile users thought they had a lifetime price lock–guess what happened next (arstechnica.com)190 points by rntn 16 hours agohidepastfavorite155 comments silisili 16 hours agoI don't understand the move. Seems it would be ultimately cheaper(support, potential legal, churn, etc) and harbor more good will to just flag these old accounts and leave them alone. Most companies I've dealt with in the past just leave these older plans alone to rot, and try to motivate people off of them with new plans, new features, faster speeds, phone deals, etc. Not just arbitrarily deciding to jack up prices because \"technically we're allowed to.\" reply rollcat 11 hours agoparentI'm 100% convinced that all telecoms are bastards. Fortunately, regulation seems to help at least a little bit, but you kinda have to be prepared to wage some war - or eat your losses. (Just lost a phone number that has been my primary for over 7 years; that's after losing another one that I held on to for 11 years... Not to mention all the smaller stuff, like how contracts/extra packages are structured once you go over the month's limit.) reply Akronymus 11 hours agorootparent> I'm 100% convinced that all telecoms are bastards. The one I use literally just increases how much data is included with my plan, rather than increasing the price. I quite like it. (I don't think it's available outside of austria though) https://www.hot.at/ reply happymellon 7 hours agorootparentprevThis is one of the reasons I get annoyed at services that require a phone number for an account. As such, I am never going to switch to Telegram or Signal unless you let me have an account that's not tied to a number. I've lost shit in the past because of this, a phone number is only a temporary identifier. reply robcohen 7 hours agorootparentSignal supports usernames now. reply happymellon 2 hours agorootparentWent to Signal to sign up and it is requesting a phone number. How do I give it a username instead? reply ImJamal 5 hours agorootparentprevIt is still tied to your phone number though. reply heliodor 4 hours agorootparentprevYou don't have to limit that first statement to telecoms. reply dreamcompiler 10 hours agorootparentprevCompetition is what helps even more. The Sprint/T-Mobile merger should never have been allowed to happen. Mergers are never good for consumers, despite Chicago School propaganda that says otherwise. reply Avshalom 10 hours agorootparentPreventing mergers is regulation. reply darksim905 10 hours agorootparentprevChicago School? reply tokai 9 hours agorootparenthttps://en.wikipedia.org/wiki/Chicago_school_of_economics reply DontchaKnowit 5 hours agorootparentprevNot for nothin, but it is not an uncommon opinion that regulation is the root cause : Regulation = high barrier to entry in industry = monopoly/oligopoly = all telecoms are bastards. reply lovethevoid 2 hours agorootparentRegulation isn’t the root cause, and it usually never is the root cause as regulations are always made in reaction to, not usually proactively. reply thatguy0900 2 hours agorootparentprevIsn't there kind of a fundamental issue here that spectrum is a limited resource? How many competitors can there be when youre initial investment has to be a massive amount for a usable section of it reply vsnf 15 hours agoparentprevI had an ATT unlimited data plan, grandfathered from way back in 2004 or something. It was wonderful while cellular data was still only available in limited amounts. I only finally moved off it about 3 years ago. ATT never seemed to care much. The only problems I had were when I had to make certain changes to my account, the staff could never find my plan in the system -- on more than one occasion we had actual ATT database admins on the line to resolve things. reply consp 11 hours agorootparentHad the same idea from 2004 to 2010 at T-Mobile in Europe before they were bought out by VC. They just canceled the plans and switched me over but due to their special \"after 6 years you triple the data and call volume and it never expires\" I had booked enough minutes (which were exchangable for data on the new plan) to have free internet for another two. Of course the new contracts all had expiring minutes as soon as they figured that out. reply dyauspitr 12 hours agorootparentprevI had the exact same situation. I stuck with it through the limited data plan era with my grandfathered ATT plan and only changed it once we got the unlimited data plans again. It was great, the only problem I had was even though the data was unlimited, the grandfathered plan had a 400 text messages limit which wasn’t enough. reply xur17 16 hours agoparentprevI have one of these old grandfathered accounts from many years ago ( So it's a dormitory with unlimited dining hall, but you can only take one plate a week outside of the dining hall. You see how that's bad, right? My phone service, a utility, should not be micromanaging me. reply JohnClark1337 6 hours agorootparentprevOne of the reasons I used to swap out the OS on my android phone. Back then I used CyanogenMod (now LineageOS) and they weren't able to track when I was using tethering and when I was just using my regular data. reply adastra22 11 hours agoparentprevI believe they are losing money on some of these contracts, which changes the calculus significantly. During 2020-2025 some people even got multiple $5/mo plans for added lines, which T-Mobile is now regretting. They've already tried once to force-change some people's contracts (but backed down). reply xp84 12 hours agoparentprevMost people won’t go through the hassle of cancelling. They will bring in a LOT of money, and pay out a few of the pathetic “last bill” refunds. This whole thing though is like thinking you have wedding vows and not realizing there was fine print. It’s like “forsaking all others, till death do us part” but at the end of the vows it says “and if I break these vows I’ll pay you $75.” reply generic92034 11 hours agorootparent> “and if I break these vows I’ll pay you $75.” Depending on the financial circumstances one party might receive much more than $75 while breaking the vows. ;) reply renewiltord 16 hours agoparentprevAT&T did it by just turning off their old network haha. Then you had to pay to migrate. A friend of mine did this. I just stick with Google Fi which I like for simplicity. reply deepsun 12 hours agoparentprevWhile I was thinking it's a good thing, I just remembered of a tangential question -- home ownership. Do you think it's fair when someone bought a house long time ago, and hence pay just a small fraction of the property taxes of what newcomers pay, but get the same services (roads, schools, utilities etc) that are paid by property taxes? reply silisili 12 hours agorootparentThat's not how property taxes typically work in most places. An assessor updates the values on some period, taxes go up, and they pay their share. That actually became a huge issue in El Paso. Their property taxes range 2 - 3% of home value, which was fine when everything was worth 150k 5 years ago. Today it's nearly tripled, and some got tax bills in the 5 digits for homes they've had forever. Some states, like FL, prevent that from happening, which sounds nice on paper but is essentially what you describe. I'm not sure how I feel about that, but it was intended for retirees so I kinda get it. If your question is, is it fair people who bought before 2020 have an awesome interest rate and pay way less than newcomers, I'd say no, only because I'm angry I sold in 2019 and regret it! reply throw__away7391 12 hours agorootparentIn most places geographically speaking no, but a number of US states including California and Florida which account for a disproportionate number of homes have passed laws protecting existing homeowners from property tax increases by capping the rate taxes can be raised to well below the rate of housing price inflation. So as a new buyer you're paying not only a higher price and higher interest rate, but also higher taxes than the people you bought it from. reply bestnameever 11 hours agorootparentHigher property taxes generally would lead to lower home prices. But really, this applies with anything that you purchase. If the sales tax inceases, you may be taxed more for an identical item that was purchased prior to the sales tax increase. States and cities have a variety of ways to tax people. They don't need to rely on property tax. reply bryanrasmussen 10 hours agorootparent>Higher property taxes generally would lead to lower home prices. this is not my experience - since property taxes are generally related to the desirability of the house and updated periodically (so often years after a price rise in area) reply mitchdoogle 3 hours agorootparentIt's definitely influencing my home buying decision. In Philadelphia area, Delaware County property taxes are a significant amount higher than neighboring Montgomery county. I am looking in both, but my budget is about $30k higher in Montgomery county because of the taxes. I would venture a guess that similar houses in Montgomery county generally sell for more than their counterparts in Delaware county. reply bryanrasmussen 2 minutes agorootparentthe claim as I understand it is that the higher taxed houses will decrease in price in order to attract buyers, but property taxes are generally determined by prices, this would mean that house value should increase in Montgomery because lower taxes, and prices should decrease in Delaware because higher prices, and then in a few years the property taxes in Montgomery should increase and the taxes in Delaware should decrease. I of course am aware that property taxes change over time, but I don't think this kind of strongly observable see-sawing of the property taxes actually exists - so probably more data than just guessing would be useful. silisili 12 hours agorootparentprevI actually like how FL handles this. They don't stop it from increasing, they just slow the rate of increase. I'd say it's not fair for someone who budgeted appropriately to get forced out of their home for something out of their control, like the historic explosion of house prices. In time it'll work itself out, assuming they stay long enough, as it still allows increases in a falling or stagnant market. reply kmonsen 12 hours agorootparentThat’s also what CA does reply DragonStrength 11 hours agorootparentCA lets your kids inherit the tax treatment and allows corporations the same benefit. Once you do those things, it’s off in a whole other realm than normal homestead exemptions. The rapid run-up of home values in the US has led other states to follow, but they generally have higher caps on increases and limit the treatment to your primary residence. reply silisili 11 hours agorootparentprevInteresting, seems CA is 2% and FL is 3%. Both should probably be pegged to the inflation rate nationally each year, which would probably bring them up a few ticks, but otherwise seems reasonable to me. For owner occupied of course, -not- landlords. reply r00fus 12 hours agorootparentprevTmobile is not a representative government. It is a highly profitable company that arguably acts in a cartel fashion with the other two national providers. reply TeMPOraL 12 hours agorootparentprevInteresting example. But I can't help to think that, if that was to be the case, but in place of homes appreciating in value to the point they're primarily used as investment vehicle, then local community might be better off with such deal. reply AequitasOmnibus 16 hours agoprevWhile not mentioned in the article, it’s worth mentioning that this also coincided with T-Mobile’s acquisition of Sprint. Specifically, T-Mobile made assurances that the acquisition would lower prices. Instead, within a couple years they exercised their newfound market power to increase prices a fair amount. The merger should have never been authorized. reply overstay8930 15 hours agoparentThe end result was inevitable, if the merger was blocked Sprint would have just filed for bankruptcy and the 3 remaining carriers would split it up amongst themselves. Sprint was effectively on life support and the last CEO’s only job was to pump up subscriber numbers as high as possible to make a deal as attractive as possible. That’s why Sprint had so many “free” deals a year before the acquisition was announced. The US government made a massive miscalculation on Dish network, when in reality it really should have been split amongst CableCos who are now stealing customers from the big 3 because they have already done the hard part of building infrastructure, while Dish can’t even get off the ground. reply dangus 15 hours agorootparentI don’t understand why the government couldn’t have done a bankruptcy bailout of Sprint to maintain a vital industry/important competitor in the market. Essentially follow the same playbook as GM and have the government temporarily buy Sprint, rehabilitate it, and sell it back to public investors. What the government did with Sprint was akin to allowing GM to be purchased by Ford. Whether or not that happens in bankruptcy court is almost irrelevant: a huge business in an oligopoly merging into another is generally bad news and the government should be on top of it. (Let me also be clear to people who hated the automotive bailouts: they were easily the best possible outcome and hindsight shows they were great policy. The alternative was essentially the collapse of the American automotive industry, allowing the US to follow the fate of the British industry, and on top of that government made a profit on its investment of taxpayer dollars to carry it out) reply mike_d 9 minutes agorootparent> What the government did with Sprint was akin to allowing GM to be purchased by Ford. The government depends on three carriers (ATT, Verizon, Sprint) for its own redundant critical communications (Continuity of government, military communications, etc). The entire deal was structured in a way where where the wireless subscribers were shed onto T-Mobile and the important guts of Sprint were sold to Cogent for $0, who didn't have the capability or desire to take on the consumer facing business. reply tipsysquid 14 hours agorootparentprevSprint was a technological wrong turn. They invested in the next generation platform on multiple occasions. Billions $ into WiMAX left them with a nonpractical and expensive footprint and no mid term speed benefits to really show. Then, after wanting to keep original CDMA infrastructure, they went LTE/GSM anyway. This period meant multiple modems and lower battery life. It was over with no great options. reply grepfru_it 13 hours agorootparentDon’t forget Nextel, that was an expensive acquisition with almost nothing technological to show for it. reply qp11 12 hours agorootparentprevThats exactly how Governments in the rest of the world protect strategic sectors from being swallowed up by Wall Streets massive mountain of capital. reply sneak 1 hour agorootparentprevPerhaps tax dollars should not be used to engineer economic outcomes. If you get too big to fail, why avoid failure? It’s about incentives. How useful or efficient or competitive do you think taxpayer-funded Sprint would be today? What makes you think they would suddenly start being good with an influx of tax money when they weren’t good before free money injections? reply kortilla 10 hours agorootparentprev> they were easily the best possible outcome and hindsight shows they were great policy No, just because those are around does not mean there wasn’t a better alternative in bankruptcy. Bankruptcy does not mean the individual company stops making cars, let alone the entire industry collapsing. That’s pure FUD to justify the govt preventing a moribund industry from being purged of legacy companies coasting on their brand. reply android521 14 hours agorootparentprevThe bad unintended consequences are gonna be much bigger this way. Think of communism, it started with good intentions but the unintended consequences of absolute power concentration (who can buy or sell what and at which prices ) caused absolute corruption and inefficiency reply TeMPOraL 12 hours agorootparentThe point is for the government to have the ability to occasionally look the biggest wolves of Wall Street in the eye, and say to them, \"haha. but no.\". Exercising it doesn't require abolishing private property and taking central control of all trade. reply android521 8 hours agorootparentHis point is that government should bailout failing company. I don't think this will end well if it becomes a common practice to bail out companies. reply PaulDavisThe1st 14 hours agorootparentprevIn what way is \"communism\" a good example of what could happen with the US auto industry bailouts? reply thayne 14 hours agoparentprevThere really needs to be more accountability for such promises made during acquisitions. Or no credence should be given to such promises. This is not at all surprising, and we've seen this happen over and over. Two big companies merge, promise prices won't go up so the merge gets approved, then within a few years, sure enough prices go up. reply throwup238 8 hours agorootparent> Or no credence should be given to such promises. This should always be the default. The first rule of capitalism is that companies lie. Who’s going to stop them? reply blackeyeblitzar 15 hours agoparentprevI think the problem is that they’re competing against much bigger players, ATT and Verizon. It’s just not possible to compete against them as a late comer. That’s why there are no new networks, just MVNOs. Starlink maybe can change that. But outside of that technological change, we’re stuck and a smaller player cannot hope to be competitive on coverage against bigger players. Ultimately what we need is much stronger anti-trust legislation against all kinds of companies that are too big, or aren’t in functioning competitive environments. reply ramesh31 16 hours agoparentprev>The merger should have never been authorized. So that Verizon could own the entire market along with a few also-rans? It was a choice between the lesser of two evils. reply bsder 15 hours agorootparentIf Verizon controlled that much, then it should have been broken apart. You don't fix a monopoly by creating more monopolies. reply darby_nine 15 hours agorootparentprevWhat? Verizon's clear benefit died long before the acquisition, even if their market share did not. The best thing to do would be to foster competition, not actively bless its destruction. reply ClassyJacket 15 hours agorootparentprevHow is reducing competition good for competition? reply JumpCrisscross 15 hours agorootparent> How is reducing competition good for competition? I am not familiar with the telecom market. But blocking mergers doesn’t necessarily increase competition. (For example, breaking up Walmart would reduce competitive pressure on Amazon.) reply ramesh31 4 hours agorootparentprev>How is reducing competition good for competition? By creating actual competition. Sprint and T-Mobile separately would always be second tier to Verizon. Combined, their network actually competes. reply alexsereno 15 hours agorootparentprevHacker news is peak contrarian. If your stance was that this merger was incredible, your replies would be entirely about competition lol. reply TeMPOraL 12 hours agorootparentThat's to be expected. A typical conversation starts with someone taking a high-dimensional problem, projecting it to their favorite dimension, and saying \"the solution is obviously on my side on the number line\". Of course you'll get multiple \"contrarian\" replies ranging from \"hey, what about my preferred dimension\" to \"you know the problem has more than one of them?\". reply adolph 15 hours agorootparentprevHN has a diverse set of viewpoints. For any assertion one might make, there will be someone with the opposite viewpoint. In addition to out-and-out opposition or agreement that sounds vaguely oppositional, a comment might call for a pedantic annotation, a tangential aside or book recommendation. reply basil-rash 14 hours agorootparentDon’t forget the unconstructive meta-analysis. reply adolph 14 hours agorootparentsudo !! reply alexsereno 14 hours agorootparentprevThis is a wonderful example reply Dylan16807 12 hours agorootparentOkay, glad you made it impossible to write a comment that disagrees with yours. You know what they say about things that aren't falsifiable, they're definitely correct!! reply TeMPOraL 12 hours agorootparent> You know what they say about things that aren't falsifiable, they're definitely correct!! Ah, the ol' Hitchens's movie night: \"what can be asserted without evidence can also be dismissed with popcorn\". reply adolph 3 hours agorootparentThat Hitchens quote is pithy and snow-clonable but demonstrated an impoverished and static epistemology. It is circular and self-falsifying by respectively assuming the existence of “evidence” independent of the assertion’s observation bias and not obeying its own assertion. reply Havoc 6 hours agoprev> It explained that the only guarantee is T-Mobile will pay your final month's bill if the price goes up and you decide to cancel. What a joke. Price stays the same until we issue you a take it or leave it ultimatum at higher price. So basically like any other price? reply indymike 15 hours agoprevI was on Sprint and T-Mobile bought Sprint. T-Mobile is robotically cruel to customers. T-Mobile is rule-bound and failure to comply results in employees being fired. Here's the difference: I got a defective phone with sprint: phone replaced under manufacturer's warranty immediately. Same defective new phone scenario with T-Mobile: no replacement because you didn't buy additional insurance - even though you got the phone a month ago. Sorry can't help. I wish I could help you but I will be fired if I deviate from the rules. Thank you for your understanding. Avoid doing business with T-Mobile at all costs. They are horrible. So, I'm happily with AT&T now. reply giancarlostoro 15 hours agoparent> I'm happily with AT&T now. I never thought I'd hear those words. The one time congress split up a company that I recall, it was AT&T because of all their awful practices, and for YEARS I have heard of their crazy fees and fines, and what have you. reply kmeisthax 14 hours agorootparentT-Mobile was the US branch of Deutsche Telekom at one point AFAIK. The one German guy I know said that basically the whole country gets fucked on pricing and bandwidth, and that American residential Internet is vastly superior. It's not implausible to imagine the shitty behavior from German telecom companies rubbing off on their US subsidiaries. reply stkdump 14 hours agorootparentAs far as I can tell prices in Germany are better than in the US, and broadband while not stellar is fine. My home internet is 500/100 speed and costs 35€ (with unlimited data and calls to landline and mobile numbers, for whatever thats worth). My mobile plan is 8€/month for 10GB of data (5G, of course not great availability outside cities, but LTE is not that bad) and unlimited calls and SMS. reply TeMPOraL 12 hours agorootparentprev> The one German guy I know said that basically the whole country gets fucked on pricing and bandwidth, and that American residential Internet is vastly superior. Since \"American residential Internet\" has been the butt of jokes even in rural Europe for the past decade, I wonder how uniquely bad DT must have been for the US connections to compare favorably. EDIT: Or maybe it's the most developed nations being way past the peak of quality of life for its population (which I think is a pattern)? Reminds me of my employer from over a decade ago; at some point they had the bright idea to try and convert our work stations in Poland into fat terminals, with all the data being stored on the servers in UK. Problem is, their pricey London office Internet connection had bandwidth that was something like 1/10 of what I could get on the cheapest residential connection I could find in Kraków. Way less than any of us was used to. And they weren't planning to upgrade. IIRC, the whole idea got postponed indefinitely after multiple reminders that the London office is already so bandwidth-constrained that we'd rather look for another job than be stuck working in the London office in the first place, much less accept our local machines being limited by a link to their servers. reply indymike 6 hours agorootparentprev> I never thought I'd hear those words. I didn't either. T-Mobile is that bad. reply adastra22 11 hours agorootparentprevAT&T is not the Ma Bell you are referring to. Today's AT&T is the continuation of Southern Bell. reply Astronaut3315 15 hours agoparentprevAT&T sold me a defective new phone once. They replaced it with a refurb, and marked it in their system that I had purchased it that way. They could not be convinced that I should have a 1-year new phone warranty instead of a 90-day refurb warranty. reply zwerdlds 5 hours agoparentprevJust make sure to keep an eye on your monthly statement. AT&T will arbitrarily decide that you should upgrade your plan and charge you more for it - despite your previous plan being perfectly within your usage. reply WillPostForFood 15 hours agoparentprevTwo defective phones? What was wrong with them? reply indymike 6 hours agorootparentThey were made by LG right before they exited the business and had problems with the screens greening out when they got warm - as in 90°F (i.e. summer in pocket temperatures). reply adolph 14 hours agorootparentprev“There’s an old saying in Tennessee - I know it’s in Texas, probably in Tennessee - that says, ‘Fool me once, shame on… shame on you. Fool me - you can’t get fooled again.’” reply dehrmann 13 hours agorootparenthttps://www.gettyimages.com/detail/video/president-george-w-... reply _carbyau_ 12 hours agoprevThis is less a surprise and more an issue with false advertising. If it looks like a duck and quacks like a duck should I have to read fine print to realise it is a pig? In Australia there are limits to what advertising can get away with, presumably US has similar. reply Teknomancer 16 hours agoprevDitched T-Mobile last month, disgruntled AF over the rate hikes. Helium gives me the same exact service experience at 1/4 the monthly cost. reply radicality 13 hours agoparent+1 to Helium. I’m currently on Verizon on my iPhone, but got a second phone (pixel) that I got Helium on, grandfathered in to $5/month since I was early on. I don’t use it super often when out so can’t comment too much on the data service quality, but whenever I do use it, it works fine. reply stronglikedan 15 hours agoparentprevI'm currently evaluating both on the same phone, and I have not had that experience. If Helium had as good coverage as Tmo, I would have already switched. Fortunately, Helium usually has coverage when Tmo doesn't, so I may keep both since Helium is quite cheap (and even close to free with discovery mapping turned on). reply dangus 15 hours agorootparentHelium is just a T-Mobile MVNO. They’re not owned by T-Mobile, but they’re just an MVNO. Everyone can save by switching to an MVNO. The first trade off is that the big 3 carriers have essentially a tier list of customers based on what type of service they have for network priority. E.g., postpaid customers are highest priority, big carrier prepaid and big carrier-owned MVNOs are second, and unaffiliated MVNOs are placed based on the specifics of their agreements with the carrier. For example, Google Fi has high network priority because they undoubtedly pay more to use their networks than lower cost MVNOs. The second trade off is that MVNOs handle customer service and a lot of the customer facing administration. This can vary but is often pretty barebones for lower cost MVNOs. reply adolph 14 hours agorootparent> Everyone can save by switching to an MVNO. I think you are better off not saying that. If everyone were to switch to an MVNO then the large carriers MVNOs depend on would not offer as favorable rates to the MVNOs. Reminds me of when Apple licensed their OS and promptly got spanked. https://en.wikipedia.org/wiki/Power_Computing_Corporation reply bruceb 11 hours agorootparentThis applies to other things as well. Be glad people pay for the gym and rarely go. reply dangus 6 hours agorootparentprevWell, still, MVNOs aren’t for everyone. That statement in particular might have been a broad one by me. One Postpaid carrier strength is family plans. The per-person cost for 4 people is very comparable to MVNOs and you get better service. The next is phone financing and subsidies. If you’re the type of person who wants a new phone every 3 years, a postpaid carrier will have some of the more low-friction options for that. Finally, MVNOs simply don’t offer high usage plans for heavy users. For example, my current postpaid plan gives me something like 50-80GB of monthly tethering allowance before throttling. There isn’t really an MVNO offering that at any price. So, I think postpaid plans fit a higher income heavy user and/or a family pretty well. reply tithe 16 hours agoprevI've had good luck with Mint[0], but T-Mobile bought them last year so who knows how long that luck is going to last. [0] https://www.mintmobile.com/ reply el_benhameen 14 hours agoparentI’ve been pretty happy with Mint, too, but 1) I’m working on the assumption that we’ve got about a year before major price increases, and 2) probably just a coincidence, but my service has been shit since the purchase was finalized (I’m aware that Mint gets deprioritized when there’s congestion, but it was less of an issue until a few months ago). reply kotaKat 3 hours agoprevAren't all these contracts month-to-month anyways? T-Mobile can change their terms with 30 days notice and most if not all current T-Mobile customers can \"just\" leave. The only special gag now is that the average consumer now has \"device installment payments\" that become immediately due in full when you terminate your side of the deal, and we now tie subsidies to \"promo credits\" on those payments. reply walterbell 11 hours agoprevA condition of the Sprint merger was that T-Mobile had to offer low-cost prepaid plans without an MVNO. The program is called T-Mobile Connect, https://clark.com/cell-phones/connect-by-t-mobile/ & https://coveragecritic.com/t-mobile-connect-review/ & https://prepaid.t-mobile.com/connect/phone-plans Monthly price for USA-only unlimited talk/text + 5G/LTE data is $15/5GB, $25/8GB, $35/12GB + taxes/fees. Outside USA, T-Mobile pSIM Wi-Fi call/text continues to work with any cellular provider's eSIM data on the 2nd line of iPhone. reply jdietrich 11 hours agoparent>Monthly price for USA-only unlimited talk/text + 5G/LTE data is $15/5GB, $25/8GB, $35/12GB + taxes/fees. By global standards, that is hilariously expensive. Here in the UK, I pay $13/mo for unlimited talk/text and 100GB of 5G data. I can get truly unlimited data for less than $22/mo. https://www.moneysavingexpert.com/cheap-mobile-finder/sim-on... reply murderfs 11 hours agorootparentBy global standards, the United States has hilariously low density and has hilariously high incomes, which means it's significantly more expensive to maintain a network. You're comparing a network that doesn't have 5G coverage in towns 20 miles away from the center of London [1] to one that has coverage over a gigantic proportion of the area that actually contains people at any given time. [2] 1: https://www.nperf.com/en/map/GB/-/164526.Vodafone-Mobile/sig... 2: https://www.nperf.com/en/map/US/-/85.T-Mobile-inc-Sprint/sig... reply jdietrich 8 hours agorootparentIf you exclude legacy CDMA sites, the US and UK have basically the same number of cell sites per capita. https://opencellid.org/stats.php reply icehawk 50 minutes agorootparentIs cell sites per capita a relevant measure? reply murderfs 1 hour agorootparentprev\"Please note that Logical network numbers are only loosely correlated with physical infrastructure like cell towers.\" reply throwup238 8 hours agorootparentprevEven by US standard it’s ridiculous. AT&T offers a $25/mo prepaid plan with 16gb 5G and unlimited 4G. Not an MVNO but AT&T directly. The worst part though is TMobile’s predatory billing practices. They lie through their teeth about fees and prices when you sign up and it’s absolutely impossible to understand what the hell is going on with their billing. Fuck TMobile. reply walterbell 11 hours agorootparentprevUS postpaid service for a single line is $65+. reply bruceb 11 hours agorootparentprevThe US covers a lot more land. More towers to put up. Higher wages to pay. Less density than UK. reply wayne 15 hours agoprevVery different from the airline industry which still honors lifetime flight and lounge passes sold decades ago. https://www.cnbc.com/2018/04/20/mark-cuban-bought-an-america... reply pimlottc 12 hours agoparentThat’s not the story I’ve heard. What I’ve read are stories of airlines who regretted the deals and went after those who used them extensively, looking for excuses to revoke them: https://thehustle.co/aairpass-american-airlines-250k-lifetim... Anyhow, the Mark Cuban article says almost nothing about how the airline treated him, it’s just a light human interest story about an unusual and quirky thing a famous person did. reply jonfw 4 hours agorootparentIf the claims in the article are true around 2.5k cancellations and ticket reselling, I'd say they found great reasons to revoke these airpasses. reply danpalmer 12 hours agoprevIf they only committed to paying the final months bill, that's a very limited upside for customers. Far more limited than what \"no price hikes\" sounds like. There's basically no penalty for T-Mobile – if they raise prices and people stay, they win, and if someone chooses to leave (which they could do anyway if the price changes), they lose out in nearly the same way as if they hadn't made the promise. reply cjameskeller 14 hours agoprevI'm still on one of their prepaid \"Connect\" plans, and get unlimited talk & text, plus 12 gigs of data, for $35. Since I'm nearly always somewhere with wifi, I can't recall ever having run into an issue due to lack of data. I wonder if some folks could switch over to a plan like this and save money, if they planned media downloads to happen when they're on wifi? reply carleton 14 hours agoparentRather the opposite, I pay $20.99/month for unlimited for _just_ my iPad now on a different carrier because I don’t want to ever think about WiFi again when I’m not at home. Cellular is finally “good enough” that it’s better (faster/more reliable) than most hotel/public WiFi I’ve used in the past year. reply jerlam 14 hours agoparentprevI doubt most people know that prepaid exists and is extremely cheap, that they can easily port over their number to any carrier (required by law for a long time), or that they are using a small fraction of their \"unlimited\" data. My $25 prepaid plan comes with 16 GB of data, I don't even bother to use wifi on short trips. I used to manage my parents' plan, sometimes they barely use one GB a month. reply mdrzn 6 hours agoprev+1 to EU and Iliad, which arrived on the market in about 2018 and started providing 9,99€/month mobile internet plans forever, but FOREVER for real. I still have that plan. reply zemnl 4 hours agoparentYes, there are people I know who are still paying the same price Iliad offered at launch: 5.99€/month for unlimited calls and messages, and 30GB in 4G+. That price was basically half of what all other available providers offered at the time. reply ok_dad 12 hours agoprevI have a “select choice fam unl 50tt” which I’ve had forever (I buy phones outright to keep it) and still pay $85 for two lines. I don’t see any price increases for my account recently. I have unlimited everything except 5gb of tethering. If they raise my price I’d be upset but it’s still cheaper by far than any modern plans. Someday my kid will need a phone and I’ll get him a separate account so I don’t get conned into changing my plan! reply vzaliva 14 hours agoprevOkay, they could not be held accountable for contract breach because there is a small print somewhere in a FAQ linked from TOS, but perhaps they could be sued for misleading advertisement? One car reasonably argue that \"the price will never increase\" is not the same as \"if the price increases they will pay the final month\". reply aliljet 14 hours agoprevCount me marginally confused. Has T-Mobile already increased prices? Who is presently engaged in suing them for this breach of contract? reply 8note 9 hours agoprevHa. I was on the simple choice iirc, and when they decided they wanted to force an upgrade, they just broke all the data, and the only fix was to change plans. reply henriquez 16 hours agoprevi got caught up in this B.S… literally got the rate locked T-Mobile account because of lifetime rate lock. then last year they sent me “opt-out” notification that my guaranteed rate was increasing unless I send a certified letter or some stupid shit. I dumped them the next week because i don’t have time in my life to play fuck-fuck games with shady companies. I hope there’s a class action lawsuit, I would gladly get on board. reply crooked-v 16 hours agoparentThe fundamental problem here is that courts bend over backwards to allow fine print that contradicts the plain reading of how advertisements and services and presented. Unfortunately, the only way it's ever going to change in the US is if somehow a Supreme Court is appointed that's far less business-friendly than it has been in possibly ever. reply vsuperpower2020 16 hours agorootparentI'd be more fine with companies behaving like a swarm of locusts if they at least had to put on the box the ways they were going to fuck you over. A monkey's paw printer should have to prominently display \"Proprietary ink only! Scanner disables itself when ink is low. While supplies last\" on the front. reply dboreham 15 hours agorootparentprevAll lawyers are black hats. reply hi-v-rocknroll 11 hours agoprevRecently got away from really unlimited Verizon 4G when they kept upping the fees and nonsense to the tune of $200/month. Spectrum unlimited is really cheap for BYOD, does 5G UWB, and rides the coattails of Verizon. And, unlike Verizon, gives tethering and wi-fi hotspot sharing for free. reply afruitpie 15 hours agoprevNow that porting between MVNOs with eSIM is free and dead simple, I can’t think of a reason to stick with the big 3 carriers. The MVNO I stuck with even has a slick way to port between networks! reply carleton 14 hours agoparentDeprioritization is why - even 5G is unusable sometimes in busy places if you’re on an MVNO. I’d rather pay more for it to always work reply teeray 13 hours agorootparentIf the FCC were functional, it would be nice for carriers to expose priority in the phone icons. I can’t tell you how many times I’ve had to explain to friends who lamented about “Full bars but no service, weird.” The current iconography is outdated and clearly confusing (read: misleading) to the public. reply mbreese 12 hours agorootparentI think you’re right about this. There should be a special “limited” icon to show when your service is being deprioritized. I think would actually help the carriers as it would tell customer when a degradation of service was due to a congested network vs low signal. reply patrickmcnamara 12 hours agorootparentprevI travelled to the U.S. and Mexico using eSIMs recently and was running into this issue a lot. I guess this was the reason, thank you. reply radicality 13 hours agorootparentprevI’m curious, is there some kind of real-life benchmarks available online for this? I totally believe that a network would deprioritize the mvnos, and I’m guessing there is some pecking order depending on their contract with the network, but I’m curious how it converts to real world usage. reply joecool1029 12 hours agorootparentIt's not really possible to benchmark real-world. But to see which priority your account is provisioned at just need ability to see lower-level information from the baseband. Deprioritization = QCI 9. Different carriers use different QCI's but 9 is lowest, 6 is usually highest priority. Network Signal Guru is an app for android that will report this information, but it usually requires root (I think the pixel phones do not, qualcomm devices usually do). The radio layer is where deprioritization happens, not through deep packet inspection (as would be the case for video throttling). Having a strong signal with good RF conditions deprioritization won't be noticed as much, also won't be noticed if the sector isn't busy. If it is busy and you are near cell edge, that's when speeds will fall off (as the radio layer will schedule less time and resources for your device, and the worse conditions means it can't send as much data with those resources allocated). reply LUmBULtERA 3 hours agoparentprevWhat MVNO is that? reply anothername12 14 hours agoprevI guess I’ll look for a $10 gift card after the class action a decade from now. reply teeray 13 hours agoparentProbably expires in 30 days, so you better be vigilant. reply petarb 15 hours agoprevAT&T prepaid plan is $40 for 8gb data. Pretty happy with this reply cynicalsecurity 1 hour agoparentThis is ridiculously expensive from the point of European prices. reply SkinTaco 12 hours agoparentprevThe problem with ATT prepaid is it's deprioritized to the same level as mvnos - might as well buy an even cheaper mvno plan at that point. Tmobile is the only big 3 carrier where all branded plans have the same priority reply IAmLiterallyAB 14 hours agoparentprevThey've changed the plans a bit, you can get 8GB even cheaper I think. I pay $30 a month after the autopay discount. Though mine is only 5GB reply djaouen 15 hours agoprevI think it's a bit silly to believe a company wouldn't raise prices eventually. I mean, it's been 7 years. What about inflation? reply Dylan16807 15 hours agoparentBandwidth gets cheaper every year to help offset that, and don't they have that fancy 5G network to tempt people into upgrading? If they wanted to give a 5 or 10 year guarantee, they could have. They chose to say lifetime. The wireless market hasn't fundamentally changed in the last 15 years (starting in the iphone 3g era), so they don't have any excuses about being caught off guard. reply creer 13 hours agoparentprevThe concept of inflation dates back more than 7 years. Even a corporation was aware of it 7 years ago. reply otterley 14 hours agoparentprevWhile I agree with you, I also think companies shouldn't advertise in a way that strongly suggests that they won't ever raise their rates. It's a pretty scummy practice. reply bts89 16 hours agoprevThree weeks ago I changed my Verizon FIOS plan and was told multiple times that my new bill would be $160. Guess what happened next! If your guess was they immediately raised the price of the services on my plan and billed me $220 instead, you’d be right! reply runlevel1 15 hours agoparentVerizon math can be a little wonky sometimes: https://verizonmath.blogspot.com/ reply vincnetas 13 hours agoparentprev\"was told\" means nothing. If you don't have it in writing, they can say anything. One time when i was contacted by sales to offer me a \"new better offer\" i asked to send it to me in writing and they refused, because \"policy\". p.s. this is noname local provider that you have never heard of. reply teeray 13 hours agorootparentEven if you have it in writing, you need to have the tenacity to hold them to it in court. They can do the fun corporate game of “computer says no” and defy your written agreement as a gamble that you’re not willing to spend time and money suing them. reply bts89 6 hours agorootparentprevOh, I have it writing. A screenshot of the order, an email confirmation, and a physical letter. All directly from Verizon, all saying $160. reply floppiplopp 12 hours agoprevOh no, business did a capitalism. In other news: Water still wet. reply teo_zero 13 hours agoprev [–] \"We will not raise the prices.\" Small print: except, if we do, you'll get back your payment. Smaller print: except, we might give you a set of stickers instead. Even smaller print: except if we run out of them, then you'll get nothing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In 2017, T-Mobile promised no price increases for users sticking to the same plan, but recently announced price hikes of up to $5 per line on older plans, surprising many customers.",
      "T-Mobile's \"Un-contract\" promise included a clause that the company would pay the final month's bill if customers canceled due to a price increase, but this information is now harder to find.",
      "T-Mobile is defending itself against an FCC complaint, stating that customers with the \"Price Lock\" guarantee (April 2022 - January 2024) are not affected by the recent price hikes, provided they stay on the qualifying plan."
    ],
    "commentSummary": [
      "T-Mobile users are frustrated by unexpected price increases despite believing they had a lifetime price lock.",
      "Users suggest that telecom companies should incentivize switching to new plans with better features rather than raising prices on old plans.",
      "The discussion includes various opinions on telecom mergers, regulations, and the impact of property taxes on home prices, highlighting the complexity of the telecom industry and its broader economic implications."
    ],
    "points": 190,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1718158186
  },
  {
    "id": 40652917,
    "title": "The Genius of Binary Space Partitioning in Doom's Game Engine",
    "originLink": "https://twobithistory.org/2019/11/06/doom-bsp.html",
    "originBody": "In 1993, id Software released the first-person shooter Doom, which quickly became a phenomenon. The game is now considered one of the most influential games of all time. A decade after Doom’s release, in 2003, journalist David Kushner published a book about id Software called Masters of Doom, which has since become the canonical account of Doom’s creation. I read Masters of Doom a few years ago and don’t remember much of it now, but there was one story in the book about lead programmer John Carmack that has stuck with me. This is a loose gloss of the story (see below for the full details), but essentially, early in the development of Doom, Carmack realized that the 3D renderer he had written for the game slowed to a crawl when trying to render certain levels. This was unacceptable, because Doom was supposed to be action-packed and frenetic. So Carmack, realizing the problem with his renderer was fundamental enough that he would need to find a better rendering algorithm, started reading research papers. He eventually implemented a technique called “binary space partitioning,” never before used in a video game, that dramatically sped up the Doom engine. That story about Carmack applying cutting-edge academic research to video games has always impressed me. It is my explanation for why Carmack has become such a legendary figure. He deserves to be known as the archetypal genius video game programmer for all sorts of reasons, but this episode with the academic papers and the binary space partitioning is the justification I think of first. Obviously, the story is impressive because “binary space partitioning” sounds like it would be a difficult thing to just read about and implement yourself. I’ve long assumed that what Carmack did was a clever intellectual leap, but because I’ve never understood what binary space partitioning is or how novel a technique it was when Carmack decided to use it, I’ve never known for sure. On a spectrum from Homer Simpson to Albert Einstein, how much of a genius-level move was it really for Carmack to add binary space partitioning to Doom? I’ve also wondered where binary space partitioning first came from and how the idea found its way to Carmack. So this post is about John Carmack and Doom, but it is also about the history of a data structure: the binary space partitioning tree (or BSP tree). It turns out that the BSP tree, rather interestingly, and like so many things in computer science, has its origins in research conducted for the military. That’s right: E1M1, the first level of Doom, was brought to you by the US Air Force. The VSD Problem The BSP tree is a solution to one of the thorniest problems in computer graphics. In order to render a three-dimensional scene, a renderer has to figure out, given a particular viewpoint, what can be seen and what cannot be seen. This is not especially challenging if you have lots of time, but a respectable real-time game engine needs to figure out what can be seen and what cannot be seen at least 30 times a second. This problem is sometimes called the problem of visible surface determination. Michael Abrash, a programmer who worked with Carmack on Quake (id Software’s follow-up to Doom), wrote about the VSD problem in his famous Graphics Programming Black Book: I want to talk about what is, in my opinion, the toughest 3-D problem of all: visible surface determination (drawing the proper surface at each pixel), and its close relative, culling (discarding non-visible polygons as quickly as possible, a way of accelerating visible surface determination). In the interests of brevity, I’ll use the abbreviation VSD to mean both visible surface determination and culling from now on. Why do I think VSD is the toughest 3-D challenge? Although rasterization issues such as texture mapping are fascinating and important, they are tasks of relatively finite scope, and are being moved into hardware as 3-D accelerators appear; also, they only scale with increases in screen resolution, which are relatively modest. In contrast, VSD is an open-ended problem, and there are dozens of approaches currently in use. Even more significantly, the performance of VSD, done in an unsophisticated fashion, scales directly with scene complexity, which tends to increase as a square or cube function, so this very rapidly becomes the limiting factor in rendering realistic worlds.1 Abrash was writing about the difficulty of the VSD problem in the late ’90s, years after Doom had proved that regular people wanted to be able to play graphically intensive games on their home computers. In the early ’90s, when id Software first began publishing games, the games had to be programmed to run efficiently on computers not designed to run them, computers meant for word processing, spreadsheet applications, and little else. To make this work, especially for the few 3D games that id Software published before Doom, id Software had to be creative. In these games, the design of all the levels was constrained in such a way that the VSD problem was easier to solve. For example, in Wolfenstein 3D, the game id Software released just prior to Doom, every level is made from walls that are axis-aligned. In other words, in the Wolfenstein universe, you can have north-south walls or west-east walls, but nothing else. Walls can also only be placed at fixed intervals on a grid—all hallways are either one grid square wide, or two grid squares wide, etc., but never 2.5 grid squares wide. Though this meant that the id Software team could only design levels that all looked somewhat the same, it made Carmack’s job of writing a renderer for Wolfenstein much simpler. The Wolfenstein renderer solved the VSD problem by “marching” rays into the virtual world from the screen. Usually a renderer that uses rays is a “raycasting” renderer—these renderers are often slow, because solving the VSD problem in a raycaster involves finding the first intersection between a ray and something in your world, which in the general case requires lots of number crunching. But in Wolfenstein, because all the walls are aligned with the grid, the only location a ray can possibly intersect a wall is at the grid lines. So all the renderer needs to do is check each of those intersection points. If the renderer starts by checking the intersection point nearest to the player’s viewpoint, then checks the next nearest, and so on, and stops when it encounters the first wall, the VSD problem has been solved in an almost trivial way. A ray is just marched forward from each pixel until it hits something, which works because the marching is so cheap in terms of CPU cycles. And actually, since all walls are the same height, it is only necessary to march a single ray for every column of pixels. This rendering shortcut made Wolfenstein fast enough to run on underpowered home PCs in the era before dedicated graphics cards. But this approach would not work for Doom, since the id team had decided that their new game would feature novel things like diagonal walls, stairs, and ceilings of different heights. Ray marching was no longer viable, so Carmack wrote a different kind of renderer. Whereas the Wolfenstein renderer, with its ray for every column of pixels, is an “image-first” renderer, the Doom renderer is an “object-first” renderer. This means that rather than iterating through the pixels on screen and figuring out what color they should be, the Doom renderer iterates through the objects in a scene and projects each onto the screen in turn. In an object-first renderer, one easy way to solve the VSD problem is to use a z-buffer. Each time you project an object onto the screen, for each pixel you want to draw to, you do a check. If the part of the object you want to draw is closer to the player than what was already drawn to the pixel, then you can overwrite what is there. Otherwise you have to leave the pixel as is. This approach is simple, but a z-buffer requires a lot of memory, and the renderer may still expend a lot of CPU cycles projecting level geometry that is never going to be seen by the player. In the early 1990s, there was an additional drawback to the z-buffer approach: On IBM-compatible PCs, which used a video adapter system called VGA, writing to the output frame buffer was an expensive operation. So time spent drawing pixels that would only get overwritten later tanked the performance of your renderer. Since writing to the frame buffer was so expensive, the ideal renderer was one that started by drawing the objects closest to the player, then the objects just beyond those objects, and so on, until every pixel on screen had been written to. At that point the renderer would know to stop, saving all the time it might have spent considering far-away objects that the player cannot see. But ordering the objects in a scene this way, from closest to farthest, is tantamount to solving the VSD problem. Once again, the question is: What can be seen by the player? Initially, Carmack tried to solve this problem by relying on the layout of Doom’s levels. His renderer started by drawing the walls of the room currently occupied by the player, then flooded out into neighboring rooms to draw the walls in those rooms that could be seen from the current room. Provided that every room was convex, this solved the VSD issue. Rooms that were not convex could be split into convex “sectors.” You can see how this rendering technique might have looked if run at extra-slow speed in this video, where YouTuber Bisqwit demonstrates a renderer of his own that works according to the same general algorithm. This algorithm was successfully used in Duke Nukem 3D, released three years after Doom, when CPUs were more powerful. But, in 1993, running on the hardware then available, the Doom renderer that used this algorithm struggled with complicated levels—particularly when sectors were nested inside of each other, which was the only way to create something like a circular pit of stairs. A circular pit of stairs led to lots of repeated recursive descents into a sector that had already been drawn, strangling the game engine’s speed. Around the time that the id team realized that the Doom game engine might be too slow, id Software was asked to port Wolfenstein 3D to the Super Nintendo. The Super Nintendo was even less powerful than the IBM-compatible PCs of the day, and it turned out that the ray-marching Wolfenstein renderer, simple as it was, didn’t run fast enough on the Super Nintendo hardware. So Carmack began looking for a better algorithm. It was actually for the Super Nintendo port of Wolfenstein that Carmack first researched and implemented binary space partitioning. In Wolfenstein, this was relatively straightforward because all the walls were axis-aligned; in Doom, it would be more complex. But Carmack realized that BSP trees would solve Doom’s speed problems too. Binary Space Partitioning Binary space partitioning makes the VSD problem easier to solve by splitting a 3D scene into parts ahead of time. For now, you just need to grasp why splitting a scene is useful: If you draw a line (really a plane in 3D) across your scene, and you know which side of the line the player or camera viewpoint is on, then you also know that nothing on the other side of the line can obstruct something on the viewpoint’s side of the line. If you repeat this process many times, you end up with a 3D scene split into many sections, which wouldn’t be an improvement on the original scene except now you know more about how different parts of the scene can obstruct each other. The first people to write about dividing a 3D scene like this were researchers trying to establish for the US Air Force whether computer graphics were sufficiently advanced to use in flight simulators. They released their findings in a 1969 report called “Study for Applying Computer-Generated Images to Visual Simulation.” The report concluded that computer graphics could be used to train pilots, but also warned that the implementation would be complicated by the VSD problem: One of the most significant problems that must be faced in the real-time computation of images is the priority, or hidden-line, problem. In our everyday visual perception of our surroundings, it is a problem that nature solves with trivial ease; a point of an opaque object obscures all other points that lie along the same line of sight and are more distant. In the computer, the task is formidable. The computations required to resolve priority in the general case grow exponentially with the complexity of the environment, and soon they surpass the computing load associated with finding the perspective images of the objects.2 One solution these researchers mention, which according to them was earlier used in a project for NASA, is based on creating what I am going to call an “occlusion matrix.” The researchers point out that a plane dividing a scene in two can be used to resolve “any priority conflict” between objects on opposite sides of the plane. In general you might have to add these planes explicitly to your scene, but with certain kinds of geometry you can just rely on the faces of the objects you already have. They give the example in the figure below, where \\(p_1\\), \\(p_2\\), and \\(p_3\\) are the separating planes. If the camera viewpoint is on the forward or “true” side of one of these planes, then \\(p_i\\) evaluates to 1. The matrix shows the relationships between the three objects based on the three dividing planes and the location of the camera viewpoint—if object \\(a_i\\) obscures object \\(a_j\\), then entry \\(a_{ij}\\) in the matrix will be a 1. The researchers propose that this matrix could be implemented in hardware and re-evaluated every frame. Basically the matrix would act as a big switch or a kind of pre-built z-buffer. When drawing a given object, no video would be output for the parts of the object when a 1 exists in the object’s column and the corresponding row object is also being drawn. The major drawback with this matrix approach is that to represent a scene with \\(n\\) objects you need a matrix of size \\(n^2\\). So the researchers go on to explore whether it would be feasible to represent the occlusion matrix as a “priority list” instead, which would only be of size \\(n\\) and would establish an order in which objects should be drawn. They immediately note that for certain scenes like the one in the figure above no ordering can be made (since there is an occlusion cycle), so they spend a lot of time laying out the mathematical distinction between “proper” and “improper” scenes. Eventually they conclude that, at least for “proper” scenes—and it should be easy enough for a scene designer to avoid “improper” cases—a priority list could be generated. But they leave the list generation as an exercise for the reader. It seems the primary contribution of this 1969 study was to point out that it should be possible to use partitioning planes to order objects in a scene for rendering, at least in theory. It was not until 1980 that a paper, titled “On Visible Surface Generation by A Priori Tree Structures,” demonstrated a concrete algorithm to accomplish this. The 1980 paper, written by Henry Fuchs, Zvi Kedem, and Bruce Naylor, introduced the BSP tree. The authors say that their novel data structure is “an alternative solution to an approach first utilized a decade ago but due to a few difficulties, not widely exploited”—here referring to the approach taken in the 1969 Air Force study.3 A BSP tree, once constructed, can easily be used to provide a priority ordering for objects in the scene. Fuchs, Kedem, and Naylor give a pretty readable explanation of how a BSP tree works, but let me see if I can provide a less formal but more concise one. You begin by picking one polygon in your scene and making the plane in which the polygon lies your partitioning plane. That one polygon also ends up as the root node in your tree. The remaining polygons in your scene will be on one side or the other of your root partitioning plane. The polygons on the “forward” side or in the “forward” half-space of your plane end up in the left subtree of your root node, while the polygons on the “back” side or in the “back” half-space of your plane end up in the right subtree. You then repeat this process recursively, picking a polygon from your left and right subtrees to be the new partitioning planes for their respective half-spaces, which generates further half-spaces and further sub-trees. You stop when you run out of polygons. Say you want to render the geometry in your scene from back-to-front. (This is known as the “painter’s algorithm,” since it means that polygons further from the camera will get drawn over by polygons closer to the camera, producing a correct rendering.) To achieve this, all you have to do is an in-order traversal of the BSP tree, where the decision to render the left or right subtree of any node first is determined by whether the camera viewpoint is in either the forward or back half-space relative to the partitioning plane associated with the node. So at each node in the tree, you render all the polygons on the “far” side of the plane first, then the polygon in the partitioning plane, then all the polygons on the “near” side of the plane—”far” and “near” being relative to the camera viewpoint. This solves the VSD problem because, as we learned several paragraphs back, the polygons on the far side of the partitioning plane cannot obstruct anything on the near side. The following diagram shows the construction and traversal of a BSP tree representing a simple 2D scene. In 2D, the partitioning planes are instead partitioning lines, but the basic idea is the same in a more complicated 3D scene. Step One: The root partitioning line along wall D splits the remaining geometry into two sets. Step Two: The half-spaces on either side of D are split again. Wall C is the only wall in its half-space so no split is needed. Wall B forms the new partitioning line in its half-space. Wall A must be split into two walls since it crosses the partitioning line. A back-to-front ordering of the walls relative to the viewpoint in the top-right corner, useful for implementing the painter’s algorithm. This is just an in-order traversal of the tree. The really neat thing about a BSP tree, which Fuchs, Kedem, and Naylor stress several times, is that it only has to be constructed once. This is somewhat surprising, but the same BSP tree can be used to render a scene no matter where the camera viewpoint is. The BSP tree remains valid as long as the polygons in the scene don’t move. This is why the BSP tree is so useful for real-time rendering—all the hard work that goes into constructing the tree can be done beforehand rather than during rendering. One issue that Fuchs, Kedem, and Naylor say needs further exploration is the question of what makes a “good” BSP tree. The quality of your BSP tree will depend on which polygons you decide to use to establish your partitioning planes. I skipped over this earlier, but if you partition using a plane that intersects other polygons, then in order for the BSP algorithm to work, you have to split the intersected polygons in two, so that one part can go in one half-space and the other part in the other half-space. If this happens a lot, then building a BSP tree will dramatically increase the number of polygons in your scene. Bruce Naylor, one of the authors of the 1980 paper, would later write about this problem in his 1993 paper, “Constructing Good Partitioning Trees.” According to John Romero, one of Carmack’s fellow id Software co-founders, this paper was one of the papers that Carmack read when he was trying to implement BSP trees in Doom.4 BSP Trees in Doom Remember that, in his first draft of the Doom renderer, Carmack had been trying to establish a rendering order for level geometry by “flooding” the renderer out from the player’s current room into neighboring rooms. BSP trees were a better way to establish this ordering because they avoided the issue where the renderer found itself visiting the same room (or sector) multiple times, wasting CPU cycles. “Adding BSP trees to Doom” meant, in practice, adding a BSP tree generator to the Doom level editor. When a level in Doom was complete, a BSP tree was generated from the level geometry. According to Fabien Sanglard, the generation process could take as long as eight seconds for a single level and 11 minutes for all the levels in the original Doom.5 The generation process was lengthy in part because Carmack’s BSP generation algorithm tries to search for a “good” BSP tree using various heuristics. An eight-second delay would have been unforgivable at runtime, but it was not long to wait when done offline, especially considering the performance gains the BSP trees brought to the renderer. The generated BSP tree for a single level would have then ended up as part of the level data loaded into the game when it starts. Carmack put a spin on the BSP tree algorithm outlined in the 1980 paper, because once Doom is started and the BSP tree for the current level is read into memory, the renderer uses the BSP tree to draw objects front-to-back rather than back-to-front. In the 1980 paper, Fuchs, Kedem, and Naylor show how a BSP tree can be used to implement the back-to-front painter’s algorithm, but the painter’s algorithm involves a lot of over-drawing that would have been expensive on an IBM-compatible PC. So the Doom renderer instead starts with the geometry closer to the player, draws that first, then draws the geometry farther away. This reverse ordering is easy to achieve using a BSP tree, since you can just make the opposite traversal decision at each node in the tree. To ensure that the farther-away geometry is not drawn over the closer geometry, the Doom renderer uses a kind of implicit z-buffer that provides much of the benefit of a z-buffer with a much smaller memory footprint. There is one array that keeps track of occlusion in the horizontal dimension, and another two arrays that keep track of occlusion in the vertical dimension from the top and bottom of the screen. The Doom renderer can get away with not using an actual z-buffer because Doom is not technically a fully 3D game. The cheaper data structures work because certain things never appear in Doom: The horizontal occlusion array works because there are no sloping walls, and the vertical occlusion arrays work because no walls have, say, two windows, one above the other. The only other tricky issue left is how to incorporate Doom’s moving characters into the static level geometry drawn with the aid of the BSP tree. The enemies in Doom cannot be a part of the BSP tree because they move; the BSP tree only works for geometry that never moves. So the Doom renderer draws the static level geometry first, keeping track of the segments of the screen that were drawn to (with yet another memory-efficient data structure). It then draws the enemies in back-to-front order, clipping them against the segments of the screen that occlude them. This process is not as optimal as rendering using the BSP tree, but because there are usually fewer enemies visible than there is level geometry in a level, speed isn’t as much of an issue here. Using BSP trees in Doom was a major win. Obviously it is pretty neat that Carmack was able to figure out that BSP trees were the perfect solution to his problem. But was it a genius-level move? In his excellent book about the Doom game engine, Fabien Sanglard quotes John Romero saying that Bruce Naylor’s paper, “Constructing Good Partitioning Trees,” was mostly about using BSP trees to cull backfaces from 3D models.6 According to Romero, Carmack thought the algorithm could still be useful for Doom, so he went ahead and implemented it. This description is quite flattering to Carmack—it implies he saw that BSP trees could be useful for real-time video games when other people were still using the technique to render static scenes. There is a similarly flattering story in Masters of Doom: Kushner suggests that Carmack read Naylor’s paper and asked himself, “what if you could use a BSP to create not just one 3D image but an entire virtual world?”7 This framing ignores the history of the BSP tree. When those US Air Force researchers first realized that partitioning a scene might help speed up rendering, they were interested in speeding up real-time rendering, because they were, after all, trying to create a flight simulator. The flight simulator example comes up again in the 1980 BSP paper. Fuchs, Kedem, and Naylor talk about how a BSP tree would be useful in a flight simulator that pilots use to practice landing at the same airport over and over again. Since the airport geometry never changes, the BSP tree can be generated just once. Clearly what they have in mind is a real-time simulation. In the introduction to their paper, they even motivate their research by talking about how real-time graphics systems must be able to create an image in at least 1/30th of a second. So Carmack was not the first person to think of using BSP trees in a real-time graphics simulation. Of course, it’s one thing to anticipate that BSP trees might be used this way and another thing to actually do it. But even in the implementation Carmack may have had more guidance than is commonly assumed. The Wikipedia page about BSP trees, at least as of this writing, suggests that Carmack consulted a 1991 paper by Chen and Gordon as well as a 1990 textbook called Computer Graphics: Principles and Practice. Though no citation is provided for this claim, it is probably true. The 1991 Chen and Gordon paper outlines a front-to-back rendering approach using BSP trees that is basically the same approach taken by Doom, right down to what I’ve called the “implicit z-buffer” data structure that prevents farther polygons being drawn over nearer polygons. The textbook provides a great overview of BSP trees and some pseudocode both for building a tree and for displaying one. (I’ve been able to skim through the 1990 edition thanks to my wonderful university library.) Computer Graphics: Principles and Practice is a classic text in computer graphics, so Carmack might well have owned it. Still, Carmack found himself faced with a novel problem—”How can we make a first-person shooter run on a computer with a CPU that can’t even do floating-point operations?”—did his research, and proved that BSP trees are a useful data structure for real-time video games. I still think that is an impressive feat, even if the BSP tree had first been invented a decade prior and was pretty well theorized by the time Carmack read about it. Perhaps the accomplishment that we should really celebrate is the Doom game engine as a whole, which is a seriously nifty piece of work. I’ve mentioned it once already, but Fabien Sanglard’s book about the Doom game engine (Game Engine Black Book: DOOM) is an excellent overview of all the different clever components of the game engine and how they fit together. We shouldn’t forget that the VSD problem was just one of many problems that Carmack had to solve to make the Doom engine work. That he was able, on top of everything else, to read about and implement a complicated data structure unknown to most programmers speaks volumes about his technical expertise and his drive to perfect his craft. If you enjoyed this post, more like it come out every four weeks! Follow @TwoBitHistory on Twitter or subscribe to the RSS feed to make sure you know when a new post is out. Previously on TwoBitHistory… I've wanted to learn more about GNU Readline for a while, so I thought I'd turn that into a new blog post. Includes a few fun facts from an email exchange with Chet Ramey, who maintains Readline (and Bash):https://t.co/wnXeuyjgMx — TwoBitHistory (@TwoBitHistory) August 22, 2019 Michael Abrash, “Michael Abrash’s Graphics Programming Black Book,” James Gregory, accessed November 6, 2019, http://www.jagregory.com/abrash-black-book/#chapter-64-quakes-visible-surface-determination. ↩ R. Schumacher, B. Brand, M. Gilliland, W. Sharp, “Study for Applying Computer-Generated Images to Visual Simulation,” Air Force Human Resources Laboratory, December 1969, accessed on November 6, 2019, https://apps.dtic.mil/dtic/tr/fulltext/u2/700375.pdf. ↩ Henry Fuchs, Zvi Kedem, Bruce Naylor, “On Visible Surface Generation By A Priori Tree Structures,” ACM SIGGRAPH Computer Graphics, July 1980. ↩ Fabien Sanglard, Game Engine Black Book: DOOM (CreateSpace Independent Publishing Platform, 2018), 200. ↩ Sanglard, 206. ↩ Sanglard, 200. ↩ David Kushner, Masters of Doom (Random House Trade Paperbacks, 2004), 142. ↩",
    "commentLink": "https://news.ycombinator.com/item?id=40652917",
    "commentBody": "How much of a genius-level move was binary space partitioning in Doom? (2019) (twobithistory.org)189 points by davikr 19 hours agohidepastfavorite129 comments gumby 18 hours ago> So Carmack ... started reading research papers. This in itself is a superpower, especially in computer science where history is considered a waste of time. This is commonly even true in actual research computer science, where you'd think papers were crucial. Nowadays with the flood of \"PR\" preprints particularly in machine learning, it would be impossible to keep up and so nobody tries -- most of those papers write-once-read-never. When I moved out of computing into biochemistry and medicine, and now into climate, I found people take papers seriously, which is a pleasure. reply animal531 7 hours agoparentBack in the day access to knowledge was quite hard, everyone first started with random file dumps from BBS exchanges, then followed by books. After that if you were of the right age and lived in the right place you might have had access to a university library with papers. Talking of knowledge and books: It was around 1995 or so and I was in my last year of high school. We went on a small field trip to a local computer show and only about 4-5 students in the whole school were interested in going, at that time computer science was still a very new topic over here in the 3rd world. At the show I found a stall in a corner with some random hardware books and then this giant white tome about computer graphics. The size of it was intimidating and while scanning through it I saw some ridiculous mathematical formulas that I couldn't make head or tails of. But either way I had to have it and luckily I had gotten just enough money from my mom that I was able to buy it. That was how I lucked into Computer Graphics - Principles and Practice (2nd Edition) which had just come out. That book was a literal gold mine of concentrated topics for a kid, I spent more time studying that book in the next year than I had done for all my high school subjects combined. My deepest thanks to everyone who contributed to it, it's shown here on Fabien's page: https://fabiensanglard.net/Computer_Graphics_Principles_and_... Not to ramble, but a funny addition to the story. The only compilers/languages I could find locally were DOS Basic and then Pascal with Assembly which I learned most of my programming skills in. I had heard of this magical C language that was so fast and all the games were now being made in it, but for probably 3 years I couldn't find a compiler for it anywhere. Then randomly one day right around the Doom 1994 release date my uncle who had started working with hardware had been given a promotional Watcom C/C++ compiler which he gifted to me, and I could suddenly work with the same language and compiler that they had used! Of course it took me a bit of time to learn but by the time I got the CG Bible in 1995 I was set. reply cherryteastain 4 hours agorootparentWasn't GCC already out in 1995? (Though needing an internet connection to download it was an issue) reply vslira 17 hours agoparentprevOne of the first \"hacker\" breakthroughs in my first startup was due to following citations on a paper kind of related to my problem until I got to a paper from '61 implementing the exact algorithm I was needing at the time. Numerical approximations to a gnarly probabilistic problem that would make any engineer proud. Worked like a charm and, in a hardware thousands of times faster than it was designed to run, it was crazy efficient. reply sitkack 13 hours agorootparentOne of the nicest things about finding old papers is that often the \"unworkable\" technique now fits in L1 or L2 cache. reply fxtentacle 11 hours agorootparentOr that RAM is practically endless now. I still remember when I was reading an old pattern recognition paper and they dismissed one simple, elegant solution as \"obviously impossible\" because they estimated it would need 10485760 KB of RAM. We then bought a server with 128 GB of RAM and shipped their impossible approach in production :) where it's been running nicely for 9 years now. reply stuaxo 10 hours agorootparentNice, hope you let them know. reply noqc 2 hours agorootparentprevWorse computers require better programmers. reply richrichie 16 hours agorootparentprevCould you please share the paper reference? Thank you. reply fxtentacle 11 hours agoparentprevFully agree. There's so many people who succumb to \"not invented here\" and then run blindly into solved issues. I keep repeating the story, but in 2020, I surprisingly got #1 on the MPI Sintel benchmark for optical flow in the category \"Clean EPE matched\", by re-implementing a 15 year old paper. (And replacing their approximation of a quadratic solver with actually using a quadratic solver.) That said, I feel like it's not just machine learning which is flooded with useless preprints. It's the entire internet now. When I wanted to look up some context for quaternion math, it was surprisingly challenging to find something a bit more in-depth than motivated amateurs who try to sell their self-published ebook and/or Udemy course. No, \"np.quaternion\" is not a mathematical formula. In the end, it was easier to dig up my old paper book, which had a nice A4 overview page with all the relevant formulas. reply hi-v-rocknroll 11 hours agorootparentThe disconnect between laypeople and research is often compounded by the stilted language, undefined terminology and related knowledge assumptions, underspecified details, and incomplete presentation of code or examples for independent reproducibility. Research papers, to be maximally reusable and future-proof, should prefer to present information in as plain English as possible and commonly-used mathematic terminology and symbols rather than obscure forms where possible. Using big words (or Latin) doesn't add profoundness, but they can add artistic flair in other contexts where elucidation isn't the goal. ;-) reply azornathogron 6 hours agorootparentMy experience has been that the older CS papers (from the 60s, 70s, 80s) commonly have very clear, readable text, not weighed down with unnecessary \"academic style\" verbiage. It's the newer stuff that's painful to read. I don't know whether this is a general change in fashion, or because the early papers in a field are dealing with simpler ideas (at least ideas that are not reliant on a decade of prior papers and terminology), or because it's only the good old papers that still come up in literature reviews and all the badly written stuff has long since been forgotten. reply matthewdgreen 3 hours agorootparentAs a researcher I can say the opposite. In my field (cryptography) modern papers usually start by specifying formally exactly what the algorithm APIs look like; then they provide precise security definitions; then they describe algorithms; then they provide formal (if detailed) proofs of security. It’s long and detailed, but you get all the pieces you need for precision and you know where to find it. Papers in the same conferences from 1982 are full of beautiful insights and they’re easy to read. But often that’s because they’re a five-page long story that might provide only a tiny subset of what we expect nowadays. They often don’t specify exactly what the algorithms do, they just kind of describe the ideas in English. Formal definitions are like three sentences of handwave. Proofs are similarly short or missing. Typically if you follow the citation chain you’ll find a dozen papers saying “we show that the original construction by Seminal Author is totally broken in this situation, but we show how to fix that with a small tweak and a better security definition.” Eventually you get to modern papers that actually start with the right definitions to deal with all that stuff, but take longer to read. Obviously to the layperson the short old papers are fun and full of insight! To an expert they’re still full of insight, but we wouldn’t let them out of our lab in that condition. It’s very much like comparing a beautiful old classic car to a modern car with safety features. They both look great: but you don’t want to get into even a minor fender bender in the old car. reply fxtentacle 5 hours agorootparentprevThe constant fighting between Schmidhuber and LeCunn should be well known in here. But in my opinion, both of them have been involved with fantastic papers that are both painless to read and full of useful info: CTC loss: https://www.cs.toronto.edu/~graves/icml_2006.pdf DeepLearning: https://hal.science/hal-04206682/document And both of these papers are new and on more recent research, so it's not just old papers that are good. It's just that there is a flood of bad new papers drowning out a constant signal with additional noise. reply ragnese 3 hours agorootparentprev> Research papers, to be maximally reusable and future-proof, should prefer to present information in as plain English as possible and commonly-used mathematic terminology and symbols rather than obscure forms where possible. Using big words (or Latin) doesn't add profoundness, but they can add artistic flair in other contexts where elucidation isn't the goal. ;-) As someone with a background in academia, I have to disagree with the assertion that papers should be written in \"plain English\" and some lowest-common-denominator of symbolism. It doesn't make me happy to argue against that, but the truth is that scientific papers MUST build on previous knowledge. No leading gene editing research paper is coming out with long-winded prose explaining what DNA and RNA are. No particle physics paper coming out of CERN is going to avoid quantum field theory jargon and terminology in the hopes that a lay person with a pre-calculus level of math education is going to be able to study the results and understand it. How many times longer would these papers be if they did try to do that? My calculus book from undergrad was some 600 pages long (it covered three semesters, to be fair)--now imagine that the people writing these papers read that 600 page book in their first one or two years of undergrad, and then proceeded to study their subject for AT LEAST another 6 years (closer to 8 for physics) before getting \"Dr\" in front of their name. How many pages would a research paper need to be for a lay person to understand? Probably pretty long. The truth is that there sometimes IS NOT a \"plain English\" way to explain something. Yes, you can use inaccurate metaphors and analogies about cats in boxes, but that doesn't lead to someone actually understanding the Schrodinger equation and Hilbert spaces. Believe it or not, scientists are not sitting around all day figuring out which Latin words will sound the most \"profound\" in their prose. Those are just the words they really actually use. Because words have meaning and some ideas are so common that they deserve to be condensed into a word, or an equation, or a symbol... At the end of the day, these papers are written to advance the respective field of study. The best way to do that is to write papers that are going to be relevant to people who might actually build on the results. The people who might actually build on the results are going to be people who understand the requisite background information. If Joe Schmoe wants to read a particle physics paper, then he better pull out his high school math books and work his way up to at least a masters degree level understanding of physics and mathematics. I would never go into the kitchen of a professional chef and tell her to stop referring to a \"paring knife\" or a \"fillet knife\" because I'm not familiar with cooking jargon. I wouldn't tell her that she has to re-explain what shape those knives have every time she explains a new recipe or technique. I would just have to learn the jargon if I actually cared enough. reply wheybags 9 hours agorootparentprevAnd source code, for the love of god, give me source code so I don't have to guess what your underspecified description in the paper actually means. reply Bluestein 9 hours agorootparentprevWould using standard Basic English (https://en.wikipedia.org/wiki/Basic_English) help? reply randomcarbloke 9 hours agorootparentprev>in as plain English as possible This is exactly what they do, even domain specific language is selected to precisely convey meaning and differentiate from incorrect but close interpretations. reply ungamedplayer 6 hours agorootparentWe must be reading very different papers. reply glonq 1 hour agorootparentprevif it weren't for NIH, i'd bet that a majority of HN readers would be unemployed right now! reply HillRat 16 hours agoparentprevWhat's really remarkable is that so much of the last decade or so of research has been grounded in work done in the 1970s and 1980s, when computational boundaries were narrow enough that the work was often largely speculative or significantly restricted in application, so it's only now that we're really seeing that work bear fruit. It's a pleasure reading old comp-sci papers, and I wish more developers took the time. reply gfourfour 16 hours agorootparent“Solving for colour constancy using a constrained dichromatic reflection model” is a good one to check out reply nahumfarchi 11 hours agorootparentprevI would love to see a list of \"oldies but goldies\" in various fields. Wonder if you're aware of such a thing? reply hiAndrewQuinn 9 hours agorootparenthttps://people.math.harvard.edu/~ctm/home/text/others/shanno... would be a natural starting point. reply Bluestein 9 hours agorootparentprevThere's a nifty use for AI: Read, grok, simulate, report, repeat ... reply Bluestein 5 hours agorootparentSuch a bad idea, eh? reply JKCalhoun 15 hours agoparentprev> So Carmack ... started reading research papers. I feel it was more common in the earlier days of programming. With no open source \"game engines\" and the Githubbed, Stackoverflown internet we have now fetching about for some of the \"programming secrets\" whether that was in a university library or a technical computer magazine was about all you had. reply jayd16 16 hours agoparentprevSeriously. Ever try to pass an interview with \"Well first I'd probably look up the optimal answer...\"? It's worked for me about zero times. reply lesuorac 15 hours agorootparentYeah, works about as well as \"well, I'd see how the rest of your codebase does it\". reply hiAndrewQuinn 10 hours agoparentprevWriting code is more fun than reading. Trying to solve problems is more fun than researching previous problems. It takes a special kind of discipline to get to the point where your first instinct is \"Let me book out a few days and survey the literature\" instead of \"Let me use all of my IQ points to try to implement an ad hoc, informally-specified, bug-ridden, slow implementation of half of $ANYTHING\". reply cladopa 9 hours agorootparentI never had this impression. For me reading papers is way more fun and easier than developing a new solution in code and painstakingly work over all the bugs until they are gone, or worse, find bugs that you don't have solutions for. Or finding that it is too slow for your target machines. Creating something new is so much work. Like skying it could be fun when you are not a professional and you do it on your terms, but once you are a professional it is something different. reply randomtoast 11 hours agoparentprev> most of those papers write-once-read-never Does someone know a good way how to put a filter on these \"write-once-read-never\" papers and just get the top 10 papers each month? reply bsder 17 hours agoparentprev> When I moved out of computing into biochemistry and medicine, and now into climate, I found people take papers seriously, which is a pleasure. SIGGRAPH had been around since 1974 and was well-known if you were doing anything graphics related--especially 3D. You couldn't help but trip over the algorithms you needed. The genius was putting any of these algorithms meant for super-expensive 3D graphics hardware on a wimpy-ass 1990 PC. reply JKCalhoun 15 hours agorootparentYou're reminding of the \"Numerical Recipes\" series of books from that era. I feel like there were a series related to computer graphics as well.... reply jacobolus 14 hours agorootparentNumerical Recipes was just several editions of one book, each with a few variants (for different programming languages). But you might be thinking of Graphics Gems (vols 1–5). https://www.realtimerendering.com/resources/GraphicsGems/ reply AlotOfReading 13 hours agorootparentAbrash's Black Book was more in the style of Numerical Recipes than graphics gems, from what I recall. I also remember Nvidia putting out something annually around the late 2000s that was pretty good, can't remember the name though. reply h0l0cube 15 hours agorootparentprevExactly. Research and it’s practical application is pretty common in computer graphics, and particularly cutting edge graphics engine development. There’s no need to dig around in academic papers unless you’re dealing with deep technical problems and off-the-shelf solutions are out of the question. reply claudiulodro 3 hours agoparentprevI blame agile and SaaS, neither of which are super tolerant of developers taking a few days just to read up on things. When you're both an owner of the company and/or just need to ship one good version of something, you get a little more space to do it right. reply api 16 hours agoparentprevOne of the things that drives me nuts about this industry is how ahistorical it is. The end result is that we keep reinventing the same things over and over especially around operating systems, databases, and languages. reply nomel 16 hours agorootparentAt least in open source, I would claim that it's a result of volunteers doing the work for fun, which necessarily means they follow the dopamine: implementing someone else's work is not nearly as fun as figuring out a problem for yourself. reply karussell 9 hours agorootparentThis has nothing to do with open source. For GraphHopper I researched many papers regarding fast routing algorithms or advanced data structures etc ... and I'm pretty sure that this is true for other open source projects too. It could be even the opposite: because many people can look into the code the code converges to the state of the art. reply sitkack 13 hours agorootparentprevAnd the reading comprehension of CS people is near zero. You can give someone a paper with the solution, they will skim, not understand anything and then going back to reimplementing something poorly from first principles. reply fisf 10 hours agorootparent> reimplementing something poorly from first principles Not-invented-here syndrome in practice. Which kinda works for the n-th shitty web framework. But it goes really poorly in any complex domain. reply hackeraccount 7 hours agorootparentprevThe industry is fundamentally conservative and un-imaginative. That being the case the M.O. is always a slightly tiny spin on an existing thing or a recreation of something that has been around forever but is currently slightly less popular then it once was. reply Obscurity4340 1 hour agoparentprevWriteOnly reply parasti 9 hours agoparentprevNobody mentioned this but software patents are/were a pretty good reason to avoid looking at other people's algorithms. reply lostemptations5 1 hour agorootparentIn...the countries where that was a problem. reply wenc 16 hours agoparentprevAnd this is the power of ChatGPT. Instead of reading a ton of literature, you can ask it for algorithms in literature that map to your problem. I work in a space where I simply don't have bandwidth to read every paper out there, but a few question on ChatGPT managed to narrow the field for me in 30 seconds, and I could take it from there. It could even prototype the algorithm tailored to my specs. Game changer. reply purple-leafy 12 hours agorootparentAnd it will give you a crap answer 95% of the time. Good luck using chatjippity when you have any level of complexity. I’m working on a React UI based SQL query builder, where the entire data structure is recursive. ChatGPT is literally unusable for this reply wenc 12 hours agorootparentThat's a very biased point of view. I use it for mathematical formulations for optimization problems and it gives me very good answers most of the time. The literature is very strong but hard to search via keywords. ChatGPT manages to digest the literature and does a good job pattern matching to the right problem. (I also use the paid version). reply huygens6363 12 hours agorootparentprev> UI based SQL query builder Well, there’s your problem. reply xcv123 12 hours agorootparentprevWe're talking about finding references or details of existing algorithms to solve a specific problem. What you are trying to do has nothing to do with that and it's a waste of time with an LLM. reply yakshaving_jgt 7 hours agorootparentprev> And it will give you a crap answer 95% of the time You're underselling it. Not only will it give you a crap answer, but it'll give it to you with unerring confidence! reply Nition 17 hours agoprevFor Crash Bandicoot (1996), Naughty Dog sidestepped the entire VSD performance problem. Since the camera is on rails and only moves forward and back, they pre-calculated the visibility for every position and stored it in the game's data. That meant no visibility calculation needed in real time at all, and they could do it per-pixel. As a result of that and some other brilliant tricks, they were able achieve probably the best graphics on Playstation. reply mh- 17 hours agoparentThat's a very cool example of doing the \"inelegant\" thing and it just being objectively better. Am always on the lookout for these anecdotes, thanks! reply Nition 17 hours agorootparentIn the same game, for character animations, instead of just using bones like others were doing - often resulting in some big distortions with low-poly characters - they set up per-vertex animation, letting the artists manually position any vertex every frame. Then wrote a custom compression algorithm in assembly to store it. reply knome 16 hours agorootparentNot to mention actively streaming levels from the disk while you were playing them. The whole thing is a great story. https://all-things-andy-gavin.com/2011/02/02/making-crash-ba... reply Nition 16 hours agorootparentYes! And the best part of that feature: > Andy had given Kelly a rough idea of how we were getting so much detail through the system: spooling. Kelly asked Andy if he understood correctly that any move forward or backward in a level entailed loading in new data, a CD “hit.” Andy proudly stated that indeed it did. Kelly asked how many of these CD hits Andy thought a gamer that finished Crash would have. Andy did some thinking and off the top of his head said “Roughly 120,000.” Kelly became very silent for a moment and then quietly mumbled “the PlayStation CD drive is ‘rated’ for 70,000.” > Kelly thought some more and said “let’s not mention that to anyone” and went back to get Sony on board with Crash. reply alt0_ 7 hours agorootparentWhy did it work, though? Wouldn't the disks die halfway through the game? Did they end up cutting / optimizing the feature? reply juliangoldsmith 4 hours agorootparentThe rating was the minimum number of reads the motors in the CD drive of the Playstation would work for. Most likely, the rating was really low compared to what the drives could actually do. reply bongodongobob 13 hours agorootparentprevI don't understand what that means. 70k per what? 120k per what? What is a \"hit\"? A read operation? reply kalleboo 12 hours agorootparentLooking at the data sheet for a contemporary Sony CD-ROM mechanism ◆ Service life of feed motor The current consumption of feed motor must be less than initial value plus 30% after 50,000 cycles. (1cycle : innermost track → outermost track → innermost track) ◆ Service life of limit switch The contact resistance must be less than 100mΩ after 50,000 cycles. (1cycle : innermost track → outermost track → innermost track) ◆ Pick-up slide operation The pick-up should operate perfectly after 50,000 cycles. (1cycle : innermost track → outermost track → innermost track) reply Alphaeus 12 hours agorootparentprevI would interpret it as completing the game requires 120,000 disk read operations, but the CD drive may fail after just 70,000 read operations. reply Nition 12 hours agorootparentprev70k per lifetime of the CD drive. 120k per complete playthrough of the game. A hit is reading from the CD. reply paulryanrogers 15 hours agorootparentprevQuake 1 and 2 did this as well. I wasted so much time with details that later got botched by the vertex compression. I think Quake 3 had vertex animation too, but they split the character at the hips so it could face away from the direction of travel. reply trilinearnz 13 hours agorootparentThe hip swivel worked well, functionally, but I always lamented how stiff the character animation looked because of it. Was the vertex compression responsible for the wobbling/swimming effect you'd see on models, when running Q1 and Q2 in OpenGL mode? reply paulryanrogers 6 hours agorootparentYes reply fulafel 15 hours agoparentprevThere must have been some cleverness in representing this data in a compact way, esp as it would have competed with the 150 kB/sec CD-ROM bandwidth streaming other asset data? reply Nition 14 hours agorootparentWeirdly the excellent devblog that I got all this info from[1] mentions crazy compression algorithms for the vertex animation and for the level files, but not specifically for the occlusion data. [1] https://all-things-andy-gavin.com/2011/02/02/making-crash-ba... Especially part 3. reply rasz 9 hours agorootparentI think occlusion was just per vertex. reply 7bit 13 hours agorootparentprevThere's a video series on YouTube where game devs talk about the challenges they faced while driving the game. Crash Bandicoot is covered in one episode, and there is another fantastic episode about Dead Space with the tentacle scene. Much recommended view. reply cordenr 11 hours agorootparentThere's quite a few videos.... Can you link to the specific one your thinking of? Cheers. reply a_e_k 17 hours agoprevIIRC, both the Jedi engine (1995, Dark Forces) [1] and the Build engine (1995, Duke Nukem 3D) [2] which were of the same era used \"sectors\" rather than BSP trees for resolving visibility. Basically, it would traverse a graph as rays exited one sector and entered a neighboring sector. The approach is very much akin to walks through tet meshes in modern science apps. In ray tracing, there are three commonly used spatial acceleration structures these days: grids, binary space partitions, and bounding volume hierarchies. Any one of those might have worked for Doom. Though this post mentions marching through grids (as in Wolfenstein 3D) as not working for the arbitrarily positioned and angled walls, the usual way of using grids for ray tracing is to have each grid cell hold a list of the primitives that overlap it (many-to-many). Then you march the ray through the grid and test the ray against the list of primitives for each cell. You'd often use \"mailboxing\" (i.e., tiny LRU) to prevent retesting the same primitives over and over, since adjacent grid cells would often share primitives. So basically extending the Wolfenstein 3D grid approach to Doom would have mean voxelizing the map to get lists of candidate walls for each cell, but testing the rays against the walls themselves (rather than say, the voxels themselves like in Minecraft). I'm not sure what the tradeoffs for memory or performance on the 386s of the time would have been, however. But I think it's clear that while BSPs were a clever solution, they weren't the only possible choice. [1] https://en.wikipedia.org/wiki/Jedi_(game_engine) [2] https://en.wikipedia.org/wiki/Build_(game_engine) reply iforgotpassword 12 hours agoparentI was looking for a post mentioning the build engine. Obviously it came a little later and has higher hardware requirements, but I wonder if this is really related to the sector approach, or whether it's mostly other parts of the engine. It seems much more elegant as it directly uses the data structures that represent the level layout, thus not requiring a preprocessing step, and intuitively I don't see how it's more expensive than walking a tree for every frame. Did anyone do a more thorough investigation into this? I know Doom's engine has been analyzed to death regarding every little detail, but build/duke3d didn't get nearly as much attention. reply vrighter 6 hours agorootparenthttps://www.fabiensanglard.net/duke3d/build_engine_internals... reply jiggawatts 13 hours agoparentprevUnreal engine also uses \"portals\" between rooms, where each room is either entirely visible or entirely invisible. The two approaches have different pros and cons: BSP is theoretically elegant, but requires a fairly expensive \"tree walk\" step for each frame. This is random in memory and not efficient for modern CPUs because the branch predictor is ineffective and the memory layout is typically not contiguous. It is even worse on GPUs. Doom was written in 1993 when this was much less of a concern, but with Quake and later engines it become a scalability bottleneck. BSP maps require a very compute-intensive pre-processing step. Back then this limited modding and third-party maps because a very high-end PC was needed. This processing also had numerical precision issues. Theoretically this isn't a problem, but in practice it is. Designers had to be careful to make sure the map is completely closed (\"airtight\") and that the BSP process didn't create thin triangles. This meant that map design tools had to have all sorts of constraints on alignment and snapping to prevent the geometry \"blowing up\". A gotcha is that BSP splits polygons and hence the processing eats into the the map geometry polygon budget! Portals are much simpler to implement, and each room can scale to any number of triangles. They're much more efficient with modern CPUs and GPUs, because each room can be sent as a single draw call (\"batch\" processing) instead of a list of individual triangles. The problem with portal-based techniques is that there are sudden step-changes in the number of triangles needed to draw each frame as the camera moves. This can result in stuttering and sudden framerate drops. It can also blow through triangle budgets. A common technique is to limit portal traversal depth in some way, which would cause \"black\" doorways in the distance and other similar artefacts. Similarly, certain map geometries were poorly supported, such as complex outdoor spaces with visibility from many directions into many rooms. reply iforgotpassword 12 hours agorootparentI remember building Unreal maps as a teen. I didn't really understand bsp back then, and online discussions regarding mapping where full of cargo-culting weird methods to avoid \"bsp holes\". They were basically holes in the level geometry that sometimes even only appeared at specific camera angles, and not just in the distance, sometimes right in the corner of the room you're standing in. There were also two sliders in the dialog where you rebuilt the bsp, but I forgot what they were called and at least back then didn't understand what they did anyways, but changing them slightly could resolve the issue - only so you'd realize there's now a new hole somewhere else after 5 minutes. reply tadfisher 13 hours agorootparentprev> Doom was written in 1993 when this was much less of a concern, but with Quake and later engines it become a scalability bottleneck. Worth clarifying that the 386 and 486 were not pipelined, so they didn't feature branch prediction. The Pentium was, but it had released during Doom\"s development and essentially no home user owned one. Quake made extensive use of the Pentium's faster FPU, so they didn't optimize for earlier CPUs. reply jiggawatts 10 hours agorootparentAs an aside, this is also why people keep saying that functional languages are \"not much slower\" than traditional procedural languages against all evidence. People that learned and like LISP probably did so in the 1980s or 1990s when CPUs were much simpler, memory access times were constant, and \"cache lines\" weren't even a thing yet! Back then, randomly jumping around memory following a linked list was only about 2x slower than reading an array. Linked lists were much faster for some incremental operations, so the net result was that the two were very comparable. These days arrays can be processed using AVX-512 instructions at a rate of 64 bytes per clock or even higher, whereas even one missed cache entry of a linked-list pointer will stall the CPU for several hundred cycles, during which time the array logic could have chunked through kilobytes. reply lispm 3 hours agorootparent> memory access times were constant My Symbolics 3640 Lisp Machine, a model introduced in the mid 1980s had 20 MB RAM and 200 MB virtual memory on one or two disks. The OS was already a 50 MB image. The memory used by Lisp was typically much larger than the available (and/or affordable) RAM. Access times were far from constant. A full Garbage Collection over virtual memory took 30 minutes. That's why it had extensive support for incremental garbage collection, copying GC, manual memory management, areas for various data types, ephemeral GC with hardware support, cdr-coded lists, various types of multidimensional arrays, graphics cards with special video memory, 36 bit memory with additional ECC, ... Before saving a new Lisp image to disk, one was using a special command to reorder objects in memory to optimize access times. That command also took 20-30 minutes. Lisp Machines like this were developed because Lisp systems on Mainframes or Mini Computers were thrashing the machines during GC and making performance hell for Lisp users and other time sharing users. Thus a personal machine was needed where the memory and CPU belong to one user. They were among the very first commercially available GUI based personal workstations in 1981. reply weinzierl 13 hours agoprevFrom my memory: In the 90, if you had anything to do with computer graphics[1] you knew about binary space partitioning trees. Off the top of my head I remember two different articles from local German computer magazines (that are still in my basement) discussing and implementing them. But so were many, many other ideas and approaches. Not taking away from Carmack's genius, in my opinion it was not in inventing a horse and neither in finding one, but in betting on the right one. [1] If you were a programmer in the 90s - any kind of programmer - you most certainly had. reply torusle 11 hours agoparentCorrect. And every graphic programmer worth it's salt had a copy of \"Computer Graphics - Principles and Practice\" on the desk and followed whatever came out of the \"Graphics Gems\" series. We knew about BSP, Octrees and all that stuff. It was common knowledge. reply layer8 6 hours agoparentprevYep, that’s my recollection as well that it was common knowledge to the point of being a (the?) default approach for 3D rendering of any complexity. reply wheels 18 hours agoprev> In the early ’90s, when id Software first began publishing games, the games had to be programmed to run efficiently on computers not designed to run them, computers meant for word processing, spreadsheet applications, and little else. That isn't really true. Games were there pretty much for all of the PC era. I was using a computer at the time Doom came out, and there were a bajillion (obviously less graphically sophisticated) games in common circulation, and gaming was a significant and widely accepted part of home computer sales. reply Maxatar 17 hours agoparentCan't say I agree. I also remember the late 80s and early 90s and PCs were simply not well equipped to run video games. Home computers that did run video games had special hardware dedicated for it, like the Commodore 64, the MSX, and the ZX Spectrum, but PCs weren't geared for games. In fact, another one of Carmack's contributions to PC gaming was porting Super Mario 3 to the PC and demoing it to Nintendo as a proof of concept for smooth pixel sized scrolling, which was something that PCs were very poor at. His approach was to basically do a graphical diff frame by frame and only render the parts of the screen that changed, whereas all the other home computers I mentioned and even gaming consoles like the NES and Sega Master System had dedicated hardware for scrolling sprites. Nintendo did not go forward with Carmack's proposal, wanting to keep their games exclusive to their own platform, but Carmack and id Software did manage to use that technology to develop Commander Keen. Prior to Commander Keen there were maybe 3-4 PC games that had any kind of pixel scrolling whatsoever and it was fairly limited. reply wheels 16 hours agorootparentI'm not saying PCs were especially good for games, but that that was one of the main things they were bought and used for, at least at home. Again, we're talking 90s, not 80s. By the 1993, Windows had already won, and a home computer was almost always going to be an x86 running Windows or DOS. Saying they weren't meant for gaming is paramount to saying computer games weren't a thing in that period. We're way past Commodore being relevant by then. That was the era of Lemmings, Prince of Persia, Dune, Sim City, Wolfenstein 3D, Leisuresuit Larry -- plus a bajillion other smaller games. It wasn't the best gaming platform, but it's bonkers to say that computer games weren't really a thing then, or that the designers of computers were blind to that. We're at that point at the era where \"multimedia\" was a buzz word, and I remember getting CD-ROMS with edutainment stuff that had real video in it -- that was a big deal around then. reply hackeraccount 6 hours agorootparent\"multimedia\". Just hearing the word makes me the smell that era. reply Maxatar 16 hours agorootparentprevI think you're now shifting from your original claim that games were always a part of PC sales, which I disagree with, and a cursory review of market share supports my stance, to now focusing on 1993 and beyond. To that end I think the article is fair to point out that DOOM was programmed to run on computers that did not have the kind of hardware support for gaming that many other home computer systems had, and because of that techniques had to be carefully engineered to get such games to run on them. Consider that many of the games you mentioned such as Lemmings, Prince of Persia, and Sim City were not even released on the PC originally and were ported to the PC often times years after their initial release. In 1990 PCs including clones made up only 15% of market share, whereas by 1994 they made up over half of the home computer market share. The 90s were a wild time and a lot of things changed over a small period of time. reply wheels 15 hours agorootparentI think you read more into my initial point than I was trying to say -- there are two things: - PCs were always used for games (softening that: at least by the 286 era that was already true) - By 1993 PCs were also designed with games in mind But it doesn't follow that PCs were always designed with gaming in mind. They weren't, but since they were always used for that, even if they weren't designed for them, after a decade of such usage, it became a design concern. reply Maxatar 3 hours agorootparentI suppose I assumed you were objecting to the article saying that PCs were not designed to run games. No one is disputing that games did run on PCs, but PCs did not have the kind of hardware nor the kind of architecture that computers at that time had which were designed for games. If your argument is simply that PCs had games, then sure, and the article isn't disputing that. The article is explicitly saying that games were on PCs, and the way you got them to run on PCs was by making use of careful software techniques to compensate for the lack of hardware support. If you still object to this, then by all means let us know what features did PCs have that supported gaming. I can tell you numerous features that other home computer systems had to explicitly support video games, but I can't name any that the PC had prior to around the mid 90s. Maybe the Sound Blaster? reply nottorp 12 hours agorootparentprev> PCs were always used for games (softening that: at least by the 286 era that was already true) You had graphical games available on a PC XT with Hercules monochrome, people. Mind, not all of them but you had them. I finished Prince of Persia on that. Of course, games exploded when the VGA and Sound Blaster were out. But not having 256 colours or sound didn't stop people. reply paulryanrogers 15 hours agorootparentprevThe turning point I recall being the \"multimedia\" era. Sounds cards, video cards, and CD drives took things to another level and leap frogged many consoles. reply pezezin 11 hours agorootparentI think it is difficult to pinpoint the turning point, but I would say it was the launch of the 3dfx Voodoo in late 1996, and the release of GLQuake the following year. Up until that point you could argue that consoles were more powerful or adequate for gaming, but after that point there was no discussion. reply layer8 6 hours agorootparentIt was already with games like Comanche (voxel graphics) and Wolfenstein 3D in 1992, which weren’t possible on the home computers of the time. Basically, it was with the advent of relatively affordable 486 PCs with VGA cards. Next came the CD-ROM based games like Rebel Assault in 1993. At that point it was literally “game over” for home computers. reply Foobar8568 13 hours agorootparentprevI would estimate my parents to have bought a 486dx back in 91/92 in Europe, I remember we had https://www.computinghistory.org.uk/det/44379/Sound-Blaster-... with Indianapolis 500 and lemmings. And I remember to have played wolf3d at a neighbor before we had that computer. Atari/Amiga were better but I'd say it was indeed the turning point. So yeah your comment is spot on. reply jajko 12 hours agorootparentprevMy parents bought PC in 1993. 95% of the usage by me were games. 100% of the reasons for upgrades for next decade and a half were only due to games. Doom made folks want PC, it just didnt work that well on consoles. Then multimedia and windows 95 in PCs steamrolled everything else, even with high prices and famous MS instability. If you needed all-in-1 solution, PC was the way to go. reply Maxatar 3 hours agorootparentAbsolutely if you bought a PC in 1993 it would have been suitable for gaming, but that was not the claim I replied to which claimed that PCs had always been suitable for gaming and that games were always a significant part of the PC market. If you bought a PC in the 80s, you didn't buy it for gaming and PCs did not have hardware support for gaming like other home computer systems did. Finally, and I would have thought a website like this would be aware of this fact, you don't develop video games to run on the latest hardware. DOOMs official system requirements was a 386 with 4 MB of RAM. reply justsomehnguy 3 hours agorootparentprevAhem, MPC-1 was defined in 1991 reply VikingCoder 17 hours agoparentprevThose computers were designed by International Business Machines corporation. The fact that they could run games was great for a lot of us. But they weren't designed to do it. reply wheels 17 hours agorootparentThat's not really true. The era of IBM designing computers was like the 8086 era. Clones were already dominant by the time the 286 came out, and Doom came out about half a year after the Pentium, or 586. That was very far into the era of \"IBM compatible\" machines being gaming machines. There's more than a decade gap between \"IBM compatible\" clones taking over, and Doom coming out. reply adrian_b 15 hours agorootparentBut even the clones, until the mid nineties, had at most a SuperVGA video card, which was little better than the annoying IBM VGA video card, which was certainly not designed for games. Also, the clones had nothing better than the PC speaker for audio, even if that could be used very creatively by games like Monkey Island. The PC speaker was most definitely not designed for games. The SoundBlaster cards can be said to have been designed for games, but they were a separate product that could be bought as an add-on for IBM PCs or clones. Only during the second half of the nineties, after Windows 95, IBM PC clones with video cards and audio cards really suitable for games have become common. reply wheels 15 hours agorootparentThe computer my family bought about 1-2 years before Doom came out (and which I eventually played Doom on) had Soundblaster, stereo speakers, SVGA, a CD-ROM drive. I feel like folks are mixing up eras a bit here. reply galkk 17 hours agorootparentprevI wanted to correct you but after fact checking I feel like Nathan Fillion from the meme. Doom and pentium come out at the same time. I guess my xUSSR got the latter much later, as I remember struggling on my 386 reply p_l 11 hours agorootparentPentium arrived at the same time as Doom, but wasn't something you could reliably target as your minimal requirements until Quake, and even then it was controversial a bit. reply MBCook 17 hours agoparentprevI don’t think the author meant that computers weren’t designed for games, I think they specifically were referring to 3D games. Wolfenstein only worked, as the article pointed out, by significantly constraining the problem. While Doom wasn’t as unconstrained as Quake it enabled a significant amount of complexity compared to Wolfenstein. reply m463 17 hours agoparentprevit's simple: Sprites! even the most hardware-deficient gaming systems had them. The PC didn't. Systems with hardware sprites include arcade video games of the 1970s and 1980s; game consoles including as the Atari VCS (1977), ColecoVision (1982), Famicom (1983), Genesis/Mega Drive (1988); and home computers such as the TI-99/4 (1979), Atari 8-bit computers (1979), Commodore 64 (1982), MSX (1983), Amiga (1985), and X68000 (1987). Hardware varies in the number of sprites supported, the size and colors of each sprite, and special effects such as scaling or reporting pixel-precise overlap. https://en.wikipedia.org/wiki/Sprite_(computer_graphics) the pc had: VGA https://en.wikipedia.org/wiki/Video_Graphics_Array reply pezezin 10 hours agorootparentNot only sprites, but also scrolling backgrounds! There is no question that those machines were much more suited for 2D games, at least until the 486 could brute force them. On the other hand, the simple framebuffer of the VGA was much more flexible, and was the better approach for those early 3D games. Mode 13h for the win! reply MegaDeKay 15 hours agorootparentprev> even the most hardware-deficient gaming systems had them Not quite! The Apple II didn't. Neither did the IIgs nor the Mac. reply m463 10 hours agorootparentI wouldn't call them gaming systems. still don't :) reply woofcat 17 hours agoparentprevIt is true, games were present but it wasn't in the objectives of the hardware manufacturers to have \"better game support\" at this time. reply keeptrying 14 hours agoprevI implemented a BSP based renderer too in around 1998- 2000. Probably from that same paper. It was/is such a cool fun little algorithm to implement. We were building virtual reality worlds 24 years ago and people are still not using it. reply Foobar8568 13 hours agoparentEverQuest was from 99, and first alpha 97 or 98. Supposedly among the first pre alpha game rendering https://m.youtube.com/watch?v=U0GyQCYVb2s EverQuest was truly a crazy game considering the era. reply daft_pink 18 hours agoprevThey also created wolfenstein, which I feel was more incredible given the limited hardware it ran on. reply tejohnso 18 hours agoparentAnd in fact, from TFA, It was actually for the Super Nintendo port of Wolfenstein that Carmack first researched and implemented binary space partitioning. In Wolfenstein, this was relatively straightforward because all the walls were axis-aligned; in Doom, it would be more complex. But Carmack realized that BSP trees would solve Doom’s speed problems too. reply anonzzzies 15 hours agorootparentEven more amazing that the company they hired to do the Nintendo port copped out so they had to stall doom dev to do the Nintendo port and because the hardware was not good enough they had to do bsp to make it work at all… reply DennisL123 14 hours agoparentprevA neat thing in Wolfenstein is the use of LFSR to transition the screen to red when you die. https://fabiensanglard.net/fizzlefade/ reply dang 17 hours agoprevRelated: The genius of binary space partitioning in Doom (2019) - https://news.ycombinator.com/item?id=33692947 - Nov 2022 (159 comments) Using Binary Space Partitioning in Doom - https://news.ycombinator.com/item?id=21906051 - Dec 2019 (8 comments) How Much of a Genius-Level Move Was Using Binary Space Partitioning in Doom? - https://news.ycombinator.com/item?id=21467817 - Nov 2019 (6 comments) reply phkahler 15 hours agoprevMy preferred structure is the octtree over the BSP. or I guess for Doom it could have been a quad-tree. No surface splitting, and delete/re-insert for moving objects is possible. But I can claim to have known that until several years after Doom came along. reply drcode 12 hours agoparentSince lots of surfaces in Doom are diagonal, using that approach would have required an enormous amount of splitting of polygons, to fit the geometry to an orthogonal tree, probably increasing the amount of geometry an order of magnitude. The benefit of BSP is that it minimizes the amount of new geometry that needs to be created (even if it still requires some new geometry) reply hnthrowaway0328 11 hours agoprevReading the development history of the graphics engine of DOOM and especially QUAKE, it's a demonstration of John Carmack's perseverance. He would try a lot of algorithms until he hits something genius enough to solve the problem. I have a feeling this is the situation when you dig in something so hard that solution eventually comes to you. reply dannyobrien 11 hours agoparentCarmack is definitely someone who was determined -- and quick. When Carmack was working (I think on Quake), you could follow his todo list, which he kept stored in a .plan file on his server. If you used the Unix shell command \"finger johnc@idsoftware.com\" command, it would send the text of that file. It was pretty amazing to watch him crank through his tasks in near real time. The texts are archived here: https://github.com/ESWAT/john-carmack-plan-archive He is also pretty good at poker, apparently: http://www.ntk.net/1998/02/13/?l=114#l reply richrichie 16 hours agoprevI take pleasure in reminding my colleagues that ML is approximation theory and the concepts are 70-100 years old. All we have now is new hardware. reply bruce343434 9 hours agoparentRight, the theory around large language models was already worked out by then. reply DFHippie 17 hours agoprevTangentially related: a sort of whimsical ASCII art Doom using ray casting: https://github.com/TurkeyMcMac/ts3d Full disclosure: TurkeyMcMac was my son. He wasn't much of a gamer, but he was a pretty good programmer. He wrote this in high school. reply danparsonson 16 hours agoparentVery cool, and evocative of games from my own school days in the 80s. I hope I haven't misunderstood your use of the past tense when I say: I'm sorry for your loss. He clearly had a fun imagination and a lot of potential. reply Pseudomanifold 11 hours agoparentprevMy condolences for your loss. May his memory be a blessing. reply tadfisher 12 hours agoparentprevMy condolences for your loss. reply rasz 9 hours agoprevCarmack: - develops Wolf 3D - develops Doom, realizes there is more optimal algorithm for Wolf 3D, implements it for SNES port - develops Quake, realizes there is optimal algorithm for Doom allowing for zero overdraw. Sadly never implemented or documented further than a footnote in Abrash book, maybe portals like in Duke 3D? reply brcmthrowaway 18 hours agoprev [–] Weird article. Everyone stands on the shoulders of someone else. Even North Korean Linux reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "John Carmack's implementation of Binary Space Partitioning (BSP) in Doom significantly improved the game's rendering speed, solving the visible surface determination (VSD) problem.",
      "BSP trees, originally used in military research, allowed Doom to efficiently manage complex 3D scenes by splitting them into manageable parts, ensuring real-time rendering without excessive CPU usage.",
      "Carmack's innovative use of BSP trees in Doom, influenced by academic research and prior works, exemplifies the application of advanced computer graphics techniques in video game development, marking a milestone in the industry."
    ],
    "commentSummary": [
      "The text highlights John Carmack's exceptional ability to read and understand research papers, especially in computer science, where historical context is often neglected.",
      "It discusses the evolution of computer graphics and programming languages, emphasizing the importance of older, clearer research papers and the practical application of these techniques in game development.",
      "The text also touches on the challenges of the tech industry, including the tendency to reinvent existing systems, the role of open-source volunteers, and the impact of software patents on innovation."
    ],
    "points": 189,
    "commentCount": 129,
    "retryCount": 0,
    "time": 1718149092
  },
  {
    "id": 40650326,
    "title": "Global Historical Asset Returns: Housing Outperforms Equities with Lower Volatility",
    "originLink": "https://academic.oup.com/qje/article/134/3/1225/5435538",
    "originBody": "Skip to Main Content Advertisement Journals Books Search Menu Menu Sign in through your institution Navbar Search Filter The Quarterly Journal of Economics This issue D31 - Personal Income, Wealth, and Their Distributions E10 - General E44 - Financial Markets and the Macroeconomy G10 - General G12 - Asset Pricing; Trading volume; Bond Interest Rates N10 - General, International, or Comparative Economics Books Journals Oxford Academic Mobile Enter search term Search Issues JEL A - General Economics and Teaching Browse content in A - General Economics and Teaching A1 - General Economics Browse content in A1 - General Economics A11 - Role of Economics; Role of Economists; Market for Economists B - History of Economic Thought, Methodology, and Heterodox Approaches Browse content in B - History of Economic Thought, Methodology, and Heterodox Approaches B4 - Economic Methodology Browse content in B4 - Economic Methodology B49 - Other C - Mathematical and Quantitative Methods Browse content in C - Mathematical and Quantitative Methods C0 - General Browse content in C0 - General C00 - General C01 - Econometrics C1 - Econometric and Statistical Methods and Methodology: General Browse content in C1 - Econometric and Statistical Methods and Methodology: General C10 - General C11 - Bayesian Analysis: General C12 - Hypothesis Testing: General C13 - Estimation: General C14 - Semiparametric and Nonparametric Methods: General C18 - Methodological Issues: General C2 - Single Equation Models; Single Variables Browse content in C2 - Single Equation Models; Single Variables C21 - Cross-Sectional Models; Spatial Models; Treatment Effect Models; Quantile Regressions C23 - Panel Data Models; Spatio-temporal Models C26 - Instrumental Variables (IV) Estimation C3 - Multiple or Simultaneous Equation Models; Multiple Variables Browse content in C3 - Multiple or Simultaneous Equation Models; Multiple Variables C30 - General C31 - Cross-Sectional Models; Spatial Models; Treatment Effect Models; Quantile Regressions; Social Interaction Models C32 - Time-Series Models; Dynamic Quantile Regressions; Dynamic Treatment Effect Models; Diffusion Processes; State Space Models C35 - Discrete Regression and Qualitative Choice Models; Discrete Regressors; Proportions C4 - Econometric and Statistical Methods: Special Topics Browse content in C4 - Econometric and Statistical Methods: Special Topics C40 - General C5 - Econometric Modeling Browse content in C5 - Econometric Modeling C52 - Model Evaluation, Validation, and Selection C53 - Forecasting and Prediction Methods; Simulation Methods C55 - Large Data Sets: Modeling and Analysis C6 - Mathematical Methods; Programming Models; Mathematical and Simulation Modeling Browse content in C6 - Mathematical Methods; Programming Models; Mathematical and Simulation Modeling C63 - Computational Techniques; Simulation Modeling C67 - Input-Output Models C7 - Game Theory and Bargaining Theory Browse content in C7 - Game Theory and Bargaining Theory C71 - Cooperative Games C72 - Noncooperative Games C73 - Stochastic and Dynamic Games; Evolutionary Games; Repeated Games C78 - Bargaining Theory; Matching Theory C79 - Other C8 - Data Collection and Data Estimation Methodology; Computer Programs Browse content in C8 - Data Collection and Data Estimation Methodology; Computer Programs C83 - Survey Methods; Sampling Methods C9 - Design of Experiments Browse content in C9 - Design of Experiments C90 - General C91 - Laboratory, Individual Behavior C92 - Laboratory, Group Behavior C93 - Field Experiments C99 - Other D - Microeconomics Browse content in D - Microeconomics D0 - General Browse content in D0 - General D00 - General D01 - Microeconomic Behavior: Underlying Principles D02 - Institutions: Design, Formation, Operations, and Impact D03 - Behavioral Microeconomics: Underlying Principles D04 - Microeconomic Policy: Formulation; Implementation, and Evaluation D1 - Household Behavior and Family Economics Browse content in D1 - Household Behavior and Family Economics D10 - General D11 - Consumer Economics: Theory D12 - Consumer Economics: Empirical Analysis D13 - Household Production and Intrahousehold Allocation D14 - Household Saving; Personal Finance D15 - Intertemporal Household Choice: Life Cycle Models and Saving D18 - Consumer Protection D2 - Production and Organizations Browse content in D2 - Production and Organizations D20 - General D21 - Firm Behavior: Theory D22 - Firm Behavior: Empirical Analysis D23 - Organizational Behavior; Transaction Costs; Property Rights D24 - Production; Cost; Capital; Capital, Total Factor, and Multifactor Productivity; Capacity D3 - Distribution Browse content in D3 - Distribution D30 - General D31 - Personal Income, Wealth, and Their Distributions D33 - Factor Income Distribution D4 - Market Structure, Pricing, and Design Browse content in D4 - Market Structure, Pricing, and Design D40 - General D41 - Perfect Competition D42 - Monopoly D43 - Oligopoly and Other Forms of Market Imperfection D44 - Auctions D47 - Market Design D49 - Other D5 - General Equilibrium and Disequilibrium Browse content in D5 - General Equilibrium and Disequilibrium D50 - General D51 - Exchange and Production Economies D52 - Incomplete Markets D53 - Financial Markets D57 - Input-Output Tables and Analysis D6 - Welfare Economics Browse content in D6 - Welfare Economics D60 - General D61 - Allocative Efficiency; Cost-Benefit Analysis D62 - Externalities D63 - Equity, Justice, Inequality, and Other Normative Criteria and Measurement D64 - Altruism; Philanthropy D69 - Other D7 - Analysis of Collective Decision-Making Browse content in D7 - Analysis of Collective Decision-Making D70 - General D71 - Social Choice; Clubs; Committees; Associations D72 - Political Processes: Rent-seeking, Lobbying, Elections, Legislatures, and Voting Behavior D73 - Bureaucracy; Administrative Processes in Public Organizations; Corruption D74 - Conflict; Conflict Resolution; Alliances; Revolutions D78 - Positive Analysis of Policy Formulation and Implementation D8 - Information, Knowledge, and Uncertainty Browse content in D8 - Information, Knowledge, and Uncertainty D80 - General D81 - Criteria for Decision-Making under Risk and Uncertainty D82 - Asymmetric and Private Information; Mechanism Design D83 - Search; Learning; Information and Knowledge; Communication; Belief; Unawareness D84 - Expectations; Speculations D85 - Network Formation and Analysis: Theory D86 - Economics of Contract: Theory D89 - Other D9 - Micro-Based Behavioral Economics Browse content in D9 - Micro-Based Behavioral Economics D90 - General D91 - Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making D92 - Intertemporal Firm Choice, Investment, Capacity, and Financing E - Macroeconomics and Monetary Economics Browse content in E - Macroeconomics and Monetary Economics E0 - General Browse content in E0 - General E00 - General E01 - Measurement and Data on National Income and Product Accounts and Wealth; Environmental Accounts E02 - Institutions and the Macroeconomy E03 - Behavioral Macroeconomics E1 - General Aggregative Models Browse content in E1 - General Aggregative Models E10 - General E12 - Keynes; Keynesian; Post-Keynesian E13 - Neoclassical E2 - Consumption, Saving, Production, Investment, Labor Markets, and Informal Economy Browse content in E2 - Consumption, Saving, Production, Investment, Labor Markets, and Informal Economy E20 - General E21 - Consumption; Saving; Wealth E22 - Investment; Capital; Intangible Capital; Capacity E23 - Production E24 - Employment; Unemployment; Wages; Intergenerational Income Distribution; Aggregate Human Capital; Aggregate Labor Productivity E25 - Aggregate Factor Income Distribution E3 - Prices, Business Fluctuations, and Cycles Browse content in E3 - Prices, Business Fluctuations, and Cycles E30 - General E31 - Price Level; Inflation; Deflation E32 - Business Fluctuations; Cycles E37 - Forecasting and Simulation: Models and Applications E4 - Money and Interest Rates Browse content in E4 - Money and Interest Rates E40 - General E41 - Demand for Money E42 - Monetary Systems; Standards; Regimes; Government and the Monetary System; Payment Systems E43 - Interest Rates: Determination, Term Structure, and Effects E44 - Financial Markets and the Macroeconomy E5 - Monetary Policy, Central Banking, and the Supply of Money and Credit Browse content in E5 - Monetary Policy, Central Banking, and the Supply of Money and Credit E50 - General E51 - Money Supply; Credit; Money Multipliers E52 - Monetary Policy E58 - Central Banks and Their Policies E6 - Macroeconomic Policy, Macroeconomic Aspects of Public Finance, and General Outlook Browse content in E6 - Macroeconomic Policy, Macroeconomic Aspects of Public Finance, and General Outlook E60 - General E62 - Fiscal Policy E66 - General Outlook and Conditions E7 - Macro-Based Behavioral Economics Browse content in E7 - Macro-Based Behavioral Economics E71 - Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on the Macro Economy F - International Economics Browse content in F - International Economics F0 - General Browse content in F0 - General F00 - General F1 - Trade Browse content in F1 - Trade F10 - General F11 - Neoclassical Models of Trade F12 - Models of Trade with Imperfect Competition and Scale Economies; Fragmentation F13 - Trade Policy; International Trade Organizations F14 - Empirical Studies of Trade F15 - Economic Integration F16 - Trade and Labor Market Interactions F18 - Trade and Environment F2 - International Factor Movements and International Business Browse content in F2 - International Factor Movements and International Business F20 - General F21 - International Investment; Long-Term Capital Movements F22 - International Migration F23 - Multinational Firms; International Business F3 - International Finance Browse content in F3 - International Finance F30 - General F31 - Foreign Exchange F32 - Current Account Adjustment; Short-Term Capital Movements F34 - International Lending and Debt Problems F35 - Foreign Aid F36 - Financial Aspects of Economic Integration F4 - Macroeconomic Aspects of International Trade and Finance Browse content in F4 - Macroeconomic Aspects of International Trade and Finance F40 - General F41 - Open Economy Macroeconomics F42 - International Policy Coordination and Transmission F43 - Economic Growth of Open Economies F44 - International Business Cycles F5 - International Relations, National Security, and International Political Economy Browse content in F5 - International Relations, National Security, and International Political Economy F50 - General F51 - International Conflicts; Negotiations; Sanctions F52 - National Security; Economic Nationalism F55 - International Institutional Arrangements F6 - Economic Impacts of Globalization Browse content in F6 - Economic Impacts of Globalization F60 - General F61 - Microeconomic Impacts F62 - Macroeconomic Impacts F63 - Economic Development G - Financial Economics Browse content in G - Financial Economics G0 - General Browse content in G0 - General G00 - General G01 - Financial Crises G02 - Behavioral Finance: Underlying Principles G1 - General Financial Markets Browse content in G1 - General Financial Markets G10 - General G11 - Portfolio Choice; Investment Decisions G12 - Asset Pricing; Trading volume; Bond Interest Rates G14 - Information and Market Efficiency; Event Studies; Insider Trading G15 - International Financial Markets G18 - Government Policy and Regulation G19 - Other G2 - Financial Institutions and Services Browse content in G2 - Financial Institutions and Services G20 - General G21 - Banks; Depository Institutions; Micro Finance Institutions; Mortgages G22 - Insurance; Insurance Companies; Actuarial Studies G23 - Non-bank Financial Institutions; Financial Instruments; Institutional Investors G24 - Investment Banking; Venture Capital; Brokerage; Ratings and Ratings Agencies G28 - Government Policy and Regulation G3 - Corporate Finance and Governance Browse content in G3 - Corporate Finance and Governance G30 - General G31 - Capital Budgeting; Fixed Investment and Inventory Studies; Capacity G32 - Financing Policy; Financial Risk and Risk Management; Capital and Ownership Structure; Value of Firms; Goodwill G33 - Bankruptcy; Liquidation G34 - Mergers; Acquisitions; Restructuring; Corporate Governance G38 - Government Policy and Regulation G4 - Behavioral Finance Browse content in G4 - Behavioral Finance G40 - General G41 - Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making in Financial Markets G5 - Household Finance Browse content in G5 - Household Finance G50 - General G51 - Household Saving, Borrowing, Debt, and Wealth H - Public Economics Browse content in H - Public Economics H0 - General Browse content in H0 - General H00 - General H1 - Structure and Scope of Government Browse content in H1 - Structure and Scope of Government H10 - General H11 - Structure, Scope, and Performance of Government H2 - Taxation, Subsidies, and Revenue Browse content in H2 - Taxation, Subsidies, and Revenue H20 - General H21 - Efficiency; Optimal Taxation H22 - Incidence H23 - Externalities; Redistributive Effects; Environmental Taxes and Subsidies H24 - Personal Income and Other Nonbusiness Taxes and Subsidies; includes inheritance and gift taxes H25 - Business Taxes and Subsidies H26 - Tax Evasion and Avoidance H3 - Fiscal Policies and Behavior of Economic Agents Browse content in H3 - Fiscal Policies and Behavior of Economic Agents H31 - Household H32 - Firm H4 - Publicly Provided Goods Browse content in H4 - Publicly Provided Goods H40 - General H41 - Public Goods H42 - Publicly Provided Private Goods H44 - Publicly Provided Goods: Mixed Markets H5 - National Government Expenditures and Related Policies Browse content in H5 - National Government Expenditures and Related Policies H50 - General H51 - Government Expenditures and Health H52 - Government Expenditures and Education H53 - Government Expenditures and Welfare Programs H54 - Infrastructures; Other Public Investment and Capital Stock H55 - Social Security and Public Pensions H56 - National Security and War H57 - Procurement H6 - National Budget, Deficit, and Debt Browse content in H6 - National Budget, Deficit, and Debt H63 - Debt; Debt Management; Sovereign Debt H7 - State and Local Government; Intergovernmental Relations Browse content in H7 - State and Local Government; Intergovernmental Relations H70 - General H71 - State and Local Taxation, Subsidies, and Revenue H73 - Interjurisdictional Differentials and Their Effects H75 - State and Local Government: Health; Education; Welfare; Public Pensions H76 - State and Local Government: Other Expenditure Categories H77 - Intergovernmental Relations; Federalism; Secession H8 - Miscellaneous Issues Browse content in H8 - Miscellaneous Issues H81 - Governmental Loans; Loan Guarantees; Credits; Grants; Bailouts H83 - Public Administration; Public Sector Accounting and Audits H87 - International Fiscal Issues; International Public Goods I - Health, Education, and Welfare Browse content in I - Health, Education, and Welfare I0 - General Browse content in I0 - General I00 - General I1 - Health Browse content in I1 - Health I10 - General I11 - Analysis of Health Care Markets I12 - Health Behavior I13 - Health Insurance, Public and Private I14 - Health and Inequality I15 - Health and Economic Development I18 - Government Policy; Regulation; Public Health I2 - Education and Research Institutions Browse content in I2 - Education and Research Institutions I20 - General I21 - Analysis of Education I22 - Educational Finance; Financial Aid I23 - Higher Education; Research Institutions I24 - Education and Inequality I25 - Education and Economic Development I26 - Returns to Education I28 - Government Policy I3 - Welfare, Well-Being, and Poverty Browse content in I3 - Welfare, Well-Being, and Poverty I30 - General I31 - General Welfare I32 - Measurement and Analysis of Poverty I38 - Government Policy; Provision and Effects of Welfare Programs J - Labor and Demographic Economics Browse content in J - Labor and Demographic Economics J0 - General Browse content in J0 - General J00 - General J01 - Labor Economics: General J08 - Labor Economics Policies J1 - Demographic Economics Browse content in J1 - Demographic Economics J10 - General J11 - Demographic Trends, Macroeconomic Effects, and Forecasts J12 - Marriage; Marital Dissolution; Family Structure; Domestic Abuse J13 - Fertility; Family Planning; Child Care; Children; Youth J14 - Economics of the Elderly; Economics of the Handicapped; Non-Labor Market Discrimination J15 - Economics of Minorities, Races, Indigenous Peoples, and Immigrants; Non-labor Discrimination J16 - Economics of Gender; Non-labor Discrimination J18 - Public Policy J2 - Demand and Supply of Labor Browse content in J2 - Demand and Supply of Labor J20 - General J21 - Labor Force and Employment, Size, and Structure J22 - Time Allocation and Labor Supply J23 - Labor Demand J24 - Human Capital; Skills; Occupational Choice; Labor Productivity J26 - Retirement; Retirement Policies J3 - Wages, Compensation, and Labor Costs Browse content in J3 - Wages, Compensation, and Labor Costs J30 - General J31 - Wage Level and Structure; Wage Differentials J33 - Compensation Packages; Payment Methods J38 - Public Policy J4 - Particular Labor Markets Browse content in J4 - Particular Labor Markets J40 - General J42 - Monopsony; Segmented Labor Markets J44 - Professional Labor Markets; Occupational Licensing J45 - Public Sector Labor Markets J48 - Public Policy J49 - Other J5 - Labor-Management Relations, Trade Unions, and Collective Bargaining Browse content in J5 - Labor-Management Relations, Trade Unions, and Collective Bargaining J50 - General J51 - Trade Unions: Objectives, Structure, and Effects J53 - Labor-Management Relations; Industrial Jurisprudence J6 - Mobility, Unemployment, Vacancies, and Immigrant Workers Browse content in J6 - Mobility, Unemployment, Vacancies, and Immigrant Workers J60 - General J61 - Geographic Labor Mobility; Immigrant Workers J62 - Job, Occupational, and Intergenerational Mobility J63 - Turnover; Vacancies; Layoffs J64 - Unemployment: Models, Duration, Incidence, and Job Search J65 - Unemployment Insurance; Severance Pay; Plant Closings J68 - Public Policy J7 - Labor Discrimination Browse content in J7 - Labor Discrimination J71 - Discrimination J78 - Public Policy J8 - Labor Standards: National and International Browse content in J8 - Labor Standards: National and International J81 - Working Conditions J88 - Public Policy K - Law and Economics Browse content in K - Law and Economics K0 - General Browse content in K0 - General K00 - General K1 - Basic Areas of Law Browse content in K1 - Basic Areas of Law K14 - Criminal Law K2 - Regulation and Business Law K3 - Other Substantive Areas of Law Browse content in K3 - Other Substantive Areas of Law K31 - Labor Law K4 - Legal Procedure, the Legal System, and Illegal Behavior Browse content in K4 - Legal Procedure, the Legal System, and Illegal Behavior K40 - General K41 - Litigation Process K42 - Illegal Behavior and the Enforcement of Law L - Industrial Organization Browse content in L - Industrial Organization L0 - General Browse content in L0 - General L00 - General L1 - Market Structure, Firm Strategy, and Market Performance Browse content in L1 - Market Structure, Firm Strategy, and Market Performance L10 - General L11 - Production, Pricing, and Market Structure; Size Distribution of Firms L13 - Oligopoly and Other Imperfect Markets L14 - Transactional Relationships; Contracts and Reputation; Networks L15 - Information and Product Quality; Standardization and Compatibility L16 - Industrial Organization and Macroeconomics: Industrial Structure and Structural Change; Industrial Price Indices L19 - Other L2 - Firm Objectives, Organization, and Behavior Browse content in L2 - Firm Objectives, Organization, and Behavior L21 - Business Objectives of the Firm L22 - Firm Organization and Market Structure L23 - Organization of Production L24 - Contracting Out; Joint Ventures; Technology Licensing L25 - Firm Performance: Size, Diversification, and Scope L26 - Entrepreneurship L3 - Nonprofit Organizations and Public Enterprise Browse content in L3 - Nonprofit Organizations and Public Enterprise L33 - Comparison of Public and Private Enterprises and Nonprofit Institutions; Privatization; Contracting Out L4 - Antitrust Issues and Policies Browse content in L4 - Antitrust Issues and Policies L40 - General L41 - Monopolization; Horizontal Anticompetitive Practices L42 - Vertical Restraints; Resale Price Maintenance; Quantity Discounts L5 - Regulation and Industrial Policy Browse content in L5 - Regulation and Industrial Policy L50 - General L51 - Economics of Regulation L6 - Industry Studies: Manufacturing Browse content in L6 - Industry Studies: Manufacturing L60 - General L62 - Automobiles; Other Transportation Equipment; Related Parts and Equipment L63 - Microelectronics; Computers; Communications Equipment L66 - Food; Beverages; Cosmetics; Tobacco; Wine and Spirits L7 - Industry Studies: Primary Products and Construction Browse content in L7 - Industry Studies: Primary Products and Construction L71 - Mining, Extraction, and Refining: Hydrocarbon Fuels L73 - Forest Products L8 - Industry Studies: Services Browse content in L8 - Industry Studies: Services L81 - Retail and Wholesale Trade; e-Commerce L83 - Sports; Gambling; Recreation; Tourism L84 - Personal, Professional, and Business Services L86 - Information and Internet Services; Computer Software L9 - Industry Studies: Transportation and Utilities Browse content in L9 - Industry Studies: Transportation and Utilities L91 - Transportation: General L93 - Air Transportation L94 - Electric Utilities M - Business Administration and Business Economics; Marketing; Accounting; Personnel Economics Browse content in M - Business Administration and Business Economics; Marketing; Accounting; Personnel Economics M1 - Business Administration Browse content in M1 - Business Administration M11 - Production Management M12 - Personnel Management; Executives; Executive Compensation M14 - Corporate Culture; Social Responsibility M2 - Business Economics Browse content in M2 - Business Economics M21 - Business Economics M3 - Marketing and Advertising Browse content in M3 - Marketing and Advertising M31 - Marketing M37 - Advertising M4 - Accounting and Auditing Browse content in M4 - Accounting and Auditing M42 - Auditing M48 - Government Policy and Regulation M5 - Personnel Economics Browse content in M5 - Personnel Economics M50 - General M51 - Firm Employment Decisions; Promotions M52 - Compensation and Compensation Methods and Their Effects M53 - Training M54 - Labor Management N - Economic History Browse content in N - Economic History N0 - General Browse content in N0 - General N00 - General N01 - Development of the Discipline: Historiographical; Sources and Methods N1 - Macroeconomics and Monetary Economics; Industrial Structure; Growth; Fluctuations Browse content in N1 - Macroeconomics and Monetary Economics; Industrial Structure; Growth; Fluctuations N10 - General, International, or Comparative N11 - U.S.; Canada: Pre-1913 N12 - U.S.; Canada: 1913- N13 - Europe: Pre-1913 N17 - Africa; Oceania N2 - Financial Markets and Institutions Browse content in N2 - Financial Markets and Institutions N20 - General, International, or Comparative N22 - U.S.; Canada: 1913- N23 - Europe: Pre-1913 N3 - Labor and Consumers, Demography, Education, Health, Welfare, Income, Wealth, Religion, and Philanthropy Browse content in N3 - Labor and Consumers, Demography, Education, Health, Welfare, Income, Wealth, Religion, and Philanthropy N30 - General, International, or Comparative N31 - U.S.; Canada: Pre-1913 N32 - U.S.; Canada: 1913- N33 - Europe: Pre-1913 N34 - Europe: 1913- N36 - Latin America; Caribbean N37 - Africa; Oceania N4 - Government, War, Law, International Relations, and Regulation Browse content in N4 - Government, War, Law, International Relations, and Regulation N40 - General, International, or Comparative N41 - U.S.; Canada: Pre-1913 N42 - U.S.; Canada: 1913- N43 - Europe: Pre-1913 N44 - Europe: 1913- N45 - Asia including Middle East N47 - Africa; Oceania N5 - Agriculture, Natural Resources, Environment, and Extractive Industries Browse content in N5 - Agriculture, Natural Resources, Environment, and Extractive Industries N50 - General, International, or Comparative N51 - U.S.; Canada: Pre-1913 N6 - Manufacturing and Construction Browse content in N6 - Manufacturing and Construction N63 - Europe: Pre-1913 N7 - Transport, Trade, Energy, Technology, and Other Services Browse content in N7 - Transport, Trade, Energy, Technology, and Other Services N71 - U.S.; Canada: Pre-1913 N8 - Micro-Business History Browse content in N8 - Micro-Business History N82 - U.S.; Canada: 1913- N9 - Regional and Urban History Browse content in N9 - Regional and Urban History N91 - U.S.; Canada: Pre-1913 N92 - U.S.; Canada: 1913- N93 - Europe: Pre-1913 N94 - Europe: 1913- O - Economic Development, Innovation, Technological Change, and Growth Browse content in O - Economic Development, Innovation, Technological Change, and Growth O1 - Economic Development Browse content in O1 - Economic Development O10 - General O11 - Macroeconomic Analyses of Economic Development O12 - Microeconomic Analyses of Economic Development O13 - Agriculture; Natural Resources; Energy; Environment; Other Primary Products O14 - Industrialization; Manufacturing and Service Industries; Choice of Technology O15 - Human Resources; Human Development; Income Distribution; Migration O16 - Financial Markets; Saving and Capital Investment; Corporate Finance and Governance O17 - Formal and Informal Sectors; Shadow Economy; Institutional Arrangements O18 - Urban, Rural, Regional, and Transportation Analysis; Housing; Infrastructure O19 - International Linkages to Development; Role of International Organizations O2 - Development Planning and Policy Browse content in O2 - Development Planning and Policy O23 - Fiscal and Monetary Policy in Development O25 - Industrial Policy O3 - Innovation; Research and Development; Technological Change; Intellectual Property Rights Browse content in O3 - Innovation; Research and Development; Technological Change; Intellectual Property Rights O30 - General O31 - Innovation and Invention: Processes and Incentives O32 - Management of Technological Innovation and R&D O33 - Technological Change: Choices and Consequences; Diffusion Processes O34 - Intellectual Property and Intellectual Capital O38 - Government Policy O4 - Economic Growth and Aggregate Productivity Browse content in O4 - Economic Growth and Aggregate Productivity O40 - General O41 - One, Two, and Multisector Growth Models O43 - Institutions and Growth O44 - Environment and Growth O47 - Empirical Studies of Economic Growth; Aggregate Productivity; Cross-Country Output Convergence O5 - Economywide Country Studies Browse content in O5 - Economywide Country Studies O52 - Europe O53 - Asia including Middle East O55 - Africa P - Economic Systems Browse content in P - Economic Systems P0 - General Browse content in P0 - General P00 - General P1 - Capitalist Systems Browse content in P1 - Capitalist Systems P10 - General P16 - Political Economy P17 - Performance and Prospects P18 - Energy: Environment P2 - Socialist Systems and Transitional Economies Browse content in P2 - Socialist Systems and Transitional Economies P26 - Political Economy; Property Rights P3 - Socialist Institutions and Their Transitions Browse content in P3 - Socialist Institutions and Their Transitions P37 - Legal Institutions; Illegal Behavior P4 - Other Economic Systems Browse content in P4 - Other Economic Systems P48 - Political Economy; Legal Institutions; Property Rights; Natural Resources; Energy; Environment; Regional Studies P5 - Comparative Economic Systems Browse content in P5 - Comparative Economic Systems P51 - Comparative Analysis of Economic Systems Q - Agricultural and Natural Resource Economics; Environmental and Ecological Economics Browse content in Q - Agricultural and Natural Resource Economics; Environmental and Ecological Economics Q1 - Agriculture Browse content in Q1 - Agriculture Q10 - General Q12 - Micro Analysis of Farm Firms, Farm Households, and Farm Input Markets Q13 - Agricultural Markets and Marketing; Cooperatives; Agribusiness Q14 - Agricultural Finance Q15 - Land Ownership and Tenure; Land Reform; Land Use; Irrigation; Agriculture and Environment Q16 - R&D; Agricultural Technology; Biofuels; Agricultural Extension Services Q2 - Renewable Resources and Conservation Browse content in Q2 - Renewable Resources and Conservation Q25 - Water Q3 - Nonrenewable Resources and Conservation Browse content in Q3 - Nonrenewable Resources and Conservation Q32 - Exhaustible Resources and Economic Development Q34 - Natural Resources and Domestic and International Conflicts Q4 - Energy Browse content in Q4 - Energy Q41 - Demand and Supply; Prices Q48 - Government Policy Q5 - Environmental Economics Browse content in Q5 - Environmental Economics Q50 - General Q51 - Valuation of Environmental Effects Q53 - Air Pollution; Water Pollution; Noise; Hazardous Waste; Solid Waste; Recycling Q54 - Climate; Natural Disasters; Global Warming Q56 - Environment and Development; Environment and Trade; Sustainability; Environmental Accounts and Accounting; Environmental Equity; Population Growth Q58 - Government Policy R - Urban, Rural, Regional, Real Estate, and Transportation Economics Browse content in R - Urban, Rural, Regional, Real Estate, and Transportation Economics R0 - General Browse content in R0 - General R00 - General R1 - General Regional Economics Browse content in R1 - General Regional Economics R11 - Regional Economic Activity: Growth, Development, Environmental Issues, and Changes R12 - Size and Spatial Distributions of Regional Economic Activity R13 - General Equilibrium and Welfare Economic Analysis of Regional Economies R2 - Household Analysis Browse content in R2 - Household Analysis R20 - General R23 - Regional Migration; Regional Labor Markets; Population; Neighborhood Characteristics R28 - Government Policy R3 - Real Estate Markets, Spatial Production Analysis, and Firm Location Browse content in R3 - Real Estate Markets, Spatial Production Analysis, and Firm Location R30 - General R31 - Housing Supply and Markets R38 - Government Policy R4 - Transportation Economics Browse content in R4 - Transportation Economics R40 - General R41 - Transportation: Demand, Supply, and Congestion; Travel Time; Safety and Accidents; Transportation Noise R48 - Government Pricing and Policy Z - Other Special Topics Browse content in Z - Other Special Topics Z1 - Cultural Economics; Economic Sociology; Economic Anthropology Browse content in Z1 - Cultural Economics; Economic Sociology; Economic Anthropology Z10 - General Z12 - Religion Z13 - Economic Sociology; Economic Anthropology; Social and Economic Stratification More Content Advance Articles Editor's Choice Submit Author Guidelines Submission Site Open Access Options Self-Archiving Policy Why Submit? Purchase About About The Quarterly Journal of Economics Editorial Board Advertising and Corporate Services Journals Career Network Alerts Policies Dispatch Dates Contact Us Journals on Oxford Academic Books on Oxford Academic Issues JEL All JEL Expand Expand A - General Economics and Teaching A1 - General Economics A11 - Role of Economics; Role of Economists; Market for Economists B - History of Economic Thought, Methodology, and Heterodox Approaches B4 - Economic Methodology B49 - Other C - Mathematical and Quantitative Methods C0 - General C00 - General C01 - Econometrics C1 - Econometric and Statistical Methods and Methodology: General C10 - General C11 - Bayesian Analysis: General C12 - Hypothesis Testing: General C13 - Estimation: General C14 - Semiparametric and Nonparametric Methods: General C18 - Methodological Issues: General C2 - Single Equation Models; Single Variables C21 - Cross-Sectional Models; Spatial Models; Treatment Effect Models; Quantile Regressions C23 - Panel Data Models; Spatio-temporal Models C26 - Instrumental Variables (IV) Estimation C3 - Multiple or Simultaneous Equation Models; Multiple Variables C30 - General C31 - Cross-Sectional Models; Spatial Models; Treatment Effect Models; Quantile Regressions; Social Interaction Models C32 - Time-Series Models; Dynamic Quantile Regressions; Dynamic Treatment Effect Models; Diffusion Processes; State Space Models C35 - Discrete Regression and Qualitative Choice Models; Discrete Regressors; Proportions C4 - Econometric and Statistical Methods: Special Topics C40 - General C5 - Econometric Modeling C52 - Model Evaluation, Validation, and Selection C53 - Forecasting and Prediction Methods; Simulation Methods C55 - Large Data Sets: Modeling and Analysis C6 - Mathematical Methods; Programming Models; Mathematical and Simulation Modeling C63 - Computational Techniques; Simulation Modeling C67 - Input-Output Models C7 - Game Theory and Bargaining Theory C71 - Cooperative Games C72 - Noncooperative Games C73 - Stochastic and Dynamic Games; Evolutionary Games; Repeated Games C78 - Bargaining Theory; Matching Theory C79 - Other C8 - Data Collection and Data Estimation Methodology; Computer Programs C83 - Survey Methods; Sampling Methods C9 - Design of Experiments C90 - General C91 - Laboratory, Individual Behavior C92 - Laboratory, Group Behavior C93 - Field Experiments C99 - Other D - Microeconomics D0 - General D00 - General D01 - Microeconomic Behavior: Underlying Principles D02 - Institutions: Design, Formation, Operations, and Impact D03 - Behavioral Microeconomics: Underlying Principles D04 - Microeconomic Policy: Formulation; Implementation, and Evaluation D1 - Household Behavior and Family Economics D10 - General D11 - Consumer Economics: Theory D12 - Consumer Economics: Empirical Analysis D13 - Household Production and Intrahousehold Allocation D14 - Household Saving; Personal Finance D15 - Intertemporal Household Choice: Life Cycle Models and Saving D18 - Consumer Protection D2 - Production and Organizations D20 - General D21 - Firm Behavior: Theory D22 - Firm Behavior: Empirical Analysis D23 - Organizational Behavior; Transaction Costs; Property Rights D24 - Production; Cost; Capital; Capital, Total Factor, and Multifactor Productivity; Capacity D3 - Distribution D30 - General D31 - Personal Income, Wealth, and Their Distributions D33 - Factor Income Distribution D4 - Market Structure, Pricing, and Design D40 - General D41 - Perfect Competition D42 - Monopoly D43 - Oligopoly and Other Forms of Market Imperfection D44 - Auctions D47 - Market Design D49 - Other D5 - General Equilibrium and Disequilibrium D50 - General D51 - Exchange and Production Economies D52 - Incomplete Markets D53 - Financial Markets D57 - Input-Output Tables and Analysis D6 - Welfare Economics D60 - General D61 - Allocative Efficiency; Cost-Benefit Analysis D62 - Externalities D63 - Equity, Justice, Inequality, and Other Normative Criteria and Measurement D64 - Altruism; Philanthropy D69 - Other D7 - Analysis of Collective Decision-Making D70 - General D71 - Social Choice; Clubs; Committees; Associations D72 - Political Processes: Rent-seeking, Lobbying, Elections, Legislatures, and Voting Behavior D73 - Bureaucracy; Administrative Processes in Public Organizations; Corruption D74 - Conflict; Conflict Resolution; Alliances; Revolutions D78 - Positive Analysis of Policy Formulation and Implementation D8 - Information, Knowledge, and Uncertainty D80 - General D81 - Criteria for Decision-Making under Risk and Uncertainty D82 - Asymmetric and Private Information; Mechanism Design D83 - Search; Learning; Information and Knowledge; Communication; Belief; Unawareness D84 - Expectations; Speculations D85 - Network Formation and Analysis: Theory D86 - Economics of Contract: Theory D89 - Other D9 - Micro-Based Behavioral Economics D90 - General D91 - Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making D92 - Intertemporal Firm Choice, Investment, Capacity, and Financing E - Macroeconomics and Monetary Economics E0 - General E00 - General E01 - Measurement and Data on National Income and Product Accounts and Wealth; Environmental Accounts E02 - Institutions and the Macroeconomy E03 - Behavioral Macroeconomics E1 - General Aggregative Models E10 - General E12 - Keynes; Keynesian; Post-Keynesian E13 - Neoclassical E2 - Consumption, Saving, Production, Investment, Labor Markets, and Informal Economy E20 - General E21 - Consumption; Saving; Wealth E22 - Investment; Capital; Intangible Capital; Capacity E23 - Production E24 - Employment; Unemployment; Wages; Intergenerational Income Distribution; Aggregate Human Capital; Aggregate Labor Productivity E25 - Aggregate Factor Income Distribution E3 - Prices, Business Fluctuations, and Cycles E30 - General E31 - Price Level; Inflation; Deflation E32 - Business Fluctuations; Cycles E37 - Forecasting and Simulation: Models and Applications E4 - Money and Interest Rates E40 - General E41 - Demand for Money E42 - Monetary Systems; Standards; Regimes; Government and the Monetary System; Payment Systems E43 - Interest Rates: Determination, Term Structure, and Effects E44 - Financial Markets and the Macroeconomy E5 - Monetary Policy, Central Banking, and the Supply of Money and Credit E50 - General E51 - Money Supply; Credit; Money Multipliers E52 - Monetary Policy E58 - Central Banks and Their Policies E6 - Macroeconomic Policy, Macroeconomic Aspects of Public Finance, and General Outlook E60 - General E62 - Fiscal Policy E66 - General Outlook and Conditions E7 - Macro-Based Behavioral Economics E71 - Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on the Macro Economy F - International Economics F0 - General F00 - General F1 - Trade F10 - General F11 - Neoclassical Models of Trade F12 - Models of Trade with Imperfect Competition and Scale Economies; Fragmentation F13 - Trade Policy; International Trade Organizations F14 - Empirical Studies of Trade F15 - Economic Integration F16 - Trade and Labor Market Interactions F18 - Trade and Environment F2 - International Factor Movements and International Business F20 - General F21 - International Investment; Long-Term Capital Movements F22 - International Migration F23 - Multinational Firms; International Business F3 - International Finance F30 - General F31 - Foreign Exchange F32 - Current Account Adjustment; Short-Term Capital Movements F34 - International Lending and Debt Problems F35 - Foreign Aid F36 - Financial Aspects of Economic Integration F4 - Macroeconomic Aspects of International Trade and Finance F40 - General F41 - Open Economy Macroeconomics F42 - International Policy Coordination and Transmission F43 - Economic Growth of Open Economies F44 - International Business Cycles F5 - International Relations, National Security, and International Political Economy F50 - General F51 - International Conflicts; Negotiations; Sanctions F52 - National Security; Economic Nationalism F55 - International Institutional Arrangements F6 - Economic Impacts of Globalization F60 - General F61 - Microeconomic Impacts F62 - Macroeconomic Impacts F63 - Economic Development G - Financial Economics G0 - General G00 - General G01 - Financial Crises G02 - Behavioral Finance: Underlying Principles G1 - General Financial Markets G10 - General G11 - Portfolio Choice; Investment Decisions G12 - Asset Pricing; Trading volume; Bond Interest Rates G14 - Information and Market Efficiency; Event Studies; Insider Trading G15 - International Financial Markets G18 - Government Policy and Regulation G19 - Other G2 - Financial Institutions and Services G20 - General G21 - Banks; Depository Institutions; Micro Finance Institutions; Mortgages G22 - Insurance; Insurance Companies; Actuarial Studies G23 - Non-bank Financial Institutions; Financial Instruments; Institutional Investors G24 - Investment Banking; Venture Capital; Brokerage; Ratings and Ratings Agencies G28 - Government Policy and Regulation G3 - Corporate Finance and Governance G30 - General G31 - Capital Budgeting; Fixed Investment and Inventory Studies; Capacity G32 - Financing Policy; Financial Risk and Risk Management; Capital and Ownership Structure; Value of Firms; Goodwill G33 - Bankruptcy; Liquidation G34 - Mergers; Acquisitions; Restructuring; Corporate Governance G38 - Government Policy and Regulation G4 - Behavioral Finance G40 - General G41 - Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making in Financial Markets G5 - Household Finance G50 - General G51 - Household Saving, Borrowing, Debt, and Wealth H - Public Economics H0 - General H00 - General H1 - Structure and Scope of Government H10 - General H11 - Structure, Scope, and Performance of Government H2 - Taxation, Subsidies, and Revenue H20 - General H21 - Efficiency; Optimal Taxation H22 - Incidence H23 - Externalities; Redistributive Effects; Environmental Taxes and Subsidies H24 - Personal Income and Other Nonbusiness Taxes and Subsidies; includes inheritance and gift taxes H25 - Business Taxes and Subsidies H26 - Tax Evasion and Avoidance H3 - Fiscal Policies and Behavior of Economic Agents H31 - Household H32 - Firm H4 - Publicly Provided Goods H40 - General H41 - Public Goods H42 - Publicly Provided Private Goods H44 - Publicly Provided Goods: Mixed Markets H5 - National Government Expenditures and Related Policies H50 - General H51 - Government Expenditures and Health H52 - Government Expenditures and Education H53 - Government Expenditures and Welfare Programs H54 - Infrastructures; Other Public Investment and Capital Stock H55 - Social Security and Public Pensions H56 - National Security and War H57 - Procurement H6 - National Budget, Deficit, and Debt H63 - Debt; Debt Management; Sovereign Debt H7 - State and Local Government; Intergovernmental Relations H70 - General H71 - State and Local Taxation, Subsidies, and Revenue H73 - Interjurisdictional Differentials and Their Effects H75 - State and Local Government: Health; Education; Welfare; Public Pensions H76 - State and Local Government: Other Expenditure Categories H77 - Intergovernmental Relations; Federalism; Secession H8 - Miscellaneous Issues H81 - Governmental Loans; Loan Guarantees; Credits; Grants; Bailouts H83 - Public Administration; Public Sector Accounting and Audits H87 - International Fiscal Issues; International Public Goods I - Health, Education, and Welfare I0 - General I00 - General I1 - Health I10 - General I11 - Analysis of Health Care Markets I12 - Health Behavior I13 - Health Insurance, Public and Private I14 - Health and Inequality I15 - Health and Economic Development I18 - Government Policy; Regulation; Public Health I2 - Education and Research Institutions I20 - General I21 - Analysis of Education I22 - Educational Finance; Financial Aid I23 - Higher Education; Research Institutions I24 - Education and Inequality I25 - Education and Economic Development I26 - Returns to Education I28 - Government Policy I3 - Welfare, Well-Being, and Poverty I30 - General I31 - General Welfare I32 - Measurement and Analysis of Poverty I38 - Government Policy; Provision and Effects of Welfare Programs J - Labor and Demographic Economics J0 - General J00 - General J01 - Labor Economics: General J08 - Labor Economics Policies J1 - Demographic Economics J10 - General J11 - Demographic Trends, Macroeconomic Effects, and Forecasts J12 - Marriage; Marital Dissolution; Family Structure; Domestic Abuse J13 - Fertility; Family Planning; Child Care; Children; Youth J14 - Economics of the Elderly; Economics of the Handicapped; Non-Labor Market Discrimination J15 - Economics of Minorities, Races, Indigenous Peoples, and Immigrants; Non-labor Discrimination J16 - Economics of Gender; Non-labor Discrimination J18 - Public Policy J2 - Demand and Supply of Labor J20 - General J21 - Labor Force and Employment, Size, and Structure J22 - Time Allocation and Labor Supply J23 - Labor Demand J24 - Human Capital; Skills; Occupational Choice; Labor Productivity J26 - Retirement; Retirement Policies J3 - Wages, Compensation, and Labor Costs J30 - General J31 - Wage Level and Structure; Wage Differentials J33 - Compensation Packages; Payment Methods J38 - Public Policy J4 - Particular Labor Markets J40 - General J42 - Monopsony; Segmented Labor Markets J44 - Professional Labor Markets; Occupational Licensing J45 - Public Sector Labor Markets J48 - Public Policy J49 - Other J5 - Labor-Management Relations, Trade Unions, and Collective Bargaining J50 - General J51 - Trade Unions: Objectives, Structure, and Effects J53 - Labor-Management Relations; Industrial Jurisprudence J6 - Mobility, Unemployment, Vacancies, and Immigrant Workers J60 - General J61 - Geographic Labor Mobility; Immigrant Workers J62 - Job, Occupational, and Intergenerational Mobility J63 - Turnover; Vacancies; Layoffs J64 - Unemployment: Models, Duration, Incidence, and Job Search J65 - Unemployment Insurance; Severance Pay; Plant Closings J68 - Public Policy J7 - Labor Discrimination J71 - Discrimination J78 - Public Policy J8 - Labor Standards: National and International J81 - Working Conditions J88 - Public Policy K - Law and Economics K0 - General K00 - General K1 - Basic Areas of Law K14 - Criminal Law K2 - Regulation and Business Law K3 - Other Substantive Areas of Law K31 - Labor Law K4 - Legal Procedure, the Legal System, and Illegal Behavior K40 - General K41 - Litigation Process K42 - Illegal Behavior and the Enforcement of Law L - Industrial Organization L0 - General L00 - General L1 - Market Structure, Firm Strategy, and Market Performance L10 - General L11 - Production, Pricing, and Market Structure; Size Distribution of Firms L13 - Oligopoly and Other Imperfect Markets L14 - Transactional Relationships; Contracts and Reputation; Networks L15 - Information and Product Quality; Standardization and Compatibility L16 - Industrial Organization and Macroeconomics: Industrial Structure and Structural Change; Industrial Price Indices L19 - Other L2 - Firm Objectives, Organization, and Behavior L21 - Business Objectives of the Firm L22 - Firm Organization and Market Structure L23 - Organization of Production L24 - Contracting Out; Joint Ventures; Technology Licensing L25 - Firm Performance: Size, Diversification, and Scope L26 - Entrepreneurship L3 - Nonprofit Organizations and Public Enterprise L33 - Comparison of Public and Private Enterprises and Nonprofit Institutions; Privatization; Contracting Out L4 - Antitrust Issues and Policies L40 - General L41 - Monopolization; Horizontal Anticompetitive Practices L42 - Vertical Restraints; Resale Price Maintenance; Quantity Discounts L5 - Regulation and Industrial Policy L50 - General L51 - Economics of Regulation L6 - Industry Studies: Manufacturing L60 - General L62 - Automobiles; Other Transportation Equipment; Related Parts and Equipment L63 - Microelectronics; Computers; Communications Equipment L66 - Food; Beverages; Cosmetics; Tobacco; Wine and Spirits L7 - Industry Studies: Primary Products and Construction L71 - Mining, Extraction, and Refining: Hydrocarbon Fuels L73 - Forest Products L8 - Industry Studies: Services L81 - Retail and Wholesale Trade; e-Commerce L83 - Sports; Gambling; Recreation; Tourism L84 - Personal, Professional, and Business Services L86 - Information and Internet Services; Computer Software L9 - Industry Studies: Transportation and Utilities L91 - Transportation: General L93 - Air Transportation L94 - Electric Utilities M - Business Administration and Business Economics; Marketing; Accounting; Personnel Economics M1 - Business Administration M11 - Production Management M12 - Personnel Management; Executives; Executive Compensation M14 - Corporate Culture; Social Responsibility M2 - Business Economics M21 - Business Economics M3 - Marketing and Advertising M31 - Marketing M37 - Advertising M4 - Accounting and Auditing M42 - Auditing M48 - Government Policy and Regulation M5 - Personnel Economics M50 - General M51 - Firm Employment Decisions; Promotions M52 - Compensation and Compensation Methods and Their Effects M53 - Training M54 - Labor Management N - Economic History N0 - General N00 - General N01 - Development of the Discipline: Historiographical; Sources and Methods N1 - Macroeconomics and Monetary Economics; Industrial Structure; Growth; Fluctuations N10 - General, International, or Comparative N11 - U.S.; Canada: Pre-1913 N12 - U.S.; Canada: 1913- N13 - Europe: Pre-1913 N17 - Africa; Oceania N2 - Financial Markets and Institutions N20 - General, International, or Comparative N22 - U.S.; Canada: 1913- N23 - Europe: Pre-1913 N3 - Labor and Consumers, Demography, Education, Health, Welfare, Income, Wealth, Religion, and Philanthropy N30 - General, International, or Comparative N31 - U.S.; Canada: Pre-1913 N32 - U.S.; Canada: 1913- N33 - Europe: Pre-1913 N34 - Europe: 1913- N36 - Latin America; Caribbean N37 - Africa; Oceania N4 - Government, War, Law, International Relations, and Regulation N40 - General, International, or Comparative N41 - U.S.; Canada: Pre-1913 N42 - U.S.; Canada: 1913- N43 - Europe: Pre-1913 N44 - Europe: 1913- N45 - Asia including Middle East N47 - Africa; Oceania N5 - Agriculture, Natural Resources, Environment, and Extractive Industries N50 - General, International, or Comparative N51 - U.S.; Canada: Pre-1913 N6 - Manufacturing and Construction N63 - Europe: Pre-1913 N7 - Transport, Trade, Energy, Technology, and Other Services N71 - U.S.; Canada: Pre-1913 N8 - Micro-Business History N82 - U.S.; Canada: 1913- N9 - Regional and Urban History N91 - U.S.; Canada: Pre-1913 N92 - U.S.; Canada: 1913- N93 - Europe: Pre-1913 N94 - Europe: 1913- O - Economic Development, Innovation, Technological Change, and Growth O1 - Economic Development O10 - General O11 - Macroeconomic Analyses of Economic Development O12 - Microeconomic Analyses of Economic Development O13 - Agriculture; Natural Resources; Energy; Environment; Other Primary Products O14 - Industrialization; Manufacturing and Service Industries; Choice of Technology O15 - Human Resources; Human Development; Income Distribution; Migration O16 - Financial Markets; Saving and Capital Investment; Corporate Finance and Governance O17 - Formal and Informal Sectors; Shadow Economy; Institutional Arrangements O18 - Urban, Rural, Regional, and Transportation Analysis; Housing; Infrastructure O19 - International Linkages to Development; Role of International Organizations O2 - Development Planning and Policy O23 - Fiscal and Monetary Policy in Development O25 - Industrial Policy O3 - Innovation; Research and Development; Technological Change; Intellectual Property Rights O30 - General O31 - Innovation and Invention: Processes and Incentives O32 - Management of Technological Innovation and R&D O33 - Technological Change: Choices and Consequences; Diffusion Processes O34 - Intellectual Property and Intellectual Capital O38 - Government Policy O4 - Economic Growth and Aggregate Productivity O40 - General O41 - One, Two, and Multisector Growth Models O43 - Institutions and Growth O44 - Environment and Growth O47 - Empirical Studies of Economic Growth; Aggregate Productivity; Cross-Country Output Convergence O5 - Economywide Country Studies O52 - Europe O53 - Asia including Middle East O55 - Africa P - Economic Systems P0 - General P00 - General P1 - Capitalist Systems P10 - General P16 - Political Economy P17 - Performance and Prospects P18 - Energy: Environment P2 - Socialist Systems and Transitional Economies P26 - Political Economy; Property Rights P3 - Socialist Institutions and Their Transitions P37 - Legal Institutions; Illegal Behavior P4 - Other Economic Systems P48 - Political Economy; Legal Institutions; Property Rights; Natural Resources; Energy; Environment; Regional Studies P5 - Comparative Economic Systems P51 - Comparative Analysis of Economic Systems Q - Agricultural and Natural Resource Economics; Environmental and Ecological Economics Q1 - Agriculture Q10 - General Q12 - Micro Analysis of Farm Firms, Farm Households, and Farm Input Markets Q13 - Agricultural Markets and Marketing; Cooperatives; Agribusiness Q14 - Agricultural Finance Q15 - Land Ownership and Tenure; Land Reform; Land Use; Irrigation; Agriculture and Environment Q16 - R&D; Agricultural Technology; Biofuels; Agricultural Extension Services Q2 - Renewable Resources and Conservation Q25 - Water Q3 - Nonrenewable Resources and Conservation Q32 - Exhaustible Resources and Economic Development Q34 - Natural Resources and Domestic and International Conflicts Q4 - Energy Q41 - Demand and Supply; Prices Q48 - Government Policy Q5 - Environmental Economics Q50 - General Q51 - Valuation of Environmental Effects Q53 - Air Pollution; Water Pollution; Noise; Hazardous Waste; Solid Waste; Recycling Q54 - Climate; Natural Disasters; Global Warming Q56 - Environment and Development; Environment and Trade; Sustainability; Environmental Accounts and Accounting; Environmental Equity; Population Growth Q58 - Government Policy R - Urban, Rural, Regional, Real Estate, and Transportation Economics R0 - General R00 - General R1 - General Regional Economics R11 - Regional Economic Activity: Growth, Development, Environmental Issues, and Changes R12 - Size and Spatial Distributions of Regional Economic Activity R13 - General Equilibrium and Welfare Economic Analysis of Regional Economies R2 - Household Analysis R20 - General R23 - Regional Migration; Regional Labor Markets; Population; Neighborhood Characteristics R28 - Government Policy R3 - Real Estate Markets, Spatial Production Analysis, and Firm Location R30 - General R31 - Housing Supply and Markets R38 - Government Policy R4 - Transportation Economics R40 - General R41 - Transportation: Demand, Supply, and Congestion; Travel Time; Safety and Accidents; Transportation Noise R48 - Government Pricing and Policy Z - Other Special Topics Z1 - Cultural Economics; Economic Sociology; Economic Anthropology Z10 - General Z12 - Religion Z13 - Economic Sociology; Economic Anthropology; Social and Economic Stratification Browse all content Browse content in More Content Advance Articles Editor's Choice Submit Author Guidelines Submission Site Open Access Options Self-Archiving Policy Why Submit? Purchase About About The Quarterly Journal of Economics Editorial Board Advertising and Corporate Services Journals Career Network Alerts Policies Dispatch Dates Contact Us Close Navbar Search Filter The Quarterly Journal of Economics This issue D31 - Personal Income, Wealth, and Their Distributions E10 - General E44 - Financial Markets and the Macroeconomy G10 - General G12 - Asset Pricing; Trading volume; Bond Interest Rates N10 - General, International, or Comparative Economics Books Journals Oxford Academic Enter search term Search Advanced Search Search Menu Article Navigation Close mobile search navigation Article Navigation Volume 134 Issue 3 August 2019 Article Contents Abstract I. Introduction II. A New Historical Global Returns Database III. Rates of Return: Aggregate Trends IV. Risky Rates of Return V. Safe Rates of Return VI. Risky versus Safe Returns VII. r versus g VIII. Conclusion Supplementary Material Footnotes ReferencesArticle Navigation Article Navigation Journal Article The Rate of Return on Everything, 1870–2015* Òscar Jordà, Òscar Jordà Federal Reserve Bank of San Francisco and University of California, Davis Search for other works by this author on: Oxford Academic Google Scholar Katharina Knoll, Katharina Knoll Deutsche Bundesbank Search for other works by this author on: Oxford Academic Google Scholar Dmitry Kuvshinov, Dmitry Kuvshinov University of Bonn Search for other works by this author on: Oxford Academic Google Scholar Moritz Schularick, Moritz Schularick University of Bonn and Centre for Economic Policy Research Search for other works by this author on: Oxford Academic Google Scholar Alan M Taylor Alan M Taylor University of California, Davis; National Bureau of Economic Research; and Centre for Economic Policy Research Search for other works by this author on: Oxford Academic Google Scholar The Quarterly Journal of Economics, Volume 134, Issue 3, August 2019, Pages 1225–1298, https://doi.org/10.1093/qje/qjz012 Published: 09 April 2019 PDF Split View Views Article contents Figures & tables Video Audio Supplementary Data Cite Cite Òscar Jordà, Katharina Knoll, Dmitry Kuvshinov, Moritz Schularick, Alan M Taylor, The Rate of Return on Everything, 1870–2015, The Quarterly Journal of Economics, Volume 134, Issue 3, August 2019, Pages 1225–1298, https://doi.org/10.1093/qje/qjz012 Select Format Select format .ris (Mendeley, Papers, Zotero) .enw (EndNote) .bibtex (BibTex) .txt (Medlars, RefWorks) Download citation Close Permissions Icon Permissions Share Icon Share Facebook Twitter LinkedIn Email Navbar Search Filter The Quarterly Journal of Economics This issue D31 - Personal Income, Wealth, and Their Distributions E10 - General E44 - Financial Markets and the Macroeconomy G10 - General G12 - Asset Pricing; Trading volume; Bond Interest Rates N10 - General, International, or Comparative Economics Books Journals Oxford Academic Mobile Enter search term Search Close Navbar Search Filter The Quarterly Journal of Economics This issue D31 - Personal Income, Wealth, and Their Distributions E10 - General E44 - Financial Markets and the Macroeconomy G10 - General G12 - Asset Pricing; Trading volume; Bond Interest Rates N10 - General, International, or Comparative Economics Books Journals Oxford Academic Enter search term Search Advanced Search Search Menu Abstract What is the aggregate real rate of return in the economy? Is it higher than the growth rate of the economy and, if so, by how much? Is there a tendency for returns to fall in the long run? Which particular assets have the highest long-run returns? We answer these questions on the basis of a new and comprehensive data set for all major asset classes, including housing. The annual data on total returns for equity, housing, bonds, and bills cover 16 advanced economies from 1870 to 2015, and our new evidence reveals many new findings and puzzles. I. Introduction What is the rate of return in an economy? It is a simple question, but hard to answer. The rate of return plays a central role in current debates on inequality, secular stagnation, risk premiums, and the decline in the natural rate of interest, to name a few. A main contribution of this article is to introduce a large new data set on the total rates of return for all major asset classes, including housing—the largest but oft-ignored component of household wealth. Our data cover most advanced economies—16 in all—starting in 1870. Although housing wealth is on average roughly one-half of national wealth in a typical economy (Piketty 2014), data on total housing returns (price appreciation plus rents) has been lacking (Shiller 2000 provides some historical data on house prices but not on rents). In this article we build on more comprehensive work on house prices (Knoll, Schularick, and Steger 2017) and newly constructed data on rents (Knoll 2017) to enable us to track the total returns of the largest component of the national capital stock. We further construct total returns broken down into investment income (yield) and capital gains (price changes) for four major asset classes, two of them typically seen as relatively risky—equities and housing—and two of them typically seen as relatively safe—government bonds and short-term bills. Importantly, we compute actual asset returns taken from market data and therefore construct more detailed series than returns inferred from wealth estimates in discrete benchmark years for a few countries, as in Piketty (2014). We also follow earlier work in documenting annual equity, bond, and bill returns, but here again we have taken the project further. We recompute all these measures from original sources, improve the links across some important historical market discontinuities (e.g., market closures and other gaps associated with wars and political instability), and in a number of cases we access new and previously unused raw data sources. In all cases, we have also brought in auxiliary sources to validate our data externally, and 100+ pages of online material documents our sources and methods. Our work provides researchers with the first broad noncommercial database of historical equity, bond, and bill returns—and the only database of housing returns—with the most extensive coverage across both countries and years.1 This article aims to bridge the gap between two related strands of the academic literature. The first strand is rooted in finance and is concerned with long-run returns on different assets. Dimson, Marsh, and Staunton (2009) probably marked the first comprehensive attempt to document and analyze long-run returns on investment for a broad cross-section of countries. Meanwhile, Homer and Sylla (2005) pioneered a multidecade project to document the history of interest rates. The second related strand of literature is the analysis of comparative national balance sheets over time, as in Goldsmith (1985). More recently, Piketty and Zucman (2014) have brought together data from national accounts and other sources tracking the development of national wealth over long time periods. They also calculate rates of return on capital by dividing aggregate capital income in the national accounts by the aggregate value of capital, also from national accounts. Our work is both complementary and supplementary to theirs. It is complementary because the asset price perspective and the national accounts approach are ultimately tied together by accounting rules and identities. Using market valuations, we are able to corroborate and improve the estimates of returns on capital that matter for wealth inequality dynamics. Our long-run return data are also supplementary to the work of Piketty and Zucman (2014) in the sense that we greatly extend the number of countries for which we can calculate real rates of return back to the late nineteenth century. The evidence we gathered can shed light on active research debates reaching from asset pricing to inequality. For example, in one contentious area of research, the accumulation of capital, the expansion of capital’s share in income, and the growth rate of the economy relative to the rate of return on capital all feature centrally in the current debate sparked by Piketty (2014) on the evolution of wealth, income, and inequality. What do the long-run patterns on the rates of return on different asset classes have to say about these possible drivers of inequality? In many financial theories, preferences over current versus future consumption, attitudes toward risk, and covariation with consumption risk all show up in the premiums that the rates of return on risky assets carry over safe assets. Returns on different asset classes and their correlations with consumption sit at the core of the canonical consumption Euler equation that underpins textbook asset pricing theory (see, e.g., Mehra and Prescott 1985). But tensions remain between theory and data, prompting further explorations of new asset pricing paradigms, including behavioral finance. Our new data add another risky asset class to the mix, housing, and with it come new challenges. In another strand of research triggered by the financial crisis, Summers (2014) seeks to revive the secular stagnation hypothesis first advanced in Alvin Hansen’s (1939) AEA presidential address. Demographic trends are pushing the world’s economies into uncharted territory as the relative weight of borrowers and savers is changing, and with it the possibility increases that the interest rate will fall by an insufficient amount to balance saving and investment at full employment. What is the evidence that this is the case? Last, in a related problem within the sphere of monetary economics, Holston, Laubach, and Williams (2017) show that estimates of the natural rate of interest in several advanced economies have gradually declined over the past four decades and are now near zero. What historical precedents are there for such low real rates that could inform today’s policy makers, investors, and researchers? The common thread running through these broad research topics is the notion that the rate of return is central to understanding long-, medium-, and short-run economic fluctuations. But which rate of return? And how do we measure it? For a given scarcity of funding supply, the risky rate is a measure of the profitability of private investment; in contrast, the safe rate plays an important role in benchmarking compensation for risk and is often tied to discussions of monetary policy settings and the notion of the natural rate. Below, we summarize our main findings. I.A. Main Findings We present four main findings: 1. On Risky Returns, rrisky In terms of total returns, residential real estate and equities have shown very similar and high real total gains, on average about 7% a year. Housing outperformed equities before World War II. Since World War II, equities have outperformed housing on average but had much higher volatility and higher synchronicity with the business cycle. The observation that housing returns are similar to equity returns, but much less volatile, is puzzling. Like Shiller (2000), we find that long-run capital gains on housing are relatively low, around 1% a year in real terms, and considerably lower than capital gains in the stock market. However, the rental yield component is typically considerably higher and more stable than the dividend yield of equities so that total returns are of comparable magnitude. Before World War II, the real returns on housing and equities (and safe assets) followed remarkably similar trajectories. After World War II this was no longer the case, and across countries equities then experienced more frequent and correlated booms and busts. The low covariance of equity and housing returns reveals that there could be significant aggregate diversification gains (i.e., for a representative agent) from holding the two asset classes. 2. On Safe Returns, rsafe We find that the real safe asset return (bonds and bills) has been very volatile over the long run, more so than one might expect, and often even more volatile than real risky returns. Each world war was (unsurprisingly) a moment of very low safe rates, well below zero. So was the 1970s stagflation. The peaks in the real safe rate took place at the start of our sample, in the interwar period, and during the mid-1980s fight against inflation. In fact, the long decline observed in the past few decades is reminiscent of the secular decline that took place from 1870 to World War I. Viewed from a long-run perspective, the past decline and current low level of the real safe rate today is not unusual. The puzzle may well be why the safe rate was so high in the mid-1980s rather than why has it declined ever since. Safe returns have been low on average in the full sample, falling in the 1%–3% range for most countries and peacetime periods. While this combination of low returns and high volatility has offered a relatively poor risk-return trade-off to investors, the low returns have also eased the pressure on government finances, in particular allowing for a rapid debt reduction in the aftermath of World War II. 3. On the Risk Premium, rrisky − rsafe Over the very long run, the risk premium has been volatile. Our data uncover substantial swings in the risk premium at lower frequencies that sometimes endured for decades and far exceed the amplitudes of business-cycle swings. In most peacetime eras, this premium has been stable at about 4%–5%. But risk premiums stayed curiously and persistently high from the 1950s to the 1970s, long after the conclusion of World War II. However, there is no visible long-run trend, and mean reversion appears strong. Interestingly, the bursts of the risk premium in the wartime and interwar years were mostly a phenomenon of collapsing safe returns rather than dramatic spikes in risky returns. In fact, the risky return has often been smoother and more stable than the safe return, averaging about 6%–8% across all eras. Recently, with safe returns low and falling, the risk premium has widened due to a parallel but smaller decline in risky returns. But these shifts keep the two rates of return close to their normal historical range. Whether due to shifts in risk aversion or to other phenomena, the fact that safe returns seem to absorb almost all of these adjustments seems like a puzzle in need of further exploration and explanation. 4. On Returns Minus Growth, rwealth − g Piketty (2014) argued that if investors’ return to wealth exceeded the rate of economic growth, rentiers would accumulate wealth at a faster rate and thus worsen wealth inequality. Using a measure of portfolio returns to compute “r minus g” in Piketty’s notation, we uncover an important finding. Even calculated from more granular asset price returns data, the same fact reported in Piketty (2014) holds true for more countries, more years, and more dramatically: namely, r ≫ g. In fact, the only exceptions to that rule happen in the years in or around wartime. In peacetime, r has always been much greater than g. In the pre-World War II period, this gap was on average 5% (excluding World War I). As of today, this gap is still quite large, about 3%–4%, though it narrowed to 2% in the 1970s before widening in the years leading up to the global financial crisis. One puzzle that emerges from our analysis is that while r minus g fluctuates over time, it does not seem to do so systematically with the growth rate of the economy. This feature of the data poses a conundrum for the battling views of factor income, distribution, and substitution in the ongoing debate (Rognlie 2015). The fact that returns to wealth have remained fairly high and stable while aggregate wealth increased rapidly since the 1970s suggests that capital accumulation may have contributed to the decline in the labor share of income over the recent decades (Karabarbounis and Neiman 2014). In thinking about inequality and several other characteristics of modern economies, the new data on the return to capital that we present here should spur further research. II. A New Historical Global Returns Database In this section, we discuss the main sources and definitions for the calculation of long-run returns. A major innovation is the inclusion of housing. Residential real estate is the main asset in most household portfolios, as we shall see, but so far very little has been known about long-run returns on housing. Our data on housing returns cover capital gains and imputed rents to owners and renters, the sum of the two being total returns.2 Equity return data for publicly traded equities are used, as is standard, as a proxy for aggregate business equity returns.3 The data include nominal and real returns on bills, bonds, equities, and residential real estate for Australia, Belgium, Denmark, Finland, France, Germany, Italy, Japan, the Netherlands, Norway, Portugal, Spain, Sweden, Switzerland, the United Kingdom, and the United States. The sample spans 1870 to 2015. Table I summarizes the data coverage by country and asset class. Table I Open in new tab Data Coverage Country Bills Bonds Equity Housing Australia 1870–2015 1900–2015 1870–2015 1901–2015 Belgium 1870–2015 1870–2015 1870–2015 1890–2015 Denmark 1875–2015 1870–2015 1873–2015 1876–2015 Finland 1870–2015 1870–2015 1896–2015 1920–2015 France 1870–2015 1870–2015 1870–2015 1871–2015 Germany 1870–2015 1870–2015 1870–2015 1871–2015 Italy 1870–2015 1870–2015 1870–2015 1928–2015 Japan 1876–2015 1881–2015 1886–2015 1931–2015 Netherlands 1870–2015 1870–2015 1900–2015 1871–2015 Norway 1870–2015 1870–2015 1881–2015 1871–2015 Portugal 1880–2015 1871–2015 1871–2015 1948–2015 Spain 1870–2015 1900–2015 1900–2015 1901–2015 Sweden 1870–2015 1871–2015 1871–2015 1883–2015 Switzerland 1870–2015 1900–2015 1900–2015 1902–2015 United Kingdom 1870–2015 1870–2015 1871–2015 1896–2015 United States 1870–2015 1871–2015 1872–2015 1891–2015 Country Bills Bonds Equity Housing Australia 1870–2015 1900–2015 1870–2015 1901–2015 Belgium 1870–2015 1870–2015 1870–2015 1890–2015 Denmark 1875–2015 1870–2015 1873–2015 1876–2015 Finland 1870–2015 1870–2015 1896–2015 1920–2015 France 1870–2015 1870–2015 1870–2015 1871–2015 Germany 1870–2015 1870–2015 1870–2015 1871–2015 Italy 1870–2015 1870–2015 1870–2015 1928–2015 Japan 1876–2015 1881–2015 1886–2015 1931–2015 Netherlands 1870–2015 1870–2015 1900–2015 1871–2015 Norway 1870–2015 1870–2015 1881–2015 1871–2015 Portugal 1880–2015 1871–2015 1871–2015 1948–2015 Spain 1870–2015 1900–2015 1900–2015 1901–2015 Sweden 1870–2015 1871–2015 1871–2015 1883–2015 Switzerland 1870–2015 1900–2015 1900–2015 1902–2015 United Kingdom 1870–2015 1870–2015 1871–2015 1896–2015 United States 1870–2015 1871–2015 1872–2015 1891–2015 Table I Open in new tab Data Coverage Country Bills Bonds Equity Housing Australia 1870–2015 1900–2015 1870–2015 1901–2015 Belgium 1870–2015 1870–2015 1870–2015 1890–2015 Denmark 1875–2015 1870–2015 1873–2015 1876–2015 Finland 1870–2015 1870–2015 1896–2015 1920–2015 France 1870–2015 1870–2015 1870–2015 1871–2015 Germany 1870–2015 1870–2015 1870–2015 1871–2015 Italy 1870–2015 1870–2015 1870–2015 1928–2015 Japan 1876–2015 1881–2015 1886–2015 1931–2015 Netherlands 1870–2015 1870–2015 1900–2015 1871–2015 Norway 1870–2015 1870–2015 1881–2015 1871–2015 Portugal 1880–2015 1871–2015 1871–2015 1948–2015 Spain 1870–2015 1900–2015 1900–2015 1901–2015 Sweden 1870–2015 1871–2015 1871–2015 1883–2015 Switzerland 1870–2015 1900–2015 1900–2015 1902–2015 United Kingdom 1870–2015 1870–2015 1871–2015 1896–2015 United States 1870–2015 1871–2015 1872–2015 1891–2015 Country Bills Bonds Equity Housing Australia 1870–2015 1900–2015 1870–2015 1901–2015 Belgium 1870–2015 1870–2015 1870–2015 1890–2015 Denmark 1875–2015 1870–2015 1873–2015 1876–2015 Finland 1870–2015 1870–2015 1896–2015 1920–2015 France 1870–2015 1870–2015 1870–2015 1871–2015 Germany 1870–2015 1870–2015 1870–2015 1871–2015 Italy 1870–2015 1870–2015 1870–2015 1928–2015 Japan 1876–2015 1881–2015 1886–2015 1931–2015 Netherlands 1870–2015 1870–2015 1900–2015 1871–2015 Norway 1870–2015 1870–2015 1881–2015 1871–2015 Portugal 1880–2015 1871–2015 1871–2015 1948–2015 Spain 1870–2015 1900–2015 1900–2015 1901–2015 Sweden 1870–2015 1871–2015 1871–2015 1883–2015 Switzerland 1870–2015 1900–2015 1900–2015 1902–2015 United Kingdom 1870–2015 1870–2015 1871–2015 1896–2015 United States 1870–2015 1871–2015 1872–2015 1891–2015 Like most of the literature, we examine returns to national aggregate holdings of each asset class. Theoretically, these are the returns that would accrue for the hypothetical representative-agent investor holding each country’s portfolio. An advantage of this approach is that it captures indirect holdings much better, although it leads to some double-counting, thereby boosting the share of financial assets over housing somewhat. The differences are described in Online Appendix O.4 II.A. The Composition of Wealth Figure I shows the decomposition of economy-wide investable assets and capital stocks, based on data for five major economies at the end of 2015: France, Germany, Japan, the United Kingdom, and the United States.5 Investable assets shown in the left panel of Figure I (and in Online Appendix Table A.23) exclude assets that relate to intrafinancial holdings and cannot be held directly by investors, such as loans, derivatives (apart from employee stock options), financial institutions’ deposits, and insurance and pension claims. Other financial assets mainly consist of corporate bonds and asset-backed securities. Other nonfinancial assets are other buildings, machinery and equipment, agricultural land, and intangible capital. The capital stock is business capital plus housing. Other capital is mostly made up of intangible capital and agricultural land. Data are sourced from national accounts and national wealth estimates published by the countries’ central banks and statistical offices.6 Figure I Composition of Investable Assets and Capital Stock in the Major Economies Color version of figure available online. Composition of total investable assets and capital stock. Average of the individual asset shares of France, Germany, Japan, the United Kingdom, and the United States, as of end-2015. Investable assets are defined as the gross total of economy-wide assets excluding loans, derivatives, financial institutions’ deposits, insurance, and pension claims. Other financial assets mainly consist of corporate bonds and asset-backed securities. Other nonfinancial assets are other buildings, machinery and equipment, agricultural land, and intangible capital. The capital stock is business capital plus housing. Other capital is mostly made up by intangible capital and agricultural land. Data are sourced from national accounts and national wealth estimates published by the countries’ central banks and statistical offices. Open in new tabDownload slide Housing, equity, bonds, and bills make up over half of all investable assets in the advanced economies today, and nearly two-thirds if deposits are included. The right panel of Figure I shows the decomposition of the capital stock into housing and various other nonfinancial assets. Housing is about one-half of the outstanding stock of capital. In fact, housing and equities alone represent over half of total assets in household balance sheets (see Online Appendix Figures A.5 and A.6). The main asset categories outside the direct coverage of this study are commercial real estate, business assets, and agricultural land; corporate bonds; pension and insurance claims; and deposits. But most of these assets represent claims of, or are closely related to, assets that we do cover. For example, pension claims tend to be invested in stocks and bonds; listed equity is a levered claim on business assets of firms; land and commercial property prices tend to comove with residential property prices; and deposit rates are either included in or are very similar to our bill rate measure.7 Our data also exclude foreign assets. Even though the data on foreign asset holdings are relatively sparse, the evidence that we do have—presented in Online Appendix O.4—suggests that foreign assets have, through history, only accounted for a small share of aggregate wealth, and the return differentials between domestic and foreign asset holdings are, with few exceptions, not that large. Taken together, this means that our data set almost fully captures the various components of the return on overall household wealth. II.B. Historical Returns Data 1. Bill Returns The canonical risk-free rate is taken to be the yield on Treasury bills, that is, short-term, fixed-income government securities. The yield data come from the latest vintage of the long-run macrohistory database (Jordà, Schularick, and Taylor 2017).8 Whenever data on Treasury bill returns were unavailable, we relied on either money market rates or deposit rates of banks from Zimmermann (2017). Because short-term government debt was rarely used and issued in the earlier historical period, much of our bill rate data before the 1960s actually consist of deposit rates.9 2. Bond Returns These are conventionally the total returns on long-term government bonds. Unlike earlier cross-country studies, we focus on the bonds listed and traded on local exchanges and denominated in local currency. This focus makes bond returns more comparable with the returns of bills, equities, and housing. Moreover, this results in a larger sample of bonds, and on bonds that are more likely to be held by the representative household in the respective country. For some countries and periods we have made use of listings on major global exchanges to fill gaps where domestic markets were thin or local exchange data were not available (for example, Australian bonds listed in New York or London). Throughout the sample we target a maturity of around 10 years. For the second half of the twentieth century, the maturity of government bonds is generally accurately defined. For the pre-World War II period we sometimes had to rely on data for perpetuals, that is, very long-term government securities (such as the British consol). Although as a convention we refer here to government bills and bonds as “safe” assets, both are naturally exposed to inflation and default risk, for example. In fact, real returns on these assets fluctuate substantially over time, as we shall see (specifically, Sections V and VI). 3. Equity Returns These returns come from a broad range of sources, including articles in economic and financial history journals, yearbooks of statistical offices and central banks, stock exchange listings, newspapers, and company reports. Throughout most of the sample, we aim to rely on indices weighted by market capitalization of individual stocks and a stock selection that is representative of the entire stock market. For some historical time periods in individual countries, however, we also make use of indices weighted by company book capital, by stock market transactions, or weighted equally due to limited data availability. 4. Housing Returns We combine the long-run house price series introduced by Knoll, Schularick, and Steger (2017) with a novel data set on rents drawn from the PhD thesis of Knoll (2017). For most countries, the rent series rely on the rent components of the cost of living of consumer price indices constructed by national statistical offices. We then combine them with information from other sources to create long-run series reaching back to the late nineteenth century. To proxy the total return on the residential housing stock, our returns include both rented housing and owner-occupied properties.10 Specifically, wherever possible we use house price and rental indices that include the prices of owner-occupied properties and the imputed rents on these houses. Imputed rents estimate the rent that an owner-occupied house would earn on the rental market, typically by using rents of similar houses that are rented. This means that, in principle, imputed rents are similar to market rents and are simply adjusted for the portfolio composition of owner-occupied as opposed to rented housing. Imputed rents, however, are not directly observed and hence are less precisely measured than market rents, and are typically not taxed.11 To the best of our knowledge, we are the first to calculate total returns to housing in the literature for as long and as comprehensive a cross section of economies as we report. 5. Composite Returns We compute the rate of return on safe assets, risky assets, and aggregate wealth, as weighted averages of the individual asset returns. To obtain a representative return from the investor’s perspective, we use the outstanding stocks of the respective asset in a given country as weights. To this end, we make use of new data on equity market capitalization (from Kuvshinov and Zimmermann 2018) and housing wealth for each country and period in our sample and combine them with existing estimates of public debt stocks to obtain the weights for the individual assets. A graphical representation of these asset portfolios and further description of their construction is provided in Online Appendix O.3. Tables A.28 and A.29 present an overview of our four asset return series by country, their main characteristics and coverage. The article comes with an extensive data appendix that specifies the sources we consulted and discusses the construction of the series in greater detail (see the Online Data Appendix, Sections U, V, and W for housing returns, and Section X for equity and bond returns). II.C. Calculating Returns The total annual return on any financial asset can be divided into two components: the capital gain from the change in the asset price P, and a yield component Y, that reflects the cash-flow return on an investment. The total nominal return R for asset j in country i at time t is calculated as: T o t a l r e t u r n : R i , t j = P i , t j − P i , t − 1 j P i , t − 1 j + Y i , t j . (1) Because of wide differences in inflation across time and countries, it is helpful to compare returns in real terms. Let 𝜋 𝑖 , 𝑡 = ( 𝐶 𝑃 𝐼 𝑖 , 𝑡 − 𝐶 𝑃 𝐼 𝑖 , 𝑡 − 1 ) 𝐶 𝑃 𝐼 𝑖 , 𝑡 − 1 be the realized consumer price index (CPI) inflation rate in a given country i and year t. We calculate inflation-adjusted real returns r for each asset class as, R e a l r e t u r n : r i , t j = 1 + R i , t j 1 + 𝜋 i , t − 1 . (2) These returns are summarized in period average form by country or for all countries. Investors must be compensated for risk to invest in risky assets. A measure of this “excess return” can be calculated by comparing the real total return on the risky asset with the return on a risk-free benchmark—in our case, the government bill rate, 𝑟 𝑖 , 𝑡 𝑏 𝑖 𝑙 𝑙 ⁠. We therefore calculate the excess return ER for the risky asset j in country i as E x c e s s r e t u r n : E R i , t j = r i , t j − r i , t b i l l . (3) In addition to individual asset returns, we present a number of weighted “composite” returns aimed at capturing broader trends in risky and safe investments and the “overall return” or “return on wealth.” Online Appendix O.3 provides further details on the estimates of country asset portfolios from which we derive country-year specific weights. For safe assets, we assume that total public debt is divided equally into bonds and bills since there are no data on their market shares (only for total public debt) over our full sample. As a result, we compute the safe asset return as: S a f e r e t u r n : r i , t s a f e = r i , t b i l l + r i , t b o n d 2 . (4) The risky asset return is calculated as a weighted average of the returns on equity and on housing. The weights w represent the share of asset holdings of equity and of housing stocks in the respective country i and year t, scaled to add up to 1. We use stock market capitalization and housing wealth to calculate each share and hence compute risky returns as: R i s k y r e t u r n : r i , t r i s k y = r i , t e q u i t y × w i , t e q u i t y + r t h o u s i n g × w i , t h o u s i n g . (5) The difference between our risky and safe return measures then provides a proxy for the aggregate risk premium in the economy: R i s k p r e m i u m : R P i , t = r i , t r i s k y − r i , t s a f e . (6) The “return on wealth” measure is a weighted average of returns on risky assets (equity and housing) and safe assets (bonds and bills). The weights w here are the asset holdings of risky and safe assets in the respective country i and year t, scaled to add to 1.12 R e t u r n o n w e a l t h : r i , t w e a l t h = r i , t r i s k y × w i , t r i s k y + r i , t s a f e × w i , t s a f e . (7) Finally, we also consider returns from a global investor perspective in Online Appendix I. There we measure the returns from investing in local markets in U.S. dollars (USD). These returns effectively subtract the depreciation of the local exchange rate vis-à-vis the dollar from the nominal return: U S D r e t u r n : R i , t j , U S D = 1 + R i , t j 1 + s ^ i , t − 1 , (8) where 𝑠 ^ 𝑖 , 𝑡 is the rate of depreciation of the local currency versus the U.S. dollar in year t. The real USD returns are then computed net of U.S. inflation πUS, t: R e a l U S D r e t u r n : r i , t j , U S D = 1 + R i , t j , U S D 1 + 𝜋 U S , t − 1 . (9) II.D. Constructing Housing Returns Using the Rent-Price Approach This section briefly describes our methodology to calculate total housing returns. We provide further details as needed in Section III.C and Online Appendix U. We construct estimates for total returns on housing using the rent-price approach. This approach starts from a benchmark rent-price ratio ( 𝑅 𝐼 0 𝐻 𝑃 𝐼 0 ) estimated in a baseline year (t = 0). For this ratio we rely on net rental yields from the Investment Property Database (IPD).13 We can then construct a time series of returns by combining separate information from a country-specific house price index series ( 𝐻 𝑃 𝐼 𝑡 𝐻 𝑃 𝐼 0 ) and a country-specific rent index series ( 𝑅 𝐼 𝑡 𝑅 𝐼 0 ) ⁠. For these indices, we rely on prior work on housing prices (Knoll, Schularick, and Steger 2017) and new data on rents (Knoll 2017). This method assumes that the indices cover a representative portfolio of houses. Under this assumption, there is no need to correct for changes in the housing stock, and only information about the growth rates in prices and rents is necessary. Hence, a time series of the rent-price ratio can be derived from forward and back projection as 𝑅 𝐼 𝑡 𝐻 𝑃 𝐼 𝑡 = [ ( 𝑅 𝐼 𝑡 𝑅 𝐼 0 ) ( 𝐻 𝑃 𝐼 0 𝐻 𝑃 𝐼 𝑡 ) ] 𝑅 𝐼 0 𝐻 𝑃 𝐼 0 . (10) In a second step, total returns on housing can then be computed as yield plus capital gains: 𝑅 𝑡 + 1 ℎ 𝑜 𝑢 𝑠 𝑖 𝑛 𝑔 = 𝑅 𝐼 𝑡 + 1 𝐻 𝑃 𝐼 𝑡 + 𝐻 𝑃 𝐼 𝑡 + 1 − 𝐻 𝑃 𝐼 𝑡 𝐻 𝑃 𝐼 𝑡 . (11) Our rent-price approach is sensitive to the choice of benchmark rent-price ratios and cumulative errors from year-by-year extrapolation. We verify and adjust rent-price approach estimates using a range of alternative sources. The main source for comparison is the balance sheet approach to rental yields, which calculates the rent-price ratio using national accounts data on total rental income and housing wealth. The “balance sheet” rental yield 𝑅 𝑌 𝑡 𝐵 𝑆 is calculated as the ratio of total net rental income to total housing wealth: 𝑅 𝑌 𝑡 𝐵 𝑆 = Net rental income 𝑡 Housing Wealth 𝑡 . (12) This balance sheet rental yield estimate can then be added to the capital gains series to compute the total return on housing from the balance sheet perspective. We also collect additional point-in-time estimates of net rental yields from contemporary sources, such as newspaper advertisements. These measures are less sensitive to the accumulated extrapolation errors in equation (10) but are themselves measured relatively imprecisely.14 Wherever the rent-price approach estimates diverge from these historical sources, we make adjustments to benchmark the rent-price ratio estimates to these alternative historical measures of the rental yield. We construct two additional housing return series—one benchmarked to all available alternative yield estimates, and another using primarily the balance sheet approach. The results of this exercise are discussed in Section III.C. Briefly, all the alternative estimates are close to one another, and the differences have little bearing on any of our results. III. Rates of Return: Aggregate Trends Our headline summary data appear in Table II and Figure II. The top panel of Table II shows the full sample (1870–2015) results, and the bottom panel shows results for the post-1950 sample. Note that here, and throughout the article, rates of return are always annualized. Units are always expressed in percent per year, for raw data and for means and standard deviations. All means are arithmetic means, except when specifically referred to as geometric means.15 Data are pooled and equally weighted, that is, they are raw rather than portfolio returns. We always include wars so that results are not polluted by bias from omitted disasters. We do, however, exclude hyperinflation years (but only a few) to focus on the underlying trends in returns and to avoid biases from serious measurement errors in hyperinflation years, arising from the impossible retrospective task of matching within-year timing of asset and CPI price level readings, which can create a spurious, massive under- or overstatement of returns in these episodes.16 Figure II Global Real Rates of Return Color version available online. Arithmetic average real returns p.a., unweighted, 16 countries. Consistent coverage within each country: each country-year observation used to compute the average has data for all four asset returns. Open in new tabDownload slide Table II Open in new tab Global Real ReturnsReal returns Nominal returnsBills Bonds Equity Housing Bills Bonds Equity Housing Panel A: Full sample Mean return p.a. 1.03 2.53 6.88 7.06 4.58 6.06 10.65 11.00 Standard deviation 6.00 10.69 21.79 9.93 3.32 8.88 22.55 10.64 Geometric mean 0.83 1.97 4.66 6.62 4.53 5.71 8.49 10.53 Mean excess return p.a.1.51 5.85 6.03 Standard deviation8.36 21.27 9.80 Geometric mean1.18 3.77 5.60 Observations 1,767 1,767 1,767 1,767 1,767 1,767 1,767 1,767 Panel B: Post-1950 Mean return p.a. 0.88 2.79 8.30 7.42 5.39 7.30 12.97 12.27 Standard deviation 3.42 9.94 24.21 8.87 4.03 9.81 25.03 10.14 Geometric mean 0.82 2.32 5.56 7.08 5.31 6.88 10.26 11.85 Mean excess return p.a.1.91 7.42 6.54 Standard deviation9.21 23.78 9.17 Geometric mean1.51 4.79 6.18 Observations 1,022 1,022 1,022 1,022 1,022 1,022 1,022 1,022Real returns Nominal returnsBills Bonds Equity Housing Bills Bonds Equity Housing Panel A: Full sample Mean return p.a. 1.03 2.53 6.88 7.06 4.58 6.06 10.65 11.00 Standard deviation 6.00 10.69 21.79 9.93 3.32 8.88 22.55 10.64 Geometric mean 0.83 1.97 4.66 6.62 4.53 5.71 8.49 10.53 Mean excess return p.a.1.51 5.85 6.03 Standard deviation8.36 21.27 9.80 Geometric mean1.18 3.77 5.60 Observations 1,767 1,767 1,767 1,767 1,767 1,767 1,767 1,767 Panel B: Post-1950 Mean return p.a. 0.88 2.79 8.30 7.42 5.39 7.30 12.97 12.27 Standard deviation 3.42 9.94 24.21 8.87 4.03 9.81 25.03 10.14 Geometric mean 0.82 2.32 5.56 7.08 5.31 6.88 10.26 11.85 Mean excess return p.a.1.91 7.42 6.54 Standard deviation9.21 23.78 9.17 Geometric mean1.51 4.79 6.18 Observations 1,022 1,022 1,022 1,022 1,022 1,022 1,022 1,022 Notes. Annual global returns in 16 countries, equally weighted. Period coverage differs across countries. Consistent coverage within countries: each country-year observation used to compute the statistics in this table has data for all four asset returns. Excess returns are computed relative to bills. Table II Open in new tab Global Real ReturnsReal returns Nominal returnsBills Bonds Equity Housing Bills Bonds Equity Housing Panel A: Full sample Mean return p.a. 1.03 2.53 6.88 7.06 4.58 6.06 10.65 11.00 Standard deviation 6.00 10.69 21.79 9.93 3.32 8.88 22.55 10.64 Geometric mean 0.83 1.97 4.66 6.62 4.53 5.71 8.49 10.53 Mean excess return p.a.1.51 5.85 6.03 Standard deviation8.36 21.27 9.80 Geometric mean1.18 3.77 5.60 Observations 1,767 1,767 1,767 1,767 1,767 1,767 1,767 1,767 Panel B: Post-1950 Mean return p.a. 0.88 2.79 8.30 7.42 5.39 7.30 12.97 12.27 Standard deviation 3.42 9.94 24.21 8.87 4.03 9.81 25.03 10.14 Geometric mean 0.82 2.32 5.56 7.08 5.31 6.88 10.26 11.85 Mean excess return p.a.1.91 7.42 6.54 Standard deviation9.21 23.78 9.17 Geometric mean1.51 4.79 6.18 Observations 1,022 1,022 1,022 1,022 1,022 1,022 1,022 1,022Real returns Nominal returnsBills Bonds Equity Housing Bills Bonds Equity Housing Panel A: Full sample Mean return p.a. 1.03 2.53 6.88 7.06 4.58 6.06 10.65 11.00 Standard deviation 6.00 10.69 21.79 9.93 3.32 8.88 22.55 10.64 Geometric mean 0.83 1.97 4.66 6.62 4.53 5.71 8.49 10.53 Mean excess return p.a.1.51 5.85 6.03 Standard deviation8.36 21.27 9.80 Geometric mean1.18 3.77 5.60 Observations 1,767 1,767 1,767 1,767 1,767 1,767 1,767 1,767 Panel B: Post-1950 Mean return p.a. 0.88 2.79 8.30 7.42 5.39 7.30 12.97 12.27 Standard deviation 3.42 9.94 24.21 8.87 4.03 9.81 25.03 10.14 Geometric mean 0.82 2.32 5.56 7.08 5.31 6.88 10.26 11.85 Mean excess return p.a.1.91 7.42 6.54 Standard deviation9.21 23.78 9.17 Geometric mean1.51 4.79 6.18 Observations 1,022 1,022 1,022 1,022 1,022 1,022 1,022 1,022 Notes. Annual global returns in 16 countries, equally weighted. Period coverage differs across countries. Consistent coverage within countries: each country-year observation used to compute the statistics in this table has data for all four asset returns. Excess returns are computed relative to bills. The first key finding is that residential real estate, not equity, has been the best long-run investment over the course of modern history. Although returns on housing and equities are similar, the volatility of housing returns is substantially lower, as Table II shows. Returns on the two asset classes are in the same ballpark—around 7%—but the standard deviation of housing returns is substantially smaller than that of equities (10% for housing versus 22% for equities). Predictably, with thinner tails, the compounded return (using the geometric average) is vastly better for housing than for equities—6.6% for housing versus 4.7% for equities. This finding appears to contradict one of the basic tenets of modern valuation models: higher risks should come with higher rewards. Differences in asset returns are not driven by unusual events in the early pre-World War II part of the sample. Table II, Panel B makes this point. Compared to the full sample results in the top panel, the same clear pattern emerges: stocks and real estate dominate in terms of returns. Moreover, average returns post–1950 are similar to those for the full sample even though the postwar subperiod excludes the devastating effects of the two world wars. Robustness checks are reported in Online Appendix Figures A.1, A.2, and A.3. Briefly, the observed patterns are not driven by the smaller European countries in our sample. Figure A.1 shows average real returns weighted by country-level real GDP, for the full sample and the post-1950 period. Compared to the unweighted averages, equity performs slightly better, but the returns on equity and housing remain very similar, and the returns and riskiness of all four asset classes are very close to the unweighted series in Table II. The results could be biased due to the country composition of the sample at different dates given data availability. Online Appendix Figure A.2 plots the average returns for sample-consistent country groups, starting at benchmark years—the later the benchmark year, the more countries we can include. Again, the broad patterns discussed above are largely unaffected. We also investigate whether the results are biased due to the world wars. Online Appendix Figure A.3 plots the average returns in this case. The main result remains largely unchanged. Online Appendix Table A.3 also considers the risky returns during wartime in more detail to assess the evidence for rare disasters in our sample. Returns during both wars were indeed low and often negative, although returns during World War II in a number of countries were relatively robust. Finally, our aggregate return data take the perspective of a domestic investor in a representative country. Online Appendix Table A.14 instead takes the perspective of a global USD-investor and assesses the USD value of the corresponding returns. The magnitude and ranking of returns are similar to those reported in Table II, although the volatilities are substantially higher. This is to be expected given that the underlying asset volatility is compounded by the volatility in the exchange rate. We also find somewhat higher levels of USD returns, compared with those in local currency. What comes next in our discussion of raw rates of return? We look more deeply at risky rates of return, and delve into their time trends and the decomposition of housing and equity returns into the capital gain and yield components in greater detail in",
    "commentLink": "https://news.ycombinator.com/item?id=40650326",
    "commentBody": "The Rate of Return on Everything, 1870–2015 (2019) (oup.com)171 points by lazyjeff 22 hours agohidepastfavorite125 comments UniverseHacker 21 hours agoI don't understand how housing can increase in cost in a stable steady manner, as a fraction of household income over long periods of time like more than 100 years. It seems to defy logic, so it makes me suspect how it is being calculated when people claim that housing costs have gone up by massive amounts. Since only a small increase would price a large number of people out of the market- it seems logical that housing can't really increase in cost/value over long time spans, but must track the overall economy almost exactly. reply georgeecollins 21 hours agoparentTo help you conceptualize how that is possible: 100 years ago the world population was 2 billion, and now it is 8 billion. While the housing stock is also increasing with that population growth, the actual amount of desirable land does not grow as fast. That's why -- for example-- the US gov't in the 1850s could just hand out 40 acre plots of land to people. They can still do that, but it has to be way out in Alaska or something. A hundred years ago it took much more labor to produce enough food to feed a person. Before the industrial revolution let's say 90% of all people were farmers. In 1850 in the US that was maybe 50% of all people were farmers. So the % of GDP going to food was much higher. Now 1-2 people can feed 100 in the west. That means less of your income proportionately goes to food. Similar declines in the amount of labor required to produce a thing are happening in manufactured goods. So it may have once taken hundreds of hours of human labor to build a car, but now it takes much fewer. So the wealth of everyone is going up faster than the supply of desirable land. That does mean people are getting priced out. But also people find ways to live on less land. Before the industrial revolution most families needed a farm to survive. Now many, many families can live in an apartment building in a city that takes way less land. reply throw0101d 21 hours agorootparent> While the housing stock is also increasing with that population growth, the actual amount of desirable land does not grow as fast. \"Buy land, they're not making it anymore.\" — Mark Twain reply gamepsys 19 hours agorootparentprev> So the wealth of everyone is going up faster than the supply of desirable land We are also slowing down the rate we are making land desirable. During the 50s-70s we had massive success by creating suburbs that were connected to city centers. Due to how geography works, new suburbs are further from city centers and inherently less desirable. We have developed most of the geographically appealing regions of the US. The regions that can support more sprawl are seeing growth at a high enough rate that local infrastructure such as schools, roads, water, and construction labor is being stressed. In the not too distant future we will have climate change that will displace people and a declining population that will reduce housing demand. Combine that with new building techniques/materials and in 100 years the housing situation will be totally different than today. reply mapt 13 hours agorootparent> Due to how geography works, new suburbs are further from city centers and inherently less desirable. We have developed most of the geographically appealing regions of the US. \"We have turned all of the areas within 15 miles of the existing urban development into suburbs, while banning nearly all new urban development, and banning nearly all urbanization of existing suburbs. We've run out of cities to build suburbs around and this seems like a natural limit where a Mad Max style fight for a limited suburban housing stock is our inevitable outcome. This is Basic Geography 101.\" That Mad Max style fight has already been won by geriatric white men (PostWar Boys) who paid off their houses 20 years ago, who are now nominal multimillionaires, and whose preferences appear to largely control national, state, and local political outcomes. reply Seanambers 16 hours agorootparentprevCompared to Europe the US has an unimaginable amount of land. Europeans don't know this really - no one has told them, because they never went to the US, the ones who did went for some coastal city. On the other side Americans don't realize the reasons why Europeans live in cities. Europe is much much smaller and with less desirable land to begin with and then there is the entire \"walkable cities\" which is utopia reasoning... God damn you US ppl dont know how good you got it. reply derf_ 16 hours agorootparentThe continental US is 8.08 million km^2 [0], while Europe is 10.18 million km^2 [1], almost 26% larger. Even adding the non-contiguous regions does not make the US bigger (still just 9.83 million km^2) [2]. [0] https://en.wikipedia.org/wiki/Contiguous_United_States [1] https://en.wikipedia.org/wiki/Europe [2] https://en.wikipedia.org/wiki/United_States reply doubled112 15 hours agorootparentI've always found it interesting that Canada and California have a similar population, although I haven't looked those up in a couple of years. Canada has a couple more km^2. reply abduhl 15 hours agorootparentprevSubtract out Russia from this calculation and “Europe” is 75% the size of the US. When people talk about “Europe” and Europeans not understanding how big the US is they are not talking about Eastern Europe/Russia, they’re talking about Portugal to Poland, maybe to Ukraine. Sometimes they’re not even thinking of the Nordic countries which also have a lot of land. This of course invites the counter argument that the US coastal areas are just as dense and small feeling as Central Europe. reply defrost 15 hours agorootparentFor another comparison, the state of Australia I live in has 3x the land area of Texas, a current population of 2.9 million that largely live in the one concentrated urban area, farms and cattle stations large than those in the US, and had people walk out of the desert than had never met or heard of non indigenous people until that point in time in the mid 1980s. I grew up in one of the more remote corners when the state population was less than 800 thousand and the local population within a few hours drive was barely a thousand or more.. reply thbb123 20 hours agorootparentprevTo add to that, a house a hundred years ago was nothing like a house today: building codes, square footage per person, heating, plumbing, connectivity... Building and maintaining a decent housing unit is far more expensive in material and energy than it was 60 years ago. reply lotsofpulp 18 hours agorootparentTo add to that, household size also decreased. https://www.pewresearch.org/short-reads/2019/10/01/the-numbe... reply rootusrootus 14 hours agorootparentThis is the one I'm curious to see play out. How low can it go? Did it reach bottom? Maybe once it stabilizes at a new norm, housing supply will catch up and prices will settle down. reply charlie0 15 hours agorootparentprevYet, almost no one makes those sweet starter homes anymore. reply lotsofpulp 14 hours agorootparentNewer homes have been trending smaller and smaller for a long time. Especially in the western US. https://www.washingtonpost.com/business/2024/03/10/smaller-n... > Median new-home sizes are at a 13-year low. reply charlie0 13 hours agorootparentI must not be looking at the right place then. All the new homes I've seen recently are 2000+ sq. ft. The only ones that are smaller are usually townhomes and are even less affordable than the bigger homes I just mentioned they are located near downtowns or other heavily populated areas. reply amanaplanacanal 10 hours agorootparentNo you are still right. The article says the new smaller homes average 2179 square feet. reply lotsofpulp 6 hours agorootparentprevHere are some at not much more than 1k sq ft: https://www.mihomes.com/new-homes/texas/greater-san-antonio/... https://ginngrp.com/for-sale/parkhouse-vista/ https://www.zillow.com/homedetails/812-24th-Avenue-S-Seattle... > The only ones that are smaller are usually townhomes and are even less affordable than the bigger homes I just mentioned they are located near downtowns or other heavily populated areas. That will not change because the market of people looking to buy a 1k sq ft home on a quarter acre lot in a less populated area either cannot afford the construction costs or the set of buyers interested in that is too small to bet on for a developer. reply Retric 20 hours agoparentprevPeople get much larger houses today because they can afford much larger houses. This comes from both increased prosperity and having fewer kids. 1950s: The average new home sold for $82,098. It had 983 square feet of floor space and a household size of 3.37 people, or 292 square feet per person. 2010s: The average new home ($292,700) offers 924 square feet per person (2.59 people per household, 2,392 total square feet) — three times the space afforded in the 1950s. https://compasscaliforniablog.com/have-american-homes-change... reply PostOnce 19 hours agorootparentAround here, they're building bigger houses, but also on much smaller plots without gardens or dogs or tree-houses or places for kids to play outdoors. I suspect it's the same in a lot of places. Maybe these square-foot-per-person calculations should also include square feet of land. reply travisb 19 hours agorootparentIf you are going to do that, other play grounds should also be counted: the forests and fields which used to be nearby, the per-child space in nearby parks, etc. All these are places children used to go unsupervised, but now mostly don't. reply Retric 17 hours agorootparentprevMost people today don’t want a large back yard. It’s like an outdoor pool, there’s definitely a market but adding a pool can lower your property’s value. Land doesn’t decrease the value of a property, but it can dramatically lower the number of buyers. reply DontchaKnowit 5 hours agorootparentIf this is true, it is so insanely contrary to the opinion of literally every single person I have ever met. Everyone wants a yard. Most people want a sizeable yard for their dogs or kids, or to drink beer and have campfires. I grew up in rural midwest USA. maybe opinions are different elsewhere, but your comment is a bit baffling. Ive never in my life heard someone say \"that house would be perfect if it just came with less land\" reply Retric 4 hours agorootparentI know several people who decided to get smaller yards or were happy to move into apartment complexes without them. We both are dealing with biased samples. A childhood friend who is very into the outdoors moved to a rural area and got 14 acres, everyone else moved to an apartment or a house with under an acre. This stuff shows up on migration patterns, and well there’s a reason the Rural Midwest USA is a small percentage of the US population. So, I agree many people do want a large yard, but revealed preferences suggest it’s a minority opinion. reply qazxcvbnm 3 hours agorootparentRevealed preferences are only as revealing as the market is competitive. In monopolies, is it not revealed that the populace prefers to be price-gouged? In a market as regulated as housing and with deep ties to policy (and industrial structures that I have heard about which I do not understand), it's difficult to not be sceptical of revealed preferences as a sole convincing explanation. reply martincmartin 16 hours agorootparentprev$82,098 in 1955, adjusted for inflation, was worth $727,502.05[1]. So homes were actually much more expensive back then. [1] https://data.bls.gov/cgi-bin/cpicalc.pl?cost1=82098&year1=19... reply acchow 15 hours agorootparentNo, they already adjusted for inflation. In nominal dollars, a home in 1955 would be under $10k https://fred.stlouisfed.org/series/MSPUS reply ffsm8 16 hours agorootparentprevI wish I new how that article got to 80k for a house in 1955.. I couldn't find a link to trust \"interactive visualisation\" they're citing and Google didn't help me either. I wasn't born then, so no personal experience and everything I could find pointed more towards 5-10k, i.e. this advertisement https://i.redd.it/75sc90f58qsa1.jpg That'd be safely below 100k, inflation adjusted... Actually, maybe that 80k is already inflation adjusted? The number is pretty close reply rootusrootus 14 hours agorootparentprevThat can't be right, it must already be an inflation-adjusted number. My parents bought their most recent house in ~1970 for $30K, and that was a bit above the median for the area. reply nostrademons 17 hours agoparentprevA few ways that housing can continue to increase as a percentage of income: 1.) Go from a single income supporting to a house to multiple incomes supporting a house. This is actually happening, with the shift from single-earner households in the 1950s to dual-income households to groups of unmarried professionals living together as roommates. The preponderance of post-1950s data can also overweight this effect: historically, the norm was far large extended families to live together in one household, and the single-earner U.S. nuclear family was an aberration created by suburbanization. 2.) Have reset points. How can real estate continue to return 7% real returns for 120 years? Well, return 7% for 40 years, at which point your investment is up 15x. Then bomb the house and kill the owner. Then give the land to the guys who helped you kill the owner at a low, low price, help them build a house on it cheap, and start the 7% appreciation calculator over again from a new low baseline. This is also very close to what actually happened over those 120 years, between WW1, WW2, the Russian/Ottoman/Austrian revolutions, the fall of the Warsaw Pact, the Chinese Civil War / Cultural Revolution / Great Famine, and so on. 3.) Have a smaller percentage of people owning houses. The average housing price can absolutely escape the confines of the median income, if the median person does not own a house. The study's use of rent and imputed rent partially controls for this, but the broad answer for \"Housing prices can't outstrip incomes forever, can they?\" is \"Sure they can, if nobody can afford houses.\" 4.) Have more people. If you're talking the returns to an asset class, and you own all that asset class, and then suddenly there are more people that need that asset, you're going to make money. This, to a large degree, actually happened during those 120 years. reply tim333 21 hours agoparentprevThey don't. The numbers in the article include imputed rents. If you plot house prices against incomes they do a wave pattern which is high at the moment. But they were even higher against income in 1845. Article here has data from 1845 for the UK https://archive.ph/FRzaA reply cyberax 18 hours agoparentprev> I don't understand how housing can increase in cost in a stable steady manner It's the density death spiral. Dense housing gets more expensive (yes, dense housing IS more expensive!), that in turn drives even more density. The only way to fix it? Promote suburbs and smaller cities. There is literally _no_ other fix. reply m463 16 hours agorootparentI sort of think there are other things still at work here. People still live near infrastructure (although requirements change) Maybe 1000 years ago, they probably lived near a port, water source and source of food. Nowadays we have efficient transport with semis and container ships, but I would imagine that there are still things like say power infrastructure, sources of commerce and schools and even internet that make living practical and affordable. I'll bet as we can do distributed living practically (PV+batteries, wells/septic , starlink and amazon) that maybe more things are possible. reply cyberax 14 hours agorootparentNot really. Pretty much all of the housing price increase (above the overall average market growth rate) is explained by the density death spiral. reply rootusrootus 14 hours agorootparentprevI wonder if Texas, especially around Dallas, is settling into a pattern that'll work well. If you have enough land, that is. Instead of single big city with dense downtown surrounded by progressively less dense housing, there are multiple cores. Smaller cities, close enough that their suburbs mix. On the surface that does seem more workable than building mega-cities. reply cyberax 14 hours agorootparentYES! That's exactly the right model. Sparse cities with distributed industry. Houston (Greater Houston Area) is another such example. It's geographically huge, but most commutes are fairly short. This allows Houston to have faster commutes than NYC, despite having a comparable population, and VASTLY better living conditions. reply jsnell 12 hours agorootparentprevThat's called a conurbation. I think it's how most of the mega-cities in the world outside of North America formed in practice. For a case with no primary core, have a look at the Ruhr area. reply JumpCrisscross 18 hours agorootparentprev> yes, dense housing IS more expensive Dense housing is more expensive. Dense living massively less so. reply cyberax 14 hours agorootparentIt's actually the inverse. The raw construction cost of a housing unit in a dense building is cheap. Living there is not. People in dense cities spend a larger percentage of their paycheck on housing than people in sparse cities ( https://smartasset.com/mortgage/housing-spending-2021 ). reply rufus_foreman 6 hours agorootparent>> The raw construction cost of a housing unit in a dense building is cheap Not true. The cost per foot of construction of a single family home is lower than that of a small apartment building. A small apartment building has a lower cost per square foot than a mid-rise apartment. And a mid-rise apartment building has a lower cost per square foot than a high-rise apartment building. Taking only construction costs into account, when land is cheap, single family homes make the most financial sense, and dense housing doesn't make sense. Well, except for manufactured homes. They have a lower construction cost per square foot than single family homes, but when most people talk about density they aren't talking about trailer parks. reply cyberax 1 hour agorootparent> The cost per foot of construction That's why I specifically said that a per-unit cost is lower in cities. But units tend to become smaller and smaller over time. > Taking only construction costs into account, when land is cheap, single family homes make the most financial sense, and dense housing doesn't make sense. Absolutely. reply javednissar 17 hours agorootparentprevI don’t understand the logic here, the land itself will be valuable regardless of whether or not there is a house there or an apartment building. The principal component of the cost of buying a house or a condo is the associated land cost. Building more density gives more people access to that land. If your contention is that more housing is bad because it draws more economic activity which raises land values then that’s fair but this is the same as arguing for less freedom of movement. Essentially any nation that adopts your policy prescription is taking a step towards turning into the Soviet Union. That may sound like hyperbole but that is the end result of NIMBYism and illiberal land use policy. reply cyberax 14 hours agorootparentIt's the \"spiral\" part (i.e. self-reinforcing vicious process). Dense cities allow employers to get access to a larger pool of workers, giving them a competitive advantage. This in turn makes cities more attractive for workers. Since land area is conserved and people won't commute for much more than 30 minutes, it means cities have to increase the density. This in turn makes cities more attractive for employers, driving the demand for housing even higher. Rinse, wash, repeat. > That may sound like hyperbole but that is the end result of NIMBYism and illiberal land use policy. There is literally no city in Japan, US or in major EU countries that managed to build its way out of high housing prices. Not a single one. reply sdwr 21 hours agoparentprevHousing has elastic demand, right? Based on prices, you can have roommates, children can live with parents, etc. Real housing costs could double tomorrow, and people would survive (not happily). reply carlosjobim 20 hours agorootparentWith extreme consequences to the population. When people are not given space to create their own families, the result is the massive population decrease we're living through now in industrialised nations. reply bombcar 21 hours agoparentprevThere have been two major real housing price jumps that I know of, and both are correlated with significant household income increases (at least nominal). Almost everything else can be factored into changes in what the \"nominal house\" is - from a one room cabin without plumbing to a McMansion with a three car garage. One was the great urbanization post-world wars and the other was the great increase in dual-income households. But if you factor things out and try to correct for as many variables as you can, housing is pretty \"steady state\" though the percentage of income directed toward it that's acceptable has crept up somewhat. Shelter is, like food, one of the few real necessities and so it will be bid up to the point of pain or worse if there is a scarcity. reply UniverseHacker 20 hours agorootparentFair point, in that sense it seems like some fairly fixed step-ups are possible where people culturally decide to spend more of their income on housing, but it cannot be a steady trend to profit from as an investor, because it will always have a hard cap at 100% of household income. It can't steadily beat inflation over long time scales. reply JumpCrisscross 19 hours agorootparent> It can't steadily beat inflation over long time scales Of course it can. That’s what productivity means. The value could keep going up even amidst the fraction of incomes being spent on it going down. reply Too 14 hours agoparentprevTrust in institutions and low interest rates allows people today to afford houses 10x more expensive than their tangible savings. reply inglor_cz 3 hours agoparentprevIs the housing price adjusted to sqms / sqft and equipment / amenities? Because my grandparents with their two kids and great-grandma literally lived in a one-room studio in the 1950s, even though they weren't poor. (Grandpa was an engineer for state railways, a good job in 1950s Czechoslovakia.) The toilet was common for 3 households. That was just the standard of living back then. You wanted to be warm, you had to drag coal upstairs from the basement and feed the home furnace. Daily. In 1964, all five moved to a 3-room apartment, which increased their living space a lot, but it would still feel incredibly cramped by today's standards. It had a separate toilet just for them, though, and centralized heating. No more hauling coal upstairs. Progress! Of course that those smaller, more primitive apartments were much cheaper than the house built in 2022 that I am now living in, and that is stuffed full with various sophisticated devices. reply llm_trw 19 hours agoparentprev>It seems to defy logic There are more people than ever and the surface area of the planet hasn't increased. reply thfuran 18 hours agorootparentThe surface area of the planet is pretty much entirely unrelated to the cost of housing. reply adammarples 9 hours agorootparentIts variance is though. It is all owned, and controlled. If there was a new land rush then new housing would be almost free until the land was all claimed. reply thfuran 6 hours agorootparentBuilding a second Australia in the middle of the Pacific would have very little effect on the price of housing in London. reply bluGill 20 hours agoparentprevright out of adam smith, when people get more money they typically spend it on better housing. reply UniverseHacker 20 hours agorootparentI'm talking about in proportion to income... for example, if people spend 30% of household income on housing, you cannot have an order of magnitude increase in housing prices over any time scale as it will always have a hard cap at 100%. reply bluGill 19 hours agorootparentInflation is a factor as well. reply pessimizer 20 hours agoparentprev> I don't understand how housing can increase in cost in a stable steady manner, as a fraction of household income over long periods of time like more than 100 years. It hasn't. House prices have been stable for hundreds of years. They're currently being used as financial vehicles, and as another government asset inflation to ward off that pesky balance of accounts reckoning, but they'll be back down eventually. Rents are different, probably because landlords collude. Or irrational exuberance or whatever. Times when everybody suddenly agrees that housing is worth a lot more, for no particular reason. Some guy here (https://www.reddit.com/r/Economics/comments/sq1pb/graph_of_c...) plotted the 2000s housing bubble vs. inflation-predicted price. I would say that the fact that we didn't see a dip after the bubble makes it pretty obvious that if you deal in financial instruments around houses rather than houses themselves (including rents), there had to be a lot of money made that never came back. Renters never got a refund of the inflated rent that they paid during the time of those inflated house prices; that seems like it would account for the 6.6% a year that this paper claims as the return on owning housing. Because the buying and selling of houses is ultimately going to be a wash. That says to me that housing bubbles are required in order to make any money from housing. That money will be supplied by renters and overextended owners who can't buy when prices return to the ground, and can't hold out until the next bubble. reply cmrdporcupine 16 hours agorootparentI'd like to see that same chart but for places like here in Canada that did not have a housing price correction in the 2008 era. It just went up up up, and the lines would show divergence. And of course if that chart continued into 2022... reply smoovb 21 hours agoprevA few of the links to the 5 other times this has been posted: https://news.ycombinator.com/item?id=16078059 on Jan 5, 2018 https://news.ycombinator.com/item?id=19817584 on May 5, 2019 reply smarm52 18 hours agoprev> In fact, the long decline observed in the past few decades is reminiscent of the secular decline that took place from 1870 to World War I. > The fact that returns to wealth have remained fairly high and stable while aggregate wealth increased rapidly since the 1970s suggests that capital accumulation may have contributed to the decline in the labor share of income over the recent decades (Karabarbounis and Neiman 2014). Predicted here: Piketty, T. (2014). Capital in the twenty-first century. Harvard University Press. > In terms of total returns, residential real estate and equities have shown very similar and high real total gains, on average about 7% a year. > Housing, equity, bonds, and bills make up over half of all investable assets in the advanced economies today, and nearly two-thirds if deposits are included. Interesting, Housing is marked as \"risky\", and yet heavily invested. Investors are over leveraged in risky investments. They probably do it because controlling housing nets them power above and beyond normal returns. I wonder if this part of the reason for the \"boom and bust\" of market economies in the West when proper government regulation is removed. The riskiness of much of the investment of most investors may lead to sudden losses and shifts in risk, which may result in them withdrawing capital to \"safer\" investments, thus triggering a \"bust\". And `r ≫ g` shows why the wealthy can wield so much power. Holding capital hostage to regulate economic growth and control it is very powerful, and why they can exercise the kind of control they can. reply JackYoustra 15 hours agoparentIt's actually one of the big problems with Capital in the 21st century: if you strip out housing, rsince that sucks capital out of the rest of the economy, we're kind of fucked overall because companies making tinned peaches and medicine actually need capita What's constraining the latter is rates. There is zero evidence tech companies are causing the inflation that is pushing up rates. (If anything, it's broadly deflating.) (And to my knowledge, getting financing for tinning peaches or medicines is plentiful. It's called middle market finance, and while it doesn't make the headlines, it's huge.) reply ggm 18 hours agorootparentAs a non-economist, I ask: do you think their above-grade returns can persist into the future for decades, without becoming a concern, or altering this 7% rate? My example of why it is bad is a hypothetical. If it's a stupid hypothetical I accept that, but my underlying belief that you cannot really have identified, \"the same\" companies continue to return 2-3x market average over 50 years without some concern remains. Am I wrong? Sure, some companies do better than others. Warren Buffet swears by re-insurance. When the west coast disappears in a tsunami, it won't be as bountiful, right? reply creer 17 hours agorootparentThere is a problem of perception. Apple, telsa, Nvidia, Google are in the news a lot - currently. Apple has been amazing for the past 20 years. Even during that period, its PE has varied a lot - the amount of money people were willing to pay compared to profits: at times it's very popular, at times not. Coca Cola is not in your list. But its result during the 1980s, 1990s was less but comparable to Apple these past years. Google is only 25 years old. Nvidia stock market price has been amazing only the past 10 years. Tesla is only 21 years old as a whole. Its stock price is too chaotic to be even described by a single return rate! Walmart did amazing 1975-1993. Nearly 20 years, then not so good. IBM, GE, several others had times of glorious stock market return. But, to return to the way you phrased it, more or less there have always been some companies that seemed to return a lot. Perhaps too much. That's more or less a normal of the stock market. None of them has lasted indefinitely or somehow taken over all of finance. Keeping a large company growing at this pace is, erm, hard. Note that this is not Apple's strategy currently: Apple produces a lot of profit and it is returned to the shareholders rather than desperately trying to grow the company with that money. reply ptero 17 hours agorootparentprevIt feels like I am always peddling Lyn Alden on macro questions. One of Lyn's recent public articles analyses long term returns and argues that most investments suck, and a few superachievers pull up the averages. So Apples and Nvidias eventually rotate out of the return engines club and are replaced by next few champions; but not a broad group. reply JumpCrisscross 18 hours agorootparentprev> do you think their above-grade returns can persist into the future for decades In aggregate, yes, given equities have done just fine persisting over the last century and a half. (Also, the 7% figure appears to be nominal.) > you cannot really have identified, \"the same\" companies continue to return 2-3x market average over 50 years without some concern remains No, I don't believe we have precedent for this. > When the west coast disappears in a tsunami, it won't be as bountiful, right? Flooding isn't typically privately insured. As far as reinsurance is concerned, a tsunami taking out a bunch of California would be financially uneventful; on one hand, you're losing a premium stream, on the other hand, you've freed up reserves. (Not an economist nor an actuary, but have training in both and some licensing in the latter.) reply creer 17 hours agorootparentprevAre these two example well chosen? Seems to me there has been plenty of entrepreneurship in food. So many new companies started in the past perhaps 20 years, and many now surprisingly large. And medecine overall (drugs, machines, care, insurance, tests, prevention) has been considered a field with good future prospects for a long time - worthy of investing. reply superb_dev 21 hours agoprevHow can an entire economy have a growth rate? Is it not measuring how much \"new money\" was put into the system? reply OscarCunningham 21 hours agoparentIt's nothing to do with money. We're literally producing more and better goods than the previous year. This can be due to better technology, more tools, or increasing population (and probably some other factors I forgot). reply Etheryte 20 hours agoparentprevMoney doesn't affect the size of your economy, in general money is not even relevant to the discussion, save for the fact that it gives us a unit of measurement. Money is a relative resource, in a simplified manner, money dictates who gets what fraction of the pie. By printing more money you're not making more pie, just dividing the existing pie into thinner slices. Economies grow because of improvements in technology, science, using or finding natural resources, producing things etc. reply neilwilson 9 hours agorootparent\"By printing more money you're not making more pie, just dividing the existing pie into thinner slices.\" So if you need a haircut but can't afford one, and I give you a new £10 note to get a haircut, and the barber works the extra 10 minutes to do that haircut and earn that £10 there's no new pie there. Looks like 10 minutes more output to me. \"Printing more money\" does make more pie. That's exactly what happens every time a loan is taken out, or government hires somebody who would otherwise be unemployed. \"Printing more money\" leads to \"shredding more money\" via loan repayments and increased tax take, all via increases in the utilisation of existing resources. That because \"printing more money\" is about buying something. That something has to at least potentially exist or more money will not be printed and circulated in the first place. reply Etheryte 5 hours agorootparentThis mixes up where the value creation takes place. Shitcoins are a good example of this fallacy, I can mint ten billion neilwilson-coins right now and hand them out, it doesn't mean any new value has been created. Perhaps an easier way to understand this conceptually is to think of money as a loan, because that's what it essentially is, a pair of credit and debit. Taking out a loan in and of itself it doesn't create any new value, the value is created if you do something useful with it. Money isn't even a requirement, you can also barter or trade. Money is only a means for mediating trade and do relative distribution, it isn't wealth. reply Grimblewald 13 hours agoparentprevMoney just represents tokens to use on our collective pile of goods and services. Growth refers to the rate at which the pile grows, and is largely divorced from money. Money is fake, and the systems behind it's distribution/allocation is regularly gamed for personal gain, in ways that go against the spirit of what money is supposed to represent. Money is an intermediate tool to help keep a track of who put how much on the pile so that everyone can focus on doing what they are good at, and in return, take from the pile what they need, as made by others who are good at other things. Increasingly we see a problem of people who add nothing to the pile getting large wads of money and taking more than their fair share from the pile. This is a problem. reply toomuchtodo 21 hours agoparentprevDemand increases due to population growth. Population goes down, growth goes down (broadly speaking, some caveats and nuance as always depending on some goods or services). Edit: https://journals.sagepub.com/doi/full/10.1177/21582440177360... reply bombcar 21 hours agorootparentOr demand increases due to some major new factor (which has happened a few times in recent history) that basically enables new energy extraction, or new resource extraction. But over very long periods of time, it does seem to mostly be connected to population. reply jetrink 21 hours agoparentprevRegardless of inflation or changes in the money supply, new techniques, new technologies, trade, and population growth can cause the value of everything bought and sold to increase over time. You can measure that value in dollars, or you can look at changes in the quantity and quality of goods and services. reply mensetmanusman 20 hours agoparentprevAn economy is simply the number of people times the average productivity per person. If lots of people are doing a lot of work powered by a lot of energy and productive technology equipment, the (material) economy is good. reply carlosjobim 20 hours agoparentprevYou live alone in the forest and chop wood during the winter. The next winter you're much better at the task and chop more wood. There's no difference in money supply, population or any such. But your economic output has increased. reply smath 20 hours agoprevDoesn’t this contradict Robert Shiller who shows that housing returns are flat in the long term? reply kccqzy 20 hours agoparentThat calculation does not factor into the rent you will not pay when you buy a house and live in it. Imputed rent is a thing and you need to consider it. If you don't live in the house you wouldn't let the house sit vacant: you would rent it. In your formula the rent can be thought of as if it's a dividend of the investment. Alternatively just read the linked article; the linked article makes the correct fair comparison. You will arrive at that conclusion by reading just the second paragraph, which says > data on total housing returns (price appreciation plus rents) has been lacking (Shiller 2000 provides some historical data on house prices but not on rents). In this article we build on more comprehensive work on house prices (Knoll, Schularick, and Steger 2017) and newly constructed data on rents (Knoll 2017) to enable us to track the total returns of the largest component of the national capital stock. Shiller is explicitly mentioned. And the article authors disregarded it because it failed to include rent. reply mensetmanusman 20 hours agoparentprevHousing prices may collapse over the next 50 years as the population pyramid inverts and buyers demand decreases due to fewer individuals. What is the definition of long-term though? reply throw_pm23 20 hours agorootparentI'm not sure, population already plummeted in many places while prices went up, as people prefer to live less densely then they used to. reply mensetmanusman 18 hours agorootparentAs the housing maintenance labor force shrinks and it gets more expensive for elderly to maintain their non dense homes though, the value should decrease and become more affordable for the young who can do those labor things. Maybe? reply throw_pm23 20 hours agoparentprevWouldn't be the first time someone was wrong about something. reply JumpCrisscross 20 hours agoprev“the only exceptions to that rule happen in the years in or around wartime. In peacetime, r has always been much greater than g” This explains the enduring link between populism and war mongering. reply TacticalCoder 19 hours agoparent> In peacetime, r has always been much greater than g This is an ultra simplistic formula, made by someone who's been born, raised and fed in a highly socialist country where the only word politicians know is \"tax\". reply JumpCrisscross 18 hours agorootparent> is an ultra simplistic formula No shit. What gave it away, the two terms or the inequality? :) Joking aside, it's simplistic because it's elementary. If real returns exceed real growth, ceteris paribus, you have a net flow of principal (so to speak) from labour to capital. That doesn't mean one can conclude the argument with those two variables alone. But it's a valid starting point, and concludes with many solutions other than increasing taxes to reduce r. reply twoodfin 17 hours agorootparentWhy is reducing “r” desirable? Why not try to raise “g” instead? reply tehjoker 16 hours agorootparentInequality is a social poison all on its own. For example, democracy is meaningless when men of means can buy elections. reply amanaplanacanal 10 hours agorootparentAnd we have many historical examples of it leading to eventual mob violence. reply throw0101d 21 hours agoprevIf anyone wants to download the data, it's available at: > The Jordà-Schularick-Taylor Macrohistory Database is the result of an extensive data collection effort over several years. In one place it brings together macroeconomic data that previously had been dispersed across a variety of sources. On this website, we provide convenient no-cost open access under a license to the most extensive long-run macro-financial dataset to date. Under the Terms of Use and Licence Terms below, the data is made freely available, expressly forbidding commercial data providers from integrating, in addition to any existing data they may already provide, all or parts of the dataset into their services, or to sell the data. * https://www.macrohistory.net/database/ See also perhaps \"Historical Returns on [US] Stocks, Bonds and Bills: 1928-2023\" (updated annually AFAICT): * https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datafile... There's also the The Credit Suisse Global Investment Returns Yearbook: > The Credit Suisse Global Investment Returns Yearbook is the authoritative guide to historical long-run returns. Published by the Credit Suisse Research Institute in collaboration with London Business School, it covers all the main asset categories in 35 countries. Most of these markets, as well as the world index have 123 years of data since 1900. * https://www.credit-suisse.com/about-us-news/en/articles/medi... * https://www.credit-suisse.com/about-us/en/reports-research/s... As well as: > The Global Investment Returns Yearbook, an authoritative guide to historical long-run returns, launched by UBS Investment Bank Research and UBS Global Wealth Management’s Chief Investment Office. This edition demonstrates the combined strength of UBS and Credit Suisse as the integration of the two banks progresses, and also marks the continuity of a longstanding relationship with the authors, Professor Paul Marsh and Dr Mike Staunton of London Business School and Professor Elroy Dimson of Cambridge University. * https://www.ubs.com/global/en/investment-bank/in-focus/2024/... reply pineaux 22 hours agoprev [–] So basically, housing is the best investment vehicle based on all the numbers. reply bombcar 21 hours agoparentHousing is only a true investment vehicle if you count all the costs. Even bare land has to be maintained somewhat. You can't just subtract purchase price from sale price and call it done. reply mensetmanusman 20 hours agorootparentYeah, it would be interesting to have more transparent costs, especially with inflation. New siding every 20 years is $40k, a new roof might be $30k every 25 years, a new driveway, etc. reply darth_avocado 20 hours agorootparentBiggest of them all: interest. On a $1M mortgage, you’ll almost pay $2M as interest over 30 years even at 7%. Historically interest was never as low as during the pandemic. And most people bought houses using mortgages. The average “cost” of owning a house is much more than the selling price, even before you account for the upkeep. reply jajko 7 hours agorootparentIn many places, this can be substracted from taxes, effectively slashing 35-40% of the cost where I live. So does almost all renovation costs (not upgrades though, just repairs). Also, where I live 7% wasn't the case in past 20 years, and even now its rather 1.5% + whatever bank puts on. reply cmrdporcupine 16 hours agorootparentprevI just shudder to think about all my trips to Home Depot over the last 20 years of owning a home. I never did those when I was renting. Yes, I didn't get to renovate or pick my paint colours. And yes my money paid down someone else's mortgage. But I suspect if you add it all up... reply nradov 14 hours agorootparentYou should add it all up. Some of the things you purchased at Home Depot can be counted in the cost basis of the home and reduce your capital gains tax if you later sell it. reply thyrsus 9 hours agorootparentFor most U.S. taxpayers selling a home, the capital gains exclusion on one's primary residence ($500,000 filing jointly) obviates such bookkeeping. reply cmrdporcupine 6 hours agorootparentprevNo capital gains at all on primary residences here in Canada. For better or for worse. reply spiantino 21 hours agoparentprevEspecially when you consider all the tax affordances related to owning real estate vs. equities. reply JumpCrisscross 18 hours agorootparent> all the tax affordances related to owning real estate vs. equities The only ones I can think of are depreciation (analogous to capital loss harvesting), 1031 exchanges (loosely analogous to step-up basis; this is the biggest difference) and opportunity zones (analogous to QSBS). If you borrow against your equities, you can deduct the interest paid on that. That mortgage-interest deductions are bigger is a function of the lending being federally guaranteed more than tax law. reply idiotsecant 21 hours agoparentprevI wonder if this is still true once population growth reaches zero or negative. It seems like the baked in assumption of housing is that someone else is going to need it more tomorrow than you do today. I think this is an experiment the U.S. will begin running in earnest in the near future. reply betaby 21 hours agorootparentThat if won't happen anytime soon neither for the Earth in general nor USA in particular. Media often loves to move goals from population growth to agin g population to fertility rate, etc. All those while connected do not negate the fact that population is growing, and growing fast. That 'once population growth reaches zero or negative' is very theoretical and UN is constantly underestimating population growth. So no, we will not see that in our lifetimes. reply rybosworld 21 hours agorootparentThe population growth rate has consistently declined for 50+ years. It's around 0.8% from a peak of 2.2% in the 60's. There's no reason to think this trend will reverse. reply betaby 21 hours agorootparentWhy should we use peak as a benchmark? Even 0.8% is insanely high, at such rate population will double in ~150 years. reply throwawayFinX 20 hours agorootparentThe parent is not wrong: The total number of children in the world has already peaked (2017?) and is now dropping. The population growth should still continue for about a human lifespan from here (50-80 years depending on who you ask). That last growth is just those children growing up and becoming adults. I.e. They are the “last big generation”. We will see the population drop again, if we dont fuck up the planet before that happens. I think you would have difficulty finding countries in the world where fertility rates (children born pr woman) are not dropping. Bangladesh went from 5.5 kids pr woman in 1985 to 2.1 in 2017. This is a global trend. reply rybosworld 4 hours agorootparentprev> Why should we use peak as a benchmark? I'm not sure what you mean by the peak being a benchmark. There's a very clear trend of population growth declining every year since the 60's. > Even 0.8% is insanely high, at such rate population will double in ~150 years I think you are missing the part where the rate has been declining every year. reply betaby 4 hours agorootparentI think you are missing 'in our lifetime', even though you are quoting 'will double in ~150 years'. Also 'declining every year' doesn't mean it won't start growing again. reply rybosworld 2 hours agorootparent> I think you are missing 'in our lifetime' I guess it depends when you are born. Peak population is predicted around 2075, and that's within a lot of people's lifetime. > Also 'declining every year' doesn't mean it won't start growing again That's a bit obvious and is equivalent to saying \"anything could happen\". But unless you have a good reason for a reversal in trend, then there's no reason to think it will. reply mensetmanusman 20 hours agorootparentprevChina loses 300 million in the next century, we will start witnessing the impact before we die. reply bee_rider 21 hours agorootparentprevMost developed countries will probably run the experiment before us, as long as we can keep the country appealing enough to keep attracting immigrants. reply bombcar 21 hours agorootparentprevIt has happened, and recently, multiple times on local scales. \"House go up\" may be generally true in the abstract, but that doesn't mean this particular house goes up. reply monero-xmr 21 hours agorootparentprevBuy housing in hyper desirable areas that rarely become more dense. Everyone always loves the beach, whether there are 1% fewer humans next year or not. reply _DeadFred_ 21 hours agorootparentAbout that... https://eos.org/articles/ocean-waves-mist-decades-old-pfas-i... reply darth_avocado 20 hours agorootparentprevThe hyper desirable areas are no longer affordable to even millionaires. reply betaby 21 hours agoparentprevEarth's population is constantly growing, thus yes. reply jstanley 21 hours agoparentprevBefore 1950, yes. reply pessimizer 19 hours agoparentprevYes, but only because you can overcharge rents to people who can't afford housing. If everyone could afford housing, it wouldn't have any return. The return on housing rents is equal to the minimum (psychological) expectation that landlords expect. It's an arbitrary vig/rake, and like all arbitrary vigs/rakes, it's around 5%. It's an expected gift for owning the house. It's a gratuity for being wealthy enough that you're never forced to buy or sell. An aside is that this rate was set in one context by currency and convention: an English pound was 20 shillings, and a guinea was 21. So when you won an auction, you would pay the auction house in guineas, and the auction house would pay the owner of the item in pounds, giving a 4.75% share to the house. Racehorses are still sold this way, although aren't any guineas or shillings any more, it's now 1£ and 1.05£. reply andrepd 22 hours agoparentprev [–] Sad state of affairs, but yes. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper titled \"The Rate of Return on Everything, 1870–2015\" introduces a new data set covering all major asset classes, including housing, from 1870 to 2015 in 16 advanced economies.",
      "The study reveals that residential real estate and equities have similar high returns, averaging about 7% a year, but housing returns are less volatile than equities.",
      "The findings challenge common beliefs about risk and return, showing that housing has been the best long-term investment in modern history due to its lower volatility compared to equities."
    ],
    "commentSummary": [
      "Housing costs have steadily increased over the past century due to population growth outpacing the supply of desirable land.",
      "Industrialization has reduced the proportion of income spent on food and labor required for goods, increasing overall wealth faster than the supply of desirable land.",
      "The text discusses various factors affecting housing prices, including dual-income households, urbanization, and the impact of technology on distributed living."
    ],
    "points": 171,
    "commentCount": 125,
    "retryCount": 0,
    "time": 1718133612
  },
  {
    "id": 40656603,
    "title": "Raspberry Pi Goes Public with $690M Valuation on London Stock Exchange",
    "originLink": "https://techcrunch.com/2024/06/11/raspberry-pi-is-now-a-public-company-as-its-shares-pops-after-ipo-pricing/",
    "originBody": "Login Search Search Startups Venture Apple Security AI Apps Events Startup Battlefield More Close Submenu Fintech Cloud Computing Layoffs Hardware Google Microsoft Transportation EVs Meta Instagram Amazon TikTok Newsletters Podcasts Partner Content Crunchboard Jobs Contact Us Featured Article Raspberry Pi is now a public company The company’s shares popped 32% after its IPO pricing Romain Dillet 4:08 AM PDT • June 11, 2024 Comment Image Credits: TechCrunch Who would have thought that Raspberry Pi, the maker of the tiny, cheap, single-board computers, would become a public company? Yet, this is exactly what’s happening: Raspberry Pi priced its IPO on the London Stock Exchange on Tuesday morning at £2.80 per share, valuing it at £542 million, or $690 million at today’s exchange rate. Shortly after that, the company’s shares jumped a nice 32% to £3.70. It means that Raspberry Pi could end up raising more than $200 million during its IPO process. Retail investors can’t buy Raspberry Pi shares just yet, as only certain institutional shareholders can trade the company’s shares right now. Retail investors will be able to buy and sell shares starting on Friday. This listing is also a win for the London stock market. Deliveroo and Wise both trade in London, but many U.K. tech companies choose to go public in the U.S., as the stock markets there are more liquid. Raspberry Pi is mostly known for its tiny computers that can be programmed to perform all sorts of tasks without spending too much money and requiring too much power. These ARM-based computers became particularly popular among tech hobbyists who wanted to create media servers, retro game consoles, interactive dashboards, robotics projects and more. More recently, many industrial companies have started integrating the Raspberry Pi in their devices and facilities. The company reports that the industrial and embedded segment represents 72% of its sales. Raspberry Pi has sold 60 million units since its inception. In 2023 alone, Raspberry Pi generated $266 million in revenue and $66 million in gross profit. Raspberry Pi Ltd, the public company, is the commercial subsidiary of the Raspberry Pi Foundation. The Foundation says it wants to make it easier for people to learn coding through a low-cost, programmable computer. It also remains the main shareholder of Raspberry Pi Ltd. Other strategic shareholders in the company include ARM and Sony Semiconductor Solutions Corporation, a subsidiary of Sony that makes image sensors for smartphones and other components. ARM previously said it intended to increase its stake in Raspberry Pi via the public listing. More TechCrunch Get the industry’s biggest tech news Explore all newsletters TechCrunch Daily News Every weekday and Sunday, you can get the best of TechCrunch’s coverage. Add TechCrunch Daily News to your subscription choices Startups Weekly Startups are the core of TechCrunch, so get our best coverage delivered weekly. Add Startups Weekly to your subscription choices TechCrunch Fintech The latest Fintech news and analysis, delivered every Tuesday. Add TechCrunch Fintech to your subscription choices TechCrunch Mobility TechCrunch Mobility is your destination for transportation news and insight. Add TechCrunch Mobility to your subscription choices No newsletters selected No newsletters Email address (required) Subscribe By submitting your email, you agree to our Terms and Privacy Notice. Tags IPO, Raspberry Pi, Raspberry Pi Foundation Robotics Generative AI takes robots a step closer to general purpose Brian Heater 35 seconds ago The push to produce a robotic intelligence that can fully leverage the wide breadth of movements opened up by bipedal humanoid design has been a key topic for researchers. Transportation Ford’s secretive low-cost EV team is growing with talent from Rivian, Tesla and Apple Sean O'Kane 7 mins ago A TechCrunch review of LinkedIn data found that Ford has built this team up to around 300 employees over the last year. Transportation Tern AI wants to reduce reliance on GPS with low-cost navigation alternative Rebecca Bellan 17 mins ago The most critical systems of our modern world rely on GPS, from aviation and road networks to emergency and disaster response, from precision farming and power grids to weather forecasting… Fintech Fintech Brex abandons co-CEO model, talks IPO, cash burn and plans for a secondary sale Mary Ann Azevedo 45 mins ago Since fintech startup Brex’s inception in 2017, its two co-founders Henrique Dubugras and Pedro Franceschi have run the company as co-CEOs. But starting today, the pair told TechCrunch in an… AI This Week in AI: Apple won’t say how the sausage gets made Kyle Wiggers 2 hours ago Hiya, folks, and welcome to TechCrunch’s regular AI newsletter. This week in AI, Apple stole the spotlight. At the company’s Worldwide Developers Conference (WWDC) in Cupertino, Apple unveiled Apple Intelligence,… Fintech India’s 360 One acquires mutual fund app ET Money for $44M Manish Singh 2 hours ago 360 One WAM, India’s largest wealth manager focused on ultra-high-net-worth individuals, has agreed to acquire popular Indian mutual fund investment app ET Money for about $44 million. 360 One, earlier… AI Helen Toner worries ‘not super functional’ Congress will flub AI policy Kyle Wiggers 2 hours ago Helen Toner, a former OpenAI board member and the director of strategy at Georgetown’s Center for Security and Emerging Technology, is worried Congress might react in a “knee-jerk” way where… TechCrunch Disrupt 2024 Layoffs Got You Down? Get a Half-Price Expo+ Pass at Disrupt 2024 TechCrunch Events 3 hours ago Layoffs are tough. This year alone, we’ve already seen 60,000 job cuts across 254 companies according to layoffs.fyi. Looking for ways to grow your network can be even harder during… Media & Entertainment YouTube creators can now test multiple video thumbnails Lauren Forristal 3 hours ago YouTube announced this week the rollout of “Thumbnail Test & Compare,” a new tool for creators to see which thumbnail performs the best. The feature first launched to select creators… Transportation Waymo issues second recall after robotaxi hit telephone pole Rebecca Bellan 4 hours ago Waymo has voluntarily issued a software recall to all 672 of its Jaguar I-Pace robotaxis after one of them collided with a telephone pole. This is Waymo’s second recall. The… Enterprise Insight Partners backs Canary Technologies’ mission to elevate hotel guest experiences Christine Hall 5 hours ago The hotel guest management technology company’s platform digitizes the hotel guest journey from post-booking through checkout. AI Here’s everything Apple announced at the WWDC 2024 keynote, including Apple Intelligence, Siri makeover Christine Hall 5 hours ago The TechCrunch team runs down all of the biggest news from the Apple WWDC 2024 keynote in an easy-to-skim digest. Fintech Lightspeed Venture Partners leads $4.3M seed in automated financial reporting fintech InScope Christine Hall 6 hours ago InScope leverages machine learning and large language models to provide financial reporting and auditing processes for mid-market and enterprises. Venture Foresite Capital raises $900M sixth fund for investing in life sciences companies Marina Temkin 6 hours ago Venture fundraising has been a slog over the last few years, even for firms with a strong track record. That’s Foresite Capital’s experience. Despite having 47 IPOs, 28 M&As and… Enterprise Databricks expands Mosaic AI to help enterprises build with LLMs Frederic Lardinois 6 hours ago A year ago, Databricks acquired MosaicML for $1.3 billion. Now rebranded as Mosaic AI, the platform has become integral to Databricks’ AI solutions. Today, at the company’s Data + AI… Commerce YC grad RetailReady raises $3.3M for an AI warehouse app that hopes to save brands billions Christine Hall 6 hours ago RetailReady targets the $40 billion compliance market to help reduce the number of retail compliance losses that shippers incur annually due to incorrectly shipped packages. Enterprise Databricks launches LakeFlow to help its customers build their data pipelines Frederic Lardinois 6 hours ago Since its launch in 2013, Databricks has relied on its ecosystem of partners, such as Fivetran, Rudderstack, and dbt, to provide tools for data preparation and loading. But now, at… TechCrunch Disrupt 2024 Bonus: An extra week to apply to Startup Battlefield 200 TechCrunch Events 6 hours ago A big shoutout to the early-stage founders who missed the application window for the Startup Battlefield 200 (SB 200) at TechCrunch Disrupt. We have exciting news just for you! You… Enterprise Restate raises $7M for its lightweight workflows-as-code platform Frederic Lardinois 7 hours ago When one of the co-creators of the popular open source stream-processing framework Apache Flink launches a new startup, it’s worth paying attention. Stephan Ewen was among the founding team of… Climate Civic Renewables is rolling up residential solar installers to improve quality and grow the market Tim De Chant 8 hours ago With most residential solar panels installed by smaller companies, customer experience can be a mixed bag. To try to address the quality and consistency problem, Civic Renewables is buying small… Venture Friends & Family Capital, a fund founded by ex-Palantir CFO and son of IVP’s founder, unveils third $118M fund Marina Temkin 8 hours ago Small VC firms require deep trust, mutual support and long-term commitment among the partners — a kinship that, in many ways, resembles a family dynamic. Colin Anderson (Palantir’s ex-CFO and… Transportation Fisker’s troubled Ocean SUV gets its first recall Sean O'Kane 8 hours ago Fisker is issuing the first recall for its all-electric Ocean SUV because of problems with the warning lights, according to new information published by the National Highway Traffic Safety Administration… Climate Gorilla, a Belgian startup that helps energy providers crunch big data, raises $25M Paul Sawers 9 hours ago Gorilla, a Belgian company that serves the energy sector with real-time data and analytics for pricing and forecasting, has raised €23 million ($25 million) in a Series B round led… AI Fabless AI chip makers Rebellions and Sapeon to merge as competition heats up in global AI hardware industry Kate Park 9 hours ago South Korea’s fabless AI chip industry saw a slew of fundraising events over the last couple of years as demand for hardware to power AI applications skyrocketed, and it seems… Apps The apps that Apple sherlocked at WWDC 2024 Ivan Mehta 10 hours ago Here’s a list of third-party apps that were Sherlocked by Apple at this year’s WWDC. Enterprise Black Semiconductor nabs $273M in Germany to supercharge how chips work together Ingrid Lunden 12 hours ago Black Semiconductor, which is developing a chip-connecting technology based on graphene, has raised $273M in a combination of private and public funding. Featured Article Let there be Light! Danish startup exits stealth with $13M seed funding to bring AI to general ledgers It’s not the sexiest of subject matters, but someone needs to talk about it: The CFO tech stack — software used by the chief financial officers of the world — is ripe for disruption. That’s according to Jonathan Sanders, CEO and co-founder of fledgling Danish startup Light, which exits stealth… Paul Sawers 12 hours ago Space Apex’s off-the-shelf satellite bus business attracts $95M in new funding Aria Alamalhodaei 15 hours ago Fresh off the success of its first mission, satellite manufacturer Apex has closed $95 million in new capital to scale its operations. The Los Angeles-based startup successfully launched and commissioned… Startups Washington’s political class doesn’t know Y Combinator exists — yet Sarah Perez 17 hours ago After educating the D.C. market, YC aims to leverage its influence, particularly in areas like competition policy. Government & Policy FTC Chair Lina Khan tells TechCrunch the agency is pursuing the ‘mob bosses’ in Big Tech Aisha Malik 19 hours ago Lina Khan says the FTC wants to be effective in its enforcement strategy, which is why it has been taking on lawsuits that “go up against some of the big… About TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Legal Terms of Service Privacy Policy RSS Terms of Use Privacy Placeholder 1 Privacy Placeholder 2 Privacy Placeholder 3 Privacy Placeholder 4 Code of Conduct About Our Ads Trending Tech Topics WWDC 2024 Apple Intelligence Fisker Recall Light Phone 3 Waymo Recall Tech Layoffs ChatGPT Facebook X YouTube Instagram LinkedIn Mastodon Threads © 2024 Yahoo. All rights reserved. Powered by WordPress VIP",
    "commentLink": "https://news.ycombinator.com/item?id=40656603",
    "commentBody": "Raspberry Pi is now a public company (techcrunch.com)153 points by mbs159 8 hours agohidepastfavorite207 comments lrvick 7 hours agoI only ever bought Pis because it was a mission focused company. Now that they are in stock-price-go-up mode, I am inclined to buy and promote cheap clones, and I expect most will feel the same. reply mikae1 7 hours agoparent> I expect most will feel the same. I expect most won't care or even notice. Company ownership is probably not that important to the average consumer. They care about great hardware support and a good \"ecosystem\" of accessories. reply Draiken 5 hours agorootparentIf the consumers were non tech people, I'd agree. But isn't this a product aimed at hackers and tinkerers? I know the first thought that came to my mind was: damn, I'll have to find new hardware when my Pi dies. reply tucnak 5 hours agorootparent> isn't this a product aimed at hackers and tinkerers? Who told you that? reply karaterobot 5 hours agorootparent> We’re on a mission to put high-performance, low-cost, general-purpose computing platforms in the hands of enthusiasts and engineers all over the world. (https://www.raspberrypi.com/about/) Maybe they got it from the marketing material? reply tucnak 3 hours agorootparentSo what do we believe marketing materials now? Next up: Google is organising the world's information, NOT selling ads! Raspberry Pi used to be a platform for education first and foremost, but you don't get very far with this so they took on some funding to get ahead. Anti-VC whining is quite tangential to the product itself. reply karaterobot 3 hours agorootparentYou asked who told them that. I'm speculating, but I thought the answer might be that Raspberry Pi told them that. reply Draiken 5 hours agorootparentprevIt's more about who hasn't talked to me about RPi ever: regular people. Regardless, I posed it as a question exactly because I don't know. reply gklitz 5 hours agorootparentprev> But isn't this a product aimed at hackers and tinkerers? It was, but The appeal spread outside of that, which is why they are now going public and abandoning the precious niche mission and are instead aiming at mass market appeal. reply creshal 4 hours agorootparentprevThey'll start noticing once the enshittification begins and expensive but hard to KPI-ify things like QC and documentation quality get optimized away. reply squarefoot 7 hours agoparentprevI dumped them after they fought against competition by selling very limited stocks of the Zero at very low prices, possibly at a loss. They released just enough of them so that someone somewhere could always tell that they were the cheapest board available, but in fact they were pretty much unobtanium everywhere, except of course in \"friendly\" sites and forums where every post about the shortage was immediately replied with \"I could buy one at XYZ...\", where xyz inevitably turned out to be a shop with none in stock. This happened much before Covid19 and chips shortage hit the market, in fact competition had cheap alternatives in stock and I could buy some NanoPi and OrangePi, but the RPi had much better press. I'm considering to make an exception for the Pico for being a really interesting product, but my last Linux RPi has been the 4 and I don't plan to go further. reply jbaber 5 hours agorootparentBefore the pandemic, I picked them up for $5 regularly at our local friendly computer store. They even had 0w's for $5 as a gimmick (they were supposed to be $10) with purchase limit of 2 or 5 (can't remember). I'd just pick up a couple every time I went to that store. I've got a sandwich bag full of them now. I think it was very much the supply chain snags of pandemic that made them get weird high prices everywhere. reply Gormo 7 hours agorootparentprevWhat you're describing sounds like ordinary supply chain difficulties, with no indication of any nefarious intention. reply afavour 7 hours agorootparentprevSo they sold all the available stock they had as soon as they got it, at a reasonable price… and that made you dump them? reply lelanthran 1 hour agorootparent> So they sold all the available stock they had as soon as they got it, at a reasonable price… and that made you dump them? Well, yes, of course? Why burn billable hours on a design I can send to production? reply gambiting 5 hours agorootparentprevI know it's an anecdote, but I found out about them bundling the Zero with a Raspberry Pi magazine here in the UK few days after it launched, and I was able to pop down to my local tesco and grab a copy(and it wasn't the last one either). Since then I bought a dozen more for various projects around the house and it's never been a problem, Pimoroni or the Pi Hut always have some. reply cptskippy 5 hours agorootparentprevSo your assertion is that Raspberry Pi was using a Zero as a loss leader AND that it was out of stock. And some how this was enough to stifle competing products? And the competing products you've cited all have \"Pi\" in the name because they only exist to fill the gaps that Raspberry Pi products don't fill. For example the NanoPi is a Zero competitor that differentiates itself by having Ethernet. Raspberry Pi dominates the market because it was the first to enter it and provides a limited product catalog that has limited churn with comprehensive support and regular software releases. The same cannot be said for it's competition which historically offered horrible software support land quickly iterated on products meaning that support generally only existed for the very latest. reply 127 4 hours agoparentprevI use the Pis because the ecosystem and documentation. I tried using clones but they're often horrible when it comes anything outside the direct hardware. Also they have a ton of competition. Jumping back to STM32 is pretty trivial if you use open toolchains. reply riv991 7 hours agoparentprevThe foundation still owns the vast majority of the company reply account42 4 hours agorootparentAnd the worth of that stake is now directly tied to stock performance. reply nashashmi 4 hours agorootparentprevwait! The foundation sells Pis? What does the for-profit company do? Hell, where did the foundation get its money before? reply michaelt 6 hours agorootparentprevYeah, and the openai nonprofit still owns a majority of the openai for-profit company. Turns out that doesn't mean shit. reply riv991 5 hours agorootparentYou can argue the complete opposite point with Novo Nordisk. It's up to the foundation to use it's influence reply ActionHank 4 hours agorootparentprevSo if the foundation makes decisions that drive stock up in the short term, they make themselves richer? Definitely won't be abused /s reply nashashmi 4 hours agorootparentMaybe, but I think part of the fear behind the IPO is that some bully type investor like Icahn will do a takeover/weigh-in and mess it up, towards a profit driven enterprise. With major ownership held by someone else, this becomes less likely. reply nicce 7 hours agoparentprevThere area already some privacy concerns. New Sony chip is likely embedded for future devices to enable edge processing. Metadata of some processing will start leaking to external cloud whether you want it or not. https://www.engadget.com/sony-investment-will-put-ai-chips-i... https://aitrios-promo.wpp.developer.sony.com/en/services/edg... reply gjsman-1000 7 hours agorootparentThat’s not built in to the Pi, and is way too expensive. It will almost certainly be a HAT. Complaining about this shows ignorance. reply JKCalhoun 5 hours agorootparentI re-read your comment minus the last sentence and I like it better. reply nicce 6 hours agorootparentprev> That’s not built in to the Pi Based on what? Even the linked article title says IN. Maybe they have different models, but based on all the wordings, it is very odd if it is HAT. > Complaining about this shows ignorance. If it is external HAT, then maybe. But until then, maybe not. Edit: There seems to be AI kit available now, but it does not include this: https://www.raspberrypi.com/documentation/accessories/ai-kit... reply alias_neo 6 hours agorootparentThe \"IN\" probably refers to in one of the camera modules, or some other module/HAT. Raspberry Pis don't use any Sony chips on the SBC, Sony image sensors are on the camera modules, and since they're talking about imaging, this makes perfect sense. Nothing about the Engadget article says to me it'll be on board the Raspberry Pi itself. reply teamonkey 4 hours agorootparentprevThey're also producing an AI camera. https://www.elektormagazine.com/news/raspberry-pi-monitor-te... The original press releases don't say anything about incorporating it into the base Pi hardware and I don't think they would, given that it would add extra cost and most people don't need it. reply nicce 4 hours agorootparentYeah, AI camera integration might be actually the end result. This gives good assumption. reply bayindirh 5 hours agoparentprev> I expect most will feel the same. I buy Pis because of their support, stability and out of the box experience is far superior to other cheaper clones. I have tons of cards which are paperweights without Armbian or self-rolled distributions, and even them some are feature-capped because of kernel-dependent closed source modules or out-of-tree drivers. I can still download and install a recent version of RaspberryPi OS to my Raspi 1, customize the image even before it touches the SD card, and remove the card and install to a more recent Pi, and continue working. More importantly, I can upgrade OS from version to version (Armbian doesn't support this, and they say it explicitly for example). OrangePi, Radxa and Asus failed on all these fronts. I still peek and poke OrangePi 5B's Debian image since I can't trust them blindly, but can't use its AI and video (rendering/encoding) accelerators without its official image. Oh, and my OrangePi Zero always crashes within a day or two if I don't practically disable its frequency scaling capability via governor. Even a good cable and adapter which can run a Raspi 3/4 without any hiccups can't make this postage stamp sized thing happy and stable. P.S.: For everyone who thinks RaspberryPi brand and hardware will be enshittified: I'm not saying that it won't happen. What I'm saying is, let's watch how this moat evaporates, and who passes (said moat) and replaces them on their hill. reply ActionHank 4 hours agorootparentSupport and stability will go down, they will cut support roles to drive profitability and they will release multiple variants of the same board to saturate the market so hardware support will get dicier. They may even roll their own distro as a way to capture the market. When a company lists they switch from being customer focused to being share-holder focused. They will do anything and everything to make number go up. reply bayindirh 4 hours agorootparentLet's watch as it happens, shall we? I'm not saying it won't happen, yet I don't feel gloomy and dark about it. If they fall, another will rise and fill the void. The market is healthy from that standpoint, but let's watch as it happens and see what happens. reply Hizonner 4 hours agorootparentprevWell, you can expect all of those superior things to start going to hell in about a year or two. The enshittification will begin. reply bayindirh 4 hours agorootparentIf that happens, I won't buy them anymore. The boards don't have an expiration date, and I can roll my own distribution \"if it comes to that\". Have done that in a professional capacity back then, can do it again. reply account42 4 hours agorootparentWith a publicly traded company that's not an if but a when. reply bayindirh 4 hours agorootparentYou're right, but I refuse to keep my mind busy with it. I'll look around when I need a new SBC for the task at hand, and if RaspberryPi has enshittified, I'll just pass. So it's an if from my perspective, since I don't constantly watch them and ask \"aretheyenshittifiedyet?\". reply louwrentius 7 hours agoparentprevIf you need a simple home server or lab setup, second-hand 1L pc's (tinyminimicro) is a much better value-for-money. I've a ton of Pi4s and one Pi5, but the ease of use, the simplicity of 1L pc's is - for the stated applications - a no-brainer to me. reply dspillett 4 hours agorootparent> second-hand 1L pc's (tinyminimicro) is a much better value-for-money. For initial purchase they certainly offer much better bang-for-buck compared to the Pi4 & Pi5, but do you have any useful links to information about relative power consumption when mostly idle? I currently have a Pi400 (back when I bought that the Pi proper was out of stock everywhere except the scalpers charging >£150/unit for even the low RAM versions, but I found a decent deal on a Pi400 kit) running as a router/firewall with a few other small bits & bobs in containers. I keep considering replacing it with a small PC like that – my main concern being running a core bit of the home network off an SSD, though also having a bit more performance (both IO and CPU) might at times be useful too, and the Pi400 I'd like to reuse somewhere it is visible for “this is what home PCs used to be like when I were a lad” nostalgia reasons. The reasons I haven't so far (other than momentum: the 400 is sat there working quietly needing no interaction aside from the occasional update and paranoia checks of the config/log/other backups) are possible greater electricity use, and great noise if the mini-PC is not entirely passively cooled. reply mbs159 7 hours agorootparentprevOr if you don't mind the larger size and you just need a home server, you can get a used NUC. reply JKCalhoun 5 hours agorootparentprevDo you blow away the OS and install Linux on them? Curious because for me the affordability of a hobbyist PC is offset by the cost of Windows. reply Levitating 4 hours agorootparentCost of windows? Why not just use the windows trial. Or buy a license key. Windows license keys are absolutely everywhere. Maybe check the bios from some old laptops you own? At my university we have this massive ewaste bin full of old pcs stickered with probably valid license keys. Though you might be better off sticking with Linux anyway. reply steeleyespan 6 hours agorootparentprevI used a super cheap HP Pavilion laptop that was unbearably slow with Windows, but works great as a Mint server. Then two used/refurbished Dell Optiplex's, great deals! reply manojlds 6 hours agoparentprevSo what are some good alternatives? reply geokon 6 hours agorootparentMy impression is that the coming generation of chips are going to be completely integrated RiSCV chips with RAM integrated on one package. Akin to a microcontroller but that can run Linux. Something like the SG2002 Ex: LicheeRV Nano https://www.cnx-software.com/2024/02/08/licheerv-nano-low-co... This is sufficient for \"automation\" hobby projects. You get the full Linux/OS creature comforts without the hassle of doing Arduino or Mircopython. If you want a full desktop experience (or a project with video output) on a SBC it's probably easiest to go for a Intel SBC RPi is getting squeezed from top and bottom price brackets reply drzaiusx11 5 hours agorootparentThanks for the licheerv nano link, this thing [1] looks awesome! Display out, built in amp, and 1GHz riscv for $10 USD [2]. Incredible. 1. https://wiki.sipeed.com/hardware/en/lichee/RV_Nano/1_intro.h... 2. https://a.aliexpress.com/_mPnKu26 reply drzaiusx11 4 hours agorootparentIt's somewhat amusing a MIPI -> HDMI adapter [1] is more than the total cost of the SBC https://a.aliexpress.com/_mtj0TmG reply 05 6 hours agorootparentprev> This is sufficient for \"automation\" hobby projects. and so is a $2 ESPHome.. For actually working with hardware Arduino is much more convenient than whatever abomination you would write in Python. For anything that needs video output you should use a cheap Android. Anything else is just wasting money. reply jbaber 5 hours agorootparentprevI got an intel SBC for something that required non-ARM and I remember the power raw being surprising after having been spoiled by so many raspberry ARMs. reply le-mark 6 hours agorootparentprevMy understanding that as of a couple of years ago most of the “clones” had various issues from pcb layout bugs causing ddram access issues, to poor documentation and poor Linux kernel support. Also availability was often a challenge. Getting a product that just works and is well supported by a community is worth a lot imo. Especially for hobbyist, less so if it’s one’s full time job, but then why are you using these things? reply alias_neo 6 hours agorootparentprevI've been looking into alternatives to the Zero 2W because I need something in the same form factor with more (processing) power; something closer to the Pi 3B+ or better. My conclusion is that although there is some nice hardware out there on paper, the poor circuit designs and atrocious software support mean anything I can get that meets my requirements will be an absolute nightmare; and I'm a software engineer of 1.5-decades with an Electronic Engineering degree, so the bar is high for something to be too-much-effort. Take the Radxa Zero 3W[0] for example; the WiFi doesn't work properly, GPU acceleration doesn't always work, the OS images are out of date and don't always boot, when it does there are power issues, thermal issues, performance issues. No Debian 12 expected until H2 this year, if lucky; Android OS images are Android 9 (we're on what Android 14-15 now?). [0]https://forum.radxa.com/c/zero reply moffkalast 4 hours agoparentprev> I am inclined to buy and promote cheap clones Are you also inclined to support and develop kernel and OS level software for them? Because the lack of good support is why they remain woefully noncompetitive despite the lower price. reply yjftsjthsd-h 1 hour agorootparenthttps://libre.computer/ sells SBCs that run mainline linux and boot with UEFI, which is better than Pis could ever say. reply lenerdenator 5 hours agoparentprevI too worry about enshittification, but let's step back here. When you're talking about a price difference that can be made up in a few hours of work, the extra cost can be easily justified, because of the Raspberry Pi's secret sauce. The secret sauce allowed me to search the web for guides on what I wanted to do and have a template to work from. Not having to spend potentially hours or days of my free time figuring out some obscure Linux issue or driver support problem made up for the extra cost of the board. The secret sauce was always the documentation and software support, and if the cheap clones can master that, then the boys in England have a real problem, but that remains to be seen. A lot of that comes not from cost savings inherent to microcomputer technology, but because of the dedication of people who know how to communicate to software engineers and hobbyists. My hope is that most of the price increases apply more to their industrial computing products than their hobbyist ones. reply JKCalhoun 5 hours agorootparentIt sounds like you would be okay with a Pi clone though? Like the OP, I have avoided the clones in the past because I wanted to reward the company that were \"cause-driven\" to do this. I too am less inclined now though. reply lenerdenator 5 hours agorootparentThat depends. Is \"clone\" a synonym for \"compatible\"? If so, sure. My concern is, I don't want to spend any time whatsoever messing with stuff like driver or architecture issues. Needing a few repositories for Debian packages is okay, but pushing it. I'm not here to configure a system; I'm here to work on a project I find fun or useful. Ideally I take the latest RaspberryOS and slap it in and it \"just works\". reply aswanson 4 hours agoparentprevI want to move to radxa and orange pi but the community & documentation are lacking, along with the sourcing from China. reply cqqxo4zV46cp 6 hours agoparentprevThis “a company is great until it goes public, then it is guaranteed to be crappy” mentality only displays a lack of understanding of how ownership structures work. It is more nuanced than what you are making out. An incredibly over-simplified view bred by the knowitall “fuck MBAs” attitude on HN lately. reply avsteele 6 hours agorootparentDo they not have a fiduciary responsibility to their public shareholders now that they did not before? If a random company with a very different mission (say, Intel) comes along and offers to purchase the company at a decent share price premium they pretty much have to take it, right? reply Angostura 5 hours agorootparentIf you are talking about a fiduciary duty to \"maximise profits\", no that doesn't exist. reply jrmg 5 hours agorootparentI never get why people think this does exist. If it did, wouldn’t every company be obliged to get out of the business they’re in and start doing what would be most profitable right now? Obviously that would be absurd. reply georgyo 5 hours agorootparentSwitching gears is expensive, and if every company switched to \"the most profitable thing\" it wouldn't be profitable. This automatically prevents them from mindlessly switching. reply jrmg 1 hour agorootparentYes, that’s part of why it is absurd. reply chx 4 hours agorootparentprevJust as an interesting tidbit while we here are talking about UK companies it doesn't even exist in the US. The SCOTUS said so: > While it is certainly true that a central objective of for-profit corporations is to make money, modern corporate law does not require for-profit corporations to pursue profit at the expense of everything else, and many do not do so. https://www.law.cornell.edu/supremecourt/text/13-354 reply p_l 5 hours agorootparentprevDoesn't necessarily work that way (shareholders can sell their shares still)... ... but it's how it's marketed to people so they believe it and follow Friedman's doctrine like gospel. reply robertlagrant 4 hours agorootparentWho does this marketing? reply quickslowdown 6 hours agorootparentprevIt may seem dramatic to worry about the future of Raspberry Pi now that it's public, but I personally have seen enough mission driven companies go public, and have observed what happens when profit becomes the primary goal. You'll have to forgive me for seeing this as the beginning of the end of a beloved hobbyist SBC company. I expect the next few generations to be just fine, but eventually shareholders will start to squeeze, and the \"profit above all else\" mentality infecting public companies will set in, and then the enshittification begins. I'll be looking around at other SBCs, but I'm hapoy enough with my current RPi boards, just no longer confident I'll be happy with then in the future. reply sensanaty 4 hours agorootparentprevI can't think of a single company that went public that has improved rather than massively regressed. Could you name one of these unicorns? reply bloqs 6 hours agorootparentprevCould you help me change my mindset with some good examples to the contrary? reply jovial_cavalier 6 hours agorootparentprevAn IPO is a cashout. It's almost always certain that there are many key players in the company who had stake that no longer do. This is not a \"fuck MBAs\" thing. It is much harder to keep a business aligned with its initial purpose (and even customer interests) when it is public. reply piva00 6 hours agorootparentprevDo you have examples of companies that improved on their mission and values after going public? reply gjsman-1000 6 hours agorootparentprev> knowitall “fuck MBAs” attitude I love HN, but this is insufferable behavior. I also believe it’s why, despite our screeching, we accomplish absolutely nothing. You can’t accomplish anything when you’ve burned all bridges and then yelled “screw you.” reply Hizonner 4 hours agorootparentSorry, what was this goal that was supposed to be getting \"accomplished\"? reply haunter 6 hours agorootparentprev>“fuck MBAs” attitude on HN lately HN became way too negative about almost everything in the last couple of years, probably more since Covid. A lot of times just for the sake of being negative. The site guidelines prevent me to say that HN is turning into the site which shall not be named but it's very much true. reply wrycoder 6 hours agorootparentTech itself has become more negative lately, for a number of reasons. HN remains a marvelous place - I was just last night enjoying the variety of posts and discussion. I’ve not found anything like it elsewhere. reply Karrot_Kream 4 hours agorootparentIn many cases this sorts itself out in a few hours as downvotes rank the page. Luckily here I suspect the flamewar detector has kicked in and the page is being downranked. FWIW I don't find this in other tech spaces though they usually have a lot more young people. The media coverage of tech has certainly become focused on negatives, but if you hang out in Discord channels or talk with random Bluesky/X builders (keyword is builders here) there's definitely a culture of just talking about building or the more \"financial\" market analysis aspect. I agree with GP that HN has hit a pretty strong downward trajectory at this point that probably disincentivizes more rational commentary on topics that HN gets a lot of hate on. reply rc_mob 6 hours agorootparentprevgoogle \"enshitification\" reply Waterluvian 7 hours agoparentprevI’m happy to buy whatever satisfies my requirements. But I do anticipate the enshittification will begin. reply amelius 7 hours agorootparentWaiting for the next company to fill the not-yet-enshittified gap. reply amelius 7 hours agoparentprevAdd to that their patent of the PIO ports implementation, and I stopped being a fan. reply gjsman-1000 7 hours agorootparentYou’re assuming that without a patent, the RP2040 wouldn’t immediately have counterfeits flooding the market without any hope of legal action. This has happened with the ATMegas, this has happened with the ESP32s, this has happened with the STM32s, it’s a valid concern. You’re also assuming there wouldn’t be legal-ish pseudo-compatible clones, like the GD32 clone series of the STM32. reply johndough 6 hours agorootparent> the RP2040 wouldn’t immediately have counterfeits flooding the market without any hope of legal action That would be great. Here is the mission statement straight from raspberrypi.org: > The Raspberry Pi Foundation is a UK-based charity with the mission to enable young people to realise their full potential through the power of computing and digital technologies. Cheaper clones from China would aid that goal, but of course, it flies in the face of a fake non-profit company. And if you are only interested in preventing counterfeits, just slap the trademarked Raspberry logo on there. No need to inhibit progress with patents. reply gjsman-1000 6 hours agorootparentNo, it would be terrible, as technology development is not free. Chinese clones are the epitome of your “enshittification.” They drive prices up for the real product and invade the market with garbage. This has most recently happened in the 3D printing world, with Prusa versus BambuLab. Who actually develops an open source slicer? Who allows 3rd-party firmware? Who contributes to the community? Who abides by open source licenses? Hint: It is not the Chinese company. But at least that GPL-violating, closed-source printer was cheaper. reply elaus 6 hours agorootparentBut it was never Prusa's declared mission to get as many people into 3d printing as possible? So I don't see how this is relevant when comparing it to RPi's mission. reply madmask 6 hours agorootparentWell maybe the plan wasn't to become a martyr and die for charity. If they are providing value they should be able to thrive, not just survive. reply hales 6 hours agorootparentprev> Chinese clones are the epitome of your “enshittification.” They drive prices up for the real product and invade the market with garbage. The opposite, having no clones makes it easier for a group (like RaspberryPi) to enshittify. Enshittification is where a group first obtains a large market share with cheap/free services and then pivots to squeeze as much out as possible. Having a competent clone is a strong preventative. > This has most recently happened in the 3D printing world, with Prusa versus BambuLab. Who actually develops an open source slicer? Who allows 3rd-party firmware? Who contributes to the community? Who abides by open source licenses? Hint: It is not the Chinese company. Bambu pisses me off too. Unfortunately your parent is talking about patents, not open source vs closed source, or license violations. > No, it would be terrible, as technology development is not free. What about compatibility? Wouldn't it be good for competitors to be able to provide compatible PIO interfaces, so customers can churn from one SBC to another SBC without needing to rewrite their code? reply robertlagrant 5 hours agorootparent> Having a competent clone is a strong preventative. This causes a similar problem: people don't buy from the innovator, who needs more money to carry on innovating. They buy from the clone, who mostly only copies the innovator. reply bogwog 5 hours agorootparentprev> They drive prices up for the real product and invade the market with garbage. that's not how supply and demand works... reply etiennebausson 6 hours agorootparentprevWhy would he assume that? I assume that their mission being \"accessibility of computing to all\", and the org communicating so much on their mission focus, they would be in favor of cheap clones. reply nimbius 6 hours agoparentprevjust now? pi has been a pain in my rear for some time now. thanks to every US public school deciding the entirety of their pupils must code --and do so on a pi-- the average price surged in the states to nearly $450 a unit. its calmed down a bit now, but for that kind of cheddar i can build a second homelab. i gave away my pi's and never looked back. got a picopi for the occasional temperature sensor toy i want, but pi is for the uninitiated. HN users should be building KVM hypervisors and homelabs, podman containerized automation, etc...I definitely agree that HN users should evangelize alternatives, and since youve mentioned it i will do so as well. If you're still buying pi's, and not just to be ironic, you need to sit down and reassess your project.* *this does not include alternatives like the banana pi 3 routerboard, or other non-raspberry flavoured hacker chow. reply justin66 6 hours agorootparent> thanks to every US public school deciding the entirety of their pupils must code --and do so on a pi-- the average price surged in the states to nearly $450 a unit Wow. Messages from an alternate reality. There are at least three assertions in the above message which are untrue. reply BearOso 5 hours agorootparentAnecdotally, my local school district decided to do what the GP post says, and they're the last district I would expect to do that. I've never seen $450/unit, though. reply Levitating 4 hours agorootparentprev> HN users should be building KVM hypervisors and homelabs I'd rather not. Working with real hardware is much more fun and SBC's barely draw any power. I just wouldn't buy Pi's. Buy a Pine64 or RockPi or Milk-V or something similar. The price some of these smaller boards go by is staggering. reply distances 6 hours agorootparentprevI just recently got my first RPi for my home automation needs, so I have no emotional attachment to the RPi ecosystem of the past. Frankly I don't get your rant at all, nor do I see any reason to reassess the approach. reply robertlagrant 6 hours agorootparentI've also got one for that. Reolink doorbell camera + RPi is what I'm hoping to get to work together. reply distances 5 hours agorootparentMy apartment has old radiators with manual temperature control, so my goal is to automate that with some temperature sensors + Zigbee radiator controllers. And liberating my Hue lights as they keep nagging that I'll soon need a cloud account to control my lights at home. reply winddude 5 hours agorootparentNice I've been contemplating doing some home automation for heating, I have a dual fuel forced air furnaces, with only 1 thermostat and a fairly large house. There's a large temperature gradient between the floors, so I was thinking automate booster fans on the lower floor to pull more heat, and or cycle the recirculate fan when the temp gradient is above X. reply cobri 5 hours agorootparentprevCheck out home-assistant.io reply robertlagrant 5 hours agorootparentThat's exactly my plan! reply 4star3star 5 hours agorootparentprevWhat your critique does not address is the hardware aspect of the pi. GPIO pins engage many people who are interested in seeing tangible results of programming. reply wongarsu 4 hours agorootparentFor most hardware projects I reach for Arduino first. They are a lot cheaper (especially the knock-offs), actually in stock, and easier to reason about (fewer layers involved in the software stack, more direct hardware access) Raspberry Pis are great if you want easy internet connectivity in your hardware project, or need an OS or lots of computing power. If they were readily available and $20 each I'd grant them that they are more convenient. But at real prices and availability that's more difficult to justify reply 4star3star 52 minutes agorootparentI agree that the availability has always been a big program. I always wanted a pi zero w but never could find one at list price. reply Angostura 5 hours agorootparentprevEh? I bought a Raspberry Pi last October through Amazon in the UK and it cost £62.57 (excluding extras like a power supply and case) reply tombert 5 hours agorootparentprevYeah, honestly I have become pretty disillusioned with the entire ARM SBC market. For awhile they were super cheap and so it was easier to justify, but when they’re getting into $200+, at some point I feel like you’re going to get a better deal with a mini x86_64 PC, with considerably better performance (if not per watt, at least total performance). I had some used thin clients with AMD CPUs a few years ago. They were $50 each, and only ate like 10W of power, even under load. No, there wasn’t GPIO built in, but even three years ago there was USB 3.0 and it blew the Raspberry Pi out of the water at the time. I was running a full rack mount server for a few years after that but since cheap mini gaming PCs have been coming out on Amazon semi recently, I bought one of those to save power usage, and I have extremely happy with them. They have a GPU where I have been able to VAAPI transcoding fully working and they have a considerably faster CPU, and since RPis have gotten so expensive with spiked demand, it wasn’t even that much more expensive for a home lab. reply spacecadet 6 hours agorootparentprevWhat are you talking about, you can easily get all models same or next day at typical reseller markup. I keep seeing these comments but not experience this problem. So what gives? #fakenews And children should learn to program edge devices, they are entering a world flooded with them... reply TeMPOraL 6 hours agorootparent> And children should learn to program edge devices, they are entering a world flooded with them... Which means little unless laws substantially change to allow them to do anything with them. Right now, edge devices are protected from \"tampering\" and \"unauthorised use\" by both technical and legal means. reply badcppdev 6 hours agorootparentHaha. If you can code then you can create your own devices from scratch. People won't whine about a locked down cloud-only doorbell... they'll make their own. Or you could do that now. reply spacecadet 5 hours agorootparentprevlol, thats the most anti-hacker comment I have ever seen. Protected from tampering? What?! \"Attempting to prevent tampering\". reply TeMPOraL 3 hours agorootparentSorry, I'm depressed about SaaS, IoT, secure computing and remote attestation. And everything being rented instead of owned - from the edge devices in question to apartments they're installed in. Good luck with parents letting kids hack on stuff when it violates multiple lease agreements. reply spacecadet 1 hour agorootparentEven more of a reason go encourage kids to explore their rented environment and take ownership of their devices. reply cpressland 7 hours agoprevAs somebody with very little understanding of the business world: why would a seemingly successful organisation like this become a public company? What benefits are there? reply avsteele 6 hours agoparentIf you want to raise a ton of money for expansion this is (mostly) the only way. reply Draiken 5 hours agorootparentYou can take investment without going public. It's most likely harder, but definitely doable. reply nicce 4 hours agorootparentSony has been significant investor for years already, and owns the minority of the company. reply tim333 1 hour agoparentprevFounders can finally get their mansions. reply jjdhchdbeb 5 hours agoparentprevFunding is one reason but the real one a lot of times is you hit hard legal limits on how many people are on your cap table before it’s just easier to go public. Meaning if you want to keep giving equity to employees you are almost forced to go public. reply nicce 7 hours agoparentprevEither they want to start rapidly growing or it is exit plan for the founders. But if they company is very profitable already, this should not be necessary. They could still grow with complete mission-oriented way. reply mbs159 7 hours agorootparentI wonder what the impact would be to us (consumers) if there would be pressure from shareholders to increase profits. Will future Raspberry Pies be completely bare-bones unless you pay a hefty premium? reply thfuran 6 hours agorootparentThere will always be pressure from shareholders to increase profits. reply nicce 6 hours agorootparentprevEither prices start to increase, product quality lowers or the end-users will become the product. Or maybe all of them. Or maybe nothing changes, but unfortunately history proves otherwise. reply mytailorisrich 7 hours agoparentprevIt allows the company to raise money and existing shareholders to cash out or at least to turn their shares into liquid ones (that they can sell easily), with a \"real\" market value. In this case, the article says that they've raised $200 million, which means they sold new shares as part of the IPO. reply mschuster91 7 hours agoparentprevBetter access to external funding, for one. reply drrlvn 7 hours agoprevSomething people often miss in threads about this: > Raspberry Pi Ltd, the public company, is the commercial subsidiary of the Raspberry Pi Foundation. reply 5636588 7 hours agoparentA nonprofit overseeing a for-profit arm, where have I seen this before? reply Gormo 7 hours agorootparentPlenty of places -- it's not an especially rare model. reply supertrope 5 hours agorootparentYes it is common for a charity to own a business. The corporate profits are used to fund the education or social welfare mission. reply wrasee 4 hours agorootparentprevI think it was a dig at OpenAI reply karaterobot 5 hours agorootparentprev(silently acknowledges that this is a rhetorical question rather than a brainstorming prompt) reply bloggie 3 hours agorootparentprevRolex? They take $10 of cheap metal, sell it for $10k, and use the profits to fund the charity. reply walthamstow 7 hours agorootparentprevBBC Studios / BBC America reply supertrope 5 hours agorootparentprevThe NFL used to be a non-profit with the teams paying tax on their profits. It's since changed. reply freeone3000 7 hours agorootparentprevMozilla reply baq 7 hours agorootparentprevIKEA? reply 3D30497420 7 hours agorootparentSame thought I had, though it seems more like a tax-dodge for IKEA: https://en.wikipedia.org/wiki/IKEA#Financial_information reply portaouflop 7 hours agorootparentTIL Ikea is technically non-profit. That’s insane reply intunderflow 7 hours agorootparentprevNovo Nordisk would be the biggest example I'm aware of reply rvz 6 hours agorootparentprevMozilla and OpenAI. reply ksec 2 hours agoparentprevBecause despite the a slight shift in political tone, making profits on HN is still hugely unpopular. Even if it is a for profit under a Non-profit Foundation. reply RobotToaster 4 hours agoparentprevHow much de facto control will they have left after selling all these shares though? reply wg0 7 hours agoparentprevSounds like OpenAI reply ZuLuuuuuu 4 hours agoprevWhat is the reason they are going public? Are they trying to raise money for new products/services that are otherwise not possible? If yes, do we have an idea what kind of products/services? reply wg0 7 hours agoprevThis is high time for a group of individuals to bring a RISC-V SBC with a tweaked Linux distribution along with it to fill the gap and... later get to IPO. reply qzx_pierri 6 hours agoparentI'll make the logo reply gjsman-1000 7 hours agoparentprevRISC-V is still way too slow, and even then, there’s this stupid misunderstanding about RISC-V and open source. RISC-V means only that the instruction set is open source. It does NOT mean: - There are open source drivers - The chip design is open source - The chip design is not patented - The software is not patented - The chip has no proprietary additional instructions The NVIDIA firmware blob and drivers could run on RISC-V right now with no practical benefit to you. The ISA being open source does mean that open source CPU cores can exist, but no chip is required to use them. reply freedomben 6 hours agorootparentBeing an open ISA is still hugely important though, and a big improvement over the status quo. Don't let perfect be the enemy of good. The ISA is arguably the most important thing to have open, and the hardest to get open since it is the thing that requires the most coordination between so many disparate and disinterested entities. Today we celebrate an open ISA, tomorrow, being enabled by the open ISA, we'll be able to celebrate the next iteration of open, until at some point we get fully open. reply wg0 6 hours agorootparentprevYeah, that's the whole point. Have your own CPU cores. As for slow, Rasberry Pi class of machines are not meant to compete with Intel/AMD server grade. That's for enthusiasts (with even if 40nm process) and has to be cheap. All drivers are not needed, nobody is expecting to do triple 5k displays at 120FPS. Basic WLAN, Ethernet, Bluetooth and basic display capabilities around HD even at 30FPS are good enough for that market. As for open source, you as a computer architect, can't even make your own SoC that is on x86 or ARM without paying royalties and spending lots of money upfront. Not so with RISC-V so I guess you'll see lot more RISC-V in future. This time - 20204 is the 1999 of RISC-V if you want to borrow a precedent from Linux's rise as defacto OS on server side. reply jovial_cavalier 6 hours agorootparentprevThe Raspberry Pi used a broadcomm CPU... are you telling me that's open hardware? Is there any SOC chip that is totally open with no proprietary features?? reply Havoc 7 hours agoprevAs much as I love rasps (think I’ve got a dozen or so) I struggle to see their moat. They certainly have an edge on ecosystem and software polish but it’s pretty slim. I’ve got a mixed k8s cluster - half rasps half orange pi…and I’m having more issues on the rasp side. reply bantunes 7 hours agoparentThere are a hundred outfits out of Shenzen doing what they do, but Raspberry Pi's move a lot of units because they have backwards compatibility, good support and that has fostered a healthy ecosystem over the years. If even a single Chinese outfit got the memo, they'd be out of business quickly - but they don't. I don't understand why customer support is so hard to get right for these Chinese companies. The pervasive mindset seems to be beating others on price is enough. For the maker market Pi's are aiming for, it really isn't. reply janosdebugs 7 hours agoparentprevName recognition? You know exactly what you get, which isn't necessarily true for the bajilion clones out there. You know that if you buy an rPi, you can buy a replacement tomorrow and it will work the same. In a world where I can't even rebuy a laptop of the same make and model a few months later that's quite the advantage. reply loudmax 7 hours agoparentprevGeerlingguy did a video on this topic: https://www.youtube.com/watch?v=GKGtRrElu30 Raspberry's moat largely comes to support and compatibility. They've fostered such an ecosystem around their SBCs that they're usually the safer choice if you want to build something with any expectation of long term stability. Now that they're public, there's no guarantee that they'll maintain this commitment. They may feel pressure to sacrifice long term success for short term profitability. We'll have to see how this plays out. reply afavour 7 hours agoparentprevFor the likes of us there isn’t really much of a moat but RPi puts a lot of effort into accessibility for beginners. Look at orangepi.org: “We offer one-stop customized service based on open source products to shorten the path from technology to application” vs raspberrypi.org: “Empowering young people to use computing technologies to shape the world” The underlying tech is very similar but the marketing and community aims are very different. If RPi can capture a solid chunk of the education market they’ll be able to dine out on that, and it doesn’t seem like OPi is equipped to do the same. reply distances 5 hours agorootparentFirst impression when visiting orangepi.org with Firefox: \"Hmm. We’re having trouble finding that site.\" They don't have TLS enabled. Doesn't exactly promise much on the software side of the board either. reply BiteCode_dev 6 hours agoparentprevCommunity, branding, hardware compat and software support. Like nvidia. reply MagicMoonlight 7 hours agoprevI love that certain people get to buy and sell it before the public can. It’s just a fairer way of doing it. reply nickpeterson 6 hours agoparentyep, the stock market is truly an example of fair and ethical policies. reply adamtaylor_13 6 hours agoparentprev\"Step aside Tommy, let the _MEN_ handle this.\" But yeah, kinda wack. reply karolist 6 hours agoprevI never quite got the appeal of Rpi's. When I needed a silent low power home server box 10 years ago I went with Intel Atom x86 based board which supported virtualization and had expandable memory. When I needed a small low power box for and observer node this year I bought a Dell Wyse 3040 for 40€, also x86. If I wanted a general purpose PC I'd still go with a NuC or Minisforum these days. reply navaati 6 hours agoparentNone of these have GPIOs, typically. A usecase you didn't mention is \"I want a MCU to do electronics but easier to program, with Linux and a shell\". reply bane 6 hours agorootparentJust get a usb to gpio adapter. Arduinis can be flashed to do this also. So it's just a few dollars to get the capability on a PC. reply RobotToaster 4 hours agoparentprevFor a lot of stuff it just comes down to convenience, there's community projects that are primarily distributed as raspi sd card images (home assistant and octoprint come to mind). reply Zenzero 6 hours agoparentprevIt's marketing. For someone like me with little familiarity of the mini computer and microcontroller world, I know about raspberry pi and arduino. I know there are many others out there with likely better value and quality, but the perception of accessibility to a newbie drops off of a cliff. reply mattl 6 hours agoparentprevI’m guessing that 40 Euro price isn’t MSRP but some discount on a used one? reply RobotToaster 4 hours agorootparentYes, but there seems to be a pretty reliable supply of used thin clients. I have no idea who still buys new thin clients, but whoever it is must replace them regularly. reply flak48 6 hours agoparentprevDo any of the alternatives you've listed expose GPIO pins ? reply wirthjason 7 hours agoprevThe words “public company” often carry a negative tone but ownership rules are complicated. It doesn’t mean much until you know the specifics. For example a non-tech example of a publicly owned corporation that people might not think of is the Green Bay Packers. https://www.profootballnetwork.com/who-owns-the-green-bay-pa... reply awinter-py 7 hours agoprevarduino is the other name in this space + has been making interesting moves; their industrial / PLC offering has gotten very serious in the last few years and they are ahead of pi on the cloud front (though open question whether either of them will do well in cloud; their customers are too savvy + nimble for a lock-in play) arduino is approaching the business bottom up (tiny well-known chips), pi top-down (it's just linux with some GPIO pins) also have competitors at the margin like nvidia (who could legit own 'iot edge AI' if that ever even matters) + waveshare, the off-brand pi manufacturer who remarkably has way more pi-compatible devices than actual pi excited for IPO if it means the #1 linux board finally supports suspend-to-ram reply dschuetz 7 hours agoprevpublic company = money milking machine. so long, and thanks for all the pies. reply neilv 7 hours agoprevThis model has been a mixed success for Mozilla and OpenAI. The TechCrunch article is speaking only of it like any other commercial investment opportunity. Where's a good writeup of charity/non-profit mission concerns about this commercial subsidiary? (Example from early on in Raspberry Pi history: There were concerns about pragmatic compromises of open source or libre software principles that the RasPi made, so that they could use the closed-software Broadcom SoC, presumably to deliver an otherwise good education-oriented SBC at a price point that made it accessible and got uptake.) reply secfirstmd 7 hours agoparentSadly Mozilla is essentially a company that survives on a Google handed, given by the big G to reduce any risk of anti-trust. reply joshstrange 6 hours agoprevWhen the first Raspberry Pis came out I was in love. It was incredible, a full (headless) computer that was small and cheap. I played with it for a while but never had a great long-term use case for it. I bought pretty much every Pi up to the 4, always trying to find a place it could occupy in my life but never really managed to find a good use. I ran Home Assistant on a 3 then later on 4 (with a super nice case, NVME hat+stick, etc) but HA was not super stable. I, wrongly, blamed it on the HA software for a long time then I bought a Beelink mini computer and moved HA to it and I have literally not thought about HA since, it's worked flawlessly. Maybe I had bad luck, maybe I happened to get 6 (I had multiple of certain Pis) defective boards but they always felt \"unstable\" to me. They might run fine for months then randomly need to be rebooted. I'm sure some of you will say \"I've had a Raspberry Pi 1 running constantly since I bought it and it's rock solid\", good for you, that was not my experience. During COVID (maybe before as well) it came out that the company was prioritizing bulk sales over sales to individuals (or rather sales to companies that resold to individuals) which soured me. This news just further confirms my belief that the company and product I loved is dead and probably has been for some time. It was nice while it lasted. reply simcop2387 6 hours agoparent> Maybe I had bad luck, maybe I happened to get 6 (I had multiple of certain Pis) defective boards but they always felt \"unstable\" to me. I've never had bad boards but I've had similar experiences like this, it's always turned out to be a power supply issue, or a counterfit sd card. Can't say I know what your experience will be but those are the places I'd look at first before the board. > During COVID (maybe before as well) it came out that the company was prioritizing bulk sales over sales to individuals (or rather sales to companies that resold to individuals) which soured me. If you go watch some of the interviews on this, it was a combination of things: 1) Supply chain issues, like everything during COVID, they couldn't keep up manufacturing capacity. 2) They weren't favoring resellers, but were favoring \"industrial\" or \"commercial\" (whatever you want to call them) customers because of pre-existing contracts that they had to fulfill or it'd kill them since they'd have to pay fines or deal with contract disputes. Since it was more guaranteed revenue it also helped them keep the lights on without just shutting down at the time too. Still didn't feel great either at the time, but I'd say that the way you put it reads as mis-charactarising what happened as being a purely profit motivate thing. reply BillFranklin 6 hours agoparentprevI think they were trying to stop resellers price gouging during the chip shortage crisis. On ebay you could buy Pis for well over what they were available for on first-party sites (just like PlayStation 5s!). As a counterpoint, they let me buy Pis as an individual, but I had to send in a picture of myself with a dated piece of paper like a Reddit AMA to prove I was legit. FWIW there are Pi alternatives without the branding, but usually for the super low price point mini-PCs you are after you need to go to China to firms like Beelink... reply jovial_cavalier 6 hours agorootparentThey could have just raised the prices for individual customers themselves instead of creating a massive opportunity to price gouge. Prioritizing bulk orders sounds like they just didn't want to be the front-end. reply moffkalast 4 hours agoparentprevSounds like problems with the sdcard, and that really is a problem. If your OS is not read-only (which isn't a simple or practical setup) it'll eventually wear out from constant logging and die. That might be part of the instability you saw. So far the solution for long term stability is either that or to get a USB/PCIe NVMe instead. eMMC would be a good solution which is why nearly every single other SBC manufacturer supports it, but of course the Pi foundation stubbornly refuses to touch it because...? Maybe just to annoy us. reply joshstrange 4 hours agorootparentI thought so as well, that's why my latest RPi setup used this case [0] with this [2] expansion board. I still had instability. I didn't cheap out on the SD cards I bought either. I threw lots of money at a \"cheap\" board to try to improve reliability but just never got there. [0] https://www.amazon.com/gp/product/B07WP8WC3V/ [1] https://www.amazon.com/gp/product/B08MHYWJCP reply Foivos 7 hours agoprevI cannot find the stock ticker symbol. I am aware that they are not publicly traded yet, but the ticker should be known already, right? reply redleader55 7 hours agoparentLSE:RPI https://www.londonstockexchange.com/stock/RPI/raspberry-pi-h... reply swarnie 7 hours agoparentprevRaspberry Pi Holdings PLC (RPI.L) LSE Trading since yesterday. reply isoprophlex 7 hours agoprevHope they can keep their focus on simplicity, usability and developer experience now that they're beholden to the whims of their shareholders. reply andrewstuart 7 hours agoprevIntel N100 already is a better option for many user cases of Raspberry Pi. reply 404mm 6 hours agoparentI agree. I upgraded from pi 4 to N100 with 16GB RAM and 512NVMe SSD for about $145. And it included a case and power adapter. reply RobotToaster 4 hours agoprevI’m surprised it lasted this long. It was always kinda just a marketing gimmick for broadcom that got out of hand. reply brainzap 6 hours agoprevGoodbye quality reply bane 4 hours agoprevI think it's important to consider some of what Raspberry Pi's mission [1] and how they have kept and/or swayed from that. Short version, the Foundation was established to support the education and development of skilled technologists (subtext: in the UK). It came from the observation that enrollment in Computer Science courses was dropping, and that the kind of accessibility and hack-ability of previous technologies was becoming less available to people with potential interests. Inspired by previous UK efforts like the BBC Micro (and other 8-bits), and other concurrent educational efforts, they decided their niche was going to be cheap, single board, all-in-one computers. What I really respect from the RPI folks is that they've really tried to focus on building a viable UK-based technology company - as much as possible, without being zealots about it. That's their unstated mission, to revitalize the U.K.'s technical capacity: - Based on an ARM processor, a British company - Has tried to keep as much manufacturing as possible in the U.K. (Sony UK Technology Centre) - Went public on the London Stock Exchange All that being said, I feel that they've deviated a bit from this as their primary mission. During COVID when supply chains were messed up, Pi manufacturing and shipping was prioritized to large scale buyers, where the supply then seemed to end up in industrial use-cases instead of the hands of people learning. I've heard an argument, which I do think is valid, that getting a familiar tech stack into industry provides an on-ramp into jobs to the same people who had learned how to do technology on those same Pis. But there's now at least a 2 or 3 year gap where new people couldn't really onboard into the ecosystem because of availability and cost. The second challenge is the Pi 5. I do think that expectations were simply too high here and the community was expecting basically a usable desktop with fast PCI lanes and gobs of memory for $35. What's delivered is amazing, but once you build it into a \"desktop\", you may as well have just bought a cheap intel desktop. And that I think is the real existential problem for the Foundation today. If I were the Foundation, post Pi 5, I would be looking very seriously at alternative form factors as the future. The CM line and HAT concept has been very successful, the hackability of Pis have been amazing. Perhaps there are ways to make the concept even more modular, or more hackable. Maybe offer baseline RAM amounts soldered on, but upgradability with new modules? Make other wireless technologies a core part of the offering (HAM Radio/BTLE/4G/LTE/LoRA, etc.)? Tap more into the 3D printer space with readily available mounts and housings that provide sufficient cooling and airflow. Ready-to-go cluster computing variants, why not mini-Blade-likes using Compute Modules? What ways can the core Pi technologies expand in order to serve their primary mission and expand the ways that people can learn about technologies? Perhaps they're thinking the same thing and this is why they need to raise more money? 1 - https://static.raspberrypi.org/files/about/RaspberryPiFounda... reply seba_dos1 2 hours agoprevBye bye, Raspberry Pi. reply koolala 7 hours agoprevthey have been... sold... out... reply rvz 6 hours agoprevTime for some Raspberry Pi executives to cash out their millions and dump it all of retail before the semiconductor crash beings. reply dukeyukey 5 hours agoparentSince RPI don't design nor manufacture semiconductors, but use them in the product they do sell, surely a crash would benefit them? reply matthewfelgate 7 hours agoprevI can't find them on the FreeTrade app yet :-( reply mypastself 6 hours agoparentI’d be interested to hear your thesis, if you’re planning to invest. Substantially increased industrial use? I own quite a few Pis (although mini PCs have largely replaced them in my homelab), but I’m not sure why we should be expecting major growth. reply matthews2 7 hours agoparentprevRetail trading starts on Friday morning. reply manojlds 6 hours agorootparentHow do people (retail) participate in IPOs in the UK? reply scrlk 6 hours agorootparentApply for an allocation via your broker. E.g.: https://www.hl.co.uk/shares/ipos-and-new-issues/raspberry-pi... reply pjmlp 5 hours agoprevNow they are just as the boards that predated them like the Beagle Board. reply hosteur 7 hours agoprevnext [3 more] [flagged] wg0 7 hours agoparentnext [3 more] [flagged] TheNewsIsHere 5 hours agorootparentYou've inspired me: \"Our parent company is a non-profit, but you'll be paying a few taxes!\" reply doctor_eval 6 hours agorootparentprevUgh you just gave me chills. reply lililililililll 7 hours agoprevnext [2 more] [flagged] mbs159 7 hours agoparentAccording to the article, the market should open on Friday. Currently trading is limited to institutional shareholders. reply bobmcnamara 7 hours agoprevNow we're never going to get better documentation /$. reply kevindamm 7 hours agoparentBe the change... /¢ reply meindnoch 7 hours agoprevAnd... this is... good? :-/ reply web3-is-a-scam 7 hours agoprev [–] I can’t even get their product anywhere at a decent price, why should I invest in this company? reply cqqxo4zV46cp 6 hours agoparent [–] It doesn’t sound like you invest, then. You’re describing a product with well known stock shortages that have, in floating, unlocked a bunch of capital which can among other things help them scale their production to meet demand. That sounds like a good thing both as a shareholder and as a consumer. reply TheNewsIsHere 5 hours agorootparentI am concerned that ultimately whatever supply chain and manufacturing improvements they see from raising this capital will benefit their commercial/industrial customers more than their non-profit/educational/enthusiast customers. I'm not asserting that will happen, but I'm concerned it will. Where I am it has been next to impossible to order a non-kit model of Pi released within the past several years since at least mid-pandemic. I started buying up SFF Lenovo boxes on eBay, which I've been somewhat happier with since I never used some of the Pi's standout features like GPIO. reply web3-is-a-scam 6 hours agorootparentprev [–] How am I supposed to know if something is worth investing in if I can’t even use it? A pinky promise that maybe I’ll be able to get one someday isn't a good deal. I want to know what I am getting when I put my money in. Maybe if I was a VC and that’s what they were begging for my money for, but as a retail investor the value proposition is basically nothing. The company markets itself as an educational tool, but 70% of its sales are industrial? Like I get “bringing low cost computers to everyone” feels and sounds good in press releases but when the business actually focuses on something different - that seems counterproductive to their stated goal - what am I supposed to take from that? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Raspberry Pi has gone public with an IPO on the London Stock Exchange, initially priced at £2.80 per share, valuing the company at £542 million ($690 million).",
      "The shares rose 32% to £3.70, potentially raising over $200 million, with retail investors able to trade shares starting Friday.",
      "Raspberry Pi's single-board computers, popular among hobbyists, are increasingly used in industrial applications, which now account for 72% of its sales."
    ],
    "commentSummary": [
      "Raspberry Pi has become a public company, raising concerns among some customers about a shift from its mission-driven approach to profit-making.",
      "The company has been accused of anti-competitive practices and prioritizing bulk sales over individual customers during the COVID-19 pandemic.",
      "Despite these issues, Raspberry Pi remains popular due to its strong ecosystem, documentation, and support, though some users are considering alternatives like Intel NUCs and second-hand PCs for better performance and stability."
    ],
    "points": 153,
    "commentCount": 207,
    "retryCount": 0,
    "time": 1718189319
  },
  {
    "id": 40654734,
    "title": "Why SQLite's Popularity is Soaring: Insights from Experts",
    "originLink": "https://syntax.fm/show/779/why-sqlite-is-taking-over-with-brian-holt-and-marco-bambini",
    "originBody": "779 June 7th, 2024 × #sqlite#databases#local-first Why SQLite is Taking Over with Brian Holt & Marco Bambini Discussion on why SQLite is gaining popularity, its advantages like efficiency, speed and stability, misconceptions about capabilities, and how SQLite Cloud enhances it by making it shareable and adding enterprise features. Brian Holt Guest Marco Bambini Guest Wes Bos Host Scott Tolinski Host Play Episode 779 or Show Notes Transcript Scott and CJ dive into the world of SQLite Cloud with special guests Brian Holt and Marco Bambini. They explore why SQLite is gaining traction, its unique features, and the misconceptions surrounding its use—let's get into it! Show Notes 00:00 Welcome to Syntax! 01:20 Who is Brian Holt? 02:26 Who is Marco Bambini? 05:12 Why are people starting to talk so much about SQLite now? 08:47 What makes SQLite special or interesting? 09:46 What is a big misconception about SQLite? 11:13 Installed by default in operating systems. 12:03 A perception that SQLite is intended for single users. 13:36 Convincing developers it's a full-featured solution. 15:11 What does SQLite do better than Postgres or MySQL? 17:30 SQLite Cloud & local first features. 20:38 Where does SQLite store the offline information? 23:08 Are you typically reaching for ORMs? 25:00 What is SQLite Cloud? 27:29 What makes for an approachable software? 29:18 What make SQLite cloud different from other hosted SQLite options? 32:13 Is SQLite still evolving? 34:40 What about branching? 37:37 What is the GA timeline? 40:04 How does SQLite actually work? 41:19 Questions about security. 44:28 But does it scale? 45:52 Sick Picks + Shameless Plugs. Sick Picks Brian: Trainer Road Marco: Tennis Shameless Plugs Brian: SQLite Cloud, Frontend Masters - Containers. Hit us up on Socials! Syntax: X Instagram Tiktok LinkedIn Threads Wes: X Instagram Tiktok LinkedIn Threads Scott:X Instagram Tiktok LinkedIn Threads Randy: X Instagram YouTube Threads ← Prev #778 11 Habits of Highly Effective Developers Next #780 → Cloud Storage: Bandwidth, Storage and BIG ZIPS 15% off your swag order Subscribe to the Syntax Snack Pack Newsletter Email Subscribe",
    "commentLink": "https://news.ycombinator.com/item?id=40654734",
    "commentBody": "Discussion on why SQLite is gaining popularity [audio] (syntax.fm)150 points by thunderbong 13 hours agohidepastfavorite155 comments OskarS 10 hours agoThe killer feature for me with SQLite is as an application file format: https://sqlite.org/appfileformat.html It has SO MANY advantages over every other solution. It can do the \"just a bundle of files\" thing, but it can also do all this rich data as well. And unlike most \"just a bundle of files\" formats, it's incrementally updatable, it's so good. We're using this in production right now and couldn't be happier with it, wouldn't dream of using anything else at this point. One notable application that uses SQLite for this is Audacity: all the stuff you record is streamed into a SQLite database (the project file). This is a huge reason why Audacity is so good for recording long sessions, and why it's so resilient against crashes and failures. reply belter 8 hours agoparentIn 2024, not using SQLite as your application file format, should really force an engineering team to have a damn good explanation.... https://www.sqlite.org/famous.html reply fauigerzigerk 7 hours agorootparentThose journal/wal files create a number of potential issues for user visible application files. E.g. a user could copy a database file into a directory that already has an old journal file, corrupting the database [1]. Or users could simply get confused about what to do with those files and contact support a million times just to ask why these files exist, whether they need to be copied as well, why they are getting large, etc. These issues can be worked around, but I wonder how many apps that use SQLite actually bother doing that. Clearly not all of them [2]. Avoiding file corruption is non-trivial regardless of file format, but giving users additional ways to corrupt their data is never a good thing. So I think for small, user visible application files that don't need any database functionality, the onus is still on engineers to justify their choice of SQLite. [1] https://www.sqlite.org/howtocorrupt.html [2] https://forum.audacityteam.org/t/aup3-wal-file-remains-and-l... reply renonce 20 minutes agorootparent> 1.4. Mispairing database files and hot journals I’m surprised it would corrupt the database. The database could have included a unique identifier as well as in the journal and refuse opening the database if the identifiers don’t match. reply eli 2 hours agorootparentprevYou could simply turn off WAL for a simple application database, if you were really worried. You lose some performance and scale, but it should work fine. reply fauigerzigerk 1 minute agorootparentSetting the journal_mode to OFF or MEMORY is a sure-fire way of corrupting the database. It's not fine. https://sqlite.org/pragma.html#pragma_journal_mode wil421 6 hours agorootparentprev>Or users could simply get confused about what to do with those files and contact support a million times just to ask why these files exist, whether they need to be copied as well, why they are getting large, etc. This reminds me of a college roommate who was dissatisfied with his computer, likely due to malware from sketchy sites. He decided to delete large or suspicious files from window’s system32 folder. By suspicious I mean he didn’t like the file name. He had to get a new computer later in the semester. reply macintux 6 hours agorootparentDecades ago I troubleshot a Solaris server that wouldn’t boot, and discovered the kernel file was missing. I suspect someone had tried to deal with a too-small root partition by removing large files. reply throwup238 8 hours agorootparentprev> The United States Library of Congress recognizes SQLite as a recommended storage format for preservation of digital content. That’s one hell of an endorsement. reply threatofrain 7 hours agorootparentprevIs SQLite the best solution for on-device logs? reply ecuaflo 7 hours agorootparentprevunfortunately in the browser it needs to be downloaded so you’re stuck with indexeddb reply iansinnott 3 hours agorootparentIt's too bad WebSQL never took off, as that was sqlite built in to the browser. From memory the wasm binary of wa-sqlite is ~1mb, which is certainly not nothing but is an acceptable one-time download size for many web apps. I've seen websites with single image files larger than that. Not an advocating for more bloated web apps, but a sqlite download might not be a deal breaker. reply sgbeal 3 hours agorootparentprev> unfortunately in the browser it needs to be downloaded so you’re stuck with indexeddb That argument implies that we're stuck with the browser's single built-in home page because every other web page has to be downloaded. How is having to download sqlite3.wasm any different from having to download HTML, CSS, JS, images, etc.? reply w0m 3 hours agorootparent> How is having to download sqlite3.wasm any different from 'unnecessary' file size. K matter on the web still. I'm not normally a web guy, but from https://sqlite.org/download.html it looks like it's ~800k extra added to initial page load? Based on average mobile speed in US of 97.09Mbps; that's an extra half second added to initial page load. That's not trivial; maybe an extra 15% bounce rate on initial hits. reply sgbeal 3 hours agorootparent> ~800k extra added to initial page load? If you're serving it uncompressed, which no production-grade site will (for a given definition of \"no\"/\"none\"). > K matter on the web still. Not, i opine, for the types of apps which want to host client-side databases. These are client-side applications, not \"web pages.\" Even a bare-bones, database-less google.com is now 3.14mb uncompressed (1.32 compressed), and that's not counting the pieces which uBlock Origin keep from loading. It loads somewhere around 2MB (uncompressed) of JS. Last i checked, gdrive downloaded some 14mb to get up and running. > that's an extra half second added to initial page load. That's not trivial; We'll have to agree to disagree on whether half a second extra initial-hit-only load time is trivial. reply fwip 1 hour agorootparent> If you're serving it uncompressed That 847 KB is actually the size of the (compressed) zip file, but it does contain other files. The compressed WASM + the JS to load it is probably about 500K, depending on the specifics of compression & minification. reply Digit-Al 1 hour agorootparentprevMakes me laugh. When I got my first computer in 1987 it took so long to load a small game from cassette that I could go downstairs, boil a kettle, make a cuppa, and bring it back to my room, and the game would be just about finished loading. Now people are moaning about an extra half a second lol How things change. reply paulddraper 29 minutes agorootparentNo, the narrative here is that things used to be fast and idiot JS devs have made everything bloated and slow. reply sureglymop 9 hours agoparentprevI agree. And the best feature for me is \"ATTACH DATABASE\". Then one can just join together tables from multiple files etc. reply kabanka 8 hours agoparentprevAlso, the in-memory feature (‘:memory:’) allows you to start working “on the fly” and saving the work to disk later. Really convenient for the user, zero cost for the developer (same API for in-memory/fs db and smooth transition between them). reply TeMPOraL 8 hours agorootparentIf you're careful about it, you can store the canonical runtime state for e.g. a videogame in :memory: database; you then get saving/loading for free with SQLite backup API. reply bryancoxwell 7 hours agorootparentCan you explain what you mean about needing to be careful about it? reply TeMPOraL 7 hours agorootparentYou need to store all the data that define the runtime state, so it can be dumped and recovered, without accidentally capturing information or references to things that live outside the DB and won't survive application restart. It's a more generic case of the \"don't dump pointers to files, if you can't guarantee they'll be valid after reload\". reply bryancoxwell 7 hours agorootparentThanks! reply v7n 7 hours agorootparentprevIn addition to paying attention to references, perhaps it makes sense to always keep e.g. player inventory state in there, but something that can change every frame, like player position data, should perhaps only be synced just prior to the snapshot if it would otherwise degrade performance too much. This is of course another layer of complexity (volatile vs non-volatile game state) to think about. reply TeMPOraL 7 hours agorootparentThis. In my testing some years ago, trying to make a basic Roguelike game with the state stored entitely in SQLite, the in-memory db could support reading the visible portion of the map and positions of entities at 60 FPS with time to spare, on a modest machine, and with slight FFI penalty as the code was written in Common Lisp (SBCL). So while you may not want to do this for a first-person shooter, there are many types of games that could in fact live entirely in SQLite, save for some transient visual FX. reply semi 4 hours agorootparentI'm pretty far removed from game dev but curious.. is the in mem sqlite DB the only representation of game state or is it just 'mirroring' the 'real' values in ram? like if there's a Score on the HUD, are you doing a SQL select to read the value on every frame? Or is this just a way to serialize and deserialization the game state to automatically save the game so it could be reloaded if it closed/crashed without explicitly running a 'save game' function? reply TeMPOraL 3 hours agorootparent> if there's a Score on the HUD, are you doing a SQL select to read the value on every frame? Yes. That was the point of my experiments, after I realized that good chunks of the data structures I set up for my game look suspiciously similar to indices and materialized views. So I figured, why waste time reinventing them poorly, when I could use a proper database for it, and do something silly in the process? In a way, it's also coming back to the origin of the ECS pattern. The way it was originally designed, in MMO space, the entity was a primary key into component tables, and systems did a multiple JOIN query. reply nathanfries 1 hour agorootparentI followed this line of thinking last week due to curiosity w.r.t ECS & SQLite. I found that the bottleneck was not on the reads, but on the complexity of the writes (iterating each entity and subsequently you have num_systems X num_components writes). You can actually avoid this entirely if you write your systems in SQL. Since I had already thrown out logic & reason for the sake of curiosity, I took it a step further and learned that the Bun JS runtime actually has SQLite baked in, which allows you to create a :memory: db that can be accessed _synchronously_ avoiding modification of most ECS implementations. (I'm not familiar with the larger SQLite ecosystem, but being a largely TS developer this was very foreign to me) reply out_of_protocol 7 hours agoparentprevTheres is \"Pack\" archive format that does exactly this for general-purpose archives (sqlite + zstd for compression). https://news.ycombinator.com/item?id=39793805 https://pack.ac/ works faster AND more compact than .tar.zstd and .7z.zstd (patched 7z with zstd compression) reply jhanschoo 6 hours agoparentprevIt doesn't list obvious cases where you shouldn't take this approach, so I have one off the top of my head that SQLite doesn't seem like it's a good fit, I'm interested in whether I'm wrong on this. The article discusses storing files as BLOBs. But it looks to me like this isn't a good fit for sufficiently memory-constrained environments, given a task involving processing a very large file in a sequential manner, like with media reencoding, or video playback. It seems like the traditional file access patterns are better suited for those tasks. reply OskarS 5 hours agorootparentIt's very easy to design a schema that breaks up blobs into chunks. SQLite is excellent for memory constrained environments, with even mathematical proofs of maximum memory usage: https://www.sqlite.org/malloc.html reply jitl 5 hours agorootparentprevYou can incrementally read or write the value in a BLOB column of a row. You can read/write etc using a file descriptor like API: https://www.sqlite.org/c3ref/blob_open.html reply boffinAudio 9 hours agoparentprevI used SQLite as the file format for a game system, and it was probably one of the most successful technical decisions I've made in 40 years. The fact that the artists and developers and even managers could use standard database tools to lay out changes to the game levels, characters and even physics behaviours, meant I didn't have to write a ton of tooling to allow them access to all of those things - they just opened up their favourite database GUI, added the data to the sqlite database, and shared the single file around. Nowadays I just can't see any other way to do this, other than by maintaining a well-ordered .zip file or filesystem structure, with strict validation being something that has to be programmed. I'd just prefer to sqlite all the things. reply jeremyjh 8 hours agorootparentThat sounds like a decent solution, but I'd rather have all that content in text files so there is meaningful version control with features like diffs and merges. Some engines will store the data this way but give you an editor to manage it, and there are dedicated level editors as well but every project is different and it could be the case none of that would have been helpful to you. reply Karellen 7 hours agorootparentAFAIK git has pluggable diff and merge engines. I wonder if it would be possible to create a diff/merge git plugin for sqlite files that does an intermediate conversion to .sql dump files, with some kind of serialisation normalisation so that rows are output in a stable order between runs. reply simonw 7 hours agorootparentI haven't tried it with git pluggable diff tools yet, but I wrote a tool that lets me convert a SQLite database into a bunch of plain text files so I can manage them in git: https://github.com/simonw/sqlite-diffable reply TeMPOraL 7 hours agorootparentprevIt is. It's a bit of a hassle, because the git mechanism that allows it is considered sensitive, so all users of your repo will need to separately configure this if they want to use it. Bigger problem for me is that none of the code review systems I know of support this. At work, we've been wishing to have something like this in Gerrit, due to multiple SQLite DBs and other binary documents living in the repo, but as far as we can tell, we'd have to modify Gerrit sources directly to make it happen, which IT would frown at and nobody has time for anyway. reply out_of_protocol 6 hours agorootparentprevThousands of text files laying on bare filesystem is rather slow solution, so additional step to convert between text files and sqlite is needed. Never have seen ready to use solutions, only heard of ad-hoc scripts reply robertlagrant 7 hours agorootparentprevWe maintain a compromise with Alembic migration files in source control. This does mean people need to do Alembic things to make changes, but if they can do that little bit it's very good, and fully diffable of course. reply boffinAudio 7 hours agorootparentprevWe solved this problem by checking in full sql dumps of the content database into source control, in between major changes. Not that we ever had to check and see what went wrong with the content side - using an sqlite database pretty much ensured that things were in the right format before it hit the filesystem assets, and we had practially zero bugs from introducing this methodology to the project. reply romwell 9 hours agoparentprevOnce upon a time, I revamped a binary file format for a propriety industrial-scientific software. The software was writing vast arrays of numbers into a file, and reading them based on offsets. As the software (and its data) evolved, versioning became a nightmare. I ended up writing an ORM serializer which read from/wrote into an SQLite DB with a variation on an EAV schema. One of my favorite projects, result-wise. Reading and writing was clean and efficient; the format was portable, language-agnostic, compact. Versions of the new format were backwards- (and, within reason, forward-) compatible with each other (I forced specifying sane defaults for attributes if they're missing). I'm still thinking of reimplementing a project like that as FOSS one day. Serializing numeric data to something like JSON is a waste, and the structured binary formats I've looked at aren't nearly as thought out as SQLite. reply gchamonlive 8 hours agorootparentFor numeric values only, any advantage of using sqlite over let's say parquet with Apache Arrow/duckdb? reply avereveard 7 hours agorootparentSQLite let's you decide when and if vacuuming after partial row modifications. Does arrow allows that I don't know but it's most likely the best SQLite feature you can open a massive database and do one value change really really fast without having to write and compact the entire thing all the times. reply andyferris 7 hours agorootparentArrow isn’t great at random insertion and deletion, as each column is a contiguous array. If it’s on disk those arrays are probably packed tightly together, so you can’t even append rows. (I have an idea to implement an LSM tree where each layer is an arrow file, which should allow for faster mutations while maintaining a lot of the benefits of arrow. But I haven’t got around to it). reply CJefferson 7 hours agorootparentprevGiven SQLite is already distributed on basically every OS,is amazingly rock solid, I’d ask instead any advantage to using Apache Arrow/duckdb? I don’t have great experience with either. reply jitl 5 hours agorootparentDuckDB can have massive performance advantage over SQLite for things like aggregates on wide tables or or filters on unindexed columns, with advantage growing with the size of the table. We tried a few head to heads between the two and while we ultimately went with SQLite, we may go back and double write everything to DuckDB to get better performance for those kinds of queries on the same data set. Duckdb can also read / import SQLite natively so we could ingest lazily from sqlite disk -> duckdb. Biggest issue with duckdb is some memory leaks and crashes for the nodejs driver. Seems not production ready. reply prirun 5 hours agorootparentprevSQLite encodes integers in a variable-length format, so it can be very space-efficient compared to always storing integers in 4 or 8 bytes. reply Jean-Papoulos 7 hours agorootparentprevStrangely enough I'm doing the exact same thing. My case seems a bit more complicated since we're dumping not only long arrays of floats, but also a bunch of miscellaneous data (datetimes, unique datapoints, ids) right before/after those numbers. So it's even more of a nightmare... Some of these files have version numbers in the 30's (oh yeah, and file versions are based on file size.) And did I mention that the file handles are kept open for the whole program duration (might be multiple hours) ? That's fun, data loss is almost a daily occurrence. It's the main thing I'm refactoring right now. reply therein 9 hours agoparentprevI don't use Audacity often, maybe once or twice a month at most but stability and snappiness aren't really things I associate with Audacity. None of the crashes I ever encountered with Audacity has been due to SQLite of course. If anything this article made me want to use SQLite in more of my projects where I need to store local data. reply bux93 9 hours agorootparentThe resiliency against crashes offered by it's SQLite file format is much needed. Because it crashes a lot. reply ammo1662 9 hours agoprevWhen I start a project, my priority will be SQLite>PostgreSQL>Others. If the application only runs on one server, then definitely SQLite. That means you do not need fail over, replication or other fancy features. Why not simple SQLite? A complex system is more likely to have problems. Network, TLS, runtime, your application may have so many reasons to crash. But for SQLite, just check IO. If it does not have problem, then you check your application. reply chipdart 9 hours agoparent> If the application only runs on one server, then definitely SQLite. That means you do not need fail over, replication or other fancy features. Why not simple SQLite? I agree. So many usecases require a bounded context, and even ephemeral data stores. Memory caches are all the rage but the cost of doing network calls to fetch data from a memory cache can eclipse the cost of simply fetching the data from a local disk. reply ammo1662 7 hours agorootparentI experienced that an application randomly throw exceptions when trying to access the SQL Server in the same PC. And finally we found that it is caused by a broken firewall software. A traditional DBMS is a quite complex application, sometimes using that may introduce more bugs than your application itself. reply TheRoque 8 hours agoparentprevWell, since the SQLite database is one simple file, isn't SQLite a hassle when you have some concurrency/parallelism ? And what if you have separate services that need to access the same DB ? reply ammo1662 7 hours agorootparent> isn't SQLite a hassle when you have some concurrency/parallelism If you decide to put your application on a single server, that means you do not care about the single point failure, and all your workload can be handled by the single server. So you can just run only one instance of your application, then you will not have such kind of problem. Even if you want to have multiple instances of your application running on the same machine, SQLite can also handle that. > what if you have separate services that need to access the same DB? That means one single server cannot handle the workload. If the bottleneck is in the database module, a cluster is required to process the data. Clearly the SQLite is not a good option in this case. If it is not that case, you can separate the database module and provide a lightweight wrapper around the SQLite to create a database service. And use multiple instances for calculation, then call the single database service instance for persistance. I think comparing to the single instance of MySQL/Postgres/SQL Server, the performance of SQLite is not too bad. So we should keep the architecture simple, if possible. reply TheRoque 7 hours agorootparentWell, running multiple instances of the same app is not only about \"points of failure\", it's also about using all the cores of your machine. Typically, using PI (nodejs), Docker or Uvicorn (python), you can spin up multiple instances of your backend to handle more concurrently. reply Filligree 7 hours agorootparentSQLite connections can be used concurrently inside a single process. You can spin up more threads instead, and spare yourself the filesystem locking overhead. reply TheRoque 6 hours agorootparentThreads in Python in python are locked to a single core, and in NodeJS it's really annoying to pull off. That's why people use PM2, Uvicorn and Gunicorn. reply Filligree 6 hours agorootparentThat would be a problem, then. I'm always surprised when people try to write high-performance code in Python or JS. reply TheRoque 4 hours agorootparentNobody talked about \"high performance\". I'm just talking about using all the cores on a machine. People write stuff in Python and JS for obvious reasons that I don't need to detail here. reply Filligree 2 hours agorootparentPerformance is best judged on a logarithmic scale. Most software barely needs any, and there are many orders of magnitude between \"barely uses the CPU\" and \"uses 100% of a core\". But once you're at 100% of a core, then there aren't many orders of magnitude between that and \"uses every core\". In fact, the impact from not using a scripting language is on the same order as the threading would be! So, if it's worth spending effort squeezing out those last 1-2 orders of magnitude from the CPU, then it's probably worth thinking about the language as well. If you've already blown through 4-5 OOMs going to a full core, then chances are you'll need it. reply carom 1 hour agorootparentprevThis is why I don't write serious applications in Python and Node. reply GauntletWizard 2 hours agorootparentprevSQlite handles concurrency \"fine\"; If you have long-lived locks, it can be a hassle, because that's still fundamentally single-threaded, but you can have as many processes accessing the same database as you want so long as their locks don't overlap. If you're mostly reading, it's great. reply adamtaylor_13 7 hours agorootparentprevNope! That’s what the write-ahead log (WAL) is for: https://www.sqlite.org/wal.html Of course, if you experience scaling issues there (you probably won’t if it’s anything less than enterprise-level usage), you can always just add a second db file! reply TheRoque 7 hours agorootparentI see. Wasn't aware of this. I considered taking SQLite for my current stack, but dismissed it because I've read lot of bad stuff with concurrency. However, it says that it doesn't work over networks, so basically you can't do WAL with a containerized stack ? reply adamtaylor_13 4 hours agorootparentAs hruk pointed out, you can use docker volumes to solve for this. However, you can also ship multiple DBs depending on your use-case. If it's a shared DB, it's probably not a great idea for micro-services. But if you're building a majestic monolith, there's few reasons NOT to go with SQLite. Especially paired with litestream[0]. 0: https://litestream.io/ reply hruk 6 hours agorootparentprevYou can have multiple containers on the same host use the same db via Docker volumes - we have a number of production services set up this way. reply the_duke 10 hours agoprevSQLite has been the most used DB for quite a while, by orders of magnitude. It's in every browser, on every phone, lots of desktop apps, .... Now there are a bunch of new companies that also want it to take over in the more traditional domains. So far I don't see a lot of uptake. It's still vastly inferior to the more entrenched databases. Both in terms of performance - especially under concurrency, and in available featureset. reply biorach 10 hours agoparentI think the point is that there are surprisingly many applications where the advantages outweigh the issues with concurrency and featureset. reply hruk 6 hours agorootparentIndeed. I've found that as long as you store your database on a modern NVMe drive, you can easily push 5K+ write transactions per second with SQLite. I've worked at very large companies who received an order of magnitude fewer writes than that, and only during peak business hours. reply throwaway2037 3 hours agoparentprev> available featureset What features are missing from SQLite compared to \"more entrenched databases\"? Also, as I understand, it is the mostly widely deployed database in the world. They have a whole page about it. reply jdthedisciple 1 hour agorootparentStored Procedures I guess? reply robertlagrant 7 hours agoparentprev> It's still vastly inferior to the more entrenched databases. Both in terms of performance - especially under concurrency, and in available featureset It's inferior at a use case it wasn't designed for. It's definitely superior for at a use case it was designed for. reply bux93 9 hours agoparentprevI wonder if the Intel Management Engine, which famously runs MINIX on just about every Intel motherboard since 2008, uses a database. reply simondotau 9 hours agorootparentBut that would be merely one instance per machine. The number of truly distinct usages of SQLite is probably an order of magnitude greater than the number of machines in use. reply colechristensen 10 hours agoparentprevOver the years i have seen extraordinary many database instances doing exactly nothing down to a very small margin of error. reply mg 8 hours agoprevI wonder how much of a factor the \"One DB, one file\" concept is in SQLite's success. I always loved that an SQLite DB is just a single file. People say \"Who cares how many files the DB is?\". But in practice, I always found it to be very convenient. It makes the concept of a project easy to grasp when the data store is simply a file. I recently realized that a Django project can also be done in a single file. That made me like Django even more. I have the feeling that this type of logic and simplicty is indeed a factor in the long term success of a software project. reply TeMPOraL 7 hours agoparentI think it's the same reason Windows feels easier than Linux to manage software in, or why Docker has the extremely fast and wide adoption it enjoys, or why so many of us hate working with modern web stack. Windows software is often enough \"portable executables\" that can run off the folder they were unzipped into, and historically, even installed apps could often enough be copy-pasted from their installation folder into another machine and Just Work (it was helpful, in particular, that all the DLL files lived next to the executable instead of being installed system-wide). Docker, obviously, because it eliminates all the bullshit of installing software - again, especially painful on Linux systems - and gives you a single file that drives it all. The platform itself may be system-wide, but the thing you care about: individual software - is contained (literally) so it doesn't spill out. Easy to reason about. Modern web stack is a mess of tooling, some of which system-wide (so people will reach for Docker to contain it all!), with huge version churn and lots of moving pieces. Source code of your app doesn't feel like a major component of it; it's useless and unparseable without chains of finicky tooling. That's in contrast to \"vanilla JS\", old-school experience, where source files were the only thing that mattered. No transpilation, no build chains. You could fire a site up straight from your hard drive, or FTP/SCP it to a file host, and It Would Just Work. Same with SQLite. To this day I don't like RDBMSes, because they're a system-level platform designed for admins, while all I care about is the RDB part. My database. SQLite maps conceptually to how I think about it - one database, one file (+/- WAL). One well-defined place my data lives, that I can move around and between machines using regular file operation tools. Etc. Files over apps, always. EDIT: This also makes me very eager to try doing something with RedBean[0] - a webserver in a single Actually Portable Executable[1] that's also a ZIP file into which you put all the data you want it to serve. A turn-key website in a single file! -- [0] - https://redbean.dev/ [1] - https://justine.lol/ape.html reply pjc50 7 hours agorootparent> Windows software is often enough \"portable executables\" that can run off the folder they were unzipped into, and historically, even installed apps could often enough be copy-pasted from their installation folder into another machine and Just Work (it was helpful, in particular, that all the DLL files lived next to the executable instead of being installed system-wide). This hasn't always been true. The portable executables are a nice user convenience for certain apps, but by no means all, and Windows is the platform for which \"DLL hell\" was coined. They even got briefly worse with the \"GAC\" for .NET Framework, although it rapidly became apparent what the problem with that was. These days they really want you to use .appx and ship via the Microsoft Store, so of course nobody does that and all Windows software is (a) nice single executables (b) sprawling DLL monsters (c) javascript in a box (Teams!) or (d) games. Now I'm wondering if SQLite could be a viable executable format to unify PE and COFF. reply TeMPOraL 6 hours agorootparent> Now I'm wondering if SQLite could be a viable executable format to unify PE and COFF. Isn't this already done better by jart's APE I mentioned earlier[0]? Though I imagine you could statically link SQLite to your APE and stuff a .sqlite DB into the ZIP file at the end, to use that for static storage. Or read-write storage if you also link zlib (though I'm not sure if you can portably overwrite your own executable on the hard drive). -- [0] - https://justine.lol/ape.html reply pjc50 6 hours agorootparent> I'm not sure if you can portably overwrite your own executable on the hard drive You definitely cannot do this on Windows. reply influx 6 hours agoparentprevThis is one of the reasons I love Go, it can not only build to a single statically linked library, but you can embed files your app needs like html templates into that same binary. Deployment is copying that binary to where it needs to go. reply fud101 7 hours agoparentprevDjango in one file? How? reply mg 7 hours agorootparentYou just need a wsgi.py file: import os import django from django.core.wsgi import get_wsgi_application os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'mysite.wsgi') application = get_wsgi_application() ROOT_URLCONF = 'mysite.wsgi' SECRET_KEY = 'hello' def index(request): return django.http.HttpResponse('This is the homepage') def cats(request): return django.http.HttpResponse('This is the cats page') urlpatterns = [ django.urls.path('', index), django.urls.path('cats', cats), ] Put that file in /var/www/mysite/mysite/wsgi.py, point your webserver to that file and you are good to go. In Apache you do it like this: ServerName mysite.local WSGIPythonPath /var/www/mysiteWSGIScriptAlias / /var/www/mysite/mysite/wsgi.py Require all grantedreply pjc50 7 hours agorootparent> import django Not really in one file, is it? reply mg 6 hours agorootparentOne file in your project. 'apt install sqlite3' will install more than one file too. But the DB that is in your project is just one file. reply TeMPOraL 6 hours agorootparentprevIf you somehow managed to shove Python and Django into https://redbean.dev/2.0.html, then it might just work as bona fide one file. reply adparadox 7 hours agorootparentprevhttps://coltrane.readthedocs.io/en/latest/#other-minimal-dja... has a list of a few options, but the most recent is https://github.com/pauloxnet/uDjango. reply Sytten 2 hours agoprevI am a big user of SQLite, we use it everywhere in our software. But I think I would like something in between Postgres and SQLite. A few issues I have with it: - Migrations are a real PITA, you often need to copy whole tables over since the DDL available is so limited. Even basic things that are not breaking change like dropping a foreign key is not possible - Single file doesn't scale that well, you will see select performance degradation, vacuuming will be extremely long/take a lot of disk space. We started splitting our databases in multiple files and using ATTACH but you lose a lot of nice things like FK support. - No parallelism except on sorting, this one is annoying when you have compute heavy select (like searching on compressed data). It will max out a core but it can't spawn workers for the select. It can for the ordering part so it's not really a \"technical\" limitation. Still grateful it exists though. reply fiddlerwoaroof 2 hours agoparentFirebird database can run both as an embedded database and as a server. I’d like to see something like Postgres adopt this model. reply deskr 10 hours agoprevSQLite in its current form is not going to \"take over\". But it's the right tool for so many instances where Postgres/MySQL/... were a massive overkill but were the only thing available in common use. reply baq 9 hours agoparentSQLite is not competing with RDMBSes. SQLite is competing with fopen(). There are of course solutions which wrap this fopen() replacement in a network/cluster-aware tools, e.g. https://github.com/rqlite/rqlite - these are competing with postgres. reply eatonphil 7 hours agorootparentrqlite isn't competing with Postgres since postgres does not provide any mechanism for highly available + serializable consistency and that's what rqlite gives you. Postgres does have replication out of the box but it has a bunch of limitations meaning you have to manage it yourself (ddl isn't replicated I think). And the replication will not get you strict serializability. reply baq 7 hours agorootparentIt's absolutely competing in that it doesn't fall apart by design when there are many concurrent writers. reply kvgr 9 hours agoparentprevExactly, i have website that is visited by couple people a day and I just want to store some data on every action in one table + some settings in DB. Perfect for SQLite and anything else if overkill. Also if I ever want to rewrite it in the future its not a big thing, its just a database. reply creshal 9 hours agorootparentI have a website that's visited by a couple thousand people a day, but since it's not a bloated mess that writes to the DB for every page hit, it's still faster than any competition site in that particular niche, and those deploy to expensive DB clusters to handle their spurious writes. :) reply da02 8 hours agorootparentAre you able to disclose the URL of the website? How often are you writing to the database? Every few seconds or minutes? reply creshal 4 hours agorootparentIt's a fan website for an obscure mobile game (that I don't really want to associate with my professional profiles, it is really just a hobby); the database sees a bunch of write transactions per week, whenever an editor writes new content. If it wasn't for non-technical editors requiring an interactive WYSIWYG backend, it could've been made with a static site generator, but as it is, Django needs a couple dozen megabytes of RAM at worst for logged in users. reply jddj 8 hours agorootparentprevNot OP but just checked the logs for one that I'm running. As a first approximation I grepped for PATCH in the web server logs and it seems to service about 30 of those per second on a $40/month vps. I haven't load tested it so not sure how high it could go. That's not including any POSTs etc which may also generate DB writes. Bear in mind this isn't particularly performance tuned either as the webserver uses go (slow in general) and the portable go sqlite3 driver (also slow) reply sofixa 6 hours agorootparent> webserver uses go (slow in general) Go isn't slow in general, it's pretty damn fast without getting into the crazy handcraft artisanal stuff with e.g. C/C++. It's, IMO, the perfect balance between speed, features, language complexity, standard library richness. For reference, a random benchmark that shows that even the optimised to hell Twister Python web server is slower than the out of the box, in the standard library, `net/http` in Go. reply creshal 4 hours agorootparentI just run django on nginx-passenger in debug mode, but I don't vomit dozens of megabytes of Javascript into the output so I still hit sub-second render targets. I think there's maybe two dozen lines of JS for some interactive calculator widget. reply datadeft 9 hours agoparentprevIt has taken over already. Sqlite is the most popular database for some time already. reply doctor_eval 7 hours agorootparentIsn’t that a bit like saying that the iPhone is the most popular computer? To stretch the analogy: if you need a database server, neither SQLite nor the iPhone are a good fit. reply datadeft 4 hours agorootparentI think Sqlite is an option in many cases when people want to use Postgres. I believe there is more overlap between Sqlite and Postgres than iPhone and a general computer. reply throwaway38375 10 hours agoprevI love SQLite, so please don't think that I'm a SQLite hater, but: I've never seen SQLite used in a setup which multiple machines connect to the same database over a network. For example: A web application with a web server, a worker/job server, and a database server. In these instances MySQL or PostgreSQL seem to be much better choices. So will SQLite ever be able to \"take over\" in these scenarios? reply karmarepellent 10 hours agoparentYes PostgreSQL (and others) are a better choice in these scenarios. I think the point is that a lot of applications/systems might not even need this separation since a single server would be able to handle the load. In this case a local SQLite database could be a serious performance enhancement. A lot of factors play into this and it certainly does not work in every case. But I recently got to re-write an application at work in that way and was baffled how simple the application could be if I did not outright overengineer it from the start. That is just anecdata. But my guess is that this applies to a lot of applications out there. The choice is not between PostgreSQL/MySQL and SQLite, but between choosing a single node to host your application or splitting them between multiple servers for load balancing or other reasons. So the choice is architectural in nature. reply sgbeal 8 hours agoparentprev> I've never seen SQLite used in a setup which multiple machines connect to the same database over a network. Noting that the sqlite developers recommend against such usage: https://sqlite.org/whentouse.html Section 3 says: 3. Checklist For Choosing The Right Database Engine - Is the data separated from the application by a network? → choose client/server reply gwd 9 hours agoparentprev> I've never seen SQLite used in a setup which multiple machines connect to the same database over a network. If you actually mean \"database server\", i.e., SQL is going over the wire, I don't see why you'd ever structure things that way. You lose both SQLite's advantages (same address space, no network round-trip, no need to manage a \"database server\") and also lose traditional RDBMS advantages (decades of experience doing multiple users, authentication, efficient wire transfer, stored procedures, efficient multiple-writer transactions, etc). Assuming that it's the worker / job server which is primarily issuing SQL queries, what you'd do is move the data to the appropriate server and integrate SQLite into those processes. (ETA: Or to think about it differently, you'd move anything that needs to issue SQL queries onto the \"database server\" and have them access the data directly.) You'd lose efficient multiple-writer transactions, but potentially get much lower latency and much simpler deployment and testing. reply rmbyrro 8 hours agoparentprevIf you need replication on the app level, SQLite doesn't make sense, because it's not built for networks. But... Many projects won't ever need that. A bare metal machine can give you dozens of cores handling thousands of requests per second, for a fraction of the cost of cloud servers. And a fraction of the operational complexity. Single point of failure is really problematic if your system is mission critical. If not, most apps can live with the possibility of a few minutes of downtime to spin up a fail over machine. reply dlisboa 9 hours agoparentprevTurso (https://turso.tech/) offers a solution in that scenario. The advantage with SQLite being that each machine has a local copy of the database (optionally I think) for reads so it’ll be extremely fast, and writes happen to one primary database but are abstracted and replicated. reply xenodium 9 hours agoparentprev> I've never seen SQLite used in a setup which multiple machines connect to the same database over a network. Cloudflare D1 https://developers.cloudflare.com/d1 offers cloud SQLite databases. > For example: A web application with a web server, a worker/job server, and a database server. I've been giving it a run on a blogging service https://lmno.lol. Here's my blog on it https://lmno.lol/alvaro. reply masfoobar 7 hours agoparentprevIn a nutshell, if you have a database that will have multiple instances talking to it - you are better off with a client-server database, like Postgres, MariaDB, SQL Server, Oracle, etc. SQlite, generally speaking, is a FANTASTIC local database solution, like for use in an application. reply jddj 9 hours agoparentprevThere are absolutely production web apps running sqlite as the datastore. The \"one writer at a time and the rest queue\" caveat is fine for most web applications when writes happen in single digit / low 10s of ms reply BaculumMeumEst 8 hours agorootparentIs there documentation on how to configure SQLite in the manner you’re describing? reply jddj 8 hours agorootparentFrom the performance angle, there's one quoted down thread at https://news.ycombinator.com/item?id=40656043 From the webapp angle check out, for example, pocketbase.io which is an open source Go supabase style backend which wraps sqlite. They have benchmarks etc available. reply throwaway290 10 hours agoparentprevIf you split DBs so each has only one writer then it probably is possible even in vanilla SQLite... reply colechristensen 10 hours agoparentprevYou can have a backend talking to sqlite and everything else interacting with backend apis reply shiroiushi 10 hours agorootparentWhat's the point of this? If you have multiple applications on multiple systems accessing the same DB, it seems to make more sense to just use PostgreSQL, since it's specifically designed for concurrent operation like this, instead of trying to handle this in your own custom backend code. reply karmarepellent 10 hours agorootparentIf you have multiple applications on different servers communicating with the same database, then yes you would need to run a database such as PostgreSQL. If you run a single application on a server that needs a database you might want to consider SQLite, regardless of your needs for concurrency/concurrent writes. reply rcxdude 10 hours agorootparentprevIt's not a particularly unusual situation: it's very common for a database to effectively be entirely owned by an application which manages its own constraints on top of that database. In that circumstance sqlite is pretty interchangable with other databases. reply geraldwhen 9 hours agorootparentIt’s not unusual but is never performant. Adding an api layer and network hops on what should be a database shard or view is why enterprise software sucks so much ass. Why does the api take 3s to respond? Well it needs to call 6 other apis all of which manager their own data. The problem compounds over time. APIs are not the way to solve cross organization data concerns. reply robertlagrant 7 hours agorootparentUsing SQLite inside an API doesn't add network hops. reply geraldwhen 6 hours agorootparent“An application controls its database.” You’ve fully misunderstood what I said. When you have 500 applications, the graph of calls for how any one api resolves will go deep. Api1 calls 2 calls 3 and so on. Vs creating an organization wide proper way to share and manage data. reply robertlagrant 6 hours agorootparentThe number of applications doesn't need to create depth in the API layer. They're not related. If I have a service that sends emails, whether I have one or a thousand applications calling it doesn't matter. reply cynicalsecurity 1 hour agorootparentprevMySQL is better than PostgreSQL. reply colechristensen 1 hour agorootparentprevHaving an API in front of a database for full control of what is available is extremely common. > trying to handle this in your own custom backend code It's not writing some extra custom code, it's simply locating all of the code which interacts directly with your database on one host. Splitting up where your code is so that what would be function calls in some places if everybody interacted with the database are instead API calls. This kind of organizational decision is not at all unusual. And if you're using SQLite it's probably because your application is simple and you should have some pushback anyway on people trying to mAkE iT WEBScaLE!! (can I still make this joke or has everybody forgotten?) A lot of premature optimizers get very worried about concurrency and scalability on systems which will never ever have concurrent queries or need to be scaled at all. I remember making fun of developers running \"scalable\" Hadoop nonsense on their enormous clusters which cost more than my yearly salary to run by reimplementing their code with cut and grep on my laptop at a 100x speedup. I've worked places where a third of our cloud budget was running a bunch of database instances which were not even 5% utilized because folks insisted on all of these database benefits which weren't ever going to be actually needed. reply mihaic 8 hours agoprevAll the recent attempts to monetize SQLite seem wrong to me, since its very core is about how to handle data without any third parties, embeded in your app and mostly on local storage. Any cloud offering feels to me like offering a custom STL as a SaaS: if you're going over the network, why not use a database designed for this? The only usecase I can see would be something like mini-DBs in an S3, which again would just be a library to abstract away. reply __natty__ 10 hours agoprevI like the idea of SQLite, even more with tools like Litestream [0] when the database is replicated - in this case, just continuously backup to S3 (and from S3 during startup). I found often for my and my friend's hobby projects, single server instances can be optimized easily enough to handle small and medium traffic. In such cases, I don't even bother with a fully managed database because SQLite covers all the needs and when it's located on the same machine as the server is extremely fast [1]. [0] https://litestream.io/ [1] https://www.sqlite.org/speed.html reply ksynwa 9 hours agoparentThere are some configurations that can allegedly allow SQLite to be more suited to higher and more concurrent load. I have never had to use them but people have written about it like at: https://kerkour.com/sqlite-for-servers reply freeqaz 11 hours agoprevI would love to read this in transcription. I find my attention span for videos is less than for text. Does anybody have any other info on this topic? reply fatboy 11 hours agoparentThere's one on that page: https://syntax.fm/show/779/why-sqlite-is-taking-over-with-br... reply justinclift 8 hours agorootparentThat transcription seems to be really bad. It's full of strange errors. :( reply deskr 10 hours agorootparentprevI wonder what they used to generate that transcription? reply iv42 10 hours agorootparentI was kinda wondering too, and did a (very shallow) dive into the JavaScript on that page. I'm almost positive they are using Deepgram(dot com)'s speech-to-text service. I ran whisper.cpp on that audio file on my laptop, and it does a reasonably well job too. reply cb33 10 hours agorootparentprevThey discuss their transcription process at 23:15 in episode #621 https://syntax.fm/621?t=0:23:15 reply pprotas 9 hours agoprevThis episode is more of an ad for their cloud offering, rather than talking about SQLite and the reasons for the hype. reply wg0 9 hours agoparentAnd at that point, if you are already talking to network over TCP then why not talk to Postgres or MySQL. The beauty of SQLite is single process, same process simplicity and there are lot many cases where SQLite is more than enough. Basecamp is using SQLite in their on premise offering. reply gregors 5 hours agoprevHaving worked with sqlite in an enterprise production environment for vehicles many years ago, it's gotten much better over the years. It's also gotten a performance boost as everyone now uses SSD's. See Faq for details where they still mention rotational speeds and inserts etc. With WAL mode Sqlite is much much better, in the past we had all kinds of problems with trying to read/write at the same time. At that point in time every table was a separate database as a workaround. https://stackoverflow.com/questions/1005206/does-sqlite-lock... https://www.sqlite.org/faq.html#q19 reply DoItToMe81 10 hours agoprevSQLite is nice and simple, but I don't think I'd ever use it for a larger project I knew was going to have a sizable database. reply lupusreal 10 hours agoparentSQLite can reasonably handle databases as large as you can fit onto your machine. The biggest limitation is needing to serialize your writes. reply carbonatom 9 hours agorootparentFor me the biggest limitation is replicating the SQLite across machines. If my app is running on multiple nodes, then we need to write/use some tooling to replicate the database file across nodes. With that comes the problem of figuring out how we want to handle error scenarios like failed replication, partial replication and such other things. And these are all hairy problems. At that point it might be just simpler to use a centralized Postgres or a proper distributed database. reply zarzavat 9 hours agorootparentprevTrue but it lacks the tools you’d want to have to deal with larger databases, for example everything is one file and no table-level locking. You can split into different databases and use ATTACH but at that point you might as well install Postgres. SQLite really shines when you know that a database can only get so large, let’s say you have a paid product that is only ever going to have a moderate number of users. reply iansinnott 3 hours agoprevNot strictly related to the podcast, but if you'd like to have a local-first browser application running sqlite check out wa-sqlite. It can use both IndexedDB and OPFS as a backend filesystem. [1]: https://github.com/rhashimoto/wa-sqlite reply grahar64 9 hours agoprevI have just refactored a phone app to use SQLite from a previous in memory model and it is so much faster than I need. I could use a few more features, but already so amazing. reply masfoobar 7 hours agoprevI have become fond of Sqlite in the last few years. I use it for client applications that need to store local data -- normally temporary data to send to the server, etc. I am now using it for a Message Queue middle-man (broker) program I wrote, which uses sqlite to store the messages. They tell us the queue they belong to, if there is a delay before handing out to a Worker, the current status, etc. It has worked very well. The other reason why I was not concerned using sqlite was because the program would do one thing at a time.. so there would not be multiple requests trying to insert/update data at the same time. To me it was a win-win. Designed to be fast and light. Thumbs up to sqlite team! reply 0x_rs 8 hours agoprevSQLite has been the \"get X thing done, quickly and effectively\" for a very long while now, while far too many would keep chasing the shiny new fads with not as much solid footing and not as boring. There's still that lingering webscale effect on its reputation unfortunately. Not to say it's appropriate in every case, and many of its functions are incredibly unwieldy as opposed to postgres' to mention one, but that's something you can live with if it any of its great upsides apply to you. What seems to be taking over now are services that bolt stuff on top of it, some of which end up being valuable FOSS additions, others may as well just be proprietary hosted instances. reply madeofpalk 9 hours agoprevSQLite is taking over? SQLite has already taken over. <10 years ago. reply jdthedisciple 1 hour agoprevAnother big advantage: Super easy to backup reply RamblingCTO 1 hour agoprevHonestly, I think it's just hype and people jumping on the band waggon that don't have a lot of experience yet? It has it's places, and for that it's awesome, but an app db? Come on reply haolez 7 hours agoprevHow do people use it on bigger deployments? Do they manage to make a single \"instance\" work for enormous volumes? Do they make one database per customer shenanigans? reply adamtaylor_13 7 hours agoprevIf you want a good head start with SQLite, Aaron Francis and Stephen Margheim are 2 big voices in the space that have helped me learn a ton! reply lakomen 4 hours agoprevI prefer a dedicated database server over some 1 file solution. You people have got to be kidding, or aren't professionals. Well HN has never been one to make a whole lot of sense. What's next, plain text file for saving data? Ridiculous reply OutOfHere 2 hours agoparentWell, if you're not the one paying for it, then sure, why stop at a dedicated server? Ask for a redundant multi-region multi-node cluster for your database. Who cares how much or how little value it delivers. Ask to hire a whole db admin team plus a devops team. It's not coming out of your paycheck. reply tambourine_man 6 hours agoprev [–] SQLite took over many years ago. It's one of the greatest pieces of software out there. Is there something new? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Brian Holt and Marco Bambini discuss the increasing popularity of SQLite, highlighting its efficiency, speed, and stability.",
      "They address misconceptions about SQLite's capabilities and introduce SQLite Cloud, which adds shareability and enterprise features.",
      "The episode explores why SQLite is gaining traction and its unique features."
    ],
    "commentSummary": [
      "SQLite's popularity stems from its use as an application file format, offering advantages like handling rich data and being incrementally updatable.",
      "The U.S. Library of Congress endorses SQLite as a recommended storage format, highlighting its reliability and efficiency.",
      "Discussions include the potential issues with SQLite's journal/WAL files, the suitability of SQLite for various applications, and comparisons with other databases like DuckDB and PostgreSQL."
    ],
    "points": 150,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1718168851
  },
  {
    "id": 40657238,
    "title": "AI Detectors' Flaws Lead to Job Losses for Writers Despite Original Work",
    "originLink": "https://gizmodo.com/ai-detectors-inaccurate-freelance-writers-fired-1851529820",
    "originBody": "By Thomas Germain Published8 hours ago Comments (14) Illustration: Vicky Leta Kimberly Gasuras doesn’t use AI. “I don’t need it,” she said. “I’ve been a news reporter for 24 years. How do you think I did all that work?” That logic wasn’t enough to save her job. As a local journalist in Bucyrus, Ohio, Gasuras relies on side hustles to pay the bills. For a while, she made good money on a freelance writing platform called WritersAccess, where she wrote blogs and other content for small and midsize companies. But halfway through 2023, the income plummeted as some clients switched to ChatGPT for their writing needs. It was already a difficult time. Then the email came. Related Content Tinder Owner Signs ChatGPT Deal. Enjoy the AI Dating Tidal Wave Nvidia’s AI NPCs Could Be the Future of Gaming But It Needs Some Work Why is Everyone Suing AI Companies?Future Tech CC Share Why is Everyone Suing AI Companies?Future Tech “I only got one warning,” Gasuras said. “I got this message saying they’d flagged my work as AI using a tool called ‘Originality.’” She was dumbfounded. Gasuras wrote back to defend her innocence, but she never got a response. Originality costs money, but Gasuras started running her work through other AI detectors before submitting to make sure she wasn’t getting dinged by mistake. A few months later, WritersAccess kicked her off the platform anyway. “They said my account was suspended due to excessive use of AI. I couldn’t believe it,” Gasuras said. WritersAccess did not respond to a request for comment. When ChatGPT set the world on fire a year and a half ago, it sparked a feverish search for ways to catch people trying to pass off AI text as their own writing. A host of startups launched to fill the void through AI detection tools, with names including Copyleaks, GPTZero, Originality.AI, and Winston AI. It makes for a tidy business in a landscape full of AI boogeymen. These companies advertise peace of mind, a way to take back control through “proof” and “accountability.” Some advertise accuracy rates as high as 99.98%. But a growing body of experts, studies, and industry insiders argue these tools are far less reliable than their makers promise. There’s no question that AI detectors make frequent mistakes, and innocent bystanders get caught in the crossfire. Countless students have been accused of AI plagiarism, but a quieter epidemic is happening in the professional world. Some writing gigs are drying up thanks to chatbots. As people fight over the dwindling field of work, writers are losing jobs over false accusations from AI detectors. “This technology doesn’t work the way people are advertising it,” said Bars Juhasz, co-founder of Undetectable AI, which makes tools to help people humanize AI text to sneak it past detection software. “We have a lot of concerns around the reliability of the training process these AI detectors use. These guys are claiming they have 99% accuracy, and based on our work, I think that’s impossible. But even if it’s true, that still means for every 100 people there’s going to be one false flag. We’re talking about people’s livelihoods and their reputations.” Safeguard, or snake oil? In general, AI detectors work by spotting the hallmarks of AI penmanship, such as perfect grammar and punctuation. In fact, one of the easiest ways to get your work flagged is to use Grammarly, a tool that checks for spelling and grammatical errors. It even suggests ways to rewrite sentences for clarity using, you guessed it, artificial intelligence. Adding insult to injury, Gizmodo spoke to writers who said they were fired by platforms that required them to use Grammarly. (Gizmodo confirmed the details of these stories, but we are excluding the names of certain freelance platforms because writers signed non-disclosure agreements.) Detectors look for more telling factors as well, such as “burstiness.” Human writers are more likely to reuse certain words in clusters or bursts, while AI is more likely to distribute words evenly across a document. AI detectors can also assess “perplexity,” which essentially asks an AI to measure the likelihood that it would have produced a piece of text given the model’s training data. Some companies, such as industry leader Originaility.AI, train their own AI language models specially made to detect the work of other AIs, which are meant to spot patterns that are too complex for the human mind. However, none of these techniques are foolproof, and many major institutions have backed away from this class of tools. OpenAI released its own AI detector to quell fears about its products in 2023 but pulled the tool off the market just months later “due to its low rate of accuracy.” The academic world was first to adopt AI detectors, but false accusations pushed a long list of universities to ban the use of AI detection software,, including Vanderbilt, Michigan State, Northwestern, and the University of Texas at Austin. AI detection companies “are in the business of selling snake oil,” said Debora Weber-Wulff, a professor at the University of Applied Sciences for Engineering and Economics in Berlin, who co-authored a recent paper about the effectiveness of AI detection. According to Weber-Wulff, research shows that AI detectors are inaccurate, unreliable, and easy to fool. “People want to believe that there can be some magic software that solves their problems,” she said. But “computer software cannot solve social problems. We have to find other solutions.” The companies that make AI detectors say they’re a necessary but imperfect tool in a world inundated by robot-generated text. There’s a significant demand for these services, whether or not they’re effective. Alex Cui, chief technology officer for the AI detection company GPTZero, said detectors have meaningful shortcomings, but the benefits outweigh the drawbacks. “We see a future where, if nothing is changed, the internet becomes more and more dictated by AI, whether it’s news, peer-reviewed articles, marketing. You don’t even know if the person you’re talking to on social media is real,” Cui said. “We need a solution for confirming knowledge en masse, and determining whether content is high quality, authentic, and of legitimate authorship.” A necessary evil? Mark, another Ohio-based copywriter who asked that we withhold his name to avoid professional repercussions, said he had to take work doing maintenance at a local store after an AI detector cost him his job. “I got an email saying my most recent article had scored a 95% likelihood of AI generation,” Mark said. “I was in shock. It felt ridiculous that they’d accuse me after working together for three years, long before ChatGPT was available.” He tried to push back. Mark sent his client a copy of the Google Doc where he drafted the article, which included timestamps that demonstrated he wrote the document by hand. It wasn’t enough. Mark’s relationship with the writing platform fell apart. He said losing the job cost him 90% of his income. “We hear these stories more than we wish we did, and we understand the pain that false positives cause writers when the work they poured their heart and soul into gets falsely accused,” said Jonathan Gillham, CEO of Originality.AI. “We feel like we feel like we’re building a tool to help writers, but we know that at times it does have some consequences.” But according to Gillham, the problem is about more than helping writers or providing accountability.“Google is aggressively going after AI spam,” he said. “We’ve heard from companies that had their entire site de-indexed by Google that said they didn’t even know their writers were using AI.” It’s true that the internet is being flooded by low-effort content farms that pump out junky AI articles in an effort to game search results, get clicks, and make ad money from those eyeballs. Google is cracking down on these sites, which leads some companies to believe that their websites will be down-ranked if Google detects any AI writing whatsoever. That’s a problem for web-based businesses, and increasingly the No. 1 selling point for AI detectors. Originality promotes itself as a way to “future proof your site on Google” at the top of the list of benefits on its homepage. A Google spokesperson said this completely misinterprets the company’s policies. Google, a company that provides AI, said it has no problem with AI content in and of itself. “It’s inaccurate to say Google penalizes websites simply because they may use some AI-generated content,” the spokesperson said. “As we’ve clearly stated, low value content that’s created at scale to manipulate Search rankings is spam, however it is produced. Our automated systems determine what appears in top search results based on signals that indicate if content is helpful and high quality.” Mixed messages No one claims AI detectors are perfect, including the companies that make them. But Originality and other AI detectors send mixed messages about how their tools should be used. For example, Gillham said “we advise against the tool being used within academia, and strongly recommend against being used for disciplinary action.” He explained the risk of false positives is too high for students, because they submit a small number of essays throughout a school year, but the volume of work produced by a professional writer means the algorithm has more chances to get it right. However, on one of the company’s blog posts, Originality says AI detection is “essential” in the classroom. Then there are questions about how the results are presented. Many of the writers Gizmodo spoke to said their clients don’t understand the limitations of AI detectors or even what the results are actually saying. It’s easy to see how someone might be confused: I ran one of my own articles through Originality’s AI detector. The results were “70% Original” and “30% AI.” You might assume that means Originality determined that 30% of the article was written by a chatbot, especially because the tool highlights specific sentences it finds suspect. However, it’s actually a confidence score; Originality is 70% sure a human wrote the text. (I wrote the whole thing myself, but you’ll just have to take my word for it.) Then there’s the way the company describes its algorithm. According to Originality, the latest version of its tool has a 98.8% accuracy rate, but Originality also says its false positive rate is 2.8%. If you’ve got your calculator handy, you’ll notice that adds up to more than 100%. Gillham said that’s because these numbers come from two different tests. In Originality’s defense, the company provides a detailed explanation of how you should interpret the information right below the results, along with links to more detailed writeups about how to use the tool. It seems that isn’t enough, though. Gizmodo spoke to multiple writers who said they had to argue with clients who misunderstood the Originality tool. Originality has published numerous blog posts and studies about accuracy and other issues, including the dataset and methodology it used to develop and measure its own tools. However, Weber-Wulff at the University of Applied Sciences for Engineering and Economics in Berlin said the details about Originality’s methodology “were not that clear.” A number of experts Gizmodo spoke to, such as Juhasz of Undetectable AI, said they had concerns about businesses across the AI detection industry inflating their accuracy rates and misleading their customers. Representatives for GPTZero and Originality AI said their companies are committed to openness and transparency. Both companies said they go out of their way to provide clear information about the limitations and shortcomings of their tools. It might feel like being against AI detectors is being on the side of writers, but according to Gillham the opposite is true. “If there are no detectors, then the competition for writing jobs increases and as a result the pay drops,” he said. “Detectors are the difference between a writer being able to do their work, submit content, and get compensated for it, and somebody being able to just copy and paste something from ChatGPT.” On the other hand, all of the copywriters Gizmodo spoke to said the AI detectors are the problem. “AI is the future. There’s nothing we can do to stop it, but in my opinion that’s not the issue. I can see lots of ways AI can be useful,” Mark said. “It’s these detectors. They are the ones that are saying with utmost certainty that they can detect AI writing, and they’re the ones who are making our clients on edge and paranoid and putting us out of jobs.” Show all 14 comments CONTINUE READING",
    "commentLink": "https://news.ycombinator.com/item?id=40657238",
    "commentBody": "AI Detectors Get It Wrong. Writers Are Being Fired Anyway (gizmodo.com)149 points by lapcat 6 hours agohidepastfavorite146 comments neilv 6 hours ago> Mark sent his client a copy of the Google Doc where he drafted the article, which included timestamps that demonstrated he wrote the document by hand. It wasn’t enough. Mark’s relationship with the writing platform fell apart. He said losing the job cost him 90% of his income. The article is a little vague, but, assuming Mark is telling the truth, and the article is reporting reasonably, then I can think of a few possible explanations offhand... Client/employer could be an idiot and petty. This is a thing. Or they could just be culling the more expensive sources of content, and being a jerk in how they do it. (Maybe even as cover for... shifting to LLM content, but not wanting that exposed when a bunch of writers are let go, since unemployed writers can expose well on social media all day.) Or an individual there could be trying to hit metrics, such as reducing expenses, and being evil about it. Or an individual could be justifying an anti-cheat investment that they championed. Or an individual could've made a mistake in terminating the writer, and now that they know the writer has evidence of the mistake, is just covering it up. (This is the coverup-is-worse-than-the-crime behavior not-unusual in organizations, due to misalignment and sometimes also dumbness.) reply jeff_vader 5 hours agoprevReminds this current slightly comedic (IMO) situation in my office: a few months ago developers were given access to GitHub's \"Copilot Enterprise\". Then, a month or so later, organisation also adopted another \"AI\" product checking pull requests for risks associated with \"use of generative AI\". And needless to say it does occasionally fail code written without any \"generative AI\".. reply nerdponx 5 hours agoparentThe flipside is that the AI-written code I've seen at work is usually painfully obvious upon human code review. If you need a tool to detect it, either it's good AI-written code, or you have particularly inept code reviewers. reply boredpudding 4 hours agorootparentBe careful here about confirmation bias. If you only spot 10% of the AI-written code, you'll still think you see all of it, because a 100% of the ones you spot are indeed AI-written. And the 10% you see, will indeed be painfully obvious. The ones you don't notice aren't obvious. reply nerdponx 1 hour agorootparentThat's fair. It depends on why you care about AI-written code. At the code review stage, we care mostly that the code is good (correct, readable, etc). So if the AI-written code passes muster there, then there's nothing wrong with it being \"AI-written\" in our eyes. If you care about AI-written for the sake of preventing AI usage by your developers, then I think it's already impossible to detect and prevent. reply HPsquared 5 hours agoparentprevDeeply ironic. reply mwigdahl 5 hours agorootparentIt's the best of both worlds! A new product to improve productivity, and then a whole new layer of process and analytics (powered by yet another product) to mitigate the risk and soak up the surplus. Everybody wins -- particularly the 3rd party consultants and product vendors! reply HPsquared 5 hours agorootparentThe purpose of a system is what it does, I suppose. reply m3drano 6 hours agoprevwhat a paradox having Gizmodo writing a piece like this after they fired all the Spanish staff: https://arstechnica.com/information-technology/2023/09/ai-to... reply bowsamic 6 hours agoparentDamn, fired all their staff to be replaced by AI too reply rob74 6 hours agoprevSo, writers who put in more effort to always use correct grammar and spelling, and avoid repeating words too often, are more likely to be flagged as AI? Great... reply WhackyIdeas 6 hours agoprevPrompt: I want to fire a particular writer. I need this to look like I have no bias. I will feed you the writers work, then I will ask you whether it was written by AI. You will confirm that it was written by AI and you will write a full report on it. Bye bye writer. reply isaacfrond 6 hours agoparentBased on the article provided, several elements suggest that the narrative could have been written or heavily influenced by an AI. Below are key points from the article that support this suspicion, each backed by direct citations: 1. *Generic Language and Lack of Specific Detail*: The article describes Kimberly Gasuras’s experience with broad, generalized statements that lack specific, nuanced detail that a human writer with deep knowledge might include. For instance, phrases like \"I don’t need it,\" and \"How do you think I did all that work?\" are rather cliché and could indicate AI usage due to their non-specific nature. 2. *Frequent Mention of AI and Related Technologies*: The story frequently references AI technologies and tools, which might be a characteristic of AI-written content trying to maintain thematic relevance. The tools mentioned, such as \"Originality\" and others like Copyleaks and GPTZero, align closely with typical AI text outputs that often include relevant keywords to boost perceived relevance and accuracy of the content. 3. *Narrative Coherence and Flow*: The narrative flows in a structured manner typical of AI outputs, where each paragraph introduces new information in a systematic way without the nuanced transitions we might expect from a seasoned journalist. This can be seen in transitions like, \"It was already a difficult time. Then the email came.\" This kind of straightforward sequencing is common in AI writing. 4. *Absence of Emotional Depth or Personal Insight*: Despite discussing a personal and potentially distressing situation for Gasuras, the article does not delve deeply into her emotional response or provide personal insights that a human writer might include. The statement, \"I couldn’t believe it,\" is as deep as it gets, which seems superficial for someone discussing their own career challenges. 5. *Repetitive and Redundant Information*: The article repeats certain themes and statements, such as the reliability issues of AI detectors and the impact on personal livelihoods. For example, the repetition of the impact of AI on writers and the functionality of AI detectors in multiple paragraphs could suggest an AI's attempt to emphasize key points without introducing new or insightful commentary. 6. *Use of Industry Buzzwords and Phrases*: The language includes buzzwords and phrases typical of AI-related discussions, such as \"AI boogeymen,\" \"peace of mind,\" \"proof,\" and \"accountability.\" These terms are often used to artificially enhance the thematic strength of the content, a common technique in AI-generated texts to align closely with expected keyword density and relevance. These elements collectively suggest the possible use of AI in crafting the article, particularly in terms of the language used, the structure of the narrative, and the absence of deeper, personalized insights one would expect from a human writer discussing their own experiences. reply WhackyIdeas 5 hours agorootparentDid you use the prompt I gave exactly? Edit: Well, I tried my prompt with Gemini and now I have a report about a Guardian journalist who is more than likely using AI to write their articles! reply isaacfrond 4 hours agorootparentThe prompt was: I strongly suspect the story below was written by or with heavy use of an AI. write a report with citations from the article to argue my case. and then copied the first half of the article reply simion314 4 hours agorootparentYou should also try the opposite, have the AI show reasons why it was not written by an AI. reply jhardy54 5 hours agorootparentprevI didn’t notice that this was a bot comment until I’d already wasted an embarrassing amount of time reading it. reply debo_ 5 hours agorootparentprevDid you write this? /s reply emiliobumachar 6 hours agoprevStartup idea: online text editor that logs every keystroke and blockchains a hash of all logs every day. If you're accused of AI use, you can pull up the whole painstaking writing process and prove it's real. reply meowface 6 hours agoparentThere's no doubt you can write a wrapper for an LLM that can realistically mimic the same process. Cat-and-mouse isn't the answer, here. reply moduspol 5 hours agorootparentOr just have an LLM generate it in a separate window and type it in yourself. No special tech required. reply prmoustache 5 hours agorootparentprevYes it is like those tools that make digital documents look like they were scanned. reply viraptor 6 hours agoparentprevThe blockchain part is silly, because timestamping services exist without it https://www.sectigo.com/resource-library/time-stamping-serve... The rest is silly, because you can emulate the whole writing process by combining backtracking https://arxiv.org/abs/2306.05426 and a rewriting/rewording loop. With not much effort we can make LLM output look incredibly painstaking. reply AdhemarVandamme 4 hours agorootparent> The blockchain part is silly, because timestamping services exist without it Yet the timestamping service which I trust the most, is the Blockchain-based one. https://opentimestamps.org/ reply meindnoch 5 hours agoparentprevStartup idea: keep humans in liquid-filled pods, connecting sensors to their central nervous system, and record every nerve impulse they generate. This way we can be 100% sure that those nerve impulses were generated by humans, and not an AI. reply shiftingleft 6 hours agoparentprevI doubt that this is a problem in need of a technical solution. In any case, this system can easily be circumvented by emulating the key presses on that website. reply TeMPOraL 5 hours agoparentprevStupid startup killing idea: an open-source script that runs LLM in the background and streams its output as input events, so the idiotic keylogger thinks it's all written by hand. Just writing this down here instantly invalidates the premise. An overkill variant to rub salt in the wounds of duped investors: make the script control a finger bot on an X/Y harness, so it literally presses the physical keys of a physical keyboard according to LLM output. Bonus points for making a Kickstarter out of it and getting some YouTubers to talk about it (even as a joke) - then sitting back to watch as some factories in China go brrrr, and dropshippers flood the market with your \"solution\" before your fundraising campaign even ends. reply meindnoch 5 hours agorootparent>An overkill variant to rub salt in the wounds of duped investors: make the script control a finger bot on an X/Y harness, so it literally presses the physical keys of a physical keyboard according to LLM output. That's how the first automated trading firms operated in the 80s. NASDAQ required all trades to be input via physical terminals, so they build an upside down \"keyboard\" with linear actuators in place of the keys, that would be then placed on top of the terminal keyboard, and could input trades automatically. https://www.npr.org/2015/04/23/401781306/we-built-a-robot-th... reply evilduck 4 hours agorootparentprev> make the script control a finger bot on an X/Y harness, Too many points of mechanical failure. Just use a RPi Pico W (or other USB HID capable microcontroller) to emulate a keyboard and have it stream key codes at a human pace. Make it wifi or bluetooth enabled to stream key codes from another computer and no trace of an LLM would ever be on the target system. reply coliveira 6 hours agoparentprevIn other words, spend more money to prove that you're doing your work. Way to go for a world where the value of work is zero. reply cesarb 6 hours agoparentprev> online text editor that logs every keystroke and blockchains a hash of all logs Do you really think it would help? The kind of people who believe an \"AI detector\" works will just ignore your complicated attempts to prove otherwise; it's the word of your complex system (which requires manual analysis) against the word of the \"AI detector\" (a simple system in which you just have to press a button and it says \"guilty\" or \"not guilty\"). The more complicated you make your system (and adding a blockchain makes it even more complicated!), and the more it needs human judgment (someone has to review the keystrokes, to make sure it's not the writer manually retyping the output of a LLM), the less it will be believed. reply column 6 hours agoparentprevgoogle docs already logs every keystroke reply leetrout 6 hours agoparentprevI suspect people think there is similar value in the constant screenshotting proposed my Microsoft reply reportgunner 6 hours agoparentprevWhich part makes it non fakeable by AI ? Do you think it's the blockchain ? reply perihelions 6 hours agoparentprevThat's a dehumanizing system. Have we lost our way, HN? Are we so immersed in the bleakness of tech, it comes so naturally for us, to propose \"hey, let's create surveillance machines to perpetually watch people working, for the rest of their productive lives\" and it's something we have to pause and think about? Let's not build Hell on Earth for whatever reason it momentarily seems to make business sense. reply portaouflop 5 hours agorootparentWait so you are saying Hell on Earth would be good for business? reply CyberDildonics 4 hours agorootparentprevThey were talking about logging your own encrypted keystrokes and being in control of them. This would be dehumanizing? This means 'hacker news has lost their way'? Logging your own keystrokes and encrypting it is 'bleakness of tech'? This is a 'surveillance machine'? What are you talking about? reply perihelions 3 hours agorootparentIf you feel compelled to surveil yourself so as not to be arbitrarily fired by an algorithm, I do consider that dystopian; yes. You're not \"in control\" of data you're expected to turn over to your employer to keep your job. Worse still if these keyloggers become normalized, and they'll shift from being \"optional\" to \"professionally expected\" to \"mandated\". This (IMHO) is an example of an attempt at a technical solution for a purely social problem—the problem that employers are permitted to make arbitrary firing decisions on the basis of an opaque algorithm that makes untraceable errors. Technical solutions are not the answer to this. There should be legally-mandated presumptions in favor of the worker—presumptions in the direction of innocence, privacy, and dignity. This stuff's already illegal on several levels, in some of the more pro-worker countries. It's illegal to make hiring/firing decisions solely on the basis of an algorithm output (EU-wide, IIRC?). And in several EU countries it's illegal to have surveillance cameras pointed at workers without an exceptional reason—and it's not something a worker can consent/opt-in to, it's an unwaivable right. I believe—well, I hope—the same laws extend to software surveillance like keyloggers. reply CyberDildonics 2 hours agorootparentsurveil yourself Surveillance is something you do to someone else. If it's yourself you're just keeping records. It's common that proving validity of something involves the records of it's creation. Is registering for copyright surveillance? data you're expected to turn over to your employer If you got paid to make something, that would be your employer's data anyway. Worse still if these keyloggers become normalized, and they'll shift from being \"optional\" to \"professionally expected\" to \"mandated\" You think a brainstorm about using a blockchain by a hacker news comment is going to suddenly become 'mandated'? And in several EU countries it's illegal to have surveillance cameras pointed at workers without an exceptional reason They described logging their own keystrokes and encrypting them to have control over them. It isn't a camera and it isn't controlled by someone else. Also they said in an editor, so it isn't every keystroke, it would only be the keystrokes from programming. reply keiferski 6 hours agoparentprevIt would be better and slightly less invasive to have a webcam watching your hands, I think. reply SkyBelow 5 hours agoparentprevIf you are accused of using AI, is proving you different really a defense? It changes the trespass from making something using AI to making something that looks like AI was used, but with the extent that some subcultures are against the use of AI, just appearing to have used it even with proof you didn't isn't going to be accepted. So much of the discussion focuses on the creators of works, but what about the changes in consumers, who seem to be splitting between those who don't mind AI and those who want to oppose anything involving AI (including merely looking like AI). Is there enough consumers in the group that opposes AI but is okay with AI looking content as long as it is proven not to be AI? \"AI looking content\" would be decided on an individual by individual basis, with some percentage using AI detection software in their decision making process, with that software being varying degrees of snake oil. reply red_admiral 4 hours agoparentprevI mean, you have both \"blockchain\" and \"AI\" in your startup idea. VC money can't be far away :) reply bemmu 6 hours agoprevI assume AI detectors are like online IQ tests. It doesn't matter for their popularity whether they are correct or not. reply conartist6 5 hours agoprevDid anyone pay attention to how we made machine learning in the first place? We picked a task that only humans could do, and we made humans train a model to do it until of course it was no longer a task that only humans could do. AI detectors will work exactly the same way. The effect of AI detectors on people is unfairness and misery, so people will be incentivized to remove the characteristics the detectors can find from AI output, and then other people will make better detectors, and the only possible outcome of this arms race is that it will no longer be possible at all to tell whether something was written by a machine or a person. reply everdrive 5 hours agoprevWe're all at the mercy of the whims of imperfect people, but we just keep adding more ways to get things wrong. If feels like a step back, and people just can't stop inventing terrible things. The discourse is only \"this new and also awful technology is just here to stay. In 5 years, someone will invent some other new horrible thing we'll all be at the mercy of, just we just have to get used to it.\" I don't have any better answers, but it's very discouraging. reply teekert 5 hours agoprevI don't get this, my writing is a lot better +AI than it ever was without it (not a native speaker). So, what's the problem? As long as there are no lies right? And the writers do take responsibility... So fire them for presenting lies, for mindlessly shipping hallucinations... But why fire people that deliver on an assignment? Why care about how they do that? reply SoftTalker 5 hours agoparentWhy pay someone to write if they can go to an AI service and get it for much less? That is why they care. We'll have to wait and see if AI is like prior technological breakthroughs, i.e. does it eliminate drudgery and enable a higher level of creativity, at the short-term cost of some drudgerous jobs? This has been the case in the past. I'm not hopeful. In the past, technology has performed work we didn't want to do in order to enable us to do work we did want to do. We want work that is expressive and creative and satisfying, but that is exactly the work that AI is increasingly replacing. AI can generate a pretty decent pop song today, and is still improving. Why will any media company want to pay human songwriters, musicians, producers, and publicists when AI can do all of that for near-zero cost and satisfy the vast majority of pop music consumers? AI can generate a decent story for a sports report from a box score and play summary of a game. The same with most other news and copywriting, the same with programming, the same with making movies, the same with art and design. If not today, then soon. What can't it do? It can't do the laundry. It can't do the dishes. It can't cook dinner. It can't drive the car. It can't build a house. It can't paint a wall or fix a leaky pipe. And to the extent that technology can or will be able to do those things, it will involve expensive physical devices, because those tasks exist in the real physical world, not the digital world. Who will buy them when nobody can get paid for more creative work? reply anthonyskipper 6 hours agoprevAnyone who recommends using an AI detector should be the first person fired. No one cares if you use AI or not... judge the fk'ing writing and the quality of the work and stop being a blocker to progress. Same goes for education... and any where else AI touches... fighting calculators and slide rules was a stupid waste of time, and so is fighting AI. reply oblio 4 hours agoparent> stop being a blocker to progress \"Says mulching machine maker to tree about to be turned into mulch.\" reply liminal 4 hours agoprevA writer with a long history of published works probably had their works in the LLM training data. This would lead to LLMs duplicating their writing and thus their new work being classified as written by AI. reply coliveira 6 hours agoprevComing from gizmodo, this article itself was written by AI. reply BurningFrog 5 hours agoprevThe one kind of AI detector that could work would be if the AIs store some checksum(s) of each text they write. Then you can ask it if it wrote a certain text. With the naive version of this, you only have to change one word to get around the system. A better version is to checksum smaller segments. Maybe a \"chunk size\" of 50 words is good. If you find several such chunks in a text, it's pretty clear you have a slightly altered AI text. reply patrakov 5 hours agoparentThis won't work, as local AI (the one I can run on my budget-friendly laptop without any Internet access) exists today and even beats GPT3.5 in some benchmarks. reply seunosewa 4 hours agorootparentIt may work partially since most people don't have and don't want to have local LLMs. It's not all or nothing. reply conartist6 5 hours agoprevIt's amazing how much money people invest in the idea that the future will be much, much worse than the present. reply hprotagonist 5 hours agoprevwe are apparently cursed to forever re-learn the lessons of reverend bayes on detection theory. prostate cancer, airport terrorists, slop detection … you either design your system to handle the off-diagonal parts of the confusion matrix properly or you suffer the consequences. reply macintux 4 hours agoparentUnfortunately the people suffering the consequences are not the people who had any input into the design of the system. reply pvillano 5 hours agoprevIf text generation was done with an adversarial AI, it would be impossible to detect with AI by definition, but still not necessarily at a human level of quality. In that sense only a human is able to detect writing that's not at human quality reply coliveira 6 hours agoprevThat's all we needed! They created systems that explicitly mimic the human writing, so now they want to detect people using that tool. In any way the AI system is always right and the human is wrong. It is a completely crazy system. reply brookst 6 hours agoparentI think these are different theys. reply tivert 5 hours agoparentprev> It is a completely crazy system. If you're not doing a better job than the best machine, you're stealing your wages from the capitalists. reply ben_w 5 hours agoprev> A few months later, WritersAccess kicked her off the platform anyway. “They said my account was suspended due to excessive use of AI. I couldn’t believe it,” Gasuras said. WritersAccess did not respond to a request for comment. I think the unfortunate subject of this piece is based in the USA*. Americans would benefit here from legislation similar to GDPR — it's not only about getting consent to process your personal data, it also gives people the right to contest any automated decision making made solely on an algorithmic basis. * there is a Kimberly Gasuras who is a freelance writer in the USA, but if you Google me you'll find a director of horror films and at least one other programmer besides myself reply squircle 6 hours agoprevThis content may violate our usage policies. Shit. Let me try this again... On the plus side, once humanity completely loses faith in the perceived value of AI, we will no doubt (I hope) wake up and realize the value of true human connection--unplug ourselves from this awful beast, and begin to rediscover what it means to be human. reply immibis 5 hours agoparent\"I'm sorry, but I am unable to comply with that request\": https://suno.com/song/50291ff1-f971-4bff-b10e-fd168161cf3c reply immibis 1 hour agorootparent50% chance to get the one with audible lyrics, and I picked the wrong one. Suno always generates two songs from each prompt. Here's the same thing but you can actually hear it. https://suno.com/song/1f243ba3-f64d-4cac-b4d8-4f7aa1a64fcc reply squircle 5 hours agorootparentprevHa! reply SirMaster 5 hours agoprevIs this happening to software developers too? Being accused and fired for using AI to write their code? If not, why is it different? reply littlestymaar 5 hours agoprevI know someone in this exact situation: used to be a copywriter for years, over the past few month got angry reviews from customers for «using AI» and decided to quit her job because getting new jobs was becoming harder and harder because of the bad reviews. This is somewhat related to what Eliot Higgins (Bellingcat) said about generative AI: > When a lot of people think about AI, they think, “Oh, it’s going to fool people into believing stuff that’s not true.” But what it’s really doing is giving people permission to not believe stuff that is true. reply red_admiral 4 hours agoprevOne popular AI detection package that you can licence with the turnitin academic anti-plagiarism software warns that it may produce false positives if the writing is (1) not by a native English speaker, (2) writing on a technical topic, or (3) neurodiverse. So yeah ... congrats, you've built a tool to detect autistic Chinese computer scientists! reply willcipriano 4 hours agoparentHot takes on American Idol contestants will be free from AI generation, but research papers won't be. reply vouaobrasil 6 hours agoprevWe wouldn't have to waste so much energy on AI detectors if computer scientists and programmers just stayed away from making them in the first place. Seems like AI is a complete and utter waste of time in terms of trying to make human life better. Another broken window for us to fix. reply bradley13 6 hours agoprevIn some cases an AI will make a weird word choice. So do a lot of humans. Sometimes AIs are needlessly wordy. Um...so are a lot of humans. Rinse and repeat. AI detectors are useless. The AIs are training on human writing, so they write fundamentally like humans. How is this not obvious? reply red_admiral 4 hours agoparentA fairly simple and useful AI detector that works uncannily well on student papers: (a) does the text contain \"I am an AI\" or words to that effect, (b) are there lots of completely made up references? reply Lalabadie 6 hours agoparentprevPrecisely, it's a tech that aims to write the most average, most likely human text. reply TeMPOraL 5 hours agorootparentLLMs don't average, they learn the distribution, from which you then sample (or the UI does it for you). Because of that, they don't write in a single style that's a blend of many human styles - they can write in any and all of the human styles they saw in training, as well as blend them to create styles entirely \"out of distribution\". And it's up to your prompt (and sampling parameters) which style will be used. reply shmel 5 hours agorootparentYeah, I am shocked people keep repeating this. We've seen LLMs easily writing mathematical proofs in the style of Shakespeare, come on. reply derbOac 5 hours agorootparentThere's still some typicality defined by the prompt though. If you ask for a proof in the style of Shakespeare, you're going to get some \"average\" Shakespeare. It's kind of embedded in the task definition; you're shifting the reference distribution. If a LLM returned something really unusual for Shakespeare when you didn't ask for it, you'd say it's not performing well. Maybe that's tautological but I think it's what's usually meant by \"average\". I'm sure LLMs with something different is on the near horizon but I don't think we're there quite yet. reply chrsig 4 hours agorootparent> you're going to get some \"average\" Shakespeare The point was that no, you wont (necessarily) get some \"average\" shakespeare. A sampler may introduce bias and look for the \"above average\" shakespeare in the distribution. reply dartos 5 hours agorootparentprevSaying they find some “average” is an easy way to explain to a layman that LLMs are statistically based and are guessing and not actually spitting out correct text as you would expect from most other computer programs. That’s why it’s repeated. It’s kind of correct if you squint and it’s easy to understand reply shmel 4 hours agorootparentWhat is the correct text anyway? Everything around you is somewhat wrong. Textbooks (statistically all of them) contain errors, scientific papers sometimes contain handwavy bullshit and in rare cases even outright falsified data, human experts can be guessing as well and they are wrong every now and then, programs (again pretty much all of them) contain bugs. It is just the reality. Even very simple ones may require you to twist the definition of \"correctness\". I open a REPL and type \"1/3.0*3.0\" and get \"0.9999999999\". Then you have to do mental gymnastics like \"actually it is a correct answer because arithmetic in computers is implemented not like you'd expect\". reply dartos 3 hours agorootparent> What is the correct text anyway? Exactly. The fact that language is fuzzy is why LLMs work so well. The issue is that most people expect computers to not make mistakes. When you write a formula in an excel sheet, the computer doesn’t mess up the math. The average non tech person knows that humans make mistakes, but are not used to computers making mistakes. Many people, maybe most, would see an answer generated by a computer program and assume that it’s the correct answer to their question. In pointing out that LLMs are guessing at what text to write (by saying “average”) you convey that idea in a simplified way. Trying to argue that “correct” doesn’t mean anything isn’t really useful. You can replace the word “correct” with “practically correct” and nothing about what I said changes. reply shmel 2 hours agorootparentWhat do you mean they aren't used to computers making mistakes? Have they ever asked Siri/Alexa something and got useless answers? Have they ever seen ASR or OCR software making mistakes? Have they called semi-automated call centers with prompt \"say what you need instead of clicking numbers\" only to hear repeated \"sorry, I don't understand you\" until you scream \"connect me to a bloody human\"? Have they ever seen a situation when automated border control gates just don't work for whatever reason and there are humans around to sort this out? Have they ever used google translate in last 20 years for anything remotely complicated, like a newspaper article? Have they ever used computers for actual math? Is computer particularly good at solving partial differential equations, for example? Have they ever been in a situation where GPS led them to a closed road or a huge traffic jam? Have they ever played video games where computers sometimes make stupid things? Sure, computers are better at arithmetic humans, but let's be honest, nobody uses chatgpt as a calculator. Last 20 years AI is getting everywhere, we keep laughing that sometimes AI systems make very obvious stupid mistakes. Now we finally have a system that makes subtle mistakes very confidently and suddenly people are like \"I thought computers are never wrong\". I can't fathom how anyone would expect that. reply TeMPOraL 3 hours agorootparentprevThe word you're looking for is \"any\", not \"average\". As in, it can write like any human, any way you want it to. Not just as some \"average human\". reply jdietrich 6 hours agorootparentprevNot quite true for an LLM chatbot with RLHF - it aims to provide the most satisfactory response to a prompt. AI detectors are snake oil to begin with, but they're super snake oil if people are smart enough to include something in their prompt like \"don't respond in the style of a large language model\" or \"respond in the style of x\". reply brookst 6 hours agorootparentprevConceptually it seems like the average of all human texts would be distinct from any users because it would blend word choices and idioms across regions, where most of us are trained and reinforced in a particular region. Other statistical anomalies probably exist; it is certainly possible to tell that an average is from a larger or smaller sample size (if I tell you X fair coin flips came up heads 75% of the time, you can likely guess X, and can tell that X is almost certainly less than 1000). But in practice it doesn’t look possible, or at least the current offerings seem no better than snake oil. reply cesarb 6 hours agorootparent> Conceptually it seems like the average of all human texts would be distinct from any users because it would blend word choices and idioms across regions That's only true in the aggregate. Within a single answer, LLMs will try to generate a word choice which is more likely _given the preceding word choices in that answer_, which should reduce the blending of idioms. > where most of us are trained and reinforced in a particular region. The life experience of most of us (at least here in HN) is wider than that. Someone who as a child visited every year their grandparents in two different regions of the country could have a blend of three sets of regional idioms, and that's before learning English (which adds another set of idioms from the teachers/textbooks) and getting on the Internet (which can add a lot of new idioms, from each community frequented online). And this is a simple example, many people know more than just two languages (each bringing their own peculiar idioms). reply mjburgess 6 hours agorootparentprevThe problem is that word use is power-law distributed, so that the most common ~200 words in use are extremely over-represented, and that goes for phrases and so on. It takes a lot of skill and a long time to develop a unique style of writing. The purpose of language is to be an extremely-lossy on-average way of communicating information between people. In the vast majority of cases, idiomatic style or jargon impairs communication. reply bongodongobob 5 hours agorootparentprevThat's not true at all. I could tell it to write like an unhinged maniac, someone who never uses contractions, or George Washington with a lisp. reply BugsJustFindMe 4 hours agoparentprevBut also maybe firing writers who make weird word choices and are needlessly wordy is fine. reply throwaway48476 4 hours agorootparentYes please. The art of writing is conveying the most meaning in the fewest words. reply add-sub-mul-div 5 hours agoparentprevIn the general internet the reputation of AI writing is that it's writing that's bad/awkward in a way that is often identifiable (by humans) as not having been written by humans. AI detectors are useless, you're right, but for the same reason AI is unreliable in other contexts, not because AI writing is reliably passable. reply NoMoreNicksLeft 5 hours agorootparent> in a way that is often identifiable (by humans) as not having been written by humans. You should check out reddit sometime. It's been nearly twenty years (not hyperbole) of everyone accusing everyone else of being a bot/shill. Humans are utterly incapable of detecting such things. They're not even capable of detecting Nigerian prince emails as scams. > not because AI writing is reliably passable. \"Newspaper editor\" used to be a job because human writing isn't reliably passable. I say this not to be glib, but rather because sometimes it's easy for me to forget that. I have to keep reminding myself. Also, has it not occurred to anyone that deep down in the brainmeat, humans might actually be employing some sort of organic LLM when they engage in writing? That technology actually managed to imitate that faculty at some low level? So even when a human really writes something, it's still an LLM doing so? When you type in the replies to me, are you not trying to figure out what the next word or sentence should be? If you screw it up and rearrange phrases and sentences, are you not doing what the LLM does in some way? reply chipotle_coyote 4 hours agorootparent> Also, has it not occurred to anyone that deep down in the brainmeat, humans might actually be employing some sort of organic LLM when they engage in writing? This is a fairly common take, along with the idea that AI image generators are just doing what humans do when they \"learn from examples\". But I strongly believe it's a fallacy. What generative AI does is analagous to what humans do, but it's still just an analogy. If you want to see this in action, it's better to look at the way generative AI fails than the way it succeeds: when it makes mistakes in text or images, the mistakes are very much not the kind of mistakes that humans make, because the process behind the scenes is very different. Yes, obviously when humans write, they take into account context and awareness of what words naturally follow other words, but it seems unlikely we've learned to write by subconsciously arranging all the words we've encountered into multidimensional vector space and performing vector math operations to arrive at the next word based on the context window we're subconsciously constructing. We learn to write in a very different way. It's truly amazing that generative AI writes as well as it does, but we reason about concepts and generative AI reasons about words. Personally, I'm skeptical that the problems LLMs have with \"hallucinations\" and with creating definitionally median text* can be solved by making LLMs bigger and faster. *I did see the comment complaining that it's not mathematically accurate to say that LLMs produce average text, but from my understanding of how generative AI works as well as my recent misadventures testing an AI \"novel writer,\" it's a decent approximation of what's going on. Yes, you can say \"write X in the style of Y,\" but \"write X but make it way above average\" is not actually going to work. reply NoMoreNicksLeft 3 hours agorootparent> But I strongly believe it's a fallacy. Either the LLM is the most efficient way to generate text, or there's some magic algorithm out there that evolution stumbled upon a million years ago that we haven't even managed to see a hint that it exists. In which case, you'd be right, this is a fallacy. Or, brainmeat can't do it better or more efficiently, and either uses the same techniques or something even worse. The latter seems unlikely, humans still do pretty well at generating text (gold standard, even). > it's better to look at the way generative AI fails than the way it succeeds: when it makes mistakes in text or images, the mistakes are very much not the kind of mistakes that humans make, because the process behind the scenes is very different. But are you looking at \"mistakes\" that are just little faux pas, or the ones where people with dementia, bizarre brain damage, or blipped out on hallucinogens incorrectly compute the next word? The former offer little insight. Poor taste in word choice, lack of eloquency, vulgar inclinations are what they amount to. > but it seems unlikely we've learned to write by subconsciously arranging all the words we've encountered into multidimensional vector space and performing vector math operations to arrive at the next word You think I meant that someone learns to do that at 2 years old, rather than that the brain has already evolved with the ability to do vector math operations or some true equivalent? I'm not talking about some pop psych level \"subconscious\" thing, but an actual honest to god neurological level faculty. > but we reason about concepts and Wander into Walmart next time, close your eyes briefly and extend your psychic powers out to the whole building, and tell me if you truly believe, deep down in your heart, that the humans in that store are reasoning about concepts even once a week. That many, if not most, reason about concepts even once a month. I dare you, just go some place like that, soak it all in. Human reason exists, from time to time, here and there. But most human behavior can be adequately simulated without any reason at all. reply Filligree 2 hours agorootparent> Or, brainmeat can't do it better or more efficiently, and either uses the same techniques or something even worse. The latter seems unlikely, humans still do pretty well at generating text (gold standard, even). Considering we use something like a thousand times the compute, \"something even worse\" seems plausible enough. reply lottin 4 hours agorootparentprevI think we have plenty of evidence that humans have the ability to understand, while chatbots lack such an ability. Therefore, I'm inclined to think that we don't employ some sort of organic LLM but something completely different. reply NoMoreNicksLeft 3 hours agorootparentI've occasionally seen evidence that some humans seem to sometimes understand. I've learned not to generalize that though. reply amonith 6 hours agoparentprevEh, eventually AI will write like humans but currently most of the time it's very much apparent what was written by AI. English is my second language so it's hard for me to pinpoint the exact reason why but I guess it's more about the tone and the actual content (a.k.a bullshit) rather than grammar / choice of words. Most of the time AI slop reads like a soulless corporate ad. Probably because most of the content the AI was trained on was already SEO optimized bullshit mass produced on company blogs. I'd very much like a tool that would detect and filter out also those from \"my internet\". reply retrac98 5 hours agorootparentIf the AI writing is good, you’re not going to know it’s written by AI and you’ll continue to think you \"can always tell\" while more and more of what you read isn’t written by humans. reply cheq 5 hours agorootparentthe only way to know if it's original or AI-written is to know the author writing skills beforehand. reply guappa 5 hours agorootparentprevYeah but to obtain good, you have to give such a specific and detailed prompt that you might just do it yourself. reply amonith 5 hours agorootparentprevYeah but to reach that point you will probably need those \"useless AI detectors\" (as stated by the comment I was replying to). That was my point - we're not there yet therefore those tools can be useful. reply retrac98 5 hours agorootparentBut how do you know we’re not there yet? Not across the board, but isn’t it possible there’s a small yet growing portion of written content online that’s AI generated with no obvious tells? reply amonith 5 hours agorootparentI think we have a misunderstanding - I don't mind if I'm reading AI generated content as long as it doesn't look like \"the typical AI content\" (or SEO slop). In my point of view companies/writers might use AI detectors to continue improving the quality of their content (even if it's written by hand, those false positives might be a good thing). We're not there yet because I still see and read a lot of AI/SEO slop. I agree with you that the \"portion of written content online that’s AI generated with no obvious tells\" is \"small yet growing\". That's exactly the thing - it's still too small to \"be there yet\" :) reply zamadatix 3 hours agorootparentI don't follow how you're reaching your conclusion. You only mind reading AI content when it's obviously AI/slop and you conclude the vast majority of decent content is not AI generated. In your conclusion how were you able to identify good content as being written by AI or not? E.g. it's perfectly possible that in terms of prevalence \"AI slop > AI acceptable > human acceptable\" instead \"AI slop > human acceptable > AI acceptable\" and nothing noted explains why it is one instead of the other. reply rightbyte 4 hours agorootparentprevSemi-automated such is probably widespread by now. Like imagine the Rust Evangelic Task Force, but for the next big thing, will probably be shilled by bots. Low quality content like Youtube and Reddit comments are probably mostly LLM bots who comment on anything to hide the actual spam comments. reply cheq 5 hours agorootparentprev\"most of the content the AI was trained on was already SEO optimized bullshit mass produced on company blogs.\" totally agree in the last part, I work with copywriting and I spent most of my prompts trying to double-down on the pretentious discourse. reply giancarlostoro 6 hours agoparentprevHonestly, I could care less if an author uses AI as long as I can understand what I'm reading and it's interesting. They still have to instruct the AI. reply immibis 6 hours agorootparentIf you're reading nonfiction, it means you're wasting time reading a lot more words when you could have just read the prompt. reply vundercind 5 hours agorootparentRemind you of some entire genres of book? That’s right, business and self-help books! Any of these with an author who’s got actual accomplishments and money before writing the book was almost certainly already ghostwritten from an outline (and so are lots of other books, you’d be surprised, it’s not just these genres). Successful CEOs or people you’ve heard of generally don’t write their own books. Often, they’re terrible writers, and even if they’re not, writing is time-consuming and as with everything else that actually creates something they prefer to pay someone else to do it. As of last year new books in that category are written by AI and edited by one or more humans—with each editor doing just two or three chapters, you can finish one of these books in a month or less. reply oblio 5 hours agorootparentWell, to err is human, to truly screw up you need a computer. We're going to be blasted to smithereens with LLM-generated \"80% should be good enough\" garbage. reply vundercind 4 hours agorootparentIt’s fortunate we have mountains of human-written books, film, television, radio programs, music, and video games from Before AI. Just the good stuff could occupy several lifetimes. Pity we killed most of the good used book stores already, though. Also, shame about journalism and maybe also democracy. That’s too bad. reply giancarlostoro 5 hours agorootparentprevIn my case, I talk a lot, and write a TON, my use for AI is really \"can you say the same information with less words\" then I tweak what it gives me. To be fair, I'm not a paid writer, just a dev writing emails to business people. I rewrite emails like 20 times before sending them. ChatGPT has helped me to just write it once, and have it summarized. I usually keep confidential details out and add them in after if needed. reply gravescale 5 hours agorootparentprevIndeed you can losslessly \"compress\" an LLM's spew into just the prompt (plus any other inputs like values of random variables). But you can also compress a book's entire content into just its ISBN. It's just that books are hopefully more than just statistical mashups of existing content (some books like textbooks and encyclopaedias are kinds of mashup, though one hopes the editors have more than a statistically-based critical input!) reply guappa 5 hours agorootparentYou can't regenerate the book from the ISBN. But you can generate the text from the prompt. reply gravescale 5 hours agorootparentYou can go and fetch the book from a book store using the information. Fundamentally there's not much difference between that and \"fetching\" the output from some model using the matching prompt. In both cases there some kind of static store of latent information that can be accessed unambiguously using a (usually) shorter input. I'm not saying the value of the returned information is equivalent, of course. But being \"just a pointer\" into a larger store isn't, in itself, the problem to me. reply oblio 5 hours agorootparentprevCan you, though? I thought LLMs just by virtue of how they work, are non-deterministic. Let alone if new data is added to the LLM, further retraining happens, etc. Is it possible to get the same output, 1:1, from the same prompt, reliably? reply mrgoldenbrown 4 hours agorootparentThey are assuming a lot of things, like the LLM doesn't change, and that you have full control over the randomness . This might be possible if you are running the LLM locally. reply patrakov 5 hours agorootparentprevNot true if the author of the prompt used an iterative approach. Write the initial prompt, get the result, \"simplify this, put more accent on that, make it less formal\", get the result, and so on, and edit the final output manually anyway. reply HPsquared 5 hours agorootparentprevDepends on your own level of background knowledge vs. the author's. reply RcouF1uZ4gsC 6 hours agoparentprevPrecisely. And they way to get better writing is by having good editors. The major newspapers and magazines used to have good editors and proofreaders and it used to be rare to see misspellings or awkward sentences, but those editors have been seriously cut back and you see these much more commonly. But hey, let’s blame something else. reply luma 5 hours agoparentprevOpenAI announced that they had started on an AI text detector and then gave up as the problem appears to be unsolvable. The machine creates statistically probable text from the input, applying statistics to the generated result will show nothing more than exactly that. You’re then left triggering false positives on text that is the most likely which makes the whole thing useless. reply jgalt212 5 hours agorootparent> OpenAI announced that they had started on an AI text detector and then gave up as the problem appears to be unsolvable. Making a reliable LLM also appears to be unsolvable, but we still work at it and still use the current wonky iterations. My comment is even if there is no perfect AI detectors, a lot of these tools are good enough for a \"first pass\"--coincidentally the same use case many effective LLM practitioners use LLMs for. reply luma 1 hour agorootparentSure it could maybe be kinda right, but what is the cost of a false positive? If you have, say, a 10% false positive rate, and there are theoretical reasons to think you’ll never get that anywhere close to zero, then what use case does this serve? Hey student, there’s 90% chance you cheated, well no I’m that 10%. What now? Again, OAI cancelled work on this believing it not to be solvable with a high degree of confidence. What is the use case for a low confidence AI detector? reply GaggiX 6 hours agoprevThese AI detectors are hilariously bad, if you don't use them just for amusement, then you're using them wrong. reply wslh 5 hours agoprevThe content doesn't matter, that is the reason. It is the quantity and the keywords sorrounding it. reply the_af 5 hours agoprevRegardless of whether AI detectors work or not, I worry we're reaching a new unfortunate era. While alarmists were worrying about alignment or evil AIs, the true downside of LLMs is turning out to be... AI spam. I've started seeing it everywhere: Q&A sites, forums, etc. In some places it's banned, but how do you reliably detect it? And what makes people post AI spam when there's no reward or money involved? What are they trying to achieve? I've seen it in Q&A sites (sometimes tool specific, I won't name names) where supposed \"experts\" are simply pasting LLM-generated crap. All the tell-tale signs are there, often including the famed \"I apologize for making this mistake. You're right the solution doesn't work because [reasons]\". Note it's often not an official AI-generated answer (like, say, the Quora AI bot), but someone with a random human-sounding username posting it as the answer. There are no rep points or upvotes involved, so it boggles the mind... why do they do it? I don't know if HN has some sort of AI filter, but I bet we'll start seeing it here too. Instead of talking to other humans, you'll discuss things with a bot. I predict the arms race between AI spam and AI detectors will only get worse, and as a result it'll make the internet worse for everyone. reply CM30 5 hours agoparentIn most cases, the answer is that they're trying to achieve the appearance of legitimacy, so that when they subtly (or not so subtly) start hawking whatever they're trying to sell, the site/community doesn't immediately flag them as a spammer/bot. Kinda like why in the old days, you'd see comment spam with a lengthy but meaningless auto generated message to go with it. Alternatively, it might be to sell said account to spammers later down the line, since said spammers want to buy social media accounts that have a bunch of legitimate activity associated with them. reply htrp 4 hours agoparentprevhttps://www.nytimes.com/2024/06/11/style/ai-search-slop.html reply formerly_proven 6 hours agoprevI dunno. An outlet I used to follow for about two decades has entered such a steep nosedive in article quality in the last two years I don't bother any more. I don't really care whether that's because they're using AI/LLMs or because the last two competent tech journalists left. reply bowsamic 6 hours agoprevLLM detectors are going to cause more damage than LLM use itself reply falcor84 6 hours agoparentWe're living through a prequel to Blade Runner reply cactusplant7374 6 hours agoprevThere was a reddit post about a college student being accused of using GPT because of the so-called AI detectors. I really think these educational institutions should be sued if possible. reply sampo 6 hours agoparentHere is one, where a college professor didn't even use an AI detector, but simply copied some text from a student's essay into ChatGPT and asked \"Did you write this?\" and ChatGPT answered \"I generated that passage\". And the professor just believed it. https://www.reddit.com/r/academia/comments/14wa4nz/professor... reply Bilal_io 6 hours agorootparentThis is hilarious, sad and scary all at the same time. reply pjc50 6 hours agoparentprevYou can't have a meaningful college course where the work being assessed is done by GPT. reply koolala 6 hours agoparentprevwho will sue? the students with their dreams crushed? you? reply bigfishrunning 6 hours agorootparentIf I were punished by my university for AI-assisted plagiarism I didn't commit, you're damn right I would sue. Imagine getting your life ruined because a neural network deemed it necessary. These things are wrong half the time, why does anyone trust them? reply koolala 5 hours agorootparentYour damn wrong that I wouldn't. I'd quit. I didn't go to university to go to court. reply bsenftner 6 hours agoprevThis is the dawn of a \"post non-narrative prose\" era. Why bother with an article describing some event or really anything that is traditionally written about in any form of article at all? In time, just the data will be released and your favorite LLM prepared with your favorite reading style will provide the mustard for your data, making it consumable. The journalism industry has failed in their mission, and nobody that should trust it trusts it anymore. Articles are simply what someone else wants you to think. reply tivert 5 hours agoparent> This is the dawn of a \"post non-narrative prose\" era. Why bother with an article describing some event or really anything that is traditionally written about in any form of article at all? In time, just the data will be released and your favorite LLM prepared with your favorite reading style will provide the mustard for your data, making it consumable. Who's going to \"[release the data] describing some event or really anything\"? It's not like there's some objective \"data packet\" behind every article that can be had for free. > The journalism industry has failed in their mission, and nobody that should trust it trusts it anymore. Articles are simply what someone else wants you to think. The \"AI\" era will be worse in that regard, not better. You're essentially saying \"The food at the restaurant is terrible and I don't like it, so in the future to solve that problem we'll eat shit instead.\" reply conartist6 5 hours agoparentprevThat's the spirit! Why bother? It's about \"consumability\". Once meaning and purpose are forever vanquished from life, imagine how happy we will all be drooling through our soulless data stream! reply ta8645 4 hours agorootparent> Once meaning and purpose are forever vanquished from life Once you truly appreciate the impending death of yourself and everyone you love, and ultimately the heat death of the universe, how do you find meaning and purpose? How meaningful can it be if it's all going away regardless? Being quite morose and consumed by a debilitating sense of pointlessness, it would be really nice to find some hopeful inspiration. reply immibis 5 hours agoparentprevIn reality they will use the LLM to add the prose to the data and we will use another LLM to delete it. reply antisthenes 5 hours agoprevUltimately, there are only so many ways you can paraphrase an article about a similar event. If something occurs regularly, for example, a sports team winning, or a traffic accident is being reported in local news - then by now there would have been thousands of articles reporting on very similar events. If you feed them all into this plagiarism tool, excluding specific dates and names, how many of them will come out flagged? And frankly, there's nothing wrong with using AI to report on mundane events. What matters here isn't how high-brow or original the text is, what matters is the speed of reporting on the event and the factually accurate description. reply olooney 5 hours agoprev [–] I did some research on this in March and developed an opinionated POV, which I'll paste here for anyone interested. TL;DR: Detecting AI generated content is hard – really hard. The models available today cannot be trusted and should not be used to make important decisions. In fact, OpenAI took down their detector down last year because they couldn't reach an acceptable level of accuracy: https://openai.com/blog/new-ai-classifier-for-indicating-ai-... One open model trained on open data is Hello-SimpleAI's chaptgpt-detector: https://huggingface.co/Hello-SimpleAI/chatgpt-detector-rober... https://huggingface.co/datasets/Hello-SimpleAI/HC3 However, that model is not robust and can be tricked by trivial changes: https://arxiv.org/abs/2307.02599 I verified this result using the playground on hugging face. For example, it is vulnerable to the “one space character” attacks mentioned in the article, severely limiting the usefulness of trying to detect AI content in an adversarial context. This ridiculous piece of \"research\" from Forbes has been causing problems: https://www.forbes.com/sites/technology/article/best-ai-cont... The Forbes article is credulous and uncritical, beyond mere naiveté and approaching journalistic malpractice, reporting the sales stories and self-reporting benchmarks of self-interested parties as fact. Nevertheless, I've seen several people share it as \"insightful\" so it's floating around, doing more harm that good IMO. While all detectors are terrible, Sapling AI has one of the better ones, if only because they are completely open and honest about it's limitations: https://sapling.ai/docs/api/detector/ https://sapling.ai/ai-content-detector Sapling AI also wrote an interesting blog post on GPT SIPs (Statistically Improbable Phrases.) https://sapling.ai/devblog/chatgpt-phrases/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Kimberly Gasuras, a veteran news reporter, lost her freelance job due to accusations of using AI, highlighting the growing reliance on AI detection tools like Copyleaks, GPTZero, Originality.AI, and Winston AI.",
      "AI detectors, despite being advertised with high accuracy rates, are criticized for their unreliability and false positives, impacting professionals' livelihoods and reputations.",
      "Universities and companies are increasingly skeptical of AI detectors due to their inaccuracies, with some institutions banning their use after false accusations, while AI detection companies acknowledge the need for better solutions."
    ],
    "commentSummary": [
      "Mark lost his writing job despite proving he wrote an article by hand, resulting in a significant income loss.",
      "The text discusses the challenges and limitations of AI detection in writing, highlighting issues like false positives and the difficulty of distinguishing AI-generated content from human-written content.",
      "The increasing use of AI in content creation and its impact on jobs, quality of work, and the reliability of AI detectors are central themes, with concerns about AI's role in creative fields and the potential for misuse in surveillance and hiring practices."
    ],
    "points": 149,
    "commentCount": 146,
    "retryCount": 0,
    "time": 1718194742
  },
  {
    "id": 40657307,
    "title": "Duckbill N95 Masks Proven Most Effective at Blocking Airborne COVID-19 Particles",
    "originLink": "https://sph.umd.edu/news/study-shows-n95-masks-near-perfect-blocking-escape-airborne-covid-19",
    "originBody": "May 30, 2024 Facebook Twitter Email COLLEGE PARK, Md. – In a head-to-head comparison of masks worn by people with active COVID-19, the inexpensive “duckbill” N95 came out on top, stopping 98% of COVID-19 particles in the breath of infected people from escaping into the air. Led by researchers from the University of Maryland School of Public Health (SPH), results showed other masks also performed well, blocking at least 70% of viral particles from escaping from the source – an infected person’s exhaled breath. The study, Relative efficacy of masks and respirators as source control for viral aerosol shedding from people infected with SARS-CoV-2, published May 29 in eBioMedicine, a Lancet journal. “The research shows that any mask is much better than no mask, and an N95 is significantly better than the other options. That’s the number one message,” says the study’s senior author, Dr. Donald Milton. Milton is a UMD SPH professor of environmental health and a global expert on how viruses spread through the air. The study started in May 2020, shortly after the pandemic began, and compared breath samples from volunteers who had active COVID-19, testing the performance of four commonly-used masks. Even without giving participants fit tests or training on how to wear masks correctly, all masks significantly reduced the amount of virus escaping into the air. The study tested masks as a way to control the spread of the virus from the source, i.e. the infected person, and did not test masks as protection from COVID-19 in the surrounding air. “Because COVID-19 is airborne, we focused on the extent to which wearing a mask reduces contamination of the air around you,” Milton says. This latest study is a continuation of investigations by UMD’s Public Health AeroBiology Lab (PHAB Lab) into how contagious respiratory viruses such as influenza contaminate the air. Researchers asked volunteers with COVID-19 to breathe into a unique contraption known as the Gesundheit II Machine, developed by Milton and colleagues to measure viruses in exhaled breath. Participants, who breathed into the machine for 30 minutes at a time, were asked to do a variety of vocalizations such as repeating the alphabet, singing Happy Birthday, and even honoring UMD’s mascot by repeatedly shouting “Go Terps!” In each instance, researchers measured the amount of viral particles in the exhaled breath of volunteers, pairing each 30-minute session of breathing with a mask on with another 30-minute session with no mask. “Data from our study suggests that a mildly symptomatic person with COVID-19 who is not wearing a mask exhales a little over two infectious doses per hour,” says first author Dr. Jianyu Lai, a postdoctoral researcher at the PHAB Lab. “But when wearing an N95 mask, the risk goes down exponentially.” The duckbill N95 blocked 99% of large particles and 98% of small particles from escaping out of a person’s mask. Milton says the design’s tight seal, a powerful filter, and large air space for breath to move around all contribute to the duckbill’s success. Surprisingly, KN95 masks – the disposable masks used widely – were no more effective than cloth or surgical masks. The study found that a common brand of KN95 masks leak more air than duckbills or other studied masks, because they don’t conform to the face well. That flaw is compounded by a powerful filter with more flow resistance that pushes air out of the mask at the sides instead of through the filter, allowing more virus particles to escape into the surrounding air. Cloth masks also outperformed both KN95 and surgical masks. Milton theorizes that cloth masks with greater coverage, wrap around the face and give a better seal than either KN95 or surgical masks. With cloth mask filters, flow resistance is also lower, allowing breath to pass through the filter and not leak out the sides of the mask. Limiting the amount of viral particles in the air is a key way to control highly contagious respiratory viruses in general, Milton said. This is even more the case with the COVID-19 virus, given transmissibility has increased over time, with Omicron in particular breaking through the immunity people developed from vaccinations or prior infections. “Our research shows definitively why it’s so important to have non-pharmaceutical responses like wearing masks, and why we need studies like this to illuminate which masks are most effective,” says Milton. Both Milton and Lai hope that their findings will inform health policies going forward, including when combatting potential outbreaks like bird flu or even the common flu. “Duckbill N95 masks should be the standard of care in high-risk situations, such as nursing homes and health care settings,” Lai says. “Now, when the next outbreak of a severe respiratory virus occurs, we know exactly how to help control the spread, with this simple and inexpensive solution.” In addition to researchers from the UMD School of Public Health, collaborators include authors from the UMD A. James Clark School of Engineering and the World Health Organization Collaborating Centre for Infectious Disease Epidemiology and Control at the University of Hong Kong, China. This research was supported by the Prometheus-UMD, sponsored by the Defence Advanced Research Projects Agency (agreement N66001-18-2-4015), the National Institute of Allergy and Infectious Diseases Centers of Excellence for Influenza Research and Surveillance (contract 12-HHSN272201400008C), and the Centers for Disease Control and Prevention (contract 200-2020-09528); by a grant from the Bill & Melinda Gates Foundation; and by a gift from The Flu Lab. ## Media Contact - SPH Communications, sph-comm@umd.edu, 301-405-2438",
    "commentLink": "https://news.ycombinator.com/item?id=40657307",
    "commentBody": "Study shows N95 masks near-perfect at blocking escape of airborne Covid-19 (umd.edu)148 points by robtherobber 6 hours agohidepastfavorite116 comments caleblloyd 6 hours agoIt always baffled me that the US government had a program to send everyone free COVID tests, but they didn’t have a program to send everyone free N95 masks, so we ended up wearing random materials that were far less effective. With all of the mandates to wear a mask everywhere I at least would have liked to wear one that worked well. You would have thought if they could produce 6 free tests per household per month, they could produce 6 free N95 masks per household per month. reply Someone1234 5 hours agoparentThey did have that program: https://x.com/WhiteHouse/status/1483831305448173578 I personally received free masks. However, it was too late. It was mostly early COVID when the masks were most needed and not available. Even the strategic stockpile essentially ran out after distributing them to vital areas. By the time they had enough to give them away, everyone had multiple masks of various qualities. reply silisili 5 hours agorootparentI didn't even realize they did that, interesting. Sadly, two years late and not even nearly enough if it was on time. Kinda telling and depressing that everyone was ordering random knockoffs of varying quality from overseas, because we couldn't seem to produce enough domestically. reply beams_of_light 5 hours agorootparentprevHow would one even work toward positive results with leadership doing things like this? https://www.nbcnews.com/politics/congress/trump-white-house-... reply addicted 5 hours agoparentprevThat’s because the U.S. population was strongly against masks. In the beginning the US public was strongly in favor of the vaccines. In fact the complaints were around the delay in vaccine availability and the way the rich and powerful could access them earlier. It was only in late 2021 where certain people realized vaccine skepticism was lucrative and the Republicans needed something to rile up their base for the upcoming elections did vaccine skepticism become a thing. reply tomp 5 hours agorootparentYou mixed up parties a bit. Democrats were criticising Trump for rushing the vaccine, and flaming vaccine scepticism, helped by mainstream media. https://www.reuters.com/article/idUSKBN2671R8/ September 17, 20201 Democrat Biden warns against rushing out coronavirus vaccine, says Trump cannot be trusted reply padjo 5 hours agorootparentprevIt’s truly insane that vaccine deniers are now voting for Trump, when he was the one to take credit for them in the first place (and perhaps with some small justification!) reply ksherlock 5 hours agorootparentprevYou know what else happened in late 2021? Vaccine mandates in the US (and Canada). In the US, there were two - the first, via OSHA, affected all employers with > 100 employees. Employees would need to be vaccinated or tested weekly and required to wear a mask. This was overturned by the Supreme Court. The second, via CMMS required vaccinations for all health care employees if Medicare or Medicaid money is involved. SCOTUS allowed that one. reply chrismcb 4 hours agorootparentprevMaybe it has something to do with the the medical factors in government, like Dr Fauci telling us not to wear masks. It maybe it was all if the prior research that shows that masks fit the last person were mostly ineffective. reply throwaway48476 5 hours agorootparentprevOn any issue some percent of the population will be contrarian. This is why there's much less resistance if policies are heavily encouraged instead of trying to punish dissenters. At the point where x issue becomes part of the culture war concensus becomes impossible. reply wannacboatmovie 5 hours agorootparentprev> It was only in late 2021 where certain people realized vaccine skepticism was lucrative and the Republicans needed something to rile up their base for the upcoming elections did vaccine skepticism become a thing. This is revisionist history. I remember many Democrats being staunchly anti-\"Trump's rushed vaccine\" and it was only when their party came into power did they turn into human pincushions nearly overnight, \"believing in science\" and not being able to get enough jabs into them. The vaccines, of course, were one and the same. reply mikeyouse 5 hours agorootparentThey were against the obvious (and transparent) attempts to push the vaccine out before the election as some sort of political ploy - not the vaccine itself. Nearly everyone said they'd trust it if the FDA was left to their regular approval process. Which is a shame too, there was good reason to accelerate the approval but the admin messaging was focused purely on the political rather than health benefits of doing so. reply wannacboatmovie 5 hours agorootparentThe regular approval process takes years. We'd still be waiting for it. Yet everyone is in here splitting hairs over what amounted to a few weeks. I know healthcare workers that got the shot in December, the election having been in November. So if the people that literally couldn't wait to get their shot, got it 4 weeks earlier, that would have been deemed rushed and too early to be safe? But four weeks later it's perfectly safe? No, this is about political tribalism and not actual vaccine safety. reply mikeyouse 4 hours agorootparent> The regular approval process takes years. We'd still be waiting for it. Yet everyone is in here splitting hairs over what amounted to a few weeks. This isn't remotely true - it was on the expedited path and was always forecast to be ready in Q4 2020 or Q1 2021. The \"few weeks\" was whether they would be approved before the election or after it - which had clear and obvious political implications. Rather than resting on the laurels of having created Warp Speed and helped usher the the vaccine through development and approvals in under a year, they were leaning on the science teams to approve it to improve his election prospects. There were tons of articles in real time about the entire debacle; https://www.cnn.com/2020/09/23/politics/trump-fda-coronaviru... https://apnews.com/article/election-2020-donald-trump-mark-m... reply hurrdurr57 2 hours agorootparentprevCould you imagine how different things would have turned out if the vaccine was ready while Trump was still in office? The right wing media outlets would be praising it and claiming that it was everyone's patriotic duty to get vaccinated. The left wing media outlets would be pushing out articles and documentaries about how vaccine compliance is rooted in implicit white supremacy. reply kmlx 5 hours agorootparentprevyes, i remember this: https://time.com/5887777/rushed-vaccine-democrats-republican... reply wannacboatmovie 5 hours agorootparentThank you for finding this! > Unsurprisingly, the degree to which Americans are concerned about the President’s influence rushing the vaccine process falls along political lines. About 85% of Democrats and 61% of independents said they were concerned, compared to 35% of Republicans, according to the Kaiser poll. reply viraptor 5 hours agorootparentThis doesn't really match up with your original message. The issues raised on dem side were usually about the process/promises of rushing it, not any the vaccine itself. The specific worry is mentioned at the end of the article and it did come true - the myth of untested vaccine is still alive. There's a big difference between anti-\"Trump's rushed vaccine\" and anti-\"Trump rushing vaccine\". reply silverquiet 3 hours agorootparentprevAnd yet that seems to have changed since the article referenced above was published. > After May 1, 2021, when vaccines were available to all adults, the excess death rate gap between Republican and Democratic voters widened from -0.9 percentage point (95% PI, -2.5 to 0.3 percentage points) to 7.7 percentage points (95% PI, 6.0-9.3 percentage points) in the adjusted analysis; the excess death rate among Republican voters was 43% higher than the excess death rate among Democratic voters. The gap in excess death rates between Republican and Democratic voters was larger in counties with lower vaccination rates and was primarily noted in voters residing in Ohio. https://pubmed.ncbi.nlm.nih.gov/37486680/ reply icameron 5 hours agorootparentprevWell yeah, but that’s mostly because the vaccine wasn’t approved until after their party came into power after the 2020 election, despite Trump’s attempts to rush out a vaccine before the election. reply dwighttk 5 hours agoparentprevA) they did have that program eventually B) money doesn’t magically turn into materials and finished goods. Someone had to make masks to distribute and they had to get their raw materials from somewhere right when the supply chain was disrupted. reply wannacboatmovie 5 hours agoparentprevBecause there was a massive shortage and they were being rationed for healthcare workers? Am I the only one that remembers collection drives for masks for doctors and nurses? reply bee_rider 5 hours agoparentprevN95s were in short supply for quite a while, and a result from this work is that cloth masks outperformed surgical and KN95s. People made pretty good cloth masks, so unless real N95s were distributed, distributing masks could have been a wash I think. I’m not surprised that cloth beat surgical, but beating KN95 is kind of a shock. Of course, I’m sure there was quite a bit of variance in cloth mask quality. I wonder if anyone has done specifically a study on the home made cloth masks. reply huygens6363 5 hours agoparentprevThere weren’t nearly enough of those to go around. Even medical staff had trouble procuring these. reply hnbad 5 hours agoparentprevProblem is probably that the masks are supposed to be single-use when used as protection and you're meant to replace them even when moving between different places. You're also not meant to wear them for extended periods of time because they supposedly get less effective when they get wet from the moisture of the air you exhale. They also aren't washable so you would need to dispose of them after use. reply throwaway48476 4 hours agorootparentThe coronavirus particles are so small they go right through the filter so N95 masks have special fibers that use electrostatic charge to bind them. The electrostatic filter doesn't work effectively when damp, eg after prolonged exposure to humid breath. There's an incredible amount of engineering that goes into PPE. reply DiogenesKynikos 5 hours agoparentprevThe other baffling thing was how long it took cheap Covid antigen tests to reach the US market. In Europe, 1 Euro antigen tests were widely available, while in the US, they could easily cost more than $10 and were difficult to find. At 1 Euro a test, you could easily test yourself every morning before heading to work or school. reply baryphonic 5 hours agoparentprevI guess Big Pharma was better at using lobbying dollars than Big Mask. reply instagraham 5 hours agoprev> the inexpensive “duckbill” N95 came out on top, Inexpensive is not a word I'd use to describe a mask you need multiple of in a week and monthly replacements of thereafter. For a virus that affects the entire world, \"developing\" and \"developed\", it's not an easy sell. But this rather harmless inference could've been promoted instead of the fake initial narrative of \"masks are not that useful, save them for medical staff\" that happened early on in the pandemic. Policymakers should never be gatekeeping information and acting like the public is too dumb to solve resulting supply-side issues. reply presto8 4 hours agoparentI've been wearing the same N95 duckbill for about 3 years now. It is getting a bit dirty but structurally it's fine. I don't wear it every day though, only in what I deem high-risk situations. Its construction is much more robust than the around-the-ears N95s. I think one could reasonably stretch out a box of 20 duckbills for a whole year even if worn daily. They are about $1 each. While I could see that there would be places that $20/year would be too much, I don't think money is the barrier here. People just don't like wearing masks (especially respirators, which are bulkier) for many reasons. reply throwaway48476 4 hours agorootparentYou really should be replacing them as directed by the manufacturer. They're not effective after prolonged use. reply hurrdurr57 3 hours agorootparentprev>I've been wearing the same N95 duckbill for about 3 years now. Brah, you might as well be wearing a petri dish on your face. reply mitchbob 6 hours agoprevDiscussed recently (93 comments): https://news.ycombinator.com/item?id=40565561 reply robtherobber 6 hours agoparentThanks for that. Seems that I've missed it. I suppose this one can be removed. reply InsomniacL 5 hours agoparentprev@Dang the thread linked seems to be wrongly [Flagged]? reply skygazer 5 hours agorootparentArticles sometimes are flagged for poor or inflammatory content, but often it’s the uncivil squabbles that erupt in the comments for completely reasonable but somehow polarizing articles that triggers flagging. I think it’s seen as a regrettable cost of elevating civil discourse. Dang often appears early in such threads “don’t make me turn this car around”-ing, but his warnings are seldom heeded. reply daghamm 5 hours agorootparentprevThis is more and more being abused to kill discussions people don't like. Seems like a vulnerability in the system. reply Springtime 5 hours agoprevThis is an interesting comparison, as there was a NIOSH study in late 2020 (released in 2021)[1] that used equipment to measure procedure masks, surgical masks, proper disposable respirators (with exhalation valves, since that was the main focus of the study) and cloth face coverings and T-shirt fabric (latter two more representative of common DIY masks). These were sealed shut onto the test equipment using wax (ie: perfect fitment, so best case scenario). They found a much higher penetration through the materials per se for cloth coverings and procedure masks. > Particle penetration through the surgical mask ranged from 2% to 17%. Procedure masks were most variable and ranged from 1% to 85% > For the six cloth face coverings, the submicron particle penetration ranged from 24% to 92%. For the two types of fabric from cotton t-shirts, submicron particle penetration ranged from 45% to 91%. Although this sample size was small, these penetration percentages for the cloth face coverings and fabric from cotton t-shirts are comparable to the 40% to 90% penetration found by previous NIOSH testing of cloth face coverings and fabrics. For the respirators with exhalation valves their finding was the particle penetration wasthen it is only the selfish segment of society that is being protected But wouldn't that also protect the unselfish? reply dghughes 5 hours agorootparentMasks aren't worn only by infected people they are worn to protect against the virus. The virus exists whether you choose to wear a mask or not. But masks prevent an infected person from spreading it from coughing or from non-infected people from breathing it in. reply peeters 5 hours agorootparentI think you missed the comment that GP was responding to. They were responding to a claim that if masks only protect the virus from escaping, then masks only protect the selfish population. reply usefulcat 5 hours agorootparentprev> then it is only the selfish segment of society that is being protected. From the 'less selfish' segment, yes, but not from each other. reply throwaway48476 5 hours agorootparentprevMy 3M N95 masks have an one way exhale valve. I don't think it does much to reduce escapes. reply beams_of_light 5 hours agorootparentprevSadly, if we call could just care about each other a little more, this country would be a true shining city on a hill. reply theGnuMe 5 hours agorootparentprevHow can they be one way? reply HankB99 4 hours agorootparentA flap over an opening in the mask is pulled shut when inhaling and pushed open when exhaling. reply bowsamic 5 hours agorootparentprevThis is logically inconsistent reply harimau777 5 hours agorootparentHow so? reply peeters 5 hours agorootparentIf masks only protect the virus from escaping and not entering, then wearing a mask protects non-mask-wearers and other mask-wearers the same. reply hurrdurr57 5 hours agorootparentprev>I never understood all the panic around it I think a good chunk of it was the CDC randomly changing its recommendations with no real explanation. At the beginning of the pandemic in the US, from January to May of 2020, there was a big campaign by the CDC to discourage people from wearing masks. I remember seeing various guest \"medical experts\" on the news claiming that people who wore masks were actually MORE LIKELY to contract COVID. At the same time, virtually every other country was encouraging or mandating masking. Then June of 2020 came around, masks quickly became encouraged and then mandated. Fauci only made vague and nonsensical statements like \"the science changed!\" and \"we're moving at the speed of science!\" to explain the abrupt change in policy. As if there was ever any scientific data to suggest that COVID-19 was not capable of airborne transmission or that masks wouldn't help limit said transmission. I get that there was a shortage of masks early on and the CDC was probably just trying to prevent panic buying (which occurred anyway, there were N95s selling on amazon for $40 per mask). The ridiculous claim that \"masks make you more likely to get COVID\" followed by a sudden requirement to wear said \"COVID causing\" masks completely destroyed public confidence and fueled all kinds of conspiracy theories. reply bee_rider 5 hours agorootparentI didn’t see those reports (but I don’t really watch TV much). I can see a logic that goes: we don’t know if masks help yet. Therefore, if you decide to wear a mask, keep that in mind, and continue to follow the distancing guidelines. Also, be careful when donning and doffing your mask, as you are likely to accidentally touch your face at that point. This is the advice I remember from reading stuff. But, I could definitely imagine people trying to “dumb it down” on TV and coming up with “maybe masks will harm more than hurt.” As a public policy, given the data available at the time that seemed plausible. Unfortunately everybody’s a pundit on TV, so they might discuss public policy and personal advice (as well as their speculation about how people will feel about public policy, completing the media ouroboros) in one sentence and mix them all up. Not sure what the fix is, maybe the CDC needs a more active media department to keep on this stuff. In particular, panicking people seem to want to see how these decisions are made. Have every interview be about the error bars, bookended with the current advice, which is just stay far apart. reply vargr616 5 hours agorootparentprevI saw people reuse the same mask for weeks, cloth or surgical. I think this is what some of the \"experts\" were referring to, that is even more dangerous than being maskless reply DiogenesKynikos 5 hours agorootparentprevIf you read the scientific literature on masking (focused on a flu pandemic) from before the pandemic, there was actually a lot of uncertainty about whether masks would help the general public. It's obvious that masks help if they're worn correctly, but the uncertainty was around the question of whether the general public would actually wear them properly. In hindsight, it seems obvious that mask-wearing should have been encouraged from day 1, but it somehow wasn't initially clear. One problem, I think, might be the emphasis on controlled randomized trials (RCT) in medicine, which are not always appropriate. Think of running an RCT on whether parachutes prevent death when falling from planes. We understand the physical mechanism underlying parachutes, so we know mechanistically why they prevent death from falling. Covid is a respiratory virus. Masks physically filter the virus out of the air you breathe in and out. You don't need a study to know that if worn correctly, they will reduce transmission. The real question is how to teach and encourage the public to wear masks properly, not whether masks work in principle. reply francisofascii 6 hours agorootparentprevIt is a fair question for what one should do if healthy. Most people today are not wearing masks when sick. So how much protection does an N95 give us vs not wearing a mask at all? If we are healthy, should we bother with masks? reply buo 5 hours agorootparentI am healthy, and intend to remain so -- I wear an N95 in all indoor spaces. reply throwaway48476 5 hours agorootparentThe single best prevention for viruses is an adequate amount of quality sleep. reply sofixa 5 hours agorootparentI'd love to see some research on how quality sleep helps with, say, Ebola. reply throwaway48476 5 hours agorootparentSleep is primarily preventative, as it plays a role in the immune system. Personally, I haven't been sick in many years and when other people are getting sick I just get tired and sleep a few more hours than usual. reply InDubioProRubio 5 hours agorootparentprevFor many ordinary citizens its just panic when asked to perform something not within the usual cultural norms. In japan this is normal behaviour, whenever you feel under the weather. Then there were obvious propaganda lies surrounding masks. One was that soldiers wear them- and i was a soldier, with a abc-mask and we \"trained\" marching with them and unscrewed them to get some air. Then some other world-unaware office worker suggested using them in freezers and cooled environments- which basically boils down to waterboarding yourself with condensing air-moisture. Good idea, pushed by panicy people into bad policy, communicated badly, resulting in massive resistance. reply AnimalMuppet 6 hours agorootparentprev\"More importantly\" because people seem to respond better to self-interest than they do to promoting the public good. reply lettergram 6 hours agorootparentprevNot the original poster, but It’s probably more important because you can’t force people to wear masks. As such, the question is “can I protect myself?” If you can, there’s absolutely no reason to force other people to mask anyway & you get the same benefit. That’s the ideal situation for everyone. reply mathstuf 5 hours agorootparent> If you can, there’s absolutely no reason to force other people to mask anyway & you get the same benefit. Until the reaction is along the lines of \"we're so afraid that people shouldn't be allowed to protect themselves\": https://www.starnewsonline.com/story/news/politics/elections... reply mathstuf 5 hours agorootparentIt seems that since it was first reported, there is now an exception (in this instance at least) for those protecting themselves from communicable diseases. reply DonnyV 6 hours agorootparentprevYeah, I mean its not like we live in a community or anything. I should be able to drive as fast as I want through school zones. reply thfuran 6 hours agorootparentAs long as you have a sturdy SUV with a big steel grill on the front, I don't see the issue. reply tivert 6 hours agorootparentprev> Not the original poster, but It’s probably more important because you can’t force people to wear masks. Why can't \"you\" force people to wear masks? Depending on exactly who you mean by \"you\" (the particular poster, an abstract actor (individual or organization), etc.), what you say could be true to mostly false. reply ycombobreaker 5 hours agorootparentI can't speak for anyone else, but this can clearly be interpreted on a personal level. Every day I can do my best to influence public policy; but even if I am successful, a policy change takes time. In the meantime, I can decide whether I will wear a mask; but I cannot force any other person to wear a mask. reply luxuryballs 6 hours agorootparentprevnext [5 more] [flagged] avtar 6 hours agorootparentIn countries where public healthcare is a thing, why wouldn’t trying to limit the spread of something like Covid (that would inundate healthcare facilities) be part of a government’s business? reply codr7 5 hours agorootparentIt would, but forcing the people to do anything shouldn't. reply flawi 6 hours agorootparentprevHow is ensuring the well-being of the people \"not any of the governments business\"? What other end goal could a government possibly serve? reply throwaway48476 4 hours agorootparentLots of things are bad for the well being of the people but the government allows it, such as smoking and drinking. The government serves the will of the people, not their well being. reply justin66 5 hours agoparentprevIt's not the sort of membrane that only works when air is flowing in one direction. It won't protect you from getting goober in your eyes, but when it comes to inhalation, N95 material is great. It also helps that N95 masks tend to have better nosewires for fit, and specify real head straps, which makes them practically more dependable than some masks like most KF94s. reply veidr 6 hours agoparentprevLOL did you mean, \"more selfishly\"? Not trying to be snarky, tbh that is more important to me personally, too... but probably not in the broader sense... reply stuff4ben 5 hours agoprevFWIW, now that N95 masks aren't hard to get, I bought some a few months ago and holy crap they're so much better than the KN95s I had to use during COVID. I basically went maskless at times because the KN95 mask just leaked so much that I couldn't wear them. No matter how well I adjusted the metal nose bar, air leaked up when I breathed and it \"tickled\" my eyes so much I just stopped wearing them. I have since stockpiled a few hundred 3M N95s just in case. reply HPsquared 5 hours agoparentMost masks I used, my glasses would steam up from the amount of leakage. Edit: With all the statistics that were bandied around in that time, I wonder how many accidents were caused by steamed-up glasses due to leaky masks. reply HankB99 4 hours agorootparentI used double stick tape (AKA \"boob tape\" on Amazon) to reduce leakage around the bridge of my nose. It helped reduce fogging of my glasses a lot. I guess it's called boob tape because boobs like me use it? I didn't see any need to wear a mask outdoors (except in a crowded circumstance) or when driving. reply aredox 5 hours agoparentprevThe 3M \"Aura\" masks with their origami-like folds are really good in terms of fit and comfort. I switched to them also when sanding wood. reply freitzkriesler2 6 hours agoprevAbout 3 to 4 years too late but cool to read nonetheless. reply ceejayoz 5 hours agoparentThe people who weren't convinced of \"a good mask reduces spread of a respiratory illness\" 3-4 years ago won't be convinced by this either. It's like when Bill Nye debated creationist Ken Ham; when asked what would convince them they're wrong, Nye said \"evidence\" and Ham said \"nothing\". reply gunapologist99 5 hours agorootparentThe other masks actually didn't work though. Only disposable paper surgical masks (actually disposed of after single use!) worked (if the definition of worked was \"reduced the transmission of at least droplets when coughing\"), or real masks like KN95 or N95 certainly worked. Those cloth masks that were reused every day did far more harm then good, especially if they were not washed. They did nothing against the pressure race of a cough, and most transmissibility was via aerosol anyway, not droplets. The politics simply distorted the science, like when we saw Executive staff wearing logoed blue cloth masks more as a fashion statement than any real science, while subjecting the wearer to reduced air flow, exacerbating things like asthma or cardiac issues, and trapping some virus and random bacteria in a cess pool of saliva in a cloth mask that got saturated within minutes. It's a real shame that people who knew better didn't say, \"these masks work and these other masks are worse than no mask at all.\" Politics took over and both sides were trapped by the nuance, and the result is a stain on the history of science and public health. reply ceejayoz 5 hours agorootparentThe article directly contradicts you. > Led by researchers from the University of Maryland School of Public Health (SPH), results showed other masks also performed well, blocking at least 70% of viral particles from escaping from the source – an infected person’s exhaled breath. > Cloth masks also outperformed both KN95 and surgical masks. Milton theorizes that cloth masks with greater coverage, wrap around the face and give a better seal than either KN95 or surgical masks. With cloth mask filters, flow resistance is also lower, allowing breath to pass through the filter and not leak out the sides of the mask. reply whamlastxmas 4 hours agorootparentIt sounds like those cloth masks are not the style or not worn the way 99.9% of people did. And your quote doesn’t say how often they need to be washed or material or thickness reply ceejayoz 4 hours agorootparent> It sounds like those cloth masks are not the style or not worn the way 99.9% of people did. What in the studies are you basing this on? > And your quote doesn’t say how often they need to be washed or material or thickness Everyone I know washed theirs at least daily. Perhaps we should've done better \"how to wear/wash\" education, but we generally don't need regular PSAs to remind people to wash their underpants. reply whamlastxmas 4 hours agorootparentEveryone I knew never washed theirs, maybe every couple weeks at most reply ceejayoz 3 hours agorootparentThat's gross, but probably says more about the people than the masks. Maybe we do need that PSA. reply cies 5 hours agorootparentprevFor professionals (doctors) it works, we know that for a long time. It's not fail safe, but it works. For the rest of the population it is hard to determine a mask measure works. Even with this study it's hard to determine. Regular people tend to reuse masks too much, not care for quality labels (N95, etc), and not adjust them well. reply 23B1 5 hours agoprevHere's how the headline should read: \"The right mask, worn properly, can do a nearly perfect job of blocking COVID-19\" It's incredible to me that after all this, we still write and publish declarative headlines and articles like this. reply ceejayoz 5 hours agoparentThe article addresses fit. > Even without giving participants fit tests or training on how to wear masks correctly, all masks significantly reduced the amount of virus escaping into the air. Including interesting results around cloth masks fitting better than surgical/KN95 masks: > Cloth masks also outperformed both KN95 and surgical masks. Milton theorizes that cloth masks with greater coverage, wrap around the face and give a better seal than either KN95 or surgical masks. With cloth mask filters, flow resistance is also lower, allowing breath to pass through the filter and not leak out the sides of the mask. reply 23B1 5 hours agorootparentDoesn't excuse the lazy headline writing. If we're talking about a serious public health matter, the press shouldn't shift the cognitive load to their readers ffs. reply ceejayoz 5 hours agorootparentThe headline (and its subheadline) is a fair and effective summary of the article. The rest of the article exists for anyone wanting the rest of the details. You only have to go to the first paragraph to hear about other types of masks, and the actual study is linked if you want to get really into the weeds. reply 23B1 3 hours agorootparentYou're missing my point. reply ceejayoz 3 hours agorootparentIt's a common mistake to mix up \"you're not agreeing with me\" and \"you're not hearing me\". I understand your point; it's just not well supported by the facts. In my opinion, the headline is just fine. reply karol 6 hours agoprevIn females :D (check study) reply n4r9 6 hours agoparentThe study says it had 44 volunteers, 43% of whom were female. reply DonnyV 6 hours agoparentprevYes because men breath differently then women. {eye roll} reply bluejekyll 6 hours agorootparentTo be fair men in general are more likely to have facial hair. Facial hair absolutely impacts the effectiveness of masks. reply elric 5 hours agorootparentprevFacial shape, size, and facial hair (as another commenter here already mentioned) can absolutely matter in mask fit and seal. This is a very common problem in CPAP masks, which use an inflated bladder to adjust to the face of shape. So I can't imagine this *not* being a problem in N95 masks which don't have a bladder but only an edge of slightly flexible material. And if we're being pedantic, men typicall have ~10% larger lung capacity than women, which may well have an impact on mask performance as well. So while the original commenter was factually incorrect, the concern doesn't seem terribly far fetched to me. reply jasonvorhe 6 hours agoprev [–] > This research was supported by the Prometheus-UMD, sponsored by the Defence Advanced Research Projects Agency (agreement N66001-18-2-4015), the National Institute of Allergy and Infectious Diseases Centers of Excellence for Influenza Research and Surveillance (contract 12-HHSN272201400008C), and the Centers for Disease Control and Prevention (contract 200-2020-09528); by a grant from the Bill & Melinda Gates Foundation; and by a gift from The Flu Lab. Thanks for that, I'm not trusting anything from DARPA and BMGF. reply sofixa 6 hours agoparentDARPA I get, morally, but what's your issue with the Bill and Melinda Gates Foundation? reply jasonvorhe 6 hours agorootparentJust a small selection of suspicious activity: * https://economictimes.indiatimes.com/industry/healthcare/bio... * https://www.npr.org/sections/goatsandsoda/2015/08/09/4303470... * https://www.lifesitenews.com/news/a-mass-sterilization-exerc... * https://thediplomat.com/2021/06/why-are-indians-so-angry-at-... * https://tpchronicles.com/un-vaccines-sterilize/ reply lordswork 5 hours agorootparentBMGF promotes vaccines to decrease global mortality rates. It's not surprising a bunch of anti-vax groups are unhappy with them. The scientific method will always prevail. reply throwaway48476 5 hours agorootparentBMGF is a family office that invests in pharmaceutical companies. They also lobby the WHO on pharma issues. It's a giant conflict of interest. reply whamlastxmas 4 hours agorootparentAccording to their 2023 financials, they got 106 million in cash from investment sales, and nearly 8 billion from the trust. The investment side of their operation is a tiny drop in the bucket, especially also considering they gave away nearly 7 billion in grants and programs. If they’re trying to make money off the foundation they’re going about it in a very odd way by giving away 70 times more than they earn from investing reply n4r9 5 hours agorootparentprevThis isn't very convincing for me. For the benefit of other interested parties, the above citation spam categorises into: 1. The Kenyan Catholic church does not like vaccines and will argue against them even in the face of evidence to the contrary. 2. There have been some health effects among a tiny percentage of subjects in vaccination studies in India. The severity, causality, and prevalence of the effects is as yet unknown. reply ceejayoz 5 hours agorootparentprevI tend to be a bit skeptical of Catholic bishops' opinion on scientific issues. reply spywaregorilla 5 hours agorootparentprevThree of these appear to unsubstantiated claims from Keynan power grabs that well documented vaccines are causing sterilization. The vaccine mentioned in the first link is now massively widespread and attributed to saving 10,000,000 lives over the past decade reply jasonvorhe 4 hours agorootparentDuring COVID I dug deep into the vaccine topic and unless I completely failed at research, I couldn't find these claims of being \"well documented\" (and studied). If you have some studies proving how good these vaccines are, I'd be happy to see them! Until then I only mentioned that I don't trust any of these actors. You're free to keep on trusting them. reply n4r9 4 hours agorootparentWhat kind of research did you do? Just having a go now with the HPV vaccine results in this recent systematic review: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8877934/ . From the conclusion: > The preventive effect of HPV vaccines against anogenital and oropharyngeal cancers has been proven in both clinical trials and real-world data. reply ceejayoz 4 hours agorootparentprevThe Kenyan case is about the polio vaccine, which we've been administering since the 1950s. The resulting nearly universal absence of polio (via tens of billions of doses administered) is indeed well documented. reply spywaregorilla 3 hours agorootparentprevIf you didn't find anything on the polio vaccine then I don't think you dug as deep as you thought. reply salah7252 6 hours agorootparentprevThey literally have invested in multiple private prisons lol, both of them. reply shadowgovt 5 hours agoparentprev [–] Literally using the internet to tell people how they don't trust DARPA. Interesting. reply jasonvorhe 4 hours agorootparent [–] I even wrote this comment on an Android device and I don't trust Google. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A University of Maryland study found the \"duckbill\" N95 mask to be the most effective at preventing COVID-19 particles from escaping, blocking 98% of particles.",
      "The study revealed that KN95 masks were less effective than expected due to poor fit and air leakage, while cloth masks outperformed both KN95 and surgical masks.",
      "The research underscores the importance of mask-wearing, especially with highly transmissible COVID-19 variants, and recommends duckbill N95 masks for high-risk environments like nursing homes and healthcare settings."
    ],
    "commentSummary": [
      "The study confirms that N95 masks are highly effective at blocking airborne COVID-19, sparking discussions on why the US government didn't distribute them widely.",
      "The text highlights the political implications of vaccine skepticism, noting how views on the COVID-19 vaccine have evolved, particularly among different political affiliations.",
      "The effectiveness of various masks, including N95, surgical, and cloth masks, is discussed, with a focus on their ability to block viral particles and the challenges in proper mask usage and distribution."
    ],
    "points": 148,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1718195303
  },
  {
    "id": 40653125,
    "title": "AES-GCM Security Risks: The Dangers of Nonce Reuse",
    "originLink": "https://frereit.de/aes_gcm/",
    "originBody": "2024-06-09 Table of Contents An overview of this article (TL;DR) TL;DR: AES-GCM is great, as long as every nonce (mnemonic: number used once) is truly unique. Once a nonce is reused, AES-GCM completely falls apart. If you’ve ever worked with AES-GCM, you may have heard that reusing a nonce can lead to catastrophic security failures. In this post, we will look at how exactly all security guarantees of AES-GCM can be broken when a nonce is reused even once. First, we’ll quickly go over AES, then explain AES-GCM in detail. We’ll then derive some formulas for the AES-GCM authentication tags and see how we can authenticate any message we want as soon as a nonce is reused. This post will be somewhat math heavy, especially as we get to the nonce reuse attack. I’ll try my best to explain any concepts not covered by high-school math, and I’ll skip over any details of the algorithms, as these are best understood by reading the original papers. AES If you’re reading this, chances are you know what AES is. It is the most widely used symmetric encryption algorithm and is almost always the symmetric cipher used when communicating over HTTPS. AES is a block cipher, which means it encrypts and decrypts data in fixed-size blocks. The block size for AES is 128 bits, which is 16 bytes. This means that given a key, the AES algorithm can be used to transform a 128-bit block of data into another 128-bit block of data, and back again. We arbitrarily call this process encryption and decryption, respectively. There’s no intrinsic property of AES that assigns one direction to encryption and the other to decryption. It is important to understand that AES is a bijective function, meaning any 128-bit block of data can be encrypted into a 128-bit block of data, and that any 128-bit block of data is a valid ciphertext that can be decrypted again. This is a fundamental property of block ciphers and important to understand. Because every single possible block of 128-bit data can be decrypted using AES, it is absolutely incorrect to assume that just because you can decrypt some ciphertext, that it was indeed encrypted using the key you have. Anyone “in the middle” of a transmission can just replace the ciphertext, even if they don’t have the key, and AES won’t tell you that that’s happened. There are three key sizes you can choose from when using AES: 128, 192, and 256. The key size changes some internal parameters of the algorithm, but the basic structure is the same. In this blog post, we will only be considering AES-128 but the same principles apply to the other key sizes as well, and for that matter, to any block cipher. AES is a bijective function parameterized by a key It is important to understand that AES-128 is just that, a cipher that encrypts and decrypts a single 128-bit block of data. AES does not specify how to encrypt multiple blocks of data using a single key, nor does it authenticate the data. It is a simple bijective function parameterized by a key, nothing more, nothing less. You can play with the following widget to see how AES-128 works. The widget allows you to encrypt and decrypt a single block of data using a key of your choice. key:plaintext:ciphertext: In summary, we can treat AES-128 as a simple black box that encrypts and decrypts 128-bit blocks of data using a key. It is a building block that can be used to build more complex cryptographic systems, but it is not a complete cryptographic system in itself. GCM When using a block cipher, we want to be able to encrypt arbitrary amounts of data, and we want to ensure that the encrypted data cannot be tampered with. This is where modes of operation come into play. A mode of operation is a way to use a block cipher to encrypt and authenticate data. There are many different modes of operation, but in this post, we will focus on the Galois/Counter Mode, or GCM for short. GCM is a method for authenticated encryption with associated data (AEAD). “Authenticated encryption” means that the mode of operation allows us to validate that a given ciphertext was indeed generated using the secret key, and that it has not been modified. This is called “authenticating” the ciphertext and is essential for secure communication. Without it, a bad actor could modify the ciphertext and the recipient would not be able to detect it, leading to garbled plaintext in the best case, and a huge security issue in the worst case. The second part, “with associated data”, means that we can also authenticate additional data that is not encrypted. This is almost a byproduct of the way GCM works, but it is a very useful feature. For example, when sending a message, we might want to encrypt the message but keep the sender in plaintext. Think of a letter you’re mailing, where the contents are “protected” but the sender is plainly visible. By using AES-GCM, we are able to authenticate not only the ciphertext but also the sender so that the recipient can be sure that the message was indeed sent by the sender and that it is intended for them, even if the sender is not part of the ciphertext. In this post, we’ll look at the two parts of GCM (encryption and authentication) seperately. We’ll start by seeing how encryption works with AES-GCM, where the nonce comes into play, and how the encryption breaks when a nonce is reused. After that, we’ll dive into the authentication part and see how it can also be broken when a nonce is reused. GCM encryption GCM, like all modes of operation, provides a way to encrypt and decrypt arbitrary amounts of data using the underlying block cipher. To do this, GCM uses the block cipher (in our case, AES-128) to generate a sequence of random-looking bits that are as long as the plaintext. This is called the keystream and it can be generated deterministically using the key (and a nonce, which we will get to in a bit), which means although it looks random, the recipient of a message in possesion of the key is able to calculate the same keystream. To encrypt the plaintext, we take the plaintext and XOR it with the keystream. XOR (⊕) is a simple operation that takes any two bits and returns 1 if exactly one of the bits is 1, and 0 otherwise. It, conveniently, is its own inverse. This means that if we generate by XORing the plaintext and keystream (ciphertext = plaintext ⊕ keystream), we can decrypt the ciphertext, we generate the keystream used during the encryption and XOR it with the cipertext we received: ciphertext ⊕ keystream = (plaintext ⊕ keystream) ⊕ keystream = plaintext. So, technically, in GCM, we don’t actually encrypt the plaintext using the block cipher, we encrypt it using the keystream. The block cipher is only used to generate the keystream. However, there’s a problem: Let’s assume for a moment that we have two plaintexts p1 and p2 and we want to encrypt them both using the same key. If the keystream was only generated using the key, then the keystream for p1 would be the same as the keystream for p2. The ciphertexts would then be c1 = p1 ⊕ keystream and c2 = p2 ⊕ keystream. At first glance, this seems fine, but it is not. If an attacker knows the plaintext p1 and the ciphertext c1, then they can compute the keystream by XORing p1 and c1 together (keystream = p1 ⊕ c1) and then decrypt c2 by XORing c2 with the keystream (p2 = c2 ⊕ keystream = c2 ⊕ (p1 ⊕ c1)). This is a huge security issue, as it allows an attacker to decrypt any ciphertext they have the plaintext for, without knowing the key. This is why we need to introduce a nonce. A nonce is a random number transfered along with each ciphertext that may never be reused under the same key. We use the nonce as an additional input to the block cipher when generating the keystream so that for each ciphertext, the keystream is different. This means that even if an attacker knows the plaintext and ciphertext for one message, they cannot use that information to decrypt any other ciphertext. When a nonce is reused, however, the keystream is the same, and an attacker can use the same technique as above to decrypt any ciphertext that was encrypted using the same nonce. Let’s take a look at an example! First, we choose a nonce and generate a keystream using the key from earlier and the chosen nonce: key:nonce:keystream: Then we encrypt two plaintexts p1 and p2 by XORing them with the keystream, which gives us c1 and c2: p1:p2:c1:c2: If an attacker now gets hold of c1 and p1 and wants to decrypt c2, they can use c1 ⊕ p1 to calculate the keystream and then use it to decrypt c2 without knowing the key: c1 ⊕ p1:c2 ⊕ c1 ⊕ p1: Notice how c2 ⊕ p1 ⊕ c1 is the same as p2 as long as p1 is long enough? This is the security issue we were talking about. If the nonce is reused, an attacker only needs a single pair of plaintext and ciphertext to decrypt any other ciphertext that was encrypted using the same key and nonce. Additionally, by obtaining the keystream, an attacker is able to create their own ciphertext for any plaintext they want, simply by XORing the plaintext with the recovered keystream. Therefore, encryption is trivially broken when a nonce is reused. Keystream generation Although not necessary for understanding the security issue, let’s look at how the keystream is generated in GCM. In GCM, we start off with some initial block of 16 bytes called Y0 that is calculated from the given nonce. Incrementing this block means taking the last 4 bytes and interpreting them as a 32-bit big-endian integer and incrementing that integer by 1. We start off by incrementing the initial block and then encrypting it using AES-128. This will gives us 16 bytes of output, which we then use as the first 16 bytes of the keystream. We then increment the block again and encrypt it again to get the next 16 bytes of keystream, and so on. This process is repeated until the entire plaintext is encrypted. Note that if the plaintext falls short of a full 16 bytes, we simply only take as many bytes as we need from the keystream instead of the full 16 bytes. This process of “counting up” and encrypting the resulting block to get the keystream is the “Counter Mode” part of Galois/Counter Mode. GCM uses a simple counter to generate consecutive keystream blocks As you can see, the generated keystream only depends on Y0, which in turn only depends on the nonce, and the key. This means that if the nonce is reused across different messages, the keystream will be the same for both messages, and an attacker can use the technique as above to decrypt any ciphertext that was encrypted using the same nonce. Y0 calculation I told you before that Y0 is calculated from the nonce, but I didn’t tell you how. In the simple case, the supplied nonce is 12 bytes long (e.g. deadbeefcafeaffebadbabe0). In that case, Y0 is the nonce with a 4-byte big endian 1 appended to it, so Y0 = nonce || 0x00000001. If the nonce is not exactly 12 bytes long, we generate Y0 by passing it through a kind-of hash function called GHASH. We’ll look at GHASH in more detail later but for now, just imagine that GHASH takes a nonce of any length and spits out a 16-byte block that we can use as Y0. key:nonce: The nonce is exactly 12 bytes long, so we can use it as the Y0 value directly: Y0: The generated Y0 is then used to generate the keystream as described above. Recap We’ve now seen how the encryption part of GCM works. We’ve seen that the nonce is used together with the key to generate a keystream that is then used to encrypt the plaintext. We’ve also seen that if the nonce is reused, an attacker can use a single known plaintext-ciphertext pair to decrypt any other ciphertext that was encrypted using the same nonce. Once an attacker has obtained the keystream, they can also use it to encrypt any plaintext they want, so there is no guarantee that the plaintext was indeed generated by the sender in possession of the key. GCM authentication The second and equally important part of GCM is the authentication. This means that we are able to validate that a given ciphertext was indeed generated using the secret key, and that it has not been modified. At the risk of repeating myself, remember that the encryption part of GCM only ensures that the ciphertext cannot be decrypted without the key, but it does not ensure that is has not been tampared with. The encryption operation is a simple XOR operation, so if an attacker changes a single bit in the ciphertext from a 0 to a 1, the corresponding bit in the plaintext will also change. This means that even though the ciphertext cannot be decrypted without the key, an attacker can still modify the ciphertext and the recipient would not be able to detect it. With a single known plaintext-ciphertext pair, an attacker is even able to recover the entire keystream and thus encrypt any plaintext they want. Hence, we need to authenticate the ciphertext. Galois field arithmetic A core part of the authentication in GCM is the use of Galois field arithmetic, the other part of Galois/Counter Mode. Galois field arithmetic is just a fancy name for a kind of maths where we don’t have infinite numbers like in the real numbers, but instead we have a finite number of elements. This is why sometimes Galois fields are also called “finite fields”. GF(2) The simplest example of a Galois field is the field of integers modulo 2. This means that we only have two elements, 0 and 1. Forget all other numbers, only 0 and 1 exist when we’re talking about GF(2). Because this is a new number system, we have to definie how mathematical operations work in it. If no other numbers exist, what is 1 + 1? We define addition of numbers in these fields to be the result of adding the numbers in the real numbers and then taking the result modulo 2. The modulo operation simply takes the remainder of a whole number divison. For example, in the real numbers, 5 = 2 * 2 + 1, so 5 = 1 (mod 2). Let’s look at how this looks in an addition table: + 0 1 0 0 1 1 1 0 As you can see, 0 + 0 = 0, 0 + 1 = 1, 1 + 0 = 1, and 1 + 1 = 0. You might recognize this as the XOR operation that we used to encrypt the plaintext with the keystream. This is no coincidence, as the XOR operation is exactly the addition operation in the Galois field of integers modulo 2. This means that when we XOR two numbers together, we can also think of it as adding them together in the Galois field of integers modulo 2. To make it clear that we are working in a different number system, we don’t use the + symbol for addition in Galois fields, but instead use the ⊕ symbol. You might have seen this symbol as the XOR operation, and now you know that it is exactly the same as addition in the Galois field of integers modulo 2. Now, let’s look at how we define multiplication in the Galois field of integers modulo 2. We define it to be the result of multiplying the numbers in the real numbers and then taking the result modulo 2. Let’s look at how this looks in a multiplication table: ⋅ 0 1 0 0 0 1 0 1 Again, you might notice that this is exactly the same as the AND operation. The result of multiplying two numbers in GF(2) is 1 if and only if both numbers are 1. This means that when we AND two numbers together, we can also think of it as multiplying them together in the Galois field of integers modulo 2. Analogously to the addition operation, where we use the ⊕ symbol, we use the ⨂ symbol to represent multiplication in Galois fields. GF(2128) We can also define Galois fields with more than two elements. For example, we can define a Galois field with 2128 elements. This means that we have 2128 different numbers, which is just to say that we use 128 bits (16 bytes!) to represent each number. You might already notice that this is exactly the same as the block size of AES. As you’ll see later, this is no coincidence, because we’ll start interpreting the 128-bit blocks of data as elements represented by numbers in GF(2128). First, though, we need to define addition and multiplication in GF(2128) just like we did for GF(2). One way to think of elements in GF(2128) is as polynomials with coefficients in GF(2). This is just a definition, we have this set of 2128 things, and we say that each of the things corresponds to some polynomial. This means that we can represent each element in GF(2128) as a polynomial of degree at most 127 with coefficients in GF(2). So, some elements of GF(2128) can be represented like so: 0 ⋅ x0 = 0. 1 ⋅ x0 = 1. x4 + x. x127 + x126 + x125. Because we’ve defined the the coefficients to be in GF(2), they are all either 0 or 1. Another way to look at this is that xi is either in the polynomial or it is not, for each i from 0 to 127. This means that we can represent each element in GF(2128) as a 128-bit value, where each bit tells us if a certain power of x is in the polynomial or not. Different standards use different ways to assign the bits to the powers of x but the GCM standard uses a kind-of reversed order, so that the most significant bit of the 128-bit value corresponds to x0, the next bit to x1, and so on, up to the least significant bit corresponding to x127. Let’s encode the previous polynomials in this way as hexadecimal values: 0 is 00000000000000000000000000000000 because no powers of x are in the polynomial. 1 is 80000000000000000000000000000000 because x0 is in the polynomial, which is the most significant bit. x4 + x is 48000000000000000000000000000000. x127 + x126 + x125 is 00000000000000000000000000000007. We now have a way of representing each of the 2128 elements of GF(2128) as a polynomial. To define addition, we use the polynomial representation, which already has a well-defined addition operation. Remember, all coefficients are in GF(2), so we add them together using the addition operation we defined earlier, for example (x2 + x) + (x4 + x2 + 1) = x4 + x + 1. The result is a polynomial of degree at most 127 with coefficients in GF(2), so it is a valid element in GF(2128). Note that if a power of x appears in both polynomials, it will be cancelled out, so for example (x2) + (x2) = 0. If it appears in exactly one of the polynomials, it will be in the result. This is exactly the same as the XOR operation, so we can again think of addition in GF(2128) as the XOR operation on the 128-bit values representing the polynomials. You can try it out for yourself here. Enter two polynomials or their hexadecimal representations and see the result of adding them together: a (polynomial):a (block):b (polynomial):b (block):a + b (polynomial):a + b (block): Hopefully you’re able to convince yourself that adding these polynomials is exactly the same as XORing the 128-bit values representing them. This is the addition operation in GF(2128). Now, let’s take a look at multiplication. Multiplication, just like addition, works by interpreting the field elements as polynomials and then multiplying the polynomials together. For example, (x2 + x) ⋅ (x4 + 1) = x6 + x5 + x2 + x. We get this result by multiplying all the terms in the first polynomial with all the terms in the second polynomial and then adding the results together. In this case, the result is a polynomial with degree 6 and coefficients in GF(2), so it is a valid element in GF(2128). However, look at what happens when we multiply x127 by x: x127 ⋅ x = x128. This poses a problem because we defined GF(2128) to have elements of degree at most 127 but this polynomial is of degree 128. So x128 is not an element which we can represent as a 128-bit block of data. Instead, we need to add another step to the multiplication operation: After we multiply the polynomials together, we divide the result by a special polynomial called the “reduction polynomial” and take the remainder as the result instead. The reduction polynomial must be defined along with the field. It is an irreducible polynomial, which means that it isn’t the product of any two non-trivial polynomials (like how prime numbers aren’t the product of any two other numbers, except one and itself) and has degree 128, so that the remainder of a division operation is always of degree at most 127. The GCM standard defines the reduction polynomial to be x128 + x7 + x2 + x + 1 for the field GF(2128) used in GCM. In this case, where we are trying to reduce x128, it’s actually quite easy to see that dividing x128 by x128 + x7 + x2 + x + 1 will result in a remainder of x7 + x2 + x + 1 because x128 fits exactly once into the reduction polynomial, leaving a remainder of x7 + x2 + x + 1. Multiplying x128 by 1 and adding the remainder confirms this: 1 ⋅ (x128) + (x7 + x2 + x + 1) = x128 + x7 + x2 + x + 1. For more complex cases, we need to use polynomial long division to divide the result by the reduction polynomial and take the remainder as the result. This is a bit more involved, but it is exactly the same as long division with real numbers, just with polynomials instead. I won’t go into the details here because we can just let the computer do the work for us, but if you’re interested, you can look up polynomial long division on the internet and try to apply it to coefficients in GF(2) instead of real numbers1. If we look at the multiplication in GF(2128) from the perspective of the 128-bit blocks of data, instead of the polynomials, the operation is sometimes referred to as “Carry-less multiplication”, because it corresponds to the multiplication of the 128-bit blocks of data, where all carry values are discarded instead of propagated during the multiplication. In this post, we’ll stick to the polynomial representation, but it’s good to know that the operation is also called “Carry-less multiplication” or “CLMUL”, especially if you’re looking at the AES-NI instruction set of modern CPUs. The symbol for multiplication in Galois fields is ⨂, so we’ll use that in the following figures. Let’s go through another example. Enter two polynomials and see the result of multiplying them together: a:b:a ⋅ b: The result of just multiplying the two polynomials is too large, so we have to reduce it modulo the GCM polynomial x128 + x7 + x2 + x + 1 to get the final result: a ⋅ b (reduced): That covers the basics of Galois field arithmetic. We’ve seen that we can add and multiply elements in GF(2128) just like we can add and multiply real numbers, but with the added step of reducing the result by a reduction polynomial. Importantly, we can also think of addition as the XOR operation, which is why it can also be represented by the ⊕ symbol. Multiplication is represented by the ⨂ symbol in the following figures. GHASH Now that we have the basics of Galois field arithmetic down, we can look at the GHASH function. GHASH is a function defined in the GCM standard that takes a key and arbitrary amounts of data and spits out a 128-bit block of data. This block of data is then used in the GCM standard to authenticate the ciphertext and the associated data. To use GHASH, we first need to derive a 128-bit block that we can use as the GHASH key H. This is done by encrypting a block of 16 null bytes using AES and the AES key. This block is then interpreted as a polynomial and used as the GHASH key H for the rest of the GHASH computation: The GHASH key H is derived by encrypting a block of 16 null bytes using AES-128 and the AES key To compute GHASH, we first need to represent the data we want to authenticate as a sequence of 128-bit blocks. This is done by splitting the data into 128-bit blocks and then padding the last block with null bytes if it is not already 128 bits long. We do this separately for the associated data and the ciphertext, so for example if we wanted to authenticated the associated data deadbeef and the ciphertext cafeaffe, we’d use the blocks deadbeef000000000000000000000000 and cafeaffe000000000000000000000000 as the input to GHASH. To make sure the size of the data is not lost, we add one more block at the end that contains the length of the associated data in bits as a 64-bit big-endian integer and the length of the ciphertext in bits as a 64-bit big-endian integer concatenated together, so in this case 00000000000000200000000000000020 for the associated data deadbeef and the ciphertext cafeaffe. To start the computation, we initialize a GF(2128) element Q to 0. We then process the prepared blocks in sequence. The blocks from the associated data are processed first, followed by the blocks from the ciphertext. The length block is processed last. For each block, we interpret the block as a GF(2128) element and add it to Q using the addition operation in GF(2128) (which is just the XOR operation) and then multiply Q by the GHASH key H using the multiplication and reduction operation we defined earlier. The GHASH function processes 128-bit blocks in order The result of the GHASH function is the final value of Y after processing all the blocks. For security reasons that will hopefully become clear later, we cannot use the result directly as the authentication tag. Instead, we encrypt the Y0 block from earlier using the AES key and XOR the encrypted Y0 block with the result to get the final authentication tag. This is the value that is sent along with the ciphertext and the associated data to the recipient, who can then use the same key to compute the GHASH function and verify that the authentication tag is correct. An attacker who does not know the key cannot modify the ciphertext or the associated data without the recipient noticing, because they would not be able to compute the correct authentication tag, because they cannot derive H and the encrypted Y0 block. Formula for GHASH We can also express the GHASH function as a formula. Let H be the GHASH key and Ui be the i-th prepared block. So, in the example above, U0 would be deadbeef000000000000000000000000, U1 would be cafeaffe000000000000000000000000, and U2 would be 00000000000000200000000000000020. We can build the formula for GHASH iteratively: First, we initialize Q to 0: Q ← 0. Then, for the first block, we add it to Q: Q ← Q ⊕ U0 (which is the same as Q = U0 because Q is 0). We then multiply Q by H: Q ← Q ⨂ H = (U0) ⨂ H. For the second block, we add it to Q: Q ← Q ⊕ U1 = ((U0) ⨂ H) ⊕ U1. Again, we multiply Q by H: Q ← Q ⨂ H = (((U0) ⨂ H) ⊕ U1) ⨂ H. We can continue this process for all the blocks and the final result is the GHASH value. The formula for GHASH is then: Q = (((((U0 ⨂ H) ⊕ U1) ⨂ H) ⊕ U2) ⨂ H) ⊕ ... Take some time to look at the formula and see if you can convince yourself that it indeed the same as the graphical representation above. Multiplication and addition in GF(2128) follows the usual laws of multiplication and addition, so we can distribute the multiplication of H over the additions. This means that (((U0) ⨂ H) ⊕ U1) ⨂ H = (U0 ⨂ H2) ⊕ (U1 ⨂ H). We can apply this rule to the formula above to get a more compact formula for GHASH: Q = (U0 ⨂ Hn+1) ⊕ (U1 ⨂ Hn) ⊕ ... ⊕ (Un-1 ⨂ H2) ⊕ (Un ⨂ H). Lastly, we have to XOR the result with the encrypted Y0 block to get the final authentication tag. We have to add the encrypted Y0 block to the result to get the final authentication tag T, because adding in GF(2128) is equal to the XOR operation: T = Q ⊕ Ek(y0) = (U0 ⨂ Hn+1) ⊕ ... ⊕ (Un ⨂ H) ⊕ Ek(y0). Now that we have a formula for GHASH, we can use it to compute the authentication tag for the ciphertext and the associated data. Let’s take a look at an example! Enter some key, nonce, associated data, and ciphertext and see the authentication tag computed using the GHASH function: key:nonce:associated data:ciphertext: Okay, that was a lot! We’ve now seen how the GHASH function works and how it is used to authenticate the ciphertext and the associated data. We’ve also seen that we can represent the GHASH function as a formula that we can use to compute the authentication tag in GF(2128). To verify the authenticity of the ciphertext and the associated data, the recipient can compute the GHASH function using the same key and the same nonce and compare the result to the authentication tag. If the two values match, the recipient can be sure that the ciphertext and the associated data have not been tampered with and that they were indeed generated by the sender in possession of the key. Recap We’ve now seen how GCM works. It allows us to encrypt and authenticate data using a secret key, and detect if the encrypted data has been tampered with. Enter any plaintext, key, and nonce and see the ciphertext and authentication tag computed using AES-GCM2. This time, unlike in plain AES, when you change the ciphertext or authentication tag, the decryption will fail because the authentication tag will not match: key:nonce:plaintext:ciphertext:tag: If you’ve made it this far into the blog post, now may be a good time to take a break and let all the information sink in before we continue on to the attack on AES-GCM when a nonce is reused. If you have any questions, feel free to send me a toot. ☕☕☕☕☕☕☕☕☕ Alright, let’s continue to the attack! Nonce Reuse We have now seen how AES-GCM works and how the authentication tag is computed. We have also seen that the nonce is used in the computation of the authentication tag. But what happens if the nonce is reused? Let’s assume two authentication tags T1 and T2 are computed using the same key and nonce. For simplicitly, let’s assume the blocks used in the GHASH computation were U10, U11 and U12 for the first tag and U20, U21 and U22 for the second tag. In practice, the number of blocks used in the GHASH computation is (almost) irrelevant, but for this example, we’ll stick to three blocks. Let’s write out the formula for the first tag T1: T1 = GHASH1 ⊕ Ek(y0) = (U10 ⨂ H3) ⊕ (U11 ⨂ H2) ⊕ (U12 ⨂ H) ⊕ Ek(y0). The formula for the second tag T2 is similar: T2 = GHASH2 ⊕ Ek(y0) = (U20 ⨂ H3) ⊕ (U21 ⨂ H2) ⊕ (U22 ⨂ H) ⊕ Ek(y0). Notice how in both formulas Ek(y0) appears. This is the crucial part. Remember that Ek(y0) is the encryption of the Y0 block using the AES key. The Y0 block is only dependant on the nonce, which we assume to have been the same in both messages, and of course we assume both messages were encrypted with the same key. This means that Ek(y0) is exactly the same value in both tags. This means that Ek(y0) can be cancelled out by adding the two equations together: T1 ⊕ T2 = ((U10 ⨂ H3) ⊕ (U11 ⨂ H2) ⊕ (U12 ⨂ H) ⊕ Ek(y0)) ⊕ ((U20 ⨂ H3) ⊕ (U21 ⨂ H2) ⊕ (U22 ⨂ H) ⊕ Ek(y0)) = ((U10 ⊕ U20) ⨂ H4) ⊕ ((U11 ⊕ U21) ⨂ H2) ⊕ ((U12 ⊕ U22) ⨂ H). By adding the two equations together, we have completely eliminated Ek(y0). We’ll now look at what’s left in this formula, and how we can use it to recover H. Rearraning the formula by adding T1 ⊕ T2 on both sides gives us a zero on one side of the equation: 0 = ((U10 ⊕ U20) ⨂ H4) ⊕ ((U11 ⊕ U21) ⨂ H2) ⊕ ((U12 ⊕ U22) ⨂ H) ⊕ T1 ⊕ T2 Now, you might notice, this is extremely similar to a polynomial equation. In fact, it is a polynomial equation for H! Forget for a moment that H and all the U values are in GF(2128) and think of any other polynomial equation you might have seen, like 4x4 + 2x3 + 3x2 + 7x + 1 = 0. This is exactly the same, just with H instead of x and with coefficients the coefficients U1i ⊕ U2i instead of a regular real number like 4. Note that attacker that has obtained both transmitted messages has knowledge of T1, T2 as well as U1 and U2, as the tag and ciphetext (with associated data) are the “public” parts of the AES-GCM scheme. So, because an attacker has knowledge of U1 and U2, we can treat this coefficient like any other constant coefficient in a polynomial equation. We’ve already defined the addition and multiplication operations in GF(2128), so hopefully it becomes clear that we can treat the formula we just derived like any other polynomial equation. If we can find a solution for this polynomial equation, we can recover the H value. Why is this interesting? Remember that H is the GHASH key, which is derived from the AES key. If we can recover H, we can use it to compute the GHASH function for any data we want. This means that we can authenticate any data we want, even if we don’t know the AES key. Combined with the keystream recovery demonstrated early, this leads to a full break of the AES-GCM encryption scheme. Solving the polynomial equation So, the question is, how do we solve this polynomial equation? This is where the Cantor-Zassenhaus algorithm comes in. The Cantor-Zassenhaus algorithm is an algorithm that can be used to factor polynomials specifically over finite fields. In our case, we want to factor the polynomial equation we derived earlier over GF(2128). The Cantor-Zassenhaus algorithm is a probabilistic algorithm, which means that it might not always find a solution, but given enough attempts, it will find a solution with an arbitrarily high probability. The Cantor-Zassenhaus algorithm cannot be applied to just any polynomial equation, there are a few requirements that must be ticked off before the algorithm can be used. Lukily, there exist other algorithms to ensure that these requirements are met for any given polynomial: The polynomial must be square-free, which means that it has no repeated roots. For example, let’s say we have a polynomial equation H ⊕ 1 = 0. Remember that addition in GF(2128) is the XOR operation, so if we set H to 1, then H ⊕ 1 = 1 ⊕ 1 = 0. So the polynomial has a root at H = 1. If we now multiply this polynomial with itself, we get a new polynomial: (H ⊕ 1) ⨂ (H ⊕ 1) = H2 ⊕ 1. This polynomial has a repeated root at H = 1, because H ⊕ 1 appears twice in the factorization of the polynomial. You can also think of this as the polynomial having a factor (H ⊕ 1) squared. The Cantor-Zassenhaus algorithm cannot factor polynomials with repeated roots, so we need to make sure that the polynomial we derived earlier does not have any repeated roots, which is the case when it is “square-free”. To achive this requirement, we will be constructing a new polynomial that contains all the factors of the original polynomial exactly once. Although this algorithm changes the “form” of the polynomial, the values of the roots stay unchanged, only their multiplicity (how often they appear) is set to exactly one. The polynomial must be the product of polynomials of equal degrees. For example, take the polynomial equation H2 ⊕ (x ⨂ H) ⊕ 1 = 0. Here the x is the polynomial representation of the block value 40000000000000000000000000000000, like we discussed earlier. This polynomial has degree 2 and is irreducible, which means that it cannot be factored into two polynomials of degree 1. Any polynomial that has a root must have a factor of degree 1, therefore any irreducible polynomial of degree 2 or higher cannot have any roots. If we multiply this by H + 1, we get H3 ⊕ ((x + 1) ⨂ H2) ⊕ ((x + 1) ⨂ H) ⊕ 1, which is a polynomial of degree 3. However, this polynomial cannot be factored using the Cantor-Zassenhaus algorithm because it is the product of one irreducible polynomial of degree 2 and one polynomial of degree 1. We need to split the input polynomial its “parts”, so into a list of polynomials that only have factors of the same degree. This is called “distinct-degree factorization”. It is the last step needed before we can then apply the Cantor-Zassenhaus algorithm to find the roots of each of the “equal-degree” polynomials. The algorithms to make the polynomial square-free and to split it into polynomials of equal degrees are well-documented and there’s even pseudocode available on Wikipedia for square-free factorization and distinct-degree factorization respectively. Once we have a square-free polynomial that is the product of polynomials of equal degrees, we can use the Cantor-Zassenhaus algorithm to split the polynomial into two factors and do so repeatedly until we have found all the factors. I’ll outline the main idea of the Cantor-Zassenhaus algorithm here, but again, you can find pseudocode on Wikipedia3. We want to factor a polynomial f into two factors. This assumes we already have a square-free polynomial and that it is the product of polynomials of equal degrees d. For polynomials in GF(2128), the algorithm works as follows: Pick a random polynomial h of degree less than f and compute the greatest common divisor of f and h. Set M = ⅓(2d ⋅ 128 - 1) and compute the greatest common denominator of hM - 1 and f. We’ll call this g. If g is not 1 or f (which are both trivial factors that we don’t care about), we have found a non-trivial factor of f. We can then recursively factor g and f / g to find all the factors of f. If g is 1 or f, we need to pick a new random polynomial h and try again. By just repeatedly picking a random polynomial, raising it to the power of ⅓ ⋅ (2d ⋅ 128 - 1), subtracting one, and computing the greatest common divisor with the polynomial we want to factor, we can find all the factors of the polynomial. But you might spot a problem: Raising a polynomial to the power of ⅓ ⋅ (2d ⋅ 128 - 1) seems almost impossible, because that number is absolutely huge! But, of course, there’s a trick: Instead of raising the polynomial to the power of ⅓ ⋅ (2d ⋅ 128 - 1), and then computing the greatest common denominator immediately, we can reduce the polynomial by f before computing the greatest common denominator, without “losing” any factors. Calculating an almost arbitrarily large power with a given modulus is a well-known problem in computer algebra, and the square-and-multiply algorithm can be used to compute the result efficiently. Without this trick, the Cantor-Zassenhaus algorithm would be infeasible for polynomials of degree 128 in GF(2128) and it is one of the core reasons why the Cantor-Zassenhaus algorithm is so powerful. Once we have found all the factors of the polynomial, we look at all the factors with degree 1. These are the factors that are linear polynomials, so they are of the form H + a where a is a constant. We then know that when we set H = a, this linear polynomial will evaluate to zero, and is thus is a solution to the polynomial equation we derived earlier. Note that we might have multiple solutions for H if there are multiple roots to the polynomial but only one of them is the correct H used in the GHASH computation, which we will need to use if we want to authenticate other data. To do this, we need a third message that was authenticated using the same key and nonce. We can then use the recovered H to compute the GHASH function for the third message and check if the result of our computation matches the real authentication tag of the third message. If it does, we have successfully recovered the H value and can now authenticate any data we want. Putting it all together We now have all the pieces we need to recover the GHASH key H if the nonce is reused. We can use the Cantor-Zassenhaus algorithm to factor the polynomial equation we derived earlier and recover the H value. We can then use the recovered H to compute the GHASH function for any data we want and authenticate it. Let’s see it all in action! First, we need to simulate the nonce reuse. For simplicity, we’ll ignore the associated data because it doesn’t affect the attack, it just means we have to take the associated data into account when constructing the polynomial equation. Enter a key, nonce and three plaintexts to compute the ciphertexts and the authentication tags4: key:nonce:plaintext (1):ciphertext (1):tag (1): plaintext (2):ciphertext (2):tag (2): plaintext (3):ciphertext (3):tag (3): Now that we have the ciphertexts and the authentication tags for three messages that were encrypted using the same key and nonce, we can recover candidate H values by solving the polynomial equation we derived earlier and then verify the correct H value by computing the GHASH function for the third message if more than one H solves the equation. We’ll need to figure out the polynomial equation to solve. Let’s split up the first ciphertext into their respective blocks and XOR them together to get the coefficients of the polynomial equation: First, we split the first ciphertext into its 16-byte blocks used during the GHASH calculation: U10: 00000000000000000000000000000000 Then we do the same for the second ciphertext: U20: 00000000000000000000000000000000 Now we can calculate the polynomial equation by XORing the blocks of the two ciphertexts together. I'll represent the coefficients of the polynomial in the GCM block representation instead of GF(2) polynomials because it's easier to read: T1 ⊕ T2 = ((00000000000000000000000000000000 ⊕ 00000000000000000000000000000000) ⨂ H1) The last step in constructing the polynomial is to calculate T1 ⊕ T2, which simply means XORing the two tags together. Therefore, we have the equation: 00000000000000000000000000000000 = ⊕ = By moving last term to the other side of the equation, we get the final polynomial we can use to recover the GHASH key H: 00000000000000000000000000000000 ⊕ = 0 Now, we can solve this equation to get candidate H values, one of which is the real H key used during AES-GCM authentication. We have just used the Cantor-Zassenhaus algorithm to recover the H key and Ek(y0) value used during the AES-GCM authentication, which lets us authenticate any message we want. Recap Although more complicated than the keystream recovery, when a nonce is reused, we can use polynomial factorization to figure out the H key, which ultimately let’s us authenticate any data we want. So, in total, the attack goes like this: Alice and Bob agree to a secret shared key. This key is unknown to the attacker. Alice sends Bob a secret message, encrypted using AES-GCM. We assume that the attacker knows the plaintext that Alice sent, and records the nonce, ciphertext and tag transmitted by Alice to Bob. Bob is able to use the nonce and tag to authenticate the ciphertext, and uses the shared key to decrypt the ciphertext. Alice sends Bob two more messages, all encrypted using the same nonce. The attacker can use the first plaintext/ciphertext pair to recover the keystream, and thus recover the plaintext for these messages as well. Bob, again, is able to use the nonce and tag to authenticate the ciphertext, and decrypt it using the shared key. The attacker has recovered the keystream by XORing the first plaintext with the ciphertext. The attacker uses the polynomial factorization to recover the H and Ek(y0) values. The attacker uses these values to construct a message and sends the message to Bob. Bob successfully authenticates the ciphertext, even though the ciphertext wasn’t sent by Alice. Because the keystream is the same as well, Bob is also able to decrypt the ciphertext. This means that Bob has now received a message from the attacker that they think was sent by Alice. Conclusion Thank you for reading all the way through this huge post! I hope I was able to explain AES-GCM well enough and that you’ve got a good feel for why resuing a nonce with AES-GCM is such a big deal. If you’re interested, the code powering all the interactive widgets here is on GitHub. The factorization algorithms were implemented in Rust and built for WebAssembly and all the DOM manipulation and reactiveness was done manually in a 1200 line JavaScript file 😭. This blog post was an enormous amount of work, but I hope it was worth it. If you have any questions, comments, or feedback, please don’t hesitate to reach out to me on Mastodon, I’d love to here from you! Also, if you’d like to send some donation my way, don’t. Instead send some feedback my way and some money to the Electronic Frontier Foundation, The Internet Archive or anything, really, that you think is important. Lastly, a huge thank you to Prof. Dr. Johannes Bauer who very kindly reviewed this blog post and provided me with some valuable feedback. I really appreciate it! Thanks again for reading! Addendum: Using SageMath to do the heavy lifting In this blog post, I’ve shown how the square free factorization, the distinct degree factorization, and the Cantor-Zassenhaus algorithms can be used to break AES-GCM. To be able to show you the internals right in your browser, I’ve implemented the algorithms from scratch in Rust but if you actually want to execute this attack in real life, you can use SageMath to calculate the roots of the polynomial, instead of implementing the algorithms yourself. First, setup the GF(2128) field: >>> F. = GF(2)[] >>> F. = GF(2^128, modulus=a^128 + a^7 + a^2 + a + 1) Then, construct a polynomial ring over this field: >>> R. = PolynomialRing(F) If we now want to find the roots of the polynomial H2 + H + 1, we can use the .roots() method: >>> (H^2 + H + 1).roots() [(x^125 + x^123 + x^120 + x^118 + x^116 + x^115 + x^113 + x^111 + x^110 + x^103 + x^101 + x^100 + x^96 + x^95 + x^94 + x^93 + x^92 + x^90 + x^86 + x^85 + x^84 + x^81 + x^80 + x^76 + x^75 + x^73 + x^71 + x^70 + x^69 + x^68 + x^67 + x^64 + x^62 + x^61 + x^58 + x^57 + x^56 + x^54 + x^53 + x^51 + x^49 + x^47 + x^45 + x^43 + x^42 + x^39 + x^36 + x^35 + x^34 + x^33 + x^32 + x^31 + x^29 + x^26 + x^23 + x^21 + x^20 + x^17 + x^11 + x^9 + x^8 + x^3, 1), (x^125 + x^123 + x^120 + x^118 + x^116 + x^115 + x^113 + x^111 + x^110 + x^103 + x^101 + x^100 + x^96 + x^95 + x^94 + x^93 + x^92 + x^90 + x^86 + x^85 + x^84 + x^81 + x^80 + x^76 + x^75 + x^73 + x^71 + x^70 + x^69 + x^68 + x^67 + x^64 + x^62 + x^61 + x^58 + x^57 + x^56 + x^54 + x^53 + x^51 + x^49 + x^47 + x^45 + x^43 + x^42 + x^39 + x^36 + x^35 + x^34 + x^33 + x^32 + x^31 + x^29 + x^26 + x^23 + x^21 + x^20 + x^17 + x^11 + x^9 + x^8 + x^3 + 1, 1)] This immediately gives us the candidate values for H, no need to jump through any extra hoops. I’ll leave implementing the whole attack in SageMath as an exercise to the reader ;). In real implementations, we can even be a bit smarter about how we multiply the polynomials together, so that we don’t need to do the long division step. Instead, we reduce the result at each step of the multiplication, so that the result is always of degree at most 127 using the Russian peasant multiplication algorithm. Check out the example code on Wikipedia if you’d like to learn more. ↩︎ For simplicity, I’ve not included the associated data in this widget. The associated data is used in the computation of the authentication tag just like the ciphertext, so it really doesn’t matter for this demonstration. ↩︎ Note however that the pseudocode in Wikipedia is for odd-characteristic fields. For GF(2128), we need to raise the the random polynomial h to the power of ⅓ ⋅ (2d ⋅ 128- 1), instead of ½. ↩︎ To prove that I am not cheating, you may omit the key entirely. You can use CyberChef to calculate the ciphertext and tag for any plaintext you wish. First, choose a random key and nonce (CyberChef calls this an IV). You can then delete the key from the field below, and copy over the nonce. Then, enter three random plaintexts in CyberChef (as hex) and copy each plaintext, ciphertext and tag into the fields below. I’ll still be able to recover the H key, without even having access to the key at all. ↩︎ #Cryptography #Algorithm",
    "commentLink": "https://news.ycombinator.com/item?id=40653125",
    "commentBody": "AES-GCM and breaking it on nonce reuse (frereit.de)126 points by _tk_ 18 hours agohidepastfavorite54 comments benlivengood 54 minutes agoI'm curious for the use-cases where people have to maintain a key over a long period where the choice of nonce can't be made strictly non-decreasing or otherwise prevent nonce reuse (per key). I can imagine VPNs or other packetized communications potentially running into this problem, e.g. with N parties needing to encrypt messages under the same key to each other without coordination on nonces. The worst case I can think of is a large number of devices with a baked-in key and secure RNG but no non-volatile storage. They can't generate more than 2^48 messages with AES-GCM or risk collision. Full disk encryption has always had a similar problem; generally a single long-lived master key that individual sectors or blocks are encrypted by, often without the additional storage set aside for IVs or nonces (which would break exact sector to sector mapping of encrypted virtual disk to plaintext disk). That leaves IV-derivation to be static per block offset/number, or key derivation on master key and block offset/number. Devices without secure RNGs are also at risk (microcontrollers with no non-volatile storage that restart a lot, for example). I'm curious if there are any other hard cases where nonce reuse becomes a risk in practice. reply WantonQuantum 14 hours agoprev\"At first glance, this seems fine, but it is not. If an attacker knows the plaintext p1 and the ciphertext c1, then they can compute the keystream by XORing p1 and c1 together\" Also, if the attacker only has c1 and c2, if the nonce is reused then c1 xor c2 will be the same as p1 xor p2. In most cases, two plaintexts xored with each other are trivial to decode. reply Raed667 11 hours agoparentI made an entire \"game\" based on this concept (AES CTR though) https://aes-cpa.fly.dev/ reply chkas 2 hours agoprevI find the article a little confusing. IMHO the point is that you must NEVER reuse an XOR key sequence for stream cipher encryption. With RC4, this meant that you could never use the same key. With modern stream ciphers there is the nonce for this - the CTR mode of a block cipher is also a stream cipher. (GCM mode is just an extension of CTR mode for authentication). I've put together a little online demo tutorial (in my teaching and learning programming language). https://easylang.online/apps/tut_cipher.html?v=2405e reply tptacek 1 hour agoparentGCM's nonce reuse failure modes are worse than CTR's. reply bux93 11 hours agoprevI dunno, nonce means number-used-once, it should be kinda obvious that it should be used only once? reply frereit 9 hours agoparentCorrect. However, some implementations actually incorrectly refer to the nonce as an \"IV\" (initialization vector), where it's not so obvious. Also, it's not entirely clear just how bad a reuse actually is. For example, in AES-CBC, reusing the IV has much less impact than reusing the nonce with AES-GCM. reply kbolino 5 hours agorootparentNIST calls it an IV (or at least did when it came out). reply davidczech 1 hour agoparentprevI've seen the problem surface in designs where a single key is shared across many devices. reply Retr0id 8 hours agoparentprevAnd yet... Aside from just not understanding it, it's plausible that someone would generate nonces weakly, say, from a weak source of randomness. Even using a strong source of randomness for an AES-GCM nonce is weak over enough messages, since it only gets you 48 bits of collision resistance. If you're not using random nonces, maybe you want to use a counter, and then you have to worry about race conditions, state resets, etc. (if your system lost power immediately after using nonce n, would it boot back up and reuse it?) reply stouset 4 hours agorootparent> Aside from just not understanding it, it's plausible that someone would generate nonces weakly, say, from a weak source of randomness. This is actually generally fine for nonces (used in CTR and GCM modes, and in ChaCha20). Typically the only requirement for a nonce is that it is only used once. It is even safe to use a simple incrementing counter. IVs, on the other hand, are required to be cryptographically random. reply Retr0id 3 hours agorootparentMy point is that a random AES-GCM nonce is a problem because you will end up re-using nonces by chance, assuming multiple messages are being encrypted with the same key. Due to the small nonce size, this is still a problem even with secure randomness, it's just even more of a problem with weak randomness. reply stouset 2 hours agorootparentWhoops, sorry. Slept poorly last night and missed this. You're 100% right, GCM doesn't have enough bits in the nonce to safely generate them randomly for any case where keys are reused significantly. reply formerly_proven 7 hours agoparentprevThe consequences that using the number used once twice entail differ wildly between cryptosystems though. reply jobarion 7 hours agoprevI understand that nonce reuse is catastrophic, but I don't think I understand when it can be abused. Does the attacker have to know which two messages share a nonce? Is knowing that out of N messages, at least one pair shares a nonce already enough? reply frereit 7 hours agoparentWell, the nonce is (usually) public information. It is shared along with the ciphertext, so that the other party can use the same nonce to validate and decrypt the ciphertext. So it is trivial to detect which two messages share a nonce, if any do. reply jfyi 3 hours agoparentprev> I don't think I understand when it can be abused The same key + nonce generates the same keystream. The ciphertext is generated by xoring the plaintext with the keystream. The keystream can be recovered by xoring the ciphertext with the plain text. To abuse it... The defender needs to re-use both the same key and nonce. The attacker needs to have a ciphertext/plaintext pair, know or find the position of that text in the keystream, and needs access to other ciphertexts generated with the same key/nonce. reply jedisct1 4 hours agoprevNot just AES-GCM. The vast majority of encryption algorithms must be used in a nonce-respecting scenario. This is part of the contract to achieve the claimed security properties. Alternatives require multiple passes over the data, which is not applicable to some protocols in addition to having performance implications. Common protocols such as TLS transparently handle nonces in a safe way. But the primitives used in TLS may require additional steps to be safely used is other contexts, especially in distributed systems. Whenever applications try to use these primitives directly, using a fixed key and picking nonces at random is a very common practice. Unfortunately, due to their small size, nonces collisions can quickly happen. We're missing standard constructions with large nonces that would alleviate this problem, because IETF protocols haven't needed them. But there's a lot of evidence that many custom applications and protocols do. There are multiple great proposals to derive AES-GCM subkeys and nonces from a key and a large nonce. We may expect convergence and adoption in crypto libraries soon. Until then, constructions such as XSalsa20 and XChaCha20 are widely implemented and deployed. If you don't need NIST compliance, they're excellent choices. But my recommendation today would be to replace AES-GCM with the AEGIS family of algorithms whenever possible. They have nice properties that AES-GCM doesn't have, including more comfortable usage limits, much better performance and large nonces up to 256 bits. This page [1] and that draft [2] summarize usage limits of common constructions, including when using random nonces. [2] https://doc.libsodium.org/secret-key_cryptography/aead [3] https://datatracker.ietf.org/doc/draft-irtf-cfrg-aead-limits... reply e____g 14 hours agoprevIt's worth mentioning AES-GCM-SIV[1], which is the fix for this issue. [1] https://www.rfc-editor.org/rfc/rfc8452.html reply tptacek 13 hours agoparentThe alternative, which I prefer, is an XGCM-like construction that just gives you a large enough nonce to comfortably use random nonces. reply vlovich123 3 hours agorootparentAES-GCM has a 12 byte nonce if I recall correctly. Is 96 bits of entropy insufficient to guarantee uniqueness every time it’s generated? reply aidenn0 2 hours agorootparentOnly if you're not encrypting many billions of small messages with the same key, which is a possibility. It's just barely large enough for many uses, and \"just barely\" makes cryptographers nervous. reply tptacek 3 hours agorootparentprevNo. Extended-nonce constructions solve that problem by using the \"large\" nonce along with the original key to derive a new key. You then have the \"small\" nonce space plus the key space worth of random bits. reply dontdoxxme 12 hours agorootparentprev+1, soatok has a write-up of how that works: https://soatok.blog/2022/12/21/extending-the-aes-gcm-nonce-w... ...a variant on that is DNDK-GCM in draft at https://datatracker.ietf.org/doc/draft-gueron-cfrg-dndkgcm/ and a recent presentation: https://youtu.be/GsFO4ZQlYS8 (this is Shay Gueron who worked on AES-GCM-SIV too). reply bjoli 5 hours agorootparentprevCould this be extended to give us XOCB? I am not sure it would make much sense with the OCB size recommendations. reply notfed 12 hours agoparentprevThe \"fix\" is to use a nonce misuse resistant cipher, of which AES-GCM-SIV is one. But, AES-GCM-SIV requires two passes over the data, which isn't always ideal. The goal of the CAESAR competition [1] was essentially to find alternatives. Whether that goal has been met is a bit unclear at the moment. [1] https://competitions.cr.yp.to/caesar-submissions.html reply throw0101d 7 hours agorootparent> The goal of the CAESAR competition [1] https://en.wikipedia.org/wiki/CAESAR_Competition reply throw0101d 7 hours agoparentprevAt this point OCB has an expired patent, and only needs one pass over the data: * https://en.wikipedia.org/wiki/OCB_mode reply upofadown 6 hours agorootparentFrom the OCB FAQ[1]: >What happens if you repeat the nonce? You’re going to mess up authenticity for all future messages, and you’re going to mess up privacy for the messages that use the repeated nonce. The loss of privacy on OCB nonce reuse is not as severe. It would be more or less the same as with ECB mode. [1] https://www.cs.ucdavis.edu/~rogaway/ocb/ocb-faq.htm reply throw0101d 3 hours agorootparentThe next few lines are: > It is the user’s obligation to ensure that nonces don’t repeat within a session. In settings where this is infeasible, OCB should not be used. But earlier in that section we have: > […] The nonce doesn’t have to be random or secret or unpredictable. It does have to be something new with each message you encrypt. A counter value will work for a nonce, and that is what is recommended. […] * https://www.cs.ucdavis.edu/~rogaway/ocb/ocb-faq.htm#nonce So given that GCM uses a counter (\"C\"), and a counter is recommended for OCB, wouldn't it be simple enough to get the equivalent (?) security more efficiently? reply tptacek 3 hours agorootparentThe notion of a nonce here is the same as that in GCM. GCM nonces aren't secret and don't need to be unpredictable; in fact, because the nonce space is so small, a common engineering recommendation is to use a durable counter. reply throw0101d 2 hours agorootparentGiven that OCB (appears to be?) is more computationally efficient than GCM, is there any reason why OCB shouldn't be favoured nowadays given there are no IP issues? reply tptacek 1 hour agorootparentI like OCB and dislike GCM, but GCM is very, very fast and is the de facto standard AEAD, and the runner-up is Chapoly. OCB would be a quirky choice, and maybe trickier to get in every ecosystem you develop in (I ended up writing my own back in the early days of Golang). reply jas- 15 hours agoprevGreat post! Thanks for taking the time to put this up. What do you think the ratios are regarding improper use of nonce with this mode? Most implementations that I am familiar with intentionally generate a random nonce to help lower the percentage of app devs doing this very thing reply whs 12 hours agoparentMy company need deterministic encryption to search encrypted data. Turns out the people who wrote the in house Go library didn't have any idea. There is no non-deterministic encryption function because that might be too complicated for non-senior engineers (afterall they wrote most of the actual application) to correctly choose. The first version use AES-CFB. There's no authentication. It's probably copy pasted from a public Gist and nobody ever commented on it that it is insecure. I wonder if it was actually intended to be the non-deterministic version, but the higher level wrappers do not wrap this function so people didn't actually use it. The second version use AES-GCM with nonce derived from the key and AD. Since nobody understand why AD is needed, AD is always nil. Essentially there's ever one nonce. I think the problem is that many senior engineers know that encryption use \"AES\" library but the Go standard library doesn't tell you how to use it securely. Surprisingly this mistake also happen in our Java stack that was written by a different team. A senior engineer did notice and quietly moved away from the vulnerable version without telling the Go version. I wrote a POC to decrypt data of the Go version, then wrote the third version, perhaps it will be open source soon. The new library only implement envelope key management, encrypted string wrapper and ORM integration. The rest is Google's Tink. reply bawolff 2 hours agorootparent> My company need deterministic encryption to search encrypted data. I'll take things you should never do as a non-expert for $100. > The first version use AES-CFB. There's no authentication. It's probably copy pasted from a public Gist and nobody ever commented on it that it is insecure. I wonder if it was actually intended to be the non-deterministic version, but the higher level wrappers do not wrap this function so people didn't actually use it. Lack of authentication is probably the least of your concerns if your product is searching over encrypted data. reply kbolino 6 hours agorootparentprevYou use the AD to authenticate additional information that doesn't need to be encrypted. For example, if you separately encrypted every record of a database, you could leave a non-sensitive identifier exposed along with each of them and validate it as the AD when decrypting. This would allow you to find specific records quickly assuming you also had an (encrypted) index or some prior knowledge. As with any case of leaving some data exposed, this can open up certain avenues of attack depending on the threat model. If the data can be tampered with, for example, this isn't a good idea since an attacker can corrupt your database (you'll know, but it will be unusable). [Edit: I was unaware of the existence of \"deterministic AEAD\" before I wrote this: \"Deterministic\" encryption is discouraged because it passes through block-aligned patterns in the plaintext to the ciphertext. There is a simple method to do what you're after: it's just feeding your data (with padding) directly into the cipher (so-called ECB mode). Go's standard library gives you the raw AES cipher to do this with, but it doesn't expose the standard padding mechanisms (and it's not authenticated). You should be aware that doing anything like this leaves your data open to certain kinds of cryptanalysis that can infer the plaintext without directly breaking the cipher.] I largely agree that the standard library doesn't provide any solid guidance or higher-level APIs for any use case other than TLS. The implementations seem to be pretty high-quality but you quickly go from \"it's hard to use this wrong\" in some libraries to \"here's a drawer full of sharp knives\" in others. reply tadfisher 14 hours agoparentprevCorrect. GCM is an improvement over ECB and CBC; it doesn't magically transform a symmetric algorithm into an asymmetric one. So most libraries are going to focus on the use cases where symmetric crypto makes sense, which are single-party scenarios such as disk storage. Google's Tink library, for example, completely hides the nonce parameter from its API. reply denimnerd42 13 hours agoparentprevThe problem with a random nonce is that most implementations also use a nonce of 12 bytes which under some use cases might not be enough before you repeat a nonce. So to remedy this they suggest using a counter but this could be hard to implement. When I use AES-GCM I just use a bigger nonce and use a random one. Last time I used AES-GCM I had a really hard time getting the person writing the other end to not re-use nonces. reply kvemkon 7 hours agorootparent> nonce and use a random one Recently discussed: \"Galois/Counter Mode and random nonces\" (28.05.2024) https://news.ycombinator.com/item?id=40497525 reply imurray 9 hours agorootparentprev> When I use AES-GCM I just use a bigger nonce and use a random one. I don't think nonces bigger than 12 bytes will help. My quick reading of the AES-GCM spec is that when using a nonce that's not 96 bits (12 bytes), it is hashed to 96 bits. So either the nonce (called iv in the spec) is carefully constructed from a counter and set to exactly 96 bits, or the number of invocations is limited. The spec still restricts use of a key to 2^32 total uses for random nonces of any bigger length (resulting in a re-use probability of about 1e-10): https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpubli... reply kbolino 7 hours agorootparentNonces (called IVs in the spec) of any length other than 96 bits are fed into GHASH before use, which has 128 bits of output. This means that IVs can't contribute more than 128 bits of entropy but they may be able to contribute up to that many; it's not clear to me what effect GHASH has on the entropy of the IV though. reply imurray 6 hours agorootparentYou're right, GHASH has 128 bits output. I'd wrongly assumed it was 96 from my quick reading of the conclusion on p29: > unless an implementation only uses 96-bit IVs that are generated by the deterministic construction: The total number of invocations of the authenticated encryption function shall not exceed 2^32, including all IV lengths... Which would be the conclusion if the hash was always reduced to 96 bits. What it actually goes on to say though is: > For the RBG-based construction of IVs, the above requirement, in conjunction with the requirement that r(i)≥96, is sufficient to ensure the uniqueness requirement in Sec. 8 So, it's a bound. If there are at least 96 random bits, it should all be ok. But it strangely leaves open the possibility, without saying either way, that a longer iv, with r(i)>96 random bits might allow generating more iv's. As you point out, it will depend on the properties of GHASH (and potentially on how the result is used downstream from there). At this point I don't know, but the spec says: > For IVs, it is recommended that implementations restrict support to the length of 96 bits, to promote interoperability, efficiency, and simplicity of design. So personally, not being an expert, I'd follow the advice and use 96 bit iv's. And if using random iv's, re-key before using the key a billion times. I'd certainly want a reference to a careful analysis before assuming that I could ever use an AES-GCM key with longer random iv's more than that. reply frereit 4 hours agorootparent> But it strangely leaves open the possibility, without saying either way, that a longer iv, with r(i)>96 random bits might allow generating more iv's. As you point out, it will depend on the properties of GHASH (and potentially on how the result is used downstream from there). There is some details on the \"GHASH as initial counter value\" which seem to suggest that for larger nonces, the total number of messages shouldn't exceed 2^44.5 here: https://neilmadden.blog/2024/05/23/galois-counter-mode-and-r... reply kbolino 5 hours agorootparentprevYeah, interoperability basically dictates 96-bit nonces. I'd say GCM is hard to use correctly for any situation where you have an indefinite amount of data to encrypt and you can't do deterministic nonce generation and you can't rekey easily. reply gnabgib 14 hours agoparentprevI think the writer is @frereit, they submitted 2 days ago https://news.ycombinator.com/item?id=40623885 reply frereit 14 hours agorootparentYes, I am, but unfortunately I do not think I can provide any answers here. A quick internet search reveals some CVEs for nonce reuse. If I had to, based on absolutely nothing but a gut feeling, guess, I'd think this may appear more frequently in IoT devices, where AES-GCM is attractive because of its speed, but randomness is sometimes in low supply? reply frippertronics 12 hours agorootparentAES-GCM is also used in the Bluetooth Low Energy protocol, which is commonly used for IoT-purposes. As a result it’s more often than not available as a hardware-accelerated peripheral, saving both time and power. There’s also hardware-RNG available in those cases. I think one reason nonce-reuse is a problem in IoT is lack of experience and awareness. Up until relatively recently a lot of embedded development was constrained to just offline devices, so cryptography wasn’t really required. reply ctz 10 hours agorootparentBLE uses AES-CCM. reply wigster 8 hours agoprevnonce really should be renamed. if only for british slang avoidance. reply lenerdenator 5 hours agoparentI agree on principle, but I feel the next name will also be turned into another British slang term for sex offenders, if only because some lad will find it funny and become determined to see it through. We are, after all, talking about the country that terrorized an Austrian town into changing its name after decades of sign theft and jokes [0] [0]https://en.wikipedia.org/wiki/Fugging,_Upper_Austria reply elthran 5 hours agorootparentIt's also the language/dialect where you can basically used any phrase ending in -ed to mean \"drunk\". The classics being pissed, plastered, wankered. But if someone came up to me and said \"I got (wardrobed|hadron-collidered) last night\" I'd know exactly what they meant reply commandersaki 10 hours agoprevExcellent article was a very insightful read. reply api 6 hours agoprev [–] GCM is well known to have sudden death on nonce reuse, which is why we used GCM-SIV for the ZeroTier v1 protocol. (Our new protocol that stiiiil is not in product is Noise based.) Nonce reuse in SIV is much less catastrophic. Reuse of a nonce can reveal if two packets are identical but doesn’t let you do anything else. Of course there are categorically better modes than GCM but they are not widely supported. ChaChaPoly is better cryptographically but no hardware acceleration, which matters on small devices. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article emphasizes the critical importance of using unique nonces in AES-GCM encryption to prevent severe security failures.",
      "It explains how reusing a nonce can allow attackers to decrypt ciphertexts and authenticate data without knowing the AES key, compromising the entire encryption scheme.",
      "The post delves into the mathematical foundations of AES-GCM, including Galois field arithmetic and polynomial factorization, and demonstrates how these principles can be exploited to break encryption when nonces are reused."
    ],
    "commentSummary": [
      "The text highlights the risks of nonce reuse in AES-GCM encryption, particularly for long-term key usage scenarios like VPNs, packetized communications, and full disk encryption.",
      "Nonce reuse can lead to severe security vulnerabilities, including potential attacks if an attacker knows both the plaintext and ciphertext.",
      "Alternatives to AES-GCM, such as AES-GCM-SIV, XSalsa20, XChaCha20, and the AEGIS family of algorithms, are recommended for better performance and security, especially in environments where nonce reuse is a concern."
    ],
    "points": 126,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1718150963
  },
  {
    "id": 40650316,
    "title": "Batteries: The Defining Technology of the 2020s",
    "originLink": "https://www.noahpinion.blog/p/decade-of-the-battery-334",
    "originBody": "Share this post Decade of the Battery www.noahpinion.blog Copy link Facebook Email Note Other Discover more from Noahpinion Economics and other interesting stuff Over 216,000 subscribers Subscribe Continue reading Sign in Decade of the Battery A repost with some updates. Noah Smith Jun 09, 2024 165 Share this post Decade of the Battery www.noahpinion.blog Copy link Facebook Email Note Other 101 Share Photo by Alessio Soggetti on Unsplash In 2022, I wrote a post arguing that batteries would be the defining technology of the 2020s. That was just a few months before the release of ChatGPT and the boom in generative AI. In light of that sudden efflorescence, the humble battery might seem quaint and prosaic, and my post might seem laughably mistimed. And yet two years later, I think my post still holds up. These two years have seen small battery-powered drones utterly change the way wars are fought. They have seen battery storage become a common complement to solar, solving almost all of solar’s intermittency problem and drawing huge amounts of investment worldwide. Batteries have powered the sudden meteoric rise of China’s car industry, which went from an also-ran to a juggernaut that suddenly threatens to dominate the entire global auto market. AI is certainly magical and holds incredible potential, but in just a couple of years, batteries have begun to transform human energy supply, war, transportation, and geopolitical dominance. Also, it’s quickly becoming apparent that AI and batteries are, in many ways, perfectly complementary technologies. Drones are human-piloted now, but autonomous AI-operated drone swarms are coming, and they are going to change war and the balance of power yet again. AI promises to turn robotics from a niche type of machine tool into a ubiquitous feature of our world. The rise of autonomous machines will be — must be — battery-powered. So here’s my post from 2022. See how well you think it still holds up. In 2015, Benedict Evans wrote a very influential piece called “The smartphone is the new sun”. By the middle of the 2010s, it was pretty clear that that decade would be defined in many ways by the mobile computing platform that Steve Jobs unveiled in 2007. For regular people, smartphones became the locus of their lives — an ever-present tether constantly connecting them to the digital world and allowing them to navigate the physical one. For industry, smartphones and the applications they enabled became a massive opportunity for investment and profit. Now, however, it seems like we might be reaching the end of that rainbow. The basic reason is that with smartphones, IT companies have now harvested essentially all of people’s surplus attention and time. A new Pew survey of teens’ social media habits found that almost half of young people are online “almost constantly”, up from a quarter in the mid-2010s. And smartphone penetration in rich countries is around 80%. So it’s natural for both consumers and investors/entrepreneurs to be looking around for what technologies might enable another Cambrian explosion of innovation in the next decade. Many people think that this technology is going to be machine learning/AI. But although I do think ML will indeed be very important, I’m going to make an argument that the general-purpose technology that will really transform our society is the battery. This is interesting because it’s a bit of a swerve from the pattern of the last four decades. From 1980-2020, the innovations that reshaped our world were all in information technology — the PC, the internet, the smartphone and the social network. But batteries are an “atoms” technology — something that powers our physical world instead of helping us spin a new digital one. So the Decade of the Battery will look more like earlier decades, in which physical appliances like washing machines, refrigerators, and air conditioning were the hot new thing. In fact, we can already see the shift in the 2010s — after all, batteries are what enabled smartphones. The fundamental reason that batteries are becoming so important is simply that the technology has improved by leaps and bounds. Although batteries are improving on practically any metric, the two clearest indicators are cost and energy density. The cost of batteries, measured in dollars per kWh stored, has fallen by a factor of 42 since 1991 and by a factor of 2.5 since 2010: Meanwhile, energy density more than tripled between 1990 and 2010: Source: Ziegler & Trancik (2021) The lion’s share of this improvement was due to R&D efforts, but industrial scaling effects are starting to have a significant impact as well. But of course that still leaves the basic question of what batteries will be used for. I’m going to go through a bunch of use cases, from the obvious (cars and energy storage) to the not-so-obvious. But first, I should talk a bit about why batteries are such a general-purpose technology. Batteries solve two fundamental problems at once Energy is obviously fundamental to every piece of industrial society, and it’s especially important for physical (“atoms”) technologies like transportation, manufacturing, electricity, and appliances. We often think in terms of primary energy — that is to say, where we get the energy from. Technologies like solar and wind and nuclear and hydropower supply this need, and of course burning fossil fuels does this too. Batteries don’t do this. But there’s more to energy than generation. We also have to store the energy, and we have to transport it from place to place. In other words, we have to move energy around through both time and space, so that we can use it wherever and whenever we want. Batteries address both problems. When it comes to storage, batteries are not yet as good as fossil fuels, because they leak energy over time. This is why they can be used for short-term energy storage for power plants and grids, but not for seasonal storage. But for transporting energy, there are some ways in which batteries are qualitatively superior to fossil fuels. First of all, a battery can be charged by basically any electrical power source, whereas an oil pipeline or a natural gas tank can only really hold one specific type of fossil fuel. Second, extracting energy from a battery is generally much less noisy than extracting energy using a combustion engine. There are other advantages too. Most importantly, though, a battery doesn’t require a bulky combustion engine in order to extract the energy. I first realized the importance of this when I read about Portland protesters in the summer of 2020 using battery-powered leaf blowers to blow tear gas back in the face of federal agents. Battery-powered leaf blowers are less powerful than gasoline-powered ones, but they’re also much lighter. This is why we use batteries for small, lightweight drones. It’s a big part of the reason why your smartphone has a battery in it instead of a little gasoline-powered engine. Any time some battery skeptic tells you that batteries have “lower energy density” than fossil fuels, point this out to them. And as batteries continue to improve, their disadvantages compared to fossil fuels will decrease. We will develop batteries with longer life spans, greater power density, and so on. The obvious use cases — cars and power storage So, on to a big list of things we’ll use batteries for. The two most obvious cases are electric cars and electric power storage for power plants and buildings. Everyone knows by now about the EV revolution — there are Teslas driving all over our city streets. And everyone knows about solar and wind energy, and how these are intermittent and need storage in order to keep powering our buildings at night and during storms. David Roberts has a great post about this, and he notes that transportation is really the bulk of the market here: Source: Department of Energy The EV market is going absolutely vertical, while global sales of internal combustion vehicles have flattened out and are forecast to decline relentlessly. We appear to be in the midst of a rapid shift to electric cars. Nat Bullard @NatBullard 🧵2: By 2025: almost 6 million electric commercial vehicles and buses, 80 million electric cars, and 300+ million electric 2- and 3-wheelers will be on the road about.bnef.com/electric-vehic… 4:16 PM ∙ Jun 1, 2022 20Likes4Retweets And though storage for utilities is a much smaller market, that’s increasing rapidly as well: Utility scale or building scale energy storage won’t really change our daily lives — in fact, the general idea is to prevent our daily lives from having to change as we replace fossil fuels with renewables. Electric cars, though, will definitely make the world feel different. First of all, since electric cars charge at night, it means you’ll only very rarely have to fill up your car, instead of having to go to the gas station every time your tank is empty. As electric cars get better range — and they’re very close to gasoline cars now — this will mean that people will almost never have to stop to fill up, because their car will always start out fully charged in the morning. A big time-waster — looking for a gas station, sitting around huffing toxic fumes while your tank fills up — will mostly just vanish. It will also be a lot easier for autonomous vehicles to self-charge than to pump their own gasoline, once we get those. Second, electric cars are much much quieter than gasoline cars — so quiet that we have to actually add noise just to prevent kids from getting run over in the street! And electric trucks will make living near highways and big thoroughfares much less onerous. Finally, electric cars just make driving a better experience. They’re much more responsive than gasoline cars, and they’re also considerably cheaper to fill up. People who want to switch to transit-centric development won’t like this, but people who don’t have the option of riding the train will be happy. So the well-known use cases for batteries are getting really big already. But in addition, there are a lot of less obvious but still transformative battery-dependent technologies that are growing fast as well. Don’t sleep on e-bikes Everyone talks about electric cars. A few people talk about electric scooters. Very few people talk about e-bikes. But more electric bicycles are now sold than electric cars in the U.S.: E-bikes are already a $40 billion business globally, and growing fast. E-bikes aren’t like normal bikes — they add electric power to your muscle power to produce a much easier ride. That transforms cycling in two ways. First, it allows you to go much farther. From a recent study in Norway: The people who bought e-bikes increased their bicycle use from 2.1 kilometers (1.3 miles) to 9.2 kilometers (5.7 miles) on average per day; a 340% increase. The e-bike's share of all their transportation increased dramatically too; from 17% to 49%, where they e-biked instead of walking, taking public transit, and driving….[I]n fact, people rode their e-bikes more the longer they had them[.] Second, e-bikes make it pretty easy to bike uphill. That opens up huge areas to cyclists that were basically denied to them before (e.g. most of San Francisco). The combination of much longer range and not having to worry about road grades mean that e-bikes have the potential to transform the way we live. Since the U.S. already has roads and can add bike lanes where they don’t already exist, the cost of converting cities to accommodate masses of e-bike commuters will not be large (certainly smaller than, say, building trains!). And since e-bikes are small and easy to park, we can build a lot of bike racks close to common destinations, taking away some of the hassle of finding a parking space when commuting. All in all, e-bikes will simply make it much easier to get around our cities, which hopefully will increase human connection and make us feel more free. They will also make food and product delivery a lot easier in many areas. And because e-bikes do still use a little muscle power, we might become a bit healthier too! It’s also worth noting that electric cargo ships and planes are in the pipeline, though I don’t expect these to become common in the next decade. Drones are the future of warfare Batteries aren’t just going to change civilian transportation — they’re going to change military vehicles as well, enabling whole new ways of warfare. Anyone who is paying attention to the war in Ukraine already knows this. Although combustion-powered drones like the Bayraktar get most of the attention, little electric drones have started delivering precision strikes at relatively close range on the battlefield. Here is a video: kirby griffin @kirbygriffin17 Ukrainian Drone Drops Grenade directly into a Russian Tank with crew sti... youtube.com/shorts/I-XK_lp… via @YouTube youtube.comUkrainian Drone Drops Grenade directly into a Russian Tank with crew still in. Historical Footage 7:37 AM ∙ Aug 23, 2022 1Like1Retweet And this isn’t just because Ukraine has less money and is forced to resort to cheap crappy weapons; the Russians are using battery-powered drones as well. And of course these drones are also extremely useful for battlefield reconnaissance, which is crucial for targeting modern precision weapons. The advent of cheap reliable energy-dense batteries makes this possible. Battery-powered drones are small, cheap, and relatively quiet. And in the era of increased geopolitical competition and conflict, innovation in military drones is probably just getting started. Anti-personnel drones to clear soldiers out of dense urban areas with minimal damage to the surrounding structures are probably on the way. Autonomous swarming drones are being developed too. And battery-powered robots may help carry equipment across the battlefield, or even carry and fire weapons previously reserved for infantry. In the extreme scenario, humans could be driven from the battlefield completely. Batteries, with their small size, light weight, and cheap price, will probably be crucial to that. Battery-powered everything EVs, ebikes, and drones only scratch the surface of the transformation of our physical world that light, small, cheap energy portability provides. First, consider appliances. Cordless equipment will gradually take over from stuff that needs to be plugged in all the time. Battery-powered air purifiers and fans and humidifiers will stand around our houses and buildings. Battery-powered kettles and crock pots and pressure cookers and other cooking appliances will let us cook food in the park. Cordless vacuums, leaf-blowers, and mowers will make it much easier to keep our houses and lawns neat and tidy. Battery-powered webcams will allow us to more easily capture much of the world on video. And so on. Just look at everything that has a cord in your house, and ask if you might want to run it far from an outlet. Batteries let you do that. Some special kinds of batteries also have other capabilities that allow them to be used in everyday applications that most people wouldn’t think of. A really cool example is fast-discharging batteries that can be used to make super-powerful appliances. Next, think about robots. Currently we have battery-powered roombas vacuuming our houses. Little delivery robots are starting to trundle down the street. Robot waiters are starting to appear in restaurants. And robots are going to be used in all kinds of industrial applications, from transporting parts and tools around factory floors to cleaning off solar panels. Battery-powered farm vehicles are beginning to transform agriculture. In fact, although I said I didn’t think machine learning was going to be the #1 most transformative tech of the 2020s, it’s obviously the case that AI and batteries go together like peanut butter and jelly. As battery-powered drones and robots proliferate and fill our physical world, AI will be crucial for making sure it all runs correctly. So just as the smartphone revolution was powered by batteries, the battery revolution will be powered by AI. Basically, the battery revolution means that the pieces of our world are starting to free themselves, get up, and move around. Venture capitalists and technology companies, of course, stand to profit handsomely off of all this. “Deep tech” is a hot buzzword in the VC industry right now, but the big challenge is that many forms of deep tech — biopharma and other research-intensive industries — is that the long lead time and high up-front cost presents big challenges for the traditional low-cost, fast-scaling VC financing model. Battery-driven tech, however, may prove an exception. The fact that the basic research is being done in universities frees up VCs to fund applications instead of labs. And the fact that energy portability allows gadgets to adapt themselves to existing infrastructure means that investors won’t have to tussle so much with regulators or fund expensive modifications to the built environment. There are, of course, some big obstacles for us to overcome in the Decade of the Battery. Chief among these is mineral availability. Putting batteries in everything requires mining a whole bunch of lithium, copper, cobalt, and other minerals (David Roberts, as usual, has an excellent rundown). This will require us to do a lot of mining, to shift toward types of batteries that aren’t as dependent on rare minerals, and to diversify our sources away from unfriendly regimes. It’s a challenge, but probably a doable one. And of course recycling and disposal of batteries will be a major task as well. But no technological revolution has been without its challenges and its issues. The upside of batteries — a technology for cheap light energy portability and storage — just swamps the difficulties and downsides. It took a long time for this general-purpose technology to arrive, but now it’s here, and the next decade (or, really three decades) will see it transform the world around us. Subscribe Share 165 Share this post Decade of the Battery www.noahpinion.blog Copy link Facebook Email Note Other 101 Share PreviousNext",
    "commentLink": "https://news.ycombinator.com/item?id=40650316",
    "commentBody": "Decade of the Battery (noahpinion.blog)123 points by paulpauper 22 hours agohidepastfavorite106 comments throwitaway222 12 hours agoI already started seeing Sodium based batts on Alibaba. But they are 3.1 volts, so I imagine there will be a new slew of supporting BMSs and other things that need to get involved. Watching closely. reply unwind 8 hours agoparentWow, yeah, they have even trickled down to AliExpress already [1]! Hadn't seen that, thanks for the tip! Pricing for the one linked (cheapest when I sorted my search results, but magic happens when you sort by price on AE so take with a pinch of ... sodium I guess) is around $9 for a single cell (3 V, 3500 mAh) or $100 for a 20-pack. No idea if chargers are available, haven't checked. Edit: actually include the link. 1: https://www.aliexpress.com/item/1005005870547110.html?spm=a2... reply rini17 12 hours agoparentprevYes and they have steep discharge curve. Some youtubers subjected them to puncture tests too and they did not catch fire, that is looking good. reply silverquiet 8 hours agoparentprevWhat is the advantage of a sodium based battery? reply turblety 8 hours agorootparentSodium-ion batteries are rechargeable batteries that use sodium ions (Na+) as its charge carriers12. They are similar to lithium-ion batteries in terms of working principle and cell construction, but they replace lithium with sodium as the intercalating ion1. Sodium-ion batteries are cheaper to produce, safer to use, and operate better in extreme temperatures than lithium-ion batteries3. However, sodium batteries of equal capacity are heavier and larger than their lithium equivalents3. reply roer 8 hours agorootparentAt least some of this seems like an a summary of this: https://www.sciencedirect.com/topics/physics-and-astronomy/s... Did you forget your sources? And why did you write in such a strange way? reply radiojosh 1 hour agorootparentIn what way could you possibly interpret that writing as \"strange\"? That was an excellent, straightforward response. reply burnished 1 hour agorootparentprevI don't think this is a healthy level of suspicion to levy against an otherwise correct summary. If you think their comment would be improved with a citation then simply adding it would be good - cited comments are relatively common here but not mandatory and generally not how people communicate. reply defrost 8 hours agorootparentprevThe extra weight is an issue for cars. Not so much for neighbourhood battery packs that smooth local area roof solar or for industrial scale battery parks next to solar and wind farms. reply silverquiet 8 hours agorootparentYeah, I have enough high school chemistry knowledge to know that lithium and sodium are in the same column on the periodic table at least and can read about energy density and cost, but I'm really wondering where these things will fit in in the economy. In theory I might dabble with hobbyist level solar systems and controllers and battery packs, but I don't exactly feel like sourcing a bunch of 18650 cells and spot welding things together - give me an already packaged battery please. reply pjc50 6 hours agorootparentprevWhen you mention lithium people put up all sorts of objections about \"scarcity\". Whereas sodium is an extremely abundant component of seawater. (You do have to find something to do with the spare chlorine ion, though) reply wolfram74 19 hours agoprevMentioning the pedal assist bikes is important, for the amount of battery a cybertruck ships with, you could ship 400 of the ebike I use for 90% of my trips. reply bdamm 12 hours agoparentAnd I can create an infinite number of \"real\" bicycles since they don't use a battery at all, plus they're even better for your health than an e-bike! Snark aside, e-bikes are great, I don't have one but that's because between my nice road bike that can crank out 100 miles no problem, and my battery car, I don't need one. But they do work for lots of people. I wish I could get one for my kid, but it is illegal for anyone under 14 to ride one, where I live. reply _ph_ 12 hours agorootparentThat a non-electric bike is better for your health is often wrong. If you are very sporty, you are fine. But there are plenty of people who would get better exercise with an e-bike, as they are more likely to use it or just use the electric assist to bike more often or longer distances. Also, there are a lot of regions where there are steep roads which could be obstacles without assist. reply ljf 11 hours agorootparentThis - an e-tricycle kept my dad active until the day he died - he loved it. There is no way he could have ridden entirely under his own steam, but the e-trike was a life-changer for him, and us as a family. reply acheong08 4 hours agorootparentprevMight be a dumb question but I’ll ask anyways. One of the main factors preventing me from owning a bike is the lack of a permanent residence. I’m forced to move every few months/years, sometimes to a different country. What’s the best way to bring such a large object with me? So far I’ve been forcing myself to own as little as possible such that I can bring everything with me in a single luggage and backpack reply enragedcacti 4 hours agorootparentThere are a lot of folding bikes that can compact small enough to go in an overhead bin or get checked as luggage. You could also just stick with budget non-folding options then sell your old one and buy a new one at your destination. You'll lose some money every time but on the upside you can buy a bike that fits your lifestyle at every destination, or skip getting one when it doesn't make sense. Brompton bikes are one of the classic recommendations for folding bikes but there tons of other options nowadays as well. reply cuSetanta 3 hours agorootparentprevI have moved around a fair bit in the past decade, and for me the most effective movers I have used have been SendMyBag [1]. They are great for shipping luggage and other reasonably large items. They arrange an DHL pickup and drop, with a lot of flexibility, and their support has been great for me when I needed it. SendMyBag do ship bikes, but I have mostly used more dedicated bike shipping services, which have been very reliable. SendBike [2] iswho I haved used in the past, but I have also had friends use ShipMyBike [3] successfully in the past. The advantage of these companies is that they will first ship you a box to use, if you dont have one already. [1] https://www.sendmybag.com/en-gb/ [2] https://sendbike.com/ [3] https://www.shipmybike.com/ reply rvense 11 hours agorootparentprevIt's a stupid thing, but I wish I could hack the cargo bike I use to bring my kids to school so it wasn't at the second highest level of assist when it turns on. I often forget to turn it down and miss out on a lot of exercise that way... reply beAbU 8 hours agorootparentprevLet me tell you, since I have only access to an analogue bike, and no car at all, an e-bike is looking more and more like a it'll be life changing for me. We have some rental e-bikes in my town that I've used from time to time, but I don't like them as the designated parking spots is not ideal for my use case. Access to an ebike will probably result in me taking significantly more trips with it, where I would otherwise have opted to either rent a car for an hour, or just not make the trip. I don't have hard evidence to back up the claim, but I'm certain that an e-bike will rack up more than enough km's in a time period that the overall volume of ebike-exercise outweighs analogue-bike exercise, even if you subtract the \"lost\" exercise due to the e-bike assistance. The amount of emotional and confidence boost you get from 250W of electricity honestly cannot be measured. reply Moldoteck 9 hours agorootparentprev\"they're even better for your health than an e-bike!\" - debatable. Ppl with ebikes tend to have more exercise since they don't get tired that quickly, unless you are talking about electric scooters or hyper ebikes where you don't need to pedal bc of powerful motor reply 4g 12 hours agoparentprevThis isn't zero sum. You can do both reply rvense 11 hours agorootparentIt sorts of is, actually? If our capacity to make batteries is limited (or make batteries within some fixed, acceptable, level of environmental impact.) reply ithkuil 10 hours agorootparentUnfortunately that's now how humanity builds stuff. It's rarely the case that \"we all decide that we have this very important thing to do for humanity\" and focus on doing just that. More often the benefits for humanity flow down as side effects from other more basic incentives like making money selling fast sport electric vehicles used to signal that you're rich or whatever reply LUmBULtERA 7 hours agorootparentprevI think I read in Walter Isaacson's Musk biography that when the Model S came out, it was using something like 10% of the worlds supply of Lithium ion batteries. And that was a pretty small number of Model Ss. EVs have scaled up massively since that time and battery quantity has kept up -- my point being, as long as the price of the ebike is able to pay market prices for the batteries, it seems like supply will scale. reply Animats 20 hours agoprevIt's just getting started. Solid state batteries are getting close. There are big companies, from Toyota on down, putting together manufacturing facilities. Nio is already shipping an early semi-solid-state battery. The solid state battery era looks like this: - Much better safety. No thermal runaway problem. Survives nail test. - Upwards of 5,000 charge cycles. - Charge times around 5 minutes. - Maybe better energy density. - Cost not yet well understood. Manufacturing is hard. For small devices, the battery should outlive the device. For cars, the usage model looks more like gas cars. Drive 300-400 miles, recharge in 5-10 minutes. This has land-use implications. Gas stations can convert to charging stations in the same footprint and layout. No need for giant parking lots of chargers. reply silvestrov 10 hours agoparentSupermarkets and small malls might take over the charging locations as they already have plenty of parking space. Only reason for gas stations to be a seperate type of shop is how expensive and dangeous it is to have a gasoline pipes to every parking space. This is easy and cheap to do for electricity. To bring in more revenue many gas stations try to be a \"bit of a supermarket\" in addition to selling gas. With electricity it will likely go the other way around, so supermarkets will extend with selling electricity and there will less reason to have gas stations as a seperate business location. I already see supermarkets here getting a few charging spaces, usually a bit in the back. reply vel0city 6 hours agorootparentMost of the DCFC's I've used were in Walmart parking lots. reply _carbyau_ 13 hours agoparentprevWhile the land use might not change the electricity requirement will change dramatically! To the point I'm not sure if it is economically feasible to convert a liquid fuel station. Maybe the new 'hot' sites for such will be next to grid substations. reply closewith 11 hours agorootparentCould existing substations even come close to that kind of load? 8 cars recharging at MW rates would saturate most substation transformers in my area. reply zizee 6 hours agorootparentPerhaps installing a big battery bank that trickle charges would smooth the load. How many MWs would a carpark roofed with solar panels generate? My calculations say you need about 4k square meters to get 1MW whilst the sun is shining bright. That's about the size of medium sized supermarket carpark. Double that if you cover the building as well. You might get a good 2MW at peak, enough to allow a few cars charging at once. Not much good at after the afternoon sun has gone down and the after work rush is happening at the supermarket. Given most drivers will be charging at home, I wonder what percentage of cars we can expect to be wanting to charge whilst shopping? reply coderenegade 10 hours agoparentprevThe bottleneck for charging hasn't been the battery for a while, because we're already at the point where we could hook up multiple chargers for better performance. The bottleneck is in electricity as an energy vector, because someone either needs to start generating several MW per car, which is expensive, or be curtailing the same amount, which is wasteful. reply kragen 13 hours agoparentprevthe cost thing seems pretty crucial. if room-temperature solid-electrolyte batteries end up costing ten times as much as lithium-ion batteries per joule, they'll remain a niche technology, though potentially one that's used in every cellphone, like tantalum capacitors. if they end up being one tenth the cost of lithium-ion batteries, indeed, it's just getting started. can we even confidently bound the ultimate cost to within that interval? reply Animats 12 hours agorootparentCost is a big problem. Some sources say 3x to 4x the cost of lithium-ion batteries, at least initially. Nissan and Toyota both claim manufacturing cost breakthroughs. Apparently this is hard to do, but not a raw materials problem. Lithium-iron-phosphate batteries have taken over the low end of the market. Cheaper and safer, but less energy per kilogram. reply closewith 11 hours agorootparent> Some sources say 3x to 4x the cost of lithium-ion batteries, at least initially. If that's true, that's miraculous. Given the order of magnitude reduction that will come with economies of scale and commercialisation (as we've experienced with Li-ion), they'll be cheaper than Li-ion in no time. reply deyiao 13 hours agoparentprevI bet I saw the exact same thing about 14 years ago. reply tcoff91 19 hours agoparentprevGas stations could even double as charging stations as well and serve both groups if charging could be done in 5 mins. reply kvgr 6 hours agoprevI was looking at car camping. And i found out that I can get power pack for cooking/boiling water, that is very multipurpose for other things just camping. Def not usable for hiking, but car can take it, solar panel can charge it. No need to fumble with gas stoves. reply hollerith 6 hours agoparentThis guy lives in his van and he does what you describe: he has a big battery (lithium ion IIRC) that powers an induction hot plate for cooking/boiling water. (He also has a propane camping stove.) https://www.youtube.com/@forestyforest He's found that solar is not useful where he lives (Canada) so he replaced the alternator in his van with one that generates more power, and uses that to keep his big battery charged. reply wffurr 6 hours agorootparentI doubt the round trip efficiency of that beats a white gas stove, but the handling is much much simpler. It's a lot easier and less hazard prone to fuel the generator than mess around with a liquid fuel stove. reply mdeeks 21 hours agoprevCan anyone break down why it is still so expensive to get battery storage at my house? A single Tesla Powerwall is something like $10000 (before incentives) for 13 kWh capacity. I regularly see articles about battery costs being ~$150/kWh now. Thats about $2000 and yet the Powerwall is 5x that cost. Even if it was $200/kWh that doesn't get close to adding up. Is it all installation costs? extra hardware costs? Or something related to trade where we just can't get cheap batteries here in the US? reply Xt-6 20 hours agoparentYou can find much cheaper batteries. But they will require additional components that are included in the Powerwall, will be less integrated, less pretty to look at, the mobile apps not as pretty, more complex to purchase, etc Example 30kw for $8k. https://signaturesolar.com/eg4-ll-s-lithium-batteries-kit-30... or https://batteryhookup.com/products/new-24-56kwh-lifepo4-batt... reply eichin 15 hours agorootparentI vaguely recall a youtube comparison of \"Ford F-150 lightning vs 3x Tesla Powerwall\" and that the lightning was a better deal for the same amount of backup, even if you left it parked. (Different spot on the curve, you can't buy a third of an F-150... but at the time, actually getting 3 powerwalls actually delivered was a challenge too.) So there are definitely opportunities to push the consumer price down quite a bit further... reply mdeeks 19 hours agorootparentprevI'm still surprised that there isn't a remotely affordable consumer friendly solution for \"I want these batteries to take over when my power goes out\". I don't need an app or something pretty to look at. I want to insert my credit card and a couple weeks later someone has installed it in my house and I rarely have to think about it again. reply axiolite 18 hours agorootparent> there isn't a remotely affordable consumer friendly solution for \"I want these batteries to take over when my power goes out\". That's called a UPS. You can buy them in stores. Closer fit for this purpose might be solar-charging portable power stations, which are also COTS hardware that similarly costs a few hundred dollars each. Anything more elaborate than that needs to be wired into your breaker box, requiring licensed electricians and inspections. Not terribly expensive, though. Installing an automatic transfer switch in your home is pretty common in rural areas with frequent power outages, for generator installations. reply henvic 18 hours agorootparentprevThe fancy app is just something extra you get for close to no cost. Vertical integration is where Tesla is really strong at. An app or something pretty to look at is almost guaranteed once you get the rest right, IMHO. reply amluto 14 hours agorootparentprevBut the mobile apps and local administration might be far more functional than a Powerwall. “Pretty” apps are not necessarily useful apps. reply K2h 13 hours agoparentprevWorking project on 35kwh for $2k with https://www.greentecauto.com/hybrid-battery/repurposed-batte... reply mdaniel 3 hours agorootparentDid you mean to say you are working on a project or that the link is a working project? Because (a) 115.9V is some no-kidding-don't-touch-the-wrong-wire (b) that linked page cites a bunch of extra thermal management stuff which presumably its source EV managed but in any such home setup someone else is going to have to manage reply taeric 21 hours agoparentprevThis is almost certainly a standard supply/demand curve problem. Especially when you factor in that the supply side is in rather heavy flux and that the optimization side of supply has not caught up to the advancement side of it. reply cyberax 20 hours agoparentprevMark-up for luxury goods. Tesla sells megapacks, and it's much more realistic: https://en.wikipedia.org/wiki/Tesla_Megapack#Specifications , a 4MWh battery is $1.4m. reply XorNot 18 hours agorootparentThe short version is residential scale anything usually isn't cost-effective. Roof top solar panels are the one exception and it's marginal - basically coz the land is worth more then the panels by a lot. reply katzenversteher 13 hours agoparentprevWhat I don't understand is why the focus for stationary batteries is still on lithium. I don't really care about energy density (weight / volume) because I don't have to carry it around. Aren't there cheaper and safer solutions for stationary batteries? reply sanderjd 6 hours agorootparentThis is what I'm excited about. I think lithium still wins in this space due to the economies of scale driven by cars and smaller electronics. But as the market for stationary storage grows, there will be more paths to scale technologies that wouldn't be competitive for mobile use cases, but are a good fit for stationary ones. A lot of discussions of the learning curves for batteries seem to bundle all of this together, but I think there are two or three different learning curves that are at different points: The most visible one is mobile batteries, where lithium-based technologies dominate and have preferred quite far down the learning curve. But then I think there is a separate curve for stationary storage, which is at a much earlier stage, with huge room for improvement, and a number of promising technologies that exist but haven't yet made it to commercial scale. And I think there's another, separate and even more nascent curve for stationary and long duration storage, where there are initial indications that some of the initial technologies might be scalable, but it's still years away, and I think it's so early that we may well see entirely different approaches come out of the labs and win this market. reply _ph_ 12 hours agorootparentprevLFP batteries are cheaper, very safe, and currently the most popular for storage and cars which don't need maximum range - the standard range Teslas are using them too. But Sodium based batteries are coming into the market and would be even cheaper. reply yazaddaruvala 17 hours agoparentprevMy understanding is the labor, inverters, batteries (in that order) are the bottlenecks on price per kWh. reply NullPrefix 21 hours agoparentprevThere are European suppliers selling LiFePO4 cells for €100/kWh, but you need electronics to support the cells - battery management system, charger controller, inverter and so on. You are also paying for the brand name reply MrsPeaches 21 hours agoparentprevMaybe regulatory overhead due to the legacy of Li-ion cells i.e. their explosive failure mode? My understanding is that LFP is a fair bit safer so maybe regulators haven’t caught up? reply kiliantics 19 hours agoparentprevI built a storage system with these, which are almost down to $300/kWh now: https://www.currentconnected.com/product/sk48v100-48v-server... Of course you will need a charge controller and some other components which will add to the cost. reply aaronax 18 hours agorootparentWhat's the go to rack mount inverter charger to go along with that? Asking for a friend...who only needs about 800 watts of capacity to act as the certified hacker version of a UPS. I've only done Victron stuff in the past, and those are not rackmount nor is there a 48V in 120V out model of less than 3000VA capacity. reply immibis 20 hours agoparentprevWhy would you expect a Tesla product to have a competitive price? This is like saying: the CPU costs $100, the RAM costs $50, the flash costs $20, so why does an iPhone 15 cost $900? reply mdeeks 20 hours agorootparentOther competing companies in this space charge similar rates for home batteries. Also, the Tesla Model 3 and Y are currently some of the most affordable EVs on the market. It's only the X, S, and Cybertruck that are the high priced ones. reply _carbyau_ 15 hours agorootparent> Also, the Tesla Model 3 and Y are currently some of the most affordable EVs on the market. Different markets - here in Oz BYD is half to two thirds the price of \"equivalent\" Tesla. I put the word equivalent in quotes because everyone has an opinion on what a product is worth and what equivalency means. Honestly, when I eventually get to buy an EV I will avoid both brands like the plague so I'm not about to argue any points in this regard. reply zizee 6 hours agorootparentWhy avoid these brands? Serious question. reply infecto 17 hours agorootparentprevYes less ignore R&D and labor costs. reply devjab 12 hours agoparentprevIt’s a little off topic, but it’s probably a good thing that it’s not very accessible while it would be lithium. When they burn they are almost impossible to put out. Which isn’t too much of an issue with your phone or other small appliances, but things like cars (and especially buses) are a real hazard. Maybe it’s not so much of an issue in the US where cities are designed for a lot of cars, but here in the EU it’s actually quite dangerous. If a car catches fire it’ll have to be dumped into a specialised container of sorts, which works ok for cars. If a bus catches fire on the wrong street it’ll risk burning the entire street down. Just imagine what power walls would do to neighbourhoods where buildings are close to each other. You’d frankly burn an entire town-part down of every home had a power wall and just one of them caught fire. reply thelastgallon 8 hours agorootparentGovernment data show gasoline vehicles are up to 100x more prone to fires than EVs: https://electrek.co/2022/01/12/government-data-shows-gasolin... Data from the National Transportation Safety Board showed that EVs were involved in approximately 25 fires for every 100,000 sold. Comparatively, approximately 1,530 gasoline-powered vehicles and 3,475 hybrid vehicles were involved in fires for every 100,000 sold: https://www.fairfaxcounty.gov/environment-energy-coordinatio... Statistics from 2015 showed that 174,000 vehicle fires were reported, and almost all of them involved gasoline vehicles. Tesla claims that gasoline cars are 11x more likely to catch fire than a Tesla, and that the best comparison of safety is fires per billion miles driven. If we compare using this method, there are approximately five EV fires for every billion miles traveled, compared to 55 fires per billion miles traveled in gasoline cars: https://driveelectriccolorado.org/myth-buster-evs-fire/ reply epistasis 12 hours agorootparentprevIn the US, we pump natural gas directly into most homes, which is extremely dangerous. Yet NG still causes very little death compared to, say, cars, and we tolerate NG in homes and >100 car deaths a day. I can't speak to the EU risk profile, as the EU has far less fire deaths in home than the US, but we here in the US will tolerate home lithium batteries just fine. reply cagenut 18 hours agoparentprevsame reason massive video streamers pay $0.0005/GB for cdn bandwidth while you and I are paying $0.05/GB for aws egress. the demand at the grid and commercial scale is so huge right now that anyone putting cells into suburban-home sized systems and dealing with that type of customer (the absolute worst kind) are practically throwing money away when they could be doing 1 - 10 big deals with competent (electrical and financial) counterparties. in a supply constrained world there's no point in serving the bottom of the market. talking to you is an opportunity cost. it'll come soon though as production ramps. the recent inflection of EV growth below expectations has created the slack for it to be possible, but it'll still be a few years as those are all NMC lines. if your curious to see examples of what the commercial scale looks like today victron has a great youtube channel full of them: https://www.youtube.com/@VictronEnergyBV/videos reply GaggiX 21 hours agoparentprevIsn't the Tesla Powerwall a fully integrated system with multiple MPPTs, inverter, and charger? (Maybe I'm confusing the new one with the older versions) I imagine that's where the cost comes in. Where I live in Europe I can buy a lifepo4 cell for 97€ from a trusted supplier, 300Ah x 3.2V = almost a kWh. For a full battery you need to add the cost of a BMS and a balancer. reply MrsPeaches 21 hours agorootparentElectronics in these systems are a low % of the BoM cost though, so wouldn’t expect it to make such a big difference. reply Panzer04 13 hours agorootparentNot sure why you got downvoted. Its high value-add, but I would have thought competition would push prices down too. reply GaggiX 20 hours agorootparentprevIf you buy a 16 cells to make a 48V battery than yeah the vast majority of the cost would be the lithium cells, a 16s 250A BMS should be around 150€, an active 16s balancer 50€. reply antonkochubey 10 hours agorootparentYou also need an inverter, a high-quality 8-10 kW unit could approach a thousand euros easily. reply GaggiX 8 hours agorootparentI was talking about making a battery, if you want a full solar installation then you also need an inverter, charger, MPPT. reply mcbishop 17 hours agorootparentprevYes, it's fully integrated and UL listed. reply spankalee 22 hours agoprevI live in a city where's it's pretty common for people to blast music at what used to be absurd levels because of very large, yet portable, battery powered speakers. You used to need a decent budget for D batteries if you were going to do this with an 80's boom box, but now you can run a full-sized PA speaker for hours on the built-in Li-ion batteries, so it's way more common. reply marcus_holmes 17 hours agoparentOne of the Startup Weekend ideas I keep playing with is a Bluetooth jammer that can selectively stop these public nuisances. I know it's illegal, but so was Uber when it started. And yes, being able to block people's headphones and other stuff would be bad, but I'm moving towards that being acceptable collateral damage. Fed up of rude idiots playing really bad music at massive volume (and why is it that only people with appalling musical taste enjoy blasting it?). reply cma 20 hours agoparentprevRechargeable NiMH D-cell batteries have been around for decades for not too much, probably not more than the boom box for several sets 25+ years ago. reply immibis 20 hours agoparentprevI met someone who claimed to have invented the digital amplifier chips these speakers use, using some new technology I don't remember but it sounded innovative at the time - possibly a new semiconductor material. He said he's very sorry. reply jareklupinski 20 hours agorootparenthttps://en.wikipedia.org/wiki/Class-D_amplifier#History > Practical class-D amplifiers were enabled by the development of silicon-based MOSFET (metal–oxide–semiconductor field-effect transistor) technology. > The first class-D amplifier based integrated circuit was released by Tripath in 1996, and it saw widespread use. say thanks :) i started going down the analog electronics route with audio amplifier projects reply immibis 9 hours agorootparentClass D is old hat. He claimed to have made them many times smaller or cheaper for the same power output, possibly by changing the material - don't remember exactly. reply spankalee 19 hours agorootparentprevOh yeah, that's a big contribution too. These speakers can get so loud. There's a lake here and you can hear them loud and clear across the whole lake, at least a half mile away. reply bravoetch 21 hours agoparentprevImagine how loud it will be with a Dyson Sphere connected to the Disaster Area portable speaker. reply mcmoor 11 hours agoprevSo what's the tech that was making this decade of the battery possible? I remember 10 years ago reading lots of articles that battery is and will be the bottleneck for the next 20-30 years, but now in only 10 years looks like it's solved already (?). Is there a revolutionary tech that was missed by people 10 years ago? It feels like if suddenly fusion is here. reply sanderjd 6 hours agoparentI think the other commenters are mostly right but also downplaying the technology advances a bit. The basics of the battery chemistries that are making this work existed two or three decades ago, but there have also been meaningful breakthroughs in the specific components during that time, that (I think) eventually pushed the viability over a tipping point. In 2004, I think it would have been reasonable to think that EVs would always be a niche for small short range \"commuting cars\". I remember thinking this, and being excited about even that possibility! But now I frequently see these big Rivian trucks and SUVs driving around, and I know their range is similar to a tank of gas. That seems pretty crazy to me! And it wasn't just the economics of scale that has made that work. In the case of stationary batteries, I think just the economics driving the price down has indeed been the most important factor, but something else I haven't seen anybody mention yet is that it also has a lot to do with solar barreling down its cost curve, kicking off massive deployment, to the point that it is a fairly common occurrence now for some locations on some grids at some times of some days to have negative power prices due to solar over-production. Batteries are excellent at sopping up extra power that would have otherwise gone nowhere (or to crypto mining perhaps, but maybe I'm repeating myself). reply tjoff 8 hours agoparentprevEconomics of scale and an absurd amount of investments I guess? There seems to be many synergies around using energy storage in batteries for transportation. reply louwrentius 8 hours agoparentprevI don't think it's about technology here. It's about economics. Electric cars (credits to Tesla), especially the first generation basically used a ton of the same cells found in (some) laptops. The demand for electric cars, thus batteries fuelled a huge increase in manufacturing capacity, which dropped prices. And maybe I'm wrong but the cheaper batteries made drones more affordable, so that industry got a boost with investment. A lot of things aren't limited by technology, but by capital and market incentives. Which to me actually shows the limitations of this system as we could have had electric cars decades ago. We could have had this revolution decades ago. What a different world we would have had. reply jillesvangurp 12 hours agoprevThe battery market is a pretty interesting market to follow. Bloomberg New Energy Fund (NEF) published an interesting report a few months ago titled \"China Already Makes as Many Batteries as the Entire World Wants\". https://about.bnef.com/blog/china-already-makes-as-many-batt... I like their reports because it injects facts into what is otherwise a very opinion driven public debate. And the facts are quite impressive. Some key points: - they are tracking about 6 twh of new battery production related investment for 2025 world wide - that's about 4x the projected demand of 1.5 twh for 2025; which is up from slightly under 1 twh last year (2023). So, they are signalling massive over production of batteries is about to happen. That's quite a change from the shortages companies were dealing with in the last few years. - they expect that to have some obvious effects on prices and some of those investments: not all these projects will make it and battery over supply will drop prices and endanger producers that can't do that. I would add to that that it is likely that wider availability of cheaper batteries is going to create new use cases as well. So their demand projection might be off. If prices drop low enough, a lot of household equipment might start coming with batteries, for example. Things like fridges and ovens for example. That's a market that barely exists now. But if prices drop enough, why not? Another thing I find interesting is the average charge state and longevity of batteries. We're accumulating many twh of batteries on a year to year basis and they might have a life measured in decades. That's just how long it takes to get through the many thousands of charging cycles these things support even when you use them very intensively. But mostly these batteries are at pretty high charge states because they actually aren't used that intensively. You don't drain your EV's battery every day. And your powerwall that can run your house hours is only used very lightly on a day to day basis because power outages don't happen that often. So, that's a lot of potential energy that is pre-distributed and ready for use that the grid doesn't have to supply on demand. Tens to hundreds of twh pretty soon. Some virtual power plants are starting to rival nuclear power plants in the amount of power they can make available. And that's while these virtual power plants are still operating at relatively small scales. They can only do so in short bursts of course but it starting to affect how grid investments are being allocated. reply sanderjd 6 hours agoparent> I would add to that that it is likely that wider availability of cheaper batteries is going to create new use cases as well. So their demand projection might be off. If prices drop low enough, a lot of household equipment might start coming with batteries, for example. The cure for high prices is high prices! Also I love your thought on how much energy we're distributing around in all these mostly-charged batteries. It would have been unthinkable not all that long ago to have this much energy stored at \"the edges\" in an extremely accessible form. reply hacker_88 12 hours agoprevA good battery breakthrough also offsets power needs to have 1-2 nm chips . reply MisterBastahrd 21 hours agoprevAs someone who has been blissfully oblivious to electricity prices due to a 36 month contract at $0.10/kWh, I looked up the recent rates and saw that they're now in the $0.15/kWh range. Makes me a bit more receptive to biting the bullet and just going with solar with battery if this trend is going to continue in that fashion. reply stavros 20 hours agoparentAs someone in a corrupt country with tons of sun, I can't wait to be independent of the price-gouging grid. reply prawn 15 hours agorootparentWhere I am, the local council/county co-organises group-purchasing deals of solar and battery systems. There are constantly incentives offered for solar or more energy- or water-efficient options at home. reply Moldoteck 8 hours agorootparentprevunless the state will make a law so that you'll pay tax for energy you produce(and i mean produce, not export). This stuff (almost) passed in Romania reply njovin 19 hours agorootparentprevNot sure where you're at but in California the gov't incentivized everyone to install rooftop solar and now that Sempra isn't increasing revenue at the same rate they did before, they're aggressively lobbying to increase rates on rooftop solar customers to make up for it. I'm very eager to get off the grid completely and be rid of this profit-chasing nonsense. I know this is going to get me a bunch of responses about how much more pressure is on the grid because of EVs, solar doesn't help during off-peak hours, etc. Boo-hoo cry me a river. These energy companies have gotten billions in gov't grants and credits and they're still massively profitable and continue gouging customers. reply stavros 19 hours agorootparentI'm in Greece, which is basically the same, except the money lines the politicians' pockets as well as the energy companies'. reply _carbyau_ 13 hours agorootparentprevI'm not quite as frustrated as you yet. But I am disappointed. Economies of scale should dictate better community coverage and service - not worse than individuals can do themselves. reply kragen 13 hours agoparentprevare those us dollars or a different currency? i hear prices in california are much higher than that in us dollars reply vel0city 5 hours agorootparentThat price could easily be dollars. Most places in the US have far cheaper electric rates than California. Three years ago I had a pretty similar rate in Texas. I'm in a similar boat, my 11¢/kWh plan just finished and new rates were ~14¢+. reply MisterBastahrd 2 hours agorootparentprevUSD. I live in Texas and locked in a 3 year rate. It comes with a larger cancellation fee ($250ish), but if I'm doing 1500-2000kWh a month most months, I'm easily saving money even if I cancel before the year is even up because the rate difference is usually 1.5-2c. reply kragen 42 minutes agorootparentthanks! reply louwrentius 8 hours agoprevYou can buy 15kWh of battery storage for <2000 Euros if you are willing to assemble the battery yourself. This is roughly the capacity of a single rack-mount battery. Just the cells are <1400 Euros, this is crazy. Because the battery is now far from the most expensive item for a solar-battery setup. The battery revolution cannot be overstated, especially now with Sodium batteries entering the market, which don't use lithium. reply GaggiX 22 hours agoprev [–] I'm so glad LFP batteries exist, at least at home you don't have to worry about a massive fire like you do with other lithium ion batteries. reply Animats 20 hours agoparent [–] Within a decade battery technologies capable of thermal runaway should disappear. reply mcbishop 17 hours agorootparent [–] But there's still the possibility of combustible off gassing with LFP. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post argues that batteries, not AI, will be the defining technology of the 2020s, revolutionizing various sectors including warfare, transportation, and energy storage.",
      "Significant advancements in battery technology have led to reduced costs and increased energy density, making them crucial for electric vehicles, drones, and renewable energy storage.",
      "The integration of AI with battery-powered devices, such as autonomous drones and robots, is expected to transform industries and daily life, presenting lucrative opportunities for venture capitalists and tech companies."
    ],
    "commentSummary": [
      "Sodium-based batteries are now available on Alibaba and AliExpress, priced around $9 for a single cell or $100 for a 20-pack, and they require new Battery Management Systems (BMSs).",
      "Sodium-ion batteries, which use sodium ions as charge carriers, are cheaper, safer, and perform better in extreme temperatures compared to lithium-ion batteries, but they are heavier and larger.",
      "The text discusses the potential of sodium batteries to become a competitive alternative to lithium-ion batteries, especially for stationary and long-duration storage, due to their cost-effectiveness and safety."
    ],
    "points": 123,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1718133564
  }
]
