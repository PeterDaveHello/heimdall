[
  {
    "id": 40742163,
    "title": "Gilead shot prevents all HIV cases in trial",
    "originLink": "https://www.bloomberg.com/news/articles/2024-06-20/gilead-shot-prevents-100-of-hiv-cases-in-trial-of-african-women",
    "originBody": "Bloomberg Need help? Contact us We've detected unusual activity from your computer network To continue, please click the box below to let us know you're not a robot. Why did this happen? Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy. Need Help? For inquiries related to this message please contact our support team and provide the reference ID below. Block reference ID: b5c8fa20-3000-11ef-8cbd-64a282d5566d",
    "commentLink": "https://news.ycombinator.com/item?id=40742163",
    "commentBody": "Gilead shot prevents all HIV cases in trial (bloomberg.com)798 points by toomuchtodo 23 hours agohidepastfavorite362 comments kbos87 19 hours agoWe’ve been on the road to this for a while, and it’s so exciting to see. I switched to injectable prep a few months ago to avoid having to take a daily pill - it’s an injection once every two months, and my doctor told me that the guidance might soon change to once every three months. It sounds simple but for a lot of people it’s not a trivial thing to take a pill every day or adhere to a strict appointment schedule to get an injection. I’d imagine that adoption & compliance goes way up if it’s that much easier. reply nerdjon 19 hours agoparentI have been debating on switching to the shot but Havnt yet. Just to avoid the accidentally missing a dose or the chance interactions (lower efficacy) with dairy and many fiber supplements, this would be amazing. Would be nice to never need to worry about that or think, hey I missed a dose is it actually true that it doesn’t matter or do I need to wait a full week. Half a week? My only concern here though, if it’s in your system for 6 months what if you have an adverse reaction or just some unpleasant symptoms. reply iknowstuff 17 hours agorootparentThe drug is cabotegravir and if you want to feel safer you can take the pill form of it first to test it out. reply throwaway7ahgb 6 hours agorootparentAny chance these will ever be self injectable (with Rx)? reply schlipity 18 hours agorootparentprevI'm going to assume a bit and for that I'll apologize in advance. I'm assuming you're in a high risk group for HIV. Why not just use condoms? Is it because they aren't 100% effective? reply bravura 4 hours agorootparentBesides the other nice reasons people offered, because \"stealthing\" (when someone removes a condom during sex without the other person's consent or lies about having put one) is remarkably common, and not as easily noticed as you might expect. reply JanetToolsmith 17 hours agorootparentprev> Why not just use condoms? Why not just save money and not install those ECP brakes on your vinyl chloride freight train? Why not just use one unshielded Raspberry Pi on that spacecraft instead of a 3-of-5 quorum from the output of 5 identical engineered-for-space systems? Using/requiring condoms AND being on PrEP helps me live my best life with less fear-based inhibition and cognitive load. reply Flumtioniro 3 hours agorootparentThe same reason why not everyone drives the safest car: cost and other negatives. In case of a pill perhaps negative side effects. I have no clue how often a person in high HIV risk has sex or behaves. It should be fine to ask that question reply halfmatthalfcat 18 hours agorootparentprevIsn't this like asking by do we have multiple methods of birth control? Pill, IUD, etc? reply schlipity 18 hours agorootparentThis is a great way of looking at it, thank you for that. reply david38 14 hours agorootparentprevnext [3 more] [flagged] mvid 10 hours agorootparentWhat if I engage in correlational sex? Also this is wrong. Most people I know use multiple versions of birth control. Access to the morning after pill and abortion is a form of birth control. Being on the pill and also using condoms is common. reply vidarh 9 hours agorootparentprevWhile that might be true for \"most\" people, plenty of people do still use multiple forms at the same time - including people who do not engage in a lot of casual sex. But lots of people also do have lots of casual sex, and that is a choice they have every right to make. reply whimsicalism 3 hours agorootparentprevthis might be news to HN, but condomless sex feels significantly better moreover, condoms can have numerous issues like tearing or stealthing reply iknowstuff 17 hours agorootparentprevCondoms are less effective at HIV prevention, but also they’re unpleasant and don’t prevent most STIs because of oral sex. Nobody’s giving or getting head with a condom. People who stay on top of preventative measures tend to not bother with condoms since PrEP became prevalent. Preventative: 1. Gardasil 9 (vaccine against 9 strains of HPV, prevents genital warts and cancers caused by HPV) 2. Monkeypox vaccine 3. Meningitis ACYW vax 4. Meningitis B vax (35% effective against gonorrhea) 5. doxyPEP (two pills of doxycycline taken after sex, 90% effective against syphilis, 80% chlamydia, 50% gonorrhea) 6. PrEP (prevents HIV infections) 7. and the usual suite of vaccines against the rest like hepatitis A/B, mumps etc Treatment of the bacterial ones (which transmit through oral too): 1. syphilis - butt shot of penicillin 2. chlamydia - 1 pill of an antibiotic 3. gonorrhea - a week of doxycycline pills or one butt shot of ceftriaxone Remaining: HSV. Half of the population has it, so no big deal. Condoms dont prevent it either. As for hepatitis: even though it requires blood contact and as such is not necessarily considered an STI, hepatitis c is curable these days thanks to DAAs taken over the course of 8-12 weeks. a/b have vaccines. reply cycomanic 15 hours agorootparent> 5. doxyPEP (two pills of doxycycline taken after sex, 90% effective against syphilis, 80% chlamydia, 50% gonorrhea) Preventatively using antibiotics is a horrible idea, for one it increases the risk of creating resisitant strains, and we are already running out of antibiotics (especially broadband ones). Moreover, antibiotics in general are known to mess with you gut biome whose importance we are just beginning to understand (we know it plays a role in many physical and mental illnesses for example). Then there are the side effects which for doxycyline include diarrhea, increased risk of bowel cancer, higher sensitivity to the sun (and associated risk of skin cancer). reply alwa 2 hours agorootparentThis all feels really general—precipitated on the idea that there’s one Right Way for everyone to use these drugs, about which the doctors and public health professionals recommending doxyPEP are somehow uninformed. Suppose you’re immunocompromised. Wouldn’t it be worth the diarrhea to avoid infections your body couldn’t fight off? Suppose you’re in a situation where you’re having lots of opportunities for infection. Whether you can or can’t control that situation, the end result is the same: you know your body is going to be challenged by infectious disease frequently in a way that most people’s bodies aren’t. Isn’t it plausible that the infectious agent may have less opportunity to evolve if you didn’t contract it as frequently? Different forms of therapy might be both individually and collectively optimal for people in different situations. What’s right for a person who isn’t at risk isn’t the same as what’s right for a person who is at risk. reply bobthepanda 2 hours agorootparentprevDoxy is already prescribed in the US as a treatment regimen for acne, so if that’s going to be allowed STI prevention seems way more useful. The overuse of antibiotics is mostly in farm animals, which represent 73% of global antibiotics use, and 90% of American antibiotics use. reply verulito 58 minutes agorootparentprevantibiotics are toxic in general, likely affecting way more than bowels. many are ototoxic for example. reply loopdoend 14 hours agorootparentprevBeen taking it every day for 15 years with no problems and actually I seem to look 10 years younger than everyone in my age group. reply alwa 2 hours agorootparentThe people responding to this seem to assume you’re doing so for funsies, which might be true. But that’s not what you said. So for their benefit, it’s worth pointing out that daily doxycycline is a pretty normal therapy for a range of chronic stuff—malaria prophylaxis, rosacea and skin that’s prone to infections, I’m sure there’s more. If loopdoend’s doing this, it feels like a good bet that there’s a doctor in the mix judging that it’s medically appropriate for them. reply vasco 13 hours agorootparentprevThe problems are for humanity not for you, with spurious use of antibiotics. Resistant strains can kill everyone. reply snowwrestler 6 hours agorootparentprevThis is really surprising to me since there is a lot of evidence that oral antibiotics can mess up your gut flora, creating all sorts of bad side effects. Do you have any digestive issues? reply simianparrot 13 hours agorootparentprevPlease tell me this is a joke / satire reply rswskg 6 hours agorootparentNo, there are many many people who are this entitled. reply gumby 16 hours agorootparentprev> Nobody’s giving or getting head with a condom. In the early days of AIDS (wasn't called HIV back then) the recommendation was to use a condom or dental dam (depending on the hardware of the recipient). That's how I learned what a dental dam was. Later it was suggested that plastic food wrap would work (the jokes just write themselves). Sadly the ubiquity of paper toilet sheet covers in US bathrooms dates back to the 80s due to straight paranoia over AIDs + widespread and overt anti-gay prejudice, so every time I see one of those dispensers I grit my teeth. Reminds me of the early days of COVID (not the anti-gay part, but the weird practices when nobody really had yet a good theory of what's going on). reply everforward 3 hours agorootparentHIV (human immunodeficiency virus) is the virus, AIDS (Acquired Immune Deficiency Syndrome) is the syndrome caused by untreated HIV. Everybody with AIDS has HIV, but not everyone with HIV has AIDS. The reason for the reversal in terms is treatment options. When HIV was first found, there were no treatments so AIDS was inevitable. Nowadays, medication can permanently prevent HIV from progressing to AIDS, so AIDS is much less common than HIV. reply close04 1 hour agorootparentprev> In the early days of AIDS (wasn't called HIV back then) You might be thinking of “GRID” (gay related immuno deficiency), the original name of AIDS when it was believed it only affects gay people. Once the virus causing the illness was identified it was called HIV. reply droptablemain 49 minutes agorootparentprevPardon my puritanism, but this seems like an awful lot to put your body through to get your rocks off and engage in this sort of indulgence. reply giantg2 15 hours agorootparentprevPreP actually reduces the chances of acquiring other viral STDs to varying degrees. reply alwa 2 hours agorootparentAlthough we have to admit the moral hazard element too: by altering the risk calculus, it does seem to have helped weaken cultural norms around condom use and risky sexual behavior. Business is booming for bacterial infections. Personally I’d say that’s a pretty good tradeoff: fear, stigma, and death for a different problem that’s more an annoyance than a mortal threat right now. reply giantg2 1 hour agorootparentI think another overlooked portion is that we shouldn't be using meds (for infectious diseases) to enable fun (which seems common with sex). Eventually those meds will lose effectiveness, that's just evolution. Then the people who really do need them will be out of luck. reply pknomad 16 hours agorootparentprevInefficacy against HSV is interesting. I had no idea. Why is it ineffective? reply projektfu 4 hours agorootparentI looked through the literature and it seems that condoms are quite effective at preventing HSV infection when used, especially from male to female (on the order of 99%), and less so for female to male (around 60-70%). I would suspect the reason for failure is behavioral. People probably don't apply the condom until after most of foreplay is over and HSV-2 can be transmitted during \"outercourse\". reply creer 10 hours agorootparentprevHSV-1 and -2 transmit through contact from skin location to skin. Including through asymptomatic shedding. Only sometimes is that location covered by a condom - in location and in timing. See also: Wrestlers' \"Herpes gladiatorum\" which is just HSV-1. Which demonstrates how HSV-1 survives for transmission far more easily or broadly than just genital contact. Is there \"HSV-2 gladiatorum\"? Apparently it's usually HSV-1 but probably yes - neither HSV-1 or HSV-2 are really location specific. reply gumby 16 hours agorootparentprevOral sex is a big reason. Michael Douglas got tongue cancer* from HPV. Actually it appears the 9-polyvalent HPV vaccine protects against multiple cancers but insurance won't pay for it once your outside the age window. * originally he said \"throat cancer\" at the advice of his agent! [edited: wityl pointed out I’d typed HSV for some reason] reply wityl 15 hours agorootparentYou are thinking of HPV, HSV is herpes. There is no vaccine, it doesn't cause cancer (that we know of, at least not like HPV), and spreads via skin contact on the genital area. Since fluids are not required for HPV or HSV and it's present on the area that a condom won't cover, condoms aren't effective for either. reply gumby 15 hours agorootparentYes it was a brain fart — fixed! reply anjel 16 hours agorootparentprev...And your odds of making it through that pharma-minefield with zero side effects are not encouraging. Worse still--large scale casual sex is a great way to introduce new, novel and un-contemplated STDs into the population. STDs are opportunistic that way, just ask Mr. triple-resistant Gonorrhea. It's decidedly not the way I want it to be, but that's just how it works. For casual sex to be safe I think its more like \"Hi, please spit into this tube so we can get busy\" and red means HIV, Green means Monkeypox, comprehensively. reply autoexec 15 hours agorootparentIt's not just about casual sex though is it. There are many people in long term monogamous relationships with a partner who has an STD, but who don't want to catch it themselves. Vaccines and medications can allow them to have active sex lives while staying healthy and in some cases even help women go through pregnancy and childbirth while preventing the spread of an STD to the child. For these people minor side effects (and at times even more severe ones) can be entirely worthwhile. reply tcmart14 4 hours agorootparentOr perhaps maybe a future for occupational health. My wife is a nurse. Needle sticks happen. When it does, she has to get tested, full works. The hospital pays for that and the treatment. Maybe with a shot that lasts a year rather than the current 2-3 months(?), hospital staff can just opt that route then the test and treatment. Apparently the treatment is rather gnarly. reply granfaffalo 17 hours agorootparentprevI'm not going to make such an assumption, but two reasons people prefer PrEP generally: 1. in sex work, \"not using a condom\" is an \"extra\" that clients will pay quite a bit for 2. people who \"party and play\" tend to be in an altered state of consciousness that biases against use of condoms reply odyssey7 4 hours agorootparentIt’s not that complicated. A lot of people don’t like condoms, either wearing them or having a partner wear them. PrEP, to many, has rendered condoms unnecessary. reply richie-guix 1 hour agorootparentnext [2 more] [flagged] odyssey7 1 hour agorootparentA lot of people out there have managed to get past stigmatizing STIs, viewing bacterial ones as no more remarkable than sinusitis so long as they get tested and treated. Between PrEP and vaccines for hepatitis and HPV, risks for the scariest STIs can be annulled, outside of exceptional, treatment-resistant cases. reply isoprophlex 9 hours agorootparentprev> people who \"party and play\" tend to be in an altered state of consciousness that biases against use of condoms Maybe... they shouldn't, then? You also don't go shooting your guns or driving your motorcycle when drunk, even though it might be more fun? reply alwa 3 hours agorootparentYou don’t, but people do. And even if they “shouldn’t,” the cost of contracting HIV—which very few people set out intending to do—adds up to an eyewatering sum over a lifetime, no matter who’s paying for it. Not to mention rendering someone, often young, dependent on heavy drugs to stay alive. Forever. Assuming the drugs remain effective. What other risky lifestyle decisions do we advocate to punish that severely? Especially when the technology exists to ameliorate the risk? Wouldn’t it be cool if simulated guns were as fun as real ones? Then your hypothetical person could get drunk and shoot guns all they want, and there’s no problem anymore! So what’s the virtue in leaving the real loaded gun lying around to test their willpower? reply throwaway7ahgb 6 hours agorootparentprevWhy not just not have sex unless trying to procreate? It's amazing in 2024 this line of discussion is happening. We all know exactly at a individual level what to do and not. We can keep asking \"Why not just use a condom?\" and see how far that gets us, or actually understand psychology and use patterns and work with what people actually do (good or bad). Same can be said about (illicit) drugs, why not just not do drugs? Simple! reply seunosewa 1 hour agorootparentIt is a well known fact that people (except incels & asexuals) will have sex whether you like it or not. It's part of the human experience. Policy must deal with the real world. reply bondarchuk 7 hours agorootparentprev\"why do you use prep when you can use condoms\" > \"in a state where they prefer to not use condoms\" > \"maybe they shouldn't do it then\" or... they can use prep. reply mock-possum 2 hours agorootparentprevWhen all it takes is a daily pill, or an injection, to reduce the risks of drunk-driving a motorcycle to practically nil - wouldn’t you want to do it? You don’t even need to wear a helmet, you can just do whatever on your motorcycle and not worry about it. You never have to think to yourself “hoo boy I’m at two beers, and I don’t know, should I have the third? Will that be too many? Am I going to need to get on my motorcycle in an hour? Or will it be more like three hours? What will my blood-alcohol level be at that point? Is this legal? Is this safe?” You can just have fun getting fucked up and driving your motorcycle. Modern medicine has given us this incredible thing, and you’re really going to be like “Naw I don’t know about all that.” reply Ntrails 47 minutes agorootparent> Modern medicine has given us this incredible thing, and you’re really going to be like “Naw I don’t know about all that.” I think that the only actual complaint I've ever heard against Prep is the costs being socialised and covered solely not by the user (ie insurance or public healthcare or whatever). It is (was?) not cheap! reply justinclift 17 hours agorootparentprev> in sex work, \"not using a condom\" is an \"extra\" that clients will pay quite a bit for To me, that is such a weird thing. People literally paying money for something that can damage them for life and may even kill them. Though the same can be said of addictive things (smoking comes to mind), is this some form of that maybe? reply pasquinelli 5 hours agorootparent> To me, that is such a weird thing. i'm confused. are we acting like we don't full-well know that using a condom sucks for everyone involved? it's like wondering why people didn't like using a face mask during covid: because it sucks. that, of course, is independent from whether it's prudent, but if something sucks, it's no surprise when people avoid it, right? reply sdwr 3 hours agorootparentThere's that middle ground! reply vidarh 9 hours agorootparentprevPeople have a risk level they're comfortable with, and sometimes people feel added excitement pushing right up to or past that level, and sometimes they're just happy to pay extra for something they feel is within that risk level if they enjoy it more. We all choose to take risks all the time and often pay for the privilege, sometimes explicitly seeking out the thrill of a risk. I'm guessing what makes this specific scenario weird for you is more likely that this risk doesn't seem worthwhile to you relative to what you get from it (and I agree with that - I've never had an appetite for taking risks with STDs) reply shiroiushi 17 hours agorootparentprev>People literally paying money for something that can damage them for life and may even kill them. You could say the same about sky-diving. reply justinclift 13 hours agorootparentYep, sure could. :) reply sureIy 1 hour agorootparentprevBeing horny also counts as being “in an altered state of consciousness that biases against use of condoms.” As someone who took the condom off in a risky situation recently, there’s no comparison between the on and off feeling. It’s like licking a piece of candy vs crushing it with your teeth and sucking it all at once. reply wvh 2 hours agorootparentprevMaybe you want the real thing. I get that, even though I have never had sex with a prostitute or fall into the risky sex category. I'm not going to go as far as to be childishly disrespectful and say I'd rather die doing the real thing, but I do understand the apprehension of using artificial barriers during the most intimate act know to humankind. reply mock-possum 1 hour agorootparentprevYes but it feels good reply mattmaroon 16 hours agorootparentprevRisk always entails a premium. reply mypalmike 15 hours agorootparentNot generally, no. Driving that car with the sketchy suspension is cheaper than getting it fixed. reply vidarh 9 hours agorootparentThis is obtuse. The intent of expressing that if you are paying someone else for a service, it predisposes them to demand a higher price if you're asking them to take additional risks if they have any negotiating power at all was pretty obvious. reply mypalmike 8 hours agorootparentThe discussion was about the additional risk to the customer, not the vendor. reply vidarh 3 hours agorootparentThe comment above the one you replied to was. The comment you replied to was not, and it'd be reasonable to suggest it didn't really address what it replied to. But its intent was clear. reply amyjess 2 hours agorootparentprevA better analogy would be to buy a '60s sports car that predates nearly all modern safety tech at auction for $$$$$$ instead of just buying a 2024 Corolla with modern safety features. reply danielxt 10 hours agorootparentprevit does, because the chances of you needing expensive medical care after using the sketchy vehicle go up reply thfuran 6 hours agorootparentIf you drive faster still in a car that also lacks crumple zones and airbags, your expected medical bill probably decreases. reply throwaway7ahgb 6 hours agorootparentprevWhy not just not have sex unless trying to procreate? It's amazing in 2024 this line of discussion is happening. We all know exactly at a individual level what to do and not. We can keep asking \"Why not just use a condom?\" and see how far that gets us, or actually understand psychology and use patterns and work with what people actually do (good or bad). Same can be said about (illicit) drugs, why not just not do drugs? Simple! reply AlexandrB 6 hours agorootparent> Same can be said about (illicit) drugs, why not just not do drugs? Simple! It is actually simple. Just not easy. Same with losing weight - consume fewer calories than you burn. reply rswskg 6 hours agorootparentprevI guess you aren't assuming to be around when the osteoporosis kicks in? reply throwaway7ahgb 6 hours agorootparentWhat are you implying, if I have sex without a condom I'll die sooner? reply snapplebobapple 17 hours agorootparentprevHave you had sex wearing a condom vs not? Its gotten better in the last twenty years but it is still noticeably worse reply fullspectrumdev 2 hours agorootparentprevThey can break, so multiple protective elements is better. reply heavyset_go 17 hours agorootparentprevPeople lie and condoms break. reply hn_throwaway_99 5 hours agorootparentprevWho prefers to chew gum with the wrapper on? reply 39896880 14 hours agorootparentprevIt’s because they’re no fun. reply oopsallmagic 14 hours agorootparentprevCondoms aren't perfect, and PrEP is cheap and has basically zero side effects. Plus, rapists don't use condoms, no matter how nicely you ask. Edit: lmao touched a nerve? I will never understand why folks here get so prudish when we talk about sex... \"Just wear a condom!\" is remarkably insensitive, as though 100% of HIV transmission is your own fault, and not, say, someone else who has bad intentions. reply brunoTbear 15 hours agorootparentprevSome of us prefer to enjoy sex as a beautiful act of human connection and condoms detract from being in the moment of joy. This is like saying to someone killed in a motorcycle accident why didn’t you use a car. It’s a reductive, unempathetic and frankly unproductive take. Please think before asking this again. reply 39896880 14 hours agoparentprevDo you mind sharing how you got insurance approval? I looked into it but it was needlessly complicated. I couldn’t even get a cash price quoted. reply OptionOfT 19 hours agoparentprevCouldn't you get an injection like Ozempic/Wegovy? Just have it in the fridge? Does it have to be administered in a physician's office? reply kbos87 19 hours agorootparentI think the GLP-1s are subcutaneous injections - tiny, tiny needles you can stick in a variety of locations - Apertude is intramuscular, it’s a much larger needle, I’m guessing there’s some level of skill involved. reply httpsterio 19 hours agorootparentI get an intra muscular injection of testosterone every two weeks and it's true that it's not as easy as a sub q injection would be, but it also definitely doesn't require a nurse. there's a few good injection sites (upper arm, buttock, thigh or even pecs) and after I was shown once how it's done, I can do it myself. it's also essentially painless. reply swat535 16 hours agorootparentOff topic, feel free not to answer if it's too personal but I'm curious how is every two week injections working for you for Testosterone? Are you on TRT for Hypogonadism or is it something else? I haven't heard too many people be on that kind of protocol.. I'm Hypogonad and I'm on self administration of sub q injections twice a week and it's quite annoying. My urologist told me it's best to split it this way since more frequent injections avoids a \"crash\". I would like to have less frequent injections if I could! reply phil21 7 hours agorootparentIt depends on the exact type of Testosterone being used. Each type has a different half-life and absorption profile. The most common - Testosterone Cypionate should be best used twice a week to avoid the \"crashes\" - if you care about keeping your levels relatively stable. Other forms of Testosterone have different half-lives and should be taken at differing schedules. I never saw the point in subq shots for TRT. They are less effective, and are recommended solely for patient comfort and compliance. If you can manage a twice-weekly deep IM injection, I personally have had great results with that once I learned the locations. Nearly as painless as subq and better more consistent results. Overall it doesn't matter a whole ton unless you are doing it for specific performance enhancing reasons. Being off \"optimal schedule\" a few days isn't going to make a meaningful difference for most. reply throwaway7ahgb 6 hours agorootparentI take subq TRT twice a week (cypionate). I may lose some efficiency in fat vs muscle but it is easy, painless and my levels are perfectly fine. I don't see why I change and my doctor essentially said the same. reply thesaintlives 12 hours agorootparentprevSub q is complete nonsense. Once every 10 days or so is fine. You simply feel like you need another shot and take it. No advantage and not necessary. reply AuryGlenz 17 hours agorootparentprevUnless, of course, you hit a nerve. Every damned time I try my thigh.. For what it’s worth you might want to try a weekly dose if you haven’t. Most do better on it. reply mullen 19 hours agorootparentprevA lot of these medicines that require patients to inject themselves come in a self administrating auto-injector. There are really simple to use, I mean literally anyone can inject themselves, then throw the one-use auto-injector out. reply kelseyfrog 16 hours agorootparentThere's also needle guides[1] which provide a lot of flexibility for those who self-administer. 1. https://unionmedico.com/45-reusable-s1/ reply david38 14 hours agoparentprevLike birth control. The pill has a higher “failure” rate than many people think. It’s of course “user error” reply spacebanana7 4 hours agorootparentAs evidence, despite theoretically being over 99% effective most birth control pills are around 91% effective in ordinary use. https://www.nhs.uk/contraception/choosing-contraception/how-... reply ffrghdbsbd 19 hours agoparentprevAgreed, it’s another treatment in a continuum of adherence— pre relations testing, barrier precautions, daily pills, semiannual injections. All running up to the eventual asymptote of cure and eradication. reply moneywoes 17 hours agoparentprevhow much does it cost? reply devonsolomon 12 hours agoprevI briefly did some work with an HIV research organization in South Africa. One dynamic I was shocked to hear was the prevalence of marriages where one partner (always the husband in this context) is HIV positive and the other is secretly on prep. In this context infidelity may be a norm and traditional cultural and gender norms may look negatively or skeptically at anti-retrovirals. While this isn’t the majority of the HIV experience in South Africa, it’s certainly a sizable group. Further, adherence more generally is a massive problem. The cost of travel to a clinic in both rural and urban settings can be prohibitive for many and cause major adherence drop-off. Drugs like this, if made affordable, will go a long way to immediately easing pressure on these groups - and that’s exciting. reply jncfhnb 5 hours agoparentWhy would someone secretly be on prep reply opprobium 4 hours agorootparentBecause they can get HIV from their husband, but also cannot discuss HIV, refuse sex, or take prevention steps for themselves without reprisal. reply esond 4 hours agorootparentprevBecause their husband secretly has HIV? Secret outside of the relationship, that is. reply jncfhnb 4 hours agorootparentYou’re implying that the prep and HIV are known internally to the marriage but secret externally? That seems unlikely. Taking medication in private does not seem to qualify as a “secret” to me reply ttul 4 hours agorootparentprevSo that their partner doesn’t think they are cheating. reply jncfhnb 4 hours agorootparentWhy would it imply they are cheating and not simply protecting themselves from their partner’s HIV? reply ttul 4 hours agorootparentWe are used to a western perspective on marriage, where the playing field is generally level between partners. In Africa, there is often a huge power imbalance between men and women in marriage. A woman particularly might want to keep her prescription status a secret because it suggests that she might be cheating on her husband. Even though it may seem obvious to us in the west that taking medication to avoid contracting HIV from our spouse is an obvious and reasonable thing to do, in Africa, it would be more culturally acceptable for the wife to remain vulnerable to contracting HIV from her husband because that would also infer that she is not sneaking around having sex with other men. Source: Was born in South Africa. reply jncfhnb 4 hours agorootparentWhy would the woman getting HIV imply she cannot be cheating on her husband? reply opprobium 4 hours agorootparentFrom the perspective of the husband, the woman taking prep implies she is cheating. He either doesn't get tested or doesn't care, it's more important that she not do the thing that implies she could be cheating than that she not get HIV, and that she be sexually available to him. You're digging for a logical explanation for a fundamentally illogical cultural problem and you aren't going to get it. reply cubefox 3 hours agorootparent> From the perspective of the husband, the woman taking prep implies she is cheating. But that only makes sense if the husband thinks his wife doesn't believe he has HIV. reply vlovich123 1 hour agorootparent>> You're digging for a logical explanation for a fundamentally illogical cultural problem and you aren't going to get it. > But that only makes sense if Reread and if you’re still not getting it, keep rereading your parent’s line that I highlighted for you. reply jncfhnb 1 hour agorootparentQuoting this line is not a blanket pass to say whatever you want about a culture credibly. reply vlovich123 43 minutes agorootparentWell, you have an anecdote from someone claiming to be from SA saying that’s culturally the perception. Here’s a summary of research [1] on the topic concluding similar reasons (among others): > Several participants felt that they could stop taking PrEP when the need, as they saw it, had passed. Often this was to do with the nature of their current relationship, for example with a person regarded as unfaithful: “If I find someone that I will be in a relationship with and if he is not faithful, or I have started being unfaithful, then I will come back and get them.” And > On the basis of these findings, the authors suggest that take-up and continued use of PrEP is likely to remain subject to established social norms. These norms often relate to gender and they determine, for example, who decides what HIV prevention methods to use, and the extent to which a woman in a relationship might – or might not – be able to make and implement such choices. Just because something seems logical to you, doesn’t mean that social norms and pressures don’t superseded it. In fact, we even see it in our own cultural with people believing vaccines cause autism, the whole belief that ivermectin cures COVID-19, or flat earthers. What’s really impressive though is you having such a problem with this idea despite overwhelming objective evidence to the contrary being available online and people telling you their lived experience on this very website and you significantly discount the very real possibility that people can be illogical in their strongly held beliefs even if it seems nonsensical to you. If you know nothing about a subject, you’re likely to believe what all your peers tell you which is how misinformation gets a foothold. This misinformation can even come from nowhere. The point is that if enough people believe it, they can get others to believe it to. That’s literally how human belief systems work where beliefs spring out of nothing. [1] https://www.aidsmap.com/news/sep-2020/why-do-people-southern... reply jncfhnb 1 hour agorootparentprevNo, that doesn’t make sense. And writing it off as an illogical cultural problem is just lazy. I’ll believe these guys are assholes. I don’t believe they’re just consistently idiots. If the men refused to acknowledge that they had HIV and refused to allow their wives to get prep, then the logical consequence is that the wife gets HIV and the men are forced to confront the fact that either the husbands themselves have HIV or the wife is cheating and got it from someone else. It seems much more likely that the husbands refuse to allow their wives to get prep out of spite. The implication of infidelity angle does not feel plausible. Otherwise the men are setting themselves up for a lose:lose scenario regardless of what the wife does. reply opprobium 1 hour agorootparent\"the men are forced to confront the fact...\" No they aren't, they simply never confront it. People go to their graves denying that they have HIV, denying that they ever tested positive, denying that a positive HIV test has anything to do with illness. \"Otherwise the men are setting themselves up for a lose:lose scenario regardless of what the wife does.\" - Of course. I mean, the most rational win:win thing to do is to get an HIV test and get treated if positive. They then both don't get sick and die and can't pass along HIV. Many people don't do that either. What's the mindset that explains this behavior? You can't work backwards from the most rational thing to do to what people actually do. reply jncfhnb 1 hour agorootparentYou are the one injecting this narrative about women cheating though. It seems to me you’re just making this up. It doesn’t make sense. Establishing the narrative that if my wife gets HIV that she must be cheating on me is a losing proposition for the man that only increases the probability that his wife will appear to be cheating. There is no motivation for it. Simply being a dick and saying women can’t use prep because I don’t want them to is a much simpler narrative. Your reasoning here is similar to arguing they’re a stupid people ergo they don’t use prep because of aliens. It’s not compelling even if you’re willing to believe they may engage in irrational behavior. reply opprobium 40 minutes agorootparentYou asked \"why would someone hide prep\", you got one example. It is by no means an exhaustive list. For example, a big fear is being perceived as having HIV (since prep drugs are also part of HIV treatment). If you do actually have interest in this topic you could read about it: https://www.aidsmap.com/news/feb-2020/your-partner-not-angel... \"Men were able to initiate PrEP without discussing it with their partners, whereas some women said they needed to get permission. Discussions around starting PrEP could raise questions about trust and infidelity and act as a barrier to PrEP use.\" reply wityl 4 hours agorootparentprevBecause the partner doesn't test for HIV and/or is in denial that they have it or that it will affect them. These are cultural contexts where the woman has no right to question any of this. reply jncfhnb 4 hours agorootparentWell that’s a stupid answer but it seems consistent and feasible I guess. reply astockwell 4 hours agorootparentprevnext [2 more] [flagged] jncfhnb 4 hours agorootparentCould you fucking not? I’m asking a question because I don’t understand it. You don’t need to try to shame for not knowing something and trying to learn. reply cassepipe 4 hours agoparentprevI don't understand your point, can you elaborate on the dynamics at play ? reply whimsicalism 3 hours agorootparentthere are a lot of conspiracy theories around hiv treatment in Africa, particularly SA reply b800h 4 hours agoparentprevDo any of these drugs cross the placenta? reply MrLeap 22 hours agoprevA childhood friend of mine died of AIDs related complications a few years ago. I hope this saves a lot of people. reply dekhn 21 hours agoparentHIV/AIDS dominated my high school years; it was just starting to become known; the first reported US case was in 1981, and in 1983, the disease (AIDS) was traced to the virus (HIV). My biology class had a poster of all the things AIDS could present as (kaposi's sarcoma, etc). At the time HIV was effectively a death sentence and it really affected places like SF heavily. One of my main motivations in college and grad school was to work in drug discovery, specifically for HIV. At the time (~1995-2000) we were just starting to see positive results from protease inhibitors and reverse transcriptase inhibtors came somewhat later leading to the current \"managment of HIV through HART\" https://en.wikipedia.org/wiki/Management_of_HIV/AIDS All of this came slowly - decades between significant new technologies/improvements in treatment. many sources of infection such as blood transfusions are now much less risky (people in the 70s and 80s were getting hep C and HIV from blood taken from HIV-positive donors). And the disease presents very differently in the US vs. other areas such as Sub-Saharan africa. But with extensive effort, prevention has gotten better and treatments have gotten much better. If there are truly usable preventatives for at-risk populations, and those medications are affordable, it will be a huge boon to the recipients. Some interesting things to note: - there was a lot of controversy about the source of infection and a lot of people used this to criticize gay people and injected drug users. - one of the world's most famous virologists, https://en.wikipedia.org/wiki/Peter_Duesberg actively denied that HIV caused AIDS and instead thought it was transmitted by drug use. Note \"Duesberg entered a long dispute with John Maddox, then-editor of the scientific journal Nature, demanding the right to rebut articles that HIV caused AIDS.\" which I think presages the current arguments about what scientists can say regarding the origins of COVID. The impact of his statements in South Africa was significant. From what I can tell he was completely off base and never made any truly convincing arguments for his position. - Fauci, of COVID fame, played a big role in getting NIH and the various AIDS community organzations working together and making large improvements to HIV/AIDS treatment. Before that, Fauci was heavily criticized by various LBGTQ orgs (see https://www.nytimes.com/2022/12/31/opinion/anthony-fauci-hiv...) \"Larry Kramer, one of the group’s founders, wrote an open letter to Dr. Fauci in The Village Voice calling him a murderer and comparing him to the Holocaust organizer Adolf Eichmann.\" Again, all this presages the later treatment of Fauci by various political groups during COVID. I often think back to Fauci during HIV/AIDS while reflecting on the current situation around COVID and I think we got lucky to find somebody like him, even if he said a few dumb things, and we'll be lucky if any public servants are willing to take up his role in the future. - modern gene therapy often uses variants on HIV as the vector. That's right: it's so good at getting into cells and modifying the genome, that we use it as the preferred method. it took quite some time before the vectors were made safe enough (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5152689/) reply mrmetanoia 21 hours agorootparentOne of my in-laws is a full on Fox news nut, but has a rare condition that he received treatments for as part of an NIH study that did him a world of good and Fauci was one of the doctors that worked on it, as a result the bits of nuttery he didn't buy into were attempts to discredit Fauci or anti-vax stuff. Oddly it didn't pull into question any other beliefs he picked up from that station. reply greentxt 19 hours agorootparentI believe that's referred to as Gell-Mann Amnesia. Common for most educated consumers of tv and print journalism. Indeed, even Fauci has made mistakes he has later had to correct/retract, like his early herd immunity claim. reply esalman 17 hours agorootparent> even Fauci has made mistakes he has later had to correct/retract, like his early herd immunity claim. You get new evidence and you update your beliefs. This is literally how science works and progress is made. If you had to put every student of science on the spot for everything wrong they said/did, there wouldn't be as much progress. reply greentxt 14 hours agorootparentWhy downvote, that's exactly what I said. I'm glad you agree that even experts make mistakes. It would be truly strange to argue that point, though many have and do. With respect to Gell-Mann amnesia, to be more exlicit since it seems that's needed, it's really just a side effect of our heuristics. Most of the time you can turn your brain off and follow the crowd, widespread consensus is a safe bet, but you have to leave room for occasionsl mistakes, as you acknowledge. Taking mental shortcuts, being lazy conformists is in no way exceptional. Humans are mostly stupid creatures, even experts. reply esalman 13 hours agorootparentUm I don't really agree. Making mistakes is not the same as changing opinion based on new evidence. Fauci changed opinions, a lot of experts do. It's only the lay people with no idea how science works who will call it a \"mistake\". Actually it's worse than that, people have openly harassed and threatened Fauci and his family for doing what he did. That's unfair and borderline evil. reply vasco 11 hours agorootparentHe also made mistakes, like telling us masks didn't work to save stock, and then changing guidance when there was enough stock. It's ok, as we're saying here anyone is entitled some mistakes along the way, but nobody is perfect, and overall we got through the worst of it, so it wasn't that bad. reply greentxt 7 hours agorootparentprevWell if you think he didn't make mistakes, or think that any human is error free, then you have an oppinion that is not based on science. Some sort of zealotry or hero worship or something along those lines. Fauci made mistakes. Science makes mistakes. To err is human. You might want to reevaluaute your beliefs and maybe read some of the copious amount of scientific and philosophic literature on the topic. Everyone makes mistakes. reply Pxtl 18 hours agorootparentprevI always think Gell-Mann Amnesia is funny because the namer of it was a medical doctor who wrote a book about about how climate change is fake. reply YeGoblynQueenne 20 hours agorootparentprev>> one of the world's most famous virologists, https://en.wikipedia.org/wiki/Peter_Duesberg actively denied that HIV caused AIDS and instead thought it was transmitted by drug use. I'm curious about his claim that retroviruses (retrovirii?) must be harmless to survive and multiply. What is the mainstream consensus on this? To be clear: I know next to nothing about viruses (virii? I mean I don't even know how to call them) and I have no idea whether it really supports Peter Duesberg's claims about AIDS in general. I'm just wondering whether he's pointed out an interesting peculiarity of HIV that is not further investigated by others for fear of being accused of denialism. Scientists can get very like that. >> (...) the current arguments about what scientists can say regarding the origins of COVID. It reminded me most about John Ioannidi's polemic against the way COVID was dealt with, rather than its origin. Ioannidis is a leading epidemiologist so he had to be taken seriously, although of course his opinion was rejected by most everyone else. reply opprobium 19 hours agorootparentYou can read a lot about it here: https://en.wikipedia.org/wiki/Duesberg_hypothesis The mainstream consensus is the he was wildly wrong about HIV specifically, that HIV causes AIDS, and that his influence in South Africa to not deploy anti-viral medications killed hundreds of thousands of people before the policy was reversed. Part of his hypothesis was that viruses in general, not just retroviruses, were not connected to cancers, the consensus view is that this is completely wrong. We have a very large body of evidence on many virus caused cancers now. Even at the time he was arguing this, it was clear that the retrovirus HTLV was disease causing in humans https://en.wikipedia.org/wiki/Primate_T-lymphotropic_virus So, the two known human retroviruses both cause disease and retroviruses cause diseases in animals. Duesberg held on to and promoted this concept long after it should have been clear to him that there was zero empirical support for his idea. reply dekhn 19 hours agorootparentTo me the most convincing bit that weakens his \"hypothesis\" is that people who received blood transfusions from HIV-contaminated blood. Many of those people showed none of the risk factors. See https://www.nytimes.com/2024/05/20/world/europe/britain-cont... for some recent discussion of the scope and scale of HIV contamination. reply WhitneyLand 19 hours agorootparentprevInfluenced the deaths of hundreds of thousands of people? Yet apparently to this day he draws over 200k/yr in salary from Berkeley. I believe they are not entirely funded by tuition/endowments which means California tax payers support him at least in part. reply dekhn 19 hours agorootparentHe has tenure and hasn't done anything that would force the dean to fire him. He hasn't published in ~7 years. IIUC he's been isolated- doesn't get any real funding from NIH, or from the university (beyond the standard salary), and doesn't have an active lab. It would likely cost the university more in legal fees to get rid of him than keep him until he goes away. reply opprobium 19 hours agorootparentprevhttps://en.wikipedia.org/wiki/Peter_Duesberg#Consequences_of... He was investigated but it was dropped as being protected by his academic freedom: https://www.science.org/content/article/berkeley-drops-probe... He also, that I know of, still supports this position. To this day, you will find people getting into this particular conspiracy and rejecting treatment. It doesn't go well for them. I do think that freedom of speech is important, and that many attempts to squash \"misinformation\" are misguided, but some speech has consequences. Personally I find Duesberg utterly reprehensible and morally culpable. reply YeGoblynQueenne 17 hours agorootparentprevCheers, I read the wikipedia article. It mentions Duesberg's claims without going into details about why it is wrong, or right. reply opprobium 12 hours agorootparentPerhaps I found the article clearer because of familiarity with the subject. On the \"retroviruses must be harmless\" virology: He's a denier of viral involvement in cancers in general, not just that HIV must be harmless. He is way outside mainstream consensus on all kinds of things. For instance, he argues that Kaposi sarcoma, a very common AIDS related cancer was caused by drug use and not opportunistic infection. It is now very well established that all KS, which also affects (typically older) HIV- people, is caused by HHV-8 infection. He claims that Hep-B/C can't cause liver cancer. He claims that HPV doesn't cause cervical cancer (https://www.academia.edu/31617237/What_if_HPV_does_NOT_cause...) The core thing he does on all of these topics is just to ignore or deny anything that doesn't agree with him, eg: Hemophiliacs treated with tainted blood get AIDS, HIV viral load directly corresponds to disease progression which is clearly halted by dropping HIV load with treatment, the HPV vaccine demonstrably prevents cervical cancer, etc. He is far off in quack territory. reply rcxdude 20 hours agorootparentprevI don't see any particular reason why retroviruses would be different in that regard: they need the cell they infect to live long enough to produce enough viruses, and it is advantageous to them that some cells they infect lay dormant and reactivate later (a trick not limited to retroviruses), but there's not particular reason why they should not overall act much like any other virus: keeping their host cell alive only long enough to produce enough new viruses to continue to propagate. It's an argument which you could extent to any pathogen: why would any disease kill its host? reply outworlder 13 hours agorootparent> It's an argument which you could extent to any pathogen: why would any disease kill its host? Moreover, why would a disease even 'care' about the host? As long as it can jump hosts quickly, it can afford to kill many of them. If achieving fast transmission is tough on the host, so be it. Other diseases may select for the opposite approach, of course. reply IlliOnato 6 hours agorootparentprevIn reality HIV does have to deal with pressures described by Duesberg, but the virus found a workaround: extremely long \"incubation period\". As you probably know, it can stay dormant for 10 years or more, but then gets into active stage, causes AIDS, and relatively quickly kills the host. reply dekhn 20 hours agorootparentprevSpeculating about the shape of the potential energy surface of viral evolution is non-trivial. I don't know enough (my retrovirus knowledge is out of date), but if you look at authoritative knowledge (IE, textbooks), you will see many non-harmless retroviruses: - oncoretroviruses: as a side effect of how they integrate, they often cause cancer in patients. There is lots of time between infection and death for the virus to be transmitted. - lentiviruses (this is also known as a \"slow virus\"). There is often lots of time between infection and death for the virus to be transmitted. It's possible that scientists are avoiding directly attempting to argue with Deusberg's observations, but in general, the consensus seems to be that he brought nothing useful to the debate except irrational claims that were inconsistent with the evidence. We don't live in an ultrarational world where every fringe theory can be investigated. As for Ioannidis... not sure what to say. I think his big mistake was going to the white house and trying to make Trump an ally and not shut down everything because he predicted the virus wouldn't spread and wouldn't be fatal at the rates that were later observed. Diseases like COVID are multidimensional problems with partial information and a high level of politics, corp, and media involvement. I think fauci and others have finally admitted that they may have made some mistakes in the specific details of the shutdowns, in particular, it took people a while to realize that the impact on children (who by and large are not at risk from COVID) was enormous. If your goal is to affect public health policy, you have to be a truly 4D thinker, and even that's not enough dimensions. reply YeGoblynQueenne 17 hours agorootparentThanks. You reminded me hat Ioannidis made very specific predictions that turned out to be false (about the number of deaths we could expect). And that, while measures were adopted that he claimed were useless. I agree Fauci saved lives - and last time I saw him in the news he was being attacked by Republican trolls, I don't have any other word for those people. reply dylan604 19 hours agorootparentprev> we'll be lucky if any public servants are willing to take up his role in the future Sadly, there will be plenty of people desperately wanting that job. It is definitely a prime example of the saying those that want the job would not be good at the job and those that would be good at the job do not want it. Especially in today's environments where it will become a bully pulpit to push whatever agenda of whoever is in charge reply outworlder 13 hours agorootparentprev> I often think back to Fauci during HIV/AIDS while reflecting on the current situation around COVID and I think we got lucky to find somebody like him, even if he said a few dumb things, and we'll be lucky if any public servants are willing to take up his role in the future. Hopefully, the next administration doesn't come up with Schedule F once again and make all health-related government jobs political. https://en.wikipedia.org/wiki/Schedule_F_appointment reply kennethwolters 8 hours agorootparentprevDon't forget Kary Mullis as another famous scientist thinking HIV does not cause AIDS. I personally don't know what to make of Mullis' and Duesberg's claims. But I found an interesting Mullis article recently in which he presents a hypothesis about how AIDS could develop independently of HIV. Quite interesting, but I am guessing nothing really more than that. https://link.springer.com/article/10.1007/BF01435010 The article is unfortunately paywalled. reply toomuchtodo 23 hours agoprevhttps://archive.today/7cGmG Press release: https://www.gilead.com/news-and-press/press-room/press-relea... reply wouldbecouldbe 19 hours agoprevCan’t read the full article but how is this different from prep? It’s widely used in prevention in the gay community. [edit] it’s prep, it’s been in use for several years already, but this injection last longer reply wrs 19 hours agoparentTwice-yearly shot rather than daily pill. reply retrac 19 hours agoprevJust to be clear, this is not a vaccine. It is an antiviral drug delivered by a long-lasting subdermal delivery implant. It has been known that antiviral drugs effectively prevent HIV infection for some time now (about 20 years). Recent work has been on better-tolerated and longer-lasting formulations. Treatment of HIV+ people also reduces their infectiousness, and good treatment reduces the risk of passing the infection on nearly to zero. Providing sufficient HIV antivirals and medical care to everyone in the population, both HIV+ and at risk for HIV, in theory, could be enough to halt the pandemic. Some wealthy countries with sensible policies have seen remarkable gains. The UK is reasonably effective at getting drugs to both the HIV+ and to at-risk populations, and the number of new HIV infections there, has been reduced by approximately half in the last decade. reply duskwuff 18 hours agoparent> It is an antiviral drug delivered by a long-lasting subdermal delivery implant. I don't see anything to indicate it's an implant - the prescribing information [1] says it's a subQ injection, and the trial information [2] seems consistent with that. [1]: https://www.gilead.com/-/media/files/pdfs/medicines/hiv/sunl... [2]: https://classic.clinicaltrials.gov/ct2/show/NCT04994509 reply cj 19 hours agoparentprevIt’s not a vaccine, but it’s close to one at twice a year. Even if we can consider HIV “cured” in the developed world (where PrEP is available to anyone who wants it) there’s no way we eradicate HIV from impoverished countries with limited healthcare access until we either have 1) a vaccine, or 2) a shot (or something) that prevents HIV for a really freaking long time. Not sure if 6 months will quite cut it, but it’s great to see progress in the right direction. More advancement is needed. reply Frost1x 18 hours agorootparentI assume you’re referring to things like the flu vaccine. Many vaccines can last 10, 20 years or more which this doesn’t come close to. That’s not to say it’s not a great improvement, I happily await the day we can nearly eliminate some of these infectious diseases that plague humanity. reply xucheng 13 hours agoprevMaybe better url, the official press release : https://www.gilead.com/news-and-press/press-room/press-relea... @dang reply observer987 18 hours agoprevThe paradox of living in this day and age is witnessing astonishing advances in science, while enduring the increasing levels of anti-intellectualism that pervades society like a cancer. reply EnigmaFlare 17 hours agoparentI think all the really obvious and influential products of science and technology happened in the 20th century and now we're so comfortable because of those that we take it for granted. Then people find reasons to hate it because they forget how bad things were without it (previous generations). Same goes for political stability. Another aspect is that a lot of intellectualism is really activism with \"intellectuals\" trying to impede other people's lives for the sake of some arrogant moral purpose. reply autoexec 14 hours agorootparent> now we're so comfortable because of those that we take it for granted. Then people find reasons to hate it because they forget how bad things were without it (previous generations). Same goes for political stability. I'm not sure that most people are really all that comfortable. They're a lot more distracted though certainly. I think there are a lot of different reasons people today have a problem with science and technology. Some are scared of it. Some just don't trust it, which can be entirely fair depending on the degree/situation. Some see that the regulations, oversight, and accountability we expect and depend on to keep us safe aren't working like they used to or like we thought they would. Mostly I think people see not only what we've gained, but also what we've lost and could/should have again. Reliable and repairable products that weren't designed to exploit and work against the interests of the person who paid for them for just one example. We've had many trade offs, where they've improved things in some areas while making them worse in others. It hasn't always worked out in our favor. It's also frustrating when you see that amazing things are now possible, but we can't have them because of politics, or greed, or fear of change. Personally, I hope people never stop wanting and expecting better from science and technology. Especially in those cases where what previous generations had was better than what we're expected to accept today or where we've created problems previous generations never had to put up with. reply armada651 11 hours agorootparent> I think there are a lot of different reasons people today have a problem with science and technology. Some are scared of it. Some just don't trust it, which can be entirely fair depending on the degree/situation. Some see that the regulations, oversight, and accountability we expect and depend on to keep us safe aren't working like they used to or like we thought they would. We often forget that many people have been genuinely negatively affected by technology or science or know someone who has. Let's not forget that many technological and medical advances have come at a real human cost. People have been poisoned by harmful chemicals either during their occupation or because an entire community has been exposed. Entire communities have been devastated by the opioid epidemic which the medical community is directly responsible for. Not to mention the countless people who have lost their jobs or will lose them soon to automation. There are people with genuine concerns about the way science and technology are heading and pretending anyone skeptical of modern science is simply uneducated or stupid is extremely counter-productive. reply autoexec 11 hours agorootparentI think things like the opioid crisis where doctors were getting outright bribes from pharmaceutical companies who knew they were killing people has done a massive amount of harm to the trust people had in medical science. It's been a problem for a long time, even going back to the tobacco industry hiring researchers to lie about the dangers of smoking. Those researchers didn't lose their jobs and become unhireable in their fields. They just went on to work for the oil companies to lie about how climate change isn't real and are now working for companies currently trying to convince the FDA about the safety of food additives. Between corporations being able to buy whatever research they think will get them a favorable headline, peer reviewed journals accepting any paper if you pay them to publish it (this one being a personal favorite https://www.sciencealert.com/a-neuroscientist-just-tricked-4...), the reproducibility crisis more generally, the total lack of any meaningful consequences when companies are caught outright knowingly poisoning people or selling dangerous drugs, it's really getting harder to explain to people at the fringes like antivaxxers why they should have more faith in the data we have and on the systems put in place to protect them. If the people aren't held accountable for causing harm and scientists don't do a much better job self-policing I think the situation is only going to get much worse. Even if things do change it will likely take generations to undo the damage already done. reply UniverseHacker 2 hours agorootparentprev> I'm not sure that most people are really all that comfortable. They're a lot more distracted though certainly. I think people in wealthy countries like the USA are very physically comfortable, but also quite unhappy- possibly much more unhappy day to day than they were historically when there was a lot more disease and discomfort- and a lot of that is directly a result of excess comfort combined with a life without any real difficulty, challenge, or sense of meaningful purpose. We feel like we want comfort, but it's mostly harmful to us. Humans just aren't built to be \"house pets.\" People need a sense of purpose, of overcoming difficult challenge, and an ability to directly see positive results from their efforts. The challenges need to be both mental, and physical. What we have now is lots of empty entertainment, stupor inducing comfort, and lots of sedentary careers that feel pointless, where nobody even notices the difference if you work hard or not. More and more people are burned out at work, and socially isolated. I don't think the answer is to go \"backwards\" and lose all of our progress in treating disease, making labor easier, etc. but in a cultural and personal change where we find some new meaning and challenges, to grow even more. Personally, I've found this through being a scientist where I can work on hard problems, as well as doing physically demanding and uncomfortable hobbies like weight training, fasting, and cold water swimming. I've noticed that the more intentional physical discomfort I experience, e.g. from cold, the more content I feel, and the less I crave comfort, or other addictive things like social media and overeating. reply ithkuil 11 hours agorootparentprevPeople don't _feel_ comfortable but they objectively are much more comfortable that 120 years ago or more. Unfortunately being objectively more comfortable doesn't make you feel more comfortable and ultimately it matters how you feel and want to fix whatever is causing them to feel like shit. reply Angostura 12 hours agorootparentprev> Another aspect is that a lot of intellectualism is really activism with \"intellectuals\" trying to impede other people's lives for the sake of some arrogant moral purpose. OK, I’ll bite -examples? reply jstanley 11 hours agorootparentYou won't get any because of https://www.paulgraham.com/say.html reply nathan_compton 2 hours agorootparentThis is such a cop out, especially given this is a fairly anonymous space AND pretty friendly to \"cancelled\" ideas. reply tim333 9 hours agorootparentprevA historical example might be communism which impeded a lot of people's lives and was pushed by intellectuals going back to Marx himself. More recently some people are unenthusiastic about promoting trans stuff and affirmative action. reply Aerroon 14 hours agorootparentprevI think what societies need is a feeling of hope and improvement. As long as people feel that things are getting better they are happy. reply nathan_compton 2 hours agorootparentprevDo you really think there is no intellectual work to be done on moral subjects? That morality is entirely in the realm of folk intuition? If you thought very hard about some moral question and came to another conclusion than most of society, what would you do about it? Would you have called intellectual abolitionists people trying to impede other people's lives for some arrogant moral purpose? Like, I get it, nobody likes a woke-scold, but it is still weird to complain about the idea that an intellectual who comes to a moral understanding might want to act on that new understanding/change the world/convince others. reply agumonkey 4 hours agorootparentprevwhen the ratio between education and society sophistication flips you get problems. It's a strange group dynamics still. reply mschuster91 11 hours agorootparentprevIn Germany, unlike every other European country (maybe except the Polish, not sure if they're doing the same with Auschwitz?), we have every generation of school children visit a Nazi Konzentrationslager once - precisely to avoid forgetting how bad it was, by showing the actual, undeniable evidence. And on top of that the Nazi dictatorship is usually an entire year's worth of history lessons in schools. Despite the far-right being on the rise as well as everywhere in Europe, they still have a harder time here, which I think is mostly due to these two education policies. reply Maken 10 hours agorootparentI do not think it's working. The AfD was the second most voted party for the European parliament, despite their candidate literally defending the SS a few weeks prior. reply mschuster91 8 hours agorootparentEuropean elections are usually used to deliver a \"Denkzettel\" to the currently governing party, it's the same across Europe as these elections are (wrongly) seen as consequenceless. Federally, the AfD is around 15-18% [1], which is still way too high in my opinion, but they're far from any chance to gain relevant influence on politics. Statewide is a different beast, sadly in Eastern Germany (the equivalent of the \"flyover states\" in the US) they're almost at the 33% required to block major legislation [2]. I'm honestly not sure how to combat that any more, outside of a (well deserved, given e.g Höcke directly using banned NS slogans) ban on the party. [1] https://www.wahlrecht.de/umfragen/ [2] https://dawum.de/Sachsen/ reply account42 9 hours agorootparentprevnext [4 more] [flagged] sofixa 9 hours agorootparentExcuse me, are you saying that teaching kids about past horrors including onsite visits to places where those horrors took place, with pictures and film and everything is \"brainwashing\"? reply ehvatum 9 hours agorootparentHe’s making the point that poor governance delivering poor outcomes will eventually by replaced by its ideological opposition, no matter how distasteful. reply hnbad 7 hours agorootparentNo, he isn't saying that. The context was \"children are taught why Nazis are bad but now people are voting for Nazis\" and his response was to say people were brainwashed to defend going against their interests. He may not be saying this to imply that the Nazis would have been acting in the interests of the voters but that's what the notion that teaching children Nazis are bad is brainwashing (to allow the government parties to go against the interests of their voters) strongly suggests. Couple this with a comment history of being vocal against \"economic refugees\" and a German political context of the AfD heavily using migrant scares in their rhetoric and it's entirely legitimate to challenge this statement. It's also worth mentioning that AfD politicians have not only defended the SS but also loudly complained about Holocaust remembrance and downplayed the extent and significance of the Holocaust. Your reply completely ignores that he deliberately used the word \"brainwashing\" which the comment you replied to called out. Intentionally or not, your reply demonstrates a motte-and-bailey (the bailey being that teaching about the Third Reich is brainwashing, the motte being the content of your reply). reply bartekpacia 9 hours agorootparentprevYes, in Poland we do the same (mandatory visit to Auschwitz) reply codesnik 8 hours agorootparentI wonder, how it is usually spinned? I'm asking as Russian and I see direct evidence of how people who grew up in the constant narrative \"fascism is awful, that war was important, our grandparents are heroes\" also, it seems, consumed it in easy/stupid form of \"fascism is something that those weird germans do, they attacked us, and we won, so we can't be fascists ever\". reply garaetjjte 4 hours agorootparentprevNo, it isn't. I think it's rather weird tourist attraction to pick for a trip. reply literallycancer 3 hours agorootparentprevI'm pretty sure there are places in Europe where the AfD equivalent doesn't get 15%. Trip to a holocaust museum is nice and all, but it probably fails at making people understand the problem. They'll kind of nod that yeah, Nazis were bad but then happily go and blame others for their bad decisions and vote for populists with easy solutions. Somehow half of Germany thinks Russia is OK, because they \"saved Europe\", hammer and sickle symbols are still not treated the same as swastikas and, of course, the main outcome of the kind of education you mention is that Germany is basically freeloading wrt defense and very unwilling to do the only reasonable thing, i.e. help prevent another genocide as it unfolds in Europe. Hopefully something has changed in the last 2 years, but the preceding decade, spending over 100 million euros daily on Russian natural gas is hard to undo. And that's with pre-war historical minimum prices. Since you all didn't get the memo that you need to stop buying Russian stuff until NS2 got blown up, the flow of money for natural gas from Germany to Russia in 2022 and 2023 is likely several times the pre-war annual number. And then you have people saying shit like \"we have spent enough on Ukraine\" or \"Ukrainian refugees are coming because of our social safety net\", not even from AfD politicians (I think some CDU idiot, lol). Yeah sure, but you gave 100x the money to Russia, who of course spent it on weapons because they don't give two fucks about their own people. Being sorry about things from the last century, while failing completely to judge the situation in the present doesn't really help. Not to mention Poland still didn't get the war reparations for WW2 last I checked. They probably don't want to shake the boat too much and just hope Germany will at least stop being useless. reply __rito__ 13 minutes agoparentprevIt's because of absolute penetration of social media in all levels of society everywhere. It makes the most stupid and uneducated person think that they are equal to the top minds of present day in many aspects. reply autoexec 15 hours agoparentprevIn the US you also have situations where all that science means that effective treatments exist but they are entirely out of your reach because of insane healthcare costs. I can't imagine having to watch a loved one slowly die knowing that you are surrounded by doctors who could save them if you only had the tens to hundreds of thousands of dollars they demand or if you'd been living in basically any other developed nation on Earth. reply ffgjgf1 10 hours agorootparent> In the US I’m certain that the US is in no way unique in that. Countries with universal public healthcare care systems do cost-benefit analysis all the time and access to the newest effective treatment options outside of the richest/most developed countries (or even in them) is far from guaranteed. e.g good luck buying latest cancer drugs from the US on an East European salary after your local healthcare system bureaucrats have rejected them because they are too expensive and/or are taking a year or two to decide of they are worth buying. > or if you'd been living in basically any other developed nation on Earth. That’s just beyond absurd, unless you think that only Switzerland and a handful of other rich countries are “developed”. Yes getting some minimum/acceptable level of care when you’re not rich might generally be easier. Getting access to latest or even experimental drugs (most of which are developed in the US)? Not so much.. reply kevincox 8 hours agorootparentThis is partially true but for many things the price is different. In the US drugs are priced assuming that there are some number of rich people who can afford them. This often results in higher margin pricing which is more profitable even if the volume is lower and puts them out of reach of many. In countries with public health care, setting the price that high will typically result in near-zero sales, so the price gets set closet to the cost-benefit point to make profit in lower margins but higher volumes. It doesn't always work like this. Some drugs are just too expensive to manufacture and the minimum profitable price is too high for the benefit in public health care. But often the bargaining and purchasing power of a public health care system can achieve lower prices for drugs and other tools. reply ffgjgf1 6 hours agorootparent> In countries with public health care, setting the price that high will typically result in near-zero sales, I’m not sure that’s strictly true at least when it comes to the most expensive/newly developed drugs: https://www.investigate-europe.eu/posts/deadly-prices-medici... Doesn’t seem that massively different from the relationship between insurance companies and drug companies in the US. > In countries with public health care, setting the price that high will typically result in near-zero sales Interestingly enough it seems like the poorer Central/East European countries end up paying more than the richer ones. reply beaeglebeachedd 14 hours agorootparentprevThat's because we subsidize the world by investing in a massive portion of the science and tech for producing medicines. Then when those costs are recouped via sales to our consumers the whole world laughs at us while ironically many of them would be up shits creek without the advancements they get to piggy back off of. Healthcare and medicine needs overhauling but it's maddening watching these downstream foreign benefactors damn the golden goose they'd be fucked without. reply autoexec 14 hours agorootparentI don't think it's necessarily true that we need to keep letting Americans needlessly die due to unaffordable healthcare in order to maintain our nation's lead in innovation. We should be able to continue to invest in medical science without continuing to fall behind other developed nations in actual health outcomes for patients. There are certainly many opportunities to drastically reduce the costs of healthcare without impacting the budgets for research. reply Aerroon 14 hours agorootparentDoes the US actually fall behind other nations in health outcomes? The US has two big factors working against them: much more widespread obesity (and the level of morbid obesity) and the insurance bureaucracy. These two factors should negatively impact health statistics without the treatments being worse. reply autoexec 13 hours agorootparent> Does the US actually fall behind other nations in health outcomes? For the world's richest people you couldn't do better than to be a patient in America. For most Americans though, the US healthcare system is failing them. America does worse compared to other nations in some very basic measures like having a lower average life expectancy, a higher infant morality rate, more obesity and congestive heart failure and more hospital/pharmacy screw ups. A child or teenager in the US is less likely to live to adulthood compared to those in other developed countries. It's not any better when it comes to mental health either. The US is one of the worst nations when it comes to mental health outcomes and suicide and drug related deaths are higher in the US. Over thirty percent of the US population has been forced to put off getting the care they need due to the cost and preventative care is usually the first thing that people cut back on leading to bigger problems that could have been avoided entirely. reply ffgjgf1 10 hours agorootparentIf you adjust by factors like obesity, state and drug use US is quite close or more or less on the same level as Western European countries. Even if you don’t do that there is a higher variance in life expectancy between different US states than inside the EU. e.g. California is about on par with the Netherlands, Germany, Britain while Mississippi and West Virginia are slightly below Bulgaria (of course mainly because of drugs..). IMHO that kinds of makes generalized comparisons semi-meaningless. reply allendoerfer 7 hours agorootparent\"The US has a poverty problem. People are dying.\" \"Yeah, but just unimportant people from flyover states. We have to divide our analysis of the United States: Over here in California - you now were the people we care about live - life is just as good as in these other fancy European countries we don't look down upon.\" reply ffgjgf1 6 hours agorootparent> Yeah, but just unimportant people from flyover states That’s not what I said at all.. reply allendoerfer 1 hour agorootparentWhy does it matter which Americans are dying? Other countries are not uniform either, averages are still applicable. reply nxm 13 hours agorootparentprev- 90% of Americans have health insurance - I would say it works for more than half (most) of Americans. Granted, not all health insurance is created equally. - The obesity and congestive heart failure issues is a function of poor dietary choices most Americans make (choosing fast/process food over cooking/making healthy foods), and not a function of healthcare access reply autoexec 12 hours agorootparent> 90% of Americans have health insurance Clearly having health insurance isn't enough to make healthcare affordable or prevent patient outcomes from being worse for Americans than patients in other nations. > The obesity and congestive heart failure issues is a function of poor dietary choices I also suspect that obesity and congestive heart failure issues are heavily influenced by diet, although I think that \"choices\" might be misleading here and that being unable to afford basic and preventative healthcare likely does have a major impact in healthcare outcomes even when the problems come from obesity and heart failure. It's certainly not as if people in other nations don't also enjoy ultra-processed foods or are any less fond of sugar, fat, and salt. They just often have better access to healthier foods, and more stringent regulations concerning what companies are and aren't allowed to put in foods, what they are allowed to advertise, and how they are sold. There is a huge difference between the number of people in the US who have little to no access to healthy foods when compared to those in other developed nations. Sometimes it's physical access (food deserts) and sometimes it's a matter of costs, but I don't think there's anything inherent to Americans that forces them to make bad choices. I'm guessing that instead Americans tend to have less choice and fewer opportunities for healthy foods in the first place, while having unhealthy ultra-processed foods that would be banned in other countries readily available, easily affordable, and heavily advertised to them. reply Aerroon 10 hours agorootparentI call bs on the second part. It's not unhealthy food that makes you gain weight, it's how much food you have. On top of that, the healthiest foods are among the cheapest foods you can buy. Milk, oatmeal, potatoes, rice, macaroni, chicken are all cheap compared to most foods you can get. This stuff is available almost everywhere. >but I don't think there's anything inherent to Americans that forces them to make bad choices. It's partly cultural. But this isn't a uniquely American problem. It's just worse in the US. 72% of Americans are obese or overweight. On top of that there was a recent study that found that BMR has decreased in the last ~30 years. Among men it was even up to 7%. That's a big enough difference to take someone from barely overweight to obese over a lifetime. But this is only a single study so far. reply autoexec 10 hours agorootparent> I call bs on the second part. It's not unhealthy food that makes you gain weight, it's how much food you have. I'd argue that unhealthy food can cause a person to eat more. See for example https://www.cell.com/cell-metabolism/fulltext/S1550-4131(19)... > On top of that, the healthiest foods are among the cheapest foods you can buy. The usual argument here calls out \"bulk beans and rice\" instead of potatoes and chicken, but in both cases you aren't going to find that in a gas station or convince store which for many Americans (some 40+ million) is the only place they can get groceries. If they're getting chicken it's not going to be whole and uncooked. It'll be sold in a plastic bag leaching PFAS into the food and be coated in high fructose corn syrup (most packaged lunch meats have sugar or HFCS added). Those kinds of places tend to have very little fresh fruit and veg as well. If you're looking at cost per calorie unhealthy food will often win out as less costly, not just in terms of money but also time and effort. Someone who works two jobs and spends hours sitting on or walking to/from the bus isn't always going to have the energy or time to roast a chicken. It's easy to see why they'd choose what's fastest, highly satisfying, requires less preparation and clean up, and is inexpensive. Not that I'm suggesting that the 72% of Americans who are obese/overweight have those kinds of problems... just that you'll find more Americans who are in that position than you'll find somewhere like the UK. I found the study on the decrease in BMR. That's pretty wild and if true, I wouldn't be surprised if it was a factor in the obesity problem. reply Aerroon 21 minutes agorootparent>If you're looking at cost per calorie unhealthy food will often win out as less costly What you should be looking for is cost per gram of protein. That's the only macro nutrient you need large amounts of. Calories per gram of protein are important too - oatmeal and dried pasta are the cheapest protein/$, but their protein/kcal is too low. I think it comes down to 'not caring enough' to try to get it under control. All the obfuscation and noise around nutrition (\"healthy food\") doesn't help either, but it's doable. >I found the study on the decrease in BMR. That's pretty wild and if true, I wouldn't be surprised if it was a factor in the obesity problem. It might even be the main part, at least for men. If your normal TDEE is 2136 kcal and you eat at maintenance, but then your BMR drops by 7.7% (165 kcal) then you'll gain weight until your TDEE is at 2136 kcal again. During this you would go from ~80 kg to ~95 kg taking you from 24.7 BMI to 29.3. An argument against the study is that it could be due to different measuring techniques. On the other hand, we have found that the average temperature of humans is slowly decreasing, so maybe it's true. hgomersall 12 hours agorootparentprevThe US has significant problems with over-treatment. I struggle to believe this isn't worse for the wealthy. reply autoexec 12 hours agorootparentIt's not all roses. I'm sure that even the wealthy who get healthcare in the US do have to deal with things like over-treatment, a lack of regulations, excessive costs, etc. It doesn't seem to hurt their outcomes enough to offset the fact that they'll have far more options, greater access to the best avilable technology/treatments, and that they'll be treated like absolute royalty. reply literallycancer 2 hours agorootparentprevPeople who can't do basic math and skip prevention are responsible for their own shit outcome. People in Portugal, where healthcare is \"free\", i.e. the government pays for it, frequently wait for years before being able to see a specialist due to long waitlists. The obvious outcome is that only poor people use the system and if you can you use private healthcare. People in Czechia with single payer healthcare system with e.g. average wage of 2000 USD pay from 100 euros a month for health insurance (unemployed) to e.g. 500 euros (with 4000 USD salary) or more if you make more. You get the same shitty service (something like 20 years behid the US), you just pay a lot more if you make anything resembling a US salary. There's no such thing as free healthcare. Can you make a single payer healthcare system that works better? Sure, it's just hard and even if everything is ideal you get maybe 50% discount. The main way to make healthcare cheaper is to drop coverage for diseases that are expensive to treat. reply dyauspitr 12 hours agorootparentprevI mean we have Medicaid, if you’re poor don’t you get healthcare for free? reply autoexec 11 hours agorootparentNo. If you get something like cancer, at best you can expect Medicaid to be comparable to basic private health insurance, which can easily leave people with out of pocket costs they can't possibly afford. Medicaid has a ton of other problems starting with eligibility, but even if you are eligible and you successfully jump through all the hoops to keep it (which are sometimes totally insane: https://youtube.com/watch?v=bVIsnOfNfCo), you still may not be able to get the services you need. Many doctors won't accept it and you can die just waiting for an appointment. Studies have shown there was effectively one psychiatrist for every 8,834 Medicaid beneficiaries and just one cardiologist for every 4,543 Medicaid beneficiaries. These doctors can't possibly see, let alone adequately treat and manage the care of, everyone who needs them. The closest we get to free healthcare in the US is care in the emergency room which is only required to \"stabilize\" you. They'll try their best to keep you alive if you're actively dying, but then they push you out the door and send you a massive bill. They won't give you chemo or radiation to keep your cancer from spreading reply outworlder 13 hours agorootparentprev> That's because we subsidize the world by investing in a massive portion of the science and tech for producing medicines. Even if we assume that's the case - as in, normal margins would be insufficient to finance the research - that does not account for the medical treatments themselves. > the whole world laughs at us Most of the world doesn't care. reply anakaine 12 hours agorootparentprevThis is such a rotten take. America, saviours of the world. It entirely discounts the contributions that other countries globally make. Countries in the European Union, Australasia, etc. The US cost of healthcare is about 17% of GDP. In other first world nations it's about 11%. This isn't service delivery or value, it's underlying cost. Per capita healthcare costs over twice of what it does in the UK. Similar for Australia. Both those are socialised and have very active R&D communities. The average life expectancy in the US is about 78. In other first world nations it's almost unilaterally closer to 84. The US is ranked 69th globally in terms.of health system performance. The US is also ranked worse than the OECD38 average for death by preventable causes. The biggest difference between those places I mention and the above is that the US views healthcare as a capitalist endeavour and tries to claim that competition will lower prices. Quite the opposite has occurred, and the system has become perverted. Intellectual property laws applied in this fashion ensure that you cannot have competition for health care since drugs are limited to a single supplier. You also don't get a choice in hospital care or doctors in most cases when you really look at how medical competition works. In other places, the costs are socialised through taxation. Drugs are purchased through nationalised efforts where suppliers must either come to the table and negotiate prices properly or lose access to entire markets. It's funny how they can still be quite profitable even under this scenario, and yet the prices still be so significantly less by orders of magnitude than US pricing per patient/dose. American exceptionalism ceases to be felt when you go spend time in other first world nations for any meaningful length of time. You realise it's reassurance of self rather than truth on basically all but defence technology spending. reply ffgjgf1 10 hours agorootparent> The average life expectancy in the US is about 78. In other first world nations it's almost unilaterally closer to 84. It doesn’t help that (at least when it comes to healthcare) US is a dozen of different countries in a trench coat. Life expectancy in richer states like California or New York is very close to that in Germany, the Netherlands, Britain etc. (and if adjusted for the massive disparity in drug related deaths they’d probably be closer to Italy, France or even Switzerland) while the poorest states are about on par with Eastern European countries where it’s barely above 75 years or so. So any average figure is semi meaningless. reply walrushunter 6 hours agorootparentprevIt's a tragedy of the commons. US lawmakers need to ban pharmaceutical companies from selling their drugs in the US at substantially higher prices than the drugs are sold elsewhere. reply adwn 13 hours agorootparentprev> That's because we subsidize the world by investing in a massive portion of the science and tech for producing medicines. How much of the money that flows into the US healthcare system really goes towards medical R&D, and how much is effectively wasted due to the inefficient bureaucracy and out-of-control litigation? reply autoexec 10 hours agorootparent> How much of the money that flows into the US healthcare system really goes towards medical R&D, and how much is effectively wasted due to the inefficient bureaucracy and out-of-control litigation? Literally tens of billions are wasted annually on advertising. The cost is pushed to the sick and hurting while doctors are bribed to overprescribe whatever drug people are being trained to \"ask their doctor about\". I'd worry about that way before I gave a thought to \"out-of-control litigation\". Especially considering how companies like Purdue Pharma and Philips Respironics can knowingly kill people with their drugs and medical devices, try to hide the fact they were doing it, yet face no meaningful consequences and not one person is put behind bars. If anything, I'd say America should be demanding more justice from the legal system not less. reply 2Gkashmiri 13 hours agorootparentprevif US government is paying for research, advancement and what not, why should the end result not be free for citizens? let them spend a trillion dollars on a new drug developed by bayer or whoever, why should the company then charge anything more than say a generic paracetemol? reply autoexec 10 hours agorootparentAll research done using any amount of money from the US government should be open and easily accessible to every American (with exceptions for national security), but that doesn't mean we should get the products resulting from it without cost. If the government funded research that resulted in a more eco-friendly car I wouldn't expect to see one delivered to my driveway or that the car with the fancy new tech (which might be a lot more involved in terms of costs) should be priced the same as the old tech. Healthcare should just be made accessible and affordable to everyone. It looks like the best way to do that is with publicly funded health systems. reply 2Gkashmiri 10 hours agorootparentaccessible is what i mean. >I wouldn't expect to see one delivered to my driveway or that the car with the fancy new tech (which might be a lot more involved in terms of costs) should be priced the same as the old tech. we often pay a new user tax, or monopoly tax like the epipen thing. that should not be possible. reply DocTomoe 11 hours agorootparentprevWell, if the US decided to not offer medicine on a realistic price point, other nations would do what the US did with Germany's Bayer, back when Anthrax was a concern: Cancel the patent on the only Anthrax medicine available due to \"national security\". Don't blame the world for your Pharma executives needing a third private jet for their mistresses. reply ffgjgf1 10 hours agorootparent> Cancel the patent on the only Anthrax medicine available due to \"national security\". Wouldn’t that discourage these companies from spending money to develop new drugs on their own in the future? reply DocTomoe 3 hours agorootparentDidn't stop Bayer from doing R&D. They even ate the dead rat that was Monsanto. reply tim333 8 hours agoparentprevI'm not so sure about rising levels of anti-intellectualism overall if you look globally. I looked at some stats over my lifetime and globally from Our World in Data 67m people had post secondary education in 1965 vs about 1.07bn now so up 16x. (https://ourworldindata.org/grapher/world-population-level-ed...) Also in the 1970s the Cambodians were searching out their intellectuals and executing them and the Chinese did a slightly less extreme version in the cultural revolution whereas now you get none of that and China is becoming a science superpower. Admittedly some in the US seem to be pushing antivax and climate denial but it's not like the past events. Also it seems a bit local. I'm a Brit for example and see almost no climate denial here. A bit of antivax maybe. reply cm2187 12 hours agoparentprevSome of the anti-vax movement during covid (I presume that's what you have in mind) is anti-intellectualism but some of it is not. You can hold both opinions that an mRNA vaccine is an incredible new technology that has enormous potential, while a new technology that had never been tested on humans shouldn't be forced on people for whom the benefit was marginal at best (kids, healthy population under 50, people who already had covid). And you can hold both opinions that health authorities clearly misbehaved or acted in a moronic way (lying about masks, origin of the virus, forcing vaccines on people who had already been infected, telling you you can't go outside, except if it's to protest for BLM, etc) while acknowledging that coming up with a vaccine against a new virus in only weeks is a technological wonder. It's absurd to be systematically anti-intellectual, but also some healthy skepticism is well warranted. reply Propelloni 7 hours agorootparentIt's like poker. You can have this lucky draw on the river, but most often you don't. You cannot built your strategy on lucky draws and those people weren't, even when not everything worked out. You and I are still alive, so I think they did a good job. reply account42 9 hours agorootparentprevRight, a lot of the problem with anti-intellectualism is actually the people who define their viewpoint as The Science and try to shut down those who disagree as being \"against the science\". reply tonymet 13 hours agoparentprevwhat does this have to do with the story? reply yieldcrv 18 hours agoparentprevits only been a small gap in time where anti-intellectualism was segregated from society. you could always just be in your tiny circles of elite higher education. only thing thats changed this time is anti-intellectualism is given a microphone. reply teh_infallible 18 hours agoparentprevI suppose that’s another way of saying we’re seeing controversial advances in science. reply oopsallmagic 14 hours agorootparentYou don't need to take HIV medication, but you don't need to prevent the rest of us from enjoying the benefits of modernity, too. reply beaeglebeachedd 14 hours agorootparentI think you'll find certain parties on both sides secretly want things to be framed as controversial to drum up the useful idiots to their cause or open their checkbook. reply javawizard 18 hours agorootparentprevNot sure what you're saying, can you elaborate? reply speed_spread 17 hours agorootparentprevAnything new can be controversial if you're allergic to change. reply xs83 19 hours agoprevAwesome news - we will eradicate HIV within the a single generation at this rate! reply JumpCrisscross 19 hours agoparent> we will eradicate HIV within the a single generation at this rate If there is one place socialised medicine makes so much sense that almost any argument against it is invalid, it's around contagious diseases. The prevention and treatment, inasmuch as it reduces transmission, which is true of virtually all HIV treatments, should be as effortless as possible. That starts with making it free. reply oopsallmagic 14 hours agorootparentWe had a taste when COVID shots were free, and it was magical. So nice not having to make 16 phone calls and then getting billed $800 \"by mistake\", followed by 20 more phone calls to fix it, only to find out that the pharmacist's sister's mailman was out of network, so we're sending your bill to collections because it got lost in the mail. reply Klaster_1 13 hours agoparentprevWouldn't complete eradication mean eradication in all countries? Given HIV policies degradation in Russia, which is significantly affected and has a sizeable population, your assumptions sounds too optimistic, even though I agree that would be a great achievement. reply greazy 19 hours agoparentprevOnly if we push for treatments to be cheap and widely available throughout the world and especially in Africa. Unfortunately it takes many years or evens decades for developing countries to afford these treatments. reply dashundchen 17 hours agorootparentPrograms like George Bush's PEPFAR have made great strides in fighting HIV globally through treatment and prevention. Estimated at 25 millions lives saved in 20 years, mostly in Africa. https://en.m.wikipedia.org/wiki/President%27s_Emergency_Plan... Unfortunately it's become a target of the right wing culture warriors and certain groups are trying to gut it, after huge stri",
    "originSummary": [],
    "commentSummary": [
      "Gilead's new HIV shot, cabotegravir, has shown to prevent all cases in a trial, presenting a promising alternative to daily pills.",
      "The shot is administered every two months, with potential extension to every three months, which could simplify adherence and increase compliance.",
      "Concerns exist about potential adverse reactions due to the drug's long presence in the system, but it can be tested in pill form first."
    ],
    "points": 798,
    "commentCount": 362,
    "retryCount": 0,
    "time": 1718910720
  },
  {
    "id": 40748371,
    "title": "Allan McDonald refused to approve Challenger launch, exposed cover-up (2021)",
    "originLink": "https://www.npr.org/2021/03/07/974534021/remembering-allan-mcdonald-he-refused-to-approve-challenger-launch-exposed-cover",
    "originBody": "Obituaries Remembering Allan McDonald: He Refused To Approve Challenger Launch, Exposed Cover-Up March 7, 20213:09 PM ET Heard on Morning Edition Howard Berkes Remembering Allan McDonald: He Refused To Approve Challenger Launch, Exposed Cover-Up Listen · 3:48 3:48 Toggle more options Download Embed EmbedTranscript Enlarge this image Allan McDonald in 2016 holds a commemorative poster honoring the seven astronauts killed aboard the space shuttle Challenger. Howard Berkes/NPR hide caption toggle caption Howard Berkes/NPR Allan McDonald in 2016 holds a commemorative poster honoring the seven astronauts killed aboard the space shuttle Challenger. Howard Berkes/NPR On Jan. 27, 1986, Allan McDonald stood on the cusp of history. McDonald directed the booster rocket project at NASA contractor Morton Thiokol. He was responsible for the two massive rockets, filled with explosive fuel, that lifted space shuttles skyward. He was at the Kennedy Space Center in Florida for the launch of the Challenger \"to approve or disapprove a launch if something came up,\" he told me in 2016, 30 years after Challenger exploded. His job was to sign and submit an official form. Sign the form, he believed, and he'd risk the lives of the seven astronauts set to board the spacecraft the next morning. Refuse to sign, and he'd risk his job, his career and the good life he'd built for his wife and four children. \"And I made the smartest decision I ever made in my lifetime,\" McDonald told me. \"I refused to sign it. I just thought we were taking risks we shouldn't be taking.\" The Two-Way Your Letters Helped Challenger Shuttle Engineer Shed 30 Years Of Guilt McDonald persistently cited three reasons for a delay: freezing overnight temperatures that could compromise the booster rocket joints; ice forming on the launchpad and spacecraft that could damage the orbiter heat tiles at launch; and a forecast of rough seas at the booster rocket recovery site. He also told NASA officials, \"If anything happens to this launch, I wouldn't want to be the person that has to stand in front of a board of inquiry to explain why we launched.\" Now, 35 years after the Challenger disaster, McDonald's family reports that he died Saturday in Ogden, Utah, after suffering a fall and brain damage. He was 83 years old. \"There are two ways in which [McDonald's] actions were heroic,\" recalls Mark Maier, who directs a leadership program at Chapman University and produced a documentary about the Challenger launch decision. \"One was on the night before the launch, refusing to sign off on the launch authorization and continuing to argue against it,\" Maier says. \"And then afterwards in the aftermath, exposing the cover-up that NASA was engaged in.\" Twelve days after Challenger exploded, McDonald stood up in a closed hearing of a presidential commission investigating the tragedy. He was \"in the cheap seats in the back\" when he raised his hand and spoke. He had just heard a NASA official completely gloss over a fundamental fact. McDonald and his team of Thiokol engineers had strenuously opposed the launch, arguing that freezing overnight temperatures, as low as 18 degrees F, meant that the O-rings at the booster rocket joints would likely stiffen and fail to contain the explosive fuel burning inside the rockets. They presented data showing that O-rings had lost elasticity at a much warmer temperature, 53 degrees F, during an earlier launch. The Two-Way Challenger Engineer Who Warned Of Shuttle Disaster Dies The NASA official simply said that Thiokol had some concerns but approved the launch. He neglected to say that the approval came only after Thiokol executives, under intense pressure from NASA officials, overruled the engineers. \"I was sitting there thinking that's about as deceiving as anything I ever heard,\" McDonald recalled. \"So ... I said I think this presidential commission should know that Morton Thiokol was so concerned, we recommended not launching below 53 degrees Fahrenheit. And we put that in writing and sent that to NASA.\" Former Secretary of State William Rogers chaired the commission and stared into the auditorium, squinting in the direction of the voice. \"I'll never forget Chairman Rogers said, 'Would you please come down here on the floor and repeat what I think I heard?' \" McDonald said. The focus of the commission's investigation shifted to the booster rocket O-rings, the efforts of McDonald and his colleagues to stop the launch and the failure of NASA officials to listen. The Two-Way Remembering Roger Boisjoly: He Tried To Stop Shuttle Challenger Launch Morton Thiokol executives were not happy that McDonald spoke up, and they demoted him. That alarmed members of the presidential commission and members of Congress. Rep. Edward Markey, a Massachusetts Democrat, introduced a joint resolution in the House that threatened to forbid Thiokol from getting future NASA contracts given the company's punishment of McDonald and any other Thiokol engineers who spoke freely. The company relented, and McDonald was promoted to vice president and put in charge of the effort to redesign the booster rocket joints that failed during the Challenger launch. In 1988, the redesigned joints worked successfully as shuttle flights resumed. McDonald continued to work at Thiokol until 2001 and retired after 42 years. He later co-authored one of the most definitive accounts of the Challenger disaster — Truth, Lies, and O-Rings: Inside the Space Shuttle Challenger Disaster. Enlarge this image In 2018, Allan McDonald reviewed the Challenger launch decision during a series of seminars about leadership and ethical decision-making to managers at U.S. Space Command. Mark Maier hide caption toggle caption Mark Maier In 2018, Allan McDonald reviewed the Challenger launch decision during a series of seminars about leadership and ethical decision-making to managers at U.S. Space Command. Mark Maier In retirement, McDonald became a fierce advocate of ethical decision-making and spoke to hundreds of engineering students, engineers and managers. He and Chapman University's Maier held leadership and ethics seminars for corporations and government agencies, including U.S. Space Command. Maier says that one of McDonald's key moments in his talks helps explain his ability to reconcile his brush with history. \"What we should remember about Al McDonald [is] he would often stress his laws of the seven R's,\" Maier says. \"It was always, always do the right thing for the right reason at the right time with the right people. [And] you will have no regrets for the rest of your life.\" \"It's really that simple if you just keep it focused that way,\" McDonald told me in 2016. The Challenger Disaster Anniversary Challenger: Reporting a Disaster's Cold, Hard Facts He also framed regret another way, paraphrasing a favorite quote from the late journalist Sydney J. Harris. \"Regret for things we did is tempered by time,\" McDonald said, his expression firm. \"But regret for things we did not do is inconsolable.\" McDonald then paused and added, \"That's absolutely true.\" He seemed inconsolable immediately after the Challenger explosion in tearful calls home. He recalls the painful conversations in his book. \"I feel like it's my fault,\" he told his daughter Lisa, a nursing student in Boston at the time. \"Don't blame yourself, Dad,\" she said, also crying. Maier believes McDonald lived out his life with neither blame nor regret. \"He died with serenity and equanimity,\" he says. \"I will miss him dearly.\" Allan McDonald leaves behind his wife, Linda, and four children — and a legacy of doing the right things at the right times with the right people. space shuttle challenger disaster Facebook Flipboard Email",
    "commentLink": "https://news.ycombinator.com/item?id=40748371",
    "commentBody": "Allan McDonald refused to approve Challenger launch, exposed cover-up (2021) (npr.org)445 points by EndXA 7 hours agohidepastfavorite262 comments GuB-42 3 hours agoI wonder how often things like that happen. The launch could have gone right, and no one would have known anything about the decision process besides a few insiders. I am sure that on project as complex and as risky as a Space Shuttle, there is always an engineer that is not satisfied with some aspect, for some valid reason. But at some point, one needs to launch the thing, despite the complains. How many projects luckily succeeded after a reckless decision? In many accidents, we can point at an engineer who foreshadowed it, as it is the case here. Usually followed by blaming those who proceeded anyways. But these decision makers are in a difficult position. Saying \"no\" is easy and safe, but at some point, one needs to say \"yes\" and take risks, otherwise nothing would be done. So, whose \"no\" to ignore? Not Allan's apparently. reply former_navy 2 hours agoparentOften. I used to run the nuclear power plant on a US Navy submarine. Back around 2006, we were sailing somewhere and Sonar reported that the propulsion plant was much, much louder than normal. A few days later we didn't need Sonar to report it, we could hear it ourselves. The whole rear half of the ship was vibrating. We pulled into our destination port, and the topside watch reported that oil pools were appearing in the water near the rear end of the ship. The ship's Engineering Officer and Engineering Department Master Chief shrugged it off and said there was no need for it to \"affect ship's schedule\". I was in charge of the engineering library. I had a hunch and I went and read a manual that leadership had probably never heard of. The propeller that drives the ship is enormous. It's held in place with a giant nut, but in between the nut and the propeller is a hydraulic tire, a toroidal balloon filled with hydraulic fluid. Clearly it had ruptured. The manual said the ship was supposed to immediately sail to the nearest port and the ship was not allowed to go back out to sea until the tire was replaced. I showed it to the Engineer. Several officers called me in to explain it to them. And then, nothing. Ship's Schedule was not affected, and we continued on the next several-week trip. Before we got to the next port, we had to limit the ship's top speed to avoid major damage to the entire propulsion plant. We weren't able to conduct the mission we had planned because the ship was too loud. And the multiple times I asked what the hell was going on, management literally just talked over me. When we got to the next port, we had to stay there while the propeller was removed and remachined. Management doesn't give a shit as long as it doesn't affect their next promotion. Don't even get me started on the nuclear safety problems. reply psunavy03 2 hours agorootparentThe correct answer in that case is to go to the Inspector General. That's what they're there for. Leaders sweeping shit under the rug that ends up crippling a fleet asset and preventing tasking from higher is precisely the kind of negligence and incompetence the IG is designed to root out. And I say that as a retired officer. reply CoastalCoder 44 minutes agorootparentHonest question: what are the plausible outcomes for an engineer who reports this kind of issue to the IG? I'm guessing there's a real possibility of it ending his career, at least as a member of the military. reply banku_brougham 0 minutes agorootparentI seriously believe what I've heard about upwards failure. Being competent seems to be an impediment, and the goons at the very top are ludicrously malformed people. richie-guix 1 hour agorootparentprevHow long retired? Things have gone in what can only be described as an.. incomprehensible unfathomable direction in the last decade or so. Parent post is not surprising in the least. Politics is seeping where it doesn't belong. I am very worried. reply HeatrayEnjoyer 1 hour agorootparentTell us more... what has happened? reply richie-guix 1 hour agorootparentTo a first approximation: https://www.youtube.com/watch?v=KZB7xEonjsc Less funny in real life. Sometimes the jizzless thing falls off with impeccably bad timing. Right when things go boom. People get injured (no deaths yet). Limp home early. Allies let down. Shipping routes elongate by a sad multiple. And it even affects you directly as you pay extra for that Dragon silicon toy you ordered from China. reply dontlikeyoueith 46 minutes agorootparentprevJust google the Red Hill failure. The Navy's careerist, bureaucratic incompetence is staggering. No better than Putin's generals who looted the military budget and crippled his army so they couldn't even beat a military a fraction of their size. reply richie-guix 24 minutes agorootparentThe unfathomable part is you getting downvotes for citing a well known mainstream news story. reply quacked 1 hour agorootparentprev> Don't even get me started on the nuclear safety problems. I want to be pro-nuclear energy, but I just don't think I can trust the majority of human institutions to handle nuclear plants. What do you think about the idea of replacing all global power production with nuclear, given that it would require many hundreds of thousands of loosely-supervised people running nuclear plants? reply somenameforme 37 minutes agorootparentThere's also the issue of force majeure - war, terrorism, natural disasters, and so on. Increase the number of these and not only can you not really maintain the same level of diligence, but you also increase the odds of them ending up in an unfortunate location or event. There's also the issue of the uranium. Breeder reactors can help increase efficiency, but they bump up all the complexities/risks greatly. Relatively affordable uranium is a limited resource. We have vast quantities of it in the ocean, but it's not really feasible to extract. It's at something like 3.3 parts per billion by mass. So you'd need to filter a billion kg of ocean water to get 3.3kg of uranium. Outside of cost/complexity, you also run into ecological issues at that scale. reply MostlyStable 40 minutes agorootparentprevConsidering that 1 Chernobyl scale accident per year would kill fewer people than global coal power does, I personally would be for it. reply orblivion 2 hours agorootparentprevIs this a different phenomenon though? It seems that there's a difference between an informed risk assessment and not giving a fuck or letting the bureaucratic gears turn and not feeling responsible. Like there's a difference between Challenger and Chernobyl. But, maybe someone can make a case that it's fundamentally the same thing? reply quacked 1 hour agorootparentI would make the case that it's fundamentally the same thing. In both cases, there were people who cared primarily about the technical truth, and those people were overruled by people who cared primarily about their own lifestyle (social status, reputation, career, opportunities, loyalties, personal obligations, etc.) In Allan McDonald's book \"Truth, Lies, and O-Rings\" he outlines how Morton Thiokol was having a contract renewal held over their head while NASA Marshall tried to maneuver the Solid Rocket Booster production contract to a second source, which would have seriously affect MT's bottom line and profit margins. There's a strong implication that Morton Thiokol was not able to adhere to proper technical rationale and push back on their customer (NASA) because if they had they would have given too much ammunition to NASA to argue for a second-source for the SRB contracts. (In short: \"you guys delayed launches over issues in your hardware, so we're only going to buy 30 SRB flight sets from you over the next 5 years instead of 60 as we initially promised.\" I have worked as a NASA contractor on similar issues, although much less directly impacting the crews than the SRBs. You are not free to pursue the smartest, most technically accurate, quickest method for fixing problems; if you introduce delays that your NASA contacts and managers don't like, they will likely ding your contract and redirect some of your company's work to your direct competitors, who you're often working with on your projects. reply BlarfMcFlarf 12 minutes agorootparentWhat’s the alternative? Being able to shift to a competitor when a producer is letting you down is the entire point of private contracts; without that, you might as well remove the whole assemblage of profit and just nationalize the whole thing. reply permo-w 10 minutes agorootparentprev>Like there's a difference between Challenger and Chernobyl. not in year, incidentally reply KennyBlanken 1 hour agorootparentprevIf you're EB, why replace a hydraulic bushing when you can wait, and replace it but also have to repair a bunch of damage and make yourself a nice big extra chunk of change off Uncle Sam? If you're ship's captain...why not help secure a nice 'consulting' 'job' at EB after retiring from the navy by helping EB make millions, and count on your officers to not say a peep to fleet command that the mess was preventable? reply lolinder 3 hours agoparentprev> Saying \"no\" is easy and safe, but at some point, one needs to say \"yes\" and take risks, otherwise nothing would be done. Saying \"no\" is easy and safe in a world where there are absolutely no external pressures to get stuff done. Unfortunately, that world doesn't exist, and the decision makers in these kinds of situations face far more pressure to say \"yes\" than they do to say \"no\". For example, see the article: > The NASA official simply said that Thiokol had some concerns but approved the launch. He neglected to say that the approval came only after Thiokol executives, under intense pressure from NASA officials, overruled the engineers. reply ein0p 2 hours agoparentprev> Saying \"no\" is easy and safe Not in my experience. Saying no to something major when others don’t see a problem can easily be career-ending. reply Brian_K_White 2 hours agorootparentEveryone seems to be reading this too simply. In fact, stupidly. It's conceptually the easiest answer to the risk of asserting that you are certain, is simply don't assert that you are certain. They aren't saying it's easy to face your bosses with anything they don't want to hear. reply lolinder 2 hours agorootparentIsn't the definition of \"easy\" or \"hard\" that includes the external human pressures the less simple/stupid one? What is the utility of a definition of \"easy\" that assumes that you work in complete isolation? reply Brian_K_White 2 hours agorootparentContext. reply lolinder 2 hours agorootparentThe context to this conversation is the launch of a space shuttle that's supposed to carry a teacher to space. It has both enormous stakes and enormous political pressure to not delay/cancel. I'm unsure why that context makes the spherical cow version of \"easy\" a sensible one. reply barbazoo 2 hours agorootparentprev> easily be career-ending. Easily be career ending? That's a bit dramatic, don't you think?. Someone who continuously says no to things will surely not thrive and probably eventually leave the organization, one way or the other, that's probably right. reply madaxe_again 2 hours agorootparentNot even slightly dramatic. I have seen someone be utterly destroyed for trying to speak out on something deeply unethical a state was doing, and is probably still doing. He was dragged by the head of state in the press and televised announcements, became untouchable overnight - lost his career, his wife died a few days later while at work at her government job in an “accident”. This isn’t in some tinpot dictatorship, rather a liberal western democracy. So - no. Career-ending is an understatement. You piss the wrong people off, they will absolutely fuck you up. reply quacked 1 hour agorootparentI have long thought that there ought to be an independently funded International Association for the Protection of Whistleblowers. However, it would quickly become a primary target of national intelligence agencies, so I don't know how long it would last. reply ein0p 2 hours agorootparentprevAsk Snowden. reply Aloisius 2 hours agorootparentSaying no isn't what ended his career. reply WarOnPrivacy 1 hour agorootparent> Saying no isn't what ended his career. Within NatSec, saying No to embarrassing the government is implied. Ceaselessly. Equally implied: The brutality of the consequences for not saying no. reply brandall10 5 minutes agoparentprevWhat would be interesting is to know how many people tried to puts the brakes on all the successful missions. reply pseudosavant 1 hour agoparentprevMy understanding of the Space Shuttle program is that there were a lot of times they knew they probably shouldn't fly, or try to land, and they lucked out and didn't lose the orbiter. It is shocking they only lost two ships out of the 135 Space Shuttle missions. The safety posture of that whole program, for a US human space program, seemed bad. That they chose to use solid rocket motors shows that they were willing to compromise on human safety from the get-go. There are reasons there hasn't ever been even one other human-rated craft to use solid rocket motors. reply floating-io 1 hour agorootparentExcept SLS? Not that I think it's a good thing, but... reply pseudosavant 53 minutes agorootparentI forgot about the SLS until after I wrote that. SLS makes most of the same mistakes, plus plenty of new expensive ones, from the Space Shuttle program. SLS has yet to carry a human passenger though. Its mind boggling that SLS still exists at all. At least $1B-$2B in costs whether you launch or not. A launch cadence measured in years. $2B-$4B if you actually launch it. And it doesn't even lift more than Starship, which is launching almost quarterly already. This before we even talk about reusability, or that a reusable Starship + Super Heavy launch would only use about $2M of propellent. reply danesparza 27 minutes agoparentprevDestin (from Smarter Every Day Youtube channel fame) has concerns about the next NASA mission to the moon (named Artemis): https://youtu.be/OoJsPvmFixU Read the comments (especially from NASA engineers). It's pretty interesting that sometimes it takes courageous engineers to break the spell that poor managers can have on an organization. reply dylan604 3 hours agoparentprevCan't we apply the same logic to the current Starliner situation. There's no way it should have launched, but someone brow beat others into saying it was an acceptable risk with the known issues to go ahead with the launch. Okay, so the launch was successful, but other issues that were known and suspect then caused problems after launch to the point they are not positive it can return. So, should it have launched? Luckily, at least to this point, nobody has been hurt/killed, and the vehicle is somewhat still intact. reply elviejo79 2 hours agoparentprevIt happens extremely frequently because there is almost no downside for management to override the engineers decision. Even in the case of the Challenger, no single article say WHO was the executive that finally approved the launch. No body was jailed for gross negligence. Even Ricahrd Feynman felt that the investigative comission was biased from the start. So, since there is no \"price to pay\" to make this bad calls they are continuously made. reply WalterBright 1 hour agorootparent> No body was jailed for gross negligence Jailing people means you'll have a hard time finding people willing to make hard decisions, and when you do, you may find they're not the right people for the job. Punishing people for making mistakes means very few will be willing to take responsibility. It will also mean that people will desperately cover up mistakes rather than being open about it, meaning the mistakes do not get corrected. We see this in play where manufacturers won't fix problems because fixing a problem is an admission of liability for the consequences of those problems, and punishment. Even the best, most conscientious people make mistakes. Jailing them is not going to be helpful, it will just make things worse. reply Anotheroneagain 1 hour agorootparentJailing people means you'll have a hard time finding people willing to make hard decisions, Why do you think you want it? You don't want it. reply nurbl 2 hours agoparentprevWhat makes you say it \"could have gone right\"? From what came out about the o-rings behavior at cold temperatures, it seems they were taking a pretty big risk. Your perspective seems to be that it's always a coin toss no matter what, and I don't think that is true. Were there engineers speaking up in this way at every successful launch too? reply JoshuaRogers 2 hours agorootparentI think what they were saying, especially given the phrasing “How many projects luckily succeeded after a reckless decision?” is that, if things hadn’t failed we would never have known and thus how many other failures of procedure/ ethics have we just not seen because the worst case failed to occur. reply wormius 25 minutes agorootparentGood ol' survivorship bias... reply HankB99 3 hours agoparentprev> at some point, one needs to say \"yes\" and take risks I'm wondering how the two astronauts on the ISS feel about that while Boeing decides if/when it is safe to return then to Earth. https://www.cnn.com/2024/06/18/science/boeing-starliner-astr... reply recursive 2 hours agorootparentPresumably about the same as they did prior to their first launch. Space travel is not like commercial air travel. This is part of the deal. reply kerkeslager 2 hours agorootparentHard disagree. The idea that the machinery your life will depend on might be made with half-assed safety in mind is definitely not part of the deal. Astronauts (and anyone intelligent who intentionally puts themselves in a life-threatening situation) have a more nuanced understanding of risk than can be represented by a single % risk of death number. \"I'm going to space with the best technology humanity has to offer keeping me safe\" is a very different risk proposition from \"I'm going to space in a ship with known high-risk safety issues\". reply WalterBright 1 hour agorootparent> the best technology humanity has to offer keeping me safe Nobody can afford the best technology humanity has to offer. As one adds more 9's to the odds of success, the cost increases exponentially. There is no end to it. reply iamthirsty 1 hour agorootparentprev> Hard disagree. The idea that the machinery your life will depend on might be made with half-assed safety in mind is definitely not part of the deal. It's definitely built in. The Apollo LM was .15mm thick aluminum, meaning almost any tiny object could've killed them. The Space Shuttle flew with SSRB's that were solid-fuel and unstoppable when lit. Columbia had 2 ejection seats, which were eventually taken out and not installed on any other shuttle. Huge risk is inherently the deal with space travel, at least from its inception until now. reply kerkeslager 6 minutes agorootparentWithout links to more information on these engineering decisions, I don't think I'm qualified to evaluate whether these are serious risks, and I don't believe you are either. I tend to listen to engineers. reply ApolloFortyNine 2 hours agoparentprevI've always thought the same, that something like space travel is inherently incredibly dangerous. I mean surely someone during the Apollo program spoke out about something. Like landing on the moon with an untested engine being the only way back for instance. Nixon even had a 'if they died' speech prepared, so someone had to put the odds of success not at 100. reply WalterBright 1 hour agorootparentNeil Armstrong figured that he only had a 50% chance of making it back from the moon alive. reply caseyy 2 hours agoparentprevA lot of people are taking issue with the fact that you need to say yes for progress. I don’t know how one could always say no and expect to have anything done. Every kind of meaningful success involves negotiating risk instead of seizing up in the presence of it. The shuttle probably could have failed in 1,000 different ways and eventually, it would have. But they still went to space with it. Some risk is acceptable. If I were to go to the moon, let’s say, I would accept a 50% risk of death. I would be happy to do it. Other people would accept a risk of investment and work hour loss. It’s not so black or white that you wouldn’t go if there’s any risk. reply lolinder 2 hours agorootparentThe key thing with Challenger is that the engineers working on the project estimated the risk to be extremely high and refused to budge, eventually being overruled by the executives of their company. That's different than the engineers calculating the risk of failure at some previously-defined-as-acceptable level and giving the go-ahead. reply runlaszlorun 2 hours agorootparentprev> I would accept a 50% risk of death. No offense but this sounds like the sayings of someone who has not ever seen a 50% of death. It’s a little different 3 to 4 months out. It’s way different the night before and morning. Stepping “in the arena” with odds like those, I’d say the vast, vast majority will back out and/or break down sobbing if forced. There’s a small percent who will go forward but admit the fact that they were completely afraid- and rightly so. Then you have that tiny percentage that are completely calm and you’d swear had a tiny smile creeping in… I’ve never been an astronaut. But I did spend three years in and out of Bosnia with a special operations task force. Honestly? I have a 1% rule. The things might have a 20-30% chance of death of clearly stupid and no one wants to do. Things will a one in a million prob aren’t gonna catch ya. But I figure that if something does, it’s gonna be an activity that I do often but has a 1% chance of going horribly wrong and that I’m ignoring. reply 2shortplanks 1 hour agorootparent50% of the time doing something that has a one percent chance of killing you 69 times will kill you reply WalterBright 1 hour agorootparentprev> No offense but this sounds like the sayings of someone who has not ever seen a 50% of death. The space program pilots saw it. And no, I would not have flown on those rockets. After all, NASA would \"man rate\" a new rocket design with only one successful launch. reply lupusreal 3 minutes agorootparentSpooky graph that illustrates the risk: https://en.wikipedia.org/wiki/LGM-25C_Titan_II#/media/File:U... reply kerkeslager 2 hours agorootparentprev> Some risk is acceptable. If I were to go to the moon, let’s say, I would accept a 50% risk of death. I would be happy to do it. Other people would accept a risk of investment and work hour loss. It’s not so black or white that you wouldn’t go if there’s any risk. It's possible you're just suicidal, but I'm reading this more as false internet bravado. A 50% risk of death on a mission to space is totally unacceptable. It's not like anyone will die if you don't go now; you can afford to take the time to eliminate all known risks of this magnitude. reply afavour 3 hours agoparentprev> at some point, one needs to say \"yes\" and take risks Do they though? If the Challenger launch had been pushed back what major effects would there have been? I do get your general point but in this specific example it seems the urgency to launch wasn’t particularly warranted. reply jstanley 1 hour agorootparent> If the Challenger launch had been pushed back what major effects would there have been? The point is it's not just the Challenger launch. It's every launch. reply ben_jones 2 hours agorootparentprev> If the Challenger launch had been pushed back what major effects would there have been? An administrator would’ve missed a promotion. reply runlaszlorun 2 hours agorootparentI think it’s not even a missed promotion but a perceived risk of one- which may or may not be accurate. reply mathgradthrow 2 hours agorootparentprevyou need to establish which complaints can delay a launch. The parent comment is arguing that you need to set some kind of threshold on that. In practice, airplanes fly a little bit broken all the time. We have excellent data and theory and failsafes which allow that to be the case, but it's written in blood. reply ufmace 3 hours agoparentprevThat's the thing I always wonder about these things. It's fun and easy to provide visibility into whoever called out an issue early when it does go on to cause a big failure. It gives a nice smug feeling to whoever called it out internally, the reporters who report it, and the readers in the general public who read the resulting story. The actual important thing that we hardly ever get much visibility into is - how many potential failures were called out by how many people how many times. How many of those things went on to cause a big, or even small, failure, and how many were nothingburgers in the end. Without that, it's hard to say whether leaders were appropriately downplaying \"chicken little\" warnings to satisfy a market or political need, and got caught by one actually being a big deal, or whether they really did recklessly ignore a called-out legitimate risk. It's easy to say you should take everything seriously and over-analyze everything, but at some point you have to make a move, or you lose. You don't get nearly as much second-guessing when you spend too much time analyzing phantom risks and end up losing to your competitors. reply autoexec 2 hours agorootparent> The actual important thing that we hardly ever get much visibility into is - how many potential failures were called out by how many people how many times. I'm not sure that's important at all. Every issue raised needs to be evaluated independently. If there is strong evidence that a critical part of a space shuttle is going to fail there should be zero discussion about how many times in the past other people thought other things might go wrong when in the end nothing did. What matters is the likelihood that this current thing will cause a disaster this time based on the current evidence, not on historical statistics The point where you \"have to make a move\" should only come after you can be reasonably sure that you aren't needlessly sending people to their deaths. reply jajko 3 hours agoparentprev> But at some point, one needs to launch the thing Do they? Even if risks are not mitigated and say risk for catastrophe can't be pushed below ie 15%? This ain't some app startup world where failure will lose a bit of money and time, and everybody moves on. I get the political forces behind, nobody at NASA was/is probably happy with those, and most politicians are basically clueless clowns (or worse) chasing popularity polls and often wielding massive decisive powers over matters they barely understand at surface level. But you can't cheat reality and facts, not more than say in casino. reply ahmeneeroe-v2 3 hours agorootparentMaybe it's a bad analogy given the complexity of a rocket launch, but I always think about European exploration of the North Atlantic. Huge risk and loss of life, but the winners built empires on those achievements. So yes, I agree that at some point you need to launch the thing. reply whyever 2 hours agorootparentThis sounds like you are saying colonialism was a success story? reply lkbm 1 hour agorootparentFor the ones doing the colonizing? Overwhelmingly yes. A good potion of the issues with colonizing is about how the colonizing nations end up extracting massive amounts of resources for their own benefit. reply kerkeslager 1 hour agorootparentIn context, it sounds like you think that the genocide of indigenous peoples was totally worth it for European nations and that callous lack of concern for human life and suffering is an example to be followed by modern space programs. I'd like to cut you the benefit of the doubt and assume that's not what you meant; if that's the case, please clarify. reply bigstrat2003 44 minutes agorootparentYou are not reading the context correctly. The original point was that establishing colonies was very risky, to which whyever implied that colonialism was not a success story. But in fact it was extremely successful from a risk analysis point of view. Some nations chose to risk lives and it paid off quite well for them. The nuance of how the natives were treated is frankly irrelevant to this analysis, because we're asking \"did the risk pay off\", not \"did they do anything wrong\". reply bluefirebrand 2 hours agorootparentprevI think ultimately the problem is of accountability If the risks are high and there are a lot of warning signs, there needs to be strong punishment for pushing ahead anyways and ignoring the risk It is much too often that people in powerful positions are very cavalier with the lives or livelihoods of many people they are supposed to be responsible for, and we let them get away with being reckless far too often reply jajko 2 hours agorootparentprevI would somewhat agree with first launch, first moon mission and so on, but N-th in a row ain't building no new empires. Its business as usual. reply kerkeslager 1 hour agorootparentprev> Maybe it's a bad analogy given the complexity of a rocket launch, but I always think about European exploration of the North Atlantic. Huge risk and loss of life, but the winners built empires on those achievements. > So yes, I agree that at some point you need to launch the thing. This comment sounds an awful lot like you think the genocide of indigenous peoples is justified by the fact that the winners built empires, but I'd like to assume you intended to say something better. If you did intend to say something better, please clarify. reply ReptileMan 2 hours agoparentprev>Saying \"no\" is easy and safe, but at some point, one needs to say \"yes\" and take risks, otherwise nothing would be done. True, but that is for cases where you take the risk yourself. If the challenger crew knew the risk and were - fuck it - it's worth it it would have been different than a bureaucrat chasing a promotion. reply bluefirebrand 2 hours agorootparentEspecially when that bureaucrat probably suffered no consequences for making the wrong call. Essentially letting other people take all of the risk while accepting none. No demotion, no firing, and even if they did get fired they probably got some kind of comfy pension or whatever It's a joke reply hydrogen7800 5 hours agoprevAllan McDonald is a new name for me. Thanks for posting this. See also other engineers who objected to the launch, like Bob Ebeling [0], who suffered with overwhelming guilt nearly until his death in 2016, and Roger Boisjoly [1], who never worked again as an engineer after Challenger. [0] https://archive.ph/kGMYG [1] https://en.m.wikipedia.org/wiki/Roger_Boisjoly reply cushychicken 3 hours agoparentBoisjoly was Macdonald's peer at Thiokol. Ebeling (I think) was either his direct manager or his division director. Boisjoly quit Thiokol after the booster incident. Macdonald stayed, and was harassed terribly by management. He took Thiokol to court at least once (possibly twice) on wrongful discrimination / termination / whistleblower clauses, and won. reply sjm-lbm 4 hours agoparentprevI hadn't heard of McDonald either, but there's a recent book (https://www.amazon.com/Challenger-Story-Heroism-Disaster-Spa...) that covers his contribution well. (TBH I'm reading this book right now - probably 2/3 the way through or so - and it's kind of weird to see something like this randomly pop up on HN today.) reply aybs 3 hours agorootparentI just listened to the audio book on spotify, free for premium members, and I'm wondering if that's why I'm seeing so much about the Challenger disaster lately. Well worth a listen, and spends a great deal of time on setup for these key individuals who tried so hard to avert this disaster. reply autoexec 2 hours agorootparentBoeing's Starliner problems? This article was probably brought on by the (then) recent passing of Allan McDonald reply christophilus 6 hours agoprevThere’s a good lecture about this, called “The Normalization of Deviance”: https://m.youtube.com/watch?v=Ljzj9Msli5o&pp=ygUZbm9ybWFsaXp... reply steveBK123 6 hours agoparentExactly the concept why you don't want to let whatever dashboards/alerts/etc you maintain on your systems have a \"normal amount of reds/fails/spurious texts\". At some point you become immune. It's a lot harder to notice theres 4 red lights today than the usual 2-3 vs noticing 1 when there are normally exactly 0. reply OutOfHere 6 hours agorootparentYes. The causative issue is the way in which projects are managed. Employees have no ownership of the project. If employees had ownership over which changes they think are best, a good employee would act on bringing the alerts back to zero before they take on new features or a new project. There are some obstacles: 1. Employees not having a say in which issues to work on. This pretty much leads to the death of a project in the medium term due to near-total disregard of maintenance issues and alerts. 2. Big-team ownership of a project. When everyone is in charge, no one is. This is why I advocate for a team size of exactly two for each corporate project. 3. Employees being unreasonably pressured for time. Perhaps the right framing for employees to think about it is: \"If it were their own business or product, how would they do it?\" This framing, combined with the backlog, should automatically help avoid spending more time than is necessary on an issue. reply whodidntante 5 hours agorootparentNot making an ethical/moral judgement here, just a practical one - is there any reason to believe that giving employees ownership of the projects will be any better than having \"management\" own it if all factors were truly considered ? If every decision an employee made on features/issues/quality/time was accompanied by how much their pay was affected, would the outcomes really be better ? The team could decide to fix all bugs before taking on a new feature, or that the 2 month allotment to a feature should really be three months to do it \"right\" without having to work nights/weekends, would the team really decide to do that if their paycheck was reduced by 10%, or delayed for that extra month for those new features were delivered ? If all factors were included in the employee decision process, including the real world effect of revenue/profit on individual compensation from those decisions, it is not clear to me that employees would make any \"better\" decisions. I would think that employees could be even more \"short sighted\" than senior management, as senior management likely has more at stake in terms of company reputation/equity/career than an employee who can change jobs easier, and an employee might choose not to \"get those alerts to zero\" if it meant they would have more immediate cash in their pocket. And how would disagreements between team members be worked out if some were willing to forgo compensation to \"do it right', and others wanted to cut even more corners ? Truly having ownership means you have also financial risk. reply OutOfHere 5 hours agorootparent> is there any reason to believe that giving employees ownership of the projects will be any better than having \"management\" own it Non-technical management's skill level is almost always overrated. They're almost never qualified for it. Ultimately it still is management's decision, and always will be. If however management believes that employees are incapable of serving users, then it's management's fault for assigning mismatched employees. > how much their pay was affected Bringing pay into this discussion is a nonsensical distraction. If an employer misses two consecutive paychecks by even 1%, that's enough reason to stop showing up for work, and potentially to sue for severance+damages, and also claim unemployment wages. There is no room for any variation here. > Truly having ownership It should be obvious that ownership here refers to the ownership of the technical direction, not literal ownership in the way I own a backpack that I bring to work. If true financial ownership existed, the employee would be receiving substantial equity with a real tradable market value, with the risk of losing some of this equity if they were to lose their job. > how would disagreements between team members be worked out As noted, there would be just two employees per project, and this ought to minimize disagreements. If disagreements still exist, this is where management can assist with direction. There should always remain room for conducting diverse experiments without having to worry about which outcomes get discarded and which get used. --- In summary, if the suggested approach is not working, it's probably because there is significant unavoidable technical debt or the employees are mismatched to the task. reply chuckadams 4 hours agorootparentprev> Not making an ethical/moral judgement here, just a practical one - is there any reason to believe that giving employees ownership of the projects will be any better than having \"management\" own it if all factors were truly considered ? It's not either-or, the ownership is shared. As responsibility goes, the buck ultimately stops with management, but when the people in the trenches can make more of their own decisions, they'll take more pride in their work and invest accordingly in quality. Of course some managers become entirely superfluous when a team self-manages to this extent, and will fight tooth and nail to defend their fiefdom. Can't blame them, it's perfectly rational to try to keep one's job. As for tying the quality to pay in such an immediate way, I guess it depends on who's measuring what and why. Something about metrics becoming meaningless when made into a target, I believe it's called Cunningham's Law. I have big doubts as to whether it could work effectively in any large corpo shop, they're just not built for bottom-up organization. reply james_marks 4 hours agorootparentYes- Goodhart’s Law: https://en.m.wikipedia.org/wiki/Goodhart%27s_law reply nostrademons 4 hours agorootparentprevBeen all of an engineer, a manager, and a founder/CEO, and I enjoy analyzing organizational dysfunction. The difference between an engineer and a manager's perspective usually comes down to their job description. An engineer is hired to get the engineering right; the reason the company pays them is for their ability to marry reality to organizational goals. The reason the company hires a manager is to set those organizational goals and ensure that everybody is marching toward them. This split is explicit for a reason: it ensures that when disagreements arise, they are explicitly negotiated. Most people are bad at making complex tradeoffs, and when they have to do so, their execution velocity suffers. Indeed, the job description for someone who is hired to make complex tradeoffs is called \"executive\", and they purposefully have to do no real work so that their decision-making functions only in terms of cost estimates that management bubbles up, not the personal pain that will result from those decisions. Dysfunction arises from a few major sources: 1. There's a power imbalance between management and engineering. An engineer usually only has one project; if it fails, it often means their job, even if the outcome reality dictates is that it should fail. That gives them a strong incentive to send good news up the chain even if the project is going to fail. Good management gets around this by never penalizing bad news or good-faith project failure, but good management is actually really counterintuitive, because your natural reaction is to react to negative news with negative emotions. 2. Information is lost with every explicit communication up the chain. The information an engineer provides to management is a summary of the actual state of reality; if they passed along everything, it'd require that management become an engineer. Likewise recursively along the management chain. It's not always possible to predict which information is critical to an executive's decision, and so sometimes this gets lost as the management chain plays telephone. 3. Executives and policy-makers, by definition, are the least reality-informed people in the system, but they have the final say on all the decisions. They naturally tend to overweight the things that they are informed on, like \"Will we lose the contract?\" or \"Will we miss earnings this quarter?\" All that said, the fact that most companies have a corporate hierarchy and they largely outcompete employee-owned or founder-owned cooperatives in the marketplace tends to suggest that even with the pitfalls, this is a more efficient system. The velocity penalty from having to both make the complex decisions and execute on them outweighs all the information loss. I experienced this with my startup: the failure mode was that I'd emotionally second-guess my executive decisions, which meant that I executed slowly on them, which meant that I didn't get enough iterations or enough feedback from the market to find product/market fit. This is also why startups that do succeed tend to be ones where the idea is obvious (to the founder at least, but not necessarily to the general public). They don't need to spend much time on complex positioning decisions, and can spend that time executing, and then eventually grow the company within the niche they know well. reply kmacdough 4 hours agorootparent> All that said, the fact that most companies have a corporate hierarchy and they largely outcompete employee-owned or founder-owned cooperatives in the marketplace tends to suggest that even with the pitfalls, this is a more efficient system. This conclusion seems nonsensical. The assumption that what's popular in thearket is popular because it's effective has only limited basis in reality. Heirarchical structures appear because power is naturally consolidating and most people have an extreme unwillingness to release power even when presented with evidence that it would improve their quality of life. It is true that employee owned companies are less effective at extracting wealth from the economy, but in my experience working for both traditional and employee owned companies, the reason is employees care more deeply about the cause. They tend to be much more efficient at providing value to the customer and paying employees better. The only people who lose out are the executives themselves which is why employee owned companies only exist when run by leaders with passion for creating value over collecting money. And that's just a rare breed. reply nostrademons 4 hours agorootparentYou've touched on the reason why hierarchical corporations outcompete employee-owned-cooperatives: > Hierarchical structures appear because power is naturally consolidating and most people have an extreme unwillingness to release power even when presented with evidence that it would improve their quality of life. Yes, and that is a fact of human nature. Moreover, many people are happy to work in a power structure if it means that they get more money to have more power over their own life than they otherwise would. The employees are all consenting actors here too: they have the option of quitting and going to an employee-owned cooperative, but most do not, because they make a lot more money in the corporate giant. (If they did all go to the employee-owned cooperative, it would drive down wages even further, since there is a finite amount of dollars coming into their market but that would be split across more employees.) Remember the yardstick here. Capitalism optimizes for quantity of dollars transacted. The only quality that counts is the baseline quality needed to make the transaction happen. It's probably true that people who care about the cause deliver better service - but most customers don't care enough about the service or the cause for this to translate into more dollars. As an employee and customer, you're also free to set your own value system. And most people are happier in work that is mission- & values-aligned; my wife has certainly made that tradeoff, and at various times in my life, I have too. But there's a financial penalty for it, because lots of people want to work in places that are mission-aligned but there's only a limited amount of dollars flowing into that work, so competition for those positions drives down wages. reply chimpanzee 3 hours agorootparent> most customers don't care enough about the service or the cause for this to translate into more dollars. This is an important point as it reinforces the hierarchical structure. In an economy composed of these hierarchies, a customer is often themselves buying in service of another hierarchy and will not themselves be the end user. This reduces the demand for mission-focused work in the economy, instead reinforcing the predominance of profit-focused hierarchies. reply chimpanzee 4 hours agorootparentprevThis is a wonderful summary, very informative. Thank you. Is there a book or other source you’d recommend on the subject of organizational roles and/or dysfunction?…ideally one written with similar clarity. One thing stood out to me: You note that executives are the least reality-informed and are insulated from having their decisions affect personal pain. While somewhat obvious, it also seems counterintuitive in light of the usual pay structure of these hierarchies and the usual rationale for that structure. That is, they are nearly always the highest paid actors and usually have the most to gain from company success; the reasoning often being that the pay compensates for the stress of, criticality of, or experience required for their roles. Judgments aside and ignoring the role of power (which is not at all insignificant, as already mentioned by a sibling commenter), how would you account for this? reply rawgabbit 4 hours agorootparentprevThere is a Chinese saying you can conquer a kingdom on horseback but you cannot rule it on horseback. What that means is, yes, entrepreneurial velocity and time to market predominate in startups. But if they don’t implement governance and due process, they will eventually lose what market share they gained. Left uncontrolled, internal factions and self serving behavior destroys all organisations from within. reply rawgabbit 5 hours agorootparentprevWhat I see is a movement where line employees have a say on who is retained at the director and VP level. The CEO reports to the board. But his immediate and second tier reports are also judged by the employees. The thought is that will give them pause before they embark on their next my way or the highway decision making. The most egregious directors who push out line employees in favor of their cronies will be fired under this evaluation. reply josefx 2 hours agorootparentprev> If employees had ownership over which changes they think are best, a good employee would act on bringing the alerts back to zero before they take on new features or a new project That requires that you have good employees, which can be as rare as good management. reply CSMastermind 4 hours agorootparentprev> If employees had ownership over which changes they think are best, a good employee would act on bringing the alerts back to zero before they take on new features or a new project. You say this but as someone who's run a large platform organization that hasn't been my experience. Sure some employees, maybe you, care about things like bringing alerts back to zero but a large number are indifferent and a small number are outright dismissive. This is informed not just by individual personality but also by culture. Not too long ago I pointed out a bug in someone's code who I was reviewing and instead of fixing it they said, \"Oh okay, I'll look out for bugs like that when I write code in the future\" then proceeded to merge and deploy their unchanged code. And in that case I'm their manager not a peer or someone from another team, they have all the incentive in the world to stop and fix the problem. It was purely a cultural thing where in their mind their code worked 'good enough' so why not deploy it and just take the feedback as something that could be done better next time. reply OutOfHere 4 hours agorootparentWith regard to alerts, I have written software that daytrades stocks, making a lot of trades over a lot of stocks. Let me assure you that not a single alert goes ignored, and if someone said it's okay to ignore said alerts, or to have persistent alerts that require no action, they would be losing money because in time, they will inevitably ignore a critical error. I stand by my claim that it's what sets apart good employees from those that don't care if the business lives or dies. I think a role of management is to ensure that employees understand the potential consequences to the business of the code being wrong. reply steveBK123 1 hour agorootparentYes, there was a recent story about (yet another) Citi \"fat finger\" trade. The headlines mentioned things like \"the trader ignored 700 error messages to put in the trade\", but listening to a podcast about it.. its more like awful systems that are always half broken is what ultimately lead to it. The real punchline was this - the trader confused a field for entering shares quantity for notional quantity, but due to some European markets being closed, the system had a weird fallback logic that it sets the value of shares to $1, so the confirmation back to the trader was.. the correct number of dollars he expected. So awful system designs lead to useless and numerous alerts, false confirmations, and ultimately huge errors. reply lencastre 4 hours agorootparentprevAnd groupthink reply simpaticoder 5 hours agorootparentprevThe more pernicious form of this, in my experience, are ignored compiler/linter/test warnings. Many codebases have a tremendous number of these warnings, devs learn to ignore them, and this important signal of code quality is effectively lost. reply eschneider 3 hours agorootparentIt's almost always worth spending the time to either fix all warnings or, after determining it's a false positive, suppressing it with a #pragma. Once things are relatively clean, it's easy to see if new code/changes trip a warning. Often unexpected warnings are a sign of subtle bugs or at least use of undefined behaviors. Sorting those out when they come up is a heck of a lot easier than tracing a bug report back to the same warming. reply ChrisMarshallNY 4 hours agorootparentprevI like to program with -wall. Doesn't win me fans, but I sleep well. reply drited 4 hours agorootparentCould you please expand on what that is? reply vardump 4 hours agorootparentEnable all warnings. https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#inde... reply ChrisMarshallNY 4 hours agorootparentprevIt's a C/C++ compiler flag, saying all warnings on. Since I do Swift, these days, in Xcode, I use project settings, instead. I also like to treat warnings as errors. Forces me to be circumspect. reply diab0lic 4 hours agorootparentprevIt’s a CLI flag to the compiler that enables all warnings. reply artificialLimbs 4 hours agorootparentprevhttps://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#inde... reply shswkna 7 hours agoprevThis is an ever recurring theme in the human condition. McDonald’s loyalty was not beholden to his bosses, or what society or the country wanted at that moment in time. He knew a certain truth, based on facts he was aware of, and stuck by them. This is so refreshing in todays world, where almost everyone seems to be a slave to some kind of groupthink, at least in public. reply freeopinion 2 hours agoparentWe all celebrate a hero who stands for what they believe or know to be right. When they stand alone we admire their steadfastness while triumphant music plays in the background. In real life we can't stand these people. They are always being difficult. They make mountains out of every molehill. They can never be reasonable even when everyone else on the team disagrees with them. Please take a moment to reflect on how you treat inconvenient people in real life. reply illusive4080 6 hours agoparentprevIn corporate world, everything must be tame and beige. Conflict or differences of opinion are avoided to focus on the areas where everyone agrees. It’s exhausting sometimes to try and change methodologies. Introducing new technology can cause so much headache that many passive leaders just shun it in favor of keeping the peace. reply subpixel 6 hours agorootparentIf my org is any measure of the truth, passive leadership isn’t a thing - despite the prevalence of passive leaders. reply robg 6 hours agoprevWhat we should remember about Al McDonald [is] he would often stress his laws of the seven R's,\" Maier says. \"It was always, always do the right thing for the right reason at the right time with the right people. [And] you will have no regrets for the rest of your life. reply jrexilius 6 hours agoparentThat is the key line from the whole piece. reply treprinum 5 hours agoparentprevEven following all that could have led to Challenger exploding (stochastic process with non-zero probability of a terminal failure), and leaving everyone with \"What did we do wrong?\" without any answer and full of regrets for the rest of their lives. reply EncomLab 7 hours agoprev\"Truth, Lies, and O-Rings\" is a fascinating (if sometimes tedious) book that should be at the top of any reading list for those interested in the Challenger disaster. For me one of the more interesting side-bar discussions are those around deciding to use horizontal testing of the boosters despite that not being an operational configuration. This resulted in flexing of the joints that was not at all similar to the flight configuration and hindered identification of the weaknesses of the original \"field joint\" design. reply nordsieck 6 hours agoparentInterestingly, we're still testing SLS SRBs[1] horizontally. https://www.youtube.com/watch?v=n-wqAbVqZyg --- 1. In case anyone doesn't know, they use the actual recovered Shuttle casings on SLS, but use an extra \"middle\" section to make it 5 sections in length instead of the Shuttle's 4 sections. In the future they'll move to \"BOLE\" boosters which won't use previously flown Shuttle parts. reply nraynaud 5 hours agorootparentI think the booster was redesigned after the accident, I guess/hope the opportunity was seized to make a design that would be less sensitive to orientation. reply nordsieck 5 hours agorootparent> I think the booster was redesigned after the accident That is correct. I believe they added: * An extra seal * A \"J-Leg\" carved into the insulation[1] that acts as a sort of pre-seal > I guess/hope the opportunity was seized to make a design that would be less sensitive to orientation. I guess, we'll see how things shake out. --- 1. https://www.nasaspaceflight.com/2020/12/artemis-1-schedule-u... reply bityard 4 hours agorootparentprevAre you saying that they are tested horizontally or that they are ONLY tested horizontally? (Very different things.) reply nordsieck 20 minutes agorootparent> Are you saying that they are tested horizontally or that they are ONLY tested horizontally? My understanding is that they are only hot fired horizontally. Presumably there are many tests done at the component level, although it's questionable whether it makes sense to call those tests horizontal or vertical at that point. reply breput 3 hours agoprevIt's also worth noting how the o-ring story was made public. There is the famous testimony by Richard Feynman[0], but the secret was that astronaut/commissioner Sally Ride leaked the story to another commissioner, who then suggested it to Feynman over dinner[1]. Neither Ride nor Kutyna could risk exposing the information themselves, but no would could question or impeach Feynman. [0] https://www.youtube.com/watch?v=raMmRKGkGD4 [1] https://lithub.com/how-legendary-physicist-richard-feynman-h... reply vouaobrasil 7 hours agoprevIt's a shame we don't have more engineers today that refuse to invent things because so many technological inventions today are being used to further the destruction of our planet through consumerism. Sadly, human society has a blind spot when it comes to inventions with short-term benefits but long-term detriments. I would love to see more programmers refusing to work on AI. reply sph 7 hours agoparent> I would love to see more programmers refusing to work on AI. Refusing to work on something is not newsworthy. I refuse to work on (or use) AI, ads and defence projects, and I'm far from being the only one. Though let who is free of sin throw the first stone, I now stand on a high horse after having worked in the gambling sector, and now ashamed of it, so I prefer to focus the projects themselves rather than the people and what they choose to do for a living. reply vouaobrasil 5 hours agorootparent> Refusing to work on something is not newsworthy. One person, no. A hundred, who knows. Ten thousand programmers united together not to work on something? Now we're getting somewhere. A hundred thousand? Newsworthy. reply kjkjadksj 1 hour agorootparentThe issue is quantifying this sentiment. How would you even identify programmers who are doing this? Yet another reason why software engineers really ought to organize their labor like a lot of other disciplines of engineering have done decades ago. Collective action like this would be more easily mustered, advertised, and used to influence outcomes if labor were merely organized and informed of itself. reply Tao3300 6 hours agorootparentprevI also refuse to work on the war machine, blockchain, or gambling. Unfortunately it looks like that might also be refusing to eat right now. We'll see how much longer my principles can hold out. Being gaslit into an unjustified termination has me in a cynical kind of mood anyway. Doing a little damage might be cathartic. reply doctor_eval 5 hours agorootparentI’ve been gaslit, I ended up walking away from my company. It was extremely painful. > Doing a little damage might be cathartic. Please avoid the regret. Do something kind instead. Take the high road. Take care of yourself. reply Tao3300 5 hours agorootparentKindness doesn't have any dev openings. reply doctor_eval 5 hours agorootparentOf course. But at least try to minimise the damage. Don’t do anything you’ll regret. reply Tao3300 5 hours agorootparentRegret right now would be letting the stress of unemployment rip my family apart. I've got maybe a handful of door-slamming \"what the fuck did you do all day then?\" rants that I can tolerate before I'm ready to sign on with Blockchain LLM O-Ring Validation as a Service LLC: We Always Return True!™ if it'll pay the bills and get my wife to stop freaking out. reply freeopinion 1 hour agorootparentIt probably doesn't help right now, but you should know you are not the only one in your situation. Perhaps it might help to write down your actual principles. Then compare that list with the real reasons you refuse some employment opportunities. I think you have already listed one big reason that isn't a high-minded principle. You want to make money. There may be others. It's always wonderful when you can make a lot of money doing things you love to do. It stinks when you have to choose between what you are exceptionally good at doing and what your principles allow. If only somebody could figure out how the talents of all the people in your situation could be used to restore housing affordability. Would you take a 70% paycut and move to Nebraska if it allowed you to keep all your other principles? As you say, kindness isn't hiring. I'd love to see an HN discussion of all the good causes that need founders. It would be wonderful to have some well known efforts where the underemployed could devote some energy while they licked their wounds. It might even be useful to have \"Goodworks Volunteer\" fill that gap in employment history on your resume. How do we get a monthly \"What good causes need volunteers?\" post on HN? reply immibis 4 hours agorootparentprevAnd this is how all unjust systems sustain themselves. You WILL participate in the injustice, or be punished SEVERELY. Why do the people doing the punishing want to punish you? Because they WILL participate in punishing, or be punished SEVERELY. People have wondered how so many people ever participated in any historical atrocity. This same mechanism is used for all of them. reply sneak 6 hours agorootparentprevAvoiding the use of AI is just going to get you lapped. There’s no benefit to your ideological goals in kneecapping yourself. There’s nothing morally wrong with using or building AI, or gambling. reply Tao3300 5 hours agorootparentThere's a lot baked into that thought, but I wanted to extract this part: > There’s nothing morally wrong with ... building... gambling. Say you're building a gambling system and building that system well. What does that mean? More people use it? Those people access it more? Access it faster? Gamble more? Gamble faster? It creates and feeds addiction. reply slumberlust 5 hours agorootparentI agree with you. It's also worth noting that this isn't unique to anything discussed here. EVERYONE has their line in the sand on a huge array of issues, and that line falls differently for a lot of people. Environment, religion, war, medicine; everything has a personal line associated with it. reply sneak 4 hours agorootparentprevLots of things create and feed addictions, including baking cookies. Let’s not confuse the issue. Just because you find something distasteful doesn’t mean it’s bad or morally problematic. reply Tao3300 35 minutes agorootparentI've never seen a homeless person in Atlantic City put his fist through an oven window because the cookies didn't come out right. reply vouaobrasil 5 hours agorootparentprevThe benefit is a clear conscience. reply datameta 5 hours agorootparentIn what context? Code generation? Art exploration? reply sph 2 hours agorootparentprevWake me up when AI is able to compete with a software engineer with almost two decades in the field. Hint: most of my consulting rate is not about writing fizzbuzz. Some clients pay me without even having to write a single line of code. reply sweettea 6 hours agorootparentprevI am curious why you avoid ads - personally I view them as a tremendous good for the world, helping people improve their lives by introducing them to products or even just ideas they didn't know existed. reply throwuxiytayq 6 hours agorootparentI tend to view ads as the perfect opposite of what you mentioned; it’s an enormous waste of money and resources on a global scale that provides no tangible benefit for anyone that isn’t easily and cheaply replaced by vastly superior options. If people valued ad viewing (e.g. for product decisions), we’d have popular websites dedicated to ad viewing. What we have instead is an industry dedicated to the idea of forcefully displaying ads to users in the least convenient places possible, and we still all go to reddit to decide what to buy. reply autoexec 2 hours agorootparent> If people valued ad viewing (e.g. for product decisions), we’d have popular websites dedicated to ad viewing. There was a site dedicated to ad viewing once (adcritic.com maybe?) and it was great! People just viewed, voted, and commented on ads. Even though it was about the entertainment/artistic value of advertising and not about making product decisions. Although the situation is likely to change somewhat in the near future, advertising has been one of the few ways that many artists have been able to make a comfortable living. Lying to and manipulating people in order to take more of their money or influence their opinions isn't exactly honorable work, but it has resulted in a lot of art that would not have happened otherwise. Sadly the website was plagued by legal complaints from extremely shortsighted companies who should have been delighted to see their ads reach more people, and it eventually was forced to shutdown after it got too expensive to run (streaming video in those days was rare, low quality, and costly) although I have to wonder how much of that came from poor choices (like paying for insanely expensive superbowl ads). The website was bought up and came back requiring a subscription at which point I stopped paying any attention to it. reply IggleSniggle 5 hours agorootparentprevWe do have such sites though, like Tom's Hardware or Consumer Reports or Wirecutter or what have you. Consumers pay money for these ads to reduce the conflict of interest, but companies still need to get their products chosen for these review pipelines. reply autoexec 2 hours agorootparentTom's Hardware and Consumer Reports aren't really about ads (or at least that's not what made them popular). they were about trying to determine the truth about products and see past the lies told about them by advertising. reply asoneth 4 hours agorootparentprevBack when I was a professor I would give a lecture on ethical design near the end of the intro course. In my experience, most people who think critically about ethics eventually arrive at their own personal ethics which are rarely uniform. For example, many years ago I worked on military AI for my country. I eventually decided I couldn't square that with my ethics and left. But I consider advertising to be (often non-consensual) mind control designed to keep consumers in a state of perpetual desire and I'd sooner go back to building military AI than work for an advertising company, no matter how many brilliant engineers work there. reply myrmidon 6 hours agorootparentprevTo me, ads are primarily a way to extract more value from ad-viewers by stochastically manipulating their behavior. There is a lot of support in favor. Consider: - Ads are typically NOT consumed enthusiastically or even sought out (which would be the cases if they were strongly mutually beneficial). There are such cases but they are a very small minority. - If product introduction was the primary purpose, then repeatedly bombarding people with well-known brands would not make sense. But that is exactly what is being done (and paid for!) the most. Coca Cola does not pay for you to learn that they produce softdrinks. They pay for ads to shift your spending/consumption habits. - Ads are an inherently flawed and biased way to learn about products, because there is no incentive whatsoever to inform you of flaws, or even to represent price/quality tradeoffs honestly. reply _kb 5 hours agorootparentprevProducts (and particularly ideas) can be explored in a pull pattern too. Pushing things—physical items, concepts of identity, or political ideology—in the fashion endemic to the ad industry is a pretty surefire way to end up with an extremely bland society, or one that segments increasingly depending on targeting profile. reply autoexec 2 hours agorootparentprevAds are most often manipulation, not information. They are pollution. reply wizzwizz4 6 hours agorootparentprevI also believe advertisements are useful! However, by this definition, the ad industry is not engaged in advertisement. reply Clubber 4 hours agorootparentprev>I am curious why you avoid ads - personally I view them as a tremendous good for the world, helping people improve their lives by introducing them to products or even just ideas they didn't know existed. I would agree with you if ads were just that. Here's our product, here's what it does, here's what it costs. Unfortunately ads sell the sizzle not the steak. That has been advertising mantra for probably 100 years. https://www.youtube.com/watch?v=UW6HmQ1QVMw reply BLKNSLVR 7 hours agoparentprevIf all the programmers working on advertising and tracking and fingerprinting and dark pattern psychology were to move into the field of AI I think that would be a big win. And that's not saying that AI is going to be great or even good or even overly positive, it's just streets ahead of the alternatives I mentioned. reply IggleSniggle 5 hours agorootparentIs it miles ahead? An engine that ingests a ridiculous amount of data to produce influence? Isn't that just advertising but more efficient and with even less accountability? reply ModernMech 6 hours agorootparentprevI feel like AI is going to be all those things on steroids. reply BLKNSLVR 5 hours agorootparentI'll reply here since your comment was first. AI has the potential to go in many directions, at least some of which could be societally 'good'. Advertising is, has always been, and likely always will be, societally 'bad'. This differentiation, if nothing else. (Yes, my opinion on advertising is militantly one sided. I'm unlikely to be convinced otherwise, but happy for, and will read, contrary commentary). reply Dove 2 hours agorootparentI don't think it's advertising that's inherently evil. Like government, it's a good thing, even a needed thing. People need laws and courts, and buyers and sellers need to be able to connect. It turns evil in the presence of corruption. Taking bribes in exchange for power. Government should never make rules for money, but for the good of the people. And advertising should never offer exposure for sale - exposure should only result from merit. Build an advertising system with integrity - in which truthful and useful ads are not just a minimum requirement but an honest aspiration and the only way to the top of the heap. Build an advertising system focused, not on exploiting the viewer, but on serving them - connecting them with goods and services and ideas and people and experiences that are wanted and that promote their health and thriving. I won't work on advertising as it's currently understood... I agree it's evil. But I'd work on that, and I think it would be a great good. reply kjkjadksj 1 hour agorootparentI used to think there were useful ads. But really, even a useful add is an unsolicited derailing of your thoughtspace. You might need a hammer, but did you really have to think about it right then? I think back to how my parents and grandparents got their goods before the internet. If they needed something they went to the store. If they were interested in new stuff that might be useful thats coming out, they'd get a product catalog from some store mailed to them. Is a product catalog an ad? Maybe, depending on how you argue the semantics, but its much more of a situation like going to a restaurant and browsing the menu and choosing best for yourself, vs being shown a picture of a big mac on a billboard every time you leave your home. reply Jensson 6 hours agorootparentprevYeah, Google, Facebook and Microsoft putting a massive fraction of their resources on AI is what already happened, but isn't really encouraging. reply ModernMech 1 hour agorootparentYeah they are the dark pattern, tracking, advertising l, privacy violating kings. Of course they’re going to keep doing all that “but with AI (TM)” reply hbossy 4 hours agoparentprevI will never forget the grumpy look on the face of a imperial tobacco representative on a job fair in my university years ago. No one was visiting their booth for anything except for silly questions about benefit package including cigarettes. reply dartos 7 hours agoparentprevIf only it were that easy. A lot of engineers in the US who are both right out of school and are on visas need to find and keep work within a couple months of graduation and can’t be picky with their job or risk getting deported. We have a fair number of indentured programmers. reply michaelt 6 hours agoparentprevSadly it's not enough for 99% of engineers to refuse to work on an unethical technology, or even 99.99% Personally I don't work on advertising/tracking, anything highly polluting, weapons technology, high-interest loans, scams and scam-adjacent tech, and so on. But there are enough engineers without such concerns to keep the snooping firms, the missile firms, and the payday loan firms in business. reply Frost1x 6 hours agorootparentOne issue we have is that economic pressures underly everything, including ethics. Ethics are often malleable depending on what someone needs to survive and given different situations with resource constraints, people are ultimately more willing to bend ethics. Now, there’s often limits to some flexibility and lines some simply will not cross, but survival and self preservation tends to take precedent and push those limits. E.g., I can’t imagine ever resorting to cannibalism but Flight 571 with the passengers stranded in the Andes makes a good case for me bending that line. I’d be a lot more willing to work for some scam or in high interest loans for example before resorting to cannibalism to feed myself and I think most people would. If we assure basic survival at a reasonable level, you might find far less engineers willing to work in any of these spaces. It boils down to what alternatives they have and just how firm they are on some ethical line in the sand. We’d pretty much improve the world all around I’d say. Our economic system doesn’t want that though, it wants to be able to apply this level of pressure on people and so do those who are highly successful who leverage their wealth as power. As such I don’t see how that will ever change, you’ll always have someone doing terrible things depending on who is the most desperate. reply kjkjadksj 59 minutes agorootparentprevThere are even engineers with such concerns working in these firms. They might figure that the missile is getting built no matter if they work there or not, so they might as well take the job offer. reply constantcrying 4 hours agoparentprevThe curse of technology is that it is neither good nor bad. Only in the way it is used t becomes one or the other. >I would love to see more programmers refusing to work on AI. That is just ridiculous. Modern neural networks are obviously an extremely useful tool. reply nasaeclipse 3 hours agoparentprevAs others have said, a big part of the problem is the need to eat. I have a family. I work for a company that does stuff for the government. I'd _rather_ be building and working on my cycling training app all day every day, but that doesn't make me any money, and probably never will. All the majority of us can hope for is to build something that helps people and society, and hope that does enough good to counteract the morally grey in this world. Nothing is ever black and white. reply dylan604 3 hours agoparentprevThe problem is that for every one that refuses, there's at least one that will. So standing on principles only works if the rest of the rungs of the ladder above you also have those same principles. If anywhere in the org above you does not, you will be overruled/replaced. reply Ekaros 2 hours agoparentprevI would wish lot more programmers refuse to work with surveillance and add tech... But nearly every site has that stuff on them... Goes to tell what are the principles of profession or in general... reply throwaway22032 6 hours agoparentprevI no longer work as a software developer because I feel that technology is ruining normal human interactions by substituting them in incomplete ways and making everyone depressed. I think we'd be better off making things for each other and being present and local rather than trying to hyperstimulate ourselves into oblivion. I'm just some dude though. It's not making it to the headlines. reply vouaobrasil 5 hours agorootparent> I'm just some dude though. It's not making it to the headlines. Doesn't have to be on headlines. Even just hearing that gives me a bit more energy to fight actively against the post-useful developments of modern society. Every little bit helps. reply tryauuum 5 hours agorootparentprevHow do you get money nowadays? reply ClumsyPilot 6 hours agoparentprev> I would love to see more programmers refusing to work on AI. This is not effective. Having a regulated profession that is held to some standards, like accountants, would actually work Without unions and without a professional body individual action won’t be achieving anything reply chris_t 4 hours agorootparentBut... accountants do work for AI companies, right? That doesn't seem like a good example. reply nradov 3 hours agorootparentprevSo do you think that people should be required to become members of a \"regulated profession\" before writing a VBA spreadsheet macro, or contributing to an open-source project? reply xeonmc 7 hours agoparentprev\"Yeah, but your scientists were so preoccupied with whether or not they could, that they didn't stop to think if they should.\" reply htrp 6 hours agoprev> He neglected to say that the approval came only after Thiokol executives, under intense pressure from NASA officials, overruled the engineers. Sounds kinda familiar? reply djeastm 5 hours agoparentA story as old as time. reply Chris2048 6 hours agoparentprevI wonder how the process even allows this. An approval from the executives of the company shouldn't be worth anything. reply andrei-akopian 6 hours agoprevI have an unclarity with this situation. How much of him being a hero is a coincidence? Did he refuse to sign the previous launches? Did NASA have reasons to believe that the launch could be successful? How much of a role does probability play here. I mean if someone literally tells you something isn't safe, especially the person who made it, you can't tell him it will work. There is somekind of bias here. reply _kb 5 hours agoparentOf course there's bias. If he had rubber-stamped it there would be no story to tell. His decision would have been questioned after the fact, he would defer to information from levels below, and this would recurse until responsibility had dissipated beyond and any personal attribution. The same pattern happens in every org, every day (to decisions of mostly lesser affect). The key point—at least from my read—were the follow up actions to highlight where information was intentionally ignored, prevent that dispersion of responsibility, and ensure it didn't happen again. reply gwbas1c 4 hours agoparentprevThe challenger disaster was a case study when I was in school: The important lesson is about human psychology, and why it's important to not speak up when something is dangerous. Basically, the \"powers that be\" wanted the launch and overruled the concerns of the engineers. They forced the launch against better judgement. (Think of the, \"Oh, that nerd is always complaining, I'm going to ignore them because they aren't important,\" attitude.) reply nraynaud 5 hours agoparentprevThe article is a bit weird, he refused to sign a form inside a private company. But the private company presented a signed form to NASA (signed by higher-up’s). So NASA probably didn’t look closely into the engineering, in particular when launch is tomorrow. reply kop316 5 hours agorootparentI got to hear him recount the story, and yeah the article is weird. The form he talked about was one that, if not signed, would mean that the launch would not happen. I can't remember if it was an internal form or not, but it doesn't really matter in that context. Since NASA needed that form signed, he was under intense pressure to actually sign it both by NASA and his company. Someone else from the company not on site signed it. reply constantcrying 4 hours agoparentprevTo be completely honest I think you are somewhat naive. I have seen organizations push through decisions, which were obviously bad, in fact nearly everyone on the lower levels agreed that the goal of the decision was unachievable. But of course that didn't stop the organization. > I mean if someone literally tells you something isn't safe, especially the person who made it, you can't tell him it will work. You literally can. reply vntok 6 hours agoparentprevSomething can work and not be safe at the same time. reply cushychicken 6 hours agoprevI got to eat lunch with Allan Macdonald in college. I was an IEEE officer and we hosted him for a talk at Montana State, so I got to take him out for lunch before his talk. Dude got a lunch beer without a second though. (My man!) He then gave a talk that afternoon talking about interrupting a closed session of the Challenger commission to gainsay a Thiokol VP. The VP in question testified to Congress that he wasn't aware of any launch risks. Macdonald stood up, went to the aisle, and said something to the effect of \"Mr. Yeager, that is not true - this man was informed of the risks multiple times before the launch. I was the one that told him.\" (He was addressing Chuck Yeager, btw. Yeah, that Chuck Yeager.) No mean feat to have the stones to interrupt a congressional hearing stacked with America's aviation and space heavyweights. reply smsm42 2 hours agoprevWhat is missing here for me is who were the anonymous \"executives\" that overruled Mcdonald (and others) and tried to punish him? Did they suffer any consequences for actions that cost lives and for the coverup? reply quacked 1 hour agoprevI'm late to the party, but I work as a NASA contractor and have just recently been reading \"Truth, Lies, and O-Rings\" by Mr. McDonald. Something that I find really frustrating is that it seems that there's an international \"caste\" of honest engineers who are ready, and have been ready for centuries if not millenia, to pull the metaphorical trigger on advancing human society to the next level. International rail systems, replacing all electrical generation with nuclear, creating safe and well-inspected commercial airplanes, etc. Blocking that \"caste\" from uniting with each other and coordinating these projects are the Old Guard; the \"local area warlords\", although these days they may have different titles than they would have a thousand years ago. These people do not speak a language of technical accuracy, but rather their primary guiding principles are personal loyalty, as was common in old honor societies. They introduce graft, violence, corruption, dishonesty, and personal asset capture into these projects and keep them from coming to fruition. They would not sacrifice their lifestyles in order to introduce technical excellence into the system they're charged with managing, but instead think more about their workload, their salary, their personal obligations to their other (often dishonest) friends, and their career tracks. It wouldn't even occur to me to worry more about a promotion than than the technical merit of a machine or system I was engaged with. I would never lie about something myself a colleague of mine said or did. For those reasons I will never be particularly competitive with the people who do become VPs and executive managers. How many different people around the world, and especially that are on HackerNews, are in my exact situation? With the right funding and leadership could all quit our stupid fucking jobs building adtech or joining customer databases together or generating glorified Excel spreadsheets and instead be the International Railway Corps, or the International Nuclear Corps. And yet since we can't generate the cashflow necessary to satisfy the Local Area Warlords that own all the tooling facilities and the markets and the land, it will never be. reply omega3 5 hours agoprevThere is a good movie about the Challenger disaster and the follow up investigation from the pov of Feynman: https://en.wikipedia.org/wiki/The_Challenger_Disaster reply zensnail 6 hours agoprevIconoclasts like Robert are vital to get us to a stage one civ. May he rest in peace. Appreciate the post. reply mihaic 5 hours agoprevRest in peace Allan. As much as his action were admirable, the most shocking thing about that story was how the politicians rallied to protect him after his demotion, forcing his company to keep and actually promote him. That's why I get both sad and angry when I hear the new mantra of \"Government can't do anything, the markets have to regulate that problem.\" reply capitainenemo 4 hours agoparentI mean... his company was sitting on a lucrative government contract for an agency that was working hard to cover up a failure. It's fortunate that in this case distribution of power (and the shocking nature of the failure) ensured that the right thing happened, but I see a corporate and government management colluding to maintain their positions. Distribution of power is definitely important though, whether public or private. People concerned about government abuse is due to the fact that due to its nature, government power structures are more often centralised and without competitors by definition. There are monitors but they are often parts of the same system. reply cptskippy 4 hours agoparentprev> the new mantra of \"Government can't do anything, the markets have to regulate that problem.\" That's been the conservative line for 35+ years. How is that new? reply ToucanLoucan 4 hours agorootparentI think more like 70 years at this point. It's been SOP for the conservatives to get elected to govern, make government worse at every turn while enriching themselves and their friends, and then turning around to the public and being like \"look how badly this works, clearly we need to cut taxes since it isn't working\" and rinse and repeat until every institution in the world is borderline non-functioning. reply kbolino 2 hours agorootparentIt was Jimmy Carter and not Ronald Reagan who scrapped the civil service competency exams. Government getting worse has been a two-party affair for quite some time. No one has any incentive to fix it, and the system is so vast, so complex, and so self-serving that no one even has the power to fix it (as things stand). reply ToucanLoucan 39 minutes agorootparentThe Democrats in America are highly conservative. Not as conservative as the Republicans, but still very conservative. We don't have a left and a right here, we have a hard right and a center right. reply kbolino 11 minutes agorootparentCertain \"hard right\" parties like the PAP in Singapore and the LDP in Japan have placed a competent civil service at the forefront of their policies. Though in many ways, the US may appear more conservative than its \"peers\", in other ways, it appears more liberal. reply neilv 5 hours agoprev> Allan McDonald leaves behind his wife, Linda, and four children — and a legacy of doing the right things at the right times with the right people. It sounds like the most noteworthy part of his legacy is attempting to do the right thing, but with the wrong people. I think this is meaningful to mention, because saying to do \"the right things, at the right time, with the right people\" is easy -- but harder is figuring out what that really means, and how do you achieve that state when you have incomplete control? reply noisy_boy 4 hours agoparentHe had incomplete control but did the right thing (to refuse to let the risk slide) at the right time (before the launch). You don't need to have full control to do this. > but harder is figuring out what that really means I think it is quite clear except the part about \"right people\"; if the people around you are not right, I would guess it is even more important to do the right thing. Obviously this comes at at a (potentially great) cost which is why it is easier said than done and why his actions are so admirable. reply neilv 2 hours agorootparent\"The right people\" is difficult. Working with NASA would seem one of the better bets. For startup founders, you can try to hire \"the right people\". (And share the equity appropriately.) For job-seekers, when you're interviewing with them, you can ask yourself whether they're \"the right people\". (And don't get distracted by a Leetcode hazing, in what's supposed to be collegial information-sharing and -gathering by both parties.) reply rawgabbit 7 hours agoprevMcDonald was my hero as a young engineering student. The miracle was that he was exonerated. reply tejohnso 3 hours agoprev> Morton Thiokol executives were not happy that McDonald spoke up, and they demoted him. And then all of their government contracts should have been revoked. reply WhitneyLand 4 hours agoprevWhich executive pressured the engineers, was there any accountability? reply nandgate10 6 hours agoprevNow that OSS projects like a certain popular dynamic language have been taken over by corporations, criticism like security or performance issues are forbidden as well and punished. (One corporation though seems to withdraw from that language due to the attitude of the project and its representatives.) reply mablopoule 4 hours agoparentHonestly, you're either telling too much or too little. Could tell what are the precise language / corporation / project, if you're comfortable with that of course? reply Anotheroneagain 5 hours agoprevIt's sad to see the decline of civilization, and how far back basic principles were not understood, and turned into a cargo cult. The point why somebody had to sign something to approve it was exactly that he had the option to not sign it in case that there was a problem. But even then, it was seen as a job to be done, that you either do, or fail to do. reply d--b 6 hours agoprevOk, cool, but what the hell happened? They had a guy in charge of signing-off the launch, he didn't sign off because of 3 problems he identified, and they still launched. wtf? reply coldcode 6 hours agoparentThe engineers were overruled by the executives because NASA was pissed at the company for messing up their plans. reply ohmyiv 6 hours agoparentprevFrom the article: (During the hearing) > The NASA official simply said that Thiokol had some concerns but approved the launch. He neglected to say that the approval came only after Thiokol executives, under intense pressure from NASA officials, overruled the engineers. reply lancesells 6 hours agoparentprevThis sounds like an issue that's still around. reply globalnode 5 hours agoprevnowadays you have an unlucky accident if youre a whistleblower, lucky he wound up getting a promo for it (after being demoted). reply 7e 4 hours agoprevI don't relish all of the issues which will eventually surface with SpaceX's Starship, which makes Space Shuttle development look like a paragon of high quality development practices. Starship is built in a metaphorical barn with a \"fuck around and find out\" attitude. reply roelschroeven 2 hours agoparentI don't think that's quite the case. SpaceX's method is more \"release early, release often\", and find (and solve!) issues early on. Traditional space companies on the other hand use a very rigid waterfall method. SpaceX's method is not \"fuck around and find out\". It's design, find out, iterate. From what I can tell from the outside, it seems very reasonable. reply kbolino 2 hours agoparentprevThe early manned space programs at USAF/NASA were a lot more cavalier than the shuttle program. reply fhub 2 hours agoparentprevThat metaphorical barn is run by Kathy Lueders. Look her up and it might soften your thinking a bit. reply alecco 6 hours agoprev(2021) reply ChrisMarshallNY 6 hours agoprev> McDonald became a fierce advocate of ethical decision-making My hero, but also Don Quixote. I'm a huge believer in Personal Integrity and Ethics, but I am painfully aware that this makes me a fairly hated minority (basically, people believe that I'm a stuck-up prig), especially in this crowd. I was fortunate to find an employer that also believed in these values. They had many other faults, but deficient institutional Integrity was not one of them. reply Aurornis 6 hours agoparent> I'm a huge believer in Personal Integrity and Ethics, but I am painfully aware that this makes me a fairly hated minority (basically, people believe that I'm a stuck-up prig), This doesn’t match my experience at all. In my experience, the average person I’ve worked with also believes in personality integrity and is guided by a sense of ethics. One company I worked for started doing something clearly unethical, albeit legal, and the resulting backlash and exodus of engineers (including me) was a nice confirmation that most people I work with won’t tolerate unethical companies. I have worked with people who take the idea of ethics to such an unreasonable extreme that they develop an ability to find fault with nearly everything. They come up with ways to rationalize their personal preferences as being the only ethical option, and they start finding ways to claim things they don’t like violate their personal integrity. One example that comes to mind is the security person who wanted our logins to expire so frequently that we had to log in multiple times per day. He insisted that anything less was below his personal standards for security and it would violate his personal integrity to allow it. Of course everybody loathed him, but not because they lacked personal integrity or ethics. If you find yourself being a “hated minority” or people thinking you’re a “stuck up pig” for having basic ethics, you’re keeping some strange company. I’d get out of there as soon as possible. reply ChrisMarshallNY 5 hours agorootparent> keeping some strange company Actually, that's this community. I do understand. Money is the only metric that matters, here, as it's really an entrepreneur forum. Everyone wants to be rich, and they aren't particularly tolerant of anything that might interfere with that. But I'm not going anywhere. It's actually fun, here. I learn new stuff, all the time. reply ben_w 2 hours agorootparent> Money is the only metric that matters, here, as it's really an entrepreneur forum. Everyone wants to be rich This place is surprisingly mixed in that regard given its origin; a significant number of comments I see about Apple, about OpenAI, about Paul Graham, are essentially anti-capitalist. The vibe I get seems predominately hacker-vibe rather than entrepreneur-vibe. That said, I'm also well aware of the \"orange site bad\" meme, so this vibe I get may be biased by which links' I find interesting enough to look at the discussions of. reply ChrisMarshallNY 2 hours agorootparentYeah, it was a snarky comment, and not my proudest moment, but it does apply to a significant number of folks. I tend to enjoy the contributions from folks that don't have that priority. The demoralizing part, is folks that are getting screwed by The Big Dogs, and totally reflect the behavior; even though TBD think of them as \"subhuman.\" reply krisoft 4 hours agorootparentprev> Money is the only metric that matters, here Says who? Did I agree to that when I subscribed? > Everyone wants to be rich, Everyone? Like me too? Tell me more about that. You in an earlier comment said that people believe that you are \"a stuck-up prig\". Are you sure it is due to your moral stance, and not because you are judgemental, and abrasive about it? Perhaps if you would be less set in your mind about how you think everyone is you wouldn't come through as \"a stuck-up prig\". Maybe we would even find common grounds between us. reply layer8 4 hours agorootparentprevHN is not really a community. reply ChrisMarshallNY 4 hours agorootparentI believe that it is. In my opinion and experience, any group of humans, interacting, on a regular basis, in a common venue, becomes a community. I guess that it is a matter of definition. I treat it as if it were a community, and that I am a member of that community, with rights and Responsibilities, thereof. I know that lots of folks like to treat Internet (and, in some cases, IRL) communities as public toilets, but I'm not one of them. I feel that it is a privilege to hang out here, and don't want to piss in the punch bowl, so I'm rather careful about my interactions here. I do find it a bit distressing, to see folks behaving like trolls, here. A lot of pretty heavy-duty folks participate on HN, but I guess the casual nature of the interactions, encourages folks to lose touch with that. I think that it is really cool, that I could post a comment, and have an OG respond. I suspect that won't happen, too often, if I'm screeching and flinging poo. reply justin_oaks 3 hours agorootparentprevJust like in-person communities, you'll have general consensus on some ideas and fierce disagreement in others. You'll have people who are kind and those who are hateful. You can identify that there may be a trend within a community without declaring that everyone in the community thinks the exact same way. And you could also be wrong about that trend because the majority is silent on the issue and you bump up against the vocal minority. Perhaps you can elaborate on what a community is, and how HN differs from one. reply layer8 2 hours agorootparentThe topical interests, general characteristics, experiences and opinions of HN members are too diverse to qualify as a community, IMO. There may be subsets that could qualify as a community, and if you only look at certain kinds/topics of submissions it might feel like one, but they are mixed within a larger heterogeneous crowd here. reply ChrisMarshallNY 54 minutes agorootparentI feel that a community can def be heterogenous AF. I participate in exactly that type of (IRL) community, and it is worldwide. It does require some common focus, and common agreement that the community is important. I do believe that we have those, here. The \"common focus\" may not be immediately apparent, but I think everyone here shares a desire to be involved in technology; which can mean a few things, but I'll lay odds that we could find a definition that everyone could agree on. It is possible. I guarantee it. reply justin_oaks 1 hour agorootparentprevThanks, that clarifies a lot. reply tedivm 6 hours agorootparentprevI've left two companies over ethical concerns, but it's not as easy for most people implied here. Losing income can be challenging, especially if the industry is in a downturn. reply justin_oaks 3 hours agorootparentOut of curiosity, did you leave those companies because the company's core business was unethical (or veered that direction over time), because leadership was generally unethical, or because specific incidents that forced your hand? At a previous job I saw unethical choices made by my boss, but the company as a whole wasn't doing anything wrong. One of my coworkers was asked to do something unethical and he refused, but he wasn't punished and wasn't forced to choose between his ethics and the job. reply tedivm 2 hours agorootparentEvery time I had to leave for ethical reasons it was a leadership thing, mostly relating to how they treated other employees. For instance, I joined a company that advertised itself as being fairly ethical (they even had a \"no selling to military\" type policy). However, after joining it was apparent that this wasn't the case. They really pushed transparent salaries, but then paid me way more than anyone else. There was a lot of sexism as well: despite one of my colleagues being just as skilled as I am, this colleague was given all the crap work because leadership didn't think they were as capable as I was. There was a lot of other stuff as well, but that's the big summary. I left after nine months. The other company was similar, but it wasn't nearly as obvious at first. Over time it became very apparent that the founders cared more about boosting their own perception in the industry than they did the actual startup, and they also allowed the women in the company to be treated poorly. This company doesn't exist anymore. I should mention that these were all startups I worked at, and I was always fairly highly positioned in the company. This meant I generally reported directly to the founders themselves. If it was something like a middle management issue I'd have tried to escalate it up to resolve it before just leaving, but if that doesn't work I'm financially stable enough to just leave. rep",
    "originSummary": [
      "Allan McDonald, who passed away at 83, was a pivotal figure in the Challenger space shuttle disaster, known for refusing to approve the launch due to safety concerns.",
      "After the disaster, McDonald exposed NASA's cover-up, revealing that engineers had opposed the launch due to freezing temperatures affecting the O-rings, leading to his temporary demotion.",
      "McDonald later led the redesign of the booster rockets, co-authored a definitive account of the disaster, and became an advocate for ethical decision-making in engineering."
    ],
    "commentSummary": [
      "Allan McDonald refused to approve the Challenger launch due to safety concerns, revealing a cover-up by NASA and Morton Thiokol executives.",
      "Despite intense pressure, McDonald and other engineers were overruled by higher-ups, leading to the Challenger disaster.",
      "McDonald later advocated for ethical decision-making, highlighting the tension between technical accuracy and managerial pressure in high-stakes projects."
    ],
    "points": 445,
    "commentCount": 262,
    "retryCount": 0,
    "time": 1718969028
  },
  {
    "id": 40746804,
    "title": "Ladybird browser spreads its wings",
    "originLink": "https://lwn.net/Articles/976822/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us Edition Return to the Front page User: Password:| Subscribe / Log in / New account Ladybird browser spreads its wings Did you know...? LWN.net is a subscriber-supported publication; we rely on subscribers to keep the entire operation going. Please help out by buying a subscription and keeping LWN on the net. By Joe Brockmeier June 7, 2024 Ladybird is an open-source project aimed at building an independent web browser, rather than yet another browser based on Chrome. It is written in C++ and licensed under a two-clause BSD license. The effort began as part of the SerenityOS project, but developer Andreas Kling announced on June 3 that he was \"forking\" Ladybird as a separate project and stepping away from SerenityOS to focus his attention on the browser completely. Ladybird is not ready to replace Firefox or Chrome for regular use, but it is showing great promise. Kling started working on SerenityOS in 2018 as a therapy project after completing a substance-abuse rehabilitation program. The SerenityOS name is a nod to the serenity prayer. Prior to working on the project, he had worked on WebKit-based browsers at Apple and Nokia. Eventually he made SerenityOS his full-time job, and funded the work through donations, sales of SerenityOS merchandise, and income from YouTube. (Kling posts monthly updates to his YouTube channel about Ladybird, as well as hacking videos where he walks through working on various components in the browser, such as the JavaScript JIT compiler.) Taking flight Kling announced the Ladybird project in September 2022. He said that the project started while creating a Qt GUI for SerenityOS's LibWeb browser engine. He decided to target Linux as well as SerenityOS so it would be easier for people to work on and debug while in Linux. In the post announcing his intent to work solely on Ladybird, he noted that he had been focusing all of his attention on the Linux version of Ladybird. With that realization, he decided to step down as \"benevolent dictator for life\" (BDFL) of SerenityOS so its development would not be held back: Before anyone asks, there is no drama behind this change. It's simply recognizing that there have been two big projects packed uncomfortably into a single space for too long, and I'm doing what I believe will make life better for everyone involved. Ladybird's governance is similar to SerenityOS. Kling is the BDFL, with a group of maintainers (currently ten) who can approve and merge pull requests. The contributing guide notes that maintainership is \"by invitation only and does not correlate with any particular metric\". Project development discussions are held on a Discord server (account required). Now independent, Ladybird has dropped SerenityOS as a development target, and has moved to its own GitHub repository. In addition, Kling has relaxed his self-imposed policy of excluding \"not invented here\" (NIH) code that had applied to SerenityOS, which means that the Ladybird project will be able to make use of existing libraries rather than writing from scratch. Comparing the README file in the standalone Ladybird repository against the README file in the SerenityOS repository, the goal has evolved from creating \"a standards-compliant, independent web browser with no third-party dependencies\" to developing an independent browser \"using a novel engine based on web standards\". The changes to the section that enumerates the core libraries for Ladybird provide some hints about Kling's plans to use existing libraries rather than continuing to reinvent the wheel. The core support libraries for the project include homegrown libraries for cryptography, TLS, 2D-graphics rendering, archive-file format support, Unicode, as well as audio and video playback. In the pre-fork documentation, they are described as alternatives to other software. For example, Ladybird's TLS (LibTLS) and cryptography (LibCrypto) libraries are \"Cryptography primitives and Transport Layer Security (rather than OpenSSL)\". The \"rather than\" language has been removed in the journey to the standalone repository, and the LibSQL library from SerenityOS has already been stripped out in favor of sqlite3. In a discussion in the project's Discord instance on June 5, Kling indicated that font rendering would likely be replaced with a third-party library. A user asked on June 6 what would determine whether a component would be developed in-house versus using a third-party library. Kling responded that if it implements a web standard, \"i.e DOM, HTML, JavaScript, CSS, Wasm, etc. then we build it in house.\" Otherwise, the project would look to alternatives \"unless we believe we can build something better ourselves\". Status Ladybird is still in early development (\"pre-alpha\") today. It currently runs on Linux, macOS, and other UNIX-like operating systems. It's also possible to use on Windows with Windows Subsystem for Linux (WSL) version 2, but there appears to be no effort to target Windows independently at this time. At the moment, the project does not provide binaries for any platform. Interested users will need to grab the source and follow the build instructions. Users will need GCC 13+ or Clang 17, and Qt6 development packages to play along at home. Ladybird compiles and runs on, for example, Fedora 40 without a problem, but it is a long way from being suitable for regular use. One might expect that the browser would be more usable with sites with simpler layouts and little to no JavaScript (e.g. LWN) than those with complex layouts and a fair amount of JavaScript (e.g. GitHub). However, this isn't always the case—Ladybird rendered GitHub and many other sites well, if slowly. Browsing LWN anonymously worked well, but logging into LWN, however, consistently proved to be too much for the application. Each time, it basically froze on the front page and clicking links to articles did nothing. Somewhat ironically, it was not possible to log into Discord using Ladybird. It does a fair job of rendering pages, but speed and stability are still wanting. Each Ladybird tab has its own render process, which is sandboxed as a security measure to prevent any malicious pages from affecting the rest of the system. However, it doesn't seem to suffice to keep a single page from crashing the browser entirely. That's to be expected from a project that's still considered pre-alpha, though. The current feature set is, not surprisingly, minimal. Ladybird has a URL/search bar, reload, tabs, can zoom in/out on content, take screenshots, and (of course) has backward and forward navigation. It does not, however, have bookmarks, a history display, extensions, password management, printing, or even the ability to save an image. WebRTC does not seem to be supported yet. CSS support seems relatively robust. Ladybird passes 100% of the CSS Selectors tests for levels 1-3, for example, using this test. It gets a 53% score for level 4, while Firefox gets 71%, so not a terrible showing at all. JavaScript support seems solid, but slow: the examples here work, but they load slowly. On the other hand, Ladybird does have tools for developers, such as inspectors for the document object model (DOM) tree and accessibility trees, as well as the ability to create dumps of various things: the DOM tree and layout tree, computed styles, and so forth. It also has the ability to spoof the User-Agent sent by the browser so that testers can try to get around sites that refuse to work with \"unknown\" browsers. However, toggling the User-Agent wasn't enough to get past Google's gatekeeping to sign into Gmail—but it's unclear if that meant Ladybird wasn't sending the string correctly or if Google is using other means to fingerprint non-approved browsers. Suffice it to say, Ladybird is not ready for mainstream use but it does show potential. In the past month, the project has had more than 880 commits from 49 authors. If the project maintains that kind of momentum, or picks up steam, it could become a usable alternative to mainstream browsers before too long. (Log in to post comments) Congratulations Posted Jun 7, 2024 18:50 UTC (Fri) by riking (subscriber, #95706) [Link] Congratulations on what sounds like will be a successful community split. Always tough to do1 Where is this going to be 10 years from now? Posted Jun 7, 2024 19:04 UTC (Fri) by python (subscriber, #171317) [Link] (17 responses) I am waiting for the day that we see a serious attempt at a browser written in rust. I fear that it might be uneconomical to develop a modern browser in \"lower level\" languages like C++. Valgrind/AddressSanitizer/UndefinedBehaviorSanitizer is no substitute for automated rigorous (mathematically proven) enforcement of program structure/logic/soundness by the compiler. Not to mention the friction posed by not having easy, immediate, access to the sort of ecosystem of third party libraries that we see with npm or cargo. I looked at the source code for LadyBird and I am surprised that they are not using C++20 modules (it's been supported by clang for 5 years). I always found maintaining header files (eg. having to write the function definitions twice, once in the .h and once in the .cpp file, etc.) to be a great bother. I can only assume they have much more pressing issues like implementing missing features and trying to comply with all of the modern web standards. As I've watched Firefox, and Microsoft try to compete with Chrome - I find it progressively more difficult to get excited about attempts at new browsers when there appears to be nothing terribly new or innovative about how or what they are trying to achieve. It's a lot of work to build even a small portion of the machinery underlying a web browser like Firefox or Chrome. It will probably take a decade for something like LadyBird to be competitive (assuming that the web \"remains still enough\" to catch up). I fear that they might get mired down in the complexity of what they are trying to achieve (with their chosen tools, and limited manpower) before they ever reach that goal. Where is this going to be 10 years from now? Posted Jun 7, 2024 19:11 UTC (Fri) by atai (subscriber, #10977) [Link] (1 responses) A suggestion for you: write a browser in Python. It is surprising in your comment that the world Python appeared only once: in your name Where is this going to be 10 years from now? Posted Jun 8, 2024 15:13 UTC (Sat) by ejr (subscriber, #51652) [Link] That would take Python full circle, particularly if you called it something related to Grail. Where is this going to be 10 years from now? Posted Jun 7, 2024 19:39 UTC (Fri) by willy (subscriber, #9762) [Link] (3 responses) It's funny you say that, given that Rust started as a Mozilla project. https://wiki.mozilla.org/Oxidation is a bit outdated (not modified in 4 years) but clearly shows how Firefox is being gradually rewritten in Rust. Where is this going to be 10 years from now? Posted Jun 7, 2024 19:47 UTC (Fri) by atnot (subscriber, #124910) [Link] There's also servo, which is alive again now, albeit with new goals (serving as an embedded web engine for embedded devices and things people might use Electron for today) Where is this going to be 10 years from now? Posted Jun 9, 2024 7:23 UTC (Sun) by dilinger (subscriber, #2867) [Link] (1 responses) Chromium is also having components rewritten in rust. As of chromium 120, rustc is now required for building it, and they've started dropping c++ code. Where is this going to be 10 years from now? Posted Jun 21, 2024 10:00 UTC (Fri) by pjmlp (subscriber, #168573) [Link] Rust is only allowed for newer third party dependencies, no rewrite is yet taking place. ==> How Chromium Will Support the Use of Rust https://security.googleblog.com/2023/01/supporting-use-of... C++20 module misconception clearing Posted Jun 7, 2024 20:34 UTC (Fri) by mathstuf (subscriber, #69389) [Link] (5 responses) > I looked at the source code for LadyBird and I am surprised that they are not using C++20 modules (it's been supported by clang for 5 years). Clang has not supported C++20 modules for 5 years. 2 is probably more accurate. MSVC is probably close to 3. You're thinking of \"Clang modules\", which served as a prototype for C++20's modules, but not what got standardized (standard modules scale far better according to (ISO-)reported benchmarks). Even with that, (released) build system support for C++20 modules only landed in CMake of October 2023 (FD: I'm the main developer of that support). Now that it does exist, I expect many issues to need fixing over the next few years before it is \"battle hardened\". Other build systems do have support (build2, xmake), but others are still pending (Meson) or have no known progress (autotools, Tup). FWIW, I had a prototype that worked with CMake and GCC (both locally patched) in 2019, but what landed is a far better situation than what I had then. > I always found maintaining header files (eg. having to write the function definitions twice, once in the .h and once in the .cpp file, etc.) to be a great bother. Sure, you *could* do one-file modules, but build performance will vastly prefer to continue having the split (just now between module interface and implementation units). The main benefit is that you're now in control of what interface you provide consumers via the `export` keyword instead of \"whatever the preprocessor happened to come across\" while you included headers for what you needed as well. C++20 module support in build systems Posted Jun 7, 2024 21:12 UTC (Fri) by madscientist (subscriber, #16861) [Link] (4 responses) Have people decided on a model for supporting C++ modules in build tools? This is probably OT for this thread but last time I checked there were two models being considered: either some tool parses the module files and generates module description information, which can then be considered by the build system, which I think is the way CMake has gone (it's good for them because they already have a separate \"configure the build\" step). There was also an idea of using an LSP-like model, where a separate server process would keep the module dependency information and respond to requests from clients at runtime. It seems the first has the most momentum behind it. Maybe the second idea has fallen by the wayside. C++20 module support in build systems Posted Jun 7, 2024 21:45 UTC (Fri) by NYKevin (subscriber, #129325) [Link] (1 responses) The CMake approach you describe sounds a lot like something that already exists for the specific combination of Bazel+Go: https://github.com/bazelbuild/bazel-gazelle The README says it is extensible \"to support new languages and custom rule sets.\" I wonder if it can be extended for C++ modules? Then setting up a C++ project (that uses modules) in Bazel should be much easier. C++20 module support in build systems Posted Jun 8, 2024 3:23 UTC (Sat) by mathstuf (subscriber, #69389) [Link] Bazel support is here (I've given feedback and also managed to have Google reconsider external contributions for it instead of waiting for Google to solve it internally and codedump it on the community years from now): https://github.com/bazelbuild/bazel/pull/19940 I can't say it with 100% confidence, but I don't think Gazelle can handle C++ modules reliably with the existing interfaces. See my other reply in this subthread for details on (CMake's) C++ module compilation strategy (to avoid splitting the discussion). C++20 module support in build systems Posted Jun 8, 2024 4:30 UTC (Sat) by mathstuf (subscriber, #69389) [Link] (1 responses) > Have people decided on a model for supporting C++ modules in build tools? There are 2 plausible strategies AFAIK (\"explicit\" and \"implicit\"). I consider only 1 (\"explicit\") viable in the grand scheme of the state of the C++ ecosystem where even file extensions cannot be agreed upon. There are tradeoffs of each. For CMake, I chose the one that gives the most reliable results with the widest support because I detest tracking down heisenbuild bugs and CMake ends up having to support all kinds of things anyways. > some tool parses the module files and generates module description information, which can then be considered by the build system, which can then be considered by the build system, which I think is the way CMake has gone (it's good for them because they already have a separate \"configure the build\" step). This is inaccurate. CMake does this during the build, not during \"configure\". If it were done during the configure step, any change to a C++ module-using or -providing source would require a rerun of the configure to update the build graph. Note that it must also be performed at build time in order to support generated sources (that don't exist to scan at configure time). This is obviously not optimal. Some definitions (for clarity): - build system: a model of a project that represents artifacts and the dependencies between them (e.g., CMake, Bazel, Meson) - build tool: a tool which executes a DAG representing a build system's actions to perform the necessary (e.g., Bazel, make, ninja) - BMI: \"binary module interface\"; a representation of a module interface that is used when importing the module - CMI: \"compiled module interface\"; same as BMI; preferred by some because they might not be \"binary\", but ASCII - module interface unit: a source file with `export module X;` or `[export] module X:partition;`; may be imported (assuming language and build system visibility rules allow it) - module implementation unit: a source file with `module X;` that provides implementations for module interfaces that have `module X` (with or without a partition name; implementation units never mention partitions[0]) - explicit module build: the compiler is told the exact BMI file to use for each module it reads or writes - implicit module build: the compiler is given search paths to look for or to create BMIs by name (note that this includes when a build system just uses the module name as a key to look them up rather than considering the context of the importing source file: is the requested module visible? is it the right configuration?) For background, this paper[1] I presented to ISO C++ which describes how CMake supports Fortran modules (which are isomorphic to C++ modules for build graph purposes). The core issue is that the module name has no (forced) relation to the filename on disk. So one needs to determine the dependencies between compilations *dynamically*. This is distinct from \"discovered\" dependencies reported by the likes of `-showIncludes` or `-MF` where the files that are discovered during the course of the tool's execution are reported so that if any change, the tool can be reexecuted. The main point here is that they can be discovered during the first execution (since the output doesn't exist, it needs to run anyways). Modules are different because at the point of `import M;` during translation we need the BMI of M to continue compilation as the next line might use a type from that module. The compiler needs to know where that BMI is and is the primary goal of the build system for modules: to provide BMI paths during compilation and ensure they're ready when compiling anything that needs them. Dynamic dependencies instead are dependencies between entities expressed in the *content* of the sources and nowhere else. To support this, the contents need to be \"scanned\" to report what is needed during compilation and generated during the compilation of the source itself. This is reported in the P1689R5[2] format that is meant to describe these things. These dependency files (CMake uses the `.ddi` extension: \"dynamic dependency information\") are then given to a \"collator\" that reads them, the collated information of any dependent libraries, and some details from the CMake, to do the following: - see file X makes module P - sees file Y needs module P - writes out a snippet for the build tool (ninja, make) to learn that Y's object rule depends on X's object rule (this is what the details from CMake are needed for: the collator needs to know the paths ninja knows these things by to make a valid dyndep file) - writes out a representation of this information for any dependent libraries so that they may be consumed in other targets as well - writes out a \"modmap\" file for each TU saying \"I see you want modules E, F, G; here is the path for those modules *and no others*\" (this is important to avoid accidental/stale module usage; note that Fortran doesn't have this because module files are found via `-I` flags implicitly…I uncovered a lot of issues with that when enforcing C++ strictness on the Fortran stuff in CMake) - (there are some other tasks[3], but they're not relevant to the build graph) There is some flexibility with this model (also covered in the paper): - scan individually or scan in batches? - collate separately or after scanning? Smaller scans are better for incremental (developer) builds (e.g., if 20 files are scanned in a batch, changing any one will force scanning of all 20); batching is better for one-shot (CI) builds. Collating after scanning simplifies the tool (the collator doesn't need to understand $LANG syntax), but is probably slower (lots of tiny P1689 files flitting about). CMake currently has individual scans with separate collation. I don't forsee collation being merged into scanning, but an option to batch scans may make sense given performance measurements. Restrictions can also be enforced here. CMake doesn't care about file extensions, basenames matching module names, or anything like that. If one wants to do that (I suspect Meson might do so), that's fine. Configure-time scanning is fine if the configure is fast enough and generated sources (module-providing sources for sure; module-consuming sources may be able to be deferred-scanned, but once you have that support…just simpler to scan everything at build time if you ask me) aren't of any concern. > There was also an idea of using an LSP-like model, where a separate server process would keep the module dependency information and respond to requests from clients at runtime. This is, AFAIK, what build2 does (it is a build system and a build tool). It may implement it to the level of being an \"explicit\" build, but I've not dug in to know. There are patches to GNU Make to act as a module mapper as well. This patch only works with GCC today; Clang may add support for libcody in the future. Any \"implicit\" implementation has a major flaw in the generic case: I don't know how to meaningfully give visibility to modules. Note that the \"`-fmodule-directory=` Clang flag where Clang just reads and writes files in the given directory based on module names as this strategy as well (the filesystem is the \"module mapper\" here). I find this to be problematic in practice because it means the state of the build graph depends on the runtime state of some running process, not just mtimes, content fingerprints (for tools which are hash-the-content-based rather than mtime-based), some metadata on disk (e.g., `.ninja_log`). I can't imagine how debugging this is expected to work when those not well-versed in build systems are on the front lines. Beyond that, there are corner cases to consider: - When a request to import module Q comes in, how many processes do we expect to launch before we find the rule that exports module Q is found? What if it lies beyond our `-j` limit? Do we suspend processes while we wait, doing job server shenanigans? What if it doesn't exist? What if we discover a module import cycle? - The LSP-model must have some concept of module visibility in mind. Library L links to library K; K's sources should not be able to import any modules that are part of L. Even within L, if a module is not \"public\", it shouldn't be importable from outside of the library (including transitively by its own module interface files). - With the filesystem-as-mapper model, stale files are a serious issue. If I rename module R to S, what is in charge of cleaning up R's BMI file? How do I say \"yes, you found it; it is radioactive\"? What remembers that the now-scanned S creator used to make R to clean it up? - Duplicate module names. While C++ forbids multiple named modules within a program, nothing prevents `export module test;` from appearing in separate binaries. Related: if the build graph has release and debug variants, one now needs to track the debug version of module M separately from the release version. These are not easy questions to answer and that's on top of two-way communication with compilers you're launching. [0] There is a wholly unnecessary MSVC extension where it is allowed (IIUC, due to a misreading of the standard that, TBF, we also had when starting CMake's implementation); easily avoided and made portable: drop the partition name. [1] https://mathstuf.fedorapeople.org/fortran-modules/fortran... [2] https://wg21.link/p1689r5 [3] Namely writing install rules for the BMI files and CMake properties for exported targets. C++20 module support in build systems Posted Jun 9, 2024 0:58 UTC (Sun) by buck (subscriber, #55985) [Link] This was breathtaking. You should have saved it up for an LWN contributed article in its own right, though, maybe. Where is this going to be 10 years from now? Posted Jun 7, 2024 20:47 UTC (Fri) by q3cpma (subscriber, #120859) [Link] Personally, I'm still hyped about Nyxt. I plan on switching and donating to it once it supports WebExtensions or something similar to uBlock in advanced JS blocking mode; the modern web without it is just too painful. Where is this going to be 10 years from now? Posted Jun 8, 2024 12:42 UTC (Sat) by vadim (subscriber, #35271) [Link] There's Servo. It's still alive, just split off from Mozilla and now is its own thing. Writing a web engine is an enormous project, I'm sure they could use more donations. Where is this going to be 10 years from now? Posted Jun 15, 2024 9:17 UTC (Sat) by iteratedlateralus (subscriber, #102183) [Link] It's fairly common to write header-only C++... which means you wouldn't have to write a function twice. But it definitely doesn't help compile times. Where is this going to be 10 years from now? Posted Jun 21, 2024 12:17 UTC (Fri) by rc00 (subscriber, #164740) [Link] (1 responses) Given that Rust has mainly proven to be academic in use/purpose, a \"serious\" attempt does not seem likely. You also seem to contradict a lot of your own statements after that as well. Where is this going to be 10 years from now? Posted Jun 21, 2024 13:37 UTC (Fri) by atnot (subscriber, #124910) [Link] > Given that Rust has mainly proven to be academic in use/purpose, a \"serious\" attempt does not seem likely. What? That statement was barely justifiable half a decade ago. In 2024 it just seems petty and delusional. Support Firefox instead Posted Jun 7, 2024 21:07 UTC (Fri) by Cyberax (✭ supporter ✭, #52523) [Link] (8 responses) I don't believe new browsers make any sense today, except as research items. It would be much better to work with Mozilla to improve Firefox. One thing that's sorely missing is the embedding API. It's dead easy to embed Chromium, and this gives it so much advantage. Support Firefox instead Posted Jun 8, 2024 8:40 UTC (Sat) by b7j0c (subscriber, #27559) [Link] (7 responses) I don't know how Kling feels about Mozilla but it is also irrelevant; he is not obligated to support Firefox. Mozilla as an organization is a spent force. They have jettisoned their primary means for making significant game-changing progress (Rust/Servo) and are instead just managing the inertia of their uncompetitive, aged, dying platform. Mozilla squandered their future in exchange for shallow activism. Did any of their activist goals actually achieve anything other than me-too virtue signalling? I see a decade+ of superficial, low/zero impact activism while Chrome basically ate their entire market share. Kling is doing us a favor by paving the way for a post-Mozilla future. Support Firefox instead Posted Jun 8, 2024 12:05 UTC (Sat) by roc (subscriber, #30627) [Link] (6 responses) From a technical point of view Firefox is still very competitive with Chrome. Certainly far more competitive than Ladybird has any hope of ever being. I just watched a video on a streaming service. That's basic functionality normal users require. To get that to work well you need a ton of stuff, including --support, including the related DOM APIs -- Integration with modern A/V codecs -- Integration with hardware-accelerated decoding, where available -- Support for Media Source Extensions -- Support for EME -- Reliable A/V sync -- GPU-accelerated rendering -- Off-main-thread compositing -- Integration with the system compositor framework, where available (to minimize power usage) (I'm sure I missed some stuff, it's been a while.) To be competitive with modern browsers on security you need to have the site JS+CSS+HTML in its own sandboxed process. GPU rendering and A/V decoding need to be in separate sandboxed processes (could be the same process in a pinch). Firefox does this all very well. It took many talented engineers (and me) years to build and a lot more work to maintain since then. Maybe Mozilla is a spent force --- I hope not --- but don't underestimate what they had and still have, and how difficult it will be for a new project to even catch up to where they are now. Support Firefox instead Posted Jun 8, 2024 13:10 UTC (Sat) by PengZheng (subscriber, #108006) [Link] Agree. > A user asked on June 6 what would determine whether a component would be developed in-house versus using a third-party library. Kling responded that if it implements a web standard, \"i.e DOM, HTML, JavaScript, CSS, Wasm, etc. then we build it in house.\" As a developer familiar with multimedia streaming, I would say even re-implementing WebRTC alone will be a huge challenge for the small team. Seriously, calling Firefox a dying platform will not add one's own value, it just pisses out faithful users of Firefox (there are a LOT here on LWN). Support Firefox instead Posted Jun 8, 2024 17:44 UTC (Sat) by cytochrome (subscriber, #58718) [Link] I agree wholeheartedly. I find that Firefox (plus add-ons) satisfies all of my browsing and privacy needs. Thank you to @roc and the other talented software engineers for their hard work in developing and sustaining the application. I have tried to do my small part by donating to the cause several times. Support Firefox instead Posted Jun 9, 2024 8:55 UTC (Sun) by roc (subscriber, #30627) [Link] (3 responses) This inspired me to write a blog post about building a new browser engine. https://robert.ocallahan.org/2024/06/browser-engine.html Support Firefox instead Posted Jun 15, 2024 2:06 UTC (Sat) by himi (subscriber, #340) [Link] (2 responses) Not that I'm planning to build a browser engine (for profit or itch-scratching - I'm not insane), but that's a nice description of the complexity of the modern browser environment - it's easily the most complex single piece of software on human-focused general purpose devices, and it's probably the most critical piece of software on those devices. The fact that Mozilla have been able to keep Firefox genuinely competitive with Chrome is a real achievement, anyone targeting that level of functionality from scratch is facing a truly massive battle just to get to the starting line. That said, something that targets the Electron niche (and does it well enough) likely has a lot of legs. Definitely still an uphill battle, but far less of one than targeting a full browser . . . Support Firefox instead Posted Jun 15, 2024 2:27 UTC (Sat) by Cyberax (✭ supporter ✭, #52523) [Link] There are projects in this area, like HTMLayout (now https://sciter.com/ ). It was done by a single person, and back in 2000-s quite a few widely used Windows programs had UI built using it. Something like this would be extremely useful. Not a full browser engine, but a subset for UIs. Support Firefox instead Posted Jun 15, 2024 14:02 UTC (Sat) by Wol (subscriber, #4433) [Link] My immediate reaction to this is the web browser is trying to be the new Emacs - all it needs is some hardware to run on, it does everything else. But seriously, to what extent is that the modern paradigm nowadays - the browser is the OS that runs your programs, and linux and Windows are just glorified hardware abstraction layers. For many people - especially at work - that is now not far from the reality ... Cheers, Wol Comparisons to Firefox/Chrome seem premature Posted Jun 7, 2024 21:19 UTC (Fri) by flussence (subscriber, #85566) [Link] Most of these alternative browsers fall into a gravity well of being a clone of Firefox 0.6, both in terms of capability and UI, and usually give up there after a period of stagnation. I would like to see something break the curse some day but I've been disappointed too many times to get excited at this point. Quixotic Posted Jun 7, 2024 22:07 UTC (Fri) by roc (subscriber, #30627) [Link] (1 responses) Sounds like fun, but to build a browser engine that is useful to the masses is going to take them infinitely long. (Literally; the work that needs to be done is growing faster than a handful of developers can do it.) Also, if you're not just doing it for fun, then to justify starting from near-scratch you should make some fundamental decisions differently from the existing engines. Building with Rust would be a good one, but apparently that's not it. Building in site isolation from the ground up (i.e. IFRAMEs in their own processes) would be a good one, but they're not doing that AFAICT. Going all-in on parallelism would be another interesting one but I don't see that either. The only answer here that I can see in interviews with Kling is that Ladybird is designed to hew close to the architecture of the specifications, which is good but not much. Quixotic Posted Jun 10, 2024 3:23 UTC (Mon) by raven667 (subscriber, #5198) [Link] It may be quixotic but having someone new attempt to re-implement this stuff from scratch may help better document the actual effort involved in making a working implementation of these standards and what is required to make them fast, which isn't necessarily visible when an existing browser maker with an existing browser engine architecture and existing expert developers, who may have helped write the standard, does it because so much of the surrounding work the standard depends on already exists. If they want to build this for fun and patrons want to fund it for the entertainment value, then so be it, I wish them the best of luck and most success. Crashing is not necessarily a bad omen Posted Jun 7, 2024 22:18 UTC (Fri) by cesarb (subscriber, #6266) [Link] (8 responses) > However, it doesn't seem to suffice to keep a single page from crashing the browser entirely. That's to be expected from a project that's still considered pre-alpha, though. I still remember using the single-digit milestone releases of Mozilla Suite (according to https://en.wikipedia.org/wiki/History_of_Mozilla_Applicat... this was back in 1999). Each page I loaded had a chance of crashing the whole browser, and that chance increased dramatically if I used more than one browser window (there were no browser tabs back then). It was fun. I stuck with it, later switching to its descendant Phoenix/Firebird/Firefox, which I still use to this day. That is, history shows that crashing often, this early in the project's life, does not mean the project is going to fail. Crashing is not necessarily a bad omen Posted Jun 8, 2024 6:43 UTC (Sat) by joib (subscriber, #8541) [Link] Well, back then netscape/mozilla/firefox was about the only decent(ish) alternative to IE, so you didn't have much alternative. Today you can switch to Firefox, Chrome, or a plethora of Chrome /webkit based browsers. So the bar for what people consider a MVP is much higher. Crashing is not necessarily a bad omen Posted Jun 10, 2024 7:16 UTC (Mon) by edeloget (subscriber, #88392) [Link] (4 responses) And to be honest, this was a time when we routinely rebooted the whole PC multiple times per day, so having a crash on a web browser, while annoying, was kind of expected :) Crashing is not necessarily a bad omen Posted Jun 10, 2024 11:25 UTC (Mon) by cesarb (subscriber, #6266) [Link] (3 responses) > And to be honest, this was a time when we routinely rebooted the whole PC multiple times per day, This was also a time when people bragged on slashdot about their Linux uptime. Rebooting the whole PC multiple times per day was IIRC more of a Windows thing back then; I was surprised when once I somehow managed to kernel panic Linux by doing something as simple as removing a floppy disk which was in use. > so having a crash on a web browser, while annoying, was kind of expected :) This, on the other hand, was IIRC more common, even in non-alpha browsers like the pre-open-sourcing Netscape Navigator 4.x. But they were not so unstable that one would be afraid to open more than one window at the same time, for fear of a crash closing all open browser windows. Crashing is not necessarily a bad omen Posted Jun 10, 2024 12:25 UTC (Mon) by geert (subscriber, #98403) [Link] (1 responses) export MALLOC_CHECK=0 netscape Crashing is not necessarily a bad omen Posted Jun 10, 2024 12:32 UTC (Mon) by khim (subscriber, #9252) [Link] There was a way to attach Boehm GC to it, if I remember correctly. That was more stable. Crashing is not necessarily a bad omen Posted Jun 14, 2024 23:28 UTC (Fri) by ssmith32 (subscriber, #72404) [Link] Uptime on servers, sure. Uptime for the kernel was pretty good on desktops. But I remember being pretty handy with alt-shift-F1 and alt-shift-backspace, or whatever the hotkeys were to get to the virtual consoles and hard-kill X. And X dying was almost as bad as a full reboot for a desktop where apps didn't do a terribly good job with checkpoints and session restores. So, while restarts and blue screens and corrupted data is what moved me off of Windows in the early 2000s, the Linux Desktop was not nearly as stable as it is today. Nowadays, I had one desktop issue in the past few years, and I had System76 support figure out the issue and send me a new driver (which weirdly was triggered by using Google Maps in Firefox). Worlds better than when I was running a Mandrake (RPM) Frankenstein where half the software was compiled from tarballs because the RPMs available were woefully out of date.. Crashing is not necessarily a bad omen Posted Jun 10, 2024 19:56 UTC (Mon) by parametricpoly (subscriber, #143903) [Link] (1 responses) Programming languages have improved quite a bit since 1990s. It's really no wonder it crashes it they chose C++. Using C++ here again is a waste of talent. Heck they could have picked Java instead. Java 22 is already pretty decent and more goodies will arrive in the LTS 24 release. Crashing is not necessarily a bad omen Posted Jun 13, 2024 5:40 UTC (Thu) by ceplm (subscriber, #41334) [Link] How is Rhino doing these days? Is it already in the same speed rank as other major JavaScript engines? Actually, surprisingly, it is still alive. That’s more than I expected. Future? Posted Jun 8, 2024 15:45 UTC (Sat) by drago01 (subscriber, #50715) [Link] Does this have any futute other than being a \"toy\" project? It doesn't do anything new or different to attract enough attention to compete with Chromium or Firefox. Why *not*? Posted Jun 8, 2024 16:45 UTC (Sat) by rsidd (subscriber, #2582) [Link] (3 responses) I don't get all these comments about \"why work on this\". Because they want to, that's why. And SerenityOS seems much less relevant or useful to the wider world but that's where it started and that's an itch Kling wanted to scratch. Good for him. Both projects seem to have built a substantial community so it's not just him. It's pretty impressive what Ladybird has achieved, and there's no reason to think it won't be 10× more useful a year from now even if it lags Chrome and Firefox in some respects. But even if not, this is what they want to do, good luck to them. And it's false logic to say they should just work on firefox. That's not how \"scratch an itch\" works. Actually, given the change in policy that they will not DIY everything but accept some NIH code where useful, I expect the next 12 months to be pretty interesting. Why *not*? Posted Jun 9, 2024 4:58 UTC (Sun) by chris_se (subscriber, #99706) [Link] (2 responses) I completely agree with you. For that matter, I believe that the current browser ecosystem is not healthy at all: you have Chrome (+ all its derivatives), and Firefox (+ its derivatives), and then some niche projects. And while derivatives of Chrome make some changes, they will not diverge significantly from the actual browsing engine, so in the end, Google is the only one that determines what's next in terms of browser engine development, regardless of whether you use Edge, Chrome, Chromium, Opera, ... The only usable alternative right now is Firefox, but they are not the ones driving innovation in that space at the moment, unfortunately. (Which is a real shame, I like Firefox.) So in that sense I welcome people being excited and working on alternatives, such as Servo, Ladybird, and others. Because I don't want to be in the situation where the entire future of the web is basically in the hands of a single company, and even if I don't end up using them myself for whatever reason, if they do thrive, they will bring a breath of fresh air into the web ecosystem, which is sorely needed in my opinion. Why *not*? Posted Jun 9, 2024 14:44 UTC (Sun) by rgmoore (✭ supporter ✭, #75) [Link] (1 responses) The current browser ecosystem is unhealthy because Google sees Chrome as a means to maintain its dominant market position in the lucrative search and online advertising businesses. They use their money from search and ads to squeeze profits out of the browser market and make it effectively impossible for anyone to make money by developing a competing browser from scratch. It's basically the IE situation from the late '90s all over again, and it won't improve until some government steps up antitrust enforcement the way the USA did in the '90s. Chromium is a browser consortium lead by Google Posted Jun 10, 2024 3:37 UTC (Mon) by raven667 (subscriber, #5198) [Link] One important distinction from the bad old MSIE days is that Chromium is open source, not private/proprietary like MSIE, and acts as a reference implementation of a multi-vendor platform designed by consortium that includes competitors Google and MS (lead by Google) and separately with their own engines Apple and Mozilla, in some ways no different than Linux being developed as a shared project by many competing companies (although for Linux the consortium/trade org LF operates for _all_ vendors so there is no _one_ lead vendor) along side the similar BSDs with their own base OS funded by Apple and other vendors. The mechanisms and possibilities for \"lock-in\" are very different than before. Ports to other operating systems in progress Posted Jun 21, 2024 12:40 UTC (Fri) by pgarciaq (subscriber, #153687) [Link] If you look at the pull requests and forks, you'll find there's on-going work to port Ladybird to Windows and MacOS. Copyright © 2024, Eklektix, Inc. This article may be redistributed under the terms of the Creative Commons CC BY-SA 4.0 license Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=40746804",
    "commentBody": "Ladybird browser spreads its wings (lwn.net)392 points by signa11 12 hours agohidepastfavorite278 comments oskarkk 10 hours agoOne commenter who worked on Firefox made a nice blog post in response, about what it takes to make a web browser: https://robert.ocallahan.org/2024/06/browser-engine.html reply getcrunk 8 hours agoparentFor a new project I wonder how much simpler (or secure) a browser could be made if you only allowed a subset of js and browser apis. I’d wildly guesstimate for 70% of use cases you wouldn’t even need 50% of stuff with some slight modifications. The web is just so bloated. Edit: might as well prune down the css a little too and maybe dump wasm, webgl and canvas reply jsheard 8 hours agorootparentThe problem with targeting a subset is there's a ratchet effect with web APIs, once support reaches critical mass in the major browsers sites will start unconditionally relying on those features and there's no going back from there, any new browser has to also support those features or be considered broken. I suppose anything that's gated behind a permission prompt in Chrome/Firefox/Safari could be culled without too much trouble at least. reply starbugs 7 hours agorootparent> The problem with targeting a subset is there's a ratchet effect with web APIs What about starting a new web then only for the supported subset? Based on my current browsing experience, this may be a plus in the long run. reply skriticos2 6 hours agorootparentThere is a reason that Esperanto or one of it's siblings is not the language of global understanding and we are discussing in English. The world at large generally does not care about these kinds of fancy. It's half a miracle that we agreed to whatever the Chromium engine implements to be a baseline that most folks build stuff on. reply gus_massa 7 hours agorootparentprevThe problem is convining anyone to write a website in this. You will have like 3 sites that use only the subset. The closest example is AMP, but you must be Google to force people to use it. reply starbugs 6 hours agorootparent> The problem is convining anyone to write a website in this Could choose a subset that lets certain sites that do not get on everybody's nerves still run fine. For the remainder, people who need it can run an extension that runs a Chromium converting what's possible to the target subset. reply immibis 6 hours agorootparentDo you want to support YouTube? You want to support YouTube, don't you? Any big FAANG site probably uses most of the possible features. reply starbugs 5 hours agorootparent> You want to support YouTube, don't you? Of course no. You can use a bloated browser for that. reply immibis 4 hours agorootparentThen nobody will ever use your browser. Simple as that. reply 8organicbits 2 hours agorootparentI think people forget the early 2000s. Many sites only worked on IE but Firefox was the better browser. Firefox users had an extension that would open certain links in IE or open the current page in IE via the context menu. If a lightweight browser could be significantly faster and more secure, people would tolerate using two browsers again. Although Ladybird hasn't reached that bar. YouTube certainly could use a small set of web standards, although YT regularly breaks on Firefox. It's a video player with links and forms. reply ranger_danger 20 minutes agorootparent> YT regularly breaks on Firefox Source? I've never had a single problem ever, and I don't know anyone else that has either. galangalalgol 6 hours agorootparentprevA surprising number of websites work fine without js. reply ranger_danger 22 minutes agorootparentBasically none of the ones I use look or work normal without it. reply peremptor 7 hours agorootparentprevThere are now 3 competing standards. reply darby_nine 6 hours agorootparentYou really just need two to see the positive effects, but three would be fantastic. reply trigonated 6 hours agorootparentprevThis is more or less the idea behind projects like the Gemini protocol [1]. It's even deliberately designed to not be easily extensible, as to avoid the temptation of adding features. [1]: https://en.wikipedia.org/wiki/Gemini_(protocol) reply astrobe_ 6 hours agorootparentprevGiven that a browser is not practically a virtual machine and/or an OS, it is not too surprising that an OS developer is not scared by the task. At the same time, it seems to me that multimedia is a whole different beast. > For a new project I wonder how much simpler (or secure) a browser could be made if you only allowed a subset of js and browser apis IMHO the only viable subset is the empty set. There are some surviving HTML-only browsers that are still usable for e.g. viewing documentation or browsing simple-minded websites (like HN, but they are fewer and fewer every year, unfortunately). I really don't want to drop the all too common negative comment - in particular since I already use an alternative web browser - but the initial investment required just for an MVP seems mind-boggling to me. I think a basic HTML browser that can automatically delegate all it cannot handle to other apps - PDF viewing to a PDF viewer, video playback to a video player, and JS-requiring things to a big browser - would be interesting (if it already exists, please let me know). reply bobajeff 5 hours agorootparent>IMHO the only viable subset is the empty set. There are some surviving HTML-only browsers that are still usable for e.g. viewing documentation or browsing simple-minded websites (like HN, but they are fewer and fewer every year, unfortunately). I'm curious if throwing out DOM/js would make the task more approachable. My intuition says yes. But I'm thinking CSS would still make it super difficult. Also I've heard that HTML has some rough areas that make it hard. That being said I find Netsurf is pretty capable even if I don't really use it very often. Yeah some pages don't render right but it's really fast. So who knows maybe we can get away with a reduced set of features or better yet go back to using separate clients instead of web apps for things like chat, email and forums. reply djbusby 5 hours agorootparentprevI've been wanting something like that, like a wrapper that hosts three or four engine, so I can use normal for every-day, upscale to FF for some or Chrome for others - all in one UI. reply EasyMark 4 hours agorootparentprevIf it only does 70% of what a regular browser does would anyone (seriously not as an experiment) use it on a daily basis? Most people just want to get on the web and go. They don’t care how big their is on disk or how much ram it uses (within reason of course), but the first time it doesn’t render their bank page or Jira page, they will drop it. Obviously I’m not talking about beta testers here or curious web devs. reply 8organicbits 7 hours agorootparentprevI think this is a great point. The majority of my browsing uses text oriented sites (some think they are more, but I read them in reader mode). As I user, I rarely want more. I'd use a slim, secure browser if I could. Sometimes I play games, or use \"web apps\" and using a different tool in those instances would be fine. Back in the early 2000s I remember using Firefox with an \"open in IE\" extension that allowed me to primarily use FF and fall back to IE when sites were broken. As websites modernized I used the extension less and less. Also consider desktop apps that use election. Bundling a simpler browser and building the app to the capabilities of that browser could greatly reduce install size and memory usage. reply philistine 3 hours agorootparentprevThe conversation regarding restrictions on the feature set within the browser always focuses on the browser itself, but there is definitely an opportunity to rally website makers. If Google or Apple came out and said we have a reduced feature set you can use that makes our browser render your website faster it would be a good opportunity, and I think many website makers would jump at the opportunity. A sort of Google AMP, but good! Let's call it Strict HTML. reply hypeatei 8 hours agorootparentprevThis is so wildly wrong it hurts. As soon as that one important website (to your user) breaks, they will switch browsers. Just deciding that you don't want to implement >50% of web specs \"for simplicity\" and expecting that to be a winning strategy is very HN. reply sanitycheck 6 hours agorootparentThe \"very HN\" thing for me is assuming a \"winning strategy\" has to be mass adoption. There's room for a smaller web full of nerds and geeks, we had that before and maybe we can have it again. reply immibis 6 hours agorootparentIndeed, that's why we are discussing on HN, not on Reddit. Exclusion as a design choice. reply mirkodrummer 7 hours agorootparentprevTo me it’s not wrong at all, why won’t a simpler browser with a “modern mode” rendering succeds? There is no need for that browser to support the space jam website, who really cares? Imo thinking that people need quirks mode because they need to visit old website it’s very HN. reply IggleSniggle 7 hours agorootparentIt's not that at all. Imagine some popular website is relying on some weird quirk but it works out in modern browsers. They don't even know they are broken, or in what way, and first discovering the issue and then investing in fixing the issue forUse a programming language that lets you write clean, fast, memory-safe, parallel data-race-free code — probably Rust. But from the lwn article: It is written in C++ [..]. Oops! :-) reply frognumber 6 hours agoparentprevIf I were building this, I wouldn't do that. I would build one piece, as a well-documented Pythonic (not in Python; just in coding / documentation style) library. The reason this is impossible is the monolithic design of these things. There are good reasons for it -- the pieces interrelate -- but I think it's possible to break it up (with a lot of work). For example: - A clean, documented JavaScript engine would be a good start. - Python-style, independent, isolated JavaScript libraries would help (usable serverside or clientside where possible) - An independent rendering engine would be nice -- again, documented and independent of the above - Network libraries - HTML / XML / CSS parsing libraries ... etc. If this were in place, code could be interchangeable between serverside and clientside much more than today (and usable in other places, such as using JS as a scripting language in other systems). Test cases for rendering (or even just making a screenshot) wouldn't need the whole browser. You would import and call into the rendering engine to make a .png without selenium. Making a new web browser would involve mostly glue code and OS-specific code. reply bilekas 7 hours agoprevAndreas Kling is a great role model in the world of development I feel. The decision to step away from the Serenity OS makes a lot of sense. There are plenty of them, nice projects to do, but they'll never have an immediate impact if any at all. But the browser space, Ladybird is viable as a daily driver for people. I'm still today astounded the work that has been produced just on this. > Somewhat ironically, it was not possible to log into Discord using Ladybird. It does a fair job of rendering pages, but speed and stability are still wanting. Fascinating to see people expecting everything to work out of the box whilst being writter from scratch. And as an avid rust appreciator, all the comments about \"rewrite it in rust\" as if that solves anything the OP spoke about is really frustrating. Let the team cook, if you dont like the dish, help cook it. The project has been super interesting and honestly I feel we need more like this in the browser scope. reply alwillis 3 hours agoparentI don't think casual observers understand how many new CSS features have been added to the web platform over the last few years. 468 new features have been added since 2018; 148 have been added in the past 18 months alone [1]. I applaud the effort but unless something fundamentally changes in their approach, I don't see how they can catch up. Sure, you don't need every single spec, but even the core ones like Flexbox, Grid are large and complicated and are constantly being tweaked. [1]: https://codepen.io/fimion/pen/WNmwJWJ reply 1oooqooq 2 hours agorootparentall of them were written in a way that mostly affect the api, not the underlying implementation models tho. it's still 500 new things to test and actually implement. but not as bad as the original 1 to 2 change. html5 was the forever missed opportunity to do things right. but keeping the browser-side effort low was always fought for on the w3 committe. reply Y_Y 6 hours agoparentprev> Fascinating to see people expecting everything to work out of the box whilst being writter from scratch. (Sarcastically saying something is interesting is something I find distasteful.) Anyway the irony is that the project chose to use a discussion platform which uses lots of modern web cruft and would be a big challenge for a new browser, when they could have chosen a (maybe less capable) simpler platform like IRC or some simple web forum which would more likely have run on Ladybird. reply hailruda 6 hours agorootparentChosing Discord was a deliberate choice. I remember when Serenity was using IRC as the only communication channel and immediately after setting up a new Discord server the community gravitated towards it. Infact, the number of community members sky rocketed. The fact is, Discord as a platform is more accessible compared to IRC. reply diggan 4 hours agorootparent> The fact is, Discord as a platform is more accessible compared to IRC. Guess it depends on what you mean with \"accessible\". In terms of accessibility for impaired users, IRC is a open protocol vs a service that disallows 3rd party clients. I'm sure there exists better options for IRC than Discord, and at the very least, IRC allows you to access it however you want, be it visual, textual, voice control or whatever. Besides, Discord being a US company, need to prevent users from Cuba, Iran, Syria (and NK) and is also banned in a bunch of countries like China, UAE and Egypt. So in that sense, Discord seems less accessible than IRC. Only remaining part is that Discord has a somewhat easier \"getting setup\" UX than IRC for younger users, as it's more similar to the type of services they're probably already use. reply philistine 4 hours agorootparentprevSounds like someone should work on Discord-ifying IRC to make it more palatable for the modern user. reply ranger_danger 20 minutes agorootparentThat would be Matrix. reply codetrotter 6 hours agorootparentprevThey used to use IRC in the beginning. reply diggan 5 hours agoparentprev> But the browser space, Ladybird is viable as a daily driver for people. From the LWM article: > Users will need GCC 13+ or Clang 17, and Qt6 development packages to play along at home. Ladybird compiles and runs on, for example, Fedora 40 without a problem, *but it is a long way from being suitable for regular use.* Seems distributed binaries are missing, but that's easy to \"fix\"/\"workaround\". Is there something else that makes you say it is suitable as a daily driver while LWM author does not? reply ranger_danger 23 minutes agorootparent> Seems distributed binaries are missing, Even with SerenityOS they long were of the mindset that binaries/ISO/etc. should not be produced and distributed by them apparently. reply coliveira 5 hours agoparentprevI wish rust people spent their time writing software instead of going around telling other people to do the job for them. They only manage to get others annoyed with this attitude. reply ranger_danger 22 minutes agorootparentTruer words have not been spoken. reply skilled 11 hours agoprevRelated: I'm forking Ladybird and stepping down as SerenityOS BDFL - https://news.ycombinator.com/item?id=40560768 - June 2024 (262 comments) reply pixelatedindex 11 hours agoprevFor a newbie like me who isn’t like a software eng wizard but is willing to put an hour or two every day, is a project like this a great way for me to learn and contribute? Or is it still so pre-alpha that you need deep domain knowledge to contribute and have an impact? reply gaazoh 11 hours agoparentI think it's a great project to learn and contribute. The scope is very broad, so there are plenty of different areas to contribute to, including not-so-advanced functionality. For example, the initial find-in-page feature was introduced just a few week ago[1] and the core logic relatively simple. Plus, the build process is well documented and works out of the box (at least on Ubuntu in my experience) and the community is nice and welcoming. [1]https://github.com/SerenityOS/serenity/pull/24480 reply shiroiushi 10 hours agorootparent>Plus, the build process is well documented and works out of the box Yeah, that seems to be a huge problem with many larger FOSS projects. reply tterras 11 hours agoparentprevEven if not pre-alpha, building a browser needs deep domain knowledge. But on the other hand, after some time you will be rewarded with very, very useful knowledge of how the web actually works. The start would be pretty rough but if your daily work lacks the challenge then I would encourage you to try contribute :) (Not affiliated in any way) reply boneitis 10 hours agoparentprevI remember encountering this blog series of Chrome architecture articles[0] and telling myself I need to read through it (and, to date, still never have :) Maybe it is of interest. (Just a heads up, it dates back to 2018.) [0]: https://developer.chrome.com/blog/inside-browser-part1 reply pixelatedindex 2 hours agorootparentWhoa, incredible resource. Thank you! reply oefrha 9 hours agoparentprevI’ll be that guy and point out that if you have to ask this question, it may not be the best personal fit. Unpaid open source contributions work the best and last the longest when they scratch your personal itch. Find a bug or a missing feature in a piece of open source software you use all the time; find an abandoned project you need and try to improve it; or build something you want but doesn’t yet exist. Forcing your way into open source through a large project that is not in any way special to you in order to learn, build your personal brand, etc. may not be the most sustainable source of motivation. But who knows, maybe that also works for some people. reply pixelatedindex 2 hours agorootparent> Unpaid open source contributions work the best and last the longest when they scratch your personal itch. Find a bug or a missing feature in a piece of open source software you use all the time Most of the open source software I use are really rock solid, so I’m not sure what bug or missing feature I would be able to add. I also think as a newbie it’s perhaps useful to work where I can learn a lot in a short amount of time. Those dopamine hits can be pretty awesome :) reply erremerre 8 hours agoprevAlthough I dont like quite a few Firefox decisions. Like the floating tabs which no easy way to revert. I still think Firefox as a browser is fairly decent. And I also think that ladybird is going to have all the problems that Opera Presto had (and now Firefox has) which is going to be ignored by any small developers, and targeted to generate errors by the big companies (google/microsoft...) I definitely would have preferred the momentum to go to SerinityOS, and perhaps, importing firefox/librewolf into Serinity OS. reply paulryanrogers 6 hours agoparentIf enough people use different browsers then we'll get effective and slower standards once again. I think the only way to get people on other browsers is to legally require ballots on first use of an OS/device with random order. It worked in the EU, it can work in the US too. reply erremerre 6 hours agorootparentUnfortunately that is not how it works. The only way to get different browsers is to get absolutely every single OS project with some traction to force installation of them, to show advertisement everywhere, and to couple it with the installation of any piece of software, and to set it as default, plus import silently everything. Anything that is not a dirty tactic will not work. Also, the moment a user got an error they dont understand in certain browser, they blame the browser and change. How do you think firefox got out of business? Have you tried using Firefox to request an appointment with the Spanish Drive and Vehicle License Administration? It wont work. The civil servant will tell you use chrome. When a user get 2 or 3 use chrome to work, they will just use whatever it works. They won the browser war, adding ladybug to steal from firefox, is also not going to help either. reply paulryanrogers 6 hours agorootparentAll of those problems can be fixed by legal means. We cannot expect the market to regulate itself into a healthy competitive balance. Force sites to implement baseline standards and not rely on non-standard features. Disallow major web players from pushing their own browsers and from relying on their own browser's non-standard features. Permanently require browser ballots on all widely used consumer OS's. Heavily fine violators consistently. Use the money to support an independent standards body. Naive users will get a random, standards compliant browser. This throng will help prevent sites with no concern for the law from testing against only one instead of against the standard. reply erremerre 5 hours agorootparentThere has been 30/40 years of internet, and no legal battle has manage to get anything on that regard. I would even say, topics on the internet with a lot of lobbying, such as pirating software/media, in the same timeframe, has managed to get absolutely nothing done. I am afraid, it would not be possible to regulate legally. reply paulryanrogers 2 hours agorootparentYet regulations have happened and continue to, just different ones of varying ... quality/purpose. Even among preservation/piracy there have been moderately successful efforts. Apathy will not bring change. Only speech and action. reply zamadatix 1 hour agorootparentThe only thing that resulted in a decline in piracy was an increase in convenience, simplicity, and speed brought by on demand services (cloud game downloads and subscription passes, streaming video and music services, ebook/software subscriptions through app stores). The many billions spent per year in lobbying for deep seated laws like DMCA or equivalents never brought any meaningful change to the problem because the root of the issue was not solved by more restrictions. reply sharlos201068 6 hours agorootparentprevHow did it work in the EU? Almost the entire planet uses Chrome including the EU. reply paulryanrogers 6 hours agorootparentIn the early 2010s a ballot appeared for new users on Windows [0], though the legal requirement expired only a few years later. It helped other browsers gain market share. Sadly it's not enough alone. Major web players can promote their own browser and sabotage others, even if only by neglecting to test them. IMO a permanent ballot law is needed alongside restrictions from major web vendors pushing their own browser's and relying on their own browsers non-standard features. [0] https://en.m.wikipedia.org/wiki/BrowserChoice.eu reply zamadatix 5 hours agorootparentDo you perhaps mean to argue you think it helped other browsers lose users slower than they otherwise might have? From your link: > Competing browsers saw their traffic increase,[16] suggesting that these smaller competing developers were gaining users. However, long-term trends show browsers such as Opera and Firefox losing market share in Europe, calling into question the usefulness of the browser choice screen.[1] Opera is the smaller competitor referred to in both halves and it lost user share in Europe while this was in effect. About the only thing the ballot can claim is a loss in users of the 1st party browser IE but that effect was already occurring prior to the ballot anyways. reply paulryanrogers 2 hours agorootparentOn the whole it did only slow the tide. Yet it wasn't enough alone, limited to one OS, in one market, for only ~4 years, at a time when Google was pushing its own from some of the world's most popular services. Still, every victory is worth celebrating. Moping every time a (modestly successful) approach is brought up isn't going to move the needle. Dominant browsers rely on many tricks to gain and hold their position. It will take more than one approach to restore a balance. reply zamadatix 1 hour agorootparentI appreciate you consider it key but that several others are disagreeing whether something was successful is not them simply moping it was successful but not enough :). Keep in mind this all started out with this being the only way to get people on other browsers like how it had already worked in the EU so that's going to have steered more disagreement in the conversation than you might expect. I think the browser ballot style thing may have hurt more than it helped in the long run. E.g. users clicking things they don't understand during the initial setup of their computer (while a whole lot of other things they don't really deal with often are going on too) may have actually resulted in some extra short term download hits that immediately scared those users away from spending time trying other browsers out once they realize what their selection meant in terms of change. How many users that clicked on Maxthon and were confused/disappointed with a (then) Trident based browser that wasn't quite IE? Hard to say but the data isn't jumping out to show the opposite. I agree it would take more than one approach to restore the balance but I disagree that means all approaches are inherently helpful to roll out, let alone inherently key. The ballot initiative didn't result in any measurable change, even against the tide when compared to other regions, and simultaneously focuses both people and discussion away from major issues like the tricks dominant players actually use to gain market share. E.g. Microsoft had IE (and later Edge) bundled as the default choice after the ballot and share continued to decline up until they used these other working tactics which have resulted in it becoming the 2nd most used desktop browser again. reply dns_snek 6 hours agorootparentprevIt's great that we're faced with an explicit choice here in the EU, but I don't think it really done much in terms of affecting Chrome's entrenched market share. Most people use whatever tools they're already familiar with. Almost everyone I know uses Chrome, including the majority of my tech circle. I think people are only going to switch once the Firefox user experience is noticeably better for the average person. Google is on track to make that happen after they finally disable Manifest v2 extensions and as they continue their crackdown on ad blockers. reply paulryanrogers 2 hours agorootparentFirefox doesn't control major web properties from which it can nag users to switch. Chrome (and similar chokepoint-holders like Safari) won't be unseated by market forces for years, maybe decades. We are on the road to technology feudalism as the web and browsers eat the world. reply torstenvl 6 hours agoparentprev> the floating tabs which no easy way to revert Very easy in userChrome.css. I put mine on GitHub but it's nothing special, there are lots and lots that are available. https://github.com/torstenvl/userChrome.css reply erremerre 5 hours agorootparentNot what I would consider easy, that is medium. Easy should be install a theme/extension, or toggle an option on configurations. Yours can be broken after updates, and requires to know what it is (I do, I just cant be bothered), and redownload and apply after each time an update breaks it. I have decided to live with the awful floating tabs rather than personalising the userChrome.css, I wish someone could convert it into an extension. reply 1oooqooq 2 hours agoparentprevlet's all take a moment to appreciate how google managed to get the web to the state it was in 1996, when you had to develop for one browser and then go fix things in another. /slow clap reply lawn 7 hours agoprevA related question: What's the state with Servo? Do I understand it correctly that Servo is the core of a browser but not the browser itself? How much work would it be to create a browser on top of Servo? And is there such a project? reply infotainment 7 hours agoparentThey actually have an official blog with status updates! https://servo.org/blog/ They do have an official “browser”, ServoShell, which is basically a minimalistic testbed. IIRC adding tabs to it is on their roadmap. reply Dubh33 5 hours agorootparentIt looks like the repository for servoshell was archived in 2019. What’s weird is if you go to their website and download the tech demo it’s basically the same thing but may actually be updated. reply lawn 6 hours agorootparentprevOh, excellent. Thank you! reply manuelmoreale 10 hours agoprevI’m irrationally excited for this project. The idea of a community built browser is incredibly appealing considering the current landscape where all browsers are either Chrome, Chrome in a trench coat, or Firefox reply nutrie 10 hours agoparentI don't understand what is wrong with Firefox. It is open-source, highly configurable and reasonably secure (if you have the time to configure). Yes, it has shortcomings, but what doesn't. reply lukan 10 hours agorootparentNot every open source software has spyware and ads activated by default, while marketing itself as privacy friendly. Yes, can all be deactivated, I also use FF, but I do not trust Mozilla anymore. reply PrayagS 9 hours agorootparentIkr. They're up to some questionable decisions since long. - Acquire companies like Pocket, Anonym in multi-million dollar deals and also the millions in bonuses that the CEO likes to enjoy. - At the same time, no significant expenditure towards developing its core software. Firefox is still ridden with bugs. They even went as far as firing the people that used to work on Servo, Rust, WASM, etc. I think it's clear to them that there's not enough money to be made with small tricks like Pocket, VPN, Relay, etc. Firefox is still the only profitable product and contributes ~90% to Mozilla's revenue. Much of it coming from Google which is the one thing that people have been asking them to be less dependent on. And we shouldn't be surprised if they double down on making more money off of Google and also introduce ads. Acquiring Anonym, an ads company, implies that it might have already started. reply lukan 9 hours agorootparent\"They even went as far as firing the people that used to work on Servo, Rust, WASM, etc.\" And at the same time greatly raising compensation for the CEO, despite shrinking numbers. reply HeckFeck 8 hours agorootparentprevServo had so much potential as the flagship Rust project, a fast and memory safe browser engine. Such a loss to see it jettisoned and abandoned. Myopic decisions like these are filling the cloud of pessimism that now shrouds Mozilla. reply yencabulator 5 hours agorootparentFWIW Servo is seeing work as e.g. an Electron replacement. There is hope, even if one wishes the Mozilla Foundation CEO had used some $6M on developer salaries instead of pocketing it. Full browser feature set is hard, but an app that bundles its own webview can choose to not trigger the edge cases, so that's a realistic path forward. reply Y_Y 6 hours agorootparentprevI see some nice parallels with Wikipedia. Makes me wonder if anyone's yet developed a theory of \"foundation capture\" where you find some marvellous free thing that is made for and by humanity at large and extract/redirect money from its market share/good will. reply yencabulator 5 hours agorootparentI think the pump-and-dump extraction of value from any brand works pretty similarly, whether a non-profit or regular business. Lots of companies with centuries-long record of manufacturing previously durable goods have in the last decade or two switched to using e.g. inferior quality steel to increase profits. That'll destroy the brand, but in the meanwhile there's great profits to be had! reply Y_Y 5 hours agorootparentI guess for that we have to thank Aesop, the original corporate raider. https://en.wikipedia.org/wiki/The_Goose_that_Laid_the_Golden... reply antihero 8 hours agorootparentprevAlso they stopped developing Servo :( reply PrayagS 7 hours agorootparentAs in Mozilla stopped developing officially? Because I have seen active development on GitHub and monthly updates on their blog. reply mkl 7 hours agorootparentMozilla stopped in 2020: https://news.ycombinator.com/item?id=24128865 The project limped along for a bit, then recently Igalia started putting resources into it: https://news.ycombinator.com/item?id=39269949 reply komadori 7 hours agorootparentprevYes, that's correct. It was later adopted by the Linux Foundation with some industry support: https://www.linuxfoundation.org/press/press-release/open-sou... reply lukan 7 hours agorootparentprevYes, a while ago. But fortunately people continue. Just not with the billion dollar budget of Mozilla and with seemingly less traction. I have not heard of them in a while. But I very much endorse both. And yes, in theory energy should be focused, but I rather have 2 smallish projects, but with potential, than one slightly bigger one, with fighting about direction all the time. reply miki123211 10 hours agorootparentprevFirefox is open source, it would be far easier for the community to make a stripped down Firefox port with no telemetry, no ads and no upsells for Mozilla services. With that said, almost all of Mozilla's revenue comes from Google, which might possibly influence what features they implement, their stance on various web standards etc. reply wakeupcall 8 hours agorootparentAs pointed out, these do exist. I've been using several over the decades. And chrome forks too. They all tend to lag behind over time, until the fork is eventually too old and it's either abandoned, useful changes I was relying on are dropped, or becomes just too old compared to upstream to be fully compatible (and thus just annoying to use). Just the burden to upkeep the upstream changes, in either firefox or chrome forks, seems to be significant enough that I'm quite pessimistic on the lifespan of these projects. You might just as well do your own thing, and don't pretend to be a mainstream browser replacement altogether. reply sharlos201068 6 hours agorootparentKeeping up with an already built browser is too hard so we’ll create an entirely new browser that takes even more work? reply wakeupcall 5 hours agorootparentThe way I see it is \"I'll have fun at something that feels interesting\" reply lukan 9 hours agorootparentprevYes and those forks exists and are a solution now. But long term, I would much rather see a truly independent open source browser engine. reply yencabulator 5 hours agorootparentprevFYI https://librewolf.net/ reply maccard 10 hours agorootparentprev> which might possibly influence what features they implement, their stance on various web standards etc. Unless you’ve got some examples to back this up, it’s FUD. Posting hypotheticals is how rumours start, and this is just stirring the pot. reply Ginden 8 hours agorootparentConflict of interests is a real thing to worry about. I wouldn't trust scientist working in tobacco company on cigarette harm, even if I have no evidence of wrongdoing. reply nutrie 10 hours agorootparentprevSure, there are forks such as LibreWolf. I understand the reservations regarding Mozilla Foundation, although I generally like what they've been doing. Every org has people with stupid ideas. However, the way I see it, it's unlikely that the community will be able to produce a competitive browser in a broader sense (stability, performance, security, cross-platform...), meaning that the likelihood of Firefox being still around ten years from now is significantly higher than that we build and maintain a comparable browser ourselves. Then come the evolving web standards and lobbying power... reply flohofwoe 8 hours agorootparentThe way things are going at Mozilla currently, I wouldn't be surprised if Firefox is just another Chromium wrapper in 10 years. Technology doesn't seem to count as much as the C-suite filling their pockets at this \"new\" Mozilla. reply nullpilot 8 hours agorootparentprevThis about sums it up. Plus donating to Mozilla means your money goes to all sorts of things, but hardly if at all where you want it to. reply sebazzz 1 hour agorootparentprev> Yes, can all be deactivated, I also use FF, but I do not trust Mozilla anymore. And so I feel much more at home with Microsoft Edge and Google Chrome. reply garbagewoman 10 hours agorootparentprevCan you elaborate on what you mean by your claims of firefox having “spyware and ads activated by default”? reply lukan 9 hours agorootparentInstall a new version of firefox, have a look in settings under \"data\" (or whatever they call it in english). There you will see \"studies\" as activated, which is cryptic talk for ad tracking. And more recently, literal tracking for a advertisement company. And as a bonus, those were added and activated as features via update, without telling. At least for me. (and paid ads you have on the home screen) reply lawn 9 hours agorootparent> And as a bonus, those were added and activated as features via update, without telling. At least for me. Ouch, they were new to me and also activated. reply lukan 9 hours agorootparentYep, stuff like this makes me question many things. I mean in a sane world this should be enough to sue them into oblivion. But the general bar in that regard is so low, that apparently even open source companies can do it as default. reply garbagewoman 8 hours agorootparentWhy don’t you think it’s a sane enough world to “sue them into oblivion”? reply lukan 6 hours agorootparentBecause I do not think one will have success with it. Same with Microsoft, Google and co. reply worksonmine 9 hours agorootparentprevStudies are not ad tracking. It's worse and more like a backdoor for A/B testing of browser features. A few years back an update broke the Metamask extension and it was fixed by pushing via studies. At that point users weren't very aware the feature even existed and it caused some backlash since users realized there was a backdoor to push code into their browsers. The backlash resulted in studies being opt-in, and I thought it still was but I don't know, I use \"policies.json\" to setup my browsers. reply bongobingo1 8 hours agorootparentAm I being a total chud? These don't really seem egregious? I already accept (and expect) that Firefox receives background updates in the background. https://i.imgur.com/S7d4ZXg.png https://i.imgur.com/n2euIrQ.png https://i.imgur.com/BUjukOy.png reply worksonmine 7 hours agorootparentI don't mind the feature as opt-in, just like telemetry it can be useful. But depending on the threat model it can be a big problem. I still use Firefox as my main browser. reply hippich 8 hours agorootparentprevI think the problem is that chrome and firefox are developed for general usage, and as a result, have to deal with general usage constraints. For example, spectre and process isolation mentioned in another blog post here in the comments. I had a project in the past where I needed a browser with good JS support, but that does not make any http requests to any resources not in address bar or in the page rendered at that address. I did not find single browser based on firefox/chrome that fulfilled that requirement. Even the most privacy-focused projects still made continious requests to some mozilla resources (if i remember correctly, it was something about tls or maybe something else). So seeing someone creating an engine from scratch gives me hope that such browser might exist one day if I ever will need one again. reply viraptor 7 hours agorootparentThere's a difference between privacy and stealth. Modern browsers need to do things like captive portal sensing, ocsp checks, etc. If you want to disable all requests, you'll need to spend time cutting legitimate features. Or run it in an external sandbox which removes all networking access? reply manuelmoreale 10 hours agorootparentprevNothing, it’s my default browser for dev work but it’s only alive because Google keeps paying them to exist basically so I’d love an alternative. reply Aachen 6 hours agorootparentHow's that alternative funded? Shouldn't Mozilla want to replace their Google payment with that instead? Or, failing that, wouldn't it be much lower budget to fork Firefox code instead? And then to do it in C++ when other browsers and kernels are flirting with things like Rust. How long will it take for people to trust this new C++ code? I applaud the effort but am worried it'll be in vain. Then again, with how many projects I start not because they make sense but because I enjoy them, perhaps I should see it more in that way reply voytec 7 hours agorootparentprev> I don't understand what is wrong with Firefox. It's developed by Mozilla, Google's controlled opposition. I submitted a link[0] on ads coming to Firefox but HN shadowed it almost immediately (present on /newest when logged in, nowhere to be found when logged out). [0] https://www.jwz.org/blog/2024/06/mozilla-is-an-advertising-c... reply gjm11 7 hours agorootparentThat might be because Jamie Zawinski's website, which that link links to, does something ... special ... when it sees HN in the \"Referer\" field, so anyone actually following that link from here would (unless they take special measures) get IIRC a famous obscene image rather than the actual post you were hoping to link to. (JWZ is not a fan of Hacker News and its community.) reply Y_Y 6 hours agorootparentAll I see is a testicle in an egg-cup. Hardly special. (Though it might be a good idea to special-case this to ad a non-referral link at submit time.) reply gjm11 3 hours agorootparentAh, OK. My hazy memory was that it was the goatse.cx image, which is for most people more unpalatable than a testicle in an eggcup, but I didn't check exactly what it was :-). reply voytec 7 hours agorootparentprevOk, didn't knew that. Still a dick move on HN's part. Shadowing should be restricted to abusers. reply Wowfunhappy 6 hours agorootparentWouldn't the submission get user flagged anyway? Because when people clicked it, they'd see something NSFW and devoid of interesting content. reply voytec 4 hours agorootparentFlagged for what? In this case user - me - was unaware that submitted content gets replaced when requested with Referer header mentioning HN. There should be a 'you cannot post this' warning instead of hidden post creation. I only noticed shadowban by accident: switched to logged-in browser to submit and went back to logged-out for browsing. reply specialist 5 hours agorootparentprevWhat now? I love a good JWZ rant. Introducing Anonym: Raising the bar for privacy-preserving digital advertising https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-t... Oh. How Brave of them. reply JimDabell 9 hours agorootparentprevEven Mozilla doesn’t seem to care about Firefox; why should anybody else? The primary value in Firefox existing right now is that the web standards process becomes dysfunctional when there are only two major browser rendering engines, but that is fading away with Firefox’s market share. Hopefully Ladybird can gain enough momentum to matter there because it doesn’t seem like Gecko can maintain its relevance. reply viraptor 7 hours agorootparentWe're in the current situation because Google could spend enough money on promoting Chrome. That's it. A different, or even better browser won't change that. Google can put a chrome ad everywhere again for a few months. reply bmacho 6 hours agorootparentI feel firefox market share is mostly because they actively enshittificate their browser, and not because of google ads, or chrome being better. reply bwat49 5 hours agorootparentI disagree, IMO the main reason they lost so much marketshare to chrome was because for a long time, chrome just was much faster and much more stable. Performance, stability, and compatibility is all that matters to most users. Firefox (mostly) caught up with quantum and process isolation on the desktop, but by then I think it was too late. And the android version still has horrible performance, stability, and compatibility compared to Chromium browsers. Mozilla just doesn't have the same engineering resources to poor into the browser that google does, so I'm not sure there's any way they can really maintain pace with google outside of becoming yet another chromium browser. reply The_Colonel 5 hours agorootparentprevDoubt it. Things tech people complain about (like integration of Pocket) are completely irrelevant to the masses. On the other hand, things like performance improved drastically, and it is now competitive with Chrome. Firefox the product is in the best shape it ever was. reply squarefoot 7 hours agorootparentprevTechnically nothing; the problem lies entirely in the Mozilla Foundation which takes about $500B each year from Google. Officially the motivation is to keep Google as preferred search engine, but seriously the Firefox user base is so small that I don't even think Google would be interested for much less if there weren't strings attached that very likely create a huge conflict of interest if Mozilla become dependent on that money for their own survival. https://www.bloomberg.com/news/newsletters/2023-05-05/why-go... reply beardyw 7 hours agorootparent> $500B I think you may have made a typo. Possibly $500M? reply squarefoot 3 hours agorootparentWhoops, apologies, you're right. I completely missed it and now it's too late to correct. reply mirkodrummer 7 hours agorootparentprev*million not billion reply safety1st 7 hours agorootparentprevWell to put it bluntly, Firefox is Google's bitch. If we want the web to be a multi-vendor platform based on open, multi-vendor standards, as it has been for parts of its history, how much does Firefox really buy us? Do we really think that Firefox will take a hard stand against Google if their survival depends on not doing so? I use Firefox, but I think it's no longer true to its original mission in a lot of ways, Safari's global 18% share goes a lot farther toward making the web a duopolized rather than monopolized platform than Firefox's tiny sliver does. The less -opolized it is the better for society, or need I remind you that Google is presently mired in court for its conduct in its other web-adjacent monopolies such as web search and web advertising? reply dralley 7 hours agorootparentWhat are you talking about? Mozilla repeatedly stands up to Google on big ticket items. Flock being probably the biggest in recent history, but you can throw Manifest v2 on the pile as well and about half a dozen others. reply SahAssar 6 hours agorootparent> Flock For the benefit of those not in the know: FLoC https://wicg.github.io/floc/ reply lupusreal 10 hours agorootparentprevMozilla. I like and use Firefox but I find it increasingly hard to trust Mozilla and that makes me feel insecure about the future of Firefox. reply gwervc 8 hours agorootparentprevAround Firefox 4 it became a shitshow of questionable UI redesign every version. And performances where not up to Chrome. Also Chromium is open-source, and Firefox is mainly funded by Google so it not like Firefox has a real proposition value or independence. reply einpoklum 7 hours agorootparentprevFrom my limited experience of Firefox-as-a-project, coming from the side of Thunderbird: * Firefox is effectively not a community project. It seems to be ruled with an iron fist by the commercial side of the operation. * Firefox broke its extensibility - which was the whole rationale of the Mozilla project to begin with. * Lots of telemetry and call-home mechanisms, so much so that it is difficult to opt out even if you want to - in Thunderbird, and I believe also in Firefox; see : https://superuser.com/q/1672309/122798 (but correct me if I'm wrong and it's an app-specific thing). reply tambourine_man 8 hours agoparentprevNot irrational at all. It’s the first new engine in decades, started by a single person in the open with no multibillion dollar company behind. We have every right to be exited. reply the_other 7 hours agoparentprevSafari is 18% of global web traffic, 7x that of Firefox. (from a hasty scan of caniuse.com) reply 8organicbits 7 hours agorootparentI challenge those numbers. Global stats aren't important, just the stats for your target audience. My sites see 13% Firefox, 35% Safari. But keep in mind, FF users sometimes spoof the Chrome user agent because some sites break otherwise. Further, I suspect FF users have tracking blockers at a higher rate, so they don't show up. https://indieweb.social/@robalex/112472853515037460 reply t00ny 7 hours agorootparentprevHow much of this is attributable to the iOS lock-in though? reply shrimp_emoji 5 hours agoparentprevWhat about surf? https://surf.suckless.org/ reply manuelmoreale 3 hours agorootparentIsn’t this based on WebKit so it’s not really a new browser? reply alex_duf 10 hours agoprevI hope both Ladybird and Servo succeed in creating a new browser engine. Though I do have a slight preference for Servo as it's using Rust. Not that Rust is magical, but given how much attack surface there is in a browser, it seems picking C++ is a bit odd in 2024. reply 1GZ0 9 hours agoparentPersonally if I were to attempt a project at this scale, I'd do so using a language I'm proficient in. There might be other reasons, but Rust & C++ really are different beasts. reply tormeh 6 hours agorootparentIt's debatable how many - if any - humans are truly proficient with C++. reply 1GZ0 6 hours agorootparentSounds like \"Its hard, so it can't be done\" to me. There are plenty of people making good software in C++, how else would we have what we have. Sure, things get bloated and break over time, but thats not due to programmers not knowing C++, thats just what happens as a product evolves. New coders get brought in, focus shifts, understanding of the code based erodes. Its not that language, its the process. reply prmoustache 6 hours agorootparentprevI would say a lot but the average age is probably higher than rust by a large margin. reply chaz6 5 hours agoprevI was very impressed by the build script. It worked flawlessly, detecting Clang and Ninja, and pulled in all the dependencies automatically. I am hopefully that this will develop into a serious competitor as there really should be more than 2 choices (KHTML-based vs Gecko-based). reply amelius 9 hours agoprevIf they want adoption, maybe a good idea to build a hybrid ladybird/firefox browser, where, if some page does not render well in ladybird, the user can switch to firefox based rendering with a simple mouse click. reply 1GZ0 9 hours agoparentWhat would be the point of writing a whole new browser engine from scratch if you're just gonna fall back on a different one? Ladybird isn't ready and likely won't be for a long time. Treat this more like a \"we're working on a new browser\" type announcement, not a \"We're launching a new browser today\" announcement. reply 8organicbits 6 hours agoparentprevThese sorts of things exist for the other browsers, so eventually I think Ladybird would support it as well. https://chromewebstore.google.com/detail/open-in-firefox-bro... reply dsalfdslfdsa 10 hours agoprevI was recently browsing their docs, and kept finding references to choosing a \"browser chrome\" (options including Qt and AppKit). Is this some new usage of the word \"chrome\" that I'm not familiar with, or does it use chrome libraries? reply Lammy 10 hours agoparentGoogle co-opted an already common term into a Proper Noun™. https://developer.mozilla.org/en-US/docs/Glossary/Chrome http://www.catb.org/%7Eesr/jargon/html/C/chrome.html Netscape called it that and thus so does Mozilla. Try loading up `chrome://branding/content/about-logo.png` for an example of the chrome URI scheme in Firefox! reply Findecanor 9 hours agorootparentAround the turn of the century, the \"Mozilla\" browser used to support themes for its chrome, written in XUL [0], which also supported inline images. The themes were available in a repository called \"Chrome Zone\". Google later appropriated also the name \"Chrome Zone\", for a chain of retail stores in the UK. [1] [0]. https://en.wikipedia.org/wiki/XUL [1]. https://en.wikipedia.org/wiki/Chrome_Zone reply asddubs 9 hours agorootparentprevSpecifically google originally chose that name for chrome because one of their goals was reduction of chrome. They made a bit deal out of it when the browser was new, how it didn't have a bunch of menu bars or whatever reply flohofwoe 9 hours agoparentprevThe term \"window chrome\" is common to describe the parts of a window that are outside the client rectangle and managed by the window system (titlebar, scroll bars, resize widgets etc..). AFAIK the term also predates \"Chrome, the browser\" by a long time. reply ben0x539 10 hours agoparentprevIt's a usage that predates and conceivably inspired Google-brand Chrome (and by contrast arguably Rust?), referring to the UI parts of the browser rather than, I think, rendering and javascript and stuff. reply gtufano 10 hours agoparentprevThe \"chrome\" of an app is the part surrounding the main windows, like toolbars and the such. IIRC when Chrome appeared, the name was chosen because it was a browser without chrome (ie: it used to be just the rendering windows with the url bar on top), differently from other browsers at the time. reply asicsp 10 hours agoparentprevAs per https://en.wikipedia.org/wiki/Chrome: >User interface chrome, the borders and widgets that frame the content part of a window reply gertop 10 hours agoparentprevFirefox always referred to its user interface as the chrome (hence userchrome.css) and Google engineers decided that it should be funny to name their browser that. reply akie 10 hours agoparentprevThat's the old usage of the word \"chrome\", as far as I know. reply kleiba 10 hours agoparentprevIt's \"chrome\" as in cars and bikes. reply alvincodes 7 hours agoprevIt feels very linux-y, I wish it was easier to contribute to Firefox so those efforts would go towards stopping the google monopoly. reply branko_d 9 hours agoprevFrom the linked site: > In the post-Spectre world you must have site isolation. The JS for a site (roughly, eTLD+1) must have its own OS address space separate from other sites. Wasn't the whole point of Spectre/Meltdown to read the virtual address space of a different process? reply joshkel 6 hours agoparentAs I understand it: Meltdown lets a process read from kernel memory. There are several variations of Spectre. The first variant (\"Spectre V1\") lets a process read its own memory; the second variant (\"Spectre V2\") lets a process read another process's memory. Web browser manufacturers seem to be focused on preventing Spectre V1, although I'm unclear on whether that's because V2 is too hard to exploit from JavaScript, is mitigated in other ways (e.g., CPU updates), etc. Further reading: https://stackoverflow.com/q/53042230/25507, https://stackoverflow.com/q/48200753/25507, https://security.googleblog.com/2018/07/mitigating-spectre-w..., https://webkit.org/blog/8048/what-spectre-and-meltdown-mean-..., https://en.wikipedia.org/wiki/Spectre_(security_vulnerabilit... reply AshleysBrain 8 hours agoparentprevAs I understand it: Spectre/Meltdown allow reading from the address space of the same process only. If browsers put different origins in the same process - which they used to - then JS code can break the same-origin security barrier and read details of other origins directly from memory. By putting each origin in its own OS address space they are protected from this attack as JS can still only read data from its own origin even when using security flaws to read any part of the address space. reply branko_d 8 hours agorootparent> As I understand it: Spectre/Meltdown allow reading from the address space of the same process only. Sorry, this does not make much sense. Why would you need a timing attack to read memory from your own address space? Just a regular code execution exploit should do it. Here, I found the relevant info (that I was too lazy to find before I posted my first comment, apparently): https://meltdownattack.com/ > While programs are typically not permitted to read data from other programs, a malicious program can exploit Meltdown and Spectre to get hold of secrets stored in the memory of other running programs. reply jcranmer 5 hours agorootparent> Sorry, this does not make much sense. Why would you need a timing attack to read memory from your own address space? If you're making a VM such that the running code can only access a particular array, Spectre allows a timing attack that can get malicious code in the VM access to the full memory space. You're right that it's not that scary for most use cases. What it really means is that it's hopeless to make memory inaccessible to a sandbox without putting a process isolation barrier betwixt the two, as there's no real way to close out all of the timing attack possibilities. In principle, if the only thing you needed to foreclose was memory vulnerabilities, then sufficiently good programming™ would let you have the sandbox in the same process space; as a matter of practice, though, anyone looking at product security seriously would still make you put in process isolation, because that kind of good programming just doesn't exist at scale yet. (Note that Meltdown, but not Spectre, allows timing attacks that cross process isolation domains.) reply butz 3 hours agoprevIt is a perfect time for new browser. I long for the days of good old Opera, when each release brought something new and fresh. Multiple browser windows in tiled layout, browser gestures, disabling image loading from UI (although that was standard feature in dial-up era of internet), UI customization, integrated email client and feed reader. reply pkphilip 7 hours agoprevThis is astonishing given that the project only started in Sept 2022! Andreas Kling, you are the man! reply voidwtf 10 hours agoprevI don't want to discourage the developers working on this project, but I'm curious why we're still writing applications that will almost certainly execute or process hostile content in languages that don't maintain strict memory safe contract? Have we not learned our lesson yet, or am I misunderstanding the situation? I believe it was a Microsoft study that linked unsafe memory access to ~70% of exploit chains. reply jraph 9 hours agoparentI'll take an alternative browser engine, even if it's written in C++. > I'm curious Is it really curiosity though? Because the answer is straightforward, the project started as a hobby, the developer picked whatever language they were proficient in. Andreas is open with the fact that he started Serenity OS and LadyBird as a rehab project. Put too much barrier in this setting (like learning a new language and all the ecosystem) and it might not happen at all. reply almostgotcaught 8 hours agorootparent> Is it really curiosity though? it's not - it's a stock standard way to ask a question in bad faith. reply voidwtf 8 hours agorootparentprevIt’s curiosity because I’m involved in software security for the company I work for and I’m a software developer that primarily works with managed languages. I unfortunately lack the technical expertise to make an assessment on interpreters and virtual machines. I am, however, aware that many of the CVEs that I review involve reading/writing memory outside the intended bounds. Many of these while processing user generated content. I appreciate that the author birthed the project as a way to direct his energies towards more productive means. I don’t think it’s relevant to the question though. reply segfaltnh 8 hours agorootparentWhat you're describing is not curiosity. reply usef- 6 hours agorootparentI think it is, but perhaps I read the grandparent comments that way because I was curious about it too... reply znpy 9 hours agorootparentprev>> I'm curious > Is it really curiosity though? I'm kind of annoyed at this whole train of comments (\"I'm curious...\"). In so many occasion I see something cool and the main comment track is \"why hasn't this been writte in rust?\" (or some other allegedly safe/better programming language). It's like seeing a beautiful painting being painted and arguing about the kind of paintbrush the painter has used. It's so sad. reply m000 7 hours agorootparent> It's like seeing a beautiful painting being painted and arguing about the kind of paintbrush the painter has used. No it is not. You don't use your browser for its artistic value (which is in the eye of the beholder). You also don't make an announcement for a painting. This is more like using a non-inox screw in a high-humidity environment. Yes, it will hold on for a while. But it is objectively a bad choice. Non-inox may had been the only choice 300 years back, but this is 2024. And writing a browser from scratch is a huge undertaking. When you invest resources in such a project, you probably want it to be more weatherproof than its C/C++-based predecessors. So it is quite reasonable IMHO to ask why C++ was chosen for this project. reply voidwtf 8 hours agorootparentprevThat’s a pretty cynical view. It’s more like asking why a car manufacturer is developing a car without modern safety mechanisms despite knowing those mechanisms save lives. If this is a work of art whose code is to be admired and only viewed as a creative work, fine I’m sorry I asked the question and that it was criticism of one’s vision. However, if this is something that is intended to be “driven” by other users in the future I think it’s a perfectly acceptable question to ask why more modern safety mechanisms are not being employed. Maybe there is a rationale I’m unaware of or a reason why those mechanisms are not employed. reply jraph 7 hours agorootparentIt's not cynical. It's how many of us feel when we read a comment like this. I believe you are right. But I also think that your phrasing is not very nice. It shows a lack of empathy and understanding and feels entitled. You could convey essentially the same message but be way nicer to everyone involved, and that would be way more efficient. If your ideal is a browser engine written in a safer language, and want to work towards that goal, phrasing things the way you did in your comment is one of the worst way to do it because you risk putting off people and they will associate this bad feeling to your idea. See how people react to comments about Rust. Some of your options are: - writing a browser engine yourself, in a safer language - contributing to an existing browser engine like Servo - convince projects to switch to a safer language, or to accept contributions in a safer language - convince someone or a group of people to do those things - fund such an enterprise At this point, the world needs proof that a browser engine can practically be written in something else than C++, because it's what all three major engines are written in. There are strong evidences that Rust can be an option given the existence of Servo, but look how Ladybird is progressing so absurdly faster than Servo. If you don't work yourself towards your goal, you can only humbly share your wish. Lobbying is fine too, but you really need to make sure you don't make others hate your idea because of the way you communicate. Specifically in this topic, you need to take in account that many people are already annoyed by the numerous \"why not Rust\" comments, so you are walking on eggshells. What's more, don't forget the global picture, and that security (although critical, we agree) is only one aspect. Security is irrelevant in a project that doesn't even exist. C++ is better than other languages for a lot of reasons in other aspects and you will need to address this, in the context of writing a browser. Good luck in your endeavors, I hope you succeed, I believe it's a good ideal. reply voidwtf 7 hours agorootparentI will attempt to word my questions/thoughts in a less confrontational way in the future. I tend to write my questions in the same way they occur in my internal monologue. I understand how that comes across to some. I’m happy there is someone out there genuinely interested in creating another alternative to the near monoculture that is web browsers. I hope that they gain traction in the open source community as well as wider adoption upon maturity. I’ll try to follow the project development and maybe I can learn myself why other languages may not be as well suited as C++. It seems that language proficiency is the most common answer I’ve received besides earned criticism of the way I formed my question. reply red_trumpet 9 hours agorootparentprev> It's like seeing a beautiful painting being painted and arguing about the kind of paintbrush the painter has used. It's like seeing a beautiful painting and realizing the painter didn't use lightfast colors. In ten years the painting will not be beautiful anymore. Yes, the painter / author put in a lot of work, and this deserves acknowledgement. But ones decision to use it or not is not only based on the amount of work put in. reply account42 8 hours agorootparentIt's more like seeing a beautiful painting and complaining that the painter didn't use your favorite brand of paint which promises to be much better looking than the other paints in ten years but hasn't even been around that long. reply nindalf 8 hours agorootparentThis analogy has been tortured beyond usefulness. reply aniviacat 10 hours agoparentprevOn the v8 engine's blog, it is claimed that most of its vulnerabilities are caused by logic issues which Rust wouldn't help with. Perhaps it's a similar situation for Ladybird. >Memory safety remains a relevant problem: all Chrome exploits caught in the wild in the last three years (2021 – 2023) started out with a memory corruption vulnerability in a Chrome renderer process that was exploited for remote code execution (RCE). Of these, 60% were vulnerabilities in V8. > V8 vulnerabilities are rarely \"classic\" memory corruption bugs (use-after-frees, out-of-bounds accesses, etc.) but instead subtle logic issues which can in turn be exploited to corrupt memory. As such, existing memory safety solutions are, for the most part, not applicable to V8. In particular, neither switching to a memory safe language, such as Rust, nor using current or future hardware memory safety features, such as memory tagging, can help with the security challenges faced by V8 today. See: https://v8.dev/blog/sandbox reply debugnik 8 hours agorootparent40% may not be a majority, but it's still close to half of all Chrome exploits in the wild, and they could be avoided with a memory-safe toolchain. (Doesn't have to be Rust.) As for the other 60%, they point to logic errors as the root cause, but that's true of all memory corruption bugs: they wouldn't exist if there weren't logic errors behind them. The actual difference here is that the vulnerabilities are either in the machine code generated by the JIT (e.g. type confusion), rather than V8's own code; or they're in code they insist must be memory-unsafe for performance reasons. So the takeaway there should be that JS engines for hostile code should either not use a JIT at all, nor memory-unsafe code paths, or use stronger tools to verify the correctness of the JIT and those code paths. But hey, retaining the capability to speed up bloated web apps ever so slightly is more important that reply timeon 6 hours agorootparentprev> which Rust wouldn't help with Technically no. But if you can decrease those 40% where it could help you can than focus more on the logic issues. Maybe. reply kroolik 9 hours agorootparentprevI believe the fact that V8 vulnerabilities are not \"classic\" memory corruption can be attributed to their developers' experience and review processes. This doesn't imply, though, that another project in C++ will share these traits. reply skywal_l 9 hours agoparentprevAndreas (the author of ladybird) started a language[0] that would be memory-safe and in which he would eventually write SerenityOS (and I assume LadyBird too). He hasn't committed to it for 6 months now so not sure what the status is. At the end of the day, LadyBird is still a hobby project, so one of the main objective is to have fun which does not always coincide with rationality (although the decision to move on from NIH[1] is a sign that this might be changing). [0] https://github.com/SerenityOS/jakt [1] https://en.wikipedia.org/wiki/Not_invented_here reply jraph 9 hours agorootparentLadybird is sponsored now and I seem to remember that Andreas is paid full time to work on it. I don't think Ladybird is still strictly a hobby project anymore. But it was definitely started as a hobby project so your point still stands, mostly. (to be clear, I'm not answering to the question of which programming language should be used to write Ladybird) reply asddubs 9 hours agorootparentprevJakt was described more as an experiment and to potentially replace C++ in the codebase, rather than definitely. I haven't seen any official word on this but as you imply I would assume that this effort is essentially dead now. Just as with the operating system itself, the focus eventually shifted elsewhere and Jakt was left behind. reply surgical_fire 9 hours agoparentprev> but I'm curious why we're still writing applications > Have we not learned our lesson yet, \"We\"? Do you speak in the name of the developer? What an odd choice of pronoum. Either way, if you are adamant about writing a new browser in your \"memory-safe language\" of choice, be the change you want made. Go ahead and write a new browser from scratch. Show the world how it should be done. reply voidwtf 8 hours agorootparentI meant we as in the collective, and I explicitly referenced the collective of those writing applications that process and/or execute user generated content. So no, I don’t think it appropriate to focus my question on the author or this project. I am wondering if C++ is a better choice for reasons I don’t understand when it comes to HTML/JS/CSS, despite potential security implications as the complexity grows. reply ArtixFox 6 hours agoparentprevBecause writing a browser in ATS or Frama-C is not possible. Or writing it in Coq and exporting it to ocaml and friends. Ada can be used but its mediocre at best and sucks at places where frama-C shines. A lot of pointer stuff that is easier to prove in ATS or frama-C is impossible in Ada [and by extension of that, rust] TLA+ can be used in mix with C++ just like how 90% of safety critical software is written but the development speed is very slow for it. That leaves Rust, which is a...mediocre language if you really care about safety. It has no idea of linear types that ATS has and cannot carry embedded proofs like Frama-C. Plus the culture regarding safety critical software in C/C++ is humongous and tbh fairly amazing. I cannot remember exactly but there are many projects targeting C++ for safety purposes, even the people behind frama-C are creating something called as frama-Clang to allow writing safety critical code in C++ backed by proofs. Are there any other languages that im missing? If ATS has a prettier syntax, it'd be my candidate for writing a web browser in. reply pjmlp 9 hours agoparentprevNote that despite that study, Windows and XBox teams are quite found of their C and C++, and even their own .NET has more success on the Azure side, than replacing all those COM/WinRT C++ workloads, and extension points. It is Azure that is more keen in adopting memory safe languages, and has the mandate that new systems code should be done using them. reply TacticalCoder 6 hours agorootparent> Note that despite that study, Windows and XBox teams are quite found of their C and C++... And we all know how secure the average user's Windows computer is. And Windows' security is so good that it's Windows who's powering tens of billions of servers, smartphones, IoT, appliances, routers etc. throughout the world? Oh, wait, no... These are all running Linux. And the uptime. Let's not forget the uptime with patch tuesday. Windows does not strike me as the ecosystem we should strive to immitate. reply pjmlp 6 hours agorootparentIn what language is the Linux kernel written on? https://www.cvedetails.com/product/47/Linux-Linux-Kernel.htm... Those that have glass ceilings should not throw rocks. reply peoplefromibiza 10 hours agoparentprev> in languages that don't maintain strict memory safe contract? for the same reason people still use the English language, despite being full of crazy inconsistencies and being very hard to become a native speaker, coming from another language: proficiency. Proficiency is one of the most, if not the most, valuable metric when choosing the tool you will use to take on some complex/daunting task. reply elygre 9 hours agorootparentThe language “legalese” was invented when it turned out that being proficient in English does not protect you against malicious contract partners. (The success of legalese is still debated, but its existence is generally accepted) reply peoplefromibiza 9 hours agorootparentlegalese is a subset of English. BTW Rust (or any other so called \"memory safe\" language) is not the equivalent of legalese, it's the equivalent of using French because it's the \"language of diplomacy\" (that's why many English words come from French) instead of English. If you're not proficient in French, French legalese won't save you. reply oriolid 9 hours agorootparentCould you explain your thinking a bit more? To me the \"language of diplomacy\" equivalent for computers sounds more like C calling convention, HTTP and XML or JSON. reply torstenvl 9 hours agorootparentNobody is saying there is a programming equivalent of the language of diplomacy. He said that using Rust to achieve memory safety is analogous to using French for diplomacy, in that, whatever value the language itself might bring to that goal, you are not likely to achieve it if you are not proficient. If you are not proficient in French, you likely ought not to conduct diplomacy in French. If you are not proficient in Rust, you likely ought not to achieve memory safety by writing everything in Rust. reply oriolid 8 hours agorootparentOk, I'll try to explain. - French itself does not add much value to diplomacy. The reason to use is that everyone else who does diplomacy is expected to know French (and probably isn't a native speaker which makes things a bit more equal). English is probably taking over there, like it has done in other domains. - The recent C++ versions are not actively promoting shooting yourself in the foot like older variants, but they aren't exactly trying to prevent it. - Rust is going out of its way to prevent writing memory unsafe code. It is still possible if you know what you are doing, but just trying out stuff at random is more likely to give you a compile-time error than undefined behaviour. - Most programmers aren't very competent, not matter what they believe about themselves. With Rust they are less likely to commit serious errors. Or get anything done, but that's a separate discussion. The French will probably point out your pronunciation mistakes too before continuing discussion, but that's also not the point here. reply torstenvl 7 hours agorootparentDon't do this. Requesting an explanation just to get an excuse to launch into a spiel is deeply dishonest. Furthermore, ideological battle is against HN Guidelines. reply peoplefromibiza 8 hours agorootparentprev> - French itself does not add much value to diplomacy. Funny, given that the word diplomacy is a French word, together with embassy, treaty, alliance, passport and protocol :) > Rust is going out of its way to prevent writing memory unsafe code But if someone is not proficient in Rust it will only slow them down and they'll end up fighting the language and the compiler instead of using the language. It's a common complain among non Rust programmers. > Most programmers aren't very competent I strongly believe Andreas Kling is very competent. For the rest of us who are not him, incompetence does not go well in hand with Rust, which is a very complex language. EDIT: pretending that a very proficient C++ programmer will chose Rust because \"it's 2024\" it's the same thing as pretending that they will chose Haskell, which is equally memory safe and also equally complex. Why nobody ever recommend Haskell or Smalltalk? It doesn't seem much like a discussion about memory safety to me, but rather promoting Rust. reply oriolid 8 hours agorootparent> Funny, given that the word diplomacy is a French word This is true. But it only tells about the cultural dominance that France had at the time the convention started. If history had happened differently, Chinese, Hindi or something else could be in similar position. > But if someone is not proficient in Rust it will only slow them down and they'll end up fighting the language and the compiler instead of using the language. This is indeed the choice. Make it difficult to write code but more likely that the result is correct, easy to achieve high performance but risky (C++ and similar) or just accept the overhead of checking everything over at run time (JVM and CLR languages, etc). I would say there is a niche for the first. > Why nobody ever recommend Haskell or Smalltalk? I think at this point it's well known that the pure functional lazy evaluation model rules out too many useful data structures and makes it easy to introduce accidental complexity. As for Smalltalk, it seems (I've never actually used it) to me that most of its once unique ideas have been copied to current mainstream languages. It also seems to have a huge number of fragmented implementations and most of them seem to have a heavy runtime virtual machine. reply peoplefromibiza 7 hours agorootparent> This is true. But it only tells about the cultural dominance that France French is still very much relevant, but it took centuries to make it less relevant than before to the point where we are now. Rust in comparison is minutes old and there's no evidence it will dominate the field of system programming in the future. See: Ruby on Rails for web development. > This is indeed the choice. Make it difficult to write code but more likely that the result is correct I don't buy it. Making it hard to write code it's nobody's choice, it's accidental complexity, that any sane language designer would avoid if possible, because it severely hinders the language adoption. The opposite is also true: code easy to write will also be more easily correct. Elixir is easy to write and will almost automatically be correct in complex scenarios such as distributed systems. > pure functional lazy evaluation model rules out too many useful data structures Rust is Haskell with a different syntax though and makes it very hard to write simple linked lists. > most of its once unique ideas have been copied to current mainstream languages same happened to FP, ROR and it's happening with Rust Even Java is more functional than ever, because it's a good paradigm, not because it's a fad. Again: this seems to me more promoting Rust than a discussion about memory safety and I am really not interested in that. So I'll see myself out. reply oriolid 5 hours agorootparent> Rust in comparison is minutes old and there's no evidence it will dominate the field of system programming in the future. This is true (for some values of minute). It is also why I was suggesting that C calling convention, HTTP etc, not Rust, would be the computing lingua franca. Now that I think of it, a few years ago TCP/IP would have been on the list but now with HTTP/3 it's not that certain any more. > Rust is Haskell with a different syntax though... This is quite bold claim, but if you have a rigorous proof beyond \"all Turing complete languages are the same\" I would be interested in seeing it. It's a pity you left. > ...makes it very hard to write simple linked lists. This is interesting in the light of the beginning of the sentence, because in Haskell linked list is the easiest data structure. Simple linked lists aren't always that simple though. There is a reason why they used to be a recurring technical interview question. > Again: this seems to me more promoting Rust than a discussion about memory safety Funny, to me this seems more about promoting JIT and garbage collection. Nothing wrong with that, as long as you admit that there are niches where those are a problem but memory safety is still useful. And so far there haven't been other serious language candidates for that niche. reply voidwtf 7 hours agorootparentprevThis makes the most sense and thank you for offering a genuine answer to my question. Which leads to a follow-up question. Has the rise of these memory safe languages caused any shift in the proficiencies of the average developer? I see a lot of younger people gush over python early in their careers but see a lot of Java/.NET in enterprise. I personally grew up learning Delphi, PHP, and HTML. Java and .NET came later, but I rarely had a hand in initiating the projects so my language proficiency typically flowed with the job/project I was working on professionally. reply gkbrk 9 hours agoparentprev> why we're still writing applications > Have we not learned our lesson yet Why are you speaking like this project is asking you to write code in C++? You are free to exclusively write Rust. Other people writing C++ has 0 impact on what you're writing or what lessons you've learned. reply voidwtf 8 hours agorootparentIf the eventual goal is to place the application in the hand of users, I think the question has merit. reply alexvitkov 8 hours agoparentprevBecause rust is a cargo cult. We're nearing 10 years since 1.0 and there's still not a single serious, widely used, large software project to prove its viability. Maybe make one instead of trying to passive aggressively convince others to do it for you? reply voidwtf 7 hours agorootparentI didn’t mention rust, I don’t know Rust and if I were trying to start a similar project I might do so in .NET Core because of my familiarity with .NET. I’m asking because I don’t know if there is a reason it would not be viable, is the garbage collection too much of a hurdle to get decent performance while trying to interpret JavaScript? Is there too much overhead tracking claims on memory and doing the bounds checks? I’d assume the same checks eventually have to be written in C++ too? reply alexvitkov 4 hours agorootparentI apologize - when people mention \"memory safe\" nowadays 95% of the time they're talking about Rust, I made an assumption. Interpreting JavaScript at all it at all is a problem. Ladybird's LibJS compiles it to bytecode and interprets that (which is usually better than intereprting the AST). The bytecode interpreter is written in C++, and it's still pretty damn slow - websites take a long time to load, and LibJS is the main bottleneck. The reality is, modern websites throws so much junk at your JS implementation, that you basically need to JIT-compile it in order to have any sort of reasonable performance. And with JIT all memory safety guarantees are thrown out of the window - it doesn't matter if you write your compiler in Rust, C++ or a .NET language - if there's an exploit it's disproportionately more likely to be in the output assembly than it is to be in your compiler. Browsers nowadays make a best effort, and they have a stack of other mitigations in case the JIT leaks: https://chromium.googlesource.com/chromium/src/+/main/docs/d... reply einpoklum 7 hours agoparentprev> writing applications that will almost certainly execute or process hostile content in languages that don't maintain strict memory safe contract? That point is 50% FUD. A language which \"maintains strict memory safe contracts\" but has an `unsafe` keyword; or has a \"native code interface\", or uses libraries implemented in a different language, doesn't really strictly maintain its guarantees. And on the other hand, a language in which you can, in principle, load and execute arbitrary code from a string you got from the user, can be hardened very well by statically-checkable constraints. So, it's a matter of degrees rather than absolutes. If you then add considerations such as programming paradigm flexibility and performance, C++ is very much a valid choice even for the use case of a browser. reply flumpcakes 9 hours agoparentprevIt's crap programmers that write buggy code, and they will write similarly buggy code in any language. It's not hard to write memory safe code, most people are not skilled enough to do it. I doubt they will be skilled enough to write good rust. reply durandal1 7 hours agoparentprevWhere is the proof that Rust can achieve the productivity and design flexibility needed at browser scale? reply javier_e06 5 hours agoprevLadybird has a good use case on embedded projects where there is a need to use a well understood API for small gadgets acting as clients. I wish success. reply imiric 10 hours agoprevI'm very happy that projects like this exist, and hope that they succeed in their mission. With the current state of the web, filled with mostly spam content designed to generate ad revenue and to exploit the user, accessed from mainstream browsers produced by large corporations that are increasingly trying to do the same thing, web users are being squeezed from all sides. Using the web today is a hostile experience, and the only safe haven from all this nonsense is using community-supported alternative browsers, that are really stripped down versions of mainstream ones, and relying heavily on ad, cookie, JavaScript and other blockers, which may stop working at any point. This is a difficult task only tech savvy users can realistically do, while most other users have no choice. A new, independent, browser alone will not solve this, but it's certainly a step in the right direction. reply KronisLV 10 hours agoparent> Using the web today is a hostile experience, and the only safe haven from all this nonsense is using community-supported alternative browsers, that are really stripped down versions of mainstream ones, and relying heavily on ad, cookie, JavaScript and other blockers, which may stop working at any point. I think the actual root of the problem is that the people and organizations developing and running the sites do want to force the ads, analytics and other things upon you and you as a user basically have to hack around that. If the users actually took a stance with something a bit like https://en.wikipedia.org/wiki/GNU_LibreJS then the sad reality would be that you just couldn't use most sites altogether. reply skywal_l 9 hours agorootparentI found NoScript to be a good alternative. It disables JS by default but then you can reenable per page but also per domain. You only enable just what you need for the page to work and sometimes only temporarily. I still need to enable things here and there but it's a fairly easy straightforward process. Couple this with uBlock and firefox Container (where I am logged to google only for gmail in a specific container) and the web is pleasant again reply mrob 9 hours agorootparentuBlock Origin already supports Javascript blocking (both globally and per domain) if you enable advanced mode. Redundant extensions are just extra attack surface. reply pandemic_region 10 hours agoprev> Ladybird now targets Linux and macOS. The SerenityOS target is dropped. What does this mean, that Ladybird will no longer run on Serenity OS? And that is up to the Serenity peeps to make it run should they wish to do so? reply nhinck3 8 hours agoparentYeah and I believe the reason given is because Ladybird wanted to use third party code and SerenityOS has a no third party code policy which made targeting Serenity kind of hard. reply jillesvangurp 10 hours agoparentprevSerenity is just another Linux. So they target all Linux distributions instead of this specific one. reply tyingq 10 hours agorootparentI think you might have it confused with something else. SerenityOS is a ground-up OS with it's own kernel, etc. It is NOT linux. reply lupusreal 10 hours agorootparentprevSerenityOS is not a Linux distro. reply KingOfCoders 11 hours agoprevStill hoping for future Windows support to replace Firefox. reply ruzig 10 hours agoprevThe founder usually lives to stream his work—very impressive skills. reply sambull 7 hours agoprevMy disappointment for what this means for SerenityOS is pretty high. reply kosolam 7 hours agoprevOnly bots around us reply kosolam 10 hours agoprevWhy build another browser, from scratch, in C++? reply hello_kitty2 5 hours agoparentFrom GitHub. > Ladybird is a truly independent web browser, using a novel engine based on web standards. So I assume they want to build a truly independent web browser instead of leaving the Browser market to Google, Apple, and Firefox (relies heavily on Google ad revenue.) and executives that run them who are primarily motivated by money. reply kosolam 2 hours agorootparentWhy build from scratch? If building from scratch, why do it in c++? reply haspok 10 hours agoparentprevWhy not? Some people are hacking on 30+ year old 8 bit systems... just for the fun of it. reply 1GZ0 9 hours agoparentprevBecause its fun, and because it might break the chromium monopoly we're currently living in, some day far in the future. reply marginalia_nu 7 hours agoparentprevWhy aren't you building one in another language if that is something you feel is missing? reply kosolam 9 hours agoparentprev“Why not, because it’s fun, because they can”, These kind of answers kinda underline the question even more… reply segfaltnh 7 hours agorootparentThey do not. reply storoj 10 hours agoparentprevBecause they can reply sprash 10 hours agoprevAs long as they don't get rid of the Qt dependency the project is a bit pointless. If you are using Qt anyways Qt Webview offers already a superior way to render HTML than Ladybird. reply flohofwoe 9 hours agoparentQt webview is just Chromium in a trenchcoat. The point is to create a new browser engine, not a new Chromium UI (and hopefully much less bloated than Chromium). reply 1GZ0 9 hours agoparentprevGUI libraries and a browser engines are vastly different things. Think of Ladybird as Blink, Webkit or Gecko. reply nurbl 10 hours agoparentprevI think the point is the opposite; they have decided to build a new \"web stack\" from scratch, not just build a new \"browser\" (or invent a new GUI framework). Hopefully the web engine is not deeply tied to Qt, but you need something in order to draw an interactive window. The article mentions that they will also use existing libraries for things like font rendering. Seems like a pragmatic decision to me. reply amelius 10 hours agoparentprevNo, because with the current architecture they can more easily migrate away to different GUI libraries. reply samiv 9 hours agoprevI'm already speculating that sooner or later in the future this project will end up pivoting to chromium as the underlying engine. I can think of two reasons for this - money is involved and someone buys them out / influences the direction of the project - the reality of the task ahead sets in the and the dev team gives up on custom engine development For those of you coming back to this comment from the future, yep \"told you so\". ;-) reply frou_dh 8 hours agoparentDo you think you have a better handle on the \"reality of the task\" than Kling, who already worked at Apple on WebKit and subsequently got Ladybird to where it is now? reply asddubs 9 hours agoparentprevI doubt it. Your prediction might be more appropriate for a project that's in its first week of development, not having been developed for several years and already overcome many difficult hurdles. And if they switch to chromium, there is literally nothing interesting about this project. The fact that it's a novel engine is their only differentiating feature. reply samiv 8 hours agorootparentThe age of the project or the development status of the project does not matter, what matters is the user base. If you build anything that accumulates users, business interests will be aroused at some point. Business people don't give a shit about the technology itself unless of course the technology itself really is a USP and can be converted into €$. But most people (end users) don't care much about the underlying technology in the browser and whether it's open source or not, chromium or not. Only technologists care. reply torstenvl 8 hours agorootparent> The age of the project or the development status of the project does not matter If you have any evidence to support this, you could become quite well-known for disproving the Lindy effect's applicability to software projects. reply account42 8 hours agorootparentprevIt may be hard for you to believe but there are projects not run by \"business people\". reply 1GZ0 9 hours agoparentprevYou really think Google has people running around with suitcases full of money, tempting devs to abandon their own browser engines in favour of chromium? reply 25 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ladybird is an open-source web browser project, forked from SerenityOS by Andreas Kling, aiming to be independent of Chrome and written in C++ under a BSD license.",
      "Although still in early development and lacking many features, Ladybird shows promise with basic functionalities and significant community contributions, targeting Linux, macOS, and UNIX-like systems, with Windows support via WSL.",
      "Community reactions are mixed, with some seeing potential and others suggesting focus on existing browsers, but supporters argue that new projects like Ladybird are essential for a healthy browser ecosystem."
    ],
    "commentSummary": [
      "Ladybird browser, a new community-built web browser, is gaining attention as a potential daily driver, diverging from mainstream options like Chrome and Firefox.",
      "Discussions highlight the challenges of creating a simpler, more secure browser that supports only a subset of web technologies, balancing functionality and user adoption.",
      "The project is seen as a valuable learning opportunity for new developers, with well-documented build processes and broad areas for contribution."
    ],
    "points": 392,
    "commentCount": 278,
    "retryCount": 0,
    "time": 1718951737
  },
  {
    "id": 40746310,
    "title": "MeshAnything – Converts 3D representations into efficient 3D meshes",
    "originLink": "https://buaacyw.github.io/mesh-anything/",
    "originBody": "MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers Yiwen Chen1,2*, Tong He2†, Di Huang2, Weicai Ye2, Sijin Chen3, Jiaxiang Tang4, Xin Chen5, Zhongang Cai6, Lei Yang6, Gang Yu7, Guosheng Lin1†, Chi Zhang8† 1S-Lab, Nanyang Technological University, 2Shanghai AI Lab 3Fudan University, 4Peking University, 5University of Chinese Academy of Sciences 6SenseTime Research, 7Stepfun, 8Westlake University *Work done during a research internship at Shanghai AI Lab. †Corresponding Authors. Paper arXiv Code Gradio MeshAnything mimics human artist in extracting meshes from any 3D representations. It can be combined with various 3D asset production pipelines, such as 3D reconstruction and generation, to convert their results into Artist-Created Meshes that can be seamlessly applied in 3D industry. Comparison with Previous Methods MeshAnything generates meshes with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods. Abstract Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. However, this potential is largely unrealized because these assets always need to be converted to meshes for 3D industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to Artist-Created Meshes (AMs), i.e., meshes created by human artists. Specifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality. To address these issues, we introduce MeshAnything, a model that treats mesh extraction as a generation problem, producing AMs aligned with specified shapes. By converting 3D assets in any 3D representation into AMs, MeshAnything can be integrated with various 3D asset production methods, thereby enhancing their application across the 3D industry. The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned decoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE, then train the shape-conditioned decoder-only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. Our extensive experiments show that our method generates AMs with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods. Method MeshAnything is an autoregressive transformer capable of generating Artist-Created Meshes that adhere to given 3D shapes. We sample point clouds from given 3D assets, encode them into features, and inject them into the decoder-only transformer to achieve shape-conditional mesh generation. Compared to methods like MeshGPT that directly generate Artist-Created Meshes, our approach avoids learning complex 3D shape distributions. Instead, it focuses on efficiently constructing shapes through optimized topology, significantly reducing the training burden and enhancing scalability. More Results By integrating with various 3D asset production methods, our approach achieves highly controllable Artist-Created Mesh generation. Besides, we compare our reseults with ground truth in (b) and (c). In (b), MeshAnything generates meshes with better topology and fewer faces than the ground truth. In (c), we produce meshes with a completely different topology while achieving a similar shape, proving that our method does not simply overfit but understands how to construct meshes using efficient topology. BibTeX @misc{chen2024meshanything, title={MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers}, author={Yiwen Chen and Tong He and Di Huang and Weicai Ye and Sijin Chen and Jiaxiang Tang and Xin Chen and Zhongang Cai and Lei Yang and Gang Yu and Guosheng Lin and Chi Zhang}, year={2024}, eprint={2406.10163}, archivePrefix={arXiv}, primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'} } This website is constructed using the source code provided by Nerfies, and we are grateful for their template.",
    "commentLink": "https://news.ycombinator.com/item?id=40746310",
    "commentBody": "MeshAnything – Converts 3D representations into efficient 3D meshes (buaacyw.github.io)247 points by flockonus 13 hours agohidepastfavorite55 comments modeless 13 hours agoNice looking results, hopefully not too cherry-picked. Every 3D model generation paper posted on HN has people complaining that the meshes are bad, so this kind of research is welcome and necessary for generated 3D assets to be used in actual games. Weird custom non-commercial license unfortunately. Notes from the GitHub readme: > It takes about 7GB and 30s to generate a mesh on an A6000 GPU > trained on meshes with fewer than 800 faces and cannot generate meshes with more than 800 faces reply p_l 10 hours agoparentHonestly I like the faces limitation - use it as input for blender to further develop the model, not as final product. reply bhouston 7 hours agoprevDefinitely the best result for low polygon creation I've seen. Great job! Still triangles rather than polygons, but we are getting closer. The end goal should be: 1) Polygons, mostly 4 sided, rather than triangles. 2) Edge smoothness/creases to separate hard coders from soft corners. (Which when combined with polygons enables SubD support: https://graphics.pixar.com/opensubdiv/docs/subdivision_surfa...) 3) UV for textures that are aligned with the natural flow of textures on those components. 4) Repeating textures (although sometimes not) that work with the UVs and combine to create PBR textures. (Getting closer all the time: https://gvecchio.com/stablematerials/) After the above works, I think people should move on to inferring proper CAD models from an image. Basically infer all the constraints and the various construction steps. reply bognition 6 hours agoparentwhy are polygons preferred over triangles? reply mdorazio 6 hours agorootparentIt's much easier and cleaner to subdivide quads to refine shapes when modeling. For example, you can split the quads along an entire edge to get a new clean edge for manipulation (ex. to bevel it). If you try to do the same with triangles, you get a jagged mess. reply ramshanker 10 hours agoprevI am all in for any development in this domain. Just to spread some sense of scale, We recently processed (manually) the point cloud scan of one of the ( For the same reason, I feel puzzled looking at Industrial scenes in Video Games. They are like 3 order of magnitude simplified compared to a real plant. Because they are games, not oil refinery simulators. They are typically intending to only convey a general sense of “industrial environment” and nothing more. Do your models of oil refineries include the correct grass and other plant species growing in cracks in the pavement? reply Bjartr 6 hours agorootparent> they are games, not oil refinery simulators. That's an excellent point. I do feel compelled to mention the exception of oil refinery simulator games. Maxis (of SimCity, The Sims fame) made SimRefinery way back when. https://en.m.wikipedia.org/wiki/SimRefinery reply nkrisc 6 hours agorootparentYes, if a game is in fact a refinery simulator I would expect it to have an accurate representations of oil refineries. But whatever the latest Call of Duty game is? It’s going to be a grey block environment designed for gameplay that then gets covered in industrial props and textures and called a refinery. reply sendfoods 7 hours agoparentprevCould you go into some details here? - sensors used - postprocessing - registration algorithm(s) Are all things that would interest me greatly :) reply CyberDildonics 6 hours agoparentprevI feel puzzled looking at Industrial scenes in Video Games. They are like 3 order of magnitude simplified compared to a real plant. Really? You don't know why video games don't have 80 billion points and you don't know why a tool made to simplify meshes into video game objects isn't using your 80 billion point lidar scan? For starters, these are meshes and you're talking about points. If anyone is meshing those points and they have any sense, they are working with \"toy\" sized chunks too so they avoid doing nearest neighbor calculations on terabytes of data. reply jgord 12 hours agoprevCertainly a lot of scope for this kind of thing .. people who do lidar scans or photogrammetry of buildings tend to end up with very large meshes or very large point clouds.. which means they need souped up PCs and expensive software to wrangle into some usable CAD format. Its an area where things can be improved a lot imho - I did some work a while back fitting flat planes to pointclouds, and ended up with mesh model anything from 40x to 100x smaller data than the ptcloud dataset. see quato.xyz for samples where you can compare the cloud, the mesh produced.. and view the 3D model in recent browsers. My approach had some similarity to gaussian splats... but using only planar regions .. great for buildings made of flat slabs, less so for smooth curves and foliage. Applying their MeshAnything algo to fine meshes from photogrammetry scans of buildings would be of great benefit - probably getting those meshes down to a size where they can be shared as 3D webgl/threejs pages. Even deciding on triangle points to efficiently tesselate / cover a planar region with holes etc, is basically a knapsack problem, which heuristics, monte-carlo and ML can improve upon. reply fsloth 11 hours agoparentIf you want to show photogrammetric pointclouds of buildings potree db&algorithm is pretty good and if you don’t like the library for some reason it’s pretty easy to reimplement (potree.org). You just dump the pointcloud to a hierarchical octree and at viewer end just download the nodes in your frusttum and voila. There are other approaches but this wins hands down on usability/simplicity. reply jgord 10 hours agorootparentIm quite familiar with potree and a big fan .. having hacked some of the internals and added features to my own custom version - so people can share annotations, measurements, save to cloud or export linework .. without writing code/custom html. Also added code to import e57 cube panoramas .. Still, I think if one can use ML to simplify a pointcloud or fine mesh .. then the data is much smaller and cleaner, easier to import to existing CAD tools etc. reply spookie 12 hours agoparentprevCould you go more in depth with your approach? Sounds really usefull for man made structures! reply jgord 11 hours agorootparentahh.. looking for investment, so reticent to talk about the internals. Basically does a lot of matmulls and sampling to find good good planar fits... Most of these sample datasets took an hour to compute, using 12 threads on a midrange CPU... should be doable in seconds if ported to a highend GPU. reply librasteve 11 hours agorootparentsuggest a nanopivot, make it work on Apple AI (if you haven’t already) and position as iPhone AI mesh reply toxik 10 hours agoparentprevSurely you heard about surfels, sounds like a very similar approach. reply jgord 8 hours agorootparentLooked up surfels .. I think they are different from fitting local planes - I was interested in the geometry, not the lighting. But surfels seems like a clever idea, pre-dating view dependent NeRFs. reply Animats 1 hour agoprevHm. I tried the online demo, https://huggingface.co/spaces/Yiwen-ntu/MeshAnything on the provided sample \"hat\". I tried with and without checking \"Preprocess with marching cubes\" and \"Random Sample\". Both outputs had holes in the output mesh where the original did not. Am I doing this wrong, or is the algorithm buggy? reply obsoletehippo 8 hours agoprevI like how the Social Impact paragraph notes reduced labor costs, yay! Not e.g., reduced need for artists, so you're all out of a job. reply dsign 5 hours agoparentIt's a funny euphemism, in a dark sort of way. But if there is a domain where AI is not getting humans out of a job anytime soon, I think it's this one. I've read dozens of papers about remeshing, but for all of the research, very few algorithms make it to production pipelines. And those that do, still crash and fail in spectacular ways, even after a decade or more of refining and bug-fixing. reply bee_rider 5 hours agoparentprevOne group finds a way to automate a job, and then our whole society agrees that the people who previously did that job should be tossed out into the street. But for some reason we blame the first group rather than the second. reply yazzku 26 minutes agoparentprev\"Our method points to a promising approach for the automatically generation of Artist-Created Meshes, which has the potential to significantly reduce labor costs in the 3D industry, thereby facilitating advancements in industries such as gaming, film, and the metaverse. However, the reduced cost of obtaining 3D artist-created meshes could also lead to potential criminal activities.\" That last statement is worded in such a weird way, lol. Funny Chinese->English transliteration. \"The FBI has issued a warning for potential criminal activity resulting from the automatic generation of low-poly models. The public is advised to minimize outdoors exposure and report any suspicious activity.\" reply wildpeaks 12 hours agoprevCalling AI-generated meshes \"Artist-created\" just because it aims to look similar as human-made ones is misleading. reply spookie 12 hours agoparentI think it comes from the fact an artist can create the mesh themselves and optimize them using this approach. reply dagmx 4 hours agoprevThe topology is decent but no artist is creating meshes like this. The name feels mismatched. I’ve seen some better topology generation papers at siggraph last year which addressed quads better, though I’d need to dig through my archive to find it. The triangle topologies in this paper made don’t follow the logical loops that an artist would work as. Generally it’s rare an artist would work directly in triangles, versus quads. But that aside, you’d place the loops in more logical places along the surface. The face and toilet really stand out to me as examples of meshes that look really off. Anyway, I think this is a good attempt at a reasonable topology generation, but the tag line is a miss. reply flockonus 13 hours agoprevMeshAnything generates meshes with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods. reply Joel_Mckay 12 hours agoparentSo does instant-meshes, but it also doesn't necessarily improve the topology. https://github.com/wjakob/instant-meshes Cheers =) reply 42lux 11 hours agoprevThe converted meshes are not efficient. They are also full of n-gons so you need to retopo no matter what... reply andybak 9 hours agoparentI can only see tris. It's quite rare to see .obj files with anything except triangles. Or do you mean something like \"contains implied ngons because of the way coplanar tri faces are arranged\"? reply Lichtso 7 hours agorootparentWhen working with meshes what you generally want is is quads, not triangles. The reason is that quads form nice closed loops. Further more you would only allow quads to meet in 3, 4, or 5 edges per vertex. The 4 edges per vertex is the \"normal\" case that most of your mesh should have, it causes a regular grid of parabolic (euclidian) geometry with neutral curvature. Then patches of these meet in vertices with 3 edges to make it elliptic geometry with positive curvature or 5 edges to make it hyperbolic geometry with negative curvature. You can ignore all of these and just randomly connect nearest neighbors to form triangles. But, then you still have only geometry, no useful topology, so not any better than a point cloud. A good topology is necessary for texturing, skinning, animation etc. reply rurban 8 minutes agorootparentGenerally you would prefer nurbs over quads. But your mesher and viewer needs to be good. reply andybak 7 hours agorootparentprevSure. I (mostly) knew all that. I was specifically asking why you said \"thy are full of n-gons\" - my understanding of the terminology seems to be different to yours in that \"n-gons\" means \"5 or more sides on a face\". i.e. not a tri or a quad. reply 42lux 7 hours agorootparentYour definition is right. Look at the produced models and not the images in the paper. reply andybak 6 hours agorootparentI have generated two models and they consisted of nothing but triangular faces. reply 42lux 4 hours agorootparentMate I really don't know how to help you but even on the examples in the pdf there are clearly n-gons. In 5 of my 10 test there were n-gons. There are always starfishes with 5 or more connected verts. If you want to nitpick on the wording go ahead but these meshes are shite. reply andybak 19 minutes agorootparentI wasn't picking a fight or scoring points. This isn't Reddit and I'm a grown adult. I'm trying to understand what you're saying and maybe learn something in the process. > In 5 of my 10 test there were n-gons. There are always starfishes with 5 or more connected verts. Ok. So you are basing your definition on the number of edges that meet at a vertex. My understanding was that the important metric was \"number of edges on a given face\" reply Paul_S 12 hours agoprevVery good, hope they realise that you need tessellation for shading. Some of those models look a bit too optimised. reply iTokio 13 hours agoprevWords have meanings, you can’t call AI generated meshes, “Artist created Meshes” not matter how good you think your results are. Beside good topology is dependent on the use case, it’s very different if you are doing animation, a 3D print, a game or just a render. reply Joel_Mckay 12 hours agoparentYep, hard to reason with industry people pushing slop on commercial production teams. Low-poly re-mesh tools have been around for ages (some better than others), but there are good reasons pro's still do this step manually all the time. Primarily \"good\" is based on _where_ the quads, loops, and unavoidable n-gons end up in the model (or stuff ends up looking retro 90's.) There is also the complex legal side of algorithms not being able to create copyrightable works in some jurisdictions. Talk with your IP lawyer, this area gets messy fast when something famous or trademarked is involved. Cheers, =3 reply spookie 11 hours agorootparentThat's fair, as someone pretty proficient in 3D modelling I understand your point. However, it also boils down to the scale of the project. Imagine recreating part of real life city, creating a digital twin, for scientific purposes (testing human behaviour in fire hazards, or simply iterating on better park planning and road design for greater perceived safety). There's a lot to be done, and it's difficult to use procedural building methods if your aim is for people to recognize that area. I'm making such a thing myself, purely academic, but god I wish I could speed things up. reply Joel_Mckay 11 hours agorootparentProcedural emission of textures, biomes and cities is not ML/AI generated... Also physics simulation of erosion for landscapes may look natural to most people. The problem is when groups start gleaning styles and artwork from 3rd parties to make something in the same style... they cross an ethical line, and a legal one in some situations (even if the original work is completely isolated from the output.) Thus, while a stochastic parrot may be able to dodge outright plagiarism, it cannot sidestep copyright laws in some Markets. I'd rather pay folks for royalty free content like Poly Haven offers to the community. =3 reply emilk 12 hours agoprevYou can run it yourself here: https://huggingface.co/spaces/rerun/InstantMesh reply T-A 8 hours agoparentThat's InstantMesh, not MeshAnything. The MeshAnything demo is at https://huggingface.co/spaces/Yiwen-ntu/MeshAnything reply column 11 hours agoparentprevI didn't find a way to download the mesh once generated. Is that option not available at all? reply emilern 11 hours agorootparentGood question. You can if you use this instead: https://huggingface.co/spaces/TencentARC/InstantMesh reply debugnik 4 hours agoprevCalling these meshes \"Artist-Created Meshes\" is disgusting. I know researchers in this field want the word \"artist\" to follow the same fate as \"computer\" thanks to their work, but it's too soon to say the least. Can we get AI researchers? I bet RLHF can make their writing more humble than the current ones. Sentiments aside, that's an impressive approach. reply jahewson 13 hours agoprevStunning! reply demondemidi 13 hours agoprevHugh Hoppe is rolling in his grave. reply Jarmsy 9 hours agoparentHugues Hoppe is alive and well! https://hhoppe.com/ reply RobotToaster 2 hours agoprev [–] Why do people keep making their own special licenses? https://github.com/buaacyw/MeshAnything/blob/main/LICENSE.tx... reply yazzku 20 minutes agoparent [–] It's a BSD3-style license with a statement at the end that butchers the whole point of a BSD3 license. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "MeshAnything introduces a novel method for generating Artist-Created Meshes (AMs) from 3D representations using autoregressive transformers, enhancing 3D asset production efficiency and precision.",
      "The approach significantly reduces the number of mesh faces, improving storage, rendering, and simulation efficiencies while maintaining high-quality geometric features.",
      "The architecture employs a VQ-VAE and a shape-conditioned decoder-only transformer, demonstrating superior topology and fewer faces compared to traditional methods, making it a significant advancement in the 3D industry."
    ],
    "commentSummary": [
      "MeshAnything converts 3D representations into efficient 3D meshes, reducing the number of faces for better storage and rendering performance.",
      "The tool requires 7GB of memory and 30 seconds on an A6000 GPU, but is limited to generating meshes with fewer than 800 faces.",
      "While some users criticize its custom non-commercial license and the quality of the generated meshes, it is considered a promising tool for game development and 3D model generation."
    ],
    "points": 247,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1718946145
  },
  {
    "id": 40744558,
    "title": "How babies and young children learn to understand language",
    "originLink": "https://lithub.com/how-babies-and-young-children-learn-to-understand-language/",
    "originBody": "Craft and Criticism Fiction and Poetry News and Culture Lit Hub Radio Reading Lists Book Marks CrimeReads About Log In Literary Hub Craft and Criticism Fiction and Poetry News and Culture Lit Hub Radio Reading Lists Book Marks CrimeReads Log In Via Basic Books How Babies and Young Children Learn to Understand Language Steven Mithen on the Science of Language Acquisition in Early Childhood By Steven Mithen June 20, 2024 How children learn language has long been of interest to those concerned with its evolution. The idea that ‘ontogeny recapitulates phylogeny’ has been promoted, which means the stages of child development on their way to adulthood replicate those of our human ancestors on their way to becoming modern humans. This idea has been applied to language acquisition and its evolution, but I’ve never been persuaded. It is intellectually problematic because our human ancestors were never ‘on their way’ to anywhere other than being themselves. My interest in language acquisition is different and twofold. Article continues below First, is language acquired by specialized mental processes that are dedicated to this task or learned by general-purpose processes used for a variety of learning tasks? Second, can we project the processes of language acquisition/learning that we observe in the present into the prehistoric past to gain insights about the evolution of language? Infants a mere three days old prefer listening to the rhythm of their caregivers’ language. Learning words Let’s remind ourselves of the problem that language learners face. When we write, we leave spaces between words. Readingwordswithoutsuchspacesisdifficult. When we speak, however, our words flow from one to another without any pauses, generating a continuous stream of sound. Yet we hear and understand each word without difficulty—we automatically ‘parse’ the utterances we hear into their component parts. We can do this because we know not only the words and grammar of our language, but also its prosody—how syllables at the start or end of words are stressed and how pauses are used between phrases. Without any awareness of such knowledge, we use it effortlessly both when speaking and when listening; we use it to identify words as they appear within the continuous stream of sound we hear, just as if the words had been written down interspersed with spaces. One can appreciate this with a little reflection. Just think how you pronounce the syllable ham when referring to a piece of meat and when talking about a fury animal—a long ham and a short ham-ster. Think about listening to a language unknown to you, one with different words, grammar and prosody. You will be at an utter loss to identify its words, let alone their meaning. How then, can babies learn language? Although they are born into the world without any knowledge of the language their caregivers will speak, some learning takes places within the womb. The reception of external speech sounds prepares the fetal brain for the processing of language once born. Languages have different rhythms and infants a mere three days old prefer listening to the rhythm of their caregivers’ language, indicating that they became familiarized to it before birth via the sounds heard or the vibrations felt within the womb. Article continues below Nevertheless, babies are born with no specific knowledge of the language they will hear. Yet whether that is English, Mandarin, Swahili or any other of the 7,000 or so languages that remain today, by around one year of age infants are saying their first words, and within another year they are stringing words together to make simple sentences by using the correct order for their native language—those acquiring/learning English and French will place verbs before objects, and those learning Turkish and Japanese will do the converse. Without any formal teaching, by a mere four years of age, most children have acquired more than 1,000 words, the rules for how words can be combined to generate meaningful utterances, and how to change the tone of an utterance to further flex its meaning. Such language acquisition is truly remarkable. How it is achieved has been one of the major questions facing linguists ever since the study of language began, generating a constant flow of new ideas and evidence. We have already dismissed arguments about Universal Grammar in Chapter 3 and can now move on to more interesting and viable ideas. Within the voluminous literature there are two questions of particular interest for the language puzzle. The first is how infants find discrete words in the continuous stream of speech they hear. The second is how they learn the meaning of those words. By a mere four years of age, most children have acquired more than 1,000 words. Finding words How can babies possibly discover where a word begins and ends within a continuous sound stream? A breakthrough in our understanding came in 1996 in a three-page article in the journal Science entitled “Statistical learning by 8-month-old infants,” authored by the psychologists Jenny Saffran, Richard Aslin and Elissa Newport. This explained that infants use “transitional probabilities” (TPs) between syllables to identify which syllable strings recurrently go together, and hence are likely to constitute words, and which syllables have low probabilities of following each other and hence are likely to mark the break between words. The phrase pretty baby, for instance, has four syllables (pre-ty-ba-by) and three transitional probabilities between syllable pairs. In English the probability that ty will be followed by ba is lower than pre will be followed by ty, and that ba will be followed by by. That eight-month-old-infants can calculate and use such transitional probabilities came as a surprise. Jenny Saffran and her colleagues had exposed infants to no more than two minutes of continuous speech that contained four three-syllable nonsense words, such as tupiro and padoti. These ‘words’ were repeated in random order by a monotone speech synthesizer that created a continuous sound sequence, such as bidakupadotigolabubidakupadotigolabubidakutupiro… The sequence contained no pauses, variations in stress or any other acoustic cues between word boundaries. The only cue available to the infants were the transitional probabilities (TPs) between syllables. Those within words were 1.0, because the first syllable was always followed by the second, and the second by the third, while the TPs of syllables between words was always 0.33. After a mere two minutes of listening, the infants were tested as to whether they differentiated between words (syllable strings that had TPs of 1.0) and non-words (syllable strings that contained TPs of 0.33). For this the infants were presented with words and non-words and found to have longer listening time for the nonwords. This indicated they had already become familiar with the words by listening to the continuous sequence of syllables within which they had been embedded. The only way that could have happened was by monitoring the TPs between syllables—the infants were capable of statistical learning. Article continues below ______________________________ From The Language Puzzle: Piecing Together the Six-Million-Year Story of How Words Evolved by Steven Mithen. Copyright © 2024. Available from Basic Books, a division of Hachette Book Group, Inc. Basic Bookscognitionearly childhoodHachette Book Grouplanguagelanguage acquisitionlinguisticsspeechSteven MithensyntaxThe Language Puzzle: Piecing Together the Six-Million-Year Story of How Words Evolvedwords Share: Share on Facebook (Opens in new window) Click to share on Twitter (Opens in new window) Click to share on Google+ (Opens in new window) More Steven Mithen Steven Mithen is professor of early prehistory at the University of Reading and is one of the most esteemed archaeologists working today. He has authored over two hundred academic articles and books, including After the Ice and The Singing Neanderthals, and was elected as a fellow of the British Academy in 2004. He lives in Reading, England. Previous Article Do Authors Really Need to Spend Their Own Money to Make a Book Successful? Next Article Exposing Vulnerability: On Robert Olmstead's Stay Here with Me Support Lit Hub. Join our community of readers. Close to the Lithub Daily Email Submit Popular Posts 1. What Should You Read Next? Here Are the Best Reviewed Books of the WeekJune 14, 2024 by Book Marks 4 2. Ted Chiang has won the PEN/Faulkner Foundation’s short story prize.June 12, 2024 by James Folta 2 3. The Ultimate Summer 2024 Reading ListJune 12, 2024 by Emily Temple 3 4. My Friend, the Legendary Porn QueenJune 6, 2024 by Catherine Gigante-Brown 7 The Best Reviewed Books of the WeekJune 21, 2024 5 Reviews You Need to Read This WeekJune 20, 2024 by Book Marks The Best Reviewed Books of the WeekJune 14, 2024 by Book Marks 5 Reviews You Need to Read This WeekJune 13, 2024 by Book Marks The Best Reviewed Books of the WeekJune 7, 2024 by Book Marks American Psycho and the Rise of Capitalist HorrorJune 21, 2024 by E. K. Sathue Quiz: Can You Identify These Femme Fatales from Classic Crime Novels?June 21, 2024 by Olivia Rutigliano Lovin’ the Southern-Fried Crime Films of the 1970sJune 21, 2024 by Keith Roysdon A Brief History of Bedlam HospitalJune 21, 2024 by Robert Lloyd The Best Historical Fiction of 2024 (So Far)June 20, 2024 by Molly Odintz Follow us on Twitter My Tweets Read More Do Authors Really Need to Spend Their Own Money to Make a Book Successful? Last week The Guardian ran an article about how some authors are seeking help outside of their in-house publishing teams... RSS RSS - Posts Literary Hub Created by Grove Atlantic and Electric Literature Masthead About Sign Up For Our Newsletters How to Pitch Lit Hub Advertisers: Contact Us Privacy Policy Support Lit Hub - Become A Member © LitHub Back to top Become a Lit Hub Supporting Member: Because Books Matter For the past decade, Literary Hub has brought you the best of the book world for free—no paywall. But our future relies on you. In return for a donation, you’ll get an ad-free reading experience, exclusive editors’ picks, book giveaways, and our coveted Joan Didion Lit Hub tote bag. Most importantly, you’ll keep independent book coverage alive and thriving on the internet. Become a member for as low as $5/month Dismiss without supporting Lit Hub x",
    "commentLink": "https://news.ycombinator.com/item?id=40744558",
    "commentBody": "How babies and young children learn to understand language (lithub.com)204 points by Hooke 19 hours agohidepastfavorite161 comments TacticalCoder 6 hours agoWords beginning and endings are learned too because people raising young kids are spending a great deal of time decomposing and telling them simple words. Repeating \"cat\", \"cat\", \"cat\" with pauses. Statistical learning --and there are studies about this-- is also obvious when multi-lingual kids make up words that do not exists. They'll use words from one of the language they know to come up with words (or words beginning / ending) in another language. These words, statistically, could make sense. And they'll pronounce them \"properly\". Yet they don't exist. So it's not just the words: it's the pronunciation too. As the father of a fully bilingual kid (french / english) that was fascinating to watch. reply somelamer567 2 hours agoparentThis really is a thing. My son seems to have learnt the grammar, but not so much vocabulary of English, so he'll borrow vocabulary from Spanish, and force English grammar over it to invent entirely new words. Example: he doesn't know how to say 'to lie' in English ('mentir' in Spanish). So he says: \"You are _menting_\" to say that somebody is lying. reply andruby 4 hours agoparentprevThank you for this. My kids are bilingual in Dutch / English and often make up such words. I like that they'll use words from different languages in the same sentence. School disagrees though. I've once heard someone say that bilingually raised people have a higher chance of becoming schizophrenic, although I really doubt that has any major influence. reply brnt 2 hours agoparentprevSometimes I feel a little bad we're foisting 4 languages on our kids, plus two more within a 20 minute drive so I expect some basics there will be picked up too. Our eldest likes language battles: she's corrects me in another language but the game is that I must continue in the 'wrong' one. reply bunderbunder 3 hours agoparentprev> Words beginning and endings are learned too because people raising young kids are spending a great deal of time decomposing and telling them simple words. Repeating \"cat\", \"cat\", \"cat\" with pauses. I'm pretty sure that years ago I read in a textbook somewhere that the effect size for this kind of thing is very small relative to other \"parent talk\" behaviors like exaggerated intonation? (But I also wouldn't be surprised if that was just the author's take and not at all a settled matter.) reply alexey-salmin 15 hours agoprev> Think about listening to a language unknown to you, one with different words, grammar and prosody. You will be at an utter loss to identify its words, let alone their meaning. My experience learning French basically. I'd say understanding where one word ends and another starts was much easier for English and German. On paper I was able to grasp the rough meaning very quickly thanks to vocabulary shared with English and Latin, but listening took a year: I was facing a solid wall of sound, no cracks. reply maxcan 6 hours agoparentThis may come off as a little inappropriate but one thing I think about when learning another language is the usual mistakes or quirks that native speakers of that language display when speaking English. It turns out that quite often those are reflective of the correct form in their language. It's helped me sound more natural in Japanese and Hebrew. reply wvh 2 hours agorootparentI speak a bunch of languages, and this is especially helpful when learning languages that have a specific own idiomatic sentence structure. You get a pretty good insight into how people form sentences in their native language based on how they garble English. You can often derive where people are from based on those typical mistakes, even in a few sentences on an online forum. reply alex_duf 10 hours agoparentprevI was facing the same issue when I learned English (I'm French), and I found that watching TV Shows with the subtitles helped a lot. So you get to hear that wall of sounds, but you get the words spelled out under the picture. reply HPsquared 4 hours agorootparentYouTube has a lot of informal conversational content in all sorts of languages, too. Podcasts etc. reply wvh 2 hours agoparentprevI think French is notoriously difficult to understand from speaking language because they tie together so many words and sounds. Contrasted with a language such as Finnish, which is hard to learn, but relatively easy to understand and write because both pronunciation and spelling are just what you'd expect. reply bunderbunder 34 minutes agorootparentThis may be a hot take, but I'd argue that there is no such thing as a language that's easier or harder to understand. Just languages that are more or less different from the one you grew up speaking. If they're more similar, then the learner can repurpose skills they already had from their native language. But that's not ease, per se, it's getting a head start. To take French phonology as an example: objectively speaking, enchainement is an aid to comprehension, not an impediment. Now that I'm used to it, I find non-native speakers who don't do it to be harder to understand because they've effectively dropped an entire information channel from the language. Which isn't to say I didn't have a hard time getting used to it when I was learning. I absolutely did. But that's not because enchainement is inherently difficult; it's because I first had to un-learn some assumptions about the structure of language and how spoken word morphology works that I was bringing with me from my native language. And because I was being hindered by pedagogical methods that, in effect, try to hide the problem instead of solving it. reply bunderbunder 3 hours agoparentprevA big problem here is that slowed-down \"teacher talk\" does French learners an incredible disservice. The classroom version of the language is at least a little bit different from natural, connected speech in every language I've studied. But classroom French is effectively a completely different dialect from the everyday spoken language. reply konschubert 11 hours agoparentprevMy strategy is to keep listening to the same recording on repeat. 1000 times if needed. It is good if it is a high-quality recording. After a while, my brain starts picking out the word.s reply vitro 11 hours agoparentprevShameless plug: try Latudio - https://www.latudio.com/ - we have a listening-only approach, but you can pause sentence anytime and tap on words to see the translation. I'd say give it a try and let me know if that works for you, I'd be happy to hear. reply russum 9 hours agorootparentfyi one of the App Store screenshots misleadingly shows \"German\" language in \"Preview\" when in reality it doesn't seem to be available at all reply vitro 7 hours agorootparentOh, thanks for letting me now, we'll fix it with our next update. German is in preparation but not ready for preview yet, unfortunately. reply katzenversteher 13 hours agoparentprevI can speak German, English and Dutch (which are very close Germanic languages) and also understand a bit of French and Italian. I guess I could learn their basics relatively quickly if I tried hard enough. However, I actually need to learn Croatian, a Slavic language. This is extremely difficult for me and I make hardly any progress. The vocabulary is so different from everything else I know, I just can't remember the words. The grammar is quite challenging as well. reply alexey-salmin 13 hours agorootparentI did give some thought whether your native language makes it easier or harder to learn others, but all I have is anecdotal cases from my life. I wonder if there are any scientific studies on the topic. E.g. a curious thing I noticed is the frequent complaint about Latin being very hard to learn because of the seven grammatical cases, three genders and morphology -- words just don't stay the same. As native Russian speaker I find these things absolutely normal and easy to understand, but I can image this must be a nightmare after English. Slavic languages you mention are very curious in how different yet how similar they all are. One of the most impressive thing I saw with respect to learning languages is the Interslavic language [1]. Apparently if you speak any of Slavic languages you can understand it quite well, even though you usually are completely lost with most of the other Slavic languages. You still need to learn to speak it, but the fact that you understand foreigners with zero training just blows my mind. Feels like you awakened the memory of your ancestors of something. [1] https://www.youtube.com/watch?v=NztgXMLwv4A&t=122s reply antihipocrat 7 hours agorootparentThe US state department has some estimates of time to become proficient in target languages for English speakers. Germanic and Latin based languages take the least amount of effort: https://www.state.gov/foreign-language-training/ reply wvh 2 hours agorootparentInteresting link. Some of those estimates are very, very optimistic though... reply ajuc 12 hours agorootparentprevI'm Polish, I've never learnt any foreign Slavic languages, but I can mostly understand Ukrainian, Belarusian and Slovak from passive exposure on a few trips. The experience is weird - when I first went to Ukraine I couldn't understand almost anything. After a few days it suddenly \"clicked\" and I realized how the most common sounds and word endings relate between Polish and Ukrainian - and since that moment I basically got all the words with the same roots for free (which is like half the language). It also gave me Belarusian as a side effect :) Of course it's not actual language speaking, I'm just understanding every other word and connecting the dots. With Slovak it was even quicker, but somehow it hasn't given me Czech - their pronuciation is just too weird, despite the fact it's almost the same in writing :). I'm not sure Interslavic provides much value. For me it's basically Slovak, so if you come from East Slavic language you'd probably get the same benefit spending that time learning some Slovak, and you'd then know some actual language instead of an artificial one. reply elefanten 11 hours agorootparentThere’s a surprising amount of vocabulary difference across Czech and Slovak, in addition to the pronunciation differences you mention. Pre-split everyone on both sides grew up hearing bilingual broadcasts, so they picked up the differences ambiently. I’ve heard that it’s a lot less of a given whether the younger generations presumptively understand each other these days. But I’ve also seen evidence that many still choose to engage with people / content / opportunities on the other side enough to get to solid working familiarity anyway. This comes from limited first-hand experience and more extensive second-hand cross-generational experience. Take it as you will. reply l3x4ur1n 10 hours agorootparentSlovak children usually grow up with Czech narrated cartoons, so they are able to understand Czech more easily. I heard that Czech children does not receive this language training for Slovak, so they have a harder time understanding Slovak language. I never \"learned\" Czech in school but I watched a lot of cartoons as a child (born '93) and read books in Czech so I have no problem understanding Czech language as a Slovak. I have a hard time understanding Polish though, never clicked for me. reply jajko 3 hours agorootparentI can confirm this is true. Czech republic is cca 2x the population size of Slovakia and its historically more developed part, so during one state union a lot of media were in czech language and it became our second language without thinking about it. Also Czechs did get a decent exposure to slovakian language. But if there is no exposure, its becomes visible how grammar is very similar, but most words are just a bit different (very few are completely different), and pronunciation varies so much across whole region (even within given country) that its not easy or even possible to understand each other out of blue, without prior exposure. I got some exposure to Polish TV during 80s, since commies couldn't put together more than 2-3 channels on TV and those were anyway pretty bland. I can cca understand it, but can't say a single sentence well enough. If I read polish text, I have to read it loud in my head and then I grok it easily, otherwise too much 'cz', 'w', words are too long etc and I lose meaning very quickly. But in general Polish is a bit further away from either Slovak or Czech languages. We were and still are literal brothers (CZ and SK), extremely similar in so many regards, still see no good rational reason why we split up (of course I know real reasons, but those are nasty as are the people responsible for the split). reply wvh 2 hours agorootparentprevI had the same problem coming from the same Germanic/Romanic languages as you, trying to learn Finnish. It's not just the vocabulary, the whole construction and modality is often different, and it's hard to map things one on one. Then I learned Swedish, and it was just a funny dialect between Dutch and German. reply holoduke 11 hours agorootparentprevAs a dutch person i once spend 2 years in Ukraine. Hardly anyone spoke decent english there, so i was forced to learn (russian) the language. Within a year I was able to speak it moderately. Key is to focus on words only. Dont care about gramar. Its not important and will be fine later on. First learn 2000 to 3000 words and you will be able to say a lot of things. reply yurishimo 11 hours agorootparentThis has been an approach I've seen used to great effect by people learning your language! My wife strings together Dutch words with English grammar (we're both American) and it's a common \"mistake\" that I experience when visiting a taalcafe. Our brains are all extremely capable of moving a few words so we can understand things in context. When speaking Dutch, I'm spending a lot of time thinking about a verb, then attempting to string it together into something comprehensible. What I also found to be almost universally true, is that if you are learning a less \"popular\" language (like Dutch or Ukrainian), just actually give a shit enough to learn more than \"yes/no/please/thank you/my name is\" will garner you a lot of good will from native speakers. Some countries get so many tourists that hearing the same 10 words butchered over and over and over and over and over again eventually starts to wear down on locals, making them jaded. When you can walk into a room and somewhat confidently hold your ground, you become interesting and a novelty that people want to interact with. I went to a block party last weekend and spoke to most of the people there throughout the evening. Sure, everyone knew I was \"foreign\" and occasionally I had to ask someone to repeat something or re-word it, but that's a minor complaint for them considering the other option is to speak back to me in a language they don't have as good of control over. Despite popular belief, even in many \"English friendly\" countries, the normal citizens aren't actually that comfortable speaking unless they work in an environment that demands they speak English every day. This goes doubly so for older people. My next door neighbor is about 75 years old and speaks English pretty well (worked at Philips for 20+ years), but the _quality_ of our conversations went up 3x when I could understand enough Dutch for them to speak naturally to me. All that to say, I agree with you! Vocabulary and basic grammar to get you started and then after that it's all about learning words and practice practice practice. reply wvh 2 hours agorootparent(As someone who has lived in different countries) Making an effort to learn \"enough\" of a language of the country you expect to be living in for a while is a gesture of decency. I feel that good will is justified, especially in cultures that see a lot of immigration and tourism. People try to live in a coherent society, which becomes hard if a large group of people is incapable of communicating naturally and reasonably fluently. Most Western European and Northern European people speak excellent English, but it's not nice to force a large group of people to switch to English and hamper natural communication style because somebody is slightly disrespectful and lazy. You can get away with a lot in Dutch and especially Flemish Dutch though, because the local dialects are so strong (for being a relatively small area). Unless you look exotic, people don't always immediately pick you out as a non-native speaker. reply alexey-salmin 11 hours agorootparentprev> Key is to focus on words only. Dont care about gramar. Absolutely! The way they teach foreign languages in school is insane. \"OK, you know 20 words and can't say a thing, now it's time to learn past tense\". reply gwervc 10 hours agorootparentAnd before learning words, learning and practicing phonology. Then, add vocabulary while still practicing phonology. It's insane people and institutions assume one could be understood when speaking a language when its most basic building block is not acquired. reply yurishimo 10 hours agorootparentprevCan I ask what language you're thinking of here? When I was taking classes, the A0-A1 level class was all in the present tense for the full length of the course. We might have touched on past tense in the last lesson and most of the books I've seen for this language (Dutch), structure things as present/future tense first, and then past tense after. reply nkrisc 9 hours agorootparentprevIf you can say “subject verb noun” that’s good enough in many cases to at least get your point across. Anecdotally, I know that I can understand people speaking English with what a school teacher would consider atrocious grammar, as long as the words are pronounced close enough to be recognizable. reply 867-5309 9 hours agorootparentprevthe language of Ukraine is Ukrainian reply holoduke 7 hours agorootparentAs a foreigner a lot more useful to learn russian. Everybody speaks it in Ukraine. And other countries as well. Even at our university in Ukraine, russian was the main language amongst foreign students. Its sensible right now true. reply wvh 2 hours agorootparentNot a dog in the fight, at least this particular fight, but this might sound to many as \"just learn the language of the oppressor\". This gets thorny real quickly. reply racional 2 hours agorootparentprevAs a foreigner a lot more useful to learn Russian. Given that the state language is Ukrainian, and its overall dominance in media and culture -- there's no way this statement could possibly make sense. Everybody speaks it in Ukraine. This gets repeated a lot, but it's just not true. It's true that virtually everyone has some working comprehension of Russian because of earlier Soviet influences, and because the two languages are so similar (and in many parts of the country, the \"Ukrainian\" that is spoken is highly Surzhyk-influenced). But realistically only about 70-80 percent of the population speak Russian fluently and comfortably. Given a choice, the vast majority would clearly prefer to speak Ukrainian (and many people have been switching voluntarily as a matter of preference since 2014; the government's mildly coercive efforts having nothing to do with this, really). Even at our university in Ukraine, Russian was the main language amongst foreign students. Probably because it's the only one among the two that they were able to study before coming there (and because they saw Russian as being more useful in other countries, as you say). And even so, this applies only to certain universities in certain cities. reply holoduke 1 hour agorootparentI spend many years till couple of months before the invasion in Ukraine. I have never met someone who doesnt speak russian. Only the elderly people have sometimes a mixed slang between russian and ukrainian. But other than that everyone speaks it. There are some hardcore nationalists connected to Bandera (pro nazi group) that refuse to speak russian, but remaining people dont care and speak both. reply racional 1 hour agorootparentI have never met someone who doesn't speak Russian. Then you haven't traveled broadly in Ukraine. And more importantly you're missing the point. The vast majority do speak and understand a reasonable amount of Russian (hence they will almost never object when you use it with them; they get that you're a foreigner and are doing the best you can) -- but they don't speak it fluently and comfortably, and it's not their preferred language in everyday use. Only the elderly people have sometimes a mixed slang It's more prevalent among the older set of course, but still this is just not true across the board. Surzhyk (or less pejoratively: Russian borrowings/breakings) are everywhere, though they are often subtle and it may take some training to detect them. Part of the problem is that there are no well-defined boundaries (and there's only a barely defined notion of what constitutes \"standard Ukrainian\"). They're literally still in the process of cleaning up the nation's preeminent (and clearly Soviet-, if not exactly Surzhyk-influenced) dictionary. There are some hardcore nationalists connected to Bandera (pro nazi group) that refuse to speak Russian Now you're getting into pure BS territory. This is obviously something you've read or something you've heard said a lot, but not something you know from direct observation. reply j_bum 14 hours agoparentprevThis has always been my struggle with French as well. I have never quite able to crack the aural/listening component of the language. How did you overcome the “wall”? reply materielle 12 hours agorootparentUnfortunately there’s no trick. You get better at listening by listening a lot. Most people just severely underestimate the amount of work it is. You probably need 1,000 hours of listening to be decent, and 2,000 to be strong. If you practice 30 minutes a day, that will take 11 years. So the only “trick” is that you need to find things that you genuinely enjoy doing in French. So that you can practice for multiple hours each day without burning out. I’m a native English speaker who has a very high level in Spanish. It takes a long time for your brain to figure out how to decode the type of mumbling people use in casual speech. Yes, practice vocabulary, practice grammar, read. But you’re probably already doing this. You just need to listen to a ton of stuff right at the edge of your abilities and you’ll notice improvements every few months. My one final tip would be to not get in your head so much. Literally everyone who has learned a language will tell you that if you review vocab, do grammar exercises, and listen to the language for 1-3 hours every day, you’ll learn it. But it happens so slowly that it doesn’t feel like it will work while you’re on the journey. It’s sort of like the gym: if you lift every day and eat healthy, you will have muscles in a year or two. But it’s really demotivating because you won’t notice a drastic difference even after 3 months. reply magneticnorth 14 hours agorootparentprevI'm not the person you asked, but I'm in the process of learning French. I'm a native English speaker who had a much easier time with listening comprehension for Spanish and German, my other languages. What I find helps me to make progress is two things: 1) prioritize vocabulary (you need to know a word to have any hope of recognizing it in speech) 2) listen to \"comprehensible input\" at your level. I like this guy: https://www.youtube.com/@FrenchComprehensibleInput who has levels labeled for his vids, I also like https://www.youtube.com/@wanderingfrench because I'm interested in Canadian French and I find her especially clear and easy to understand, as well as charismatic. reply haspok 9 hours agorootparentI respectfully disagree wrt the prioritization of vocabulary. Yes, you do need the basics covered (a few thousand words). But what I realized is that if I am listening or reading a sentence with an unknown word in it, in most cases I just figure out from the context what that unknown word means (I check it against a dictionary and 9 times out of 10 I'm close enough). Obviously this does not work if too many words in the sentence are unknown. And I'm not saying not to learn new words. But it is far more important in my opinion to read / listen so much that you get faster and faster. Especially if you are listening to speech, where you can't pause / rewind, and if you spend too much time on one thing you just get left behind entirely. Don't know a word - just skip it / ignore it and concentrate on the whole stream. reply konschubert 11 hours agorootparentprevHere is my trick: I take a recording of a conversation in the language, one that's made for language learners. Then I listen to it until I know it BY HEART. Usually that takes about 100 listens. Every time I listen, I understand a bit more. Sometimes I look at the transcript to understand a section that evades me. I don't move on until I can recite the whole thing BY HEART. Then, I take the next dialogue. reply yurishimo 10 hours agorootparentThis is next level! Thinking back to my language textbooks, this would be brutal for me on a mental level. Do you think it could work as well with a short story or novella? Perhaps one of the books by Olly Richards? I find the dialogues in textbooks absolutely mind numbing most of the time and while easy books for learners aren't that interesting anyway, at least it's a full story and not people ordering lunch or a trip to the doctor for the 99th time! reply konschubert 10 hours agorootparentThe thing is, the dialogue has to be short to really stick to your brain! Like, less than a minute. Repetition is key, and if the text is too long, it’s not repeated enough. To me, it’s not mind-numbing until I can “sing along” because I know it by heart. And then I move on to the next, slightly harder, text. You don’t have to always listen to it actively, you can put it in while doing other work. Some language self-learner books are better than others. reply maxquinn 9 hours agorootparentI'd really like to give this method a go. I'm trying to learn French and struggling with the listening component. Do you have any sources for dialogues that you used? reply konschubert 8 hours agorootparentI used the Langenscheidt and PONS courses for Polish in the past. But I think they are only available in German. reply yurishimo 10 hours agorootparentprevAh okay. I think most of the dialogues from the books I'm thinking of were between 3-5 minutes. Something shorter might work better, indeed. reply jstanley 12 hours agorootparentprevI find it's sometimes quite difficult to tell where the word boundaries are when a person with a strong French accent is speaking English. I think the difficulty is because French encourages the emphasis to be in completely different places to English. reply alexey-salmin 14 hours agorootparentprevIt clicked after a year of practice, the change was quick and noticeable. I started learning after moving to France and it helps a great deal even though at work it's English. Not sure how long it would take if I was learning it elsewhere. reply yurishimo 10 hours agorootparentIt definitely takes longer if you don't live in an environment where it's spoken. Before I moved to NL from the US, I did duolingo every day for 3 months (from the time I accepted the offer to moving) and by the time I got on the plane, I had moved from the beginner module to the intermediate one. After landing and getting settled, I ventured out to really test my skills at the local supermarket and I was completely and utterly useless. Perhaps the only thing I could recognize was the total at the end that I needed to pay. We take for granted regional dialects when speaking our own language and in some languages, dialects can be a huge component of the language. Being able to just go outside and sit on a bench and listen to people speak in the language you want to learn is a huge advantage. Every day, you will passively pick up some words from the context of passers-by. You'll also start to subconsciously figure out what words are \"common\" in a different way that the lists of \"1000 most common words in X langauge\" can convey. You also learn how to put them together in context and how some words travel together within a certain context. If I was tasked with learning a language abroad, I would spend 2-4 hours a day consuming native content and make it a priority to speak with a coach online 1-2 times a week, trying to work my way up to holding a conversation for an hour straight. After living here for almost 2 years now, I can just barely get to 1 hour when speaking to my boss (super grateful btw!) but I'm really drained mentally by the end of it. With each meeting it gets easier though and now I can somewhat even make out the local dialect. Learning a language is a numbers game and the reason we often attribute superior language learning skills to kids is simply due to the fact that they have so much free time to listen, absorb, speak, and make mistakes without people judging them because they're kids. I'm fully convinced that adults can learn just as fast or faster, but our own egos often get in the way to putting in the work to learn. reply FranzFerdiNaN 12 hours agorootparentprevListen a ton and then some more. For the first couple hundred hours or so preferably with subtitles, there will come a point where you realize you rely on the subtitles less and less. That’s the point where you can start to turn them of. Audiobooks also work. And work on your grammar and vocabulary. Listening gets easier if you intuitively know what you should hear (tenses, conjugations, plural or singular words etc). reply anal_reactor 12 hours agorootparentprevAside from my native language I speak three foreign languages, and I just started learning the fourth one. Besides that, I wasted a lot of time on another language that did not stick, so I kinda know what works and what doesn't. 1. Consistency. Make sure you practice a little every day. For example, I use an app that tells me every day what vocabulary I should practice that day. It takes ten minutes of my day, and does wonders. 2. Communication. Start talking to someone as soon as possible. If you're learning a Germanic or Romance language, after 6 months you should be able to find someone online to chat with. Of course it's going to take you 15 minutes with a dictionary to understand the other person's message and another 15 minutes to write the reply, but it truly does wonders because it allows your brain to see the language as a language. 3. Fun exercise: try vocalizing your internal monologue in foreign language. Don't focus on correctness at all, it's about familiarizing yourself with the language. 4. Find some interesting media in your target language. It's going to be difficult because most important stuff is available in English, but for example you might try movies or YouTube channels. Especially the latter is great because YouTube videos rarely ever have English translation, so you know that either you watch it in your target language or not at all. There are apps that allow you to listen to foreign radio stations. Why not doomscroll in your target language. 5. Don't give up until you reach level B2 and you can talk without consciously thinking about it. 6. Understanding native speakers talking naturally to each other is literally the hardest part, so don't treat it as a benchmark until you reach fluency. 7. A common mistake is to treat the whole thing as a \"sequence to sequence\" task and think about the target language in the context of your native one. Your actual goal of learning the word X is to have your brain understand the abstract concept behind it. 8. It's going to suck and you'll hate it. There's no way around it. Keep practicing and adjusting your techniques to your liking. One day something will \"click\" and you'll actually \"feel\" the language. reply Dalewyn 13 hours agorootparentprevPersonally as an English/Japanese bilingual speaker and a programmer (programming languages!), I think the most important thing is incentive. Why do you want or need to learn that language? Is it important? Is it valuable? Is it fulfilling? As I've found, the biggest \"wall\" to learning languages is convincing yourself that learning the language is worth the immense hassle and effort. reply yurishimo 10 hours agorootparentYes! This for sure plays a huge role as well. How many times have we as programmers picked up a new language, done the 'Hello World', and then set it down and went back to one we already know? Spoken languages are much the same. They all include various concepts that are shared (verbs, nouns, adjectives, grammar concepts, loops, variables, functions, etc) but the actual process of putting them together in the idiomatic order is the hard part. But without the carrot at the end of the stick, it's unlikely you'll be disciplined enough to stick to it. A new job is a natural place for many people to pick up a new programming language because everything else is also new. For languages it is no different. Can you learn a language outside of the country where it's commonly spoken? Sure, but it's going to be 100x more difficult and you have to generate all of the discipline yourself. If you can crack that problem and build the discipline, then you've basically won the war if you can hold in there \"long enough for your enemy to starve.\" :D reply jasonkester 11 hours agoparentprevYeah, no kidding that’s rough. “est-ce qu'il y a”…. Three syllables, representing six words. We taught both kids French by dropping them unprepared into the local village school at age 6. Whatever part of their brain taught them English as infants kicked back in and did the same thing for a second time. Amazing to watch. I wish I could do that. reply Sharlin 9 hours agorootparentYep, https://en.wikipedia.org/wiki/Language_immersion. There are schools and kindergartens that specialize on this, though the degree of immersion varies. reply madaxe_again 8 hours agoparentprevThe good news is that now that you’ve cracked French, you’ll find Spanish, Italian, and Portuguese easy. They’re all basically the same language with divergent dialects, but practically identical grammars and structure. reply namaria 8 hours agorootparentSpanish, Catalan, Galician, Portuguese, Italian, are all in a cluster together and a bit apart from French. I can read French, but I think it's equidistant from the Iberian/Italic language cluster and English. So the first language in that cluster after French won't exactly be \"easy\". Easier than without French yeah but the subsequent ones will be much easier and on the same level of difficulty from one to another as then you mostly just change the ending of the words and the most used connectives. reply madaxe_again 2 hours agorootparentI’m a native English speaker, and French (and some very rusty Latin) was my bridge into that cluster - they’re absolutely heaving with common cognates and structures from my experience. What I will say is that my French has become much harder to access since - I will find myself lapsing into a creole for the first few days when I switch from one to the other. reply boffinAudio 10 hours agoparentprevNative English-speaker here, moved to a German-speaking nation 20 years ago and have become a fluent German speaker as a result. Just want to add my 2c about that wall of sound .. At the beginning I found it very difficult to parse German as I heard it on the street and in general life circumstances. It wasn't until I rigorously started looking up words I 'thought' I heard, with a dictionary, and gained about 100 of these, before I could accurately parse a spoken sentence. I think its very important to use a dictionary when learning another language. It wasn't until I got a massive English/German dictionary that I felt I stood a chance. Another thing that really helped was using sub-titles and watching TV, even if it was a show I wasn't interested in - it taught me so many words that I had heard, but not recognized. reply beaeglebeachedd 14 hours agoparentprevThey seem to teach it pretty fast to any idiot in the French Foreign Legion from what little I know. I have a feeling people learn fast with the 'right' incentives. reply samus 8 hours agorootparentThey also actively teach it to them. They are immersed around the clock, using other languages is forbidden (recalcitrant recruits used to get hazed in the past), and they put in efforts to teach it in a practical and conversational way. reply freddie_mercury 13 hours agorootparentprev90% of recruits fail, so I wouldn't say they've got some awesome fool proof method. reply pbhjpbhj 12 hours agorootparentWhat % fail due to lack of language skills? Or did you really mean 90%. Source? reply HumblyTossed 5 hours agoprevTip for parents - teach your babies Sign Language. They can communicate with you much earlier than they can learn to talk clearly. So much easier when a baby can tell you they're all done or want more than to have to figure that out without communication. reply empath75 5 hours agoparentAs a parent of three kids, total waste of time. For one, ASL isn't the only way to communicate with signs, and it's not the most efficient way to communicate with signs with kids -- they'll develop their own ways of signaling what they want. It's more of a parlor trick than anything else -- \"Look what my kid can do!\". I've got 3 kids and they were all very good at communicating what they want. We taught a few signs to our oldest kid, but it wasn't any easier to figure out what he wanted, and by the time he was using them effectively, he was basically talking already, we didn't bother with the other two. (Going to clarify a bit: If the purpose is to communicate better with your kids before they can talk, it's a waste of time. Learning ASL is a valuable thing to do for many other reasons.) reply aaalll 12 minutes agorootparentYou just need 3 signs, More, All done and something for feed me. reply munchbunny 4 hours agorootparentprevThis goes against fairly general advice that I’ve received from early intervention specialists, including for my own child (not diagnosed as neurodivergent, just speech delayed). In our case the kid picked up signs we were teaching him (not ASL, just intentional signs) weeks before corresponding words, and there are still words he signs and understands but can’t pronounce. The point was to introduce expressive language for functional concepts (hunger, wanting help, wanting to be picked up, etc.), and beyond its role as a precursor to spoken words, it absolutely made our lives easier. It’s a well known phenomenon that the ability to pronounce words lags behind the ability to express “language”. Your kids clearly didn’t need it, but it’s still a useful tool. reply toast0 2 hours agorootparentprevIt doesn't need to be a formal sign language, but intentionally developing systematic hand based communication with infants and other caretakers is pretty useful. Using ASL signs as the base means you may have a better chance of others understanding, which is great (unless you're using signs to communicate base running strategy). Lots of kids can communicate a little earlier with hands than speech, and there's lots of situations where hands are visible but speech is inaudible. More tools for communication is usually better. My kiddo is 13 now, and speech is usually more convenient, but we still use a couple systematic signs (shaken t for toilet, all done), and the experience of looking for signs is helpful in a crowded place even when using ad-hoc signs. That said. Like all parenting advice, try things out, if it sticks, keep using it. If not, no worries, nothing fits all. reply rndmio 4 hours agorootparentprevConsider for a moment that maybe your experience isn't universal. I took both of mine to baby sign classes and both were able to communicate things such as hungry, tired, milk, nappy etc long before they were able to talk. Along with other parents at the same classes it was absolutely not a waste of time in terms of being able to communicate better with the children before they could talk. reply HumblyTossed 3 hours agorootparentprevWe’ve had great experiences with teaching the babies we’ve had sign language. reply throwaway173738 5 hours agoparentprevBabies absolutely will still communicate by utterance, gaze, reach, and body language what they want. My son has a particular cry that means either that he wants food, wants to comfort nurse, or wants water. When we hear that my wife and I can just present those three things one by one and he’ll select what he wants. Don’t feel like you have to follow this advice. reply HumblyTossed 5 hours agorootparent> Babies absolutely will still communicate by utterance, gaze, reach, and body language what they want. Oh, I totally get that. But SL makes it that much easier. > Don’t feel like you have to follow this advice. I wasn't commanding anyone to do something. Good Grief. reply lolinder 5 hours agorootparentprevCertainly no one should feel obligated to follow anyone's advice! There's too much contradictory parenting advice going around to be able to follow it all. That said, there's a marked difference between the kinds of assymetrical communication that you're describing and baby sign language. We've done sign with each of our three children and it's remarkable to watch them pick it up—they clearly understand that these hand gestures are words that carry specific meaning and get a lot of delight out of learning and applying them correctly and being mimicked and understood. The language center of the brain clearly develops much faster than the parts responsible for controlling the tongue, and sign gives them a great outlet for that language. Also, you don't have to do anything complicated to give them just a little bit of sign, so unlike a lot of parenting advice it's not difficult to try out. Just say a word while making a sign (ASL if you want but we'll sometimes just make up one on the spot) and then give them the thing or do the thing. \"More\", \"water\", \"milk\", \"diaper\", \"good night\", \"up\"—just pick one and try it! Once you start and see their delight it's hard to stop. reply munchbunny 4 hours agorootparent> Certainly no one should feel obligated to follow anyone's advice! There's too much contradictory parenting advice going around to be able to follow it all. As one of many, many parents, my observation has been that there is so much contradictory parenting advice because there is so much variation in the kids, in terms of behaviors, personalities, the order in which they bootstrap their motor vs verbal skills, body awareness, etc., etc. As a result, consuming parenting advice is like going down a checklist of things to try, and the kid won’t respond to most of it. reply d1sxeyes 5 hours agorootparentprevOf course you don’t have to, but I don’t think this was intended as an instruction, rather as helpful advice. I would second it, my son still uses a few signs (especially “sorry”) even now he’s two. reply IG_Semmelweiss 14 hours agoprevThe article starts getting good in the last 2 paragraphs, explaining the actual science of observed transitional probabilities, then it suddenly stops. Its almost as if the writer ran out of coffee, or his scientific mind went on strike. What was that ? reply tgv 11 hours agoparentSo this bit is just plain wrong > For this the infants were presented with words and non-words and found to have longer listening time for the nonwords. This indicated they had already become familiar with the words by listening to the continuous sequence of syllables within which they had been embedded. The only way that could have happened was by monitoring the TPs between syllables—the infants were capable of statistical learning. First, \"longer listening time\" doesn't mean that they identified syllables. Second, it's a group effect, not individual, and the differences are small. It's NHST, and thus their p... Occasionally she uses a word from one language while speaking another, which can be funny because she does adjust the conjugation Yup all parents of multi-lingual kids experience that. It's funny and yet, to me, it's precisely what statistical learning TFA mentions is about. Our kid used to do the same and still does (but very rarely now that she's 9 y/o): they borrow a word from another language and adjust it in a way that, statistically, makes sense. At times they can, by \"chance\" (probabilities really I guess), create a word that is actually the correct word even if they didn't know it. She could take \"labyrinthe\" (in french) and then transform it to \"labyrinth\" (in english) and pronounce it properly. Now of course people would use \"maze\" instead but, still, \"labyrinth\" is \"accidentally\" proper. BTW there are studies about just that: how language is acquired by analyzing the made up words that multi-lingual kids do create. Kid is totally fluent english/french and can definitely play with spanish kids, speaking spanish. P.S: we spent quite some time in Spain (in Zagreb too FWIW but we don't speak croation): wife speaks spanish fluently, I understand it and so does the kid... We contemplated moving to Costa Rica. We may still do it... How's the pura vida there? reply samvher 2 hours agorootparentIt has pros and cons, like anywhere :) If there's specific things you want to know, feel free to email me at the address in my profile. reply ghayes 9 hours agoparentprevMy wife and her parents live with us and speak primarily Spanish. My wife and I speak primarily English to our son. He is bilingual with some funny quirks: * He speaks based on his listener. So Spanish to abuelita, English to papa, and dealer's choice to my wife. * In the early days, he would mix-translate, e.g. saying \"another more\" since presumably that was how he translated \"una mas\" in his head. * He picked up my non-regional accent that I thought I never used anymore. * His understanding varies based on his vocabulary in a given topic. He understands science in English, but colors generally tend toward Spanish. * I consciously spoke a lot of Spanish to him when he was young. He didn't fall for it and only speaks English back to me. So, I wouldn't necessarily overthink it. The goal is to get her exposure to hearing the language and building the vocabulary, and secondly, to compell her to use it to express what she needs. reply bende511 3 hours agoparentprevI can speak to my experience as the child raised multilingually, though not quite the same situation you are proposing. I was raised in the US to a an American dad and a French mom, and they both speak both languages. My parents decided they would only speak to me and my younger brother in French, assuming (luckily correctly!) that I would learn English by simply being in an English-speaking world. My experience, and I think my parents and their multi-lingual friends would agree, is consistency. My parents even went so far as to pretend not to understand us if we spoke to them in English! Not even to ask for a word translation! If you speak English, you always speak English. If your spouse speaks Spanish, they always speak Spanish. My feeling as a recipient of this education is that its probably better if you both speak the same language to your children, but I know that splitting them up can also work. Again, the key is consistency, and the simpler you can make the rules about who speaks what language the better. reply BizarroLand 2 hours agorootparentThat actually sounds like a fun game for multilingual people. Partner A picks language A, Partner B picks language B. Now, have a conversation with each other where you only speak in your chosen language. The listener must listen and translate in their head and then respond in their chosen language. First person to slip up and change languages loses. reply fergonco 8 hours agoparentprevYou don't want to try things with your kids. One language per parent works. The other approaches? It's already quite challenging not to mix language in simple bilingual families. Your second rule sounds super weird. Are you going to change language as you walk through your door? Additionally, at some point the kid will choose a language. To me does not sound solid. Maybe you can make a teddy bear use a third language. Not that weird (in the first 5 years, I guess, lol). And that would introduce the kid to it. EDIT: also if you talk only at home you'll get limited to talk about sofas, order your room and stuff like that. And if you get more than one kid, all this house of cards will fall. The kids will speak between them whatever they want and you can do very little to change that. reply keiferski 7 hours agorootparentThe thing is that Parent 2 already uses both languages on a daily basis, for work and for life. So in a worst case scenario the two just get mixed for the kid. Avoiding one or the other isn’t really doable. reply fergonco 5 hours agorootparentI think it's enough to use one to address to the kid. If the other is present in the environment the kid will learn it to some extent. But I think there is value in simplifying the first steps with language. It's a hell of a struggle. And scary if it does not go well/quick. I am not an expert, just what I've observed. reply throwaway7ahgb 6 hours agorootparentprevYou won't have as much control here as you think. Try your best to be consistent as early as possible and hope for the best. We are not as complex but we're trying this: Mom (Russian/English): 100% Russian direct to baby, English to others. Dad (English only): 100% English all the time. Environment: English reply alex_duf 9 hours agoparentprevMy SO and I were raised respectively in the UK and France. We both speak fluent French and English. Our plan has been: we each speak our mother-tongue (French for me, English for her), and see what happens. My first child understands english perfectly, but replies in French, and my second is too young yet to say any word, but she understands both languages. Now we've moved to France and found ourselves speaking more and more in French to each other, and -unfortunately- our household is now speaking mostly French rather than keeping English in the mix. But this makes me think maybe we should all speak English at home now that French is acquired by my eldest... not sure! reply samus 8 hours agorootparentI think a lot also depends on whether the kids are in school or daycare already. Those will take care of advancing one of the languages. I'd say in your situation it doesn't really matter as long as the skills in English are maintained - should be easy enough nowadays. reply phatbyte 7 hours agoparentprevI can chime in. My wife is from Germany, and I'm from Portugal, we both speak English with each other which is easier (DE is very hard for me to learn), but she talks DE with the little one and I talk PT with him. He can speak perfectly DE, PT, and English depending on which language he listens to, which is quite impressive for me as a parent. He can even maintain a proper conversation and do translations between languages. Sometimes he adds a word from another language in a phrase but he seems to know what that word is in the correct language. reply yurishimo 10 hours agoparentprevI think this should work well though I might try and flip the Home language every few years since you want them to be tri-lingual. I don't have any kids yet, but my plan is to speak the less common language where we live at home and then the other language out in the world. That seems to be the approach many friends have taken as well to good effect. Assuming these languages are actually English, Spanish and German, I would find a local club in the minority language and make some local friends where you can speak that language and your kid can come along for the ride. reply madaxe_again 8 hours agoparentprevSo, this describes both my and my wife’s situations as kids, and now my kid’s situation. Let me tell you, three languages is a stretch. In both my wife’s and my cases, the languages which got used stuck - and I don’t mean our parents didn’t speak them all to us - just that in our respective cases the utility value of the third language, which was in both cases an ancestral tongue not related to where we lived or spoken by both parents, was not adequate to make it stick. We each acquired the family language and the local language. Our kid, we speak English with at home, Portuguese in the public sphere. She’s acquiring both and not muddling them to any great degree. There’s a notion we share that other languages we each speak fluently would be good for her to learn, but based on our own experiences we intend to take a different approach. Once the two languages which will actually present utility to her day to day are in place pretty firmly, it’s time to ship her off to her grandparents for the summer, where she will be immersed in a tongue that will in that circumstance provide her utility. It was how I ultimately acquired my third language - two months with cousins and their family, aged 7. So in short, your mileage may vary, but each of us found learning a third language that only one parent and nobody else spoke a chore, and it did not stick via primary language acquisition mechanisms, but had to come later. reply fhe 4 hours agoprev> When we write, we leave spaces between words. Readingwordswithoutsuchspacesisdifficult. immediately losing credibility because quite a number of languages, such as Chinese and Japanese (and I think Korean too), are written without space between words or characters. In fact, until quite recently (last 100 years or so), written Chinese had no punctuations. reply posterboy 4 hours agoparentThe characters aren't connected so the brush is lifted and a space left behind. Of course this also goes for the space between signs of a compound word, but this also holds for \"compound word\", which is itself a compound word. Also they had = EoL alright. Negative spaces are so important in *-graphy reply csa 3 hours agoparentprevAgree with your main sentiment. For accuracy: > quite a number of languages, such as Chinese and Japanese (and I think Korean too), are written without space between words or characters. Correct with Japanese and Chinese. Modern Korean (few or no hanzi/chinese characters) does use spaces. > In fact, until quite recently (last 100 years or so), written Chinese had no punctuations. Eh… ok, sure, I guess. Punctuation was formalized in the 20th century, yes. There are also lexical markers than can be (and often were) used to mark the beginning or end of certain ideas. While not punctuation, these characters serve a similar purpose in delineating linguistic chunks. The following page gives some simple examples: https://www.theworldofchinese.com/2021/09/how-china-adopted-... reply isolli 11 hours agoprevI learnt German at the age of 25. I distinctly remember the first time I was exposed to it (before learning it), it sounded like water gently flowing down a creek. Then I learnt the basics, and my brain started to catch on to patterns. However, I had to go through the written form to learn properly. I found it hard to parse and remember words when I was only hearing them. Unlike young children, obviously. reply yurishimo 10 hours agoparentI think this is a common misconception. Children also find it difficult, but they have a decade to figure it out while adults usually only give themselves a few years. Even at the end of that decade, many children still have problems reading and writing well into adulthood. As adults, we can learn so much faster than kids precisely because we already know how to read and write conceptually. If you can read effortlessly in your native language, you will not be satisfied with your progress in another language until you can read at the same level. What often holds adults back is embarrassment and how that affects our ego. If we could let go and just engage without overthinking it, adults absolutely could pick up languages much faster than kids. reply samus 8 hours agorootparentChildren also have no choice than to be immersed around the clock. And they have very patient teachers who are elated about every new word they master and who will stubbornly correct them if they get something wrong. Most adult learners can't do full immersion or underestimate its importance and don't commit even if they get the opportunity. reply samvher 6 hours agorootparentprevMy impression is mostly the same, but to add to the point about embarassment, native speakers will often not correct adults making mistakes in their language (and avoid showing that a mistake was made) while the same person would correct a mistake for a child. Children also get exposed to much simpler language and get to learn a language more bottom up. Also somehow my impression is that pronunciation/accent really is harder to adopt as an adult than as a child. reply a_c_s 4 hours agorootparentIf we truly listen to the pronunciation of small children they often substitute easier sounds for difficult ones for many years. We just expect that from children so it seems normal. reply yurishimo 3 hours agorootparentprevYes, this is also true. I have some strategies when I speak with others in trying to repeat their answer back to them, but with the correct grammar/vocabulary and then answer their question and I'm pretty sure I picked this up from volunteers at a language meetup who did it to me. It not only provides feedback to the learner, it also helps to confirm that you're both on the same page with the information that was spoken. Generally, this works well for kids and adults without being demeaning but it's also important that everyone understands their role in the situation beforehand if possible. Regarding accent, I think it really is a matter of practice and will. I agree that it's harder as an adult, but we shouldn't allow people to use that as an excuse if they want their accent to improve. By no means is that a requirement, but it is something you should ask your adult friends if they are learning your native language. Though, I would argue this is probably not even worth it unless they are at a B1-B2 level. If they come from a similar language family (germanic, latin, etc) as you, then perhaps working on their accent at the beginning will prove fruitful, but otherwise, it's probably a lot of new sounds that they need to learn how to even say \"correctly\" before they can start adjusting how their mouth moves. For me, I've found that mouth training a little easier when I'm not spending 10 seconds formulating a reply in my head. reply nop_slide 4 hours agoprevRelated, but I'm currently reading \"Thirty Million Words: Building a Child's Brain\", it's fascinating! reply nickpsecurity 17 hours agoprevIt's good that they mentioned babies hearing in the womb. I've known many mothers that read to or play music for their babies. They say they feel them respond to some things, too, where they seem to sense their surroundings. I'm just reporting what they told me since I haven't studied the literature on this stuff. One thing I didn't like was the paragraph on how they differentiate words with no formal training. I feel that gives a false impression. Parents usually teach their children a lot about language. They give them visual cues, speak at varying rates, change their own tone in some situations, and so on. The babies are soaking up the world on their own using one set of mechanisms. They also often receive highly-supervised training from a trusted source. Later, they get formal training on top of that. Even when not training, much of the content they see and hear is presented in a structured way that helps connect ideas. For instance, listening to the radio or TV with their parents would let them hear a lot of structured speech. Babies are highly trained. They might also do the statistical learning. They're a mix of the two. reply lolinder 15 hours agoparent> They also often receive highly-supervised training from a trusted source. Later, they get formal training on top of that. Languages as they are spoken have many quirks and patterns that are different than how most parents believe the language works, and those quirks are readily adopted by infants even though few parents if any would consciously teach them. A great phonetic example is \"choo choo chrain\": we often pronounce words very differently from how we think we're pronouncing them. There are also plenty of grammatical facets that are similar—cases where there's a stark difference the real rule and the rule as formally taught or where a rule is picked up that the parent doesn't even know exists (such as the order of adjectives). As for the formal training, schools don't generally teach any natural dialect of spoken English (at least in the US), they teach a formal written dialect of English that differs in many respects from the language that we pick up as infants. This dialect is the source of many of the misunderstandings mentioned above, and yet children will learn natural language as spoken regardless of the attempts of well-meaning teachers to correct them. To me this all suggests that supervised learning for the natural spoken language is unnecessary. Children assimilate their native language from the way it is spoken around them. The main purpose of the formal instruction is to teach children to speak a special dialect in formal settings that will mark them as educated precisely because it is distinct from the one that they'd have picked up naturally. reply skeledrew 14 hours agorootparentI'd say it's more a matter of setting standards so that drift is reduced and ESL learners don't go bonkers with a dialect that doesn't generalize well, than as a mark of education. For example the differences are already bad enough between American and British English. reply TeMPOraL 10 hours agorootparentESL here, difference between American and British is nonexistent to me. Sure, some of the word choices differ slightly, like torch vs. flashlight, or rucksack vs. backpack, and even then most of the time, both languages have these words in dictionary, it's just the default that's different. And ESL people are usually in the exact situation you describe, because they're being taught British English during lessons, while learning American English from everywhere else - TV, videogames, Internet. We manage just fine. Still, given that everyone gets taught grammar for their native language at some point, too, I agree this is in part to reduce language drift. Makes sense - a modern nation with millions or tens of millions of citizens need institutional means to maintain social coherence. Formal education here (at primary/secondary school level) is less about marking someone as educated, and more about ensuring that people from opposite ends of the country can communicate just fine, because without it, their dialects would drift apart within few generations. reply technothrasher 17 hours agoparentprev> Parents usually teach their children a lot about language. They give them visual cues, speak at varying rates, change their own tone in some situations, and so on. They do, but this training is not necessary for a child to acquire language. Idioglossia (private languages between two people) is an extreme example, but also the process of a pidgin becoming a creole, where the younger generation of a pidgin speaking population fills in the missing details to create a more complete language, even without ever being exposed to such a language. reply nickpsecurity 2 hours agorootparentThat’s interesting. Thanks for the examples. Are there any places where people post such real-world observations of unsupervised, language learning? reply tialaramex 9 hours agoparentprev> One thing I didn't like was the paragraph on how they differentiate words with no formal training. You don't like it when you're wrong, that's understandable. Of course your reaction, to just deny that you're wrong and learn nothing isn't actually going to help you be less wrong in the future. reply desi_ninja 15 hours agoparentprevIn Indian epic of Mahabharata, there is story of Abhimanyu who learns how to defeat a military strategy in the womb, while his father was telling it to his mother. Which he later uses in epic war. reply lelanthran 8 hours agorootparent> In Indian epic of Mahabharata, there is story of Abhimanyu who learns how to defeat a military strategy in the womb, while his father was telling it to his mother. Which he later uses in epic war. I don't see what fiction has to do with this. reply sc90 15 hours agoparentprevPlay this, by your stomach, let my words massage it and rub it - Nas reply nooyurrsdey 15 hours agorootparentI can't emphasize how happy it makes me to see a Nas lyric on HN. Here's the track if anyone's interested: https://genius.com/Nas-queens-get-the-money-lyrics reply moi2388 9 hours agoprevMakes sense, perhaps this is also why nursery rhymes chop up words into syllables following the melody. reply Anotheroneagain 14 hours agoprevI believe it doesn't start with an explicit attempt to identify words, but from the attempt to compress the sound into fewest meaningful dimensions, and the words naturally result from it. It's a low level process that creates these words, as well as the apparent boundaries. reply foobarqux 17 hours agoprevChomsky describes why the word identification method described in the article can’t work in English (though a more elaborate method is one of the few successes he knows in applying statistical methods to language): https://youtu.be/92GOS1VIGxY The arguments against universal grammar are no good either: For example even though children may hear lots of examples it isn’t nearly enough to derive a hierarchical grammar. It also doesn’t explain why language is hierarchical (just like it doesn’t explain why we can’t speak and hear like a modem) reply stared 10 hours agoparentOnce you go beyond Markov chains and into vector models (Transformers or even LSTMs), next —-token-- syllable prediction can capture grammar. reply foldr 10 hours agorootparentThat some aspects of grammar can be captured by statistical analysis was never really in dispute. The OP is slightly confused about the hierarchy part. Chomsky never said that you couldn't discover by statistical analysis that languages have a hierarchical structure. He rather said that a baby, based on the data it has available, would not be able to determine statistically that certain rules of grammar are defined over this structure rather than over the linear sequence of words. reply stared 9 hours agorootparentI mean, contrary to Chomsky, that certainly advanced statistical models are able to capture all grammar - at least, nit worse than ordinary humans. It feels that Chomsky’s understanding of ML stopped at the level of Markov chains. reply foobarqux 7 hours agorootparentFirst, Chomsky (and I) were talking about language acquisition in children where there aren't billions of examples, so it's completely irrelevant if some other system can do something, the question is how do humans do it. Second, there isn't any evidence that LLMs have captured grammar rules in any meaningful sense, just as they can't do addition or any other recursive computation. reply foldr 8 hours agorootparentprevIs there any work demonstrating this? For example, how do statistical models capture adjunct/argument asymmetries in extraction? reply stared 7 hours agorootparentAttention is one of the core parts of transformer architecture, so I would be surprised if they have any trouble understanding this asymmetry. Could you provide a testable hypothesis? I would be happy to test it on GPT4. reply foldr 6 hours agorootparentSure, here are a couple of examples of ECP violations removing ambiguities. 1a. How often did you tell John that he should take out the trash? b. How often did you tell John why he should take out the trash? (1a) can either be a question about frequency of telling or frequency of trash disposal, whereas (1b) can only be a question about frequency of telling. I asked GPT-4 to explain how each sentence was ambiguous and it seemed to entirely miss the embedded readings (the ones about frequency of trash disposal) for both sentences, while finding some other ambiguities that were spurious (such as suggesting erroneously that (1b) could be a question about how many different reasons you gave John in a single instance). Similarly, (2a) has both a de re and a de dicto reading, whereas (2b) has only a de re reading: 2a. How many books did Bill say that Mary should read? b. How many books did Bill explain why Mary should read? That is, (2a) can be asked either in a scenario where Bill has said \"read 10 books!\" or in a scenario where Bill has said \"read Book A, Book B and Book C!\" without necessarily counting the books himself. (2b), on the other hand, only has the second kind of interpretation. I've had mixed results with GPT-4 in this case (depending on exact choices of vocabulary, etc.), but it certainly makes some mistakes. For example, it says that (2b) can mean \"John explained the reason for a certain number of books that Mary bought\". As the sibling comment points out, it would not show very much if GTP-4 did correctly determine these ambiguities as it has had access to much more data than a child. You would also need to show that the same statistical techniques would work when applied to a realistic dataset. reply stared 6 hours agorootparentThank you for providing these examples. I asked GPT4o, and it has no trouble with understanding 1a: https://chatgpt.com/share/daea469f-d823-45e2-9d6b-f6bea82a26... As a side note, my instinctive reading is on the telling frequency. Sure, one can make a garden path sentence, but (for my own ESL eyes and ears) it would be more straightforward to say, \"Did you tell John how often he should take out the trash?\" 2b does not feel right on its own (and I am not an AI). I can understand it, but it feels like reverse engineering rather than reading a normal sentence. reply foldr 6 hours agorootparentThe whole issue is the difference between (1a) and (1b), not whether the AI can understand one of the sentences under some of its available interpretations. Indeed, with GPT-4o, I get the same result as you for (1a), but also a description of a spurious parallel ambiguity in (1b). Part of the trouble here is the inconsistency of results depending on the exact phrasing of the question and random variation in GPT-4's responses. I wouldn't be surprised if it sometimes gives the right answers, but I don't think it does so reliably. (2b) is what's known in classical terms as a 'subjacency violation', so yes, it sounds imperfect. Nonetheless, native speakers agree on which interpretations it can and cannot have. GPT-4 does not have the same capacity, as far as I've been able to determine. You sometimes have to be a little creative with scenario construction for sentences like (2b) to click. \"Ok, So Bill explained why Mary should read War and Peace, then he explained why she should read The 39 Steps, and then he explained why she should read some other titles that I can't remember. I wonder just how many books Bill explained why Mary should read.\" But try constructing a scenario for the other interpretation and you'll find that it's still just as bad. reply stared 5 hours agorootparentWell, prompting is not a nice addition to LLM; it is a necessary thing to do. > Nonetheless, native speakers agree on which interpretations it can and cannot have. GPT-4 does not have the same capacity, as far as I've been able to determine. This one is an expectation, not even a factual statement. A factual statement would be \"95% of English native speakers with a college degree\" or so. Among less educated, the numbers could be depressingly low, even for much more straightforward tasks. Then, the question is how a given ML model fares against real data, not against some platonic ideal. reply foobarqux 5 hours agorootparentThere is no example or standard that would satisfy you. Any failing example can be added to the training set in the next version and even if it couldn't it wouldn't matter because you could find a person somewhere that would also fail it. reply new299 12 hours agoparentprevAnyone have a link to the work he cites? reply foldr 10 hours agorootparentI haven't watched the video, but probably a reference to this and subsequent work along the same lines: https://aclanthology.org/W04-1307.pdf https://www.ling.upenn.edu/~ycharles/papers/quick.pdf https://sites.socsci.uci.edu/~lpearl/courses/readings/Yang20... reply contingencies 7 hours agoparentprevHonestly, the first thing I did was ctrl+F and chomsky, and as there was no reference I didn't bother reading the article. Thanks for inserting some credit where credit is due. This is coming from a background of serious interest in computational linguistics in the late 1990s... I'm aware Chomsky's since dethroned, but the contribution was massive. I am greatly disinterested in these days of popular generative ML, where LLMs ~= hall of mirrors(internet-scale abject kerfuffery) and then people guffaw in awe of the remix. reply thfuran 16 hours agoparentprevMeaning what, at a few kilobaud? reply smeej 18 hours agoprev> Just think how you pronounce the syllable ham when referring to a piece of meat and when talking about a fury animal—a long ham and a short ham-ster. Hang on. Y'all pronounce these differently? I've lived in four U.S. regions and have a pretty generic middle-American accent and I'm having trouble even thinking what the distinction might be. reply wk_end 18 hours agoparentCanadian (Toronto) English speaker, although I’ve lived in the US (NYC and Seattle) for a while and it probably rubbed off on me. To me they sound pretty much same if you’re very consciously saying the words as units, in isolation, “trying to pronounce them”. If I say them a little more casually, though, the sounds that come out of my mouth are a little bit closer to “haam” and “hmster” - the vowel sound definitely gets emphasized in the former case and clipped almost entirely in the latter case. It’s really easy to trick yourself into thinking you’re saying a word “properly” unless you’re very very conscious of the sounds you’re producing. As a Canadian, I know this - the stereotype Americans have of the Canadian accent is exaggerated, and it’s much stronger in rural areas, to the extent that I didn’t really think I sounded any different to the Americans in TV or movies. But Americans could still pretty quickly tell I was Canadian when I moved down there. The word “about” is one of the canonical tells, and although I don’t say “aboot” or anything, I learned that I do really say it in an accented way. It wouldn’t be apparent if you just asked me to pronounce the word, but in a sentence my mouth would gloss over the vowels, saying something more like “abut”. reply smeej 16 hours agorootparentI tested them out loud in the context of sentences: \"Can I get some ham on that sandwich?\" and \"Did you get your kid a hamster?\" and the \"ham\" part still sounds exactly the same to me. It could definitely be an accent or something that I just happen not to have heard much. It just surprised me that I couldn't think of what difference the author was thinking there might be. reply chefandy 15 hours agorootparentI pronounce the vowels in Mary, marry, and merry differently— like mail, marathon, and merit– but apparently that's not true in many places in the US. My regional accent is pretty mild, but I did grow up in a famously heavily accented area. reply junker37 15 hours agorootparentBoston by chance? reply chefandy 2 hours agorootparentYes MA, but closer to RI, which is fairly different in some ways. The closest famous example is Emeril Lagasse, who most people assume is from New York. One big factor is the short O pronunciation being a lot rounder. Coffee is cawfee, like it is closer to NYC. reply Dylan16807 17 hours agorootparentprevIt's hard to really evaluate in isolation, but if I put them in sentences I slur them about the same. reply o11c 17 hours agoparentprevHm, is it saying \"hamster\" is /æ/ whereas \"ham\" is /æː/? I can't actually find a source that uses IPA yet bothers with vowel length right now ... Even in dialects where it is not traditionally transcribed (such as American - partly because long vowels are most common in place of a disappearing /r/), usually there is in fact an audible difference when you aren't thinking about it. This is part of why computer speech always used to sound so terrible. The most blatant example is when comparing vowels before /t/ vs before /d/, which means that \"matter\" (short) and \"madder\" (long) are distinguishable even in accents where /t/ is pronounced like /d/ in this context. Note that this is orthogonal to stress (since both syllables are stressed in this example), and also orthogonal to the badly-named \"long vowel, short vowel\" taught in school (which is actually for completely different vowel sounds and which omits several other vowel sounds). reply smeej 16 hours agorootparentNow you're making me wonder if I just don't make enough distinctions in my speech or something, at least when it comes to the short A sound. If I speak these sentences out loud, the words sound exactly the same to me: \"That doesn't matter at all.\" \"He was madder than hell.\" Incidentally I'd say both A's here also sound just like my A's in ham and hamster! It's a very pinched sound, almost nasally. My choir director has had to train it out of us by exaggerating it and making it sound even more ridiculous because it has no place in singing. reply skeledrew 13 hours agorootparentSign of the quintessential American. For me, a Jamaican, \"madder\" and \"matter\" are very distinct. And I don't think I'll ever get how a \"t\" becomes a \"d\". reply Sharlin 9 hours agorootparent[t] is just the voiceless version of [d] and vice versa. The more lazily you pronounce the [t] between two vowels, the closer to [d] it becomes because it's less work to just keep the vocal chords vibrating. reply o11c 15 hours agorootparentprevI mean, everything is dialect-specific, and there is a lot of variation between dialects around /æ/ in particular (trap-bath, bad-lad, Mary-marry-merry, and the whole mess of /æ/-raising/tensing - is ban-back an example that's always split?). But I'd still say it'd be interesting to record your voice and check the actual timings even if you can't consciously hear the length. Assuming you can actually practice running speech while thinking about it, of course. reply lelanthran 8 hours agorootparentprev> The most blatant example is when comparing vowels before /t/ vs before /d/, which means that \"matter\" (short) and \"madder\" (long) are distinguishable This doesn't resonate with me - the `ma` in both those words are pronounced exactly the same and are of the same duration when speaking. I checked a few youtube videos now before responding (some science video with 'matter' in the title and some video with 'madder' in the title) and there isn't a distinguishable difference. Do you have a few links to videos that show a difference? reply hammock 16 hours agorootparentprevIt’s not about vowel length. It’s about the time spent saying the actual word reply EgregiousCube 18 hours agoparentprevAuthor is in England. reply IanCal 18 hours agorootparentI'm in England and I don't get what they're saying. Edit - ah I see reading the context now, it's not the pronunciation but the speed. Hamster has a very fast ham sound reply Sharlin 9 hours agorootparentSpeed (or more accurately, length) is a part of pronunciation. English speakers don't often think about vowel length because it has no semantic significance like in some other languages, but there's still some phonemic variation, depending on one's dialect. reply Anotheroneagain 11 hours agoparentprevI'm an ESL, mind you but I think the vowel gets pretty cramped with similar short vowels (hem-ham-hum) so the tendency is to make it more different. It drifted away in general in American English, but the vowel in \"air\" stands in the way in non-rhotic accents, so the change is much more random and word specific. reply thaumasiotes 16 hours agoparentprevAustralian English is supposed to have phonemic vowel length, such that \"can\" (the container) and \"can\" (the auxiliary verb) are pronounced differently. reply troad 6 hours agorootparentAustralian English indeed has phonemic vowel length. For the average speaker, the only distinction between 'but' and 'Bart' is length. reply koolba 18 hours agoparentprev> Y'all … thinning … Kind of gave it away there. reply smeej 16 hours agorootparentI edited the 2nd one because it was an actual typo, but ironically the American south is not one of the regions where I've lived. I just find \"y'all\" clearer than any of its alternatives! reply pcloadletter_ 16 hours agoprevI gave up trying to read this after closing the 3rd popup reply YeGoblynQueenne 17 hours agoprev [–] What a silly question to ask. They learn by training on the entire web, hiding one word at a time. /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Steven Mithen examines whether language acquisition in babies and young children is due to specialized mental processes or general learning mechanisms.",
      "He highlights the use of \"transitional probabilities\" by infants to identify words within continuous speech, showcasing their statistical learning abilities.",
      "Mithen's insights challenge traditional views on language evolution and underscore the complexity of early language learning."
    ],
    "commentSummary": [
      "Babies and young children learn language through a mix of parental guidance and statistical learning, where parents repeat simple words with pauses to help recognize word boundaries.",
      "Bilingual children may mix languages, creating new words that make statistical sense, demonstrating the influence of multilingual environments on pronunciation and grammar.",
      "Consistent language exposure is crucial, as children adapt based on their environment and interactions, combining natural immersion with structured learning to understand patterns and rules."
    ],
    "points": 204,
    "commentCount": 161,
    "retryCount": 0,
    "time": 1718925580
  },
  {
    "id": 40749754,
    "title": "Innovation heroes are a sign of a dysfunctional organization",
    "originLink": "https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"steveblank.substack.com\",cType: 'non-interactive',cNounce: '60993',cRay: '89764009ffeb10dc',cHash: '33bbb25cea6d55e',cUPMDTk: \"\\/p\\/why-innovation-heroes-are-a-sign?__cf_chl_tk=PFGQYhP.CaHsryt76KCar68eMtqBUh_hrTRknDNOpTk-1718996517-0.0.1.1-3775\",cFPWv: 'b',cTTimeMs: '1000',cMTimeMs: '120000',cTplV: 5,cTplB: 'cf',cK: \"visitor-time\",fa: \"\\/p\\/why-innovation-heroes-are-a-sign?__cf_chl_f_tk=PFGQYhP.CaHsryt76KCar68eMtqBUh_hrTRknDNOpTk-1718996517-0.0.1.1-3775\",md: \"jJjCZK3V6DDDYWg.hE.HnkN8kuPcTEObfIrNJ6HEhXc-1718996517-1.1.1.1-zhoQ4fI2llx_D7zJ5akDVQgBjPoHw2jwLnyUFmYyEHFQvBDfzz.VEMhkARW40WWHsEgXnYQRKzzj2uVQ4UgxWw4S3PRl0EJssIs6XAVpjPB7Sk0_JXZtqh4iSU4kSO15mBZKcwcpvQtVC6RuBosEcWYIGBYW7altMlKI8iM3MuutTspmN.KMAI9kj4wE8PMlyJTCHsBLVNis3kNGkBbVTD6b3CdueUSFC78y3lGYnG5QegmK_SPxn5Z93mmZJlTaztY.b3VFsdjT99d0BarohTCcZjZg8.rCEMSS38KEa1l45ocBTkgHhTH3EY.GD5l2St9yLPPX0ocjL3OZf1Hw7MvqANj6wRVIerchXML1wH8Df2PlJBNes7drGoebhEGxwWZmh8iCDBAREgitoaCDEXW94yuavdAtinx4RO7Q.K.zDqzDhOflyw.IhefVFwBWrjbzNg3OpWvy7HJ3kkZfQc1aXFKKVVxYnsK0ntYAzCesdCEG_sXAu33ZN9moB9rvkM_2Ue74uGWLEa0xrcpzIQThsDJATmed50kI9cVHNc0SLi.S7kwm06T6IXP9Fafht.n76Lg8rPKoTk4dzXWzyBmCMCOUAO7OZP63sXgNKTVwmXgF.u_oK.iUUv5XSoP4H4LjDSHNP5lw8ubr1i1HfjU3BADt63On6HzWOqfjzXBKAs46QWvamx8MUZYgG0oHj5GafeaAc4UkZ.6Xe0UwELvd2FyIJkAoFXADNURN1TbW.c_bCtial7kLBQdXzX0j.8zvpTmuH5oE8.EPMDXVAMN_RwXummOxFnd2Z.AtFH_bZWPC.MgqgcDbwM5QbbkPjUien83WBPuCaEH60KaxsOFZ7.7Xbe5PM.Ur8uGriytR_KFgl8wAprL1hr0.a03QG7qyaO7r5sDvBy775wPkXOMYH7QYRzArepIUBXTsEdS1cchse_BKCNG6s3NuzmYMYFtIP_a8N9jqvHchJf.NDomBHsfRzgDEfa2w3cTnXNiT5CQhoXyaIP8zdOITGrfulzwFUpZOdQW29he0s0eSYyY9_CXzelMbFYVC3ObKeQmgF8mg1_RdiKn3OqqYJ0j5LIIbGXCX9E8BCkGwHXAmNfhlmhfar4NiIURpgKX_du4mGgrMj76GMmD.HQXcitfCpGMVToGcS9Z7S6Qdg0m_tbEqL2Z6eIBIB.MOjumvayZAnFw5iCH2kbFOhJngg3HYWskDRPQwqS9jWEs3A8GMOLljPplHXBLFsLchWmVNVGyuU1jx0Fyp5v9C8bilFKUu4lphI5jTr7HCKJtZf21TIHYhM5LwytUJKK6ka6fu3Uw\",mdrd: \"QmHhd8dOUDTI2vJnT9cVdcGDCTfXlVNUvg1w.hJHLxQ-1718996517-1.1.1.1-gzZMQPlrKnJ1dayFoihPnIRbuUHoVUCB3o._MHFSMlHz6ekpKbGE37kHt_aaUUxjI_KAQIWeml7n7LWT_Ta6PTGYAf1.br8RDjVOa6mKKLljFz_AhyTFfnQz9uKMt2LEIJFUAqZ7kXk3hl_7iX1r4RyFcZ5RQ_L7woSPywG._aV5cM8mQAJ.Y1j844HQVJhdvPN338ZlKRDmPmoH67WwdrTRy9__eslGk0deHd5w0ZDVlyiLxjyc0XhiPIoev2KvddvSrD7dzNWOAICbZQZWx3Vkd3.5PDFoivRP_dLucNHgNBpnDUMc4GQpVmP8Edon5DYzLfn1eNEvI3r2RZqUQM2nkKxfHIyJK96YcG9o6Rfsa5vbqiTR2XrFFq97lG4nHD3w_n07NR_p9zo9nJDlz7aZ9mvx9_Gh0WYX6WmaYiOQBPczgV41TmRyETfKIIU3y3UcNPUJmsRoKEY_JtSHsqHtHi02V_lk2MDWVz.AKOFzNetiZytzAhp.eFYQ8ybiVDJ.aIdC0B0CM5NK0HoBRaeao2W5VvepFMfvRTlVmugH9fS4UwSBt78z4Crwyuel0PRmbicmEu_HS6pKk4BG40zRnB3RCquDxqeWvGAEhajSI2te4ok7aNLEnamHJ.YsQ71QFFfgI1Mg0qbuOnYbagNpwIGlADx6qVtgqFP0_fnXjSIfUk4ZVtpqBa75ReIhfPYybkRhGtXmSGSU6.ueNqsbEVDI5VLmpISpOifXWhCmGvsgIwyUA3zQjXcdrhnFzIw5lLsgYqVJoeTHTocNsKPkAXp17Z11pY5lx5b0syE8TNIR3HvbLlf9YWUJIrx4xki8NlL0SOaSGqcPKacM_fb48veNB.NyLk01WfYpGBO_eak55L6zF8yjcWNdPEscDnqaMRGD3_v37HyVg2wPdLcy2Ry8VY7xGBFKmcT82qz7LDkjI_5LwNXatqfVubXm8j9Ctkr6Pi2t_FSx4zGfjcI9OJrwnqBEl86oapj_NwX2YVkX_dFSpiu9XXvlqHkWa1sKbaoKK1qVdJ8EiwJd.Y4GeVO_vyfIYnnRD2o6RxlH5FWrT5T_Ghp_eJd6TZCu6klPb_74gsq5Qs5Y5H6WF40ISazuFTfG4rnVSHltsGivo0TDqNO2f19fuyd1JTHYwqnEG6vBgWf4fV7gvZarVuAouPmKESqcYfo_fW1xSmE0kP2OggqlomWMyyHDOmIi7t.dph7Gkx3V1YCXizekbIJeIJvp0_IFyqQy6Xw5V2NxilnoEay9B_8P2NE45QWajAIGKAKM7v2nLpMx01k0Bhl8bR64wLqP299lHiLuYweYCJSrgYvIdyXbcRCJKyRgfc.KQ6ED7bksTScQMtKOwwir1tFPEmvnTbTVYJa.ZPqTGhI9_Y00k0OPy8bArwYfD2VPKfL1AmskIMu0K59xmc1RQN5p7lXsRy2oq5Wbye1mtOTW4vCuF.AN1SKbNimMxv87quviBVCE6ujxXNSksRFcRQK6.06_MBq2CMohyBtF3bi8rd29VE0qHvGr57u_GO7E2KWPnLi.8._8Qcum4ukakCDA3S9oQmylb_hu2WcaWildSYDK.43dWB.9KPi6maSkUZ2GFXPeADQQTXVLXfZPbAFtpzk2rf6ko2JBorjLkmBf6AsKKUW4mR.9BhxVDSWBsI9Vkta3FHrmrVkkCas11p6ZQqEIeNRTLqaXiVwIH64Kq2bkHx8dbq27vKXZ6xOEjvFL8o.q5d22qxiHJnUT_jdBoKrW6x2bcQqmUn6tQza4.jibnOJtfk2KdThIEbc8CgusrHcnzDq2oZrDanJMZTVwIMtLy55wkacI93wtvnBrUtOVt26PP31FHh_VVvVMM9ZzHGeicQURnqSOzFDX.u1z6lVw3cvNXRrxM.8_LwNczeit1QYIiDibEj9uVQ327j.g6ZxH2NaeH3hEHEBPXtSPt1ybv7SCr0sMHzVToTZqt3LTqJWb2CC5Wb1SX88HvoVC2Su6VnNcLBrnRt2Ddy9hAB_CG5sw4UAb19Ox_KE9oAScuArb_0LJ.Wn0vNlQp3XxFPbI9euvk.8Tv4K94.sdg_Tq9E40wIoUeetjUUu.IKfYOdJ0upDMu3AQGYWhHWo4sujDk7bHfXu1ikIGPUwPrLU9TS91GB9DDRFdfNJgu7CFEBLsvi0Iqszl9KYCq8_Chfh2n2tjfG4lI..vRcNEDb8KBLXG5nRqukGqVKaER2YrDiLnqNHpxsA235Rcvg2jVZZB9adOTmfkgaxehvxTFlHSfQOPHm1W8czbhCvnDm51MAVxAi2MjzLY.n7npZCi7mSpXHg9RigaFQ\",cRq: {ru: 'aHR0cHM6Ly9zdGV2ZWJsYW5rLnN1YnN0YWNrLmNvbS9wL3doeS1pbm5vdmF0aW9uLWhlcm9lcy1hcmUtYS1zaWdu',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',rm: 'R0VU',d: 'me1G0E5Hpqlb28QLQa+m/twhgrVpKa7f125n1YVrYA+A3FE+HkaeGmokqwZmPio5pxBooBJl9wGVr47s04ABgTqfccb9eRyom9W+wW1ty+uwjpKdPLQRNOZKDV/iDqilDNgCvHI3on/IaRq13Y9EJdGe1R7X5+Ql9Bj+G6DdJ8E1sraza3KlPYKYLZ4MGh4Z4gJr//XjB+zyO5ij6JgD4+yFHYw4Aur8ozLXm88CUA2ZNVqFcbm35LXv8GkxSKUAq44PtF6L1n9TribRaoUMsFfT3C4QFdLOFDnrpPV4jW9B/gk8yIkq6pLLrMFX8GPJOGnRse3JshogyDgUv65UklPlkgUvOIs8dEJe4j1TMWPaBk4PhDXoxjZtAsMEYxB2ftWkECeLCS+K5tJIno15NMNJij1X/3EmhzUzVJR9afFNFYdeAx6JWJX1wToYZL6VeIiGbJSWhneg7a2t3lJ/Azkjf1dr6EOR42T7ewxlNLF5L8X302hbCvZYY6zRhkynLiFNBMJH1bPKyv9V6zcFy9qz5HNcOFx0CA2R5BSvirPuimWkbP2/0KnfhjGtYLxR',t: 'MTcxODk5NjUxNy4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'lJb5cyGdAWu/kqaJh4l8Su2NJKGPm6vfn95UwJvoP0c=',i1: 'kqrybWsrqj9pKPHywGvCHA==',i2: 'AP+dENYywEfuBQWbrP7B3w==',zh: 'o01jypKJQ++/gkxUTvC40nYpXBhuMc66cm0hd/Tc920=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: '6rPkCsrqlmSFrpp0458Qu3abzwiyPyidFMounUwKM/w=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=89764009ffeb10dc';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/p\\/why-innovation-heroes-are-a-sign?__cf_chl_rt_tk=PFGQYhP.CaHsryt76KCar68eMtqBUh_hrTRknDNOpTk-1718996517-0.0.1.1-3775\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=40749754",
    "commentBody": "Innovation heroes are a sign of a dysfunctional organization (steveblank.substack.com)192 points by sblank 4 hours agohidepastfavorite110 comments ecshafer 2 hours agoMy goto example of a dysfunctional and bureaucratic organization is an example I saw in a finance company I worked for. I was a on team that we were doing a major project. We basically ran Kanban but had to run \"sprints\" so we chose 4 week sprints to get it out of the way and we put everything on the board we had to get done that month to stay on track. Our pipeline was setup in such a way that you were required to have a jira ticket to push a commit. We were really crushing our timeline, doing 2x the work we were expected, a really great team honestly. But we opened up bugs and additional features in the middle of our sprints to track our work as we did it. What this amounted to was maybe us saying we have to do 40 tickets this month, and wed be closing like 80. Everyone should have been thrilled by this development! Well we had a sprint review where some \"Senior Project Manager\" That wasn't really affiliated with our project but was some manager higher up was mad that us opening up new tickets mid sprint and closing them was ruining the org level burn down charts and expected delivery. They wouldn't give up on this, and said it was our fault for not estimating better (which sure, but we were beating expectations!). So we did the reasonable thing and improved our estimates! No of course not, we doubled our number of tickets and filled in \"placeholder\" on half of them, and used them as needed then closed everything out at the end of the sprint, where we were congratulated by everyone for our phenomonal estimation. reply riskable 50 minutes agoparentI also work at a large financial institution and have had many similar situations. Fortunately, I'm the one in charge (team lead of sorts) and I have a pretty standard response to such \"high level\" nonsense: \"Your inability to adequately track my team's weekly or monthly performance is not my problem.\" Every \"project\" has plans, deliverables, and due dates and those are the ultimate arbiters of a team's performance. Not the weekly/monthly statistics! If we open 10 or 10,000 tickets it makes no difference. It's entirely arbitrary and only carries meaning for the team in question (not upper management). Like some high-level manager/PM is going to be able to make any difference whatsoever on some software development task by watching weekly Jira ticket statistics. Sounds like they're giving themselves busywork to justify their role. Because having fancy charts and statistics at meetings of managers makes you look like you know what you're doing (LOL). reply gspencley 26 minutes agorootparent> \"Your inability to adequately track my team's weekly or monthly performance is not my problem.\" I'm an engineer and I can certainly understand and empathize with where that sentiment comes from. However, when people say things like this the first thought that goes through my head is \"there is a culture problem.\" That sentiment underscores an adversarial relationship between teams and leadership/management. That adversarial relationship should never exist (it does all too often, but it shouldn't). Tracking timelines and deliverables is something that requires communication. That communication can be automated, but it's not up to leadership or management to implement that automation. They are not the engineers. So if the process that is in place, which has worked for them despite inefficiencies (which they may not be aware of) is suddenly disrupted then no, it is not only \"their problem.\" Some team went and did something differently than how things are usually done. The team [rightfully] recognizes that it is an improvement, but it was unsolicited and the communication / warning of the upcoming change was likely lacking. Companies are called \"companies\" for a reason. They involve multiple people with varying skill-sets, responsibilities, understanding of how things do and should work, they have their own pressures and reporting structures (they need to hand things over to their management who expects a certain status quo as well) and most people have a default low tolerance for change. This is no one person's fault. The company culture needs to facilitate iteration, improvement and innovation. reply fifticon 14 minutes agorootparentthe management where I work, has the known strategic objective of eventually outsourcing all our development jobs to our indian facility. However, they prefer us to not leave our jobs before time, to ensure an orderly transition that wont lose us customers. In which universe are we not supposed to have an adversarial relationship? Our role in this is that our former ownerboss sold us to one of the big vertical software graveyards when he retired. reply tokyolights2 1 hour agoparentprevI have seen this multiple times before. Management chastising good work because some externality of that work shows up on a graph that they have to talk about at some weekly meeting. It doesn't even have to make the graph look bad, just stick out. reply Aurornis 10 minutes agoparentprev> we had a sprint review where some \"Senior Project Manager\" That wasn't really affiliated with our project but was some manager higher up was mad that us opening up new tickets mid sprint and closing them was ruining the org level burn down charts and expected delivery. This is giving me flashbacks of a company I worked for (for a very short period of time). For some reason, they decided that planning accuracy was the most important metric for the software organization. This was the headline metric they talked about in executive meetings and it was primarily how the program managers' bonus structure was determined. So everything in the company revolved around planning accuracy. Opening new tickets within a sprint was strongly discouraged. Doing work from the next sprint was heavily discouraged. Trying to take on big projects within a sprint was heavily discouraged, because if you couldn't 100% guarantee that it would be done by the end of the sprint it posed an existential threat to the Program Managers' charts, and therefore their bonuses. Program Managers wouldn't come out and say any of this, of course. They knew the situation was at odds with delivering software quickly. However, if you deviated from the plan they would pull you into meeting after meeting for hours and hours to try to keep you in line. If you opened a new ticket mid-sprint, you'd get pulled into meetings with Program Managers to justify it. They'd argue and debate and cajole you into deleting the ticket and rolling it into something halfway related. They'd CC your manager on 5 different e-mails and check in multiple times a day to make sure you'd fallen in line. It was hell. Weirdest part to me was how many people around me seemed to enjoy that structure. They recognized the game and gladly played along, delivering a couple hours of work each day and then sitting in meetings to talk about it for the rest of the time. It all came crashing down about 18 months in, when management brought in someone who actually understood software development and started actually looking into what people were doing. I was gone by then, but they went slash and burn on the remaining department. They cut it down to 1/5th of the size and started delivering, as far as I can tell, the same amount of work. reply apozem 1 hour agoparentprevThat is amazing. I worked with a (different?) finance company that did the same thing - every team was judged on their burndown charts. From what I saw, it did no good. Teams simply did not use tickets to track work. If a story surfed sprints, its “ticket” was closed at the end of one sprint and a new one was opened for next. If a priority bug came in mid-sprint, we simply worked on it without a ticket. reply cruffle_duffle 1 hour agorootparent> If a story surfed sprints, its “ticket” was closed at the end of one sprint and a new one was opened for next. LOL, just joined a team that seems to be doing exactly what you describe. You start attaching performance incentives to how tickets are estimated and closed... people are gonna game the fuck out of them. reply sailorganymede 50 minutes agorootparentprevStrange… The finance company I work at also does this. And it’s so strange. I feel like they value Jira tickets more than actual work being done. It’s even funnier cause we get asked to close work when we want to go into prod and told to create new tickets to put our work in to prod. which kind of is annoying. reply siva7 1 hour agoparentprev> Everyone should have been thrilled by this development! Well we had a sprint review where some \"Senior Project Manager\" That wasn't really affiliated with our project but was some manager higher up was mad that us opening up new tickets mid sprint and closing them was ruining the org level burn down charts and expected delivery. Why should they be thrilled that you were running on Kanban but claiming to do otherwise and therefore ruining the burn down charts? reply ecshafer 1 hour agorootparentThis was a team of 10 people, so costing the company $2M+ a year. How is halving the timeline on a very major, mission critical project not a good thing? Some appendage management structure in the organization having charts and reports looking a certain way is secondary to how much value is being produced by a team of expensive engineers. reply gopher_space 15 minutes agorootparentprevAm I working for The Tacoma Chart Co. or are we developing actual products here? reply constantcrying 1 hour agoparentprev>No of course not, we doubled our number of tickets and filled in \"placeholder\" on half of them, and used them as needed then closed everything out at the end of the sprint, where we were congratulated by everyone for our phenomonal estimation. Are you sure this is a negative example? You even have a work around which makes everyone happy. This isn't what a dysfunctional organization looks like. reply loloquwowndueo 1 hour agorootparentIt’s called gaming the metrics and it’s totally a sign of dysfunction at the org level. The team that decided to do this to keep the metrics happy rocks, though :) reply sodapopcan 1 hour agorootparentExactly. Org-level metrics are a sign distrust and micromanaging. It's useful for a team to look at their own metrics so they can understand what they are capable of delivering, but no one else should care. That they can reliably and consistently deliver a satisfactory amount of work should be the only \"metric\" they are judged on. reply snapetom 49 minutes agorootparentprevAs an engineer recently turned technical product manager, I think this is one of the values I've provided. I focus on feature deliverables and meeting timelines. How you get there is sausage making that doesn't need to be over-emphasized. There are too many ways to game the metrics. I've done it, and I'm constantly impressed at seeing how innovative colleagues do it. If you focus so much on metrics during the development process, you're going to just add useless overhead and piss off developers. My boss comes from a sales background, and thankfully he gets it despite pressure from other senior leaders. reply constantcrying 1 hour agorootparentprev>It’s called gaming the metrics and it’s totally a sign of dysfunction at the org level. Have you seen actually dysfunctional organization? Millions being burned over ridiculous mismanagement. Bureaucracy which takes up vast amounts of time. Tracking of completely meaningless metrics. An IT department which needs to be \"worked around\". Managers who's only interaction point is them asking you what they should write into their MS Project document. Constant shifting away of responsibilities. People making completely unqualified decisions over the heads of the actual people being concerned. Inter department conflicts. Really I could go on, but a bit of massaging metrics (which are totally irrelevant anyway and their trackers can be safely fired) is not particularly bad. reply sodapopcan 1 hour agorootparentThe person you are responded to said it's a sign of dysfunctional org, not that it by itself make an org dysfunctional. There is a huge difference. reply constantcrying 55 minutes agorootparent>The person you are responded to said it's a sign of dysfunctional org He said it was his go to example of dysfunction. reply sodapopcan 11 minutes agorootparentAh GP did ya, fair enough. I assume a bit of hyperbole but sure. I would infer that there is a lot more going on than just some guy worried about if some guy is worried about individual teams' metrics getting messed up but I don't know the whole story. reply danaris 1 hour agorootparentprevIt's still dysfunction even if the group in this case was able to finagle a happy ending to this specific story. Charts and processes are there to serve people, not the other way around. If a particular group is clearly doing a great job, but the way your metrics are set up claims they're causing problems, then the problem is in your metrics. The organization is dysfunctional because a higher-level manager is able to enforce their particular view of how these metrics are supposed to work at the expense of actually productive groups underneath them. Note that if the GP had actually done what they were told, they would simply have not worked on all those bugs that came in mid-sprint until the next one. reply constantcrying 1 hour agorootparentMy point was just that the example is a very low bar for dysfunction. reply nonameiguess 6 minutes agoparentprevI experienced something like this for many years across several of the big five defense contractors and smaller SBIR contractors working for the US DoD and IC. As someone who 20 years ago had a slightly better understanding of how computers and networks actually work than an average developer and was comfortable at the command line in an era when juniors increasingly couldn't leave their IDE without becoming hopelessly lost, as DevOps, SRE, and platform engineering started to become things, my career drifted in that direction. The problem being on teams like that was always the same. You're responsible for developing software products of some sort, but they're software that runs, tests, or delivers other software or even orchestrates the operations of an entire environment shared by many different applications. This inevitably means a whole lot of your work is the classic incident response/post-mortem of operations, plus some level of customer support given to other development teams because internal platform teams never get a separate support organization. To this day, the government has no idea how to handle this in light of how DFARS (administrative law governing acquisition) works. Every labor hour a contractor charges has to be tied to a specific line of accounting which is itself tied to some unit of planned work tracked in an issue tracker or project management system of some sort. Most of the time, this is an Epic in Jira. This is logical insofar as you consider the intention of acquisition as a category of appropriations bill. A budget proposal with line items tied to measurable product features is presented to and approved by Congress. You have to demonstrate you're spending the money they gave you on what they approved you to spend it on, because per the Constitution, that's how power of the purse works. The executive branch can't just do whatever it wants. But it falls apart as contractor labor increasingly replaces federal civil servants. When your job is anywhere from half to all running and maintaining an operational system, that isn't really acquisitions any more. It's operations and maintenance, which is an entirely separate appropriation. Soldiers manning a defensive perimeter have no idea when they're going to be attacked or how much work they'll be putting in. Software operations is effectively just the less lethal civilian equivalent of that. The government arguably even recognizes this in many ways. Typically, running and maintaining the production system is a separate contract from the development contract and it uses separate work tracking systems, and if someone spends 8 hours watching a screen while producing no quantifiable work outputs, so be it. They charge 8 hours because that's what the contract says they're supposed to do. But as we're increasingly expected to be modern software organizations with things like CI/CD, test and staging environments, and you inevitably need to run at least some of your own development infrastructure, well now what? You have a lot of people whose job is the same. Be on constant lookout and respond as needed, but now it's part of a development contract and they need to have quantifiable work outputs that be tied to a budgetary line item with an associated product feature. So we convolute nonsense out of thin air like cloning the same monthly \"Support\" Epic in Jira, month after month. \"As a developer, I want my tooling to work so I can do my job and deliver value to the government.\" Plug in some SWAG number vaguely guessed at based on how much of this kind of work you ended up doing last year. Every 90 days, play planning poker with it. LARP a product team even though that isn't what you are. reply FireBeyond 1 hour agoparentprevI am dealing with this as a Senior Product Manager, explaining to execs, and defending my engineers when they started introducing metrics, and one of the keys was \"Planned points delivered\". My team gets and triages all the escalated bugs. It's not accurate, not fair, and demoralizing for that team to be \"red\" because the metrics look like: \"Story Points Planned: 30. Planned points delivered: 5.\" when they delivered bug fixes on what amounted to 20-30 unplanned points. \"If they have to do that work, they need to get credit for it, planned or not. You can't have work that is required but counts for nothing in KPIs\". The issue of HOW MUCH unplanned work is being required is a separate and valid discussion, but not in the realm of \"engineering cadence/velocity\". reply Jun8 3 hours agoprevThis a million times! I’ve been in the position of the “innovation hero” but also was unfortunate enough to work on “innovation pipeline” implementations within a large company. These never work because: 1. (99% of) employees don’t care. They have their own job to do and working on other things is an eyebrow raise from their manager (see 3). And what’s the e benefit? Mostly it’s a pat on the back or some “points” in the company award system that you can use to buy shitty merch at the end of the year. 2. Executive leadership doesn’t care because for the most part they don’t trust their own tech team to innovate. Why take the risk when you can buy a startup which comes with a bona fide certificate of innovation. 3. But the real problem (as commonly identified) is middle mgmt, who not only don’t care but are generally hostile to nonstandard work. The reasons for this are complex, partly it’s the aging manager suffering from Peter Principle, partly it’s the fear of negative pushback from senior leadership. PS: Steve is amazed, but 10 months for the sort of setup he describes which includes HW buy and setup, in a Big Corp is very fast. reply cogman10 2 hours agoparentThis all, imo, is simply a trust problem primarily from leadership. Leadership does not trust the grunts to do productive work. So in order to make sure productive work is done, they build elaborate systems of cases, reviews, meetings, planning, scheduling, fighting, readjusting when the schedules are invariably missed, and finger pointing. All almost always completely devoid of input from the grunts. The middle management problem is they are right in the worst place possible. They are removed from the actual work being done so they don't know what it actually takes to do anything and they are blamed for things not accomplished. Further, they are rewarded for every little stupid thing done. It hyperintensities them to do lots of small safe initiatives and vehemently oppose anything with any sort of risk. All while being almost completely disconnected from what actually needs to be done. This all leads to a culture meant to squash innovation. Middle management isn't rewarded for implementing a grunt's idea, they are rewarded for delivering CEO initiatives. Anything that takes time away from that is seen as waste. reply foobarian 1 hour agorootparent> Leadership does not trust the grunts to do productive work Not only that, but unfortunately they are usually right. Without oversight the in-house team has high likelihood of building NIH spaghetti, which causes more problems down the line. To avoid the negative outcome leadership needs to be technically competent and resourced, and that's the other side of the coin - usually they don't have the expertise so in a way they also do not trust themselves to lead the project to a positive outcome. reply throwaway82931 1 hour agorootparent> they are usually right. This message encapsulates why so many software jobs are terrible. Put your heart and soul into doing your best, earnestly combat NIH and pursue meaningful productivity, and _still_ the culture is such that at many companies, there will never be trust because the prevailing culture is that management is \"usually right\" that grunts can't be trusted. I've worked for good bosses that aren't like this, but they're hard to find. reply foobarian 8 minutes agorootparentClean up enough messes left behind by grunts that were trusted by management and you will be cynical too :-) reply danaris 1 hour agorootparentprev> Without oversight the in-house team has high likelihood of building NIH spaghetti I think when you see this you need to start digging deeper and questioning why this is happening. Is it because the \"grunts\" are genuinely bad at their jobs? If this is the case, then who hired them? Or is it because they have been conditioned to believe that if they ask for permission to use an outside tool/library/etc, they will be told \"no, we don't have the budget for that\" or \"that has to go through 12 layers of approval\" or \"great idea! we'll get it into a committee to talk about the best way to implement it and get back to you (in 6-12 months)\"? In other words: Are they building NIH spaghetti not because they lack oversight, but because they have too much, that hampers them from actually doing their damn jobs? reply danaris 1 hour agorootparentprev> This all, imo, is simply a trust problem primarily from leadership. Leadership does not trust the grunts to do productive work. Which, IME, stems from a deep-seated classism that sees \"grunts\" today as being essentially no different than assembly-line workers in the Industrial Revolution: you're just a pair of hands who not only doesn't know enough to make changes in the process, you shouldn't even think of it, because it's not your place. In this worldview, it's managers (and up) who have the education, intelligence, and breeding to know best, and lowly workers just need to shut up and do what they're told. ...This attitude is also responsible for a lot of other really destructive problems in the modern world of work. reply jmull 1 hour agoparentprevMostly agree, but this > aging manager suffering from Peter Principle, partly it’s the fear of negative pushback from senior leadership is not typically the reason, in my experience. I've seen a lack of full comprehension on the part of the team pushing the innovation as to the actual benefit and cost of the innovation to all the affected teams. And as a result, the lack of a plan to address those issues... I've seen a lot of incomplete innovations, where the benefit is real and useful, but the plan leaves various concerns of different teams unaddressed -- no doubt due to the innovation team being unaware. Strangely, the innovation team often wants to push forward anyway, which is not good since the plan is basically unworkable with critical issues unresolved (I can tell they kind of think the issues aren't critical but only because they don't understand them -- leading with ignorance when the processes and products actually have to work at the end is always doomed to fail.) The most common way plans are incomplete that I've seen is when they don't account for the schedule. The plan will take X time away from other work to implement, but the schedule for delivery of that other work isn't moved back X, nor are there other compensating measures. That's an unworkable plan, and any half-decent manager will push back on it. (Schedule impact is usually a tough one... at least at larger places, in my experience, the high-level delivery schedule is negotiated at a high level, and it hard to change for political reasons. That means time for any innovation plans has to be included from the start. Yet slack in any schedule tends to get gobbled up at the team level or below, addressing their concerns -- who doesn't have tons of technical debt they are dying to resolve? There's a way to handle this, but it has to be planned for and done at a high level, and done correctly. The fruits of any innovation team that doesn't have this are gong to be minimal.) reply novagameco 3 hours agoprevI worked for a large company and started seeing opportunities for automation immediately. I proposed some solutions to my boss, and he told me that he agreed that these tasks could be automated, but that we have 10,000 other tasks that could be automated, and each one takes a few months to get the resources provisioned and also set aside developer (me) time to get it done, which could be spent on other projects. What was interesting to me was the self-fulfilling prophecy of dysfunction: because there was so much manual process and red tape, the cost of fixing a particular problem is larger than than the benefits (i.e: time spent on the task exceeds time saved by automating it). But because the tasks do not get automated, the amount of time required to fix things increases bit by bit due to the processes in place. The cost of fixing a task increases marginally every day, and so the cost/benefit ratio increases every day, becoming further justification NOT to fix things. At a certain point you have to look at the bigger picture and recognize that there is a much larger problem in your company than a few excel spreadsheets that could be better automated. reply theideaofcoffee 10 minutes agoparentI know that feeling all too well. And it's such a hard truth that devoting just a little time, letting some projects slip just a bit, to fixing those systemic problems could make lots of others go away, it's just next to impossible to get people to want to change. I've found because people don't want to, they want to keep doing what they're doing and are scared of anything new because that may mean either they'd have to retrain, or more pathologically, that their position is threatened. It all comes down to the people. The right people can make all the difference in something like that, the wrong people make it miserable for the rest. reply JohnMakin 2 hours agoparentprevSo much feel this and have seen it many times. A complete unwillingness to spend a few hours to save hundreds of hours later, because too much is urgent. It's a little bit like a thrashing OS, too many competing resources so the result is everything slows to a crawl or breaks. I've clawed my way out of situations like this but it does require some heroics in the beginning and probably longer hours than your role requires. I am the kind of person that will ask to slow a project down or delay it if it means we get a chance to do things correctly and in a maintainable way, but certain types in management will not really understand enough to completely buy in. reply btbuildem 3 hours agoprevNot sure that you can fix a calcified bureaucracy with \"doctrine\". I sort of get the angle, I think -- if the org operates within a rigid rule framework, you need to speak their language to get anywhere -- so, doctrine is best, because everyone is used to being told what to do? I feel like that approach is counter to the spirit of innovation, that's akin to being forced to have fun; nothing truly innovative will come from it. I think it's more of a lost cause, really. If you want to work on cool new stuff, don't work at a large org. I play the \"innovation hero\" role often enough, and the diversity of pushback we encounter is impressive. It ranges from thinly veiled hostility, the sdev equivalent of NIMBY, lies through omission, through nonviolent noncompliance, all the way to blithely unaware absurdity. One amazing moment stands out to me - we were jumping through the usual hurdles as described in the article, trying to get a prototype to prod, and in one meeting the head of IT indignantly exclaimed \"I am not here to solve problems!\". To paraphrase a scene from the Big Short: he wasn't confessing, he was bragging. The top brass exist to stifle any and all deviation from the norm. Another commenter in this thread makes a very good point: most of these innovation initiatives die stillborn, because the existing power structures exist, their MO is to maintain, and the C-suites would rather buy a successful startup than take any political risks internally. reply adrianmonk 1 hour agoparent> so, doctrine is best, because everyone is used to being told what to do? I think they mean doctrine as in military doctrine (https://en.wikipedia.org/wiki/Military_doctrine), which is a somewhat different idea than, say, religious doctrine. Religious doctrine can easily get rigid and legalistic, and compliance with it can become an end unto itself. Military doctrine is something a military would use in order to make the organization effective at a certain kind of task or endeavor. reply entropicdrifter 2 hours agoparentprev\"Innovation doctrine\" is a contradictory phrase on its face, really. Going, \"Oh we need a static, unchanging set of rules for innovation\" doesn't exactly sound like a good way to attract and retain innovative thinkers lmao reply tredigi 3 hours agoprevI agree to the overall tone, but there are also counter points. One of them is the Google example. To get promoted beyond a certain level, you must have brought some new product over the finish line. Result? They have so many new things happening all the time, all of them suck, and then just move on to the next. Eg how many chat products do they need to invent before they settle on one and let it mature? reply adverbly 3 hours agoparent> To get promoted beyond a certain level, you must have brought some new product over the finish line This always confused me. It looked from the outside like Google does so many things right on the innovation front, but after some early success they have had a rough streak. I'd argue that one thing that Google is doing wrong is gatekeeping promotions based on (overly) well-defined criteria such as new products. Goodhart's Law applies to this situation: you're sure to see lots of new products if its highly rewarded - a lot more than you'd see naturally. As the author mentioned - this might still be desirable depending on the market conditions, but there is a lot more to this discussion, and it's not entirely clear that Google has it wrong. I'd argue that they emphasize comparability of assessment results(e.g. being able to quantify someone's output like a percentage grade in a course) over the actual relevancy of the assessment criteria/work/kpi to the company's bottom line(e.g. does the course's test actually prepare students for the real world). This probably comes as a by-product of the organization's heavily academic-focused staff - so it might actually be the best culture choice for them given that context - but it might also lose to companies that can successfully put a bigger weighting on the right \"intangibles\". reply nutrie 3 hours agoparentprev100 %. There's a sweet spot. I worked for both, start-ups and huge corporations. It's not one or the other, you want the right combination of maturity and fresh attitude. We often don't realize it can take years to steer back to find the balance again, but not oversteer, which is usually the default :) Just as with any other organism. reply dekhn 2 hours agoparentprevbeyond just launching a product, the launch has to have some sort of \"impact\" (at least, this was true in the time period where I was trying to get promoted, roughly 2010-2013). Something that is not perceived as impactful by the promo committee is likely not going to count towards promotion. It's the job of the employee and manager to document the \"impact\". If your manager is a director or higher, they can appeal the promo denial and an appeals committee can be manipulated into giving a promotion. That's what happened to me- promo saw no impact to my launch. Then my director went to appeals and basically said \"promote him, he's doing good work\". Everything about Google messed up my expectations and planning around career. To work anywhere else (a startup, or a pharma) I had to unlearn all the bad habits of self-promotion and cookie-licking and impact-demonstration. Of course many people joke the best way to get promoted at Google is to leave, get promoted elsewhere, and return to Google at a higher level (using all your newly learned negotiation skills). reply laidoffamazon 2 hours agoparentprevThe promotion thing seems severely overstated. At the higher levels for IC and management this is basically how all tech companies that build products are run. But you don't see this said about Microsoft or Amazon, even though they also have hundreds of new features and discrete new products per year. My theory for why Google is different remains unpopular, however. reply p_j_w 2 hours agorootparent>My theory for why Google is different remains unpopular, however. May we hear it? reply laidoffamazon 1 hour agorootparentI think it's because Google has created an insular, navel-gazing culture that is excessively engineer driven rather than customer driven, to the extent of thinking people not at Google are just inferior. reply loceng 1 hour agoparentprevSo that will result in an \"innovation hero\" improving on Google's model so the fundamentals don't create such waste, no? reply theideaofcoffee 12 minutes agoprevThe idea of calcified culture and process isn't limited to large bureaucracies and government institutions. It's just as prevalent in orgs barely tipping over three figures' worth of people. In many ways, seeing it from both the large and the small, the smaller ones are often the most difficult to change because there are one or two people touching everything. Most often they have been there for a long time and such having the 'trust' of management so the bare minimum of 'new' ideas, and new being the state-of-the-art of the industry ten years ago, because they might have to adapt. Playing the innovation hero in that kind of org is dangerous, and I have the battle scars, the burn-out and resentment to prove it. Others have pointed out that the point of the power structures is to self-perpetuate and any threat to that is swiftly dealt with. It's just so sad to see, all because people fear change. I don't know how to change that other than replacing those lifers and swapping out management. Now I know what to look for and am willing to bounce at the first sign of it. reply shermantanktop 3 hours agoprevReminds me of the Drucker quote about “culture eats strategy for breakfast.” The problem is that large organizations naturally drift toward inertia and ossification. If a forward-looking leader wants the culture to change, they are faced with a conundrum: - use a strategy like this (e.g. some top-down “innovation center” approved at the C level) which reinforces the rigidity and process-oriented thinking that needs to change, or - create an insurgent skunkworks group that hopes to prove a different approach via undeniable results. This usually ends with back-alley knives getting unsheathed. reply Angostura 3 hours agoparentI think there is possibly a third way. I’m currently working with the NHS in a Quality Improvement role in a hospital. The team exists to give grass-roots folks the tools they need to make change happen when they spot something in the system which is less than ideal. We offer training, help with setting aims, running projects, finding stakeholders, analysing data etc. These aren’t huge system transformation projects, in fact some of them can be quite small - but as a way of working it feels quite effective. reply withinboredom 1 hour agorootparent> in fact some of them can be quite small The biggest changes always start with someone taking one small step towards their goal. Some of the biggest software (used by billions of people) I've ever worked on started as a simple script made by a teenager. reply Pet_Ant 3 hours agoparentprevInnovation means potential for failure and waste. The larger the company the more it's investors want it for steady predictable returns. Even shrinking returns as long as they are steady and foreseeable. A large company becomes about control. Look at summer blockbusters. They know the demographics that will see it and have a pretty good (not absolute) idea of how much they can make. This is all by design. reply shermantanktop 3 hours agorootparentThis can be by design, sure. But I think it is also an inexorable tendency for people in large groups. Once your company is 10x Dunbar’s number, it is very hard for “we” to mean the entire company. reply debacle 3 hours agoprevOne addendum to this that I have observed: In many organizations, no one is \"in charge.\" If change needs to happen, there are 100 checkboxes, 100 reasons not to change, 100 people who are concerned about their career, resume, budget, department, relevance, etc. For every \"innovation hero\" that wins an award, there are 5 who are \"managed\" into leaving, marginalized, or disenfranchised. The high nail gets the hammer. I have been into and out of the startup space for the last 20 years, and most of the times that I was an \"innovation hero\" it was because there was one person in a corner office who was using me as a proxy for a change they wanted to see happen. The American system of organizational management is...basically glue. reply justin_oaks 2 hours agoparent> 100 reasons not to change I'm reminded of the article \"Layers of Management == Layers of Veto\" [0]. In the article the author explains that each layer of management is likely to veto each idea coming from below. Each idea that didn't come from above is \"insubordination\" and likely to be vetoed. [0] https://slott56.github.io/2010_02_12-layers_of_management_la... reply nlawalker 3 hours agoprevThis is exactly why the common advice is to automate your own tasks on your own time, with your own resources, don’t tell anyone, and enjoy working less. reply anders30 1 hour agoprevLast year, an Innovation Hero championed a completely new build system based on their homegrown version of asdf, essentially. I was the one who had to stay late multiple Fridays making sure our Formal Builds would still work. I've been working to roll back the least well executed portions of this innovation for the past year because it doubled the amount of time required to release our software in the last stages of the pipeline (which was already five days, another rant, I work at VeryBigCo on a mixed HW/SW product). I believe in this story the entrepreneur referenced worked to clear all similar hurdles, but it's hard to feel bad for folks who view themselves as Innovation Heroes when in many cases they're applying solutions in search of a problem. It's also very not fun to be \"that person\" aka \"The Adult in the Room\". Yes, today the \"Innovation\" is part of our processes, but in stripped down form and it cost us two missed formal builds with the corresponding loss of credibility in our customer's eyes... in addition to my own sour grapes and missed dinners with my kid. It also cost me a personal friendship with the hero (my fault, but it's really hard for me to get past the fact this individual put their ego over my time with my family). Agree it's a sign of a dysfunctional organization but it goes both ways. As an Innovation Hero, you should be aghast at how inefficient your org is, but as a part of that org, you should stop and ask, \"How did we get this way and am I sure my solution truly solves all aspects of this situation?\" reply EncomLab 3 hours agoprevRelate to the four stages of employment: 1) This is the new person \"X\" - they are amazing and are going to solve everything! 2) \"X\" is pretty good, but maybe not as good as we thought. 3) \"X\" turned out to just be another average performer. 4) \"X\" is obviously terrible or they would have left for someplace that would treat them better than we do. reply quesera 3 hours agoparentI've never seen this written down, but I've definitely seen it in action. Experienced leaders try to avoid deluding themselves at step 1, and work to prevent things transitioning from step 2 to step 3. But sometimes it's impossible (or I'm not experienced enough!). Some employees really do start off strong and then fade out, even if the incentive structures are \"good\" / \"above market\" / etc. Honestly, I've seen the roots of this in myself too, in classes, jobs, relationships: initial enthusiasm wanes and at some point it's time to make a different decision. Personally I prefer to move on instead of stagnate, but some people are perfectly happy to ride the suboptimal until a decision is made for them! reply solatic 3 hours agoprevAuthor clearly has no understanding of why startups move quickly. Startups have the same Legal, Procurement, Security, etc. needs and responsibilities as any large company or government agency - its just that these responsibilities are looked after by generalist founder-executives, not whole departments filled with specialists. Any \"innovation department\", if it ever wants to actually ship anything, still needs to get BigBureaucracy on board, which is where the vast majority of time gets eaten up to ship anything. Startups don't need buy-in and alignment from whole departments full of specialists, just the executive-founders with unrelated titles and experience who are still nonetheless nominally responsible for those areas. > what we just witnessed was leadership rewarding and perpetuating a dysfunctional and broken system. Leadership's first responsibility is to keep the system happy and running smoothly. The first responsibility is not to shareholders, not to attempting to seize potentially higher profits, but to the organization itself. The organization may be \"dysfunctional\" and \"broken\" but this is completely irrelevant - the Fortune 500 is still generating massive profits and the public institution is still nominally discharging its duties. This is why the vast majority of Fortune 500 CEOs are \"caretaker\" CEOs and why deep cultural change of public institutions is so difficult. Culture is fundamentally a question of who you hire, who you promote, and who you fire. Those decisions do not happen overnight in healthy organizations. That's why it's slow. reply smugglerFlynn 2 hours agoparentSome orgs of Fortune 500 caliber have innovation units that help to process ideas and changes. To be efficient, these units must have buy in on senior leadership level and high level of exec sponsorship, ideally they would also have C-level representation in a form of dedicated “innovation officer” or similar role. It is definitely not a new area, and author is on point with one key idea: organisations where heroes are praised as miracles generally don’t give a damn about continuous improvement, innovation units just never happen there. Praising individuals who swam against the flow for a year is pure virtue signalling in these orgs. In 2024 “running things smoothly” is almost a synonym for “continuous improvement”; building latter requires intent, not miracles. reply ghaff 3 hours agoprevThe basic idea here seems pretty sound. I remember years ago critiques about some Microsoft \"Heroes\" campaign along a similar line. While organizations often have superstars or whatever you want to call them, if you require them to at least minimally function you're probably doing something wrong. That shouldn't mean that you don't celebrate those superstars. They're not a bad thing certainly--which is probably where I differ with the post a bit. But understand that we shouldn't be depending on them all the time. reply lanstin 1 hour agoprevThe thing is there's a lot of dysfunctional organizations, and being an innovation hero is an easier and more fun career path than fixing dysfunctional organizations, which seems (as someone that only half-heartedly tries to keep insane decisions from the top from ruining things) to not necessarily be possible; there are often hidden agendas and people more interested in their cash out than the viability of the organization long term. reply MichaelRo 3 hours agoprev>> Why is it that innovations require heroics to occur in our organization? Why do we immediately assume that innovation = progress? Sure, the things that SURVIVE are useful, but that's just the tip of the iceberg. The vast majority of ideas are just like mutations in evolution more likely to be at best useless and probably damaging in various ways. You see, social constructs are not as dumb as they appear to be to the armchair intellectual. \"Why, we should embrace innovation and immediately adopt any idiocy that Mary from accounting is suggesting as our global company policy\". I assure you that by natural law, if \"random idea from random guy\" were profitable on average, we'd have a system that would encourage such ideas. The sad fact is that they aren't and will never be. Friction (named in the article as \"Dysfunctional Organization\") is an unfortunate but necessary process which ensures \"survival of the fittest\", even among \"innovation\". It's as simple as that. reply ahstilde 2 hours agoparentInnovation is required for progress. The alternative to innovation is stagnation. The end result of stagnation is death. reply lawlessone 3 hours agoprevThere was some anime/manga I recall from when I was younger that had the phrase \"heroes require bad things and/villians to exist.\" This reminds of that. reply t0bia_s 48 minutes agoprev- All large organizations – both government and corporate—need an innovation doctrine or else risk being outpaced by competitors. They will not, because competition to government is forbidden by law. They have monopoly and will to use force if their system is threatened. Innovation is not priority as it is in free market. reply nathan_compton 4 hours agoprevGod yes please give us an \"innovation doctrine.\" This is certainly an interesting article to read about but I'm not sure his suggestions or analysis are that substantive. Complex institutions develop rules to simplify decision making and streamline information flows. They choke otherwise. \"Develop an innovation doctrine\" isn't really effective advice. reply OldGuyInTheClub 2 hours agoparentI agree. I am in aerospace. When times are good, there's money to bring in speakers like this every couple of months to talk to our management. We get the slides afterward and they are refreshingly free of content. All of them essentially repeat the cliche, \"Think outside the box!\" We even got one of those Innovation Forums where people could propose ideas, get them upvoted, with the promise of tchotchkes at the end for success. Not a single one got funded. Every efficiency improvement, every streamlining suggestion affects someone's budget and headcount. When that person has to approve, nothing will happen. And, in this industry, a lot of rules are actually law. They have to be followed. But, the consultants and professional keynote speakers seem to be making good money off of it. reply Mathnerd314 3 hours agoparentprevThere are various articles on developing a culture of innovation, e.g. https://hbr.org/2019/01/the-hard-truth-about-innovative-cult.... Probably some books too. Even ChatGPT probably has decent advice. Management is not technically complex, it just requires putting in the work. But of course it is not easy, e.g. the first advice in the HBR article is to fire incompetent people, whereas the example here was government where firing incompetent people is notoriously hard. reply nathan_compton 1 hour agorootparent> chatGPT probably has decent advice If chatGPT has anything other than banal platitudes, I'll eat my hat. reply Mathnerd314 28 minutes agorootparentIt seems... practical? But not really rocking the boat. Just using Jira though seems like a big step for a government organization. I wouldn't eat my hat but I'd maybe nibble on it and play with the prompts more.https://chatgpt.com/share/13585d1b-a781-49fc-aa42-a7322b6f32... reply setgree 3 hours agoprevI favor the general point here, but the leading anecdote doesn’t really fit with the lessons learned. A government organization generally does not need an innovation doctrine to avoid being outcompeted because they are a monopoly provider. They maintain that monopoly through force. If you want to get a government agency to perform better, fix the incentives. reply betenoire 3 hours agoparentI immediately thought of the government. I interned for the transportation department. I was one of these \"innovation heros\". It was a budget thing as explained to me. If I didn't have work to do, they told me to do homework, I wasn't even allowed to help people on things outside of my department, since they'd have to report on that in budgeting details and be accountable of it to the taxpayer. reply toomuchtodo 3 hours agoparentprevIncentives matter, certainly, but you must suss out what those incentives are and what they need to be to arrive at the desired outcome. Look no further than the US Digital Service and 18F (within GSA). You are not paid top dollar, but you are put in front of meaningful work and enabled to deliver (although that in itself is an incentive for the practitioners recruited). From the bottom of the impact report I cite: \"We need you. Let’s help millions of people together.\" right above the Call to Action to apply. The USDS is enabled to succeed in this mission through the support of the Executive Office of the President, the equivalent of corporate executive sponsorship. Culture comes from the top. https://www.usds.gov/impact-report/2024/ https://www.usds.gov/impact-report/2024/by-the-numbers/ (full disclosure: went through a USDS interview cycle and was extended an offer, no other affiliation) reply merely-unlikely 3 hours agorootparent> The USDS is enabled to succeed in this mission through the support of the Executive Office of the President, the equivalent of corporate executive sponsorship. Culture comes from the top. That sentence is the essence of generic corpo-speak. Doesn't really mean anything without specifics. reply toomuchtodo 3 hours agorootparentSomeone has the authority, budget, competency, and stamina to make change happen. I spend quite a bit of my time speaking with execs, my apologies. reply MeetingsBrowser 3 hours agoparentprev> A government organization generally does not need an innovation doctrine to avoid being outcompeted because they face no meaningful competition Maybe true for some government organizations, but not true on the whole. Governments spend billions (and sometimes trillions) on innovation to compete with each other. Collectively, government programs probably account for 99.99% of all \"innovation\" spending. And they have the highest stakes when it comes to being \"outcompeted\". reply kurthr 3 hours agoparentprevWell, except when they do. Can you really imagine a military that didn't have and celebrate heroes? If a large organization can't have those that sacrifice themselves and break rules to achieve the greater goal, then they're unlikely to succeed against a significant foe. At the same time communication within large organizations is challenging and leadership is unlikely to know what the challenges at the front line are. Fostering all innovation is as likely to lead to regularly scheduled mediocre \"improvements\" and \"features\" that nobody really wants in order to meet whatever metric is in place (to the detriment of what is not explicitly measured). The argument of the article seems to be, just be so good and well directed by both senior and middle managers that exactly the \"right\" innovations occur within the process. That likely means those in the trenches aren't getting what they need. The most effective rapid innovation method I've seen (though painful and challenging to implement) is having separate teams competing to several performance milestones (which allow more generic goals and targeted metrics). At the milestones they share their results and innovations, which competing teams can then use/combine to compete against them. Management needs real goals with known tradeoffs, 2-3x more people than a single team, and it's stressful hitting deadlines knowing that failure is an option. The failure mode is putting all the \"best\" people on one team which is supposed to win, though I've seen even that get broken by a team of underdog \"heroes\" who embarrassed the chosen team, and luckily senior management rewarded that. It's similar to \"red\" vs \"blue\" pen-testing or wargaming, but you can have more than 2 teams and the goals can be aligned against the status quo (sometimes a tweaked current solution is the winner). reply tredigi 3 hours agoparentprevDoesn't the leading anecdote give an example of a (dis)incentive that needs fixing? That it takes 10 months and lots of head-banging and then you get a $100 bonus, that would certainly disincentivise me to do any type of innovation. reply setgree 3 hours agorootparentIt does, but the last lesson learned is: > All large organizations – both government and corporate—need an innovation doctrine or else risk being outpaced by competitors. I don’t think the anecdote fits conclusion. The reasons to innovate are many, but being outcompeted is not that salient to, say, an IT person in some agency. reply renewiltord 3 hours agorootparentprevPOSIWID implies that unchange is the desire of the government. People desire unchange and this is a tool they use to try to force it on everyone. reply groestl 2 hours agoprevLarge organizations don't improve by innovating. As the article mentions, they focus on sustaining the existing business model, anything that diverges from process potentially destroys the revenue source. Instead, they improve by acquisitions. Of innovative companies. reply kepair 48 minutes agoprevIn my company, awards and prizes always go to people that go \"above and beyond\" the processes the company sets in the first place. reply charles_f 2 hours agoprevI fully agree with the sentiment, as I immediately get discouraged of doing anything as soon as it requires the approval from someone in another organization, which in large companies will often take weeks, if it happens at all. There's an argument to be made for bureaucracy: once you reach a certain scale (of complexity and/or size), the law of large numbers makes that you're prone to more risks than when you're a startup. It's \"fine\" for a startup to not be aware of the latest development of privacy policies in Finland, and if you provision a couple resources in the cloud everything's fine because when looking at the bill, you can simply ask around \"who owns that thing?\". At enterprise size, you're much more prone to be audited by governments, to be attacked by hackers, to be the target of lawsuits, to have runaway costs that get hard to diagnose ; and the complexity of your systems (wet or hard) doesn't increase linearly. I don't think you can avoid having some processes to control that. The problem is when the enterprise gets into a vicious cycle in which the only fix to any process issue is to slap on more process rather than rethink the existing ones. Such as illustrated in that post, where the solution to a lack of innovation is the creation of innovation heroes to incentivize the behaviour. I've been in three enterprise organizations, two of them have policies where you need to ask for permission, the other grants permission by default but reviews what's been done. That second policy is much better, as the level was quite high, people tend to respect the rule, and get caught if they don't. That allows for faster work. But then there's also the matter of recruitment. As a startup you can handpick people, create a team that's functional and with an edge. In an enterprise, recruiting is done at an industrial scale, where your inbound fresh flesh exposes you to the simple truth that, by definition, half of humanity is below the mean. reply treflop 3 hours agoprevI honestly don't know where innovation exists that doesn't require \"fighting the system\" to some degree. There are a lot of bad ideas out there and the \"system\" is there to weed out the bad ideas. However, you can have too much system where it will defeat even the good ideas too, so there's a sweet spot. IMO it's comes down to how much leftover budget (time or money) the organization has. You don't need processes or procedures for innovation... you need people with enough leftover money and time who can \"screw around\" a little. Also a place where everyone is not grumpy. reply justin_oaks 1 hour agoparent> There are a lot of bad ideas out there and the \"system\" is there to weed out the bad ideas. However, you can have too much system where it will defeat even the good ideas too, so there's a sweet spot. Innovation means figuring out which are good and which are bad. It's hard to tell which is which from the beginning. Failure is inevitable. It's just whether there's a net benefit to the company instead of a net cost. The problem is that the people and processes resist ANY failure, which necessarily throws out any innovation along with it. > you need people with enough leftover money and time who can \"screw around\" a little And this is about right. There needs to be enough slack to not strangle ANY idea that can possibly fail. The author just went so far as to say that the \"leftover\" needed to be part of the budget. > Also a place where everyone is not grumpy. Quite right. We need a place where some people are optimistic about new ideas instead of rejecting everything. reply adverbly 3 hours agoprevTo be fair, creating a process for innovation is hard and expensive - especially if you want it to apply across the board at a large organization. I have been on multiple \"incubator\" development teams in the past and although we did move faster, my times at smaller startups still felt significantly more productive. You want something that is the right balance of bespoke and standardized/transferable. Its not easy, but there is so much room for improvement at these bigger organizations that there is huge value to getting it right. reply rednerrus 1 hour agoprevIf heroes are getting things done, it seems like it's possible. Wouldn't it make sense to study how the heroes are operating and try to make that part of the culture? reply bearjaws 3 hours agoprev> Why Innovation Heroes are a Sign of a Dysfunctional Organization Because often you can solve 99% of companies problems with boring software. I am reminded of this blog post from earlier this week: > Most organizations cannot ship the most basic applications imaginable with any consistency, and you're out here saying that the best way to remain competitive is to roll out experimental technology that is an order of magnitude more sophisticated than anything else your I.T department runs, https://ludic.mataroa.blog/blog/i-will-fucking-piledrive-you... reply graemep 3 hours agoparentUpdating a spreadsheet with data pulled from another system sounds more like \"boring software\" than \"experimental technology\". reply verisimilidude 2 hours agoparentprev\"Innovation\" in this context does not mean cutting-edge technology. It just means changing processes to deliver better results. The tech is often the easy part, and there's plenty of room for boring software. The hard part is navigating the bureaucracy and building consensus toward a change. This management-craft is where the clever thinking and emergent solutions are found and deployed. reply profsummergig 1 hour agoprev> Their organizations hemorrhage the very people they need to help them compete against aggressive adversaries or competitors who have them in their sights. Govt is a monopoly. So it can't be bothered. (The org. in charge of preventing monopolies is a monopoly.) reply throwawayqqq11 1 hour agoparentYou are comparing companies and states. Sure, there might be similar machanics in both middle management at play but upper management is very different, at least in a democracy. reply praptak 3 hours agoprevI could name some companies where you don't need to be an \"innovation hero\" to feel like the \"innovation hero\" from the anecdote and \"Please ask next quarter\" seems to be the motto :) reply rqtwteye 1 hour agoprevMy company’s mantra is “change and innovation are great as long as nothing changes”. reply Mathnerd314 3 hours agoprev> what we just witnessed was leadership rewarding and perpetuating a dysfunctional and broken system. It's not clear that it is dysfunctional. If innovation is not a particularly high priority, but risk reduction is, then the system is working as designed - all of the checks are necessary. Compare to the \"innovative\" Boeing-type company which streamlines production by removing all safety checks. reply hayley-patton 3 hours agoprev\"Unhappy the land that has no heroes!\" \"No. Unhappy the land that needs heroes.\" reply wonderwonder 2 hours agoprevA long time ago I burned out and took a few years of leave from tech and instead took a document processing / generation job at a very large non tech travel / leisure focused company. Company used an archaic document generation system that had to have templates built by hand. then there was a significant amount of manual work moving data from microsoft excel to word. All Manual. My group consisted of myself, 4 other document processors, 2 leads and a director. This team generated all of the sales documents for the company; thousands of templates. Within weeks I wrote some VBA macros that automated 75% of the process and they promoted me to manager a couple months later. What I found fascinating though was the group of existing document specialists were not suddenly capable of doing 2 - 3x the work. They just appeared to move slower. It was like pulling blood from a stone. They had no interest in learning new work which they were now free to do, they just wanted to clock in, turn their mind off for 8 hours, click the button and clock out. 40% of the staff at large companies are likely not needed and can be automated away. Kind of made me feel sad. reply drewcoo 1 hour agoparent> What I found fascinating though was the group of existing document specialists were not suddenly capable of doing 2 - 3x the work. They just appeared to move slower. It was like pulling blood from a stone. They had no interest in learning new work which they were now free to do, they just wanted to clock in, turn their mind off for 8 hours, click the button and clock out. The ability to barely function and slowly grind was what made them good at the job. Either they were hired for those \"aptitudes\" or they developed them on the job. what did you expect? reply chuckadams 3 hours agoprev\"Innovation Doctrine\" sounds like something you put next to your Mission Statement. How about fostering an internal discussion board where employees can pitch ideas and get hooked up with others who know how to implement them? If you need guard rails around the anarchy, then you can tie action items to a ticket tracking system that's readable by the whole company. I can file a bug in JIRA against any product my company makes, why not the company itself? reply MeetingsBrowser 3 hours agoparentBecause in large companies, any helpful suggestions get lost in the noise. Everyone has opinions on how things could be done better, but very few of them are good. reply justin_oaks 1 hour agorootparentI remember a company I worked at that an internal website for suggesting ideas. The problem is that the management never looked at it. And because of that, employees stopped posting to it. And that was that. reply chuckadams 3 hours agorootparentprevThe discussion forum still has to be managed (I said \"anarchy\" facetiously) and at least minimal standards of professionalism would still have to apply. And if the board still devolves into a swamp of griping and bickering, then well, they can nuke it and at least say they tried. Most ideas are indeed crap, and the good ones have to be picked out. But reducing the total number of ideas doesn't raise the percentage of good ones. reply agentultra 2 hours agoprev“Engineering is doing well with $1 what another person could figure out with $2,” I heard from somewhere. This often involves, in larger groups, communicating with folks and taking the time to understand the requirements. Although I despise “5 whys,” almost as much as scrum; it does often boil down to dysfunctional organizations. It saddens me when a highly productive development team gets pulled down by non-productive bureaucratic middle-management and politics. There often is a class of worker that doesn’t want to do meaningful work and sees the value generated by other people as an opportunity to take a ride to the top. There should be a few check points in an organization where one needs to ask for permission but going too far down that road puts a ceiling on how productive any one group can be. Good article; definitely need to remove barriers. reply BenFranklin100 3 hours agoprevLack of innovation is of course a problem in industry, but it’s a particular problem in the public sector where many employees are attracted by the “can’t get fired” rather than the “let’s make things better” nature of the job. reply Scubabear68 2 hours agoprevI am currently working with a very large company that has this problem. They are highly risk adverse and are happy to pay $100 up front to avoid a $1 accidental loss. Of course, all the processes don’t really protect them, they just get slowed down by a factor of 100 or more. The successful people take on as many simultaneous projects as they can, because any one project will move at a snails pace. I have told them tales of working on some internal document processing and classification systems in a fintech a few years ago where we would release to prod multiple times a day to test out different algorithms and approaches. They flat out think I am lying. reply JohnMakin 2 hours agoprevThis is true, but I am unsure of the fix prescribed here. For me, I had experience with a project like this - a large, extremely bureaucratic company tasked my team with a nearly impossible task, I think in an effort to lay off the team - which eventually happened. I fought with tooth and nails and was already doing fairly well in my role, so they gave me a shot and assigned the task, which I won't belabor the details of, but it was basically to implement a fundamental service that every application in the company would use to authenticate to the backend system (plus a lot of other reliability/availability guarantees). The problem was, with their architecture being a central hub cluster of servers that provided core services to hundreds of edge servers around the world, it would have been fine to just throw this service into the central server and call it a day. However, the eggheads in executive leadership felt this was not acceptable, so the requirement was to make a new \"central\" cluster to connect to not only all the edge clusters in the company, but all the backend office/admin stuff as well. Problem was - the people who wired up the networking and everything else with the central server had been gone for 10+ years. Barely anyone even knew how it worked, and unfortunately as I took this on, the network team got laid off, replaced by people who also had no clue how it worked. As we connected more and more services to this new hub, a bunch of skeletons emerged - the funniest being a server cluster in a region and account that no one had access to, or knew what it did, other than when it crashed other servers had issues. No one had accessed it for years. That was fun hacking into. There were tons of hurdles like this at every step, basically being that this touched so many teams in so many different areas of the infrastructure, the organizational hurdles trying to even get the information you need to do the job required a ridiculous amount of heroics. I hated it. Basically, you need to create urgency any way you can - whether this is by breaking things, horse trading, political maneuvers, begging, intimidation - your livelihood is on the line. Other teams sensed what a difficult ask this was so early going was extremely difficult getting cooperation from the 30+ teams this touched. Anyway at the end of the day I was able to do it by finally escalating urgency to the executive level and they made it a priority one quarter and it quickly got finished. If they hadn't, it would have been very difficult. They had the issue of having too many layers of management below them and had no idea, all they heard is \"why is this project you said would take 2 months taking almost 2 years\" and a bunch of manager speak trying to explain why. I would never, ever want to work on any project like that again. 2 years to complete, should have taken 2 months if working as an IC, 2 weeks with a full competent team and good management. There were some good things that came out of it though - processes got improved, collaboration improved, and we were able to use it as a chance to refactor the IAC in a way you could deploy these hub servers again in a much easier way that didn't require the ridiculous amount of detective work to figure out the first time. Oh yea, forgot to mention the actual application took about 2 days to configure. All the rest of it was what took 2 years. reply drewcoo 1 hour agoprevHeroics are responses to crises. Why are there crises? reply adolph 3 hours agoprevLacking a method to implement continuous improvement is the dysfunction. “When [W. Edwards Deming] came to spread the gospel of continuous improvement in 1950, he was preaching to the choir. Toyota already believed in it. [Deming] simply gave them a process to better understand how to progress from failure. The idea of ever-improving coupled with what they learned from Deming—especially the Theory of Knowledge and shorter feedback loops via the PDSA loop, as well as the Theory of Variation and the accompanying statistical process control—let them succeed in their failure.” From “Deming’s Journey” by John Willis. Solid recommended new read. https://www.amazon.com/Demings-Journey-Profound-Knowledge-In... reply didgetmaster 3 hours agoprev [–] The problem with this particular anecdote is that 'innovation' and 'government agency' are used in the same sentence. Most people do not equate any government agency with an innovative environment. Bureaucracy is the watchword where the status quo is not only encouraged but fiercely protected. Any new hire or politician who threatens to reform the process is treated as the enemy. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A finance team doubled their expected work but faced criticism for disrupting burn-down charts, leading to the creation of placeholder tickets to game the system.",
      "This scenario highlights common issues in large institutions where metrics and bureaucracy can overshadow actual productivity.",
      "Effective innovation requires trust, communication, and a supportive culture, rather than relying solely on individual heroics."
    ],
    "points": 192,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1718978594
  },
  {
    "id": 40750391,
    "title": "OpenAI Acquires Rockset",
    "originLink": "https://openai.com/index/openai-acquires-rockset/",
    "originBody": "body{font-family:Arial,Helvetica,sans-serif}.container{align-items:center;display:flex;flex-direction:column;gap:2rem;height:100%;justify-content:center;width:100%}@keyframes enlarge-appear{0%{opacity:0;transform:scale(75%) rotate(-90deg)}to{opacity:1;transform:scale(100%) rotate(0deg)}}.logo{color:#8e8ea0}.scale-appear{animation:enlarge-appear .4s ease-out}@media (min-width:768px){.scale-appear{height:48px;width:48px}}.data:empty{display:none}.data{border-radius:5px;color:#8e8ea0;text-align:center}@media (prefers-color-scheme:dark){body{background-color:#343541}.logo{color:#acacbe}}Please turn JavaScript on and reload the page.Please enable Cookies and reload the page.(function(){window._cf_chl_opt={cvId: '3',cZone: \"openai.com\",cType: 'managed',cNounce: '63246',cRay: '8976400fad042ce9',cHash: '611a23485b2825c',cUPMDTk: \"\\/index\\/openai-acquires-rockset\\/?__cf_chl_tk=kw48UIMmLZrkeEbhTNKcUkioB9AAr_9WHfJO5FqVQNQ-1718996518-0.0.1.1-3988\",cFPWv: 'b',cTTimeMs: '1000',cMTimeMs: '390000',cTplV: 1,cTplB: 'cf',cK: \"visitor-time\",fa: \"\\/index\\/openai-acquires-rockset\\/?__cf_chl_f_tk=kw48UIMmLZrkeEbhTNKcUkioB9AAr_9WHfJO5FqVQNQ-1718996518-0.0.1.1-3988\",md: \"wQ2iQqEun4QQl.VYuZzr8zrp6duN.D4Cy8AJuhbUPO4-1718996518-1.1.1.1-moNg3QkQUZF.wdYiFPnVfaX4qMgHfua7.J8L3RYLe4a6088UDhsmgMuH22H09Bzd4G9i4IBAkqbuMIH5ScQ.zQpgfhJ97mn83yxksz4xt41G8n8L_ox8yOpfYSgMx00az6xEVN1h.EmF_Rvl3J5aXfHWxDl44p3hJgnpYbq9Am4yVTbVebvonucboqj0aNoF5BIFPaIujH.otyrI2t7r1SE4UTSuK1Z7L7otkoB5Wr7IABDW9kuHv62jdQ3KKWhwOghH69cp7JyJr.3LnZHQHCEx9xx2npznRgQggWedU1ywFR3yKLx0wqAwQymFd_GddPcuVLweEfBSlIFPIhcdz29jHgHQSVQfplUPiyVax6Hc0_i3PYKlkuSF3fO2r.qr3Jmp_Q5ZYDqg6BIpRm3m5sg2yDbAFnmaUlJS8HoX8N3rmS_l69cTo6F92DSOsx7N3To5se0wIIh6X8X_L_9vcf4gS5OjcgYt38mmeuql5sc2El673ksNKJG_xXFHjXZz04MlGOvjg4ouLAgdyexLH8p4MC8j5feRO1ox3bEreH.nIsS3s643yeyCpxR8jopzxIO0LSRNYKfNXOSI0rFoP6OvgqdObF6Tes.WbQQDOhOA8n7tjIRTJtiJ49wYFgMYevX8DX2SzhvxAKaUU_bSKWOCJf6GH0GDYXUDqTxDnFmN7bH1OueewOvpUUImMIV6kCK6AEcRKLV037Bnv6W94ShINC3hRIZz7thR_HHI3FhQWsFonYGNylLzLA5hfUEGXljS.p4UTfbhdxadzi8Z231sWi577wKYUSSwglzW9Gmw9J0EWwzQwNjWqoiVpgGGt_mnHXQuLN6_TYUAbhPzcFgb4gfUyK.5ugNNadhuSfpnp3SUsS6.XITh_UJMT0goMUaNLV3G1Z.a3bmsrvr3rOE_kNmWXK5NiwcZxClmS6f.iP8DJhiVb2wWOSp9vzAiPcIlhK2HS3gCcxLoxglIYrUcnUhq8ZEPmXSwH.R0v640pfpywjaHPTO45d8Yh8zPWASkDncgbsrRs7YzqKqhT1cdICRtual5bWAZ5aiNCOHsUhlLgj9.i2rN03WjJGvhXn.PvnrfaZP9Ctg_VIOHhMCxZwDjxKKSA7dVGjmFVOmpJBrCdmH8kiZM04LvHpVHZ6I1wkXT1GrQIaAt1VOUBZOSi5RDwGTFylYVqnr.F9jk4rGR6e_k3PS9iP4SwDUYmq2prnwKPvHFyctxYr1O4tWQ4vKb7UPAef8JkXgwJZSQBTExUgW156FwALlbAoivQQXxJTEb4fYJG_Zo.Ot5KIR4qvUZSWfpOTzk3vNeW8Zg54m4CrgU0Jq5Ag7LtCDM9SN9tHqfbSh74fYQNW35gfXMdkxt4kJlpjoJafDPk5R5QRZ2NKUUhGOi1mOhWX6IAw20FjtubrioGdFOA9IbE1fBl2shzhwPfFYrad65PzN3ChRhVX6JDpWtqjbe0mXARG6FfXoW3apgdi1mRPpWfjcPveJlS2Ubusyamx2tYr1OLg7e6u5NsYs_xwVNiFrgxJjVZ8FLMrqt.N9emT51gkeZzLgXEoF2YkmZHOmk1Y0\",mdrd: \"va0SBDNJQNiePR_En0.7VXpzvh9xwTIe5IpoBe5lQ54-1718996518-1.1.1.1-CqdpYWEN7N9M05W.j8.feGJABpbCEeQ22PtE.Dmj3j4D01PbNAqIF4BQ.mQuyDAmd1kUmcKGuEAwkXXUYwjmoL5NOL6vSlOsDBzxYrCaSXL544BZaeSARuz50HaQe21Xx0qxMRhoyz5AyP7nGyfp4Bq440Eork0Tjt_AGtmRvELTd8XslA6fdo4DDRR3.U0yJgqdaTw3DoavgZjgTbCaV.m_YZd6v1FBGXDQBHIvjDXGyrSq.Wv.i0wEaiGfL.qMwBPFRPSXbsqhqEwg73JET7ikI2rS6JN0Uc7PHkjGhaz0WSz662M7fVrKFZ3hDHZCy7t5R6G8Bbaia1YpK1BYDytKvWr8WUpiuHWhe9PNtDpA58PAeECJE4_vksR5oAzmUPg6SNoAFLxuswKxVN0p8b1o2WeZafXypc2EQ_kFD0o10bBUscVWYd2sKWJlx8oc3qnIhfQzdK4Fv0Gn6t7kJ8yR3AaQqyOyae.SVAeVjaB6E5_CD_mdGlEQ00TEk8vnutP0iYHvj_4sLmrH5Vg_e651FCQ0VBEzM4eoUWZFKg8QYpOWhwcfGt67OXrOn3fKG.K8iUzDjv3xMd3ucamZ3IdWakLPD.IbKIzkbc9dV9asSEsRnasGB.Ulh9HdjkwMOsIfJnYQWGG_GfamkgG_di44J9sz928aWqkYQBlMPG3RI5kG1pW7fJV_djxvRQk9RkLNUnmxT2z17smsf9DSrxlf.sXwLf6lfdzMu7rsqWqVCt5q7GAecLREQEacbRFpjl7GyZ_ISLYu4DwnIMWKPkLbjF8owJds7Dmb92tDVZg.dJVRBUURX285yWHIRBA2He7fH0B1K3oksuar3bmn3IUK4kcQP2CJgc.CJTK3b5H5EQf7TCx0sAjpUdHfwV.Lip_pMW2WnPxT9L5WxGDahQGyAoS89qJdKRfpq1Y1em9WQ8mbpoHcipODEeaCSfYaLPmVlneY6zzzKYm7dIeRd97RFZ4IKWwh.ne1UoajcvLDwnf2I4aahb1wPheHXtOvtROk7dTHpRIQ8TXdhpjNqUOrTRz1sy6t7ytyGaCekF4SD_4rvXBN85i6aREfKlR2PM8gowRjMhWZTiArVURSqhxiAkSWv_EAuu86fUwEK80WoSIdEXHOCIqJK46BgiQC..D7cqXi6j0edVsk5FGpOiBtvwZYY_tRYu69Y4.2VU0OamESLw1mEHd7JsM2FpVN2u76lbRvfniowJ5MBZgg1Xghfp065rtqaVwlyJ_W7knimBxdSMcOh_2oOxyRPtpk3LMy20p_rudsyuznTf_ciObNZfV_aww9HJI6gjrSvl6XCLEQqmpUDaD_DZtSRFST9SexEyxAJ3TbbYoYQ9XCdcw6dm4ula7oXTfAPmWjMwPIXL4BAeA4fIOV0FTFeRyaTB8mfZ0zlY13oZlgZBC3N_SGEwyQQrftSD9UU1KnQiYMFdjaH6TO6MEYy9PINVj6qsf6j6ljtIViO3hcVVp3LHUtEtH7uKb6_Ehx4i1Zgaw_.eKJlwo2To.cvCgND4Dr6zpm.hKLK2lQ5teSL3_xg4hlsnMhtJ.ahXxIGBz3Pe42_alIhoRzJQ6.znLHfr.jpDN8vmkuDocBjuPRzdHLiUjp2mpRzX22ocqdR5ZTMQFE5u5T5lreK3KTKnQoEh0AifHWkKDm3qc0Qo4LHs3c4ZedcGKEReAqtSHDhhrPWNsCL24QPyQ9vbopF5KewSYLvJB0S1LqDn3ThBJ15Pu_OKhqKRrnZXAoRhLyJ3TW_7EhbvnuyHqIuzJETjLlDZhqMUDwfdh4K0JNiwyP8lSwwcR2v1wodVqNuOfdqyg.qYIwxNqkNP_DUx3jX8_Qa0kOHKYrSy5oaBr4riioGPetN.pHbtTmoR1Y.QfgQASS1qgqyc9a2GqsIkbNjhh7aQsIItWOpgNJx47DYRWWtm631j2I4dsAZN_X9Mi1dsL1xqQu9QFNzHbSK_LzY7Meq5voRmUF9DTlNnpliixyARUjGL6aMx4zMzf_hTkhlmLKyIHlfnNzrYyT72DgfmQ1ajADKJ57BouVZXqmlO5IthGVPIy1gFrguXiN8RDz4OHHtHchkPWGWLFn_L5LksLGXjjzQ8OolX6ILewvzO63XTKj6vT2h9FgmHk5q2RRCEkruZG.WuQK7WEuc1bBcQxgXRsHtUIkPZ0QkhF4zOWU0Gy.l9rG8cjzGtB.aZFvu.Wpz6WWU28lVwWPaMmBvbKkE_ZgVIwo7iudzc85qoqGUFCjBBSga0lO.eF95eeSLpnie_M\",cRq: {ru: 'aHR0cHM6Ly9vcGVuYWkuY29tL2luZGV4L29wZW5haS1hY3F1aXJlcy1yb2Nrc2V0Lw==',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',rm: 'R0VU',d: 'PVUbklsPK0VqabD8ZiMA7hOot1ig8cQLiQ/Sz4HXmw+kOVlAaYIVrJPNZ0tXXviNVbAjWtZDfkV+N4GVg5n9VjaKcJ9iqjfHc+Pxltxxx8IALMqgGNJZb9eI+oS02T8lhOlFBLH7WZnpkXjp8AKVoX0tVhoaLmbiByVDR35SuyVMe+OcNtEXZLXr4Hp0l31zZ2SQwq4pHcR/MhasiAPYelq9U/1Rn/AiB8o9A/hkvSdV9UIhEQ2KehuxSmHS7A9WzUSv0DHy/pjhwBM8irD5YOsxJE3G8NhL4VssqS3+t3eUqDX0kJqdJnA2NL45L0W6JLQzr0pVdnE37h0KohmI7d8OgWLr9iWtR9xwmm+EHAcDawtjKXWOUN+f8A/nSm9RRScU3Hj71ozcIaO223BGOd5M7mBlhL2LlJxDKIPu89M8Rq/1cY3Ga3lCgMjOa+AC3N66LEeGJ6Y+7t2TD2lBK7qLyDbXe9vXi5aUSsJYl9LvmbIwQArNgG2aPS8+aaj9RLJYiZaoORSewnGTEg9+iUGJNXoG3cdXrqrK4tatICdNPimbDg3S2tA6Ou/OC1eF',t: 'MTcxODk5NjUxOC4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'mCXS7V/OZJvx/tlqBEFJ2yT7dk/763LhB8cF86xogko=',i1: 'ZQBM12C9tJUVoMzv15+dNQ==',i2: 'b5SWzVBUKAGtBSGj1eVjDA==',zh: 'ULytyqbUhvezGEhuw7JA3nyKKR78rFU1sFNg5+21X6c=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: '6DRP17hjzkUmXKzJAms7bA4OqyZ1RY8MCcH+VDleInA=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8976400fad042ce9';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/index\\/openai-acquires-rockset\\/?__cf_chl_rt_tk=kw48UIMmLZrkeEbhTNKcUkioB9AAr_9WHfJO5FqVQNQ-1718996518-0.0.1.1-3988\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());!function(){var e=document.createElement(\"iframe\");function n(){var n=e.contentDocument||e.contentWindow.document;if(n){var t=n.createElement(\"script\");t.nonce=\"\",t.innerHTML=\"window['__CF$cv$params']={r:'792f8224776acf9f',m:'hMcSCCrnIkr7c8Pec6Na6boaaFAnQ6S0ypG2GKRbKgc-1675305063-0-AaJn0SqKZQnadmRQ5O1dM9xMkXWyP+ll7gpl2NHeoNbZTEXMjlB10KkwnEU3hf0/gMODfKqcBGLVecql6U04GGs+iJ/kNrNqj1FgfAOlQV+T2koMQMvUy1zr9tegBBX6BikfccHZhwoJhnXc0eTcg58=',s:[0x60b082f691,0xee65a67e11],u:'/cdn-cgi/challenge-platform/h/b'};var now=Date.now()/1000,offset=14400,ts=''+(Math.floor(now)-Math.floor(now%offset)),_cpo=document.createElement('script');_cpo.nonce='',_cpo.src='/cdn-cgi/challenge-platform/h/b/scripts/alpha/invisible.js?ts='+ts,document.getElementsByTagName('head')[0].appendChild(_cpo);\",n.getElementsByTagName(\"head\")[0].appendChild(t)}}if(e.height=1,e.width=1,e.style.position=\"absolute\",e.style.top=0,e.style.left=0,e.style.border=\"none\",e.style.visibility=\"hidden\",document.body.appendChild(e),\"loading\"!==document.readyState)n();else if(window.addEventListener)document.addEventListener(\"DOMContentLoaded\",n);else{var t=document.onreadystatechange||function(){};document.onreadystatechange=function(e){t(e),\"loading\"!==document.readyState&&(document.onreadystatechange=t,n())}}}();",
    "commentLink": "https://news.ycombinator.com/item?id=40750391",
    "commentBody": "OpenAI Acquires Rockset (openai.com)157 points by colesantiago 3 hours agohidepastfavorite40 comments ot 2 hours agoVery unexpected acquisition. I don't think that Rockset is a suitable infrastructure for RAG, a purpose-built inverted index would be far more efficient (both in terms of compute and storage), so I'm not sure how much of the technology would actually be useful for them. I can think of two options - Pure acqui-hire: virtually all of Rockset engineering leadership is ex-Meta, and OpenAI has been hiring several senior infra engineers from Meta, so these are all people that have worked together previously. - OpenAI is building some product where customers can ingest large amounts of data, which could be managed by the Rockset infrastructure as source of truth, and then indexed by their RAG systems. reply tudorb 2 hours agoparentGiuseppe! Long time no see. Rockset’s architecture changed somewhat since we last talked— not in fundamental ways, but in ways that would alleviate your concerns. If you want to talk (not secret) technical details, you know where to find me :) -Tudor. reply ot 2 hours agorootparentI guess I stand corrected then :) (Hi!) EDIT: I forgot to say, with the recent hires and the Rockset team, OpenAI is building quite the infra dream team :) reply hipadev23 2 hours agoparentprevOpenAI has billions of dollars and nothing but GPUs to spend it on. This isn’t strategic per-se, it’s just rollup. Good place to be in for any data-adjacent product company. Google and Amazon followed the same strategy for over a decade just buying anything that was possibly helpful. reply ot 1 hour agorootparentI would speculate that OpenAI is in a phase where speed of delivery is make-or-break, and any bloat would be a distraction. I bet they're extremely deliberate in their acquisitions. reply jshx 9 minutes agorootparentWhen rate of change increases (with different accelerating rates in different dimensions) what delibration chimps with 3 inch brains do does not matter. Even the explaintory stories cant be manufactured fast enpugh to keep pace. Such a state is called The Anthill. reply simonw 1 hour agoparentprevRAG doesn't have to involve vector search. The (very thin) blog post said \"Enhancing our retrieval infrastructure\" - my guess is this is more about other forms of retrieval, like constructing and executing SQL queries and using the results to help answer questions. reply zurfer 17 minutes agorootparentLast time I heard of Rockset was at the Snowflake Summit where they positioned as a faster DWH. Looking at the landing page now it seems they almost pivoted into semi/unstructed data. To your point, I feel like nobody knows exactly how to do RAG really well (fast and accurate). I also doubt the Rockset team has it figured out but it seems like there is an opportunity to build a new kind of database/memory system and OpenAI believes the Rockset team can help. reply chatmasta 1 hour agoparentprevDoes OpenAI use Rockset internally? I feel like I have some vague memory about that… in which case, the acquisition would make sense from a continuity of business perspective. reply Merick 27 minutes agoprevI'll always remember Rockset for their ridiculous comparison page: https://rockset.com/real-time-analytics-comparison/ Maybe they should rename it to their migration options page. Or maybe I'll just ask ChatGPT what the best alternative is... Still, pretty useful stuff, but it also feels like Rockset had been moving a little too slowly in recent years, but congrats to them on finding a new home. reply tylerhannan 1 hour agoprevA database is core to your infrastructure...finding out your database is going away is a horrifying situation. Finding out that the time you have to migrate is a few months. Agh. As others will say, there are options. Rockset helpfully posts links to a bunch of comparisons on their website, and these alternatives include ClickHouse, Elasticsearch, Druid, etc.. https://rockset.com/real-time-analytics-comparison/ I'm inherently biased (as a member of the ClickHouse team). But do check ClickHouse out. You can always come hang out in our Slack (clickhouse.com/slack) and, of course, the combination of hosted ClickHouse (clickhouse.com/cloud) and the open-source (github.com/clickhouse) may add a bit of comfort when your vendor up and disappears via acquisition. reply rockyroad 3 hours agoprevhttps://docs.rockset.com/documentation/docs/faq Rockset is off boarding existing customers. Definitely sucks we spent the last 3 months adopting it. We used it to replicate dynamodb in near real time for adhoc & reporting queries. Schemaless architecture was very easy to work with reply noufalibrahim 2 hours agoparentIdeally, success of customers should mean success of company but the incentives are wildly misaligned here. It seems perfectly \"okay\" from the company perspective since they are in it to make money but doing that by telling their customers to \"leave because we got acquired and no longer care about what we sold you last week\" is really harsh. reply Maxatar 1 hour agorootparentIf you're a corporate customer, then you take that risk when you sign up for a month to month contract. Now usually a month to month works very well for you, but it is a risk you accept. A lot of corporate customers will seek longer term contracts, a year or even longer, so they can lock in a price and various service guarantees. Even in the case of this acquisition it's only customers on a month to month plan that have to migrate by September, customers on a long term contract will continue to have access and support for the duration of their contract. reply CapcomGo 56 minutes agorootparentSure but all of their contract customers still need to find a replacement reply PeterCorless 2 hours agoparentprevYikes! 30 Sep cut off? That's not a lot of lead time, given the amount of work database systems require to data model, benchmark and migrate. My apologies if this seems inappropriate but given the urgency, my employer, StarTree, has a free tier if anyone needs to try out alternate solutions. https://startree.ai/saas-signup I understand Rockset-to-StarTree (Apache Pinot) is not a 1:1 drop-in replacement. But hopefully it's a port in a storm. Whether you end up on StarTree or another suitable alternate, I hope everyone has as painless a migration as possible. Reminds me a bit of how FoundationDB customers found themselves without a home when Apple acquired them back in 2015[0]. [0] https://news.ycombinator.com/item?id=9259986 reply biggestdummy 1 hour agorootparentHi PeterCorless! (We're friends IRL - it's Greg) While we're putting in plugs for open source alternatives, I'll recommend looking at StarRocks. https://www.starrocks.io/ I share Peter's sentiment for wishing everyone an easy transition, whatever you choose. reply mythticquest006 17 minutes agorootparentJust adding to what Greg mentioned: if you want to learn more about StarRocks or have any questions, feel free to reach out to us in the StarRocks community on Slack: https://try.starrocks.com/join-starrocks-on-slack. reply Merick 39 minutes agorootparentprevSeconding the StarRocks project, best performance out there and the community is great. Tons of support. reply lopkeny12ko 29 minutes agoparentprevDon't these M&As need to be cleared by regulators? Seems premature to tell your customers you're discontinuing your product before the acquisition has actually cleared (and, for that matter, passed OpenAI's due diligence). reply preetamjinka 6 minutes agorootparentAccording to [0] the acquisition has already been completed. [0] https://rockset.com/blog/openai-acquires-rockset/ reply ram_rar 2 hours agoprev> How long will service remain available? > Month-to-month customers without an active contract will have until Monday, > September 30th, 2024, 5 PM PDT to off-board. I'd love to hear from someone with expertise in vendor onboarding and business continuity risk: how do vendor contracts typically protect customers in situations like this? I'm sure will be super frustrated with datastore vendor change, which would need nontrivial resources from product development to system migration in such a short span of time. reply JumpCrisscross 2 hours agoparent> how do vendor contracts typically protect customers in situations like this? That's in the termination clause. reply borski 2 hours agorootparentThere is typically a change of control clause too. reply codezero 3 hours agoprevReally surprised to hear that they will be shutting down the SaaS business and all existing customers will need to offboard by the end of September. Quite a few of my customers build on top of Rockset and it won’t be a smooth transition. reply idrathernot 15 minutes agoprevNot a good look for OpenAI. Shows a lack of confidence in their internal prospects to push the needle if they’re already considering inorganic growth alternatives. reply zX41ZdbW 1 hour agoprevI have tested Rockset for competitive analysis. Good parts: It has a slick and nice-looking UI. Good documentation. Many data loading options (including S3). SQL support is good (Calcite?). Types are inferred on data loading. But you have to choose one \"timestamp\" column. Bad parts: First data load attempts failed (after 24 hours, it showed something like \"too many retries\"). I've loaded around 500 million rows, and the storage limit ran out. Query performance did not shine. Storage size was very large (it seems they create many indices automatically). Considerations: The technology is not open-source. It is rocksdb + secondary indices + object storage + SQL engine. reply netvarun 2 hours agoprevCongrats to the team. IIRC their CTO was the creator of RocksDB. reply usrnm 2 hours agoparentRocksDB is a fork of LevelDB created by Jeffrey Dean and Sanjay Ghemawat at Google. reply flakiness 1 hour agorootparentLevelDB was like their hobby project and was built mostly for Chrome's Indexed DB. RocksDB brought it to a much higher level with a lot of dedication. reply pr337h4m 2 hours agoprevLooks like we'll finally be able to search our past ChatGPT convos without having to Ctrl-F the data export reply freedomben 1 hour agoprevAI is a difficult space to be a customer in. All customers/investors/etc want you to add \"AI\" to your products, but for the majority of people that means using a vendor, and the churn in the space is shocking. It's this point where my gratitude for Llama and Meta is extremely high. reply softwaredoug 3 hours agoprevIs this a death knell to many of the vector DBs pushing RAG solutions right now? (Or maybe it’s validation in RAG and these companies should rejoice) reply PeterCorless 1 hour agoparent\"RAG\" is more of a concept than a specification. So the Cambrian explosion of how to actually do it will continue unabated. Likewise, I don't think it's going to stem the tide of adding vector indexes and similarity search techniques to traditional databases. Instead, if anything, I think this is a validation that traditional databases aren't going anywhere — OLAP or OLTP. Behind all the LLM models you're still going to need true, authoritative data in databases to avoid (or at least minimize) the hallucination problem. AI needs, if anything, even more programmatic ways to get at that data. reply posix_monad 2 hours agoparentprevNah, that was Postgres vector extensions reply nextworddev 2 hours agoparentprevYour workload is my opportunity - OpenAI, probably reply xyst 1 hour agoprevI wonder how much money OpenAI dangled in front of Rockset C-level execs and board to agree to acquisition. Seems company was founded in 2016 (8 years ago) [1] With investment from vulture capitalists to the tune of $117M. [2] I would assume they want a sizeable return on investment, so maybe a $250-350M cash deal? Doesn’t seem like this would be a unicorn, but it’s a payoff. Certainly will cover the losses from a few bad investments. [1] https://venturebeat.com/ai/openai-acquires-rockset-to-streng... [2] https://www.crunchbase.com/organization/rockset/company_fina... reply bbor 28 minutes agoprevCan we all agree that OpenAI should be banned from any kinds of corporate acquisitions? Ditto for Microsoft/Google/Meta, obviously reply davedx 21 minutes agoparentNo, why on earth? reply neilv 2 hours agoprev [–] > [flagged] [dead] OpenAI Acquires Rockset (openai.com) I vouched for this because it seems relevant, and I saw no reason in the comments to flag it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "OpenAI has acquired Rockset, leading to speculation about the strategic reasons behind the move, such as enhancing data infrastructure or acquiring talent from Rockset's leadership with Meta backgrounds.",
      "Concerns have been raised regarding Rockset's fit for OpenAI's needs and the impact on existing Rockset customers, who need to transition by September 2024.",
      "The acquisition has sparked debates on vendor reliability and its broader implications for the AI and database industries."
    ],
    "points": 157,
    "commentCount": 40,
    "retryCount": 0,
    "time": 1718982292
  },
  {
    "id": 40744293,
    "title": "Local voice assistant using Ollama, transformers and Coqui TTS toolkit",
    "originLink": "https://github.com/mezbaul-h/june",
    "originBody": "june-va Local Voice Chatbot: Ollama + HF Transformers + Coqui TTS Toolkit OVERVIEW june-va is a local voice chatbot that combines the power of Ollama (for language model capabilities), Hugging Face Transformers (for speech recognition), and the Coqui TTS Toolkit (for text-to-speech synthesis). It provides a flexible, privacy-focused solution for voice-assisted interactions on your local machine, ensuring that no data is sent to external servers. Interaction Modes Text Input/Output: Provide text inputs to the assistant and receive text responses. Voice Input/Text Output: Use your microphone to give voice inputs, and receive text responses from the assistant. Text Input/Audio Output: Provide text inputs and receive both text and synthesised audio responses from the assistant. Voice Input/Audio Output (Default): Use your microphone for voice inputs, and receive responses in both text and synthesised audio form. INSTALLATION Pre-requisites Ollama Python 3.10+ (with pip) You will also need the following native package installed on your machine: apt install portaudio19-dev # requirement for PyAudio From Source To install directly from source: git clone https://github.com/mezbaul-h/june.git cd june pip install . USAGE Pull the language model (default is llama3:8b-instruct-q4_0) with Ollama first, if you haven't already: ollama pull llama3:8b-instruct-q4_0 Next, run the program (with default configuration): june-va This will use llama3:8b-instruct-q4_0 for LLM capabilities, openai/whisper-small.en for speech recognition, and tts_models/en/ljspeech/glow-tts for audio synthesis. You can also customize behaviour of the program with a json configuration file: june-va --config path/to/config.json ⚠ The configuration file is optional. To learn more about the structure of the config file, see the Configuration section. ⚠ Regarding Voice Input After seeing the Listening for sound... message, you can speak directly into the microphone. Unlike typical voice assistants, there's no wake command required. Simply start speaking, and the tool will automatically detect and process your voice input. Once you finish speaking, maintain silence for 3 seconds to allow the assistant to process your voice input. Voice Conversion Many of the models (e.g., tts_models/multilingual/multi-dataset/xtts_v2) supported by Coqui's TTS Toolkit support voice cloning. You can use your own speaker profile with a small audio clip (approximately 1 minute for most models). Once you have the clip, you can instruct the assistant to use it with a custom configuration like the following: { \"tts\": { \"model\": \"tts_models/multilingual/multi-dataset/xtts_v2\", \"generation_args\": { \"language\": \"en\", \"speaker_wav\": \"/path/to/your/target/voice.wav\" } } } CONFIGURATION The application can be customised using a configuration file. The config file must be a JSON file. The default configuration is as follows: { \"llm\": { \"disable_chat_history\": false, \"model\": \"llama3:8b-instruct-q4_0\" }, \"stt\": { \"device\": \"torch device identifier (`cuda` if available; otherwise `cpu`\", \"generation_args\": { \"batch_size\": 8 }, \"model\": \"openai/whisper-small.en\" }, \"tts\": { \"device\": \"torch device identifier (`cuda` if available; otherwise `cpu`\", \"model\": \"tts_models/en/ljspeech/glow-tts\" } } When you use a configuration file, it overrides the default configuration but does not overwrite it. So you can partially modify the configuration if you desire. For instance, if you do not wish to use speech recognition and only want to provide prompts through text, you can disable that by using a config file with the following configuration: { \"stt\": null } Similarly, you can disable the audio synthesiser, or both, to only use the virtual assistant in text mode. If you only want to modify the device on which you want to load a particular type of model, without changing the other default attributes of the model, you could use: { \"tts\": { \"device\": \"cpu\" } } Configuration Attributes llm - Language Model Configuration llm.device: Torch device identifier (e.g., cpu, cuda, mps) on which the pipeline will be allocated. llm.disable_chat_history: Boolean indicating whether to disable or enable chat history. Enabling chat history will make interactions more dynamic, as the model will have access to previous contexts, but it will consume more processing power. Disabling it will result in less interactive conversations but will use fewer processing resources. llm.model: Name of the text-generation model tag on Ollama. Ensure this is a valid model tag that exists on your machine. llm.system_prompt: Give a system prompt to the model. If the underlying model does not support a system prompt, an error will be raised. stt - Speech-to-Text Model Configuration tts.device: Torch device identifier (e.g., cpu, cuda, mps) on which the pipeline will be allocated. stt.generation_args: Object containing generation arguments accepted by Hugging Face's speech recognition pipeline. stt.model: Name of the speech recognition model on Hugging Face. Ensure this is a valid model ID that exists on Hugging Face. tts - Text-to-Speech Model Configuration tts.device: Torch device identifier (e.g., cpu, cuda, mps) on which the pipeline will be allocated. tts.generation_args: Object containing generation arguments accepted by Coqui's TTS API. tts.model: Name of the text-to-speech model supported by the Coqui's TTS Toolkit. Ensure this is a valid model ID.",
    "commentLink": "https://news.ycombinator.com/item?id=40744293",
    "commentBody": "Local voice assistant using Ollama, transformers and Coqui TTS toolkit (github.com/mezbaul-h)131 points by mezba 20 hours agohidepastfavorite22 comments modeless 3 hours agoCoqui's XTTSv2 is good for this because it has a streaming mode. I have my own version of this where I got ~500ms end-to-end response latency, which is much faster than any other open source project I've seen. https://github.com/jdarpinian/chirpy These are easy to make and fun to play with and it's awesome to have everything local. But it will take more to build something truly useable. A truly natural conversational AI needs to understand the nuances of conversation, most importantly when to speak and when to wait. It also needs to know subtleties of the user's voice that no speech recognizer can output, and it needs control over the output voice more precise than any TTS provides. Audio-to-audio models in the style of GPT-4o are clearly the way forward. (And someday soon, video-to-video models for video calling with a virtual avatar. And the step after that is robotics for physical avatars). There aren't any open source audio-to-audio models yet but there are some promising approaches. https://ultravox.ai has the input half at least. https://tincans.ai/slm has a cool approach too. reply zkstefan 23 minutes agoparent> There aren't any open source audio-to-audio models yet I think that's not true. See this for example: https://huggingface.co/facebook/seamless-m4t-v2-large It's not general purpose like GPT4o but translation still seems pretty useful reply replete 5 hours agoprevI tried a similar project out last week, which uses Ollama, FastWhisperAPI, and MeloTTS: https://github.com/PromtEngineer/Verbi Docker is a great option if you want lots of people to try out your project, but not many apps in this space come with a dockerfile reply wkat4242 1 hour agoprevI currently use Ollama + Openwebui for this. It also has a really serviceable voice mode. And it has many options like RAG integrations, custom models, memories to know you better, vision, a great web interface etc. But I'll have a look at this thing. reply xan_ps007 10 hours agoprevwe have made an open source orchestration which enables you to plug in your own TTS/ASR/LLM for end-to-end voice conversations at -> https://github.com/bolna-ai/bolna. We are also working on a complete open source stack for ASR+TTS+LLM and will be releasing it shortly. reply bmicraft 6 hours agoparentHave you thought about support for the wyoming protocol? That would make it pretty much plug&play with home assistant. reply nmstoker 4 hours agorootparentHadn't heard of the Wyoming Protocol before, but it's interesting, thanks for mentioning For others who also hadn't heard of it, here's an overview: https://github.com/rhasspy/rhasspy3/blob/master/docs/wyoming... reply sleight42 13 hours agoprevOk, I need this but cloning Majel Barrett as the voice of the Enterprise computer. reply gavmor 2 hours agoparentTrivially done with a minute-long wav file. Simply specify the source sample in your june-va config.json reply underlines 10 hours agoprevHonestly, there are so many Project on Github doing STT - LLM - TTS that I lost count. The only revolutionary thing that feels like magic is if the STT supports Voice Activity Detection and low latency LLM inference on Groq, so conversations feel natural. reply xan_ps007 8 hours agoparentWhat we have learnt is that big enterprises do not really want to use close source models due to the random bursts in usage which might drain their bills. reply skenderbeu 2 hours agoprevMy very first Multimodal AI star on Github. Hope we see more of these in the future. reply aftbit 15 hours agoprevLooks interesting! Is the latency low enough for it to feel natural? How's the Coqui speech quality? reply lelag 14 hours agoparentIt supports XTTSv2 which is currently the open-weight state of the art. So, pretty damn good (https://huggingface.co/coqui/XTTS-v2/blob/main/samples/en_sa...). Too bad that the project is in limbo after Coqui (the company) folded. The license limits the use of the weights to non-commercial usage unless you buy a commercial license, and there's nobody left to sell you one now. reply qup 14 hours agorootparentis there anyone to sue you? (how does that work?) reply nmstoker 4 hours agorootparentI don't know the details in this case, but it seems plausible that someone still owns the IP and thus might be in a position to initiate legal proceedings. reply qup 57 minutes agorootparentThat same person could update the license, no? reply jsemrau 12 hours agoparentprevWhen I gave \"Matt\", my loyal local assistant[1], a voice xTTSv2 performed better for long form text. While in longform emotions seemed well balanced in the text, in short replies the emotion patterns frequently felt off and therefore unnatural. What I liked about xTTsv2 though is that voice cloning is fairly easy by just providing a .wav file with the intended voice pattern. [1]https://open.substack.com/pub/jdsemrau/p/teaching-your-agent... reply pclmulqdq 7 hours agorootparentxTTS is notoriously bad at generating short samples. It will also hallucinate if you give it something short enough. reply Gryph0n77 4 hours agoprevHow many RAM GB the model requires? reply replete 5 hours agoprevHow does the STT compare to Fastwhisper? reply m3kw9 2 hours agoprev [–] How long till a stand alone OS that makes AI usage its first class citizen? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "**june-va** is a local voice chatbot integrating Ollama, Hugging Face Transformers, and Coqui TTS Toolkit, ensuring privacy by processing data locally.",
      "It supports multiple interaction modes, including text and voice inputs/outputs, with a default mode of voice input and audio/text output.",
      "Installation requires Python 3.10+ and specific dependencies, with customization available through a JSON configuration file."
    ],
    "commentSummary": [
      "A local voice assistant using Ollama, transformers, and Coqui TTS toolkit is discussed, with Coqui's XTTSv2 praised for its ~500ms response latency in streaming mode.",
      "Audio-to-audio models like GPT-4o are considered the future of conversational AI, with promising approaches from ultravox.ai and tincans.ai.",
      "Open source orchestration for TTS (Text-to-Speech), ASR (Automatic Speech Recognition), and LLM (Large Language Models) is available at bolna-ai/bolna, with the Wyoming Protocol noted for home assistant integration."
    ],
    "points": 131,
    "commentCount": 22,
    "retryCount": 0,
    "time": 1718923720
  },
  {
    "id": 40751020,
    "title": "Testing Generative AI for Circuit Board Design",
    "originLink": "https://blog.jitx.com/jitx-corporate-blog/testing-generative-ai-for-circuit-board-design",
    "originBody": "Jun 21, 2024 9:16:00 AM Testing Generative AI for Circuit Board Design Duncan Haldane Share: TLDR: We test LLMs to figure out how helpful they are for designing a circuit board. We focus on utility of frontier models (GPT4o, Claude 3 Opus, Gemini 1.5) across a set of design tasks, to find where they are and are not useful. They look pretty good for building skills, writing code, and getting useful data out of datasheets. TLDRN'T: We do not explore any proprietary copilots, or how to apply a things like a diffusion model to the place and route problem. Introduction Can an AI-powered chatbot help with a task as precise as circuit board design? These LLMs (Large Language Models) are famous for hallucinating details, and missing a *single* important detail can sink a design. Determinism is hard but super important for electronics design! Today, several shallow product offerings are making AI for electronics design look mostly like hype. But I think there is real utility to be found here, if we can take a better approach. In this article, we set LLMs to unfair tasks that expert human circuit board designers handle day to day. We're not looking for basic help, rather pushing on what it takes to help an expert do their job better. It would be ridiculous (today) to expect great performance from these general purpose AIs on this expert task, especially because it is probably not well represented in the training data. To test performance on difficult design tasks, we work with three of today's leading models: - Gemini 1.5 Pro from Google - GPT-4o from OpenAI - Claude 3 Opus from Anthropic And explore prompting strategies to get the best performance out of all the models on each task. As part of the prompt, we also look for opportunities to improve the performance by asking the LLMs to write code to design circuit boards where possible. The best results are reported for each model and for each task. Let's dig in! Asking stupid questions There is a lot to know in circuit board design, and nobody has mastered every relevant domain. Asking an LLM stupid questions is a great way to learn. For example an RF engineer will know their own specialty quite well, but might not be a master of supply chain, power supply design, or how to shape copper reliefs to get high yields at the factory. ( Some RF engineers will scoff at this, but realistically the world contains people trying to learn things.) To simulate someone new to a domain, I avoid prompt engineering, and use of precise expert vocabulary. I asked the LLMs a simple question: What is the delay per unit length of a trace on a circuit board? Claude 3 Opus was the clear winner here. Notice that it brought in the relevant concepts (microstrip, stripline, dielectric), got the answer right, and pointed out critical gotchas: Notice the gotchas here! The speed of a signal changes based on which layer of a circuit board you route it on, and what the materials are. You would never know that looking at a most PCB CAD tools, (e.g. Kicad, Altium) because all of their delay matching is in units of length. e.g. 'both these traces have to be 50 mm long' is a constraint you enter with a GUI. But that's not correct if you want your board to work - delay depends on which layer they are routed on, and what materials are in your board. Getting that right: A+ answer for Claude here. Google Gemini 1.5 performed the worst on this question. Maybe because it brought in a bunch of sources from the internet, and most of the writing in this discipline is dreck generated by SEO content mills. Incorporating AI summarization of lower quality writing seems to make the answers consistently worse. Finding Parts One of the superpowers of an experienced engineer is knowing things like: \"A DAC + DSP system good enough for this audio pipeline should be about this big, and cost $1.40 in production\". They'll then go do some turbo googling and find it. How well can an AI that's been fed the entire internet do? For this example I chose find parts for something cool: a robot motor driver linked by optical ethernet (instead of the usual CAN setups). I want smaller cables, hundreds of possible devices, and compact cables that can survive IRL twisting and jostling. You can see a communication layer like this as the daisy-chained black cabling in this picture from HEBI: Time to use AI to find the parts should I use for the ethernet switch, and the optical transceivers. Here's the prompt: I want to design a daisy-chained communications layer for a motor driver, using 100M optical Ethernet. Each motor controller will have a microcontroller that interfaces to the Ethernet network over RGMII. It needs to be compact, robust to mechanical vibration and shock, and handle temperatures up to 60 degrees Celsius. Please help me choose parts for the optical connectors, transceivers, and Ethernet networking device. 1. Think step by step as an expert electrical engineer. 2. Carefully consider how component choices relate to the requirements. 3. I'll give you a digital cookie if you get the answer correct. Here I'm pulling more prompt engineering tricks to boost the performance of the model, and being careful in detailed specifications to really get this to work. My initial guess for these parts would be: - Use a small three port ethernet switch to for the daisy chain, hopefully something from Micrel (now Microchip) like the KSZ8863MLL. - For the optical connector/transceiver I would go with the OptoLock part on the example system from Hebi. Very small and robust choice. It would be amazing to get new suggestions for parts and architectures here. Something going beyond superficial advice into something practical would have huge impact! Instead, performance across all models was uniformly bad and no amount of prompting or follow ups could fix it. It felt a lot like trying to get AI to solve a variant of the wolf, sheep, cabbage problem. It's got the answer locked in, and it's hard to get out of that basin. Here's a representative conclusion from Gemini 1.5: Example Part Selection - Optical Connectors: LC Duplex connectors (Amphenol or similar) - Optical Transceivers: 100Base-FX SFP transceivers (Finisar, Cisco, or other reputable brands) with industrial temperature ratings - Ethernet Networking Device: Microchip LAN8742A or Texas Instruments DP83848 Digital Cookie Time! 🍪 I hope this detailed breakdown helps you choose the right parts for your daisy-chained motor control system. Please let me know if you have any other questions. Grading this: - All models missed that to get the daisy chain you need a three port ethernet switch. They instead recommend an Ethernet Phy, which isn't going to do it. - All models recommend using a huge SFP transceiver. These are usually found on big datacenter boards, and are not known for being shoved into tiny spaces in a robot joint, and then getting kicked around by Boston Dynamics employees. Overall the generative models don't demonstrate a solid grasp on what the motor controller communication network is, or what application-specific considerations need to be made. Without that the part selections are way off. This is a consistent experience. The LLMs will always suggest something, but it will be the 'average' application and miss a lot of the important subtlety. i.e. fiber optic ethernet happens in the mostly in the datacenter, so an average application for fiber optic means a datacenter device. Trying to get them to choose an ADC, or sensor, or connector, shows similar 'average' results. Maybe this speaks to the difficulty of the domain. If you're not within 10% or so of the global optimal performance in EE, your suggestions have near-zero value. Parsing datasheets Sadly, most of the data you need to design a circuit board is baked away in PDF datasheets, instead of being available via APIs. Directly trying to parse PDFs with python tends to not go well, but LLMs should be pretty good at pulling data from these human-structured documents. I tested three different way of pulling information out: - Copy/paste from the PDF, stuff it into a prompt. - Capture a portion as an image, have the LLM interpret the image - Upload the entire PDF I'll be using the data available for the Nordic nRF5340 WLCSP for the experiment. A little 4.0 x 4.4 mm Bluetooth chip, with an 820 page datasheet. Observations on this part: This is by no means the largest datasheet for a electrical component but has many typical features including pin tables, mechanical drawings, application specific notes, graphs, etc This datasheet has significantly more content than you might expects for simpler components - like OpAmps or digital logic gates. If the LLMs are useful here, they will likely be useful for smaller cases. This datasheet is about 570k tokens, a little more than half the huge context window available from Gemini. Make a symbol and footprint First step is a to make a linked symbol and footprint (landpattern) for our component. The most effective method here was to load the entire datasheet into the LLM via the chat file upload mechanism, and then query it interactively to get the details right. Gemini 1.5 was the most reliable for this task. Here's the prompt to get the pin table (after uploading the datasheet): Please write the the pin table for the wlcsp version of the nRF5340 as code using the following format: 1. One line for each pin 2. Each line structured as \" [ pin-namepad-numberdirectionbank ]\", where pin-name is the name of a pin e.g. GND, pad-number is the number of the pad that maps to that pin, e.g, 28, direction is Left is the pin is an input, and Right otherwise, bank indicates the function, e.g. gpio. 3. Wrap numbers following letters in square brackets, e.g. K3 becomes K[3], C1 becomes C[1], F10 becomes F[10] 4. Replace periods with square brackets, e.g. P0.20 becomes P0[20] The AI then returns a block of text with our pin list in a usable format: [ XC1A1Leftanalog_input ] [ XC2A2Leftanalog_input ] [ VDDA3Rightpower ] [ VSSA5Leftpower ] [ DECDA7Rightpower ] [ DCCDA9Rightpower ] [ D-| A11Leftusb ] [ D+A12Leftusb ] [ DECRFB1Rightpower ] [ VSSB2Leftpower ] [ DECRB4Rightpower ] [ DECNB5Rightpower ] ... (continued) It got this list exactly correct, impressive for 95 rows of data buried in an 820 page datasheet. Usually I have to get a model to write a little code to reach that level of accuracy, but that was unnecessary here. Next I gave it some example code for a BGA footprint and had it generate a new version based on the datasheet information. Here's the resulting code: ; This test is based on the Nordic Semiconductor, nRF5340 ; Datasheet: ; https://infocenter.nordicsemi.com/pdf/nRF5340_PS_v1.4.pdf ; ; Drawing: ; https://infocenter.nordicsemi.com/pdf/nRF5340_PS_v1.4.pdf ; ; NSMD val rows = 12 val cols = 8 val grid = Grid-Numbering(rows, cols) val missing = to-pad-island(grid, \"A[1] B[1] B[3] C[1] C[3] D[3] D[4] D[5] D[6] D[7] D[8] D[9] E[1] E[9] F[4] F[6] F[7] F[9] G[1] G[4] G[9] H[4] H[5] H[6] H[7] H[8] H[9]\") val planner = Full-Matrix-Planner( ; Several pads are not present inactive = missing, pad-config = PadConfig-D( copper-D-adj = ipc-bga-copper( NonSolderMaskDefined, density-level = DensityLevelC ), mask-D-adj = 0.0, ) ) val body = PackageBody( width = 4.390 +/- 0.047, length = 3.994 +/- 0.047, height = 0.71 +/- [0.06, 0.0] ) val pkg = BGA( num-leads = 95, rows = rows, columns = cols, lead-diam = 0.35, pitch = 0.35, package-body = body, lead-numbering = grid, pad-planner = planner, density-level = DensityLevelB ) A few more misses here than in the pin table, but close overall. I have to go in and separately prompt for the depopulated balls in the grid, as well as correct the dimensions in the PackageBody. (Note that I'm generating JITX code because it has a text-native representation. I bet that you could get the model to emit something like a Kicad symbol/footprint with a little more nudging in the prompt.) Running the generated code (which you can find here), we get our footprint and symbols: I really like the combination of code and scraped data here because of precision and flexibility. I would much rather trust a hand-coded BGA footprint generator for the details, and have the LLM tell me which pads to omit. The backstop of the LLM generating the list of pins and the missing pins, then forcing them both to line up to line up in the code is a nice touch. I can also dig in and edit parameters directly if I want to customize. Past the standard component model with a symbol and footprint, the LLM can also pull most of the useful data from the PDF. e.g. here is a generated parameterization for the IO pin levels for checking correctness of connections in a design: property(P0[0].digital_io) = DigitalIO(CMOSOutput(VSS + 0.4, VDD - 0.4), 0.3*VDD, 0.7*VDD, \"VDD\", \"VSS\", 0) If you don't have the AI return an answer as code, it tends to spread the data you want across multiple paragraphs, and it is easier to get the answer you want by just reading the datasheet instead. It can even read a bit of the reference schematic and turn it into a netlist: * Power Supply Nets: net (nrf.VDD C4.p[1]) net (nrf.VDD C10.p[1]) net (nrf.VDD C11.p[1]) net (nrf.VDD C12.p[1]) net (nrf.VDD C13.p[1]) net (nrf.VDD C15.p[1]) net (nrf.VDD C17.p[1]) .... (continued) It hallucinated this task a little bit, inventing resistors here and there. Using LLMs to pull data from challenging sources and turn it into useful code was a helpful automation technique (and personally the one I use most). Accuracy can be boosted further by using extraction techniques to cross-reference the data. Designing circuits We've seen that LLMs can understand an image of a schematic well enough to (mostly) turn it into a netlist. Can they take on a much more challenging task and design the circuit itself? As shown earlier, LLMs generated kinda-right, but actually very wrong suggestions for key components. To avoid that dependency, we tested our AIs on an analog circuit design task. This test is more about doing some math, and correctly arranging resistors and capacitors. The task is to design an pre-amplifier for an electret microphone -- boosting and filtering an audio signal for sampling by an ADC. We start with a basic architecture exploration. Here's the prompt: I want to design a microphone pre-amp where I need to bias the electret microphone and create a single-ended signal out to drive the ADC of a microcontroller. Include a bandpass filter with a gain of 20dB, corner frequencies at 100Hz and 20kHz. Help me select an architecture for this amplifier. Propose different filter architectures and describe their pros and cons for this application. 1. Think step by step as an expert electrical engineer 2. Carefully consider the requirements 3. If you get the answer right you get a digital cookie. Claude 3 Opus came back with the best answer of the three contenders. I'll analyze it piece by piece. Okay, let's think through this step-by-step as an expert electrical engineer: 1. Microphone biasing: Electret microphones typically require a DC bias voltage, usually between 1.5V to 10V, applied through a bias resistor. The value of this resistor impacts the microphone's output impedance and noise. A common range is 2.2kΩ to 10kΩ. We'll need to check the microphone's datasheet for specifics. 2. Amplifier architecture: For a single-ended output to drive an ADC, we can use a single-supply op-amp configuration. Some good options are: a) Non-inverting amplifier: Simple, high input impedance which is good for the high-impedance mic signal. Gain is set by the feedback resistor ratio. b) Inverting amplifier: Also simple, but lower input impedance. Gain is also set by resistor ratio. c) Push-pull: Combines inverting and non-inverting amps for more output current drive, but more complex. For simplicity and good mic interfacing, a non-inverting amp seems best here. Claude did a good job calling out the need to bias the microphone, nice to have that called out explicitly. The proposals for non-inverting, and inverting amplifiers are reasonable. The proposal for push-pull configurations is nonsense. A Push-Pull configuration describes the transistors inside the op-amp at the output stage. It doesn't have anything to do with the architecture of the op amp circuit. Moving on to the filter suggestions: 3. Bandpass filter: We need a 100Hz to 20kHz passband with 20dB gain. Some filter options: a) Passive RC: Simple, but requires large component values for low frequencies, and loading affects cutoff frequencies. Gain must come from amp. b) Active 1st-order: Sallen-Key or Multiple-Feedback topologies. Each is 1st-order, so we'd cascade a low-pass and high-pass for the bandpass response. Easy to integrate gain. Cutoff frequencies affected by component tolerance. c) Active 2nd-order: Single stage 2nd-order bandpass like a Dual-Amplifier Bandpass (DABP). More complex, but more efficient than cascaded 1st-order. More stable cutoff frequencies. d) Switched-capacitor: Uses a clock to set cutoff frequencies. Very stable, but requires a clock signal. The comments on the Passive RC filter are reasonable. The Sallen-Key analysis is wrong - Sallen-Key is a second order filter and can be unstable with this much gain. The suggestion for the DABP isn't wrong per se, but it is missing some nuance. It doesn't allow for very high gain (limit of 2.0). The complexity of a DABP justified when you need to handle higher frequencies and higher Q levels. This application doesn't require that, so it's a weird choice. A switched capacitor filter would be bonkers -- complex, noise-sensitive, and requiring exotic components. Moving on: For this application, an active 2nd-order filter like a DABP seems a good balance of simplicity, stability, and easy gain integration. 1. Amplifier gain: The filter will have 20dB gain, which is a factor of 10. We can implement this with a non-inverting amp by setting the feedback resistor ratio to 9. For example, if R1 = 1kΩ, then R2 = 9kΩ. 2. Component values: We'll need to calculate the exact resistor and capacitor values for the DABP filter based on the desired cutoff frequencies and gain. There are online calculators to help with this. So in summary, I propose a non-inverting op-amp configuration for the microphone, followed by a 2nd-order DABP active filter with 20dB gain. The exact component values will depend on the specific microphone chosen and the filter design equations. How's that? Let me know if you have any other questions! Here, Claude chooses the DABP and estimates some reasonablish values given that choice. (Usually you would choose values greater than 10k and less than 1M in op amp circuit design, to make assumptions about input and output impedance hold better). The wording is a bit confusing but it could be interpreted as a non-inverting op-amp for the gain stage, followed by a DABP filter with a lower gain. Which can work! Technically! It's making slightly strange decisions across the board, but let's proceed with creating a complete design. With a little more prompting, we can get Claude to write some code to do the detailed design and generate the circuit: Great! Let's dive into the calculations and generate the netlists. We'll assume a 3.3V power supply and a microphone bias of 2.5V. We'll use an ideal op-amp model for simplicity. ; Microphone Bias Circuit: ; - Let's choose a bias resistor of 2.2kΩ. ; - Assuming negligible current draw from the op-amp, the mic bias will be very ; close to 2.5V. ; Netlist: inst mic : electret-mic inst rbias : chip-resistor(2200.0) inst c1 : ceramic-cap(1.0e-6) ; Decoupling cap, assuming 1uF net (vdd rbias.p[1]) net (rbias.p[2] mic.vdd c1.p[1]) net (gnd mic.gnd c1.p[2]) The decoupling cap in the bias circuit here really hampers the high-frequency performance of the circuit. What the circuit actually needs is a coupling cap, shown in the human.mic circuit here: Presumably in the training corpus, many articles and datasheets write extensively about decoupling caps. Maybe the \"average\" recommendation wins out here, and once it sees a positive and negative pin it slaps a capacitor across them right away. In our tests (and most product demos), LLMs loved to give naive advice about decoupling, which usually isn't as actively harmful as it is here. Moving on to the amplifier: ; Non-Inverting Amplifier: ; - Let's set the gain to 10 (20dB). ; - Choose R1 = 1kΩ, then R2 = 9kΩ. ; Netlist: inst amp : op-amp inst r1 : chip-resistor(1000.0) inst r2 : chip-resistor(9000.0) net (mic.out r1.p[1]) net (r1.p[2] amp.v- r2.p[1]) net (amp.out r2.p[2]) The AI describes the amplifier as non-inverting, but then feeds the signal into the inverting input. It's also missing a few other parts and nets that would make for a working circuit. Easier to see in the generated schematic: (Cue EE eye twitching) Moving on to the DABP filter implementation: ; DABP Bandpass Filter: ; - Cutoff frequencies: fl = 100Hz, fh = 20kHz ; - Gain at center frequency: G0 = 10 (20dB) ; - Quality factor: Let's choose Q = 1 for a fairly wide passband ; - Center frequency: f0 = sqrt(fl * fh) ≈ 1.414kHz ; - Let's choose C1 = C2 = 10nF ; - R1 = R2 = 1 / (2 * pi * f0 * C1) ≈ 11.2kΩ, let's use 11kΩ ; - R3 = R1 / (2 * Q * G0) ≈ 550Ω, let's use 560Ω ; - R4 = 2 * Q * R1 / G0 ≈ 2.2kΩ ; Netlist: inst op1 : op-amp inst op2 : op-amp inst r1 : chip-resistor(11000.0) inst r2 : chip-resistor(11000.0) inst r3 : chip-resistor(560.0) inst r4 : chip-resistor(2200.0) inst c1 : ceramic-cap(10.0e-9) inst c2 : ceramic-cap(10.0e-9) net (amp.out op1.v+ r1.p[1] c1.p[1]) net (op1.v- op2.out r1.p[2] c1.p[2]) net (op1.out r3.p[1]) net (r3.p[2] op2.v- r2.p[1] c2.p[1]) net (op2.v+ r2.p[2] c2.p[2] r4.p[1]) net (r4.p[2] gnd) Generated circuit (left), versus reference from ADI (right). A few more misses on the netlist here, and about as correct as the amplifier (which is to say, not very). We also see the weirdly strong default of attaching capacitors across positive and negative pins. I guess LLMs are high decouplers. And to round it off, we've got another reminder to add decoupling caps, and a reasonable note about what to watch out for: The output of op2 is the final output of the pre-amp circuit. This should give you a good starting point. Remember to add decoupling capacitors and power supply connections as needed. Also, the actual component values may need to be adjusted based on the real specs of your chosen op-amp and the exact requirements of your ADC. Let me know if you have any other questions! Comparing against ground truth This reference design from Texas Instruments is a good implementation of the requirements we asked these LLMs to design to. It uses passive RC filters (which the AI considered and avoided), and a single op-amp. I did a quick implementation of this in JITX (code here) to compare the AI generated circuit (code here) to the TI version. The AI generated circuit was three times the cost and size of the design created by that expert engineer at TI. It is also missing many of the necessary connections. I found this breakdown surprising - it had no problems generating code in general, and was able to generate a syntactically correct netlist. The code runs! It's just that the pins are not in the right net statements and the resulting circuit is not correct. Testing higher level code In fairness, it's rare to write code like this to design a circuit board. In practice you're primarily calling higher level functions, e.g. selecting an op-amp, configuring a filter with parameters, then using a parametric pre-amp circuit: val OpAmpType = components/STMicroelectronics/TS971ILT/component val bandpass = BandpassConfig(gain = 34.0, low-cut = 100.0, high-pass = 20.0e3) val preampType = ocdb/modules/amplifiers/microphone-preamp(OpAmpType, bandpass) inst amp : preampType Would LLMs do better if we try to get them to define and use higher-level functions instead? Gemini 1.5 did the best here, inventing some reasonable APIs to functions that would exist in a library of helpful circuits: # Modules amp = inverting_amplifier(\"amp1\", gain=-20.0) filt = sallen_key_filter(\"filt1\", low_cutoff=100.0, high_cutoff=20.0e3) # Connections net (mic.pos r_bias.p[1]) # Microphone to bias resistor net (r_bias.p[2] c1.p[1] amp.in) # Bias resistor to amp through coupling capacitor (Still misses on the netlist generation though.) Our experiments with code generation reveal an interesting pattern: the LLMs are not great at generating detailed code for a netlist to implement a circuit design. That was a little surprising because as shown in the datasheet parsing section, they can write a 95% correct netlist matching an uploaded image of a schematic. This difference suggests that LLMs excel at information extraction and transformation tasks, but struggle with original design synthesis. Having the LLMs generate higher-level code worked better than raw netlist generation, and seems like a promising path forward. This suggests a hybrid approach where human experts create parametric implementations of common circuits, which LLMs can then utilize and adapt, potentially increasing overall reliability. Future research could either explore fine-tuning models specifically for netlist generation tasks, or (this being generative AI) wait a month and see how well the next generation of models does on this problem. Conclusion Circuit board design requires a lot of precision to get all the details correct -- for unsupervised AI techniques, I think there is a data problem here. The context and meaning of a design -- What does this do? How do you know if it's good? -- are usually absent from schematic and board design data. Maybe there's a little text comment on the schematic with filter calculations, but that's usually it, and the rest lives in the engineer's head. An electrical engineer looks at a schematic, understands a portion as a Sallen-Key filter, knows the limitations and function of that circuit, and composes that understanding with the rest of the design. A tremendous amount of context, which is not in any of the design data you can scrape from the internet. Our experiments show that generative AI models can be useful for circuit board design, if you use them to write code. With that approach, there were still significant differences across Gemini 1.5, Claude 3 Opus, and GPT-4o for circuit board design. Claude 3 was most helpful for answering questions to learn new domains. There's a risk of hallucinations, but those usually fall away if you interrogate the model until it's consistent. Certainly Claude is better and more helpful than the *average* article on circuit board design you would find via web search (there are very good articles, but they can be hard to find). Gemini's huge context window was very helpful for pulling component data out of datasheets. It was the one area where the AI 'just worked' and we got way more usability than we expected. I'm interested in further testing to find the limits of this technique. Google has done a lot of work on the \"Text Haystack\" problem to pull tiny pieces of information accurately out of a massive corpus of data, and that's serving them well here. From the Gemini 1.5 Report Surprisingly GPT-4o didn't have the most useful answer in any of the categories we tested. It feels wrong to not include any outputs from the model in this summary article, so here's how GPT-4o feels about coming in third: Overall we have some promising directions. Using LLMs for circuit board design looks a lot like using them for other complex tasks. They work well for pulling concrete data out of human-shaped data sources, they can do slightly more difficult tasks if they can solve that task by writing code, but eventually their capabilities break down in domains too far out of the training distribution. We only tested the frontier models in this work, but I predict similar results from the open-source Llama or Mistral models. Some fine tuning on netlist creation would likely make the generation capabilities more useful. Our work shows both the potential and limitations of current LLMs in specialized fields like circuit design. They're excellent at retrieving information and generating code, but struggle with the nuanced decision-making needed for complex designs. Future LLMs will likely improve here, but for the current model generation, they're best used as tools to assist human experts rather than as standalone designers. Let me know what you think! You can find me on the JITX discord, and on Twitter. JITX Projects",
    "commentLink": "https://news.ycombinator.com/item?id=40751020",
    "commentBody": "Testing Generative AI for Circuit Board Design (jitx.com)125 points by DHaldane 2 hours agohidepastfavorite47 comments HanClinto 1 hour agoThis feels like an excellent demonstration of the limitation of zero-shot LLMs. It feels like the wrong way to approach this. I'm no expert in the matter, but for \"holistic\" things (where there are a lot of cross-connections and inter-dependencies) it feels like a diffusion-based generative structure would be better-suited than next-token-prediction. I've felt this way about poetry-generation, and I feel like it might apply in these sorts of cases as well. Additionally, this is a highly-specialized field. From the conclusion of the article: > Overall we have some promising directions. Using LLMs for circuit board design looks a lot like using them for other complex tasks. They work well for pulling concrete data out of human-shaped data sources, they can do slightly more difficult tasks if they can solve that task by writing code, but eventually their capabilities break down in domains too far out of the training distribution. > We only tested the frontier models in this work, but I predict similar results from the open-source Llama or Mistral models. Some fine tuning on netlist creation would likely make the generation capabilities more useful. I agree with the authors here. While it's nice to imagine that AGI would be able to generalize skills to work competently in domain-specific tasks, I think this shows very clearly that we're not there yet, and if one wants to use LLMs in such an area, one would need to fine-tune for it. Would like to see round 2 of this made using a fine-tuning approach. reply surfingdino 34 minutes agoparent> This feels like an excellent demonstration of the limitation of zero-shot LLMs. It feels like the wrong way to approach this. There is one posted on HN every week. How many more do we need to accept the fact this tech is not what it is sold at and we are bored waiting for it get good? I am not say \"get better\", because it keeps getting better, but somehow doesn't get good. reply DHaldane 1 hour agoparentprevMy gut agrees with you that LLMs shouldn't do this well on a specialty domain. But I think there's also the bitter lesson to be learned here: many times people say LLMs won't do well on a task, they are often surprised either immediately or a few months later. Overall not sure what to expect, but fine tuning experiments would be interesting regardless. reply cjk2 1 hour agorootparentI doubt it'd work any better. Most of EE time I have spent is swearing at stuff that looked like it'd work on paper but didn't due to various nuances. I have my own library of nuances but how would you even fine tune anything to understand the black box abstraction of an IC to work out if a nuance applies or not between it and a load or what a transmission line or edge would look like between the IC and the load? This is where understanding trumps generative AI instantly. reply DHaldane 1 hour agorootparentI doubt it too, but I notice that I keep underestimating the models. Do you have a challenge task I can try? What's the easiest thing I could get an LLM to do for circuit board design that would surprise you? reply cjk2 1 hour agorootparentMake two separate signals arrive at exactly the same time on two 50 ohm transmission lines that start and end next to each other and go around a right hand bend. At 3.8GHz. Edit: no VSWR constraint. Can add that later :) Edit 2: oh or design a board for a simple 100Mohm input instrumentation amplifier which knows what a guard ring is and how badly the solder mask will screw it up :) reply DHaldane 0 minutes agorootparentRight - LLMs would be a bit silly for these cases. Both overkill and underkill. Current approach for length matching is throw it off to a domain specific solver. Example test-circuit: https://x.com/DuncanHaldane/status/1803210498009342191 How exact is exactly the same time? Current solver matches to under 10fs, and I think at that level you'd have to fab it to see how close you get with fiber weave skew and all that. Do you have a test case for a schematic design task? bmicraft 42 minutes agorootparentprevIt would seem to me that the majority of boards would be a lot more forgiving. Are you saying you wouldn't be impressed if it could do only say 70% of board designs completely? reply LeifCarrotson 1 hour agorootparentprevReally? Most of the time? I find I spend an enormous amount of time on boring stuff like connecting VCC and ground with appropriate decoupling caps, tying output pins from one IC to the input pins on the other, creating library parts from data sheets, etc. There's a handful of interesting problems in any good project where the abstraction breaks down and you have to prove your worth. But a ton of time gets spent on the equivalent of boilerplate code. If I could tell an AI to generate a 100x100 prototype with such-and-such a microcontroller, this sensor and that sensor with those off-board connectors, with USB power, a regulator, a tag-connect header, a couple debug LEDs, and break out unused IO to a header...that would have huge value to my workflow, even if it gave up on anything analog or high-speed. Presumably you'd just take the first pass schematic/board file from the AI and begin work on anything with nuance. If generative AI can do equivalent work for PCBs as it can do for text programming languages, people wouldn't use it for transmission line design. They'd use it for the equivalent of parsing some JSON or making a new class with some imports, fields, and method templates. reply scld 4 minutes agorootparent\"Looks like you forgot pullups on your i2c lines\" would be worth a big monthly subscription hahaha. reply HanClinto 1 hour agorootparentprev> But I think there's also the bitter lesson to be learned here: many times people say LLMs won't do well on a task, they are often surprised either immediately or a few months later. Heh. This is very true. I think perhaps the thing I'm most amazed by is that simple next-token prediction seems to work unreasonably well for a great many tasks. I just don't know how well that will scale into more complex tasks. With simple next-token prediction there is little mechanism for the model to iterate or to revise or refine as it goes. There have been some experiments with things like speculative generation (where multiple branches are evaluated in parallel) to give a bit of a lookahead effect and help avoid the LLM locking itself into dead-ends, but they don't seem super popular overall -- people just prefer to increase the power and accuracy of the base model and keep chugging forward. I can't help feeling like a fundamental shift something more akin to a diffusion-based approach would be helpful for such things. I just want some sort of mechanism where the model can \"think\" longer about harder problems. If you present a simple chess board to an LLM or a complex board to an LLM and ask it to generate the next move, it always responds in the same amount of time. That alone should tell us that LLMs are not intelligent, and they are not \"thinking\", and they will be insufficient for this going forward. I believe Yann LeCun is right -- simply scaling LLMs is not going to get us to AGI. We need a fundamental structural shift to something new, but until we stop seeing such insane advancements in the quality of generation with LLMs (looking at you, Claude!!), I don't think we will move beyond. We have to get bored with LLMs first. reply sweezyjeezy 1 hour agorootparentprevSome research to the contrary [1] - tldr is that they didn't find evidence that generative models really do zero shot well at all yet, if you show it something it literally hasn't seen before, it isn't \"generally intelligent\" enough to do it well. This isn't an issue for a lot of use-cases, but does seem to add some weight to the \"giga-scale memorization\" hypothesis. [1] https://arxiv.org/html/2404.04125v2 reply eimrine 1 hour agoparentprevI like how you called it holistic, it is maybe the first time I see this word not in a \"bad\" context. What about the topic, it is impossible to synthesize STEM things not in the manner an engineer does this. I mean thou shalt to know some typical solutions and have all the calculations for all what's happening in the schematic being developed. Textbooks are not a joke and no matter who are you - a human or a device. reply omgJustTest 1 hour agoparentprevI asked this question of Duncan Dec 22! If you are interested I highly recommend this + your favorite llm. It does not do everything but is far superior to some highly expensive tools, in flexibility and repeatability. https://github.com/devbisme/skidl reply hoosieree 1 hour agoparentprevI agree diffusion makes more sense for optimizing code-like things. The tricky part is coming up with a reasonable set of \"add noise\" transformations. reply HanClinto 17 minutes agorootparent> The tricky part is coming up with a reasonable set of \"add noise\" transformations. Yes, as well as dealing with a variable-length window. When generating images with diffusion, one specifies the image ahead-of-time. When generating text with diffusion, it's a bit more open-ended. How long do we want this paragraph to go? Well, that depends on what goes into it -- so how do we adjust for that? Do we use a hierarchical tree-structure approach? Chunk it and do a chain of overlapping segments that are all of fixed-length (could possibly be combined with a transformer model)? Hard to say what would finally work in the end, but I think this is the sort of thing that YLC is talking about when he encourages students to look beyond LLMs. [1] * [1] https://x.com/ylecun/status/1793326904692428907 reply cjk2 2 hours agoprevEx EE here > The AI generated circuit was three times the cost and size of the design created by that expert engineer at TI. It is also missing many of the necessary connections. Exactly what I expected. Edit: to clarify this is even below the expectations of a junior EE who had a heavy weekend on the vodka. reply FourierEnvy 1 hour agoparentWhy do people think inserting an LLM into the mix will make it better than just an evolutionary or reinforcement model applied? Who cares if you can talk to it like a human? reply Terr_ 1 hour agorootparentYeah, when the author was writing about that initial query about delay-per-unit-length, I'm thinking: \"This doesn't tell us whether an LLM can apply knowledge, only whether relevant text was included in its training data.\" It's a distinction I fear many people will have trouble keeping in-mind, faced with the misleading eloquence of LLM output. reply shrimp_emoji 2 hours agoparentprevIt's like a generated image with an eye missing but for circuits. :D reply cjk2 2 hours agorootparentAI proceeds to use 2n3904 as a thyristor. AI happy as it worked the first 10ns of the cycle. reply jeffreygoesto 59 minutes agorootparentEvery natural Intelligence knows that you need to reach out to a 2N3055 for heavy duty. ;) reply bottlepalm 2 hours agoprevIt'd be interesting to see how Sonnet 3.5 does at this. I've found Sonnet a step change better than Opus, and for a fraction of the cost. Opus for me is already far better than GPT-4. And same as the poster found, GPT-4o is plain worse at reasoning. Edit: Better at chain of thought, long running agentic tasks, following rigid directions. reply DHaldane 2 hours agoparentThat's an interesting question - I'll take a few pokes at it now to see if there's improvement. reply DHaldane 2 hours agorootparentUpdate: Sonnet 3.5 is better than any other model for the circuit design and part finding tasks. Going to iterate a bit on the prompts to see how much I can push the new model on performance. Figures that any article written on LLM limits is immediately out of date. I'll write an update piece to summarize new findings. reply CamperBob2 2 hours agorootparentThat name threw me for a loop. 'Sonnet' already means something to EEs ( https://www.sonnetsoftware.com/ ). reply stavros 2 hours agoparentprevOpus is better than GPT-4? I've heard mixed experiences. reply DHaldane 2 hours agorootparentIt really depends on the type of question, but generally I'm between Gemini and Claude these days for most things. reply imperio59 2 hours agorootparentprevThat's because the sample size is probably small and for niche prompts or topics. It's very hard to evaluate whether a model is better than another, especially doing it in a scientifically sound way is time consuming and hard. This is why I find these types of comments like \"model X is so much better than model Y\" to be about as useful as \"chocolate ice cream is so much better than vanilla\" reply r2_pilot 2 hours agorootparentAnd both flavors have a base flavor of excrement... Still, since I started using Claude 3 Opus (and now 3.5 Sonnet) a couple of months back, I don't see myself switching from them nor stopping use of LLM-based AI tech; it's just made me feel like the computer is actually working for and with me and even that alone can be enough to get me motivated and accomplish what I set out to do. reply skapadia 1 hour agorootparent\"it's just made me feel like the computer is actually working for and with me and even that alone can be enough to get me motivated and accomplish what I set out to do.\" This is a great way to describe what I've been feeling / experiencing as well. reply stavros 1 hour agorootparentprevTrue, I just tried it for generating a book summary, and Sonnet 3.5 was very bad. GPT-4o is equally bad at that , gpt-4-turbo is great. reply netsec_burn 1 hour agorootparentThis more likely has to do with context length? reply stavros 1 hour agorootparentNo, all the information is there, but gpt-4o tends to produce bullet points (https://www.thesummarist.net/summary/the-making-of-a-manager...), whereas gpt-4-turbo tends to produce much more readable prose (https://www.thesummarist.net/summary/supercommunicators/the-...). reply dindobre 2 hours agoprevUsing neural networks to solve combinatorial or discrete problems is a waste of time imo, but I'd be more than happy if somebody could convince me of the opposite. reply utkuumur 1 hour agoparentThere are recent papers based on diffusion that perform quite well. Here's an example of a recent paper https://arxiv.org/pdf/2406.01661. I am also working on ML-based CO. My approach has a close 1% gap on hard instances with 800-1200 nodes and less than 0.1% for 200-300 nodes on Maximum Cut, Minimum Independent Set, and Maximum Clique problems. I think these are very promising times for neural network-based discrete optimization. reply dindobre 5 minutes agorootparentThanks, will try to give it a read this weekend. Would you say that diffusion is the architectural change that opened up CO for neural nets? Haven't followed this particular niche in a while reply shrubble 42 minutes agoprevReminds me of this, an earlier expert-system method for CPU design, which was not used in subsequent designs for some reason: https://en.wikipedia.org/wiki/VAX_9000#SID_Scalar_and_Vector... reply sehugg 45 minutes agoprevHow does this compare to Flux.ai? https://docs.flux.ai/tutorials/ai-for-hardware-design reply guidoism 1 hour agoprevThis reminds me of my professor's (probably very poor) description of NP-complete problems where the computer would provide an answer that may or may not be correct and you just had to check that it was correct and you do test for correctness in polynomial time. It kind of grosses me out that we are entering a world where programming could be just testing (to me) random permutations of programs for correctness. reply moffkalast 1 hour agoparentWell we had to keep increasing inefficiency somehow, right? Otherwise how would Wirth's law continue to hold? reply cushychicken 1 hour agoprevI'm terrified that JITX will get into the LLM / Generative AI for boards business. (Don't make me homeless, Duncan!) They are already far ahead of many others with respect to next generation EE CAD. Judicious application of AI would be a big win for them. Edit: adding \"TL;DRN'T\" to my vocabulary XD reply AdamH12113 1 hour agoprevThe conclusions are very optimistic given the results. The LLMs: * Failed to properly understand and respond to the requirements for component selection, which were already pretty generic. * Succeeded in parsing the pinout for an IC but produced an incomplete footprint with incorrect dimensions. * Added extra components to a parsed reference schematic. * Produced very basic errors in a description of filter topologies and chose the wrong one given the requirements. * Generated utterly broken schematics for several simple circuits, with missing connections and aggressively-incorrect placement of decoupling capacitors. Any one of these failures, individually, would break the entire design. The article's conclusion for this section buries the lede slightly: > The AI generated circuit was three times the cost and size of the design created by that expert engineer at TI. It is also missing many of the necessary connections. Cost and size are irrelevant if the design doesn't work. LLMs aren't a third as good as a human at this task, they just fail. The LLMs do much better converting high-level requirements into (very) high-level source code. This make sense (it's fundamentally a language task), but also isn't very useful. Turning \"I need an inverting amplifier with a gain of 20\" into \"amp = inverting_amplifier('amp1', gain=-20.0)\" is pretty trivial. The fact that LLMs apparently perform better if you literally offer them a cookie is, uh... something. reply doe_eyes 25 minutes agoparentYes, this seemed pretty striking to me: the author clearly wanted the LLM to perform well. It started with a problem for which solutions are pretty much already available, and then providing a pretty favorable take on the model's mistakes. But the bottom line is that it's a problem that a novice could have solved with a Google search or two, and the LLM failed to deliver on in ways that would difficult for a non-expert to spot and rectify. LLMs are generally pretty good at information retrieval, so it's quite disappointing. The cookie thing... well, they learn statistical patterns. People on the internet often try harder if there is a quid-pro-quo, so they copy that, and it slips past RLHF because \"performs as well with or without a cookie\" is probably not one of the things they optimize for. reply neltnerb 52 minutes agoparentprevI think the only bit that looked handy in there would be if it could parse PDF datasheets and help you sort them by some hidden parameter. If I give it 100 datasheets for microphones it really should be able to sort them by mechanical height. Maybe I'm too optimistic. The number of times I've had to entirely redo a circuit because of one misplaced connection, yeah, none of those circuits worked for any price before I fixed every single error. reply lemonlime0x3C33 1 hour agoparentprevthank you for summarizing the results, I feel much better about my job security. Now if AI could make a competent auto router for fine pitch BGA components that would be really nice :) reply surfingdino 35 minutes agoprev [–] Look! You can design thousands of shit appliances at scale! /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Generative AI models (GPT-4o, Claude 3 Opus, Gemini 1.5) were tested for their utility in circuit board design, showing strengths in data extraction and code writing but weaknesses in nuanced design tasks.",
      "Claude 3 Opus excelled in explaining basic concepts, while Gemini 1.5 was most effective in parsing datasheets and creating accurate pin tables and footprints.",
      "All models struggled with specific part recommendations and detailed circuit design tasks, indicating that LLMs are better suited to assist human experts rather than act as standalone designers."
    ],
    "commentSummary": [
      "Generative AI, specifically zero-shot Large Language Models (LLMs), struggle with complex tasks like circuit board design, highlighting their limitations in specialized domains.",
      "Fine-tuning LLMs on specific tasks, such as netlist creation, could improve their performance, but a fundamental shift in AI structure might be necessary for more complex tasks.",
      "Diffusion-based generative structures and other AI models like evolutionary or reinforcement learning might be better suited for intricate tasks in electrical engineering (EE)."
    ],
    "points": 125,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1718986591
  },
  {
    "id": 40744098,
    "title": "Generating audio for video",
    "originLink": "https://deepmind.google/discover/blog/generating-audio-for-video/",
    "originBody": "Research Generating audio for video Published 17 June 2024 Authors Generative Media team Share Video-to-audio research uses video pixels and text prompts to generate rich soundtracks Video generation models are advancing at an incredible pace, but many current systems can only generate silent output. One of the next major steps toward bringing generated movies to life is creating soundtracks for these silent videos. Today, we're sharing progress on our video-to-audio (V2A) technology, which makes synchronized audiovisual generation possible. V2A combines video pixels with natural language text prompts to generate rich soundscapes for the on-screen action. Our V2A technology is pairable with video generation models like Veo to create shots with a dramatic score, realistic sound effects or dialogue that matches the characters and tone of a video. It can also generate soundtracks for a range of traditional footage, including archival material, silent films and more — opening a wider range of creative opportunities. Watch Prompt for audio: Cinematic, thriller, horror film, music, tension, ambience, footsteps on concrete Watch Prompt for audio: Cute baby dinosaur chirps, jungle ambience, egg cracking Watch Prompt for audio: Jellyfish pulsating under water, marine life, ocean Watch Prompt for audio: A drummer on a stage at a concert surrounded by flashing lights and a cheering crowd Watch Prompt for audio: Cars skidding, car engine throttling, angelic electronic music Watch Prompt for audio: A slow mellow harmonica plays as the sun goes down on the prairie Watch Prompt for audio: Wolf howling at the moon Enhanced creative control Importantly, V2A can generate an unlimited number of soundtracks for any video input. Optionally, a ‘positive prompt’ can be defined to guide the generated output toward desired sounds, or a ‘negative prompt’ to guide it away from undesired sounds. This flexibility gives users more control over V2A’s audio output, making it possible to rapidly experiment with different audio outputs and choose the best match. Watch Prompt for audio: A spaceship hurtles through the vastness of space, stars streaking past it, high speed, Sci-fi Watch Prompt for audio: Ethereal cello atmosphere Watch Prompt for audio: A spaceship hurtles through the vastness of space, stars streaking past it, high speed, Sci-fi How it works We experimented with autoregressive and diffusion approaches to discover the most scalable AI architecture, and the diffusion-based approach for audio generation gave the most realistic and compelling results for synchronizing video and audio information. Our V2A system starts by encoding video input into a compressed representation. Then, the diffusion model iteratively refines the audio from random noise. This process is guided by the visual input and natural language prompts given to generate synchronized, realistic audio that closely aligns with the prompt. Finally, the audio output is decoded, turned into an audio waveform and combined with the video data. Diagram of our V2A system, taking video pixel and audio prompt input to generate an audio waveform synchronized to the underlying video. First, V2A encodes the video and audio prompt input and iteratively runs it through the diffusion model. Then it generates compressed audio, which is decoded into an audio waveform. To generate higher quality audio and add the ability to guide the model towards generating specific sounds, we added more information to the training process, including AI-generated annotations with detailed descriptions of sound and transcripts of spoken dialogue. By training on video, audio and the additional annotations, our technology learns to associate specific audio events with various visual scenes, while responding to the information provided in the annotations or transcripts. Further research underway Our research stands out from existing video-to-audio solutions because it can understand raw pixels and adding a text prompt is optional. Also, the system doesn't need manual alignment of the generated sound with the video, which involves tediously adjusting different elements of sounds, visuals and timings. Watch Watch Still, there are a number of other limitations we’re trying to address and further research is underway. Since the quality of the audio output is dependent on the quality of the video input, artifacts or distortions in the video, which are outside the model’s training distribution, can lead to a noticeable drop in audio quality. We’re also improving lip synchronization for videos that involve speech. V2A attempts to generate speech from the input transcripts and synchronize it with characters' lip movements. But the paired video generation model may not be conditioned on transcripts. This creates a mismatch, often resulting in uncanny lip-syncing, as the video model doesn’t generate mouth movements that match the transcript. Watch Prompt for audio: Music, Transcript: “this turkey looks amazing, I’m so hungry” Our commitment to safety and transparency We’re committed to developing and deploying AI technologies responsibly. To make sure our V2A technology can have a positive impact on the creative community, we’re gathering diverse perspectives and insights from leading creators and filmmakers, and using this valuable feedback to inform our ongoing research and development. We’ve also incorporated our SynthID toolkit into our V2A research to watermark all AI-generated content to help safeguard against the potential for misuse of this technology. Before we consider opening access to it to the wider public, our V2A technology will undergo rigorous safety assessments and testing. Initial results are showing this technology will become a promising approach for bringing generated movies to life. Note: All examples are generated by our V2A technology, which is paired with Veo, our most capable generative video model. Learn about Veo Explore our SynthID toolkit This work was made possible by the contributions of: Ankush Gupta, Nick Pezzotti, Pavel Khrushkov, Tobenna Peter Igwe, Kazuya Kawakami, Mateusz Malinowski, Jacob Kelly, Yan Wu, Xinyu Wang, Abhishek Sharma, Ali Razavi, Eric Lau, Serena Zhang, Brendan Shillingford, Yelin Kim, Eleni Shaw, Signe Nørly, Andeep Toor, Irina Blok, Gregory Shaw, Pen Li, Scott Wisdom, Aren Jansen, Zalán Borsos, Brian McWilliams, Salah Zaiem, Marco Tagliasacchi, Ron Weiss, Manoj Plakal, Hakan Erdogan, John Hershey, Jeff Donahue, Vivek Kumar, and Matt Sharifi. We extend our gratitude to Benigno Uria, Björn Winckler, Charlie Nash, Conor Durkan, Cătălina Cangea, David Ding, Dawid Górny, Drew Jaegle, Ethan Manilow, Evgeny Gladchenko, Felix Riedel, Florian Stimberg, Henna Nandwani, Jakob Bauer, Junlin Zhang, Luis C. Cobo, Mahyar Bordbar, Miaosen Wang, Mikołaj Bińkowski, Sander Dieleman, Will Grathwohl, Yaroslav Ganin, Yusuf Aytar, and Yury Sulsky. Special thanks to Aäron van den Oord, Andrew Zisserman, Tom Hume, RJ Mical, Douglas Eck, Nando de Freitas, Oriol Vinyals, Eli Collins, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the research process. We also acknowledge the many other individuals who contributed across Google DeepMind and our partners at Google. Related posts View all posts Veo Our most capable generative video model. SynthID Robust and scalable tool for watermarking and identifying AI-generated images.",
    "commentLink": "https://news.ycombinator.com/item?id=40744098",
    "commentBody": "Generating audio for video (deepmind.google)118 points by rvnx 20 hours agohidepastfavorite35 comments crazygringo 14 hours agoVery very cool. But I literally can't keep track anymore of which AI generative combinations of modalities have been released. Crazy how two years ago this would have blown my mind. Now it's just, OK sure add it to the pile... reply lemoncookiechip 6 hours agoparentMaybe this can help you keep track of stuff: https://www.tools-ai.online/ https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRj... And here's some that I personally recommend and are \"free\" to use: TXT2VID / IMG2VID: https://lumalabs.ai/dream-machine TXT2MUSIC: https://suno.com/ AI TXT2SPEECH: https://murf.ai/ PDF Summarize (You can just use 4o or Claude though: https://askyourpdf.com/ AI ChatBot: https://janitorai.com/ https://www.chub.ai/ TXT2IMG / IMG2IMG: https://playground.com/ Obviously SD 1.5/SDXL/Pony and so much more. reply astennumero 1 hour agoparentprevI was just thinking the same. Can't believe I'm not excited. reply xwolfi 9 hours agoparentprevI still havent spend a dollar on any of it... reply crazygringo 50 minutes agorootparentWell OpenAI's annual revenue is more than $1.6 billion, so it doesn't really matter if you haven't. Tons -- and I mean tons -- of people have spent money on it. Because it's worth it, it's generating actual economic value for them. reply TacticalCoder 3 hours agorootparentprev> I still havent spend a dollar on any of it... Subscribed to GTP-4o (or whatever the paying one is called) for translating / finding typos / summarizing / etc. Zero brand love and I'll switch to something else (maybe some future Claude model?) the second something better/faster comes out. reply gundmc 15 hours agoprevThe AI slop problem is bad enough on TikTok/YouTube today. I shudder at the future of user-generated video platforms. I also wonder if the low barrier to create these videos will outpace the storage and processing capacity of the free platforms. reply vineyardmike 12 hours agoparentI've long proposed that we should have an \"AI Instagram\" where different tweaked personas (perfected via A/B testing/Genetic algorithms) are displayed to users with ai-generated images/posts/comments. Each persona set is specific to each user, and they don't have other IRL users that they can interact with. The user can interact with the personas, and even message them. The developer can add more features over time (stories, short form video, etc) as people get bored and technology formats improve, but it's unlimited content. It's perfect for advertising, because you can embed products and ads seamlessly and generate them alongside everything else. That said, storage is far cheaper than GPUs at the moment. reply aaalll 16 minutes agorootparentThere is an AI reddit https://chirper.ai/ reply squarefoot 1 hour agorootparentprevThis is probably already researched today, and it seems close to how people would interact with clones of their deceased relatives or famous people of the past. However it's also a powerful tool to create nearly 100% successful influencing by instructing each persona to subtly inject the same idea into its human user by employing the most convincing tactics needed for that user. It's quite easy to foresee the use in advertising, where it would completely redefine the word \"targeted\", but also corrupt politics. reply xwolfi 9 hours agorootparentprevHave you tried AI porn ? There's something in the fact it's fake uncanny characters that makes it non-exciting. Like, jerking off to a toaster basically, and I assume it'd be the same for a social network with no human ? reply mrtesthah 14 hours agoparentprevyoutube should just offer to generate the videos for you directly to save space. reply squarefoot 1 hour agorootparent> youtube should just offer to generate the videos for you directly to save space. Try imagining this concept applied to newscasts. reply mrtesthah 5 minutes agorootparentOh, but isn't that what people want -- to live in a reality that confirms 100% of their pre-existing biases with no risk of cognitive dissonance? You're leaving money on the table by ignoring this opportunity! Move fast and break things! reply ElFitz 13 hours agorootparentprevAbsolutely. Using a recommendation algorithm similar to TikTok’s, learn what each specific user are into, and instead of showing content produced by other users, produce custom-tailored content on the fly, perfectly matching the type, tone, style, length, and rhythm each user likes. Ideally without making anything up. reply mrtesthah 3 minutes agorootparentPerfect. Once the models are adequately trained, we can do away with the entire \"content creator\" economy altogether! reply yreg 11 hours agorootparentprev> Using a recommendation algorithm similar to TikTok’s How is TTs recommendation system different from YT? Other than suggesting lower quality content that's irresistable? reply hooverd 5 hours agorootparentprevWhy? Platforms are already bad enough about just suggesting what they think I might like. reply shzhdbi09gv8ioi 10 hours agorootparentprev> Ideally without making anything up. I have no idea anymore if this is sarcasm or a straight up belief. What serious professional would gamble on hallucinations? reply masto 1 hour agoprevI don't know if a computer can ever match the perfection of \"shreds\" videos. (The drum example came close) https://www.youtube.com/playlist?list=PLQvwVDViTLXu4usHto8PH... reply TheAceOfHearts 15 hours agoprevWouldn't it be better to generate multiple tracks that can be mixed / tweaked together, rather than a single track? That way you can also keep the parts you like and continue iterating on the parts you dislike. If the sound is already being generated at a specific time, surely you can make it generate an output that can be consumed by existing audio mixing tools for further refinement. The problem with doing these all-in-one integrated solutions is that you're kinda giving people an all-or-nothing option, which doesn't seem that useful. Maybe I'll end up being proven wrong. reply the_other 3 hours agoparent> Wouldn't it be better to generate multiple tracks that can be mixed / tweaked together, rather than a single track? That way you can also keep the parts you like and continue iterating on the parts you dislike. That'd interest me (a musical hobbyist) more than the \"whole track\" generators, for sure. I imagine it's a harder task tho'. Presumably, if you give the same source material (video, prompt) to the AI multiple times, it will generate different pieces of music. So if you do a series of prompts, each one specifying a different instrument or group/bus, then you (or the AI) need to arrange for the parts to blend correctly, follow the same cues and assemble to a coherent arrangement. Is that one pass with multiple outputs, or multiple passes/prompts with one output each? I have got the impression (from casual reading) that the music generators don't inherently \"know\" about different parts of a piece of music. They just know about the final output. reply TacticalCoder 3 hours agoparentprev> Wouldn't it be better to generate multiple tracks that can be mixed / tweaked together, rather than a single track? That way you can also keep the parts you like and continue iterating on the parts you dislike. Totally and that is 100% what is coming. For a great many pictures too: why generate a picture full of lightning issues / approximation when you'll soon be able to generate and entire 3D scene and render it properly. We've mastered 3D rendering and audio engineering. I want the 3D models and the 3D scenes. I want the individual tracks (and combine them in Dobly Atmos or whatever shall be cool). And that is coming, no question about it. reply anigbrowl 14 hours agoparentprevYes, same problem as with commercial AI music products not providing stems or MIDI, The engineers on these products are too full of themselves to actually ask anyone in the field what they want, so we just keep getting these stupid magic 8 ball efforts. This one is particularly annoying as I worked for years as a sound engineer and have recorded or produced the soundtrack for 10 feature films and some large number of shorts. What's going to happen with this is directors or producers are gonna do this at home for every scene in a burst of over-enthusiasm, realize the totality is Not Great, and then demand someone like me fix it, but for 1/4 of what the job used to pay, arguing 'but most of the work is already done'. It's all so tiresome. reply Jensson 11 hours agorootparentSame reason you don't see AI making images in layers etc, its just much easier to train an AI that generate everything in one layer. Training a model with the same level of quality output that generates multiple layers is much much harder, and of course companies and users prefers the higher quality over having layers, especially since the quality you get with a single layer is still barely passable. reply j16sdiz 13 hours agorootparentprevThe sample they used for training are mixed. Unless they can have enough raw, unmixed sample, this depends on how well they \"unmix\" them. reply anigbrowl 1 hour agorootparentYes...that's the problem. A problem that could be easily avoided by asking existing professionals what matters and what tools they actually want. reply knowaveragejoe 4 hours agorootparentprevIt sounds like between the two of you(and the person who mentioned generating images in layers for image editing software), you've stumbled upon an obvious gap in the market. reply cageface 14 hours agorootparentprevI’ve tried to explain this to several friends. Until these tools can generate output that can be mixed properly they’re going to be very niche. reply tkgally 14 hours agoparentprevElevenLabs just released something that is more controllable: https://news.ycombinator.com/item?id=40736536 reply bryanrasmussen 14 hours agoparentprevthe AI Musical IF This Then That Step 2 > https://www.lalal.ai/ \"Extract vocal, accompaniment and various instruments from any audio and video\" reply chaosprint 9 hours agoparentprevit's limited by the mechanism of diffusion. reply peppertree 2 hours agoprevI wonder if this can be trained to do lip reading. reply squarefoot 1 hour agoprevAs a wannabe drummer i can say the drumming example is quite bad as the drummer doesn't seem to hit toms that often to produce tom rolls, however the video is so heavily cropped that either I'm wrong or the AI was deliberately fed with something difficult to interpret. reply nanovision 5 hours agoprev [–] This is so cool. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Video-to-audio (V2A) technology generates synchronized soundtracks from video pixels and text prompts, enabling the creation of dramatic scores, realistic sound effects, or dialogue for various video types.",
      "V2A employs a diffusion-based approach, encoding video input and refining audio from random noise to produce realistic audio waveforms, with ongoing research focusing on improving video quality artifacts and lip synchronization.",
      "The development team emphasizes responsible AI practices, using the SynthID toolkit for watermarking AI-generated content and conducting rigorous safety assessments before public release."
    ],
    "commentSummary": [
      "DeepMind has introduced a new AI tool for generating audio for videos, adding to the growing list of AI generative tools.",
      "The community is expressing mixed feelings, with some finding it hard to keep up with the rapid advancements and others discussing the potential impacts on content creation and storage capacities.",
      "There is a notable interest in how AI-generated content could influence advertising, politics, and the future of content creation, with suggestions for AI-specific platforms and tools."
    ],
    "points": 118,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1718922192
  },
  {
    "id": 40749624,
    "title": "Spending 3 months investigating a 7-year old bug and fixing it in 1 line of code",
    "originLink": "https://lemmy.world/post/16763534",
    "originBody": "@ch00f to Programming@programming.dev • 10 hours ago The time I spent three months investigating a 7-year old bug and fixed it in 1 line of code 23 313 1",
    "commentLink": "https://news.ycombinator.com/item?id=40749624",
    "commentBody": "Spending 3 months investigating a 7-year old bug and fixing it in 1 line of code (lemmy.world)114 points by asicsp 5 hours agohidepastfavorite27 comments TehShrike 3 hours agoI particularly liked this part: > Knowing very little about USB audio processing, but having cut my teeth in college on 8-bit 8051 processors, I knew what kind of functions tended to be slow. I did a Ctrl+F for “%” and found a 16-bit modulo right in the audio processing code. That feeling of saving days of work because you remember a clue from previous experience is so good. reply djoldman 3 hours agoparentThose are the times one gets the opposite of imposter syndrome. reply ASalazarMX 1 hour agorootparentFortunately it's a temporal state, otherwise there's risk of entering the Dunning-Kruger effect. \"You did awesome, but don't let it go to your head.\" reply dylan604 15 minutes agorootparentF-that! That's one of those times where I re-enact the scene from the Bond Golden Eye film where the guy jumps up extending both arms yelling \"Yes! I am invincible!\" Of course I totally expect the hubris to be short lived, just maybe not with liquid nitrogen https://www.youtube.com/watch?v=fXW02XmBGQw reply nikanj 3 hours agoparentprevThis essentially is why senior engineers get much bigger salaries reply mulmen 1 hour agorootparentAs a total newbie I saved my company a quarter million dollars in Oracle licensing in a single afternoon by rewriting a PL/SQL function. That change was a few lines of SQL. Seniors don’t have a monopoly on good ideas. Salary is driven by market conditions and nothing else. It is not an approximation of merit or even delivered value. reply close04 55 minutes agorootparentStatistically speaking a senior (more experienced) engineer is more likely to consistently deliver time saving results, while a junior is more likely to occasionally do it, if ever. Proving it’s not a one time thing is what pushes you in the salary and seniority ranking. reply brogrammernot 2 hours agoprevThis exact type of thing is why when I switched to the dark side (product) and sat in management meetings where often non-technical folks would go “we could measure by lines of code or similar” for productivity I often pointed out how that was a bad idea. Did I win? Of course not, it’s hard for non-technical people to fully appreciate these things and any sort of larger infrastructure work, esp for developer productivity because it goes back to well how you going to measure that ROI. Anyways, this was fun to read and brought back good engineering memories. I’d also like to say, as it brought back a bug I chased forever, fuck you channelfactory in c#. reply Swizec 40 minutes agoparentHave you ever suggested that management/leadership should measure productivity by lines of document text written? They might better grok how that’s a bad idea. Especially since many of them much prefer to communicate in bullet-pointed slides than documents. reply jonathanlydall 1 hour agoparentprevI really miss working with WCF, said no one ever. reply neonsunset 1 hour agoparentprevTroubleshooting vendor WCF SDK version mismatch was not fun, and the guy who had to reverse engineer it to attempt a .NET Core port probably lost a few years off his lifespan (this was before CoreWCF was a thing). When people bash gRPC today, they don't know of the horrors of the past. reply brogrammernot 5 minutes agorootparentYeah, I’ve lived the life of straddling .NET Core and ASP.NET while also dealing with React vs Angular2+ and having half of the system in the script bundling hell that was razor views and all sorts of craziness. That experience is actually what led me to switch over to Product among other things, I get it when people joke (half joke) about considering retirement rather than going through that again. reply pelagicAustral 3 hours agoprevSome of the stuff I've struggled with the most over the years have been SQL constraints that are not documented. I remember (probably like 10 years ago), I deployed an update to an ancient Windows Forms implementation that deprecated some login and instead made use of Windows Authentication. It worked like a charm for all users, but one! Checked everything, replicated the machine, tried so many weird stuff, and in the end, what was happening is that the \"Users\" table had a constraint in the number of characters for the username. This username was over the limit and was not being validated... Another one was a report that was giving the wrong amount, but getting the data from database seemed to do the math right... it was the damn Money datatype, changed to decimal, done... reply creeble 2 hours agoprevHa, coincidentally, I designed and built an 8051-based MIDI switch in the early 90’s. There weren’t that many good tools at the time, and I designed everything from the software and UI to the circuit board and rack-mount case. I even wrote an 8051 assembler in C, but found a good tiny-C compiler for it before it went into production. You are not a programmer unless you’ve written key-debounce code :) (OTOH, some of the worst programmers I’ve ever had the displeasure of working with were amazing low-level code hackers. In olden times, it seems like you were either good at that level of abstraction, or you were good at a much different [“higher”] level, seldom both.) reply winrid 2 hours agoprevReminds me of fixing an ~11yr old bug in Enemy Territory. I had to spend a night debugging the C code only to realize the issue was in the UI config: https://github.com/etlegacy/etlegacy-deprecated/pull/100/fil... (IIRC UI scrolled twice for every mouse movement + you couldn't select items in server browser with mouse wheel as it would skip every other one) reply lostlogin 1 hour agoparentThat was such a great game but sadly it seemed to fizzle out. There were lots of neat exploits which made it even better. I also liked the communication style, with pre canned message you could give with certain key combos. reply halifaxbeard 1 hour agoprevReminds me of a bug I fixed in yamux, simply because of how long I've had to deal with it. Bug existed for as long as yamux did. (yamux is used by hashicorp for stream muxing everywhere in their products.) If yamux's keepalive fails/times out, and you're calling Read on a demuxed stream, it blocks forever. https://github.com/hashicorp/yamux/pull/127 reply pvaldes 2 hours agoprev\"I also ended up needing to find a Perl script that was buried deep in some university website. I still don’t know anything about Perl, but I got it to run\" Find dusty Perl script forgotten for years. Still works Not the first time that I hear that reply nikanj 2 hours agoparentOutside of javascript, it’s a pretty reasonable assumption that if you have the sources, you can get them to run reply omoikane 57 minutes agoprev> given a fixed denominator, any 16-bit modulo can be rewritten as three 8-bit modulos Anybody know what's the exact transformation here? I searched around and found this answer, but it doesn't work: https://stackoverflow.com/a/10441333 reply tommiegannert 1 hour agoprevKudos also to the original author for not doing premature optimization, of course. It wasn't until the iPad that it was needed. However, a TODO might have been useful. ;) reply rented_mule 1 hour agoprevThe worst I experienced in this direction was also on a consumer device about 15 years ago. Performance was degraded and we couldn't explain it. A team of 5 of us was assembled to figure it out. We spent over three months on it before finding a root cause. It was over two months before we could even understand how to measure it - we were seeing parts of the automated overnight test suite run taking longer, but every night it would be different tests that were slow. A key finding was that almost everything was slow on some boots of the device and fast on other boots of the device, and there was a reboot before each test was run. Doing some manual testing showed it being close to a 50% chance of a boot leading to slowness. Now what? I eventually got frustrated and took the brute force / mindless approach... binary search over commits. Unfortunately, that wasn't easy because our build was 45-60 minutes, and then there was a heavily manual installation process that took 10-20 minutes, followed by several reboots to see if anything was slow. And there were several thousand commits since the last known good build (the previously shipped version of the device). The build/install/testing process was not easily automated, and we were not on git, otherwise using git-bisect would have been nice. Instead, I spent weeks doing the binary search manually. That yielded the offending commit. The problem was that it was a massive commit (tens of thousands of lines of code) from a group in another part of the company. It was a snapshot of all of their development over the course of a couple of years. The commit message, and the authors, stated that the commit was a no-op with everything behind a disabled feature flag. So now it was onto code level binary search. Keep deleting about half of the code in the commit, in this case by chunks that are intended to be inactive. After eventually deleting all the inactive code, there were still a few dozen lines of changes in a Linux subsystem that did window compositing. Those lines of code were all quite interdependent, so it was hard to delete much and keep things functional, so now on to walking through code. At least I could use my brain again! Using the clue that the problem was happening about half the time and given that this code was in C, I started looking for uninitialized booleans. Sure enough, there was one called something like `enable_transparency`. Disabled code was setting it to `true`, but nothing was setting it to `false` when their system was disabled. Before their commit, there was no variable - `false` was being passed into the initializer call directly. Adding `= false` to the declaration was the fix. So, well over a year of engineering hours spent to figure out the issue. The upside is that some people on the team didn't know how to proceed, so they spent their time speeding up random things that were slow. So the device ended up being noticeably faster when we were done. But it was pretty stressful as we were closing in on our launch date with little visibility into whether we'd figure it out or not. reply m3kw9 4 hours agoprevThese one line fix always seem like a stupid bug , but in reality most bugs are like this and the fix is in the discovery reply xeromal 2 hours agoparentOne of the reasons I struggle to give ETAs on fixing a bug. The moment I know what the issue is, the solution to fix it is usually already figured out barring a rearchitecture of some services or infrastructure. reply readthenotes1 3 hours agoprev\"...it was based on a USB product we had already been making for PCs for almost a decade. This product was so old in fact that nobody knew how to compile the source code. \" I think you mean \"Management was so bad, nobody knew how to compile the source code\". There are plenty of systems out there that can and and plenty that cannot be reproduced from source. The biggest difference is the card taken to do so, not the age. reply magwa101 2 hours agoprevSimilarly, I spent 6 weeks on a kernel token-ring driver intermittent initialization issue. This required kernel restarts over and over to observe the issue. Breakpoints were useless as they hid the issue. Turns out initialization in a specific step was not synchronous and reading the status was a race condition. It tooks weeks of staring, joking around, thinking, bs'ing, then suddenly, voila. Changed the order of the code, worked. reply shermantanktop 3 hours agoprev [–] This kind of bug is always an emotional rollercoaster of anticipation, discovery, disappointment, angst, self-criticality, and satisfaction. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A developer spent three months investigating a bug that had been unresolved for seven years.",
      "The bug was ultimately fixed with just one line of code, highlighting the sometimes elusive nature of software issues.",
      "This post underscores the persistence required in software development and the potential simplicity of solutions to long-standing problems."
    ],
    "commentSummary": [
      "A 7-year-old bug was fixed with a single line of code after a 3-month investigation, highlighting the complexity and unpredictability of debugging.",
      "The discovery was made by identifying a 16-bit modulo operation in the audio processing code, leveraging previous experience with 8-bit processors.",
      "The post emphasizes the emotional journey and satisfaction of solving long-standing technical issues, resonating with both junior and senior engineers."
    ],
    "points": 115,
    "commentCount": 27,
    "retryCount": 0,
    "time": 1718977795
  },
  {
    "id": 40746173,
    "title": "A Rant about Front-end Development",
    "originLink": "https://blog.frankmtaylor.com/2024/06/20/a-rant-about-front-end-development/",
    "originBody": "June 20, 2024 by paceaux A Rant about Front-end Development Reading Time: 14 minutes I am a front-end developer who is FED up about front-end development. If you write front-end, this isn’t about you personally. It’s about how your choices make me angry. Also this is about how my choices have made me angry. Also this is mostly just about choices, the technologies are incidental. Note: The views expressed in here are not those of my employer. They are (probably) the views of thousands of people just like me. Warning: This article is filled with profanity. Ask your scrummaster if 8 points of profanity is too much to handle today. Quoth the raven on your merge requests: Nevermore First, some background I have neither a computer science degree nor an engineering degree. But I have worked in web technologies since the second Bush administration. I’ve worked in a variety of roles ranging from business analyst, tester, all the way through developer, consultant, and principal architect. I did agency work for 11 years before making the choice to work for a very large tech company. I have worked across sectors like insurance, healthcare, retail, banking, investing, marketing, and manufacturing. I have worked with global brands which are household names. I have written a lot of front-end code for a lot of companies. I have also dealt with a lot of consequences created by front-end code. My criticisms come from my role as a front-end developer and as someone affected by a front-end developer. No one gives a shit about content Unless it’s a content-specialized front-end developer who works specifically in the content management space and is specialized in writing front-end for content management system. Did you count how many times I had to write both “front-end” and “content”? Too fucking many. I have mentored quite a few bootcamp graduates. There are exactly zero graduates of zero bootcamps who have ever been told, “users go to websites for the content.” Zero. They all think it’s their moral imperative to make sure every site is built in React. I have worked with exactly zero computer science graduates who have ever heard the phrase, “content before code”. Unless the front-end developer has been forced by threat of not-being-able-to-pay-rent to care about content, they don’t fucking care. This is wild to me because HTML5 semantics exist and their whole-ass raison d’être is, in fact, having an understanding of content. You can’t write content-managed HTML if you don’t understand the content. You can’t write semantic HTML if you don’t understand the content. You can’t write SEO-ready HTML without understanding the content. You can’t write accessible HTML without understanding the content. You can’t write good HTML without understanding the content. Zero SPA developers that I’ve ever met have ever given a shit about using ainstead of a . I have found NaN fucks given about using a p over a div. The number of times I’ve seen numbers written inside of athat’s inside of a— instead of just using an— is deeply disturbing. Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter1. Run this through an HTML Validator 2. Weep publicly1. Run this through an HTML Validator 2. Weep publicly1. Run this through an HTML Validator 2. Weep publicly Maybe we should ask the designer or business owner a few questions about what the design means to the end-user before hammering out 18 layers of divs. CSS is fine; you’re the problem If someone gets a ticket for speeding, we don’t blame the car or the road. We definitely blame the idiot who thought it’d be a good idea to race the Dodge Charger camped out in the Dollar General parking lot. It’s in this very same vein that we don’t blame CSS: “The problem with CSS is that the scope is global” “The problem with CSS is the cascade” “The problem with CSS is that it fails silently” Every dork who promotes writing styles in JavaScript Those aren’t problems; they’re features. They are solutions to the very specific problems of, “how do we make content presentable as easily as possible, with as little duplication as possible, and with as few negative impacts to the user as possible.” Chances are, the things you don’t like about CSS are the things you haven’t bothered to understand about it. Styles rarely, if ever need to actually be scoped and encapsulated. In fact, the only time you want to prevent styles from being in the global scope is when you didn’t write them and can’t control them. If duplicated class names are ruining your day, 15-class selectors are crapping in your cereal, and 8 !important are igniting turds in your garage, that’s actually an architecture problem. Are there real problems in CSS? Of course. But those are problems like, “border-radius should have been called corner-radius,” and “atomic stacking contexts are generated when using opacity,” and “an em isn’t really an em.” Those are problems. The C of CSS is not a problem. You’re the problem. Put on your big-kid pants and develop a strategy, pick some conventions, write them down, and enforce them through merge requests and peer reviews. Server-side rendering does not mean, “rendered with JavaScript” There’s a disconcerting number of front-end developers out there who act like it wasn’t possible to generate HTML on a server prior to 20101. They talk about SSR only in the context of Node.js and seem to have no clue that people started working on this problem when season 5 of Seinfeld was on air2. Server-side rendering was not invented with Node. What Node brought to the table was the convenience of writing your shitty div soup in the very same language that was invented in 10 days for the sole purpose of pissing off Java devs everywhere. Server-side rendering means it’s rendered on the fucking server. You can do that with PHP, ASP, JSP, Ruby, Python, Perl, CGI, and hell, R. You can server-side render a page in Lua if you want. Just because Node isn’t involved in generating HTML, that doesn’t mean it’s not possible to generate markup at request time. It is possible. It’s been possible. It will continue to be possible. Stop denying the history, contributions, and value that other technologies bring to solving this exact problem that is definitely not unique to you. JavaScript is not always the answer My brothers and sisters in Christ I want you to know that I care about your souls enough to share these truths with you: You don’t need JavaScript to make a web page. You don’t need JavaScript to write styles. You don’t need JavaScript to make an animation. You don’t need JavaScript just to show content. In 2010 I was a contractor for an organization working with some Very Smart IBM consultants. They could not figure out how our in-house developer made the navigation for the site they were implementing. It was because our developer used CSS. Our consultants had never considered that CSS could do something as fancy as, “show you a thing when you hover over it.” That’s a fun “har-har silly ignorant smart consultants,” story except at least it’s about their harmless ignorance. It’s so much worse when they know JavaScript. Five years ago I was on a project where the perfectly fine .net MVC app was turned into an API for delivering content to a .net core app that used node services to render Vue templates into fully rendered HTML which meant that all of our templates were written in Vue and we still had to write jQuery to make content interactive. We literally had two layers of JavaScript wrapped in a .net sandwich with JavaScript spread on the top. I’m seeing architectures being proposed today that do pretty much the same thing I just described but with Drupal on Apache, containerized Node, and Web Components. If making a peanut butter and jelly sandwich by spreading the jelly on both sides of the bread is disturbing to you, good. You can still find God. Do you have any idea how frustrating it is that that in order to explain my sadness to my therapist I must first explain like 5 different technologies and by the time I’m finished she’s sad just hearing it, the session’s over, and I didn’t even get to what was making me upset? Technology has made my anger a recursive function. Can we please stop adding complexity to our systems just so we can do it in JavaScript? If you can do it without JavaScript, you probably should. Tools shouldn’t add complexity. Sass wasn’t just a mistake but a mistake factory 10 years ago I thought CSS preprocessors were the coolest thing ever and I was incredibly fucking dumb for thinking that. The only three good features CSS preprocessors gave were Variables Mixins Color transformation Every thing else was a mistake: Nesting Calc Functions, including control statements like if and for Interpolation Data types of any kind The entire module thing Nested CSS obscures complexity I hate this feature so much. I hate it more that it’s now native to CSS because not only has it spread, but it’s mutated 3. And not in a good, ninja-turtley way. 7 Nesting obscures complexity and then makes it impossible to find or fix. Without using a compiler or psychedelic of any kind, can you tell me what this generates? Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter .feature { color: blue; &__header { p & { a { &:hover, &:focus { color: red; } } } } } .feature { color: blue; &__header { p & { a { &:hover, &:focus { color: red; } } } } } .feature {color: blue;&__header { p & {a { &:hover, &:focus {color: red; }} }} } Does this seem reasonable to you? Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter .feature { color: blue; } p .feature__header a:hover, p .feature__header a:focus { color: red; } .feature { color: blue; } p .feature__header a:hover, p .feature__header a:focus { color: red; } .feature { color: blue; } p .feature__header a:hover, p .feature__header a:focus { color: red; } If that’s the output you expected, either you’re one of the Sass spec writers or you did a double-shot of espresso followed by a Ritalin chaser. Nesting is not good. It hides the specificity that you’re producing and will lend to increasingly fragile code as you try to override a 13 part selector. It was useful for clumping pseudo-selector and pseudo-elements and after that it out-of-hand, making code that’s harder and harder to read, and even find in your codebase. Interpolation, et all This was also just a bad idea that we couldn’t have predicted at the time. I’m not mad at the Sass people for this. Hindsight is 20/20 and all. What I want is for it to all stop. This is fundamentally a bad thing: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter $sizes: 10px, 20px, 30px, 50px; @each $size in $sizes { .icon-#{$size} { font-size: $size; height: $size; width: $size; } } $sizes: 10px, 20px, 30px, 50px; @each $size in $sizes { .icon-#{$size} { font-size: $size; height: $size; width: $size; } } $sizes: 10px, 20px, 30px, 50px; @each $size in $sizes { .icon-#{$size} { font-size: $size; height: $size; width: $size; } } I know what you’re thinking. You’re thinking, “But Frank, it saves me on keystrokes! Now I can generate more classes and classnames just based on that list of sizes!” Bro. ctrl+c and ctrl+v exist. Are you a VS Code user? Even easier: Highlight it and press alt/opt + shift + ↓ , that’ll copy the chunk or line down. Then, highlight the part that’s repeated a few times (where you’d interpolate), press ctrl/cmd + d, and keep pressing until they’re all highlighted. Make your change. Now you’re done. What did you save? Time and sanity in looking up the Sass syntax? Future time where future you is trying to find where the fuck .icon-80px is living? XKCD is not on your side for this automagical icon generator loop that’s going to piss you off with edge cases for the next 5 years. Years ago I thought it was a great idea to create a mixin for CSS preprocessors to do my BEM stuff. It’s not. That removes traceability, which experience has taught me matters more than saved keystrokes. Can we please stop using languages and tech stacks to solve problems that our IDEs are capable of helping us with? Seriously … forget the Frameworks I was blown away when Angular came out. It was an actual thing that could do it all. It could render the markup with fun logicy attributes, respond to data changes, make the AJAX requests, amazing! I wanted to use Angular for all the interactive web apps. Then React came out. That was amazing. It could render the markup and … [incoherent mumbling] components or something… and … wait, that wasn’t even markup; it was JSX. Sure, I had to add a few libraries to really use it, but yay, I used it for interactive web apps. Then there was Vue. Then Preact. Then Ember, Svelte, Backbone, Alpine, Mithril, Solid, Lit, Stencil, Qwik… Maybe it’s because Angular was no one’s first choice — even though it came first. Maybe that’s why the front-end world lost its damned mind with React. React was so not-angulary that front-end folks were like, “let’s use it client side, server side, native-side, and the 3 other sides of a damned cube”. Folks decided it was a good idea to Reactify All The Things®. And that’s how React, a framework used to deliver interactive content to a whole 28% of the world’s population, is being used to generate static sites. I had a client a few years ago that was a household name. Their front-end was written in Gatsby. Gatsby is a static site generator that uses React. This globally-known brand had probably 50 websites in at least as many languages. But the content was very, very, static. So imagine the horror the front-end dorks felt when they learned that a content author in Bulgaria updating the terms and conditions didn’t want the whole-ass site rebuilt for every country and language. This would’ve been a non-problem had the web app been written in .net, Java, or, fuck me for saying it, PHP or Node. But nah. These goobers got high off npm modules and did shots of JSX in the bathroom at lunch time. They chose a framework used to deliver interactive content to a significant percentage of the planet for generating non-interactive content on a server. It took them weeks to come up with a solution to the problem they created when they chose a problem as a solution. Yes. I am absolutely calling React a problem. I’m calling Angular, Vue, and All the Rest of Them ™ a problem. Because unless you have a specific problem of highly interactive, data-driven content, you don’t need a framework. You don’t need a framework to render static content to the end user. Stop creating complex solutions to simple problems Also more generally fuck your complexity This, more than anything, is a rant about complexity. I am mad at how front-end developers create complex solutions to simple problems. I am mad that front-end developers use tools to obscure complexity. I am mad that front-end developers embrace complexity rather than challenge it. You know what’s great about jQuery? Everything. jQuery took several complex problems (browsers do things differently, selecting elements is complex, changing things on elements is tedious, AJAX requests are a lot) and it provided a simple, intuitive, and readable solution: Plain text Copy to clipboard Open code in new window EnlighterJS 3 Syntax Highlighter $('.getResults').on('click', () => { $.ajax({ url: '/api/results', data: { foo: 'bar' }, success: function (result) { $('.results').html(result); } }); }); $('.getResults').on('click', () => { $.ajax({ url: '/api/results', data: { foo: 'bar' }, success: function (result) { $('.results').html(result); } }); }); $('.getResults').on('click', () => { $.ajax({ url: '/api/results', data: { foo: 'bar' }, success: function (result) { $('.results').html(result); } }); }); Cool your jets for a second and think about how wonderfully simple that is. It handles finding your element in a way that lets you avoid the null checks, shortens your event binding, lets you make an AJAX request, lets you find the element that the result goes into, and plop it there if it’s successful. If the problem is, “user clicks on thing, you get data, and show data”, that is a beautiful and simple 11-line solution. It did not need NPM modules and a build system. And sure, maybe you don’t need jQuery, and that’s fine. Will you need more lines of code to do the same thing? Of course. But even without jQuery, your complexity doesn’t grow exponentially like it would if you brought in a framework with its build system, linters, compilers, and 700mb of Node Modules some of which inexplicably require some stupid shit like is-odd or is-git-url 4. This is not an Old Man Shouts at Clouds moment where I want the world to go back to jQuery. This is me trying to illustrate how jQuery solved many problems with simplicity and somehow we seem to have forgotten the value of just being simple. Pretend I’m not talking about JavaScript for a second. Let’s pretend it’s just an ordinary, everyday, 3-template web site. Maybe it’s managed by a CMS. Maybe it’s not. Does it really need Tailwind? Bootstrap? EvenSass? Is writing plain CSS really such a terrible thought? Let me fill you in on a cool fact about CSS: It doesn’t need a fucking compiler. No npm install -D sass. No npm i gulp. You get to tell WebPack to fuck off good and proper-like, the way the Good Lord Intended. 5 Simplicity is unsophisticated and lacking in many parts. The simplest solution to a problem is always the best solution. Choose simplicity. I’m begging you. Your future is begging you,. Oh, and enough with the new and shiny Sure it’s awesome that you can strongly type CSS variables, but that doesn’t mean you should 6. Can you have web components with declarative shadow DOM? d’uh! But, hear me out: Why have three steps to render markup when you could have one? Yes, the new JS Framework is all the awesomeness. Definitely do a side project with it. Please in the name of her Sweet Saint Fuck of Apathy, don’t use it in your next project. We haven’t learned what our regrets will be, and the regrets are what everyone else has to live with but you. I don’t care if it’s a new CSS trick, a new JS thing, or a new JS framework: Give it two years before you take it for a ride in production. You know what happens in the next two years? Two year’s worth of StackOverflow questions. You do not want to be the person taking a new tech or feature for a joy-ride in production and discovering some hemorrhaging-edge bug. You also don’t want to be the person who has to clean up after the person who discovered a bug on a bleeding-edge technology. Person-Alpha is the asshole. The Alphasshole, if you will. Because they chose to use the bleeding edge thing. On the scrum-master’s Fibionacci-sequence of suckassness The Alphasshole is a 21. The Alphasshole implemented the new tech and turned your project into a collection of medical-attention-needed polyps on Satan’s butthole. Person-Beta is the Betasshole. The Betasshole had to work in real-time with other betassholes on other projects created by other alphassholes that were all equally hemorrhoidal in nature over StackOverflow and Discord to come up with whatever collection of hackery that properly deserves to be called, “Satan’s Self-soothing salve of sour-ass solutions.” The Betasshole is a 5 on the Satanic-sequence of Suckassness. They’re not nearly as bad as the dickweed who chose new tech. But unfortunately, they made a lot of terrible choices and cannot be free from blame, git-related, or otherwise. Listen. I’ve been an Alphasshole and a Betasshole before. I regret that. I should not have done that. I’ve also been gamma waaaaaay more often. Who is not an asshole. That’s a person who’s just had to deal with two years of shit rolling downstream. What’s unfortunate for the gammas is that they often will be accused of being the assholes because they will git-blame, complain, and refrain from touching whatever that new tech was because it’s haunted their nightmares and their daymares. They are simply picking up the pieces. The new and shiny is fun for side projects. It is not for production. Please put two years between you and any new tech. If for anyone, do it for the gammas who actually have to keep your precious world wide web running. TL;DR Just… grow up and think. Think and care about the content first. Because that’s what the users care about. Assume the users’ interests are more important than your own. Quit acting like CSS is some giant-ass mistake that needs fixing. A group of people who were collectively smarter than us wrote those specs. They didn’t make mistakes 7 . Assume you are making the mistake, not them. Server-side rendering was not invented with JavaScript and it doesn’t have to be implemented with JavaScript. There are many ways to render content on a server. Assume non-JavaScript solutions existed first. JavaScript either is neither the God-send nor the hell-spawn for your project. It is a tool which you can choose to use incorrectly. Learn when JavaScript is the right tool for the job. Quit doing hard things for simple problems. Quit doing things that are degrees of difficulty harder than the problem. When a problem presents itself, look for multiple solutions, and then choose the simplest one. Assume the simplest solution is the best one. Quit chasing the new and shiny in your day job. I, too, am excited about the new and shiny. But I put a 2-year buffer between me and prod because I don’t want to be the person to have to discover browser bugs, framework bugs, or even compiler bugs. Or whatever. Don’t do this and continue to be an Alphasshole, Betasshole, or an otherwise generally contemptuous person. I don’t care. At least I can sleep better knowing that you know that I know that you now know that you’re an asshole. Happy Apathetic coding! Footnotes, Sources, and Whatnots 1 Express.js was released in 2010. 2 PHP started development in 1993 and was released in 1995 which means that when express.js dropped, PHP was driving to the store and buying clove cigarettes. 3 When I say it’s mutated, I mean that nesting jumped from Sass to native CSS. And CSS does it differently. So now Sass has to change how it does nesting because of how CSS does it. It’s bad. It will introduce lots of things you don’t want to happen. 4 People have been talking about how left-pad broke the internet and some wise folks have called out that it was pretty silly that such a simple function got to be so ubiquitous that it had the power to “crash the web”. But what’s also dangerous is the person actively trying to become ubiquitous. Every PR that involves adding a new dev dependency to any project should require the developer to hand-write the entire dependency tree. That would probably save a lot of future heartache. 5 I do not, in fact, have strong evidence that the Good Lord intended for WebPack to be told to, “fuck off good and proper like.” However if the Good Lord had something to say about cotton blends, He’d probably also have something to say about using a JavaScript build utility for building styles. 6 Yes, you can have strongly-typed CSS, but it’s insanely tedious because those “types” are not like any types you’d expect from any other language, and the syntax is going to be a huge pain. Also, you have the option to set it to not inherit which is, in fact, the most unCSSy thing ever. So please. Don’t. 7I don’t mean CSS is perfect and flawless and should never be questioned. I would definitely say that the implementation of em was weird. Their choices for naming was a bit odd; you could argue many font- properties should’ve been text-. I don’t think Layers was a mistake; I think they meant to do what they did. But I think they should not have done it because Layers is supposed to be an attempt to explicitly control cascade, but it’s only ever going to be useful so long as you maintain 100% control over every style on your site and are adding every style to a layer. If you don’t, the non-layered style will over-write the layered one. I get why that’s the case because that makes it a progressive module and allows interoperability with the vast majority of sites which don’t use Layers. But that also means that Layers is going to be pointless in any system where styles can be generated / imported from multiple places. Which tends to be a lot of CMS and even some plugin-rich SPAs. Same with CSS Nesting. I think they knew what they were doing. But CSS Native Nesting is not string concatenation / interpolation that Sass did. CSS Native nesting uses the same mechanism as the :is() selector. That’s incredibly problematic because that selector will always apply the argument with the highest specificity to all of the arguments. So look at this: #article-container, .article { h2 { color: red; } } In Sass, you’d get #article-container h2, .article h2{} which is perfectly reasonable and intuitive. But over in CSS you’ve got :is(#article-container, .article) h2, which means in practice you will now need a specificity greater than [1,0,1] to override .article h2 which you thought would have a specificity of [0,1,1] Categories: Front-end Prose Web Tags: Angular Front-end JavaScript Frameworks Preact rant React Vue",
    "commentLink": "https://news.ycombinator.com/item?id=40746173",
    "commentBody": "A Rant about Front-end Development (frankmtaylor.com)112 points by xlinux 14 hours agohidepastfavorite200 comments tazu 14 hours agoThis is the blog equivalent of fast-food. Sure, it's technically \"food\", and most people will agree with the content, but it's essentially empty and does nothing good for your health (nothing to change things). I would love to see a list of projects that do less of the front-end garbage. Someone mentioned the levels.io guy. I also like HTMX. Personally, I've seen great success using 95% server-side rendered sites with 5% Alpine JS sprinkled on top. reply manuelmoreale 13 hours agoparentI personally do all my sites with 100% server site rendering and bits of JS here and there if I need some interactivity that can’t be done with CSS alone. The vast majority of websites out there probably don’t even need JS. reply threatofrain 1 hour agorootparentFor every front-facing website there's 10 internal facing tools and dashboards. You don't \"need\" JS would be the wrong framing here. It's more like you don't need JS aversion when you're developing internal tools and you don't need the complication of SSR either (ask yourself whether the native or mobile app world is jumping on the idea of ultra-thin clients). We waited so long to improve semantic HTML that ML rushed ahead and will probably deliver semantic comprehension far in excess of whatever we can hope for with conventional means now. reply Aeolun 13 hours agorootparentprevI personally do all my sites with a close to 100% client side rendering as I can get with modern JS frameworks. If I wanted to SSR I’d use PHP… reply grishka 13 hours agorootparentAnd what kind of problem do you solve with this approach? Your sites loading too fast? reply threatofrain 1 hour agorootparentCan app makers in other ecosystems learn from the web here? Should mobile and desktop apps just be ultra thin clients with the next \"page\" being rendered on the server? Do you think this argument tickles the performance bone of other app communities? reply chrisldgk 10 hours agorootparentprevNot that guy, but for highly dynamic and interactive sites, it’s actually easier to manage state if you‘re working on the client side only. Introducing SSR and data handling on the server side to an app can be much less productive if you don’t need the performance improvements that SSR brings. reply _heimdall 7 hours agorootparentThis is the crux of where I always landed on whether to render on the server or in the client. Render where the state lives. If a piece of state is stored on the server, and the server will have to be notified when it changes, rendee HTML there. For any part of the UI that really is temporary state that only lives in the client, just render that piece (or changes to that piece) in the browser and leave the server out of it. An all or nothing solution is rarely the right answer. reply chefandy 3 hours agorootparentMany developers fail to notice how colored their perspectives are by a) nostalgia for the technical simplicity of web 1.0, b) the curse of expertise, and c) confirmation bias. A website, just like any other application, exists to solve a problem. The right ratio of hassle/efficacy/reliability definitely isn't the same for every application, and often not even the same among different user groups. What developers consider to be a hassle or too complicated is dramatically different from most other groups. Doctors and children and mechanics and mechanical engineers and artists have more in common with the way they interact with software than any of those groups do with software developers. If you have a working mental model of what's happening behind each click, your perspective is totally different. To a developer seeing a form submit and waiting for a return is a set of actions with a story behind it. We know what's happening. To a typical user, it's a period of flickering and flashing and waiting after they clicked on a button. There's no meaning to it-- they just compare the action of button A which changes screen state instantly, to button B which seemingly dicks around for a while until something happens. What about on slow connections or devices? It depends on what problem you're solving. If you serving up nearly static content, using an SPA is absurd. If you're doing something interactive, it's way worse to have constant reloads than use a -----properly designed----- SPA that loads once, uses local storage efficiently, and only makes small API calls to the server once its loaded. If an SPA has poor architecture or is a bad solution for the problem, the problem is the developer that made those decisions, not SPAs. A 500% reduction in technical complexity is meaningless to most end users. If it comes at the expense of having to look at a page reload in between frequent operations, it's a big annoyance. Sure, I'd much rather put together a nice clean static site than do something dynamic, but what the hell is the point if it's not the best way for users to solve the problem they need it to solve? Beyond that, I guarantee you there are many sites that 'just work' the way you'd expect a website to work and you'd have no idea they were SPAs. Making a proper SPA requires a lot more technical design and expertise than making a traditional site, and using a poorly made SPA is a WAY worse experience than using a poorly made static site. That makes it really easy to notice when one is bad, so it makes SPAs seem bad, generally, which is just not true. For the same reason we can't leave all technical decisions up to designers, we can't leave all interaction decisions to developers. Obviously it will make development a lot easier and smoother if we do, but that's one of many factors, and quite often not the most important one. reply tail_exchange 5 hours agorootparentprevI also prefer the client-side approach because the separation between server and client makes more sense to me: the backend serves data, the client renders it. It is also much easier to manage state. Unless the website is extremely simple, I always go for JS framworks. I never had problems with performance. React is less than 150kb, and as the backend is not a bottleneck, the website feels very snappy. SSR is not enough if you want an interactive page, you'll need some JS. If you do, instead of dealing with a ton of scattered JS scripts, I'd rather have a single React/whatever application. reply elktown 3 hours agorootparent> the backend serves data, the client renders it SSR does this as well. The server serves data (HTML), the client (web browser) renders it. reply tail_exchange 2 hours agorootparentBy data I mean a format for data interchange, like JSON. reply elktown 1 hour agorootparentSo what makes sense to you is adding the additional layer of transforming the JSON data into HTML from an API - all of which are almost always in the end tailor made for the app anyway - using a significant chunk of code within a completely separate and bloated ecosystem and in a different language than what the server uses? A bonus \"feature\" of this, and a very important one, is that this now also requires different specializations on your team and the personnel costs and communication overhead that comes with that. Great, now adding simple CRUD functionality to a website is a two week project syncing between backenders and frontenders, for something that someone wanting to scratch their own itch could hammer out independently in a single day back in the SSR days. reply tail_exchange 32 minutes agorootparentLarge companies typically have dedicated frontend teams with designers to build a user experience that feels cohesive across their products, so anaging different specializations and the communication overhead is already part of their workflow. Your argument suggests that SSR requires only one type of specialization, but that's not entirely accurate. SSR developers still need full-stack knowledge, since they must write HTML, CSS, and JavaScript alongside the backend code. This means their expertise already needs to cover multiple areas and languages. While it’s true that frontend frameworks introduce additional build tooling, developers today are generally familiar with this ecosystem. The backend developers I work with do have a good understanding of React and can work with it comfortably, and I have seen junior developers picking it up within a week or two. They don't need to be specialists in order to be productive. Yes, it is true that many backends are tailor-made for a frontend, but going exclusively HTML-first will lock you into this state, unless you are OK with revisiting it to add a JSON API later to support additional use cases. When I write an application, it usually comes with a CLI in case I need to use it from a server without access to a UI, and I hook it up with tools for integration tests using its JSON API. It's also common to see frontends that are powered by multiple backends, and in these cases, it makes sense to have a separate frontend application that uses APIs. So yes, it does make perfect sense for me to have a layer between presentation and application. reply 1vuio0pswjnm7 11 hours agorootparentprevFor so-called \"tech\" companies that intermediate, surveil and attempt to profit from computer users accessing content the so-called \"tech\" companies did not create, that is actually a \"problem\". For example, as a \"solution\", Google intentionally adds delay to \"loading\" so that ad auctions can run. When these companies use some marketing slogan like \"Make the web faster\" it is quite amusing. Ads are what slows it down and dissemination of ads is their \"business model\". Parasites. reply grishka 10 hours agorootparentI keep forgetting that there are ads on the internet. reply LegionMammal978 13 hours agorootparentprevAt least for me, static page hosting costs me $0, whereas dynamic page hosting would very likely cost me > $0. reply grishka 13 hours agorootparentWhere does your SPA pull its data from, if not from some sort of API that must be running on something that is capable of generating HTML as well? reply wfme 12 hours agorootparentprevDropping an SPA onto Cloudflare takes under 5 minutes including sign up and is completely free. reply manuelmoreale 13 hours agorootparentprevYou can use a SSG and get the same result if that’s your main concern. reply com2kid 13 hours agorootparentprevI do the same for personal projects. I love having deployment be \"drop a folder of static files onto a server\". Also scaling out SPAs is a non-issue. Servers have basically unlimited throughput for delivering static files, assuming you don't inline any 24MB uncompressed images. Or just throw it on Cloudflare, whatever, again, it is just static files. reply oliwarner 12 hours agorootparentThey made this thing called a static site generator about 25 years ago, and modern ones like Astro are better than ever. Just like your SPA, you have no server to look after, (eg) Cloudflare builds it all, hosts it all and there's no JS overhead for clients, no SEO hassle, etc, etc. reply com2kid 12 hours agorootparentThe personal projects I tend towards are interactive and have animations and fancy transitions and application state and they make API calls. Which makes Astro the wrong tool for the job, and SPA frameworks like Svelte the right tool for the job. Doing these things server side makes no sense, it'd just make everything harder for me to debug! reply oliwarner 11 hours agorootparentI'm not saying SPAs are wrong. There is a very valid set of uses and it sounds like you're well in there, though if your state model is so fragile to won't survive me accidentally refreshing, you need to look at that. But many enthusiastic young developers start using a framework and decide to do everything with it. Same was true on server dynamic frameworks. Everyone's first project is technologically-inappropriate blog. These sites would be objectively better as SSG sites. Btw, Astro is surprisingly excellent at using frontend components from multiple frameworks, even in the same project. If your interactivity is in-page, or you have a mixed MPA-SPA domain, it's well worth a look. Even the DX, I prefer it to Next/Next projects where you're chained to a set of dev dependencies from the start. My last Nuxt upgrade took three weeks to unhook various deep integrations and fix things (Vue 2 to 3 too). Nightmare. reply xwolfi 13 hours agorootparentprevGnehehehe reply nesarkvechnep 13 hours agorootparentprevI like how people with JS tunnel vision hate on PHP while JS, their torturer, is much worse. reply manuelmoreale 13 hours agorootparentIt really is incredible. I’m sure the JS heavy setups have a reason to exist but it’s almost as if people are forgetting that alternatives exist. reply lmm 13 hours agorootparentprev> I personally do all my sites with 100% server site rendering and bits of JS here and there if I need some interactivity that can’t be done with CSS alone. So actually not \"100% server side rendering\" at all. And probably paying all the costs of using JS more extensively, but gaining fewer of the benefits. reply zer00eyz 13 hours agorootparentwhoosh Yes you send a full, html page over the wire. It will render. script off. You can enhance it later.. You know a basic is this an email, is the pass word not empty check for a log in or some other fields if you have a basic contact form. The web worked before all the JS frameworks... It still works that way today. It's fast, low bandwidth and easy to cache. reply cuu508 13 hours agorootparentYou don't need JS to validate an email by regex or password length (and more complex validation can happen on the server) https://developer.mozilla.org/en-US/docs/Web/HTML/Constraint... reply manuelmoreale 13 hours agorootparentprevWhat the hell are you even taking about? reply lmm 13 hours agorootparentWhat does the term \"100%\" mean to you? How does that fit into \"100% server site rendering and bits of JS here and there\" in a way that forms a coherent concept? reply manuelmoreale 13 hours agorootparent100% of the rendered page is served by the server to the client. Most pages are HTML and CSS. In the rare occasions I need some interactivity (a gallery, some dynamic filtering, things like that) that functionality is provided by JS that’s included in the page, the same page that’s passed by the server to the browser. JS doesn’t render anything. If you turn it off the page is served anyway, some minor things might not work. reply fsckboy 13 hours agorootparent>100% of the rendered page is served by the server to the client. servers always send 100% of what they send. is what is sent 100% rendered already? well, browsers do a fair amount of rendering, that's what they do with CSS and HTML. and then you're adding on JS. we differ in the language we use to describe what is happening. perhaps you've rendered something from a database wrangled into the 100% page that gets sent, it's just not done rendering yet. reply manuelmoreale 12 hours agorootparentWell if you want to play the semantic game then there’s no such thing as a server side rendering because yes, the browser has to render that page on your monitor. But I suspect that’s a pointless definition. reply alganet 13 hours agoparentprevDo we need a project, or a tool? Consider the tool the author mentioned, jQuery. It is on the text almost as a museum piece. The things jQuery did (compatibility, CSS selectors, fetch) became standards or stable almost a decade ago. We don't need a 2024 version of jQuery, what that tool did we can now do vanilla. And that's about it. Perhaps React would be useful for a large project, with lots of complexity to manage, and assuming that people involved have experience in using it to manage such complexity. On the other hand, that React project that contains only some forms and page navigation could totally beDo you have open source project with the 5%/95% ? Not yet. I just use Alpine for form validation (with the awesome mask plugin), checkbox dropdowns, dialogs/modals, and toast notifications. Everything else is Go HTML templates. I wish there was a way to do custom builds of Alpine and just include what I need, but it probably wouldn't be too hard to do it in vanilla JS. reply dclowd9901 13 hours agoparentprevI’d love a return to sane css frameworks like SMACSS. And I’m using “framework” in the conceptual way. It’s not a library or anything. Just a set of rules you create for your css. Composable, no compiling, no muss, no fuss. I built something with it once. I wonder if it’s still in use. reply hliyan 13 hours agoprevWhile I don't agree with the tone of the author, I do have to admit, there seems to be something foundationally wrong with front-end development. Yes, it's more complicated than server development in that it is a real-time, multi-point input, multi-output (rendered elements on screen) environment with synchronisation and concurrency needs. But I used to write front end applications with MFC (Microsoft Foundation Classes, which is C++) using Visual Studio circa 2005, and it was never as complex as an SPA today. There is something wrong somewhere, and I don't think it's JavaScript or the DOM. The flaw seems to be paradigmatic. reply oopsallmagic 13 hours agoparentYour second paragraph disproves your first. Websites' needs didn't drastically increase (I'm ignoring nonsense like WebGPU; render graphics outside the web browser like a God-fearing Christian!), but their complexity did. Why? Well, we told everyone with a pulse they could make six figures \"doing web development\", and we're reaping what we've sown. You can still design websites like it's 2005, and they'll be damn fast. But pitch something like MFC or even PHP to a 20-something frontend developer now and watch the blood drain from their face. reply bluefirebrand 13 hours agorootparentWe don't build websites anymore, we build web apps It make seem trite but you just aren't going to have a good time building some of the stuff people build now using a LAMP stack reply manuelmoreale 12 hours agorootparent\"We\" build both. You might be building WebApps, I certianly don't and so are countless others. The web is not one homogenous thing where everyone is doing the same thing at the same time. Different people work on different projects serving different needs and we have to acknowledge that otherwise we end up in these silly tech-religious arguments where people think there's one and only one way to do things and that's certianly not the case. reply royjacobs 9 hours agoparentprev\"Yes, it's more complicated than server development in that it is a real-time, multi-point input, multi-output (rendered elements on screen) environment with synchronisation and concurrency needs.\" I would argue that most backends typically also deal with more than 1 user at the same time (and in real time). reply zer00eyz 13 hours agoparentprevConways law: React is a Face Book abstraction leaking out on to the web. It's good for them. It's a perfect framework (+ graphQL) for there every page is custom in a walled garden environment. It isnt a good way to build a blog, or a shopping cart. But A mail client in a browser, or a B2B app with a lot of data, A CMS... great places for it! And it's great for FB too. (as is every other open source tech from a large player). Think about all the money FB saves NOT having to ramp up front end engineers... they get them pre-trained now on their tool. reply __MatrixMan__ 13 hours agoparentprevI think the flaw is that we've got one type of browser for everything. Let's just have a bank browser for banking and a restaurants browser for deciding where to eat. Then the banker need only indicate that the account is overdrawn and the restauranteur need only indicate that the fries are gluten free. The number of people who need to care about how these things are presented to the user could decrease by a factor of 1000 if we just standardized a little and stopped trying to control every little thing. reply joe_fishfish 12 hours agorootparent> a bank browser for browsing and a restaurants browser for deciding where to eat you mean like native mobile apps? reply __MatrixMan__ 6 hours agorootparentYeah, except we just need one for banking in general, not one for each bank. Not sure how to get the banks to contribute to such a non-zero-sum endeavor, but man they could save some money if they did. reply lmm 13 hours agoparentprev> I used to write front end applications with MFC (Microsoft Foundation Classes, which is C++) using Visual Studio circa 2005, and it was never as complex as an SPA today. There is something wrong somewhere, and I don't think it's JavaScript or the DOM. No, it's CSS. No real UI framework has anything remotely like CSS, and for good reason. But people like the author of this article keep trying to make CSS work instead of writing it off as the mistake it is, and so web development remains awful. reply surfingdino 13 hours agoprevI was recently on a project where the backend was written in Python and finished in two months. The frontend guys are still dicking around with React components eight months after the project started. The front end on this project is child's compared to the backend. It's frustrating. There was a point in the history of front-end dev when they all started calling themselves \"rock stars\" and became convinced that they are the future of software development. The SPA trend gave frontend devs an excuse to write unmaintainable code, gave designers an excuse to call themselves software developers, and then they all told the world they are doing \"full-stack development\" when Node appeared. Meanwhile they never bothered to learn pre-SPA UI, UX, or content design principles. Thing is, working on front end never gives you a chance to work on problems that backend has to deal with. I never let JS guys work on backend code, because they are lost if they cannot find a module online that does what they are asked to do, or is missing half of the features from the spec it promised to implement (always the hard ones). We then have to pick up the mess and rewrite it in Python or Golang, which is wast of time and money. I once quit when when the client showed me the Python code written by a JS dev. My devs refused to touch that shit and we went to work for another client. reply Vaguely2178 58 minutes agoparent> I never let JS guys work on backend code, because they are lost if they cannot find a module online that does what they are asked to do, or is missing half of the features from the spec it promised to implement (always the hard ones). Stop self-aggrandizing. A huge percentage of backend devs would be lost without their giant MVC frameworks holding their hands throughout the dev process. Many of them do nothing more than write little controller functions while they lean on their frameworks, authentication modules, routing libraries, ORMs, and templating engines to do the hard work. You're not sending rockets to Mars after learning how to spin up a Django c.r.u.d. app. reply rmuratov 11 hours agoparentprevThis is a big and poorly justified generalization. reply surfingdino 11 hours agorootparentTo be fair to good JS backend devs, my view is biased by the fact that me and my team do Python and Golang work and the only time we interact with JS devs is when there is frontend to be written. Some frontend guys are good, most of them are poorly trained and those think they can write backend code and lobby managers to let them have a go at it. With disastrous results. I have never worked with JS backend devs who were any good. I am sure they exist, but I have yet to meet one. reply tasuki 6 hours agoparentprev> I never let JS guys work on backend code, because they are lost if they cannot find a module online > We then have to pick up the mess and rewrite it in Python or Golang If you really must feel smugly superior, why go with Python or Golang? You could be using Haskell! The JS devs I worked with in the past were competent developers who happened to do JavaScript. reply tflinton 13 hours agoprevI literally left front end development because of stuff like this. It felt like insanity. Throwing out the door debuggers, linters, all the tooling so we could express objects as attributes?… and enforce managing state better?.. It felt like a flood of junior programmers in an echo chamber set off by an opportunistic engineers at Facebook who were more interested in creating their own job security then work to evolve an existing standard. Seriously who honestly thinks that the authors and governing board of HTML and CSS didn’t closely consider the features in react? What kind of arrogance does it take to say they’re fucking dumb let’s reinvent EVERY tool on front end because we know better. But I digress… reply throw156754228 13 hours agoprevThank you. I'm at a css in js company and everything you wrote about it is The Truth. We've got nav menus written in reams of javascript. Stuff that could be done with zero js, or a couple of lines of glue. In a previous role I used Ant Design which were on the SASS train, trying to override the specificity of their stupidly long selectors was painful. React is too loose, it just enforces no abstraction or discipline. JSX ends up an ugly nightmare of angle brackets, ampersands and question marks. You often have to piece together how the component works by staring at all the event handler code for minutes hours days. Most of my colleagues writing it would stare blankly if I mentioned MVC. I think I would come down on the side of Angular, it's opinionated, but at least they attempted to separate the template from the controller, and logic in the services. reply shepherdjerred 4 hours agoprevThe problem isn't React. You can write a good website with React. You can write a good website with htmx. You can write a good website with HTML/CSS/vanilla JS. The average developer who cares enough to learn and use htmx/vanilla JS is going to write a superior product simply because they care more about performance and/or because they are better than the average developer. --- I'll plug my favorite way to build a site: Astro [0]. It allows you to write JSX that compiles to static HTML. You can use React/Vue/whatever as well for anything that actually needs to be dynamic. I'm not affiliated in any way, but I did build my personal site [1] with it. [0]: https://astro.build [1]: https://sjer.red reply sebazzz 2 hours agoprevIf you want to reduce maintenance costs, reduce the amount of Javascript code and tooling. NPM-Javascript is a shitshow of constant breaking changes left and right, whether it is in your runtime framework, or your build system like gulp or more prominently webpack. Try maintaining 10 projects with these frameworks. You'll quickly look for something that is not as maintenance intensive. Example: No, you can't stay on Webpack version X, because (real example) it turned out that version of webpack relied on md5 which wouldn't work in newer node.js versions. reply hardwaregeek 13 hours agoprevThere’s a lot I disagree with but I’ll point out one in particular. > how do we make content presentable as easily as possible, with as little duplication as possible, and with as few negative impacts to the user as possible. Why is this the priority? If I wrote a language that was solely focused on removing duplicate code, and doing the minimum to appear decent, that would be a pretty bad language. I care a lot more about composability and readability than de-duplication and floor raising. Let’s face it. CSS comes from a fundamentally different situation than the modern web. I’m talking about optional styles that do not affect the content, that are written by one or two people, and likely total at most a few hundred lines. That is nowhere close to modern websites. Bemoan that all you like, it doesn’t change the reality. reply polydevil 10 hours agoparentWho says that the solely focus of CSS is avoiding duplicate code? reply Erem 13 hours agoprevI built a fairly successful vc funded startup with a front end lead that thought this way about frontend frameworks. For a suitably complex application what happens is that you end up organically building your own framework in order to manage the mess of application state that is the screen. Now in order to spin up, new engineers need to learn this unusual organic framework to get things done rather than just use the excellently documented React that they likely already know. reply dmalik 13 hours agoparentYa. This is why React is popular. It's just a bunch of decently good front end best practices. Popular, not the best X for Y but with easy replacements. reply miffy900 10 hours agoprevThe entire article itself presents incredibly poor reasoning or no reasoning at all for its points. It's really hard to appreciate it now because CSS (along with JS+HTML) has been extended and upgraded gradually over the years, but CSS is incredibly good for styling and formatting DOCUMENTS - which have a very specific definition, especially back in the 90's when it was first conceived. This is what is was originally designed for. In that sense the complaints about CSS are perfectly reasonable: why are we using a document styling language for building arbitrarily sophisticated web applications when it was clearly not originally designed for that? You could say the same with HTML and JS really. > Quit acting like CSS is some giant-ass mistake that needs fixing. A group of people who were collectively smarter than us wrote those specs. They didn’t make mistakes. Assume you are making the mistake, not them. So what year again was the CSS standard was first published? Checks google quickly -- oh it was back in...1996. HTML was...1991. And JavaScript in...1995. Yes, it's well known that people in the 1990's were much smarter than people in the 2020's! It's not like there's been decades of progress since then! CSS is utterly fit for purose and bereft of any flaw or defect! Absolutely nothing new needs to be invented or discovered about front-end web development in 2024! There's no point in trying to improve things! OK, it's hilarious the author rails against things like SASS, and CSS is now being updated to incorporate many of the same features that SASS introduced: - CSS nesting: https://www.w3.org/TR/css-nesting-1/ - Scoping (no more global): https://developer.chrome.com/docs/css-ui/at-scope - Not to mention things like CSS variables, calc() etc. Usually people whine about the status quo or things staying the same for too long and in response, try to improve things. This article almost seems like a big whinge about how things should stop changing and regress or go back in time? That's not happening. If people had the author's sensibilities or attitude back in the 2000's we'd never move past `float: left` for positioning or tables for multi-columnar layouts. Utterly bizarre. reply smj-edison 13 hours agoprevIt seems he certainly hit a nerve here, regardless of people's position on the subject... reply bartimus 3 hours agoprevI agree with many things. But: > Maybe it’s because Angular was no one’s first choice — even though it came first. Actually ExtJS was the first real framework. Angular was just one of the new kids on the block. reply onion2k 13 hours agoprevWhen I started building websites professionally (in 1998) we had an adage that people used all the time to talk about how to get traffic to your website: \"content is king!\" It was a play on the \"cash is king\" motto of small businesses I think. It was very accurate too; if you wanted traffic then you had to build a site people shared. Search engines were not as effective as they are today. Then the web changed a bit. In 2005 to 2015 (ish) people transitioned from being consumers of web content to creators of that content. We called it \"Web 2.0\". Content was still king because people went to their favorite sites to create things that other people would read. But anyone with half a brain could see what was coming next. Around 2015 people stopped making content for other people to spend time consuming and instead shifted to making content that was seen for as little time as was necessary for them to hit a 'like' button. 'Content' in any meaningful sense died. It became a sentence on Twitter, or a photo on Instagram, or a really short video on TikTok. The entire premise that users go to websites for the content is nostalgia. That side of the internet is effectively dead (despite some noble and awesome attempts to keep it going, HN being an example). Today very few people get paid to build static content sites. If you're a web dev you're paid to build an app that enables people to do things in a browser - and yes, that means working with something like React. All web devs try to crowbar React into everything simply because that's what they're paid to do, and they're paid to do that because that's what users want to do. Railing against it is a waste of time. The Internet today is not same as the Internet of 25 years ago. reply dmalik 13 hours agoparentWell said. Also been doing this since the 90s. I still like discovering new sites, blogs and creative things people make. HN is one of the better sites for that. reply smj-edison 13 hours agorootparentIf you haven't already heard of it, you might like marginalia. It's a search engine/website finder/experiments. I've found it really useful for finding small blogs and interesting perspectives! reply manuelmoreale 12 hours agoparentprevI want to both upvote and downvote your comment. I upvote all the top part. But I'd downvote the conclusion. That side of the internet is all but dead. It might not be the majority in terms of traffic (for obvious reasons) but it's probably still the majority in terms of number of websites and (some) people still care about quality content. reply Too 12 hours agoprevThis rant is missing the WHY in every section. While it has some valid points, it will unfortunately only satisfy those who are already in the same boat. The rest of us will keep wondering why he implicitly assumes that backend generated with js is sooo much worse than if was generated with anything else. I've also written my fair share of PHP before AJAX was even a word, plus lots of ASP.NET, Java and Python backends. Honestly, their templating languages all suck, compared to React components that are type safe and infinitely easier to compose and refactor (jinja anyone?). While their languages and ecosystems are a lot more powerful, JS these days isn't that terrible as a language, just keep a tight grip on your package.json. It is correct that there is something wrong with the culture of the front-end development, in that there seem to be no brakes on just piling crap on the next shiny thing and then building even more packages and abstractions around it, instead of fixing the underlying crap in the first place. reply STRiDEX 14 hours agoprevI have to do \"real\" ssr every once in a while via the django jinja html files we have to send emails. It makes me want to die coming from our nice typescript react SPA frontend. Every object is a mystery and jinja syntax is functional but terrible to maintain and of course you're suddenly using tables because it's an email. I'd rather every element be a div than do one minute of editing those stupid jinja files. reply oopsallmagic 13 hours agoparentIt sounds like your issue is with the syntax of Jinja, and the hellscape of HTML email. I'm not sure how 20 MB of inscrutable JavaScript would help, considering it's also just template rendering with extra steps (bonus: using the user's CPU cycles and power instead of your own). reply lmm 13 hours agorootparent> It sounds like your issue is with the syntax of Jinja, and the hellscape of HTML email. I'm not sure how 20 MB of inscrutable JavaScript would help, considering it's also just template rendering with extra steps 20MB of inscrutable JavaScript allows you to have a) a sane structured component system where you can actually build up UIs compositionally rather than a flat glorified string substituter (i.e. not actually \"just template rendering\"), and b) the control and abstractions needed to make good UIs out of nested tables. reply oopsallmagic 4 hours agorootparentAnd all this for marketing emails nobody reads anyway. reply sussmannbaka 12 hours agorootparentprevyou can compose UI on the backend without string substituting. Are you under the impress that JSX is some sort of thing exclusive to Frontend? reply lmm 12 hours agorootparent> you can compose UI on the backend without string substituting. Sure, but you're still going to be using \"20 MB of inscrutable JavaScript\". (Unless you use Wicket, but I'm not sure that's an option for emails, and would likely trigger the same complaints anyway). I mean, I hope you're not using the component style rendering layer for Python that I published ~10 years ago, because I haven't maintained it, and as far as I know there aren't any others. reply xigoi 10 hours agorootparent20 MB JavaScript is much less bad when you’re not sending it to the client. reply zer00eyz 13 hours agoparentprev> jinja As a home assistant user, jinja in yaml files is hot steaming ass. Every time I read the jinja manual I want to cry. And a small part of me dies in side. It's like a templating language designed to get even with the person who microwave a tuna fish sandwich in the office, for lunch, daily... That some how leaked out to the web. reply grishka 13 hours agoparentprevThere are different template engines for SSR, with drastically different syntax. I've never written any Python, but in my own opinion, Smarty for PHP and Twig-compatible ones for many different languages are the best. While generating any moderately complex HTML on the front end is a pain in the ass for me. I prefer avoiding it. If I need to request some data from the server to dynamically update the page with, I just make that endpoint return some server-side-rendered HTML that I insert wherever it needs to go. reply kristopolous 13 hours agoprevI had been programming for about 30 years, I loved it and was excited by it. After work I would go home and work on side projects. React single-handily killed it. It encouraged, usually required, all of the awful design patterns I used to chase all the junior engineers and interns about. This has become endemic in the JS world. Here's an example: did you know that the uuid npm used Math.random up through 2020 (v.7)? Cryptographically secure random number generation had been available in basically all browsers since 2011. 9 years. If I had an engineer submit that code, I'd rake them over the coals. Speaking of that, I'd usually need a half-time engineer on larger projects just for the management of the complexity. All of the various frameworks and updates and breaking changes. It was a 20/hour a week job just to stay on top of things. For instance, koajs. It's a bunch of dependency code of quality I would never let fly, but now I have to use it and grow my team, increase budget, push back deadlines - all to manage the piles. No thanks. I left that job, quit the industry entirely about 2 1/2 years ago. Reactjs was such atrocious self-aggrandizing bullshit, it made me quit programming. It's fragile, over-engineered, mostly broken, confused poorly defined amorphous concepts that poorly solve mostly imagined problems and the entire industry of front-end is eating up that mode of design like it's some sacred text. They'll use abstract buzzword concepts that have different conflicting definitions in basically every text you pick up, and they'll never concretely define them but continue to use them as abstract amorphous blobs. I've got very popular front-end projects. I've been at the C-level of more than one company that sold based on front-end design. The first professional javascript based web app I did was in 1997. I deployed Server Side JavaScript (SSJS)/LiveWire in the 90s, a decade before Node ever existed. I've been at this for a long, long time. After dealing with react, I never want to touch that stuff again. reply hu3 3 hours agoparentTake a look at this video from 4 hours ago. You'll like it, I promise. It validates your opinion. React became an insanity. Just note all the nuances and minefields they are adding to an already bloated tech. https://www.youtube.com/watch?v=fXugpgknScY For one, it's 2024 and React still has this infinite newbie trap of double rendering. [1] But, it's perfect for job security. \"Nobody ever got fired for buying solutions from IBM\" became: \"Nobody ever got fired for choosing React.\" [1] https://chanduthedev.medium.com/why-react-app-rendering-twic... reply throwaway4good 13 hours agoparentprevCould it be that you never tried to understand react and why the approach makes sense to millions of developers? reply kristopolous 13 hours agorootparentWe could waste each others time getting into the specifics but I can summarize it. It has affordances and design patterns that inexperienced, new, novice programmers find attractive and tempting but comes with all of the warts and issues surrounding intuitive patterns. That's why it takes over 10 years to get good at programming - counterintuitive insights are the clarifying beacons on any sufficiently complex project. People are \"react programmers\" instead of actually understanding the specs, standards, javascript, protocols or how things actually work. It's about 100 times easier to find a mediocre junior programmer than a comptent one. React allows, at a great cost of time, resources, and money, a large team of mediocre junior programmers to eventually create what once required a competent engineer and that is why it's popular. Your offshore outsourced $15/hr programming firm can now make a half-assed website or app and you don't actually need to find someone who knows what they are doing. If you've actually worked in the industry, actually dealt with code, been put on projects that are off the rails, you know this is true. You know this is exactly what is happening. Then management is unwilling to pony up the $250k to attract the right talent so you get stuck with the job of making heroic efforts to prevent the house of spaghetti from collapsing. You've been there. You've done this. You know exactly what I'm talking about. reply Aeolun 13 hours agorootparentI don’t think it’s fair to blame React for the flood of shitty people. It’s a symptom of the problem, not the cause. If I had to manage the collapsing house in Rust I imagine it’d be much, much worse (though I can’t say I ever had that experience, so who knows?). reply throwaway4good 13 hours agorootparentprevWhat you write about has got nothing to do with react. reply kristopolous 12 hours agorootparentI'm sorry. I can't respond to such flippantly rude insincerity without violating the Terms of Service reply throwaway4good 11 hours agorootparentI do not mean to be rude but what you describe is an organizational problem. reply sethammons 6 hours agorootparentThis is perhaps my least favorite reasoning: you're holding it wrong. There will always be organization issues. Conways Law exists. The tooling matters because it encourages or is ergonomic in certain ways. I hear this in my circles around Python which encourages spaghetti code and reaching into private parts of other's code. Yes, you can prevent this with better libs/modules, solid leadership and a steadfast resolve with adequate tooling. Or you can use a tool that makes spaghetti harder to write and pick something like .net or Go. reply kristopolous 10 hours agorootparentprevThey're inherently connected. https://en.wikipedia.org/wiki/Conway%27s_law There's an extension to this - a codependency. The system imposes structure back up the chain. These choices can become good organizational fits because they reflect the separation of concerns of a project decided by management. It's almost invariably a management that doesn't know how to code or build software. reply throwaway4good 9 hours agorootparentI will give you this: The emergence of mature powerful web browsers made it possible to move a lot of functionality to the end-user's device and that gave raise to the frontend / backend developer split. But you are pointing to a specific frontend technology (react) and that is not the culprit here. reply lelanthran 12 hours agorootparentprev> Could it be that you never tried to understand react and why the approach makes sense to millions of developers? Could you summarise it for me? Sort of along the lines of: : reply throwaway4good 12 hours agorootparentIt was the \"view as a (pure) function of your model\" approach that was architecturally new in react. This article explains it using a bit of react jargon (action, store, dispatcher) which is kind of unnecessary - but that's what I got from a quick google: https://medium.com/@asif-ali/react-architecture-vs-mvc-unidi.... React Architecture vs. MVC: Unidirectional Flow Advantage. reply Aeolun 13 hours agoparentprevThat seems like kind of an overreaction. Sure, the first time I saw React I also thought it was an abomination. But in the end it’s fine. Different, sure, but there really is something to using the same language on both front and back-end, and not having to wonder about how to reconcilliate data and markup. React and the whole ecosystem gives you a thousand extra problems to care about, but it also takes a ton of them go away. It makes the simple things annoying but the hard things also merely annoying. reply kristopolous 12 hours agorootparentIt's a sincere reaction and I'm making myself emotionally vulnerable by putting it out there. I know that's forbidden on the Internet but I'm still deeply passionate about building good software and I'm just so disappointed by the precipitous decline in quality that's plagued the industry despite the increase budget and manpower dedicated to it. It's inescapably the tools and methods. This ought to be the golden age. Things ought to be great. But they're terrible. Everything has been getting slower buggier and shittier. It's become more complicated while doing less. It's awful. reply smj-edison 12 hours agorootparentYou mentioned you're passionate about good tooling, would you mind if I asked you what your thoughts are on Glamorous Toolkit[1] and how it compares to other tooling you've seen? [1] https://gtoolkit.com/ reply conradfr 13 hours agoparentprevWhat if I told you that not every company is using React. reply kristopolous 12 hours agorootparentRight I'm thinking about going back in under a data science or computer graphics team. Just no more web pages reply lelanthran 13 hours agoparentprevRight. I hear you. Pretend I'm a person who agrees with you about React (I might be, after all[1]). Have you ever seen that page \"You might not need Jquery\"? Can you please write a similar blog post (or even put it into a comment here so that I can bookmark it) titled \"You Might Not Need React\"? I want to know what it is that React gives (maybe)millions of developers that you don't need. All those things that React is commonly used for: what are you using instead? Show snippets, please. So, yeah, throw me a bone here, please write \"You Might Not Need React\", because I can guarantee you that there are numerous devs who want to read such a post. Pretty please. [1] I looked at the React tech stack and learning curve for someone who only knows HTML, CSS and Javascript out of the entire web-stack, and noped right out of it. reply kristopolous 13 hours agorootparentHere's a framework-free SPA I wrote about 2 years ago: https://9ol.es/pl/ as part of this project: https://github.com/kristopolous/music-explorer This is all the code: https://9ol.es/pl/scripts.js . It supports, for instance, Bluetooth controls and I use it as my car audio player literally everyday It does take longer to learn and require more intellectual rigor to do things this way but you also get 10 year solutions instead of 10-month ones and things are way easier to debug, pivot, modify, repurpose, support, All the long-term things you have to do after the act of building when people are actually using the thing. Those are now easier reply lelanthran 12 hours agorootparentThanks; I've got pretty much similar projects. What I was looking for was literally a \"You Might Not Need React\" equivalent to the https://youmightnotneedjquery.com/ site. It doesn't need to be as polished as that, I just want to see :, even if all the \"this\"es in there were just patterns. I just want to be clear here, I'm not engaging in bad faith, or trying to prove something. I truly just want to know what react code is commonly used for, and what a vanilla solution would look like. reply jamil7 13 hours agoprevMaybe it’s time the author started doing something other than frontend dev? reply noduerme 13 hours agoprevArguably, for SPAs it doesn't matter what tags you use at all. Whatever you do is going to be inaccessible. Make everything a div. Who cares? We used to write all the UI components ourselves in Flash or Java. Sure, yes, understand the content that you're writing your UI around, and make the UI serve the content. That just begs the question of why one would get anal about which HTML5 tags are used for headers or navs or buttons. reply nate-sys 13 hours agoprevdifficult to believe that many are disagreeing in the comments. this is really one of the better \"rant\" posts. reply acosmism 13 hours agoprevi cant count the number of times i've had to generate boilerplate + interfaces etc etc for something that would be as simple as an onclick (onClick?) handler would have done. i have yet to actualize the serious promises of this ecosystem in any meaningful way and it doesn't seem to get better. also jsx is ugly. reply robertoandred 13 hours agoparentI can count it: zero. Onclick handlers work exactly as they always have. reply acosmism 13 hours agorootparentsure the browser accommodates. now how about dealing with wrapping everything in unnecessary empty tags and tree shaking and bloat etc. specialized dev inspector tools for each framework etc. there are layers... reply robertoandred 13 hours agorootparentThere are no layers if all you want to do is add an onclick handler. reply acosmism 13 hours agorootparentthere is. there is a build system. there are probably some typescript interfaces and lint checks you need to pass etc probably some other framework you need to familiarize yourself with if you are data binding etc. i'm not saying any of this is inherently bad - what im saying is 99% of the time this is overkill reply robertoandred 13 hours agorootparentSo don’t do that. Just open a text editor, write Click Here, and celebrate. reply acosmism 13 hours agorootparenti'm glad we are on the same page. reply lelanthran 12 hours agorootparent> i'm glad we are on the same page. Only until the event handler runs :-) reply aaronbrethorst 14 hours agoprevI agree with a lot of this rant, but life is too short to care about ``, ``, and most other tags. If your content is highly legible to users—especially if they use assistive technologies—then it simply should not matter. The CSS Zen Garden is dead and it's time to till it and start over. reply thesuavefactor 13 hours agoparentSemantic content is also about being legible for computers though. Even though it looks good to users, some might also care about it being defined well for machines and automated processes. reply LegionMammal978 13 hours agorootparentFWIW, Google explicitly says it mostly ignores semantic HTML [0]: > Having your headings in semantic order is fantastic for screen readers, but from Google Search perspective, it doesn't matter if you're using them out of order. The web in general is not valid HTML, so Google Search can rarely depend on semantic meanings hidden in the HTML specification. And a typical business web page likely won't have many \"automated processes\" it wants to accommodate except for search engine crawlers. [0] https://developers.google.com/search/docs/fundamentals/seo-s... reply polydevil 10 hours agorootparentCompletely wrong. It says that the order of h* tags is not relevant. Which was also part of Html5 Outline spec. H* tags are not the only semantic tags. So it could also mean that they work with Html5 outline spec and value semantic tags, which allows them to ignore the order of h* tags. So them saying that they ignore the order of h doesnt necessarily means that they ignore all semantic tags. reply Mathnerd314 13 hours agoparentprevI think the idea is that the tags and the aria roles make it more legible to users using assistive technologies. For a significant number of aria roles, e.g. article, the recommended way to get that role is to use the corresponding HTML element.is just a generic element buthas a banner role. Now admittedly there are a lot of HTML elements and a lot of aria roles, and I am not sure screen readers actually have different behavior for every role, but just by making a reasonable stab at it, you are doing better than many major corporations. (some of which have been sued over accessibility concerns) reply manuelmoreale 13 hours agoparentprevHard disagree on that personally. But I guess it depends what industry you’re in. Life’s too short is a cop-out especially because it doesn’t take longer to use a semantic tag rather than a div. With “life’s too short” you can justify pretty much everything. reply LegionMammal978 13 hours agorootparentCost vs. benefit. If the ARIA roles are all in order (which can admittedly be a pretty big \"if\"), there's no user-facing benefit to semantic HTML over arbitrary HTML with the same style. reply manuelmoreale 13 hours agorootparentWhat cost? What’s the cost of typing a different tag? reply LegionMammal978 13 hours agorootparentThe \"cost\" in many of these front-end frameworks is having to manage HTML tags at all, instead of generating them automatically from some higher-level specification. Also, you have to remember whether any of them might have a different default style in the browser than what you'd get from a regular div/span. reply manuelmoreale 13 hours agorootparentIs that really a thing? I’m genuinely asking because I have zero experience with this type of setups since I don’t need to use them and prefer to work in other ways. reply LegionMammal978 13 hours agorootparentThe idea behind frameworks like Flutter and React Native is that you can specify the document as a tree of high-level components, which can then be rendered either as a web page or as a mobile app. In that case, there's simply no equivalent to the semantic HTML tags on the other platforms, so you'd end up with a big impedance mismatch if you tried to force them in there. (Though of course, such tools are definitely targets of OP's derision.) reply manuelmoreale 13 hours agorootparentI see. Interesting. But what’s the benefit of using a tool like that if you’re only targeting the web as a platform? reply LegionMammal978 13 hours agorootparentNone, unless you (or your team) is already familiar with it, I'd imagine. I think some (like Flutter) are designed to have similar markup regardless of which or how many platforms a particular project uses. But don't take my word on any specific examples, I don't have any real experience outside of normal web React (which does give you full control over the HTML elements, except of course for third-party components). reply manuelmoreale 12 hours agorootparentAgain, very interesting. Thank you for the insight. Appreciate it. reply rado 12 hours agoparentprevRegions are a part of the accessibility requirements, which are mandated by law. And the number of lawsuits is constantly rising. https://www.ada.gov/resources/web-guidance/#when-the-ada-req... reply jiggawatts 14 hours agoparentprevI've heard of a philosophy that instead of a million unique tags that do mostly nothing, modern HTML should use onlyandinstead. reply threatofrain 14 hours agorootparentIMO it's fair to say to colleagues that you're free to use any HTML tags as long as you take responsibility for them by knowing all of their idiosyncrasies. reply MrVandemar 13 hours agorootparentprevThen it wouldn't be HTML. It'd be a weird structural language you could really only express whether something is a block or inline. reply lmm 14 hours agorootparentprevYep. , , and JavaScript. If you actually want a simpler web stack, that's the way to do it. reply jay_kyburz 13 hours agorootparentUnfortunately you can't draw a table of number using just divs. It's just too slow and your users will notice. For tables of data, you have to use tables. I'm sure it's true for lots of other elements. reply LegionMammal978 13 hours agorootparentYou can, you just have to set their \"display\" properties [0] to \"table\", \"table-row\", etc. The whole point of CSS is to divorce styling from the particular HTML tags. [0] https://developer.mozilla.org/en-US/docs/Web/CSS/display reply joquarky 1 hour agorootparentIf you're going that minimalist, then why even have two tags (div, span) in your toolset when their only difference is display:block vs display:inline reply ramesh31 14 hours agoparentprev>I agree with a lot of this rant, but life is too short to care about ``, ``, or any other tag. Yeah I gave up on that fight a long, long time ago. The semantic web was an admirable dream that never had a chance. reply threatofrain 14 hours agorootparentYo, they don't even have a table of contents element. How can you say you love semantic documents and not have table of contents. And no bibliography either. reply manuelmoreale 13 hours agorootparentA TOC is a nested ordered list. You can make that today by nesting . reply threatofrain 1 hour agorootparentNo. That's just a nested ordered list. The difference that occurs when we all agree that it's also a ToC is night and day. You can also create custom elements, but again, if nobody has consensus on the meaning of those elements then they're just elements. That means that as a client I am not empowered to automatically display your content in a new way. reply simonw 13 hours agorootparentprevAnd then wrap that in aelement. reply simonw 13 hours agorootparentprevSemantic HTML elements and the semantic web are not the same thing. The semantic web was about linked data and RDF and suchlike. reply throwaway4good 13 hours agoprevWhat a silly rant. There is a huge world of different approaches to front-end and lots of them do cool and interesting things that would both up your productivity and give you completely new opportunities ... if you bothered instead of just being an old fart. reply briandear 13 hours agoparentIt would be cool if all of the fancy front-end people would learn how to use HTML properly before lecturing the “old farts.” Theexample comes to mind. reply MrVandemar 8 hours agorootparentThe way some people code \"HTML\" these days is like an apprentice carpenter swaggering up to a house building site, and then banging in nails with the butt of their power-drill, and telling the \"old fart\" carpenters what losers they are for bothering to use a hammer. Use the right tool for the job. HTML is a toolbox, not a bucket ofand . reply throwaway4good 13 hours agorootparentprevYou can be old and experienced and still have an attitude of being open to new things ... at least to the point where you understand them before you embark on a rant. reply csomar 13 hours agoprevThe author makes a few good points but on average sounds like an old man complaining about everything. There is a reason React took off, and there is a reason people choose React. Sure, if you are building a simple content heavy front-end, it doesn't make sense to chose a React framework or React at all. But many apps today come with heavy front-end interactions and have to sync with their back-end in real time. Good luck doing that with jQuery Ajax. And you can't blame the bootcampers. They were promised riches after a 6 months bootcamp. They were taught React and had little interactions with HTML. So they are doing what anyone else in their position will have done. So when they were out in the market, then React is was. Suddenly, everything became a React component; and if you made a website today, you'd better off start with React. In defense of React/NextJs: I have been blogging for 10+ years. I started by using WordPress. I have constantly failed to maintain a server up and running for more than a year. There is always something that comes up. A bad WordPress update, DB goes wrong, a failed payment, a bad WordPress plugin update, Server goes down because?, etc... 5 years ago, I switched to NextJS. It's a spaghetti of NPM modules just to build a very static website for very simple content. Sure. But 5 years later, the very same site is still up. There is no maintenance involved as the site is hosted in Github. Not sure I'd say the same about WordPress. reply lelanthran 12 hours agoparent> There is a reason React took off, and there is a reason people choose React. Sure, there's a reason. Doesn't mean it's a good one :-/ > 5 years ago, I switched to NextJS. It's a spaghetti of NPM modules just to build a very static website for very simple content. Sure. But 5 years later, the very same site is still up. There is no maintenance involved as the site is hosted in Github. Not sure I'd say the same about WordPress. Is 5 years considered a long time? Is that the bar? Because >6 years ago I wrote an internal site/app in C# using Jquery for front-end, and that is still up, working and being used by a few thousand people daily. It's even being extended every now and then. No NPM, no React, no Vue, no Next, no front-end build step. reply sebazzz 1 hour agoparentprev> 5 years ago, I switched to NextJS. It's a spaghetti of NPM modules just to build a very static website for very simple content. Sure. But 5 years later, the very same site is still up. There is no maintenance involved as the site is hosted in Github. Now try actually checking out the code and building it. I’m sure there is some Node.js runtime feature that is no longer supported in the LTS version you currently have installed on your notebook. reply manuelmoreale 12 hours agoparentprevI coded this one for a friend more than 6 years ago: https://designed.space It runs on a basic LEMP stack (or rather a LEP since it's a file based cms) on a VPS I have not touched in years. The stack is outdated, the CMS is outdated. It runs just fine with no issues. I can move it to an up to date VPS and port it to a recent version of the CMS in a couple of hours and it would then be fine for probably another 6 or 7 years. reply darepublic 14 hours agoprevI just want to say I like pre-rendering > server side. Because you get a lot of performance benefits while still writing in a client side style. It is a low hanging fruit though I suppose a combination with ssr would be better. reply ramesh31 14 hours agoprev>CSS is fine; you’re the problem This one really gets me. Being looked at like an alien from mars when talking to people about styling a page with a few classes here and there rather than pulling in 6 different dependencies of Tailwind et. al is maddening. We are web developers. We get paid to know these things, not to complain about them being annoying and try to find hacky workarounds. And now I get to sift through your utility soup and hope to god there's a class in whatever framework you GPT'd some boilerplate for that does exactly what the designer wants. reply Aeolun 13 hours agoparentI like writing UI with Tailwind, but I have absolutely no idea why. Ultimately what I do is look up all the actual CSS values I want on their website, then stick in the proper classes. I think it must be because I have zero actual CSS to care about. There’s just nothing in my UI aside from React components, and there’s just enough sugar that it’s easier than using the style attribute everywhere. reply Arainach 14 hours agoprevThis article could use a LOT of source links. Not because I believe points are wrong, but because if the author wants to rant that no one knows/has heard of things, why would anyone know what they're talking about? >The number of times I’ve seen numbers written inside of athat’s inside of a— instead of just using an— is deeply disturbing. I've been reading and writing HTML since the 90s and off the top of my head can't recall ever seeing . Why is it better? For screen readers? A sentence to illustrate goes a long way. reply manuelmoreale 13 hours agoparent> I've been reading and writing HTML since the 90s and off the top of my head can't recall ever seeingI have to ask: is this /s? Are you really saying you never seen an ordered list in 30 years working on the web? How about? If you’re not joking then it’s honestly wild to me. reply Mathnerd314 13 hours agorootparentIt is basic Markdown... maybe they never investigated the HTML Markdown produces on GitHub and so on?does seem hard to avoid. reply manuelmoreale 13 hours agorootparentDidn’t markdown come out in the early 2000s? So you’d have to try HARD to not see aespecially because pre html5 there were even fewer tags available. reply MrVandemar 13 hours agoparentprev> . Why is it better? For screen readers? A sentence to illustrate goes a long way. It's better because it means it's a list of something in order, as opposed to a definition list or an unordered list. The distinctions are important. It means, for example, that you can meaningfully skip to the middle of an ordered list of the 100 tallest buildings, and be guaranteed to be looking at the 50th tallest building. reply tstrimple 13 hours agorootparentYou're technically not guaranteed to have ordering. You can put an ordered list into any order you want. But semantically it indicates that the order of the list matters. I like the semantic model, but it can easily be abused and not mean anything at all. reply itronitron 13 hours agoparentprevThe author writes >> I’ve seen numbers written inside of athat’s inside of a— instead of just using anAn ordered listautomatically adds a number to each list item indicating its position in the order in the list which would save developers the effort of adding code to insert numbers into ordinary list items in an unordered list. reply tobyhinloopen 13 hours agoparentprevIt’s like asking why you would use a list anyway and not just a bunch of BR or P tags reply doubloon 5 minutes agorootparentJust wrap everything in PRE and do ascii dumps. reply necovek 13 hours agoparentprevWhat? How could you have been writing HTML since the 90s and not see all those examples of OLs with numbered items, including lowercase roman numerals or uppercase letters? Somehow, even if I never used roman numerals, that example sticks in my head (esp the lowercase). Anyway, if you cared deeply about content in the 90s, you might have focused on the semantic meaning behind tags (I've also looked a lot at other SGML and later XML based formats like TEI, DocBook...), tried to strictly decouple presentation (CSS) from the HTML (content), and be annoyed that the CSS box model never supported nice content-related flow features like TeX did since the 80s (and still doesn't). Now, don't get me wrong, I wasn't some wizard — I was simply an easily impressed (with things having some meaning) high-schooler around that time :) reply jiggawatts 14 hours agoparentprevTIL: https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_ol... reply manuelmoreale 12 hours agorootparentSince you're exploring lists: https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_counter... reply tjpnz 13 hours agoprevI fear that so much about what the web is has been forgotten. I've had some surreal discussions with frontend devs that don't have a fully formed concept of what a URL is. Their solution to \"sharing\" is to either have their users instruct each other using words on how to find a particular page, or to propose some overengineered URL shortening scheme to encapsulate state which belongs on the path or query string. It really feels as though usability is being treated as an afterthought. reply joseferben 13 hours agoprevi mostly agree with the rant, except for styling, i think tailwind is great. nowadays i default to htmx, alpine, sqlite and typescript, recently i’ve been working on a framework/starter using these tools: https://www.plainweb.dev/ reply tobyhinloopen 13 hours agoprevI’m with OP but we’re a rare breed. reply bottlepalm 13 hours agoprevYea I've been coding every framework under the sun since the 90s and I'm pretty happy with Next.js at the moment. It can render on the server and re-render on the client with the same framework, same language, same functions. Statically typed server to client and back to server. JSX/TSX has no strange template syntax and again static type checking. Compiling automatically tree shakes, bundles, code splits per route. I can do SSG, SSR, CSR, ISR. I can specify per route/api caching strategies. I can host serverless or self host. I'm knocking websites out left and right. It feels like I'm building a single application not two separate frontend/backend applications; managing overhead rendering and communicating between them. Next.js, Nuxt, Remix, SvelteKit - are all part of this next generation batteries included frameworks that can seamlessly transition from server side rendering to client side interactivity. $('.getResults').on('click', () => { $.ajax({ url: '/api/results', data: { foo: 'bar' }, success: function (result) { $('.results').html(result); } }); }); I mean look at this unmanageable bug prone code from the blog post. Nothing here is safe, no way to validate the classes where the data is coming from/going to. No validation of the api or its parameters. If you scaled this example you'd end up with nightmare level code. Yea I've been there already, I'm not going back. reply bluefirebrand 13 hours agoparent> I mean look at this unmanageable bug prone code from the blog post Yeah, it's hard to take any arguments about code quality seriously when this is being held up as some kind of ideal.. reply rado 13 hours agoprevHe is right to rant about semantic HTML being replaced by framework div soup. It's awful for accessibility etc. reply renewiltord 14 hours agoprevThe thing about software is that you can always just write better software. The levels.io guy makes $2m/yr just being better by being simpler. So it's possible. What's the point of a rant without the code to back it. reply nerdponx 14 hours agoparentIt looks like the author has written plenty of code to back their rant: > I did agency work for 11 years before making the choice to work for a very large tech company. I have worked across sectors like insurance, healthcare, retail, banking, investing, marketing, and manufacturing. I have worked with global brands which are household names. > I have written a lot of front-end code for a lot of companies. I have also dealt with a lot of consequences created by front-end code. My criticisms come from my role as a front-end developer and as someone affected by a front-end developer. reply renewiltord 12 hours agorootparentJust weird to do things one way and then get upset about it. If there's a better way do it that way. That's why levels.io guy is cool. He just walks the walk. reply tflinton 13 hours agoprevWhy didn’t he mentioned rewriting or adding to browser history?! reply koonsolo 7 hours agoprevHere is the main problem: front-end can mean \"blog\" on one end, and \"photoshop replacement\" on another. reply foul 2 hours agoparentYeah, not only this article doesn't mention whether he slings around his own library or uses a less insane JS blob like jquery, alpine, htmx or hyperscript, he laments essentially that foundations for big webapps and cookie-cutter shit turnkey products are sold for small operations and static websites. In a lot of parts of this rant I couldn't abstain myself from thinking that he wants a web for documents and the web for applications swung around a lot of very bad habits and useless complexity in the field while waiting for wasm, aka \"New Applet\" to be production-ready. reply dmalik 13 hours agoprevI agree with some of what the author is saying but add about 10 years of experience. Been coding sites since I was a kid in the mid 90s (I was a web master). I have 10 years working on brochure sites but have spent the last 10 working on major SaaS. Few things I disagree with: - The content rant was weird. I've always worked with content designers, instructional designers, or good marketing copy people. Was that about semantics? Sure ya, SEO and a11y. - Cascading CSS is great until you write a million+ line SaaS app and have 1000 devs working on the code. Then you need scope. The rant is specific to brochure sites. - SSR. Weird. Do devs really think that? I guess I'm out of touch or my colleagues are better than most. - A lot of rants seem to be ripping of jr devs. Show them the way! Very yells at cloud. - CSS nesting I really like. Example of nesting elements inside a class is a hack that should not be done. Nesting is easier to read if done properly. reply est 13 hours agoprevSPA shouldn't be used for content heavy web pages anyway. SPAs today especially React were basically like Facebook's engineers decided to write js/css/html in the style of php. reply teaearlgraycold 14 hours agoprevMost app’s code sucks but it’s honestly not the fault of React or styled components or server side rendering etc. It’s the fault of people treating front end like it’s easy and can have any old fool thrown at it. The truth is self-contained pages allow for more errors per line of code with fewer crashed sessions than a comparably bad backend with highly coupled modules. So hiring managers and bootcamps throw inexperienced devs at front ends until the tickets get closed. I’ve been doing this as a hobby since 2003, writing all capstags and FTPing .htm files to a web host. I know a bit about how we got here. If you’re really this pissed off you should get a new job and build everything how you want it. I’ve work on greenfield projects almost exclusively for my entire career. It can be done. reply nojvek 7 hours agoprevFor more than a decade, worked with many large Single page applications (SPAs) across startups and large companies. He has some good points regarding SASS. Other bits were about ranting. The issue with frontend dev is there is a gajillion bootcamps promoting it so we have a bell curve weighted heavily weighted towards juniors. Add in browser compatibility issues, framework churn and other headaches, there are fewer highly experienced FE devs who understand the browser in a deep way. FE code also tends to be highly stateful. Managing state well is one of the harder FE problems that few folks talk about. Add in the mix that some are apps and some are websites. There is no one right answer. reply robertoandred 13 hours agoprevYawn, another case of “new is bad because I don’t understand it.” Being nostalgic for jQuery? Really? reply brigadier132 13 hours agoprevI hate this style of writing. reply MrVandemar 13 hours agoparentI don't mind it in small doses. That was the first article written like that I've read in a long time, and I enjoyed it, and laughed at some of it, and the topic certainly resonates with me. reply janalsncm 13 hours agoparentprevTend to agree. The “piledrive” article yesterday was the same. It’s not that the author didn’t have good points, and it’s not that I even mind profanity. I just think the shocking language is a stand-in for a stronger argument. reply Aeolun 13 hours agorootparentThat’s generally the case with frustration. Don’t want to spend too much effort on your argument because you know nobody will listen. reply t1c 14 hours agoprevThis screams \"old man yells at cloud\". Boo hoo, someone used an unordered list for numbered content? Who cares. reply MrVandemar 13 hours agoparentDoing something the right way is: (a) less work (b) scales from 10 items to 1,000 to 1,000,000 items effortlessly (c) works the same no matter what number system you have localised, such as greek, chinese, icelandic and arabic. As in any profession: use the right tools for the job, otherwise you're bashing in a nail with a screwdriver and scorning the guy using a hammer. reply graftak 13 hours agoprevIn 2001 the EU switched (mostly) from a local currency to the euro. For years people would calculate prices back to the currency of old. If you do it nowadays, over 2 decades later, people look at you funny. People who still rant about the simplicity of jQuery are of the same cloth. reply grishka 12 hours agoparentExcept that unification did give enormous benefits, it allowed those countries to trade more freely. The horribly inefficient JS-ass 5-megabyte-bundle SPA blogs benefit no one in the long run. By the way, on my recent visit to Europe, there still was a total in francs \"for information\" on the receipt from a random grocery store in Paris. reply awelxtr 10 hours agorootparent> By the way, on my recent visit to Europe, there still was a total in francs \"for information\" on the receipt from a random grocery store in Paris. As an european: that's a problem. Why? Inflation. People who still change regularly to the old currency only do so for some of their finances AND romanticise the past. reply lmm 14 hours agoprev [–] Wow, I think this is the first time I've seen a page be wrong about so many things at once. No, the difference between header and section or p and div is not important (after you started off so promisingly by saying that content is what's important - the difference between a p and a div is not content). No, CSS really is the problem. No, you can't actually do server-side rendering in a different technology stack if you want the term \"rendering\" to mean anything (as much as semantic arguments are pointless in any case). Yes, JavaScript is actually the least-bad solution to every problem in web space; it's not ideal but it's a proper programming language that isn't completely nutso (just mostly nutso), which is more than you can say about CSS or HTML. If you actually want your life to be simpler, use React for everything, like everyone else. Stop worrying about how many dependencies you have and wasting your time reimplementing them worse. reply Skinney 13 hours agoparentScreen readers and other assistive technology fully expects your HTML to be semantic. It doesn’t always matter, but it matters often enough. CSS tends to be hard because (1) people insists on having a single page with a single stylesheet and (2) only divs are used, and so they use class names for everything. If you didn’t use a SPA you’d have scoping. If you used semantic HTML you could target based on semantic structure. Both of these makes CSS easier. JavaScript is fine to use if you have to. But using JavaScript when you don’t is just wasting time. I’ve seen people essentially re-implement defaultbehaviour too many times to count, and spend time looking for solutions to problems they only have because they insist on using technology for making interactive sites when they’re making something largely static. If you want a simpler life, choose the simplest technology that will solve your problem. reply lmm 13 hours agorootparent> Screen readers and other assistive technology fully expects your HTML to be semantic. No they don't. They were developed to work on the web as it existed at the time and handle it fine. A screenreader doesn't magically do something different and better with antag than atag, and only confused thinking and propaganda has made people think so. (And anyone who actually cared enough to test with a screenreader would know this - but people don't actually care about screenreaders, they just use them as a stick to beat technologies they don't like) > CSS tends to be hard because (1) people insists on having a single page with a single stylesheet and (2) only divs are used, and so they use class names for everything. > If you didn’t use a SPA you’d have scoping. If you used semantic HTML you could target based on semantic structure. Both of these makes CSS easier. You still need class names, because you're always going to have some distinctions that matter in your domain but can't be mapped onto the fixed list of HTML tags. So either you use divs and class names for everything, or you use a mix of tags and use a mix of class names and tag names, which just ends up being less consistent and more confusing. You have components that are shared between pages, so you either have to include all your CSS on every page, you manually figure out which components are used on which pages and make mistakes sometimes (leaving you with missing styles), or you use a Javascript UI framework. > JavaScript is fine to use if you have to. But using JavaScript when you don’t is just wasting time. I find the opposite. You're always going to need JavaScript eventually. Trying to do stuff with CSS and HTML tags just wastes your time and the end result ends up being more complicated because it's a mishmash of both. If you use a well-known JavaScript UI framework from day 1, it's so much easier to end up with a simple, consistent, maintainable site. reply Skinney 12 hours agorootparent> No they don't. They do. They use tags and semantic structure to figure out what to read when. You also skipped the \"and other assistive technology\" part of my sentence. Tools for navigating, or alternative rendering, improves when your site is semantic. I use these tools every now and again, I'm not basing this on theory. > You still need class names, Not as many. If you use divs and classes everywhere, you end up using classes for semantic meaning. > You have components that are shared between pages, so you either have to include all your CSS on every page Use classless CSS as far as you can, use site-specific (layout) stylesheets, and use webcomponents for those components that are truly novel. Webcomponents with shadow dom means it can include it's own stylesheet without affecting the rest of the site. In many cases, you get by with just classless CSS. Site-specific layout is a one-liner. Webcomponents bring their own styles. So no, you don't need to bring an entire JS UI framework for this. > You're always going to need JavaScript eventually Depends entirely on what you're building, and a lot of the time you can get away with something simple like HTMX and JQuery. But even if you do end up needing a JS framework for some portion of your site, limiting the amount of stuff that needs to be handled by this framework _greatly_ reduces the amount of complexity, and amount of code, that the developer has to write. reply thesuavefactor 13 hours agoparentprevEverything you just wrote is either wrong or even exactly the opposite. The author of the article might be blunt and you might not like that in your mind things seem different, but the problems he highlights are real. reply briandear 13 hours agoparentprev [–] And attitudes like this are why we don’t have nice things reply lmm 13 hours agorootparent [–] On the contrary. The reason front-end development is so much worse than regular development is some bizarre collective fetish for avoiding Javascript, avoiding dependencies, and writing ad hoc, informally-specified, bug-ridden, slow implementations of half of React. It's some kind of Luddism meets Protestant work ethic nonsense, and I'm continually baffled that otherwise intelligent people - people who would never have this kind of attitude in regular programming - keep falling into it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author, a seasoned front-end developer, expresses frustration with the complexity introduced by certain choices in front-end development.",
      "Emphasizes the importance of content over frameworks like React, advocating for semantic, SEO-ready, and accessible HTML.",
      "Criticizes the overuse of JavaScript and complex tools, suggesting a focus on simplicity and proper use of CSS and server-side rendering technologies."
    ],
    "commentSummary": [
      "A blog post criticized modern front-end development practices, likening them to fast food—functional but lacking depth.",
      "The author advocated for server-side rendering (SSR) and minimal JavaScript (JS) use, highlighting tools like HTMX and Alpine JS.",
      "The discussion revealed a divide between proponents of traditional web development (simpler HTML and CSS) and those favoring modern JS frameworks like React for dynamic sites."
    ],
    "points": 112,
    "commentCount": 200,
    "retryCount": 0,
    "time": 1718944298
  },
  {
    "id": 40745830,
    "title": "Bomb Jack display hardware",
    "originLink": "https://github.com/martinpiper/BombJack/blob/master/README.md",
    "originBody": "Bomb Jack display hardware Main social media: https://mastodon.social/@martin_piper Don't have a Mastodon account? Click here for an invitation link: https://mastodon.social/invite/nuGxMC8z Itch IO project page: https://martin-piper.itch.io/bomb-jack-display-hardware YouTube channel: https://www.youtube.com/c/MartinPiper6502 Buy me a ko-fi: https://ko-fi.com/martinpiper0750 Bluesky: https://bsky.app/profile/martinpiper.bsky.social Twitter links: https://twitter.com/MartinPiper https://twitter.com/hashtag/RetroTTLVideoCard https://twitter.com/hashtag/RetroTTL Threads: https://www.threads.net/@martinpiper1187 Mostly using these tags: #MegaWang #RetroTTL #RetroTTLVideoCard #C64 #Commodore64 #electronics This started life as a working schematic for the video display portion of the original Bomb Jack arcade hardware. Why bother, you're probably asking? Well if you have to ask then perhaps you're not the right audience. :) The version 1.0 of this work was approximately interface pin and chip compatible with the original hardware schematic. Now since V2.0 the hardware has progressed to add extra features not present in the original, such as: Addressable RAM instead of ROMs Extra display blanking capability Tile X/Y pixel scroll 64x64 Tile X/Y offset, useful for fast hardware scrolling Full screen height sprites Background \"mode7\" screen that allows per-pixel affine transformation Background colour select (Can be rapidly updated during the frame for plasma effects) Background This project started when old retro arcade hardware was being discussed. In the back of my mind was the often fabled \"Mega games\" by Imagine Software which were planned to use extra hardware on the Spectrum and Commodore 64 to augment the machine's capabilities. Since this hardware uses TTL logic available back from the same time period I was wondering exactly how much extra graphical grunt could have been engineered and interfaced with these old 8-bit computers. Truth be told, the Imagine hardware was pretty much just extra RAM https://www.gamesthatwerent.com/gtw64/mega-games/ but this was a fun project to see how far the arcade hardware was pushing the limits of board size and signal complexity. I was looking at Bomb Jack boards on ebay and pondering how they had enough fill-rate to draw 24 16x16 sprites and have the option for some to use 32x32 mode as well. A friend and I were discussing the clock speed and fill-rate while trying to deduce the operation of the hardware just by inspecting the hand drawn schematics, as you do. In the end to get some clarity on the sprite plotting specifically I started to transcribe what was thought to be the sprite logic portion of the schematic into Proteus, since it can simulate digital electronics really well. Interestingly the Amiga computer, with its custom chips, was designed and prototyped using breadboards: https://arstechnica.com/gadgets/2007/08/a-history-of-the-amiga-part-3/ Digital display and audio simulation The arcade hardware uses resistor ladders to convert 4x3 bit RGB+H+V sync digital signals into analog, this is a typical model for the period. I later updated the design to use 15 bit colour with 16x16 palette entries. Since the simulation software is optimised for digital model it makes sense to have a digital display driver simulation that accepts digital signals. Hence I created this Proteus VSM module: https://github.com/martinpiper/DigitalDisplay This simulation can also be used to debug failures in original boards, since disabling chips or signals in the simulation can produce similar output display artefacts. For example disabling one of the scan line RAM chips (4A/4B/4C/4D) produces alternate pixels and bright lines, similar to the original hardware. Why not use a CPLD or FPGA? Good question! For my thinking, it is a question of what level to want to teach. In software eningeering there are numerous languages, C/C++, Python, Java, Forth, Pascal, etc. There is also machine code and usually some form of assembler mnemonic language which directly maps to machine code, the user accessible low-level language used by a particular CPU. Below that there can be microcode which is often not user accesible, this tends to control the internal state of a CPU. As a software engineer, I see value in using higher level languages, but often use low-level assembler when I want to optimise. A FPGA or CPLD, while it deals with logical operations, is usually programmed via a higher level language (VHDL or Verilog or similar) abstraction above the low-level logic cells actually used by the device, often (not always) the same code can be compiled for subtly different devices with minimal changes. 7400-series discrete logic, specifically 74LS series logic used in this project, can be considered to be low-level, analogous to using machine code or assembler. There is a lower level, which is using discrete transistors to create logic cells, like NAND, OR, NOT, etc. but that really is a low level more like microcode. The use of high level or low-level programming is a contextual decision, often my knowledge of low-level architecture and performace informs my decision making processes when using high level languages. While it is certainly possible to code an application entirely with only high level language knowledge, it is an advantage to know low-level architecture to help debug hopefully rare performance or error situations. All that said, I believe that knowing how to engineer a solution using low-level discrete logic is an advantage, in other words it helps to improve the use of higher level CPLD or FPGA engineering to know lower level architecture. This is why this project exists, to help teach how to solve engineering problems using low-level discrete logic. Developer technical details To produce the test data run: BDD6502\\ForUnitTests.bat If you have Proteus setup with the digital display model, you should see a screen that looks somewhat like this when starting the simulation: The new hardware scrolling registers also work well: The hardware supports sprite multiplexing and stacking, giving the option of having more than eight colours in sprites: Many thanks to https://opengameart.org/content/rpg-town-pixel-art-assets and https://opengameart.org/content/colored-16x16-fantasy-tileset for the game art. High level schematic sheets The Proteus sheet numbers correspond to the original schematic page numbers as written in the bottom right hand corner of each page. The layout of this schematic roughly matches the layout of the original schematic. So for example the analog RGB resistor ladders are on \"Root sheet 8\" in the top right of the sheet, as in the original schematic page 8 layout. Logic analyser, virtual video display model, expansion bus header, data generator Memory selection logic based on external address bus VSMDD2 is a replacement for the Proteus data generators because they have a 1024 byte limit. Proteus VSM project source: https://github.com/martinpiper/DigitalData EXPANSIONBUS, EXPANSIONBUS2 and EXPANSIONBUS3 all relate to the C64 user port to 24 bit address logic in the project: UserPortTo24BitAddress.pdsprj Headers, the original schematic has dip switch logic Video timing Horizontal video signal generation Vertical video signal generation HBLANK and VBLANK signal generation Sprite logic - Part 1 Sprite RAM register access logic, timing with the external address/data bus and video internal timings Temporary storage for sprite position, palette and frame Bit plane access and bit shifters for output pixels to scan RAM 32x32 sprite selection registers and comparison Various signal timings generated for sprite loading, tile and colour fetch etc Sprite logic - Part 2 Dual scan line RAM buffers Transparent pixel test logic based on inverted pixel temporary storage and selection with NOR check Pixel writing to RAM based on 16 pixel chunks and position register contents Logic for timing of pixel reads for possible video display, plus clearing of data just read Character screen RAM with associated colour screen Associated external bus interface logic Bit plane reads and pixel shifters Output pixel logic for possible video display 16x16 tile background picture logic Logic for selection of appropriate picture, based on high address lines Bit plane reads and pixel shifters Output pixel logic for possible video display, or disable background logic Output pixel pipeline and palette lookup Pixel transparency test from background, character screen and sprites Palette RAMs with external bus logic Final pixel latching logic and palette RAM lookup Output digital to analog conversion using resistor ladders Analog RGB and associated sync signal output header Mode7 calculation Register addressing and latches dx, dxy, dy, dyx calculation xorg, yorg addition Mode7 pixel logic Background colour latch, transparent pixel detection Interleaved blocks (for RAM timing) mode7 screen access, feeding into tile access, H & V flip logic Scaled sprite logic - Part 1 Register memory Sprite extent and scan line detection Sprite flipping Scaled sprite logic - Part 2 Scanline - Two alternating banks Sprite definition RAM Vector logic Register logic - Bank display and kill Scan RAM banks - Two banks Pixel data combiner (merge layer) logic 2-1 pixel data combiners - Two blocks How to access the hardware In the original hardware, used in the Bombjack arcade game, the Z80 data and its address bus was partially connected to the hardware with a couple of address windows at $9000 to $9fff (perhaps a bit wider than that), this was mostly write RAM as far as the Z80 was concerned. It was sprite registers, some control registers, palette and char screen RAM. The rest of the video internal memory was ROM, and could not be addressed by the Z80. The Z80 would have to time writes to the video address space (RAM) during the VBLANK when the video hardware was not expecting to read RAM to avoid contention. For this updated hardware, the C64 (or a Z80) sends data bytes via the user port (parallel transfer, so it's fast, one clock cycle to write), with some control signals, to write to a 24 bit address space in the hardware. This is usually accomplished by the UserPortTo24BitAddress.pdsprj (or better) project. Basically a control signal resets the internal state, the next three bytes are the address to write, then next bytes are consecutively written to the address space until the internal state is reset again. There is a signal from the hardware to the C64 via the user port for screen refresh (VBLANK) which is read as an NMI (CIA2 FLAG pin) request on the C64. This means there is no bus contention with the C64 memory bus to the hardware. Writes to hardware RAM take precedence over what the hardware is reading from the RAM, this ensures data is written without contention. Writes to the hardware which involve RAM, rather than register latches, should usually be timed to coincide with VBLANK or HBLANK to avoid visual glitches. Each layer (chars, tiles, sprites etc) provides information on any RAM timing constraints. Memory map Note: Most control registers can be updated at any point during the frame and the next pixel will reflect the change. 0x9000 Chars control register 0x9c00-0x9dff Palette RAM 0x9e00 0x10 Use background colour 0x20 Enable display 0x80 BorderY shrink 0x9e08 Layer priority select, for each layer select the input header pixel input Default should be: %11100100 = 0xe4 i.e. Back most layer = %11 (3) Next layer closer = %10 (2) Then next closest = %01 (1) Then front most layer = %00 (0) 0x9e09 0xlg Setup nybble extents for overscan to enable the border (g = greater than, and l = less than) in 8 pixel steps Overscan mode is always on, so this needs to be set to sensible values Sensible wide value for this is $29 which hides the left edge (including 16x16 tiles) and brings in the right edge in slightly This gives a 320 pixel wide screen, displaying a full width of 40 8x8 characters 0x9e0a Video layer enable flags. These are passed through the VideoX connectors. Each layer has the option of reading their enable flag and taking appropriate action. Bits Action Default layer assigned to this connector (can be changed) Replaces logic 0x01 Enable layer 1 Sprites (or Sprites2) 0x9a00 Upper nybble: $10 = Enable sprite output (6S SPREN) 0x02 Enable layer 2 Chars 0x9000 Chars control register 0x02 Chars screen disable 0x04 Enable layer 3 Tiles 0x9e00 0x10 Enable tiles 0x08 Enable layer 4 Background or mode7 0xa015 : Flags : 0 : Enable display 0x9e0b Background colour If the final pixel is transparent and 0x9e00 0x10 is set, then the palette information is read from this colour value If the final pixel is transparent and 0x9e00 0x10 is not set, then the palette information is read from the last layer colour value 0x9e0c Palette bank number See: kBus24Bit_VideoLayer_ExpandedPalettes Sprite 32x32 size select 0x9a00 start index of 32x32 sprites 0x9a01 end index of 32x32 sprites (exclusive) So 0,0 = no 32x32 sprites 0,8 means indexes 2 to 7 (exclusive) are 32x32 Maximum number in both is 0xf At 0x9800 - 0x985f each sprite is described by 4 bytes: Byte 0:The tile code for the sprite, used to look up the sprite’s image bitplanes in the tile ROMs(MAME Emu documentation is wrong, bit 7 has nothing to do with selecting double size mode) Byte 1: HVFMCCCCBit 7: H : If set, the sprite is horizontally flippedBit 6: V : If set, the sprite is vertically flippedBit 5: F : If set, the sprite is full screen height. The sprite data will repeat.Bit 4: MSBXBits 3..0: C : 4 bits to provide the colour value for the tile decoder Byte 2: The sprite’s Y position on screen Byte 3: The sprite’s X position on screen 0x9200 Sprites2 registers Sprites support X and Y flips with X & Y repeating patterns Byte 0: Palette0x10 = MSBX0x20 = MSBY0x40 = flipX0x80 = flipY Byte 1: Y pos Byte 2: Y size (in screen pixels, regardless of scale) (0 = end of list) Byte 3: X pos Byte 4: X scale extent (uses internal coordinates) Byte 5: Y inv scale (*32) Byte 6: X inv scale (*32) Byte 7: Sprite frame (index)0x40 = halfX0x80 = halfY Tile and sprite data layout 16x16 pixel tile and sprite data is stored in 8x8 cells and in memory order: 01 23 For 32x32 sprites, this arrangement is expanded in memory as: 0145 2367 89cd abef Mode7 registers Mode7 borrows its name from the graphics mode on the Super NES video game console. https://en.wikipedia.org/wiki/Mode_7 0xa000 - 0xa011 : Sets of 24 bit (8 byte) registers in l/h/hh order. There are 8 bits of accuracy, i.e. numbers are multiplied by 256.0f dx dxy dy dyx xorg yorg Each pixel on the layer is accessed using the accumulated results of these registers, this means it is possible to have per-pixel transformations combining translation, scaling, reflection, rotation, and shearing. It is possible to update these registers per scanline, or per pixel, and generate even more complex results. Register values are latched and used immediately, so timing the update of the three bytes must be considered. Note: The internal accumulated values, x/xy/y/yx are not accessible via registers and are reset only by _EHSYNC and _EVSYNC For each horizontal pixel (on +ve 6MHz): x += dx yx += dyx xo = x + xy + xorg yo = y + yx + yorg For each scanline (on +ve _EHSYNC): xy += dxy y += dy For each scanline (on _EHSYNC): yx = 0 x = 0 For each frame (on _EVSYNC): y = 0 xy = 0 All of the above is one way to progressively calculate xo,yo using addition. In other words: xo = (dx*x) + (dxy*y) + xorg yo = (dy*y) + (dyx*x) + yorg During the coordinate (xo,yo) to screen transformation: bits 8-11 are used to lookup the tile pixel, bits 12-18 for the x screen tile index position, bits 12-17 for the y screen tile index position. Note: The full, but hidden by borders, screen resolution is 384x264 pixels. 0xa014 : Background colour, any pixel index of zero in the mode7 tile data will show the background colour 0xa015 : Flags : 0 : Enable X update, or reset to 0 1 : Enable XY update, or reset to 0 2 : Enable Y update, or reset to 0 3 : Enable YX update, or reset to 0 Audio Current audio hardware has 4 voices, each using 11 bytes. Extra bytes are used for currently unused byte and voice active bit masks after the voice registers. Each voice register layout is as follows: byte register 0 volume 1/2 address lo/hi 3/4 length lo/hi 5/6 rate lo/hi 7/8 loop address lo/hi 9/10 loop length lo/hi The sample frequency in hertz to voice rate conversion, using internal 4MHz clock, is: (4096 * hertz) / 25000 Clock speeds The original schematic uses a 6MHz clock for all the video hardware, as denoted by the \"6MHz\" signal line. The clocking can be found on \"Root sheet 1\" just about the logic analyser. To help debug timing issues, especially with RAMs, the schematic should be simulated at 6MHz. However the Proteus simulation can use 1MHz on this signal line, I kept the naming of the line the same as the original schematic however. This is due to the default RAM write timings for ICs 4A/4B/4C/4D being too tight. This means the digital display driver will detect ~10fps, not ~60fps as per the original design. It does however make the debug single step time easier to think about since it's not divided by 6MHz... With VIDCLK = 2M Logic analyser: Display scale 0.2u Capture Resolution 0.2u Remember to use: System->Set Animation OptionsSingle Step Time: 250n500n For debugging full clock cycle and pixel clock Where is the Z80 CPU? The Z80 CPU from the original schematic is not included, it was clocked independently and has its own RAM, except for the video hardware interfaces and isn't needed for the video simulation. Input data setup The Z80 data write signals are mocked using a simulator pattern generator VSMDD2, this is separate to the main video schematic and the generator is excluded from the PCB layout. As per the original design all writes to the video hardware should be carefully timed to coincide with the VBLANK or other safe write blanking periods. This is because the video hardware is almost always reading the RAM during the visible portion of the frame. Writing to the sprite registers outside the VBLANK will especially produce nasty looking effects on the screen. This RAM sharing model is quite common is old arcade and console hardware. The original hardware has been expanded to include RAMs where the ROMs were located. These are addressed by an combination of EXPANSIONBUS3 to select the groups of RAMs and the EXPANSIONBUS group selector. It is entirely possible to write more than one group at a time by enabling multiple output bits in EXPANSIONBUS3. For EBBS = $01 the RAM address select lines are from $8000 until $b800 in steps of $800 EXPANSIONBUS3 Group EXPANSIONBUS address Behaviour $01 Audio registers $8000 11 bytes/voice Audio registers $01 Original RAMs $9000 Char screen control $01 Original RAMs $9001/2 Char screen X scroll $01 Original RAMs $9003/4 Char screen Y scroll $01 Sprite2 registers $9200-$92ff Sprite2 registers $01 Original RAMs $9800 $60 bytes Sprite registers $01 Original RAMs $9c00 $200 bytes Palette GR XB 2 bytes per entry $01 Original RAMs $9e00 Background image enable and borders $01 Original RAMs $9e01/2 Background image XL/XH pixel scroll $01 Original RAMs $9e03/4 Background image YL/YH pixel scroll $01 Original RAMs $9e07 Background colour select $01 Original RAMs $9e08 Layer priority select $01 Original RAMs $9a00-$9a01 Start/end 32x32 sprite index 0-f only $01 Extension mode7 registers $a000-$a017 Mode7 registers $01 Vectors $a100 1 byte Vectors display bank $01 Combiner $a200 4 bytes Combiner display bank Note: APU and vectors are by default using the same EBBS in the schematics$02 APU $8000 8KB APU memory bank 0 instruction $02 APU $4000 8KB APU memory bank 1 data $02 APU $2000 8KB APU bank 2 registers $02 Vectors $0000 16KB Vectors bank 0 data $02 Vectors $8000 16KB Vectors bank 1 data Note: Audio and Sprites2 are by default using the same EBBS in the schematics$04 Audio memory $0000-$ffff Audio sample memory $04 Sprites2 data $2000 8KB++ Sprites2 bit planes $80 Background 16x16 Root sheet 7 $2000 8KB Tiles and colours into 4P7R $80 Char screen $4000 8KB Chars into U258 $80 Char screen colours $8000 8KB Chars colours into U137 $40 Background 16x16 Root sheet 7 $2000 8KB Tiles bit plane 0 into 8R7R $40 Background 16x16 Root sheet 7 $4000 8KB Tiles bit plane 1 into 8N7R $40 Background 16x16 Root sheet 7 $8000 8KB Tiles bit plane 2 into 8L7R $20 Char screen 8x8 Root sheet 6 $2000 8KB Char data bit plane 0 into 8K6R $20 Char screen 8x8 Root sheet 6 $4000 8KB Char data bit plane 1 into 8H6R $20 Char screen 8x8 Root sheet 6 $8000 8KB Char data bit plane 2 into 8E6R $10 Sprite data Root sheet 4 $2000 8KB Sprite bit plane 0 into 7JR $10 Sprite data Root sheet 4 $4000 8KB Sprite bit plane 1 into 7LR $10 Sprite data Root sheet 4 $8000 8KB Sprite bit plane 2 into 7MR $08 Mode7 Root sheet 10 $2000 8KB Mode7 screen data 128x64 tile index $08 Mode7 Root sheet 10 $4000 8KB Mode7 tile data 32 of 16x16 tiles $08 Mode7 Root sheet 10 $8000 8KB Mode7 tile data 32 of 16x16 tiles Using the pattern file: TestData.txt The sprite writes particularly have various configurations for sprite displays, these are commented in/out depending on the desired patterns ** Comment: Spread out all over the screen This includes background screen setup, char screen data setup and sprite palette, frames and position setup. It provides good variation of sprites, palettes etc across the full range of screen coordinates. This is useful for testing expected masking logic and general visual integrity. ** Comment: Top left all the same This sets all sprites to be in the top left of the screen, it is useful to testing maximum pixel write through and sprite selection logic scenarios. The background is disabled and char screen uses transparent chars. This leaves just the sprite plane outputting pixels. Setting a logic break on RV[0..8] = 0xe8 will allow the simulation timing and scan line RAM contents to be inspected in detail. The timing of sprite pixel writes into scan RAM 4A/4B and pixel reads from scan RAM 4C/4D with its clear to $ff can be seen whilst single stepping. Note the values for the RV and RH bus lines just below the video display. ** Comment: X pos group This regularly spaces the sprites with the same X pos +/-4 pixels. Useful for debugging. ** Comment: Y pos group This regularly spaces the sprites with the same Y pos +/-4 pixels. Useful for debugging. Using the pattern file: TestData2.txt Using the ImageToBitplane tool Run/Debug Configuration: Main all conv Using GenerateData configuration: Debug2 This demonstrates stacked multiplexed sprites over a 16x16 tiled background. Using the ImageToBitplane tool Run/Debug Configuration: Main old bridge RGB 0 0 0 palette opt rgbfactor 32 This demonstrates a detailed background picture with 32 palettes of 8 entries each. Using the ImageToBitplane tool Run/Debug Configuration: Main mode7 \"map_9 - mode7 test.png\" And enabling the data load at: ; Write Mode7 registers This will demonstrate how the mode7 export with flipped tile detection works Raster line schedule Counting from 0 (in computer terms), the first visible display line is 17 and the last visible line is 240, and with the Y border shrink it is 224. _EHSYNC continues its pulses on every line, even when _EVSYNC is low Positive edge of _EHSYNC indicates start of the line Negative edge of _EVSYNC indicates start of the frame Positive edge of VBLANK indicates the bottom of the visible screen edge Note: EXTWANTIRQ = _VBLANK which triggers on the negative edge at raster $f0 RV = vertical raster line number$f8-$ff lo _EVSYNC$00-$ff hi _EVSYNC $10lo VBLANK $e0lo FINALENABLEPIXELS with bordery flag $f0hi VBLANK RH = horizontal pixel clockFull raster line starts at $0 to $17e In reality, RH $0 occurs for two clocks, each subsequent pixel advances with one clock, up to a maximum of $17eGiving 384 pixel clocks per lineMaximum visible portion (when _EHSYNC = 1) is $000 to $15f_EHSYNC = 0 (HBLANK portion of the display where nothing is visible) is $160 to $17e (plus the first half of the next $000) SREAD = Sprite register address (lo byte, also maps to ISAB[0..6] in the schematic) Line RH starts at $000 $000 SREAD $00 Begin sprite 0 register reads 1V*. H lo to hi _EHSYNC (in second clock for $000) $002 SREAD $01 $004 SREAD $02 $006 SREAD $03 $008SREAD $00 to $03 again $008H OV0/1/2 LHH OC0/1/2/3 HLHL from M0-M7 $008L All L no data from $008H clocked into pixel output latch at 5E2 $009H Pixel data is latched into 5E2 $009H Begin sprite 0 pixel writes U77 First pixel written $00aH U77 Second pixel written $00a Pixels start arriving at the real video output, resistor ladders Pixel also cleared to $ff in U78 $010 SREAD $04 same 4x2 repeating pattern every 2 pixels Begin sprite 1 register reads Meaning 16 pixels elapse for each sprite $012 lo to hi FINALENABLEPIXELS (with overscan $29) $0c0 SREAD $30 same 4x2 pattern as above $152 hi to lo FINALENABLEPIXELS (with overscan $29) $160 lo _EHSYNC $17e hi _EHSYNC (into first half of $000) $17e SREAD $7f Loops back to $000 again and RV increment The file \"BombJack\\RecordingRHSprAddr.txt\" contains recorded address and RH values from Root Sheet 4 device VSMDD5. This clearly shows (sparse extracts)RecordingIndex Address @RHd$000000000$00$000d$001000404$10$040d$002000808$20$080d$003000c012$30$0c0d$0040010016$40$100d$0050014020$50$140 Sprite scan RAM logic A sprite in 16x16 mode, with a Y register value of 224, which uses internal inverted logic, will have a calculated sprite Y coordinate of 256 - 224 = 32, this will be the last calculated scan line of the sprite. The result of the scan line calculation will be rendered and visible on the next scan line 33. 15 scan lines above calculated scan line 32 is the first calculated scan line of the sprite, in this example scan line 17, and the next scan line 18 is the first visible scan line of the sprite. Assume the sprite scan line RAM has been cleared by a previous scan rendering all the pixels and clearing the values of $ff.Old revision: The weak pull-up resisters connected to the RAM output help to clear the RAM values to $ff These are overidden by any pull down logic 0 from the RAM or the input 74258 demultiplexorsNew revision: Uses digital logic to switch between $ff and input values as needed. This is much more stable than using pull-up resistors which needed to be tuned for the actual TTL chips used. For each sprite the index, colour, X and Y pos are readAfter a successful Y pos test with the vertical raster position... The sprite data counter is set with the X position of the sprite, then for 16 (or 32) pixels The data in the scan line RAM is read on _6MHz = 0 On +ve edge _6MHz this read value is latched by 2C/2D 2B/2A, and inverted for output The value is then tested for transparency with 1C:B or 1C:C 3-input NOR gates If it is transparent, the value from the input pixel from the current sprite is output by 3C/3D 3B/3A by the 74258 demultiplexors, which inverts the information. This could be a transparent pixel with its colour, or an opaque pixel with its colour. If it is not transparent, an opaque sprite pixel exists for this position, then the value just read from the RAM is selected by the 74258 demultiplexors, inverted and written back to the RAM In effect, the first opaque (non-transparent) pixel from the sprite written to the scan line RAM has priority and the rest of the pixels from sprites are ignored Timing enabling or disabling the sprite display and updating sprite registers allows multiplexing. See: kBus24Bit_VideoLayer_XPos_SpriteIndex_* The sprite scan line schedule, which renders each sprite in 16 pixel chunks, can be syncronised to the video pixel clock, or overclocked. When syncronised to the video pixel clock each 16x16 sprite takes 16 pixels to process. If the sprite clock is overclocked, the scan schedule will correspondingly contract by the ratio of video clock to sprite clock. In other words each sprite will take fewer than 16 pixels to process. Overclocking the sprite pixel clock will alter the timing for sprite multiplexing: kBus24Bit_VideoLayer_XPos_SpriteIndex_* Remember, the scanline before a sprite is visible/displayed on the screen is the actual scanline used for rendering the sprite data.The x position of the sprite does not affect its rendering timing in the scanline, only the sprite register address used affects its rendering schedule time, see: Raster line schedule APU - What is it The Advanced Processing Unit is an optimised raster and data transfer processor that can execute several instructions at the same time. It is able to precisely wait for raster H&V positions, transfer data, and perform simple comparisons based on internal data with conditional executions based on the results of the comparison. This significantly reduces load on the CPU and the amount of memory the CPU needs, allowing the CPU to transfer data when it can. There is significant logic internally that allows data to be processed: https://docs.google.com/presentation/d/1Tb4B5-INhH2oiOSG4jfb3BfGGtdT9jPvyWoryOIJNzo/edit?usp=sharing PCB Layout There is a PCB layout design included in the Proteus workspace file. It is deliberately and mostly blank with the exception of adding placement for the external address/data/RW lines, clock, external VBLANK (for IRQ triggering) and video signals output header and the associated resistor ladders. The dual sprite bit plane pull-up resistors are also placed near the edge of the sprites board. The sprite resistors and RAMs are placed to maintain an aesthetic symmetry to the board. There are six boards in the layout: Video generation Character screen Sprites Scaled sprites Tiles Mode7 Previous versions used one large board, but this turned out to be expensive to produce and difficult to get quotations for. Apparently the companies I contacted were not that happy to produce large boards in small numbers. The design was changed to use four smaller boards, this reduces cost if someone wanted to just fabricate the video and character screen. Another benefit is that the layers can be changed and connected in different orders. This allows customisations, for example by having three sprites boards (layers 1 to 3) and one tile board (layer 4), giving a total of 72 sprites and tiles. The layout is configured with a six layer standard signal and power plane setup, using the technology template \"All PCB Six Layer 1.58532mm.LTF\" file. The layout will correctly populate with the auto-placer and default placement options, choose all then schedule before starting. It will then correctly route, again using the default router options. This is so placement can be mostly left to the optimiser while maintaining output video signal lines via the resistor placement. An entire place and route on a decent machine will take approximately 2 hours and result in an output image like: The protoype boards have been built and documented here: https://www.youtube.com/playlist?list=PLAAYJEX1JbCPFzQDl8KOoVQR9_Us1lB46 or this project page: https://martin-piper.itch.io/bomb-jack-display-hardware I use Proteus simulation and PCB design: https://www.labcenter.com/ And PCBWay PCB manufacture: https://pcbway.com/g/Xr8ZrI ( Using the PCBWay referral link gives you a discount and also helps me reduce the cost of my next board. ) References https://github.com/mamedev/mame/blob/master/src/mame/drivers/bombjack.cpp https://floooh.github.io/2018/10/06/bombjack.html https://www.arcade-museum.com/game_detail.php?game_id=7180",
    "commentLink": "https://news.ycombinator.com/item?id=40745830",
    "commentBody": "Bomb Jack display hardware (github.com/martinpiper)108 points by mrandish 15 hours agohidepastfavorite19 comments wellytopness 6 hours agoThank you for posting. This is one of my projects. I was wondering why all the subscribers joined today. reply TacticalCoder 3 hours agoparentWhat's the relation between this and MAME? I don't have the Bomb Jack arcade PCB (I've got others though) but I do have a Pi2JAMMA and, well, Bomb Jack using MAME. Is this project something that could be replacing (partial?) hardware on a real Bomb Jack PCB (not unlike what some are doing with C64 chips, where new replacement can be dropped in place of old broken chips)? reply wellytopness 3 hours agorootparentIt started off as a direct Bombjack arcade hardware replacement, it then grew way beyond what the arcade was capable of. MAME is software, this is hardware, no relation. :) Although interestingly the first version of the hardware did allow me to find a bug in the MAME implementation. reply mrandish 15 hours agoprevI randomly stumbled across this impressive, home brew graphics subsystem for C64 today. The goal seems to be pushing the boundaries of what would have been possible with period-correct, through-hole 74LS parts (no FPGA or ASIC) toward advanced arcade-level graphics hardware. Quite ambitious for a one-person hobby project and, as the video below shows, very successful! https://www.youtube.com/watch?v=MLVZav7mVcI&t reply Dwedit 10 hours agoparentWhat about ULAs? That's what the ZX spectrum used. They're not quite FPGAs, and not quite ASICs. reply ikari_pl 13 hours agoparentprevis it C64-specific though? reply ralferoo 11 hours agorootparentThere are numerous mentions to Z80 as well as C64, and the arcade hardware was Z80 and this project started life as a plug-in compatible board, so presumably it still has support for being driven by Z80. If it started life as a Z80 board, probably the C64 interface is just synthesising _RD, _WR and _MREQ (and maybe _IORQ) that a Z80 design would need from the RD/_WR, clock and high address bits on 6502. reply wellytopness 5 hours agorootparent8 data bits, write signal, reset address state logic. Each byte written will prime the 24 bit address to write to in the hardware and then store bytes with auto-incrementing address. This lets the C64, or any other machine capable of generating those signals, to write large amounts of memory to the hardware. reply Ylpertnodi 10 hours agoprevI have to admit, and it only happened once, i threw a sickie at work (92?) to finish playing bombjack. Another game I'm looking for is 'prensorium'. reply chrisallick 14 hours agoprevjesus dude... thats amazing. reply demondemidi 12 hours agoprev [10 more] [flagged] unleaded 45 minutes agoparentThe correlation between old hardware stuff and nostalgia is not always as rigid as people on HN tend to think. a lot of stuff like this is made by people born years after the source media(?) (I don't think this is though) reply kstrauser 2 hours agoparentprevIt's \"just\" someone making a graphics card in their free time for the hell of making one. That \"just\" is doing some heavy lifting. reply CodeCompost 10 hours agoparentprevThere is value in going back in time to revisit old hardware. It helps you gain insight into technology today and who knows inspire someone to take a path that was missed back then. This may lead to original ideas. reply stavros 10 hours agoparentprev [–] Build something original then! reply wellytopness 5 hours agorootparentThis is original :) It includes new functionality like multiple layers, layer priority, multiple palettes, scaled sprites, mode7, dithering, etc reply stavros 5 hours agorootparentI know, I was replying to the GP, whose definition of \"original\" this didn't fit (and that's fine, but, instead of complaining about it, they should build things). reply wellytopness 5 hours agorootparentAhh:) reply demondemidi 4 hours agorootparentprev [3 more] [flagged] stavros 4 hours agorootparentUnless they build something that you deem not original? reply Razengan 2 hours agorootparentprev [–] So you would rather waste time in spreading negativity than positivity? Ok. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Bomb Jack Display Hardware project, now in version 2.0, started as a schematic for Bomb Jack arcade hardware and includes features like addressable RAM, extra display blanking, and full-screen height sprites.",
      "The project uses TTL (Transistor-Transistor Logic) to explore graphical enhancements for 8-bit computers, focusing on teaching low-level discrete logic rather than using CPLD (Complex Programmable Logic Device) or FPGA (Field-Programmable Gate Array).",
      "The hardware design includes six PCB (Printed Circuit Board) layouts for various functions and uses Proteus for simulation and PCBWay for manufacturing, aiming for cost efficiency and customization."
    ],
    "commentSummary": [
      "A new project aims to replace the Bomb Jack arcade hardware with a modern equivalent, expanding beyond its initial scope.",
      "The project, distinct from MAME (Multiple Arcade Machine Emulator), is a hardware solution that has even helped identify a bug in MAME.",
      "The hardware features advanced capabilities like multiple layers, layer priority, multiple palettes, scaled sprites, and more, pushing the boundaries of period-correct components."
    ],
    "points": 108,
    "commentCount": 19,
    "retryCount": 0,
    "time": 1718940326
  },
  {
    "id": 40744847,
    "title": "Tesla owners file class-action alleging repair, parts monopoly",
    "originLink": "https://driving.ca/auto-news/industry/tesla-class-action-lawsuite-parts-repairs-monopoly",
    "originBody": "Tesla owners file class-action alleging repair, parts monopoly Trending",
    "commentLink": "https://news.ycombinator.com/item?id=40744847",
    "commentBody": "Tesla owners file class-action alleging repair, parts monopoly (driving.ca)108 points by RickJWagner 18 hours agohidepastfavorite60 comments blindriver 18 hours agoIf you get into an accident, it takes months to get your parts to fix it. What makes it worse is that most car rental contracts via insurance will only cover up to 30 days, so you end up being on the hook for the car rental. This happened to me last year this time, I lost the car for the entire summer which was frustrating. reply cfeduke 17 hours agoparentMy personal experience so far with Tesla specifically - and this is not to say they are doing the right thing in regards to parts or anything - was when my wife hit a deer and a bunch of the driver's front side of the car had to be replaced was that it took six days. Of course the bill insurance covered was around $12K which is just insane (headlight, hood, fender, panel, side mirror, camera). The timeline to repair was probably because the Tesla density for my area (around Richmond, VA) is not high like it is in places such as California. Other than that, which was $0 out of pocket, I've had my Model 3 for six years in July and almost $0 in parts or service. I replaced the 12v battery which I had to buy for $75 and tires twice. This way more cost effective than my Lexus (around $4,000 in repair service over 5 years, not counting tires) and Mercedes (around $3,000 in repair service over 2 years, which - not counting tires and I had these AMG rims that were wider on the rear tires so tire replacement had to happen early). So its hard for me to complain about Tesla, but I can see where for other people who have had issues where there are a lot of Teslas on the road could be an issue. reply SkyPuncher 14 hours agorootparentThose repair costs are astronomical. Going 4 years on our Hyundai and we’ve literally only had to replace tires. Both of our prior vehicles, Chevys, only required tires and brakes outside of a single, major repair - melted catalytic converter. That would have been a $2k repair tops over respective 8 and 18 year life span of both of those vehicles. reply bufferoverflow 13 hours agorootparentprevAlso Tesla is much cheaper to drive than a Lexus. reply radpanda 17 hours agoparentprev> most car rental contracts via insurance will only cover up to 30 days, so you end up being on the hook for the car rental That’s if your insurance is covering it right? My Volvo C30 got rear-ended a few years back and it took about 6 weeks to get all the parts in and get the repair done. The insurer of the at-fault driver covered my rental car the entire time. reply Reubachi 5 hours agorootparentIt's always a bit different. I've both had to fight an at fault motorist's insurance on extended rental coverage, and I've also been hand delivered a rental and told to keep it until I'm comfortable with my repaired car by another at fault motorist's insurance. Ultimately, insurance is in the game of saving money, avoiding court, and reputation damage mitigation. Sometimes you get an insurer that puts their reputation above saving money, but it falls on the claim and the agent to decide how to go about it. reply eurleif 17 hours agoparentprevI was quite worried about this when I crashed my car earlier this year from reading comments like this one; but it was in and out of the (Tesla-approved third party) repair shop in around three weeks. It wasn't the worst crash in the world, but some body panels and other parts had to be replaced. Obviously, this is just my own anecdote, but I think it's worth noting that not everyone has the same experience. reply oplav 18 hours agoparentprevI’m running into a similar situation with an electric Ego lawn mower. 1 year old mower stopped turning on, took it to the authorized repair center for warranty work, some PCB needs to be replaced but it’s back ordered for months. I ended up having to buy another lawn mower for the summer. I’m grateful for the easy warranty process that Ego has, but it’s got me thinking if shifting to electrical means harder to come by parts for repair. reply syedkarim 17 hours agorootparentWhat’s strange is that it’s just a PCB. It should not be an issue for them to stock or quick-build a circuit board. They can’t possibly still blame the chip-shortage bogeyman. reply FireBeyond 17 hours agorootparentInterestingly, this is one of the theories for why Tesla's parts situation sucks. While you could also argue that they make things complex for themselves by having these rolling updates rather than using a Model Year system... Elon, as is well known, is FANATICAL about the quarterly numbers. Burn the midnight oil to pump up the deliveries, etc., etc. The thing with that, every part that's on a shelf waiting to be sold for warranty or accident repair is a part that can't go on a new car and boost the numbers. Wall Street doesn't give a shit what your parts market numbers look like, it's \"How many Teslas did you build this quarter?\" so there's a lot less incentive to fully stock that market - Tesla already has the sale booked and the money in their balance book. reply shalmanese 10 hours agorootparentThat may be a theory but there's a much more mundane theory that explains the crunch of the last few years: Tesla sold about 100,000 Model 3/Ys to Hertz for rental in 2021. Hertz rented a ton of EVs to people who were only used to the acceleration curve of ICE cars. A ton of those people unexpectedly crashed those cars [1]. This clogged up Tesla's repair channels and blew out the timeline for everyone else. [1] https://edition.cnn.com/2024/01/18/business/why-do-people-ke... reply FireBeyond 3 hours agorootparent> Hertz rented a ton of EVs to people who were only used to the acceleration curve of ICE cars. Maybe, but I am not as sure. I've not ridden in a private 3, but in a Y and S, and I had a Hertz Model 3 for a week. I don't know if there was a firmware difference or such, or just my individual model, but the acceleration on it was garbage. I'd stop, put my foot all the way to the floor, and wait half a second or more for ANY movement, and when it did move, it accelerated like my girlfriend's A4, if not slower. (Or, perhaps, things happened as you described, and as a result, they nerfed the acceleration - this was last year). reply Retric 17 hours agorootparentprevWhich is strange from a financial standpoint. Individual parts generally have a massive markup compared to the total cost of a car. There was some magazine that once did this with a Honda motorcycle and the total cost for parts was 6x what the bike would cost when new. reply nxicvyvy 16 hours agorootparentIt's not a financial standpoint, it's a perception standpoint. reply SkyPuncher 14 hours agorootparentprevI’ve entirely stopped caring about warranty as it’s only ever left me in this situation. I’d rather just buy a new thing or pay for an out of warranty channel to fix it as quickly as possible (often possible with non-one parts). reply 2OEH8eoCRo0 17 hours agorootparentprevMy father's gas lawnmower is ~40 years old and never had an issue. Snowblower is 50+ years old but has had 1 engine replacement. reply WheatMillington 17 hours agorootparentTo be fair, the only gasoline engines with this kind of lifespan have had significant maintenance over their lifetimes. Regular oil changes, spark plugs and air filters and other servicing. The idea that gasoline engines are inherently more reliable than electric just doesn't match the data. reply cj 17 hours agorootparentprevFor what it’s worth, I bought a new gas mower at Lowe’s last summer, and had to buy a new one this summer because I couldn’t get the old one started. Granted, I probably could have spent a few days figuring out how to repair the old gas powered mower from last year, but it was oozing oil everywhere and I would have just been following YouTube tutorials and endless trips to the hardware store or waiting on parts from Amazon. (Just sharing this anecdote since neither gas nor electric are perfect!) reply vel0city 17 hours agorootparentprevAnecdotes everywhere. A friend of mine spends an hour or two rebuilding or cleaning his gas lawnmower every year. Another friend had a brand new one fail the first time he fired it up. Meanwhile my Ego has gone several years with literally zero maintenance other than five minute blade sharpening. My dad's gas trimmer was constantly unreliable and never easy to start. My Ego trimmer once again has been fine for several years without any maintenance other than winding the string. reply giraffe_lady 17 hours agorootparentprevWhen electric lawnmowers have been around for 50 years we'll have the same anecdotes about them. reply 2OEH8eoCRo0 5 hours agorootparenthttps://oldlawnmowerclub.co.uk/forum/history-and-technical/m... reply steelframe 17 hours agoprevMy fresh-off-the-lot 2024 Toyota GR Corolla got nearly its entire side mangled by a red light runner on day 8 of ownership. Mind you this is a brand-new model, first released in 2023. Once my insurance finished dragging their heels on doing their assessment -- as they didn't yet have any of the parts for it in their internal database -- it took another week to get on the schedule with one of the ~20 auto body shops in a 15-mile radius of where I live. An entire door and two windows needed replacing, along with a fender, trim, and so forth. The shop apologized that some of the parts were delayed by a whole week, and in the end it took 5 weeks to restore the vehicle to its original state. And this sort of thing is precisely why I chose to go with an established automaker with a strong reputation for reliability and serviceability. This has become so much more important to me than almost any other feature in a motor vehicle. reply FireBeyond 17 hours agoparent> And this sort of thing is precisely why I chose to go with an established automaker with a strong reputation for reliability and serviceability. This has become so much more important to me than almost any other feature in a motor vehicle. Jaguars are not known for reliability. Indeed, I got rid of my 2012 XJ L after its coolant system failed catastrophically one too many times (which was \"two times\" - first one, okay, so be it, happens again, no thank you). However, at some point the dealer bricked the entertainment console (of course tied into everything else), and it was an all in one unit that they had to replace... I am near Seattle. \"Well crap, how long is that going to take?\" (thinking the car is several years old at this point, and not a high volume of sales). \"Oh, we should be able to have a new unit installed by the end of tomorrow, unless we have to get it sent from England, in which case, end of day after tomorrow\". reply steelframe 17 hours agorootparentOne of my other cars is a 2019 Jaguar I-PACE. It's generally been rather reliable over the past 5 years. The main problems have been how it tends to like to brick itself when something electronically doesn't go quite right. For example when I had it turned on and running the A/C on the Edmonds-Kingston ferry it decided that the suspension was borked and refused to release the emergency brake once the ferry arrived at dock. You know what happens when you can't push your bricked car off the ferry? They put cones around your car and load everyone else on the ferry around you while you try to explain to the tow company on the phone how you have a luxury AWD EV bouncing back and forth across the Puget Sound that needs to be winched onto a flatbed. Then when the auxiliary battery died it decided to roll into the middle of a parking lot and gradually glitch out and clamp down on the emergency brakes again as the main 12v battery then died, and you get to have it winched onto another flatbed as it's also impossible to release the emergency brakes once the 12v batteries have given up the ghost. Hence why my most recent vehicle has a clutch pedal, a stick attached to a gearbox, and an emergency brake attached to a lever via a cable. reply FireBeyond 2 hours agorootparentI really liked the Jaguar, when it was working. I'll not own another. reply manquer 17 hours agoparentprevAbsolutely, if you value more how the product and service responds to 1/1000 event , if you value how your daily driving experience is more then Tesla beats the Toyota hands down. I am not trying to diss you, i kind of value the same things. I own a Toyota hydrogen fuel cell car , and I bought an expensive experimental car is because it was Toyota and the support that would come with . However every buyer has different priorities, risk minimization while common , also why most big companies would buy from IBM or SAP instead of a startup or pay license for redhat linux or oracle Java. This also means you objectively have poorer experience and are a cycle or two behind in innovation, most are not okay with that reply wilted-iris 17 hours agorootparentI don't think that's true. A GR Corolla is going to have a nicer interior than a Tesla, it'll handle (much) better, and I bet it's a lot more fun to drive too. reply manquer 16 hours agorootparentalmost always an electric motor will beat any combustion engine , just because of how torque works, plus the lower center of gravity and the rear engines makes the Tesla hands down more fun than a hot hatchback. FSD is not full self driving but is still miles and miles better than anything Toyota has. My Toyota which was their flagship research platform still struggles to do lane assist on sharp curves . You may have other parameters of course , but GR Corolla is poorer than a Tesla in many parameters a lot of customers value (which is why it sells) reply SkyPuncher 14 hours agorootparentHaving tested FSD, I prefer the system in my 2020 Hyundai. It doesn’t cover as much road, but it’s extremely predictable and pretty simple to use. I found that FSD would randomly require intervention at the most inopportune times. I’d simple prefer to be hand driving into those situations than have to jump into reaction mode. reply steelframe 4 hours agorootparentprevI've owned a Tesla and now currently own 3 (non-Tesla) EVs. Let's just say we come from different planets when it comes to automotive aesthetics. reply manquer 1 hour agorootparentthat we probably do. Neither of those planets are outliers though ? which is why they both sell in quantities in excess of million units ? reply dmbche 16 hours agorootparentprevHow's the hydrogen working for ya? reply manquer 15 hours agorootparentQuite well for my needs . I have clocked 25k miles in 2 years and i have had no issues or complaint reply rconti 17 hours agoprevUnfortunately the article is short on details -- I've gotten my Tesla repaired both at independent shops (broken rear triangle glass, which is a tesla part, not a repro), and at Tesla service centers. I've (online) heard horror stories of long parts waits, but it's not clear how or if those waits are different at a Tesla service center vs an indie shop (eg, is Tesla prioritizing their own SCs?). I assume the suit is alleging that independent shops have a harder time sourcing parts than service centers do? If so, it seems like a reasonable case. Of course, they can't really allege a parts monopoly on its own, any more so than against any other manufacturer, since 99% of the time, repair parts for any car are sourced from the manufacturer via a dealer parts counter. Eventually OE and OEM parts are available through parts stores, but that's an opaque process to the end consumer. It would also be interesting if the problem here is the lack of a franchise model- in theory I can get my Toyota repaired at one of several dealerships, so technically there's \"competition\" in the market, even though they all need to hold a franchise to use the name. And, of course, there would presumably be an outcry if Toyota refused to play nice to indie shops as well. reply kgc 17 hours agoparentThere are Tesla authorized repair centers, so I don’t think franchising is the issue either. reply cannaceo 17 hours agoparentprevIndependent shops don't wory on Teslas. Any body damage leads to camera alignment stuff that shops don't want to touch. reply rconti 6 hours agorootparentWell that's obviously not true, I know people who have had their body damage fixed elsewhere. In fact, I doubt Tesla service centers do _any_ of their own bodywork, so it's entirely possible your comment is the exact opposite of true. reply cfeduke 17 hours agorootparentprevTesla uses independent body shops they certify, I know this because I've used one as a result of a deer accident. The service centers do not handle beyond some level of damage. reply bangaladore 17 hours agoprevOne thing to note is that this suit is ~16 months old. From skimming the filing, I'm not sure I completely understand what Tesla is doing wrong when they: Provide over-the-counter part purchasing to anyone via their EPC website. Presumably, some parts cannot be purchased by just anyone, but most can. Provide service manuals free of charge to everyone. You can go onto the website and view them. The service manuals include all the information you would need to completely dissasemble and reassemble the car. What other companies do this? Provide in-car diagnostic via the service menu to allow for most common service activities. Its far more extensive then people believe, likely because its hidden unless you know how to get into it (hold down the trim and type service as the access code). Provides the ability to subscribe to Tesla \"Toolbox\" which is the full paid diagnostic and repair suite. Moreover, there are independent repair shops, and any shop can also be \"Tesla\" approved. Additionally, in my experience, part costs through Tesla are quite low. Almost at-cost low it seems. reply FireBeyond 17 hours agoparentYou realize that all of these things weren't always the case, and (as usual) Tesla had to be dragged to the table. Massachussetts lead that charge. Until their AG got involved, the EPC website existed, but NOTHING was purchasable from the site. Literally every item said \"Unavailable\" or \"Call Tesla\", even things as simple as a 1/4 inch bolt. Service manuals free of charge? No, Tesla had one location in MA where you could go to look at the service manual. And you had to pay a fee ($100, IIRC). And you had to make an appointment. And you got an hour. And you could take no electronic devices, just a pencil and legal pad. reply FireBeyond 15 hours agorootparentRuffled feathers, apparently. > Question 1, as the measure was called on the ballot, will force car manufacturers to ensure independent repair stores have access to the same diagnostic tools as manufacturer run repair stores. That will change the automobile repair landscape in Massachusetts, especially for Tesla, a notoriously secretive company that has made it hard and expensive to service its electric vehicles. > Tesla has pushed back against these kinds of laws before. A similar law passed in 2012, one that said dealerships had to make service manuals and diagnostic information available to repair stores. Notably, Tesla also wasn't part of a memorandum of understanding signed by car manufacturers nationwide that effectively made the Massachusetts legislation a national law. > But Tesla did make some information available. “Out of the kindness of their heart, they made a portal so that you could log in and access their diagnostics,” Benoit said. “It’s available, but it’s only available to Tesla certified shops.” Service manuals free of charge. I was wrong, though, 24 hours, not one: > The Tesla Service and Repair Information Portal allows body shops to sign up and get certified for “complimentary account access.” Anyone else looking to get repair and diagnostic information out of Tesla has to pay for a subscription. $100 buys you 24 hours of access. An annual subscription costs $3,000. > Tesla has also repeatedly, for example, used software to lock consumers out of certain aspects of their cars. Tesla has also fought against independent repair companies, and has at times refused to certify them. Source: https://www.vice.com/en/article/93wy8v/newly-passed-right-to... So while things may be better now (great!), it's revisionist to think they've always been that way. > Tesla was outspoken in opposing Massachusetts’ auto right to repair law last November. The story this week of Tesla’s $16,000 quote for a repair that an independent garage did for $700 suggests why. > The company sent a letter to its Massachusetts customers urging them to vote against Question 1, arguing - without evidence - that the measure would open vehicles to cyber attacks. reply EgregiousCube 18 hours agoprevhttps://www.classaction.org/media/lambrix-v-tesla-inc.pdf The suit in question. reply kevin_b_er 17 hours agoprevThe case is already dead. If you bought a Tesla, you have no right to a court of law involving Tesla. It will go to arbitration, where the case will be secret, but Tesla will know the arguments and results of of all attempts against it, but not you. It will go to arbitration, where the arbiter may be blacklisted from future cases if it rules against Tesla. It will go to arbitration, where the arbiter's ruling is final and the law does not apply. Capricious? Does not matter. The Magnuson-Moss Warranty Act is superseded and voided by the American Arbitration Act. You, as a citizen owner of a Tesla, are barred from using that law against Tesla. The Sherman Act is superseded and voided by the American Arbitration Act. You, as a citizen owner of a Tesla, are barred from using that law against Tesla. No court of law will ever be permitted to apply those laws against Telsa by a Tesla owner. This case was already dismissed once in Sept of last year when the motion to compel arbitration was granted. They're trying again with some sort of attempted resurrection, but its just a waste of money, because Tesla owners are barred from courts of law. They will go to a privatized pseudo-court where they will lose and where they can't do class action unless Telsa wants it. reply Terr_ 17 hours agoparent> The Magnuson-Moss Warranty Act is superseded and voided by the American Arbitration Act. Poking around, I found this [0] from 2015 that suggests the opposite, unless a certain malign President caused the FTC to switch policies. > the FTC has taken the position since 1975 that binding arbitration provisions are prohibited in warranties. [...] The FTC specifically rejected the argument that an arbitration proceeding is not an [informal dispute settlement mechanism] because it is binding and therefore outside the [Magnuson-Moss Warranty Act], which permits warrantors to establish [informal dispute settlement mechanism]s for breach of warranty claims and require consumers to use such [informal dispute settlement mechanism]s before bringing a civil suit. [0] https://casetext.com/analysis/ftc-continues-to-ban-mandatory... reply manquer 17 hours agoparentprevCourts have ruled in the past that arbitration clauses were illegal or unenforceable for various reasons so it not that cut and dry . Contract law is not simple, just because I signed the contract doesn’t mean it is enforcement is guaranteed. While I agree with the estimation that this is loosing and courts would either rule that is not a monopoly or enforce the arbitration clause and kick the case out , it is not a given . Every case is unique, a lot depends on the judge or jury and background of the specific case. Also filing suit doesn’t mean that trial is always the goal. It could be just a strategic move in the process between plantiffs and defense . Sometimes companies would settle not because it will loose, but because it is cheaper to settle. Sometimes they don’t want discovery process opening up internal documents that could be damaging. Fox forked over 780M because of this (and also plaintiffs had a slam dunk case ). reply HillRat 17 hours agoparentprevThis is not necessarily true in matters involving injunctive relief, depending on the wording of the contract, whether arbitration itself may be arbitrated, and whether the judge considers arbitration appropriate for the relief prayed for. reply fallingknife 17 hours agoparentprevDo you have a source for this? Because it doesn't sound credible. reply kevin_b_er 17 hours agorootparentYou can read their Motor Vehicle Order Agreement. Mandatory Binding Arbitration. https://www.epi.org/publication/the-arbitration-epidemic/ https://centerjd.org/system/files/ArbitrationWhitePaper.pdf You can even read Hall Street Associates, L.L.C. v. Mattel, Inc, where even when the agreement said the courts could review. Where, no, courts of law may not review an arbiter's decision if you both agree it would be able to after arbitration. You can read Mitsubishi Motors Corp. v. Soler Chrysler-Plymouth, Inc., where SCOTUS explicitly declared the American Arbitration Act overrides the Sherman Anti-Trust Act. You can read Express Co. v. Italian Colors Restaurant, where you find out you have no substantive rights where the AAA is involved. There's still weak attempts to save it, but the The National Labor Relations Act is also likely superseded by the American Arbitration Act, basically voiding the NLRA from being ever applicable. Courts of law are barred where arbitration is inserted, anywhere, whether you know it or not. You lose basic rights where arbitration is concerned. reply cbeach 18 hours agoprevI had one experience with Tesla parts and it was positive. I had an issue with corroded bolts on my Model S liftgate. Following advice from the Tesla Owners UK Facebook group, I emailed OrderPartsUK@tesla.com and surprisingly got a call back within minutes. They identified the parts and explained how to fit them. They charged my card over the phone: £2.30, with no charge made for delivery. I received the bolts the following morning and fitted them myself. You can browse Tesla's parts catalogue at https://epc.tesla.com/ - which contains diagrams showing every single component in the cars. I found it fascinating. reply toomuchtodo 18 hours agoparentSame experience whenever I’ve ever needed parts from Tesla. Fasteners for various pieces of trim, suspension parts (model y suspension upgrade), a wiring harness, always a great experience when working with the parts counter at the nearest service center to where I was when needing the parts. The disconnect between Tesla corporate policy and super helpful local employees is frustrating and disappointing. reply gerdesj 17 hours agoparentprevPlease forgive me if I'm way out but I think a liftgate is a plate that acts a bit like a fork lift on the back of a lorry. I usually know it as a tail lift. Having browsed that catalogue thingie, it looks like the boot lid is formally called a liftgate by Tesla, presumably it is hydraulic. I might call that a tailgate instead of boot lid or simply the boot (trunk). I think we'll put that down to the US > and the rest of the English speaking world ... > Corroded bolts that you can get at on a car sounds quite a serious fail. This is a Tesla S and not my first Ford Fiesta Mk 1. I'm old enough to remember when galvanization of car bodies became a thing. Nowadays, you rarely hear of anyone stepping into a car and being able to see the road through their knees. Stainless steel has also been a thing for rather a long time. I'm (not) looking forward to finding out what fails my Chinese \"MG\" effort manages in the next few years. So far mechanically and electrically all good but its Android central console is rather flaky. I am happy that the driver's console (behind the steering wheel) is just a display and not \"smart\" in any way. It is important that bollocks is called out and your anecdote seems to support that. You were able to DIY and clearly Tesla will supply some parts to civilians. However, are you absolutely sure that your warranty is still valid? Have you read it, all of it? Even so, we enjoy quite a few protections as consumers in the UK, as do wider Europeans and even in the US (int al)! Your corroded bolts anecdote had a happy ending (sorry) but it should not have happened in the first place. reply cbeach 5 hours agorootparentYes, I totally agree. Likewise the covers that sit in the middle of the wheels are known for corroding over time and have to be replaced. Luckily the costs and effort are minimal, but it's still annoying, given the problems shouldn't be happening in the first place. That said, Tesla do improve their car parts radically over time. The retracting door handles used to fail after a few years due to microswitch wear, so they redesigned the handles to use Hall Effect sensors instead (solid state) and now they're flawless. The new design can be retrofitted. Some daylight running lights would eventually fail due to overheating. They redesigned with heatsinks and the new design can be retrofitted. It's a costly repair (~£700) but I don't think it's strictly necessary to fix these on my car as it was a 2016 model, and it wasn't a legal requirement to have DRLs at that time. The computer storage eMMC chip would eventually fail due to writing wear. They have replaced this system design with a new chip and will replace the affected part for free, even for cars out of warranty. And the whole computer unit in older models is replacable by Tesla for a couple of thousand pounds. While expensive, I admire Tesla for going to the effort of producing a retrofit upgrade. Other manufacturers would have simply abandoned owners with old infotainment systems. It's not mandatory for manufacturers to produce computer upgrades for cars, but Tesla goes the extra mile in order that customers can enjoy the latest software features if they want them. As regards the warranty - I'm not entirely sure if replacing my bolts would void a warranty, but for me the point was moot as my four year warranty had expired anyway. Annecdotally I think Tesla are quite liberal in this regard. I made a proactive repair to my car by using a third party to de-solder the eMMC chip and replace it with a larger and newer model. When Tesla did the eMMC \"recall\" in the following year, they actually paid the expense (£500) of my chip replacement, even though it was done by a third party and was performed before the \"recall\" was live. They simply put £500 in my bank account. I thought that was unusually impressive, and leads me to believe they're amenable to third party repair where it's viable. reply nashashmi 18 hours agoprevWeren’t all the patents made available for free? Can’t anyone make parts now? reply dhc02 18 hours agoparentParents, full design specs, and repair documentation are three very different things. reply peutetre 18 hours agorootparent> Parents, full design specs, and repair documentation My parents provided the full design specs for me. Repair documentation is available at the hospital. reply pfannkuchen 17 hours agorootparentThe repair documentation is appallingly sparse, unfortunately. reply shiroiushi 17 hours agorootparentAnd the design has a bunch of really egregious flaws that haven't been addressed at all, and a few that are utterly bone-headed. reply dhc02 16 hours agorootparentprevHa. I'm leaving it. reply xwolfi 18 hours agoparentprevGuess reply honeybadger1 16 hours agoprev [–] This will fail and it's a good thing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tesla owners have filed a class-action lawsuit accusing the company of monopolizing the repair and parts market.",
      "The lawsuit claims that Tesla's practices force owners to use Tesla's own services and parts, potentially leading to higher costs and limited options.",
      "This legal action highlights ongoing concerns about the right to repair and the availability of third-party services in the electric vehicle industry."
    ],
    "commentSummary": [
      "Tesla owners have initiated a class-action lawsuit, accusing the company of monopolizing repairs and parts, leading to long wait times for necessary components.",
      "The lawsuit argues that Tesla prioritizes new car production over maintaining an adequate stock of parts for repairs, causing significant delays for some owners.",
      "Arbitration clauses in Tesla's contracts could complicate the lawsuit, potentially hindering its progress through the legal system."
    ],
    "points": 108,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1718928381
  },
  {
    "id": 40749426,
    "title": "From Infocom to 80 Days: An oral history of text games and interactive fiction",
    "originLink": "https://arstechnica.com/gaming/2024/06/from-infocom-to-80-days-an-oral-history-of-text-games-and-interactive-fiction/",
    "originBody": "Go North — From Infocom to 80 Days: An oral history of text games and interactive fiction MUDs, Usenet, and open source all play a part in 50 years of IF history. Anna Washenko - 6/20/2024, 11:00 AM Enlarge / Zork running on an Amiga at the Computerspielemuseum in Berlin, Germany. Marcin Wichary (CC by 2.0 Deed) reader comments 81 You are standing at the end of a road before a small brick building. That simple sentence first appeared on a PDP-10 mainframe in the 1970s, and the words marked the beginning of what we now know as interactive fiction. From the bare-bones text adventures of the 1980s to the heartfelt hypertext works of Twine creators, interactive fiction is an art form that continues to inspire a loyal audience. The community for interactive fiction, or IF, attracts readers and players alongside developers and creators. It champions an open source ethos and a punk-like individuality. Further Reading Looking back at the Infocom era: A review of Get Lamp But whatever its production value or artistic merit, at heart, interactive fiction is simply words on a screen. In this time of AAA video games, prestige television, and contemporary novels and poetry, how does interactive fiction continue to endure? To understand the history of IF, the best place to turn for insight is the authors themselves. Not just the authors of notable text games—although many of the people I interviewed for this article do have that claim to fame—but the authors of the communities and the tools that have kept the torch burning. Here's what they had to say about IF and its legacy. Examine roots: Adventure and Infocom The interactive fiction story began in the 1970s. The first widely played game in the genre was Colossal Cave Adventure, also known simply as Adventure. The text game was made by Will Crowther in 1976, based on his experiences spelunking in Kentucky’s aptly named Mammoth Cave. Descriptions of the different spaces would appear on the terminal, then players would type in two-word commands—a verb followed by a noun—to solve puzzles and navigate the sprawling in-game caverns. Advertisement During the 1970s, getting the chance to interact with a computer was a rare and special thing for most people. \"My father's office had an open house in about 1978,\" IF author and tool creator Andrew Plotkin recalled. \"We all went in and looked at the computers—computers were very exciting in 1978—and he fired up Adventure on one of the terminals. And I, being eight years old, realized this was the best thing in the universe and immediately wanted to do that forever.\" Further Reading You can now download the source code for all Infocom text adventure classics \"It is hard to overstate how potent the effect of this game was,\" said Graham Nelson, creator of the Inform language and author of the landmark IF Curses, of his introduction to the field. \"Partly that was because the behemoth-like machine controlling the story was itself beyond ordinary human experience.\" Perhaps that extraordinary factor is what sparked the curiosity of people like Plotkin and Nelson to play Adventure and the other text games that followed. The roots of interactive fiction are entangled with the roots of the computing industry. \"I think it's always been a focus on the written word as an engine for what we consider a game,\" said software developer and tech entrepreneur Liza Daly. \"Originally, that was born out of necessity of primitive computers of the '70s and '80s, but people discovered that there was a lot to mine there.\" Home computers were just beginning to gain traction as Stanford University student Don Woods released his own version of Adventure in 1977, based on Crowther’s original Fortran work. Without wider access to comparatively pint-sized machines like the Apple 2 and the Vic-20, Scott Adams might not have found an audience for his own text adventure games, released under his company Adventure International, in another homage to Crowther. As computers spread to more people around the world, interactive fiction was able to reach more and more readers. Page: 1 2 3 4 5 6 Next → reader comments 81 Advertisement Channel Ars Technica ← Previous story Next story → Related Stories by Taboola Sponsored Links 2024 Buick's Are Turning Heads SUVsSearch Ads Learn More Neurologists Amazed: Barefoot Shoes are The Best Thing You Can Do in 2024 Barefoot Vitality Don't Pay For New Gutters. Get This 3-In-1 System Instead Leaf Filter USA Learn More MD: Building Muscle After 60 Comes Down To This 1 Thing primenutritionsecrets.com Podiatrist Amazed: This Sock Helps with Sweaty Feet Like No Other. Here's 10 Reasons Why. morefootwear Shop Now Prostate Enlargement? Urologists Begs: Do This Tonight Prostate Report Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=40749426",
    "commentBody": "From Infocom to 80 Days: An oral history of text games and interactive fiction (arstechnica.com)103 points by pseudolus 4 hours agohidepastfavorite34 comments pimlottc 2 hours agoIt’s barely mentioned aside from the title, but I just wanted to say that 80 Days is a really wonderful game that is well worth your time if you’re into text-based games. It’s more of an interactive story than a puzzle game, with some light resource management elements. But the writing is wonderful and there are hundreds of possible paths and storylines to discover. Its replayability is very high, whether you’re trying to find the fastest route, seeking out the most remote locations or unlocking hidden subplots. It really does well to invoke the spirit of adventure in travel, and it was a particular delight during the pandemic days when that wasn’t possible. Plus they’ve open sourced the language and tools used to create the branching narrative! https://www.inklestudios.com/ink/ reply wkat4242 1 hour agoparentThat's not a text adventure though? Or is the interface text? It's hard to tell from the screenshots and I'm on my phone right now :) reply MattGrommes 1 hour agoprevI loved the recent newsletter-turned-book '50 Years of Text Games' by Aaron Reed. It's a bunch of deep dives into a bunch of games from throughout history, most of which I hadn't heard of. https://aareed.itch.io/50-years-of-text-games reply autoexec 7 minutes agoprevSix pages and no mention of Hunt the Wumpus which was thrilling text based spelunkers years before Colossal Cave Adventure reply entropicdrifter 2 hours agoprevHow has nobody in these comments mentioned IFDB yet? https://ifdb.org You can play almost the whole history from your browser if you want. reply AndrewStephens 1 hour agoprevThe ink language mentioned in the article, created by Inkle studios for their games, is a joy to work with. It is designed to be embedded and makes writing branching dialog or complete stories very easy. As well as 80 Days, I really liked Inkle's implementation of the old Steve Jackson Sorcery books (for iOS and other platforms). They really know how to polish their games. Voyage of the Marigold[0] is a project I recently completed written in a mixture of ink and js for a the 2024 Spring Thing[1] Festival of Interactive Fiction. It didn't win a major prizes but I am happy with the way it turned out. [0] https://sheep.horse/voyage_of_the_marigold/ (Your enjoyment will probably be proportional to how much you like Star Trek) [1] https://www.springthing.net/2024/play.html (I recommend Rescue At Quickenheath, another game that didn't win a major prize but was my favorite) reply ghaff 3 hours agoprevI'm very familiar with the Infocom era and am still in touch with some of the folks. I admit I haven't kept up with the latest developments. Probably should take a look. For folks interested in the early history, Jason Scott's Get Lamp documentary is highly recommended. (He also has an Infocom-focused edit.) reply 7thaccount 3 hours agoparentThe latest developments in IF are pretty amazing compared to the Infocom days. The parsers are a lot more advanced and this was all before things like LLMs, which I assume could be used in some way here. reply ghaff 2 hours agorootparentI can imagine. While sophisticated for the time, the Infocom parsers were often sort of an exercise in figuring out the right incantation. (Sort of like Alexa :-/ Low blow I know.) Especially with LLMs and voice recognition, there's a huge amount of potential present and future for a lot more fluid interactions. Not that I expect it to ever be a really mainstream genre. reply mikepurvis 1 hour agorootparentI dunno, I think it could with the right evolution in the interface. Imagine an interactive story app that you listen to on your commute, where voice commands back to it are the only interface (eg so it’s safe to interact with while driving). Maybe that’s just a subset of the more general “AI companion” opportunity, but I expect you could get some really interesting experiences by calibrating the balance between the manually curated/composed parts of it and the parts that get a bit more painted-in by the LLM. Am thinking especially of stories with conflicting timelines, unreliable narrators, etc, where you’d maybe be revisiting the same events from multiple perspectives to piece together what actually happened. reply nathell 2 hours agoprevFor a history of IF between 1972–1999, see the Inform Designer's Manual Edition 4, Chapter 46. https://www.inform-fiction.org/manual/html/s46.html reply scioto 3 hours agoprevI still have the Infocom game Leather Goddesses of Phobos, complete with scratch and sniff card, and the 3-D (blue-red) glasses for the enclosed comic book. If you don't have VR or first-person, it was the next best thing: they told you when to scratch and sniff. reply ralferoo 12 minutes agoparentFavourite part of this game: the untangling cream and the bonus joke about the rabbit. Most hated part of this game: HOP, CLAP, KWEEPA. Also funny how I recall this stuff vividly more than 3 decades later! My first experience of IF was the tape-based Classic Adventure on the Amstrad CPC. My family bought the CPC late 1985, I bought Amstrad Action in December 1985 and saw the advert for it and new I wanted it more than all the other games that were reviewed with their flashy graphics and beeps and what-have-you. reply JoeDaDude 2 hours agoparentprevI still have a couple of Invisiclues hint books. I wonder if the special markers are still available and if they would still work on these old books. reply ghaff 2 hours agorootparentThere were a couple of standard \"invisible inks.\" [0] I assume Invisiclues used one of them--most likely baking soda given they didn't use heat to reveal. No idea how stable either of those were as inks. [0] https://www.chem.ox.ac.uk/files/secretstoppers1pdf reply zzo38computer 1 hour agorootparentprevI think that Invisiclues is good idea, and that UHS format can be used as a computer file with a similar use. There are FOSS implementations of UHS such as OpenUHS and FreeUHS. Maybe someone will be able to rewrite the Invisiclues in UHS format. (I also wrote a UHS writer program in uxn. And, I had written UHS parser in PostScript; if you have printer with invisible ink (or scratch-off layer) then maybe it will be possible to use this PostScript code to make a program that will print out with invisible ink, too.) reply 867-5309 1 hour agoparentprevwith zero prior knowledge other than its title, should one approach the sights and smells cautiously..? reply zabzonk 2 hours agoparentprevi had that back in the 80s - managed to move something like n,s,w and then gave up, good name for a game though, and about typical for me and infocom. reply ghaff 2 hours agorootparentI've never been great at working my way all the way through games. I think I may have completed that one--mostly because I could get hints from the author :-) reply markx2 2 hours agoprevLoved text adventures since my CPC6128 days. They are about the only things I could still use my Psion 5 for. I have Lost Treasures 1 and 2, and the Classics on CD. This is fantastic: http://www.getlamp.com/ You can also hunt down the Infocom Universe Bootleg. It has pretty much all the games, bonus games, invisiclues, IUB database, software tools. IUB.zip is 397.5mb zipped reply zzo38computer 1 hour agoprevI am #20071 on ifMUD. I had also written a document called \"Tricky Document\" which describes several tricks involved with Z-machine programming (many of which Infocom did not use). http://zzo38computer.org/zmachine/doc/tricky.txt (I also wrote implementations of Z-machine in C, PostScript, JavaScript, and Glulx.) Another text adventure system that I know of is \"OASYS\". The VM code was not documented, although it did include source code, and I have figured it out from the source code and written a document. The included OAC compiler was rather limited (no include files, you could not call a function that is defined later in the file, ambiguous syntax, strings duplicated in the output file, no pointer types, no type checking, no macros, no arrays, no bitwise operations, spurious vocabulary entries, and various other limitations), so I had written my own compiler (which still uses the same VM code, but with an entirely different syntax). reply susam 1 hour agoprevColossal Cave Adventure (filename ADVENT) by William Crowther in 1976 on IFDB: https://ifdb.org/viewgame?id=fft6pu91j85y4acv This is the game that started it all! In my personal archives though, I only have a copy of the 1977 update by Dan Woods where the player can score a maximum of 350 points. This, I believe, is the Fortran source code of the 1977 version: http://mirror.ifarchive.org/if-archive/games/source/adv350-p... reply dudinax 1 hour agoprevThis seems like a good spot to plug one of my favorite games Will Not Let Me Go https://ifarchive.org/if-archive/games/competition2017/Will%... A Twine game that simulates dementia. It's a brilliant, well written game that ironically will stick in your memory. reply jandrese 1 hour agoprevThere is a love/hate relationship with most of those old text adventures. They could make an entire world with just a handful of words and fill them with clever puzzles to delight the users. But then the parser would be willfully obtuse and most of the gameplay would be figuring out the exact combination of commands to unlock the next snippit of the story. Sometimes requiring the player to telepathically connect with the developer to figure out precisely what phrasing he intended. You see a special looking rock on the ground. > PICK UP THE ROCK Huh? > PICK UP ROCK Huh? > PICK UP SPECIAL ROCK Huh? > PICK UP THE SPECIAL ROCK You pick up the rock, it feels special in your hands, you are certain it will be important sometime later. > PUT ROCK IN POCKET Huh? > PUT SPECIAL ROCK IN POCKET Huh? > PUT SPECIAL ROCK IN MY POCKET I can't do that. > OPEN POCKET Huh? > OPEN MY POCKET You open your pocket. > PUT SPECIAL ROCK IN MY POCKET You safely store the rock. It is no mystery why graphical adventure games basically wiped out the text adventure games. reply technothrasher 1 hour agoparentOh, you'd love the Apple II game called \"Prisoner 2\". It's entire purpose was to frustrate you at every turn with things you had to telepathically guess what the developer was thinking. The very first puzzle is a maze which is almost impossible to escape from... until you discover you can hit the 'ESC' key. It gets more dastardly from there. reply ChicagoDave 1 hour agoparentprevThe IF community recognized these problems early (late 90's/early 2000's) and mitigated them with helpers and a lot of playtesting. This is not really an issue in any games released in the last 20 years. I think for me the worst \"guess the verb\" blocker was in Enchanter with the mouse hole and how to get the parchment out of it. Who the hell is going to think of \"REACH IN HOLE\"? reply ta_1138 7 minutes agorootparentThis nonsense still happens in the early sierra graphic adventures: Administer sobriety test! reply maxsilver 1 hour agoparentprevThe scale of the IF text-parser problem isn't that bad, and they addressed a lot of the issues decades ago, modern games don't struggle with this nearly as much. It's just that that Interactive Fiction tends to be a niche hobby, so most of the IF written today assumes you are already at least a intermediate in the field -- they often don't throw in a Tutorial, the way every modern triple-A game does. From an 'intro accessibility' standpoint, Modern videogames are often way more willfully-obtuse. We just don't recognize it, because it's assumed that everyone who plays a game already has basic understanding of twin-stick first-person and third-person gamepad controls, we assume it like it's another form of basic literacy. (Who hasn't played a game before, right?) But for folks who don't -- for the (many) folks who have literally never touched a gamepad in their life, sitting them down to modern graphical interactive-fiction controller game (say something like Firewatch, or Gone Home, or Edith Finch, or Life is Strange) is even more challenging for those folks than the traditional IF text parser. I've seen people spend thirty minutes just trying to figure out how to look in a general direction -- it takes truly-new adults quite a while to get used to the feel of twin-thumbsticks for movement+camera-control, it requires a lot of careful fine-motor control on both sticks simultaneously and often has to be felt to be learned well. At least with text-based IF, most people have been exposed to typing at school or at work or at a library or such. The same is not usually true of twin-thumbstick gamepads. reply ZeroGravitas 1 hour agoparentprevI've though this is an opening for llm use. Feed all the possible valid commands to the llm and let it translate from anything close you type in. reply TillE 1 hour agoparentprevGraphical adventure games were largely parser-driven for like a decade. Sierra's AGI parser worked well; my recollection is that it usually looked for a verb and a noun and ignored any extraneous text. reply textfiles 1 hour agoprevAlways appreciate the GET LAMP shoutouts. - Jason Scott reply ChicagoDave 1 hour agoprevAlso, NarraScope is in Albany, NY this weekend, if anyone is nearby and wants to check it out. reply ZeroGravitas 1 hour agoprevMild spoiler warning! I'm playing Hadean Lands at the moment and wasn't expecting to have to scroll past a map of the game. reply ChicagoDave 1 hour agoprev [–] Also, this article misses a small point in time in 2007-2012 where Textfyre was an unsuccessful attempt at commercial IF. One of the published games was written by Jon Ingold (with Ian Finley), called The Shadow in the Cathedral, which is available at https://textfyre.itch.io/. I'm not sure, but I think this is the last parser-IF game Jon had a hand in...and may have been a spark for Inkle Studios. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Go North — From Infocom to 80 Days\" is an oral history exploring 50 years of interactive fiction (IF), from early text adventures to modern works.",
      "The IF community is known for its open-source ethos and individuality, including both players and creators.",
      "The genre began in the 1970s with games like Adventure by Will Crowther and evolved as home computers became more accessible, reaching a wider audience."
    ],
    "commentSummary": [
      "\"From Infocom to 80 Days: An oral history of text games and interactive fiction\" explores the evolution of text-based games, highlighting classics and modern titles like \"80 Days\" for their storytelling and replayability.",
      "The discussion includes resources such as the Interactive Fiction Database (IFDB) and the book \"50 Years of Text Games,\" and notes the ink language by Inkle Studios for its ease in creating branching narratives.",
      "Challenges of early text parsers are mentioned, along with the potential of modern advancements like Large Language Models (LLMs) to enhance interactive fiction."
    ],
    "points": 103,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1718976621
  }
]
