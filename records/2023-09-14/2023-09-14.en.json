[
  {
    "id": 37498979,
    "title": "Bug in macOS 14 Sonoma prevents our app from working",
    "originLink": "https://mullvad.net/en/blog/2023/9/13/bug-in-macos-14-sonoma-prevents-our-app-from-working/",
    "originBody": "Not using Mullvad VPN Stockholm, Sweden Check for leaks About Policies Blog Pricing Servers Downloads Help Account Get started Bug in macOS 14 Sonoma prevents our app from working 13 September 2023 APP The macOS 14 Sonoma betas and release candidate contain a bug that causes the firewall to not filter traffic correctly. As a result, our app does not work. During the macOS 14 Sonoma beta period Apple introduced a bug in the macOS firewall, packet filter (PF). This bug prevents our app from working, and can result in leaks when some settings (e.g. local network sharing) are enabled. We cannot guarantee functionality or security for users on macOS 14, we have investigated this issue after the 6th beta was released and reported the bug to Apple. Unfortunately the bug is still present in later macOS 14 betas and the release candidate. We have evaluated whether we can patch our VPN app in such a way that it works and keeps users secure in macOS 14. But unfortunately there is no good solution, as far as we can tell. We believe the firewall bugs must be fixed by Apple. The bug affects much more than just the Mullvad VPN app. Firewall rules do not get applied properly to network traffic, and traffic that is not supposed to be allowed is allowed. We deem this to be a critical flaw in the firewall, anyone relying on PF filtering, or apps using it in the background on their macOS devices should be cautious about upgrading to macOS 14. Our recommendations MacOS 14 Sonoma is scheduled to be released on the 26th of September, if the bug is still present we recommend our users to remain on macOS 13 Ventura until it is fixed. Technical details The following steps can be taken on macOS 14 to reproduce the issue. Warning: This will clear out any firewall rules you might have loaded in PF. In a terminal, create a virtual logging interface and start watching it for traffic matching the rules you will add later: sudo ifconfig pflog1 create sudo tcpdump -nnn -e -ttt -i pflog1 Write the following firewall rules to a file named pfrules: pass quick log (all, to pflog1) inet from any to 127.0.0.1 block drop quick log (all, to pflog1) In another terminal, enable PF and load the rules: sudo pfctl -e sudo pfctl -f pfrules Ping the mullvad.net webserver: ping 45.83.223.209 Expected results Ping is blocked, since it does not match the only pass rule’s requirements The traffic is logged to pflog1. More specifically we expect it to be logged as matching the block rule Actual results Ping is allowed out on the internet, and the response comes back No traffic is being logged to pflog1 Cleaning up after the experiment Disable the firewall and clear all rules. sudo pfctl -d sudo pfctl -f /etc/pf.conf Follow our blog for future updates to this issue. MULLVAD About Help Servers Pricing Blog What is privacy? Why Mullvad VPN? What is a VPN? Download Press Jobs POLICIES Open source Privacy policy Cookies Terms of service Partnerships and resellers Reviews, ads and affiliates Reporting a bug or vulnerability ADDRESS Mullvad VPN AB Box 53049 400 14 Gothenburg Sweden support@mullvad.net GPG key Onion service FOLLOW US @mullvadnet @mullvadnet MullvadNet Mullvad VPN mullvad LANGUAGE English",
    "commentLink": "https://news.ycombinator.com/item?id=37498979",
    "commentBody": "Bug in macOS 14 Sonoma prevents our app from workingHacker NewspastloginBug in macOS 14 Sonoma prevents our app from working (mullvad.net) 919 points by eptcyka 16 hours ago| hidepastfavorite208 comments kdrag0n 5 hours agoThe same PF bug is breaking one of OrbStack&#x27;s networking features. I found it hard to believe when I narrowed it down to this, but I guess I&#x27;m not alone. Really hoping this is fixed before the stable release.I didn&#x27;t get a chance to report this to Apple until yesterday, but I think it&#x27;s a fairly recent regression, probably from around beta 6.PF is supposed to be a last-match firewall but it&#x27;s almost like macOS is doing first-match now: an earlier \"block\" rule (without \"quick\") is overriding \"pass\" rules in a different anchor, which obviously breaks things. reply bjoli 3 hours agoparent\"really hope\" is too weak. This has to be fixed before release. A stable release with this would be unacceptable and would be reason for me to abandon Mac OS. You don&#x27;t deliver a product with this kind of regression if you want it to be used for any kind of serious business reply Sardtok 3 hours agorootparentHalf the time I hear of Mac OS updates, it&#x27;s people having their Macs bricked because they didn&#x27;t wait for patches to the main release to come out. Every major release seems to have major issues. At least this doesn&#x27;t render your Mac completely useless. reply thr0w__4w4y 1 hour agorootparentYep, been there... made that mistake ONCE. Never again. Never. reply nikanj 3 hours agorootparentprevMaybe you don&#x27;t deliver products like that, but Apple delivers them all the time. Then you have to desperately raise noise on all possible forums, and hope to catch the eye of someone who actually works at a relevant team at Apple, before your customers get too tired of the issues caused by the bug and abandon you.This is not an Apple-specific issue either, Microsoft works just the same. Large OS releases are a massive undertaking, and the release train don&#x27;t stop for a firewall bug - no matter how severe the bug is for the people it does affect. reply szundi 1 hour agorootparentI remember the time around iOS 5-6, they made some delays and probably it fired back on them and since they have delay related PTSD as an organization. I think since then they just have to push whatever it takes. I remember after iOS 7 I almost left the ecosystem it was so buggy and almost unusable. They probably measured it because after maybe iOS 9-10 they started to be very proud of fixing and stabilizing stuff even in product announcement and iOS is very stable now, not recognizing an issue for years maybe. But this had a price, they paid it, now they seem very aggressive on schedules until the next major clusterfucky year. reply buildbot 16 hours agoprevGreat writeup, very succinct and informative, they even have a simple reproduction of the bug.I love Mullvad!Tangentially, MacOS has had a lot of weird firewall bugs in the last few releases in general, I wonder what drive them to rip up and redo (I assume? so much of it recently. reply nhubbard 15 hours agoparentThe rewrite was definitely influenced by the mandatory migration from kernel extensions to userspace System Extensions, specifically NetworkExtension, between Catalina and Big Sur: https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;networkextension reply hulitu 14 hours agorootparentOne would expect they have a test suite. reply lIIllIIllIIllII 8 hours agorootparentFor a firewall I&#x27;d ideally want some degree of formal verification. It&#x27;s not something you fuck around with if you&#x27;re serious about creating a secure product - like an operating system. reply maweki 4 hours agorootparentYou can&#x27;t formally verify that your interface to the operating system works as it is supposed to work.You can only formally verify your own functions against some specification. And where do you get a bug-free specification from? reply rcme 6 hours agorootparentprevBut how do you verify the formal verification? reply szundi 1 hour agorootparentYou know having some bugs in there is not that problematic as even if they stay in the shadows, most product bugs are catched anyway. Also these make false alerts, then instead of fixing the nonexistent bug, they fix the test suite. reply mejutoco 5 hours agorootparentprevProving in Math that the building blocks have certain properties. reply skhr0680 3 hours agorootparentprevfrom my past experience with Mac OS, the users of the .0 version are the test suite reply rjzzleep 1 hour agorootparentI think this pretty accurate. After messing around with the .0 versions of macOS a few times I also learned that the common wisdom is to only install OS updates on .1 versions. reply londons_explore 13 hours agorootparentprevOne would suspect their test suite is lacking... reply MichaelZuo 12 hours agorootparentThere are a lot of errors and faults that show up in Console on a brand new MacBook just sitting on the desktop. And with every version the number seems to increase. So it&#x27;s not even their test suite that&#x27;s the issue. reply dmix 12 hours agorootparentI&#x27;m curious, why do you look at random OS errors in Console to the point you noticed such a thing? reply eptcyka 12 hours agorootparentFor me, it&#x27;s a benchmark of a well made system - the lower bandwidth of log output you get when a user machine is idling, the better. I have seen some Android phones produce megabytes of logs just sitting there - you can test this by running `pv`.It&#x27;s also a good metric to signal an anomaly after a deployment. On my desktop machines, the current culprit for most of the logs is pipewire&#x2F;alsa, generating multiple lines per second. reply tspike 5 hours agorootparentBy this metric, they could vastly improve system quality by reducing log verbosity. reply eptcyka 2 hours agorootparentOf course, if you take the cynical approach to optimising software, you can easily ruin the utility of this metric quickly. reply MichaelZuo 11 hours agorootparentprevThis, plus on MacOS &#x27;faults&#x27; specifically indicate events where the computer could not gracefully recover, so some user noticeable thing happened.e.g. when the WiFi adaptor faults and has to restart. reply astrange 10 hours agorootparentNo, it&#x27;s just the highest severity log and stays archived the longest. (And records a backtrace.)It doesn&#x27;t mean anything besides that though. reply MichaelZuo 9 hours agorootparentSource?Maybe in some scenarios it shows a fault that is absolutely unnoticeable, but every real world case I&#x27;ve seen was at least theoretically noticeable by the user. reply saagarjha 3 hours agorootparentThe literal source code in this case: you can look up yourself how os_log_fault works. reply nemo 8 hours agorootparentprevMy system&#x27;s running fine with no noticeable issues. I opened Console to check for faults and after running with logging on I eventually got a couple, all tied to Safari trying to read a preference from a domain it didn&#x27;t have permissions to. No performance issues or other user-detectable impacts. reply MichaelZuo 8 hours agorootparent&#x27;Safari trying to read a preference from a domain it didn&#x27;t have permissions to.&#x27;That doesn&#x27;t sound like normal behaviour, what&#x27;s the domain? replyMichaelZuo 12 hours agorootparentprevWhen my computer can&#x27;t even stay up for a full week without crashing, or suffer an inexplicable lag spike, etc., I become more motivated to closely examine everything to see what&#x27;s causing the crash. reply callalex 11 hours agorootparentAre you plugging&#x2F;unplugging monitors or other hardware? With macs it’s always that for some reason. reply stephen_g 5 hours agorootparentMy M2 pro gets plugged in to the work monitor most days and then unplugged and taken home and plugged in to my monitor at home - 10 or so screen configuration changes a week... I&#x27;ve never found I&#x27;ve need to restart due to any issues with any regularity with this one or my old 2015 MBP that this replaced.I installed the most recent update five days ago, would have been weeks since the last reboot. From my experience, I&#x27;d probably put it down to some software they&#x27;ve installed, or yes, possibly an actual hardware fault with that particular unit. reply MichaelZuo 11 hours agorootparentprevNo, literally just sitting there with no peripherals attached, doing nothing more complex then playing a 4k video and web browsing. reply tambourine_man 6 hours agorootparentIt’s probably a hardware issue.My Macs stay up for months and have since Jaguar days. I use them for around 7-8 years, after that hardware starts to glitch or get obsolete. I can count on one hand the number of kernel panics I’ve had on each during this time. reply nicolas_t 4 hours agorootparentMy previous mac would crash every few days inexplicably. I thought it was mac os x becoming less stable. Then I switched to a new mac (the m1) and stability improved significantly so yes that does make sense. reply astrange 10 hours agorootparentprevFile a bug in Feedback Assistant, if you want. reply MichaelZuo 8 hours agorootparentOkay, so? reply Moomoomoo309 5 hours agorootparentIf there&#x27;s an issue, how are they supposed to know about it? Via osmosis? Even if they should have found it earlier, they didn&#x27;t, so now what? You should tell them, so they can fix it. reply_jal 11 hours agorootparentprevSome of us who came up on the ops side routinely check logs. It is both how you spot problems other monitors might miss and partly how you learn how your system works (or doesn&#x27;t). Especially with MacOS, where the documentation quality ranges from shit to nonexistent and the source is unavailable. reply ls612 9 hours agorootparentIs the log clarity on macOS better than the mess that is the Event Log in windows? reply newaccount74 1 hour agorootparent\"Clarity\" is not a word I would use to describe the macOS logs.The number of log entries shown in the console is overwhelming. All the background services constantly log ~100 messages per second, so the log entries you are looking for disappear immediately.You can use the search filters to narrow down messages if you know what you are looking for, but if you don&#x27;t already have a suspicion where the problem is coming from, then the macOS logs are close to useless. reply tambourine_man 6 hours agorootparentprevApple made logging “cheap” in Mavericks, IIRC. They made a big deal that by coalescing and compressing writes, using a database instead of flat text files, it would use little memory and battery, or something like that.Anyway, Console.app got way more verbose after that. I don’t like it, it’s too noise for me, but it doesn’t necessarily mean that lots of things are breaking under the hood. reply newaccount74 1 hour agorootparentThey added a way to efficiently generate hundreds of log messages per second, but never bothered to think about what to do with all these messages. reply nvy 8 hours agorootparentprevYou mean from dmesg or something? Not once have I opened console.app and seen random OS errors just sitting at the shell prompt.edit: derp I confused console&#x2F;terminal please disregard reply dbmnt 7 hours agorootparentNo he means from the Console app on macOS. It&#x27;s not the same as the Terminal app, which it sounds like you&#x27;re referencing. It&#x27;s a log viewer that&#x27;s installed by default but tucked away in Applications&#x2F;Utilities. It&#x27;s similar to dmesg I guess. reply nvy 5 hours agorootparentYes, major derp moment there. replydclowd9901 4 hours agorootparentprevOh you poor sweet summer child reply keepamovin 6 hours agoparentprevIf you&#x27;re concerned about these kinds of bugs on your local OS platform you may consider \"abstracting away\" your local connection point via a remote browser. This way, whatever your local machine and OS, you can have a dedicated server that you run your browsing through. Granted it doesn&#x27;t enclose your entire network connection: only your browsing, but what it does there is change your IP address, mask your location, and add protection from browser 0 days.We&#x27;re constantly adding new features add BrowserBox to respect and protect privacy and improve the overall experience. It&#x27;s open source so you can change it how you want too. If you don&#x27;t like AGPL-3.0 you can get a commercial license. Come take us for a spin: https:&#x2F;&#x2F;github.com&#x2F;dosyago&#x2F;BrowserBoxProIf you don&#x27;t want something open source, but prefer the joy of a large company I think Mullvad also has their Mullvad Browser which does something similar! reply scarmig 6 hours agorootparentI&#x27;ve always imagined RBI as something primarily targeting enterprise; thanks for making it clearer the advantages for a personal end user.Do many websites end up blocking traffic if it&#x27;s originating from e.g. an EC2 instance? reply keepamovin 5 hours agorootparentYeah, thank you. It does actually have use cases for regular folk!About the IP: No actually. I&#x27;m not sure why that is, but I assume because a lot of premium VPN services and proxies need to end up going through IP blocks used by cloud providers as well these days.Most of the blocking I&#x27;ve seen comes down to how well the browser presents as a regular browser. If it looks like an automated bot browser (just raw headless, or whatever) it&#x27;s more likely to get blocked in my experience than from the IP address.Although you certainly get some IP blocks or individual IPs that are more likely to get blocked. No offence to DO, but I&#x27;ve noticed their IPs are less reliable. I&#x27;ve never really had an issue with AWS, GCP, Vultr, Linode or Hetzner, except for the odd machine that somehow has an IP address that must be on a ban list, I&#x27;d estimate it around 5% probability of getting such an IP from the regular providers tho. reply isodev 13 hours agoparentprevIt would have been even better if they had included the rdar&#x2F;Feedback number. reply tiffanyh 12 hours agoparentprevmacOS has attempted to progress its networking stack for years but would run into regressions and then revert back.Old article on the topic.https:&#x2F;&#x2F;9to5mac.com&#x2F;2015&#x2F;05&#x2F;26&#x2F;apple-drops-discoveryd-in-lat... reply cptcobalt 12 hours agorootparentDrudging up 8 year old architectural decisions that Apple rightfully reverted is hardly a charitable comment. A bug can just be a bug. reply voytec 13 hours agoparentprev> Great writeupNo. It&#x27;s a rushed and emotional response bashing OpenBSD&#x27;s pf and not Apple&#x27;s implementation. reply jxf 13 hours agorootparentIf Apple is shipping that implementation as part of their OS, doesn&#x27;t it make sense to let Apple know they should pick a different upstream target? reply jojobas 8 hours agorootparentNo, you see grasshopper, Apple is infallible. If there&#x27;s a problem with any of their devices or software, it&#x27;s always someone else&#x27;s fault. &#x2F;s reply eptcyka 13 hours agorootparentprevTo the best of my knowledge, OpenBSD&#x27;s pf wouldn&#x27;t exhibit such pathological behavior. reply voytec 13 hours agorootparentExactly - it&#x27;s a screwup on Apple&#x27;s part, not OpenBSD&#x27;s. reply alpaca128 12 hours agorootparentAnd Mullvad very clearly indicated it&#x27;s connected to Apple&#x2F;macOS:> bug in the macOS firewall, packet filter (PF) reply voytec 12 hours agorootparentIt&#x27;s not macOS firewall, but Apple&#x27;s implementation of OpenBSD&#x27;s pf used in Apple&#x27;s macOS. Mullvad is clearly pointing at a bug in OpenBSD&#x27;s \"packet filter\", mentioning that it&#x27;s used in macOS.Mullvad&#x27;s article lacks proper wording and shits on the wrong target. reply Liquid_Fire 12 hours agorootparentThere isn&#x27;t any mention of OpenBSD in the article. It says:> a bug in the macOS firewall, packet filter (PF)> We believe the firewall bugs must be fixed by Apple.I don&#x27;t see how you can interpret that as shitting on OpenBSD. reply voytec 12 hours agorootparentThere&#x27;s a mention of \"packet filter (PF)\" which is OpenBSD&#x27;s firewall with a good reputation. It&#x27;s (mis)used by Apple but Mullvad has clearly rushed the article and it points at a bug in the firewall itself. reply kungfufrog 11 hours agorootparentYou&#x27;re way off base and I can see you feel quite frustrated by what you perceive as a slight against OpenBSD. I know and have used \"pf\" in OpenBSD. Not once while reading the article did I think Mullvad were referring to pf as a technology as opposed to the macOS implementation of pf where the bug resides. reply jonhohle 11 hours agorootparentprevUnless there is an equivalent OpenBSD bug, why would it be their issue? Low level components often are patched by Apple to work with Xnu. If the same bug isn’t showing up in OpenBSD, it’s more likely Apple’s integration or a “feature” added by Apple. reply h0l0cube 10 hours agorootparent> Unless there is an equivalent OpenBSD bugMaybe there isn&#x27;t such a huge intersection between BSD and Mullvad users? It would seem though that Mullvad might try to see if the problem narrows down to the BSD implementation, but that&#x27;s hard to know if they haven&#x27;t stated that explicitly. reply SAI_Peregrinus 11 hours agorootparentprevApple forked PF, but didn&#x27;t change the name. Apple&#x27;s fork of PF has a bug. The article only mentions Apple&#x27;s fork. reply bastardoperator 12 hours agorootparentprevOr maybe they&#x27;re not shitting on anyone in particular and just trying to warn their MacOS users about a security issue? reply infofarmer 6 hours agorootparentprevWithout looking, chances are Apple is using the FreeBSD fork of pf, that diverged long ago (mostly to implement SMP to handle much more traffic). reply gonzo 5 hours agorootparentLooking reveals that Apple started with the pf from openbsdhttps:&#x2F;&#x2F;opensource.apple.com&#x2F;source&#x2F;xnu&#x2F;xnu-1699.22.73&#x2F;bsd&#x2F;n...Note: no FreeBSD copyright. reply tick_tock_tick 11 hours agorootparentprevJust because OpenBSD isn&#x27;t relevant anymore doesn&#x27;t mean anything vaguely related to it as an attack. reply infofarmer 6 hours agorootparentThe OpenBSD OS is roughly as relevant today as it ever were. The OpenBSD project has significantly grown in relevance since inception through OpenSSH, LibreSSL and dozens of other high-quality implementations. reply gonzo 5 hours agorootparentApple ships libressl reply monooso 12 hours agorootparentprev> Apple introduced a bugIt seems pretty clear that the article isn&#x27;t bashing OpenBSD in any way whatsoever. reply stouset 9 hours agorootparentOr even mentioning OpenBSD. reply Exuma 13 hours agorootparentprevWhat part is emotional reply sam_goody 11 hours agorootparentvoytec&#x27;s comment ;) reply keehun 14 hours agoprevI&#x27;m glad Mullvad is raising the public temperature on this! This one has definitely been noticed and been very concerning. reply scosman 14 hours agoparentHas this been noted elsewhere? Sounds like Mulvad reported after the 6th which is pretty close to the RC.From source: \"we have investigated this issue after the 6th beta was released and reported the bug to Apple\" reply gorkish 11 hours agorootparentMacOS has had a host of these types of issues with their network stack over the last few years. They are almost always related to some \"Magic\" technology Apple is introducing such as AirDrop (raw wifi frames), Siri (multipath tcp) et. al. Essentially Apple have been introducing these new components with special elevated privileges which allow them to bypass or have priority access to the network stack in order to implement whatever brand of cross-protocol hoodoo they may require to function. At best, it&#x27;s maddening, but at worst its a huge red flag that Apple seems ready and willing to accept these compromises into the functionality of their system. It is impossible to achieve total software control over the network stack in MacOS today. reply keehun 13 hours agorootparentprevNot publicly that I have seen, but I can assure you networking and cybersecurity companies (and others) saw this pretty quickly when the bug was first released. I was just glad to see a relatively big company calling out this rather egregious issue. reply LeoNatan25 13 hours agorootparentSecurity companies should be much more open about these issues, rather than quake the notion that if they go public, they’d lose their hush hush secret contacts at Apple that give them private entitlements for private functionality. (Source: first hand experience) reply pdimitar 8 hours agorootparentYour comment is pretty vague but intriguing. Are you allowed to share an example? reply mrcode007 7 hours agorootparentthey&#x27;re called managed capabilities and require apple&#x27;s approval for unlocking access. CarPlay is an example:https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;carplay&#x2F;requesting...edit: tap to pay is another: https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;proximityreader&#x2F;se... reply LeoNatan25 5 hours agorootparentThose are public capabilities that require explicit approval from Apple in the form of an entitlement. That’s not what I am saying.I’m talking about capabilities Apple officially denies having, or only gates to “partners”, and vends them using private header files and entitlements. One example is VPN service, which, before the NetworkExtension, were limited to the “Cisco”-branded user UI in Settings and MDM configuration files. Unless you had the (legacy) network manager private header files and a super private entitlement in you provisioning profile, allowing you to create VPN on-device without any MDM or configuration profile (or user consent), there was no way for an App Store app to create a VPN tunnel. We used to get these by mailing a contact inside Apple, asking for the latest headers before each major and minor iOS release. Before NetworkExtension, any public inquiry about creating VPN tunnels was denied by Apple and only officially supported by the Cisco app at the time.Over the years, I’ve heard of many other such “features” only available to big “partners”. reply mrcode007 4 hours agorootparentThese are broken out as standard entitlements in Xcode now and require standard approval process.Private features are a standard practice pretty much everywhere you look around. Don’t believe me? Ask big google or facebook advertisers reply dcow 3 hours agorootparentBlizzard had a hardcoded exception in OSX (pre macOS) for the longest time. replyTerretta 16 hours agoprevThis repro is a thing of beauty. reply jjcm 14 hours agoparentI love how simple it is, but also that it has a cleanup step as well! Such a missed element in many of these. reply coldtea 15 hours agoparentprevIsn&#x27;t it merely setting a simple firewall rule and trying a query that violates it?Which is the scope of the bug sure. But doesn&#x27;t make the check particularly elaborate or beautiful! reply jihadjihad 14 hours agorootparent> But doesn&#x27;t make the check particularly elaborate or beautiful!I think GP is saying it&#x27;s beautiful precisely because it needs such a simple and not elaborate test reply thehours 12 hours agoprevI don’t know if related, but immediately the last two MacOS upgrades I was unable to get my networking to work. I could connect to Wi-Fi &#x2F; Ethernet &#x2F; Hotspot. But nothing would actually connect (e.g. browser, pings, etc) , not even to my router.The fix both times was to open the Mullvad VPN app just once and everything worked again. No idea why just opening the app would fix the issue. reply oefrha 2 hours agoparentVPN apps make changes to the routing table, I suppose your traffic was routed to a nonexistent interface until the VPN app was launched. reply TheBinaryGuy 1 hour agoprevFor me the solution was to download Wireguard configs and use the Wireguard app. reply CharlesW 13 hours agoprevMullvad has been working okay for me in Sonoma betas (I had to click \"Connect\" twice), and appears to work perfectly in the final 14.0 release (23A339). A test with ipleak.net looks normal. What am I doing right&#x2F;wrong? reply contact9879 12 hours agoparentit works but requires local network connections to be enabled which I assume is the \"leak\" they note reply zshrc 13 hours agoprevJust a note, while I experienced issues connecting with the Mullvad.app, running a Mullvad Wireguard config in Wireguard.app worked fine. reply panosv 13 hours agoprevNot completely relevant, but another long standing bug: An 11 Year Old Bug in the macOS Popen(): https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37238433 reply saagarjha 12 hours agoparentIf this is your bug, consider also sending in a feedback with your patch. The open source projects don’t usually take PRs. reply paws 15 hours agoprevThe more macOS seems to break user control of networking, the more I wonder what kind of \"separate box\" solutions are out there that can intermediate _outgoing_ traffic. e.g. Something like LittleSnitch on a router, where it notifies the Mac when it detects a new outgoing connection.Do things e.g. pfSense support that already? \"Hold\" an outgoing connection from the moment the SYN is observed, notify whatever client, and only allow if the user clicks? reply smashed 14 hours agoparent> Do things e.g. pfSense support that already? \"Hold\" an outgoing connection from the moment the SYN is observed, notify whatever client, and only allow if the user clicks?Not that I am aware of.This is a desktop centric workflow where the user can react live to an application that is sending traffic.Your typical network firewall will apply a set of static rules and the decision to log&#x2F;reject&#x2F;drop is done ASAP. Waiting for user input is impossible.Some systems can show logs of recent blocked traffic, and allow an admin to quickly generate an exception&#x2F;allow rule for blocked traffic but that&#x27;s pretty much it. reply jerf 9 hours agorootparentWith some work you could integrate the two; create an external box paired with an app on your source machine. When the source machine can see more info about the connection it can cooperate to pass it along to the middlebox, and if the middlebox sees something the source system has no idea about, well, that&#x27;s useful info to annotate the network connection with. reply fiddlerwoaroof 14 hours agoparentprevMost of the alternatives that aren’t marketed to the consumer immediately have something. I ran openwrt for years and used its firewall to block a bunch of traffic and now I’ve switched to Ubiquiti because of wifi issues. reply meindnoch 14 hours agoparentprevAnd how would you decide whether an outgoing connection to a random AWS IP is legit or not? You don&#x27;t know which app is the source. reply WirelessGigabit 14 hours agorootparentActually you do. You request a port on which your process will listen to the result of the call. reply azinman2 14 hours agorootparentSo then you need each device to run software to communicate this to your router. This isn’t a purely router based solution. reply intelVISA 13 hours agorootparentIf there&#x27;s a market this could be an interesting weekend project. reply gonzo 10 hours agorootparentif you get something, lmk, and maybe we&#x27;ll put it in pfsense.that said, the description could be covered by something like captive portal. reply paws 9 hours agorootparentNot sure I&#x27;m understanding you correctly - by captive portals do you mean the \"challenge\" that renders in a browser&#x2F;webview context, commonly when joining a new network? I&#x27;m not sure that would be suitable for what I describe above.What makes LittleSnitch&#x2F;Lulu&#x2F;similar nice is they listen for \"all\" outgoing traffic types TCP&#x2F;UDP&#x2F;ICMP&#x2F;etc, and show UI immediately, including for non-browser apps, e.g. games, VoIP, P2P apps, whatever -- it tends to be covered. Unless I&#x27;m mistaken I don&#x27;t believe a captive portal can be triggered when \"just any\" process originates traffic.That&#x27;s the strength of LittleSnitch&#x2F;similar, but the major weakness with host-level filtering they rely on, is you&#x27;re 100% at the mercy of Apple&#x27;s networking stack, and this Sonoma issue isn&#x27;t the first time Apple moved the goalposts. Not too long ago Apple exempted their own services from whatever LittleSnitch hooks [1] and it could of course happen again with any macOS update.In my view this is precisely why a separate box is appealing. Ideally it&#x27;d have as tight of a UI as the incumbent apps, including the same metadata (process name + app icon, protocol, port #, most recent previous attempt, etc).[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24838816 replybonestamp2 13 hours agoparentprevI think the best you can do in pfSense would be to log it and then look at the logs regularly. reply catiopatio 5 hours agoparentprevpf’s `divert-to` can be used to divert packets to a local port; `getsockname()` will return the original destination address, so you can either close the connection, or proxy it in userspace.Alternatively, you could possibly use a divert(4) socket — coupled with a targeted firewall rule — to divert only the initial SYN packet, and if the connection is to be permitted, re-inject it and allow connection to proceed normally.OpenBSD supports using divert(4) sockets with pf; unfortunately, FreeBSD divert(4) sockets only work with the older ipfw firewall. reply _boffin_ 15 hours agoparentprevInteresting reply joomooru 15 hours agoprevNo wonder, I recently installed the latest sonoma beta and couldn&#x27;t for the life of me get Mullvad to work. Glad to hear Mullvad is working on a workaround. I even considered downgrading back to Ventura this morning. I feel validated! reply justusthane 14 hours agoparentIt doesn&#x27;t say they&#x27;re working on a workaround. It says that users shouldn&#x27;t upgrade to Sonoma until Apple fixes the bug. reply sleepybrett 15 hours agoparentprevI assume the tailscale&#x2F;mullvad stuff still works. Might be a nice workaround until apple gets it fixed. reply joomooru 15 hours agorootparentWent to look at tailscale vpn and didn&#x27;t realize it was a service entirely contained within tailscale, so I can&#x27;t use my existing Mullvad account or credit tailscale with my already-purchased Mullvad credits :( reply hnarn 1 hour agorootparentI don&#x27;t think the service is so much \"tailscale for mullvad users\" but the other way around, it&#x27;s intended for existing tailscale users to get an \"exit node\" of sorts in their tailscale network without having to set it up themselves.Tailscale&#x27;s business model has never been anonymity, last I used them they even required either a Google or O365 account to even use the service.You might want to look into Headscale[1] instead, it&#x27;s a server-side implementation of Tailscale that you can self-host. It comes with its own drawbacks but from what I&#x27;ve seen it&#x27;s worth it if you want more control over the network.[1]: https:&#x2F;&#x2F;github.com&#x2F;juanfont&#x2F;headscale reply sleepybrett 15 hours agorootparentprevAw, that sucks. Maybe possible through Mullvad support? reply ezfe 14 hours agorootparentThey&#x27;ve said they don&#x27;t plan on allowing that reply gbraad 4 hours agorootparentI have seen a case where they substituted the credits on Mullvad to the add-on for Tailscale, but this might not be a regular thing.Although, it is also still in Beta and issues have been reported&#x2F;known, so similarly if you look for a stable solution it might be best to hold out for now. reply conradev 14 hours agorootparentprevTailscale works great, yeah. reply mannyv 10 hours agoprevQuestion: if pflog1 isn&#x27;t created, does pf work as expected?Since tcpdump presumably isn&#x27;t logging anything going out of pflog1, it implies that the data isn&#x27;t being sent to pflog1. That implies that it&#x27;s something upstream from there.Was pflog1 plumbed and&#x2F;or pulled up? Back in the day you used to have to plumb and pull up an interface manually. Does MacOS do that for you?Granted, isolating this isn&#x27;t the bug reporter&#x27;s job. But at some point the engineer will be asking these questions, so you might as well answer them. reply MuffinFlavored 9 hours agoparentIs there something you could add to the reproduce case to workaround this, like plumb&#x2F;pull up after pfrules are loaded as you mentioned? reply mannyv 7 hours agorootparentOn Ventura 13.5.2 ifconfig create creates the interface UP:bash-3.2# ifconfig pflog1 createbash-3.2# ifconfig -apflog1: flags=1 mtu 33072But I don&#x27;t think tcpdump will see anything because there&#x27;s no IP associated with that interface. On some devices (Broadcom) the OS will just dump stuff out of an interface, but I&#x27;m not sure what MacOS does in this case.I presume this was tested to work on 13.5.2? reply AdamJacobMuller 6 hours agorootparenttcpdump can see traffic fine without an IP on an interface (how much traffic the interface has traffic without one is another question)pflog interface is not a \"real\" interface, there&#x27;s no point in putting an IP address on it reply mannyv 4 hours agorootparentOh, pflog interfaces are a special device that&#x27;s known by tcpdump. Interesting. reply mannyv 7 hours agoparentprevOh, you should also do a:ifconfig pflog1 destroyafterwards to whack that extra interface. reply throwaway914 10 hours agoprevHow does one do the equivalent in Linux with nftables? It&#x27;s very cool that you can create a logging interface. reply dharmab 8 hours agoparentIn Linux you can use eBPF. See https:&#x2F;&#x2F;github.com&#x2F;iovisor&#x2F;bcc for an easy way to write eBPF, or look for something in the tools&#x2F; dir that does what you want. You distro might have these packaged in bcc-tools or similar. reply OJFord 10 hours agoprevIf you use their wireguard servers, you can use wireguard directly (no Mullvad) app, and this is probably not an issue? I&#x27;m not certain and don&#x27;t have a (modern) Mac, so I can&#x27;t test. Just a reminder that it&#x27;s nothing proprietary and the app is optional I suppose. reply derefr 9 hours agoparentI would presume that the thing Mullvad are (rightfully) doing with firewall rules in their app, is to prevent non-VPN traffic from flowing whenever the VPN is enabled. The whole “leaking your IP through DNS &#x2F; privileged connections to OS update servers &#x2F; etc.” issue. reply OJFord 8 hours agorootparentI&#x27;m not saying there&#x27;s no value in the app, but there must be other ways of achieving that (though, maybe they&#x27;re broken by the same). Do you have `ip netns` on macOS? Fwmark? There&#x27;s the WireGuard app too of course, I wonder if that&#x27;s similarly broken. reply fastball 7 hours agorootparentThe way of achieving that is through the Packet Filter, which is what is broken. It seems unlikely you can block system-level traffic with anything except system-level APIs. reply hnarn 1 hour agorootparentI&#x27;m not in any way excusing the bug, but what about routing? Even if the firewall is broken, if you have a wg0 interface and your only route is through that interface, wouldn&#x27;t that also achieve the same goal? replyunnouinceput 14 hours agoprevWe, the old Windows developers, welcome you, the current Apple developers, to the 90&#x27;s, when Windows was shittier and shittier with each version. Get ready for the next decade when workarounds and basically underground techniques will be your only survivability.As MacOS becomes more popular, it seems it has to go to this shitty phase, as Windows did back in the day. We got rid of this phase with Windows XP release, so around 7 years. For you, who knows, hopefully shorter. reply scarface_74 12 hours agoparentWindows is still shitty. After three years of using an M2 MacBook Pro for work and having my own M2 MacBook Air, using a Microsoft Surface laptop is a death by a thousand cutshttps:&#x2F;&#x2F;www.amd.com&#x2F;en&#x2F;processors&#x2F;ryzen-surface-edition1. The fans are constantly going.2. Everything causes the hourglass cursor to pop up - even just clicking on a button in Outlook3. It takes awhile for the screen to redraw. Way back in the pre - OS X days, I use to be jealous of how fast Windows drawing was in comparison.4. Every time my laptop goes to sleep, I have to unplug and replug my external USB C powered external monitor.5. Did I mention the constant humming of the fans?6. Even how it handles multiple desktops is inferior to Macs7. Hopefully I can run WSL2 on my work computer. I can’t imagine being stuck with PowerShell&#x2F;cmdI don’t even want to think about how bad the battery life is going to be compared to modern ARM based Macs.Yes both my MacBook Air and Windows computer have 16 GB RAM reply jorams 29 minutes agorootparent> Everything causes the hourglass cursor to pop up - even just clicking on a button in OutlookAs someone who had to use a very slow 10-year-old iMac recently, I would prefer if macOS showed more indicators that it was busy. The macOS UI seems to assume things are near-instant that in that environment weren&#x27;t even close.For example it \"reopened\" XCode after a reboot, and after a while it seemed the system was done doing background stuff. I clicked the System Settings and another app because I was going to need them, then tried to interact with XCode. Then the window turned gray and a spinner showed up that took a few minutes to disappear. It wasn&#x27;t done at all. It turns out showing a screenshot to seem more responsive only works if you can very quickly back up the lie when necessary. reply rodnim 23 minutes agorootparentprevThe M2 was released last year. Did you start using it two years ahead of its public release? reply jojobas 7 hours agorootparentprevThis sounds very much like crippling effects of an \"antivirus\", not AMD&#x2F;Windows as such. reply keepamovin 7 hours agorootparentprevI had a surface in 2018 and it was really good. reply whyenot 14 hours agoparentprevIt&#x27;s been 23 years (to the day!) since the release of the OS X public beta, and it&#x27;s a mature product. I&#x27;m not sure it&#x27;s getting \"shittier and shittier,\" I think there are still refinements and improvements, they just aren&#x27;t as big as they used to be. reply deergomoo 12 hours agorootparentApple no longer appears to be able to keep a consistent focus on the Mac. They have some really great fits and spurts in particular areas (e.g. hardware, they’re absolutely nailing it with Apple Silicon at the minute) but it’s far too common for widely-reviled issues to linger unaddressed for literally years.The new System Settings is an obvious one; Sonoma hasn’t really touched that at all despite its glaring issues. But Notification Centre has been borderline useless ever since they redesigned it back in what, Big Sur? I saw a Mastodon post recently [0] that highlighted how bad it is today compared to the old design, yet it’s barely been touched in 3 years.macOS is stable and established and unlike iOS a lot of people rely on it to do actual work, I would rather them not mess with stuff than half-ass it and leave it unfinished.[0] https:&#x2F;&#x2F;mastodon.social&#x2F;@marioguzman&#x2F;110997716755684188 reply Hammershaft 13 hours agorootparentprevFor me, it certainly is getting less stable & more frustrating to use with each update. even elements of the ux, such as the settings app, has degraded over the years reply BizarreByte 12 hours agorootparentThat&#x27;s just modern software in a nutshell, nothing is ever \"good enough\" for designers&#x2F;companies and they must change it no matter what.The settings app for example was perfectly fine, it worked well for what...near 20 years with only slight tweaks. Now I have to use the search bar for settings, because it&#x27;s not obvious at all where to find a lot of them.And yet things that would be useful like a volume mixer are still nowhere to be found. reply Hammershaft 12 hours agorootparentI mean, there were definitely iterative improvements I think could have been made to the settings app, as with nearly all software. Instead, apple threw out the design for a ux that was clearly optimized for palm sized screens that you operate by touch in order to unify the interface between two entirely disparate forms of interaction. reply wasyl 3 hours agorootparentprevFeels like this is true only for Apple Silicon macs. Intels are getting more are more terrible with each upgrade, most noticeable is insane UI latency when doing literally anything — opening an app or a window can take seconds. Changing the tab in Settings app takes a second before the new one is rendered! And that&#x27;s with a fresh system installation and no resource-hungry apps opened. And it&#x27;s definitely not just me — colleagues have the same issues and internet is full of such reports. reply glhaynes 6 hours agorootparentprevEvery time anything breaks in macOS, people come out to claim that the Mac is dying and Snow Leopard [which shipped with a bug that deleted lots of users&#x27; home folders] was the one true version. This wouldn&#x27;t have happened if Steve Jobs were still alive. Netcraft now confirms that the BSD-based macOS is dying. reply sumuyuda 13 hours agorootparentprevThe UI has definitely gotten shittier and shittier. reply whyenot 13 hours agorootparentI don&#x27;t know about that. Aqua with it&#x27;s pin stripes jewel-like buttons and other quirks was significantly worse than what we have today. reply Angostura 11 hours agorootparentStrongly disagree. Apple’s obsession with making things like scroll bars and window chrome harder to see has been a usability nightmare for me over the last few releases.Frequently these days, with lots of overlapping windows I try to click the top of a window only to find out I’ve clicked on part of the window behind.Yes having everything one colour is lovely and ‘clean’ but horrible to use reply andrekandre 9 hours agorootparentalso all the icons now being the same shape (like ios) i find myself scanning over and over missing apps i used to find so easily before... such a usability regression (though it does look pretty) reply BizarreByte 12 hours agorootparentprevI strongly disagree. Tiger was the best Mac OS ever looked in my opinion, but this is of course subjective. reply sbuk 12 hours agorootparentTiger was brushed steel, though it still had the &#x27;lickable&#x27; buttons. reply saltminer 12 hours agorootparentprevI will confess I miss skeuomorphism, but even if Apple never embraced flatness, Ventura&#x27;s System Preferences.app is horrendous. I&#x27;ve become reliant upon the search bar to find most things in there, which I rarely had to do before. reply can16358p 13 hours agorootparentprevYup. I absolutely hated that skeuomorphic blurry 3D-like design.Flat design looks much, much cleaner.Same for iOS. iOS 7 was the first version that I actually liked looking at. reply astrange 10 hours agorootparentprevRemember that displays were worse then. One reason we have flat lower-contrast designs now is that we have much higher contrast LCD&#x2F;OLED panels. reply heyoni 13 hours agorootparentprevAnd system settings panel is so unresponsive! I think it’s written in react or something? reply SoftTalker 7 hours agorootparentprevIf you count NeXTStep, the OS is closer to 35 years old. reply wkat4242 11 hours agoparentprevYeah I moved to FreeBSD myself because macOS pissed me off too much. It&#x27;s becoming too closed, too opinionated, too much like iOS.What I loved about macOS originally was that it was a great Unix style OS but with a consistent UI and major desktop apps.Also most major headline improvements in recent macOS releases rely on iCloud and because I&#x27;ve always been a multi-os person these are not something I can use. Some iCloud stuff works on windows but most doesn&#x27;t. And pretty much none of it works on Linux or BSD. Any service I use must work on all.So after years of getting more and more annoyed with Apple removing powerful options and replacing them with dumb on&#x2F;off sliders I just can&#x27;t deal with it anymore. I still use it for work but that&#x27;s it. At the same time KDE is now mature enough to work great. And it doesn&#x27;t eschew lots of configuration settings. So it&#x27;s become my daily driver instead. reply keepamovin 6 hours agoparentprevIf you&#x27;re concerned about this kind of thing, and OK with the reduction in surface, you may consider just using your browser as the way you are protected. With a normal browser, this doesn&#x27;t exactly work, as obviously it doesn&#x27;t change your IP, still tracks you, and any zero days still expose your machine.But at dosaygo, we&#x27;re working on something called a remote browser, which runs on a remote machine, and therefore you have a different IP address. You can incorporate a proxy service or VPN if you want but it already is 1 layer removed from your local machine, which is great for protecting you from browser zero days too. Check us out, we&#x27;re open source but you can also get licenses for commercial use cases where you don&#x27;t want AGPL-3.0: https:&#x2F;&#x2F;github.com&#x2F;dosyago&#x2F;BrowserBoxPro.git reply Tempest1981 6 hours agorootparent> But at dosaygoTypo? Although on your GitHub page it&#x27;s spelled 2 ways:> For support or to purchase licenses, connect with us at sales@dosyago.com or visit: https:&#x2F;&#x2F;dosaygo.comBut that URL won&#x27;t load. reply keepamovin 6 hours agorootparentThanks! Oh geez, this is embarrassing. When I started the company I meant for it to be called : DO SAY GO...but somehow in the incorporation form I accidentally transposed it. I guess it still happens! :) hahaIt&#x27;s doubly bad because as unique as dosyago is, I think I still prefer the name dosaygo. I don&#x27;t know how to change it tho! :..( reply dcow 3 hours agorootparentLookup \"doing business as\". Also your product&#x27;s name, your domain name. your marketing slogans, taglines, trademarks, etc. do not have to match your incorporated name. Just use `dosaygo` everywhere... not sure what made you think you have to match your incorporated name exactly. reply keepamovin 3 hours agorootparentAh..might be too confusing. And I kind of what to have it correct so someone else can&#x27;t have that.And also just because I really like the original name, it&#x27;s the values (do, say, go the most common verbs in many languages besides to be; and it&#x27;s like I think the company should help people with what they do, where they go and what&#x2F;how they say), but it shouldn&#x27;t tell them who they should be so; for me it&#x27;s really core) kind of important to get right ha haIsn&#x27;t there some way to just change it? I don&#x27;t know...I think DBA would work for first-class products or whatever...but not for the name when it&#x27;s so close.Do you have a company? replysteve1977 13 hours agoparentprev> We got rid of this phase with Windows XP releaseI assume you were talking about consumer versions like Win 95, 98 and Me (the release we don’t talk about)?The NT based ones like NT 4 and Windows 2000 seemed decent when they came out. I guess MS realized that as well and started using NT for the consumer releases as well with XP. reply unnouinceput 12 hours agorootparentThe lack of unification between NT and Win9x before XP was abysmal. Basically you had to have 2 partitions, one for gaming, one for business. NT was unable to have games, Win9x was unable to be useful for business due to sheer blue screens. So yeah, I include NT 3.5, NT 4.0 and W2k in that shitty phase as well. I know it very well because I&#x27;ve lived through it. XP ended that. Hence why, after 20+years, you still have the majority of ATM&#x27;s and plenty of other KIOSKs around the world still running XP. reply steve1977 12 hours agorootparentI only ever used NT based systems, but then I also never used PCs for gaming, so I was probably „privileged“ in some regard.But I certainly agree that XP was a nice release, as was Windows 7 (in my experience). reply mtreis86 9 hours agoparentprevFeels fairly oscillatory for me, 95 98 nt*.0 were mediocre, 2k was great, me was horrible, xp was decent, xp64 was useless, vista was awful, 7 was better, 8 felt spammy, 10 was ok, 11 is bad again. reply hulitu 14 hours agoparentprevWin XP was shittier than 2000. reply xp84 13 hours agorootparentNot really. The biggest unpopular change was the polarizing UI that was simple to toggle off for those who hated it. Besides that, you just got win2k plus much better compatibility with apps written for the 9x series. reply m3kw9 13 hours agoprevNew OSs can sink your company reply jiripospisil 15 hours agoprevSpeaking of macOS&#x27;s firewall being broken, there&#x27;s a bug (or \"feature\"?) in the NetworkExtension framework which causes connections to get initiated (SYN, leaks your IP address) even if there&#x27;s an explicit rule to deny that connection. This affects LittleSnitch, Lulu, and all the other apps building on top of the framework. Bug reports have been filed and as usual ignored by Apple.More:Little Snitch \"denied\" connections leak your IP address - https:&#x2F;&#x2F;lapcatsoftware.com&#x2F;articles&#x2F;2023&#x2F;3&#x2F;4.htmlFollow-up to Little Snitch \"denied\" connections leak your IP address - https:&#x2F;&#x2F;lapcatsoftware.com&#x2F;articles&#x2F;2023&#x2F;3&#x2F;5.htmlLittle Snitch \"denied\" connections leak your IP address: Developer response - https:&#x2F;&#x2F;lapcatsoftware.com&#x2F;articles&#x2F;2023&#x2F;6&#x2F;3.htmlhttps:&#x2F;&#x2F;twitter.com&#x2F;JiriPospisil&#x2F;status&#x2F;1679838397983064064 reply sneak 5 hours agoparentThe situation is even worse on iOS where certain connections that exist in connection tracking early in boot will forever bypass your VPN.Apple doesn&#x27;t seem to care that much about privacy applications of their OSes. reply Pesthuf 15 hours agoprevNormally, when software doesn&#x27;t have big changes between releases, like macOS, there&#x27;s a feature freeze and devs are working on bugfixes.Yet, macOS seems to get buggier and buggier between releases. Something about the way it&#x27;s being developed right now is going very wrong. reply deergomoo 12 hours agoparentI wish they’d just stop with the yearly major releases.Unlike the iPhone, there’s no magic date in September when all the year’s new Macs drop and require support for all the new hardware capabilities. Sure, there are changes to system apps on iOS like Notes that want feature parity on the Mac, but you could do those in a point release. Or even better, decouple those apps from the OS and update them individually via the App Store.They clearly don’t have enough resources allocated to macOS anymore to have a big yearly release without spending the next n months fixings problems. Just release the damn thing when it’s actually ready. They didn’t do yearly releases when the Mac was still their major focus, I don’t see why they need to today. reply sneak 5 hours agorootparentUnfortunately Apple is a services company now and they have to release all platforms with iCloud (ie all platforms) roughly in lockstep as they add new functionality in iOS that then needs to also be supported on desktop.Naturally you need to be able to view your stereoscopic 3d videos on your Macbook, or something. reply rollcat 14 hours agoparentprevWhat&#x27;s interesting is that there was a PF bug in FreeBSD not long ago as well: ; . reply pohl 12 hours agoparentprevI see more and more complaints about bugs, anyway. I use it all day every day without issues, though. Not sure what to make of that. It’s not that I don’t believe those who are affected. I’m just not sure one can conclude that it’s getting more buggy with each release. Maybe the number of unusual use cases increases with popularity? reply deergomoo 12 hours agorootparentFor me personally it’s not bugs as such. Bugs usually do get fixed fairly quickly (though something like this making it into a release candidate is concerning).It’s half-baked features that don’t get revisited for years. We all know the new System Settings sucks; Sonoma hasn’t meaningfully improved it. The Notification Centre redesign introduced 3 (?) years ago is so much worse than the old design, but it hasn’t been touched since. Disk Utility is a shadow of its former self.macOS is an established operating system, I would prefer them leave perfectly good features alone unless they can actually make them better. reply hulitu 13 hours agoparentprev> Yet, macOS seems to get buggier and buggier between releases. Something about the way it&#x27;s being developed right now is going very wrongThis is a common trend in the SW development in the last decade. Aparently bug fixing is hard and expensive, that&#x27;s why they concentrate on new features or, in extreme cases, complete rewrites (GTK). reply steve1977 13 hours agorootparenthttps:&#x2F;&#x2F;www.jwz.org&#x2F;doc&#x2F;cadt.html reply andrekandre 9 hours agoparentprevnext [–]> Yet, macOS seems to get buggier and buggier between releases.i wonder how much of that is a side effect of constant rewrites of already existing features... i&#x27;m reminded of when they tried to replace mdnsresponder amd all the chaos that ensued, or even just swiftui and all its regressions... the dock being rewritten in swift or system preferences etc... reply meej 6 hours agoparentprevThey&#x27;re releasing macOS a whole month earlier than usual, with fewer beta releases than usual. I believe this is the first time they will have released macOS and iOS on the same date. reply eschaton 6 hours agoparentprevWhat gives you the idea that macOS doesn’t have big changes between releases? Every single release has a huge amount of change in it, even if you don’t happen to notice. reply dinkblam 16 hours agoprevnot surprised. we&#x27;ve filed dozens of bug reports as every new macOS release gets worse and worse. i&#x27;ve given up on filing reports now, since they won&#x27;t get fixed or even looked at anyway. i see the problem with triaging when around 4k issues are filed per day, but it&#x27;s not like Apple is hurting for cash. reply superlupo 15 hours agoparentI&#x27;ve basically given up reporting bugs with Apple as they just seem to be ignored and either never fixed, or fixed some years later when the corresponding component is completely rewritten.I basically resent filing bugs with companies that have enough money to do proper testing, I don&#x27;t want to work for them for free, especially if there is no answer, or a 1st-level answer who hasn&#x27;t even tried the filed repro case. However, I am happily reporting bugs with open source projects. reply ezfe 14 hours agorootparentI don&#x27;t put a lot of effort into bug reports, but it&#x27;s not a zero-sum game.If they never fix the bug, they got no value out of your report...If they fix your bug, then now software you use works better... reply planb 16 hours agoparentprevI raised a bug in the image capture framework which prevented scanning from sandboxed apps and it was fixed 3 betas later. But probably because Preview.app was also affected and I asked all users of my software to file a bug for the Preview app. reply sccxy 14 hours agoparentprevSame with iOS bugs.Release new feature with a lot of bugs.For example even if they are fixed (takes about a year) in webkit they are never merged to safari…So even if there is bugfix, it will never make it to live release. reply baz00 16 hours agoparentprevThey actually fixed 3 bugs I raised! reply bpoyner 14 hours agorootparentI reported a bug in the iOS Home app and they fixed it. Seems to be hit or miss on what they&#x27;ll fix. reply riscy 16 hours agoparentprevhow can you know they’re not looked at? is there a read receipt? reply eschaton 5 hours agorootparentThey can’t, they’re just presuming because Apple doesn’t communicate anything unless more information is needed or the bug is addressed in a build the reporter has access to. reply nvahalik 14 hours agoprevAnyone else get an invalid certificate on this site? What&#x27;s the deal?ETA: Hm. OpenDNS (family) blocks them... reply mzs 13 hours agoparenthttps:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230913161315&#x2F;https:&#x2F;&#x2F;mullvad.n... reply ezfe 14 hours agoparentprevmakes sense that blocking software would block VPN software reply callmeal 15 hours agoprevnext [3 more] [flagged] LeoPanthera 15 hours agoparentThis kind of cynicism is tiresome. The test case involves pinging Mullvad, not Apple. If Apple wanted no filtering to be possible, they would simply remove pf entirely. reply detourdog 15 hours agoparentprevor the process they expect is to boot oustisde SIP. reply olliej 16 hours agoprev[edit my scanning the article missed that they answered this :D]The obvious answer question is whether they reported this to apple already and are using this post to draw attention to it, or if they’ve found the bug in the betas (which is why betas exist) but then not reported it directly (defeating the purpose of betas) reply finitestateuni 16 hours agoparentIf you read the article, they mention that they’ve reported the bug in previous versions of the beta and it has still not been fixed in the latest version. They’re cautioning their users against upgrading in two weeks when the release comes out of beta unless there is confirmation that the bug has been fixed. reply rollcat 14 hours agorootparentBeware of point 0 releases.I&#x27;ve had such a bad experience with iOS 13 &#x2F; macOS 10.15 that I&#x27;m reluctant with the point 1&#x27;s as well. reply icedchai 10 hours agorootparentI typically don&#x27;t upgrade until the first minor macOS release. With Ventura, I waited much longer (at least 6 months.) reply Exuma 16 hours agoparentprevThat is not an obvious question if you read the article reply dinkblam 16 hours agoparentprevfrom the article:we have investigated this issue after the 6th beta was released and reported the bug to Apple reply Szpadel 16 hours agoprevI don&#x27;t see any action points in blog post so I&#x27;m not sure if they do it, but some warning in app when specific version of MacOS is detected or even blocking functionality that is known to be leaky would be great for anyone that might track blog posts. plus maybe link to this blog post in case MacOS resolves issue but app will not be updated reply voytec 13 hours agoprev [–] > During the macOS 14 Sonoma beta period Apple introduced a bug in the macOS firewall, packet filter (PF).Ouch, I&#x27;d not go with such statement. Maybe \"packet filter (PF) (mis)configuration\" would be a more reasonable thing to write. This reads like a flaw in OpenBSD&#x27;s pf which is untrue. reply thedanbob 13 hours agoparent [–] But it&#x27;s not a misconfiguration, it&#x27;s a bug as the article explains. And it&#x27;s not the author&#x27;s fault that Apple named their firewall the same as OpenBSD. reply voytec 13 hours agorootparentIt&#x27;s not a case of naming something similarily to other software. OSX used FreeBSD&#x27;s ipfw and around the time they renamed the OS to macOS, they switched to OpenBSD&#x27;s pf.Now they&#x27;ve screwed up either configuration or implementation but to me - it doesn&#x27;t read like a bug in pf. reply fullspectrumdev 11 hours agorootparentIt’s a bug in the program named pf on macOS.It’s not that deep. Nobody is blaming OpenBSD’s pf here. reply Khaine 12 hours agorootparentprev [–] I believe Apple used FreeBSD&#x27;s implementation of pf, as it also has the same syntax. OpenBSD pf has evolved since then and their are minor syntactic differences for some rules between freebsd pf and openbsd pf. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Mullvad VPN application is currently experiencing compatibility issues with macOS 14 Sonoma due to a firewall bug that disrupts proper traffic filtering.",
      "This bug may lead to leaks under specific settings, posing a potential security risk for users.",
      "While the issue has been reported to Apple, no remedy is available yet; therefore, users are recommended to stay on macOS 13 Ventura until a fix is presented."
    ],
    "commentSummary": [
      "Users are encountering problems with the networking functionalities of the macOS 14 Sonoma beta version, possibly because of a bug in the macOS firewall.",
      "There's a discussion around potential remedies and alternative VPN services to rectify the issue, indicating dissatisfaction with macOS's stability and usability, and Apple's closed approach.",
      "Users are questioning the utility of reporting bugs to Apple, indicating issues with the testing suite and the handling of user feedback."
    ],
    "points": 919,
    "commentCount": 208,
    "retryCount": 0,
    "time": 1694624744
  },
  {
    "id": 37496589,
    "title": "Meduza co-founder's phone infected with Pegasus",
    "originLink": "https://meduza.io/en/feature/2023/09/13/the-million-dollar-reporter",
    "originBody": "Skip to main content NEWS FEED RU STAND WITH US STORIES The million-dollar reporter How attackers hijacked the phone of Meduza co-founder Galina Timchenko, making her the first Russian journalist to be infected with Pegasus spyware 1:00 pm, September 13, 2023Source: Meduza The public has known for years that governments around the world use software developed by an Israeli cyber-arms company to spy on journalists, opposition politicians, and activists. Investigative journalists published a series of bombshell reports in July 2021 about the widespread abuse of Pegasus, a powerful tool marketed exclusively to state clients for use against only the grisliest criminals. Earlier this summer, Meduza learned that the iPhone of our co-founder and publisher, Galina Timchenko, was infected with Pegasus mere hours before she joined a private conference in Berlin attended by colleagues in the exiled Russian independent media. This is the first confirmed case of a Pegasus attack against a Russian journalist. With help from experts at Access Now and Citizen Lab, Meduza reports what we know about this notorious spyware, how it’s been used in Europe, and which states might have spent millions of dollars to hijack Ms. Timchenko’s phone. Readers, please be aware of a possible conflict of interest in this report, which focuses on Meduza co-founder and publisher Galina Timchenko. She was not involved in the preparation of this article. Galina Timchenko hurried to Meduza’s Riga newsroom on June 23. She’d just gotten a call from Alexey, the head of Meduza’s technical division, telling her to come in immediately. His voice was unusually stern, and he didn’t explain the urgency. “He simply spoke in such a way that I understood it as an order,” Timchenko later recalled. “It was clear that something had happened.” En route to the office, Timchenko wondered if one of her passwords wasn’t secure or if she’d clicked on any suspicious hyperlinks. “I thought I’d done something wrong,” she says. Alexey was waiting for her at the doorstep. He silently pointed at her bag, which held her phone and computer. “I can’t say anything just yet,” he informed her. “We’re looking into it.” He then took Timchenko’s iPhone and MacBook. A day earlier, Timchenko had received a curious text message from Apple and forwarded it to Meduza’s tech division. The message was one of Apple’s “threat notifications” about “state-sponsored attackers” — something the company sends to users who are “individually targeted because of who they are or what they do.” “State-sponsored attacks are highly complex, cost millions of dollars to develop, and often have a short shelf life,” Apple explains on its website. The notification sent to Timchenko did not identify the state in question. She says she put the message out of her mind after sharing it with Meduza’s technical team. Galina Timchenko has grown accustomed to such warnings. The Russian authorities have tried to hack or destroy her newsroom’s infrastructure for years. Meduza has weathered denial-of-service attacks and countless phishing attempts. Russia’s federal censor now even blocks the website outright. To understand what Apple’s message didn’t explain, Meduza’s technical director turned to outside help to find out who these hackers were. First, he contacted human rights activists at Access Now, a nonprofit organization committed to “defending and extending” the digital civil rights of people worldwide and helping improve digital security practices. Access Now has also alerted the public to the collateral damage of tech sanctions on civil rights activists, journalists, and dissidents from authoritarian countries, highlighting how targeted sanctions, mass corporate pullouts, and over-compliance in Russia have helped the Kremlin to silence its critics. Alexey also reached out to researchers at Citizen Lab, an interdisciplinary laboratory at the University of Toronto that investigates digital espionage against civil society, among many other things. Experts at Access Now and Citizen Lab collected the data from Timchenko’s devices and performed what they call “a rapid COVID test.” The results were quick indeed, revealing that her smartphone was infected with the spyware Pegasus on February 10, 2023. This gave the hackers total access to Timchenko’s iPhone: its microphone, cameras, and memory. The attackers could see the device’s entire contents, including Timchenko’s home address, her scheduled meetings, her photographs, and even her correspondence in encrypted instant messengers. Pegasus lets you see a device’s screen directly, reading messages as they are written. It lets you download every email, text, image, and file. Pegasus and NSO Group NSO Group, the Israeli entity responsible for Pegasus, insists that it designed the product exclusively for the surveillance of “terrorists, criminals, and pedophiles.” The firm’s co-founders include veterans of Israel’s military intelligence and the Mossad, and the company sells Pegasus only to state clients. Despite its claims about “rigorous” human rights policies, NSO Group does big business with governments around the world that have regularly used Pegasus to target critics and political adversaries, from reform-minded bishops and priests in Togo and women’s rights activists in Saudi Arabia to journalists in India and human rights defenders in Palestine. Reporters and activists tracked by NSO Group’s spyware are often arrested and sometimes even killed. For example, Saudi operatives in Istanbul murdered and dismembered Washington Post columnist Jamal Khashoggi in 2018 after numerous members of his close entourage were selected for surveillance by NSO Group customers, while the Israeli firm denies that its technology was used “to listen, monitor, track, or collect information regarding [Khashoggi] or his family members.” States pay “tens of millions of dollars, if not more,” for access to Pegasus, Citizen Lab senior researcher John Scott-Railton told Meduza. The Mexican government alone has spent at least $61 million on the technology, which it has used to spy on dangerous criminals and civil society members alike, including journalist Cecilio Pineda, who was assassinated in 2017, just a few weeks after his phone was infected with Pegasus. Even researchers in this field aren’t sure what it costs to hack a single device using Pegasus. The spyware is more a service than anything; each NSO Group contract permits so many “simultaneous infections,” says Natalia Krapiva, Access Now’s tech-legal counsel. “For example, a client state can buy a package with 20 infections, which means it can have 20 people under surveillance at one time.” Speaking to The Washington Post in July 2021, NSO Group co-founder Omri Lavie said attacks on journalists by his clients are “horrible,” but he argued that the main problem is a lack of regulation. “This is the price of doing business,” he explained. “Somebody has to do the dirty work.” A kind of regulation arrived in November 2021, albeit not what Omri Lavie and his colleagues wanted. Months after an investigation by the Pegasus Project consortium exposed the spyware’s rampant, global abuse, the Biden administration added NSO Group to a federal blacklist that bans the company from receiving American technologies. NSO spokespeople expressed “dismay” and said the firm would lobby to reverse the White House’s decision. ‘I felt dirty’ As soon as the Pegasus infection was confirmed, Meduza’s management locked itself in Timchenko’s office for an emergency meeting. “We were all terrified,” Alexey recalls, “but we pretended we weren’t.” Meduza editor-in-chief Ivan Kolpakov, who was traveling then, joined the meeting by teleconference. He was visibly at a loss and kept listing aloud what could have leaked: corporate passwords and correspondence, bank account balances, the names of Meduza staff, and — most dangerously — the identities of Meduza’s collaborators inside Russia. It was soon clear, however, that it was impossible to assess what had been compromised. “They got everything,” Kolpakov recalls. “Everything they wanted.” Those at the meeting say Meduza’s technical director was the only one who remained calm, but he remembers it differently: “I sat there, plugging my ears, and I tried to write out a checklist for Galya: new password, new device, new Apple ID, new SIM card.” Timchenko tried at first to “laugh it off,” says Alexey, but eventually she burst into tears: The most unpleasant questions came from Ivan: “What documents were you working with on your iPhone? Did you activate two-factor authentication everywhere?” I already felt like I’d been stripped naked in the town square. Like someone had reached into my pocket. Like I was dirty somehow. I wanted to wash my hands! And then my partner and best friend starts interrogating me as if I’d put everyone at risk. It really hurt… But I’d have demanded the same if I were in his shoes. Ivan was just very nervous. It is virtually impossible to prevent infection by Pegasus; it can hack any gadget running a single application vulnerable to the software, including apps preinstalled by Apple itself. A device hijacked by Pegasus isn’t easy to spot, either. For instance, Timchenko had no reason to suspect anything was amiss with her iPhone, except for moments when it seemed warmer than usual, which she attributed to her new charger. Citizen Lab’s analysis shows attackers likely infiltrated Timchenko’s iPhone through HomeKit and iMessage. Senior researcher John Scott-Railton says his team found digital footprints unique to Pegasus. “No other spyware would have left this,” he told Meduza. Researchers believe Timchenko’s hackers used the so-called “PWNYOURHOME” vulnerability, which targets iPhones’ built-in HomeKit functionality and exploits iMessage to install the spyware. Scott-Railton says this hack is possible even on devices where HomeKit was never activated. Citizen Lab collected “forensic artifacts” from Timchenko’s iPhone showing that the device was infected with Pegasus on February 10, 2023. Wild timing As managers crowded into Galina Timchenko’s office and scrambled to assess the worst intrusion in Meduza’s history, another event back in Russia suddenly demanded the newsroom’s complete attention: a mercenary leader shot down several helicopters, seized a military base, and announced a “march on Moscow.” It was June 23, 2023, and the Pegasus hack silently took a backseat to Yevgeny Prigozhin’s mutiny as Meduza mobilized its newsroom to cover the breaking story. When the senior staff could later contemplate the possible reasons for Timchenko’s Pegasus infection, the date of the infiltration (February 10, 2023) wasn’t immediately significant to managers. But it should have been. On February 11, one day after Pegasus hijacked Timchenko’s iPhone, she and Kolpakov joined other representatives of Russia’s exiled independent media in Berlin at a confidential seminar organized by the Redkollegia journalistic prize committee. Media managers and lawyers attended the private conference to discuss the legal aspects of operating in Russia under the conditions of total state censorship and the mass persecution of journalists and activists. Just two weeks earlier, Russia’s Prosecutor General formally outlawed Meduza’s reporting, designating the outlet an “undesirable organization.” Timchenko recalls that colleagues meeting in Germany expected the same thing would happen to them before long. Pegasus was already running on Timchenko’s phone when she joined the meeting in Berlin. Whoever hacked the device could have used it as a wiretap, remotely activating the microphone to record anything said within earshot. The hackers might have turned on the camera just as easily. “They could have used Galina’s phone like a bug to listen in on what the Russian journalists were planning,” says Access Now’s Natalia Krapiva. “My first thought was the Russian state and the Russian intelligence agencies, of course,” recalls Timchenko. “Who else cares about me?” The first Russian journalist The attack against Galina Timchenko is the first confirmed case of Pegasus being used against a Russian journalist. Natalia Krapiva at Access Now confessed to Meduza that she’s actually somewhat comforted to see the spyware surface here because researchers have tested the phones of nearly two dozen journalists and activists from Russia and found all manner of malware but never Pegasus. “I was afraid that [they] were being tracked by something we couldn’t detect,” she explained. “The first confirmed case was shocking, thrilling, and a relief all at once. Now, at least, we have a thread to pull.” Identifying Pegasus infections is challenging work, even for technical experts. “These spyware programs are capable of hiding logfiles and concealing traces of their own presence on a device,” explains John Scott-Railton at Citizen Lab. “It’s a constant technological race.” In 2016, it was researchers at Citizen Lab and Lookout Security who first uncovered traces of the existence of Pegasus, revealing in a bombshell report that NSO Group’s “remote monitoring solution” was used to spy on Ahmed Mansoor, an internationally recognized human rights defender based in the United Arab Emirates. In the years since this discovery, experts have tracked Pegasus’s digital footprints and learned which states are NSO Group’s clients. Much of Citizen Lab’s work is devoted to searching for the servers needed to run Pegasus. “It’s a service, and NSO Group sells access to it,” says Krapiva. “When it signs a contract, the company sends a whole team to the client state to organize training sessions on how to run the tool. All this requires technical infrastructure, and Citizen Lab is constantly trying to monitor it.” Scott-Railton told Meduza that his team looks not just for the infrastructure used in attacks but also for what’s needed to extract data. “In other words,” he explained, “[we look for] all the servers where the information collected from infected devices ends up.” A message from Galina Timchenko: Sometimes we become the heroes of our own stories: it's a rather strange experience to turn from the subject into the object. In my case, first as the object of an attack, and then as the object of an investigation. But it's at precisely these moments that you realize what good people you have in your corner: fellow journalists, developers, security specialists, and most importantly, readers. Millions of people in Russia who haven't give up, despite enormous pressure. Hundreds of thousands around the world who understand the value of freedom of speech. We need your help to continue our work. Support Meduza. The no-no list NSO Group says it sells its spyware only to vetted state agencies, but Israeli geopolitical interests often influence the company’s decision to work with particular partners. For these reasons, the firm reportedly refuses to use Pegasus against either American or Russian telephone numbers. “Infected phones cannot even be physically located in the United States; if one does find itself within American borders, the Pegasus software is supposed to self-destruct,” the spyware’s designers said in 2020. A year earlier, when the Estonian government bought access to Pegasus, NSO Group informed its new client that using the spyware against Russian targets is prohibited. Israel has also reportedly blocked Ukraine from acquiring Pegasus, fearing Moscow’s wrath. “According to people close to NSO and the Israeli government, they don’t approve such infections because it will disrupt relations with these countries,” says Natalia Krapiva. The company has also claimed that Russia and China are among the nations that will “never be customers,” citing internal due diligence that scrutinizes potential clients’ track records on human rights, corruption, safety, finance, and abuse. NSO Group chief executive Yaron Shohat told The Wall Street Journal in January 2023 that the firm was “committed to its core business of supplying governments around the world who are allies of the U.S. and Israel,” despite downsizing after losing clients because of the Biden administration’s measures. Moscow possibly has its own reasons for refusing to do business with NSO Group. Investigative journalist Andrey Soldatov has argued that Russia’s intelligence community “is a seller, not a buyer,” on the world market for espionage technology. Soldatov says this is due both to the high quality of Russian spying tech and to the authorities’ “extreme paranoia about foreign spyware.” Revelations about Pegasus, moreover, have corroborated these concerns, showing that the data stolen from targets are transferred to servers in NSO Group’s ecosystem, meaning that Russian agencies would have to share this “information goldmine” with outsiders if they were to sign up as clients. Russia’s Federal Security Service did not respond to Meduza’s questions about Pegasus. “We do not see evidence of Russia using NSO’s product, but that doesn’t mean we know everything,” says John Scott-Railton at Citizen Lab. A spokesperson for NSO Group told Meduza that the company’s technologies “are only sold to allies of the U.S. and Israel, particularly in Western Europe, for the sole purpose of fighting crime and terror, aligned with the global interests of U.S. national security and governmental law enforcement agencies.” “Pegasus systems log every attack in case there is a complaint, and — with the client’s permission — NSO can perform an after-the-fact forensic analysis,” The New York Times reported in January 2022. Six months later, NSO Group general counsel and chief compliance officer Chaim Gelfand told a European Parliament committee that these internal investigations have led to the termination of contracts in eight cases. A year earlier, however, when The Washington Post reported forensic data indicating multiple Pegasus intrusion attempts against Jamal Khashoggi’s wife in the months before his murder, NSO Group’s chief executive said a “thorough check of the firm’s client records” revealed no evidence of Pegasus used against Khashoggi or his loved ones. “After hundreds of victims, we have concluded that the internal review process either doesn’t exist or exists only for show,” says Natalia Krapiva at Access Now. “When a Human Rights Watch employee was infected, NSO responded to all the questions in just a few lines: ‘Thank you, we found nothing with our current customers. Goodbye.’ Of course, they said nothing about what their past clients could have done. It’s all gaslighting.” Kazakhstan and Azerbaijan In its study of Galina Timchenko’s phone infection, Access Now notes that either Kazakhstan or Azerbaijan — two suspected Pegasus clients — could have carried out the attack at Moscow’s request. (According to Access Now, Uzbekistan is not believed to have been a Pegasus customer during the period in question.) “There’s a provisional theory that Russia might have asked its partners,” says Krapiva. “Kazakhstan, for example, has already blocked Meduza twice without any requests.” As far as researchers know, however, neither Kazakhstan nor Azerbaijan has ever executed a Pegasus attack in Europe, and Timchenko was in Germany when the infection occurred. Moreover, evidence collected by Citizen Lab shows that Kazakhstan does not use Pegasus beyond its borders. Scott-Railton told Meduza that Azerbaijan does use the spyware abroad, but researchers have recorded these attacks in no other country except Armenia, which could explain how the phone numbers of Armenian human rights activists have been infected. BACKGROUND Pegasus spyware in the Azerbaijan-Armenia conflict Natalia Krapiva says clients need a bonus package to use Pegasus beyond their borders: “We believe that different NSO customers can purchase different types of licenses. Some buy the rights to hack only within their country. Others buy the rights to infect a large number of countries. We still don’t understand a lot about these secret contracts, but infections outside a client’s state likely require special permission.” Latvia, Estonia, and Germany Timchenko’s hacked iPhone had a Latvian SIM card. Citizen Lab recorded the first Pegasus-related activity in Latvia in 2018, and experts believe Riga still uses NSO Group’s products today, says Scott-Railton. Access Now also does not rule out that the Latvian intelligence community carried out the attack on Meduza’s co-founder. Just two months before Timchenko’s phone was infected, Latvia declared another Russian media organization in exile — TV Rain — to be “a threat to the national security and public order” and canceled its local broadcasting license. “Because of the invasion of Ukraine, there’s distrust of all Russians without exception,” says Natalia Krapiva. “If such surveillance is taking place, it’s very consistent with remarks by the president of the Czech Republic, Petr Pavel, who said intelligence agencies should place all Russians living in the West under ‘strict surveillance’ as the price of Russia’s war against Ukraine.” However, experts at Citizen Lab have never observed Riga using Pegasus against targets outside Latvia’s borders, and Galina Timchenko was in Berlin when her phone was compromised. (Whom exactly Riga has infected with Pegasus remains unknown.) Ivars Ijabs, a European Parliament member from Latvia who participates in a committee investigating Pegasus in Europe, told The Baltic Times in January 2023 that his home country is not among the E.U. members using the “famous Israeli spyware.” But NGOs that monitor Pegasus attacks treat such statements with skepticism. “He’s not the first official to say such things, even in the face of evidence,” notes Krapiva. Latvia’s State Security Service told Meduza that it “does not possess information related to possible attack against Galina Timchenko’s smartphone.” The agency declined to answer Meduza’s other questions (including questions about whether the country uses Pegasus against journalists, Russian citizens, or targets on the territories of other European countries), citing the classified nature of information about its operations. While there’s no proof that Lithuania has used Pegasus, researchers have confirmed that the Estonian authorities bought access to the spyware in 2019. Citizen Lab has corroborated these findings. More importantly, says Scott-Railton, his team has tracked Estonia “infecting targets beyond its borders in many E.U. countries, including in Germany.” Acting under the “utmost secrecy,” the German Federal Criminal Police Office procured its own Pegasus access in 2019 but acknowledged the purchase only two years later. Natalia Krapiva says Germany has tried, albeit unconvincingly, to defend its actions as in step with European laws and democratic values: The report by the European Data Protection Supervisor states explicitly that Pegasus in its original form is fundamentally incompatible with E.U. laws, so Germany, in its own words, is using a “special version that doesn’t violate privacy rights” — some kind of “Pegasus Lite.” But we’ve received no evidence of this, not even an idea of what a “lite” version might be. Also, the European Data Protection Supervisor concludes that Pegasus in any form is fundamentally incompatible with E.U. law. Germany’s Pegasus access reportedly came with “certain functions blocked to prevent abuse,” sources in security circles told journalists, but officials have not explained how this works practically. John Scott-Railton at Citizen Lab says the infection of Timchenko’s phone in Berlin “is a reminder that Europe has an unresolved problem with Pegasus.” “Why Germany isn’t interested in solving this is a mystery to me,” he told Meduza. “For example, why hasn’t Berlin signed the Joint Statement on Efforts to Counter the Proliferation and Misuse of Commercial Spyware? It’s been signed by 11 countries, including Denmark, France, and Sweden.” Access Now points out that the four E.U. members that have become new centers of Russian anti-war emigration — Latvia, Estonia, Germany, and the Netherlands — are all suspected Pegasus users. In fact, the E.U. PEGA Committee revealed at least 14 E.U. states and 22 operators of Pegasus in the European Union, and only NSO Group’s contracts with Hungary and Poland are no more. Access Now considers the attack on Galina Timchenko to be at least the fourth in a series of similar cases across Europe in the past year. (Meduza knows the details of these other attacks, but the victims have asked for privacy.) The growing tendency in Europe to treat journalists as a threat has also started manifesting in E.U. laws, says Krapiva. The European Commission recently adopted new rules intended to protect reporters against malware, but some member states — mainly France and Sweden — watered down the language in the European Media Freedom Act in such a way that the law actually legitimizes the surveillance of journalists on national security grounds, Krapiva warns. The road ahead “I’m absolutely shocked we’re seriously discussing that a European state could have done this,” says Ivan Kolpakov, Meduza’s editor-in-chief. “I’m probably naive, but this seemed impossible to me. The consequences could be devastating, and this concerns not just the news media in exile but the media in Europe generally. If such software could be installed on the phone of a journalist from Russia, who knows what’s stopping European intelligence agencies from infecting any journalist at all.” “I can’t reconstruct the logic of European intelligence agencies that might have installed Pegasus, and I don’t want to make assumptions,” says Galina Timchenko. “Moving forward, we’ll act in accordance with what our lawyers advise. I won’t be silent.” NSO Group declined to answer Meduza’s questions about whether it knew of the attack on Timchenko and which of its clients might have staged the intrusion. The company’s spokesperson also did not say if it is aware of cases in which Pegasus has been used against journalists in European countries or against Russian nationals, or if NSO Group knows of situations where one E.U. member state spied on a target in another E.U. member state. In any case, NSO Group admits no responsibility for the attack on Timchenko. The company’s spokesperson stressed that the firm “investigates all credible allegations of misuse” but did not say if NSO is prepared to conduct an internal investigation into the use of Pegasus against Meduza’s co-founder and publisher. Today, Ms. Timchenko carries two phones: a new one she bought after the intrusion and the formerly infected gadget (Citizen Lab confirmed that Pegasus is no longer installed on the device). She says she decided to keep it as a souvenir. “There’s nothing on it except messages with my hairdresser and manicurist,” she says. “Let it be. It will remind me to keep looking over my shoulder.” Given the enormous cost of using Pegasus, Timchenko is still confounded that someone infected her with the spyware. “Just what were they planning to find? They put me under a magnifying glass, hoping to catch something… Go ahead and watch, you creeps! Feast your eyes.” Whatever happens with Timchenko’s case, NSO Group currently faces multiple lawsuits, including one from Apple, which accuses the Israeli firm’s employees of being “amoral 21st-century mercenaries.” Amnesty International, members of the European Parliament, former U.N. Freedom of Expression Special Rapporteur David Kaye, and others have endorsed a global moratorium on the sale of all such surveillance technology until more rigorous rules and regulations can be implemented internationally. “State-sponsored actors like the NSO Group spend millions of dollars on sophisticated surveillance technologies without effective accountability. That needs to change,” said Craig Federighi, Apple’s senior vice president of software engineering. NSO Group has turned to lobbying as these pressures mount, especially in America. “They’re making a big effort to lift the U.S. sanctions,” Krapiva told Meduza. “Recently, Robert Simonds, a Hollywood producer who’s worked with Adam Sandler, was eyeing an investment in NSO Group. So, they’re staying the course.” Since June 2023, experts have analyzed the phones of several dozen Meduza employees. It’s still unknown what specific information Timchenko’s attackers were after. This ambiguity worries Meduza’s technical director, Alexey, more than anyone. “Until I know the motive, I have to expect the worst,” says Alexey. “I deal with our security not just in a technical but in the broadest sense of the word: every day, I think through how they’re going to kill us and bring us down. Surveillance, harassment, threats — I’ve already considered all these scenarios and experienced them myself, in a sense. As for Pegasus, until we have more details, we can’t rule out that Russia could have ordered the infection and that this spying could have the most serious consequences, right up to somebody being eliminated.” Timchenko, meanwhile, says she hasn’t yet contemplated such consequences of being watched through Pegasus. “I already look back wherever I go and watch for anyone following me in a car. Meduza’s founders have always lived like this,” she says. “If they want to do it, they’ll do it.” If you believe you may be under spyware surveillance, backup your device (here are instructions for iPhone and Android) to preserve evidence of a possible attack, and contact Access Now. AM I IN DANGER? Story by Lilia Yapparova Adapted for Meduza in English by Kevin Rothrock Share to Facebook or Twitter Site Index MEDUZA About Code of conduct Privacy notes Cookies Meduza in Russian Support Meduza PLATFORMS Facebook Twitter Instagram RSS PODCAST The Naked Pravda NEWSLETTERS WEEKLY DAILY Underreported stories. Fresh perspectives. From Budapest to Bishkek. Protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. SUBSCRIBE Found an error? Select it and press Ctrl+Enter © 2023 Meduza. All rights reserved WE USE COOKIES! WHAT DOES THAT MEAN?",
    "commentLink": "https://news.ycombinator.com/item?id=37496589",
    "commentBody": "Meduza co-founder&#x27;s phone infected with PegasusHacker NewspastloginMeduza co-founder&#x27;s phone infected with Pegasus (meduza.io) 393 points by Klaster_1 20 hours ago| hidepastfavorite241 comments ramraj07 18 hours agoWhat are the odds that NSO has like 20 other zero-days in their arsenal each set ready to deploy the day the current vulnerabilities are discovered and patched? Does Apple know or have a clue how bad this problem could be?Surely whatever money these guys spend buying these zero-days, Apple is rich enough to increase their bounties large enough to attract them to right side instead?It’s not clear in the article if the author had to take any action to get this program installed. If that’s not required, what should anyone who even vaguely suspects state sponsored spying do? Sounds like it’s safer to just not use a phone or try and circle through a series of them you buy second hand or something. reply shmatt 16 hours agoparentThis comment pretty much dissects&#x2F;explains NSO in the best terms ive seen in HN before.\"Pegasus\" is not one hacking entity like most articles make it out to be. Its1) A bunch of services that download data, given root access to a phone2) a bank of 0-days, we don&#x27;t know how deep.For all we know, there are times when \"Pegasus\" doesn&#x27;t work for hours, days, weeks, until the 0-day is rotated. We do know from some leaks that they have a mix of non-click and click exploits, and also support all different kinds of phone OS.Their hacking abilities are definitely overstated, for all we know, for smooth continuous customer support, they could be buying 100% of their 0-days, and not finding any themselves. A 0-click 0-day for iPhones is worth about $2,000,000[1], a company with contracts like NSO can afford a lot of those. IMO the media portraying them as super-hackers is pure hype. Its a bunch of crooked business people who figured out how to extract money out of countries[1] https:&#x2F;&#x2F;arstechnica.com&#x2F;information-technology&#x2F;2019&#x2F;01&#x2F;zerod... reply sugarpile 15 hours agorootparentAn extension to the link [1] above is: the price NSO pays for android zero click is higher than the price they pay foriPhone zero click exploits. This implies they do indeed a catalog of iOS exploits stashed. reply gorbypark 2 hours agorootparentI&#x27;ve heard a few people theorize about why Android exploits seem to pay more. The theory is that Android is 1) very fragmented, with each manufacturer having different versions and modifications and 2) updates are much slower&#x2F;non existent.To get the top payout, you&#x27;d need to come up with something that works across all manufacturers versions of Android and probably across 4 or 5 major versions. You might be able to find an exploit for all Androids running version x, but if that version only has 10% of the android market, you wouldn&#x27;t get a full payout.iOS users tend to heavily be on the latest version, or one version behind at most. As an example, most recent iOS exploits in the wild seem to be using iMessages. On iOS, you can focus your efforts at one thing. On Android? Your surface area is much smaller because each manufacturer is going to be shipping their own messenger app, for example. reply Veserv 12 hours agorootparentprevThe link is about Zerodium, not NSO. Also, 2.5M $ vs 2M $ is not a meaningful difference, neither presents a meaningful road bump to competent attackers. But your point that it indicates a robust stash is fair. They 100% do. reply civilitty 15 hours agorootparentprevIt doesn&#x27;t really imply anything because iPhone&#x27;s global market share is less than 30% with customers concentrated in North America and China, both danger zones for NSO operations. Android exploits might also take far longer to patch across all vendors and users might take longer to update compared to iOS.It&#x27;s fairly probable that iPhone exploits are just less valuable to a shady intel operation that sells mostly to small authoritarian regimes. reply henry2023 13 hours agorootparentYour comment is not considering that these governments are more likely to target politicians and journalists which are more likely to use iPhone regardless of where they are located. I don’t know if the implication that iPhone is less secure holds but it’s likely. reply civilitty 7 hours agorootparent> Your comment is not considering that these governments are more likely to target politicians and journalists which are more likely to use iPhone regardless of where they are located.Are you sure that&#x27;s true? In my experience governments often choose Android because they prefer the platform&#x27;s organization-wide device management options over iOS. Many dissidents&#x2F;journalists choose Android because it&#x27;s easily rootable, giving them more privacy and control (I have a very small sample of the latter, however) reply NLPaep 3 hours agorootparentYou could use Apple’s lockdown mode. It’s unmatched on Android.Google and Samsung warn you about enabling root.Samsung:Is rooting your smartphone a security risk?Rooting disables some of the built-in security features of the operating system, and those security features are part of what keeps the operating system safe and your data secure from exposure or corruption. Since today’s smartphones operate in an environment filled with threats from attackers, buggy or malicious applications, as well as occasional accidental missteps by trusted users, anything that reduces the internal controls in the Android operating system represents a higher risk.https:&#x2F;&#x2F;insights.samsung.com&#x2F;2022&#x2F;07&#x2F;28&#x2F;what-are-the-securit...Google:Security risks with modified (rooted) Android versions Google provides device security protections to people around the world using the Android operating system. If you installed a modified (rooted) version of Android on your device, you lose some of the security protection provided by Google.Important: If your account is enrolled in the Advanced Protection Program, don’t use that account on a device with a modified version of Android. Modified versions of Android can undermine Advanced Protection’s increased security features.https:&#x2F;&#x2F;support.google.com&#x2F;accounts&#x2F;answer&#x2F;9211246?hl=en replyperlgeek 3 hours agorootparentprevI could very well imagine that NSO charges per device exploited, and charges more for zero-click exploits used.Each exploited phone raises the chance of the exploit being found and burned, so they really have to incentivize their customers to use them sparingly. reply saagarjha 4 hours agorootparentprevWhat makes you think they aren’t super hackers? reply hgsgm 14 hours agorootparentprevIt doesn&#x27;t matter whether NSO are genius hackers or their freelancers are. They are still outsmarting Apple all day long. reply fatfingerd 13 hours agorootparentWhen significant functionality and backwards compatibility is required and money is limited, I&#x27;ll happily work for red team, when brick is a valid solution, I will happily work for blue team. reply jacquesm 4 hours agorootparentAs long as this attitude persists IT will continue to be viewed by people outside of it with the same degree of respect as your average mercenary.Consider that in a just world you&#x27;d be in jail. Does the money still look that good? reply fatfingerd 2 hours agorootparentThe US carefully developed its cyber security plan during the word press macro era. Let&#x27;s send the FBI to foreign countries in the hopes of arresting teenagers who learned how to cut and paste, genius.Unfortunately, it forgets how to do this if the country is Israel instead of the Philippines.Is there some solution in that to making sure 100% of possible red team members are more aligned with the profit interests of the US&#x27; strategic private companies than the US strategic partners in running illegal conspiracies?I&#x27;m baffled as to what utopia of a profession has global tool collaboration and consequences, but somehow manages to deal with 230 groups of nationalists, thousands of sects, and embargo&#x27;s on any one group paying people across all of these to provide a regulatory framework for safe and human benefiting tools in their category with no edge cases. If such a regulatory framework existed maybe it would shut down these mobile phone companies over behavioral harm? reply jacquesm 38 minutes agorootparentPersonal responsibility is where this starts. Not with the US, not with Israel or the Philippines. It starts with us, the technical people that do these things. replygeorgelyon 18 hours agoparentprevI didn’t find any mention of Lockdown Mode in the article, which is advertised as something a user in this position could use to decrease their attack surface. I find it surprising journalists covering high-risk stories don’t just all have this on by default. A lot of these no-user-interaction exploits are via vulnerabilities in decoders for images and such that run when a message is received, unless the phone has Lockdown Mode enabled (LM also disables other types of functionality). Has anyone seen evidence of a phone with Lockdown Mode enabled being compromised (not saying it’s impossible, just curious)? reply fh9302 17 hours agorootparentSo far there has not been a confirmed Pegasus infection with lockdown mode enabled. It&#x27;s certainly possible but will require more sophisticated exploits, thus increasing the price per infection. reply HenryBemis 17 hours agorootparentI will assume that unless the cost per infection is a staggering number, if a \"baddie\" wants to \"get in\" they wouldn&#x27;t be phazed by $50k or $100k. I assume that the value of the intel collected (contacts, eavesdropping, etc.) would be far more valuable as it would reveal whistleblowers, opposition tactics, contacts, candidates to fall off windows&#x2F;balconies, candidates to be chopped up, etc. reply H8crilA 12 hours agorootparent0-click 0-day costs more like $2M (from other comments and links in this thread). reply saagarjha 4 hours agorootparentThat’s how much an exploit sells for. That isn’t the cost an infection. replyinsanitybit 18 hours agoparentprev> What are the odds that NSO has like 20 other zero-days in their arsenal each set ready to deploy the day the current vulnerabilities are discovered and patched?I feel it&#x27;s the safe money, certainly. One exploit dev in a given year can churn out multiple weaponized 0 days, surely they have more than one dev working on such things, so you&#x27;re talking about a stockpile of likely dozens of vulns. Some might collide with public vulns so they lose a few, but you knock one down and I have to assume they have others staged.> Apple is rich enough to increase their bounties large enough to attract them to right side instead?That&#x27;s a good question. I think at NSO&#x27;s price point the answer is probably \"no\", but I don&#x27;t know. At best Apple could be competitive, but bug bounty work is far riskier - you might spend a long time without getting a payout, either due to some bad luck, collisions with already reported vulns, or a vendor just being a dick (pretty sure Apple have been dicks).> what should anyone who even vaguely suspects state sponsored spying do?Probably have more than one phone, for starters. Use authenticated protocols, not SMS&#x2F;MMS. It&#x27;s insane that anyone can just send data to your phone unprompted. I&#x27;d probably disable cell service altogether unless I&#x27;m actively making an outbound call to a known contact. reply wayfinder 17 hours agorootparentThe only way Apple could make them report the vulnerability is if the bounty was not far from the amount of profit that NSO is making with their software. reply Muromec 18 minutes agorootparent> The only way Apple could make them report the vulnerability is if the bounty was not far from the amount of profit that NSO is making with their software.At which point it becomes cheaper to buy a law to force disclosure of those 0 days to vendor? reply devmor 17 hours agorootparentprevThe comment is not suggesting that Apple make the vulnerability attractive to report for the NSO as an organization, but presumably attractive to report for whatever hackers the NSO may purchase vulnerabilities from - or individuals employed by the NSO.In such a case, Apple \"only\" needs to make the bounty high enough to significantly exceed the sale price of the vuln, or the salary of aforementioned employees. reply fomine3 2 hours agorootparentFor who had already sold a vuln to a criminal org like NSO once, I wonder will they switch to clean Apple. Perhaps they get more chance to be investigated, or not?. reply wayfinder 9 hours agorootparentprevYeah you&#x27;re right. For some reason I was thinking only NSO had these zero-days, which is not possible. reply andersa 16 hours agorootparentprevWhy is it on Apple to defend everyone against hackers sponsored by another country to begin with? The governments should be providing any resources necessary to defend here... reply zozbot234 16 hours agorootparentBecause Apple makes the phones, silly. The iPhone is a 100% proprietary device, we know zilch about what code is running on it. Why should anyone be responsible besides the manufacturer?Maybe the government should care about the Obamaphone, but not anything beyond that. reply austhrow743 16 minutes agorootparentIf an Israeli hit squad kills someone in a McDonalds, we dont get on McDonalds case for not providing a safe and secure place for their customers. Not putting up a sign when the floor is wet is on them. Assassins is on the government. It&#x27;s not clear to me why things being different in the software world is so obvious that not seeing why is silly.Regarding Obamaphone, in the US there is a government agency responsible for such things. The NSA. It&#x27;s tasked with securing US information infrastructure on top of its more known role of signals intelligence. It just happens to favour the latter over the former so its not about to share its stockpile of iPhone zero days with Apple. reply saagarjha 3 hours agorootparentprevNSO Group seems to know quite a bit about what code is running on it. reply kube-system 15 hours agorootparentprevClose to 100% but not quite. It has some open source components. reply Veserv 15 hours agorootparentprevBecause that is what they advertised they would do [1].“Apple makes the most secure mobile devices on the market. Lockdown Mode is a groundbreaking capability that reflects our unwavering commitment to protecting users from even the rarest, most sophisticated attacks,” said Ivan Krstić, Apple’s head of Security Engineering and Architecture.I mean, we know nobody on their team actually believes Lockdown mode can protect against state funded actors with even a tiny $10M budget since their Lockdown mode total bypass bug bounty is only $2M.But they did say it in their marketing, so they should be held to it even if we know for a fact that they are totally incapable of doing so. This is not a question of money, it is a question of ability, and we know they do not have that.[1] https:&#x2F;&#x2F;www.apple.com&#x2F;newsroom&#x2F;2022&#x2F;07&#x2F;apple-expands-commitm... reply zozbot234 15 hours agorootparent> Apple makes the most secure mobile devices on the market.Well, they&#x27;re not wrong on that one point. As it turns out, \"most secure\" is a pretty low bar. We&#x27;ll see how Purism&#x27;s Freedom Phone fares once it reaches genuine daily-driver status and it too becomes a target for this class of attacks. reply kube-system 15 hours agorootparentBeing open source doesn&#x27;t mean immune to vulnerabilities. (and Purism&#x27;s stuff will likely never be 100% open source due to regulatory complications with basebands)Niche software often fares very poorly in terms of security because few people are trying to exploit it. reply charcircuit 14 hours agorootparentprevPureOS is decades behind in security compared to Android or iOS. reply anthk 13 hours agorootparentPureOS with Flatpak, Wayland and such make it close. reply akyuu 12 hours agorootparentNot really. Even with modern technologies, the Linux desktop technology stack is very, very far behind when it comes to security.The Linux kernel itself is a very weak foundation security-wise, the only way Android and ChromeOS get away with it is by using a very small feature set and restricting everything else as much as possible with seccomp, SELinux and heavy sandboxing.The Linux desktop userland doesn&#x27;t have meaningful hardening features compared to other platforms (even Windows is ahead, sadly). For example, practically all distros use glibc&#x27;s memory allocator which has both poor performance and security [1] and their toolchain is based on gcc, with no support for modern compiler security features such as CFI (with the sole exception of Chimera Linux). Not to mention the permission model is completely outdated, like in that xkcd comic. Flatpak only mitigates this partially, because the Flatpak sandbox is very weak. The people working on Flatpak are doing their best, but from reading some GitHub issues, it&#x27;s clear they are badly overworked and not security experts. The person responsible for Flatpak&#x27;s seccomp sandbox has said it isn&#x27;t even his main responsibility and he doesn&#x27;t have much knowledge about seccomp and is learning along the way [2]. The Flatpak seccomp filter is based on a denylist rather than an allowlist, and many dangerous syscalls can&#x27;t be blocked because applications rely on them (e.g. Firefox needs ptrace for the crash reporter). You also have to be very careful and use Flatseal (which is not officially supported) to deny permissions such as &#x2F;home filesystem access, because it lets Flatpak apps override their own permissions by design [3]. And dangerous kernel components like io_uring are exposed [4], while Google disables them on their systems because of their exploitation potential.Here is a more detailed article examining the lack of security of Linux phones in case you&#x27;re interested: https:&#x2F;&#x2F;madaidans-insecurities.github.io&#x2F;linux-phones.htmlIf you want a FOSS-based secure phone, GrapheneOS is the best option.[1] Check this comment by GrapheneOS founder for some technical details and how it compares to hardened allocators such as Android&#x27;s Scudo or Graphene&#x27;s hardened_malloc: https:&#x2F;&#x2F;github.com&#x2F;NixOS&#x2F;nixpkgs&#x2F;issues&#x2F;90147#issuecomment-6...[2] https:&#x2F;&#x2F;github.com&#x2F;flatpak&#x2F;flatpak&#x2F;issues&#x2F;4466#issuecomment-...[3] https:&#x2F;&#x2F;github.com&#x2F;flatpak&#x2F;flatpak&#x2F;issues&#x2F;3637[4] https:&#x2F;&#x2F;github.com&#x2F;flatpak&#x2F;flatpak&#x2F;issues&#x2F;5447 replysaiya-jin 14 hours agorootparentprevWait, the reward for completely bypassing most hardcore security measures in their most important device for the most valuable company in the world worth over 3 trillion is mere 2 millions?Thats not a honest proposition by its very definition, just look at the assymetry of those numbers. Serious offer would add at least 2 zeroes to that. reply Veserv 13 hours agorootparentIt is actually reasonably fair, it only costs around 1-2M $ to find one. You expect Apple to pay 100M $ for 1M $ of work?The real question is why is Apple allowed to lie about providing meaningful protection against state actors when they only think it only costs 2M $ to break it. In no universe is 1&#x2F;5 the cost of a tank even a road bump for a state actor.The other question is why is their security so terrible. The short answer is that they demonstrably know nothing about security since this is the most they have been able to do after decades of work, billions of dollars, and repeated promises of meaningful security. When somebody spends billions of dollars and decades failing to achieve even 1&#x2F;10th of what they promised, you should take any new statements as extraordinary claims and demand extraordinary evidence. reply zozbot234 13 hours agorootparent> The real question is why is Apple allowed to lie about providing meaningful protection against state actorsIt&#x27;s not like anyone has been doing any better. Mobile phones are embedded devices targeted to everyday consumers, basically toys. They&#x27;ve never been engineered for anything like meaningful security against even mildly sophisticated attacks. The industry simply doesn&#x27;t care about this, e.g. most phone SoC&#x27;s are still not protected against misbehavior by any of the included devices, each of which is running some unknown proprietary firmware. That&#x27;s just par for the course in the embedded ecosystem. reply Veserv 13 hours agorootparentWhy does the quality of any other product matter here?Apple marketing claims it provides meaningful protection against state actors. Apple engineering says it does not. Even if nobody can do it, even if Apple is closer than anybody else, that does not excuse lying to people who are betting their lives on Apple’s representations that it works.Apple can not protect against state actors. Apple knows that. If you are at risk, the only safe thing to do is avoid Apple (and all other smartphones). Apple knows that. They lie and insinuate that a iPhone is fit for this task so they can sell a few more iPhones caring not a single bit for the lives at risk. That is grossly unethical. Yet, it is par for the course in “cybersecurity”. That does not make it acceptable, that just means everything is rotten. replyinsanitybit 16 hours agorootparentprevApple is welcome to seek aid from the US Government, I imagine they would be happy to assist. reply jonathanstrange 2 hours agorootparentThat&#x27;s the question, though. The NSA is known to have strongly conflicting objectives. On the one hand, they&#x27;re supposed to secure US government devices and sometimes assist US companies in securing devices. On the other hand, they&#x27;re supposed to surveil foreign citizens using such devices and the devices of US citizens who communicate with foreign citizens, as well as assisting other US agencies in doing that.In a nutshell, whether they will increase or subvert your security depends on factors outside of your control. But unless they have found ways of surveilling foreigners without compromising the security of any Apple device, it&#x27;s almost certain that they won&#x27;t disclose their own 0-day exploits to Apple. reply Dah00n 13 hours agorootparentprevThe US government have already \"assisted\" plenty. Every assist is a setback. IE. Snowden&#x27;s revelations, encryption standard weaknesses, backdoored devices, etc. reply insanitybit 12 hours agorootparentObviously not what I&#x27;m talking about. reply jacquesm 3 hours agorootparentNo, but that is exactly why Apple might be a little bit reluctant to go for assistance. It&#x27;s a bit like going to the neighborhood burglar to ask him how to secure your house. \"I&#x27;ll be right over to take a good look at your property\" is not the answer you think it is. replyrjvs 1 hour agoparentprev> what should anyone who even vaguely suspects state sponsored spying do?They should keep their phone in Lockdown mode [1]. It&#x27;s less useful as a computer in that case but the restrictions reduce the attack surface.1. https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT212650 reply Veserv 17 hours agoparentprevThey probably have around 3-10 other zero-click zero days on hand. And if NSO somehow burns all of their in-house production, the vulnerability brokers I know have a couple tens ready for usage in their inventory for a few million dollars each. This is not even private knowledge; the brokers run legal US incorporated businesses that sell to governments, businesses, and the vendors who make the insecure products such as Microsoft and Apple. Apple knows for a fact that they are delivering products with tens to hundreds of known critical security defects.Apple does not buy out the zero-days for two reasons: First, you can not buy your way to security. Second, the benefits do not outweigh the costs.For the first point, it is impossible to buy your way to serious security. Apple currently pays a $1M bounty for a zero-click RCE with persistence [1] and $2M to do the same to Lockdown Mode, around the cost of a single Tomahawk cruise missile. They set this price because it takes around 1-3 engineer-years to find such a security defect, so the bounty is approximately the cost of labor. If they paid $10M, around the cost of a single M1 Abrams tank, they would get a absolute flood of new reports since suddenly the ROI is 10x and the number of security defects detectable at the $10M level is vastly more than at the $1M level. However, to deter countries, you need to get to at least the $100M level, the cost of a single F-16. At the few million dollar level there are already tens to hundreds of known security defects, so at the $100M level there are almost certainly thousands to tens of thousands of vulnerabilities. So, to buy their way to protection against state-funded attackers would cost them trillions to tens of trillions of dollars, if it is even possible at all. Note that literally nobody has ever gotten past the few million dollar range using this strategy, or frankly using any strategy when attempting to retrofit a system not designed for security like iOS or Windows.For the second point, what does Apple gain by buying the zero-days? People keep buying iPhones no matter how many thousands of security defects get reported. All they have to do is make up new bullshit like Lockdown mode and everybody feels warm and fuzzy inside. The company, that has never once made a product within a factor of 100x of what is needed to protect against state-funded attackers, just makes up a marketing spiel about how they are \"totally going to do it this time for sure, pay no attention to our record exclusively consisting of hundreds of failures\" and everybody eats it up. We know they do not believe their own marketing fluff because they set the bounty for lockdown mode at $2M, only double the $1M for regular iOS, which is still only 1&#x2F;5 of a single tank. Do you think a single state-funded attackers will be dissuaded by the price of a fractional tank? It costs more money to start a new McDonalds store. All the companies like Apple, Microsoft, Amazon, Google, Cisco, Crowdstrike, etc. need to do is lie and for some reason everybody keeps believing them for the thousandth time and their sales are protected.Commercial IT systems are completely and utterly insecure against attacks by moderately funded attackers. If you have operations worth more than $1M or are at the risk of targeted attacks, you are completely, 100%, vulnerable no matter what or how many of these systems you use. If that is not acceptable, then you must not use standard commercial IT systems with connectivity. That is, unfortunately, the only solution that currently works. It is up to you if you think the tradeoff is worth it.[1] https:&#x2F;&#x2F;security.apple.com&#x2F;bounty&#x2F;categories&#x2F; reply comboy 18 minutes agorootparentDo we have any idea how often are these $1M &#x2F; $2M bounties collected from Apple? reply webel0 12 hours agorootparentprevA third reason Apple doesn’t increase their bounties: they don’t need to. There is no secure phone on the market. Your only options are insecure phone (iOS, android, whatever) or no phone at all. So while it might be nice to be able to claim that you’re relatively secure, there’s very little to be gained by spending all of the resources required to buy up all exploits. reply ponkipo 15 hours agorootparentprevnice comment, thanks for the very interesting perspective! reply stef25 18 hours agoparentprev> Surely whatever money these guys spend buying these zero-days, Apple is rich enough to increase their bounties large enough to attract them to right side instead?TL;DR, Apple probably doesn&#x27;t care enoughYou&#x27;re in a very exclusive club if you&#x27;re targeted by NSO (ie. very few people are victims) and most of the general public probably doesn&#x27;t understand or care enough to get their pitch forks out.Personally if I was anywhere near being a possible NSO target I&#x27;d dump all my devices or at least have them fully airgapped, the only way you&#x27;ll win that fight. reply zozbot234 16 hours agorootparent> You&#x27;re in a very exclusive club if you&#x27;re targeted by NSO (ie. very few people are victims)That&#x27;s a dangerous assumption. We only know about the victims who are clueful enough about OPSEC to even be informed about the issue, let alone find out about an attack. reply numpad0 23 minutes agorootparentIt&#x27;s hard to make yourself a target to highly qualified subgroups of intelligence people, for reasons rightful or not. Simply committing or conspiring for crimes with e.g. extremist terrorist groups won&#x27;t do that(not recommending to do so, obviously). reply saagarjha 3 hours agorootparentprevWe do know it’s somewhat expensive to target people. reply Terretta 16 hours agorootparentprev> TL;DR, Apple probably doesn&#x27;t care enough You&#x27;re in a very exclusive club if you&#x27;re targeted by NSO (ie. very few people are victims) and most of the general public probably doesn&#x27;t understand or care enough to get their pitch forks out.And yet:(a) Lockdown Mode cost money to develop and will cost support time from casuals turning it when they shouldn&#x27;t but Apple did it anyway, and(b) the journalists only know this happened because Apple told them proactively.Sounds like they care at least a little. reply Dah00n 13 hours agorootparentSomeone also cared about programming Minesweeper in Windows. That doesn&#x27;t mean Microsoft as a company care even a miniscule amount about it. Someone at Apple cared more than not at all is as true. reply devmor 17 hours agorootparentprev>Personally if I was anywhere near being a possible NSO target I&#x27;d dump all my devices or at least have them fully airgapped, the only way you&#x27;ll win that fight.You still wouldn&#x27;t win that fight without applying those rules to everyone you come in contact with. And even then, the absence of such data could create a pattern enough to identify parts of your life if they have enough data from people that are not around you.Escaping surveillance from bad actors is essentially no longer a winnable fight. you can only do your best to mitigate it. reply Roark66 4 hours agoprevI wonder how Apple decides who to inform and who not to when they detect malware like pegasus. Good they did inform a person in question.However, what if this person was much lower profile? Let&#x27;s say a person that lives in a democratic country. Does Apple even know who targeted them? If they do, let&#x27;s say \"if its China or Russia\" we inform. Then what if China or Russia does the exact same thing but using a paid agent in a democratic country?This raises so many questions. And finally, if Apple can detect such malware why isn&#x27;t there an immediate notification from some local app? Like an anti virus for your phone. They must already have something like this, otherwise how would they know? reply saagarjha 4 hours agoparent(Disclaimer: I have no idea how this actually works.) I would guess that running this on-device would be prohibitive and they probably get told accounts and whatnot that are known to have sent the messages, then go in their server logs and check who they reached out to. reply berniedurfee 9 hours agoprevAt some point in the near future “phone” will be replaced with “car” in the title and the outcome will be more tragic.What’s a Tesla 0-day worth I wonder? reply richardanaya 12 hours agoprevWas anyone else amused by the mythological significance? In Greek mythos, Pegasus spawned out of the blood of Medusa. reply 0x38B 8 hours agoparentI’ll admit I always thought ‘Meduza’ was from the Russian &#x2F;meduza&#x2F;, meaning jellyfish, but they did indeed mean the Greek Medusa (1):> Question - why Meduza in particular? That&#x27;s a slippery, unpleasant creature> Journalists are generally unpleasant and slippery and there are few who love them - such is the job. Also \"Medusa\" turns its subject to stone with a glance, which is true of journalists, too. But to be completely honest, we ended up with \"Meduza\" by chance. We thought the paper should be named after the ancient Greek monster that had its head cut off but came alive anyway. We chose \"Meduza\" and then remembered that it was a hydra, but it was too late.1: https:&#x2F;&#x2F;meduza.io&#x2F;cards&#x2F;zaday-vopros-meduze, #75 reply dotancohen 49 minutes agorootparentThe Russian word for jellyfish comes from the Greek myth. In Russian, Medusa is spelled with a Z instead of an S (Медуза). The jellyfish is called such in Russian because the tentacles are similar to Medusa&#x27;s snakes. reply heikkilevanto 1 hour agoprevI have been wondering if it could be possible to make a honeypot for all the known attacks NSO has been using, and warn the user if any such are detected. I bet some of the attackers try a few exploits before they get in, and might trigger an alert.If many people had such alerts, it would draw attention to the action. reply VagabundoP 3 hours agoprevI wonder what practical measures you could take if you believed you were a target of these guys and was a journo&#x2F;activist maybe:- A different device for each app (Whatsapp, Telegram, Signal, etc)- Use web front ends and keep the phone turned off - not sure if it works for all apps- Regularly ditch devices, sell them second hand and get new&#x2F;used phones.- Use the devices to setup meetings in more secure environments, never say&#x2F;text anything into the phone, assume its compromised at all times.And there should be some sanction against NGO, they are scumbags. reply dotancohen 46 minutes agoparentSounds like that would not leave much time for actual journalism. Or room for carrying other equipment. And it would make crossing borders very exciting, explaining all those phones to border control or customs. reply highwaylights 3 hours agoparentprevThis probably isn’t a bad idea for an open-source project.Something akin to Graphene OS where there’s a constant drive to narrow the attack surfaces, but also removing any concessions related to installing apps or Google services entirely.Basically, a phone that has access to encrypted messaging and the camera&#x2F;mics under very controlled circumstances and that’s about it.The restrictions would also limit the popularity enough that it would likely never be worth the cost of targeting, but also provide greater protection to the few people that really need that protection enough to make those sacrifices. reply gorbypark 2 hours agorootparentI&#x27;d love a phone with a bank of iPhone style mute switches, but each hooked up physically to disable the cellular radio, gps antenna, mic, camera and etc. reply Muromec 8 minutes agorootparentpinephone has that, but it&#x27;s a pinephone reply pbmonster 3 hours agoparentprevThere&#x27;s probably a way to quickly detect infection, too: constantly look at all network traffic. It&#x27;s probably pretty difficult to hide the outgoing traffic when they are pulling all your messages and run frequent screen capture. They will encrypt it, but the volume of data should be impossible to hide.Even easier if you have your phone stripped down and locked up in the first place, less apps to ever cause outgoing traffic. reply acqq 41 minutes agoprev> “I’m absolutely shocked we’re seriously discussing that a European state could have done this,” says Ivan Kolpakov, Meduza’s editor-in-chief. “I’m probably naive\"Rather then naive, I think his main problem is that he hasn&#x27;t investigated before the discussed event what the European states actually do in that context. Then he would be just worried, not shocked.Another possibility is that the \"shocked\" is the \"Casablanca shocked\":> Rick: How can you close me up? On what grounds?> Captain Renault: I&#x27;m shocked, shocked to find that gambling is going on in here!> [a croupier hands Renault a pile of money]> Croupier: Your winnings, sir.> Captain Renault: Oh, thank you very much. reply phero_cnstrcts 19 hours agoprevIs there anything that prevents Pegasus from spreading by itself or must it be installed via a targeted attack? And is there a way of scanning for it to see if a phone is infected? reply PeterisP 19 hours agoparentThere is nothing technical that prevents Pegasus from spreading by itself, some of the reportedly involved vulnerabilities could be \"wormable\", however, there are practical reasons that prevent that - for malware like Pegasus, the operator has an interest to avoid uncontrolled spread, since it relies on certain undiscovered and unpatched vulnerabilities staying undiscovered and unpatched, and uncontrolled spread makes it much more likely to be discovered, analyzed and \"killing the goose that lays golden eggs\".So at least for now we&#x27;d expect all Pegasus installations to be a result of targeted attacks. On the other hand, if the tool leaks and becomes readily available to multiple actors, then the incentives change and one of them might decide to make a worm that infects everyone in the world who&#x27;s not patched. reply ChrisMarshallNY 18 hours agorootparentAlso, NSO gets many shekels for each infection. They really don&#x27;t want it spreading. reply fullspectrumdev 19 hours agoparentprevThere is no self propagation code built into Pegasus.It would be relatively trivial to write such - simply have it send the exploit via iMessage to all of a targets contacts, rinse and repeat.This would be counterproductive though - the whole selling point of Pegasus is targeted surveillance, and such exploits are very costly - uncontrolled spreading would make it detected much faster, burning a valuable resource.If such exploits were cheap, it’s plausible you could justify writing a variant that automatically attacks a targets entire address book to mine their social graph, but then you have the problem of analysing a shitload of probably worthless data… reply ramraj07 18 hours agorootparentIf some hacker gets a clearly infectious Pegasus link they should make it spread through messages to everyone. Bricking everyone’s iPhone will probably make all the governments and Apple sit up and do some real damage to these actors. reply Veserv 18 hours agorootparentMany of the Pegasus attacks are zero-click, so no link is needed. All they need to do is send you a message and you are compromised.They presumably also configure their command and control to only persist if it is one of the designated targets and wipe all traces if it is not, so even forwarding the attack payload would probably not do anything. You would need to determine you have been compromised and then reverse engineer the exploit so you could replace the command payload with a irreversible bricking operation to do what you suggest.At that point you might as well spend the $5M-$10M to develop the entire attack yourself. If you are a competitor to Apple spending $10M to completely destroy the $2.7T Apple is literal pocket change; too small to even show up on your financials. reply coderedart 1 hour agorootparent> All they need to do is send you a message and you are compromised.How does that even work? reply Muromec 1 minute agorootparentIt works the usual way -- you make a payload that, when processed by a buggy code, executes itself. If the buggy code happens to be SMS packet parser, image decoder, text rendering, blocklist check or another 2 millions of things that happen to show you incoming SMS (or even better, flash message, or something not visible to user), then you don&#x27;t have to click on it.I mean if the bug in the browser, you have to visit the page to have the payload get to you, but it&#x27;s a phone. A device for other people to contact you. filleokus 18 minutes agorootparentprevFor example by finding an exploit in parsers of media \"attachments\": https:&#x2F;&#x2F;googleprojectzero.blogspot.com&#x2F;2021&#x2F;12&#x2F;a-deep-dive-i... reply astrange 15 hours agorootparentprev> If you are a competitor to Apple spending $10M to completely destroy the $2.7T Apple is literal pocket change; too small to even show up on your financials.You&#x27;re comparing two near completely unrelated numbers here. That&#x27;s not what enterprise value means; it doesn&#x27;t mean much of anything really. reply kube-system 15 hours agorootparentprev> make all the governments and Apple sit up and do some real damage to these actors.International weapons dealing doesn&#x27;t work that way. Point to any manufacturer of weapons and there&#x27;s a bunch of people that don&#x27;t like them. But the countries that benefit from those weapons don&#x27;t agree. reply dron57 19 hours agoparentprevSeems that the NSO business model is based on ultra exclusivity and a very small number of business clients. Technically, Pegasus could probably retransmit itself to infect another device, but it doesn&#x27;t fit their business model so I doubt NSO would do this regularly. reply SEJeff 18 hours agorootparentNation states (like KSA) will likely pay very large sums of money to use this against their perceived enemies abroad. A small and exclusive clientele is how a company like this stays out of the lime light. reply marchukov 19 hours agoparentprevFrom what I was able to read previously, it has no ability to spread by itself and has to be installed by a targeted attack. There is also a tool from Amnesty International that can detect it (or was able to): https:&#x2F;&#x2F;github.com&#x2F;mvt-project&#x2F;mvtIt is a race though, so past info may no longer be valid. However, I doubt it will ever be able to spread by itself, since it uses very expensive zero days to infect and they will be quickly fixed after detection. reply egonschiele 19 hours agorootparentYou also need to jailbreak your phone to use MVT. reply KomoD 18 hours agorootparentNo you don&#x27;t need to, you can reply ThePowerOfFuet 4 hours agorootparentpreviMazing includes the functionality of MVT, no jailbreak required. reply tamimio 18 hours agoparentprevAFAIK, phone numbers are the entry point, it’s the easiest and quickest way to target someone with it, else, it will be more involved to isolate the target, so don’t activate any number on your phone in addition to the lockdown mode, plus the usual security precautions should be in theory enough to protect you, ultimately, don’t use a “smart” phone. reply euniceee3 16 hours agorootparentPhone numbers are not targets. Baseband is the big fear vector due to it being a black box, but in reality the apps themselves are being targeted where your phone number is the primary key. reply m348e912 19 hours agoparentprevSince the type of exploit pegasus has been using has been recently seen in the wild and Apple has had to release more than one security update to address this attack vector it leads me to believe that not just targetted individuals should enable \"lock down mode\" on their apple devices. Although apple doesn&#x27;t recommend it, it could be useful if there is a major malware outbreak across the iPhone ecosystem. reply dariosalvi78 3 hours agoprevThe implication of this is that the Isreali goverment is selling a weapon to Russia to attack people on European soil. Now I am waiting a reaction from the US and Europe.Oh, wait.. reply jackleeb 3 hours agoprevthere&#x27;s a DJ named Meduza and they have a song called Pegasus. lolhttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=d_1qExe2PWk reply abhinavk 2 hours agoparentIn Greek mythology, Pegasus was the son of Medusa. You will find a lot more references based on this. reply logN_2 11 hours agoprevDoes the NSO find these zero days themselves or do they buy them from the black market?If more money is thrown at the people finding the vulnerabilities so they responsively disclose would it make NSO’s business model unprofitable?Sounds to me like the best approach to shutdown the NSO group is to make the economics not work out. reply saagarjha 3 hours agoparentNSO has a lot of people in-house to find exploits. Pricing them out is likely to be quite difficult. reply vasco 3 hours agoparentprevThe best way is to arrest them all. reply keyme 1 hour agorootparentSure. That&#x27;ll make your precious iPhones more secure. reply Moldoteck 14 hours agoprevInteresting what is the zero-day % for ubuntu touch system that can tun on fairphones. Would using it reduce the chances of being hacked? reply jeejay 14 hours agoparentHacking of fairfones would be highly unlikely simply because it is not profitable to sell these hacks. reply tevon 16 hours agoprevDo we know if the phone was in lockdown mode? Anyone know how effective lockdown mode is in preventing most of these zero-days? reply fh9302 15 hours agoparentThe phone was not in lockdown mode as it would have prevented the attack.https:&#x2F;&#x2F;citizenlab.ca&#x2F;2023&#x2F;09&#x2F;blastpass-nso-group-iphone-zer...> We believe, and Apple’s Security Engineering and Architecture team has confirmed to us, that Lockdown Mode blocks this particular attack.https:&#x2F;&#x2F;citizenlab.ca&#x2F;2023&#x2F;04&#x2F;nso-groups-pegasus-spyware-ret...> For a brief period, targets that had enabled iOS 16’s Lockdown Mode feature received real-time warnings when PWNYOURHOME exploitation was attempted against their devices. Although NSO Group may have later devised a workaround for this real-time warning, we have not seen PWNYOURHOME successfully used against any devices on which Lockdown Mode is enabled. reply pvg 14 hours agorootparentThese are about different incidents though, right? Is there some other confirmation lockdown mode would have been effective in this case as well? reply fh9302 13 hours agorootparentIt&#x27;s the same vulnerability as in the article, PWNYOURHOME would have been avoided with lockdown mode.> Researchers believe Timchenko’s hackers used the so-called “PWNYOURHOME” vulnerability reply pvg 13 hours agorootparentAh I see, thanks! Managed to miss it while ⌘-F&#x27;ing through the article for &#x27;Citizen Lab&#x27;. reply1B05H1N 14 hours agoprevHow many people died over these 0-days? reply driverdan 12 hours agoparentWe won&#x27;t know until NSO is forced to open their books, which is unlikely to ever happen given their ties to the Israeli government. reply keyme 1 hour agoparentprevHow many lived? reply egonschiele 19 hours agoprevI read that if Pegasus is on your phone, even a factory reset will not get rid of it. Could someone explain why? reply runjake 18 hours agoparentI am not an expert, but my belief is that Pegasus does not maintain persistence.While the Wikipedia article claims Pegasus \"jailbreaks\" the iPhone to maintain persistence. Every technical article I&#x27;ve read says that a reboot clears Pegasus (albeit, it is easy to re-infect with a no-click exploit without the user&#x27;s knowledge).Hopefully, someone more knowledgeable can chime in with citations. reply saagarjha 3 hours agorootparentGenerally these attacks do not persist, as this is quite a bit more challenging. reply negus 19 hours agoparentprevHaven&#x27;t read about Pegasus, but what you describe is the behavior of bootkits. Factory reset does not imply that you erase 100% of your permanent storage: some part of it should contain the system programs to restore the system. If these system programs or the clean OS image are modified, then factory reset won&#x27;t help reply scintill76 18 hours agorootparentI don’t know about the original claim either way, but I would be even more impressed and scared if it survived an iTunes restore (basically a PC reflashes the iPhone’s OS image with an image downloaded from Apple.) reply negus 18 hours agorootparentIf the malware controls the bootloader nothing will help: it can imitate any kind of restore, modifying the OS image on the fly reply scintill76 16 hours agorootparentApple has firmware restore features in ROM. I would also assume (hope?) that there’s a procedure to enter the ROM-based restore that is impossible to intercept in software (maybe holding the power button for 10 seconds initiates a hardware reset into the ROM.) reply saagarjha 3 hours agorootparentThere is. reply heywhatupboys 16 hours agorootparentpreveverything is signed.should not be even remotely possible reply saagarjha 3 hours agorootparentAll code is signed on Apple’s platforms. Most exploits have a codesigning bypass of some sort. reply negus 14 hours agorootparentprevShould. But we are talking about software vulnerabilities here. It means that things do not work as intended. replysleepybrett 18 hours agoparentprevHere is a very technical breakdown of the malware: https:&#x2F;&#x2F;info.lookout.com&#x2F;rs&#x2F;051-ESQ-475&#x2F;images&#x2F;lookout-pegas... reply saagarjha 3 hours agorootparentNote that this is a very old analysis. reply zozbot234 16 hours agoparentprevIf you&#x27;re being targeted with anything like Pegasus (i.e. a state sponsored attack), you should definitely assume that even a factory reset will not fix the issue. It&#x27;s more about \"better safe than sorry\" than anything that can be said with certainty, since these attacks may evolve over time. reply levleontiev 17 hours agoprevAlso interesting that a new “bad guy” from the Caucasus might be the actual attacker. reply iandanforth 17 hours agoprevNSO getting blacklisted is one of the great victories over the \"Israel can do no wrong\" mindset so common in Washington. reply miohtama 18 hours agoprevNSO Group: We only work with legitimate governments for lawful purposes.Israel: NSO does not pose a problem, because they work only for lawful purposes. reply stef25 18 hours agoparentIt&#x27;s even better - \"journalists getting killed is horrible but it&#x27;s due to a lack of regulations. Someone has to do the dirty work\" - NSO. reply GoblinSlayer 18 hours agoparentprevAFAIK the wording is \"vetted customers\". reply rnk 15 hours agoprevApple should use financial means to destroy these companies. Working at these companies should be a black mark on the records of the employees. I won&#x27;t hire someone who worked at one of these companies. I know probably my own government tries to hack into people&#x27;s phones, I don&#x27;t want that either; my govt should not be selling their capabilities to other governments. If we make working at these companies something terrible on someone&#x27;s jobs record, we might prevent people from going there.Companies that do these kinds of things are a menace to society, because those tools get used for evil purposes (not just spying on terrorists). Plenty of other governments benefit from using these spy tools themselves, but we all know they fall into the hands of despotic governments like Saudi Arabia and they are used to harass and attempt to control journalists, people advocating against their governments.What I&#x27;d like to see is Apple uses their enormous influence and financial power to sue these companies and drive them out of business. They should financially attack the companies doing this and make it known they will work to destroy them. reply brokenodo 9 hours agoparentApple has sued them. So has Meta, and the Supreme Court recently declined an NSO appeal, which means the lawsuits can proceed.https:&#x2F;&#x2F;www.reuters.com&#x2F;legal&#x2F;us-supreme-court-lets-metas-wh... reply trogdor 8 hours agoparentprevApple sued NSO Group in 2021.See https:&#x2F;&#x2F;www.courtlistener.com&#x2F;docket&#x2F;61570971&#x2F;apple-inc-v-ns... reply Dah00n 13 hours agoparentprevSure. Same logic fits anyone working for anything Snowden revealed too. Previous work at USG&#x2F;NSO&#x2F;other places as bad? \"Sorry, we don&#x27;t see you as a good fit in our company\". reply jonfw 15 hours agoparentprevHow would apple suing the NSO work? They&#x27;re based out of Israel. I wouldn&#x27;t imagine Israeli courts are going to let an american megacorp take down one of their biggest industries reply Dah00n 13 hours agorootparentSuing across borders is not a problem at all. It is only an issue if you want to sue someone protected by the state. So, well, yes, in this case it world be allowed as much as if NSO tried the same to a US company. reply zozbot234 15 hours agoparentprev> not just spying on terroristsAh, but what about spying on \"Nazis\" and \"foreign influence organizations\"? What&#x27;s good for the goose is good for the gander. reply voldacar 15 hours agoparentprevIf I were apple I would seriously consider hiring hitmen or at the very least PIs to surveil everyone who works at these companies reply tonyarkles 13 hours agorootparentIf I understand correctly, NSO is primarily staffed with retired or current Mossad&#x2F;Israeli Sigint folks. Have fun! reply rnk 12 hours agorootparentprevNo, that&#x27;s not helpful. No one should suggest personal harm. reply vkou 4 hours agorootparentNo, they should just be treated like the criminals that they are. Subject to arrest and prosecution to the full extent of the law should their engineers, managers, executives, owners, and financial backers even think of setting a foot outside Israel. reply game_the0ry 16 hours agoprevLet&#x27;s assume I am a savvy career criminal (I am not...promise). What would I want to use for counter-surveillance? I would probably go:- For desktop - Tails OS booted from USB + TOR for browser- For mobile - GrapheneOS on latest pixel device reply zozbot234 16 hours agoparentThis will provide decent privacy for most people against casual mass-surveilance. But you should not assume that it&#x27;s anything like sufficient protection against these kinds of state-sponsored attacks. reply game_the0ry 16 hours agorootparentAssume nothing electronic&#x2F;digital is safe? That&#x27;s my take-away. reply Cyphase 12 hours agorootparentNothing non-electronic&#x2F;non-digital is safe either. They just have different tradeoffs. reply Un1corn 16 hours agoparentprevThis is absolutely not enough against targeted attacks. It will be harder to detect you but once they do, Firefox (which Tor is based on) is a lot more vulnerable than Chrome. Same for Android, the locked bootloader and such can be helpful in this situation. reply ReptileMan 3 hours agoparentprev2 phones. One is used only for hostspot. Second is used only in wifi mode. You take a sim card that you can obtain without id. Register signal, whatsapp, whatever with pin on the second phone. Use threema. Second phone should be rooted with very strict firewall. That should make someone sweat if they want to get in the endpoint. reply wordsarelies 15 hours agoprevHaas is still selling parts to Russia for their CNC mills even though they&#x27;re sanctioned. They do it by selling to a Chinese middleman.NSO Group probably uses an Indian intermediary (my first guess) and does the same thing. reply wewxjfq 14 hours agoparentWhat makes you think they care? They don&#x27;t sell their spyware to anyone who might use it against Russian officials, which tells you a lot. reply vonnik 19 hours agoprevDaily reader of Meduza here. They publish consistent and high quality coverage both about headline events in the war as well as odd ramifications of it in Russia and Ukraine. And it doesn’t have the annoying US-centrism of Ukraine coverage that you get elsewhere. reply ponkipo 18 hours agoparentWell, yes and no, even people who are strongly anti-Russian-regime-oriented told me that they stopped reading Meduza because it&#x27;s giving info which is extremely one sided and not objective, like it&#x27;s propaganda but opposite to a Russian-state one reply oytis 14 hours agorootparentThey might not be as anti-regime as they like to think. Apart from Meduza, I also read mainstream UK, US, German and Ukrainian media, and Meduza doesn&#x27;t seem to be more biased than either of those. Their predictions of regime&#x27;s difficulties seem to be exaggerated (compared to what seems to be happening in reality), but so are predictions of Western media. reply jononomo 18 hours agorootparentprevThe entire situation is extremely one sided -- I would be highly skeptical of any source that does not paint Russia and Putin in a terrible light. reply ipaddr 16 hours agorootparentMost people are like this for many issues on either side. If your media outlet isn&#x27;t pouring kool-aid over your personally held belief it&#x27;s viewed as suspect. Meanwhile your mind quickly discounts obvious contradictions to your held belief.Popular contradictions today: It&#x27;s a human right to dress and act like any sex one chooses. It&#x27;s evil and horrible to dress and act like a different race.Global warming is the biggest threat to mankind. Coming into the office is more important.Flying around the globe is to talk down to others who are doing more about global warming earns praise. reply The_Colonel 17 hours agorootparentprevIn terms of guilt sure, but you still want to read unbiased journalism about events etc. reply 2OEH8eoCRo0 16 hours agorootparentThere is no free press in Russia:https:&#x2F;&#x2F;www.pbs.org&#x2F;video&#x2F;putin-vs-the-press-aiw7f0&#x2F; reply The_Colonel 15 hours agorootparentMeduza is based in Riga, Latvia. reply 2OEH8eoCRo0 15 hours agorootparentMy point was it&#x27;s tough to get objective news from a Russian source. reply jononomo 15 hours agorootparentprevMy point is that unbiased journalism regarding the Ukraine war is going to look extremely one-sided. reply Dah00n 13 hours agorootparentHow so? Seems you are biased to one side and see everything not agreeing with this bias as one-sided. reply xyzelement 9 hours agorootparentprevHow would you&#x2F;we know what unbiased is? reply Dah00n 13 hours agorootparentprev> I would be highly skeptical of any source that does not paint Russia and Putin in a terrible light.So basically you picked a side and trust only what news agree with your beliefs? reply urinotherapist 4 hours agorootparentWhat wrong with that? We have moral compasses built-in. When someone says that murdering of people is OK, because Russia will be great again, should we throw out our moral beliefs and carefully listen to both sides? reply pphysch 17 hours agorootparentprevDepends on who you ask. Most of the world (i.e. outside the 15% of the population that is Western) views it as a nuanced situation with guilt on both sides. The hardliners are a global minority.e.g. while Russia is responsible for invading, Victoria Nuland was caught red-handed orchestrating the Ukrainian coup&#x2F;government that precipitated it.Even Israel has more moderate&#x2F;complicated views of it. On one hand they benefit from a strong West, on the other hand they possibly suffer from this particular proxy war (as it pulls Western resources & attention away from MENA into Europe). See Naftali Bennett&#x27;s \"tell-all\" several months ago. reply nabakin 16 hours agorootparent> while Russia is responsible for invading, Victoria Nuland was caught red-handed orchestrating the Ukrainian coup&#x2F;government that precipitated itFyi the leaked Nuland call (which I assume is what you&#x27;re referring to), is of her discussing who they should support after the massive protests started and Yanukovych and his ministers left the country. She did not \"orchestrate a coup\". At most, it&#x27;s the US trying to get Ukrainian parliament to pick the interim candidate they want which while is still manipulative, is far from \"orchestrating a coup\". reply amenhotep 9 hours agorootparentThe call was leaked on the 4th of February (and apparently recorded on the 28th of January), Yanukovych fled on the 21st. The correct context is that negotiations were ongoing between Yanukovych and the Maidan organisations, with him offering Yatsenyuk the prime minister job on the 25th of January as part of a potential deal to end the protests.It&#x27;s a very tired canard to call it a coup, of course, but it&#x27;s important to be accurate about these details. reply nabakin 8 hours agorootparentYou&#x27;re absolutely right. Thanks for the correction. I&#x27;d edit my comment but HN prevents that after a period of time. reply pphysch 10 hours agorootparentprevOkay, she clearly orchestrated the political half of the coup. I&#x27;m not suggesting she was commanding the Maidan snipers! reply peppermint_gum 2 hours agorootparentThat call happened around the time when the opposition, the government, the EU and Russia were negotiating the \"Agreement on settlement of political crisis in Ukraine\"[1].The gist of the agreement was that the opposition candidate would become prime minister and Yanukovych would remain president until the elections (which didn&#x27;t materialize because he fled the country).Note that there was no US at the table. Nuland and Pyatt weren&#x27;t \"orchestrating\" anything. In fact, they were frustrated that the US wasn&#x27;t participating in the talks, hence the \"fuck the EU\" in that call.All this leak shows is that US officials were trying to influence negotiations to which they weren&#x27;t invited.[1] - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Agreement_on_settlement_of_pol... reply nabakin 9 hours agorootparentprevGiving her support of a particular candidate is not \"orchestrating\" any coup. By that logic, the Democratic party giving support to Biden means the Democratic party enacted a coup on the US. Following the law of a country is not a coup.In order for there to be a coup, there has to be an unlawful seizure of power and that never happened. The president and his ministers left. I assume because the unrest of the people was so massive, he expected there would be a revolution. That would have been a coup, but even giving her support to a particular candidate the revolutionaries put forward, is not her causing a coup. Giving the revolutionaries weapons would be causing a coup. Sparking the revolution would be causing a coup. Getting one part of the government to overthrow the controlling part would be a coup. She did none of those things though (as far as we know).Facing the reality of having no president and having no line of succession remaining, the Ukrainian parliament selected an interim president for an interim government until a new, full president could be elected and that is what happened. reply timeon 12 hours agorootparentprev> with guilt on both sides.That is kind of crazy if you take into consideration that one side invaded the other. reply oytis 13 hours agorootparentprevMost of the world&#x27;s population also doesn&#x27;t live in liberal democracies with free press and believes all kinds of conspiracy theories. reply Dah00n 13 hours agorootparentMost of the world&#x27;s population that do live in liberal democracies with free press also believes all kinds of conspiracy theories. I doubt you could find a single trustworthy source that could prove any significant difference between the two. reply urinotherapist 4 hours agorootparentThere is a lot of actual conspiracies. Much more than conspiracy theories. The only known working process to uncover a conspiracy is to conduct an investigation and then prove the findings in a court. reply nabakin 9 hours agorootparentprevI&#x27;d say the US and Russian populations are just as susceptible to conspiracy theories and propaganda, but Russia&#x27;s propaganda is on another level and much more blatant. Their line seems to be further than the US&#x27;. reply vonnik 15 hours agorootparentprev> e.g. while Russia is responsible for invading, Victoria Nuland was caught red-handed orchestrating the Ukrainian coup&#x2F;government that precipitated it.The claim of \"orchestrating a coup\" is unsupported by evidence, and any both-sidesism does not do justice to the fact that:a) Ukraine has the right to elect whomever they want to govern their country, despite Russia&#x27;s preferences to create vassals of its neighbor statesb) Russia has twice invaded Ukraine (as well as other neighbors like Georgia) and thus directly caused hundreds of thousands of deaths on both sidesBetween Ukraine and Russia, only one of them is illegally occupying the territory of the other, only one of them is operating torture chambers in the territory of the other, and only one of them has kidnapped more than a million children from the territory of the other. There is no both sides between Russia and Ukraine in terms of guilt.https:&#x2F;&#x2F;www.hrw.org&#x2F;news&#x2F;2022&#x2F;04&#x2F;03&#x2F;ukraine-apparent-war-cri...https:&#x2F;&#x2F;www.ohchr.org&#x2F;en&#x2F;press-releases&#x2F;2023&#x2F;03&#x2F;war-crimes-i...Israel is doing a great deal to support Ukraine with humanitarian and non-lethal military aid (like helmets) because Iran is on the other side, although you are correct to note that the situation is complicated, largely because of Russia support for a bloody regime in Syria.https:&#x2F;&#x2F;kyivindependent.com&#x2F;on-support-for-ukraine-israel-pe...People focused on US actions during the Yanukovych years seem to believe that he himself was legitimate, when there is much evidence that he was corrupt, anti-democratic and supported by Russia:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Viktor_Yanukovychhttps:&#x2F;&#x2F;www.opendemocracy.net&#x2F;en&#x2F;odr&#x2F;yanukovych-luxury-resid...As for the 15% claim, I would add that a large part of that 15% supporting Ukraine includes countries that share a border with Russia or its vassals, including eastern EU and NATO states, as well as Japan and S. Korea. Those countries have the most skin in the game, and their position and actions in this conflict should be given much greater weight than the rest of the world. It&#x27;s not a coincidence that they want Russia&#x27;s wars of expansion to stop in Donbass.Ask Finland, Poland, Romania, or any of the Baltic states about who they want to win in Ukraine and you will get a very clear answer. Their populations have all been under the Kremlin&#x27;s yoke or fought a war against Moscow in living memory. reply dr_hooo 16 hours agorootparentprevCould you provide some information on the Victoria Neuland thing? reply archagon 16 hours agorootparentprevYou are just making up numbers. reply somenameforme 15 hours agorootparentThe anglosphere (US&#x2F;UK&#x2F;Australia&#x2F;New Zealand&#x2F;Canada) + EU is 470 million + 448 million respectively. That&#x27;s the entirety of the Western world, and less than 12% of the world&#x27;s population. One of JFK&#x27;s greatest speeches [1] hit on this point:\"We must face the fact that the United States is neither omnipotent nor omniscient that we are only six percent [4% now] of the world&#x27;s population, and that we cannot impose our will upon the other 94 percent of mankind that we cannot write every wrong or reverse each adversity and that therefore there cannot be an American solution to every world problem.\"The sort of wisdom and pragmatism completely absent from politicians since JFK.[1] - https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=vc0WrPGvWOM reply archagon 15 hours agorootparentJust because the government of a country considers it politically expedient to treat the situation as morally grey does not mean the population uniformly shares the same opinion. reply somenameforme 15 hours agorootparentVis a vis, just because the government of a country considers it politically expedient to treat the situation as the embodiment of Good vs Evil, does not mean the population uniformly shares the same opinion. In fact, I think this is the case nowhere in the world, including Russia and Ukraine. reply archagon 15 hours agorootparentRegardless, this...> Most of the world (i.e. outside the 15% of the population that is Western) views it as a nuanced situation with guilt on both sides....is a statement that cannot be supported by any known facts. It is a falsehood (if not an outright lie) used to bolster a tenuous argument. reply somenameforme 14 hours agorootparentWell I mean you can look at what polls do exist, and it&#x27;s not ambiguous. But I&#x27;d also appeal to a logical aspect here. Homogeneous dogmatic thinking, at scale, is not natural - and arguably doesn&#x27;t exist. Instead it&#x27;s primarily a product of propaganda and efforts to drive people to self-censor.Both of these are absolutely rampant in the West at the moment, but not so much in most of the rest of the world (at least not on this topic). People, left to their own devices, are generally pretty awesome. It&#x27;s only when you introduce self righteousness and propaganda that we turn into unthinking animals. It&#x27;s no coincidence that self righteousness and propaganda go hand in hand with war. reply archagon 14 hours agorootparentI&#x27;d argue the opposite. Some things, in the moment, are really quite morally obvious — and then propaganda starts doing its work to make them seem more ambiguous than they actually are.> Both of these are absolutely rampant in the West at the moment, but not so much in most of the rest of the world.You think propaganda-driven homogeneous dogmatic thinking doesn&#x27;t exist in China and India...?! reply somenameforme 14 hours agorootparentCan you offer any examples? In general, I think you&#x27;ll immediately run into a relativism problem. What is moral for one person is amoral for another. This is one of the main reasons I think it&#x27;s safe to say that dogmatic thinking at scale is so unnatural.As for my comment, I was obviously just referring to this topic. reply MichaelMoser123 19 hours agoparentprevi wonder how they manage to get funding, they are calling for donations, but i am not sure that incoming donations are enough to keep going.also they got completely outlawed by the Russian regime, so they can&#x27;t possibly get any advertising from Russian firms. reply postsantum 10 hours agorootparentIt&#x27;s almost as if the &#x27;sponsored by foreign agents&#x27; mark imposed by the Russian government was somewhat justifiable reply justsomehnguy 19 hours agoparentprev> and high qualityNope.There were enough articles with outright lies to never believe anything from them until proven by numerous other, independent sources. reply 5e92cb50239222b 18 hours agorootparentThis can be applied to literally anybody. I&#x27;ve been reading them for many years (since their editor and most of their journalists were at lenta.ru — which they were thrown out of in ~2015 for daring to criticize the annexation of Crimea). They are not angels, but they have always at least tried to remain impartial and use relatively reliable sources of information. Many (most?) news outlets don&#x27;t even try. reply thriftwy 18 hours agorootparentThis is the actual reason why people treat Meduza as parent poster does.The job of a news source it not to criticize, or not criticize, the annexation of Crimea. They&#x27;re not a political party. Nobody but their mom really wants to know their private opinion.The job of a news source is to provide news. All the news and articles Meduza produces follows the same pattern, where they would arrive at a predetermined conclusion regardless of the facts they are discussing, and the train of thought would go from A to B in a reasonably short route. If it&#x27;s hard to derive the conclusion from some facts, they will be skipping reporting these where possible. If it&#x27;s very convenient to derive the conclusion for unproven facts, they will be using these eagerly.Propaganda is annoying to read, especially if you know you will disagree with their conclusion, which you obviously know in advance. reply 5e92cb50239222b 18 hours agorootparentYes, and I should write bug-free code, and doctors should never make mistakes. If you have any examples of a completely neutral news outlet that never made any blunders, I&#x27;d be very interested to know about and follow them. Until then, I see no point in comparing anyone against an unattainable ideal which can only exist in one&#x27;s imagination. I try to correct for their biases by reading Kremlin propaganda (and US, and Chinese, and some others) and comparing what they are saying. Know of any better ways? reply GoblinSlayer 17 hours agorootparentThe amount of junk isn&#x27;t boolean, it matters how much you have to filter. If you can find news with less junk, you can filter them with less effort. And big news are reported by everyone so you can&#x27;t miss them. reply thriftwy 18 hours agorootparentprevIt is the century XXI, and the mainstream way seems to be subscribing to Telegram channels whose vibe resonates with you.Yes, you will be living in a tiny bubble. But at least you do not get to read propaganda pieces trying to derive prefabricated conclusions out of irrelevant small events. If anything large happens, you are going to hear of it earlier or later.If you really want balanced coverage, choose a source from the other side which is so blatantly propagandist that you can have good laughs instead of grinding your teeth. I am reading The Guardian for that purpose.Perhaps there are better ways to consume your news, but I don&#x27;t know these. reply MockObject 18 hours agorootparent>> I try to correct for their biases by reading Kremlin propaganda (and US, and Chinese, and some others) and comparing what they are saying.> Yes, you are living in a tiny bubble.How is that a tiny bubble? reply PawgerZ 13 hours agorootparentI think you misread their comment. If you didn&#x27;t realize, you also misquoted their comment (unless it was edited).> It is the century XXI, and the mainstream way seems to be subscribing to Telegram channels whose vibe resonates with you. Yes, you will be living in a tiny bubble.I believe they mean \"the mainstream way\" puts you into a tiny bubble, but they go on to say:> But at least you do not get to read propaganda pieces trying to derive prefabricated conclusions out of irrelevant small events. If anything large happens, you are going to hear of it earlier or later.Thus, I believe they were advocating for a tiny bubble -- not accusing the previous commenter of being in a tiny bubble. reply inopinatus 17 hours agorootparentprevThese sentiments are insidious: they are what repressive regimes want the populace to believe.-> restricting what journalists may write-> claiming the public has no interest in editorial opinion-> labeling dissent as propagandaThe remark above is all three. reply Dah00n 13 hours agorootparentprevNot a single news source in the history of mankind lives up to your description. reply OfSanguineFire 16 hours agorootparentprevIn many (most?) developed countries, major media sources like newspapers and TV channels are each aligned with a specific political party or a specific political wing. So, their reportage is done through that political lens, and people have historically bought that newspaper because they want issues reported through that lens. It is mainly in American fora where people have this belief that news sources should be neutral. reply Dah00n 13 hours agorootparent>It is mainly in American fora where people have this belief that news sources should be neutral.Which is kind of hilarious as US news sources are far less neutral than most of those politically colored newspapers! reply asveikau 18 hours agorootparentprevImagine that same statement but with another country in there, another country that is obviously an aggressor.\"It&#x27;s not their job to have an opinion on the Nazi annexation of the Sudetenland, since they are not a political party, nobody but their mother cares if they think it is wrong.\" reply justsomehnguy 14 hours agorootparent> another country that is obviously an aggressor.If you are not an American then it&#x27;s quite obvious who is an aggressor in many, many invasions through the 20th (and now even 21st) century.Care to imagine that same statement but with US? reply asveikau 11 hours agorootparentWhat I said has nothing to do with the US. Do you think that if the US is wrong in a bunch of unrelated matters, it makes Russia&#x27;s actions ok?But even given this... Would you say \"Nobody cares what you think about Iraq, except your mother, so don&#x27;t bother telling me about ...\" Yeah it doesn&#x27;t hold. reply thriftwy 10 hours agorootparentWould you be visiting an news site where every piece gravitates towards the many failings of USA in Iraq?I believe, at most, that you would read it for a week, then give it another week and chuckle, and then abandon it.Even if all of the articles are written in the good faith towards the facts and all. reply asveikau 10 hours agorootparent> Would you be visiting an news site where every piece gravitates towards the many failings of USA in Iraq?In the mid 2000s, I pretty much did this, and I don&#x27;t regret it. It would have been better for the world if the New York Times and Washington Post had leaned a bit more that way. reply thriftwy 10 hours agorootparentThere are Russians who do just that, and not an insignificant number, but certainly not enough to affect things. They also do not have any plan even if they wanted to try. replythriftwy 17 hours agorootparentprevNazi annexation of the Sudetenland objectively happened, and was not undone until the very end of Nazi regime.Not everybody wants to read how mr. Hanz from \"Der Jellyfisch\" thinks that that the annexation of Sudetenland is wrong, day after day for a decade. We&#x27;ve got that already from you being in Switzerland, mr. Hanz. reply inopinatus 16 hours agorootparentOn the contrary, it must be repeated, when an authoritarian regime conducting a murderous war of conquest of their neighbours promulgates their twisted justifications very loudly, and have entire state bodies devoted to manipulating the press, promoting a message that if left unopposed will become the prevailing narrative, as it has in their home nation.Head-in-the-sand bullshit neutrality is why Switzerland is a moral toilet. Demanding that journalists be \"neutral\" is a sliproad to manipulation. These are nothing more than an abandonment of principles.The public in functioning democracies is most definitely interested in reading opinionated editorial. Representing otherwise is downright obnoxious. reply Dah00n 12 hours agorootparent>when an authoritarian regimeWhy more so than when the US destroy Afghanistan or some other place? What makes it worse and more worthy of being repeated because of authoritarianism? reply inopinatus 6 hours agorootparentThe structural distinction is in domestic accountability to moral standards that limits the scope of action. This highlights the false equivalence (the US did not annex Afghanistan, nor has it attempted a genocide there) that imbues the whataboutism inherent to such questions.A better comparison would be to the imperialist colonialism of the the 17th-19th centuries, as characterised by massacre, dictatorship, annexation, rape, child abduction, and the systematic and intentional destruction of entire cultures. This corresponds much more directly to the multiple Russian invasions of its neighbours in recent decades. reply thriftwy 2 hours agorootparentWhat&#x27;s the difference with regards to annexing or not annexing?You storm into a country, you kill a lot of people, repeatedly, you do not care about these people in the slightest, you bomb weddings for a decade, you put up whatever government that you want there. But at least you did not annex the country, i.e. did not take responsibility for it and its citizens.It is also \"not a genocide\", say you.It&#x27;s a totally different thing. reply thriftwy 15 hours agorootparentprevPeople will stop listening real soon.You will keep the audience who already agree with you, and often bet on that agreement (for example, by fleeing the country). You will, however, lose the rest of your potential audience by repeating your opinion over and over again. Since they know your position, they do not share it, and they no longer need that information.Especially as you cannot answer any hard questions about your position, and you could not answer even if you didn&#x27;t. As a journalist, you cannot really suggest any solutions, since you are not a politician. You can only whine. That gets old pretty fast. reply inopinatus 13 hours agorootparentStraight from the authoritarian playbook:- promote the idea of a ruling class separate from the people- journalists that publish uncomfortable truths are \"whining\"- just give up because no-one is listeningThese are neo-Tsarist civics. As before, they form conditions for decay and conflict.In reality, people have never stopped listening, and never will. They may stop hearing - when voices are intentionally silenced. It follows that a critical and editorial press is the hallmark of democracy. reply thriftwy 12 hours agorootparentRussia is an authoritarian state. \"Hallmarks of democracy\" do not work here and likely never did.Meduza and their ilk publishes the same uncomfortable truth tailored at comparatively small demographics. They fail to deliver their message to a wider audience because they don&#x27;t understand it, have no message for it and perhaps don&#x27;t really want to talk to it. That&#x27;s what I was explaining. The only thing I&#x27;m seriously criticizing Meduza here is for their failure as journalists to get better coverage of their ideas. Part of which, their ideas aren&#x27;t great. replyponkipo 18 hours agorootparentprev\"90% of Meduza&#x27;s sources&#x27; predictions didn&#x27;t come true\". Source: https:&#x2F;&#x2F;www.proekt.media&#x2F;guide&#x2F;kremlin-telegram-meduza (it&#x27;s in Russian, info is at the end of the article). reply pvg 17 hours agorootparentThat doesn&#x27;t say much about the quality of Meduza&#x27;s journalism. Most Kremlinology ends up being wrong. reply throwaway290 19 hours agorootparentprevExamples of lies they published that were known lies before they published them? reply mc32 19 hours agorootparentprevCNN and Fox also publish proven lies or reframe things. Do we not believe anything they say as well? reply edgyquant 19 hours agorootparentI assume this is a joke? These are two news companies famous for people not believing anything they say. reply berdario 18 hours agorootparentI believe they are famous for not being believed by the other side&#x27;s partisans.I.e. US republicans most often won&#x27;t believe CNN and US democrats most often won&#x27;t believe Fox.The point here is, there are plenty of topics on which CNN and Fox coverage is very very similar: off the top of my head events in Israel and TaiwanAnd on those topics, plenty of people just \"trust the consensus\" (or what appears to be the consensus, in western media)https:&#x2F;&#x2F;www.peoplesworld.org&#x2F;article&#x2F;out-of-bounds-how-media... reply rhamzeh 16 hours agorootparentNon-Americans should not, and usually do not believe either. It&#x27;s funny when Republicans&#x2F;Democrats treat either as reputable.They&#x27;re politicking 101 made into 24&#x2F;7 news media panic.They&#x27;re both charlatans and peddlers of lies and cheap tricks; they engage in propaganda and employ journalists who seem to believe that they&#x27;re anything other than foot soldiers to stir up the masses against XYZ.Everyone knows Fox News is trash, it&#x27;s laughable when some continue to argue that CNN isn&#x27;t. Where XYZ can be anything, depending on which way the wind is blowing, sometimes it&#x27;s each other, sometimes it&#x27;s internal to the US, sometimes it&#x27;s external reply blackmesaind 18 hours agorootparentprevPointing the finger at another is often not a very compelling argument. reply mc32 18 hours agorootparentwhat I&#x27;m saying is that there are very few sources that don&#x27;t publish lies or have bents, so we have to do with what we have. Use many sources and triangulate. Some sources are more believable in some areas, less believable in other areas. Some contributors are more believable&#x2F;truthful than others. It&#x27;s not all on or off. reply hkpack 19 hours agorootparentprevOf course. Why you would read any source which was caught lying? reply baybal2 19 hours agoprev [19 more] [flagged] simpleuser27 19 hours agoparentWhen I read comments like this I always wonder what the purpose is - what exactly do you want a reader to come away with?Companies did bad things until they decided it was no longer to their advantage, and stopped?And if this is the case, what does it have to do with the article, or the blame owed to the actual, literal bad actor (Putin&#x27;s Russia)? reply notarget137 19 hours agorootparentWell, again as I stated previously - it is hypocritical and these companies and states should be held accountable. If someone feeds the soil for the next dictator to grow and then all of a sudden there is a political crisis involving said dictator aren&#x27;t you directly responsible for such crisis? reply hindsightbias 14 hours agorootparentAs long as you hold everyone who voted for Gerhard Schröder too. Not like these policies came out of a vacuum. reply mcpackieh 17 hours agorootparentprev> these companies and states should be held accountable.Hold the companies accountable... okay sure. I&#x27;ll write some letters to my elected officials and federal prosecutors about holding Apple and Google accountable. Just one thing... which laws were they breaking? Or do you propose consumer boycotts of both Google and Apple? If your plan is for everybody to give up their smartphones, your plan is DOA.Hold the states accountable... What does it mean to hold a sovereign state accountable? Are you going to bend the US Government itself over your knee and spank it? I don&#x27;t think so. What exactly do you mean by holding the state itself accountable? reply negus 19 hours agoparentprevCan you show some evidence? reply mschuster91 18 hours agorootparentThe non-reaction to the invasion of Crimea and Donbas in 2014, the non-reaction to breaking numerous \"red lines\" in Syria or our (=German) continued support for Nord Stream is evidence enough. reply ImPostingOnHN 18 hours agorootparentthere are ~ 190+ countries, each of which is guilty of this \"non-reaction\" you speak ofrussia, of course, is more guilty than all these other countries, because not only are they guilty of the same \"non-reaction\", but they are guilty of the initial action, too! reply mschuster91 15 hours agorootparent> there are ~ 190+ countries, each of which is guilty of this \"non-reaction\" you speak ofWhile I agree with you, most of the blame lies on us Europeans here. We knew what a continuation of this war and the constant erosion of basic rules of war would cause (most importantly, a ton of refugees), and yet we did nothing despite us being in a position to help from a military perspective in contrast to most Global South countries. We just let Assad and Russia bomb their own people with chemical weapons and barrel bombs.We stuck our heads into the desert sand and hoped the storm would pass, and then we had the audacity of letting tens of thousands of people drown in the Mediterranean or on the Turkey-Greece route. reply ImPostingOnHN 13 hours agorootparenteverything you say is true of all 190+ countries: they all knew what a continuation of russia&#x27;s genocide of Ukraine would cause, and yet each one did nothing despite being capable of sending at least minimal aid to Ukraine, or publicly voicing opposition to russia&#x27;s genocide of Ukraineso, again, each of those countries (russia alone more than any other) is equally guilty, be they China, USA, Iran, Canada, North Korea, etc: none had any responsibility to intervene more or less than the others, and EU receives no special blame for russia&#x27;s genocide of Ukraineor, more to the point, they are all equally innocent, except for the aggressor, russia, who started the genocide of Ukraine in the first placetl;dr russia is to blame replymcpackieh 18 hours agoparentprev> Western states were aiding and abetting Putin&#x27;s regime up until last years.> until last years.Very strange phrasing, that&#x27;s not idiomatic English. How many years? That should say something like \"until last year\" or \"until X years ago\" or \"until the last X years\".What is the value of X?I might presume that you mean the last year, e.g. 2022, but there are some problems with that. You&#x27;ve claims that western states were assisting Russia, and cited the supposed actions of two American companies. But the American state itself is not those companies, and has been arming and training Ukraine to fight Russia since at least 2014. reply notarget137 19 hours agoparentprev [–] They still do to some extent. Remember that gas heater you have has to have gas from somewhere. And that somewhere is Russia. If you consider recent rulings in baltic states blocking vehicles, phones, laptops and et cetera from entering that is the highest displays of hypocrisy. Oil is fine but people with phones are not. reply 5e92cb50239222b 18 hours agorootparentYeah, the last couple of years were really eye-opening for credulous idiots like myself.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kaja_Kallas#Stark_Logistics_an... reply edgyquant 19 hours agorootparentprevI don’t have a gas heater reply The_Colonel 17 hours agorootparentprev [–] Why is it a hypocrisy? replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Galina Timchenko, co-founder and publisher of Russian news outlet Meduza, has become the first Russian journalist reported to be infected with Pegasus spyware, which was developed by NSO Group.",
      "The Biden administration blacklisted NSO Group in 2021, barring it from accessing American technologies due to concerns around misuse and regulatory oversight.",
      "Meduza, Access Now, and Citizen Lab together investigated the incident, exposing widespread Pegasus misuse and reinforcing concerns on governmental surveillance and targeting of journalists, opponents, and activists."
    ],
    "commentSummary": [
      "The debate topics encompass Pegasus spyware utilization against journalists, vulnerabilities of various operating systems, state-sponsored hacking concerns, and difficulty in finding unbiased news sources.",
      "There's a discussion on the effectiveness of security measures and differing approaches to consuming news, alongside criticisms directed towards media outlets for bias and lack of accountability.",
      "References to geopolitical situations, including the role of Western states in the Ukrainian conflict, highlight the broader context and implications for the tech and information sectors."
    ],
    "points": 393,
    "commentCount": 240,
    "retryCount": 0,
    "time": 1694612357
  },
  {
    "id": 37501231,
    "title": "Don’t mess with a genius (2010)",
    "originLink": "https://shreevatsa.wordpress.com/2010/06/04/dont-mess-with-a-genius/",
    "originBody": "The Lumber Room \"Consign them to dust and damp by way of preserving them\" Don’t mess with a genius with 9 comments Or: What happens when Newton’s laws are violated Recently, I read a book called Newton and the Counterfeiter, subtitled The Unknown Detective Career of the World’s Greatest Scientist. It focuses on an awesome phase of Isaac Newton’s later career that, like his pursuits in alchemy, gets little mention in most accounts. The story, of Newton’s job as Warden of the Mint and his efforts bringing criminals to justice, contains many elements of a modern crime thriller: including an ingenious arch-adversary, Newton visiting the gin houses of London in disguise, personally interrogating suspects, playing good cop–bad cop, and using every trick in the book, before the book had been written. The story begins, as many of them do, at the beginning. The beginning Isaac Newton, 55 years old and just recovered from his nervous breakdown, was looking for a post in the city (London), having lived in the village of Cambridge ever since his student days. As a Great Man now, he had already been rewarded with a seat in parliament (the only thing ever recorded spoken by him is a request to close the window), but it appeared harder to get him a job. Finally, his friends pulled the right strings, and Newton moved in as Warden of the Mint in 1696. The job was meant to be a sinecure (he had been promised that the position “has not too much business to require more attendance than you may spare”), and no one expected him to do anything special. Today though, we can look back and confidently say that Newton is the greatest Warden the Mint ever had. Unfortunately, this is not saying much, because it appears Newton is also the only good Warden the Mint ever had. The Mint The Royal Mint at the time had two officials in charge, both appointed by the king and with no well-defined hierarchy between them. The Warden of the Mint, with a salary of 400-odd pounds a year, was in charge of the Mint’s facilities, and the Master of the Mint, with a salary of 600 pounds a year plus (more substantial) a percentage of every coin made, was in charge of the actual production of new coins. When Newton moved in as Warden, the Master was the notoriously corrupt and incompetent Thomas Neale, who was so lost in his gambling habit and his numerous enterprises that the operations of the Mint were, well, not in mint condition. This was a bad time, because counterfeiting, clipping, and arbitrage had weakened the economy to the extent that there was a shortage of cash everywhere, most tax payments and trade had stopped, panic was rising, and civil war was imminent. Counterfeiters Counterfeiting was easy money, and everyone took to it. The government declared it high treason, a hanging offence, but this only made juries more reluctant to hang their peers. Of those counterfeiters who were brought to trial, many escaped conviction, even one of them through a wonderful incompetence defence: [I]nept counterfeiters attempting to exploit the currency crisis supplied the Old Bailey with a constant diet of rapidly dispatched defendants. Perhaps the most spectacular display of incompetence came from an unnamed “inhabitant of the parish of St. Andrews Holbourn,” brought to trial accused of copying French coins. His work was astonishingly awful, and he was acquitted, the jury accepting his rather bold argument that the poor quality of his work confirmed that “he had tryed to Coin with Pewter as aforesaid for Diversion, or the like, but never was concerned in Coining any manner of Money.” Few others tried this defense. The Great Recoinage: Newton takes over Faced with no alternative, the government had decided that the Mint would recoin everything — the Great Recoinage was to take place. It aimed to melt and restrike, in three years, more coins than it had produced in three decades. How anyone expected it to happen with Neale in charge of it is a mystery. As the recoinage began, it quickly turned into a farce, and it was clear to everyone that it would be an impossible task. Newton saw what was happening, and couldn’t stand it. He read up on the history of the Mint, studied its operations, studied Neale, accumulated all the knowledge necessary, and somehow intimidated and pushed Neale aside in a bloodless coup, and took over the Great Recoinage himself. He streamlined the production, conducting probably one of the earliest “time-and-motion studies” (he synchronized workers’ operations to the rate of their heartbeat), running the mint from 4am to midnight, and finished the “clearly impossible task” ahead of schedule. Warden duty Newton's unusual coat of arms After saving the country from economic ruin, by doing something that wasn’t even his own job, Newton finally turned to a duty that his post actually came with — protecting the currency, by “enforcing the King’s law in and around London for all crimes committed against the currency”. This meant doing a policeman’s work — or rather, that of “a criminal investigator, interrogator, and prosecutor rolled into one”. He found the idea distasteful, not to mention the kind of men he would have to come in contact with, and requested that this be assigned to someone else, but when his request was denied he turned to the task in all seriousness. Despite having hardly been a man of the world until then, he very quickly figured out what he had to do, better than anyone else had done. (In the mere four years he was Warden, he got dozens of counterfeiters hanged.) He descended into the underworld … hiring men to go undercover, interrogating suspects, planting informers in prisons, the works. To avoid issues of jurisdiction he got himself appointed Justice of the Peace for nineteen counties surrounding London. Most criminals (one is almost tempted to say victims) were entirely unprepared against this kind of systematic prosecution, “utterly unprepared to do battle with the most disciplined mind in Europe”. Advertisement Except one. Arch-nemesis William Chaloner, counterfeiter, confidence trickster, and various things besides, the ingenious man who would one day challenge Sir Isaac Newton, had started small. He had set out from home — or had been thrown out — as a youth to apprentice under a nail-maker, where he learnt the basics of counterfeiting instead. Arriving in London with its oppressively exclusionary guild system, he somehow managed to survive, going through a series of professions including being a quack doctor, progressing to fortune teller, and then becoming a locator of stolen goods, at which he succeeded through the infallible trick of being the one to have stolen them in the first place. [One of his most heinous sources of money was the following. Jacobite sedition—supporting the return of the deposed King James, over the reigning William of Orange—was treason and punishable with death, and there was a reward for those who gave up seditioners to the king. Chaloner tricked various printers into printing Jacobite propaganda, then used that as evidence to turn them in to be hanged, and claim his reward.] Finally he turned to his true calling, that of counterfeiting. After coining a great deal of money (he once claimed to have produced more than thirty thousand pounds in his life, about four million pounds in today’s money) and getting caught a couple of times — once escaping conviction by turning informer, and the other time by coming up, along with his co-accused, with such a delightfully tangled mess of accusations and cross-accusations that everyone was let go out of confusion — he began to look for more safer avenues. He realised that for a man of his skill, making good counterfeit coins wasn’t the problem; having it untraceable back to him was. In an audacious plan, he realised that the safest place from which to pass his money was the Mint itself, and resolved to get into it. He printed a couple of pamphlets giving advice to the government on how to prevent counterfeiting — here his expertise was all too evident — and even once gave a speech in parliament. Newton ignored him at first and denied him entry even to look at the machines in the mint, until Chaloner lost patience and decided to attack the man himself. (He alleged that the mint was making side money by participating in counterfeiting itself. The worst part was, some of these accusations were true: some dies had disappeared from the mint. Newton was put on trial and forced to defend himself, and nearly lost his job.) Big mistake. Newton was finally annoyed, and made it his goal to destroy him. Over the next two years, he devoted much of his life to ruining Chaloner’s. With customary ruthlessness, he set about accumulating evidence and witnesses. By now Chaloner was in custody again — bank notes and a Malt Lottery had just come into existence, and of course he counterfeited them — so he was out of the way. Newton got spies and informers planted in all the right places, he tracked down old contacts of Chaloner — friends, female coiners he’d had affairs with, wives of former associates — and subpoenaed (or just intimidated) them into giving testimony, anticipated who would try to flee to Scotland when, and prepared an impenetrable web of evidence. It is more complicated than that, and Chaloner still did his best from behind bars and the whole cat-and-mouse game has more details than I have any remaining patience to go into now :-), but you can read about them in the book. Chaloner was brought to trial. He tried every defence in succession, from pleading innocence to madness to pointing out (validly) that he was being tried by a Middlesex jury for crimes committed in London. He was convicted nevertheless, and after Newton ignored all the piteous mercy petitions he wrote, was hanged, drawn and quartered. Newton’s later years In 1699 the worthless Neale finally died, and Newton became Master of the Mint on Christmas Day, his 57th birthday. The responsibilities of the job had already been de facto handled by Newton for years, but Neale had gained all the proceeds from the coining — 22,000 pounds. Newton now became the only recorded Warden to become Master. Although the Great Recoinage was over, the Mint still was in production, and Newton made 3500 the first year. He finally gave up his Cambridge professorship which he had still retained, went on to become genuinely rich for the first time, and seems to have led a contented life. Much later he lost 20000 pounds in the South Sea Bubble, the world’s first stock market crash — Newton is attributed to have said: “I can calculate the movement of the stars, but not the madness of men”. He was knighted in 1705, the first scientist to be knighted (though possibly for political reasons rather than either his science or Mint work), and died in 1727, aged 84. Despite being one of the greatest and most influential scientists of all time, he wrote: I don’t know what I may seem to the world, but as to myself, I seem to have been only like a boy playing on the sea shore, and diverting myself in now and then finding a smoother pebble or a prettier shell than ordinary, whilst the great ocean of truth lay all undiscovered around me. His memorial at Westminster Abbey bears was proposed to bear the inscription: “If you doubt that such a man could exist, this monument bears witness”. More details: Thomas Levenson, Newton and the counterfeiter: the unknown detective career of the world’s greatest scientist, 2009. (Full disclosure: Levenson works at MIT) Wikipedia article on William Chaloner Others I haven’t seen or read: Talk by Thomas Levenson, author of the book (Running Time: 1:03:30) Book review in The Guardian Book review in The Telegraph Book review in Powell’s books Another book review in The Guardian Story in NPR radio [23 min 23 sec] Post by Levenson at Executed Today Long review/book abridgement! at Chicago Boyz Also: The author has a blog Sponsored Content Signs Of Heart Failure That May Surprise You Search AdsSponsored Here's How To Fly Business Class For The Price of Economy Business Class FlightsSearch AdsSponsored [Pics] Wolf Roams The Hospital In Search Of Fearless Nurse To Help Yeah Motor!Sponsored Man Puts Ultimate Stop To Neighbor's Using His Pool, Was He Justified? Yeah Motor!Sponsored Most Affordable Camper Vans Camper VansSearch AdsSponsored Unsold Lincoln SUVs Are Almost Being Given Away (See Prices) Auto Savings Center|Search AdsSponsored Can You Handle the Beauty? These Trucks Will Blow Your Mind Yeah Motor!Sponsored [Photos] Why Opie's Mom Was Never Mentioned On The Andy Griffith Show CulturessSponsored Forget The Blue Pill, Use This Household Food To Fight ED urologytip.proSponsored Share this: TwitterFacebook Loading... Related Mathematics and notation: the Hindu-Arabic numeral system Sun, 2008-12-14 In \"history\" “Every good theorem must have a good counterexample” Tue, 2009-03-10 In \"algebraic geometry\" Euler, pirates, and the discovery of America Mon, 2010-03-01 In \"history\" Written by S Fri, 2010-06-04 at 10:25:19 Posted in history Tagged with counterfeiting, history, isaac newton, william chaloner « New Number Nine Unwilling translators » 9 Responses Subscribe to comments with RSS. Epic post! “If you doubt that such a man could exist, this monument bears witness”. Wow. pratish Fri, 2010-06-04 at 10:35:28 Reply Thanks :) I ran out of patience to do it properly, and just posted what I had; glad to know it’s still readable… though of course it’s just that of an Epic Man, even an insignificant part of his life is to us an Epic Story. [Edit: For completeness: the actual inscription is “Nam hominem eum fuisse, si dubites, hocce testatur mormor”, and the translation Levenson gives is “If you doubt there was such a man, this monument bears witness”.] S Fri, 2010-06-04 at 10:58:14 Reply This was a fantastic read. The kooky (and otherwise unsuccessful) parts of the lives of accomplished people almost always gets no attention. (Feynman’s failed experiments on friction, Arthur Conan Doyle’s promotion of fairies!) Although the above account only embellishes Newton’s larger than life persona. karthik Fri, 2010-06-04 at 15:55:07 Reply Thanks! The book is even more cool; I think I embellished less. :-) It also has fascinating details on the first days of paper money, the (un)fairness of the judicial system, and the shockingly unbearably terrible conditions of prisons then… imagine the horrors described by Dickens, and multiply by ten. It is strange that the creator of Sherlock Holmes should have been so taken in, but I guess what with the death in the family and all that…. For Newton’s “kooky” alchemy, see (I think) Isaac Newton: The Last Sorcerer which goes into more detail (presumably) on his wingless dragons and infernal fires. But even there, Newton was far more systematic and thorough than his fellow alchemists! S Sat, 2010-06-05 at 02:54:58 Reply Awesome! Mohan Sun, 2010-06-06 at 02:56:39 Reply Thanks :) Glad you liked it, and glad I wrote down this before returning the book. The book is even more awesome; it goes into some interesting details I glossed over. S Sun, 2010-06-06 at 03:47:25 Reply […] Newton was finally annoyed, and made it his goal to destroy him. Over the next two years, he devoted much of his life to ruining Chaloner’s. With customary ruthlessness, he set about accumulating evidence and witnesses. By now Chaloner was in custody again — bank notes and a Malt Lottery had just come into existence, and of course he counterfeited them — so he was out of the way. Newton got spies and informers planted in all the right places, he tracked down old contacts of Chaloner — friends, female coiners he’d had affairs with, wives of former associates — and subpoenaed (or just intimidated) them into giving testimony, anticipated who would try to flee to Scotland when, and prepared an impenetrable web of evidence. It is more complicated than that, and Chaloner still did his best from behind bars and the whole cat-and-mouse game has more details than I have any remaining patience to go into now :-), but you can read about them in the book. Chaloner was brought to trial. He tried every defence in succession, from pleading innocence to madness to pointing out (validly) that he was being tried by a Middlesex jury for crimes committed in London. He was convicted nevertheless, and after Newton ignored all the piteous mercy petitions he wrote, was hanged, drawn and quartered. Read More:https://shreevatsa.wordpress.com/2010/06/04/dont-mess-with-a-genius/ […] coat of arms and and arcane yarnsMadame Pickwick Art Blog Wed, 2011-08-03 at 20:23:40 Reply […] Read More […] Don’t mess with a genius (2010) – TOP Show HN Wed, 2023-09-13 at 17:30:44 Reply […] この記事はHackerNewsに掲載された下記の記事を元に作成されています。Don’t mess with a genius (2010) […] 天才をバカにするな (2010) – 世界の話題を日本語でザックリ素早く確認！ Wed, 2023-09-13 at 19:24:54 Reply Leave a Reply This site uses Akismet to reduce spam. Learn how your comment data is processed. Search for: Pages About The Lumber Room Archives December 2016 (1) July 2016 (1) March 2016 (3) January 2016 (1) September 2015 (2) August 2015 (1) June 2015 (2) April 2015 (1) February 2015 (2) January 2015 (3) August 2014 (1) July 2014 (1) June 2014 (3) May 2014 (1) March 2014 (2) February 2014 (1) January 2014 (1) November 2013 (1) October 2013 (1) September 2013 (1) August 2013 (1) April 2013 (4) March 2013 (1) January 2013 (1) October 2012 (1) September 2012 (1) August 2012 (3) April 2012 (1) March 2012 (1) January 2012 (1) December 2011 (1) November 2011 (1) October 2011 (1) August 2011 (1) June 2011 (3) May 2011 (6) April 2011 (2) March 2011 (2) January 2011 (4) December 2010 (1) November 2010 (3) October 2010 (1) September 2010 (4) August 2010 (1) July 2010 (3) June 2010 (5) May 2010 (5) April 2010 (4) March 2010 (6) February 2010 (4) December 2009 (2) September 2009 (3) August 2009 (6) July 2009 (1) June 2009 (1) May 2009 (4) March 2009 (5) January 2009 (2) December 2008 (3) November 2008 (6) October 2008 (4) September 2008 (3) August 2008 (4) July 2008 (6) May 2008 (5) April 2008 (5) March 2008 (8) February 2008 (6) January 2008 (13) December 2007 (26) November 2007 (16) October 2007 (23) September 2007 (8) August 2007 (9) July 2007 (11) June 2007 (3) May 2007 (5) April 2007 (10) March 2007 (19) February 2007 (13) January 2007 (21) December 2006 (6) November 2006 (13) October 2006 (11) September 2006 (8) July 2006 (1) June 2006 (1) May 2006 (1) April 2006 (3) March 2006 (1) February 2006 (1) January 2006 (1) December 2005 (3) November 2005 (3) June 2010 S M T W T F S1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30« May Jul » americanism bollywood bookmarks clever comics commandline compknow computers copyright criticism CS culture debian design education emacs entertainment euler filmsIsaw firefox fixme funny gmail google history humour Improvement India internet javascript knuth language language log latex life literature lsc mathematics maths microsoft movies multimedia music names organisation parody personal poetry politics presentation procrastination programming psychology python quote quotes rant sad sanskrit sanskrit literature sanskrit translation science society software technology ted text thisblog toread translation trivia ubuntu usability video writing Meta Register Log in Entries feed Comments feed WordPress.com Blog Stats 1,104,508 hits Follow Blog via Email Enter your email address to follow this blog and receive notifications of new posts by email. Email Address: Follow Join 191 other subscribers Create a free website or blog at WordPress.com. Follow Privacy & Cookies: This site uses cookies. By continuing to use this website, you agree to their use. To find out more, including how to control cookies, see here: Cookie Policy",
    "commentLink": "https://news.ycombinator.com/item?id=37501231",
    "commentBody": "Don’t mess with a genius (2010)Hacker NewspastloginDon’t mess with a genius (2010) (shreevatsa.wordpress.com) 345 points by wglb 13 hours ago| hidepastfavorite87 comments dang 6 hours agoRelated ongoing thread:The Greatest Counterfeiter (2021) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37490644 - Sept 2023 (26 comments) - (via https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37499206)Normally we downweight follow-ups but this is too good. Even the parentheticals are good (\"the only thing ever recorded spoken by him [in Parliament] is a request to close the window)\". reply thedailymail 7 hours agoprevAnother fascinating thing about the Newton story is its parallels to Archimedes and the \"eureka\" moment. Both stories involve the most famous physicist of the day being asked to protect the integrity of royal gold. (Archimedes was asked by the king of Syracuse to detect whether a goldsmith had been adding baser metals to an offertory gold crown.) And both involve famous scientists bending the rules of evidence and procedure to catch a thief – in Archimedes&#x27; case, the modern consensus is that simple water displacement would not be sufficient to distinguish pure gold from a gold&#x2F;silver alloy. reply lifeisstillgood 3 hours agoparentWait what? Are you saying that the story (Archimedes invents theory of density, proves the crown is fake) is really Archimedes invents density, but then proves the fakery by ... finding the spare gold in the guys kitchen or something?I&#x27;m flabbergasted. I can of course believe it&#x27;s true. I just never thought to question it. reply bryanrasmussen 3 hours agorootparentI figured it was Archimedes invents theory of density, then he or someone else invents anecdote to show its use, anecdote actually wouldn&#x27;t work though but theory of density still good so people use it. reply WalterBright 3 hours agoparentprevI always wondered how he measured the displacement accurately enough. reply roflmaostc 37 minutes agorootparentYou simply should use a vessel which gets narrower to the top. If you put the crown, the water level rises. The smaller the cross section of the vessel the more the height has to increase to amount the volume. reply aifer4 5 hours agoprev> I don’t know what I may seem to the world, but as to myself, I seem to have been only like a boy playing on the sea shore, and diverting myself in now and then finding a smoother pebble or a prettier shell than ordinary, whilst the great ocean of truth lay all undiscovered around me.What a great quote reply munro 9 minutes agoprev> the only thing ever recorded spoken by him is a request to close the windowDoes anyone know where I can find this? reply vishnugupta 2 hours agoprevWhile appreciating his genius let&#x27;s also remember that even Newton himself couldn&#x27;t resist FOMO and lost substantial money when a bubble burst [1]. Goes to show he was a human after all.[1] https:&#x2F;&#x2F;pubs.aip.org&#x2F;physicstoday&#x2F;article&#x2F;73&#x2F;7&#x2F;30&#x2F;800801&#x2F;Isa... reply langsoul-com 41 minutes agoparentThere was a linked quote on that. \"I can predict the movement of stars, but not the madness of men\"Truly a great quote and still so valid today. reply withinboredom 2 hours agoparentprevProbably thought he could time the market. Everyone makes that mistake... reply RugnirViking 2 hours agorootparentits worse than that. He pulled his money out because he (correctly) thought it would end badly, making a large profit.But it just kept growing and growing. He eventually bought back in, and that was right at the peak before the crash, so he lost a lot. reply cornholio 1 hour agorootparentThis is almost like how it went for me, early Bitcoin \"investor\". Still managed to make some speculative profit though, take that, Newton! reply LeoPanthera 2 hours agoparentprevThis is mentioned in the linked story. reply acyou 6 hours agoprevDoes this cause anyone else to be more suspicious of Newton&#x27;s achievements and the history books, given that he at one point had accrued enough power to have his rivals drawn and quartered?Now, I have less respect for Newton than ever before. That essay repeats the details of the smear job against Newton&#x27;s rival in earnest, and parrots his praises, with zero sense of irony or context with respect to how history is formed. Reminds me strongly of Thomas Edison, prominent scientist with suspiciously high political standing and personal fortune.No one can contest this now, the histories are 300 years old. What no one can contest in good faith is that, \"The winners write the history books\". reply svat 5 hours agoparentI don&#x27;t think this is a reasonable understanding of the story. Chaloner cannot be called “Newton&#x27;s rival” in any usual sense of the word: he was born destitute, turned to various horrifying crimes from a young age, was barely educated, was arrested multiple times even before his story overlapped with Newton&#x27;s in any way, etc. He died only because his crime was a serious one, falling under the category of “high treason”. And for his part, Newton hardly had accrued enough power despite all his mathematical and scientific achievements — this job was obtained with great effort late in his life, and his (small) fortune came from his job&#x27;s salary.Regarding “context with respect to how history is formed”, I think this is best done by reading more primary sources, and not forming a quick opinion (as you seem to be doing, from a blog post). This is what the author of the book has done. From his article at http:&#x2F;&#x2F;www.executedtoday.com&#x2F;2009&#x2F;03&#x2F;22&#x2F;1699-william-chalone... —> Some historians, notably Frank Manuel, have speculated that Newton pursued this work with implausible eagerness, out of a kind of frustrated blood lust born of his abandoned and unhappy childhood. This seems to me to be nonsense. The specific historical context matters here: Newton did not author the bloody code, nor did he send everyone he could to the gallows. Rather, the record of his depositions shows him to be simply a relentless practical man doing his job. He used little fish to catch big fish, and at least some of those low on the ladder received their escape from the gibbet. What you can see here, surprisingly, is the birth of a modern idea of a civil service. The Warden -– even Isaac Newton — was simply a man in a job doing the functions of that job, which included organizing the investigation and prosecution of counterfeiters.> […] Newton did not expect as Warden to have to chase crooks; when he found out that was part of the job he wrote a rather whiny letter to the Treasury to see if he could wriggle out of the duty. When he found he could not, he responded as he always had to the job at hand. reply jdthedisciple 1 hour agoparentprevIf this story makes you lose respect for Newton rather than gain some,then I&#x27;m sorry but I&#x27;m not sure your respect means a ton.Newton was 55 already at this point, and it is but a tiny excerpt from his life. reply atomicnature 5 hours agoparentprevIf you study the details of his youth, etc, you will learn that he lived in obscurity for many, many years, toiling alone on the most difficult questions of the time.His \"political standing\" was a result purely of his scientific achievements and a general consensus of his high ability within scientific circles. He took great pains to reduce the number of acquaintances he had rather than increase them.I think Newton was more right than wrong in the beliefs he took seriously (even outside the narrow confines of \"science\"). He was anything but naive. This doesn&#x27;t mean he never lied; it just means that he lied way less compared to average standards for humanity. reply bawolff 28 minutes agoparentprevWhat a weird criticism. This story, well amusing, is a funny footnote at best. It could be 100% a fabrication and it wouldn&#x27;t really affect newton&#x27;s story at all.That&#x27;s not to say i disagree about history being written by the winners and all the potential pitfalls that entails. This story just isn&#x27;t relavent to what newton is known for. reply KaiserPro 55 minutes agoparentprevOne should always be suspicious of achievements in history books!Newton could be an unmitigated shit. He has feuds with lots of people, including Hooke, Leibniz & Flamsteed. Not to mention that he rounded up the poor and convicted them of counterfeiting for possession of coins, rather than making them.he also had a heretical secret life, properly into alchemy and other (for the time) wildly fantastical beliefs. Ones I suspect, but cant prove, he would have exploited should they have belonged to his rivals.Mark Steele covers him here: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=2CQenuI_zsw (its the radio version) reply varjag 1 hour agoparentprevThat \"rival\" got a whole bunch of innocent people hanged. reply ackbar03 4 hours agoprevThis is almost like a real-life version of Sherlock Holmes and maybe a slightly discount version James Moriarty reply thisisauserid 11 hours agoprevNewton&#x27;s obsession with counterfeiters never made sense to me until I read Neal Stephenson Baroque Cycle. He makes a good case for tying it with Newton&#x27;s prior alemetical pursuits. reply emmelaich 11 hours agoparentI don&#x27;t think it was an obsession. It was personal with Chaloner, because Chaloner called Newton a fraud.Newton was dutiful, honest and smart. reply fuzzybear3965 11 hours agoparentprevWhat does \"alemetical\" mean? reply mdp2021 2 hours agorootparent> alemeticalI&#x27;d say, \"disparaging against the Germans\" or \"repulsed by confederations\" (&#x27;Alemanni&#x27; ∩ &#x27;emetic&#x27;).If somebody is proficient in Arabic, maybe &#x27;emetic&#x27; (after article \"Al-\") makes sense also outside IE... reply acumenical 41 minutes agorootparentUnderrated comment reply mdp2021 32 minutes agorootparentName checks, Acumenical ;) :D reply jszymborski 11 hours agorootparentprevOP possibly meant alchemical? Can&#x27;t seem to find anything for almetical. reply stronglikedan 9 hours agorootparentDamn autocorrect! Also, I was amused that the parent&#x27;s question is Google&#x27;s current second result. reply benreesman 10 hours agoprevI have a dim notion that Sir Isaac also had quite the feud with Leibniz, is that also a wildly entertaining story like this one? And if so what’s the best telling? reply psychoslave 5 hours agoparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Leibniz%E2%80%93Newton_calculu...https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Leibniz%E2%80%93Clarke_corresp... reply benreesman 3 hours agorootparentAwesome thank you for the links.As an American English-speaker I think my ostensible tribal obligation is to call it a tie at worst and take Sir Isaac’s part by default, but Wikipedia seems to be going out of its way to avoid concluding that Leibniz got there first: I’m curious if Germans have a different view than us in the Empire or it’s former colonies.It seems a tough case to make that Newton wasn’t the more influential thinker overall, but on Calc I-IV? Lot of claims about stuff that he “decided not to publish”. reply psychoslave 2 hours agorootparentLeibniz certainly has a profile closer to that of Thomas Young: a polymath with a name far less known to general public. reply 3abiton 1 hour agorootparentprevGreat references! reply unsupp0rted 1 hour agoprev> To avoid issues of jurisdiction he got himself appointed Justice of the Peace for nineteen counties surrounding London. reply n4r9 12 hours agoprevI&#x27;ve heard it said that Newton was one of very few (maybe the only?) Wardens to not pardon a single counterfeiter and stay their execution. reply zgiber 1 hour agoprevThanks for the great write up! I’d watch this as a series or movie reply Izkata 11 hours agoprevSounds like the persona of Sherlock Holmes from the original stories, just with a (slightly) different focus. reply galaxyLogic 9 hours agoparentExactly maybe but I don&#x27;t know Conan Doyle knew about this reply PlunderBunny 12 hours agoprevIf this was the pitch for a movie, it would be turned down on account of being too unbelievable. reply koryk 12 hours agoparentNeal Stephenson already wrote a few books about it reply hinkley 11 hours agorootparentWhen I first read Quicksilver I assumed that Newton as Warden of the Coin was one of the speculative fiction parts of the story. Nope, he was indeed involved heavily in improving the state of the art in authentic money.I&#x27;m surprised he didn&#x27;t get more negative attention from counterfeiters. But then once you get one hanged, and drawn and quartered, maybe they realize this cat has claws and they should leave it alone. reply svachalek 12 hours agoparentprevI love the last line, “If you doubt that such a man could exist, this monument bears witness” reply williamdclt 2 hours agoparentprevAfter « Abraham Lincoln: vampire hunter », the new thriller « Detective Newton: Warden of the Mint » reply calcsam 12 hours agoparentprevPaging Christopher Nolan. reply geophile 9 hours agorootparentOh hell no, give the story to a director who can actually create a movie with good dialog and character development. reply pmontra 2 hours agorootparentGenuine question: is that up to the director or to the authors of the script? reply peoplefromibiza 1 hour agorootparentNolan is a screenwriter too, he usually writes and directs his own movies, often times with the help of his wife Emma Thomas. reply calcsam 12 hours agorootparentprevAnd John Madden (Shakespeare in Love). reply zem 9 hours agoprevhe also invented milled edges, which arguably did even more to preserve the coinage: https:&#x2F;&#x2F;www.perthmint.com&#x2F;news&#x2F;collector&#x2F;coin-collecting&#x2F;how... reply hooloovoo_zoo 8 hours agoparentIt’s an idea that goes back to at least the Romans, but Newton attracts attribution like Mark Twain. (Referring to Roman serrate-edge coins) reply dang 6 hours agorootparent> attracts attribution like Mark TwainWell put! reply hirundo 10 hours agoprev> Newton saw what was happening ... and somehow intimidated and pushed Neale aside in a bloodless coup, and took over the Great Recoinage himself.Or more charitably, Neale didn&#x27;t fight too hard for his prerogatives when he saw that one of the most competent men in the history of western civilization was trying to solve his biggest problem. reply thsksbd 10 hours agoparentGood management gets out of the way of its star worker. reply taberiand 7 hours agorootparentEspecially when getting in their way gets you hung drawn and quartered reply sirspacey 9 hours agorootparentprevAmen reply AceJohnny2 9 hours agoparentprev> when he saw that one of the most competent men in the history of western civilizationThe thing, this is only decidable in hindsight. Many a confident person has declared themselves to be \"the most competent\" and proven to be... not that.(competence & confidence are perhaps entirely uncorrelated) reply ChrisMarshallNY 12 hours agoprev(2010) -But a great story. reply emmelaich 11 hours agoparent2009.Levenson&#x27;s book publish date.https:&#x2F;&#x2F;www.amazon.com&#x2F;exec&#x2F;obidos&#x2F;ASIN&#x2F;0151012784&#x2F;exectoda-... reply gumby 12 hours agoparentprevI think you mean (1696)... reply Wowfunhappy 11 hours agorootparentWell, no, the article was not written in 1969. The year of events described is irrelevant. reply alasdair_ 7 hours agorootparent>Well, no, the article was not written in 1969.I think you mean (1696)... reply Wowfunhappy 42 minutes agorootparentOops, yes. replyJun8 11 hours agoprevSo, if someone with Newton’s caliber gets suckered into losing a lot of money in a market bubble (https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;South_Sea_Company, note that it was a slave trading company), what hope is there for the rest of us? reply autoexec 11 hours agoparentIt&#x27;s like those optical illusions that you can&#x27;t stop seeing even through you know your eyes are lying to you. Our brains are just wired to leave us vulnerable to certain cognitive biases and errors and while we can try to increase our awareness of those vulnerabilities we can&#x27;t get rid of them.To make it worse, scammers and corporations spend massive amounts of time&#x2F;money researching how to best exploit those weaknesses and so the moment our guard is down any one of us could fall for one of their traps. We&#x27;re all basically one bad day away from being suckered into losing money on something. Nobody can be hypervigilant all the time and a person like Newton probably had a lot on his mind. reply emmelaich 10 hours agoparentprevIt is notable that Newton had no experience of trading companies.He was good at things he dealt with personally.Nullius in verba and all that. reply darkerside 10 hours agoparentprevMaybe just because you&#x27;re smart about some things doesn&#x27;t mean you know everything? reply kurthr 11 hours agoparentprevWell, he was also a lifelong celibate. reply dustypotato 2 hours agorootparentAnd? reply tgv 1 hour agorootparentI suppose you&#x27;re to substitute that.> So, if someone with Newton’s caliber can&#x27;t get laid, what hope is there for the rest of us?Which is obviously false. reply bitwize 7 hours agoparentprevI&#x27;m reminded of Michael Larson, the guy who successfully hacked Press Your Luck, racked up over $110,000 while on the show, and then lost it all in bad real estate deals: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Michael_LarsonIf you find yourself in a lot of money, your best bet is a diversified portfolio of stable, slow-earning assets. Newton was brilliant, but not immune to the tantalizing excitement of \"striking it rich\" with a big investment. You might say... he pressed his luck. reply emmelaich 11 hours agoprevThis is mostly a copy of Levenson&#x27;s 2009 article, submitted a day ago.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37493372https:&#x2F;&#x2F;www.executedtoday.com&#x2F;2009&#x2F;03&#x2F;22&#x2F;1699-william-chalon...I found this interesting:> Newton did not expect as Warden to have to chase crooks; when he found out that was part of the job he wrote a rather whiny letter to the Treasury to see if he could wriggle out of the duty. reply svat 10 hours agoparentThis is not a copy of Levenson&#x27;s 2009 article. This is a copy of Levenson&#x27;s 2009 book (the good parts) (as mentioned in the first sentence), that I wrote as a blog post in 2010 before returning the book to the library, and which someone else submitted to HN today probably after I mentioned it in a comment on another post about another counterfeiter. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37490644The facts are the same (researched by Levenson), but the words and presentation&#x2F;storytelling are different, surely? You can see that Levenson and I chose somewhat different parts of his book to emphasize. I wouldn&#x27;t have been surprised even if we had chosen the same parts, because obviously I think the ones I chose were the good parts :-) reply pvg 9 hours agorootparentthe words and presentation&#x2F;storytelling are different, surely?I spent a few minutes comparing the two articles because I&#x27;m a doofus and hadn&#x27;t noticed the author of the post (you) had replied. But now that I&#x27;ve done this pointless bit of work, can confirm - the topics are similar but they focus on different facts and even time periods. One is very much not a copy of the other. reply emmelaich 10 hours agorootparentprevOK, apologies. I guess I was a little peeved that Levenson&#x27;s own article didn&#x27;t get many upvotes. (Though I wasn&#x27;t the submitter) reply svat 5 hours agorootparentNo problem :) Mysterious are the ways of upvotes. It&#x27;s not entirely random—I would expect a post from ciechanow.ski to be reliably upvoted—but there&#x27;s a fair amount of chance (which is why there&#x27;s a manually curated \"second-chance pool\"). I don&#x27;t know how Levenson&#x27;s article didn&#x27;t get more attention.It is a stunning coincidence that it was posted just yesterday though; as far as I can tell there&#x27;s no connection between that article being posted (given that it got hardly any attention) and “The Greatest Counterfeiter” being posted (which led to my comment). reply hammock 8 hours agoprev>The Warden of the Mint, with a salary of 400-odd pounds a year$91,000 a year reply mdp2021 12 hours agoprev [–] > I can calculate the movement of the stars, but not the madness of menMany will find this relatable. reply abraae 11 hours agoparentI particularly enjoyed his modest self-eulogy:> I don’t know what I may seem to the world, but as to myself, I seem to have been only like a boy playing on the sea shore, and diverting myself in now and then finding a smoother pebble or a prettier shell than ordinary, whilst the great ocean of truth lay all undiscovered around me. reply darkerside 10 hours agorootparentI wonder what Newton would think of what we&#x27;ve built reply survirtual 9 hours agorootparent\"You&#x27;ve split atoms, reached the moon, and yet the very force that keeps your feet on the ground remains a riddle? Perhaps society&#x27;s attention is too often pulled away from the fundamental questions by trivial distractions.\" reply darkerside 8 hours agorootparentI actually could see it being the opposite. Perhaps he&#x27;d be gratified and exultant to see that we took all his silly theoretical musings about calculus and gravity and created real world impact. reply survirtual 8 hours agorootparent\"My humble scribblings have led to technologies that reach the stars and devices that fit in the palm of your hand? How gratifying to know that you&#x27;ve taken the seeds of my theories and grown a veritable forest of innovation! Well done, future humans, well done indeed!\" reply ekianjo 7 hours agorootparentprevPhysics had no major discovery for a while. Its stagnating with huge gaps in theory reply owlstuffing 9 hours agorootparentprevOr, worse, what we have destroyed reply hammock 8 hours agoparentprev [–] Except self described economists of course replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The text highlights Isaac Newton's tenure as the Warden of the Mint in England, where he actively pursued counterfeiters.",
      "It emphasizes his successful endeavors in catching many counterfeiters, with a particular emphasis on his arch-nemesis, William Chaloner.",
      "Newton's significant contribution to the Great Recoinage signifies his remarkable contribution to maintaining currency integrity."
    ],
    "commentSummary": [
      "The Hacker News discussion thread provides varied perspectives on Isaac Newton's life, encompassing his conflicts, association with alchemy, crusades against counterfeiters, and his contributions to calculus.",
      "Participants in the thread debate Newton's individual character and scientific contributions, with some exhibiting skepticism and others defending his reputation.",
      "The article also uncovers the human aspect of Newton, highlighting our cognitive biases that can lead even highly intelligent people to fall prey to scams and manipulation."
    ],
    "points": 344,
    "commentCount": 87,
    "retryCount": 0,
    "time": 1694636490
  },
  {
    "id": 37498142,
    "title": "A DIY near-IR spectrometer",
    "originLink": "https://caoyuan.scripts.mit.edu/ir_spec.html",
    "originBody": "Yuan Cao Ph. D. HOME ABOUT RESEARCH EDUCATION PUBLICATION GALLERY BLOG CONTACT © Copyright ©2023 All rights reservedThis template is made with by Colorlib PROJECT A $500 DIY near-IR spectrometer that would sell for $10,000 DIYing spectrometers is not new. You can make one with your phone camera, and a broken piece from a CD-ROM as a diffraction grating. There are plenty of tutorials for this online. It takes $5 to make one. However, a silicon based camera (CMOS sensor) only respond up to ~1100 nm in wavelength, and this is a physical limitation —— silicon has a band gap of ~1.1 eV, and you cannot excite electron-hole pairs with wavelength longer than about ~1100 nm. So we need a different semiconductor if we want to measure any light above this wavelength. A popular choice is InGaAs, whose bandgap is tunable down to ~0.4 eV by changing content of indium versus gallium. But you probably don't want to know how much a InGaAs camera will cost you... While a silicon-based camera is as cheap as dirt these days, an one-dimensional InGaAs pixel array already costs upper few thousand dollars. Any full-blown IR spectrometer system goes way over $10k, with their fancy thermoelectric cooling and precision gratings (we actually have one in our lab). The reason why they are so expensive is that the target user group are scientific researchers, not consumers. Since I have been recently interested in laser optics (as a hobby) and wish to DIY some laser systems, which inevitably requires working with near-IR wavelengths (above 1100 nm), I desperately needs a way to analyze what light I'm producing out of my laser crystals. One day I tried to search for InGaAs photodiodes on DigiKey and it turns out that a single InGaAs photodiode sells for ~20 bucks. While its 100x more expensive than a silicon photodiode, I figured that you can make a spectrometer just with this one photodiode, and it's definitely within reach of DIY. And here it is —— a fiber-coupled IR spectrometer that measures from 800~1600 nm. Contents 1 Principle and Design 2 Make 3 Performance Design A spectrometer mainly consists of four components: slit, diffracting element, detector, and relaying optics between these parts. In our case, the input optical fiber acts as our input slit. I chose to use a 50 um core multi-mode fiber for a compromise between light throughput and spectral resolution. A 2-m long SMA-905 fiber patch cable can be purchased on AliExpress for about $40, or $70 from Thorlabs. The diffracting element is typically a grating. It is important to choose a grating with the right line density to obtain reasonable dispersion (wavelength per degree of angle) at the designed order of diffraction (1st-order is most commonly used). For this project since the design wavelength is 800 nm ~ 1600nm, a line density of 600 line per mm is about right, and this gives a dispersion of 40° over the designed wavelength range. See below for a plot of angle-of-diffraction for 50° incident angle onto a 600 line/mm grating. The 2nd and 3rd orders are clearly not usable as they overlap with the incident beam. Next comes the detector. How are we gonna detect light from a range of angles with only a single 'pixel' of photodiode? We mount the photodiode on a motor and scan it across! To do this, I purchased a stepper-driven linear stage from Amazon for $50. This little stage is quite well-built and allows a resolution-per-step of 5 um, more than enough for our resolution. The InGaAs photodiode mentioned above has an active area of ϕ1mm, while the image of the 50 um input slit is gonna be a bit smaller than that. So an output slit also needs to be mounted as close as possible to the photodiode, in order to make sure that the size of the photodiode does not compromise the spectral resolution. This does not need to be of great precision —— I simply used some aluminum masking tape to make a slit of ~0.3 mm wide. Lastly, we need optics to connect all these components together. It turns out that this part is quite expensive and tricky if you want high spectral resolution as well as high light throughput (which ultimately determines the signal/noise ratio). Basically, starting from the input slit (which can be viewed as a point source), we need to (1) defocus it into a parallel beam, (2) direct it onto the grating, and (3) focus the diffracted beam (which is approximately parallel for each color) onto the sensor. Traditionally, the standard way is an all-mirror configuration known as the Czerny-Turner design. The reason to use mirrors instead of lenses is to minimize chromatic aberrations, so that all wavelengths can be focus onto the same focal plane. However, the alignment of these parabolic mirrors are quite difficult without a real optics bench, so I decided to pursue a different path that is much more friendly for DIYing. The first step —— defocusing fiber output into parallel beam, can be achieved using a fiber collimator. This is basically a lens pre-aligned with its back focal point right at the interface of the optical fiber core. SMA-905 fiber collimator (picture from Thorlabs) One end of this device is already threaded with standard SMA-905 connector, so that the fiber patch cable can be directly connected to it. The other end is free-space output of parallel beam. Very easy to use! However, the lens used in the collimator does have a chromatic aberration, so that only the beam at the designed wavelength (980 nm in my case) is truly collimated. Shorter wavelengths converge a little bit and longer wavelengths diverge a little bit. Fortunately, the level of dispersion appears to be tolerable, and it can be further corrected by optimizing the location of the detector. The fiber collimator is not super cheap —— it sells for $160 on Thorlabs. While there are other options, they are either even more pricy, or appears much less well-built than the Thorlabs one. I was lucky to find a pair being sold on eBay for $70 each, so I can use one on each end of the fiber. Next, directing the beam onto the diffraction grating is easy, as you just need a good silver-coated mirror. Again, I found one (12.7mm x 12.7mm) on eBay for $25, while a new one sells for $35 on Thorlabs. I assume that any decent mirror you can find will probably work, but don't use a dielectric mirror if it's designed for visible, as it's probably gonna be out-of-band for near-IR wavelengths that we need. The last piece of optics is a lens/mirror to re-focus the diffracted beam onto the detector. Our beam is a ϕ2mm circular beam for each color. To simplify things a bit again, I chose to use a cylindrical lens to focus the light in the plane of diffraction. There are mainly two benifits of this: (1) the focused beam is a line of ~0.5 mm wide (beam waist) and ~2 mm tall (size of the original circular beam). This makes the alignment of the diffraction plane and the detector relatively unimportant (tolerance ~1 mm). (2) The rectangular shape of the cylindrical lens is easier to mount with than a circular lens. You just need a piece of double-sided tape! The optical layout is shown in the figure below, together with a raytracing of the light path using Optometrika. Left: optical configuration. Right: simulated intensity on the detector The placement of the cylindrical lens (position & angle) affects the focal point for different wavelength. I did not do a rigorous calculation here —— I simply resorted to a trial-and-error method to figure out the optimal placement. The tolerance appears to be quite large, probably because of its relatively long focal length (150 mm) compared to the beam size. The theoretical resolution (without considering optical aberrations) can be estimated based on the parameters of the optics used here. The magnification of the optical train is 150 mm (cyl. lens) / 11 mm (fiber collimator) = 13.6 times. There are three major sources of broadening: Input slit: The 50 um fiber core creates an image of 0.05 × 13.6 = 0.68 mm spot at the screen. This corresponds to about 6 nm broadening in wavelength. Grating: The finite beam diameter at the grating limits the spectral resolution. Resolving power of a grating (using 1st order) is equal to the number of lines being illuminated. A beam spot of 2mm has a resolving power of 1,200, so this contributes to a broadening of ~1 nm in wavelength. Output slit: the slit at the photodiode further broadens the spectral response by about 3 nm. You can see that the major source of broadening comes from the input slit size (fiber core size). To improve upon this without compromising light throughput, one would use a fiber collimator with larger lens/mirror and longer focal length (which is also more expensive) so that the magnification of the system is reduced. Since the input 'slit' is in fact circular, the broadening is less severe than 6 nm. My spectrometer eventually achieved a measured FWHM of 3~4 nm @ 1000 nm, and 5~6 nm @ 1500 nm, which is more than adequate for me. In fact, this resolution is pretty darn good considering the simplicity. For reference, an InGaAs spectrometer sold by Edmund Optics using the same SMA-905 fiber input claims a resolution of 4 nm, and a compact spectrometer by Ocean Insight has a nominal FWHM of 10 nm. FYI, they sell for ~$12,000 and ~$8,000 respectively. Make The core of the spectrometer is a diffraction grating, which diffracts the light towards different angles according to its wavelength. Instead of using part of a CD-ROM, here I used a legitimate blazed reflective grating from Thorlabs, which sells for $70. The parameters of the grating is: 600 line/mm, 12.7mm x 12.7mm x 6mm, blazed for 1000 nm. As you can see from the picture, for small square optical components, I made a small \"optical mount\" to clamp down the optics parts firmly without damaging them. The electronics part of the detector mainly consists of a transimpedance amplifier and a 24-bit analog-digital converter (ADC). The core is a AD8656 operational amplifier with an ultralow input bias current of 1 pA and extremely low noise. This allows to use a huge gain resistor R of 100 megaohm, which converts each pA to 100 uV! The photodiode is operated with zero bias voltage, or in the so called 'photovoltaic' mode, to eliminate dark current. After that, the signal is fed into AD7793, a precision low-noise ADC with digital filtering. The output data rate is programmable, with the slowest speed (4.17 Hz) giving the lowest noise of 40 nV, which is negligible compared to other sources of noise. One big issue for this circuit is the power supply, which is provided by the 5V USB power. USB power is notorious for its large fluctuations and ripples from the computer switching power supply. To circumvent this, I used the best possible linear regulator on the market —— LT3042 from Analog Devices, which boasts extremely high Power Supply Rejection Ratio (PSRR) of over 100 dB. This chip turns out to be working magically well for dealing with dirty power. I find that the circuit is extremely vulnerable towards capacitive coupling with any surrounding conductor with voltages, most notably myself that emanates tons of 60 Hz noise... This is mostly likely because of the high impedance node at the input of the opamp (tens of megaohm). Electrostatic shielding (NOT the typical EMI shielding) is absolutely crucial for this. I wrapped the circuit board and photodiode all around with grounded aluminum tape/foil/plate, but a better way would be to use dedicated photodiode sockets connected to coaxial cable that is fed into a PCB with double ground planes. With all external sources screened out, the remaining intrinsic noise comes from three places: (1) the photodiode, (2) the amplifier, and (3) the feedback resistor R. At the frequencies (1~100 Hz) of the measurement, the amplifier noise is mainly 1/f noise, which is hard to average out. According to the datasheet, the total noise from 0.1~10 Hz is about 0.3 uVrms, which is equivalent to 3 fA at the input. The resistor R and photodiode both contribute through thermal noise, which contributes 13 fA/√Hz and 9 fA/√Hz (assuming a shunt resistance of 200 MΩ) respectively at room temperature. If we measure at the slowest speed of 4.17 Hz, the noise can be estimated by RMS sum of these values, which equals 30 fA. By comparison, the plot on the left shows that the standard deviation of the signal when there is no light is about 18 fA, meaning that the circuit is essentially limited by thermal noise of the system. Further improvement will require putting the sensor and amplifier into liquid nitrogen! Below are some pictures of other parts of the build. This pictures shows all components in the IR spectrometer. Power supply (5V 2A) is attached to the left wall, controller board is top, and detector board is fixed to the linear stepper motor (Amazon). This is the controller board. Microcontroller is a STM32 Nucleo-32 board, and the green one is stepper motor driver. This is the detector board, which has the low-noise regulator, A-D converter, and transimpedance amplifier as described above. Note the shielding tape around the amplifier. Imporant for low noise! Actual photodiode is in the small piece of board seen below the stepper motor. In this picture, there is an aluminum block before the photodiode, which hosts the slit made of razor blades. Later on I switched to simply putting two pieces of black tape directly onto the front window of the photodiode, because it seems like if the slit is too far from the photodiode, light throughput is significantly reduced at large incident angles (i.e. near the end of the spectral range). Close-up of the fiber coupler, fixed to the base of the box by a clamp. Performance To test the performance of the spectrometer, we need a light source with known wavelength and linewidth. For a rough alignment I use my 505 nm green laser pen, which should get refracted to the 2nd order at exactly the same angle as 1010 nm light would be refracted to the 1st order. For conversion from the position of the detector to wavelength, however, a calibration source is needed. Typically, a mercury lamp or argon-neon calibrating lamp is used for this purpose, because these elements have a few bright and sharp emission lines in the visible to near-IR range. But these dedicated lamps are quite expensive (Newport sells them for ~$300). Another possibility is to use gas spectrum tubes that are used in undergrad physics labs. But these tubes require a special high-voltage power supply, which takes extra effort to acquire. Later I figured out that my desk lamp is actually a perfect calibration light source. Be aware that it has to be a fluorescent type light bulb —— either LED bulb or incandescent bulb won't work here. The reason is that fluorescent bulbs are filled with mercury vapor plus some inert gas, which get excited by the electricity to emit light mostly in the ultraviolet. These ultraviolet photons are then converted to visible light by phosphor salts coated in the inner surface of the tube. However, ultraviolet is not all that mercury emits. Neutral mercury in fact has a couple of decently strong emission lines at 436, 543, 546, 1014, 1357, 1367, and 1530 nm as well. The last 4 lines are quite useful in calibrating our near-IR spectrometer. Furthermore, my desk lamp seems to be also filled with argon gas. Argon has quite some emission lines in the near-IR range, especially in the 800~1000 nm range. The procedure of calibration is very simple. We take a spectrum (intensity versus distance), identify the lines, and find the correspondance between position and wavelength for at these lines. Then an interpolation smoothly connects between these key points, so that any position can be converted to wavelength or vice versa. The following figure shows an example of calibrated spectrum of my desk lamp, annotated with their emission lines. Pretty good, isn't it? I tried to align the cylindrical lens so that the 1014 nm peak is sharpest (FWHM ~3 nm, see left), while somewhat compromising the resolution at long wavelengths (FWHM 5~6 nm at 1500 nm). Overall, I'm quite satisfied with this resolution considering the investment. The next step is to calibrate the spectral response function, that is the amplitude of the signal per unit input flux of light at different wavelengths. This calibration can be carried out less stringently, as an error of 10% in absolute signal scale usually does not create much problem (for reflection/transmission measurements, it is the change of signal that matters anyway). I used a $10 quartz halogen lamp (a type of incandescent bulb) bought at local hardware store as a black-body source (assumed to be 3200 K) to calibrate the spectral response. Let's now use it to measure something more interesting! In fact, I bought a separate fiber illuminator on eBay for $70 for measuring transmission spectrum of filters and crystals. I also built a mini test bench with translation stage for the ease of aligning the beam and the crystal: The aperture and lens creates a (white) beam with divergence of ~2° and beam diameter adjustable from ~0.5 mm to 1 cm. On the other end of the test bench, the same fiber collimator collects the collimated light into the optics fiber to be sent to the spectrometer. A 790 nm long-pass filter is mounted right before the collimator —— this is to prevent 2nd order diffraction of visible light from interfering with near-IR measurements. With this bench, we can measure the transmission of any flat sample, that is things without any optical power. Measurement of optical elements like lens are more tricky because the shape of the beam changes when they are inserted, resulting in inaccurate measurements. Here is the transmission curve of a Nd-doped YAG crystal (2×2×10 mm, 1% doped), which is extensively used in solid-state lasers. In this curve, there is a background that comes from reflection on the interfaces (which I believe is anti-reflection coated for 1310 nm), and sharp peaks that comes from absorption in the Nd:YAG crystal. Light at 808 nm, the wavelength that is most frequently used for diode-pumping of these crystals, is almost perfectly absorbed by this crystal. I happen to have 808 nm pump diodes at hand, so we can also do a fluorescence spectrum. The diode is driven with a moderate current, which produces about ~ 100 mW of 808 nm output. Here I plotted the intensity in log scale to flush out all features. While 1064 nm is of course the strongest peak, all other major emission lines at 946 nm, 1116 nm, 1319 nm, 1338 nm, 1357 nm, 1414 nm, 1431 nm, and 1444 nm are also quite obvious. From this spectrum, we can approximately determine the dynamic range of our spectrometer. In this setting (gain 16, time constant 1/8.33 Hz), the maximum signal measurable is 800 pA, while the noise floor is about 0.2 pA. So we have a dynamic range of 4,000:1, or 72 dB. This can be further improved to 100,000:1 if we use a larger time constant and smaller gain. This is pretty much it for now. I'll put more spectra here if I take more cool ones in the future. I hope this article is useful for you if you also want to build a IR spectrometer without wanting to spend $10,000!",
    "commentLink": "https://news.ycombinator.com/item?id=37498142",
    "commentBody": "A DIY near-IR spectrometerHacker NewspastloginA DIY near-IR spectrometer (scripts.mit.edu) 342 points by johnmaguire 18 hours ago| hidepastfavorite67 comments gaze 18 hours agoThis write-up and project is from the same guy that discovered superconductivity in bilayer twisted graphene. Pretty impressive. reply nielsole 18 hours agoparentI don&#x27;t know how uncommon that is, but more than 14k citations at age 25, most of which as first author sounds pretty darn impressive. reply darkclouds 17 hours agorootparentNot knocking his efforts, but I guess its on a par to writing some code or making something which is used by many people. reply barelyauser 16 hours agorootparentI once went through quite an experience. Used to work with a guy that was awarded a big prize in his field. I think one day he got tired of being treated as \"some kind of genius\", as he said. He started to discuss a topic one day, listening to my answers and questions back and forth. At the end the discussion hit a point where we could not go further. He then told me: \"if you then submitted this as a proposal, were lucky enough to have it granted and just answered the last question you asked me with a simple experiment, you would receive the same prize as I did\".That guy was great. reply heyoni 12 hours agorootparentAnd a genius lol. Don’t tell him I said that reply zeagle 16 hours agorootparentprevProbably underestimates it. Not my field, I don&#x27;t know of this guy but a few hundred citations is you write a library that everyone uses and similar academic recognition and academic platinum for tenure on the background of other publications. 300 000 is the equivalent of writing the Linux kernel or sqlite.https:&#x2F;&#x2F;www.nature.com&#x2F;news&#x2F;the-top-100-papers-1.16224 reply gaze 16 hours agorootparentprevIt really isn&#x27;t -- unless you&#x27;re talking something like bzip2 or some extremely nontrivial thing. reply ruined 7 hours agorootparentprevi&#x27;ve written code used by at least a billion people over fifteen years. i&#x27;m nobody and it was nothing. so definitely not on par reply Thrymr 9 hours agorootparentprevThat really is knocking his efforts. How is that constructive? reply etrautmann 16 hours agorootparentprevRight - that’s impressive reply SaulJLH 12 hours agoparentprevAside from pure curiosity and or research purposes... Is there any everyday&#x2F;practical apps&#x2F;uses for something like this, for the avg joe? Building something for 500 that would normally be 10k, already has me intruiged. reply HerculePoirot 10 hours agorootparentI brought this up in a comment in another thread. I found this page because I wanted to see whether it was feasible to monitor nutrient concentration in an aeroponic solution. Typically the solution circulates between the tank and the root box, so the concentration in nutriments will decrease gradually. Near-IR isn&#x27;t enough to figure out these details I believe (I&#x27;m not a scientist), but it is enough to get a fingerprint of the nutrition \"equilibrium\". These solutions are probably the most technical aspect of aeroponics, so I wanted to use commercial solutions bundled as three bottles you&#x27;re supposed to mix in various proportions. I think that if you explore the concentrations space using such a spectrometer you may be able to train a model to interpolate the concentrations from spectrographs measured in \"production\".Anyway I think this is best to address the shortcomings of circulating the solution, for instance use a tiny tank, circulate for a few hours&#x2F;days, refill, dosing as you go. A spectrometer would be still be cool in order to get real-time concentration diffenrential for each solution component. Identifying individual chemical species could be accomplished I think using far-IR spectroscopy with a model trained on data measured using more expansive spectrometry techniques.I haven&#x27;t grown anything yet, I want to build a sap flow sensor first.https:&#x2F;&#x2F;edaphic.com.au&#x2F;products&#x2F;sap-flow-sensors&#x2F;https:&#x2F;&#x2F;dynamax.com&#x2F;products&#x2F;transpiration-sap-flow&#x2F;dynagage... reply DoctorOetker 5 hours agorootparentthis sounds like a relatively cheap project.* any microcontroller with multiple ADC channels, for example ESP32 if you want it to be wireless as well...* a heat generator (can be as simple as a transistor)* a few thermistors, which you will have to calibrate (pretty easy if you have access to a thermometer, an electric kettle and ice, basically dunk the thermistor in a glass, put the thermometer in the glass, have the microcontroller output measurement number, and measurement value over serial port, now heat water to boiling point, make a table with a first column pre-entered with the thermometer gradation, then pour it in the glass, each time it passes a gradation you look at the scrolling list of values and note it in the second column of your table. Now you have a monotonically decreasing list of measurements, some of them marked as a thermometer gradation passing. Make a preadsheet and use the value of the fixed resistor in the voltage divider [the other resistor was your thermistor] together with the standard NTC or PTC formulas, then fit the free variables of the formula to the data... An acquaintance of me was starting to brew beer had all the components, but didn&#x27;t know how to calibrate it, so I helped him out. Doesn&#x27;t take long if you set your mind to it and properly prepare.)Perhaps the difficulty is in the mechanical fabrication such that you thermally isolate from the phloem (which you puncture) and selectively measure temperature in the xylem?Perhaps standard thermistors are too large. so an alternative would be a tiny diode, whose IV curve is temperature dependent. Added benefit is lower thermal mass, although I was surprised at the response time of the thermistor once calibrated (and filtered to result in step transitions when putting in and out of hot water and icy water).EDIT: just making sure, what I&#x27;m trying to say is, don&#x27;t be intimidated, and break up the task in smaller tasks, go ask for some help on ##electronics or so when you have sketched an initial plan, and listen to their feedback. reply keithnz 8 hours agorootparentprevI used to work doing software for fruit sorting machines and we had a NIR Spectrometer to work out the sweetness of fruit, so when I&#x27;d go to actual sorting sites, especially in California which have these machines on a massive scale, I&#x27;d use it to sort out the biggest sweetest pieces of fruit. reply analog31 10 hours agorootparentprevI read a lot of articles of this nature, not necessarily in this specific subject area, and may be guilty of writing some myself. I rarely build anything exactly to its original plans, but I take away bits and pieces, theory, and techniques, and re-combine them for my own purposes.The value of the whole thing hanging together as a product is that you know the person has worked out the bugs of the individual parts, to the point where the whole thing is testable and reproducible.This is actually true of the scientific literature as well. People rarely reproduce entire research projects, but often borrow bits and pieces and adapt them for their own use. reply starshadowx2 6 hours agorootparentprevI don&#x27;t know enough to know if this one would work for this kind of usage but cheaper and easily accessible spectrometers would be amazing for harm reduction and drug testing usages. reply thatcat 11 hours agorootparentprevYou could run Quality Control on your amazon chemical purchases if you were willing to put in time to find methods using spectrophotometery in this wavelength range. reply jamal-kumar 14 hours agoprevWow this is the coolest thing. I was looking at trying to build a DIY raman spectroscope and you can make one of those for under 100$ [1], but near-IR spectroscopy opens up more possibilities for sure. From what I understand (I&#x27;m not a scientist but I love the idea of having access to these tools) raman spectrscopy is limited to asymmetric crystalline molecules, so you can&#x27;t really get a good reflection off of say salt to get a reading. (Edit - Looks like Near-IR is complimentary - \"Raman active vibrations aren’t visible in the infrared for molecules with a symmetrical stretch. Similarly, infrared active vibrations aren’t visible in the Raman spectra for molecules with asymmetric stretch. This is known as the Principle of Mutual Exclusion and is what makes NIR and Raman Spectroscopy complementary techniques.\" [2] ) What we&#x27;re really hoping to find out is if we can detect stuff like pthalates in plastics and heavy metals such as cadmium in metal products, I understand we&#x27;ll probably just want to send things off to a lab to get GC&#x2F;MS done for those things after talking to some people who have more experience than I do but I still want to see if I can build a bunch of these raman or near-IR spectroscopes for pharmacies in the third world. Counterfeit drugs are a real problem out there and they don&#x27;t need to be.[1] https:&#x2F;&#x2F;www.hackteria.org&#x2F;wiki&#x2F;images&#x2F;a&#x2F;a0&#x2F;MobPhone_RamanSpe...[2] https:&#x2F;&#x2F;www.labmate-online.com&#x2F;news&#x2F;mass-spectrometry-and-sp... reply djsamseng 3 hours agoparentThanks for the link! I’ve been trying to figure out how to buy&#x2F;make a Raman spectrometer for cheap (currently in a third world country too!). Have you built one yet? I’m having trouble finding the lenses needed (mostly because of my lack of knowledge). Any chance you know what to use?Laser ($40): https:&#x2F;&#x2F;a.co&#x2F;d&#x2F;0wjNGBzDiffraction grating ($12): https:&#x2F;&#x2F;a.co&#x2F;d&#x2F;6bpO8xmLaser focusing lens: Not foundFluorescence collection lens: Not foundFocusing lens: Not foundCollimating lenses: Not found reply rolph 18 hours agoprev>>While a silicon-based camera is as cheap as dirt these days, an one-dimensional InGaAs pixel array already costs upper few thousand dollars. Any full-blown IR spectrometer system goes way over $10k, with their fancy thermoelectric cooling and precision gratings (we actually have one in our lab). The reason why they are so expensive is that the target user group are scientific researchers, not consumers.more fundamental science collectives where amateurs can also contributeLook anywhere there is field work out in nature, there&#x27;s amateurs discovering new species, fossils, minerals etc. reply fanick 16 hours agoprevSimilar project with single photo diode enclosed in sort of a pinhole camera: https:&#x2F;&#x2F;hackaday.com&#x2F;2016&#x2F;05&#x2F;18&#x2F;using-missile-tech-to-see-li... reply pimlottc 11 hours agoprevTo save time waiting for the content to animate in: javascript:document.querySelectorAll(\".animate-box\").forEach(e => { e.classList.remove(&#x27;animate-box&#x27;) }) reply lawlessone 18 hours agoprevBetween this and the DIY radio telescopes and wifi radar, maybe we will have a real tricorder some day. reply JKCalhoun 16 hours agoparentI&#x27;m just happy with all these \"Amateur Scientist\" links lately. More of these! reply 0xdeadbeefbabe 18 hours agoparentprevhehe no mention of the new iphone 15 reply mensetmanusman 9 hours agoprevhttps:&#x2F;&#x2F;petapixel.com&#x2F;2019&#x2F;07&#x2F;13&#x2F;shooting-high-res-thermal-p...One of my favorite photoshoots merges IR and visible. reply vondur 6 hours agoprevThis is pretty cool. One of the more fun things in O Chem lab was using the IR Spec on liquids. Preparing solids for IR analysis was not fun. reply oceanplexian 16 hours agoprevI wonder if this could be used for amateur astronomy? Would be cool to point a telescope at a star, and print out a spectrum corresponding to elements with emission lines. reply jacquesm 15 hours agoparentFor that a prism and a photographic film would probably be sufficient, the developed film would serve as your print-out. reply progbits 18 hours agoprevThis is very nice, but most of that $10k probably pays for certification that you won&#x27;t get for DIY. Great for hobbyist, but you couldn&#x27;t sell it for $9500 profit. reply MostlyStable 18 hours agoparentPossibly, but he also describes pretty cheap and easy calibration methods. It&#x27;s possible that most of the cost of the commercial options is the fact that these are extremely low-volume devices and the overhead of a business with low-volume sales is quite high. You might be paying less for calibration and more for all the things a business needs to do&#x2F;have that a lone DIYer doesn&#x27;t.Not to mention the fact that a lot of researchers are buying thing with grant money, and so can be, in some cases, somewhat price insensitive.I don&#x27;t doubt that the commercial one is pre-calibrated and certified. But I would be quite surprised if that certification&#x2F;calibration _actually_ cost ~$9000. reply progbits 18 hours agorootparentSure I&#x27;m not saying that. If you give away the R&D for free and don&#x27;t seek profit you can build it much cheaper.But self-calibrating is still something else. Unless I pay some lab with traceable calibration to do it for me I can&#x27;t use the results to certify other equipment for example. I think it&#x27;s more like insurance, sure the unit cost is low but you pay extra so in case the lab screws up they pay for the mess. reply MostlyStable 16 hours agorootparentThat&#x27;s fair, but I think a lot of people other than \"DIY hobbyists\" don&#x27;t have a need for traceable, liability responsible, validation chains. reply w10-1 16 hours agoparentprevDIY is not just for hobbyists.Companies face the build-or-buy question every day. This article demonstrates that you can build and validate your build with the knowledge and skills expected of most principal investigators. If I were a lab director, I would want my people to be able to consider building when it makes sense, particularly since we can then integrate systems and tailor validation to our requirements.That&#x27;s the stuff of proprietary IP and career advancement. reply s0rce 16 hours agoparentprevI&#x27;m not sure about certification but there is a lot of overhead in designing, producing, selling and servicing products. This is basically a prototype. It will probably work for a while but will be difficult to maintain if something goes wrong and the software won&#x27;t be very polished. There are a lot of non-BOM costs that go into selling a commercial spectrometer. This is great if you want to DIY one but even a single sensor based visible light spectrometer with cheap parts can&#x27;t be bought for close to the sum of its parts. reply jacquesm 18 hours agoparentprevSo what? If you need it, now you can build it. As a hobbyist you don&#x27;t need certified gear, you need gear. reply MrBuddyCasino 18 hours agoparentprevThe point wasn&#x27;t \"you can get rich selling this\", but \"you can save $$$ building this yourself as a hobbyist\". reply progbits 18 hours agorootparentAbsolutely, you are correct. reply ortusdux 17 hours agoparentprevI wonder what 3rd party certification would cost. Industries that use sensors like this frequently require annual recertification, so there are typically multiple testing houses and competitive prices. reply HerculePoirot 10 hours agorootparentDIY recertification obviously. What&#x27;s the lowest cost for the hobbyist ? A high-end, stable, double-wall PH-meter or finding a cheap way to build single-wall PH meters ?https:&#x2F;&#x2F;www.instructables.com&#x2F;cheap-DIY-electronic-pH-meter&#x2F;You explore the solution-space, and suddenly, out of nowhere it changes your perspective on another aspect, and you realize you may not need a spectrometer at all to measure ion concentartion in aeroponic nutrient solutions.http:&#x2F;&#x2F;jupiter.plymouth.edu&#x2F;~jsduncan&#x2F;courses..&#x2F;2011_Fall&#x2F;In... reply jacquesm 15 hours agorootparentprevThere is a huge difference between calibration and certification. Calibration means that your instrument gives you absolute rather than relative output. Certification means that your instrument is precise enough to be used for specific procedures.Calibration can be done in house, certification is usually the domain of some certification institution and can be extremely expensive depending on the kind of gear. reply CamperBob2 17 hours agoparentprevIf you&#x27;re using it in a lab, you can calibrate and verify this sort of instrument yourself most of the time. Wavelength is trivial to calibrate. Absolute amplitude not so much, but you usually don&#x27;t care that much about absolute amplitude. Response flatness across the spectrum may or may not be a concern. reply version_five 18 hours agoparentprevMy first thought as well. People tend to misunderstand why things cost what they do, for stuff like this it isn&#x27;t the raw components. reply herf 13 hours agoprevI think this design is usually called a \"scanning monochromator\" - really nicely done. reply tambourine_man 8 hours agoprevGreat content and form. The site is clean, yet still uses tasteful transitions and typography. reply txnf 9 hours agoprevI don&#x27;t understand how it scans to record the spectrum with a point detector? did I miss something? reply txnf 9 hours agoparentoh n&#x2F;m it has a lead screw and moves it across the focal plane of the spectrometer.. reply client4 16 hours agoprevA different design could leverage laser micro ablations on glass over a traditional CMOS sensor. reply ckocagil 15 hours agoprevNice project. A low DA capacitor Cf in parallel with the gain setting resistor Rf can average out more noise if needed. It can even deal with the mains noise if that remains an issue. Another potential improvement: addition of an optical chopper wheel to deal with the entire system&#x27;s 1&#x2F;f noise. The downside is this would require higher sampling rate which would then get demodulated + filtered externally. Would also limit Cf to higher frequencies. reply 0xbadcafebee 17 hours agoprev [–] Articles like these are rare, but there&#x27;s 30 years of them scattered around the internet. There&#x27;s got to be a way to catalogue them in a single index somewhere. Like a Wiki, but just articles that are hard to find and extremely interesting.I&#x27;d like an index because HN&#x27;s articles are often not this caliber of \"interesting\". If I look back 1 month to the top of HN (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;front?day=2023-08-12), there&#x27;s pontifi-posting (editorials), news and editorials from large media companies, blogspam from tech companies and OSS projects, and basic tech How Tos. I don&#x27;t want to read any of that; I come back hoping there&#x27;s a single one of this kind of article, and maybe find one a month. reply mdip 16 hours agoparentI have to agree with you. While I do enjoy the OSS projects&#x2F;physics-&#x2F;math-related topics (and a lot of the Show HN), these are my absolute favorite things to read.Curious if you&#x27;ve found other resources that provide more of this sort of content?I&#x27;ve found the Hackaday blog[0] tends to have a lot of this kind of content (often summarized&#x2F;linked to the original source) and sometimes it&#x27;s tagged in a way that makes surfacing others from their archives possible, but I&#x27;ve not found any other sites that are reasonably organized to help surface write-ups of this kind[1].[0] https:&#x2F;&#x2F;hackaday.com&#x2F;blog[1] There are subreddits where this sort of thing can be found, but \"deep-dive but accessible technical articles\" tend to be placed in a sub-reddit that&#x27;s dedicated to more narrow topics and a lot are lower quality (there&#x27;s similar content to this in &#x2F;r&#x2F;Optics from 2022, but I&#x27;d never have a reason to end up there) reply walterbell 16 hours agoparentprevManually curated list could be crowdsourced via an OSS project, e.g. 1. Criteria for articles in list. 2. Example and anti-example articles. 3. Tag via HN comment with short, unique and human-meaningful phrase. 4. Query Algolia periodically, triage, PR submission&#x2F;review&#x2F;merge. 5. Syndicate list as RSS feed. reply EricMausler 16 hours agoparentprev [–] Sounds like you want a curated feed. There are some email mailing lists that aim to achieve this, usually for a specific domain (like stock market, or AI, etc)Maybe a group chat or discord of like minded people who casually share things they come across that pass a high standardOne way or another it seems like you&#x27;re going to need to rely on another person to sift through all the daily published content and mark potentially interesting ones reply MayeulC 15 hours agorootparent [–] I wonder if you could just feed HN titles with more than ~30 upvotes to an LLM and ask it to bring out similar topics. Not perfect, but I think it could work as a start. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Yuan Cao has developed a budget-friendly do-it-yourself (DIY) near-infrared (near-IR) spectrometer possessing a resolution of 6 nm.",
      "The spectrometer operates using a diffraction grating and photodiodes amongst other components; it measures the spectral resolution and wavelength of light.",
      "In addition to discussing challenges related to noise reduction and interference, Cao also talks about calibration, the spectrometer's dynamic range, and measuring transmission spectra. They may reveal more spectra in the future."
    ],
    "commentSummary": [
      "An individual has developed a low-cost self-made near-infrared spectrometer, with possible usage in diverse sectors.",
      "The creator is keen on fabricating affordable spectrometers for third world countries, aiming to aid in the identification of fake drugs.",
      "The text emphasizes the challenges in calibrating and certifying these instruments, and highlights the lack of robust articles on platforms such as Hacker News."
    ],
    "points": 339,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1694619208
  },
  {
    "id": 37500895,
    "title": "When MFA isn't MFA, or how we got phished",
    "originLink": "https://retool.com/blog/mfa-isnt-mfa/",
    "originBody": "Customers Integrations Templates Pricing Docs Sign in Start for free When MFA isn't actually MFA Snir Kodesh 13 September 2023 • 6 min read On August 29, 2023, Retool notified 27 cloud customers that there had been unauthorized access to their accounts. If you’re reading this and you were not notified, don’t worry – your account was not impacted. There was no access to on-prem or managed accounts. Nevertheless, here’s what happened, with the hope that this will help apply the lessons we’ve learned and prevent more attacks across the industry. What happened On August 27, 2023, we fell victim to a spear phishing attack. The attacker was able to navigate through multiple layers of security controls after taking advantage of one of our employees through a SMS-based phishing attack. Several employees received targeted texts, claiming that a member of IT was reaching out about an account issue that would prevent open enrollment (which affects the employee’s healthcare coverage). The timing coincided with a recently announced migration of logins to Okta, and the message contained a url disguised to look like our internal identity portal. Almost all employees didn’t engage, but unfortunately one employee logged into the link provided by the attackers. The following is a transcription of the message: Hello A, This is B. I was trying to reach out in regards to your [payroll system] being out of sync, which we need synced for Open Enrollment, but i wasn’t able to get ahold of you. Please let me know if you have a minute. Thanks You can also just visit https://retool.okta.com.[oauthv2.app]/authorize-client/xxx and I can double check on my end if it went through. Thanks in advance and have a good night A. After logging into the fake portal – which included a MFA form – the attacker called the employee. The caller claimed to be one of the members of the IT team, and deepfaked our employee’s actual voice. The voice was familiar with the floor plan of the office, coworkers, and internal processes of the company. Throughout the conversation, the employee grew more and more suspicious, but unfortunately did provide the attacker one additional multi-factor authentication (MFA) code. The additional OTP token shared over the call was critical, because it allowed the attacker to add their own personal device to the employee’s Okta account, which allowed them to produce their own Okta MFA from that point forward. This enabled them to have an active GSuite session on that device. Google recently released the Google Authenticator synchronization feature that syncs MFA codes to the cloud. As Hacker News noted, this is highly insecure, since if your Google account is compromised, so now are your MFA codes. Unfortunately Google employs dark patterns to convince you to sync your MFA codes to the cloud, and our employee had indeed activated this “feature”. If you install Google Authenticator from the app store directly, and follow the suggested instructions, your MFA codes are by default saved to the cloud. If you want to disable it, there isn’t a clear way to “disable syncing to the cloud”, instead there is just a “unlink Google account” option. In our corporate Google account, there is also no way for an administrator to centrally disable Google Authenticator’s sync “feature”. We will get more into this later. We use OTPs extensively at Retool: it’s how we authenticate into Google and Okta, how we authenticate into our internal VPN, and how we authenticate into our own internal instances of Retool. The fact that access to a Google account immediately gave access to all MFA tokens held within that account is the major reason why the attacker was able to get into our internal systems. Getting access to this employee’s Google account therefore gave the attacker access to all their MFA codes. With these codes (and the Okta session), the attacker gained access to our VPN, and crucially, our internal admin systems. This allowed them to run an account takeover attack on a specific set of customers (all in the crypto industry). (They changed emails for users and reset passwords.) After taking over their accounts, the attacker poked around some of the Retool apps. After learning of the attack, we immediately revoked all internal authenticated sessions (Okta, GSuite, etc) for employees, locked down access to the affected accounts, notified the affected customers, and restored their accounts to their original state (with original email addresses), reverting the 27 account takeovers. As an aside, we’re glad that not a single on-premise Retool customer was affected. Retool on-prem operates in a “zero trust” environment, and doesn’t trust Retool cloud. It is fully self contained, and loads nothing from the cloud environment. This meant that although an attacker had access to Retool cloud, there was nothing they could do to affect on-premise customers. It’s worth noting that the vast majority of our crypto and larger customers in particular use Retool on-premise. With the attack scope defined, let’s dig into what we learned, starting with our biggest point of vulnerability: software-based OTPs for MFA. Beware of “MFA” We have an internal Retool instance used to provide customer support; this is how the account takeovers were executed. The authentication for this instance happens through a VPN, SSO, and a final MFA system. A valid GSuite session alone would have been insufficient. The fact that Google Authenticator syncs to the cloud is a novel attack vector. What we had originally implemented was multi-factor authentication. But through this Google update, what was previously multi-factor-authentication had silently (to administrators) become single single-factor-authentication, because control of the Okta account led to control of the Google account, which led to control of all OTPs stored in Google Authenticator. We strongly believe that Google should either eliminate their dark patterns in Google Authenticator (which encourages the saving of MFA codes in the cloud), or at least provide organizations with the ability to disable it. We have already passed this feedback on to Google. Sharing our lessons Social engineering can affect anyone Social engineering is a very real and credible attack vector, and anyone can be made a target. If your company is large enough, there will be somebody who unwittingly clicks a link and gets phished. Especially as social engineering evolves (with AI and deepfakes, but also with more personal information being available on the internet), educating, preventing, and testing (via red teams) your employees regarding phishing attacks will become ever more important. Preventing systematic failures Even with perfect training and awareness of these attacks, mistakes will happen. Just like preventing a junior engineer from accidentally dropping the production database, there need to be systems in place to prevent human error from impacting the overall system. SMS as a second-factor has been rightly criticized for being too vulnerable to SIM swapping attacks. Time-based one-time password (TOTP), while resilient to SIM swapping, is still vulnerable to the same social engineering threat vectors. Hardware security keys– using FIDO2 provide resilience to these threats. A code can’t be disclosed to an attacker– since there isn’t a code to share in the first place! Defense in depth We’re equipped with many levels of protection, including multiple MFA codes, multiple lines of defense (Okta account, VPN, internal admin system, etc.). That said, technological solutions only go so far, and we believe that adding a human-in-the-loop is necessary (although again, not sufficient, given deepfakes) for important actions. We oftentimes bias towards technological solutions because they’re easy to self-serve, but ultimately, anything that can be done by an employee can be socially engineered out of that employee too. We have already implemented this at Retool internally, and expect to implement this in Retool — the product — so our customers have access to building these kinds of human-in-the-loop workflows too. Trust as little as possible This incident only affected a very small subset of Retool cloud customers– no on-prem or managed accounts were impacted. We intentionally architected Retool on-prem such that it doesn’t trust Retool cloud. Retool on-prem makes no contact to Retool cloud; it is fully self-hosted (the front-end, back-end, storage, etc. are all within your own VPC). This way, customers are fully in control of their security (as well as when they update), and do not need to trust Retool. Even though our cloud systems were compromised, there was no way for attackers to compromise Retool on-premise. We are fortunate we architected on-premise this way. The vast majority of our customers in more sensitive industries (e.g. crypto, healthcare, finance, etc.) use our on-premise solution, and we encourage our customers to consider it, if security is important. Understand your threat model Retool is in the unique position of being a platform where you can build any sort of software imaginable. However, that also means that customers still need to treat building in Retool with the same care and attention to security that traditional codebases require. We encourage you to understand your own threat model: if you are operating in an industry where apps have access to dangerous, irreversible actions, we strongly recommend customers to integrate further protections. For example, we encourage customers to require separate MFA tokens to execute actions (a first-class primitive in Retool), or escalation flows that require multiple employees to approve actions above certain thresholds (soon to become a first-class primitive in Retool). When we examined our forensics, customers who built secure apps and understood their threat matrix were effective at repelling the attack, even despite the account takeovers. Conclusion This situation was challenging. It’s embarrassing for the employee, disheartening to cybersecurity professionals, and infuriating for our customers. For those reasons, these kinds of attacks are difficult to talk about, but we believe that they should be addressed in the open. There are clear risks here that the industry needs to address (e.g. the dark patterns in Google Authenticator). Our hope is that by publishing these attack vectors we can make the industry overall more aware, and enable cybersecurity professionals to harden their own systems. Editor's note: We are actively working with law enforcement and a third party forensics firm. Want to leverage LLMs in your engineering organization? Start here. If your team is just beginning to leverage LLMs, these learnings and best practices can help you use them effectively to ship thoughtful AI-powered apps, workflows, and features. Learnings and best practices to get developers Retool Team 7 MIN READ Retool is the fast way to build internal tools. Connect to your databases and APIs, and build your own tools in minutes. Start for free Learn More Use Cases Admin panels Firebase GUI MongoDB GUI GraphQL GUI Dashboards SQL GUI React components Google Sheets GUI Integrations PostgreSQL MySQL DynamoDB Firebase GraphQL Amazon S3 Google Sheets MongoDB Developers Changelog Documentation Status On-prem deployment API Generator Developer Utilities Community Community home Support forum Show & tell Retool for Startups Internal Tools Report Engineering Time Report Company About Customers Careers Team Blog © Retool, Inc. Privacy Terms Security",
    "commentLink": "https://news.ycombinator.com/item?id=37500895",
    "commentBody": "When MFA isn&#x27;t MFA, or how we got phishedHacker NewspastloginWhen MFA isn&#x27;t MFA, or how we got phished (retool.com) 328 points by dvdhsu 14 hours ago| hidepastfavorite232 comments macNchz 12 hours agoBeyond having hardware keys, this scenario is why I really try to drive home, in all of my security trainings, the idea that you should instantly short circuit any situation where you receive a phone call (or other message) and someone starts asking for information. It&#x27;s always okay to say, \"actually, let me get back to you in a minute\" and hang up, calling back on a known phone number from the employee directory, or communicate on different channel altogether.Organizationally, everyone should be prepared for and encourage that kind of response as well, such that employees are never scared to say it because they&#x27;re worried about a snarky&#x2F;angry&#x2F;aggressive response.This also applies to non-work related calls: someone from your credit card company is calling and asking for something? Call back on the number on the back of your card. reply codebje 10 hours agoparentI&#x27;ve had a wide range of responses from people calling me when I tell them I won&#x27;t give personal details out based on a cold call.A few understand immediately and are good about it. Most have absolutely no idea why I would even be bothered about an unexpected caller asking me for personal information. A few are practically hostile about it.None, to date, have worked for a company that has a process established for safely establishing identity of the person they&#x27;re calling. None. Lengthy on-hold queues, a different person with no context, or a process that can&#x27;t be suspended and resumed so the person answering the phone has no idea why I got a call in the first place.(Yet I&#x27;ll frequently get email full of information that wouldn&#x27;t be given out over the phone, unencrypted, unsigned, and without any verification that the person reading it is really me.)The organisational change required here is with the callers rather than the callees, and since it&#x27;s completely about protecting the consumer rather than the vendor, it&#x27;s a change that&#x27;s unlikely to happen without regulation. reply Terretta 9 hours agorootparent> None, to date, have worked for a company that has a process established for safely establishing identity of the person they&#x27;re callingWhat&#x27;s fun here is, the moment they ask you for anything, flip the script and start to try to establish a trust identity for the caller.Tell them you need to verify them, and then ask how they propose you do that.Choose your own adventure from there. reply codebje 6 hours agorootparent> Tell them you need to verify them, and then ask how they propose you do that.Last time I did that, the caller said \"but you can just trust that I&#x27;m from .\" So I replied that they, likewise, could just trust that I&#x27;m me, and you could practically hear the light bulb click on. They did their best to help from there but their inbound lines aren&#x27;t staffed effectively so my patience ran out before I reached an operator. reply nanidin 9 hours agorootparentprevCredit card fraud departments are generally good about this. reply SoftTalker 7 hours agorootparentI can&#x27;t remember which company it was, but I got a call a few years ago about some issue with an account, and they wanted some information to \"verify my identity\"I said wait a minute, you called me. Shouldn&#x27;t I be verifying who you are?The guy kind of laughed and said yeah, but this is the process I&#x27;ve been given to follow. I said I would call back on public customer service number and he said that would be fine.It turned out it was a legit call, but just weird that they would operate that way.I wish I could remember who it was. A credit card, I think. reply wahern 7 hours agorootparentprevAnecdotally, I seem to have had the opposite experience. I&#x27;ve been doing this for at least 15 years, and never had a negative reaction. With bank, credit card, or finance-related companies, they seem to understand immediately. With other callers I&#x27;ve gotten awkward pauses, but ultimately they were politely accommodating or at least understanding that some issue would have to be processed through other channels or postponed.However, I don&#x27;t have strict requirements. When a simple callback to the support line on the card, bill, or invoice doesn&#x27;t suffice--and more often than not it does, where any support agent can field the return call by pulling up the account notes--all I ask for at most is an extension or name that I can use when calling through a published number. I&#x27;ll do all the leg work, and am actually a little more suspicious when given a specific number over the phone to then verify. Only in a few cases did I have to really dig deep into a website for a published number through which I could easily reach them. In most cases it suffices to call through a relatively well attested support number found in multiple pages or places[1].I&#x27;m relatively confident that every American&#x27;s Social Security number (not to mention DoB, home address, etc) exists in at least one black market database, so my only real practical concern is avoiding scammers who can&#x27;t purchase the data at [black] market price, which means they&#x27;re not very sophisticated. A callback to a published phone number for an otherwise trusted entity that I already do business with suffices, IMO. And if I&#x27;m not already doing business with them, or if they have no legitimate reason to know something, they&#x27;re not getting anything, period.[1] I may have even once used archive.org to verify I wasn&#x27;t pulling the number off a recently hacked page, as it was particularly off the beaten path and a direct line to the department--two qualities that deserve heightened scrutiny by my estimation. reply transitionnel 6 hours agorootparentprevSomeone needs to standardize a simple reverse-authentication system for this.For example whenever a caller is requesting sensitive information, they give you a temporary extension directing to them or an equal, and ask you to call the organization&#x27;s public number and enter that extension. Maybe just plug the number into their app if applicable to generate a direct call.Like other comments have mentioned, the onus should be on them. Also, they would benefit from the resultant reduction in fraud. Maybe a case study on fraud reduction savings could help speed the adoption process without having to invoke the FCC. reply tortue0 4 hours agorootparentprevI&#x27;ve had my cable company call me directly about an account issue and told them I couldn&#x27;t validate it was them and the person got somewhat irate with my response, insisting there was no one I could call to verify them and that it has to be handled on that call. Turns out it was just a sales call (up selling a product) - which probably speaks to the level of talent they hire for that. reply dataflow 11 hours agoparentprev> this scenario is why I really try to drive home, in all of my security trainings, the idea that you should instantly short circuit any situation where you receive a phone call (or other message) and someone starts asking for information.The trouble is, calling the number on the back of your card requires actually taking out your card, dialing it, wading through a million menus, and waiting who-knows-how-long for someone to pick up, and hoping you&#x27;re not reaching a number that&#x27;ll make you go through fifteen transfers to get to the right agent. People have stuff to do, they don&#x27;t want to wait around with one hand occupied waiting for a phone call to get picked up for fifteen minutes. When the alternative is just telling your information on the phone... it&#x27;s only natural that people do it.Of course it&#x27;s horrible for security, I&#x27;m not saying anyone should just give information on the phone. But the reality is that people will do it anyway, because the cost of the alternative isn&#x27;t necessarily negligible. reply strken 10 hours agorootparentI say \"If this is a scam call please hang up now, otherwise give me an invoice or ticket number or name and department and I&#x27;ll get back to you,\" and they usually do hang up. The case where you need to actually call your bank is really rare.Note that it&#x27;s very important not to let them give you an actual phone number to call on. This sounds obvious but I know someone who hung up but called back on a number given by the scammers, which was of course controlled by them and not the bank. reply intrasight 5 hours agorootparentI&#x27;m going to add to this that \"hang up\" means physically do that. I&#x27;ve heard that many are tricked by the attacker playing a \"dial tone\" sound into the phone and thus keeping the line open and \"answering\" when you thought you called you bank. reply MaxBarraclough 1 hour agorootparentYou may be right that some people are tricked into thinking the call has been terminated by the caller, when in fact the caller is playing a dial tone over the line. It&#x27;s worse than that though. In some telephone systems, the call is not ended when the callee hangs up.https:&#x2F;&#x2F;security.stackexchange.com&#x2F;questions&#x2F;100268&#x2F;does-han... reply macNchz 11 hours agorootparentprevI don&#x27;t think most people who get scammed this way pause to say \"oh, this might be someone stealing my credit card number\", then disregard that thought because it&#x27;s too much of a pain to call back on an official line. Instead I think they don&#x27;t question the situation at all, or the scammer has enough information to sound sufficiently authoritative. Most non-technical people I&#x27;ve talked to about this are pretty scared of getting scammed, but tell me the thought never crossed their mind they could call back on a trusted number.I like the \"hang up, call back\" approach because it takes individual judgment out of the equation: you&#x27;re not trying to evaluate in real time whether the call is legit, or whether whatever you&#x27;re being asked to share is actually sensitive. That&#x27;s the vulnerable area in our brains that scammers exploit. reply dataflow 10 hours agorootparentI&#x27;m sure a lot of people are like what you describe (this doesn&#x27;t occur to them), but I think it does affect those who are a bit suspicious&#x2F;on the fence, potentially like the person in the article. (\"Throughout the conversation, the employee grew more and more suspicious, but unfortunately did provide [the MFA code].\") reply couchand 10 hours agorootparentThe old adage is that a con artist makes the best mark. reply josho 11 hours agorootparentprevGreat point. But it could be easily solved with something like: “Call the number on the back of your credit card. Push *5 and when prompted enter your credit card number and you will be immediately connected back to my line” reply dataflow 11 hours agorootparentOr just connect you directly if you call back within a few minutes from the same number they called, no need to press anything. But I guess that&#x27;s too advanced for 2023 technology reply zamfi 5 hours agorootparentprevI think the parent poster is arguing that we should normalize this behavior not that there&#x27;s no excuse for not calling the number back given the reality we have today.You&#x27;re saying it&#x27;s natural for people not to want to call back and wade through a million menus, and I agree.But the conclusion from this is that companies should change their processes so that calling back is easy, precisely because otherwise people won&#x27;t do it.And the more people that do it despite the costs, the more normalized it&#x27;ll be, and the more companies will be incentivized to make it easier. reply dataflow 4 hours agorootparentWe certainly should normalize this, but my point was that it&#x27;s going against the grain, so efforts like this may be in vain without a bigger lever to pull. e.g., I imagine you&#x27;d need to convince some sort of authority (CISA? FIPS? not sure whom the right entity is) to point out the best practices here before organizations start paying attention. reply tyingq 11 hours agoparentprev>This also applies to non-work related calls: someone from your credit card company is calling and asking for something? Call back on the number on the back of your card.There&#x27;s a number of situations, not just credit card ones, where it&#x27;s impossible or remarkably difficult to get back to the person that had the context of why they were calling.Your advice holds, of course, because it&#x27;s better to not be phished. But sometimes it means losing that conversation. reply munk-a 11 hours agorootparentMy mother recently started having to deal directly with utility bills and the like and this was some information we impressed very early on. You should never agree to billing or hand over CC&#x2F;account information in a phone call you didn&#x27;t initiate. She hasn&#x27;t run into an issue yet - most utilities, online stores and other entities have call in numbers if you need to resolve a billing dispute. That random company you bought a plumbing valve from has an office somewhere with a secretary that gets a phone call maybe three times a month from customers looking to resolve issues - and Amazon has mostly centralized support for small sellers and has lines you can call to resolve any disputes you have which may forward you to the original sale party but often just resolve the issue directly.Honestly, the worst experiences are usually with large companies that funnel all customers into massive phone centers - I&#x27;ve probably lost the better part of a week to Comcast over my lifetime. reply tyingq 9 hours agorootparentYeah, I&#x27;m talking about situations where it&#x27;s a department that&#x27;s not tied to the main call center. The credit card fraud people, for example[1]. Or, with Comcast, some guy saying your modem return was missing a piece, etc. Those are often hard to reach by calling the main number.[1] For at least one place, the people that proactively identify things that could be fraud and call you...they aren&#x27;t the same people you call to report fraud on your own. Why? No idea. reply macNchz 11 hours agorootparentprevDefinitely, sometimes they&#x27;ll have a case number or agent id you can use to get back to them, but there are cases where you have to assume if it&#x27;s important to them they&#x27;ll continue to nag or reach out on another channel.I have had at least one situation where I spent a while trying to get back to a quite convincing&#x2F;legitimate sounding caller this way, where, as I escalated through support people it became increasingly clear that the initial call had been a high quality scam, and not in fact a real person from the bank. reply codebje 10 hours agorootparentI put in very limited effort in returning cold calls. The contact is being initiated by the other party, the interest in the exchange is theirs, and the onus on making it work is theirs.Companies, including banks, don&#x27;t call you to protect _your_ interests, they call you to protect themselves. reply BenjiWiebe 6 hours agorootparentExcept some banks and credit card companies will call you to notify you of fraud on your account. reply hinkley 11 hours agoparentprevAdvice I haven&#x27;t even followed myself:It&#x27;s probably a good idea to program your bank&#x27;s fraud number into your phone. The odds that someone hacks your bank&#x27;s Contact Us page are small but not zero.The bedrock of both PGP and .ssh&#x2F;known_hosts could be restated as, \"get information before anyone knows you need it\".Fraud departments contacting me about potentially fraudulent charges is always going to make me upset. Jury is still out on whether it will always trigger a rant, but the prognosis is not good. reply GauntletWizard 11 hours agorootparentAt least once I have gotten a terribly phrased and link-strewn \"Fraud Alert\" from a bank, reported it to said bank&#x27;s anti-phisihing e-mail address, gotten a personalized mail that responded that it was in fact fraud and that they had policies against using third party subdomains like... And then found out the day later that yes, that was their real new anti-fraud tool and template.There will need to be jail time for the idiots writing the government standards on these fraud departments before we get jail time for the idiots running these fraud departments before it gets better. reply hinkley 10 hours agorootparentLast time I talked to someone about this they pointed out that fraud depts are often outsourced. Which is a lovely plan because now your customers hate you for something an entirely different company did to them. And also they are directing you away from the official website every single time you interact with them.I&#x27;m not sure what grounds you issue arrest warrants on, but I appreciate the sentiment. reply GauntletWizard 8 hours agorootparentIronically, fraud. They have done substantial financial and \"real\" harm by pretending to be competent at things that they are clearly not, and have been a combination of remiss in their duties and complicit in the crimes of fraudsters.Sufficiently advanced incompetence is indistinguishable from malice, and should be prosecuted as such. reply lxgr 9 hours agoparentprev> someone from your credit card company is calling and asking for something? Call back on the number on the back of your card.This recently happened to me, and bizarrely they wouldn’t tell me what’s actually going on on my account because of not being able to verify me. (They were also immediately asking for personal information on the outbound call, which apparently really was from them.) reply OJFord 8 hours agorootparentFinancial companies, the government, ... I always try to bother to raise the issue afterwards, but (not that I think my comments alone would do anything) so far nothing changed that I&#x27;ve taken issue with, I don&#x27;t think.A big one I&#x27;m aware of many others complaining about in the industry is local governments in the UK soliciting elector details via &#x27;householdresponse.com&#x2F;&#x27; in a completely indistinguishable from phishing sort of way.(They send you a single letter directing you to that address with &#x27;security code part 1&#x27; and &#x27;2&#x27; in the same letter, along with inherently your postcode which is the only other identifier requested. It&#x27;s an awful combination of security theatre and miseducation that scammy phishing techniques look legit.) reply shortcake27 4 hours agorootparentHa, this reminds me of driver licences in Australia. So at this point almost everyones licence has been leaked multiple times (and just having the details used to be enough to open a bank account online, not sure if this is still the case).I received an email from my state’s RTA, saying they were adding 2-factor authentication to licences. Great! I assumed this might be an oauth type scenario, or maybe even just email.Nope. The “second” factor is a different number printed on the licence. Surely this communication had to go through multiple departments, get vetted for accuracy. Yet no one picked up that this isn’t multi factor authentication.Its only purpose is to make it easier for them to issue a new licence _after_ you’ve been defrauded out of all your money, because most states refuse to issue people with new licence numbers. It does nothing more than fix an incompetence in their system&#x2F;process. Yet it was marketed as some kind of security breakthrough, as if it would add protection to your licence. reply polygamous_bat 9 hours agorootparentprevThat&#x27;s the big problem, isn&#x27;t it? People think it&#x27;s okay to give out information on an incoming call because often it is really okay. If it were unreliable 99% of the time, phishers would not use this method as an attack vector. reply hn_throwaway_99 11 hours agoparentprevAmen, amen, amen. IMO \"Hang up, look up, call back\" should basically be the only thing that security training focuses on, and it should be culturally ingrained: https:&#x2F;&#x2F;krebsonsecurity.com&#x2F;2020&#x2F;04&#x2F;when-in-doubt-hang-up-lo... reply RulerOf 6 hours agoparentprev> any situation where you receive a phone call (or other message) and someone starts asking for information.I had AWS of all places do this to me a year or two ago. The rep needed me to confirm some piece of information in order to talk to me about an ongoing issue with the account. If I recall correctly, the rep wanted a nonce that had been emailed to me.\"I&#x27;m terribly sorry but I won&#x27;t do that. You called me.\"Ultimately turned out to be legit, but I admit I was floored. reply dspillett 10 hours agoparentprev> the idea that you should instantly short circuit any situation where you receive a phone call (or other message) and someone starts asking for informationIt really irritates me that some significant companies openly encourage customers to ignore this advice, teaching then had practise. The most recent case I know of is PayPal calling myself. It was actually thenm new cc account, I thought I&#x27;d setup auto payment but it wasn&#x27;t so I was a child if days late with the first payment) but it so easily could have not been. The person on the other end seemed rather taken aback that I wouldn&#x27;t discuss my account or confirm any details on a call I&#x27;d not started, and all but insisted that I couldn&#x27;t hang up and call back. In the end I just said I was hanging up and if I couldn&#x27;t call back than that was a them problem because at that point I had no way of telling if it was really the company or not. At that point she said she&#x27;d send a message that is could read via my account online, which did actually happen so it wasn&#x27;t a scammer. But to encourage customers to perform unsafe behaviour with personal and account details is highly irresponsible. reply rmbyrro 9 hours agoparentprev> someone starts asking for informationEspecially OTP codes.I can&#x27;t understand how someone works at a tech company and is clueless to the point of sharing an auth code over the phone. My grandma, sure, but a Retool employee? C&#x27;mon, haven&#x27;t we all read enough of these stories? reply ketzo 9 hours agorootparentYou can&#x27;t understand at all how someone with your coworker&#x27;s voice might lull you into a false sense of urgency and safety?Security is a weak-link problem, not a strong-link one. You have to plan for the least security-minded people, the tired and stressed employee. reply rmbyrro 9 hours agorootparentI can understand how someone with my coworker&#x27;s voice might lull myself into a false sense of urgency and safety.To the point of sharing an OTP code over the phone from a strange number? I&#x27;m sorry, no. reply SoftTalker 7 hours agorootparentYou could give them a false number and see what happens. That might trip up their script enough to reveal they aren&#x27;t who they seem to be. Just play dumb -- \"I can&#x27;t understand why it&#x27;s not working, that&#x27;s the number on my phone....\" reply rmbyrro 1 hour agorootparentThat&#x27;s also a good idea. reply matsemann 6 hours agorootparentprevThey could trivially spoof the number they&#x27;re calling from to match. reply rmbyrro 1 hour agorootparentOr I could call them on a known point of contact, like a phone number or IM. replyalsodumb 12 hours agoprevMaybe it’s just me, but I am really skeptical about the DeepFake part - it’s a theoretically possible attack vector, but the only evidence they possibly could have to support this statement would be the employees testimony. Targeting a particular employee with the voice of a specific person this employee knows requires a lot of information and insider info.Also, I think the article spends a lot of effort trying to blame Google Authenticator and make it seems like they had the best possible defense and yet attackers managed to get through because of Googles error. Nope, not even close. They would have had hardware 2FA if they were really concerned about security. Come on guys, it’s 2023 and hardware tokens are cheap. It’s not even a consumer product where one can say that hardware tokens hinder usability. It’s a finite set of employees, who need to do MFA certain times for certain services mostly using one device. Just start using hardware keys. reply dvdhsu 12 hours agoparentHi, David, founder @ Retool here. We are currently working with law enforcement, and we believe they have corroborating evidence through audio that suggests a deepfake is likely. (Put another way, law enforcement has more evidence than just the employee&#x27;s testimony.)(I wish we could blog about this one day... maybe in a few decades, hah. Learning more about the government&#x27;s surveillance capabilities has been interesting.)I agree with you on hardware 2FA tokens. We&#x27;ve since ordered them and will start mandating them. The purpose of this blog post is to communicate that what is traditionally considered 2FA isn&#x27;t actually 2FA if you follow the default Google flow. We&#x27;re certainly not making any claims that \"we are the world&#x27;s most secure company\"; we are just making the claim that \"what appears to be MFA isn&#x27;t always MFA\".(I may have to delete this comment in a bit...) reply hnburnsy 11 hours agorootparentThanks for all this insight, this is why HN rules. What is your impression of law enforcement, everyone claims to reach out after an attack, but I&#x27;ve never seen follow up of sucessful law enforcement activity resulting in arrests or prosecution. Thanks again. reply dvdhsu 10 hours agorootparent(May also have to delete this later, but...)Law enforcement is currently attempting to ascertain whether or not the actor is within the US. If it&#x27;s within the US, I (personally) believe there&#x27;s a good chance they&#x27;ll take the case on and presumably with enough digging, will find the attacker. (The people involved seem to be... pretty good.)But if they&#x27;re outside US (which is actually reasonably high probability, given the brazenness of the attack, and the fact that they&#x27;re leaving a lot of exhaust [e.g. IP address, phone number, browser fingerprints, etc.]), then my understanding is that law enforcement is far less interested, since it&#x27;s unlikely that even an identification of the hacker would lead to any concrete results (e.g. if they were in North Korea). (FWIW, the attack was not conducted via Tor, which to me implies that the actor isn&#x27;t too worried about law enforcement.)To give you a sense, we are in an active dialogue with \"professionals\". This isn&#x27;t a \"report this to your local police station\" kind of situation. reply e12e 10 hours agorootparentOn the plus side, if the attacker is outside the US, and a foreign national - the NSAs illegal wiretap evidence is legal! reply ianhawes 8 hours agorootparentprevFWIW engaging simultaneously with both the FBI and the USAO&#x2F;DOJ and putting pressure on DOJ to act on the case typically results in better outcomes than just assuming the SA assigned is going to follow through and bugging them about it. reply hnburnsy 8 hours agorootparentprevThx again! reply oldtownroad 10 hours agorootparentprev> …we believe they have corroborating evidence through audio that suggests a deepfake is likely…Does that mean they have audio of the call? reply ianhawes 8 hours agorootparentMost attacks like this use stolen credentials for VOIP providers, i.e. Twilio. It&#x27;s likely the FBI quickly obtained a subpoena which produced a recording. The attacker may not have known the call was being recorded. reply alsodumb 11 hours agorootparentprevThanks for the reply! What&#x27;s expecting one.Since you might have you delete the reply anyway, can I get a candid answer on why hardware 2FA tokens weren&#x27;t a part of the default workflow before the incident? Was it concerns about the cost, the recovery modes, or was it just the trust in the existing approach? reply apostacy 5 hours agorootparentprevThis is an example of Google sabotaging a techology it doesn&#x27;t like. I&#x27;m not saying it is a conspiracy. But by thwarting TOTP like this, Google is benefiting.I really like TOTP. It gives me more flexibility to control keys on my end. And you can still use a Yubikey to secure your private TOTP key. But you can also choose to copy your private key to multiple hardware tokens without needing anyone&#x27;s permission. Properly used, you can get most of the benefit of FIDO2 with a lot more flexibility.I actually recently deployed TOTP, and everyone was quite happy with it. But knowing that Google is syncing private keys around by default, I no longer think we can trust it. reply solatic 5 hours agoparentprevOne problem with hardware keys is still SaaS vendor support. There is a very narrow path for effective enforcement: require SSO, then require hardware tokens at the SSO level. But even that is difficult to truly enforce, because the IdP often has \"recovery\" mechanisms that grant access without a hardware key. Google is also guilty of not adding a claim to the OIDC&#x2F;SAML response verifying that a hardware token was used to login, so vendors cannot be configured to decide to reject the login because it didn&#x27;t use a hardware token.If you have any vendors without SSO (like GitHub, because it&#x27;s an Enterprise feature), you&#x27;re lucky if they support hardware tokens (cool, GitHub does) and even luckier if their \"require 2FA\" option (which GitHub has, per organization) allows you to require hardware keys (which GitHub does not).Distributing hardware keys to employees is one thing. Mandating them is quite another. reply fragmede 3 hours agorootparentIf your organization is rich enough to buy hardware keys for everybody, but too stingy to pay for GitHub Enterprise, I&#x27;m not sure what to say. reply richrichardsson 2 hours agoparentprev> unfortunately did provide the attacker one additional multi-factor authentication (MFA) codeHow is this Google&#x27;s fault?Which rock was this employee living under to not have understood you NEVER give an OTP code to anyone? reply nextaccountic 5 hours agoparentprev> the only evidence they possibly could have to support this statement would be the employees testimonyI&#x27;ve set up my phone to record all calls. The employee could have too. reply rolobio 13 hours agoprevVery sophisticated attack, I would bet most people would fall for this.I&#x27;m surprised Google encourages syncing the codes to the cloud... kind of defeats the purpose. I sync my TOTP between devices using an encrypted backup, even if someone got that file they could not use the codes.FIDO2 would go a long way to help with this issue. There is no code to share over the phone. FIDO2 can also detect the domain making the request, and will not provide the correct code even it the page looks correct to a human. reply bawolff 12 hours agoparent> I&#x27;m surprised Google encourages syncing the codes to the cloud... kind of defeats the purpose.Depends on what you think the purpose is. People talk about TOTP solving all sorts of problems, but in practise the only one it really solves for most setups is people choosing bad passwords or reusing passwords on other insecure sites. Pretty much every other threat model for it is wishful thinking.While i also think the design decision is questionable, the gain in security from people not constantly losing their phone probably outweighs for the average person the loss of security of it all being in a cloud account (as google cloud for most people is probably one of their most well secured account) reply tomatocracy 11 hours agorootparentIt&#x27;s also a reasonable defence against naive keylogging techniques - including shoulder-surfing either directly or eg via security cameras. In some places this can be a pretty big threat. reply bawolff 10 hours agorootparentI think its reasonable against spur of the moment shoulder surfing. I&#x27;m a little doubtful about how common that attack vector is - i think showing passwords as ** is a reasonable deterrant against literal shoulder surfing as well. Once you get security cameras involved things get more sophisticated and people can watch the feed live or do other things with physical access to the device.Ultimately, i think for the average user the attacker is mostly not in physical proximity (although there certainly are exceptions), and if you are being targeted explicitly then you are screwed if they are installing cameras and modifying your hardware.Maybe the big exception would be a camera in a coffee shop place looking for people (not live) logging into their bank accounts. I could inagine this being a helpful defense. reply wayfinder 11 hours agorootparentprevWell all Google needed to do to make it at least a little harder is to encrypt the backup with a password at least.The user can still put in an insecure password but uploading all your 2FA tokens to your primary email unencrypted is basically willingly putting all your eggs in one basket. reply luma 12 hours agorootparentprevTOTP is helpful when you don’t fully trust the input process. If rogue javascript is grabbing creds from your page, or the client has a keylogger they don’t know about, TOTP can help. reply hinkley 11 hours agorootparentBlizzard was one of the first large customers of TOTP, and what we learned from that saga is that 1) keyloggers are a problem and 2) impersonating people for TOTP interactions is profitable even if you&#x27;re only a gold farmer.The vector was this: Blizzard let you disable the authenticator on your account by asking for 3 consecutive TOTP outputs from your device. That would let you delete the authenticator from your account.The implementation was to spread a keylogger as a virus, and when it detected a Blizzard login, it would grab the key as you typed it, and make sure Blizzard got the wrong value when you hit submit. Blizzard would say try again, and the logger would collect the next two values, log into your account, remove the authenticator and change your password.By the time you typed in the 4th attempt to log in, you&#x27;d already be locked out of your account, and by the time you called support, they would already have laundered your stuff.This was targeting 10 million people for their imaginary money and a fraction of their imaginary goods. On the one hand that&#x27;s a lot of effort for a small payoff. On the other, maybe the fact that it was so ridiculous insulated them from FBI intervention. If they were doing this to banks they&#x27;d have Feds on them like white on rice. But it definitely is a proof of concept for something much more nefarious. reply bawolff 12 hours agorootparentprevNo it can&#x27;t.The rouge javascript or keylogger would just steal the totp code, prevent the form submission, and submit its own form on the malicious person&#x27;s server.Not to mention if your threat model includes attacker has hacked the server and added javascript, why doesn&#x27;t the attacker just take over the server directly?If the attacker installed a keylogger why dont they just install software to steal your session cookies?This threat model doesn&#x27;t make sense. It assumes a powerful attacker doing the hard attack and totally ignoring the trivially easy one. reply timando 11 hours agorootparent> Not to mention if your threat model includes attacker has hacked the server and added javascript, why doesn&#x27;t the attacker just take over the server directly?If the attacker can only hack the server that hosts your SPA, but not your API server, they can inject javascript to it, but can&#x27;t do a lot beyond that reply bawolff 11 hours agorootparentSo assuming server side compromise not xss - in theory the servers can be isolated, in practise its rare for people to do a good job with this except at really big companies.Regardless if they got your spa, they can replace the html, steal credentials, act as users, etc. Sure the attacker might want something more, but this is often more than enough to do anything the attacker might want if they are patient enough. Certainly its more than enough to do anything TOTP would protect against. reply thayne 11 hours agorootparentprev> attacker has hacked the server and added javascriptadding javascript doesn&#x27;t necessarily mean the server is hacked. XSS attacks usually don&#x27;t require actually compromising the server. Or a malicious browser plugin could inject javascript onto a site. reply hinkley 11 hours agorootparentprevrogue javascript. It&#x27;s naughty, not red. reply cottsak 7 hours agorootparentprevgreat insight:> in practise the only one it really solves for most setups is people choosing bad passwords or reusing passwords on other insecure sites. Pretty much every other threat model for it is wishful thinking.Why is no one talking about this? reply bawolff 2 hours agorootparentThe other side of this is that (to pull numbers out of my hat) 90% of non-targeted attacks are password reuse and 9.9% are phishing with 0.1% being something else. The fact that TOTP doesn&#x27;t solve phishing does get talked about.Ultimately totp & sms based 2fa is used because it solves the real business problem that websites face (the business problem being when enough users get hacked they blame the business not themselves, so we just need to save most of them not all). Yes there is some fear mongering to make people sign up for 2FA, but it is actually solving a big problem effectively. It doesn&#x27;t matter its not helpful in more fanciful scenarios since those scenarios are largely imaginary to begin with (for the average user). reply softfalcon 13 hours agoparentprev>FIDO2 can also detect the domain making the request, and will not provide the correct code even it the page looks correct to a human.I could not agree more with this sentiment! We need more of this kind of automated checking going on for users. I&#x27;m tired of seeing \"just check for typo&#x27;s in the URL\" or \"make sure it&#x27;s the real site!\" advice given to the average user.People are not able to do this even when they know how to protect themselves. Humans tire easily and are often fallible. We need more tooling like FIDO2 to automate away this problem for us. I hope the adoption of it will go smoothly in years to come. reply miki123211 12 hours agorootparentThe problem with Fido (and other such solutions, including smartphone-based passkeys) is that they make things extremely hard if you&#x27;re poor &#x2F; homeless &#x2F; in an unsafe &#x2F; violent family situation and therefore change devices often. It&#x27;s mostly a non-issue for Silicon Valley tech employees working solely on their corporate laptops, and U2F is perfect for that use-case, but these concerns make MFA a non-starter for the wider population. We could neatly sidestep all of these issues with cloud-based fingerprint readers, but the privacy advocates won&#x27;t ever let that happen. reply luma 12 hours agorootparentBiometrics aren’t a great key because they cannot generally be revoked. This isn’t a privacy concern, it’s a security problem. You leave your fingerprints nearly everywhere you go, and they only need to be compromised once and then can never be used again. At best, you can repeat this process a sum total of 10 times without taking your shoes off to login. reply netik 3 hours agorootparentI’ve always referred to biometrics as a “non revokable username” and not a “password.”100% agree with you here. reply softfalcon 12 hours agorootparentprevYou&#x27;re right, software security is only really available to rich and tech minded folks.That&#x27;s kind of what I was trying to get at with my previous statement about humans being tired and fallible. The way we access and protect our digital assets feels incredibly un-human to me. It&#x27;s wrapped up in complexity and difficulty that is forced upon the user (or kept away from, if you want to look at it that way).As it is now, all of the solutions are only really available to someone who can afford it (by life circumstance, device availability, internet, etc) and those who can understand all the rules they have to play by to be safe. It&#x27;s a very un-ideal world to live in.When I brought up FIDO2, I was less saying \"FIDO2 is the answer\" and more saying, \"we need someone to revolutionize the software authentication and security landscape because it is very very flawed\". reply supertrope 11 hours agorootparentprevStronger security can also help the marginalized. If your abusive SO has the phone plan in their name they can order up a new SIM card and reset passwords on websites that way too often fallback from “two factor” to SMS as a root password. reply adamckay 12 hours agoparentprev> I&#x27;m surprised Google encourages syncing the codes to the cloud... kind of defeats the purposeProbably so when you upgrade&#x2F;lose your phone you don&#x27;t otherwise lose your MFA tokens. Yes, you&#x27;re meant to note down some recovery MFA codes when you first set it up, but how many \"normal people\" do that? reply aeyes 7 hours agorootparentWith Google Authenticator some years ago it wasn&#x27;t even possible to restore your codes even if you had a local backup of the device. I&#x27;m not sure if that still is the case today but it was a common issue which we saw at our service desk before we switched to a different solution. reply SoftTalker 7 hours agorootparentYeah I had to re-enroll my phone when I got a new one a few years ago.I never did get around to doing all of them so I still have the old phone in a drawer for those rare times I need it. reply Master_Odin 11 hours agorootparentprevA number of sites I&#x27;ve signed up for recently have required TOTP to be setup, but did not provide back up codes at the same time. There&#x27;s a lot of iffy implementations out there. reply cottsak 7 hours agorootparentgross reply Guvante 13 hours agoparentprevOn the otherhand having your device die means without cloud backup you either lose access or whoever was relying on that 2FA needs to fall back on something else to authenticate you.After all if I can bypass 2FA with my email whether 2FA is backed up to the cloud doesn&#x27;t matter from a security standpoint.Certainly I would agree with the assertion that opting out for providers of codes would be nice. Even if it is an auto populated checkbox based on the QR code. reply pushcx 12 hours agorootparentThe workaround I&#x27;ve seen is to issue a user two 2FAs keys, one for regular use and one to store securely as a backup. If they lose their primary key, they have the backup until a new backup can be sent to them. Using a backup may prompt partial or total restriction until a security check can be done. If they lose both, yes, there needs to be some kind of a reauth. In workplace context like this it&#x27;s straightforward to design a high-quality reauth procedure. reply andersa 10 hours agorootparentprevThey could do what Authy does. Codes are backed up to the cloud, so you&#x27;re not completely fucked if the phone is stolen. But the backup is encrypted, and to access it on a replacement device you must enter the backup password. reply toast0 8 hours agorootparentThat relies on someone remembering their backup password that they probably don&#x27;t use often. reply mannykannot 6 hours agorootparentI suspect that this sort of issue is the real reason for making it difficult to not back up secrets to the cloud. On the one hand, you will have some number of people pissed off because they were taken advantage of and they realize that it was enabled by having backups in the cloud. On the other, you have people pissed off because they couldn&#x27;t manage the final step in keeping their shit secure and are now locked out of something. The number in the latter category is vastly larger than the number in the former. reply netik 3 hours agorootparentprevAuthy makes the user enter this on a periodic basis to refresh their memory, which is a good thing imho reply duderific 12 hours agoparentprevIn my company, such a communication would never come via a text, so that would be a red flag immediately. All such communications come via email, and we have pretty sophisticated vetting in place to ensure that no such \"sketchy\" emails even arrive in our inboxes in the first place.Additionally, we have a program in place which periodically \"baits\" us with fake phishing emails, so we&#x27;re constantly on the lookout for anything out of the ordinary.I&#x27;m not sure what the punishment is for clicking on one of these links in a fake phishing email, but it&#x27;s likely that you have to take the security training again, so there&#x27;s a strong disincentive in place. reply rainsford 12 hours agorootparentAfter initially thinking it was a good idea, I&#x27;ve come to disagree pretty strongly with the idea of phish baiting employees. Telling employees not to click suspicious links is fine, but taking a step further to constantly \"testing\" them feels like it&#x27;s placing an unfair burden on the employee. As this attack makes clear, well done targeted phishing can be pretty effective and hard for every employee to detect (and you need every employee to detect it).Company security should be based on the assumption that someone will click a phishing link and make that not a catastrophic event rather than trying to make employees worried to ever click on anything. And has been pointed out, that seems a likely result of that sort of testing. If I get put in a penalty box for clicking on fake links from HR or IT, I&#x27;m probably going to stop clicking on real ones as well, which doesn&#x27;t seem like a desirable outcome. reply wayfinder 11 hours agorootparentEvery company I’ve worked with has phish baited employees and I’ve never had any problem. It keeps you on your toes and that’s good.What happened in the article — getting access to one person’s MFA one time — is not exactly a catastrophic event. It just happens, as with most security breaches, a bunch of things happened to line up together at one time to make intrusion possible. (And I skimmed the article but it sounded like the attacker didn’t get that much anyway, so it was not catastrophic.)And things lining up rarely happens but it will happen enough times for there to be an article posted to Hacker News once in a while with someone saying that it’s possible to make it perfectly secure. reply duderific 10 hours agorootparentprev> constantly \"testing\" them feels like it&#x27;s placing an unfair burden on the employee.Meh, it&#x27;s not that disruptive, maybe one email every couple of months.> Company security should be based on the assumption that someone will click a phishing link and make that not a catastrophic event rather than trying to make employees worried to ever click on anything.Agreed. I think both things are important: keeping employees on their toes, which reduces the possibility of a successful attack, as well as making it not catastrophic if a phishing attack succeeds. reply victor106 7 hours agoparentprevThey could’ve just had employees use Okta Verify as opposed to Google Authenticator reply rakkhi 12 hours agoparentprevSophisticated... okI mean it&#x27;s a great reason to use U2F &#x2F; Webauthn second factor that cannot be entered into a dodgy sitehttps:&#x2F;&#x2F;rakkhi.substack.com&#x2F;p&#x2F;how-to-make-phishing-impossibl... reply halfcat 11 hours agoparentprev> I sync my TOTP between devices using an encrypted backup, even if someone got that file they could not use the codes.What do you use to accomplish this? reply mos_basik 10 hours agorootparentNot OP, but I store my TOTP secrets along with all my other passwords in a KeePass database and sync the encrypted database to my devices with Dropbox. All the clients I use to open a KeePass database can generate TOTP codes from the secrets at this point, so I don&#x27;t use a dedicated TOTP app like Google Authenticator or Authy anymore.Not multifactor anymore, but also not vulnerable to catastrophic phone destruction or Google account banning. It is what it is. reply fn-mote 11 hours agorootparentprevAfter the sync, you have exactly two devices that you can use to answer the MFA challenge, instead of one. It&#x27;s a backup. reply hn_throwaway_99 12 hours agoparentprev> Very sophisticated attack, I would bet most people would fall for this.No. If you think people at your company would fall for this, then IMO you have bad security training. The simple mantra of \"Hang up, lookup, call back\" (https:&#x2F;&#x2F;krebsonsecurity.com&#x2F;2020&#x2F;04&#x2F;when-in-doubt-hang-up-lo...) would have prevented this.Literally like 99% of social engineering attacks would be prevented this way. Seriously, make a little \"hang up, look up, call back\" jingle for your company. Test it frequently with phishing tests. It is possible in my opinion to make this an ingrained part of your corporate culture.Agree that things like security keys should be in use (and given Retool&#x27;s business I&#x27;m pretty shocked that they weren&#x27;t), but there are other places that the \"hang up, look up, call back\" mantra is important, e.g. in other cases where finance people have been tricked into sending wires to fraudsters. reply pvg 12 hours agorootparentThe ineffectiveness of \"security training\" is precisely why TOTP is on its way out - you couldn&#x27;t even train Google employees to avoid getting compromised. reply hn_throwaway_99 12 hours agorootparentIMO most of this is because most security training I&#x27;ve seen is abysmal. It&#x27;s usually a \"check the box\" exercise for some sort of compliance acronym. And, because whatever compliance frameworks usually mandate hitting lots of different areas, it basically becomes too much information that people don&#x27;t really process.That&#x27;s why I really like the \"Hang up, look up, call back\" mantra: it&#x27;s so simple. It shouldn&#x27;t be a part of \"security training\". If corporations care about security, it should be a mantra that corporate leaders begin all company-wide meetings with. It&#x27;s basically teaching people to be suspicious of any inbound requests, because in this day and age those are difficult to authenticate.In other words, skip all the rest of \"security training\". Only focus on \"hang up, look up, call back\". Essentially all the rest of security training (things like keeping machines up to date, etc.) should be handled by automated policies anyway. And while I agree TOTP is and should be on its way out, the \"hang up, look up, call back\" mantra is important for requests beyond just things like securing credentials. reply pvg 10 hours agorootparentIt&#x27;s not just because it&#x27;s abysmal, it&#x27;s because it was found, empirically, not to work, no matter how good you make it. The mitigation you&#x27;re describing is also susceptible to lapses and social engineering, just like what got them into trouble in the first place.The simpler mitigation of &#x27;the target employee with with the Google account full of auth secrets should have had it U2F protected&#x27; would have worked even if the phone person had just read out the target&#x27;s Google password to anyone who called and asked for it.They could have enforced that with a checkbox in their GSuite admin console. reply rainsford 9 hours agorootparentprevBut aside from beating employees over the head with it, how many companies actually operate in a way that encourages and reinforces such an approach? I&#x27;d bet it&#x27;s not many, and honestly if it&#x27;s a non-zero number I&#x27;d be at least a bit surprised.You can have all the security training in the world, but every time IT or HR or whoever legitimately reaches out to an employee, especially when it&#x27;s not based on something initiated by the employee, the company is training exactly the opposite behavior Krebs is suggesting. Hanging up and calling back will likely at minimum annoy the caller and inconvenience the employee. Is the company culture accepting of that, or even better are company policies and systems designed to avoid such a scenario? If a C-suite person calls you asking for some information and you hang up and call them back, are they going to congratulate you on how diligently you are following your security training?You&#x27;re not wrong that the Krebs advice would help prevent most phishing, but I&#x27;d argue it has to be an idea you design your company around, not just a matter of security training. Otherwise you&#x27;re putting the burden on employees to compensate for an insecure company, often at their own cost. reply yesimahuman 12 hours agorootparentprevThis fails to satisfy one of the core lessons here: trust nothing, not even your own training and culture. reply SoftTalker 7 hours agorootparentReminds me of a situation early in my career where I was talking with the CTO about some security concern and I said \"well it&#x27;s all on the internal company network\" and he immediately said \"why on earth do you think you can trust our internal network?\" reply hooverd 9 hours agorootparentprevEspecially not the information security team. They&#x27;re the most likely to be compromised. reply _jal 12 hours agorootparentprevSo I take it you are employed by someone that allows you to connect to nothing and change nothing? Because if you can do any of those things, your employer is clearly Doing It Wrong, based on your interpretation.(If you happen to be local-king, flip the trust direction, it ends up in the same place.) reply roywiggins 11 hours agorootparentprevThey just have to catch someone half-awake, or already very stressed out, or otherwise impaired once. reply devjab 11 hours agorootparentprevI’ve done 6 different versions of “security training” as well as “GDPR training” over the past few years. I think they are mostly tools to drain company money and wasting time. About the only thing I remember from any of it is when I got some GDPR answer wrong because I didn’t resize your shoe size was personal information and it made me laugh that I had failed the whatever quiz right after I had been GDPR certified by some other training tool.If we look at the actual data, we have seen a reduction in employees who fall for phishing emails. Unfortunately we can’t really tell if it’s the training or if it’s the company story about all those million that got transferred out of the company when someone fell for a CEO phishing scam. I’m inclined to think it’s the latter considering how many people you can witness having the training videos run without sound (or anyone paying attention) when you walk around on the days of a new video.The only way to really combat this isn’t with training and awareness it’s with better security tools. People are going to do stupid things when they are stressed out and it’s Thursday afternoon, so it’s better to make sure they at least need a MFA factor that can’t be hacked as easily as SMS, MFA spamming and so on. reply hn_throwaway_99 11 hours agorootparentTo emphasize, I 100% agree with you. I&#x27;m not arguing for more security training, I&#x27;m arguing for less.\"Hang up, look up, call back\". That&#x27;s it. Get rid of pretty much all other \"security training\", which is just a box ticking exercise for most people anyway.I also agree with the comment about better security tools, but that&#x27;s why I think \"hang up, look up, call back\" is still important, because it teaches people to be fundamentally suspicious of inbound requests even in ways where security tools wouldn&#x27;t apply. reply devjab 4 hours agorootparentThen I guess we agree! My mantra is to just never click on any links. Even when I know they aren’t phishing I don’t click on them.Of course that’s probably easier for a programmer than most other employees. I’m notoriously hard to reach unless it’s through a user story on our board that’s first been vetted by a PO. Something you probably can’t get away with if you’re not privileged enough by being in an in-demand role where they can’t just replace you by someone more compliant. I do try not to be an asshole about it, but we have soooooo many fake phishing mails and calls from our cyber awareness training that it’s just gotten to the point where it’s almost impossible to get trapped by one unless you ignore things until someone shows up in person. Luckily one of my privacy adding in FireFox prevented me from getting caught when I actually did click one of the training links on one of those famous Thursday afternoons. So I still don’t have the “you’ve clicked a phishing link” achievement… which I’m still not sure why is there, because I sort of want it now that it is, and eventually that urge is going to win. replytptacek 9 hours agoprevWe use OTPs extensively at Retool: it’s how we authenticate into Google and Okta, how we authenticate into our internal VPN, and how we authenticate into our own internal instances of RetoolThey should stop using OTPs. OTPs are obsolete. For the past decade, the industry has been migrating from OTPs to phishing-proof authenticators: U2F, then WebAuthn, and now Passkeys†. The entire motivation for these new 2FA schemes is that OTPs are susceptible to phishing, and it is practically impossible to prevent phishing attacks with real user populations, even (as Google discovered with internal studies) with ultra-technical user bases.TOTP is dead. SMS is whatever \"past dead\" is. Whatever your system of record is for authentication (Okta, Google, what have you), it needs to require phishing-resistant authentication.I&#x27;m not high-horsing this; until recently, it would have been complicated to do something other than TOTP with our service as well (though not internally). My only concern is the present tense in this post about OTPs, and the diagnosis of the problem this post reached. The problem here isn&#x27;t software custody of secrets. It&#x27;s authenticators that only authenticate one way, from the user to the service. That&#x27;s the problem hardware keys fixed, and you can fix that same problem in software.† (All three are closely related, and an investment you made in U2F in 2014 would still be paying off today.) reply corford 26 minutes agoparentFor others new to WebAuthn and Passkeys (like me), worth noting that the privacy&#x2F;ease-of-use trade-offs for Passkeys are somewhat contentious. Here&#x27;s an easily digestible summary of why: https:&#x2F;&#x2F;blog.passwordless.id&#x2F;webauthn-vs-passkeys.Though will hopefully be mitigated somewhat when more password managers like Bitwarden start supporting storing passkeys (apparently their support is coming in October: https:&#x2F;&#x2F;bitwarden.com&#x2F;passwordless-passkeys&#x2F;) reply drx 8 hours agoparentprevWhat would be your recommendation for replacing TOTP today? reply akerl_ 8 hours agorootparentFIDO2 reply rahidz 13 hours agoprev>The caller claimed to be one of the members of the IT team, and deepfaked our employee’s actual voice. The voice was familiar with the floor plan of the office, coworkers, and internal processes of the company.Wow that is quite sophisticated. reply oldtownroad 12 hours agoparentAnd obviously untrue. If you’re an employee who just caused a security incident of course you’re going to make it seem as sophisticated as possible but considering Retool has hundreds of employees from all over the world, the range of accents is going to be such that any voice will sound like that of at least one employee.Are you close enough to members of your IT team to recognise their voices but not be close enough to them to make any sort of small talk that the attacker wouldn’t be able to respond to convincingly?If you’re an attacker who can do a convincing french accent, pick an IT employee from LinkedIn with a french name. No need to do the hard work of tracking down source audio for a deepfake when voices are the least distinguishable part of our identity.Every story about someone being conned over the phone now includes a line about deepfakes but these exact attacks have been happening for decades. reply luma 12 hours agorootparentFully agreed, saying a deepfaked voice was involved without hard proof is deflecting blame by way of claiming magic was involved. reply yieldcrv 12 hours agorootparentI think its right to be skeptical, but its also easy to do this if you’ve identified the employee to train on the voice of. You could even call them and get them to talk for a few minutes if you couldnt find their instagram. reply skeaker 11 hours agoparentprevHighly reminiscent of the sort of social engineering hacks Mitnick would run. In his autobiography he would pull this sort of thing by starting small and simply asking lower ranking employees over the phone for low risk info like their name and things like that so when it came time to call higher ranking ones he could have trustworthy-sounding info to call back to. The attack is clever for sure, but not necessarily any more sophisticated than multiple well-placed calls. reply bombcar 12 hours agoparentprevSophisticated enough that I’d just suspect the employee unless there was additional proof. reply tough 13 hours agoparentprevinside job? reply dmazzoni 11 hours agorootparentAnything&#x27;s possible, but the simplest explanation (per Occam&#x27;s razor) is just that the employee was fooled.Is it plausible that if a good social engineer cold-called a bunch of employees, they&#x27;d eventually get one to reveal some info? Yes, it happens quite frequently.So any suggestion that it was an inside job, or used deep fakes, or something like that would require additional evidence.Kevin Mitnick&#x27;s \"The Art of Deception\" covers this extensively. The first few calls to employees wouldn&#x27;t be attempts to actually get the secret info, it&#x27;d be to get inside lingo so that future calls would sound like they were from the inside.For example, the article says the caller was familiar with the floor plan of the office.The first call might be something like \"Hey, I&#x27;m a new employee. Where are the IT staff, are they on our floor?\" - they might learn \"What do you mean, everyone&#x27;s on the 2nd floor, we don&#x27;t have any other floors. IT are on the other side of the elevators from us.\"They hang up, and now with their next call they can pretend to be someone from IT and say something about the floor plan to sound more convincing. reply mistrial9 13 hours agorootparentprevhow&#x27;s that Zero Trust architecture working out for everyone ? reply Wojtkie 12 hours agorootparentThey mention in the article that their zero-trust architecture is what prevented the attacker from gaining access to on-prem data. So it seemed like it worked pretty well in mitigating the damage. reply tlrobinson 1 hour agorootparentI&#x27;m curious if they actually mean \"Zero trust\" in the \"perimeterless\" sense (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Zero_trust_security_model) or if they just mean their on-prem solution doesn&#x27;t require trusting some central service operated by Retool. reply mousetree 1 hour agorootparentThe latter reply wepple 13 hours agorootparentprevWhat’s this got to do with zero trust? reply mistrial9 13 hours agorootparentit is a cynical comment that is meant to hilite the relationship between humans where oppressive and untrusting employment leads to increase in antipathy, ill-will, feelings of being abused and all of that leading to insider theft and serious pre-meditated betrayal ? reply wyldberry 12 hours agorootparentZero Trust is such a bad branding for how the architecture works. It&#x27;s just \"always prove\" architecture. reply tough 12 hours agorootparentIt does seem to sound pretty well on the mind of the executives signing the deals that hear the marketing talk replybatmansmk 12 hours agoprevAre the claims of deepfake and intimate knowledge of procedures based of the sole testimony of the employee who oopsed terribly? This is a novelisation of an eventsRetool needs to revise the basic security posture. There is no point in complicated technology if the warden just gives the key away. reply hn_throwaway_99 12 hours agoparent> Retool needs to revise the basic security posture.Couldn&#x27;t agree more. TBH I thought this post was an exercise in blame shifting, trying to blame Google.> We use OTPs extensively at Retool: it’s how we authenticate into Google and Okta, how we authenticate into our internal VPN, and how we authenticate into our own internal instances of Retool. The fact that access to a Google account immediately gave access to all MFA tokens held within that account is the major reason why the attacker was able to get into our internal systems.Google Workspace makes it very easy to set up \"Advanced Protection\" on accounts, in which case it requires using a hardware key as a second factor, instead of a phishable security code. Given Retool&#x27;s business of hosting admin apps for lots of other companies, they should have known they&#x27;d be a prime target for something like this, and not requiring hardware keys is pretty inexcusable here. reply dotty- 12 hours agorootparent> Google Workspace makes it very easy to set up \"Advanced Protection\" on accounts, in which case it requires using a hardware key as a second factor, instead of a phishable security code.This isn&#x27;t immediately actionable for every company. I agree Retool should have hardware keys given their business, but at my company with 170 users we just haven&#x27;t gotten around to figuring out the distribution and adoption of hardware keys internationally. We&#x27;re also a Google Workspace customer. I think it&#x27;s stupid for a company like Google, the company designing these widely used security apps for millions of users, to allow for cloud syncing without allowing administrators the ability to simply turn off the feature on a managed account. Google Workspace actually lacks a lot of granular security features, something I wish they did better.What is a company like mine meant to do here to counter this problem?edit: changed \"viable\" for \"immediately actionable\". It&#x27;s easy for Google to change their apps. Not for every company to change their practices. reply hn_throwaway_99 12 hours agorootparent> What is a company like mine meant to do here to counter this problem?What is hard about mailing everyone a hardware key? I honestly don&#x27;t see the problem. It&#x27;s not like you need to track it or anything, people can even use their own hardware keys.1. Mail everyone a hardware key, or tell them if they already have one of their own they can just use that.2. Tell them to enroll at https:&#x2F;&#x2F;landing.google.com&#x2F;advancedprotection&#x2F;> Google Workspace actually lacks a lot of granular security features, something I wish they did better.Totally agree with that one. Last time I checked you couldn&#x27;t enforce that all employees use Advanced Protection in a Google Workspace account. However, you can still get this info (enabled or disabled) as a column in the Workspace Admin console so you can report on people who don&#x27;t have it enabled. I&#x27;m guessing there is also probably a way to alert if it is disabled. reply toast0 7 hours agorootparentI can&#x27;t tell you how happy I am that I don&#x27;t have to fight with Google Workspace administration anymore. When I was doing it, getting TOTP enforcement enabled was very problematic. You couldn&#x27;t just set the org to be enforced, because new users wouldn&#x27;t be able to login, and then you&#x27;d have to turn it off for the org any day new people started, then make sure that everybody was enrolled (including existing employees that turned it off while they could), etc.They finally fixed it, but it took them a long time, and in the meantime, horrible workarounds.They also had no way of merging two company&#x27;s accounts; which is fine because m&a never happens, and google never aquires anyone using google workspace (i certainly would refuse to be aquired by them after using their software, but I&#x27;m extra grumpy) reply dvdhsu 12 hours agoparentprevIt is not based on the sole testimony of the employee. (Sorry I can&#x27;t go into more details.) reply dmazzoni 11 hours agoparentprevEmployees are only human. Even smart, savvy, well-trained employees can be fooled by good social engineering every once in a while.The key to good security is layering. Attackers should need to break through multiple layers in order to get access to critical systems.Compromising one employee&#x27;s account should have granted them only limited access. The fact that this attack enabled them to get access to all of that employee&#x27;s MFA tokens sounds like indeed the right thing to focus on. reply brunojppb 13 hours agoprevFantastic write-up. Major props for disclosing the details of the attack in a very accessible way.It is great that this kind of security incident post-mortem is being shared. This will help the community to level-up in many ways, specially given that its content is super accessible and not heavily leaning on tech jargon. reply hn_throwaway_99 12 hours agoparentI disagree. I appreciate the level of detail, but I don&#x27;t appreciate Retool trying to shift the blame to Google, and only putting a blurb in the end about using FIDO2. They should have been using hardware keys years ago. reply dvdhsu 12 hours agorootparentHi, I&#x27;m sorry you felt that way. \"Shifting blame to Google\" is absolutely not our intention, and if you have any recommendations on how to make the blog post more clear, please do let me know. (We&#x27;re happy to change it so it reads less like that.)I do agree that we should start using hardware keys (which we started last week).The goal of this blog post was to make clear to others that Google Authenticator (through the default onboarding flow) syncs MFA codes to the cloud. This is unexpected (hence the title, \"When MFA isn&#x27;t MFA\"), and something we think more people should be aware of. reply hn_throwaway_99 11 hours agorootparentI felt like you were trying to shift blame to Google due to the title \"When MFA isn&#x27;t MFA\" and your emphasis on \"dark patterns\" which, to be honest, I don&#x27;t think they are that \"dark\". To me it was because this felt like a mix of a post mortem&#x2F;apology, but with some \"But if it weren&#x27;t for Google&#x27;s dang dark patterns...\" excuse thrown in.FWIW, nearly every TOTP authenticator app I&#x27;m aware of supports some type of seed backup (e.g. Authy has a separate \"backup password\"). I actually like Google&#x27;s solution here as long as the Workspace accounts are protected with a hardware key.The only real lesson here is that you should have been using hardware keys. reply deepspace 11 hours agorootparentprevThere is also the bit about the phishers deep-faking an employee&#x27;s voice. Yeah, right. That happened. &#x2F;sarcasm reply hn_throwaway_99 10 hours agorootparentIn fairness dvdhsu responded about that point elsewhere: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37502239 reply darkerside 11 hours agorootparentprevThis comment reads more poorly to me than the actual blog post. It _should_ be your intention to shift partial blame to Google, and you should own it. It&#x27;s ridiculous that they make an operation like syncing your MFA keys seem so innocuous. I just changed phones, so I&#x27;m just seeing this user flow for the first time, and it is ghastly how they&#x27;ve made it the default path.Changing things to make it less offensive to someone who was offended really waters down your position. reply duderific 12 hours agorootparentprevIt was also a bit weird how they kept emphasizing how their on-prem installations were not affected, as if that lessens the severity somehow. It&#x27;s like duh, that&#x27;s the whole point of on-prem deployments. reply AYBABTME 13 hours agoprevTo deepfake the voice of an actual employee, they would need enough recorded content of that employee&#x27;s voice... and I would think someone doing admin things on their platform isn&#x27;t also in DevRel with a lot of their voice uploaded online for anyone to use. So it smells like someone with close physical proximity to the company would be involved. reply gabereiser 12 hours agoparentThere’s a lot of ways to get clips of recordings of someone’s voice. You can get that if they ever spoke at a conference or on a video. Numerous other ways I won’t list here. reply V__ 12 hours agoparentprevOne possibility would be to just call the employee and record their voice. One could pretend to be a headhunter. reply skeaker 11 hours agorootparentThis would almost certainly be it. Calling someone to record them and using their voice later to impersonate them was done even before deep-fake voices were a concept. With the tools available now, even a short call + the grainy connection of a phone voice line would be more than enough to make a simulated voice work. reply rlt 1 hour agorootparentprevI&#x27;m already cautious about answering calls from unknown numbers. This could be a good reason to be even more cautious. reply themagician 12 hours agoparentprevProbably wasn&#x27;t a \"deepfake\" just someone decent with impressions and a $99 mixer. After compression this will be more than good enough to fool just about anyone. No deepfake is needed. Just call the person once and record a 30 second phone call. Tell them you are delivering some package and need them to confirm their address. reply deepspace 10 hours agorootparentThat is more plausible. But the embellishment about the phisher knowing the layout of the office etc makes me think it was just straight up an inside job, with the employee willingly handing over the OTP and trying to cover their tracks. reply out-of-ideas 9 hours agoprev> The caller claimed to be one of the members of the IT team, and deepfaked our employee’s actual voice. The voice was familiar with the floor plan of the office, coworkers, and internal processes of the company.huh.. this raises way more questions than it answers; my first two are: - how did the voice of some random employee (in IT for that matter) get learned by outside the company (enough to be deepfaked (and i presume on the fly) for that matter)? Maybe we should record less conversations (looks at Teams, Discord, Zoom) - where there already leaks of &#x27;internal processes&#x27;? reply tamimio 11 hours agoprevYou know how I never get phished? I never answer any call or sms asking anything, and a link in a text message is ALWAYS a major red flag. I know everyone is talking about the MFA, but the entry point was the employees phone numbers, how they got that in the first place? Especially from the article the attacker knew the internals of this company..As for the MFA, google should have the on demand peer-peer sync rather than cloud save, for example, a new device is added, then your Google account is used to link between these new device and existing device, click sync and you will be asked on your old device that a new device is requesting bla bla would you allow it? And obviously nothing saved in the cloud, just a peer-peer sync and google is a connection broker. reply crabbone 9 hours agoprevMFA is a scam resulting from Google first, and then others wanting to get users&#x27; phone numbers associated with more data they collect on them. It provides no tangible security benefits, creates a lot of headache for IT department, creates big gaps in developer&#x27;s productivity (if used in a programming company) and, actually, creates a new attack vector (phones are lost or stolen a lot more often than any other means of authentication).Since Github now requires MFA, I&#x27;m throwing away my account: I&#x27;ll never give them any physical evidence to connect me to other data they have on me.In the company I work today (20-something thousands employees) the latest security breach was through MFA. Data was stolen. Perpetrators made jokes in company&#x27;s Slack etc.Last time I had to upgrade my phone (while working for the same company), it took IT about two weeks to give me all the necessary access again, which required a lot of phone calls, video conferences, including my boss and my boss&#x27; boss.It&#x27;s mind-boggling that this practice became the norm and is recommended by IT departments even of companies who have nothing to gain from collecting such data. reply Terretta 9 hours agoparentI don&#x27;t understand:> Google first, and then others wanting to get users&#x27; phone numbers associated with more data they collect on themPerhaps you mean SMS 2FA, instead of a non phone number related MFA such as T-OTP? reply crabbone 30 minutes agorootparentGoogle were sued because they were selling to advertisers information about Android users that other advertising platforms couldn&#x27;t possibly had. Advertisement data s.a. user preferences, their history of clicking on ads, browsing history etc would all be organized by the id derived from Android device. Once the court decided they cannot do that, and users should opt in to be tracked, they promptly created MFA that relied on collecting data about physical devices. Which they then again used to sell advertisement data.The whole point of this exercise is not to enhance security, but to have an edge as an advertisement platform. If today you can trick the system into not using a phone, it&#x27;s a temporary thing. The more users join, the tighter will be the system&#x27;s grip on each individual user, and the \"privilege\" of not divulging your phone number will be taken away.Google did this before with e-mail access for example, multiple times, actually. Remember how GoogleTalk used Jabber? -- Not having to use a proprietary chat protocol was a feature that made more users join. As soon as there were enough users, they replaced GoogleTalk with Hangouts or w&#x2F;e it&#x27;s called.GMail used to provide standard SMTP &#x2F; IMAP access, but they continuously undermined all clients other than Google&#x27;s. Started with removing POP access. Then requiring mandatory TLS. Then requiring a bunch of nonsense \"trusted application registration\". Finally, this feature is now behind MFA, which makes it useless anywhere outside Google&#x27;s Web client &#x2F; Android app etc. All of this was delivered as a \"security improvements\", while giving no tangible security benefits. It was a move to undermine competition. reply odux 9 hours agoparentprevWait, what? MFA - multi factor authentication- has existed long before Google was founded. RSA Securid tokens were introduced in the 1980s or so.MFA is a easy and good way to prevent hostile account takeovers. Especially with the amount of data breaches, one time passwords are way more secure than memorized “static” passwords.SMS based two factor is the one Google pushed. Even Google recommends other ways of MFA these days (using hardware like YubiKey or apps like Authy).Public’s phone numbers are not that valuable for a company like Google . Until very recently they were listed in phone books publicly available. reply crabbone 20 minutes agorootparentMFA in its current form owes its existence to a lawsuit filed against Google being a monopoly on Android when packaging and selling advertisement data to ad campaign management companies.It&#x27;s not about being able to tie your phone number to your name. It&#x27;s about being able to tie your browsing, purchasing, and other behavior history to an id that doesn&#x27;t change much.Google by itself doesn&#x27;t run ad campaigns. It sort of has API to design a campaign yourself... but that&#x27;s super ineffective. There are multiple companies who manage ad campaigns which run on Google. In order to be effective they need to have some predictive power over user&#x27;s future browsing, purchasing etc. choices. Being able to consistently identify the user (and tie that to their history) is the most valuable ad-related info anyone can sell.Whatever existed in the 80s has nothing to do with MFA is today. Today it&#x27;s a scam that helps big tech companies who want to be an advertisement platform to harvest and to catalogue data helping advertisers predict user behavior. All it does to end users is inconvenience and less security. All it does to IT is an extra headache and more procedures that may potentially go wrong. reply acegopher 9 hours agoparentprevI use 1Password&#x27;s authenticator, so no-one needs my phone number, and I don&#x27;t have to worry about losing my phone, as there is a Linux CLI, a browser extension, etc. reply crabbone 17 minutes agorootparentYou forgot to add: for now.There is no genuine interest on the other side to provide you with better security. There is no genuine interest on the other side to make your life easier &#x2F; to care about your privacy. You are allowed to opt out through a complicated mechanism because the provider needs high volume of users. As time goes, either the law will catch up to the provider and will make them make mandatory exceptions to this nonsense, or they will just exploit you whichever way they can. reply u801e 10 hours agoprevUnfortunately, MFA has become synonymous with SMS, email, and OTP. All of these methods require sharing a secret between two parties without any way to verify the authenticity of either party.Key based authentication where both parties have private keys that are not shared is a much better alternative. Unfortunately, client side TLS certificates, which are application level protocol agnostic, never really caught on. reply est31 10 hours agoparentThere is U2F&#x2F;FIDO keys &#x2F; passkeys which are what you describe, latter just very recently becoming widely available. When&#x2F;if they become successful is another question. U2F&#x2F;FIDO etc keys are only supported by a subset of websites. reply eddythompson80 10 hours agoparentprevClient side tls certificates are the worst of all the options. They only make sense fully managed or for server to server communication. reply bawolff 12 hours agoprevWhile the google cloud thing is a weird design, that seems like the wrong place to blame.TOTP and SMS based 2FA are NOT designed to prevent phishing. If you care about phishing use yubikeys. reply Thorrez 3 hours agoprevI don&#x27;t think it&#x27;s really accurate to describe it as not MFA. The attacker phished a password and 2 TOTP codes. So the attacker phished 3FA.So yes, Google Authenticator sync made the security worse, but it didn&#x27;t downgrade the security from MFA to non-MFA. And even if the sync was off, the TOTP codes in Google Authenticator could have been phished as well, so Google Authenticator can&#x27;t be blamed so heavily, because the attack could have been done without it.Disclosure: I work at Google but not on Google Authenticator. reply andrewstuart 12 hours agoprevSome startup, please make a product that uses AI to identify these obviously fake emails.Hello A, This is B. I was trying to reach out in regards to your [payroll system] being out of sync, which we need synced for Open Enrollment, but i wasn’t able to get ahold of you. Please let me know if you have a minute. ThanksYou can also just visit https:&#x2F;&#x2F;retool.okta.com.[oauthv2.app]&#x2F;authorize-client&#x2F;xxx and I can double check on my end if it went through. Thanks in advance and have a good night A. reply roamerz 6 hours agoprevI was thinking about this the other night. Is there really a solution to this? Best case scenario lets say you have a hardware key and everything is sealed up really well. You get your phishing call but instead of asking for a MFA code they have a real time IA enhanced video call from your daughter or mom with a gun to her head and they just walk you through a set of steps that will expose your IT systems. Do you do as they demand with a loved one’s life at stake? Or maybe it’s a scam? What do you do? You have 5 seconds to decide. Me? I go John Wick on them but I’ve had more than 5 seconds so that doesn’t count. reply hn_throwaway_99 12 hours agoprevQuestion for security folks out there:So often I see these kinds of phishing attacks that have hugely negative consequences (see the MGM Resorts post earlier today), and the main problem is that just one relatively junior employee who falls for a targeted phishing attack can bring down the whole system.Is anyone aware of systems that essentially require multiple logins from different users when accessing sensitive systems like internal admin tools? I&#x27;m thinking like the \"turn the two keys simultaneously to launch the missile\" systems. I&#x27;m thinking it would work like the following:1. If a system detects a user is logging into a particularly sensitive area (e.g. a secrets store), and the user is from a new device, the user first needs to log in using their creds (including any appropriate MFA).2. In addition, another user like an admin would need to log in simultaneously and approve this access from a new device. Otherwise, the access would be denied.I&#x27;ve never seen a system like this in production, and I&#x27;m curious why it isn&#x27;t more prevalent when I think it should be the default for accessing highly sensitive apps in a corporate environment. reply aberoham 12 hours agoparentTeleport has two person rule + hardware token enforcement, https:&#x2F;&#x2F;goteleport.com&#x2F;resources&#x2F;videos&#x2F;hardened-teleport-ac... reply hn_throwaway_99 12 hours agorootparentReally, really appreciate you sending this! I will dig in but this seems to be exactly what I was asking about&#x2F;looking for. I&#x27;m always really curious why the big native cloud platforms don&#x27;t support this kind of authentication natively. reply fireflash38 12 hours agoparentprevYou&#x27;re looking for quorums, or key splits. They aren&#x27;t super common. You see them with some HSMs (need M of N persons to perform X action). reply joshxyz 12 hours agorootparentnot good with acronyms, what is hsm here? reply coderintherye 12 hours agorootparentHardware security module https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hardware_security_module reply justaddwater 12 hours agorootparentprevHardware Security Module reply SoftTalker 6 hours agoparentprevI&#x27;ve worked in places where to get access to production or other sensitive stuff, an employee would need to submit a request which had to be approved by whoever was designated to approve such things. Then the employee got a short-lived credential that could be used to log in. Everything they did was logged. Once used, the credential could not be used for subsequent logins. Their session was time-limited. If they needed more time, they needed to submit another request. reply johngalt 12 hours agoparentprevMechanisms like this exist, but they probably aren&#x27;t integrated into whatever system you are using, and delays which involve an approval workflow add a lot of overhead.In most cases the engineering time is better spent pursuing phishing resistant MFA like FIDO2. Admin&#x2F;Operations time is better spent ensuring that RBAC is as tight as possible along with separate admin vs user accounts. reply landemva 12 hours agoparentprevTransactions (messages) can be required to have multi-sig, if that is desired.There are smartphone apps and various tools to send a multi-sig message:https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;pybtctools reply joshxyz 12 hours agoparentprevi wonder on this too if people really use shamir secret sharing as part of some security compliance reply lionkor 3 hours agoprevMFA still means single point of failure - the person who has all the MFA is the one who can be hacked, like in this social engineering scenario. reply miki123211 12 hours agoprevdoes iOS have a \"is there a call in progress\" API?If so, it would be a good idea for OTP apps to use it and display a prominent warning banner when opened during a call. reply wepple 13 hours agoprevWhy did they need to call? They could’ve phished the password and MFA by simply MITMing?Perhaps we need a distinction from phishable MFA and unphishable U2F&#x2F;WebAuthn style reply rsstack 13 hours agoparent> The caller claimed to be one of the members of the IT team, and deepfaked our employee’s actual voice. The voice was familiar with the floor plan of the office, coworkers, and internal processes of the company. Throughout the conversation, the employee grew more and more suspicious, but unfortunately did provide the attacker one additional multi-factor authentication (MFA) code.> The additional OTP token shared over the call was critical, because it allowed the attacker to add their own personal device to the employee’s Okta account, which allowed them to produce their own Okta MFA from that point forward.They needed to have a couple of minutes to set things up from their end, and then ask for the second OTP code. A phone call works well for that. reply wepple 13 hours agorootparentAhh, thanks and apologies for not re-reading before asking.That is indeed interesting; keep the con going a bit longer to get a proper foothold. reply fn-mote 10 hours agoprevAfter reading all of the hype in the comments, I was disappointed by the actual article. There&#x27;s about one paragraph of actual material about the (\"spear\") phishing attack.There are not any details about the progress of the attackers or the speed of the attack, which would have been interesting to me. There are no details about any losses from the attack (or profits to the attacker).Once the employee provided a TOTP code to the attacker, the only surprise is that they get control of the other codes by cloud sync (as extensively commented on here).Regardless of the hate, this could happen to anyone. But... big L for reading out your TOTP code to somebody. (If more details about the deepfake come out, then it might be more exciting.) reply boblob-law 11 hours agoprevAm I the only one questioning the deep fake of the voice? reply Izkata 8 hours agoparentIf they have an audio recording of the person, there&#x27;s a bunch of sites where you can create them on the fly for free. Don&#x27;t know about the quality, though I imagine distortion can be dismissed as being from the phone rather than the fake. reply SoftTalker 6 hours agorootparentYeah, just add some twangy dropouts like a poor cell connection has. reply kerblang 13 hours agoprevI don&#x27;t understand: Why on earth does google want to sync MFA tokens? They&#x27;re one-time use, aren&#x27;t they? Or... feh, I can&#x27;t even fathom reply kerblang 13 hours agoparentAnswering myself, this helps a bit: https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;google-authenticator-will-now-...I guess we need a better way to handle \"Old phone went swimming, had to buy another, now what?\" reply mbesto 13 hours agorootparentWhich is funny because the 2nd factor is \"something I have\", which means if you don&#x27;t \"have it\" then you can ever complete the 2nd factor. This ultimately means the 2nd factor, when you&#x27;re phone goes swimming, is ultimately your printed codes. reply jeremyjh 13 hours agoparentprevThey mean they are syncing the private key used to generate the tokens on demand. reply kerblang 13 hours agorootparentDo all these 2FA apps - like say Microsoft Authenticator - have these hidden&#x2F;not-so-hidden private keys? From other posts it sounds like you can view the token and write it down... MA doesn&#x27;t have that, I don&#x27;t think. reply e12e 9 hours agorootparentTOTP (Time-based one-time password) need a shared secret (and two synchronized clocks) to work, so yes.FIDO2&#x2F;WebAuthn relies on public key technology - so does also have a secret key - but is designed to be kept secret from the service&#x2F;server one authenticates against.For use - FIDO2 is more like a multi-use id. Like a driver&#x27;s license many services accept as id. If you lose it - you don&#x27;t restore a backup copy from a safe - you use your passport until you get a new one issued.This makes more sense than with TOTP as the services only need your public key(id) on file.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Time-based_One-time_Passwordhttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;WebAuthn reply rawgabbit 9 hours agorootparentWhich FIDO2 service do you recommend?I get tired reading all these security articles. The more I read, the more I feel they are hiding something. reply kerblang 13 hours agorootparentprevAnswering myself again, yeah, they all seem to have this private key hidden away somewhere. Didn&#x27;t know that.https:&#x2F;&#x2F;frontegg.com&#x2F;blog&#x2F;authentication-apps#How-Do-Authent...? reply magospietato 13 hours agorootparentprevWell that&#x27;s even worse isn&#x27;t it? reply AshamedCaptain 13 hours agoparentprevFor me the question is \"who the fsck uses Google Authenticator to store all their tokens, both company and personal?\" reply burkaman 13 hours agorootparentGoogle Authenticator was I believe the first available TOTP app, and is by far the most popular. It used to be open source and have no connection to your Google account. Many people installed it years ago when they first set up MFA, and have just been adding stuff to it ever since because it&#x27;s easy and it works. Even for technical users who understand how TOTP works, there is no obvious reason it appears unsafe to put all your tokens in the app (until you read this article).Look at the MFA help page for any website you use. One of the first sentences is probably something like \"First you&#x27;ll need to install a TOTP app on your phone, such as Google Authenticator or Authy...\"It really did used to be the best option. For example, see this comment from 10 years ago when Authy first launched:> The Google Authenticator app is great. I recently got (TOTP) 2-factor auth for an IRC bot going with Google Authenticator; took about 5 minutes to code it up and set it up. It doesn&#x27;t use any sort of 3rd party service, just the application running locally on my phone. TOTP&#x2F;HOTP is dead simple and, with the open source Google Authenticator app, great for the end user.- https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=6137051 reply glandium 8 hours agorootparentAlso, since it doesn&#x27;t allow to extract the private keys, you&#x27;re kind of stuck with it once you&#x27;ve started using it. reply fireflash38 12 hours agorootparentprevI think technically Blizzard Authenticator (even the app) was available before Google Authenticator, but obviously for extremely limited use. reply unethical_ban 13 hours agoparentprevSyncing of \"MFA codes\" is really syncing of the secret component of TOTP (time based one time password).And it&#x27;s a good thing, and damn any 2fa solution that blocks it. I don&#x27;t want to go through onerous, incompetent, poorly designed account recovery procedures if a toddler smashes my phone. So I use authy personally, while a friend backs his up locally. reply itake 13 hours agorootparent> I don&#x27;t want to go through onerous, incompetent, poorly designed account recovery procedures if a toddler smashes my phoneWhy don&#x27;t you use the printed recovery tokens? reply e12e 9 hours agorootparent> Why don&#x27;t you use the printed recovery tokens?I currently see 53 2fa tokens in my private bitwarden.You expect me to print, keep safe and manually reset them all when I buy a new phone? reply Zarathustra30 10 hours agorootparentprevThe toddler got there first.Seriously, though, it&#x27;s hard to keep track of something that gets used once every five years. reply monocasa 13 hours agorootparentprevWho has a printer these days? reply CameronNemo 12 hours agorootparentLocal libraries, print shops... but yeah that may be an attack vector. reply unethical_ban 12 hours agorootparentprevNot all websites offer them.Hell, no bank I use (several large and several regional) support generic totp. Some have sms, one has Symantec VIP, proprietary and not redundant.Edit: since I&#x27;m posting too fast according to HN, even though I haven&#x27;t posted in an hour, I&#x27;ll say it here. Symantec is totp but You cannot back up your secrets and you cannot have backup codes. reply CameronNemo 12 hours agorootparentSymantec VIP is TOTP under the hood.https:&#x2F;&#x2F;github.com&#x2F;dlenski&#x2F;python-vipaccess reply skybrian 13 hours agorootparentprevA better way to fix this is to have multiple ways to log in. Printed backup codes in your safe with your personal papers and&#x2F;or a Yubikey on your keychain. This works for Google and Github, at least.Passkey syncing is more convenient, though, and probably an improvement on what most people do. reply charcircuit 12 hours agorootparentprevIf you can backup a key it is not MFA. It just a second password and not another factor. The solution to having your phone smashed is to have multiple \"something you have\", so you have a backup. reply j-bos 12 hours agoprevWhere can one find a breakdown of how to build implement a TOTP generator? For curiosity&#x27;s sake reply deathanatos 11 hours agoparentThe basic premise is in https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;rfc6238, although today I&#x27;d use SHA-256, not SHA-1, if possible.But I&#x27;d disfavor TOTP over hardware tokens that can sign explicit requests. reply drx 8 hours agoprevExcellent write-up, thank you. reply xorcist 12 hours agoprevStopped reading at \"deepfake\".It&#x27;s the new advanced persistent threat, a perfect phrase to divert any resposibility.(Yes, there are deepfakes. Yes, there are APTs. This is likely neither.) reply skeaker 11 hours agoparentI am (genuinely, really asking) curious why you think so. I&#x27;ve got no hand in this, but the skepticism around phishing attacks from this site of all places really surprises me. People like Kevin Mitnick have done more sophisticated phishing with fewer tools. Why wouldn&#x27;t someone intent on running a social engineering scam use one of the widely available voice faking technologies that are available now? Keep in mind that they&#x27;re simple enough to use that people are making memes with voices generated from ~5 seconds of voice recordings. reply xorcist 1 hour agorootparentMaking a meme is nothing like an interactive telephone conversation.It&#x27;s not that it&#x27;s impossible, but it&#x27;s not trivial either. But mainly, it&#x27;s just unnecessary.If the user is not fooled by a well crafted phishing, by doing the most trivial countermeasures such as calling back, they are not going to be fooled by a deepfake. In practice work on phishing is mostly better spent elsewhere. So while we shouldn&#x27;t dismiss it completely, it&#x27;s clearly not the case with a smallish company with limited economic value, so very unlikely the case here.There has been a handful of highly profile media cases involving deepfake. None of which has held up on further investigation. It is understandable, nobody wants to be known as the one who didn&#x27;t recognize his own kid on the phone, but the truth is more simple and actually helps us when designing countermeasures. reply account-5 12 hours agoprevI wonder how long it&#x27;ll be before a similar attack happens before someone&#x27;s&#x2F;a companies passkeys are synced to the cloud. reply RcouF1uZ4gsC 12 hours agoprevOne thing that is left out it to use unphishable MFA like hardware security keys (Yubikey, etc). reply ocdtrekkie 10 hours agoprevThe only takeaways you need from this:- Your on-premise customers are the smart ones. Networks containing sensitive information should be isolated, not all pooled together.- Google still has actually no understanding of practical security. Literally ban their products from your networks. reply yieldcrv 12 hours agoprevI just call them",
    "originSummary": [
      "Retool, a platform for creating internal tools, faced a spear phishing attack that led to unauthorized access to 27 cloud customer accounts, revealing the vulnerability of the system to social engineering attacks.",
      "The attack took place when an employee succumbed to an SMS-based phishing attack and provided multi-factor authentication (MFA) codes, which subsequently compromised company's VPN and internal admin systems.",
      "Retool recommends improving security, such as avoiding the storage of MFA codes in the cloud, promoting employee training and the use of hardware security keys for MFA, and adopting defense-in-depth strategies. The company is now collaborating with law enforcement and a forensics firm to probe the incident."
    ],
    "commentSummary": [
      "The Hacker News discussion encompasses aspects like being cautious of calls/messages requesting personal information, the necessity to validate caller identities, and the need for standard reverse-authentication systems.",
      "Participants discuss the strengths and limits of different security measures including password encryption, multi-factor authentication, and biometrics, and share instances of mishandling fraud issues.",
      "Issues like the use of deepfake technology in security breaches, the protection of digital assets, and the overarching agreement on the requirement of user-friendly, robust security solutions are also detailed."
    ],
    "points": 324,
    "commentCount": 232,
    "retryCount": 0,
    "time": 1694634512
  },
  {
    "id": 37502258,
    "title": "Don't use Discord as your Q&A forum",
    "originLink": "https://kraktoos.com/posts/dont-use-discord-as-forum/",
    "originBody": "Home About Now Uses 𝕏 Don't use Discord as your Q&A forum Sep 12 2023 Discord is not the ideal choice for the Q&A forum of your next failed side-project. Seriously, please stop. Why it sucks Chaos Discord can be a whirlwind of madness. Important stuff you post can vanish into the ether within seconds, drowned by a never-ending stream of messages. Sure, they introduced threads, but they still leave much to be desired, especially because of: TERRIBLE Search and Discovery Trying to find past discussions or solutions in Discord is like trying to locate a needle in a haystack blindfolded and drunk. Discord’s search is something made by the devil to check the patience of us, mere programmers. You might search for a message you sent weeks ago and come up empty-handed because you missed a couple of words or some other infuriatingly stupid reason… The Discord Odyssey Picture this: You encounter a problem with a project that exclusively relies on Discord for support and questions. You have to embark on a quest: find their Discord link by first searching for the project name on SearX (or Goog-🤮), finding the right link, then making sure you’re logged in to Discord on your browser or app, join the server (after reading (or not?) and agreeing to the rules and choosing the roles needed to see the god damn channel), hunt down the elusive help keyword in the left side of the screen, and then spend an eternity searching for your issue or just asking it again. Meanwhile not finding the solution and ending up bun installing a new blazingly fast new JS library… Waaaay too ephemeral Here’s a curveball for you: what if Discord decides to pull the plug? Who can guarantee it’ll be around tomorrow? Something like Stack Overflow can and is scrapped every day, but Discord isn’t. Better Alternatives Dedicated Community Forums - Check out platforms like Discourse. They’re designed for structured discussions and won’t leave your posts lost in the abyss. Good SEO for users of your software to find what they need fast. Lean on the Pros - Websites like Stack Overflow or communities on Reddit, Lemmy, or Kbin can be excellent places to seek help. Git It Done - If you’re dealing with code issues, rely on GitHub, Gitlab, Gitea, or any other Git-based issue tracking system, specially if your project is open source, since it’s where everyone will be when looking for your project anyways and has good enough SEO for most daily searches. Remember, Discord is great & all but as a Q&A forum, it’s like using a spatula to fix your spaceship. Think twice (or as many times as you need to realize that it’s a bad idea), and choose wisely (just not Discord)! .,;'. .kWMNKkdc:::codxkd, .dWMMMMMMMMMMMMMMMMK; cNMMMMMMMMMMMMMMMMMM0' '0MMMMMMMMMMMMMMMMMMMWk. :OOOOOOOOOOOOOOOOOOOOOO: .:oooddddddddddddddddddddddddooo:. .oKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKo. ...'lxxxxxxxxxxxxxxxxxxxxxxxxl'... .xMNOxkkOXMMMMMMMMXOkkxONMx. ;XNl .lNMMMMMWkc;. lNX; cXNo.'0MMMMMM0' .oNXc ,ON0d0MMMMMMNkc::l0NO, .;xKWMMMMMMMMMMWKx;. .;ldxkOOkxdl;. © 2023 Kraktoos. All rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=37502258",
    "commentBody": "Don&#x27;t use Discord as your Q&A forumHacker NewspastloginDon&#x27;t use Discord as your Q&A forum (kraktoos.com) 326 points by kraktoos 12 hours ago| hidepastfavorite202 comments TaylorAlexander 1 hour agoI recently found a repo for an Xbox wireless controller kernel driver where the GitHub issues page was turned off and instead they used discord. I asked why they don’t have GitHub issues turned on and they said they “didn’t want it to become a support forum”. I couldn’t believe it. If there is a common issue, one person will ask about it on the issues page and then everyone else can benefit from that discussion. On discord, I had to log in and join their group to ask about an error message, only to find it’s a common issue and there’s a few troubleshooting steps. I could have figured that out instantly that others have encountered this problem and discussed the solution. Instead, this repo owner moved everything to discord where it was hard to discover and usually required direct human support for every new person. reply trvz 18 minutes agoparent> If there is a common issue, one person will ask about it on the issues page and then everyone else can benefit from that discussion.Have you ever seen the GitHub issues of a halfway popular repo?Once a project is big enough to attract all the low quality users, the same question&#x2F;problem will be asked&#x2F;reported over and over again.The maintainers will have to deal with a mess either way. They might as well choose the platform they themselves prefer. reply apatheticonion 1 hour agoparentprev100% this.IMO for projects that need a place for support questions but don&#x27;t want to use issues, they could use the Github \"discussions\" board feature. reply jhanschoo 57 minutes agoparentprev> On discord, I had to log in and join their group to ask about an error message, only to find it’s a common issue and there’s a few troubleshooting steps.Did you try searching for the error message on Discord? If so, what was the experience? reply OJFord 48 minutes agorootparentNot GP, but it&#x27;s miserable. Discord, and to only a slightly lesser extent Reddit, is not built for that use, it doesn&#x27;t drive engagement. reply PaulHoule 6 hours agoprevOne trouble w&#x2F; things like Discord and IRC for support or community building is that frequently you get somebody with nothing better to do who \"leans in\" and spends more time (all the time) logged in and ends up being the face of your forum for new users.(Full disclosure, I&#x27;ve been that guy) reply spankalee 6 hours agoparentThis is a real problem. My project dealt with someone who dominated discussions and frequently responded with outright false information. We tried very hard to work with them, but they wouldn&#x27;t change, and after asking a lot of other maintainers how they would handle it, and giving many warnings, we eventually banned them despite them not really breaking rules per se.What ultimately convinced us banning was appropriate was people reminding us that it was within our rights and duties as community maintainers to create a welcoming environment for everyone, and seeing regular members stop participating because of them. reply Fradow 2 hours agorootparentI&#x27;ve been there. On a Discord I used to moderate, there were a few people like that who did not really break any rule, or not in an egregious enough manner to deserve a ban individuallyA rule was made for that situation: \"If the effort and&#x2F;or stress associated with moderating you regarding rules or general behavior becomes too much of an issue, we will remove you from the server.\".That rule has been used a few times since its implementation. reply tapland 2 hours agorootparentOh, why didn&#x27;t it strike me to put that rule in writing.Some users waste hours and hours of moderator time per week and it would be nice to have a written rule to point to for a timeout reply barkingcat 6 hours agorootparentprevWhat&#x27;s wrong with banning someone you&#x27;ve asked repeated to change their behaviour while interacting in a space that you host? Nothing! reply csnover 5 hours agorootparentRight or wrong, it can be a really difficult thing to do when the problem behaviour is not due to malice. You’re taking away a community from someone, and in my experience, usually it’s someone who doesn’t have many other social outlets. It does not feel good to get pleading emails from these sorts of folks, knowing that you have to stand firm and say ‘no’ for the health of the wider community. reply cgriswald 4 hours agorootparentSometimes these people need a hard truth. I’d say most times. They usually won’t “get it” in the moment, but the only way they can ever get it is if someone respects them enough to say it.These sorts of bans aren’t just good for the community. They’re also good for the recipient. (I’d only say that in the short term, depending on the person, there could be a danger of self harm and people should always be kind and careful.)It doesn’t feel good. But that pain you feel is you caring about that person. Understanding that makes it easier. reply sublinear 3 hours agorootparentprev> usually it’s someone who doesn’t have many other social outletsYou can always push them to a misc channel and if the community really likes them they&#x27;ll chat over there? reply echelon 1 hour agorootparentprevThis is the difference between banning in the aughts versus now.Banning used to come with a lot of consideration and sympathy. Often multiple attempts to outreach were made.In the social media era, not only do we ban without prejudice, we shadow ban (leaving them to think they&#x27;re talking to people), ban on presumption (banning Redditors based on other subreddits they use), and take joy in shutting down those we disagree with (freedom of speech for me, not for thee).The new tactic on Reddit is to block someone when you disagree with them - this prevents them from ever interacting with any thread you post in, even the sibling posts.We love to silence people these days. I just wish it didn&#x27;t come with a wide blast radius beyond peoples&#x27; own personal consumption. reply jiofj 1 hour agorootparentprev>You’re taking away a community from someone, and in my experience, usually it’s someone who doesn’t have many other social outlets.This is not your problem. reply d0mine 5 hours agorootparentprev\"not really breaking any rules per se\" reply j16sdiz 4 hours agorootparentIt&#x27;s easy.You are the host, you define the rule. reply hitekker 5 hours agorootparentprevMost internet community rules seem to spawn from a fear of authority. A mutual fear from the rank-and-file and the rulers. Endlessly debating a million little rules is safer than questioning each other&#x27;s judgement. Authority resolves both those situations efficiently, but it demands a morality: a fair standard applied across all people.A lot of rules seem to be trying to create that standard on paper, when that standard does not exist in the heart. If I were to go on a limb, \"inclusivity\" is the fear of being exclusive, which forms only in a vacuum of authority: the unquestioned power to decide who is in, and who is out. reply posix86 4 hours agorootparentYou might be describing how the Bible got created reply hitekker 4 hours agorootparentPossibly. I was thinking more about Bay Area tech people who can&#x27;t handle deep conflicts and outsource morality to their managers. But hey what do you know there is a Bible verse for this: https:&#x2F;&#x2F;www.biblegateway.com&#x2F;passage&#x2F;?search=Matthew+18%3A15... reply Modified3019 2 hours agorootparentprevGood call.A good video related to this: \"Assholes Are Ruining Your Project. Donnie Berkholz (RedMonk)\" https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=-ZSli7QW4rg reply boxed 55 minutes agorootparentprevThat can happen on github too.. reply Lacerda69 2 hours agorootparentprevI swing the banhammer early and often without remorse. If you don&#x27;t learn after being reminded, there is no use keeping you around. 1 bad person can ruin it for everyone else - not having that. I believe in second chances but not in online communities. reply mid-kid 5 hours agorootparentprevPeople should be reminded that rules are more of a guideline than a strict plan to follow, especially in the online world. Banning people is a crucial part of forming a community, and you should feel no shame for applying it where necessary to guide the atmosphere towards a better place. reply Dudester230602 1 hour agorootparentprevAt least there is some punishment for you Discord-peddling lot. reply squeaky-clean 5 hours agoparentprevDoesn&#x27;t the same thing happen with old school forums? There are some avatars I remember (I don&#x27;t remember their user names) on certain product&#x27;s forums who I know I can&#x27;t trust, and they show up in every third thread I find on Google. At least they never change their avatar, if they did I might fall for their BS again.Also IMO, on forums that show a user&#x27;s response count, anyone with several thousand responses should be ignored unless their account is over a decade old. They tend to spend more time on the forums than actually using the thing the forum is based on. reply heavyset_go 5 hours agorootparentI feel like it&#x27;s different on more asynchronous mediums than just chats.Forums could have thousands of personalities and never really feel dominated by any of them, but IRC channels became dominated by a relatively small percentage of regular users. reply cgriswald 4 hours agorootparentForums can also be small. Regardless of size, one or a cadre of like-minded or easily manipulated folks can dominate a forum.I think the major difference between forums and Discord&#x2F;IRC is that the conversations are more likely to be seen on the forums because they’re more persistent and can be more easily referenced to see the entire picture.That’s certainly not impossible in IRC but it’s more work and less accessible to the less technically inclined. reply esafak 6 hours agoparentprevThat&#x27;s because those platforms provide instant gratification, which attracts people with too much time. Busy, conscientious people are not going to loiter around and repeatedly answer questions that newbies ought to be able to solve on their own with proper tools (i.e., not Discord, Slack or IRC). reply OJFord 1 hour agoparentprevThat is presumably what they want though when they say they don&#x27;t want (better options) getting bogged down in support - they mean they don&#x27;t want to be the ones dealing with it, surely?And I do understand, I know from experience how much basic third-party tooling&#x2F;OS support questioning you can get from people not even getting as far as actually running your thing successfully.I&#x27;m not defending Discord as a solution though. reply EdwardDiego 6 hours agoparentprevIt&#x27;s like that on a FOSS project Slack I&#x27;m on. He&#x27;s not the most... ...personable person, and tbh I think it&#x27;s a control thing. reply World177 6 hours agorootparentI’ve done it in different communities. I don’t think it should be discouraged even though it can seem strange. There’s a person who wants their question answered, and someone who wants to answer it. Why make the sever worse by stopping it?At times, they might be wrong, but they’re going to realize they’re a problem if they’re always wrong. I do think sometimes incentives seem misaligned, like, I was in an enterprise linux related community, where their income is related to providing support to enterprise customers. In that scenario, it seemed like someone answering questions for free could be problematic to their business.I don’t know, I don’t think it’s an issue, but I’ve noticed it does seem strange being that person. reply EdwardDiego 6 hours agorootparentIt&#x27;s only a problem because he can be an asshole sometimes, and he very nearly always gets in first, (I swear he never sleeps), so yeah, sometimes not a great intro to a community that does exist, honest, not that you&#x27;d know from the Slack threads.To be fair, when he answers a question he&#x27;s right 99.9% of the time. reply manfre 50 minutes agorootparentThe discord should clearly state the community standards, including some variant of \"don&#x27;t be an asshole\". Most importantly, the standards must be enforced.These people, while knowledgeable, drag down the community with their toxic personalities and discourage others from participating. Who wants to ask a question if there is a high probability of being called stupid.Warning and then ultimately kicking these individuals improved things as a whole. Others stepped up to answer questions and mods handled fewer complaints about the asshole&#x27;s style of communication. replyRoark66 4 hours agoprevAt least IRC is usually logged.However, I&#x27;ve noticed a disturbing trend amongst web shops. It used to be common for a shop to have it&#x27;s return policy and form linked on the main page&#x2F;menu. Right along side its terms&conditions, privacy policy etc. Now I noticed a couple of popular shops I interact with replaced it with \"chat with us\" things(IKEA in Poland, a bunch of very large clothing brands etc).I hate it when I have to talk to crappy LLM for 5 minutes to convince it I need a human, then wait 15min because \"we&#x27;re having higher than usual support volume\" (at which point it becomes the norm?) to then have a human ask one question and give me a RMA number and order their courier to pick it up.So much time wasted could be recovered by a simple RMA form. I hope this trend doesn&#x27;t spread everywhere. reply vladvasiliu 4 hours agoparentAnd then, those same companies, feign having no idea why everybody and their dog orders on Amazon.I&#x27;ve never had to jump through any hoop or deal with any clueless AI to return a product. reply Lacerda69 1 hour agorootparentAmazon is absolute garbage, no idea why people praise it so much. Scams all around and garbage products - ill take a bad return policy any day over that. reply maccard 1 hour agorootparentI&#x27;ve had 0 scams on Amazon in 15(?) Years of using it. I had one \"new\" product that was clearly opened, and two clicks on their website later I had a pickup ordered from my door the following day and a replacement in the way.That&#x27;s why people praise it. reply galkk 3 hours agorootparentprevAmazon recently started doing the same.The \"chat\" button disappeared from the help & contact us, and to talk to real person you need to go through bunch of jumps. reply fer 30 minutes agorootparentI think that&#x27;s been the case for a while other countries&#x27; marketplaces. reply dsab 1 hour agorootparentprevAmazon UI is a joke even compared with such local (Poland wide) shopping sites like allegro.pl reply dx034 22 minutes agorootparentIt&#x27;s not intuitive but hasn&#x27;t changed in a very long time. It takes me next to no time to create a return label and even for returns after 30 days, navigating to the chat, asking for a label and receiving it rarely takes longer than 5 minutes.They&#x27;ve never refused a return I asked for which has frequently happened with other shops.I really want to support local shops, but for any product where I&#x27;m not sure if I will have to return it, I usually use Amazon because of their return policy. reply anon____ 52 minutes agoparentprev> So much time wasted could be recovered by a simple RMA form.Maybe that&#x27;s the goal? To discourage the return of merchandise? They think customers rather write off the loss, than waste half an hour. reply arbitrandomuser 37 minutes agorootparentSomeone should write a browser plugin to automate the chatting with an llm , beat them at their own game . reply knallfrosch 12 minutes agorootparentChatGPT, your task is to return my item with these DETAILS, you will be talking to a bot for COMPANY.I agree though, it&#x27;s like finding the support email address. It&#x27;s hidden on purpose. Use our Contact page. Please search our FAQ before talking to our Bot! reply Operyl 6 hours agoprevI don&#x27;t know, a lot of the people that interact with my projects _want_ discord. They don&#x27;t want a forum, they don&#x27;t want Matrix. It&#x27;s a matter of knowing your audience. I&#x27;ve been working on ways to better archive support&#x2F;question threads, and the discord search isn&#x27;t that terrible. reply 0xpgm 5 hours agoparentOf course, yet another way for the new generation of programmers to relearn the hard way the lessons of the older generation, like happens a lot in software.When I started out, it was pretty clear to me that to solve problems, first read the error message clearly and try understand it, then search the relevant forum or google for someone in the past who experienced the same error. Finally, if unable to solve, post a message describing the problem clearly and what you have tried to do and still failed.Well, good luck finding any thread from discord on Google. The chat interface also wants you to post one liner chats without taking the time to properly describe the scenario.Chat platforms can exist but should not be the official support channels. reply kynetic 4 hours agorootparentWoah this is an incredibly good point I hadn&#x27;t even considered before now. After years of finding solutions to problems in obscure threads on forums I&#x27;d never have visited otherwise, I had never even considered the fact that the only way to do similar with discord is to not only have an account, but be a member of the specific discord guild, and then search that specific guild for specific keywords.That&#x27;s actually mind-blowingly horrifying. reply BanazirGalbasi 4 hours agorootparentThis is one of the most common reasons I&#x27;ve seen people use to push back on Discord for support, and the article somewhat brings it up in points 3 and 4. It&#x27;s not publicly scrapable or anonymously accessible, nor can you archive it and host the information elsewhere if it goes down. Even the forum channels they introduced aren&#x27;t useful in those regards. reply RugnirViking 2 hours agorootparent> nor can you archive it and host the information elsewhere if it goes downsome communities do, for example the webots discord i&#x27;m in. They have a mirror of all chat logs indexed and searchable on their docs website. I don&#x27;t know how they do it, I can ask if you like. reply another_story 3 hours agorootparentprevYou&#x27;re allowed bots, so why can&#x27;t we archive it and make it more easily searchable? Those programming channels with QA style threaded answers would be great to log and make searchable. Is it against Discord&#x27;s ToS or something? reply brabel 3 hours agorootparentprevSay what you will of StackOverflow, but they completely solved this problem nearly a couple of decades ago. Answers may be getting outdated depending on what you&#x27;re looking for, but new answers are always welcome and the site tends to always have the \"current\" best answer - and very search engine friendly. reply eddythompson80 6 hours agoparentprevI found that part of it is the informality of it all. There is no expectation that someone will search the chat history for an answer. When you don’t get an answer, but someone after you does, you can ping on it maybe once more. Those who don’t know the answer might chime in with “hey, I don’t know but look there”In general it has a much lower barrier of entry. With all the good and bad that brings.For the record I think it’s a miss in general. But YMMV. reply TylerE 6 hours agorootparentSome degree of informational amensia is a good thing. We&#x27;ve all hit the \"I need to do X, but every page on google tells me how to X in Version N-1, and it doesn&#x27;t work like that any more\". reply bmacho 3 hours agorootparentI don&#x27;t understand your example. How does \"informational amnesia\" help against \"I need to do X, but every page on google tells me how to X in Version N-1, and it doesn&#x27;t work like that any more\"? If you can&#x27;t find a solution, you should ask on the Q&A channel, regardless of informational amnesia or not. reply Operyl 6 hours agorootparentprevSo like IRC? It&#x27;s not like this was a new problem, we&#x27;ve had this problem in the past. Not everybody liked mailing lists&#x2F;forums 20 years ago either. reply Forgotthepass8 1 hour agorootparentIRC had a higher barrier to entry and a lot more common expectation of kickbans for various reasons (justified or not).Plus the concept of +v doesn&#x27;t really exist on discords. Sometimes you&#x27;d be stuck lurking while only a select few could speak.And frankly being able to embed images&#x2F;videos degrades the seriousness in various ways as it can become a bit of a meme&#x2F;comedy competition (and they scroll disproportionately more text off.And avatars mean less space is actually text..: is pretty much the optimal format for information density, which is ironic that twitch chat uses it (to mostly spam emotes) reply eddythompson80 6 hours agorootparentprevSure, like IRC but with a better interface, reply RadiozRadioz 3 hours agorootparentBetter? Sure, for you maybe.Overwhelmingly the word I&#x27;d use to describe Discord&#x27;s interface is \"one\". There is one interface. It&#x27;s an interface that most people seem to like, but we&#x27;ve lost an important freedom there. It&#x27;s against the EULA to modify your client or use an alternative one.If with IRC one can have any interface, I would not call Discord&#x27;s better. reply eddythompson80 2 hours agorootparentMaybe it changed, but how do you share a screenshot on IRC? reply RadiozRadioz 2 hours agorootparentUse Pidgin or The Lounge and do it like normal.Whether we should be using text chat clients as FTP clients is another issue, XMPP is better for multimedia messaging. reply voyagerfan5761 2 hours agorootparentprevSome modern clients (or clients-as-services) build in image&#x2F;file sharing. reply TylerE 6 hours agorootparentprevAnd no netsplits&#x2F;nickserv insanity. reply Operyl 5 hours agorootparentOr people purchasing a holding company and proceeding to take over the entire volunteer ran network a few years later. :) reply eddythompson80 5 hours agorootparentLuckily since there is no expectation of an archive, migration is fairly straightforward if there is an alternative. If there are no alternatives, I guess they can try IRC ;) replyRHSeeger 5 hours agorootparentprev> There is no expectation that someone will search the chat historyAt least part of that is that the search in Discord is horrifyingly bad, and isn&#x27;t available via search engines.I don&#x27;t mine Discord as a place to ask questions &#x2F; discuss; but it&#x27;s far less dysfunctional when combined with a wiki. reply muzani 6 hours agoparentprevCould be a bias too. I really dislike Discord. I&#x27;m on too many discords. It&#x27;s just one of those network effect things that I wish wasn&#x27;t everywhere.I&#x27;m happy with searchable public chat support. Unfortunately, discord seems to be the best way to do this.I think I do want a forum, but I probably wouldn&#x27;t use it because signing in is too much effort. Maybe if forums had shared profiles and better mobile support, they&#x27;d be used more. reply Operyl 6 hours agorootparentI tried launching a forum, I spent a _lot_ of time setting up Discourse and proper CDN&#x2F;uploads etc. I didn&#x27;t go all out, only a few categories based on what was commonly needed (like 5?). I did this _before_ I resorted to Discord as the only point of help. People begrudgingly used it... It got to the point where I was asked \"why aren&#x27;t you using Discord like everyone else in this space?\" enough that I asked my power users on the forum, and the broader internet via other channels, and most people overwhelmingly wanted Discord. In particular, of note, my power users on the forums wanted it. After switching, the number of people asking for help significantly increased, and we gained a fair number of new power users willing to help those people out too. reply Denote6737 4 hours agorootparentHere is the thing. You may be seeing more people seeking help because they are repeating the same questions again. Where as before they found the thread with the answer.To me this is the biggest drawback for discord over forums. reply Operyl 4 hours agorootparentFrom my experience it happens just as much on forums. Hell, just take a look at Stack Overflow! If someone is wanting to reanswer a question for the nth time, I have no problem with that, and the helpers in my community don&#x27;t either. As another poster said, as well, sometimes \"repeat questions\" age out after a certain point, where a newer method is actually more appropriate than an older one anyway. reply hutzlibu 3 hours agorootparent\"As another poster said, as well, sometimes \"repeat questions\" age out after a certain point, where a newer method is actually more appropriate than an older one anyway.\"I rather like the approach of editing the original question&#x2F;solution or posting the latest solution there as otherwise this just creates fragmention. reply xigoi 2 hours agorootparentprev> If someone is wanting to reanswer a question for the nth time, I have no problem with thatBut I do have a problem with having to ask a question that has already been answered a hundred times, instead of being able to search for the answer. reply nerdponx 6 hours agorootparentprev> I think I do want a forum, but I probably wouldn&#x27;t use it because signing in is too much effort. Maybe if forums had shared profiles and better mobile support, they&#x27;d be used more.That&#x27;s what made Reddit so popular. reply codetrotter 5 hours agorootparentprev> I probably wouldn&#x27;t use it because signing in is too much effortWhat if the forum offered the following three options for sign in:- Sign in with GitHub- Sign in with Apple- Sign in with Google reply muzani 5 hours agorootparentSpecifically signing in with a consistent persona. All of these link to an actual \"work\" identity. Sometimes I want some customer support. Other times, it&#x27;s a gaming identity, or a writer identity.Part of the problem is that communities can get really nasty, worst case going as far as writing bad reviews at the place you work at. So it&#x27;s useful to containerize your identity.Reddit actually does this very well. reply codetrotter 5 hours agorootparentSign in with Apple allows you the opportunity to generate unique email address aliases per service when you use Sign in with Apple to log in to third parties.Say for example that your “real” iCloud account is muzani at iCloud dot com.You browse to my hypothetical forum and choose Sign in with Apple. Then Apple lets you choose if you want to use your “real” mail address or if you want to make a new alias specific to this service. You make an alias an it will look something like emperor_virginia.9p at iCloud dot com.It’s super convenient and easy to use, while at the same time doing a great job of keeping your identities separate between separate services. reply RadiozRadioz 2 hours agorootparentprevI explicitly avoid all of those and am willing to do the extra two clicks to get a real account.I have no idea how long these random companies will keep providing auth services, I don&#x27;t want to be part of the mess when they inevitably stop the service or start charging. It barely solves the account fragmentation problem because every website supports a different random combination of providers. It centralizes everything more than it already is, further increasing your dependence on these companies. Not to mention the tracking.But the biggest thing is that with a native account I know that I&#x27;ll be getting all the features of the app and everything will work normally. There have been too many times where I&#x27;ve experienced \"your account does not support this feature\" because of a botched external auth integration. Remember when Spotify users with Facebook sign-in couldn&#x27;t change their name from the default string of random characters?No thanks, the \"effort\" is more than worth it for me. reply tacocataco 3 hours agorootparentprevCan I sign in as a guest&#x2F;anon user? reply Karrot_Kream 4 hours agoparentprevHN has a particular culture that dislikes social media and due to the nature of these sites, once a culture is established, it attracts more of the same since everyone upvotes the dominant cultural position. Discord is social media, so it&#x27;s bad, not like the good old days of forums&#x2F;mailing lists&#x2F;newsgroups&#x2F;IRC&#x2F;whatever. Listen to your users.My personal fear with Discord is the audience. Discord has a lot of kids and the likelihood of having kids come into your server and troll you or ask low-effort questions is much higher than Slack. But if your users want Discord, then you should use Discord. There&#x27;s nothing gained by telling your users what to like. reply rexf 3 hours agorootparentSure if your customers want to use discord and you&#x27;re ok with putting your community there, then go for it.I don&#x27;t think you can assume everyone wants to be on discord. I certainly loathe adding yet another discord or slack community that frankly I don&#x27;t check. Nobody has time to keep up with dozens or hundreds of discord communities (it&#x27;s very easy to join one).I prefer any online community that is searchable (via Google, site search, etc) so that I can find answers and past discussion without having to ask the same question for the 100th time in the channel. reply hunter2_ 2 hours agorootparentprev> Discord is social media, so it&#x27;s bad, not like the good old days of forums&#x2F;mailing lists&#x2F;newsgroups&#x2F;IRC&#x2F;whatever.How is Discord more social than the other systems you mentioned? I consider something to be social (social media, social network, etc.) when the primary utility manifests as a function of establishing friends, followers, or whatever similar jargon.That is: if the content presented to me is primarily generated by users who I&#x27;ve selected, while content generated by users I haven&#x27;t selected is unavailable or relegated to lower tiers of functionality, then it&#x27;s a social network&#x2F;medium. In other words, it&#x27;s the product of subscribing primarily to people (regardless of what they might discuss) rather than to topics (regardless of who participates).I don&#x27;t see Discord in this way. Isn&#x27;t it more about subscribing to topics than to people?I realize you&#x27;re not speaking for yourself, but for the HN hivemind; my question remains. reply TylerE 45 minutes agorootparentPretty sure you self-wooshed the sarcasm. reply roenxi 1 hour agoparentprevThere are a lot of contextual questions raised by that though - are the people who interact with the project the community? How many people are being interacted with? What are the goals of said community?2-3 people can seem like a huge crowd and a complete consensus in the right context. It is still a handful of people. And the fact is that Discord (and Slack) is long-term-toxic to building up knowledge in a community. There isn&#x27;t an available body of records to figure out what the history of the community is and what topics have been considered in the past. It is completely unsuitable for recording Q&A. It isn&#x27;t terrible as a support forum, but even then anything that can be crawled by a search engine has some serious advantages if the community cares about people who are in the silent majority. reply Forgotthepass8 1 hour agoparentprevIs it possible to somehow bridge discord and, say issues on GitHub or another forum so that people can use discord but the information is just pulled from other sources and they&#x27;re redirected there? reply muchwhales 5 hours agoparentprev\"Where your audience is, go you must\" - Master Yoda, probably reply bsder 6 hours agoparentprevWhat people want is a forum that doesn&#x27;t require another blasted login and yet another inscrutable navigation system.What forum owners want is to not have to deal with idiotic user logins and spam.Both sides think Discord is good enough even though it isn&#x27;t.The problem is that everything else is so much worse. reply Operyl 6 hours agorootparentI try not to argue against my users for \"what is best for them.\" If they overwhelmingly want Discord, I&#x27;ll give them Discord. The goal is to be ready to jump ship if need be, and there&#x27;s a few ways to do that. reply kobstrtr 5 hours agoparentprevYou could get help automating support with tools like awesomeqa or kapa.ai reply Dudester230602 1 hour agoparentprevVocal minority dominating silent majority. reply Operyl 1 hour agorootparentUnlikely, see my other comment on the matter:> I tried launching a forum, I spent a _lot_ of time setting up Discourse and proper CDN&#x2F;uploads etc. I didn&#x27;t go all out, only a few categories based on what was commonly needed (like 5?). I did this _before_ I resorted to Discord as the only point of help. People begrudgingly used it... It got to the point where I was asked \"why aren&#x27;t you using Discord like everyone else in this space?\" enough that I asked my power users on the forum, and the broader internet via other channels, and most people overwhelmingly wanted Discord. In particular, of note, my power users on the forums wanted it. After switching, the number of people asking for help significantly increased, and we gained a fair number of new power users willing to help those people out too. reply dabeeeenster 14 minutes agoprevThis depends entirely on volume IMO. We [1] use Discord for this, and its great, but we don&#x27;t have a large message volume. Discord seems way better than Slack for this use case as there&#x27;s a single login which is built around connecting to multiple servers, unlike Slack.[1]. https:&#x2F;&#x2F;www.flagsmith.com&#x2F; reply WhereIsTheTruth 5 hours agoprevDlang did it right, a mailing list that act like a forum [1] with various instant messaging bots (IRC, Discord, Matrix)I wish more projects would take inspiration from them, the software is open source [2][1] - https:&#x2F;&#x2F;forum.dlang.org&#x2F;[2] - https:&#x2F;&#x2F;github.com&#x2F;CyberShadow&#x2F;DFeed&#x2F; reply gizzlon 42 minutes agoparentLooks great, and fast too!How does the integration work? Can you post to the forum, and it&#x27;s then mailed out.. ? reply hnben 6 minutes agoprevthere is a neat tool that makes it easier to find stuff on discord https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36383773 reply tamimio 51 minutes agoprevAdd this point too: you can’t just google the problem for example and find a similar post and the resolution, you never know if the problem was addressed or resolved, even back in the days with locked forums where you needed to register or even reply to the post to see the solution, you know there’s some sort of issue and how to solve it, with Discord, nothing.So my rule of thumb, if that service or software that I’m trying to use list a discord as the mean of communication, I simply don’t use it, what’s next, holding meetings over Twitch and discussions over tiktok? reply herbst 26 minutes agoparentThis. I wouldn&#x27;t want to risk to depend on software where it&#x27;s literally impossible to Google issues. That would be self-destructive on a level I don&#x27;t want. reply joy_void_joy 2 hours agoprevI agree that the discord centralization for Q&A is becoming problematic, it makes it less discoverable and searchable (Had this problem a lot with Svelte).That said, I have had success so far using https:&#x2F;&#x2F;www.answeroverflow.com&#x2F; to search discord for questions. It sucks we have to use such tools, but given the current situation, it&#x27;s also better to adapt. reply tanepiper 4 hours agoprevSorry but if you think large-scale companies are going to use a platform designed for gaming as their main support channel - no.We&#x27;ve had one \"Enterprise\" supplier move from Slack to Discord. Their community manager in this case did not understand my argument of why this is a bad thing and kept pushing it. For example when I said there is no SSO, they said there is (of course not realising WE have to pick up the bill to set that up - also puts the work on CISOs to investigate the tool for larger use).Now they are on Discord and I will not share any NDA-related material there - nor would our Security and Data Privacy team like us to.At least they agreed to keep the Slack Connect in this case. reply cultureswitch 50 minutes agoprevIt&#x27;s kind of incredible that this even needs to be said.Discord is a great piece of software for organizing ephemeral communications. Voice chat works well. That&#x27;s 99% of the value of Discord. It also absolute dogshit as a persistent store of information.Stack Overflow-style Q&A is the definitive good choice for Q&A documentation. reply herbst 22 minutes agoparentI wouldn&#x27;t even go that far.There is not a single other popular communication software out there that a) has a pretty shitty client that wastes resources and leaks ram but b) bans account when they use better alternative clientsThat&#x27;s the most toxic environment I could imagine to communicate with reply ggm 6 hours agoprevI think proscriptive statements about what to use or not use are opinions masked to be pretend facts.You should use whatever works for you and your clients&#x2F;customers. All the channels come with plus and minus issues. It&#x27;s the support version of CAP theorem: You can be reachable, focussed and structured but probably not all three at once.I also miss email. Mainly because the expectation of \"instant\" was muted through delivery delays.When I need instant, I should possibly expect to have to pay for it. reply emodendroket 5 hours agoparentThen read the title as \"here some reasons why Discord is likely not ideal for use as a Q&A forum for you.\" Less punchy though. reply sircastor 6 hours agoprevIt’s interesting to see folks preferred solutions. Obviously, I don’t know any of you, but I’m willing to bet that most people’s preferences correlate with the period they started using the internet and what was popular at the time: mailing lists, Usenet, web-based forums, IRC, Slack…I wonder how long it’ll be before people are saying “I really wish we could just go back to Discord.” reply aworks 5 hours agoparentI&#x27;m an old guy so I never made it past lists&#x2F;usenet&#x2F;forums&#x2F;wikis (unless I was forced to use Teams or whatever at work). So FWIW:Threaded replies, in whatever form, have been the killer feature for me to grok more data that may or may not be relevant. reply david2ndaccount 6 hours agoprevIf you don’t run a discord, one of your users will “helpfully” start one anyway and the community will go there. You’ve then lost control of your Q&A forum. reply croes 5 hours agoparentOr they start a subreddit because you only have Discord reply koonsolo 2 hours agorootparentCan you give a real life example of this?I have both an official Discord and Reddit. Discord was the only platform I didn&#x27;t start myself, but eventually took over. My Reddit is as good as dead, Discord incredibly active.The theory all sounds good, but the fact is that the community will pick the place they hang out, not you. reply croes 2 hours agorootparent>The theory all sounds good, but the fact is that the community will pick the place they hang out, not you.The same will happen with Discord&#x27;s successor, but the Discord data will most likely be lost at that time. reply brucethemoose2 4 hours agorootparentprevSo many project subreddits are just dead and sterile. And Discord is a much more engaging place to just shoot the breeze.TBH a Reddit isn&#x27;t much of a \"threat\" unless its a really popular subject and the original forum is really bad. reply croes 2 hours agorootparent>So many project subreddits are just dead and sterile. And Discord is a much more engaging place to just shoot the breeze.The same could happen with amy Discord. No users means no users, no matter if it&#x27;s Discord or RedditBut I can find and read a subreddit via search engine without joining anything. reply koonsolo 2 hours agoparentprevExperience also taught me this. reply mooreds 6 hours agoprevI wrote about this a while ago for Slack&#x2F;forums: https:&#x2F;&#x2F;www.mooreds.com&#x2F;wordpress&#x2F;archives&#x2F;3451 but the points still hold.HN discussion: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29154216I&#x27;m a big fan of https:&#x2F;&#x2F;nodebb.org&#x2F; Full featured OSS forum you can self-host or let them host for you (for $).Big fan of letting people use the search interface they want, which is almost always Google. reply justsomehnguy 4 hours agoparent> I&#x27;m a big fan of https:&#x2F;&#x2F;nodebb.org&#x2F;TIL to what shit Netgate moved pfSense forums to. I&#x27;m glad you are fine with it, but not only my FullHD monitor is not a smartphone, so I don&#x27;t need 400% fonts on everything (and post dates on the faaaaar right clearly shows nobody ever even used the forum) and most importantly - search doesn&#x27;t work. It&#x27;s not like the previous forum had a good search, but at least it worked.Bonus point: try to Ctrl+mousewheel on any NodeBB (including the official one). reply galkk 3 hours agoprevThis is just wishful thinking.All the alternatives have proven that they barely work too.1. Dedicated forums - have you seen cesspools that are community.microsoft.com or apple support forums? Or few surviving audio forums, with those long taglines? Also searchability of those forums leaves to be desired too, as they rarely get to first page of google.2. Stackoverflow quality has degraded severely in recent years. Both from perspectives of someone who wants to ask a question and someone who wants to answer it. Niche stackoverflows are still fine, but general programming ones are just shit, full of obsolete&#x2F;old accepted answers that nobody cares to update to current year status3. Get it done. It is obviously to be better healthy and rich than sick and poorI grown into contrary view. Discords are fine. Based on my personal experience, at least the ones where I tried to find some help (QMK, Bambulab, bunch of gaming ones). The fact that same question is being asked again and again is fine. Somebody will answer it. Or not, and then the search for an answer will continue.They significantly reduce friction to answer for somebody who is open to help. See example about stackoverflow above. I just left it, as I don&#x27;t want to deal with mods there. reply mekoka 54 minutes agoprevDiscord is a trade-off. Don&#x27;t apply on the wrong problem.Some pros, I&#x27;ll pick three. (1) I think the appeal for something like Discord becomes more obvious when documentation feels like a moving target. For example, as a long time back-end developer, I have never disliked StackOveflow more than when I decided to learn front-end JavaScript. Every answer seemed attached to an expiration date. Anything older than 2 years and I&#x27;d wonder is this still in effect? I actually wished that JS would have its own special SO where answers can be sorted by \"number of votes this year\" or a \"deprecated\" flag. (2) The immediacy of chat, you ask and don&#x27;t have to wait half a day for someone to notice. (3) No search. It could just be an impression, but my search experience seems to just increasingly suck. Engines seem to struggle to find relevant bits, particularly if the tech is fairly recent and results are flooded with meaningless articles, that just echo the same \"Hello, world\" from the official documentation. With Discord, I don&#x27;t have to sort through the junk.Now the cons.The chat format for support certainly has its highlights, but you don&#x27;t have to wear blinders about its downsides. If you allow for time to improve information, information accrues interest. Chat doesn&#x27;t allow for time. And Discord doesn&#x27;t particularly care about the quality of information. Discord cares about (authenticated) users showing up on the platform now, now, now. The problems highlighted with the chat format (as it&#x27;s implemented on Discord), are very focused (though they have ramifications): archiving and searching capabilities are subpar. You show up on some chat channel and ask a question. You get the benefit of immediate advice, by the most authoritative voice that happens to be present. It may be good advice or it may be moot. You just have to take it at face value. On StackOverflow that advice would go unchallenged for a few months, but allow for time and you allow for scrutiny. Eventually, if there&#x27;s contention, there will be a discussion. Answers are commented upon, improved, deprecated. You can simply show up years later and benefit from the maturation of the different perspectives that contributed to crystallize our current best understanding of the issue.Obviously, YMMV. Still, don&#x27;t apply on the wrong problem. reply jansan 9 minutes agoparent> my search experience seems to just increasingly suckSO&#x27;s search engine always sucked. Whenever I cannot find the answer that I was looking for immediately, I use google with the site:staoverflow.com parameter. This always gave me much better results than the SO search itself. reply poulpy123 1 hour agoprevanother \"dark web\" I discovered few month ago is facebook. until then I was thinking that FB was just a social network for connecting people or companies but there are a crazy amount of forum-like groups with a lot of subscribers, and it&#x27;s almost as invisible and unarchivable than discord reply glaucon 7 hours agoprevI agree with not using Discord as a forum but suggested alternative number one is Discourse which starts at USD50&#x2F;month[1]. A bit of an ask for my \"failed side projects\".[1] https:&#x2F;&#x2F;www.discourse.org&#x2F;pricing reply postpawl 7 hours agoparentDiscourse is open source: https:&#x2F;&#x2F;github.com&#x2F;discourse&#x2F;discourseYou could hook it up to a mail provider and can host it yourself for less if you wanted. reply jraph 4 hours agorootparentI&#x27;m appalled by the number of people answering this comment very negatively &#x2F; quickly dismissing the self hosting option with a hostile tone.Guys, it was very common for (side-)projects to self host a forum in the 2000s, what changed since then so that many people find this unreasonable? Or are you a vocal minority?It&#x27;s not like you have to host a service that needs 5 figures SLA.I understand that people may not like having to host stuff, but seriously, there&#x27;s no need to dismiss the option that harshly, calling forums \"random shit\" and so on.Now people want it easy and free for everything: git hosting, website, communication tools… you can&#x27;t have your cake and eat it too forever. It&#x27;s very unfortunate those investors-infested companies running at loss made us used to this.It&#x27;s requiring that everything is free and easy that&#x27;s an unreasonable stance, not suggesting to self-host or to pay. At some point things need to be sustainable. Even for side projects with limited time available. A community can&#x27;t be managed if nobody has time for this anyway.Setting up a forum is not the hard part in handling a community. Moderation is a lot harder. reply musicale 2 hours agorootparent> Guys, it was very common for (side-)projects to self host a forum in the 2000s, what changed since then so that many people find this unreasonableI greatly prefer forums to ephemeral IRC or walled-garden Discord&#x2F;Slack&#x2F;etc. However I don&#x27;t like the moderation time commitment, especially with in a climate of aggressive spambots and twitter-style idiocy at scale.I also like blog comments, but those seem to have largely split into spam-filled cesspools on one hand and \"comments are now closed\" (after 10ms) on the other. HN is something of an exception, but I&#x27;m sure it requires aggressive and time-consuming moderation.Constant security threats also make self-hosting an ongoing maintenance headache even if it&#x27;s in the cloud. reply langsoul-com 7 hours agorootparentprevNobody wants to do that self hosting for a Q&A forum for a side project.Discord is free and is omega easy to setup. Low friction, low cost, that&#x27;s the real competition reply herbst 19 minutes agorootparentDiscord is slow, weird to setup and a walled garden with zero control about anything.Nobody in their right mind would want to use that for a project they care about, over using a piece of free software they have full control of.At least in my POV reply xigoi 2 hours agorootparentprevDiscord is not free, just “free”. reply MaKey 2 hours agorootparentprevI&#x27;d rather self-host a forum. reply glaucon 5 hours agorootparentprevOh, OK, I didn&#x27;t know that. That does at least suggest that it&#x27;s on a par with hosting phpBB, or similar, yourself. reply nextlevelwizard 5 hours agorootparentprevYeah, the solution to using a Discord is host random shit yourself that someone _might_ maybe need. That&#x27;s a hard pass. The very reason why I am on Discord is that I don&#x27;t need to host my own shit. Been there, done that, I have better things to do. reply xena 7 hours agorootparentprevThank you for offering to host a forum engine for me! reply asciiface 7 hours agoparentprevI also honestly find the discourse experience absolutely horrid. It&#x27;s kind of terrible to use. reply pier25 6 hours agoparentprevGithub Discussions is free reply nextlevelwizard 5 hours agorootparentFor now. Just remember who owns Github these days and what their track record is with technology. reply alchemio 4 hours agorootparentUnlikely. At least less likely compared to discord and reddit! reply nextlevelwizard 4 hours agorootparentdiscord makes bank, but not my point.who cares about reddit.microsoft has all the motivation to lock down developers. they already got most by the balls with vscode and copilot. replysebtron 4 hours agoprevIf you ever need to extract important information buried somewhere in a Discord server, I am having luck with Discord History Tracker [1] (browser-only version). It lets you download all messages in a json file, which then you can read with [2] (works offline too).[1] https:&#x2F;&#x2F;dht.chylex.com&#x2F;[2] https:&#x2F;&#x2F;dht.chylex.com&#x2F;browser-only&#x2F;build&#x2F;viewer.htm reply aleksandrm 5 hours agoprevNew World, an MMORPG game by Amazon Game Studios, have recently shut down their forums and migrated completely to Discord. It has been a complete disaster, including all the forum posts that show up in Google search are now just 404. Reporting bugs is a crap shoot into the void. reply barrkel 1 hour agoparentPresumably this is a feature, not a bug. The forum posts will age out of the search corpus, and negative feedback - evidence of low quality - will gradually disappear. reply mushufasa 6 hours agoprevI think Zulip is much better at topic threading, which makes it easy to search and organize historical conversations (though Zulip&#x27;s search function itself leaves much to be desired).hosted zulipchat.com is free for open source, and Zulip is free to self-host.self-hosting is not as easy as discord though, for sure reply uoaei 4 hours agoparentIt&#x27;s interesting seeing other comments like \"take Discord and put more layers on top for knowledge management\" rather than researching and finding an objectively way better tool for the job like Zulip.I can&#x27;t quite put my finger on why this is the dominant way people think about solutions in tech nowadays, but the impulse to reach for this kind of solution is infiltrating all levels of tech (see: every single \"wrapper around LLM to force it to do things we want\" startup). It&#x27;s like that deep learning meme from a few years ago saying \"just add layers\" except it&#x27;s webdevs saying \"just add endpoints\".It could be useful to have a periodic \"open office hours\" VoIP meeting perhaps, but that&#x27;s more an argument for Zulip adding VoIP than for contorting Discord into something it&#x27;s not suited for. reply mushufasa 4 hours agorootparentDiscord has a network effect that Zulip doesn&#x27;t, meaning a lot of times audiences are there whether creators like it or not.Sometimes there is choice though, and it takes choices from many folks to switch a community reply uoaei 4 hours agorootparentIf the wrapper is an extremely ergonomic Discord bot (which is probably what the other commenters are imagining or would settle on if they tried to implement it) then it might work but it still seems like a worse solution with a lot of extra overhead compared to software that already does what it&#x27;s meant to (and crucially, can&#x27;t do what it isn&#x27;t meant to). reply sysadm1n 11 hours agoprevRoll your own, like early and industrious folks in the 2000s did with PhpBB or Invision, or vBulletin if you had the money to fork out for VB. reply nvy 7 hours agoparentI know phpbb isn&#x27;t fashionable these days but if you set it up correctly it can be an amazingly useful tool.I know a guy who uses a private, single-user install of phpbb as his project management software. He&#x27;s got all the bells and whistles enabled like file uploads, media embeds, and markdown rather than bbcode. One subforum&#x2F;top-level category per project, and different threads tracking bugs, documentation, etc. Of course since it&#x27;s phpbb it&#x27;s available everywhere you have internet, and it&#x27;s responsive&#x2F;mobile friendly, etc.Definitely unorthodox considering github projects and redmine exist, but still really neat stuff. If only phpbb had a kanban board. reply codetrotter 5 hours agorootparentI ran phpBB for a while just for myself, partly out of nostalgia for the online forum days.I even used it for password management, because I was the only user, and I was not exposing it directly to the open internet. Instead, I had it listening only on the Wireguard interface of my computer. That way I could connect to it from all my devices anywhere in the world, while no one else could even attempt to reach it.However, because I was keeping those passwords on it I did not want to copy the data to any rented computers. And so I was running it from a computer at home without any redundancy.After a while, the SSD started to malfunction. No fault of phpBB of course. It was an old SSD I had bought several years ago. But because of that I gave up on running phpBB for myself for now.Maybe when I can afford some new, more reliable hardware and a spot in a data center for hardware that I own, I can one day return to running my own private phpBB instance accessible only to me over Wireguard reply EdwardDiego 6 hours agorootparentprevI was going to ask how he handles account creation fatigue, but then saw you&#x27;d specified single user. reply cloudking 6 hours agoparentprevhttps:&#x2F;&#x2F;flarum.org&#x2F; is a nice modern alternative, also free. reply NetOpWibby 5 hours agorootparentI haven&#x27;t seen Flarum in years, looks beautiful. reply nextlevelwizard 5 hours agoparentprevYeah, nothing like finding some old dusty phpBB that has been completely over run with bots spamming NSFW stuff. 5&#x2F;5 much better than Discord. Very industrious. reply ivanmontillam 6 hours agoparentprevAsmBB[0] if you feel bold!--[0]: https:&#x2F;&#x2F;board.asm32.info&#x2F; reply ShadowBanThis01 7 hours agoparentprevphpBB all the way.It gets the job done, and just about every host supports PHP. reply Gigachad 4 hours agorootparentHow are shared hosts at all relevant these days? For a few dollars a month you get a full linux VPS that you can install anything on. reply nologic01 4 hours agoprevPeople forget that there is a fashion and demographic element in these choices that can overwhelm rational argumemts about functionality et al.Discord had certain features that struck a chord with a generation of gaming communities, thats more or less all there is to it. Now people will pick it just to be trendy and in tune with the times.Personally I find its manipulative, over-emojed UI distasteful and it is hopeless for knowledge management. reply wodenokoto 4 hours agoprevI think there is a culture shift. When I was young all events would be on Facebook. For better or worse that created a source of truth (worse being that obviously we were forcing guests onto a platform that we shouldn&#x27;t be forcing them on) and a whole structure that kept track of things for you.Now a days, I see events being a post across a few chat groups, with no source of truth for time and place (and those often left out!)I have an impression that there is a whole new generation of people who _don&#x27;t_ want to post on or interact with a forum&#x2F;bulletin board style of interface, but want to be part of a \"living\" and instant group.I&#x27;m not part of any chat group where this is really working, but every meetup I go to, people are asking \"which chat group should I join?\" or \"why don&#x27;t I create a whatsapp&#x2F;discord for this meetup?\" reply KirillPanov 18 minutes agoparentI&#x27;ve noticed this too.Attention spans have shrunk all across society. But it seems worse with the younger folks because they don&#x27;t remember that it wasn&#x27;t always like this. I don&#x27;t think they are affected to a greater degree. It just doesn&#x27;t seem to occur to them that resisting it might be a worthwhile effort. reply _AzMoo 3 hours agoparentprevThis isn&#x27;t new though. Before Slack and Discord we were using IRC for this. Facebook serves a different need, and for me, was never the default place for event information. Up until the last 5-10 years we were always using email mailing lists, web forums, usenet, or IRC. reply KirillPanov 17 minutes agorootparentNobody ever used IRC as a project&#x27;s primary communication channel.Well nobody sane at least. Seriously, IRC was always an informal sideshow to the asynchronous mechanisms. reply phowat 6 hours agoprevEveryone is talking about forums in the comments but what I really miss are mailing lists. Those were, for me, a much better user experience than both discord and the old forums. reply sethammons 32 minutes agoparentAs someone who hardly ever attempted to use a mailing list, I preferred forums because I was worried of waisting everyone on the list&#x27;s time. In fact, the couple times I did use it, people complained about my usage. It felt like shouting for help in a room that may or may not be crowded. A forum felt more like putting a sign up and interested folks could engage though often nobody would. Chat is nice because there is an immediate back and forth and feels more beginner friendly &#x2F; welcoming. However, chat needs an FAQ section. I think LLMs trained on chat logs may be an interesting tool. reply baleo 6 hours agoprevLast time a gripe about discord being bad for documention was posted, someone linked Answer Overflow (https:&#x2F;&#x2F;www.answeroverflow.com&#x2F;) as a possible solution. Still requires people to use discords threaded feature and doesn&#x27;t capture chats, but helps with atleast having the questiond indexed in google and not sitting behind a walled garden reply Redoubts 6 hours agoprevYeah, you should probably use ponder https:&#x2F;&#x2F;we.phorge.it&#x2F;ponder&#x2F; reply spankalee 6 hours agoprevThis article (and the comments here so far) don&#x27;t mention Discord Forum Channels which solves the two actual usability&#x2F;discoverability concerns listedForum Channels FAQ: https:&#x2F;&#x2F;support.discord.com&#x2F;hc&#x2F;en-us&#x2F;articles&#x2F;6208479917079-...Forum Channels are a type of channel that&#x27;s structured as a more traditional forum. It&#x27;s similar to channels that tried to enforce that everything is a thread, but much better.* The top level shows posts only - no standalone messages* Selecting a post opens the thread to the right - it&#x27;s very easy to browse posts and threads* Posts are sorted by date or most recent activity* There&#x27;s a tag system and you can quickly filter by tags* There&#x27;s a combined new&#x2F;search field so you&#x27;re forced to see search results* Your active post threads show up in the left nav section like other active threads, making it pretty easy to see new replies.I don&#x27;t know if it&#x27;s the best forum system ever, but it&#x27;s pretty good and it&#x27;s right in Discord. And like another poster said, our users want Discord. reply croes 5 hours agoparentI can read forums without joining anything.That&#x27;s not possible with Discord. reply tacocataco 3 hours agoparentprevI&#x27;m pretty sure this feature is in beta and only available to bigger communities at the moment. reply sha-3 6 hours agoparentprevBut is it indexed by search engines? reply Phelinofist 2 hours agoprevA bit unrelated, but does someone use a forum at work? I think in times of remote work that could a better alternative to Teams discussions about features or bugs (especially from other teams&#x2F;departments) + the better search. reply sethammons 45 minutes agoparentI have never seen a company forum be well adopted. SO and&#x2F;or reddit clone. 6 engineers out of 100 in the org push for it and have to remind people it exists and people just keep asking questions in slack. People want to talk with people, esp. coworkers. reply KirillPanov 11 minutes agorootparent\"Look busy\" reply kobstrtr 5 hours agoprevAgree on the downsides, although communities are a great way to exchange and discuss a project. Instead, why don‘t we just keep the benefits of discord and build tools around it to manage its knowledge? We can publish it, we can use it for AI assistant, basically anything is possible.Disclaimer: I‘m the founder of awesomeqa.xyz reply RadiozRadioz 2 hours agoparent\"why make a good solution when we can bodge around a crappy solution using scraping and AI?\" reply heisenbit 2 hours agoprevMS Teams compared to Discord has imho a far superior implementation of the write once search forever paradigm. reply nonethewiser 7 hours agoprevSo long as we don’t lose sight of its utility as a chat app. I like that projects have that sort of communication channel. reply tilne 6 hours agoparentWhy? I think chat is a terrible way to organize this information. I don’t understand why people so readily accept working in that medium. It is all for the sake of people who don’t care enough to find the proper channels, or worse yet set them up in the first place. Even for small side projects, GitHub issues is just so so so much better than an unorganized chat history. There is no reason besides laziness to not impose at least a minimal structure on your forum&#x2F;support channels. reply bredren 6 hours agorootparentI suspect people are using it partly to deliver a more regular and visible heartbeat.Going to a project’s discord that has active messages feels like the thing is alive, and getting engagement from the maintainers or project owners feels more connected than replies in issues.GitHub issues is mechanically better but is (properly) spare in comparison.Creating an issue, (which occupies a number that can not be taken back), is heavy compared to a chat message.Discussions are GH’s answer to this but I think those muddy the waters. Having Discussions enabled on only some repos makes them unreliable. And each repo has different criteria for what should go in a discussion.As much as I avoid Discord, I don’t think the chat based medium is going away. If anything, I could see GH finding a way to enter that fray. reply EdwardDiego 6 hours agoparentprevI&#x27;m in a few FOSS chats, and I tend to find there&#x27;s a lot of people who a) need an answer right now and b) need relevant information pried out of them with a crowbar.And then they&#x27;ll DM you if you look helpful.I do appreciate the ability to on interact in real time, compared to mailing lists, but yeah, it tends to attract a certain segment of \"whatever it takes to ship\" instant gratification devs. reply TylerE 6 hours agoprevMust we beat this dead horse, AGAIN!? reply tacocataco 3 hours agoparentThe horse will be whipped until somebody invents the automobile. reply _pdp_ 4 hours agoprevOr use both? Why decide so early what works? Launch a discord server and create a dedicated forum and see which one gets most traction. reply hutzlibu 3 hours agoparentThat means double the work and half the engagement. reply koonsolo 4 hours agoprevFully agree with this reasoning.The problem though is that my community chose Discord, not me.I tried to move them away, didn&#x27;t work. reply andrewstuart 5 hours agoprevThe offered alternatives do not meet the need that discord provides for a community. reply xigoi 2 hours agoparentWhich is? Being able to react with cringe gamer emojis on messages? reply herbst 18 minutes agorootparentPretty sure he means the free for all spam they don&#x27;t care about. I mean how would you even start a project without spamming these days? &#x2F;s reply jhatemyjob 6 hours agoprevTerrible post.1. This is what the \"Ban\" feature is for.2. Anecdotal. Not really an issue in my experience.3. This is what the Discord API is for. There&#x27;s a really simple API to dump the chat history. Just make a cron job to dump every 10 min or so, and host that on a static site. Problem solved.4. See 3. reply jrflowers 6 hours agoparent> This is what the Discord API is for. There&#x27;s a really simple API to dump the chat history. Just make a cron job to dump every 10 min or so, and host that on a static site. Problem solved.I don’t see why anyone complains about Discord user experience when you can just manually implement basic functionality that the Discord devs didn’t see fit to put in.Why would anybody post on a forum when you could just write a custom client for Discord from scratch? reply herbst 16 minutes agorootparentThat&#x27;s kinda funny. I assume when discord wouldn&#x27;t have banned all the wonderful lightweight alternative clients that existed they would only get a small portion of the hate today reply MitPitt 6 hours agorootparentprevWhat are you talking about? You can&#x27;t write a custom client and the official one is closed source. Scraping chat history is also against the TOS and get get you blocked. reply jrflowers 6 hours agorootparentThat’s even better. Why would anyone post on a forum when learning the minutiae of the Discord API and its strict TOS conditions in order to implement functionality that’s broken in the client while avoiding getting banned is an option? It’s a dead simple solution! reply croes 5 hours agoparentprev1. Ban who? The other users just for posting.3. I think he writes as a user. Do you expect the user to export every discord chat history.4. See 3. reply DarkmSparks 6 hours agoparentprevplus the main advantage of discord is you can easily knock up a free Q&A bot that answers all the questions people ask because they didnt rtfm or search the forums. reply rbreaves 5 hours agoprevI think you should say the same about slack tbh. reply ronnier 5 hours agoprevIf only discord had threaded conversations reply bigstrat2003 4 hours agoparentIt does. Though maybe you&#x27;re being facetious - hard to tell through text sometimes. reply xigoi 2 hours agorootparentOnly on “servers” that enable it, which most of them don&#x27;t. reply m3kw9 5 hours agoprevI can just see people spamming emojis reply personjerry 3 hours agoprevThis article looks like it was written by ChatGPT tbh. The arguments are kinda weak, borderline nonsensical.> Chaos Discord can be a whirlwind of madness. Important stuff you post can vanish into the ether within seconds, drowned by a never-ending stream of messages.Already it looks like ChatGPT made a formatting mistake, what is \"Chaos Discord\"? Pretty sure it should says \"Chaos - Discord...\"It also concedes that they made threads a thing, so the point is moot. This is followed up with> TERRIBLE Search and Discovery Trying to find past discussions or solutions in Discord is like trying to locate a needle in a haystack blindfolded and drunk.Again there is no separation between the title and the content. And actually Discord search in my experience is really good. They can filter by media, link, text, user, and more. And this INCLUDES in threads. In comparison, have you tried searching through FAQs using Algolia or BBPost forum search in general? Those are truly awful.> The Discord Odyssey Picture this: You encounter a problem with a project that exclusively relies on Discord for support and questions.Notice the continued formatting failure, a failure of copy-pasting from ChatGPT and not separating the title from the content!This point is about all the restrictions and actions needed, but fails to address that users are already logged in and often have Discord open, and instead comes with some ? nonsensical example of installing a js library?And finally> Waaaay too ephemeral Here’s a curveball for you: what if Discord decides to pull the plug?This is interesting but, how you can really tell it&#x27;s ChatGPT is that the examples it provides as solutions below ALSO suffer this problem. Here are the options provided in the article.> Dedicated Community Forums - Check out platforms like Discourse.Again I&#x27;m pretty sure the search is worse, and it literally doesn&#x27;t solve any of the other problems.> Lean on the Pros - Websites like Stack Overflow or communities on RedditYeah I&#x27;m not sure how you&#x27;re going to use Stack Overflow as a Q&A for your own product. And again, WHAT IF THEY DECIDE TO PULL THE PLUG?> Git It Done - If you’re dealing with code issues, rely on GitHub, Gitlab, Gitea, or any other Git-based issue tracking systemActually not a bad idea if your audience is technical, but again WHAT IF THEY DECIDE TO PULL THE PLUG?Finally, nothing is more obvious than when ChatGPT says its signature \"REMEMBER, BLAHBLAHBLAH\", and lo and behold:> Remember, Discord is great & all but as a Q&A forum ...Personally, I&#x27;m 90% sure this is a ChatGPT generated low quality article. Even though it sparked some discussion here, I&#x27;m flagging this article, and I did not enjoy the article for the content. reply jansan 3 minutes agoparent> Yeah I&#x27;m not sure how you&#x27;re going to use Stack Overflow as a Q&A for your own product.How so? Just add a tag for your software and tell your users to write questions to Stackoverflow tagged accordingly. Then just put a watch on that tag. Almost zero setup effort. At least that is what I am planning to do. Am I missing something? reply xigoi 2 hours agoparentprevI don&#x27;t think ChatGPT would use such casual language, especially phrases like “Here’s a curveball for you”. reply personjerry 2 hours agorootparentI have seen it do so. Depends on the prompt, you can ask it to write in any style. reply MisterBastahrd 6 hours agoprevMaybe not, but uh... forums are not the easiest things to maintain without a lot of moderation tools at your disposal. Not when they get to a certain size. Unless you want half of your day job to be implementing forum features, I suggest purchasing a license to one that already exists for your own sanity&#x27;s sake. And if it runs PHP and has the ability to use Akismet, learn to configure it. reply javajosh 6 hours agoprevI&#x27;m surprised he didn&#x27;t mention wikis - especially easy since they come with git(hub|lab). reply SubGenius 7 hours agoprev [–] I&#x27;m building (with a fellow HNer) a publicly accessible Matrix server with forum-like features. Hoping to make it a decent foss alternative to discord and other closed-off community apps. [0][0] - https:&#x2F;&#x2F;shpong.com&#x2F;shpong reply MitPitt 6 hours agoparent [–] Looks very neat, how feature complete is it? reply SubGenius 6 hours agorootparent [–] Not a whole lot, but getting there. For now, we have:- matrix spaces not hidden behind auth- indexed by search engines- chat channels- subreddit like posts with nested replies, voting etc- private invite-only spacesHoping to get voice&#x2F;audio working soon, along with encrypted DMs. Experimenting with activitypub integration too. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author discourages the use of Discord as a Q&A (Question & Answer) forum for projects, citing its chaotic nature, subpar search and discovery features, and potential discontinuity risk.",
      "The text suggests better alternatives like dedicated community forums, reputable websites like Stack Overflow or Reddit, and Git-based issue tracking systems.",
      "The author emphasizes the significance of meticulously selecting a platform for Q&A purposes, acknowledging its impact on the project's progress and community interaction."
    ],
    "commentSummary": [
      "The primary focus of the debate is around the use of Discord as a Q&A forum, with views varying on its efficiency versus platforms like GitHub issues, dedicated forums, and mailing lists.",
      "Discord's informality and ease of use are praised, yet limitations in searchability and organization are highlighted. Choice of platform depends on individual needs, such as control, cost, ease of setup, and demographic trends.",
      "Talk around potential obsolescence of traditional forums and mailing lists, and the possibility of a more comprehensive platform with indexed search engines, nested replies, and voice functionality, were also mentioned."
    ],
    "points": 322,
    "commentCount": 198,
    "retryCount": 0,
    "time": 1694642089
  },
  {
    "id": 37497345,
    "title": "Skip the API, ship your database",
    "originLink": "https://fly.io/blog/skip-the-api/",
    "originBody": "Articles Blog Phoenix Files Laravel Bytes Ruby Dispatch Django Beats JavaScript Journal Docs Community Status Pricing Sign In Get Started RSS Feed READING TIME • 6 MIN SHARE THIS POST ON TWITTER SHARE THIS POST ON HACKER NEWS SHARE THIS POST ON REDDIT Skip the API, Ship Your Database Author Name Ben Johnson Twitter @benbjohnson With Fly.io, you can get your app running globally in a matter of minutes, and with LiteFS, you can run SQLite alongside your app! Now we’re introducing LiteFS Cloud: managed backups and point-in-time restores for LiteFS. Try it out for yourself! My favorite part about building tools is discovering their unintended uses. It’s like starting to write a murder mystery book but you have no idea who the killer is! History is filled with examples of these accidental discoveries: WD-40 was originally used to protect ICBMs from rust and now it fixes your squeaky doorknob. Bubble wrap was originally sold as wallpaper and now it protects your Amazon packages. When we started writing LiteFS, a distributed SQLite database, we thought it would be used to distribute data geographically so users in, say, Bucharest see response times as fast as users in San Jose. And for the most part, that’s what LiteFS users are doing. But we discovered another unexpected use: replacing the API layer between services with SQLite databases. How it started In the early days of LiteFS development, we wanted to find a real-world test bed for our tool so we could smoke out any bugs that we didn’t find during automated tests. Part of our existing infrastructure is a program called Corrosion that gossips state between all our servers. Corrosion tracks VM statuses, health checks, and a plethora of other information for each server and communicates this info with other servers so they can make intelligent decisions about request routing and VM placement. Corrosion keeps a fast, local copy of all this data in a SQLite database. So we set up a Corrosion instance that also ran on top of LiteFS. This helped root out some bugs but we also found another use for it: making Corrosion accessible to our internal services. Shipping the kitchen sink The typical approach to making data available between services is to spend weeks designing an API and then building a service around it. Your API design needs to take into account the different use cases of each consuming service so that it can deliver the data it needs efficiently. You don’t want your clients making a dozen API calls for every request! A different approach is to skip the API design entirely and just ship the entire database to your client. You don’t need to consider the consuming service’s access patterns as they can use vanilla SQL to query and join whatever data their heart desires. That’s what we did using LiteFS. While we could have set up each downstream service as a Corrosion node, gossip protocols can be chatty and we really just needed a one-way stream of updates. Setting up a read-only LiteFS instance for a new service is simple—it just needs the hostname of the upstream primary node to connect to: lease: type: \"static\" candidate: false advertise-url: \"http://corrosion-bridge:20202 And voila! You have a full, read-only copy of the database on your app. Moving compute to the client API design is notoriously difficult as it’s hard to know what your consuming services will need. Query languages such as GraphQL have even been invented for this specific problem! However, GraphQL has its own limitations. It’s good for fetching raw data but it lacks built-in aggregation & advanced querying capabilities like windowing. GraphQL is typically layered on top of an existing relational database that uses SQL. So why not just use SQL? Additionally, performing queries on your service means that you need to handle multiple tenants competing for compute resources. Managing these tenants involves rate limiting and query timeouts so that no one client consumes all the resources. By pushing a read-only copy of the database to clients, these restrictions aren’t a concern anymore. A tenant can use 100% of its CPU for hours if it wants to. It won’t adversely affect any other tenant because the query is running on its own hardware. So what’s the downside? There’s always trade-offs with any technology and shipping read-only replicas is no different. One obvious limitation of read-only replicas is that they’re read-only. If your clients need to update data, they’ll still need an API for those mutations. A less obvious downside is that the contract for a database can be less strict than an API. One benefit to an API layer is that you can change the underlying database structure but still massage data to look the same to clients. When you’re shipping the raw database, that becomes more difficult. Fortunately, many database changes, such as adding columns to a table, are backwards compatible so clients don’t need to change their code. Database views are also a great way to reshape data so it stays consistent—even when the underlying tables change. Finally, shipping a database limits your ability to restrict access to data. If you have a multi-tenant database, you can’t ship that database without the client seeing all the data. One workaround for this is to use a database per tenant. SQLite databases are lightweight since they are just files on disk. This also has the added benefit of preventing queries in your application from accidentally fetching data across tenants. Where do we take this next? While this approach has worked well for some internal tooling, how does this look in the broader world of software? APIs are likely stick around for the foreseeable future so providing read-only database replicas make sense for specific use cases where those APIs aren’t a great fit. Imagine being able to query all your Stripe data or your GitHub data from a local database. You could join that data on to your own dataset and perform fast queries on your own hardware. While companies such as Stripe or GitHub likely colocate their tenant data into one database, many companies run an event bus using tools like Kafka which could allow them to generate per-tenant SQLite databases to then stream to customers. Pushing queries out to the end user has huge benefits for both the data provider & the data consumer in terms of flexibility and power. LAST UPDATED • SEP 13, 2023 Share this post on Twitter Share this post on Hacker News Share this post on Reddit Previous post ↓ Automated Sentry Error Tracking COMPANY About Pricing Jobs ARTICLES Blog Phoenix Files Laravel Bytes Ruby Dispatch Django Beats JavaScript Journal RESOURCES Docs Support Status CONTACT GitHub Twitter Community LEGAL Security Privacy policy Terms of service Copyright © 2023 Fly.io",
    "commentLink": "https://news.ycombinator.com/item?id=37497345",
    "commentBody": "Skip the API, ship your databaseHacker NewspastloginSkip the API, ship your database (fly.io) 237 points by danielskogly 13 hours ago| hidepastfavorite166 comments Zvez 4 hours agoIf you give access to your DB directly, your API effectively becomes your API with all the contract obligations of the API. Suddenly you don&#x27;t completely control your schema: you can&#x27;t freely change it, you need to add things there for your clients only. I&#x27;ve seen it done multiple times and it always end up poorly. You save some time now by removing the need to build API, but later you end up spending much more time trying to decouple your internal representation from schema you made public. reply bennyelv 1 hour agoparentAbsolutely correct, listen to this article&#x27;s ideas with great scepticism!The system that I&#x27;m currently responsible for made this exact decision. The database is the API, and all the consuming services dip directly into each other&#x27;s data. This is all within one system with one organisation in charge, and it&#x27;s an unmanageable mess. The pattern suggested here is exactly the same, but with each of the consuming services owned by different organisations, so it will only be worse.Change in a software system is inevitable, and in order to safety manage change you require a level of abstraction between inside a domain and outside and a strictly defined API contract with the outside that you can version control.Could you create this with a layer of stored procedures on top of database replicas as described here? Theoretically yes, but in practice no. In exactly the same way that you can theoretically service any car with only a set of mole-grips. reply moomin 4 minutes agoparentprevI have spent my entire, long, career, fighting against someone who thought this was a good idea, unpicking systems where they implemented it or bypassing systems where this was implemented. It&#x27;s a many-headed hydra that keeps recurring but rarely have I seen it laid out as explicitly as this headline. reply DrScientist 22 minutes agoparentprevWhether it&#x27;s method calls or database schema - isn&#x27;t what really matters is control of what&#x27;s accessible and the tools you have to support evolution?So when you provide an API - you don&#x27;t make all functions in your code available - just carefully selected ones.If you use the DB schema as a contract you simply do the same - you don&#x27;t let people access all functions - just the views&#x2F;tables they need&#x2F;you can support.Just like API&#x27;s, databases have tools to allow you to evolve - for example, maintaining views that keep a contract while changing the underlying schema.In the end - if your schema dramatically changes - in particular changes like 1:1 relation moving to a 1:many - it&#x27;s pretty hard to stop that rippling throughout your entire stack - however many layers you have. reply DasIch 0 minutes agorootparent> Just like API&#x27;s, databases have tools to allow you to evolve - for example, maintaining views that keep a contract while changing the underlying schema.What are the database tools for access logs, metrics on throughput, latency, tracing etc.? Not to mention other topics like A&#x2F;B tests, shadow traffic, authorization, input validation, maintaining invariants across multiple rows or even tables...Databases often either have no tools for this or they are not quite as good. reply oreilles 3 hours agoparentprevVersioned views, materialized views or procedures are the solution to this. It is frequent that even internally, companies don&#x27;t give access to their raw data but rather to a restricted schema containing a formated subset of it. reply vbezhenar 1 hour agorootparentViews will severely restrict the kinds of changes you might want to do in the future. For example now you can&#x27;t just move some data from your database into S3 or REST service.Stored procedures technically can do anything, I guess, but at that point you would be better with traditional services which will give you more flexibility. reply indigo945 1 hour agorootparentA view can also do anything - it could query a REST service, for example. (Not saying that this is necessarily a good idea, though...) reply falcor84 41 minutes agorootparentIs that a real thing? What DBMSs support such views? reply jakewins 37 minutes agorootparentMost of the heavy artillery RDMSes at least, eg Postgres let’s you mount arbitrary HTTP resources as tables, which you then can put views over: https:&#x2F;&#x2F;wiki.postgresql.org&#x2F;wiki&#x2F;Foreign_data_wrappers reply tacker2000 2 hours agorootparentprevOf course it’s possible, but now you need more people with DB and SQL knowledge.Also, using views and stored procedures with source control is a pain.Deploying these into prod is also much more cumbersome than just normal backend code.Accessing a view will also be slower than accessing an “original” table since the view needs to be aggregated. reply _a_a_a_ 1 hour agorootparent> Accessing a view will also be slower than accessing an “original” table since the view needs to be aggregated.Where does it say anything needs aggregating. You can have a view that exists just for security.> Also, using views and stored procedures with source control is a pain. Deploying these into prod is also much more cumbersome than just normal backend code.Uh? This is normal backend code. reply madsbuch 1 hour agorootparentprevwhy would you want to develop your api in sql over a traditional language?versioned views and materialized views are essentially api endpoints in this context. just developed in sql instead of some sane language. reply vmfunction 2 hours agorootparentprevIn addition if you are using postgres, then there is postgresRest to make api really quick and nice. reply rewmie 1 hour agorootparentprev> Versioned views, materialized views or procedures are the solution to this.Wouldn&#x27;t it be far simpler to just create a service providing access to those views with something like OData? reply qwerty456127 1 hour agoparentprevThat&#x27;s [another reason] why you use stored procedures and only call them (rather than hardcoded or ORM-generated SQL queries) in your client app code. reply chedabob 59 minutes agoparentprevYeah this is my gripe with things like Firebase Realtime Database.Don&#x27;t get me wrong, the amount of time it saves is massive compared to rolling your own equivalent, but it doesn&#x27;t take long before you&#x27;ve dug yourself a big hole that would conventionally be solved with a thin API layer. reply DarkNova6 1 hour agoparentprevThis 100%.My last customer used an ETL tool to orchestrate their data loads between applications, but the only out of the box solution was a DB-Reader.Eventually, no system could be changed without breaking another system and the central GIS system had to be gradually phased out. This also meant that everybody must had to use Oracle databases, since this was the \"best supported platform\". reply Dudester230602 1 hour agorootparentOn the next iteration some consultancy will replace that with a bunch of microservices using a dynamic language.When that thing fails again they will hopefully settle on a sane monolithic API. reply MoSattler 4 hours agoparentprevI think this point is addressed in the article. reply londons_explore 1 hour agoparentprevYou could ship the database together with python&#x2F;JS&#x2F;whatever &#x27;client library&#x27; - and you tell your clients that they need to use your code if they want to be supported. reply knallfrosch 59 minutes agorootparentYou just know they&#x27;re going to run custom code, fck up their database and then still complain.I&#x27;m not tooo familiar with DBs, but I know customers. They&#x27;re going to present custom views to your client SDK. They&#x27;re going to mirror your read-only DB into their own and implement stuff there. They&#x27;re going to depend on every kind of implementation detail of your DB&#x27;s specific version (\"It worked with last version and YOU broke it!\"). They&#x27;re going to run the slowest Joins you&#x27;ve ever seen just to get data that belongs together anyway and that you would have written a performant resolver for.Oh, and of course, you will need 30 client libraries. Python, Java, Swift, C++, JavaScript and 6+ versions each. Compare that to \"hit our CRUD REST API with a JSON object, simply send the Authorization Bearer ey token and you&#x27;re fine.\" reply wmal 1 hour agorootparentprevThis is the worst of both worlds. Not only are you back to square one, as you spent the time to build an API (client libraries), but now, if the API is limiting, the users will find ways of accessing the SQLite db directly. reply oldnet 2 hours agoparentprevAlso you shouldn&#x27;t give up access to your DB for security reasons.That&#x27;s why API exists at first place. reply vbezhenar 1 hour agorootparentTechnically you can create different users with very precise access permissions. Might not be the good idea to provide that kind of API to the general public, but if your clients are trustworthy, it might work. reply Puts 3 hours agoparentprevYou can use stored procedures if you want to add another abstraction layer. reply Epa095 2 hours agorootparentThey had stored procedures in the \"old days\" when they figured out that direct access to the database was a bad idea, so what has changed? (I agree that a DB view often is good enough thoug, but they ALSO had that in the \"old days\", IDK what has changed about that:-p ) reply archibaldJ 1 hour agoparentprevyeah reminds me of meteor JS reply EspressoGPT 0 minutes agoprevThis must be from a person who never had to deal with when the database actually was the API. reply EMM_386 9 hours agoprevI&#x27;ve been doing this for a long time and all I can say this after reading this multiple times ... \"I don&#x27;t get it\".I mean, I get it, from a technical standpoint. Ok, so you&#x27;re going to send read-only Sqlite databases to everybody.Is it missing what the API (that you still need) is updating when you insert or update something and all client DBs are now stale? Is there a central database? How often are you pushing out read-only database replicas across the wire to all clients? Is that really less \"chatty\"? If so, how much bandwidth is that saving to push an entire database multiplied by the number of clients?None of this seems logical. Maybe I&#x27;m missing the real-world use-case. Are we discussing tiny Sqlite databases that are essentially static? Because in the last 30 years I&#x27;ve not run into a situation where I needed to have clients execute SQL queries on tiny, static databases let alone still need to potentially update them also. reply benbjohnson 9 hours agoparentAuthor here. We&#x27;re using LiteFS to replicate SQLite databases in real time so the changes sent are only incremental. I think there are several ideal use cases for sharing databases across applications:1. Internal tooling: you&#x27;re able to manage the contract better since you have control of the source & destination applications.2. Reporting & analytics: these tend to need a lot of query flexibility & they tend to use a lot of resources. Offloading the query computation to the client makes it easier on the source application.As for database size, the Corrosion program mentioned in the post is about 8GB and has a continuous write load so this doesn&#x27;t have to just be for tiny databases. reply ryanrussell 29 minutes agorootparentBen, fan of your work. You guys have really moved the flag on sqlite.Are there any plans for Corrosion to be published as OSS? reply rgavuliak 1 hour agorootparentprevDo you realize most reporting & analytics use cases don&#x27;t use SQLLite Databases? reply pininja 8 hours agorootparentprevI could imagine this technique being useful for kepler.gl or other data visualization tools reply LispSporks22 7 hours agorootparentprevCurious if you&#x27;ve tested it with Jepsen. Anytime I come across some distributed system, my stomach ulcers start playing up while I wonder about all the weird failure modes. I kinda looked for a caveats page on the LiteFS web site, didn&#x27;t really see one. reply TheDong 7 hours agorootparentLiteFS doesn&#x27;t try to be a correct distributed system, as you can see from: https:&#x2F;&#x2F;fly.io&#x2F;docs&#x2F;litefs&#x2F;how-it-works&#x2F;#cluster-management-...Basically, the solution they have is:1. There is a single writer. There&#x27;s optional best-effort leader election for the writer.2. If there&#x27;s a network partition, split-brain, etc, availability is chosen over consistency.Jepson&#x27;s testing is focused on databases that pick \"consistency\". Since LiteFS didn&#x27;t pick consistency, there&#x27;s really not any point in running Jepson against it. Like, jepson would immediately find \"you can lose acknowledged writes\", and LiteFS would say \"Yes! That&#x27;s working exactly as intended!\"However, another way of running LiteFS is with only a single writer ever (as in one app, one server, one sqlite database only), and all clients as read-only replicas that are not eligible for taking writes ever. In that case, you also don&#x27;t have a proper distributed system, just read only replicas, which is quite easy to get right, and mostly what this post is talking about. reply benbjohnson 5 hours agorootparentprevLiteFS works similarly to async replication you&#x27;d find in Postgres or MySQL so it doesn&#x27;t try to be as strict as something running a distributed consensus protocol like Raft. The guarantees for async replication are fairly loose so I&#x27;m not sure Jepsen testing would be useful for that per se.On the LiteFS Cloud side, it currently does streaming backups so it has similar guarantees but we are expanding its feature set and I could see running Jepsen testing on that in the future. We worked with Kyle Kingsbury in the past on some distributed systems challenges[1] and he was awesome to work with. Would definitely love to engage with him again.[1]: https:&#x2F;&#x2F;fly.io&#x2F;dist-sys&#x2F; reply lawik 5 hours agoparentprevI have more interest in this: https:&#x2F;&#x2F;electric-sql.com&#x2F;They are adding a sync-mechanism for Sqlite so local writes can be synced to a remote location and on into Postgres and so on. CRDT-based eventual consistency.So local write latency, eventually consistent sync and the tools for partial sync and user access are the primary areas of development.Early days but an interesting approach to shipping the db. reply anty 1 hour agorootparentOh, I implemented something like that for my Android app. It seems to work quite well. I don&#x27;t have many users yet, though.I replicate the clients Sqlite DB to a central server (over a REST-API) where it is synced with existing data. I use an CRDT, so changes don&#x27;t get lost and as long as the clocks of all the users devices are accurate, the merges are in the correct order.This enables offline access, but also the use of my app without an account. You can always merge the data later. Multi-device merge is also possible, if you create an account. Especially the multi-device merge was a big headache until I settled for this approach.Since it is a home-grown solution I still have to do some manual stuff that could be abstracted away, like converting datatypes to and from JSON, Kotlin, PHP, MySQL. There&#x27;s not always an equivalent datatype in those technologies.This approach probably won&#x27;t work well for big databases that share data between multiple users, though. reply zmmmmm 9 hours agoparentprevIt probably makes most sense if you have a middle-tier-less application where the UI IS the application, and effectively it just dumps the whole application state out through APIs anyway. We have ended up with this scenario playing out and you eventually just throw up your hands and make the UI hit the server with a giant \"send me a huge blob of JSON that is most of the tables in the database on first page load\" query anyway.So the assumption that \"if anybody changes anything everybody needs to know it\" is close to true. In that scenario, putting a bunch of APIs in the way just makes the data less coherent rather than more. In most other scenarios, yeah, it&#x27;s hard to see that it really makes sense. reply EMM_386 9 hours agorootparentI have never in my career spanning decades have I had to ask a server to send me a dataset so large that it \"most tables in the database on first page load\".In what use case do you run into that?I&#x27;m lead and an architect on an enterprise application at the moment that drives the whole company. It&#x27;s your standard configuration, front-end, APIs, SQL. The system requests what it needs to fulfill only what functionality the user is dealing with.Earlier in my career I was dealing with large enterprise desktop applications that talked directly to the database, with no centralized API. Some of them had thousands of individual desktop clients hitting a single multi-tenant SQL server. No problem, SQL Server would handle it without breaking a sweat. The bandwidth to an indidual client was fine. It was fast. And that was 20 years ago. reply mickael-kerjean 8 hours agorootparentI did faced this scenario a couple time when working on application that would work offline, a few of those I was involved in:- try to reduce the amount of paper based catalog we were sending out to customers, those were a couple thousands of pages and not cheap to produce, not cheap to send and would get deprecated very quickly. The web app would be pulling the entire catalog at first load so customers could go in remote location and still be able to use the catalog- a web app for sales that was intended to be use on customer site containing all the marketing materials and much more during presentations on site without ever having to connect anywhere reply cj 7 hours agorootparentprev> In what use case do you run into that?Single-tenant simple sites without permission checks with limited content volume.Of course you can also do the same in a multi-tenant environment but naturally you wouldn’t be returning all rows in the database. reply hobs 9 hours agorootparentprevThis is pretty much what many SPA were&#x2F;is, dump entire app state relevant on first load and then offline is no problem. reply emodendroket 5 hours agoparentprevI have seen some stuff where a streaming tool (Kafka or whatever) is used to just ship all updates to a database to certain clients. But I think this is a dubious architecture since it comes with basically all the downsides of a database that many applications all want to use besides the write contention one. reply jokethrowaway 9 hours agoparentprevImagine you&#x27;re building a SaaS which allows your users to do create a website, hotel booking platform and channel manaager.User can open the application, get the database, the frontend does all the offline updates the user want to perform. The user can update their website, add bookings they received on the phone, check what prices they set. This is all blazingly fast with no loading time, no matter your internet connection, because no further communication with the server is needed.At some point they press a magic Publish button and the database gets synced with upstream and uploaded. The upstream service can take the data and publish a website for its users, update availabilities, update prices, etc.It would be a better user experience than 99% of the SaaS out there. reply q7xvh97o2pDhNrh 9 hours agorootparentMost of the Internet spent the last 10-20 years moving away from this because business metrics generally benefit from realtime updates and background autosaves. By and large, users expect systems to autosave their work-in-progress nowadays.You might remember horror stories from the late 1900s when people would lose critical data and documents because they \"forgot to hit Save and the computer crashed.\" Or, maybe you&#x27;ve never experienced this — because a couple decades of distributed-systems engineers and UX researchers made that problem obsolete.So now we&#x27;ve... reinvented the Save button, but somehow needed a Paxos implementation to do it?Everything old is new again, I guess. reply Merad 8 hours agorootparentHeh, the last company I worked for had a (very popular, very successful) 20 year old desktop app that worked by downloading the client&#x27;s entire database on login, then doing regular syncs with the back end and sending batch updates as the user made changes. It was an awful system that everyone hated, and the company was desperately trying to figure out how to move away from it without doing a complete rewrite. Maybe I should let them know that they&#x27;re actually ahead of the curve if they can just hold out for a few more years... reply throwawaaarrgh 8 hours agorootparentprevHell, even with word processors we have auto save enabled by default for at least a decade. reply Freedom2 7 hours agorootparentprevI feel like I&#x27;m taking crazy pills. As HNers, we should want more control, not less over our work. Auto-save means that the company, no matter how nefarious, decides on their terms when my work is saved - regardless of what I&#x27;ve entered into the document (I write documents in a very stream of conscious manner).I want to decide when to save, and how, not the application. I am not a product, I am a user!!! reply lukevp 4 hours agorootparentSeems like on-by-default auto save with the option to disable, and a separate save button, satisfy all your requirements. That’s a pretty common set of customizations on saves from what I’ve seen. reply kristianp 7 hours agorootparentprevThis makes me smile, because it&#x27;s satire, right? reply porker 2 hours agorootparentNo. I value a meaningful \"Last updated at\" timestamp.If I open a word document, redact a few bits, save to PDF and close it - I don&#x27;t want my changes saved but they are (real scenario from last week. I&#x27;d only just moved the file into Onedrive so autosave had turned itself on)Ditto sorting and filtering spreadsheets.Software engineers: By all means save in the background so I never lose anything, but don&#x27;t assume I want the latest changes. reply rmbyrro 1 hour agorootparentWhat you want, then, is versioning of your documents. MS Office and Google Suite do that.This is not trivial to implement, though. Complexity will depend a lot of the application and the data. I&#x27;d say it&#x27;ll only make sense for mature apps. A startup will most likely don&#x27;t consider this in its roadmap. replyEMM_386 9 hours agorootparentprev> User can open the application, get the database, the frontend does all the offline updates the user want to perform.\"get the database\"How small do you think these databases are?!You&#x27;re going to download the entire hotel booking platform&#x27;s database?For how many hotels? One at a time, and then get another databse? Or are you getting a Sqlite booking database for every hotel in the world? And you&#x27;re going to send them to each user? For what date range?And even if that were possible, you then have to commit your offline updates. What if someone else booked the date range prior to you? Now your Sqlite copy is stale. Download the entire thing again? There could have been countless changes from other users in the time since you last got your copy.This explanation just leaves me even more confused. It&#x27;s illogical. reply yyyk 6 hours agorootparentI have some experience with a comparable platform. In a typical CouchDB&#x2F;PouchDB design, syncs and offline work are common and easy, and it&#x27;s pretty close to database-based design if you get fancy with views and javascript-based logic.For this project, I&#x27;d do:* A database for each user (this is natural on CouchDB, one can enable a setting to create the user DB automatically when a user is created. The platform won&#x27;t choke on many databases) - for some per-user data, like comments, if the user has already reserved a booking we can mirror booking data there + some info regarding the hotel.* Common synced databases - some general info regarding the platform and hotels. Preferably not too large (IIRC there&#x27;s no limit to attachments with 3.x, but it sounds risky). Anything too large we&#x27;d do online or use data provided with the application.* A large booking database which isn&#x27;t synced and must be modified online with a REST call - we don&#x27;t have to allow every action offline. Here I wouldn&#x27;t entirely dispense with API. This obviously needs the online component for making sure bookings don&#x27;t conflict. Could even be a regular relational database.I think it is possible to implement this the CouchDB database-way: a provisional offline reservation to the user database followed by some rejection method on the server when syncing, but I don&#x27;t think there&#x27;s much value to the user here. This design however would allow us to not sync all the data but a much smaller portion while supporting offline.---It sounds very doable, rather similar to a project I was involved with, but I miss SQL a lot with that platform (javascript NoSQL not my favorite to work with). A sqlite-based approach is an interesting alternative. reply layoric 7 hours agorootparentprevWeb applications aren&#x27;t going to get bigger just by themselves! &#x2F;sJokes aside, this extreme optimization for development does have impacts on user experience. The amount of bandwidth&#x2F;storage etc used by a \"just ship the whole db\" type applications would surely suck for most people outside of the &#x27;I measure my bandwidth in Gbps&#x27; crowd? reply throwaway290 3 hours agorootparentprevYou did not read the use case. It&#x27;s not equivalent of Booking. More like equivalent of Wix for hotels. reply hx8 9 hours agorootparentprevOnce upon a time, this is how applications worked and this was a pretty good experience.Now, you&#x27;ll introduce a huge amount of user frustration around why their changes never made it to the central database. If you have users closing a form, especially if it&#x27;s on a webpage, they are going to expect the write to happen at the same time the form closes. Making a bunch of changes and batching them in a single sync is going to be confusing to a ton of users. reply hamandcheese 9 hours agorootparentWhat happens if two users want to make changes at the same time? What if their changes conflict? How do you even detect conflicts? reply Ambolia 1 hour agorootparentIt could be done with \"select for update\" and etags+triggers checking the etag received + triggers generating a new etag on every create&#x2F;update. reply mrbadguy 45 minutes agorootparentIf everyone&#x27;s working on their own local copy of the database then the select for update isn&#x27;t going to do anything. The issue is later syncing and detecting conflicts. It&#x27;s actually easier to do this with a centralized DB, hence why everything works this way. If an app is mostly offline then, yeah, ship it with a SQLite DB and life will be good, but for something like a hotel booking app that doesn&#x27;t actually solve many problems and makes existing ones harder. reply vosper 9 hours agorootparentprevThanks for the example, it helped me understand the idea.The thing that I&#x27;m unclear on is how do I figure out what data to ship to the user? Like if I don&#x27;t already have a database-per-user then I have to extract only the data the user has permission to see, and ship that as a database?That would be the case even if I had database-per-customer - not every user is necessarily able to see all the data owned by the organization they&#x27;re a part of.It seems like a lot of extra work, and error-prone, too (what could be worse than accidentally shipping the wrong data _as an operational database_ to a client?)Edit: the article covers this at the bottom, but IMO it&#x27;s a show-stopper. How many applications of any real complexity can actually implement database-per-user (database-per-tenant is probably not enough, as I mentioned above). As soon as you need any kind of management or permissions functionality in your app then you can&#x27;t ship the database anymore, so you may as well start off not shipping it. reply Takennickname 5 hours agorootparentprevWhat is this? A scifi novel?By the time your dude downloads the database half the hotels on your imaginary website have changed status. reply RhysU 8 hours agorootparentprevWasn&#x27;t this Lotus Notes? reply whalesalad 9 hours agorootparentprevA lot of SPA’s already operate this by leveraging things like localstorage in the users browser. reply uoaei 4 hours agoparentprevI think the point is basically \"unless there&#x27;s a good reason for your API to look different than your DB schema, ontologically speaking, then the schema has already effectively defined your API and just let people interact with the DB\", or, alternatively, \"bake in as much of the data constraints as possible into your DB schema, and only when you can&#x27;t enforce them with column constraints or DB extensions should you add APIs on top\". reply greo 3 minutes agoprevI used to be a big fan of fly.io until I faced a lot of deployment issue. Often times the server will go down for few minutes and it was not even updated on the status page. Is this still the case or has things gone better? reply KRAKRISMOTT 10 hours agoprevIs this a true cloud managed SQLite or is this like their PostgreSQL documentation where it&#x27;s just a bunch of pre configured docker containers and the developer is expected to manage everything themselves? If the db goes down for an extended period of time at 3am, does fly.io have an engineer on call?I find that fly.io has been a very disingenuous startup. They position themselves as some sort of Heroku successor and hide behind their developer content marketing while providing very low quality infrastructure. At the end of the day, fancy blog posts may win hearts and minds, but downtime cost businesses actual money. reply impulser_ 10 hours agoparentI think you are thinking that Fly is like a serverless platform. They aren&#x27;t. They are the opposite. They are a server platform. They provide server for you and you have to manage your server yourself.Nothing they provide is managed by them. You have to do that.LiteFS is just a replication service for your sqlite database so you can keep your database synced across multiple nodes.https:&#x2F;&#x2F;github.com&#x2F;superfly&#x2F;litefsLiteFS Cloud which is the a service they provide just helps you backup and recover sqlite databases. You can do this yourself.https:&#x2F;&#x2F;fly.io&#x2F;docs&#x2F;litefs&#x2F;backup&#x2F; reply pests 10 hours agorootparentI go into a project folder and can run \"fly launch\" and then the application is alive in multiple regions around the world. Where exactly am I manging this server? reply impulser_ 10 hours agorootparentOpen up the Dockerfile they generate for you. reply pests 10 hours agorootparentHow does a Dockerfile change things? Your application is packaged into a container. The Fly service runs any application not just web applications or build artifacts. Of course your application is going to get a complete operating system environment to run in.I don&#x27;t think the Dockerfile has anything to do with them pulling your container from their registry and running it on their server. reply hamandcheese 7 hours agorootparentFly.io does not run containers. They convert your docker image to a VM. reply pests 6 hours agorootparentAh right okay thanks. The point still stands the Dockerfile and image is used as a container for your application and does not imply a manage-it-yourself server is being used. reply eddythompson80 9 hours agorootparentprevSo a scaffolding tool? reply TylerE 8 hours agorootparentprevThere is no such thing as \"serverless\", that&#x27;s just a higher than normal level of vendor lying. reply chrisoverzero 6 hours agorootparentI said nearly the same thing decades ago. Smashed a few friends’ so-called “wireless” telephones and showed them all the wires. Just look out a window; what’re those long, wiry things running between the utility poles, huh?! It’s ridiculous, the level of lying. reply rmbyrro 1 hour agorootparentI think it&#x27;s \"lying\" if you mislead another person.In those two examples: serverless and wireless phones, it&#x27;s not misleading consumers.Serverless is a way to communicate that the consumer won&#x27;t need to provision and maintain servers. It is true.Wireless phone means you can move for quite long distances while talking without any wire connecting your device to the wall. It is true.You may argue they&#x27;re \"misnomers\". But to call it a \"lie\" is quite a stretch... reply impulser_ 7 hours agorootparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Serverless_computing reply TylerE 7 hours agorootparentThe first sentence in your link includes \"in which the cloud provider allocates machine resources on demand, taking care of the servers\". =There is NO SUCH THING as serverless. There are always servers. reply nirushiv 10 hours agoparentprevCan second this. Tried them out at our startup, came away unimpressed. Their good engineering blog is great marketing though, although this particular post is lacklustre. What about permissions, indirections, versioning or otherwise encapsulating breaking changes? reply benbjohnson 9 hours agoparentprevAuthor here. I think we could have set better expectations with our Postgres docs. It wasn&#x27;t meant to be a managed service but rather some tooling to help streamline setting up a database and replicas. I&#x27;m sorry about the troubles you&#x27;ve had and that it&#x27;s come off as us being disingenuous. We blog about things that we&#x27;re working on and find interesting. It&#x27;s not meant say that we&#x27;ve figured everything out but rather this is what we&#x27;ve tried.As for this post, it&#x27;s not managed SQLite but rather an open source project called LiteFS [1]. You can run it anywhere that runs Linux. We use it in few places in our infrastructure and found that sharing the underlying database for internal tooling was really helpful for that use case.[1]: https:&#x2F;&#x2F;github.com&#x2F;superfly&#x2F;litefs reply pests 10 hours agoparentprevI was just setting up a new Fly account and launching a project yesterday. I will say their developer content marketing is very succesful and had me convinced they at least know what they are talking about.Any more stories or knowledge of them being disingenous? reply aledalgrande 9 hours agorootparentTheir DX is good. They just have a lot of reliability&#x2F;availability issues. reply jokethrowaway 9 hours agorootparentprevI wouldn&#x27;t say disingenuous but uptime has been disastrous.I&#x27;m definitely not buying anything there.I run a free service on fly.io.It&#x27;s been ok lately (or maybe my users just stopped complaining), so maybe they got better. reply RyeCombinator 9 hours agorootparentBut it&#x27;s free. reply rmbyrro 1 hour agorootparentIf the free juice they give you in the supermarket tastes bad, will you buy it?We assume the free version to have similar quality to the paid one. It&#x27;s in the vendor interest, so that we like it and become a paid customer. If they can&#x27;t convince us of their quality in the free version, how are we supposed to trust the paid one? reply TylerE 8 hours agorootparentprevFree can be the most expensive price of all. replyfreedomben 11 hours agoprevA very interesting idea to be sure, but IME the biggest downside (which tbf is mentioned in the article) is the contract. If you have clients with knowledge of and dependency on the schema, you can&#x27;t change it in a breaking way unless you update all the client&#x27;s code.I&#x27;ve tried various patterns in the past like one that just exposes database columns as an API, and this pain point always comes calling and it hurts. Keeping your data model simple and lean as possible is an important part of limiting complexity, which directly correlates with maintainability.The only pattern&#x2F;approach that I consistently return to is the Rails pattern (I use Elixir&#x2F;Phoenix now but same pattern). It is certainly not the most sexy, and having a client be able to graphql exactly what they need can be really helpful, but at least for me it has rarely turned out to be worth the tradeoffs. reply danielvaughn 10 hours agoparentThis is basically how Firebase is designed and it drives me nuts. I&#x27;ve freelanced with several teams, primarily comprised of front-end devs, who decided to use Firebase for their product. When they first told me they query the database directly from both their website and their mobile app, I immediately was like \"so...what happens if you need to change the structure of the database\"?Crickets. reply 10000truths 10 hours agoparentprevThe solution to this is the same as with APIs: versioning. Instead of naming your table \"my_foo\", you name it \"my_foo_v1\". Then, when you want to make a breaking change to the schema, you:1. Create a new table \"my_foo_v2\" with your desired schema2. Modify write queries for \"my_foo_v1\" so that they also write to \"my_foo_v2\"3. Copy over existing data in \"my_foo_v1\" to \"my_foo_v2\" with a migration script4. Modify read queries for \"my_foo_v1\" so that they read from \"my_foo_v2\" instead5. Remove all write queries to \"my_foo_v1\"6. Drop the \"my_foo_v1\" table reply JustLurking2022 10 hours agorootparentHoly hell does that sound awful. In practice, it doesn&#x27;t even solve the core problem as you&#x27;d have to deploy new versions of all binaries precisely in sync to avoid data incorrect because some clients know about the v2 table and others don&#x27;t. It becomes indistinguishable whether a row is missing because it was intentionally deleted or because it was written by a client who didn&#x27;t know any the new table version. There are ways to account for this but it&#x27;s layers of crap on top of crap. reply enragedcacti 10 hours agorootparent> it doesn&#x27;t even solve the core problem as you&#x27;d have to deploy new versions of all binaries precisely in sync to avoid data incorrect because some clients know about the v2 table and others don&#x27;tits painful but this approach does work as long as every client is migrated between each step.after 1: v1 is valid, reading v1, writing v1after 2: v1 is valid, reading v1, writing v1 and v2after 3: v1 and v2 are valid, reading v1, writing v1 and v2after 4: v1 and v2 are valid, reading v2, writing v1 and v2after 5: v2 is valid, reading v2, writing v2after 6: v2 is valid, reading v2, writing v2at each point, both the current and the previous version are reading a valid table and writing to a table whose values will make it into v2. reply nevir 7 hours agorootparentGood luck migrating every single client in the real world (unless you are willing to disable old clients & force an upgrade if they get too old) reply fbdab103 7 hours agorootparentprevIs there really any alternative? Data migrations are always going to be terrible. You need to slowly roll out the incremental changes so that everything is backwards compatible until you the threshold point where you can finally delete the historical route. reply nevir 7 hours agorootparentWith an API, you can often create facades and other patterns to avoid having to migrate the underlying data reply mvdtnz 4 hours agorootparentprevIn a sensible approach schema migrations are totally decoupled from the API. reply singron 10 hours agorootparentprevWith postgres, simple views are updatable, so you can often do this pattern without copying tables or dual writes. It&#x27;s particularly useful for renaming columns, but you can also do some other things. You also don&#x27;t have to use version numbers on all your tables since you only need the view temporarily. ALTER TABLE my_foo RENAME TO my_foo_tmp; ALTER TABLE my_foo_tmp ; CREATE VIEW my_foo AS SELECT ; COMMIT;DROP VIEW my_foo; ALTER TABLE my_foo_tmp RENAME TO my_foo; CREATE VIEW my_foo_tmp as select * from my_foo; COMMIT;DROP VIEW my_foo_tmp; COMMIT; reply munk-a 10 hours agorootparentWe actually used a similar tactic when transitioning our data to supporting soft deletion. We have a limited number of queries that update data or present data for administrative review and processing - but a lot of wild ones around reporting. We&#x27;d rename table `foo` to `foowithdeleted` and create a new `foo` view that excludes any soft-deleted rows. Our reporting queries keep on trucking like normal and read out of the `foo` table (now a view - but they don&#x27;t care) and we only need to adjust the administrative view to show soft-deleted rows for undeletion operations.Shell-gaming tables with views can be incredibly useful in postgres. reply manmal 2 hours agorootparentprevImagine you have Android & iOS apps which some of your users won&#x27;t ever update. You&#x27;ll never be able to drop any old version&#x27;s tables, and you&#x27;ll need to keep all table versions in sync more or less in real-time. Only when the last user has updated from a certain client version will you be able to remove that version&#x27;s tables. reply danielvaughn 9 hours agorootparentprevYou know, I&#x27;ve been toying with this very idea in my head for a while, as a way to make safe schema migrations. But I also know very little about databases so had no idea if this was a stupid idea or not. Glad to know I&#x27;m not alone.One issue I&#x27;ve considered is fk relationships - that could get complicated depending on the approach. reply clintonb 4 hours agorootparentSchema migrations are a solved problem. You make backward-incompatible changes in multiple steps.https:&#x2F;&#x2F;engineering.edx.org&#x2F;django-migration-donts-f4588fd11... reply mvdtnz 4 hours agorootparentprevI&#x27;m sorry that is insane. That approach wouldn&#x27;t pass the first sniff test of any sensible technical design review. reply SOLAR_FIELDS 10 hours agoparentprevI think about this with Postgrest&#x2F;Supabase which has a similar problem (autogenerated code based on the db schema is inherently going to conflict with api stability guarantees). I think that this approach is just fundamentally at odds with making a stable versioned controlled API.However: I think the best thing to do if you want a setup like this and want to have your cake and eat it too is something like this:Use your autogenned client for your “internal” api. This is for your clients with the autogenned schema that you directly control only so that you can ship changes and not have to worry about versioning&#x2F;backwards compatibility.Then for external users that have their own clients, you have that slimmed down more traditional API that offers less functionality but it’s properly versioned, tested etcI think this kind of hybrid setup can work well for SaaS setups where you have a cloud product that does internal stuff plus things external that end users need to operate on. You get the benefit of being able to iterate quickly without breaking your clients and since your external API is smaller it’s less maintenance overhead to keep it updated and versioned reply rmbyrro 9 hours agoparentprevThey mention one partial \"solution\" to this: have your clients query a view, instead of tables directly.When you need to change the schema, also update the view accordingly.Of course, there are still limitations. For one: you can&#x27;t \"massage\" data with a view as you could in the backend with a full featured programming language.PS: I don&#x27;t think this is a good solution, just mentioning it. reply floodle 10 hours agoparentprevIf you are in the situation where you are shipping a web app where one dev team controls the backend and the one and only client and ships both together, is this a big issue? reply simonw 11 hours agoprevI think there are some reasonable ways to avoid the schema brittleness that people are concerned about with this.Firstly, acknowledge that in this model your schema IS your API. Don&#x27;t include anything in the database that is being replicated that isn&#x27;t considered to be documented, tested and stable.With that in place, API changes become the same as changes to a JSON API or similar: you can only make backwards-compatible changes, like adding new columns or adding new tables.If you do need to make a backwards incompatible change there are ways to patch over it. Ship a brand new table with the new design, and include a SQL view that matches the name of the old table and exposes a compatible subset of it. reply maxbond 10 hours agoparentMaybe it&#x27;s better to have views in the first place, and to require (though it can only be enforced at an administrative level) external services to query against those views.We won&#x27;t have to do this switch to a view, we&#x27;ve paid that upfront.We can then include a version number in the view name, similar to how we do with APIs. So if we introduce changes we can keep services which didn&#x27;t need that change outside the blast radius.And we have a limited ability to enforce contracts and declare some columns internal. reply thestepafter 10 hours agorootparentEven better, use stored procedures &#x2F; routines on top of views and only query the stored procedures from the frontend. reply maxbond 7 hours agorootparentI like it, but the only benefit we&#x27;ve retained at that point is eliminating round trips. Which is pretty significant. But we&#x27;re still engaging in API design, we&#x27;re just doing it in Sqlite. reply jimmytucson 8 hours agoparentprevThat is an appealing idea but on second thought, haven’t you just shifted the problem to: how do I reliably replicate logical updates from my operational tables (internal, constantly changing) to my clean, public-facing tables?Basically, your public tables become materialized views over the “dirty” tables, and I’ve never met a mature, reliable implementation of materialized views, or even an immature one that is feature complete. (I would love to be wrong on the internet with this one!) reply javajosh 10 hours agoparentprevThis is correct, but there are very few people with experience with safe database modelling like this, and they may not even realize the technique exists. reply simonw 10 hours agorootparentThe same is true for APIs, too. Making changes to a JSON API that won&#x27;t break existing clients isn&#x27;t necessarily an obvious skill.But it can be learned, as can backwards-compatible database schemas. reply et1337 5 hours agoprevI have been thinking about this a lot lately. We have customers who run automation against us. They’re doing 10,000 API calls. Each one has to do a permission check, request signing, serialization, etc., etc. All just to mutate a blob of data less than say, 100MB. If they just downloaded the whole state, mutated it, and reuploaded it, the whole thing would be done in 2 seconds.We already lock the entirety of each customer’s data when processing a request, in order to avoid data inconsistency from concurrent mutations.One SQLite database per customer is a really appealing idea. reply knallfrosch 32 minutes agoparent> If they just downloaded the whole state, mutated it, and reuploaded it, the whole thing would be done in 2 seconds.Do you think your customers would accept this as a test? They would run against an in-memory database if they wanted to. reply rmbyrro 1 hour agoparentprevI&#x27;d think 100x before deploying this pattern to external customers.The use case shared by fly.io is for internal tooling. If you have a small team that most likely won&#x27;t grow to more than a few dozen devs, maybe you can get away with it.Shipping it to several external customers will be a horrible nightmare in maintenance and dev productivity. I&#x27;d switch jobs if I was working in a project that started applying this. Paying for the HTTP API upfront is a cheap price to avoid the costs of not having it down the road. reply ricardobayes 32 minutes agoprevI&#x27;ve seen teams take this ad literam and actually shipped an Elastic instance in an iFrame as a \"product\". Needless to say users abandoned it in no time. reply maxbond 11 hours agoprevA downside I didn&#x27;t see mentioned (it was gestured at with contracts and the mention of backwards compatible schema changes, but not addressed directly) was tight coupling. When you link services with APIs, the downstream changes of a schema migration end at the API boundary. If you are connecting services directly at the database level, the changes will propagate into different services.That would make me very nervous to ship something like this. You can probably cover a lot of this with views as a backwards compatibility shim, though. reply benbjohnson 9 hours agoparentAuthor here. I don&#x27;t see this is a general practice to be used for most applications. It was a side effect that we came across and it allowed us to share data between internal applications in some interesting ways. Internal apps are ideal since you have control of both sides of the contract. It definitely requires some more consideration if you&#x27;re publishing out to end users. reply maxbond 7 hours agorootparentHi Ben, thanks for taking the time to respond. It&#x27;s an interesting approach, and I&#x27;m sure that it has it&#x27;s place & you&#x27;ve made the right call for your particular situation.I didn&#x27;t think you meant pushing it out to end users, my point was more that this technique increases the blast radius. If three services are touching this database, and our changes now necessitate three deploys, that&#x27;s much higher risk.But it was an early comment, I hadn&#x27;t gotten to see the suggestions for managing this coupling yet. Each one of those does take us a step closer to designing an API, but perhaps part of the value here is that it allows you to move along a spectrum and to adopt the level of constraint you can afford, given the situation.I&#x27;m curious if this has spread to any other services at fly? Or is it just this Corrosion service? reply benbjohnson 5 hours agorootparentI think a spectrum is a great way to look at it. If you&#x27;re adding one service then it&#x27;s no big deal but as you add more and more then you get a better sense of the requirements of the API and I think you&#x27;re in a better place to build one out then.We&#x27;ve used this in a few other instances but Corrosion is the largest database we&#x27;ve done it with. reply Supply5411 8 hours agoprevI&#x27;ve written about this solution in the context of running LLM-generated SQL safely. I call it the \"cloned database view\" solution[1].The author touches on 2 issues that are tangential to this problem, restricting unauthorized data, and mutations, both of which are not really easily solvable with the cloned database solution.However, the underlying theme of skipping the API is extremely compelling. So much code, both FE and BE, are just to serve as an elaborate interface to the database. Simplifying this will be a huge gain in software maintenance. But the challenge is to do it safely.1. https:&#x2F;&#x2F;docs.heimdallm.ai&#x2F;en&#x2F;main&#x2F;blog&#x2F;posts&#x2F;safe-sql-execut... reply rmbyrro 1 hour agoparentThe pattern here involves database replicas, not clones.It&#x27;s not for a single-shot test, but for continued use with data synchronization. reply mkl95 4 hours agoprevIn the beginning of my career, I did an internship at a very small company that had been selling a desktop app for 20+ years, among other stuff. Each customer got their own database.The software was pretty stable and the team were part time consultants for that app, which mostly involved helping users perform complicated queries and creating stored procedures (they loved those). Subscription was allegedly quite expensive and had minimal churn over the years.For that kind of hyper stable software where each customer get their own database, I can see it working. In your typical 2023 SaaS where you are serving 1000s of users through a complex API and a database, I&#x27;m not sure a db can give you the level of flexibility and quality of life an API gives you. reply solatic 3 hours agoprevSounds like the next step is some way to define a multi-table view over a more traditional RDBMS (e.g. Postgres), serving as a source of truth, that can be continually exported as a read-only SQLite, so that the SQLite file can be pushed to the edge and read from the edge?I&#x27;m not sold on the idea of per-customer databases; businesses still need analytics, so if you&#x27;re not running your analytics on the centralized DB itself, your app needs to write&#x2F;push the data somewhere else, which is an unnecessary complication early in the product lifecycle at best and a recipe for frustration at worst. reply vincnetas 3 hours agoparentremember the times when software companies shipped software to users, and users used the software offline. No one from business even dreamed of knowing what each user was doing with the software at any moment in real time. What data users stored etc. Today it looks like this is MUST have for any software and no bussines is possible without this. reply shimst3r 1 hour agoprevAs someone who has worked as a data engineer on an organically grown monolith, the last thing you want to do is ceding control over your schema due to the lack of (sensible) abstraction. Everyone will change it, nobody cares about standardization. It’s utter chaos and a data engineering nightmare.This is the same, plus the lack of proper authN&#x2F; authZ. Good luck passing your next audit. reply lambda_garden 1 hour agoprevThis is not equivalent to (say) a single Postgres database where we get shared state with transactions between all users - or have I missed something? reply Hakkin 3 hours agoprevEven if you \"skip the API\" you still have to think about access patterns for the data, otherwise client-side query performance is going to be abysmal. You would basically have to define your \"API\" via what indexes you create on the data. reply abdullin 3 hours agoparentWith a local SQLite database you don’t need to pay cost of network transfers.A dozen of local SQL calls could be way faster than a single API call within a region. reply Hakkin 2 hours agorootparentThat&#x27;s true, but low query latency doesn&#x27;t mean much if you have to do a full table scan for your query. Some thought still needs to be put into how the clients will query the data, so it&#x27;s really not as simple as just \"ship your entire database to the client and let them figure it out\". reply jimmytucson 8 hours agoprevI love this kind of reasoning that forces you to climb out of the comfortable nook in a tree of thought you’ve been occupying for years to see if there’s a better vantage point!So would this work equally well? Use Postgres on some cloud and “spin up” a read-replica that your clients have access to. Let them read from the replica.If yes, I have tried that and I can tell you the problem I had. They don’t want to get to know my database schema. I know, frustrating! “If you would just get to know me better, you would have all the secrets you desire!” Sigh… they just want this overly simplistic Interface that doesn’t change, even when we rewrite things to make them better on the backend.Another thought: if sharing compute is the main problem, you can just egress the data to distributed file storage like S3 and let them write SQL on the files using some Presto, like Athena. Or egress to BigQuery and let them bring their own compute. But the problem there again is getting to know my internal data structure, and also by “egress” I really mean “dump the whole damn thing because once the data is on S3 I can’t just update a single row like I can on an OLTP database” and you can only do that every so often. reply benbjohnson 8 hours agoparentYes, Postgres replication would work as well. I agree that the \"getting to know the schema\" part is an issue. I think there&#x27;s use cases out there where you have power users that would gladly invest extra time in exchange for query flexibility.Querying to S3 is a good approach too. Those query tools can be somewhat involved to setup but definitely a good option. reply skybrian 8 hours agoprevIt sounds like this is essentially database replication, but perhaps cheaper, since it&#x27;s a SQLite file with a schema hopefully designed to contain whatever the client needs rather than random junk. You&#x27;d still have to send writes to one location and they&#x27;d fan out from there, with some latency.It seems like it would be a good fit with edge servers, assuming any node that starts an instance could get a copy of the database when it starts. Most database-as-a-service companies don&#x27;t do that.I doubt I&#x27;d try it if it isn&#x27;t a fully managed system, though. The only one I know of is Cloudflare&#x27;s Durable Objects, and it&#x27;s not in the free tier. For hobbyist programming that&#x27;s kind of a drag, so I haven&#x27;t tried it. Instead I&#x27;ll probably try Deno&#x27;s KV. (This sort of thing is what I hoped KV would be, but apparently it isn&#x27;t.)I wonder what the cold-start performance would be for downloading a small SQLite database from S3, starting it up in memory, and subscribing to a replication log? reply abdullin 5 hours agoprevThere also is a productivity boost within this approach.When you have a local DB as an integration point, you could just upload it to ChatGPT Code Interpreter for building extra reports and running analysis on it.Alternatively, upload the schema to ChatGPT and “write these few methods with unit tests for me, please”.I’m not saying that this approach is universal, but it saves me an hour of time now and then. reply thyrox 7 hours agoprevI was really excited about fly.io at one point but after reading all the recent posts(1) on HN I don&#x27;t have trust in their services anymore. Hope they are working on improving system stability first.(1) https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&page=0&prefix=true&que... reply tiew9Vii 7 hours agoprevThis looks interesting for read heavy distributed apps. I.E fly.io&#x2F;aws lambdas&#x2F;k8s.Each instance gets its own eventually consistent read only copy of the db. So instead of making a network request to go get some data, you query your local cache.Your use case is that you are read heavy for this to fit your needs.If you have a copy of the data local to you, you don’t need to go bug another team to expose it via an API.Reading fly’s blogposts it’d be good to see more on write durability. They’re batched on the client, batched on fly’s side before being compacted to s3 from reading so you can loose writes before your client as sent them to fly, possibly loose them before fly has written to s3 depending on how they implemented.FlyCloud might make a great service for small typical Wordpress type sites or low user services where writes are low. It’s a shame they don’t offer pay as you go for the LiteFS product only. I get why they don’t as they want you to host with them but it’d be great if you could pay as you go on the db only.I currently use DynamoDB for low volume stuff for cost reasons, it costs me cents. Productivity is extremely low though with Dynamo as you need to think very hard about table design and constraints and work around those constraints. It’s a lot more effort than using something like SQLLite for quick simple stuff. reply benwaffle 4 hours agoparentYou can use LiteFS Cloud without hosting your app on fly.io - https:&#x2F;&#x2F;fly.io&#x2F;docs&#x2F;litefs&#x2F;getting-started-docker&#x2F;#litefs-cl... reply k_vi 3 hours agoprevI&#x27;ve tried doing this read & updates using Postgrest and row-level-security using Supabase. When it works its an amazing experience but even for semi complex stuff you would still need to use Postgres RPC, which is still another \"API\" layer. I find writing API using SQL a nigtmare.Simple queries like this don&#x27;t work on Postgrest: `update likes set likes = likes + 1;` reply collaborative 5 hours agoprev> Moving compute to the clientThis is a great point and the reason why I think server-side rendering will ultimately fail.I&#x27;ve done something similar for a site I manage (6groups.com). Each client&#x27;s DB log is stored in compressed files on the server and only initialized via sqlite wasm on the client when needed. The trick is to periodically run compaction jobs (also on the client) to make sure DB logs don&#x27;t grow to unreasonable sizesI can&#x27;t imagine the DB consumption I&#x27;d have if all client data was in relational format on the server reply 1vuio0pswjnm7 7 hours agoprevMusing about APIs and the constant complaints about \"abuse\" and \"bots\", I think it would be an interesting experiment to set up an API where when a user hits the daily limit, instead of just being \"blocked\" (which according to complaints does not really work due to use of proxies), the user is sent a link to the database for download. The daily, updated database could be hosted on something like Backblaze. Would this satiate the demand for the data in a more efficient way than an \"API\". reply pm90 4 hours agoprevIt seems like sqlite is now acting as the API gateway. I think its an interesting idea, especially if you want to share relational data out to consumers, since they now have all the flexibility of sql. The issue though is updating the queries when schema changes are made. reply RamblingCTO 3 hours agoprev> Moving compute to the clientI thought we were slowly learning that this was a stupid idea in the first place and moving back to the old ways (tm)? reply chimen 10 hours agoprev\"Skip the api\" - seems to be stolen from sequin at: https:&#x2F;&#x2F;docs.sequin.io&#x2F;integrations&#x2F;airtable&#x2F;playbooks&#x2F;metab... (a few scrolls down) reply benbjohnson 9 hours agoparentAuthor here. I hadn&#x27;t heard of Sequin until today. \"Skip the API\" was just a snappy title so I went with it. reply emodendroket 5 hours agoprevSurely you will not ever regret exposing all the internal representations of your data to your clients. reply tiew9Vii 3 hours agoparentIt depends who your clients are. If clients are public&#x2F;external users, probably not.If clients are internal services you own and you control as part of a internal distributed service, it’s a nice pattern reply Zaheer 10 hours agoprevI&#x27;m curious the types of users this has. This feels like something that small to mid size companies may leverage but anything slightly more sophisticated would typically require access controls, merging data with other sources, etc. Doesn&#x27;t feel like a very scalable appraoch. reply benbjohnson 9 hours agoparentAuthor here. We do this with some internal applications that share internal state -- no customer data. I would expect stricter restrictions depending on the type of data shared and how bureaucratic the company is. reply zubairq 6 hours agoprevA very interesting idea. At Yazz we don&#x27;t do replication, but if someone creates a report in the tool and exports it as HTML then we actually export the entire app DB into Sqlite embedded in the HTML page, but any changes made to that embedded Sqlite DB don&#x27;t get replicated to the original database, so kind of like a read only copy, only suitable for reports reply throwawaaarrgh 8 hours agoprevA database isn&#x27;t designed to be used this way. You&#x27;ll eventually need it to do something \"extra\", and your company will probably find some workaround or hack, but eventually make an API.APIs are an intermediate layer of abstraction needed because their purpose is separate from that of a DDL&#x2F;DML&#x2F;DCL&#x2F;TCL. Having raw database access does not solve the use cases the database doesn&#x27;t account for. reply bjord 11 hours agoprevTo me, this looks a lot like Sequin (https:&#x2F;&#x2F;sequin.io), minus the ability to write back in the other direction (but plus open source!). reply LoganDark 6 hours agoprev> The typical approach to making data available between services is to spend weeks designing an APIWhat?No. reply quickthrower2 9 hours agoprevIs this spiritually just like using a Redis cache? reply langsoul-com 4 hours agoprevWhat&#x27;s the comparison of this approach VS directus, which auto creates an api layer on top of dB reply Takennickname 4 hours agoprevI asked chatgpt for a solution looking for a problem and it sent me here reply ilrwbwrkhv 7 hours agoprevIf I can&#x27;t update the data and I need an API to do that, then it&#x27;s a static site. I don&#x27;t need a database in the first place. reply giovanni_or2 4 hours agoprevBS. API layers exist for a reason. Maybe they will realize that reason if they try to expose the DB... reply munk-a 10 hours agoprevAn XMLRPC by any other name would smell as rancid. reply m3kw9 10 hours agoprevHow would I serve terabytes of stuff? reply benbjohnson 9 hours agoparentAuthor here. It&#x27;s not meant as a one-size-fits-all approach. It was an interesting side effect that we noticed that we used internally so I wanted to share my experience. The database mentioned in the post is about 8GB and it replicating it works pretty well. reply Petersipoi 9 hours agoparentprevYou wouldn&#x27;t use this if that was a requirement reply 3cats-in-a-coat 2 hours agoprevThe lifecycle of a startup full of young arrogant naïve inexperienced souls:1. Why is everyone doing all these complicated things?! We stopped doing them! We&#x27;re much smarter than everyone!2. Few years later.3. At great financial, technical and business cost we redesigned our system to do all those complicated things, because our initial version was an unmaintainable spaghetti monster that wrapped up everything its tentacles touched and hugged it until it died.There are situations when exposing SQL is a right call. I&#x27;d say those are pretty narrow, say when you have colleagues in a partner company doing integration. SQL is not a way to avoid having an API. SQL is an API. The surface of this API is gigantic, and it also binds your customers to a particular implementation of your database (the SQL vendor you happen to use). If those customers are few partners, this is easier to change. When it&#x27;s the public at large... you&#x27;re effed. This is all pretty basic systems analysis, but kids these days don&#x27;t know the first thing about it.But here&#x27;s a basic rule of thumb: exposing SQL as your API may be the right call if you&#x27;ve spoken in person with every customer who will be using that API, and especially if this in-person contact is regular, say at least once a month. If you haven&#x27;t, then SQL is not a good API for that customer. reply thinkingemote 2 hours agoparentThis is 100% the pattern. However most startups (and most software projects if we think about it) fail. They don&#x27;t make it to year 2.This in built failure rate shouldn&#x27;t be a reason against doing things right the first time but it might explain how many don&#x27;t get to learn the lessons from year 2. Most of the time we will never get to redesign and realise the mistakes we made. reply moomoo11 7 hours agoprev [–] What happened to keeping things simple and scaling them?Or did that train leave a long time ago? replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "LiteFS is a globally distributed SQLite database, originally designed to distribute data geographically, ensuring quick response times.",
      "The novel use of LiteFS was discovered, enabling it to replace API layers between services. This is achieved by distributing the complete database to client-side, allowing SQL queries and data joining per client's requirements.",
      "The replicas are read-only, necessitating an API for data updates. While it has some constraints, this technique can be beneficial in situations where conventional APIs are inadequate."
    ],
    "commentSummary": [
      "The article explores the concept of utilizing databases as APIs, outlining the potential challenges and obstacles of this method.",
      "The discourse recommends using API contracts and tools like views and stored procedures for managing changes in software systems. It also addresses such topics as the viability of downloading entire databases during initial page load, the difficulties in implementing a database-per-user model, and the issue of vendor dishonesty in serverless computing.",
      "An emphasis is placed on the benefits and drawbacks of using databases as APIs, including the practice of using views and table renaming in PostgreSQL for schema modifications."
    ],
    "points": 234,
    "commentCount": 162,
    "retryCount": 0,
    "time": 1694615875
  },
  {
    "id": 37499106,
    "title": "The Uselessness of Phenylephrine (2022)",
    "originLink": "https://www.science.org/content/blog-post/uselessness-phenylephrine",
    "originBody": "ADVERTISEMENT NEWS CAREERS COMMENTARY JOURNALS COVID-19 LOG IN BECOME A MEMBER Commentary Home Opinion Analysis Blogs GET OUR E-ALERTS HOME COMMENTARY BLOGS IN THE PIPELINE THE USELESSNESS OF PHENYLEPHRINE IN THE PIPELINETHE DARK SIDE The Uselessness of Phenylephrine 30 MAR 202212:00 AM ET BY DEREK LOWE4 MIN READ COMMENTS SHARE: Facebook Twitter Linked In Reddit Wechat Email This paper asks an excellent question that I have asked myself many times: why is phenylephrine still being sold as an oral decongestant drug? Well, OK, I know how we ended up in this situation, but why are we still there? Here's some chemistry and some history. First, we will examine a decongestant that actually works: pseudoephedrine. It's a natural product from plants of the Ephedra genus, especially E. sinica, where it occurs along with ephedrine and other alkaloids. The (S,S) and (R,R) enantiomers are known as pseudoephedrine, while the (S,R) and (R,S) pair are ephedrine. They're all phenethylamine stimulants, and there are plenty more where those came from - in fact, if you reduce off that OH group and just make that a methylene spacer, you have methamphetamine, which is technically also a name applied to that racemic pair, with the single chiral center remaining at the methyl group. Pseudoephedrine is an adrenergic receptor ligand, but it has far less stimulating properties than methamphetamine and a better pharmacological profile than ephedrine. It is famous for drying nasal passages during colds and allergies, at which it excels. Some people still notice amphetamine-like effects of sleeplessness and jitters at those doses, and at higher doses (not recommended!) pretty much everyone will. Across the board, these compounds can cause other CNS symptoms, increased blood pressure, loss of appetite, cardiac effects, difficulty urinating and more, but the window for these does seem to be widest with pseudoephedrine. Now we turn to phenylephrine. As you can see, that's a somewhat different structure - there's a phenol on the aryl ring, and there's no longer a chiral methyl group bretween the hydroxy and the N-methyl. It also has adrenergic effects, but different (and often weaker) ones than either ephedrine or pseudoephedrine. Its main effect seems to be raising blood pressure, and it has medical uses in that area as an addition to anaesthesia agents. But what it does not do well is act as a decongestant. There have been several controlled studies that show that it is indistinguishable from placebo in conditions like allergic rhinitis. Pseudoephedrine, however, is very clearly distinguished from placebo and in most people has very noticeable decongestant effects that last for several hours. Why is oral phenylephrine so useless? It is extensively metabolized, starting in the gut wall. You can find a bioavailability figure of 38% in the literature, but that appears to be the most optimistic number possible, and you can also find studies that show 1% or less. Overall, the Cmax is highly variable patient-to-patient, and the lack of cardiovascular effects at low doses argues for very low systemic effects (and expected low efficacy as a decongestant). The bioavailability increases at higher doses as you apparently saturate out some of the metabolic pathways, but at the 10mg dose typically used for decongestants, you can forget it. But here in the US, if you go to the drugstore and purchase an over-the-counter nasal decongestant (as a single agent or a combination of drugs that includes a decongestant), you will in every single case be buying phenylephrine. Which does not work. It is found (according to the paper linked above) in 261 different OTC products, and it is a useless bait-and-switch on the consumer in every one of them. I have always told friends and family members to avoid these products if at all possible, and to go back to the pharmacy counter to get something that actually works. This situation obtains, of course, because as mentioned above you can make methamphetamine from pseudoephedrine. There are a number of synthetic procedures for doing this, some of them quite alarming, and several of which can indeed be performed in the barn, garage, basement, or trailer park of your choice - if you can get enough pseudoephedrine. Thus the move to put medications containing this behind the pharmacy counter, with limits on their sale. No more walking in with a duffel bag and buying every single Sudafed package in the store. But it's quite possible that that era is gone anyway. The Mexican drug cartels have apparently been putting some real effort into process improvements and economies of scale, and I am told that the drug is (sadly) cheaper than it's ever been and available in higher purity than all but the most dedicated basement lab would be able to provide. As that article details, this includes an interesting and concerning increase in the compound's enantiomeric purity over the years. Whatever the current synthesis is - and you can be sure that the DEA knows the details, even if they are understandably not going into them in public - it is strongly skewed towards the more pharmacologically desirable D-methamphetamine. All this means that even if pseudoephedrine were more freely available, it might not be as much of an illegal article of commerce as it was twenty years ago. But be that as it may: the fact remains that its alleged replacement, phenylephrine, is of no real use and does not deserve its FDA listing. There's no reason to think that it's a safer compound than pseudoephedrine or one with fewer side effects - if you can get enough of it into your blood, you'll probaby have a rather similar profile. The only reason it's sold is to have some alternative to offer consumers, even if it's a worthless one. There have been several attempts over the years to do something about this (here's an earlier one from the authors of the current paper), but absolutely nothing has happened. Perhaps the agency does not wish to be put in the position of having nothing available than can be put out on the open shelves, and perhaps the pharmacies themselves prefer things as they are as well. It's for sure that the companies producing phenylephrine-containing products like the current situation a lot better than the alternative. But for people who actually want to be able to breath for a while as we enter allergy season, wouldn't it be better just to stop pretending and to stop wasting everyone's time and money? ABOUT THE AUTHOR Derek Lowe email Derek Lowe, an Arkansan by birth, got his BA from Hendrix College and his PhD in organic chemistry from Duke before spending time in Germany on a Humboldt Fellowship on his post-doc. He’s worked for several major pharmaceutical companies since 1989 on drug discovery projects against schizophrenia, Alzheimer’s, diabetes, osteoporosis and other diseases. COMMENTS IN THE PIPELINE Derek Lowe’s commentary on drug discovery and the pharma industry. An editorially independent blog, all content is Derek’s own, and he does not in any way speak for his employer. ADVERTISEMENT YOU MAY ALSO LIKE 1 AUG 2023BY DEREK LOWE A Room-Temperature Superconductor? New Developments 2 AUG 2023BY DEREK LOWE A New Mode of Cancer Treatment 25 AUG 2023BY DEREK LOWE Superconductor (?) Update 26 JUL 2023BY DEREK LOWE Breaking Superconductor News VIEW MORE ADVERTISEMENT CATEGORIES The Dark Side Regulatory Affairs ARCHIVES Select Year: 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 2006 2005 2004 2003 2002 January February March April May June July August September October November December VIEW POSTS RELATED JOBS ADVERTISEMENT Skip slideshow NEWS All News ScienceInsider News Features Subscribe to News from Science News from Science FAQ About News from Science CAREERS Careers Articles Find Jobs Employer Profiles COMMENTARY Opinion Analysis Blogs JOURNALS Science Science Advances Science Immunology Science Robotics Science Signaling Science Translational Medicine Science Partner Journals AUTHORS & REVIEWERS Information for Authors Information for Reviewers LIBRARIANS Manage Your Institutional Subscription Library Admin Portal Request a Quote Librarian FAQs ADVERTISERS Advertising Kits Custom Publishing Info Post a Job RELATED SITES AAAS.org AAAS Communities EurekAlert! Science in the Classroom ABOUT US Leadership Work at AAAS Prizes and Awards HELP FAQs Access and Subscriptions Order a Single Issue Reprints and Permissions TOC Alerts and RSS Feeds Contact Us FOLLOW US GET OUR NEWSLETTER © 2023 American Association for the Advancement of Science. All rights reserved. AAAS is a partner of HINARI, AGORA, OARE, CHORUS, CLOCKSS, CrossRef and COUNTER. Terms of Service Privacy Policy Accessibility",
    "commentLink": "https://news.ycombinator.com/item?id=37499106",
    "commentBody": "The Uselessness of Phenylephrine (2022)Hacker NewspastloginThe Uselessness of Phenylephrine (2022) (science.org) 221 points by ctoth 16 hours ago| hidepastfavorite320 comments buildsjets 16 hours agoThe fact that phenylepherine is ineffective is quite old and has been common knowledge for many years. What is news is that the FDA is finally acknowledging it. I&#x27;ve maintained my own reserve of pseudoephedrine for years.https:&#x2F;&#x2F;gizmodo.com&#x2F;report-many-over-the-counter-decongestan...https:&#x2F;&#x2F;www.forbes.com&#x2F;sites&#x2F;daviddisalvo&#x2F;2015&#x2F;10&#x2F;26&#x2F;the-pop...https:&#x2F;&#x2F;www.newscientist.com&#x2F;article&#x2F;2089555-breaking-bad-sn... reply jacobsimon 16 hours agoparentI just came here to say that I actually find phenylephrine nasal spray quite effective - but I use it for a different use case: stopping nosebleeds quickly. It&#x27;s quite effective at that and came recommended by an ENT, so I do believe it&#x27;s an effective vasoconstrictor when applied locally and I&#x27;m glad it&#x27;s available. reply kstrauser 16 hours agorootparentThat&#x27;s legitimate. The FDA&#x27;s issue with oral phenylephrine is that it&#x27;s metabolized in the gut before it gets to the sites where you want it to work. reply cyberax 15 hours agorootparentI wonder if it will work as a suppository. reply wmanley 15 hours agorootparentIt’s worth a try. I’d recommend buying a separate spray for your nose though. reply kstrauser 15 hours agorootparent“This medicine smells like crap.” reply phone8675309 14 hours agorootparentprevReminds me of the old joke: what&#x27;s the difference between an oral thermometer and a rectal thermometer?Answer: the taste reply aniken 11 hours agorootparentThis reminds me of the funniest scene in the movie Idiocracy, where he loses track of which cable goes in the mouth vs ear vs butt. A laugh a day keeps the doctor away, enjoy :-)https:&#x2F;&#x2F;youtu.be&#x2F;zop3VG_K9HU?si=k8LDptGhvQDxtsTB reply trifurcate 7 hours agorootparentFor the future, please remove the ?si= parameter from your YouTube share links. It uniquely identifies you and allows Google to track your social circle with certainty. (Not that they couldn&#x27;t do it already) reply Scoundreller 6 hours agorootparentwhat if he wants google to know his love for idiocracy? reply tlavoie 14 hours agorootparentprev\"Oh, Dad...\"Seriously, my late father _loved_ that joke, as do I. reply phone8675309 13 hours agorootparentI heard that for the first time from my dad. It&#x27;s a certified dad joke. reply tlavoie 13 hours agorootparentAs a certified dad, I have to agree. replymattkrause 13 hours agorootparentprevYup!https:&#x2F;&#x2F;my.clevelandclinic.org&#x2F;health&#x2F;drugs&#x2F;20786-phenylephr... reply joe5150 12 hours agorootparentprevPhenylephrine is a popular topical and suppository treatment for hemorrhoids. reply apatheticonion 2 hours agorootparentprevI wonder if it could be applied to the under-eye area to reduce dark circles caused by thin skin. People use caffeine topically for this reason.EDIT: Looks like it does work for this! Nice! reply dreamcompiler 10 hours agorootparentprevThis. Phenylephrine works great as a decongestant nasal spray, and it&#x27;s not as addictive as oxymetazoline. (It doesn&#x27;t last as long either, but that doesn&#x27;t bother me.) reply rscho 14 hours agorootparentprevYou are correct. reply raverbashing 16 hours agorootparentprevAs per the article, you would likely be correct reply EdwardDiego 5 hours agoparentprevIn NZ, pseudoephedrine products were banned, and now no-one smokes methamphetamine at all! Totally worth removing an effective medicine from the market.Haha, I am of course kidding, the meth trade is fuelling gang violence harder than ever, and it now really sucks if you get heavily congested sinuses.When I&#x27;m travelling via Australia I try to buy as much pseudoephedrine containing medicines as I think I can bring back to NZ without looking like a smuggler, and then ration them out to family members very begrudgingly.Meanwhile, methamphetamine is coming into the country in large quantities in shipping containers (along with illegal firearms). Banning pseudoephedrine was totally worth it...And now something similar is occurring with cough suppressants, the ones that contained dextromethorphan, i.e., the ones that worked, turns out some people were abusing them recreationally, (DXM is a disassociative in high enough doses), so now that&#x27;s gone, and we&#x27;re left with pholcodine, which I find far less effective, but that too is looking likely to be banned, because apparently that&#x27;s abusable too.I&#x27;m not sure our regulators have the balance right. reply stephen_g 2 hours agorootparentYou really can&#x27;t get it at all? Wow! I&#x27;m in Australia, they just have an ID check and I think they just record how many times you&#x27;ve bought it over whatever time period in a shared database.It works so well (and the PE version of everything is so useless), it must be really crap not to be able to get any! reply EdwardDiego 1 hour agorootparentMight be available by prescription, but no chemist stocks it, to be fair, they were getting ramraided a bunch back in the heyday. reply cheald 16 hours agoparentprevIndeed. I&#x27;ve known that phenylepherine is worthless for a long while, and just make it a point to buy a pack of the real stuff every 6 months or so, so that I always have it on hand when I need it. In my state, it&#x27;s controlled - you have to get it from a pharmacist (though you can do so without a prescription) - they take your driver&#x27;s license and don&#x27;t allow you to purchase it too often. reply anamexis 16 hours agorootparentThat is a US federal law (the Combat Methamphetamine Epidemic Act of 2005) reply jimmaswell 15 hours agorootparentAnd now we&#x27;ve got a fentanyl epidemic filling the void and even more power handed to Mexican cartels since meth got harder to produce domestically. Drugs continue to win the war on drugs. reply akira2501 15 hours agorootparentYou can&#x27;t fight a war against inanimate objects. You can fight a war against drug addicts. You cannot possibly win a war against drug addicts without addressing the fundamental problem. reply CoastalCoder 13 hours agorootparentUntil the past few years, I held a similar view. I.e., if we could just dry up the demand for the product, the rest of the system would crumble.Then the whole Sackler &#x2F; Oxycontin thing came to light. And I saw (well, second hand) a normal person get hooked on pain meds after back surgery, thanks to those assholes. And they&#x27;re not in prison.That&#x27;s when I concluded that we lack the collective will to take the necessary steps to fully stop the trade of addictive drugs. reply AtlasBarfed 11 hours agorootparentprevThe problem with decriminalization&#x2F;legalization is that those are the most abused drugs.However, our drug war is causing Mexico to descend in to anarchy and people are dying by the thousands, or living in total fear of cartels that we trained in the School of the Americas.Thus, I conclude we have to decriminalize and treat our addicts. Enforcement is not working. If we had a sane drug policy 30-40 years ago, Mexico might be a healthy vibrant trading partner. reply JoeAltmaier 15 hours agorootparentprevMost of us cannot make these drugs ourselves. To restrict them effectively would certainly curtail their abuse? reply akira2501 15 hours agorootparentThe presumption here is that an addict, unable to get their \"favorite\" drug, will now just stop doing drugs entirely. This is not the case.Drugs can also be stolen, from homes, and from pharmacies, and from ambulances.There are way more vectors than people care to ponder. If you&#x27;ve ever had an addict in your life, you&#x27;re forced to. reply salawat 4 hours agorootparentIn other news, the bureau of labor reports that the number of unemployed chemists have skyrocketed, and lab equipment manufacturers are reporting a surprise bump in sales.If the mountain will not go to... reply 01100011 15 hours agorootparentprevShake-n-bake meth was relatively easy to do for the average methhead.Also, do not underestimate the capabilities of a meth user when they get fixated on something. reply lazide 14 hours agorootparentBonus - meth causes among other things incredible focus. reply paiute 11 hours agorootparentI’ve noticed the comments on hn become more meth curious over the last couple of years… I know the state of CO has been pressuring doctors not to prescribe aderall. Mine got taken away because i only took it when i needed to force myself to really work (a few times a month). Now i use pseudoephedrine. reply MockObject 14 hours agorootparentprevHard drugs have been illegal in America for generations, and yet every city has had a junkie community all this time. How much more failure must we entertain? reply p_j_w 12 hours agorootparentprev\"We just need to wage the war on drugs harder, then it will surely work.\" reply fragmede 13 hours agorootparentprevVice or someone reports that you can make the new formulation, in a backpack, in a Walmart bathroom, with supplies you haven&#x27;t even bought yet. Given that, and, well, the Internet, I&#x27;m sure making it isn&#x27;t hard for an addict to learn how to make these drugs, even though it&#x27;s not common knowledge and you and I can&#x27;t do it. reply KerrAvon 10 hours agorootparentprevIt turns out it&#x27;s not possible to do that. We&#x27;ve been trying since the 1960&#x27;s. reply lazide 9 hours agorootparentLonger than that if you count one of the OG drugs - Alcohol reply underlipton 14 hours agorootparentprevThe \"fundamental problem\" is the breakdown of community (drug abuse being primarily being a function of the existence or lack of interpersonal involvement and responsibilities that would preclude spending time and money on them). People who are comfortable huddling in their cars, homes, and exclusive social clubs, and who would rather spend money on cops rather than social programs, aren&#x27;t going to like the fix for that. reply ohthehugemanate 14 hours agorootparentDo you have a source for this?> drug abuse being primarily being a function of the existence or lack of interpersonal involvement and responsibilities that would preclude spending time and money on themAFAIK there have always been addicts in every culture. Alcohol, various drugs, and strongly self destructive behaviors are not recent additions to the human experience. But I don&#x27;t know any if the numbers or the research on this root cause you mention. reply jasonfarnon 11 hours agorootparent\"AFAIK there have always been addicts in every culture.\"Surely you would agree that there the proportion of addicts varies by culture and society, that the US for example has a much higher proportion than many poorer nations? I don&#x27;t know about the first comment&#x27;s claim that lack of interpersonal relationship is the root cause, but a strong social component looks obvious to me. reply lazide 9 hours agorootparentCite? Every nation has an underbelly.Poorer nations, those folks tend to die pretty fast though. reply wheremiah 4 hours agorootparentThere was this article about Iceland combatting teen drug abuse by providing alternative activities and social outlets:https:&#x2F;&#x2F;www.naturalhigh.org&#x2F;icelands-radical-transformation-... reply post_below 12 hours agorootparentprevI suspect they&#x27;re referring to experiments like the famous \"Rat Park\". There is clearly a strong social and environmental component to addiction susceptibility, at least in rats.I don&#x27;t know if I&#x27;d go as far as to say it&#x27;s the fundamental problem, but definitely a major factor. reply paganel 5 hours agorootparentprevNot the OP, but bring back real jobs and real way of spending your time (like real family and friends) and much of the meth problem will get solved by itself.> AFAIK there have always been addicts in every cultureAlcohol addicts are a totally different thing compared to drug addicts. You can die as a result of alcohol, but there&#x27;s nothing like fentanyl in the alcohol world (I&#x27;m talking about the scale of the disaster, I know about the few alcohol intoxications here and there). reply fragmede 3 hours agorootparentThere is in countries where alcohol is illegal, like in the Middle East. There, wet have similar problems as we have in the western world with poisoned supply - people buying liquor, but it turns out to be bleach. So I&#x27;d say bleach in your liquor is the fentanyl of the alcohol world. We don&#x27;t see that in countries where alcohol is legal though, hmm. reply underlipton 9 hours agorootparentprevNo, it&#x27;s merely a personal observation. In that vein, I&#x27;d think it more correct to say that there are always addicts on the margin of every culture. reply robocat 11 hours agorootparentprev> social programsDo you have any links to social programs that have successfully dealt with meth addiction?Can&#x27;t say I have seen any locally (New Zealand) and we are a country more likely to see such programs? reply underlipton 9 hours agorootparentI don&#x27;t, but I&#x27;ll ask you to consider the following:1) The implication of my statement wasn&#x27;t that social programs would solve the problem single-handedly, but that they&#x27;re more effective than enhanced law enforcement spending.2) This would seem to be, at the very least, supported by the people who advocated for treating the (mostly white and middle-class) victims of the opioid epidemic in the US as suffering from a medical rather than criminal affliction. reply dumpsterdiver 6 hours agorootparent> I don&#x27;t, but I&#x27;ll ask you to consider the following: 1) The implication of my statement wasn&#x27;t that social programs would solve the problem single-handedly, but that they&#x27;re more effective than enhanced law enforcement spending.What do police have to do with the effectiveness of social programs? A fundamentally ineffective social program will be just about as ineffective no matter how much funding it gets - because the problem they are approaching isn&#x27;t one that they can solve. replycyberax 15 hours agorootparentprevMeth is not harder to produce. It&#x27;s now mass-produced from P2P, with chiral purification using various methods (like pulling one enantiomer with tartaric acid). reply tptacek 12 hours agorootparentYes. But now it&#x27;s produced in para-industrial labs, mostly in other countries, rather than in apartments and back yards, where those ad hoc labs were actually real public safety problems. reply bee_rider 11 hours agorootparentI wonder if the users are sad to miss out on the artisanal small-batch experience. reply jimmaswell 8 hours agorootparenthttps:&#x2F;&#x2F;slate.com&#x2F;news-and-politics&#x2F;2021&#x2F;03&#x2F;montana-steve-da... reply WalterBright 7 hours agorootparentprevAdding chili powder was his trademark. reply AnthonyMouse 11 hours agorootparentprevIsn&#x27;t increased financing of major drug cartels a serious public safety problem?Weren&#x27;t most pseudoephedrine-to-meth operations in rural areas or otherwise situated away from the general public to avoid detection? reply lazide 9 hours agorootparentWas Al Capone a public menace, or a businessman saving countless thousands from methanol poisoning and distilling accidents?Plenty of these stories documented either way: https:&#x2F;&#x2F;www.cbsnews.com&#x2F;amp&#x2F;pittsburgh&#x2F;news&#x2F;police-erie-boy-... reply AnthonyMouse 5 hours agorootparentPor que no los dos?But any \"benefit\" of the likes of Al Capone ended with prohibition, i.e. the thing forcing the alternatives to be methanol poisoning or mob hits.And you wouldn&#x27;t need to check ID to rate limit cold medicine if you would just check ID to rate limit what people make out of it. reply toast0 9 hours agorootparentprev> mostly in other countriesAnother case of needless regulation driving outsourcing, and putting our small manufacturing business out of work. reply _a_a_a_ 11 hours agorootparentprevI&#x27;m curious about the chemistry here (though I have very little chemistry background), so, bearing in mind that I have zero interest in producing methamphetamine, perhaps you can elaborate a little:– I can&#x27;t see how Phenylacetone can have chirality– I don&#x27;t understand how any enantiomer of P2P (if one exists, see above) can be selected for specifically via a chemical reaction, especially here with tartaric acid, which is a chiral molecule itself, so surely you would need a specific enantiomer of tartaric acid to start with?I&#x27;m sure these are basic questions, I&#x27;m just a dabbler. Any links welcome, no problem if this is out of place for HN.TIA reply cyberax 11 hours agorootparentP2P doesn&#x27;t have chirality, but methamphetamine does. Any carbon atom with 4 different groups attached to it will always be chiral, in meth it&#x27;s the carbon next to the nitrogen. It has: methyl group, phenol group, hydrogen, and the amino group.> – I don&#x27;t understand how any enantiomer of P2P (if one exists, see above) can be selected for specifically via a chemical reactionYou just react P2P and get a racemic mix of L- and D- enantiomers of meth. Then you react it with tartaric acid, which will preferentially react with the \"right\" meth.> tartaric acid, which is a chiral molecule itselfAh, I see the confusion. Tartaric acid derived from biological sources consists of just one enantiomer, that&#x27;s why you can use it to do chiral resolution. reply _a_a_a_ 1 hour agorootparentThanks, very informative! reply supportengineer 14 hours agorootparentprevI would like to see “taxes” be the winner of the war on drugs reply elgenie 9 hours agorootparentprevDrugs winning the “war on drugs” would be a problem for the “war on drugs” only if reducing drug use had actually been a priority of the “war on drugs” at any time in its half century of existence. reply LordDragonfang 13 hours agorootparentprevIt&#x27;s worse than that, because it&#x27;s not actually a void that&#x27;s being filled - meth production simply switched to a different pathway that&#x27;s even more effective and potent. There&#x27;s actually more meth than there was before the ban.https:&#x2F;&#x2F;dynomight.net&#x2F;p2p-meth&#x2F; \"The main thing about P2P meth is that there&#x27;s so much of it\" reply jimmaswell 8 hours agorootparentReally, I&#x27;d read somewhere (might have been browsing the subreddit out of curiosity a while ago) that the meth was much worse now that they had to switch to a process that results in more chiral junk. Interesting. reply LordDragonfang 7 hours agorootparentIt was, briefly, and then they figured out how to purify it I guess? reply notadoc 8 hours agorootparentprevThe majority of fentanyl is produced by a handful of labs in China. If the USA were serious about stopping the fentanyl overdose problem, it could be done with political pressure.Ask yourself why that isn&#x27;t happening? reply secretsatan 1 hour agorootparentprevAt least here in Switzerland, it&#x27;s available over the counter, you get quizzed by the pharmacist to check if your symptoms are worth it or not, but no id or any registration required.Actually, coming from the UK I was surprised I had to go through this, but now I realise I was previously buying the ineffective stuff, I was also surprised by the very, very mild buzz the swiss stuff gave me. reply nonethewiser 15 hours agorootparentprevYou can buy 6 boxes of sudafed per month (24 doses each). Isn’t that more than the recommended (and probably safe) daily limit? I dont doubt your intentions but I don’t think you actually need 6 boxes per month + a stash. reply cheald 14 hours agorootparentThe point is more to not be in a position to have to go out and buy some when I&#x27;m feeling cruddy. Since it&#x27;s controlled, I can&#x27;t just have some shipped to my door when I&#x27;m laying in bed sick. I just got in the habit of picking it up periodically to ensure that it&#x27;s on hand when I need it. reply lazide 14 hours agorootparentYup, nothing says fun times (and mass spreader) like sitting in line and doing paperwork while sick. reply nonethewiser 9 hours agorootparentprevSo why buy 1 every 6 months? Just go buy 6. This sounded like you were trying to avoid the limits:> Indeed. I&#x27;ve known that phenylepherine is worthless for a long while, and just make it a point to buy a pack of the real stuff every 6 months or so, so that I always have it on hand when I need it. In my state, it&#x27;s controlled - you have to get it from a pharmacist (though you can do so without a prescription) - they take your driver&#x27;s license and don&#x27;t allow you to purchase it too often. reply Spooky23 13 hours agorootparentprevWhen your kid gets sick late and the pharmacist leaves at 6, why not have a packs of pills wherever you are.Or god forbid, you run out at work and forgot the CVS takes a siesta nationwide at 1:30. reply nonethewiser 8 hours agorootparentAbsolutely. But it sounded like he was afraid he wouldn’t be able to buy enough at once, hence the accumulation. reply PixyMisa 14 hours agorootparentprevThey mention one box every six months, which is entirely reasonable. reply criddell 13 hours agorootparentFor an individual, perhaps. But if your house has a lot of people in it who get sick at the same time because they live in close quarters, then maybe not. reply 13of40 12 hours agorootparentprevNot sure how widespread it is or if there&#x27;s a federal limit, but my state (Washington) even makes it illegal to possess more than 15 grams of it. reply dmazzoni 11 hours agorootparentIf my math is correct, 15 grams would mean more than 20 normal boxes of Sudafed, if each box is 24x 30mg.I usually buy boxes of 48x 30mg, but even then I&#x27;d have to stash 10 boxes for it to be illegal. reply nonethewiser 9 hours agorootparentprevWhich is 2 boxes * n years + 6 boxes available at any given time.Im not saying it’s unreasonable, just that the monthly limit is probably higher than he realized and he doesnt need to stockpile them. reply JJMcJ 14 hours agoparentprevYou can still get pseudoephedrine, for example Claritin D, the D for decongestant. It&#x27;s not by prescription but you have to sign for it at the pharmacy desk, present ID, and I believe it&#x27;s reported to FDA&#x2F;DEA and there are limits to how much you can buy per month. reply tyingq 13 hours agorootparentThat&#x27;s mostly right, but it&#x27;s not reported to anyone. It came from the \"The Combat Methamphetamine Epidemic Act of 2005\".If you want to sell pseudoephedrine, your org has to \"self certify\", keep a logbook purchasers have to sign, require ID, enforce the purchase limits, train employees, etc. But, the logbook doesn&#x27;t go anywhere unless some investigation or audit prompts that. So, if you&#x27;re a really determined cooker, you can still go to a bunch of different stores...though you&#x27;re leaving a paper trail. And some big brands might cross-check and have an org-wide electronic log book.https:&#x2F;&#x2F;www.deadiversion.usdoj.gov&#x2F;meth&#x2F;index.html reply dragonwriter 12 hours agorootparent> That&#x27;s mostly right, but it&#x27;s not reported to anyone.IIRC, its reported to a centralized state database in (near) realtime in some states, but in any case it is definitely the case that chain pharmacies generally have their own electronic tracking and flagging to avoid getting nailed the way CVS did [0], which may be shared systems like MethCheck [1], which can also be used by independent pharmacies.So, you’ve got to choose your targets carefully for your “shop at lots of different pharmacies” plan.[0] https:&#x2F;&#x2F;www.justice.gov&#x2F;archive&#x2F;usao&#x2F;cac&#x2F;Pressroom&#x2F;pr2010&#x2F;14...[1] https:&#x2F;&#x2F;totalverify.equifax.com&#x2F;solutions&#x2F;methcheck reply salawat 4 hours agorootparentCan confirm. Snag a copy of the NCPDP Telecom spec, (pricey doc, but is basically the Bible for PBM&#x27;s) for the gorey details. reply scarface_74 10 hours agorootparentprevIt very much is reported in a national database.https:&#x2F;&#x2F;help.eaglesoa.com&#x2F;29&#x2F;en-n-eagle&#x2F;Pharmacy&#x2F;Pseudoephed...Anecdotally, I usually by psuedophredrine at CVS. I bought some in Orlando and tried to buy more at a completely unrelated store in Puerto Rico and was denied. reply tshaddox 13 hours agorootparentprev> So, if you&#x27;re a really determined cooker, you can still go to a bunch of different stores...though you&#x27;re leaving a paper trail.Isn&#x27;t meth a lot easier to come by these days? I have to imagine it&#x27;s more convenient to cook street meth back into Sudafed to fix your stuffy nose than it is to cook Sudafed into meth to get your fix. reply NickNameNick 13 hours agorootparentsee \"A Simple and Convenient Synthesis of Pseudoephedrine From N-Methylamphetamine\"https:&#x2F;&#x2F;improbable.com&#x2F;airchives&#x2F;paperair&#x2F;volume19&#x2F;v19i3&#x2F;Pse... reply dragonwriter 12 hours agorootparentprev> I have to imagine it&#x27;s more convenient to cook street meth back into Sudafed to fix your stuffy nose than it is to cook Sudafed into meth to get your fix.Maybe, but more people are willing to do felonies to get&#x2F;sell meth than to do the same thing for pseudoephedrine, and its just as illegal to by meth to cook pseudoephedrine as to cook meth from pseudoephedrine. reply pengaru 12 hours agorootparentprevA \"cook house\" burned down from me a couple months ago, so there&#x27;s clearly still domestic cooking going on here (Cali hi-desert around JTNP). reply tyingq 11 hours agorootparentprev>but it&#x27;s not reported to anyoneI&#x27;m apparently wrong about that. Seems NPLEX&#x2F;MethCheck was made a requirement US state by US state over a number of years. So there&#x27;s no national monitoring, but many (most?) US states require it and give law enforcement access. And like most things set up that way, enforcement and monitoring varies by state&#x2F;county&#x2F;etc. reply alex_young 13 hours agorootparentprevDoesn&#x27;t it take a ton of the stuff though? I thought people stealing pallets of it was the only viable way to source enough of it. Seems like just having some inventory control is a sufficient deterrent. reply dylan604 13 hours agorootparentDepends on how much you&#x27;re wanting to make. Sending smurfs to buy&#x2F;steal from several locations was a thing for a while. But these cooks were never going to be cartel level secret lab under the commercial laundry location. These were the cook a batch in the trailer out back type of cooks. Jesse before teaming up with Mr White reply tyingq 12 hours agorootparentprevGoogling around a bit, a typical 20-count box of pseudoephedrine (2.4 grams of \"active ingredient\") yields roughly 2 grams of \"high quality\" meth with the shake-and-bake&#x2F;one-pot method. Not the only ingredient, of course, but close to 1:1 of in&#x2F;out. reply terribleperson 13 hours agorootparentprevI was under the impression that there are state-level databases in some states that actually track sales. I am likely incorrect, since a quick google doesn&#x27;t find anything. reply scarface_74 10 hours agorootparenthttps:&#x2F;&#x2F;help.eaglesoa.com&#x2F;29&#x2F;en-n-eagle&#x2F;Pharmacy&#x2F;Pseudoephed... reply maxerickson 10 hours agorootparentprevThere&#x27;s probably straight pseudoephedrine available anywhere you can get something with it added.Based on a few comments here, having it behind the counter is apparently pretty much just a waste of time, with meth production having moved to some other synthesis. reply paint 13 hours agoparentprevI mean, I get the reason behind it and I honestly don&#x27;t know if it&#x27;s a good or a bad thing, but the idea of pseudoephedrene sales being monitored like that is so foreign to me. I bought cold medication in pharmacies in 3 different countries always asking specifically for pseudoephedrine and it never occured to me that was a meth cook thing. reply Damogran6 16 hours agoparentprevI haven&#x27;t tried lately (moved on to aother anti-histimines) but with a Driver&#x27;s license, I could pretty readily get ahold of it.Then had to disengage based on what it was doing to my blood pressure. reply jamiek88 15 hours agorootparentWere you using a ‘D’ version? Like Claritin-D? I got that by accident once and my heart pounded like I was on speed. I’m highly sensitive to that stuff, same at the dentist the local anesthesia makes my heart race.Third generation antihistamines are amazing.Zyrtec changed my life.I used to majorly struggle with ‘hayfever’ as we called it, to the point it ruined my summer.I was miserable as a child, only winter gave relief as I was allergic to both tree and grass pollen.Then zirtec came out. Originally as a prescription and that coupled with Flonase completely got rid of my symptoms without fatigue!The old antihistamines I’d have to take so many to get help that I could barely keep my eyes open.Then I moved to Texas from UK and the flora was completely different thus I didn’t react at all!It was heavenly.Then I moved to Oregon and many, many, trees and plants are the same as in the UK and my allergies came back.Much weaker this time as a lot of people age out of these types of allergies, as my mother did.OTC zirtec sorts it out.Anecdotal really but I thought I’d share. reply hn_throwaway_99 13 hours agorootparent> my heart pounded like I was on speed.Pseudoephedrine literally is an amphetamine, and famously the precursor to meth(amphetamine). I don&#x27;t have a similar reaction but I&#x27;m not surprised some people are more sensitive. reply p1mrx 13 hours agorootparentprevNote that Zyrtec&#x27;s patent expired in 2007, so it&#x27;s now available as generic Cetirizine. reply kerblang 13 hours agorootparentprevSurprised this is the only comment mentioning the amazing Zyrtec (with Cetirizine) (probably misspelled, even with the bottle in front of me), in poem form.But yeah it does work, smashingly. Also I found changing my AC air filter helped. reply Damogran6 15 hours agorootparentprevYeah, I cycle between kirkland nasal spray and eyedrops.And now going back on-site, the headcolds you pick up with proximity to people, Sudafed was AWESOME, it&#x27;s just a pita to get and I shouldn&#x27;t use it a lot. reply jeffbee 16 hours agoparentprev> I&#x27;ve maintained my own reserve of pseudoephedrine for years.Synthesizing it from widely-available and affordable street drugs, I assume.https:&#x2F;&#x2F;improbable.com&#x2F;airchives&#x2F;paperair&#x2F;volume19&#x2F;v19i3&#x2F;Pse... reply buildsjets 16 hours agorootparentAt least if I followed that path, I would not be required to have my purchase recorded in an insecure government database. reply tptacek 15 hours agorootparentWhy are you worried about the government recording your usage of Sudafed? What is anybody going to do with that information? Serious question! I understand the principle involved; I just want to know if there&#x27;s some practical concern. reply akira2501 15 hours agorootparentWhat they&#x27;re going to do with it is prevent me from making a purchase if I&#x27;ve crossed some arbitrary limit as if exceeding that limit automatically makes me a drug trafficker.From there, it&#x27;s not too far of a stretch to imagine the government assigning someone to review the list and to expend additional investigative resources into merely \"suspicious\" cases. Plenty of good tragedies start this way.If the government has an interest in preventing drug manufacture then this is the absolute worst way to go about prosecuting that agenda. It harms the wrong people and it offers no impediment to actual producers.The information cannot possibly be useful, it may become a liability, and I didn&#x27;t ask to be protected from myself. reply tptacek 15 hours agorootparentThey&#x27;re just going to say \"no\", right? They&#x27;re not going to \"automatically make you a drug trafficker\". They&#x27;re just trying to enforce a rate limit. reply akira2501 14 hours agorootparentSo, if the concern isn&#x27;t that I&#x27;m trafficking, then why am I being told no? Isn&#x27;t this the logic the government used to assert this right for itself in the first place?Otherwise, what right is it of the government to enact this rate limit? What interest are they protecting? How is the rate limit decided? What can I do if I disagree with the rate limit? Why is a private business being burdened with the governments agenda here?I get where you&#x27;re coming from, it _is_ a &#x2F;small&#x2F; thing, but the implications immediately become onerous if you think about their meaning inside of a \"free\" country. reply tptacek 14 hours agorootparentYou&#x27;re not being told \"no\". You can just go buy some Sudafed right now if you want to. It&#x27;s not like there&#x27;s an application process. There is a rate limit; that&#x27;s it. reply akira2501 14 hours agorootparentIn order for a rate limit to be effective, at some point it has to apply, and at that point, I&#x27;m told \"no.\" More concretely, in order to implement a rate limit, you have to sign an electronic log book, and the government can view that log book whenever it likes, without any sort of review or auditing, of course.This isn&#x27;t the primary concern anyways, the point was, in order to implement this seemingly simple action, many liberties have to be sacrificed and the boundary between innocent and criminal and government and private business become significantly marred.Perhaps those sacrifices are immaterial to most lives and could be ignored without consequence, but the originalist in me says this is folly, and as I&#x27;ve shown it&#x27;s easy to think of the dangers that some innocent people might endure as a result. Worse, comparing these dangers to the outcomes of the system itself, the whole endeavor seems to have negative value.Finally, as we&#x27;ve seen in history many times, now that this electronic logbook exists, it&#x27;s use will naturally continue to expand until the government has secured for itself the right to view nearly every single \"questionable\" purchase you make at a pharmacy.It&#x27;s an entirely unaccountable act from the government. I&#x27;m not sure why anyone would expend effort minimizing it. reply tptacek 12 hours agorootparentThe only liberty being sacrificed here is showing an ID at a pharmacy counter to get Sudafed. I&#x27;ll stipulate that&#x27;s a liberty being sacrificed, but it isn&#x27;t \"many\".This particular \"logbook\" has existed for almost 2 decades now, and its scope hasn&#x27;t creeped from there, so your second argument is pretty easy to shoot down.I think the evidence pretty strongly suggests that this policy, which was put in place to create a rate limit on the purchase of a chemical that isn&#x27;t so much a precursor to methamphetamine as it is a slightly tweaked version of methamphetamine, is in fact simply used to create a rate limit. reply AnthonyMouse 11 hours agorootparent> The only liberty being sacrificed here is showing an ID at a pharmacy counter to get Sudafed.It would be interesting to know if the people concerned about the need to show an ID to vote share that concern about the need to show an ID for access to medicine.> This particular \"logbook\" has existed for almost 2 decades now, and its scope hasn&#x27;t creeped from there, so your second argument is pretty easy to shoot down.The policy has proved ineffective at curtailing the availability of methamphetamine while inconveniencing honest people. That&#x27;s not something the public is clamoring for more of.But it never really was. It came about to begin with as part of the Patriot Act. The lack of expansion is unsurprising given the lack of any 9&#x2F;11-style crisis to use as an excuse in the intervening two decades. The test is when the next one comes.One of the failings of the existing style of government is that the \"passage by both houses of Congress and signed by the President\" system used to pass legislation is the same system required to repeal it, which rightfully makes it harder to pass bad laws, but wrongfully makes it harder to repeal them. And then they get passed during a crisis and inconveniently stick. reply tptacek 10 hours agorootparentYou already need to show ID for most medicine! This is such a weird discussion. reply natechols 9 hours agorootparentIt&#x27;s weird to me that we&#x27;ve completely normalized having to present ID to buy any medicine at all, especially one that has an excellent safety and efficacy record that I was buying without an ID for a decade. Why should I have to ask the government&#x27;s permission to cure a runny nose? reply tptacek 9 hours agorootparentThe problem with Sudafed isn&#x27;t that it&#x27;s ineffective or unsafe for its users; it&#x27;s that it has the same or higher abuse potential as codeine cough drops did. We took codeine completely off the market, but you can just go buy Sudafed whenever you want. reply AnthonyMouse 5 hours agorootparentSignificant numbers of people are not going to the pharmacy and buying Sudafed because they have a cold and consequently getting addicted to Sudafed, which is what had been happening with codeine.It&#x27;s clearly because you can make methamphetamine out of it. But you can also make methamphetamine out of other things which limiting access to cold medicine demonstrably hasn&#x27;t prevented. reply AnthonyMouse 5 hours agorootparentprev> You already need to show ID for most medicine!Do you? You have to show an insurance card to file a claim, but I imagine there is a lot of overlap between the people without ID and the people without insurance. And there appear to be free clinics that don&#x27;t require ID, implying that it isn&#x27;t required by law. reply lazide 9 hours agorootparentprevName an over the counter medicine you need to show ID for. reply tptacek 9 hours agorootparentSudafed isn&#x27;t an OTC medicine; it&#x27;s a BTC medicine. Most medicines aren&#x27;t OTC. But there are, obviously, OTC products that require ID; for instance, every alcoholic beverage. reply jccc 6 hours agorootparentPurchases of alcohol at the grocery store are not individually logged and tracked by personal ID. I’m certain that is not what happens when I show my birth date to a cashier. reply kasey_junk 2 hours agorootparentMost large retailers (e.g. Target, Walmart) now scan the barcode on your ID at the POS when buying spirits. reply AnthonyMouse 2 hours agorootparentBecause it&#x27;s required by law, or because they want to track you for their own purposes and if you don&#x27;t like it you can patronize someone who doesn&#x27;t do that? reply AnthonyMouse 5 hours agorootparentprevThat&#x27;s just begging the question. The only reason it&#x27;s BTC is that you have to show your ID to get it. replyWowfunhappy 8 hours agorootparentprev> In order for a rate limit to be effective, at some point it has to apply, and at that point, I&#x27;m told \"no.\"The alternative is you don&#x27;t buy any Sudafed in the first place. So you&#x27;ve basically told yourself \"no\" anyway.I realize you dislike the government logging your purchases, but you haven&#x27;t answered GP&#x27;s question about why you dislike it. reply phil21 7 hours agorootparentprev> You&#x27;re not being told \"no\". You can just go buy some Sudafed right now if you want to.You effectively are in many cases, just as predicted by most when these laws were put into place.First, it became far less of a relatively benign \"after the fact\" logbook as it was when first implemented. It is now networked and real-time. Many chain stores implement their own interpretation of what they feel a rate limit should be. This tends to err drastically on the side of conservative, and might look back much further than you assume.Second, it&#x27;s added risk - many independents simply don&#x27;t want to bother with such a risk, and thus don&#x27;t bother stocking it. It also adds handling costs.And third and most importantly - it completely and irrevocably ruined the supply chain. It is now legitimately difficult to find it in stock reliably - and if you run out of it at 2am, it&#x27;s no longer just a minor inconvenience to run to literally any convenience store.Most stores simply no longer carry it, and only a handful of 24 hour pharmacy options exist compared to 20 years ago. Those pharmacies also have consistent supply issues, so you better not have strong brand or dosing preference.Prices went up, and availability plummeted. It&#x27;s out of stock roughly a third of the time I&#x27;ve needed to go run out to get some, and is no longer just an afterthought item to stock up on at Costco. Typically you&#x27;re settling on buying the smallest boxes they make since all other sizes are out of stock, and paying even more per dose.And it absolutely is a slippery slope. Loperamide has now met the same fate in many states, and has become multiples of the expense it used to be and a very much pain in the butt to get. No more qty 120 bottles from Costco at 2 cents a pill - entire market killed because people got used to this new norm.It&#x27;s just one more chip away at the American quality of life. Not a huge one, but one that adds expense and friction for what amounts to a questionable amount of gain.Edit: That is all to say - it&#x27;s far more than \"just a rate limit\". Ruining the OTC market for cold medication might be worth it, but that&#x27;s more or less what happened here. It wasn&#x27;t like the top 5% of buyers got scraped off the top and sent to jail and everyone else carried on per usual. The cold aisle of your local supermarket took a giant step backwards from 20 years ago in price, availability, convenience, and effectiveness. That was the cost of these creeping regulations. Some may even say the chilling effect was the entire point. reply lazide 13 hours agorootparentprevOh, sweet summer child.Every time the DEA gets bored&#x2F;needs to pump up their stats, they look through the database to figure out who to start investigating. reply tptacek 12 hours agorootparentRespectfully, I don&#x27;t believe you. This is another one of those cases where the Bayesian Base Rate Fallacy suggests that the \"scary\" version of this supposed DEA policy, where they pursue marginal cases of people with extra-stuffy noses, can&#x27;t possibly work. reply lazide 12 hours agorootparenthttps:&#x2F;&#x2F;www.justice.gov&#x2F;archive&#x2F;ndic&#x2F;pubs36&#x2F;36407&#x2F;36407p.pdfLegit extra-stuffy-noses? Sure. But that isn&#x27;t a Brazil type situation. A Brazil type situation is where you get mistakenly caught in the gears of a no-win bureaucracy just trying to survive.Not a super high risk IMO, but definitely not ZERO risk. The DEA hasn&#x27;t always been known to give the benefit of the doubt, or ask questions before shooting. reply tptacek 12 hours agorootparentNone of this supports the argument you made? It&#x27;s just a DOJ report talking about the smurfing-to-lab pipeline, which was obviously very real, until we made it annoying enough to get Sudafed that smurfing became irrelevant and production shifted offshore.No, this is an instance where I won&#x27;t hedge, because I think my cards in this hand are strong enough: the risk of what you said earlier, of DEA getting bored and looking for marginal cases of Sudafed acquisitions in the logs to trigger enforcement work, is zero. Absent some important other high-signal source to correlate with, that tactic is mathematically guaranteed not to work. reply lazide 12 hours agorootparentDude, that report was written AFTER the laws which required it to be moved behind the counter, people to take IDs, etc. 2 years after.Smurfing was the response to that.They used those databases to identify it and target folks. That&#x27;s the whole point. reply tptacek 12 hours agorootparentI re-read it again. It says that exactly nowhere. In fact, the one specific case it did talk about, the causality was reversed: the investigation started with a couple that was recruiting other people to buy. reply lazide 11 hours agorootparentThe law causing all these effects was passed in 2005.https:&#x2F;&#x2F;www.deadiversion.usdoj.gov&#x2F;meth&#x2F;index.htmlhttps:&#x2F;&#x2F;obamawhitehouse.archives.gov&#x2F;sites&#x2F;default&#x2F;files&#x2F;pag...Page 5.I’ve read press releases where the DEA flat out says they looked at the logs to identify suspicious purchasers, but i can’t find them in the noise. replykstrauser 15 hours agorootparentprevFor me, it’s a matter of it’s none of their business. No one’s accused me of making meth, but here’s all my PII in a database “just in case”.It’s literally harder for me to buy cold medicine than a gun. I hate everything about that situation. reply bsagdiyev 8 hours agorootparentI don&#x27;t know where you live but I cannot buy a guy by just showing up. They take ID, they require a license and still run a check. The only exception is when you get your CCW which removes the license per handgun (not long gun&#x2F;shotgun) but requires a background check. reply tptacek 15 hours agorootparentprevI mean, the reason they keep the database is especially clear here, right? They&#x27;re just trying to enforce a rate limit. It&#x27;s the most obvious public policy response you can have to \"a drug that in small quantities is a useful decongestant, but in moderate quantities or above is an ultra powerful stimulant whose abuse is ravaging parts of the country\". reply kstrauser 15 hours agorootparentI get it. I just find it frustrating, and I&#x27;d like to know that my PII was removed after the rate limit period. reply tptacek 14 hours agorootparentI agree with this! reply phone8675309 14 hours agorootparentprevThe primary question to me is whether the risk of data breech outweighs the harm from not keeping the data, and while 20 years ago this tradeoff may have made sense, I&#x27;m not so sure it makes sense now. reply akerl_ 14 hours agorootparentIf the data is breached, an attacker learns... that sometimes you have a cough? reply kstrauser 14 hours agorootparent...and your SSN, and your driver&#x27;s license info, and all the other personally identifying information they collect and store in the DB. reply phone8675309 13 hours agorootparentGenerally not your SSN - I&#x27;ve never had to give it for Sudafed - but at least full legal name, birthday, gender, home address, driver&#x27;s license number, telephone number, location(s) you&#x27;ve been to buy medication, and possibly email address.Much of which would be covered as PII and PPI, and, in combination with info from other other data breaches can tell someone a lot about you. reply phone8675309 13 hours agorootparentprevIn addition to at least your full legal name, birthday, gender, home address, driver&#x27;s license number, telephone number, location(s) you&#x27;ve been to buy medication, and possibly email address. reply tptacek 12 hours agorootparentYou do not in fact provide an email address to buy Sudafed. The rest of that information is for most people already a public record.I&#x27;m not saying that makes it OK to assemble into a database, and I think the point about amassing PII is well taken, but I think people are probably overestimating the value&#x2F;hazardousness of this particular data set. reply lazide 9 hours agorootparentIt’s an attractive nuisance which seems to provide zero practical value overall, let alone for it’s stated reason to exist. reply tptacek 9 hours agorootparentIt decreases the number of backyard meth labs, which are a danger to their surroundings. Despite the availability of overseas industrial meth, meth labs would make a comeback if you could simply buy pseudo-meth at the drug store to shake-and-bake in your garage. replyfluidcruft 15 hours agorootparentprevEvery database that contains your PII increases your exposure to a breach and likelihood of having to deal with it. It&#x27;s not the sudafed use that makes it a target: it&#x27;s the driver&#x27;s license. reply tptacek 15 hours agorootparentRight, but this particular database presumably doesn&#x27;t have anything especially sensitive in it, apart from your Sudafed consumption.I don&#x27;t mean to dismiss that point, it&#x27;s a valid point, I&#x27;m just considering it closely. reply lazide 13 hours agorootparentSudafed usage, probably a copy of the drivers license, your address, any phone number you give them, and how often you buy a drug the DEA considers a precursor to something really terrible.Usually no one even looks at it (so why have it even?!?), but when they do it’s a potential intro to a full on Brazil situation.Luckily the whole thing is generally considered a waste of time, so usually nothing comes of it. But why make yourself a target if you can avoid it? reply tptacek 12 hours agorootparentWell, it seems pretty obvious why they have it: so they can match purchases of Sudafed at different retail outlets at different points in time. And it&#x27;s obvious why they want to do that: because there really was a huge epidemic of people making straw purchases of Sudafed to feed large numbers of low-volume amateur meth labs.In the years after this policy was put into place, there were news stories about how effective it had been in eliminating backyard meth production. You don&#x27;t see those stories anymore. I suggest, with weak confidence, that the reason you don&#x27;t isn&#x27;t that backyard meth has roared back, but rather that it&#x27;s so decisively not a part of the equation anymore that stats about it don&#x27;t really matter. Meth comes from overseas now, not from backyards.That doesn&#x27;t solve the meth consumption problem, but it does eliminate one significant meth externality. reply lazide 12 hours agorootparentI love how we&#x27;re still stuck not being able to buy Sudafed normally though. :( reply tptacek 12 hours agorootparentYou mean, by walking into the drug store and asking for it? reply lazide 12 hours agorootparentBy literally going to the aisle with the decongestants, taking a pack, and paying for it like every other thing I would be buying. Like every other OTC drug.Without having to give ID, find a pharmacist, etc. I remember those days. They&#x27;re long gone, obviously. reply tptacek 11 hours agorootparent\"Find a pharmacist\"? I can tell you where the pharmacist is! They have their own checkout aisle! reply lazide 11 hours agorootparentAnd bankers hours, and their own waiting line! Perfect! replyCamperBob2 14 hours agorootparentprevIt&#x27;s none of their business. Simple as that. reply fluidcruft 15 hours agorootparentprevAnything a government uses to uniquely identify and track individuals (i.e. driver&#x27;s license) is the target. reply tptacek 15 hours agorootparentUntil very recently, drivers license numbers were effectively public in a lot of states (they&#x27;re deterministically generated from public information). reply fluidcruft 14 hours agorootparentHey, that&#x27;s pretty cool. I suppose I now know Ron DeSantis&#x27;s drivers license number up to the last digit. And the soundex algorithm is cool (using fuzzy lookup in a list that way to get close&#x2F;vague matches is novel to me and might be useful). I had heard of soundex but didn&#x27;t know how it worked until now. Thanks for the TILs!https:&#x2F;&#x2F;stevemorse.org&#x2F;dl&#x2F;dl.htmlhttp:&#x2F;&#x2F;www.highprogrammer.com&#x2F;alan&#x2F;numbers&#x2F;index.html replybeanjuiceII 6 hours agorootparentprevThese people are just very paranoid. This is my field of work for the last 15 years.. people think way too highly of the govt capabilities and negatively on their goals. If we actually did 1&#x2F;10th of what people accused us of our lives would probably be much easier. reply scarface_74 10 hours agorootparentprevhttps:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;buy-too-much-sudafed-and-you-m...But if cops will choke someone to death about selling a cigarette, do you really want them to know about you buying Sudafed?It’s funny that there is no outrage about a national psuedophredrine database. But people go nuts about a national gun database reply tptacek 10 hours agorootparentIt&#x27;s almost like people generally understand why pharmacies are regulated to rate limit the purchase of Sudafed, and that the political consensus generally isn&#x27;t an ideological opposition to any kind of regulation, the way you mind find it on a message board.Sudafed PE is a much more salient and annoying topic than the behind-the-counter real Sudafed. I think it really does piss people off to get tricked into buying fake decongestant. But that&#x27;s a separable issue from having to ask (and to show ID) to buy the real stuff. reply scarface_74 10 hours agorootparentI know of families who struggle to keep enough psuedophredrine in the house when their minor kids get sick and they need some for themselves. reply civilitty 15 hours agorootparentprevIt&#x27;s legal defense in depth, similar to not talking to police without an attorney present. All it takes is an overzealous prosecutor or social worker to ruin someone&#x27;s life by misrepresenting some random circumstantial facts. This happens all the time in drug cases, for example, where the possession of innocuous items like small baggies allows prosecutors to take charges from regular possession to intent to distribute or trafficking.Even if someone isn&#x27;t doing anything illegal, since >90% of cases end in plea deals it&#x27;s good legal hygiene. reply mikeweiss 15 hours agorootparentprevSomething tells me there is more than just a government owned database involved. Likely at least one third party vendor system... Not to mention every pharmacy chain probably stores or at least transmits it through one their own systems. reply mancerayder 8 hours agorootparentprevBut what if it were a secure government database? reply hn_throwaway_99 13 hours agorootparentprevLol, that article is really brilliantly hilarious. SNL should do a \"Bizarro-World Breaking Bad\" where Walter and Jesse go around buying up crystal meth so they can cook Sudafed. reply nimish 16 hours agorootparentprevEasily the best example of why the stupid federal law limiting access to a useful decongestant backfired. reply tptacek 15 hours agorootparentIn what way did it backfire? It&#x27;s a useful decongestant. But it also basically is methamphetamine, with just a couple easily-reversible Rubik&#x27;s turns to make it a decongestant instead of an ultra-powerful stimulant. reply fragmede 13 hours agorootparentWater, consisting of hydrogen \"basically is\" and explosive, as is table salt, consisting of Sodium, which explodes when coming into contact with water. How can we let these two dangerous substances go unregulated! Think of the children! who will get to have interesting high school chemistry labs, that is.That Sudafed \"basically is\" meth misunderstands chemistry, and the resourcefulness of the real life Walter Whites of the world. Hells Angels and the Cartels operate under the nose of the DEA so there&#x27;s no way meth or drugs are going away. It&#x27;s time for the government to admit they lost the current battle of the war on drugs to drugs. Current tactics aren&#x27;t working and it&#x27;s time to take the war to the demand side of the equation. Cut down the demand for drugs by getting addicts off opiates and stimulants with an army of therapists and councilors and rehab programs. With a large helping of harm reduction and government assistance. Wishful thinking, I know, but making all forms of basic chemistry illegal is like trying to make water not wet. reply pvg 10 hours agorootparenttable salt, consisting of SodiumThe difference between a Sodium ion and a Sodium atom is one of the profoundest in chemistry and a fundamental topic of its high school version. reply tptacek 12 hours agorootparentprevNo, this analogy isn&#x27;t plausible. reply amanaplanacanal 15 hours agorootparentprevDidn’t prevent people from getting meth, but did allow drug companies to make millions selling a useless compound. I doubt that was the intent of the legislation. reply tptacek 14 hours agorootparentThe same company sells both compounds. I don&#x27;t think this was a huge moneymaker. I think the PE thing was just there to retain the sales that moving Sudafed behind the counter sacrificed. That doesn&#x27;t make it OK, but it does sort of argue against the idea that the regulation was just a moneymaking scheme. reply amanaplanacanal 14 hours agorootparentAgree with that. Just more failed war on drugs stuff. reply tptacek 12 hours agorootparentOnly if you judge the outcome strictly by how many people are consuming meth, but there are other important endpoints to this policy, like \"reducing the number of backyard meth labs\", which are problems in their own right. replyhn_throwaway_99 13 hours agorootparentprevYou realize your \"best example\" is a joke, right? reply nimish 13 hours agorootparentYes, the punchline of the joke is that synthesizing pseudoephedrine from meth is more convenient than buying Sudafed after the law on restricting sales ostensibly to reduce meth production.i.e., the law didn&#x27;t do much to halt meth production but did put a bunch of barriers around a useful drug _and_ promoted the use of phenylephrine instead, a useless drug in pill form. A huge net negative. reply purpleblue 14 hours agoparentprevThe fact that drug companies&#x27; \"tests\" indicated that it worked shows how corrupt the system is. There is in fact no way it could have worked, yet the data they provided decades ago showed that it did. This is why we should never, ever trust drug companies, it&#x27;s just kind of funny that somehow in the last few years some people decided that Pfizer was a hero instead of a psychopathic (in the purest clinical definition) money machine. reply 1MachineElf 16 hours agoparentprevIt helps me a lot with post-nasal drip. reply jawns 16 hours agoprevI laughed out loud when I read about the lobbyists who are trying to keep phenylephrine from being taken off store shelves.In one article about the FDA panel vote, a lobbyist said that most consumers hate the alternatives -- nasal sprays because of the discomfort and pseudoephedrine because it&#x27;s stored behind the pharmacy counter -- and so depriving them of a hassle-free option is anti-consumer.The fact that the hassle-free option has been shown to be ineffective was, as you might guess, never mentioned. reply ctoth 16 hours agoparentUnfortunately, this is the actual logic which will be used to justify it staying on shelves.> The decongestant is in at least 250 products that were worth nearly $1.8 billion in sales last year, according to an agency presentation.Look those yachts aren&#x27;t gonna buy themselves. reply pixl97 15 hours agorootparent\"Ok fine, you can still sell it over the counter... except a X by Y sign stating &#x27;This product is completely ineffective if taken in pill form&#x27; needs displayed on the shelf where the product is sold\" reply kstrauser 15 hours agorootparentI could actually go along with that so long as other ineffective things like homeopathic water get labeled the same way. reply dzdt 14 hours agorootparentprevThe same stores sell completely ineffective homeopathic \"medicines\" alongside real medicines already. There are no signs. Its already up to the consumer to determine what may or may not actually work at the pharmacy. reply badRNG 13 hours agorootparentA few years back, I came down with the flu and went to CVS for medicine. I purchased what I thought was a non-prescription alternative to Tamiflu. It was a bad case, and I dutifully followed the dosing instructions and took the medication throughout the whole illness. It turns out the product and the supposed function is complete homeopathic nonsense. It was sold in a medicine isle along with known \"functional\" cold medicine, and had some \"Active Ingredient\" listed on the back (similar to any other drug label.)I spent my money on a product sold by a pharmacy that is quite literally a scam. I&#x27;m naturally a skeptical person, but I didn&#x27;t think I needed to independently check whether what&#x27;s on a pharmacy store&#x27;s shelves is medicine or a scam. That&#x27;s not my lane, I trusted that the pharmacy would only sell medicine that works. reply fnordpiglet 12 hours agorootparentIf you bought it at a for profit pharmacy, then it did work precisely as they expected it to. reply lazide 13 hours agorootparentprevJust wait until you start reading drug trial data! replytshaddox 12 hours agoparentprevIf only there was a simple physical maneuver that could transform the location of something from behind a pharmacy counter to the customer-facing shelves! reply nonethewiser 15 hours agoparentprevI always assumed that was the whole point. A placater. This probably isn’t even news to them. reply saulpw 14 hours agorootparentI believe the official word is \"placebo\" :) reply selimthegrim 16 hours agoparentprevMy understanding is the nasal spray version works, but the oral just gets metabolized ineffectively? reply uf00lme 14 hours agorootparentThank you for sharing, if this is true it will win me a really good arguement. I know someone who hates Phenylephrine, so I will buy some nasal spray just to surprise them. reply lokar 16 hours agorootparentprevCorrect reply nonethewiser 15 hours agorootparentOh wow, didn’t realize this reply hinkley 7 hours agoparentprevMah liberties! reply wffurr 16 hours agoprevContext: On Sept 12th 2023, an FDA panel (the NDAC) definitively voted to find phenylephrine ineffective: https:&#x2F;&#x2F;www.medscape.com&#x2F;viewarticle&#x2F;996369 Next will be re-assessing its GRASE status and potentially removing it from stores. reply mullingitover 16 hours agoprevI always thought it was pretty brazen of them to openly label their cold medicine as &#x27;placebo effect&#x27; (pretty sure that&#x27;s what the &#x27;PE&#x27; stands for). reply samtheprogram 16 hours agoparentI always thought it was for Phenyl-Ephrine, but this is so funny&#x2F;perfect I will use this when explaining to people to not buy Sudafed unless it&#x27;s behind the counter. reply meatmanek 13 hours agorootparentI always assumed it was there to subtly confuse people into thinking they were getting Pseudo-Ephedrine. reply omginternets 16 hours agoprevThis strikes me as an example of a larger phenomenon. Take for example the counter-insurgency wars that have become normalized since 9&#x2F;11. In these conflicts, there is enormous pressure to minimize casualties because these directly undermine the political will to conduct military operations. As such, the strategic objectives eventually degenerate to mere \"presence\", and commanders are effectively unable to pursue the kinds of operations that might actually translate into military success.Similarly, we&#x27;ve collectively decided that selling an OTC drug that is effectively inert is somehow preferable to selling a useful-but-abusable substance, at which point one might argue that it&#x27;s even safer to sell nothing at all. reply tptacek 15 hours agoparent(1) I think there&#x27;s pretty wide agreement that selling Sudafed PE is worse than selling nothing at all, and I&#x27;d sort of expect it to be off the shelves sometime soon (it should have been much earlier than this).(2) \"Selling nothing at all\" is actually not the policy. Real Sudafed is simply a behind-the-pharmacy-counter drug. You don&#x27;t need a script to get it (plenty of non-abusable drugs do require scripts, a much higher bar). Real Sudafed is to a first approximation available to everybody. reply omginternets 15 hours agorootparent> Real Sudafed is to a first approximation available to everybody.I didn&#x27;t actually know this, and it&#x27;s good to know. That said, I&#x27;m not certain that most people are aware that it is available without a script. Maybe I&#x27;m wrong. reply tptacek 15 hours agorootparentIf you walk up to the pharmacy counter at any Walgreens or CVS, literally the first thing you&#x27;re going to see is a shelf of decongestants. reply clipsy 13 hours agorootparentI can’t speak for your Walgreens or CVS, but at mine you’ll see a shelf full of phenylephrine-based decongestants only, because the pseudoephedrine-based ones are behind the counter and out of sight. reply idlewords 13 hours agorootparentLots of pharmacies display cards in the regular decongestant section that you can take up to the counter to get the ID-required OTC pills. It adds an annoying step, but they are hardly out of sight. reply schrodinger 10 hours agorootparentprevNot trying to be condescending, but are you sure you&#x27;re referring to \"behind the pharmacy counter (i.e. where the pharmacist stands)\" rather than on the regular shelves? That&#x27;s where it&#x27;s prominent—although as someone else noted there are often \"see the pharmacist for the real stuff\" cards on regular shelves too. reply skipkey 12 hours agorootparentprevNot sure if it’s still the case, but a few years ago I was traveling and staying overnight near St Louis, and was shocked to find that it actually required a prescription there, but only in the county that St Louis was in. reply orangepurple 15 hours agoparentprevThe Dutch take this to the extreme, where the actually effective anti-cough medication dextromethorphan is completely unavailable, but complete bullshit herbal mixes which claim to help are a dime a dozen on every store shelf. I can&#x27;t even imagine the amount of needless suffering people with chronic cough go through in the Netherlands. reply iopq 4 hours agorootparentI bought some Chinese medicine in China for the reason that pseudoephedrine is not available and it was actually too effective, dried my nose up so much I kept having nosebleeds. But of course, I prefer the actual effect of pseudoephedrine that doesn&#x27;t have this side effect. reply jnwatson 3 hours agorootparentprevI have yet to figure out how Europeans treat minor ailments. Those green-crossed \"pharmacies\" (only open weekdays during business hours) have 4 or 5 OTC drugs and the rest of the store is just pseudoscience. reply Broken_Hippo 2 hours agorootparentMost of the time: You rest. Most folks have sick days that they can use to stay home and recover. If you need more than a couple of days, go to the doctor.And honestly, that takes care of a large portion of the medication. I bought it in the US to have the ability to survive a workday: IF I&#x27;m staying home and resting and sleeping, I tend to need less of the stuff for a minor ailment.They do sell a few things in the pharmacies here (Norway) that help, mostly with pain and to do things like dry your nose (nose spray, not as many pills) and loosen up phlem. You can get allergy medicine. And so on.And honestly, I worked at a pharmacy for 8 or 10 years in the US and there is only a handful of drugs OTC there, too - but there are more brands and more products that have multiple drugs in the same dose. The drugs just differ at times. US stores sell the pseudoscience as well (It angers me, but it isn&#x27;t just one place doing it, which is the point).Many of the pharmacies here are open longer than business hours, but it also doesn&#x27;t always matter. The doctor offices tend to close at 3pm, before the pharmacies. If you are expected to stay home from work, you can get to the pharmacy. reply sznio 1 hour agorootparentprevParacetamol, Ibuprofen, Aspirin, Xylometazoline and sleep. Or go to a doctor and get told to get the same stuff, with some prescription-only cough syrup.The rest of the pharmacy is for old people looking for miracle cures to being old. reply rscho 14 hours agorootparentprevThe effectiveness of dextrometorphan is... underwhelming in most people. But you are correct that no effective cough suppressant is available OTC in Europe. reply wrs 12 hours agorootparentDextromethorphan is one of the other “scientifically proven ineffective” OTC products mentioned by the authors.Once when I had a quite severe cough I did some (highly motivated) research and my conclusion was that there is no such thing as an effective cough suppressant. At least, none that isn’t also a consciousness suppressant. reply pitaj 6 hours agorootparentAs a consciousness suppressant I wonder if DXM is safer than diphenhydramine. When I&#x27;m sick, being able to essentially sedate myself is honestly pretty valuable. reply cmrdporcupine 6 hours agorootparentDXM is crappy as a consciousness suppressant. At low doses it doesn&#x27;t do much. At higher doses it&#x27;s likely to do the opposite: make you dissociate and hallucinate. It doesn&#x27;t help you sleep. reply rscho 11 hours agorootparentprevLow dose codeine is notoriously efficient. reply wrs 6 hours agorootparentCodeine is what I ended up using in that one (thank goodness) case. It does work to suppress coughs, as well as other functions, like breathing! Watch the dosage. reply dreamcompiler 7 hours agorootparentprevIndeed. Codeine was even available over the counter in the US until the mid 1960s. But of course it was also addictive, so now it&#x27;s prescription-only. reply orangepurple 2 hours agorootparentFunnily enough I found codeine is available OTC in the Netherlands. reply lazide 9 hours agorootparentprevHigh dose codeine too. Permanently cures coughing! replytech_ken 16 hours agoprev\"Phenethylamines I Have Known and Felt Kind of Meh About\" reply jkingsman 14 hours agoparentI laughed out loud.For those unaware, \"Phenethylamines I Have Known and Loved\" (aka PiHKAL) is Sasha and Ann Shulgin&#x27;s book that is half semi-fictional autobiography and half detailed synthesis and in-vivo effects observed in the enormous family of psychoactive phenethylamines. The autobiography part is only available in print, but the chemistry section is freely available[0].There is a sequel, a similar book for tryptamines called TiHKAL[1].They are stunningly bold in their chemistry, administration and discovery of novel drugs, and in making good science available under dubiously legal circumstances.[0] https:&#x2F;&#x2F;erowid.org&#x2F;library&#x2F;books_online&#x2F;pihkal&#x2F;pihkal.shtml [1] https:&#x2F;&#x2F;erowid.org&#x2F;library&#x2F;books_online&#x2F;tihkal&#x2F;tihkal.shtml reply tech_ken 14 hours agorootparentPsychoactive phenethylamines??? I&#x27;ve never heard of that, I&#x27;m just really into nasal decongestants. reply sitzkrieg 14 hours agoparentprevcant wait for TIKFKMA reply dang 16 hours agoprevDiscussed at the time:The Uselessness of Phenylephrine - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30858202 - March 2022 (592 comments) reply kstrauser 16 hours agoprevThe risk:reward ratio is infinite — it has adverse reactions but essentially no benefit. It’s about as much “medicine” as was drinking radium.Good riddance. reply rscho 14 hours agoprevPhenylephrine is ineffective when taken orally. It is super effective topically or iv. I can definitely tell, because it&#x27;s the most commonly used vasopressor used under anaesthesia (iv). Also used topically to avoid nosebleeds during nasal intubation. reply Tao3300 10 hours agoparentI was indeed surprised to see it go by in the mix during one of my wife&#x27;s c-sections. I&#x27;d always thought it was a completely useless thing, but there it was keeping her blood pressure stable under the anesthetics. reply tech_ken 14 hours agoparentprevCurious about how this aligns with the bioavailability point discussed in the article. Presumably if the gut lining is where the bulk of metabolism occurs then IV sidesteps this and increase the bioavail drug, leading to improved efficacy. reply wrs 12 hours agorootparentThe gut metabolism makes it less available, which is why if you put it in your stomach not much shows up in your nasal lining. If you spray it or inject it directly where it needs to go, no problem. reply hn8305823 16 hours agoprevIf you like the author Derek Lowe&#x27;s writing like I do you might like his \"Things I won&#x27;t Work with\". Here&#x27;s one of my favorites:https:&#x2F;&#x2F;www.science.org&#x2F;content&#x2F;blog-post&#x2F;things-i-won-t-wor... reply kstrauser 15 hours agoparent> If the paper weren&#x27;t laid out in complete grammatical sentences and published in JACS, you&#x27;d swear it was the work of a violent lunatic. I ran out of vulgar expletives after the second page.I love his work. reply 7thaccount 1 hour agoprevI find it interesting to see this. I use it as a nasal decongestant (pill form), and it works like magic. Completely dried up in like an hour or so. Granted, my nose leaks 24&#x2F;7 like a leaky faucet, so it doesn&#x27;t last, but that&#x27;s just bad genetics. reply blobbers 16 hours agoprevhttps:&#x2F;&#x2F;www.nbcnews.com&#x2F;health&#x2F;health-news&#x2F;fda-panel-says-co... -- \"Phenylephrine — found in drugs including Sudafed PE, Vicks Nyquil Sinex Nighttime Sinus Relief and Benadryl Allergy Plus Congestion — is the most popular oral decongestant in the United States, generating almost $1.8 billion in sales last year, according to data presented Monday by FDA officials.\"* Wasting peoples time and money while consuming their $.* key part is consuming their $. reply greatpostman 16 hours agoprevProbably will get downvoted for this, but a bunch of my family members are pharmacists. Their quiet opinion is that most pharmaceuticals don’t work. Even the ones that are thought effective. The data is anecdotal, but is over 15 years of watching people come in for prescriptions. The other aspect, is they say it’s mind boggling to see what kind of drugs people are on. reply scarface_74 16 hours agoparentOf course you are going to get downvoted and you should.I know my blood pressure medicine works for instance because every time I talk to my doctor about reducing it or getting off it, I monitor my blood pressure and it spikes.I know psuedophredrine works.And your data is just that, “anecdotal”, it’s no better than the people doing “research” by watching YouTube on the toilet. reply freedomben 11 hours agorootparentGP didn&#x27;t say your blood pressure meds or pseudoephedrine don&#x27;t work, they said \"most drugs don&#x27;t work.\" I would have preferred some specifics, but we didn&#x27;t get them so you shouldn&#x27;t argue as though they were given. reply scarface_74 10 hours agorootparentDo you think that 80% of prescribed drugs don’t work? Even 50%? I’m limiting it to prescribed drugs because those are the ones that a pharmacists would be most concerned about.And a “pharmacist” isn’t exactly the best trained person when it comes to knowing how to do a drug research or to know its efficacy for patients compared to even a GP who is monitoring patients and keeping records of blood work and other stats reply derefr 8 hours agorootparentThey didn’t say prescription drugs. OTC drugs are also drugs. (In fact, they’re the drugs the article is about!) And the majority of drugs are OTC drugs. I would fully believe that the majority of OTC drugs do basically nothing.(Consider: how many problems are there with your body that you’re actually likely to be able to solve with a drug that can be taken safely and correctly at basically any dose, basically any schedule, quitting whenever you like, and without checking for cross-drug interactions? Because those are the requirements for a drug to be OTC. There exist far more OTC drugs than there should, given the number of OTC-able problems, is what I’m saying.) reply onco 15 hours agorootparentprevEhh. Your experience is similarly anecdotal. You ignore that there is a placebo effect, blood pressure is a surrogate marker for cardiac events, and drugs can have adverse effects.It’s best to look at all cause mortality in your example. Many studies have looked at that endpoint if you are curious. reply scarface_74 15 hours agorootparentIt’s “anecdotal” that I can measure with a machine that gives a number and that I track over a period time?It’s not like I said “I feel better” when taking it. So my blood pressure just magically comes down after I’m taking it? reply bee_rider 11 hours agorootparentI mean it is technically anecdotal evidence; it is just a story about you and wasn’t collected in any systemic way, we have no way to abstract it.But there’s also lots of clinically collected evidence behind blood pressure medicine, right? I don’t see why anyone would have reason to doubt you. reply MockObject 14 hours agorootparentprevWhat did you think \"anecdote\" means? Why wouldn&#x27;t your individual report of your meter measurements count? You haven&#x27;t shown us the data, shown that you&#x27;ve controlled for confounding factors, or anything.You posted an anecdote. reply scarface_74 14 hours agorootparenthttps:&#x2F;&#x2F;www.ahajournals.org&#x2F;doi&#x2F;full&#x2F;10.1161&#x2F;01.HYP.00001036... replyflarg 16 hours agoparentprevPharmacist here. Most pharmaceuticals do actually work but finding the right one for a patient can sometimes be harder than you would think. I suspect your family members are quiet about their opinions for a very good reason. reply 303uru 14 hours agoparentprevAs a pharmacist, I can&#x27;t help but laugh. I&#x27;m guessing they&#x27;re quiet because their peers see them as quacks. Most don&#x27;t work huh? Cancer death and morbidity rates are falling like a rock. Antibiotics? Antifungals? Hep C cured? HIV&#x2F;AIDs guaranteed death turned to decades of symptom free life? Hell, we wouldn&#x27;t have an opioid crisis if the damn things didn&#x27;t work. The premise is just laughably silly.It seems some people see that placebo&#x27;s can have effects in a select few paradigms of care and extrapolate that to \"all drugs must be placebos.\" Which is just smooth brain conspiracy talk. reply kstrauser 16 hours agoparentprevI’d like to think you’ve heard them grousing about specific things like this and misinterpreting that as a general mistrust. Every medicine I’ve ever taken had an objective, quantifiable effect on me.Except for phenylephrine, of course, which only served to teach me how to ask the pharmacist for the good stuff. reply jrockway 16 hours agorootparentI think that people are hoping for larger effects. I have been a lifelong allergy sufferer. Many years ago, my doctor gave me an intranasal antihistamine. It came with a piece of paper about the studies done on it. Patients rated their symptoms on a 1-5 scale, and the study observed the ratings with and without. The placebo group rated their symptoms, say, 4.0. The people on the drug rated them 3.5. I would say that&#x27;s exactly the effect I got from the medication, an improvement, but not a complete elimination of symptoms. Certainly some people are going to go to their pharmacist and say \"this does nothing\", and that might lead them to believe it&#x27;s all a scam. It&#x27;s not really a scam, it&#x27;s just that expectations were not set correctly. People want \"this will cure you and you&#x27;ll get a gold metal in the Olympics!\" but all we have is \"this will make life slightly less miserable\".It&#x27;s probably the same for many conditions and medications. Antidepressants don&#x27;t turn you into someone who is always happy. Painkillers don&#x27;t make recovering from surgery a joy. I think that people even had the same feelings about COVID vaccines; they wanted \"one shot and you&#x27;ll never be sick again\", but all we got was being 90% less likely to get COVID or whatever. None of this makes these things a scam. Rather, they are imperfect technologies that we work to improve. Silver bullets are rare, but they do exist. Just not all the time. (Have you died from strep throat or polio recently? People used to all the time!) reply kstrauser 15 hours agorootparentHuh, that’s an interesting point and I bet there’s something to it. I’m taking OTC allergy meds right now but still have some symptoms; it’s not an instant and complete cure. Anyone who expects an instant improvement from most kinds of medicines is going to be sorely disappointed, and doctors need to diligently explain that such-and-such will help a problem, not immediately fix it altogether. reply jrockway 14 hours agorootparentYup, exactly. OTC allergy meds are often an improvement; I certainly notice if I forget to take them. But it&#x27;s not like my nose turns into the air-intake equivalent of one of those helicopters that sucks up water for fighting forest fires. Such a technology may not exist ;) (That said, pseudoephedrine is pretty good for me on the worst days.) reply adriand 16 hours agoparentprevIt’s really difficult to untangle the placebo and nocebo effects. I was recently surprised to learn there are studies showing placebos can be effective even when people are told they are taking a placebo. Ie literally telling people, “take this pill every day, but please understand it’s a placebo and has no active ingredients whatsoever”. And yet, their symptoms improve. The theory is that the subconscious mind acts on some information (white-coated professional giving me a medicine) without being fully affected by the conscious mind’s knowledge that the pill ought to do nothing.It’s even possible that the negative attitudes of your pharmacist family members toward the medications their patients are taking are affecting their efficacy! reply cheald 14 hours agorootparent\"Suggestible You\" by Erik Vance explores this phenomenon, and really turned me around on placebos. I&#x27;m, generally speaking, not particularly susceptible to placebo effects, and always kind of (arrogantly) chalked placebo effects up to people just being gullible and wanting to see something that wasn&#x27;t there. It turns out there are actually genes that might be responsible for the physical impact of the placebo effect in the brain.There&#x27;s an enzyme - catechol-O-methyltransferase - which is coded for by the COMT gene. This enzyme catalyzes the metabolism of dopamine in the brain. COMT has three common variations - AA&#x2F;AG&#x2F;GG - which substantially alters how effective the enzyme is at metabolizing dopamine. AA results in significantly reduced enzyme activity, which can result in dopamine built-up in the brain, which results in increased sensitivity to stress, anxiety, and pain, but comes with the bonus of enhanced cognition, motor skills, and memory.AA genotypes also tend to be \"placebo responders\", while GG genotypes tend to be non-responders. This tends to imply that placebo responsiveness isn&#x27;t purely psychological, but physical - and indeed, it turns out that in AA \"responders\" you can turn off the placebo effect by administrating naloxone (which works by binding to opioid receptors)!This has really interesting implications for pharmacological research, too - if there are people who are genotypically predisposed towards or against placebo effects, then a drug trial that stacks responders in the trial group and non-responders in the placebo group which would produce a drug efficacy signal that could be just the placebo effect. reply jrgoff 11 hours agorootparentIs the COMT gene stuff discussed in \"Suggestible You\"? I recently learned I have the AA variation and from what I understand so far, it seems like it might be an explanation (or at least part of one) for why I am so sensitive to a number of vitamins and medications. I hadn&#x27;t heard about the placebo related thing. But anyway, I am interested in continuing to learn more about it (it has been somewhat difficult for me to find digestible information about it online) so might look into that book if it discusses it much at all. reply cheald 9 hours agorootparentYup, it&#x27;s discussed extensively, predominantly as \"val&#x2F;val\" (GG) and \"met&#x2F;met\" (AA). reply jtriangle 16 hours agorootparentprevYou&#x27;re missing the part where they told participants what the placebo effect was, and that taking the placebo would make them feel better in spite of it having no active ingredients, and it did indeed do so. reply jonnycoder 16 hours agoparentprevAnecdotally the most useful drugs are antibiotics and corticosteroids. Other remedies I found truly impactful are neti pot usage everyday if needed and zone 2 daily exercise. reply m463 16 hours agoparentprevI think american medicine just chooses prescribing medication first.Patients are definitely more likely to take a medication than change their behavior. Patients also ask for medication. And then there&#x27;s the whole pharmaceutical system.My personal experience is that many older people I&#x27;ve known have a startling number of bottles of medication they take every day.I also know some older people that have exercise and eating right as part of their regimen and rarely take any medication. reply 303uru 14 hours agorootparent>I also know some older people that have exercise and eating right as part of their regimen and rarely take any medication.Not to be pedantic, well actually let&#x27;s be. But this is pretty poor medical thinking. Are patient&#x27;s not on meds at advanced age because they exercise? Or are they able to exercise because they&#x27;re otherwise healthy and feel good? This kind of simplifying causes a lot of harm in medicine and a lot of patient blaming. reply m463 12 hours agorootparentMy observation seems to be two broad groups. Maybe the fit ones eventually become the bottle ones? Maybe the no-bottle ones are resisting medicine?One thing is for certain - it makes me uncomfortable how many different medications some people take every day. reply matwood 15 hours agoparentprevI don&#x27;t take many medicines and then only ones that I have found to work for me. Pseudoephedrine absolutely does work, phenylephrine does not. reply bee_rider 11 hours agoparentprevIt seems like a wild breach of professional ethics to sell medical treatments that they don’t believe will help their patients. Did they mistake themselves for cashiers or something? reply ShamelessC 16 hours agoparentprevThis is a really bad take. I hope you or your family members reconsider. Also maybe they shouldn&#x27;t hold negative judgemental views on the customers that make their job possible? reply GuB-42 16 hours agorootparentThis is also a bad take.GP specifically mentioned prescriptions, so the customer&#x2F;patient is not at fault here, if there is a judgment here, it is toward the doctors. And just because something earns you money doesn&#x27;t mean it is above criticism.I still don&#x27;t share GP&#x27;s opinion in that most medicine don&#x27;t work. I think some don&#x27;t work, and some is misused or used preemptively, therefore showing no effect. And I am also dubious about a lot of comfort medicine, but for most people I know who are under proper prescription, it is obvious that it works, it includes psychatric medicine. reply nradov 16 hours agoparentprevWe now know that most published medical research findings are false. That likely includes many of the studies that the FDA relied upon to approve certain drugs.https:&#x2F;&#x2F;doi.org&#x2F;10.1371&#x2F;journal.pmed.0020124My impression is that the situation has been improving in recent years. Between study pre-registration, larger subject groups, and greater statistical rigor I have a lot more confidence that drugs getting approved now actually do what they say on the label. Of course, this is also part of the reason why it now costs >$1B to bring a new drug to market. reply scarface_74 16 hours agorootparentIt’s been known for years that Phenylephrine is useless and was just kept on the market because of lobbying. reply h2odragon 16 hours agoparentprevPlacebo effect is strong.\"Medicine\" never wandered that far from religion, after all. reply 51 more comments... Applications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author states that phenylephrine is a less effective decongestant compared to pseudoephedrine given its extensive metabolism in the body.",
      "Phenylephrine is purportedly being sold as an alternative to pseudoephedrine, due to restrictions imposed on pseudoephedrine for its potential use in illegal methamphetamine production.",
      "The article concludes with a recommendation for the FDA to reconsider the sale of phenylephrine, suggesting it does not warrant its current listing."
    ],
    "commentSummary": [
      "The discussion explores topics like drug regulation, addiction, methamphetamine production, and the effectiveness of different medication including decongestants.",
      "Issues like impact of regulations on drug availability, the role of law enforcement, social interventions, privacy concerns and limitations of pharmaceutical drugs are also debated.",
      "Factors like the placebo effect, lifestyle changes, and individual responses to medication, showcasing the complexities and challenges associated with drug regulation and medication potency, are highlighted."
    ],
    "points": 221,
    "commentCount": 320,
    "retryCount": 0,
    "time": 1694625447
  },
  {
    "id": 37497131,
    "title": "Why Fennel?",
    "originLink": "https://fennel-lang.org/rationale",
    "originBody": "Why Fennel? Fennel is a programming language that runs on the Lua runtime. Why Lua? The Lua programming language is an excellent and very underrated tool. Is it remarkably powerful yet keeps a very small footprint both conceptually as a language and in terms of the size of its implementation. (The reference implementation consists of about nineteen thousand lines of C and compiles to 278kb.) Partly because it is so simple, Lua is also extremely fast. But the most important thing about Lua is that it's specifically designed to be put in other programs to make them reprogrammable by the end user. The conceptual simplicity of Lua stands in stark contrast to other \"easy to learn\" languages like JavaScript or Python--Lua contains very close to the minimum number of ideas needed to get the job done; only Forth and Scheme offer a comparable simplicity. When you combine this meticulous simplicity with the emphasis on making programs reprogrammable, the result is a powerful antidote to prevailing trends in technology of treating programs as black boxes out of the control of the user. And yet... So if Lua is so great, why not just use Lua? In many cases you should! But there are a handful of shortcomings in Lua which over time have shown to be error-prone or unclear. Fennel runs on Lua, and the runtime semantics of Fennel are a subset of Lua's, but you can think of Fennel as an alternate notation you can use to write Lua programs which helps you avoid common pitfalls. This allows Fennel to focus on doing one thing very well and not get dragged down with things like implementing a virtual machine, a standard library, or profilers and debuggers. Any library or tool that already works for Lua will work just as well for Fennel. The most obvious difference between Lua and Fennel is the parens-first syntax; Fennel belongs to the Lisp family of programming languages. You could say that this removes complexity from the grammar; the paren-based syntax is more regular and has fewer edge cases. Simply by virtue of being a lisp, Fennel removes from Lua: statements (everything is an expression), operator precedence (there is no ambiguity about what comes first), and early returns (functions always return in tail positions). Variables One of the most common legitimate criticisms leveled at Lua is that it makes it easy to accidentally use globals, either by forgetting to add a local declaration or by making a typo. Fennel allows you to use globals in the rare case they are necessary but makes it very difficult to use them by accident. Fennel also removes the ability to reassign normal locals. If you declare a variable that will be reassigned, you must introduce it with var instead. This encourages cleaner code and makes it obvious at a glance when reassignment is going to happen. Note that Lua 5.4 introduced a similar idea withvariables, but since Fennel did not have to keep decades of existing code like Lua it was able to make the cleaner choice be the default rather than opt-in. Tables and Loops Lua's notation for tables (its data structure) feels somewhat dated. It uses curly brackets for both sequential (array-like) and key/value (dictionary-like) tables, while Fennel uses the much more familiar notation of using square brackets for sequential tables and curly brackets for key/value tables. In addition Lua overloads the for keyword for both numeric \"count from X to Y\" style loops as well as more generic iterator-based loops. Fennel uses for in the first case and introduces the each form for the latter. Functions Another common criticism of Lua is that it lacks arity checks; that is, if you call a function without enough arguments, it will simply proceed instead of indicating an error. Fennel allows you to write functions that work this way (fn) when it's needed for speed, but it also lets you write functions which check for the arguments they expect using lambda. Other If you've been programming in newer languages, you are likely to be spoiled by pervasive destructuring of data structures when binding variables, as well as by pattern matching to write more declarative conditionals. Both these are absent from Lua and included in Fennel. Finally Fennel includes a macro system so that you can easily extend the language to include new syntactic forms. This feature is intentionally listed last because while lisp programmers have historically made a big deal about how powerful it is, it is relatively rare to encounter situations where such a powerful construct is justified. For a more detailed look at the guiding principles of Fennel from a design perspective see the Values of Fennel. Home source for this site",
    "commentLink": "https://news.ycombinator.com/item?id=37497131",
    "commentBody": "Why Fennel?Hacker NewspastloginWhy Fennel? (fennel-lang.org) 218 points by hk__2 19 hours ago| hidepastfavorite89 comments Graziano_M 16 hours agoI moved my Neovim config to fennel and haven&#x27;t look back.https:&#x2F;&#x2F;github.com&#x2F;Grazfather&#x2F;dotfiles&#x2F;blob&#x2F;master&#x2F;nvim&#x2F;fnl&#x2F;... reply kbd 10 hours agoparentI moved my Hammerspoon config to Fennel and haven&#x27;t looked back either. The code becomes much more concise vs Lua.https:&#x2F;&#x2F;github.com&#x2F;kbd&#x2F;setup&#x2F;blob&#x2F;master&#x2F;HOME&#x2F;.hammerspoon&#x2F;i... reply Graziano_M 8 hours agorootparentMe too, actually. Maybe that would have been a more notable thing for me to mention. I use and contribute to Spacehammer.https:&#x2F;&#x2F;github.com&#x2F;agzam&#x2F;spacehammer reply dinkleberg 15 hours agoparentprevI’ve been thinking about doing this, but then reminding myself that this is not a wise use of my time lol. However, this looks pretty nice. I might have to do it.Thanks for sharing your setup, I’ll be stealing some ideas. reply Graziano_M 14 hours agorootparentIt was definitely not a good use of my time lol. reply toastal 7 hours agoparentprevI have a goal to move my Neovim config to LunarML one of these days. I too would love better ergonomics over Lua. reply teruakohatu 14 hours agoparentprevCan nvim read it natively or do you need to transpile it? reply gorjusborg 14 hours agorootparentYou don&#x27;t need to transpile it if you use https:&#x2F;&#x2F;github.com&#x2F;Olical&#x2F;nfnl reply delboni 13 hours agorootparentIf you want a sample nvim setup using nfnl you can check this out: https:&#x2F;&#x2F;github.com&#x2F;rafaeldelboni&#x2F;cajus-nfnl reply Graziano_M 6 hours agorootparentThis nerdsniped me. I&#x27;ve finally taken plunge and started converting my aniseed config to nfnl. It&#x27;s pretty painful, not even sure if I will keep the changes, but I want to see. reply nerdponx 8 hours agorootparentprevThere&#x27;s also Hotpot https:&#x2F;&#x2F;github.com&#x2F;rktjmp&#x2F;hotpot.nvim reply sullyj3 9 hours agorootparentprevwell, nfnl transpiles it for you reply Graziano_M 14 hours agorootparentprevI use a plugin called Aniseed which loads first and does the transpiling on the fly for me. reply gorjusborg 13 hours agorootparentI linked to aniseed originally, but nfnl is what Olical is pointing people toward now? reply Graziano_M 10 hours agorootparentYep, but I haven&#x27;t made the leap yet. reply ducktective 16 hours agoparentprevSo basically Emacs with Elisp replaced with Fennel and a more responsive UI... reply packetlost 15 hours agorootparentEmacs is a whoooooole lot more than an editor and Lispy config language. reply thih9 13 hours agorootparentWhat else is there apart from an editor and a lispy config language?I always thought of emacs as that, i.e. an editor that is extensible and dev friendly (in contrast to vimscript). I don’t know much about emacs though, so this got me curious. reply Tao3300 11 hours agorootparentThough it looks and acts like that, and can be used in such a way with no problem, in a sense, you&#x27;ve got it inside out.From the GNU Emacs homepage:At its core is an interpreter for Emacs Lisp, a dialect of the Lisp programming language with extensions to support text editing.It&#x27;s not an editor with a lispy config language, it&#x27;s a Lispy interpreter that comes with an editor that can configure it via said Lisp.It&#x27;s kinda like one of those Smalltalk VMs where the line is blurred between the code you&#x27;re writing and the environment that it runs in. reply MassiveBonk51 12 hours agorootparentprevWell the famous joke is \"Emacs is an OS that lacks a decent text editor.\" (Yes Evil mode exists)Emacs can do basically anything you write an extension for it to do. It can be your calendar, your email client, your rss reader or even your git gui on top of being an editor. Emacs can do so much that it&#x27;s daunting to start using. reply trenchgun 3 hours agorootparent> Emacs can do basically anything you write an extension for it to do. It can be your calendar, your email client, your rss reader or even your git gui on top of being an editor. Emacs can do so much that it&#x27;s daunting to start using.Vim can do all of those things. reply forward-slashed 11 hours agorootparentprevThe interactivity is one. It&#x27;s easy to evaluate expressions from within the editor. I am not sure you can programmatically change aspects of nvim on the fly, and even if so, probably not a central component of nvim experience. M-x in Emacs is much more powerful than : in (n)vim. reply eddythompson80 9 hours agorootparentprevWhat if I don’t want to run a daemon for my text editor? reply j16sdiz 8 hours agorootparentprevNo, vim is not an OS reply nmz 15 hours agoprevNow I like lua and think single pass is the way to go for interpreted, since you don&#x27;t have the disadvantage of a slow compile time no matter how big your codebase gets, BUT its not great to write in. some things are (apparently) not possible, which means the only solution is to transpile into it, which has led to some good languages like moonscript[0], and the dynamic nature is a boon as your codebase grows, which has led to teal[1] which offers static type checking.[0]: https:&#x2F;&#x2F;moonscript.org&#x2F;[1]: https:&#x2F;&#x2F;github.com&#x2F;teal-language&#x2F;tl reply speps 12 hours agoparentDon&#x27;t forget Terra as well: https:&#x2F;&#x2F;terralang.org&#x2F; reply user3939382 15 hours agoparentprev> is a boonA boon is a good thing. I think you meant a problem? reply agalunar 12 hours agorootparentPerhaps they had in mind \"bane\". reply dang 14 hours agoprevRelated. Others?Language Showcase: Fennel - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32349491 - Aug 2022 (2 comments)Fennel: A Practical Lisp - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31029478 - April 2022 (85 comments)Fennel – Lisp in Lua - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24390904 - Sept 2020 (112 comments)Fennel – Lisp in Lua - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18016168 - Sept 2018 (62 comments) reply timetraveller26 16 hours agoprevI&#x27;ve been wanting to give Fennel a try, currently I use Moonscript for my Lua transpilation (https:&#x2F;&#x2F;moonscript.org&#x2F;), but it has some criticisms and it can be a bit confusing sometimes. reply JNRowe 12 hours agoparentI&#x27;m a big fan of moonscript, but occasionally wish it was still being improved and worked on. Yuescript¹ looks like it fixes most of my bugbears with moonscript, and is for the most part a faster² drop-in replacement.There was s little discussion here ~18 months ago³, but it will feel largely circular if you look at it as people are suggesting fennel there ;)¹ https:&#x2F;&#x2F;github.com&#x2F;pigpigyyy&#x2F;Yuescript² This probably only matters if you have tonnes of moonscript, not just a little neovim&#x2F;mpv&#x2F;awesomewm config or something.³ https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29903133 reply lc9er 10 hours agoparentprevAre you using Moonscript for nvim config or something else? reply gsuuon 15 hours agoprevThis post inspired me to look for an ML-like language that compiles to lua and I found this useful list: https:&#x2F;&#x2F;github.com&#x2F;hengestone&#x2F;lua-languages reply robertlagrant 16 hours agoprev> Another common criticism of Lua is that it lacks arity checks; that is, if you call a function without enough arguments, it will simply proceed instead of indicating an error. Fennel allows you to write functions that work this way (fn) when it&#x27;s needed for speed, but it also lets you write functions which check for the arguments they expect using lambda.I don&#x27;t understand this; why is this a speed consideration? reply sullyj3 11 hours agoparentIt&#x27;s a runtime check. The following fennel: (fn add-1 [x] (+ x 1)) (lambda add-2 [x] (+ x 2))transpiles to the following lua: local function add_1(x) return (x + 1) end local function add_2(x) _G.assert((nil ~= x), \"Missing argument x on &#x2F;home&#x2F;sullyj3&#x2F;tmp&#x2F;fn-vs-lambda&#x2F;fnl&#x2F;x.fnl:3\") return (x + 2) end return add_2 reply robertlagrant 3 minutes agorootparentGotcha - thanks! reply rpdillon 16 hours agoparentprevI&#x27;m not an expert, but my understanding is the Lua has a very limited grammar that is specifically designed to be parsed in a single pass. This precluded Lua from performing arity checks on function calls.Fennel does support arity checks but it comes with a runtime cost since it&#x27;s implemented as a separate Lua call after being transpiled from Fennel. reply vore 14 hours agorootparentThe grammar shouldn&#x27;t have anything to do with arity checking – at runtime, you&#x27;re going to know if you passed a function 2 arguments or 3 arguments, and due to the presence of dynamic evaluation, knowing how many arguments a function takes is not statically knowable anyway. My best guess is that they want to avoid the overhead of a runtime check for arity. reply Joker_vD 15 hours agoparentprevIIRC (and I remember it quite vaguely from some random blog post on the Internet) the Lua implementation uses some trick that greatly speeds up setting up and tearing down activation frames (and has also something to do with the ability to easily cross-call into C back to Lua again?), but it hinges on that argument-passing&#x2F;result-returning semantics being \"use nils for missing stuff, throw away the extras\" so e.g. Python can&#x27;t use it to speed up its implementation of function calls.Unfortunately, I can&#x27;t find that blog post so take my words with a grain of salt. reply oedo 10 hours agoparentprevDeclaring a function with `lambda` prepends runtime nil checks for each argument to the function body (excluding optional args-- specified with a question mark, eg. `?x`-- and variable arguments, eg. `...`[1]).This Fennel: (fn foo [x y ...] (print x y ...)) (lambda bar [x ?y ...] (print x ?y ...))...transpiles to this Lua: local function foo(x, y, ...) return print(x, y, ...) end local function bar(x, _3fy, ...) _G.assert((nil ~= x), \"Missing argument x on test.fnl:3\") return print(x, _3fy, ...) end[1] Fennel&#x27;s alternative `& xs` vargs format does create a runtime check in functions declared with lambda, but it always evaluates to true because it simply nil-checks the table (here `xs`) that it dumps Lua `...` vargs into. reply throwway120385 12 hours agoparentprevThis is mostly based on my experience in the past with Lua circa 2015 when I developed a GUI using Crank Storyboard Engine and needed to write some stuff in C because the Lua equivalent code was too CPU-intensive.If I remember correctly, Lua is a stack-based VM. What this means is that every piece of data has a corresponding location on a stack data structure in-memory. Function arguments are pushed into the stack as-needed and popped in the function context, and vice-versa for the function return values.If you wanted arity-checking in this context, you&#x27;d have to confirm that you got exactly the right number of elements on the stack, meaning there would be an extra branch in every function call. This might reduce performance if the branch predictor gets it wrong. Plus there would need to be extra instructions to count and to check the count for each pop off the stack in the context of the function call.This is from the perspective of calling C functions in Lua, so it&#x27;s probably more complicated for the VM itself when it&#x27;s running native Lua code. reply Verdex 12 hours agorootparentLua is a register based VM. reply neutrono 11 hours agorootparentI think he means that the C API is stack based? reply gumby 16 hours agoparentprevMy memory of this is hazy, in part because I wasn&#x27;t a C programmer in those days, but ISTR C used to do this too. I think prototypes were added (and required) only by the time ANSI got involved.Maybe some other OF can confirm&#x2F;contradict? reply ahoka 15 hours agorootparentFun fact: in C, if you have a signature like “int foo()”, it means any number of arguments can be passed (vs “int foo(void)” which means no args). reply sovietswag 15 hours agorootparentGood reference for that: https:&#x2F;&#x2F;jameshfisher.com&#x2F;2016&#x2F;11&#x2F;27&#x2F;c-k-and-r&#x2F;This was also used to implement variadic functions like printf before varargs, see https:&#x2F;&#x2F;retrocomputing.stackexchange.com&#x2F;a&#x2F;20517 reply mdaniel 15 hours agorootparentprevthat&#x27;s ... (amazing|C for ya) int foo() { return 0; } int main(int argc, char* argv[]) { return foo(\"alpha\", \"beta\", 1); } $ gcc-13 -Wall -Werror -o foo foo.c # oh, sads $ clang -o foo .&#x2F;foo.c .&#x2F;foo.c:5:34: warning: too many arguments in call to &#x27;foo&#x27; return foo(\"alpha\", \"beta\", 1); ~~~ ^ .&#x2F;foo.c:5:15: warning: passing arguments to &#x27;foo&#x27; without a prototype is deprecated in all versions of C and is not supported in C2x [-Wdeprecated-non-prototype] return foo(\"alpha\", \"beta\", 1); ^ 2 warnings generated. reply astrange 14 hours agorootparentprevNo longer true in C23, it means void now. reply fulafel 15 hours agorootparentprevAnd there is no way for the callee to check the nr of passed args, unlike in Lua. reply mpenet 15 hours agoparentprevBecause it’s an extra runtime check vs a direct lua call reply iimblack 16 hours agoparentprevI think they meant speed of implementation. reply munificent 16 hours agoparentprevCaveat: I haven&#x27;t used Fennel but just inferring from what makes sense...The Lua VM that they compile to already handles missing arguments with some attendant but unavoidable performance cost.Fennel&#x27;s own `lambda` form that they layer on top does check for missing arguments, but it must do so by generating extra Lua code to do those checks. That additional code has a runtime cost.Using `fn` avoids that extra generated code and its cost.If Fennel had its own runtime and VM, then the performance story for how missing arguments are handled would be different. reply bnert 16 hours agoparentprevArity explanation: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ArityFrom the blurb you quoted, it sounds like Lua doesn&#x27;t check function argument arity natively, and fennel can. This means that fennel applies a runtime check to validate if a function was called with an expected arity, and if not, propagates an error (however Lua does that, I do not know), hence why arity checked functions are not as performant. reply nmz 16 hours agorootparentSo it basically does function M(...) if select(\"#\",...)>3 then error(\"failed arity check on M()\") end end reply ravi-delia 15 hours agorootparentpretty much, yeah reply Rochus 14 hours agoprevThere are quite many languages compiling to Lua (or LuaJIT). Fennel is just one of them. See e.g. https:&#x2F;&#x2F;github.com&#x2F;hengestone&#x2F;lua-languages. reply iLemming 14 hours agoparentWhat makes Fennel a bit different is that it is a Lisp inspired by Clojure. If for any reason neither of these ever interested you, then, yes, Fennel would not feel any different to you. reply Rochus 13 hours agorootparentThe referenced list seems to include a language for every taste; Fennel is also there as one of ~10 Lisp variants. reply iLemming 8 hours agorootparentLike I said, if one likes Clojure but has to write for a Lua-enabled platform, Fennel is the best choice today, since there&#x27;s no Clojure dialect for Lua, like ClojureScript or ClojureDart. Fennel although inspired by Clojure, still a very different language. Phil Hagelberg, AKA @technomancy, is the maintainer, a well-known and highly respected Clojurista. reply rcarmo 15 hours agoprevFunny thing, I was looking at Fennel and Lua game engines... Just updated with a few: http:&#x2F;&#x2F;taoofmac.com&#x2F;space&#x2F;games reply xwowsersx 7 hours agoprevSo I don&#x27;t think I have a strong stance on the merits or drawbacks of whitespace-significant languages versus those that use parentheses (or any other syntax choices). But, I&#x27;m curious if there are any compelling arguments that objectively demonstrate why one approach might be superior to the other or if there are at least some persuasive reasons to favor one over the other. Genuinely curious! reply aidenn0 7 hours agoparentI think (and am not alone) that indentation based languages are easier for beginners to learn.The fact that indentation (whether significant to syntax or not)helps readability is something I hope I need not demonstrate since indenting is near universal in languages that use explicit delimiters.So beginners learning a language with delimiters need to learn both delimiters and indentation, and then learn that while the latter helps them more, it&#x27;s actually just the former that is required.For beginners, mental load is death by a thousand cuts, so it may make more of a difference then an experienced programmer thinks.[Edit]My first reaction was just the benefits for beginners of indentation e syntax. I personally have a subjective feeling of advantages for non beginners to delimiter based syntax (I use common lisp regularly), but I have fairly low confidence in that, while I&#x27;m pretty sure of the advantages to beginners for indentation. reply xwowsersx 6 hours agorootparentThat does make some sense. It&#x27;s so hard to go back to the beginner&#x27;s mind (though undoubtedly valuable to try!), but I think you&#x27;re right that this has to be easier for a beginner. There&#x27;s just less syntax overall if there aren&#x27;t parentheses or curly braces. reply dunefox 3 hours agoparentprevStructural editing, macros, homoiconicity. reply calvinmorrison 6 hours agoparentprevNobody ever heard of python oneliners, for a start. reply giraffe_lady 16 hours agoprevI always say this when it comes up but fennel is very very impressive as a language project. Lua is much beloved but my professional experience is that working with it gets gnarly much faster than most other languages. Fennel focuses on a small set of the worst lua issues and addresses them without getting distracted or bogged down in language design extravagances or lisp nerd semiotics.The decision to strictly adhere to lua runtime semantics was initially repulsive to me, but I&#x27;ve come to appreciate the incredible wisdom of that approach. The practical experience of using lua is that because of the constraints of the language, every lua runtime is in some ways custom & unique. This decision of fennel&#x27;s creator allows you to drop fennel onto any lua codebase, of any version, with any amount of custom freak shit grafted in, and just use it. Not only use it, but hook its compiler into the lua module loader system, and freely & transparently mix functions and tables between the two languages. A life raft in decrepit legacy lua codebases.The restraint and technical focus of this language led to me checking out janet, another project of its creator. I&#x27;ve also come to really like that language, it makes some similarly minor-seeming but brilliant decisions like including PEGs in the core lang instead of regex.Anyway try fennel, if you have to interact with a lot of lua pattern match alone will improve your life. Try janet too it&#x27;s cool. reply throwawaypml 13 hours agoparentI think Fennel and Janet went separate ways fairly early on, and the creator of Fennel focussed primarily on Janet whilst the current maintainer of Fennel is responsible for much (most?) of what you see in Fennel today...So some very different design choices between the two languages, i believe. reply eviks 16 hours agoprev> yet keeps a very small footprint both conceptuallyit&#x27;s only a benefit for writing simplistic things (and configs for some complicated programs like a text editor or a terminal or some MMORPG is already not simple), otherwise you start appreciating that there is not just a single dictionary as a data type etc. reply whalesalad 13 hours agoprevFor the clojurists this is a cool intro: https:&#x2F;&#x2F;fennel-lang.org&#x2F;from-clojure reply magicalhippo 15 hours agoprevBecause it tastes amazing[1]!Oh... not the plant...[1]: https:&#x2F;&#x2F;www.foodrepublic.com&#x2F;recipes&#x2F;roasted-chicken-fennel&#x2F; reply madcaptenor 15 hours agoparentIt does! I came in here expecting it was going to be about food and was disappointed. reply nultxt 12 hours agoparentprevGenuinely an incredibly flexible, amazing vegetable! reply reilly3000 9 hours agoprevRefreshing pragmatism re: macros>This feature is intentionally listed last because while lisp programmers have historically made a big deal about how powerful it is, it is relatively rare to encounter situations where such a powerful construct is justified. reply kazinator 5 hours agoparentYou can hardly write anything in any mainstream Lisp without using macros. reply stonemetal12 12 hours agoprevIn there it says Lua uses &#x27;for&#x27; for looping over integer ranges and object ranges. They didn&#x27;t like that so they broke it up so that for is for integer ranges and each is for object ranges.That seems strange to me. A range is a range so why have different ways to iterate over ranges based on what type of range it is? reply giraffe_lady 12 hours agoparentI mean it is lua that has two different ways to loop, with different syntax and semantics, but calls them both \"for.\" \"Numeric for\" and \"generic for\" are the official names, with separate pages in the docs. Having different names for them seems like a reasonable and not that surprising choice. reply iLemming 14 hours agoprevIf you&#x27;re using a Mac, maybe check this out https:&#x2F;&#x2F;github.com&#x2F;agzam&#x2F;spacehammer reply SubiculumCode 7 hours agoprevAt some point, every noun and verb in existence will also be the name of a language, framework, or piece of software.not to detract attention from this cool project in the least. reply ShadowBanThis01 7 hours agoprevBecause it&#x27;s delicious. reply sdfghswe 14 hours agoprevI didn&#x27;t realize lua was a proper language until I learned that&#x27;s what factorio uses for its mods. reply dgb23 14 hours agoparentIt&#x27;s a very common language to embed in video games as a modding and configuration tool. reply SoftTalker 13 hours agorootparentAnd that&#x27;s the sort of thing it&#x27;s meant to do.Tools have their strengths. Use them for that. Don&#x27;t try to drive a nail with a screwdriver. reply whalesalad 13 hours agorootparentthis is a very vague statement. lua is used in a lot of environments and is very performant. you can use it inside nginx (openresty) to process requests at a pretty large scale. you can use it inside of redis.lua and javascript are somewhat interchangeable aside from the fact that js has a much larger ecosystem.sure there are situations where lua might be a bad choice but based on your remarks you make it sound like it sucks anywhere except inside a game engine. reply sbjs 9 hours agoprev> Finally Fennel includes a macro system so that you can easily extend the language to include new syntactic forms. This feature is intentionally listed last because while lisp programmers have historically made a big deal about how powerful it is, it is relatively rare to encounter situations where such a powerful construct is justified.In other words, there are serious drawbacks to using macros that are usually outweigh any benefits they might give you. reply freilanzer 59 minutes agoparentFalse, like with any feature there are use cases where macros make sense. reply uwagar 16 hours agoprevbecause its good for digestion reply cheeselip420 14 hours agoprev [–] Why does every single language landing page not have example code? I don&#x27;t want to read justifications. The code should speak for itself.Show me an echo server. Show me how you open a file and read data. Show me SOMETHING. Right on the landing page. How are we still doing this?EDIT: I stand corrected - this is but a subpage. The actual landing page apparently DOES have code. reply arvidkahl 14 hours agoparentLinked above is a subpage. The actual homepage at https:&#x2F;&#x2F;fennel-lang.org&#x2F; has code and even an integrated place to run it. reply giraffe_lady 14 hours agoparentprev [–] This isn&#x27;t the landing page, which does show a code example prominently. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fennel is a programming language based on the Lua runtime, aiming to offer a simpler and more concise syntax by removing complexities found in Lua.",
      "This language improves upon elements like global variable management, tables, loops, functions, and introduces features such as pattern matching and destructuring of data structures.",
      "Fennel is compatible with extant Lua libraries and tools, and incorporates a macro system to extend the language with new syntactic forms."
    ],
    "commentSummary": [
      "The post centers on using Fennel, a programming language, in Neovim and Hammerspoon configurations, appreciated for its brevity and superior ergonomics over Lua.",
      "Comparisons are made between Lua and other languages that compile to Lua like Moonscript and Yuescript, accentuating the benefits of Fennel's arity checks which Lua lacks.",
      "It broaches the topic of indentation versus delimiter-based languages and the practical use of Lua in different contexts, and includes references to a Fennel's website and the current availability of applications for YC Winter 2024."
    ],
    "points": 218,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1694614905
  },
  {
    "id": 37498473,
    "title": "Appeals court upholds right to post public laws online",
    "originLink": "https://www.eff.org/press/releases/appeals-court-upholds-publicresourceorgs-right-post-public-laws-and-regulations",
    "originBody": "Skip to main content Electronic Frontier Foundation About Issues Our Work Take Action Tools Donate SEARCH Appeals Court Upholds Public.Resource.Org’s Right to Post Public Laws and Regulations Online Our laws belong to all of us, and we should be able to find, read, and share them free of registration requirements, fees, and other roadblocks. PRESS RELEASE SEPTEMBER 12, 2023 Share on Twitter Share on Facebook SAN FRANCISCO—Technical standards like fire and electrical codes developed by private organizations but incorporated into public law can be freely disseminated without any liability for copyright infringement, a federal appeals court ruled Tuesday. Tuesday’s ruling by a three-judge panel of the U.S. Court of Appeals for the District of Columbia Circuit upholds the idea that our laws belong to all of us, and we should be able to find, read, and share them free of registration requirements, fees, and other roadblocks. It's a long-awaited victory for Public.Resource.org, a nonprofit organization founded in 2007 by open records advocate Carl Malamud of Healdsburg, Calif., and represented in this case by the Electronic Frontier Foundation (EFF) with co-counsel Fenwick & West and David Halperin. “In a nation governed by the rule of law, private parties have no business controlling who can read, share, and speak the rules to which we are all subject,” EFF Legal Director Corynne McSherry said. “We are pleased that the Court of Appeals upheld what other U.S. courts, including the Supreme Court, have said for almost 200 years: No one should control access to the law.” As part of its mission of promoting public access to all kinds of government information, Public Resource acquires and posts online a wide variety of public documents, such as nonprofits’ tax returns, government-produced videos, and standards incorporated into law by reference. These standards include electrical, fire safety, and consumer safety codes that have been mandated by governments. But without Public Resource’s work, they are often difficult to access, much less share with others, which means that areas of law that profoundly affect our daily life are obscured from our view. Even courts have had trouble accessing the laws that they are supposed to apply. The American Society for Testing and Materials (ASTM), National Fire Protection Association Inc. (NFPA), and American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE) are organizations that develop private sector codes and standards aimed at advancing public safety, ensuring compatibility across products and services, facilitating training, and spurring innovation. ASTM, NFPA, and ASHRAE sued Public Resource in 2013 for copyright and trademark infringement and unfair competition. Affirming a trial court’s March 2022 decision, the appeals court found Tuesday that Public Resource’s use is for nonprofit, educational purposes, and that this use serves a different purpose than that of the plaintiffs. The plaintiffs “seek to advance science and industry by producing standards reflecting industry or engineering best practices,” the court wrote, while “Public Resource’s mission in republishing the standards is very different—to provide the public with a free and comprehensive repository of the law.” “Public Resource posts standards that government agencies have incorporated into law—no more and no less,” the appeals court ruled. “If an agency has given legal effect to an entire standard, then its entire reproduction is reasonable in relation to the purpose of the copying, which is to provide the public with a free and comprehensive repository of the law.” The appeals court also found that although Public Resource has been posting incorporated standards for 15 years, the plaintiffs haven’t shown any evidence that this harmed them financially. And “even if Public Resource’s postings were likely to lower demand for the plaintiffs’ standards, we would also have to consider the substantial public benefits of free and easy access to the law,” the court ruled. The appeals court rejected the plaintiffs’ argument that because they make standards available for free in online reading rooms, Public Resource’s use cannot be transformative fair use. All but one of these rooms opened after Public Resource began posting incorporated standards, the court noted, and those reading rooms fail to provide convenient access. “Among other things, text is not searchable, cannot be printed or downloaded, and cannot be magnified without becoming blurry. Often, a reader can view only a portion of each page at a time and, upon zooming in, must scroll from right to left to read a single line of text. Public Resource’s postings suffer from none of these shortcomings.” Malamud praised the appeals court’s decision and said Public Resource will redouble its efforts. “It has been over 10 years since plaintiffs filed suit in this case,” he said. “Today, the U.S. Court of Appeals has found decisively in favor of the proposition that citizens must not be relegated to economy-class access to the law. This win would not have been possible without the untiring and dedicated work by the legal team of EFF, Fenwick & West, and David Halperin, as well as the dozens of amicus briefs filed on our behalf. I'm very grateful for all their help.” For the opinion of the U.S. Court of Appeals for the District of Columbia Circuit: https://www.eff.org/document/opinion-7 For more on the Public Resource case: https://www.eff.org/cases/publicresource-freeingthelaw CONTACT: Corynne McSherry Mitch Stoltz RELATED ISSUES FAIR USETRANSPARENCY RELATED CASES FREEING THE LAW WITH PUBLIC.RESOURCE.ORG Share on Twitter Share on Facebook JOIN EFF LISTS Discover more. Email updates on news, actions, events in your area, and more. Anti-spam question: Enter the three-letter abbreviation for Electronic Frontier Foundation: RELATED UPDATES DEEPLINKS BLOG BY KAREN GULLOJULY 20, 2023 Victory! Embedded Links to Photos on Instagram Don’t Infringe Photographers’ Copyrights, Court Rules Every day, we visit websites or read news articles that contain photos embedded from somewhere else, usually other websites or servers where the images were first published or stored. What’s happening behind the scenes when you click on a website chock full of photos and text is a basic function... DEEPLINKS BLOG BY CORY DOCTOROWMAY 24, 2023 To Save the News, We Must Shatter Ad-Tech This is part two of an ongoing, five-part series. Part one, the introduction, is here. Part three, about banning surveillance ads, is here. Part four, about opening up app stores, is here. Part five, about enshrining \"end-to-end\" delivery on social media, is here. Download this... DEEPLINKS BLOG BY CORYNNE MCSHERRY, CARA GAGLIANO, KATHARINE TRENDACOSTAMAY 23, 2023 What the Supreme Court’s Decision in Warhol Means for Fair Use The Supreme Court has issued its long-awaited decision in Andy Warhol Foundation v. Goldsmith, a fair use case that raised fundamental questions about rights and obligations of commercial artists. The Court’s opinion did not answer many of those questions, but happily it affirmed both important fair use precedents and the... DEEPLINKS BLOG BY CORYNNE MCSHERRYAPRIL 24, 2023 The DMCA Cannot Protect You From Your Own Words There is a loud debate raging over what companies should and shouldn’t be doing about the things people say on their platforms. What people often seem to forget is that we already know the dangers of providing a quick way for people to remove criticism of themselves from the internet.... DEEPLINKS BLOG BY MICHAEL BARCLAYAPRIL 10, 2023 In SAS v. WPL, the Federal Circuit Finally Gets Something Right on Computer Copyright Figuring out the correct boundaries of software copyright protection is a difficult task. As several judges have put it, “applying copyright law to computer programs is like assembling a jigsaw puzzle whose pieces do not quite fit.” Last week, the U.S. Court... DEEPLINKS BLOG BY CARA GAGLIANOJANUARY 20, 2023 For Would-Be Censors and the Thin-Skinned, Copyright Law Offers Powerful Tools We're taking part in Copyright Week, a series of actions and discussions supporting key principles that should guide copyright policy. Every day this week, various groups are taking on different elements of copyright law and policy, and addressing what's at stake, and what we need to do to make... DEEPLINKS BLOG BY CORYNNE MCSHERRYJANUARY 19, 2023 Fair Use Creep Is A Feature, Not a Bug Lawyers, scholars, and activists, including EFF, often highlight Section 512 of the Digital Millennium Copyright Act and Section 230 (originally of the Communications Decency Act) as the legal foundations of the internet. But there’s another, much older, doctrine that’s at least as important: Fair use, which dates back many decades... DEEPLINKS BLOG BY KIT WALSHDECEMBER 7, 2022 DC Circuit Evades Important Questions in Disappointing Ruling on Section 1201 of the DMCA The Court of Appeals for the DC Circuit has issued a disappointing ruling in the case of Green v. DOJ. The ruling left intact a law that has stifled speech and innovation for decades and forced researchers, advocates, teachers, and tinkerers to beg for government permission to do... DEEPLINKS BLOG BY CINDY COHNJULY 13, 2022 Impact Litigation in Action: Building the Caselaw Behind a Win for Free Speech A recent District Court decision in In re DMCA 512(h) Subpoena to Twitter, Inc. is a great win for free speech. The Court firmly rejected the argument that copyright law creates a shortcut around the First Amendment’s protections for anonymous critics. In the case, a company tried to... PRESS RELEASEJULY 8, 2022 Internet Archive Seeks Summary Judgment in Federal Lawsuit Filed By Publishing Companies SAN FRANCISCO—The Internet Archive has asked a federal judge to rule in its favor and end a radical lawsuit, filed by four major publishing companies, that aims to criminalize library lending.The Internet Archive, headquartered in San Francisco, is a 501(c)(3) non-profit library which preserves and provides access to cultural artifacts... FOLLOW EFF: x facebook instagram youtube flicker linkedin mastodon tiktok Check out our 4-star rating on Charity Navigator. CONTACT General Legal Security Membership Press ABOUT Calendar Volunteer Victories History Internships Jobs Staff Diversity & Inclusion ISSUES Free Speech Privacy Creativity & Innovation Transparency International Security UPDATES Blog Press Releases Events Legal Cases Whitepapers EFFector Newsletter PRESS Press Contact DONATE Join or Renew Membership Online One-Time Donation Online Giving Societies Shop Other Ways to Give COPYRIGHT (CC BY) TRADEMARK PRIVACY POLICY THANKS",
    "commentLink": "https://news.ycombinator.com/item?id=37498473",
    "commentBody": "Appeals court upholds right to post public laws onlineHacker NewspastloginAppeals court upholds right to post public laws online (eff.org) 192 points by glitcher 18 hours ago| hidepastfavorite58 comments horsawlarway 16 hours agoThe fact that this even made it to trial is flabbergasting, and truly highlights how unacceptably broken our copyright system is.They were suing people for literally explaining the laws to people... as a violation of their copyright.If that&#x27;s something we have to even consider - I want this fucking garbage copyright system destroyed. reply cvoss 16 hours agoparentThey were suing people for copying copyright-protected works. The usual arguing over exceptions ensued.In this particular case, is the problem that copyright laws confer too many rights to the owners, or is the problem that legislative bodies are adopting inaccessible text as law? reply horsawlarway 16 hours agorootparent> They were suing people for copying copyright-protected works.Those works are our literal laws.Suing people for sharing the law is not an acceptable position in ANY discussion. Period. Full fucking stop.There is NO way you can claim to be any sort of democracy if we cannot talk about our laws,I don&#x27;t fucking care how we got there (I do, but not for this discussion) - the fact that we are here AT ALL is a HUGE flashing alarm blaring about how fucking off the rails the laws here have gotten. reply smhenderson 15 hours agorootparentI agree with the decision and in principle absolutely agree with what you are saying about our laws.But you&#x27;re oversimplifying the case here. They weren&#x27;t complaining that the law was being published, they were complaining that their standards were published. The court agreed with PR that once those standards were incorporated into law, they were subject to fair use publication under the auspices of making available and explaining our laws to the public.The courts agreed and here we are. But as another user wrote, this wasn&#x27;t about copyrighting the law, it was about the inclusion of copyrighted material in the law and whether or not it fell under a different category with regards to fair use.I&#x27;m sure I&#x27;m oversimplifying or missing something too, but I, who am generally opposed to how copyright is currently handled in the US, can see that there is more nuance to this case than your post admits to. reply horsawlarway 14 hours agorootparentSo what you&#x27;re saying is that we&#x27;re A-OK as a society with the gatekeeping of critical safety information behind private paywalls and copyright.Safety information that is gathered at the expense of the public: we suffer the consequences that inform the standards. My fellow citizens have paid for those lessons with fucking blood.You are saying we are all better served if copyright (another \"benefit for the public\" according to the proponents, mind you) were to apply to that information as default.We are collectively better if we forced to enrich a private organization to simply read and understand critical safety information.That is what you are saying?Because my clear and immediate response is: fucking bullshit. reply apendleton 14 hours agorootparentThis tone doesn&#x27;t feel especially productive. You asked:> So what you&#x27;re saying is that we&#x27;re A-OK as a society with the gatekeeping of critical safety information behind private paywalls and copyright.but the comment you&#x27;re responding to explicitly said:> I agree with the decision and in principle absolutely agree with what you are saying about our laws.So like... they&#x27;re obviously not saying what you&#x27;re accusing them of saying. They&#x27;re just explaining why this litigation occurred in the first place (and agreeing with the outcome, which seems to be the outcome you also want). reply singleshot_ 10 hours agorootparentprevMaybe, but it’s the law. The feeling-based approach to what-is-right is spectacularly unproductive and also somewhat unpleasant to interact with. reply falcor84 9 hours agorootparentIsn&#x27;t the \"feeling-based approach to what-is-right\" the whole intent of the jury system? reply pavon 14 hours agorootparentprevYes, all laws should be publicly accessible, but there are multiple ways legally to address the situation. You can just as equally say that the copyright is valid and people are prohibited from sharing the incorporated code, but the state legislatures violated the constitution by making private laws and thus the laws are overturned. reply pixl97 13 hours agorootparentOr you could look at the actual intent of copyright law in the first place and invalidate any copyright that becomes part of the encoded law. This then removes the burden from any individual that publishes the law, and forces the copyright holder losing their holding to fight the state. reply Gibbon1 13 hours agorootparentprevI don&#x27;t have time to pursue this. But I suspect that you could argue that a state doing this is violating the commerce clause. I feel like states can create some types of monopolies but not others.You can create a power monopoly, like I have to connect to PG&E&#x27;s grid and pay them. But California can&#x27;t pass a law giving Hasbro the sole right to sell toys in California. reply singleshot_ 10 hours agorootparentIn law, I believe what this would violate is more properly known as the Dormant Commerce Clause, if you are interested in learning more. reply lotsofpulp 15 hours agorootparentprev> is the problem that legislative bodies are adopting inaccessible text as law?This one. Anything the government deems a standard or law should be in the public domain, accessible directly from government sources. And obviously, from there, anyone who wants to copy it would be able to.>is the problem that copyright laws confer too many rights to the ownersAlthough this is also a problem due to excessively long copyright terms, but not in this specific case. reply alwayslikethis 15 hours agorootparentAlternatively, inaccessible law should be unconstitutional and unenforceable. reply johngladtj 1 hour agorootparentAnything the government does that isn&#x27;t public should not be legal and have zero force.Anything else and democracy does not work. reply bastawhiz 14 hours agorootparentprevI don&#x27;t think this is a practical outcome. If the law for seatbelt standards references a copyrighted document for materials, I don&#x27;t want my car company throwing out the standards because the doc is copyrighted. It should simply be the case that the copyright is forcibly expired and the document with the information referenced by law is in the public domain. reply midoridensha 7 hours agorootparentThis basically allows legislators to force anything into the public domain by simply referencing it in a law.I&#x27;d rather see legislators forced to negotiate with copyright holders to make their works public domain, or else have their laws become invalid and thrown out, and have their governments subject to lawsuits for trying to enforce copyrighted laws. reply f1shy 14 hours agorootparentprevAND it must be in a language that a non-lawyer can understand reasonably well, without at least making a big mistake. reply lolinder 13 hours agorootparentprevBoth. But it&#x27;s important to clarify that the publisher is not a victim of IBR, they profit enormously by having their standards take on the force of law. ANSI is active (on behalf of organizations including the plaintiffs) in lobbying for private standards to be incorporated into public law [0], and they&#x27;re not doing it purely out of a desire to save the government time.[0] https:&#x2F;&#x2F;www.ansi.org&#x2F;outreach&#x2F;government&#x2F;ansi-activities reply pavon 16 hours agoprevExcellent ruling. The next question in my mind is whether incorporating private standards into legislation without compensation violates the the takings clause of the constitution.It is well established that this clause applies to intangible property such as copyright. Furthermore I disagree with some aspects of the courts assessment regarding the purpose and impact to revenue. On the first point, there is a large overlap between people who need to know the legal requirements for building and those who want to know best practices - the later is nearly a subset of the former, especially once these best practices become the law. Furthermore, the distribution of incorporated standards has historically been a legal gray area at best. Just because there was little evidence of lost revenue in the past doesn&#x27;t mean there won&#x27;t be significant lost revenue after it becomes unambiguously legal. reply PaulDavisThe1st 16 hours agoparent\"Let&#x27;s put it like this: we can use your standards in our legislation, and everyone will need training and information and assistance with the more technical aspects of the standard, and you can derive revenue from that. Or we can use some other standards, and yours will have approximately zero value to anyone. What do you think?\" reply tialaramex 16 hours agorootparentFurther, if in fact your standard was the only possibility (and so we can&#x27;t have a different standard), there was in fact no creative element to your standard, and so it not only isn&#x27;t protected from copying and redistribution for the purpose of informing people as to the law, but never protected at all since it had no creative element.And yes, in practice we see that there is much more demand for relatively compact, opinionated books about something standard (e.g. \"Beautiful C++\" the most recent book from Kate Gregory) than there is for the large volumes of tremendously dry technical documentation which makes up the standard itself, a draft of which (nobody who matters cares about the \"official\" document) is here: https:&#x2F;&#x2F;timsong-cpp.github.io&#x2F;cppwp&#x2F;I&#x27;d guess that you&#x27;ll shift way more of a video course on the correct use of fire extinguishers than you would of a dry document specifying exactly how many litres of what retardants shall be provided to fight a fire of so-and-such proportions. Just make sure you don&#x27;t persuade legislators to make watching your video a legal requirement and you can sell those for a nice healthy profit. reply PaulDavisThe1st 16 hours agorootparentAlso, see construction in general: far more people interested in watching people like Matt Risinger or Stud Pack deconstruct construction, so to speak, than document every last detail of the relevant building code(s).\"How to build it really good (and to code)\" is far more in demand than \"Tell me what the code is\". reply yjftsjthsd-h 14 hours agorootparentprevSeems like the obvious solution is to ban the use of private standards in public law. A vendor may choose to be eligible for having its standards used in the law by making them open&#x2F;public&#x2F;free for use, xor it can keep them private. reply PaulDavisThe1st 9 hours agorootparentPretty sure that&#x27;s what this court ruling just did. reply kelnos 14 hours agoparentprevIsn&#x27;t the fact that the private standard is becoming law compensation enough? The company that wrote&#x2F;owns the standard will certainly experience economic benefits from having its work enshrined in law.Certainly the company should have to agree: the government should not just be able to assume the company will be ok with it. But presumably in this particular case, the companies that wrote these particular fire protection and electrical standards did so because they wanted them to eventually have legal force. reply apendleton 14 hours agorootparent> presumably in this particular case, the companies that wrote these particular fire protection and electrical standards did so because they wanted them to eventually have legal force.Sure, but the reason they wanted this is at least part because they could earn revenue from selling access to them (and having the monopoly on being able to do so), which they now no longer can. It&#x27;s not obvious to me how they would otherwise materially benefit. reply Terr_ 13 hours agorootparent> [When not selling access to the documentation,] It&#x27;s not obvious to me how they would otherwise materially benefit.Two ways come to mind:1. Detailed expertise and brand-recognition would provide them an edge in selling consulting and training services.2. Depending on how the legislature incorporated the standards, they would be in a position to promote&#x2F;prevent future changes that benefit&#x2F;disadvantage their stakeholders. Potentially a kind of regulatory capture. reply apendleton 14 hours agoparentprev> The next question in my mind is whether incorporating private standards into legislation without compensation violates the the takings clause of the constitution.The standards cost money to produce, so it seems like _someone_ should pay for them, otherwise there&#x27;d be no incentive for the standards bodies to produce them in the first place. So I think maybe the best outcome would be a finding that these _are_ a taking, and that then governments who want to incorporate them by reference should have to pay to be able to do so (but then they&#x27;d be free to read to the public). reply cvalka 11 hours agoparentprev\"It is well established that this clause applies to intangible property such as copyright\".No! The taking clause does NOT apply to copyright since copyright is not private property but an artificial government granted monopoly. reply dwater 16 hours agoprevThis is great, I&#x27;ve suffered as a result of how this was previously. I built a garage and had it inspected. I failed an electrical inspection and was given a reference in the National Electrical Code. When I went to look it up to figure out how to remediate, most of the links were trying to sell a copy. NEC wants $145 or a $11.99 monthly subscription. I had to find an illicit copy in order to comply with my local regulations. reply tzs 14 hours agoparentI don&#x27;t know how long it has been this way, but for at least the last few years you can read it online for free [1] if you create an account at the publisher&#x27;s site, nfpa.org.Here&#x27;s a list of the codes NFPA publishes [2]. The NEC is NFPA 70.Checking a few random ones, the ones that were not obsolete or withdrawn are available for online free reading. That includes several past editions. E.g., for the NEC the free online access includes editions going back to 1968.(Old editions are sometimes relevant because local codes often lag in adopting the latest version).[1] https:&#x2F;&#x2F;www.nfpa.org&#x2F;Codes-and-Standards&#x2F;All-Codes-and-Stand...[2] https:&#x2F;&#x2F;www.nfpa.org&#x2F;Codes-and-Standards&#x2F;All-Codes-and-Stand... reply nico 14 hours agorootparentNot op. Thank you for the resources> if you create an account at the publisher&#x27;s site, nfpa.orgIf you need to create an account, that’s only free as in beer, but not free as in libre, which in my opinion every law&#x2F;code should be reply f1shy 14 hours agoparentprevIn Germany as far as I know (please somebody correct if knows better!) there is an association called \"VDE\" which is private. They make norms, which you must comply with. You have to pay for them, are are relatively expensive (much more than the cost of the editing and the paper in any case). I find it incredible, because the law you are subject to, is not public. But even much worst: basically the state has delegated writing laws to a private company. Please note! VDE is an association of many private companies, they could basically make a new norm, that some kind of cable, devices or such is now required, and you have to comply! That is the wolf taking care of the herd. There was already a case, where they tried to make obligatory a kind of better RCD with microprocessor, but if backfired spectacularly... but it could happen any time again. reply Supply5411 12 hours agoparentprevI would say it&#x27;s Kafkaesque, but charging money for access to the legal text seems to take it a step further. reply sullija722 15 hours agoprevIt is great to see this made clear in the U.S., as it facilitates the flow of information and efficiency. Canada also needs to do this for legally mandated regulations referring to the CSA, NRC, etc. reply bluGill 14 hours agoprevThe conflict here is it costs a lot of effort (time and money) to do the hard work of figuring out what a good code should be. Used to be large cities (New York) would pay for it, and small towns would copy it - which meant if you lived in a smaller city you were freeloading off of New York (and in some cases you had to do something that made sense for New York but not where you were) which is unfair to New York&#x27;s taxpayers. By have some organization create a standard towns can more equally share the cost of building a good standard.Standards organizations need to pay the engineers who do all the complex calculations to figure out what works. However I think everyone agrees the law shouldn&#x27;t be hidden from people who need to obey it. reply mistrial9 14 hours agoparentyou are not wrong, but missing quite a bit. In California and elsewhere, small variations in building and electrical code are used to stall, or make conflicting requirements using vague wording, that gives excess discretionary power to inspectors. In a large, wealthy city like San Francisco, inspectors routinely stall projects, or exert excessive discretionary powers over construction. Why? obviously it is an entry to corruption, but don&#x27;t overlook the convenience to the city department of being completely and totally in control, to stall.. while being on salary. reply pixl97 13 hours agoparentprevThen states need to make laws of shared governance for the entire state, and the tax base of the state is used to create and pay for said standards creation.Having hundreds of different hidden behind paywall standards for each municipality is not doing the population at large any favors. reply bluGill 13 hours agorootparentSure, which is why states mostly have taken over from cities. State have them gone together to create&#x2F;encourage these entities to create good codes they can just adopt so they don&#x27;t have to pay the full price, while they get input into the process for whatever might be unique for their state.The idea of these entities doing the work is sound. It is just that they have a little too much ability to control who gets to see the result of their work. reply chankstein38 16 hours agoprevSo, prior to this ruling, there were laws that the public couldn&#x27;t get access to? I assume you could get them from magistrates and stuff though still, right? Or was it a complete lack of access?Obviously online is still way preferred it just is crazy to me to think there was a law somewhere that you couldn&#x27;t easily get the full text of. How the heck am I supposed to follow a law I can&#x27;t read? lol reply lmkg 16 hours agoparentThis is about Incorporation By Reference.For example, ASTM (a standards body) published a safety standard for children&#x27;s toys. This was just a book that they sell, named ASTM F963.Then, in 2008, Congress passed a law that all toys sold in the US must adhere to ASTM F963. The law itself can be publicly viewed. But the law does not directly include the text of ASTM F963. It just refers to it by name.The standard is copyright ASTM, and they sell copies for about a hundred bucks.For better or worse, the law refers to the current version of the standard. It&#x27;s been updated a few times, and the legal requirement is to use the most recent version. The law has not changed, but the requirements have.I am given to understand that on the local level this is common for building codes too. A municipality will say that buildings must follow the safety standards put out by some professional body or other relating to fireproofing lumber or framing or whatever. reply f1shy 14 hours agorootparentThis is terrible not only because you have made a law which is inaccessible to the population (which is in and out itself a total aberration in the law system).But also:- You made the law a moving target: when the new book is published, how you deal with products that were developed a month before, and went to market a month later? How are companies supposed to comply?- Most important: the state is delegating the most important responsibility of making laws, and putting it in hands of profit driven companies or organisations. That cannot end well. reply bluGill 14 hours agorootparentprevSomeone should publish their own standard titled \"ASTM F963\" but with obviously different contents. With a little work a lawyer can make your version meet the letter of the law as well. reply racingmars 16 hours agoparentprevThe problem was that the laws referred to commercial documents from standards bodies. Rather than reproducing the entire text of the standard in the law itself (which would be public), the law says \"Buildings must comply with XYZ standard 123,\" where XYZ 123 has to be purchased from the standards body and they claim copyright on it.This ruling says that providing that XYZ 123 standard online for non-commercial purposes is fair use if that standard is incorporated into the law. reply emilfihlman 17 hours agoprevThis is superb, and Europe should follow suit. reply guerby 13 hours agoparentIn France it&#x27;s done this way:https:&#x2F;&#x2F;www.legifrance.gouv.fr&#x2F;loda&#x2F;article_lc&#x2F;LEGIARTI00002...\"Les normes sont d&#x27;application volontaire. Toutefois, les normes peuvent être rendues d&#x27;application obligatoire par arrêté signé du ministre chargé de l&#x27;industrie et du ou des ministres intéressés. Les normes rendues d&#x27;application obligatoire sont consultables gratuitement sur le site internet de l&#x27;Association française de normalisation.\"If a law makes a norm mandatory then those norms must be free for eveyrone to read. reply f1shy 14 hours agoparentprevLets hope... I&#x27;ve seen far too many private organisations basically writing legislation. (e.g. VDE) reply sunbum 16 hours agoparentprevIsnt this more a case of the US lagging 1000 years behind like on card payments? All EU laws are publically available? And all laws in Denmark have been publically available for who knows how many years? reply jaclaz 16 hours agorootparentNo, in Europe we have the same problem with ISO norms and likely on some national technical ones, strictly speaking they are not Laws, but they are often cited either in the Laws and&#x2F;or exposed to the public through labeling.An example:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=17587770 reply andylynch 15 hours agorootparentBang on. Until recently I had several reams of MiFID and its implemting regulations on my desk, they sensibly are very closely aligned with standards, which co-evolve with them- so they are filled with references to ISO20022, 10383, 17442 and so many others. That said, even for our very complex business needing to refer to the standards was very rare. (It still would absolutely have saved time and money, but finding someone to commit actual money to BSI or ANSI membership can sometimes get lost in bureaucracy) reply f1shy 14 hours agorootparentprevThe case with VDE is a good example, where the norms have law force. In contrast DIN makes a specific statement, that the norms are just recommendations (as it should!) and have no law force. Even then, often through \"state-of-the-art\" obligations, they end up having law force. That is BAD idea. reply techsupporter 16 hours agorootparentprevThe law itself is broadly available (there are other cases going on about \"reported\" law which is the compendium put together by private editors), but they reference technical standards that aren&#x27;t.For example, a hypothetical RCW 94.50.691 in Washington might read \"all paving stones sold for use in residential koi pond construction shall conform to American Impervious Surface Standards Body Standard Code 72-67-243\". It&#x27;s that last part that would be copyrighted by a private body and only available for sale at $47,542 per license. reply tialaramex 16 hours agorootparentYes, in the UK for example, there will be Primary Legislation which is all debated in Parliament and you can go read that online at https:&#x2F;&#x2F;www.legislation.gov.uk&#x2F;And then there&#x27;s \"Secondary Legislation\" some of which are available on that site but others are not. All these require Primary Legislation to enable them, but it means e.g. the Primary Legislation might say something like, \"All Circus Clowns are also prohibited from entering a Red Zone as shall be decided from time-to-time by the Minister for Circus Clowns\" and now that Minister just gets to make a Red Zone list, parliament says they get to do that and it prohibits Circus Clowns from entering - Parliament doesn&#x27;t get to vote on updates to the list (but they do get to decide their own Prime Minister, which is why we didn&#x27;t have Liz Truss for very long)It is not uncommon for either Primary or Secondary Legislation to cite documents which are copyrighted by somebody else. Typically they are available at your public library, but as in the case decided here, that&#x27;s not exactly convenient when compared to using Google. reply mikece 15 hours agoprev [–] Well, there goes my defense that I couldn&#x27;t know the law because it was copyrighted and I didn&#x27;t have the money to buy a copy of the law in order to know what I could and could not do. reply nico 14 hours agoparent [–] Unfortunately that’s not an accepted defense. Ignorance of the law doesn’t excuse you from complying reply f1shy 14 hours agorootparentThis principle is cited way too often in a more general way as it really apply. Is not so easy as \"you must know every single rule, norm, law that exist, with no possible excuses\". If you are not in a reasonable position to have access to the norm, you do, and it do works, can defend yourself on that basis.I had successfully defended myself in two cases where the transit law was a little diffuse, but just saying \"I understood it differently, I&#x27;m sorry\" reply mikece 14 hours agorootparentprev [–] Ignorance of the law when you could have known is no defense. If the law is copyrighted and payment is required to even know it, that changes things. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A federal appeals court ruled that technical standards incorporated into public law can be shared without risking copyright infringement, reflecting the view that laws belong to everyone and should be accessible free of charge and without registration.",
      "The case was initiated by nonprofit Public.Resource.org, represented by the Electronic Frontier Foundation, against private bodies that create codes and standards.",
      "The court ruled in favor of Public.Resource.org, marking it a triumph for public access to the laws, mainly due to the nonprofit, educational nature of its use of the standards."
    ],
    "commentSummary": [
      "An appeals court has determined that publishing public laws online does not infringe copyright and is regarded as fair use.",
      "This case raises awareness about inaccessible legislation and questions about copyright protections, spotlighting instances of copyright laws that require payment for access.",
      "The discourse also involves the incorporation of private standards into laws without compensation, and highlights the varying practices from different countries regarding the availability of legal texts and standards."
    ],
    "points": 192,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1694620798
  },
  {
    "id": 37497956,
    "title": "Bitty Engine: An itty bitty game engine",
    "originLink": "https://paladin-t.github.io/bitty/",
    "originBody": "HomeDocumentsAboutGitHub An itty bitty game engine. Get Bitty Engine The Steam version and the Itch version offer an identical set of features, and they are both DRM-free. A trial is available on Steam also. About Bitty Engine Bitty Engine is a cross-platform itty bitty Game Engine and open-source Framework. The full featured engine is programmable in Lua and integrated with built-in editors. It keeps the nature of both engine's productivity, and fantasy computer/console's ease to iterate. It boosts your commercial/non-commercial projects, prototypes, game jams, or just thought experiments. Why Bitty Engine? Bitty Engine has everything built-in for coding, graphics composing, etc; it has a full featured debugger for breakpoint, variable inspecting, stepping, and call-stack traversing; it offers a set of well-designed API with full documentation; it builds fast binaries with code and asset obfuscating, moreover its package size is small (around 10MB with an empty project, the other engines output more than 10 times bigger). It is supposed to be your ultimate 2D game creating software. Features Bitty Engine offers a set of orthogonal features that makes game development comfortable and enjoyable. Programmable in Lua, an easy to learn and widely used programming language Debugger with scope inspector, breakpoint support and stepping Easy to use API for resources, graphics, input, audio, and more other facilities Built-in libraries for File, File System, Archive, JSON, Network, Physics, etc. Various example projects Handy tools for editing sprite, map, image, text, JSON, etc. Project can be exported into standalone binary Technical specifications Display: configurable resolution Code: Lua, supports multiple source files Image: either true-color (PNG, JPG, BMP, TGA) or paletted, up to 1024x1024 pixels per file Palette: 256 colors with transparency support Sprite: up to 1024x1024 pixels per frame, up to 1024 frames per sprite Map: up to 4096x4096 tiles per page Font: supports Bitmap and TrueType Audio: 1 BGM channel, 4 SFX channels; supports MP3, OGG, WAV, etc. Gamepad: 6 buttons for each pad (D-Pad + A/B), up to 2 players Keyboard and mouse: supported Make your own games Redistribute Bitty Engine helps you to make standalone binaries for Windows, MacOS, Linux and HTML (WebAssembly). It is redistributable for both non-commercial and commercial use without extra fee, your project is totally yours. Put an image at \"../icon.png\" relative to executable to customize the icon dynamically. Put an image at \"../splash.png\" as well to customize the splash; the image could be transparent. You can also customize redistributable binary by compiling from source. Get Bitty Engine The Steam version and the Itch version offer an identical set of features, and they are both DRM-free. Get the trial version on Steam also to try out the language, libraries, editors, pipelines, etc. System requirements Minimum OS Windows 7 or later (32/64bit), MacOS 10.7 or later (64bit), Ubuntu 14 or later (32/64bit) Processor 1.5GHz Memory 512 MB RAM Graphics Intel HD Storage 150 MB available space © 2020 - 2023 Tony WangChangelog",
    "commentLink": "https://news.ycombinator.com/item?id=37497956",
    "commentBody": "Bitty Engine: An itty bitty game engineHacker NewspastloginBitty Engine: An itty bitty game engine (paladin-t.github.io) 188 points by user8501 19 hours ago| hidepastfavorite45 comments otachack 18 hours agoI love these projects so much. It must be exhilarating to work on them and I&#x27;m very tempted to work on one myself as a side project&#x2F;self learning experience. reply Waterluvian 17 hours agoparentThere&#x27;s hundreds or thousands of these indie game engines because, yes!, it is a lot of fun to write them. You&#x27;d learn a lot. The #1 tip though is that you&#x27;re writing a game, or an engine, not both. So commit to doing one of those tasks. Usually you end up with a bunch of toy games and no steam left to want to really polish them into something complete. reply JKCalhoun 14 hours agorootparent> The #1 tip though is that you&#x27;re writing a game, or an engine, not both.Not sure I agree. I have always done both.Ostensibly I am writing a game but not using someone&#x27;s engine. And then my own \"engine\" comes out of the effort.Then I write a second game and pull over the rendering-sound-etc-code from the first game. This \"engine\" gets further refined as a result of being pressed into service for the new game.Often it improves with each iteration and I move the newer version back to older games and refactor them as required. reply Buttons840 15 hours agorootparentprev> The #1 tip though is that you&#x27;re writing a game, or an engine, not both.Most of the people who follow this advice will never ship anything. Stop worrying and do what you want. It&#x27;s okay to reinvent the wheel if you want to learn how to make a wheel, or if you just find it fun. reply jerrre 4 hours agorootparentWhile I agree with your sentiment: most people who DON’T follow the advice will propably not ship anything either as shipping is hard and rare in any case. reply hutzlibu 13 hours agorootparentprevOr if you need a special wheel adopted to your needs. Because you were annoyed, that the common generic wheels blasted the way you drove them ... but also could not use a expensive high performance wheel, because they come in different sizes.(But remember, if you want to ship fast, or at all, usually better stay with the common wheels and sizes) reply phero_cnstrcts 17 hours agorootparentprevDo you know of any that use JavaScript&#x2F;typescript? reply Waterluvian 16 hours agorootparentHere&#x27;s the more popular dozen or so: https:&#x2F;&#x2F;github.com&#x2F;collections&#x2F;javascript-game-enginesIf you want to know about the really small indie ones that are less for actual use and more for just \"I want to make a game engine\" I would check out the various gamedev Discord and Reddit channels. There&#x27;s always people showing off their work. reply josemwarrior 12 hours agorootparentprevPhaser, as game engine, Pixi.js as a powerful render library (FRVR.com games were made formerly with Pixi.js). You can use Cocos Creator (not Cocos2D-x) reply gmerc 12 hours agorootparentprev2d phaser (photonstorm) 3d babylonjs (microsoft) reply impatient_bacon 16 hours agorootparentprevhttps:&#x2F;&#x2F;bitmelo.com&#x2F; reply beepbooptheory 16 hours agorootparentprevhttps:&#x2F;&#x2F;github.com&#x2F;photonstorm&#x2F;phaser reply ClassyJacket 10 hours agorootparentprevI wish AAA game developers would listen to this advice. How many times has trying to write a new game engine for one IP ended with the game in development hell?No, 343 Industries, you don&#x27;t need your own custom engine. Just use Unreal. reply jesse__ 16 hours agoparentprevCan confirm; they&#x27;re real fun. I&#x27;ve got a 7-year side-project I still grind on in the form of a voxel engine. The trick is to start small and have realistic expectations. Your progress will be glacially slow at first, and that&#x27;s okay. Eventually, if you keep grinding, you&#x27;ll have something awesome that you love working on.If you want to check out my project there&#x27;s a link in my bio. reply gabereiser 16 hours agorootparentCan confirm the first two years will suck if you’re starting from scratch. Keep with it. By the 4th year you’ll be able to use all your abstractions to whip up prototypes over the weekend. reply PcChip 15 hours agorootparentprevthat&#x27;s really cool!I&#x27;m doing an engine as side project too that I&#x27;ve just accepted will never be finished and it&#x27;s only for fun :)&#x2F;r&#x2F;TranceEngine reply JKCalhoun 14 hours agorootparentprevI checked out your side-project.> [ ] Do sound :&#x27;DWhy bother with sound? Likely a dev would just pull in SDL_Mixer for game sound. Get on with the transparency. ;-) reply jesse__ 12 hours agorootparentMostly for the learning experience of writing an MP3 decoder and WAV mixer reply _gabe_ 18 hours agoprevThat Lua debugger&#x2F;editor with ImGui looks awesome! I’ve been wanting to do a similar project since Lua makes the API for creating breakpoints and stuff pretty easy. Styling ImGui to look like a normal code editor must have been no easy task though. reply gsuuon 15 hours agoparentWonder if they&#x27;ve got support for Debug Adapter Protocol? https:&#x2F;&#x2F;microsoft.github.io&#x2F;debug-adapter-protocol&#x2F; reply hinkley 12 hours agoprevI have been contemplating lately if we need something like either the early days of computer games or the heyday of the Demo Scene, where people make amusements that are meant to run on embedded computers.I&#x27;m not quite sure what that should look like, but in particular making physical puzzles driven by say an STM32 chip could be cool. Especially if you can have it change the rules of the game once you&#x27;ve solved one puzzle.Anyway, teeny tiny game engines brings that whole line of thinking back up for me. reply MenhirMike 11 hours agoparent> where people make amusements that are meant to run on embedded computers.I&#x27;m glad that PICO-8 exists, which is kinda exactly that. Except that it&#x27;s not targeting any actual hardware (it&#x27;s an imaginary retro-system), but it ticks all the other boxes.Would be interesting to see what you can do in a physical space - your STM32 example still needs some kind of output attached to it, but with a 3D Printer and some other cheap components, you can add whatever I&#x2F;O you want. reply hinkley 11 hours agorootparentWhen I was a child, there was a hot second where someone implemented PacMan on an LCD screen.And I really do mean a hot second, because one boy showed up with one, another couple had one within days, and by the end of the week the school had banned them. As, I&#x27;m sure, did most every other school in the country. Now I can play audiobooks on my watch, but no games to speak of. reply 3seashells 16 hours agoprevThe world learns lua one game at a time. All it needs is decent standardization comitee and a better packages&#x2F;module system. Lua rocks reply brucethemoose2 5 hours agoprev> around 10MB with an empty projectI wonder if one could fit the active parts of a entire game inside an X3D CPU&#x27;s 96MB of L3.Think of the performance! reply dale_glass 3 hours agoprevWhy those limits? 256 colors? 1024x1024?That seems more of an intentionally self-imposed limit that something that happens naturally in any way. reply pringk02 2 hours agoparentPopular to do stuff like that for \"fantasy consoles\" inspired by Pico8 reply dale_glass 1 hour agorootparentYeah, but it&#x27;s just weird to me. Old architectures had logical limits. Most were because of things like memory addressing. Some were because you were writing C without any fancy modern helper libraries, and so if you wanted a hashtable you had to roll it yourself. A fixed size array for some things was just pragmatic. Graphics were 256 color because that was what the hardware supported.This thing though is in C++ which does all the dynamic allocation you want. Nobody uses 256 color video modes anymore. The limits are all completely artificial.They&#x27;re not even particularly good limits because there was console hardware that had say, 4096 colors, or per-sprite palettes which doesn&#x27;t quite match PC hardware in capabilities. reply glonq 13 hours agoprevI played around with Construct2 a lot once upon a time, but never followed them to 3 because the product got a bit too expensive for me has a hobbyist.How does this compare to that, besides the obviously nicer cost? reply tfrutuoso 16 hours agoprevI was looking for this to surprise the missus with her own JRPG for her birthday. Thanks! reply josephcsible 10 hours agoprevI&#x27;d really like this if it were actually entirely FOSS, and even more so if it didn&#x27;t support \"code and asset obfuscating\". reply 40four 14 hours agoprevSilly question from a web dev who has never written a line of Lua before (or delved into game development). What is it about the language that make it such a good fit for game development? Seems like every time I come across these engines, or read stuff about game development in general, Lua is always the language of choice. Can anyone explain it in a nutshell for me? Thanks reply fullstop 14 hours agoparentAs others have mentioned, it&#x27;s got a few things going for it. 1. It&#x27;s super easy to embed in an existing code base 2. The language itself is simple and easy to learn 3. It has native coroutines, which allows game devs to write code without a spider web of callbacks 4. For what it does, it is fast reply muchwhales 4 hours agoparentprevOne point that wasn&#x27;t mentioned: It has a performance-oriented alternative implementation, called LuaJIT.With this, you can run JIT-compiled Lua code at speeds comparable to V8 (the NodeJS runtime), and almost as good as the Java VM (depending on the workload, YMMV). LuaJIT also includes a foreign function interface (FFI) for even closer integration with native code, which makes it almost trivial to use native libraries from Lua.Essentially, this makes it easy to move logic to lower&#x2F;higher levels of the stack when&#x2F;if you need it: There&#x27;s the three layers that are increasingly more difficult to use but faster (Lua -> FFI -> C&#x2F;C++), and you can directly use gamedev-oriented C++ or even graphics APIs like WebGPU from Lua, without crippling performance or writing tons of glue code.Note that I&#x27;ve worked with Lua for many years and I&#x27;m definitely biased. I&#x27;ve also worked with JavaScript&#x2F;TypeScript-based engines, where my favorite is BabylonJS (it&#x27;s great, but JS&#x2F;browsers really aren&#x27;t...). So if you don&#x27;t want to learn Lua&#x2F;C++ I can recommend looking into BabylonJS as a starting point - it will probably be easier to get something going thanks to the browser APIs. reply Conscat 14 hours agoparentprevMy understanding is that it&#x27;s simply historically fallen into the niche. Lua early on was relatively easy to embed in C codebases, making it a natural fit for scripting C (or C++) game engines and their editors. So many hugely popular game development tools are scripted with Lua at this point that it&#x27;s somewhat breaking the mold to use anything that _isn&#x27;t_ Lua in this domain. reply 40four 14 hours agorootparentThanks that makes sense reply binary132 12 hours agoparentprevI think it’s because Lua’s C API is very very closely coupled to its native keywords and standard library, so it’s not only built for trivial embedding but also for trivial &#x2F; mechanical integration. Using the Lua API from within C or C++ is almost like the syntactic language isn’t even there and it’s just a convenient library.For example:https:&#x2F;&#x2F;www.lua.org&#x2F;manual&#x2F;5.4&#x2F;manual.html#lua_pushfstringand:https:&#x2F;&#x2F;www.lua.org&#x2F;manual&#x2F;5.4&#x2F;manual.html#lua_nextIt’s also amazingly semantically simple and dynamic, almost like a Lisp; even though it doesn’t have classes, typelevel programming is pretty straightforward.It also allows you to easily write and distribute native modules (as shared objects) without needing integration code.It is also insanely portable. reply setr 14 hours agoparentprevI think it’s less that it’s particularly good for gamedev and more that it’s particularly easy to embed in an engine, and it’s faster than it has any right to be reply ansible 13 hours agorootparent> ... and it’s faster than it has any right to beThe main Lua interpreter by PUC-Rio is among the fastest bytecode interpreters for a popular scripting language. Very efficient C code.Wizard programmer Mike Pall then came along and wrote an even faster Lua bytecode interpreter in assembly language. And added JIT for even faster performance of hot functions. reply 40four 14 hours agorootparentprevAppreciate it! reply aewens 14 hours agoparentprevProbably since you can embed it in other languages quite trivially. For instance, in C after about a dozen lines of code you can now pass around data into Lua and back to C and thus give you access to a scripting language with little fuss. It’s also a fairly small and simple language, so adding it in won’t add much more to the overall footprint of the project. reply 40four 14 hours agorootparentInteresting, thanks! reply jokoon 14 hours agoprev [–] I know I&#x27;m going to bring down the mood, but that kind of engine should support android as a priority.Godot does it, and it&#x27;s an awesome alternative to Android Studio.EDIT: although it supports WASM, and I&#x27;m curious how well does WASM works on mobile... reply hutzlibu 13 hours agoparentWell, the dev thinks different and this is what matters.And WASM by itself works quite well by now, even on older devices, but not really on stock browsers. The question is more, how will be the whole experience of an autocompiled wasm project with UI? This is hard to get right and debug, if not right. reply jwells89 9 hours agoparentprev [–] IIRC the thing about supporting Android is that with game engines, that typically means working with the NDK which as far as I can tell is a bit of a bear to deal with relative to desktop platforms. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bitty Engine is a cross-platform game engine with built-in editors, allowing user-friendly programming in Lua, accompanied by features like a debugger, API documentation, and various editing tools.",
      "Capable of creating standalone binaries, the engine can be utilized for both non-commercial and commercial purposes. It's available for platforms like Windows 7, MacOS 10.7, or Ubuntu 14, with low system requirements.",
      "The Bitty Engine, distributed on Steam and Itch, is identical in terms of features on both platforms, and a trial version is available on Steam."
    ],
    "commentSummary": [
      "The article highlights the excitement around the recent launch of the Bitty Engine, with discussion centered on game and engine development tips and the application of JavaScript/TypeScript in game engines.",
      "The potential of physical games with embedded computers and the rising popularity of Lua in game development are also discussed.",
      "Lua's key benefits, such as easy incorporation into code bases, simplicity of learning, compatibility with C and C++ engines, along with the high-performance LuaJIT alternative, are underlined. \"Fantasy consoles\" is also a topic of conversation."
    ],
    "points": 188,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1694618401
  },
  {
    "id": 37500752,
    "title": "Earth beyond six of nine planetary boundaries",
    "originLink": "https://www.science.org/doi/10.1126/sciadv.adh2458",
    "originBody": "ADVERTISEMENT NEWS CAREERS COMMENTARY JOURNALS COVID-19 LOG IN BECOME A MEMBER Current Issue First release papers Archive About Submit manuscript GET OUR E-ALERTS HOME SCIENCE ADVANCES VOL. 9, NO. 37 EARTH BEYOND SIX OF NINE PLANETARY BOUNDARIES OPEN ACCESS RESEARCH ARTICLE ENVIRONMENTAL STUDIES Share on Earth beyond six of nine planetary boundaries KATHERINE RICHARDSON HTTPS://ORCID.ORG/0000-0003-3785-2787 , WILL STEFFEN, WOLFGANG LUCHT, JØRGEN BENDTSEN HTTPS://ORCID.ORG/0000-0003-1393-3072, SARAH E. CORNELL HTTPS://ORCID.ORG/0000-0003-4367-1296, JONATHAN F. DONGES HTTPS://ORCID.ORG/0000-0001-5233-7703, MARKUS DRÜKE HTTPS://ORCID.ORG/0000-0002-8004-7153, INGO FETZER HTTPS://ORCID.ORG/0000-0001-7335-5679, GOVINDASAMY BALA HTTPS://ORCID.ORG/0000-0002-3079-0600, [...], AND JOHAN ROCKSTRÖM HTTPS://ORCID.ORG/0000-0001-8988-2983+19 authorsAuthors Info & Affiliations SCIENCE ADVANCES 13 Sep 2023 Vol 9, Issue 37 DOI: 10.1126/sciadv.adh2458 Abstract INTRODUCTION RESULTS DISCUSSION MATERIALS AND METHODS Acknowledgments Supplementary Materials REFERENCES AND NOTES eLetters (0) Information & Authors Metrics & Citations View Options References Media Tables Share Abstract This planetary boundaries framework update finds that six of the nine boundaries are transgressed, suggesting that Earth is now well outside of the safe operating space for humanity. Ocean acidification is close to being breached, while aerosol loading regionally exceeds the boundary. Stratospheric ozone levels have slightly recovered. The transgression level has increased for all boundaries earlier identified as overstepped. As primary production drives Earth system biosphere functions, human appropriation of net primary production is proposed as a control variable for functional biosphere integrity. This boundary is also transgressed. Earth system modeling of different levels of the transgression of the climate and land system change boundaries illustrates that these anthropogenic impacts on Earth system must be considered in a systemic context. SIGN UP FOR THE SCIENCEADVISER NEWSLETTER The latest news, commentary, and research, free to your inbox daily SIGN UP INTRODUCTION The planetary boundaries framework (1, 2) draws upon Earth system science (3). It identifies nine processes that are critical for maintaining the stability and resilience of Earth system as a whole. All are presently heavily perturbed by human activities. The framework aims to delineate and quantify levels of anthropogenic perturbation that, if respected, would allow Earth to remain in a “Holocene-like” interglacial state. In such a state, global environmental functions and life-support systems remain similar to those experienced over the past ~10,000 years rather than changing into a state without analog in human history. This Holocene period, which began with the end of the last ice age and during which agriculture and modern civilizations evolved, was characterized by relatively stable and warm planetary conditions. Human activities have now brought Earth outside of the Holocene’s window of environmental variability, giving rise to the proposed Anthropocene epoch (4, 5). Planetary-scale environmental forcing by humans continues and individual Earth system components are, to an increasing extent, in disequilibrium in relation to the changing conditions. As a consequence, the post-Holocene Earth is still evolving, and ultimate global environmental conditions remain uncertain. Paleoclimate research, however, documents that Earth has previously experienced largely ice-free conditions during warm periods (6, 7) with correspondingly different states of the biosphere. It is clearly in humanity’s interest to avoid perturbing Earth system to a degree that risks changing global environmental conditions so markedly. Ice cover is only one indicator of substantial system-wide change in numerous other Earth system dimensions. The planetary boundaries framework delineates the biophysical and biochemical systems and processes known to regulate the state of the planet within ranges that are historically known and scientifically likely to maintain Earth system stability and life-support systems conducive to the human welfare and societal development experienced during the Holocene. Currently, anthropogenic perturbations of the global environment are primarily addressed as if they were separate issues, e.g., climate change, biodiversity loss, or pollution. This approach, however, ignores these perturbations’ nonlinear interactions and resulting aggregate effects on the overall state of Earth system. Planetary boundaries bring a scientific understanding of anthropogenic global environmental impacts into a framework that calls for considering the state of Earth system as a whole. For >3 billion years, interactions between the geosphere (energy flow and nonliving materials in Earth and atmosphere) and biosphere (all living organisms/ecosystems) have controlled global environmental conditions. Earth system’s state changed in response to forcings generated by external perturbations (e.g., solar energy input and bolide strikes) or internal processes in the geosphere (e.g., plate tectonics and volcanism) or biosphere (e.g., evolution of photosynthesis and rise of vascular plants). These forcings were processed through interactions and feedbacks among processes and systems within Earth system, shaping its often complex overall response. Today, human activities with planetary-scale effects act as additional forcing on Earth system. Thus, the anthroposphere has become an additional functional component of Earth system (3, 8), capable of altering Earth system state. The planetary boundaries framework formulates limits to the impact of the anthroposphere on Earth system by identifying a scientifically based safe operating space for humanity that can safeguard both Earth’s interglacial state and its resilience. The Holocene state of Earth is the benchmark reference in this context, as many of the components comprising the planetary boundary framework were rather stable during this period. This is also the only Earth system state civilizations have historically known. Climate is a manifestation of external forcing, e.g., solar activity, orbital cycles, and interactions among Earth system components, and global mean surface temperature varied by only ±0.5°C (9) from the Neolithic [~9000 before the present (B.P.)] until the Industrial Revolution. Biomes across Earth have also largely been stable over the past 10,000 years, with preindustrial global terrestrial net primary production (NPP) varying by not >55.9 ± 1.1 billion tonnes (Gt) of C year−1 (2σ) (see the Supplementary Materials). Bias-corrected data (10) confirm that preindustrial global precipitation levels were also stable, particularly from the mid-Holocene onward. These data provide strong support for using the Holocene (see the Supplementary Materials) as the planetary boundaries reference state for a stable and resilient planet. All of the framework’s individual boundaries therefore adopt preindustrial Holocene conditions as a reference for assessing the magnitude of anthropogenic deviations. Available data and state of knowledge from analytics and modeling of the framework components dictate the methods for derivation and quantification of the individual boundaries and their precautionary guardrails. Despite data constraints, efforts have been made to identify suitable control variables for all boundaries, together with evidence of how much perturbation leads to generation of impacts or altered interactions/feedbacks that can potentially cause irreversible changes to Earth’s life support systems. The focus is always at Earth system rather than regional scale, even when the evidence used to establish boundaries originates from regional studies. In these cases, regional evidence is combined to assess Earth system impacts of cumulative transgressions across multiple regional systems. The planetary boundaries framework has attracted considerable scientific and societal attention, inspiring governance strategies and policies at all levels. The framework evolves through updates made in light of recent scientific understanding. Here, we bring together advances from different fields of science to update the framework and the status of its boundaries. Boundaries are, for the first time, proposed for all of the individual components of the framework. Updates of the functional biosphere integrity and aerosol loading boundaries are based on analyses presented here. Recent analyses form the basis for updates of the freshwater change and novel entities boundaries. Last, the importance of considering human impacts on components of the global environment in a system context is illustrated using a modeling exercise exploring how various scenarios of transgression of the land system (representing the biosphere) and climate change boundaries combine to affect Earth system characteristics. Framework components Understanding how biosphere, anthroposphere, and geosphere processes interact with one another is a prerequisite for developing reliable projections of possible future Earth system trajectories. A fully process-based understanding of the interactions between these domains is, however, still only partially available. The planetary boundaries framework calls for more deeply integrated modeling of Earth system by bringing together currently available evidence for the relevant processes and their interactions from different disciplines and sources. The nine boundaries all represent components of Earth system critically affected by anthropogenic activities and relevant to Earth’s overall state. For each of the boundaries, control variables are chosen to capture the most important anthropogenic influence at the planetary level of the boundary in focus. For example, land system change arises from myriad human activities, ultimately aggregating to alteration of biomes. From a planetary perspective however, during the Holocene, forests were the land biome with the strongest functional coupling to the climate system (11, 12). Therefore, global reduction in forest area is adopted as the control variable representing all land system change. Similarly, the control variable introduced here for the functional component of the biosphere integrity boundary, human appropriation of NPP (HANPP), focuses on the ability of the biosphere as a whole to provide functional feedbacks in Earth system. Control variables should ideally lend themselves to empirical determination and be computable for use in Earth system projections (e.g., process-based simulation of future change in forest cover) where possible. Boundary positions do not demarcate or predict singular threshold shifts in Earth system state. They are placed at a level where the available evidence suggests that further perturbation of the individual process could potentially lead to systemic planetary change by altering and fundamentally reshaping the dynamics and spatiotemporal patterns of geosphere-biosphere interactions and their feedbacks (13, 14). Zone of increasing risk (of Earth system losing Holocene-like characteristics) is now used to assess the status for transgressed boundaries rather than the “zone of uncertainty” (2) as demarcation of this zone is based on more than what is usually referred to as scientific uncertainty. A large body of recent research [e.g., (15–17)] provides strong evidence supporting the conclusion (2) that the climate change and biosphere integrity boundaries are in a zone of rapidly increasing and systemically linked risks. This strengthens the rationale for using the precautionary principle to set the planetary boundaries at the lower end of the zone of increasing risk. For example, for the climate change planetary boundary, we retain the boundary of 350 parts per million (ppm) CO2 with the zone of increasing risk ranging from 350 to 450 ppm before reaching high risk. This corresponds approximately to a range of global mean surface temperature rise of 1° to 2°C (assuming mainstream scenarios on non-CO2 forcing). Precaution places the planetary boundary at the start of increasing risk (350 ppm ≈ 1°C), i.e., slightly below the 1.5°C target identified in the Paris Agreement. The 1.5°C target is one that science increasingly demonstrates is associated with substantial risk of triggering irreversible large change and that crossing tipping points cannot be excluded even at lower temperature increases (18). In recognition of the buffering resilience of Earth system, most boundaries are nevertheless set at values higher than their observed range through the Holocene up to the Industrial Revolution (for CO2 ≈ 280 ppm) (see the Supplementary Materials). The stability and characteristic range of variability of interglacial Earth system states in Pleistocene paleoclimate (19) and Earth system modeling (20) suggest that Earth system would likely remain in a stable, Holocene-like state if all boundaries were respected despite their being at least temporarily outside the envelope of Holocene variability. The distinction between zones of “increasing” and “high” risk cannot be sharply defined. There is accumulating evidence that the current level of boundary transgression has already taken Earth system beyond a “safe” zone. However, we still lack a comprehensive, integrated theory, backed by observations and modeling studies, that can identify when a transition from a rising level of risk to one with very high and dangerous risks of losing a Holocene-like Earth system state may occur. Therefore, the “burning embers” approach introduced by the Intergovernmental Panel on Climate Change (IPCC) to represent the gradual transitions from moderate (yellow) to high (red) to very high (purple) risks is adopted here. Fig. 1. Current status of control variables for all nine planetary boundaries. Six of the nine boundaries are transgressed. In addition, ocean acidification is approaching its planetary boundary. The green zone is the safe operating space (below the boundary). Yellow to red represents the zone of increasing risk. Purple indicates the high-risk zone where interglacial Earth system conditions are transgressed with high confidence. Values for control variables are normalized so that the origin represents mean Holocene conditions and the planetary boundary (lower end of zone of increasing risk, dotted circle) lies at the same radius for all boundaries (except for the wedges representing green and blue water, see main text). Wedge lengths are scaled logarithmically. The upper edges of the wedges for the novel entities and the genetic diversity component of the biosphere integrity boundaries are blurred either because the upper end of the zone of increasing risk has not yet been quantitatively defined (novel entities) or because the current value is known only with great uncertainty (loss of genetic diversity). Both, however, are well outside of the safe operating space. Transgression of these boundaries reflects unprecedented human disruption of Earth system but is associated with large scientific uncertainties. OPEN IN VIEWER Throughout Earth’s history, geosphere-biosphere interactions were an internal driver of Earth system state. The climate change planetary boundary is used here as a proxy for the geosphere. Therefore, climate change and biosphere integrity are identified as “core boundaries” (2) in the framework. The introduction of novel entities is a new anthropogenic driver of Earth system change that, if sufficiently transgressed, could, on its own, alter Earth system state. However, this planetary boundary acts largely through perturbation of the core boundaries, especially biosphere integrity. In contrast to the definition applied earlier (2) where “naturally occurring elements mobilized by anthropogenic activities” were included, the definition of novel entities is now restricted to include only entities that, in the absence of the anthroposphere, are not present in Earth system. Quantifying interactions between boundaries remains a major challenge. However, some progress has been made since the last framework update (2). Recent studies (13, 14, 21, 22) have shown that additional or more extensive transgression of one planetary boundary can change risk gradients for other boundaries. For example, there is increasing evidence to suggest that transgressing either the climate change or biosphere integrity planetary boundary can potentially lead to more steeply increasing risk in the other (21). In the current absence of a comprehensive Earth system model that fully captures interactions between all component spheres, we explore below how various scenarios of transgression of the land system (representing the biosphere) and climate change boundaries combine to control biologically mediated carbon storage at the planetary level. RESULTS Biosphere integrity Myriad interactions with the geosphere make the biosphere a constitutional component of Earth system and a major factor in regulating its state. The planetary functioning of the biosphere ultimately rests on its genetic diversity, inherited from natural selection not only during its dynamic history of coevolution with the geosphere but also on its functional role in regulating the state of Earth system. Genetic diversity and planetary function, each measured through suitable proxies, are therefore the two dimensions that form the basis of a planetary boundary for biosphere integrity. As applied here, “integrity” does not imply an absence of biosphere change but, rather, change that preserves the overall dynamic and adaptive character of the biosphere. Rockström et al. (1) defined the planetary boundary for change in genetic diversity as the maximum extinction rate compatible with preserving the genetic basis of the biosphere’s ecological complexity. We retain the boundary level of 100 E/MSY (24–26). Of an estimated 8 million plant and animal species, around 1 million are threatened with extinction (16), and over 10% of genetic diversity of plants and animals may have been lost over the past 150 years (23). Thus, the genetic component of the biosphere integrity boundary is markedly exceeded (Fig. 1 and Table 1). Previously, Steffen et al. (2) proposed using the Biodiversity Intactness Index (BII) (27), an empirically based metric of human impacts on population abundances, as an interim proxy for functional biosphere integrity. It was noted, however, that the link of BII to Earth system functions remains poorly understood and BII cannot be directly linked to the planetary biogeochemical and energy flows relevant for establishing Earth system state. In addition, BII relies on expert elicitation to estimate temporal changes in species abundances/distributions, and this knowledge is not readily available for many regions, including the oceans. Martin et al. (28) have also recently suggested that BII only partially reflects human impacts on Earth system. We therefore now replace this metric with a computable proxy for photosynthetic energy and materials flow into the biosphere (29), i.e., net primary production (NPP), and define the functional component of the biosphere integrity boundary as a limit to the human appropriation of the biosphere's NPP (HANPP) as a fraction of its Holocene NPP. NPP is fundamental for both ecosystems and human societies as it supports their maintenance, reproduction, differentiation, networking, and growth. Biomes depend on the energy flow associated with NPP to maintain their planetary ecological functions as integral parts of Earth system. NPP-based energy flows into human societies should therefore not substantially compromise the energy flow to the biosphere (30). The proxy complements the diversity-based dimensions of biosphere integrity, covered by the genetic component, which captures the importance of variability in living organisms for the functioning of ecosystems. The suitability of NPP and HANPP for defining a planetary boundary has previously been discussed by Running (31) and Haberl et al. (32). We determine the terrestrial biosphere’s Holocene NPP to have been 55.9 Gt of C year−1 (2σ) and exceedingly stable, varying by not more than ±1.1 Gt of C year−1 despite regional variations in time (see the Supplementary Materials). Our model analyses suggest that NPP still had a Holocene-like level in 1700 (56.2 Gt of C year−1 for potential natural vegetation and 54.7 Gt of C year−1 when land use is taken into account). By 2020, potential natural NPP would have risen to 71.4 Gt of C year−1 because of carbon fertilization, a disequilibrium response of terrestrial plant physiology to anthropogenically increasing CO2 concentration in the atmosphere, whereas actual NPP was 65.8 Gt of C year−1 due to the NPP-reducing effects of global land-use (see the Supplementary Materials). HANPP designates both the harvesting and the elimination or alteration (mostly reduction) of potential natural NPP (32), mainly through agriculture, silviculture, and grazing. Terrestrial HANPP can be estimated both as a fraction of potential natural NPP [15.7% in 1950 and 23.5% in 2020; inferred from (33) and the Supplementary Materials] and of Holocene mean NPP (30% or 16.8 Gt of C year−1 in 2020; see the Supplementary Materials). We argue that an NPP-based planetary boundary limiting HANPP should be set in relation to preindustrial Holocene mean NPP and not the current potential natural NPP. This is because the global increase in NPP due to anthropogenic carbon fertilization constitutes a resilience response of Earth system that dampens the magnitude of anthropogenic warming. Hence, the NPP contribution to a carbon sink associated with CO2 fertilization should be protected and sustained rather than considered as being available for harvesting. Examples of large land areas under human use with declining carbon sinks, some even turning into carbon sources, i.e., due to human overexploitation of biomass, are already being observed, for example, in some Amazonian regions (34) and northern European forests. As NPP is the basis for the energy and materials flow that underpins the biosphere’s functioning (30), we argue that today’s planetary-scale impact of HANPP is reflected in the observation that major indicators of the state of the biosphere show large and worrisome declines in recent decades (16). This suggests that current HANPP is well beyond a precautionary planetary boundary aiming to safeguard the functional integrity of the biosphere and likely already into the high-risk zone. We therefore provisionally set the functional component of the biosphere integrity planetary boundary at human appropriation of 10% of preindustrial Holocene mean NPP, shifting into the zone of high risk at 20%. The boundary thus defined was transgressed in the late 19th century, a time of considerable acceleration in land use globally (35) with strong impacts on species (27), already leading to early concerns about the effects of this large-scale land transformation. Thus, while the climate warming problem became evident in the 1980s, problems arising in functional biosphere integrity due to human land use began a century earlier. Since the 1960s, growth in global population and consumption further accelerated land use, driving the system further into the zone of increasing risk. HANPP has always sustained humanity’s need for food, fiber, and fodder, and this will continue to be the case in the future, as well as for sustainable societies. The NPP required to support future societies must, however, increasingly be generated through additional production of NPP above the Holocene baseline, not including the NPP generated for biology-based carbon sinks. Feeding 10 billion people, for example, is theoretically possible within planetary boundaries but requires a number of far-reaching transformations to improve the impacts of production and regulate demand (36). To develop a deeper foundation for the HANPP-based planetary boundary for functional biosphere integrity, we need an improved understanding of how ecological dynamics generate the functions of the biosphere in Earth system. Analysis of NPP should be spatially explicit and augmented by computable metrics of ecological destabilization due to climate and land use pressures, e.g., a metric of biogeochemical disruption (37). HANPP can also be quantified for marine systems. About two-thirds of the ocean area where HANPP is >10% is found above the shallow shelf areas (38) where ecosystems are most intensely exploited. Regionally, fish catches exceed thresholds of sustainable exploitation (39). However, in contrast to land, where most HANPP occurs in the form of plant material, i.e., at the lowest trophic level, HANPP in the ocean tends to take place at higher trophic levels. This means that while HANPP reduces the absolute amount of energy available to higher trophic levels on land, much of the energy fixed through NPP is used in marine ecosystems before HANPP occurs. When the abundance of organisms at the highest trophic levels is reduced, changes in marine ecosystem structure may change energy flow in these ecosystems (40). Thus, in the marine realm, HANPP likely changes the flows rather than the amount of energy available. More information about the impacts of HANPP in the marine realm is necessary to integrate consideration of the marine systems in the functional biosphere integrity planetary boundary. Climate change Climate change control variables and boundary levels are retained (1, 2). The most important drivers of anthropogenic impacts on Earth’s energy budget are the emission of greenhouse gases and aerosols, and surface albedo changes (17). The control variables in the framework are the annual averages of atmospheric CO2 concentration and the change in radiative forcing. The planetary boundary for atmospheric CO2 concentration is set at 350 ppm and for radiative forcing at 1 W m−2. Currently, the estimated total anthropogenic effective radiative forcing is 2.91 W m−2 [2022 estimate, relative to 1750 (17)], and atmospheric CO2 concentration is 417 ppm [annual mean marine surface value for 2022 (41)], i.e., further outside the safe operating space on both measures than in the last update (2). The 350-ppm boundary would lead to a lower level of anthropogenic global warming than the internationally agreed 1.5°C target in the United Nations Paris Climate Agreement but is consistent with recent studies (17, 18, 42) suggesting the possibility of extreme Earth system impacts even at 1.5o warming, with risks increasing already markedly above 1° warming. Novel entities The definition of this boundary is now restricted to truly novel anthropogenic introductions to Earth system. These include synthetic chemicals and substances (e.g., microplastics, endocrine disruptors, and organic pollutants); anthropogenically mobilized radioactive materials, including nuclear waste and nuclear weapons; and human modification of evolution, genetically modified organisms and other direct human interventions in evolutionary processes. Novel entities serve as geological markers of the Anthropocene (5). However, their impacts on Earth system as a whole remain largely unstudied. The planetary boundaries framework is only concerned with the stability and resilience of Earth system, i.e., not human or ecosystem health. Thus, it remains a scientific challenge to assess how much loading of novel entities Earth system tolerates before irreversibly shifting into a potentially less habitable state. Hundreds of thousands of synthetic chemicals are now produced and released to the environment. For many substances, the potentially large and persistent effects on Earth system processes of their introduction, particularly on functional biosphere integrity, are not well known, and their use is not well regulated. Humanity has repeatedly been surprised by unintended consequences of this release, e.g., with respect to the release of insecticides such as DDT and the effect of chlorofluorocarbons (CFCs) on the ozone layer. For this class of novel entities, then, the only truly safe operating space that can ensure maintained Holocene-like conditions is one where these entities are absent unless their potential impacts with respect to Earth system have been thoroughly evaluated. This would imply that the quantified planetary boundary should be set at zero release of synthetic chemical compounds to the open environment unless they have been certified as harmless and are monitored. That is the target set by the Montreal Protocol with respect to the substances shown to be harmful by contributing to depletion of the ozone layer. In their analysis of various strategies for establishing a planetary boundary for novel entities, Persson et al. (43) identified the share of released chemicals with adequate safety assessment and monitoring as a candidate control variable. We here adopt this metric. The planetary boundary is then set at the release into Earth system of 0% of untested synthetics. When synthetics released to the environment are thoroughly tested, the ensuing risk of damaging effects is lowered. Admittedly, this approach has weaknesses: Data availability is incomplete; safety studies often focus on narrowly defined toxicity and do not capture the “cocktail effects” of chemical mixtures in the environment nor their effects under specific conditions. The percentage of untested synthetics released globally is unknown. However, Persson et al. (43) report that for the chemicals currently registered under the EU Registration, Evaluation, Authorisation and Restriction of Chemicals (REACH) regulation (a small subset of the chemical universe), ~80% of these chemicals had been in use for at least 10 years without yet having undergone a safety assessment. Likewise, few safety studies consider potential Earth system effects. With such an enormous percentage of untested chemicals being released to the environment, a novel entities boundary defined in this manner is clearly breached. Persson et al. (43) did not identify or quantify a singular planetary boundary for novel entities but, nevertheless, also concluded that the safe operating space is currently overstepped. Stratospheric ozone depletion Stratospheric ozone depletion is a special case related to the anthropogenic release of novel entities where gaseous halocarbon compounds from industry and other human activities released into the atmosphere lead to long-lasting depletion of Earth’s ozone layer. The boundary for the safe operating space is set at 276 Dobson units (DU), i.e., allowing a 100 E/MSY (24–26) Functional integrity: measured as energy available to ecosystems (NPP) (% HANPP) HANPP (in billion tonnes of C year−1) 90% remaining for supporting biosphere function 1.9% (2σ variability of preindustrial Holocene century-mean NPP) 20% HANPP 30% HANPP (see the Supplementary Materials) Stratospheric ozone depletion Stratospheric O3 concentration, (global average) (DU) 1.3°C compared to the preindustrial period). Only a small (cumulative 25 Gt of C) terrestrial carbon source would have developed by 2100 and a cumulative source of not >68 Gt of C after 800 years. Thus, the exercise suggests that essentially stable planetary conditions would have been maintained had human impacts on these two boundaries remained at their 1988 levels, i.e., marginally within the safe operating space. Both of these planetary boundaries have, however, since been transgressed into a zone of increasing risk of systemic disruption. If climate and land system change can be halted at 450 ppm and forest cover retained at 60%/30%/60% of boreal/temperate/tropical natural cover, then the simulation indicates a mean temperature rise over land of 1.4°C by 2100 (in addition to 0.7°C between preindustrial time and 1988) and 1.9°C after 800 years as vegetation evolves in a warmer climate and associated carbon fertilization (Fig. 2). Fig. 2. Impact of the combined effect of land system change and climate change boundary states on trajectories of terrestrial carbon stocks and global land temperature. Results are based on idealized Earth system model experiments with varying planetary boundary status, ranging from maintaining the planetary boundary (85%/50%/85% boreal/temperate/tropical forest remaining, 350-ppm atmospheric CO2, green), the upper end of the zone of increasing risk (60%/30%/60%, 450 ppm, orange), and beyond the zone of increasing risk (40%/20%/40%, 550 ppm, red). Open circles represent the short-term changes (1988–2100) of the system, while colored circles the long-term changes (2100–2770). Their colors denote the state of the land system change boundary, while the climate change boundary is shown on the y axis. The locations of the circles on the x axis represent the changes in the land carbon stocks, and the associated land temperature changes are given next to each circle, both compared to the year 1988. Transgressing the climate change boundary (y axis) is mostly connected to an increase in temperature, while the transgression of land system change leads to a loss of terrestrial carbon stocks (source) of 100 to 200 Gt of C. Expand for more OPEN IN VIEWER Carbon fertilization of vegetation growth counters the negative impacts of climate warming on the global average carbon sinks, leading to only moderate cumulative loss in terrestrial carbon due to additional deforestation. If, however, deforestation had been maintained at the level of the planetary boundary rather than having been allowed to rise in the zone of increasing risk, then the land biosphere would have developed a cumulative carbon sink rather than a source, contributing to stabilizing Earth’s conditions. In contrast, if deforestation is allowed to breach into the high-risk zone, then simulations show a substantial additional carbon leakage to the atmosphere both over the short and long term (132 and 211 Pg of C), despite strong CO2 fertilization of vegetation growth in the model (Fig. 2). The situation is even more extreme if atmospheric CO2 concentration rises above the risk zone (550 ppm; Fig. 2) and deforestation continues. Not only is the temperature on land about 2.7°C warmer than in 1988 (3.4°C warmer than preindustrial), but also around 145 Gt of C would be lost long-term from terrestrial vegetation and soils. Note that these findings reflect optimistic modeling assumptions on carbon fertilization. Many of the ecological factors not sufficiently represented in current biogeochemical models could lead to even less desirable consequences of leaving the safe operating space. These simulations illustrate clearly that human impacts on climate and forest cover must be considered in a systemic context. They furthermore support the placement of the planetary boundaries for climate and land system change at the lower end of the zone of increasing risk. Influence of climate change on biologically mediated C sinks in the ocean Approximately 450 Gt of C is bound up in terrestrial biota, primarily in plants (86), while only ~6 Gt of C is found in ocean biota (87). Biologically mediated marine carbon sinks are composed of particulate organic carbon (POC) that can potentially sink below the permanent thermocline (biological pump) and dissolved organic C. Via microbial breakdown of POC and dissolved organic C, CO2 is released. When this release influences partial pressure of CO2 in surface waters, it tends to reduce oceanic carbon uptake from the atmosphere. Microbial respiration is highly sensitive to temperature and, in a warmer ocean, an increased release of CO2 in surface waters is predicted (88). The biologically mediated carbon sink in the ocean most exposed to climate change is the amount of carbon fixed by photosynthesis (NPP), i.e., POC, in the surface ocean that is ultimately transported into the ocean interior via the biological pump. When this occurs, the resulting carbon drawdown reduces partial pressure of CO2 in the surface layer and tends to increase the atmosphere-to-ocean CO2 flux. These biological processes are implicitly and, in some cases, explicitly included in the CMIP6 models informing the IPCC. However, as these models configure biologically mediated carbon flows differently, there is considerable variability in their results. Models used by the IPCC do not even agree on the direction of change in NPP in response to climate change (89). Our model runs (see the Supplementary Materials) suggest no significant change in globally averaged ocean NPP under the different climate forcing conditions and only a modest decrease in exported material out of the surface layer [new production (ΔNP); Table 2]. Using empirical relationships (90, 91) describing the transfer of carbon to the ocean interior and derived from the contemporary ocean to estimate biological pump sensitivity to future temperature increases indicates a similar weakening of the pump in the upper ocean (Table 2 and the Supplementary Materials). That these two independent methods indicate similar decreases in the export of POC from the surface layer lends confidence both in the direction and magnitude of climate impacts on this biologically mediated global carbon sink. Scenario ΔSST ΔNPΔF500mΔΩ ΔDIC0–1000m (ppm) (°C) Model (%) Empirical (%) Model (%) Empirical (%) (−) (Gt of C) 350 0.3 2.0 2.5 1.9 1.8 0.0 38 450 1.0 0.0 1.4 0.0 −3.5 −0.4 172 550 1.7 −2.5 −1.0 −3.1 −9.4 −0.7 273 Table 2. Global averaged change in three scenarios from the initial state (1988–2018): change in sea surface temperature (ΔSST), new production (ΔNP), and biogenic particulate flux below 500 m depth (ΔF500m) including model and empirically derived values, surface saturation state of aragonite (ΔΩ), and the DIC inventory between the surface and 1000 m depth (ΔDIC0–1000m). OPEN IN VIEWER The analysis shows that DIC (dissolved inorganic carbon; including CO2) accumulates over time in the ocean as a whole, particularly in the upper ocean (20% (59). Together, these studies suggest that a raised interhemispheric AOD difference caused by persistent and widely distributed aerosol emissions could lead to major reductions in precipitation in the tropics. To examine differing scenarios of transgression of land system and climate change boundaries, we use the POEM [(85) and the Supplementary Materials], which links models of atmospheric and ocean circulation with models of the marine (BLING) (94) and terrestrial biosphere (LPJmL5) [(95) and the Supplementary Materials]. We study scenarios where each of these two planetary boundary dimensions are either fixed at the value of the boundary, a value in the zone of increasing risk, or a value in the high-risk zone. Once the respective scenario condition is attained, the associated level of scenario forcing remains constant, while the long-term implications under these fixed conditions evolve. Correspondingly, vegetation dynamics (e.g., biome distributions) and related carbon pools and fluxes develop according to biophysical climate interactions under the given forcing conditions, while biogeochemical feedbacks on the atmosphere are not considered because of the respective boundary or transgression forcing remaining fixed. Acknowledgments This paper is dedicated to our friend, colleague, and co-author, W.S., who passed away. He was deeply involved in developing this paper. Few have made a greater contribution to describing a pathway for humanity’s development in the Anthropocene than W.S. We are grateful for support from K. Noone (aerosols), B. Sakschewski (POEM), and M. Martin (comments). J. Lokrantz (Azote) and D. Biermann (PIK) produced the figures. Funding: This work was supported by the European Research Council (Project Earth Resilience in the Anthropocene, ERC-2016-ADG 743080); European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant no. 819202); German Federal Ministry for Education and Research (BMBF) through the “PIK Change” framework (grant no. 01LS2001A), and Carlsberg Foundation (Queen Margrethe’s and Vigdís Finnbogadóttir´s Interdisciplinary Research Centre on Ocean, Climate, and Society, CF20-0071). POEM development and application were supported by the Volkswagen Foundation (POEM-PBSim—A Simulator for Earth’s planetary boundaries, AZ 98046) and work on the biosphere functional integrity boundary by the Global Challenges Foundation. Author contributions: K.R., W.S., J.R., and W.L. led the study by conceiving and coordinating the analyses. K.R. led the writing process. J.B., S.E.C., J.F.D., M.D., and I.F. (alphabetical order) collected and collated data, synthesized literature, supported the analyses, prepared the tables and figures, and provided logistical support. The remaining authors (alphabetical order) contributed to the POEM modeling and/or to new analysis of individual boundaries: G.B. (aerosols), W.v.B. (POEM), G.F. (POEM), S.F. (aerosols), D.G. (fresh water), T.G. (fresh water), M.H. (POEM), W.H. (POEM), M.K. (fresh water), C.M. (fresh water), D.N.-B. (biosphere integrity), S.P. (POEM), M.P. (fresh water), S.R. (POEM), S.S. (POEM and functional biosphere integrity), A.T. (land system change), K.T. (POEM), V.V. (fresh water), L.W.-E. (fresh water), and L.W. (aerosols). Competing interests: The authors declare that they have no competing interests. Data and materials availability: All data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials. In addition, the POEM modelling data can be found at https://doi.org/10.5281/zenodo.8032156. Supplementary Materials This PDF file includes: Supplementary Information Figs. S1 to S9 Tables S1 to S3 References DOWNLOAD 2.77 MB REFERENCES AND NOTES 1 J. Rockström, W. Steffen, K. Noone, Å. Persson, S. Chapin, E. F. Lambin, T. M. Lenton, M. Scheffer, C. Folke, J. Schellnhuber, B. Nykvist, C. A. DeWit, T. Hughes, S. van der Leeuw, H. Rodhe, S. Sörlin, P. K. Snyder, R. Costanza, U. Svedin, M. Falkenmark, L. Karlberg, R. W. Corell, V. J. Fabry, J. Hansen, D. Liverman, K. Richardson, P. Crutzen, J. Foley, A safe operating space for humanity. Nature 461, 472–475 (2009). CROSSREF PUBMED ISI GOOGLE SCHOLAR 2 W. Steffen, K. Richardson, J. Rockström, S. E. Cornell, I. Fetzer, E. M. Bennett, R. Biggs, S. R. Carpenter, W. de Vries, C. A. de Wit, C. Folke, D. Gerten, J. Heinke, G. M. Mace, L. M. Persson, V. Ramanathan, B. Reyers, S. Sörlin, Planetary boundaries: Guiding human development on a changing planet. Science 347, 1259855 (2015). CROSSREF PUBMED ISI GOOGLE SCHOLAR 3 W. Steffen, K. Richardson, J. Rockström, H. Schellnhuber, O. P. Dube, S. Dutreil, T. M. Lenton, J. Lubchenco, The emergence and evolution of Earth system science. Nat. Rev. Earth Environ. 1, 54–63 (2020). CROSSREF GOOGLE SCHOLAR 4 J. Zalasiewicz, C. N. Waters, C. Summerhayes, A. P. Wolfe, A. D. Barnosky, A. Cearreta, P. Crutzen, E. C. Ellis, J. J. Fairchild, A. Gałuszka, P. Haff, I. Hajdas, M. J. Head, J. A. I. do Sul, C. Jeandel, R. Leinfelder, J. R. McNeill, C. Neal, E. Odada, N. Oreskes, W. Steffen, J. P. M. Syvitski, M. Wagreich, M. Williams, The working group on the ‘Anthropocene’: Summary of evidence and recommendations. Anthropocene 19, 55–60 (2017). GO TO REFERENCE CROSSREF ISI GOOGLE SCHOLAR 5 C. N. Waters, J. Zalasiewicz, C. Summerhayes, A. D. Barnosky, C. Poirier, A. Gałuszka, A. Cearreta, M. Edgeworth, E. C. Ellis, M. Ellis, C. Jeandel, R. Leinfelder, J. R. McNeill, D. D. Richter, W. Steffen, J. Syvitski, D. Vidas, M. Wagreich, M. Williams, A. Zhisheng, J. Grinevald, E. Odada, N. Oreskes, A. P. Wolfe, The Anthropocene is functionally and stratigraphically distinct from the Holocene. Science 351, eaad2622 (2016). 6 W. F. Ruddiman, Earth’s Climate: Past and Future (Third edition, W.H. Freeman and Co., 2014). 7 C. P. Summerhayes, Paleoclimatology: From Snowball Earth to the Anthropocene (Wiley-Blackwell, 2020). 8 H.-J. Schellnhuber, Discourse: Earth system analysis—The scope of the challenge, in Earth System Analysis: Integrating Science for Sustainability. H.-J. Schellnhuber, V. Wenzel, Eds. (Springer, Heidelberg, 1998), pp. 3–195. 9 M. B. Osman, J. E. Tierney, J. Zhu, R. Tardif, G. J. Hakim, J. King, C. J. Poulsen, Globally resolved surface temperatures since the Last Glacial Maximum. Nature 599, 239–244 (2021). 10 R. M. Beyer, M. Krapp, A. Manica, High-resolution terrestrial climate, bioclimate and vegetation for the last 120,000 years. Sci. Data. 7, 236 (2020). 11 P. K. Snyder, C. Delire, J. A. Foley, Evaluating the influence of different vegetation biomes on the global climate. Clim. Dyn. 23, 279–302 (2004). 12 P. C. West, G. T. Narisma, C. C. Barford, C. J. Kucharik, J. A. Foley, An alternative approach for quantifying climate regulation by ecosystems. Front. Ecol. Environ. 9, 126–133 (2010). 13 S. J. Lade, W. Steffen, W. de Vries, S. R. Carpenter, J. F. Donges, D. Gerten, H. Hoff, T. Newbold, K. Richardson, J. Rockström, Human impacts on planetary boundaries amplified by Earth system interactions. Nat. Sustain. 3, 119–128 (2020). 14 A. Chrysafi, V. Virkki, M. Jalava, V. Sandström, J. Piipponen, M. Porkka, S. Lade, K. La Mere, L. Wang-Erlandsson, L. Scherer, L. Andersen, E. Bennett, K. Brauman, G. Cooper, A. De Palma, P. Döll, A. Downing, T. DuBois, I. Fetzer, E. Fulton, D. Gerten, H. Jaafar, J. Jaegermeyr, F. Jaramillo, M. Jung, H. Kahiluoto, A. Mackay, L. Lassaletta, D. Mason-D’Croz, M. Mekonnen, K. Nash, A. Pastor, N. Ramankutty, B. Ridoutt, S. Siebert, B. Simmons, A. Staal, Z. Sun, A. Tobian, A. Usubiaga-Liaño, R. van der Ent, A. van Soesbergen, P. Verburg, Y. Wada, S. Zipper, M. Kummu, Quantifying Earth system interactions for sustainable food production: An expert elicitation. Nat. Sustain. 5, 830–842 (2022). 15 Intergovernmental Panel on Climate Change, Climate Change 2022: Impacts, Adaptation, and Vulnerability, H.-O. Pörtner, D. C. Roberts, M. Tignor, E. S. Poloczanska, K. Mintenbeck, A. Alegría, M. Craig, S. Langsdorf, S. Löschke, V. Möller, A. Okem, B. Rama, Eds. (Cambridge Univ. Press, 2022). 16 E. S. Brondizio, J. Settele, S. Díaz, H. T. Ngo, Global Assessment Report on Biodiversity and Ecosystem Services of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES, 2019). 17 Intergovernmental Panel on Climate Change, Climate Change 2021: The Physical Science Basis (Cambridge University Press, 2021). 18 D. A. McKay, A. Staal, J. Abrams, R. Winkelmann, B. Sakschewski, S. Loriani, I. Fetzer, S. E. Cornell, J. Rockström, T. M. Lenton, Exceeding 1.5°C global warming could trigger multiple climate tipping points. Science 377, eabn7950 (2022). 19 Past Interglacials Working Group of PAGES, Interglacials of the last 800,000 years. Rev. Geophys. 54, 162–219 (2016). 20 C. Ragon, V. Lembo, V. Lucarini, C. Vérard, J. Kasparian, M. Brunetti, Robustness of competing climatic states. J. Clim. 35, 2769–2784 (2022). 21 J. M. Anderies, S. R. Carpenter, W. Steffen, J. Rockström, The topology of non-linear global carbon dynamics: From tipping points to planetary boundaries. Environ. Res. Lett. 8, 044048 (2013). 22 S. J. Lade, J. Norberg, J. Anderies, C. Beer, S. Cornell, J. Donges, I. Fetzer, T. Gasser, K. Richardson, J. Rockström, W. Steffen, Potential feedbacks between loss of biosphere integrity and climate change. Glob. Sust. 2, 1–15 (2019). 23 M. Exposito-Alonso, T. R. Booker, L. Czech, T. Fukami, L. Gillespie, S. Hateley, C. C. Kyriazis, P. L. M. Lang, L. Leventhal, D. Nogues-Bravo, V. Pagowski, M. Ruffley, J. P. Spence, S. E. Toro Arana, C. L. Weiß, E. Zess, Genetic diversity loss in the Anthropocene. Science 377, 1431–1435 (2022). 24 H. Ceballos, P. R. Ehrlich, A. D. Barnosky, A. García, R. M. Pringle, T. M. Palmer, Accelerated modern human–induced species losses: Entering the sixth mass extinction. Sci. Adv. 1, e1400253 (2015). 25 M. D. A. Rounsevell, M. Harfoot, P. A. Harrison, T. Newbold, R. D. Gregory, G. M. Mace, A biodiversity target based on species extinctions. Science 368, 1193–1195 (2020). 26 R. H. Cowie, P. Bouchet, B. Fontaine, The sixth mass extinction: Fact, fiction or speculation? Biol. Rev. 97, 640–663 (2022). 27 R. J. Scholes, R. Biggs, A Biodiversity Intactness Index. Nature 434, 45–49 (2005). 28 P. A. Martin, R. E. Green, A. Balmford, The Biodiversity Intactness Index may underestimate losses. Nat. Evol. 3, 862–863 (2019). 29 S. E. Jorgensen, Y. M. Svirezhev, Towards a Thermodynamic Theory for Ecological Systems (Elsevier, 2004). 30 A. Kleidon, Sustaining the terrestrial biosphere in the anthropocene: A thermodynamic Earth system perspective. Ecol. Economy Soc. INSEE J. 6, 53–80 (2023). 31 S. W. Running, A measurable planetary boundary for the biosphere. Science 337, 1458–1459 (2012). 32 H. Haberl, K. H. Erb, F. Krausmann, Human appropriation of net primary production: Patterns, trends, and planetary boundaries. Annu. Rev. Environ. Res. 39, 363–391 (2014). 33 F. Krausmann, K. H. Erb, S. Gingrich, H. Haberl, A. Bondeau, V. Gaube, C. Lauka, C. Plutzar, T. D. Searchinger, Global human appropriation of net primary production doubled in the 20th century. Proc. Natl. Acad. Sci. U.S.A. 110, 10324–10329 (2013). 34 L. V. Gatti, L. S. Basso, J. B. Miller, M. Gloor, L. G. Dominigues, H. L. G. Cassol, G. Tejada, L. E. O. C. Aragao, C. Nobre, W. Peters, L. Marani, E. Arai, A. H. Sanches, S. M. Correa, L. Anderson, C. Von Randow, C. S. C. Correia, S. P. Crispim, R. A. L. Neves, Amazonia as a carbon source linked to deforestation and climate change. Nature 595, 388–393 (2021). 35 K. Goldewijk, A. Beusen, J. Doelman, E. Stehfest, Anthropogenic land use estimates for the Holocene – HYDE 3.2. Earth Syst. Sci. Data 9, 927–953 (2017). 36 D. Gerten, V. Heck, J. Jägermeyr, B. L. Bodirsky, I. Fetzer, M. Jalava, M. Kummu, W. Lucht, J. Rockström, S. Schaphoff, H. J. Schellnhuber, Feeding ten billion people is possible within four terrestrial planetary boundaries. Nat. Sust. 3, 200–208 (2020). 37 S. Ostberg, W. Lucht, S. Schaphoff, D. Gerten, Critical impacts of global warming on land ecosystems. Earth Syst. Dyn. 4, 347–357 (2013). 38 W. Schwartz, E. Sala, S. Tracey, R. Watson, D. Pauly, The spatial expansion and ecological footprint of fisheries (1950 to present). PLOS ONE 5, e15143 (2010). 39 J. S. Link, R. A. Watson, Global ecosystem overfishing: Clear delineation within real limits to production. Sci. Adv. 5, eaav047 (2019). 40 B. Planque, J.-M. Fromentin, P. Cury, K. F. Drinkwater, S. Jennings, R. I. Perry, S. Kifani, How does fishing alter marine populations and ecosystems sensitivity to climate? J. Mar. Sys. 79, 403–417 (2010). 41 P. M. Forster, C. J. Smith, T. Walsh, W. F. Lamb, M. D. Palmer, K. von Schuckmann, B. Trewin, M. Allen, R. Andrew, A. Birt, A. Borger, T. Boyer, J. A. Broersma, L. Cheng, F. Dentener, P. Friedlingstein, N. Gillett, J. M. Gutiérrez, J. Gütschow, M. Hauser, B. Hall, M. Ishii, S. Jenkins, R. Lamboll, X. Lan, J.-Y. Lee, C. Morice, C. Kadow, J. Kennedy, R. Killick, J. Minx, V. Naik, G. Peters, A. Pirani, J. Pongratz, A. Ribes, J. Rogelj, D. Rosen, C.-F. Schleussner, S. Seneviratne, S. Szopa, P. Thorne, R. Rohde, M. Rojas Corradi, D. Schumacher, R. Vose, K. Zickfeld, X. Zhang, V. Masson-Delmotte, P. Zhai, Indicators of Global Climate Change 2022: Annual update of large-scale indicators of the state of the climate system and the human influence. Earth Syst. Sci. Data 15, 2295–2327 (2023). 42 H. Schellnhuber, S. Rahmstorf, R. Winkelmann, Why the right climate target was agreed in Paris. Nat. Clim. Change 6, 649–653 (2016). 43 L. Persson, B. Almroth, C. Collins, S. Cornell, C. de Wit, M. Diamond, P. Fantke, M. Hassellöv, M. MacLeod, M. Ryberg, P. Jørgensen, P. Villarrubia-Gómez, Z. Wang, M. Zwicky Hauschild, Outside the safe operating space of the planetary boundary for novel entities. Environ. Sci. Tech. 56, 1510–1521 (2022). 44 P. J. Nair, L. Froidevaux, J. Kuttippurath, J. M. Zawodny, J. M. Russell III, W. Steinbrecht, H. Claude, T. Leblanc, J. A. E. van Gijsel, B. Johnson, D. P. J. Swart, A. Thomas, R. Querel, R. Wang, J. Anderson, Subtropical and midlatitude ozone trends in the stratosphere: Implications for recovery. J. Geophys. Res. Atmos. 120, 7247–7257 (2015). 45 A. Pazmiño, S. Godin-Beekmann, A. Hauchecorne, C. Claud, S. Khaykin, F. Goutail, E. Wolfram, J. Salvador, E. Quel, Multiple symptoms of total ozone recovery inside the Antarctic vortex during austral spring. Atmospheric Chem. Phys. 18, 7557–7572 (2018). 46 M. Porkka, V. Virkki, L. Wang-Erlandsson, D. Gerten, T. Gleeson, C. Mohan, I. Fetzer, F. Jaramillo, A. Staal, S. te Wierik, A. Tobian, R. van der Ent, P. Döll, M. Flörke, S. N. Gosling, N. Hanasaki, Y. Satoh, H. M. Schmied, N. Wanders, J. Rockström, M. Kummu, Global water cycle shifts far beyond pre-industrial conditions – Planetary boundary for freshwater change transgressed (2023); . 47 T. Gleeson, L. Wang-Erlandsson, S. C. Zipper, M. Porkka, F. Jaramillo, D. Gerten, I. Fetzer, S. E. Cornell, L. Piemontese, L. J. Gordon, J. Rockström, T. Oki, M. Sivapalan, Y. Wada, K. A. Brauman, M. Flörke, M. F. P. Bierkens, B. Lehner, P. Keys, M. Kummu, T. Wagener, S. Dadson, T. J. Troy, W. Steffen, M. Falkenmark, J. S. Famiglietti, The water planetary boundary: Interrogation and revision. One Earth. 2, 223–234 (2020). 48 L. Wang-Erlandsson, A. Tobian, R. J. van der Ent, I. Fetzer, S. te Wierik, M. Porkka, A. Staal, F. Jaramillo, H. Dahlmann, C. Singh, P. Greve, D. Gerten, P. W. Keys, T. Gleeson, S. E. Cornell, W. Steffen, X. Bai, J. Rockström, A planetary boundary for green water. Nat. Rev. Earth Environ. 3, 380–392 (2022). 49 Y. Qin, N. D. Mueller, S. Siebert, R. B. Jackson, A. A. Kouchak, J. B. Zimmerman, D. Tong, C. Hong, S. J. Davis, Flexibility and intensity of global water use. Nat. Sustain. 2, 515–523 (2019). 50 K. S. Carslaw, H. Gordon, D. S. Hamilton, J. S. Johnson, L. A. Regayre, M. Yoshioka, K. J. Pringle, Aerosols in the pre-industrial atmosphere. Curr. Clim. Chang. Rep. 3, 1–15 (2017). 51 N. Bellouin, J. Quaas, E. Gryspeerdt, S. Kinne, P. Stier, D. Watson-Parris, O. Boucher, K. S. Carslaw, M. Christensen, A.-L. Daniau, J.-L. Dufresne, G. Feingold, S. Fiedler, P. Forster, A. Gettelman, J. M. Haywood, U. Lohmann, F. Malavelle, T. Mauritsen, D. T. McCoy, G. Myhre, J. Mülmenstädt, D. Neubauer, A. Possner, M. Rugenstein, Y. Sato, M. Schulz, S. E. Schwartz, O. Sourdeval, T. Storelvmo, V. Toll, D. Winker, B. Stevens, Bounding global aerosol radiative forcing of climate change. Rev. Geophys. 58, e2019RG000660 (2020). 52 J. Hooper, S. K. Marx, A global doubling of dust emissions during the Anthropocene? Glob. Planet. Change 169, 70–91 (2018). 53 P. Kinppertz, M. C. Todd, Mineral dust aerosols over the Sahara: Meteorological controls on emission and transport and implications for modeling. Rev. Geophys. 50, RG1007 (2012). 54 M. L. Griffiths, K. R. Johnson, F. S. R. Pausata, J. C. White, G. M. Henderson, C. T. Wood, H. Yang, V. Ersek, C. Conrad, N. Sekhon, End of Green Sahara amplified mid- to late Holocene megadroughts in mainland Southeast Asia. Nat. Commun. 11, 4204 (2020). 55 M. Chin, T. Diehl, Q. Tan, J. M. Prospero, R. A. Kahn, L. A. Remer, H. Yu, A. M. Sayer, H. Bian, I. V. Geogdzhayev, B. N. Holben, S. G. Howell, B. J. Huebert, N. C. Hsu, D. Kim, T. L. Kucsera, R. C. Levy, M. I. Mishchenko, X. Pan, P. K. Quinn, G. L. Schuster, D. G. Streets, S. A. Strode, O. Torres, X.-P. Zhao, Multi-decadal aerosol variations from 1980 to 2009: A perspective from observations and a global model. Atmos. Chem. Phys. 14, 3657–3690 (2014). 56 L. Sogacheva, T. Popp, A. M. Sayer, O. Dubovik, M. J. Garay, A. Heckel, N. C. Hsu, H. Jethva, R. A. Kahn, P. Kolmonen, M. Kosmale, G. de Leeuw, R. C. Levy, P. Litvinov, A. Lyapustin, P. North, O. Torres, Merging regional and global AOD records from 15 available satellite products. Atmos. Chem. Phys. 20, 2031–2056 (2019). 57 A. Vogel, G. Alessa, R. Scheele, L. Weber, O. Dubovik, P. North, S. Fiedler, Uncertainty in aerosol optical depth from modern aerosol-climate models, reanalyses, and satellite products. J. Geophys. Res. Atmos. 127, e2021JD035483 (2022). 58 J. Haywood, A. Jones, N. Bellouin, D. Stephenson, Asymmetric forcing from stratospheric aerosols impacts Sahelian rainfall. Nat. Clim. Change. 3, 660–665 (2013). 59 K. S. Krishnamohan, G. Bala, Sensitivity of tropical monsoon precipitation to the latitude of stratospheric aerosol injections. Clim. Dyn. 59, 151–168 (2022). 60 S. Roose, G. Bala, K. S. Krishnamohan, L. Cao, K. Caldeira, Quantification of tropical monsoon precipitation changes in terms of interhemispheric differences in stratospheric sulfate aerosol optical depth. Clim. Dyn. 2023, 1–16 (2023). 61 A. Donohoe, J. Marshall, D. Ferreira, D. Mcgee, The relationship between ITCZ location and cross-equatorial atmospheric heat transport: From the seasonal cycle to the last glacial maximum. J. Clim. 26, 3597–3618 (2013). 62 M. C. MacCracken, H.-J. Shin, K. Caldeira, G. A. Ban-Weiss, Climate response to imposed solar radiation reductions in high latitudes. Earth Syst. Dyn. 4, 301–315 (2013). 63 N. Devaraju, G. Bala, A. Modak, Effects of large-scale deforestation on precipitation in the monsoon regions: Remote versus local effects. Proc. Natl. Acad. Sci. U.S.A. 112, 3257–3262 (2015). 64 I. B. Ocko, V. Ramaswamy, Y. Ming, Contrasting climate responses to the scattering and absorbing features of anthropogenic aerosol forcings. J. Clim. 27, 5329–5345 (2014). 65 M. Zhao, L. Cao, G. Bala, L. Duan, Climate response to latitudinal and altitudinal distribution of stratospheric sulfate aerosols. J. Geophys. Res. Atmos. 126, e2021JD035379 (2021). 66 J. T. Fasullo, B. L. Otto-Bliesner, S. Stevenson, The influence of volcanic aerosol meridional structure on monsoon responses over the last millennium. Geophys. Res. Lett. 46, 12350–12359 (2019). 67 S. Fiedler, T. Crueger, R. D’Agostino, K. Peters, T. Becker, D. Leutwyler, L. Paccini, J. Burdanowitz, S. Buehler, A. Uribe, T. Dauhut, D. Dommenget, K. Fraedrich, L. Jungandreas, N. Maher, A. Naumann, M. Rugenstein, M. Sakradzija, H. Schmidt, F. Sielmann, C. Stephan, C. Timmreck, X. Zhu, B. Stevens, Simulated tropical precipitation assessed across three major phases of the Coupled Model Intercomparison Project (CMIP). Mon. Weather Rev. 148, 3653–3680 (2020). 68 P. Zanis, D. Akritidis, A. K. Georgoulias, R. J. Allen, S. E. Bauer, O. Boucher, J. Cole, B. Johnson, M. Deushi, M. Michou, J. Mulcahy, P. Nabat, D. Olivié, N. Oshima, A. Sima, M. Schulz, T. Takemura, K. Tsigaridis, Fast responses on pre-industrial climate from present-day aerosols in a CMIP6 multi-model study. Atmos. Chem. Phys. 20, 8381–8404 (2020). 69 S. Fiedler, B. Stevens, M. Gidden, S. J. Smith, K. Riahi, D. van Vuuren, First forcing estimates from the future CMIP6 scenarios of anthropogenic aerosol optical properties and an associated Twomey effect. Geosci. Model Dev. 12, 989–1007 (2019). 70 N. M. Mahowald, R. Scanza, J. Brahney, C. L. Goodale, P. G. Hess, J. K. Moore, J. Neff, Aerosol deposition impacts on land and ocean carbon cycles. Curr. Clim. Change Rep. 3, 16–31 (2017). 71 L. Jiang, R. A. Feely, B. R. Carter, D. J. Greeley, D. K. Gledhill, K. M. Arzayus, Climatological distribution of aragonite saturation state in the global oceans. Glob. Biogeochem. Cycles. 29, 1656–1673 (2015). 72 EU Copernicus Climate Change Service, “Land cover classification gridded maps from 1992 to present derived from satellite observations”, ICDR Land Cover 2016–2020. 73 Food and Agricultural Organization of the United Nations, (FAO), United Nations’ Environmental Program, (UNEP), “The State of the World’s Forests 2020. Forests, biodiversity and people” (Publication 978-92-5-132419-6, 2020);. 74 Food and Agricultural Organization of the United Nations, (FOA), “Global Forest Resources Assessment 2020: Main report” (Publication 978-92-5-132974-0, 2020);. 75 S. R. Carpenter, E. M. Bennett, Reconsideration of the planetary boundary for phosphorus. Environ. Res. Lett. 6, 014009 (2011). 76 C. Liu, H. Tian, Global nitrogen and phosphorus fertilizer use for agriculture production in the past half century: Shifted hot spots and nutrient imbalance. Earth Syst. Sci. Data 9, 181–192 (2017). 77 W. J. Brownlie, M. A. Sutton, K. V. Heal, D. S. Reay, B. M. Spears (eds.), Our Phosphorus Future (U.K. Centre for Ecology & Hydrology, 2022). 78 T. Zou, X. Zhang, E. Davidson, Improving phosphorus use efficiency in cropland to address phosphorus challenges by 2050. Earth Space Sci. Open Archive, (2020). 79 D. Cordell, S. White, Life’s bottleneck: Sustaining the World’s phosphorus for a food secure future. Annu. Rev. Environ. Res. 39, 161–188 (2014). 80 Food and Agriculture Organisation of the United Nations (FAO), “World fertilizer trends and outlook to 2022 – Summary Report, Rome” (2019); ) 81 M. A. Adams, N. Buchmann, J. Sprent, T. N. Buckley, T. L. Turnbull, Crops, nitrogen, water: Are legumes friend, foe, or misunderstood ally? Trends Plant. Sci. 23, 539–550 (2018). 82 P. M. Vitousek, D. N. L. Menge, S. C. Reed, C. C. Cleveland, Biological nitrogen fixation: Rates, patterns and ecological controls in terrestrial ecosystems. Philos. Trans. R. Soc. Lond. B. Biol. Sci. 368, 1621 (2013). 83 M. V. B. Figueiredo, A. E. S. Mergulhão, J. K. Sobral, M. A. L. Junio, A. S. F. Araújo, Biological nitrogen fixation: Importance, associated diversity, and estimates, in Plant Microbe Symbiosis: Fundamentals and Advances (Springer, 2013), pp. 267–289. 84 FAO, “FAOSTAT—FAO database for food and agriculture” (2022); (accessed 4.19.22) 85 M. Drüke, W. von Bloh, S. Petri, B. Sakschewski, S. Schaphoff, M. Forkel, W. Huiskamp, G. Feulner, K. Thonicke, CM2Mc-LPJmL v1.0: Biophysical coupling of a process-based dynamic vegetation model with managed land to a general circulation model. Geosci. Model. Dev. 14, 4117–4141 (2021). 86 K.-H. Erb, T. Kastner, C. Plutzar, A. L. S. Bais, N. Carvalhais, T. Fetzel, S. Gingrich, H. Haberl, C. Lauk, M. Niedertscheider, J. Pongratz, M. Thurner, S. Luyssaert, Unexpectedly large impact of forest management and grazing on global vegetation biomass. Nature 553, 73–76 (2017). 87 Y. M. Bar-On, R. Phillips, R. Milo, The biomass distribution on Earth. Proc. Natl. Acad. Sci. U.S.A. 115, 6506–6511 (2018). 88 K. Matsuomoto, T. Hashioka, Y. Yamanaka, Effect of temperature-dependent organic carbon decay on atmospheric pCO2. J. Geophys. Res. 112, G02007 (2007). 89 L. Kwiatkowski, O. Torres, L. Bopp, O. Aumont, M. Chamberlain, J. R. Christian, J. P. Dunne, M. Gehlen, T. Ilyina, J. G. John, A. Lenton, H. Li, N. S. Lovenduski, J. C. Orr, J. Palmieri, Y. Santana-Falcón, J. Schwinger, R. Séférian, C. A. Stock, A. Tagliabue, Y. Takano, J. Tjiputra, K. Toyama, H. Tsujino, M. Watanabe, A. Yamamoto, A. Yool, T. Ziehn, Twenty-first century ocean warming, acidification, deoxygenation, and upper-ocean nutrient and primary production decline from CMIP6 model projections. Biogeosci 17, 3439–3470 (2020). 90 E. A. Laws, E. D’Sa, P. Naik, Simple equations to estimate ratios of new or export production to total production from satellite-derived estimates of sea surface temperature and primary production. Limnol. Oceanogr. Meth. 9, 593–601 (2011). 91 C. M. Marsay, R. J. Sanders, S. A. Henson, K. Pabortsava, E. P. Achterberg, R. S. Lampitt, Attenuation of sinking POC flux in the mesopelagic. Proc. Natl. Acad. Sci. U.S.A. 112, 1089–1094 (2015). 92 K. A. Chrichton, J. D. Wilson, A. Ridgewell, F. Boscob-Galazzo, E. H. John, B. S. Wade, P. N. Pearson, What the geological past can tell us about the future of the ocean’s twilight zone. Nat. Commun. 14, 2376 (2023). 93 The Royal Society, “Ocean acidification due to increasing atmospheric carbon dioxide” (Publication 0 85403 617 2, Policy Doc. 12/05, R. Soc., 2005). 94 E. D. Galbraith, J. P. Dunne, A. Gnanadesikan, R. D. Slater, J. L. Sarmiento, C. O. Dufour, G. F. de Souza, D. Bianchi, M. Claret, K. B. Rodgers, S. S. Marvasti, Complex functionality with minimal computation: Promise and pitfalls of reduced-tracer ocean biogeochemistry models. J. Adv. Model Earth Syst. 7, 2012–2028 (2015). 95 S. Schaphoff, M. Forkel, C. Müller, J. Knauer, W. von Bloh, D. Gerten, J. Jägermeyr, W. Lucht, A. Rammig, K. Thonicke, K. Waha, LPJmL4 – A dynamic global vegetation model with managed land – Part 2: Model evaluation. Geosci. Model Dev. 11, 1377–1403 (2018b). 96 NASA Earth Observation, “AURA Ozone data”; . 97 N. Ramankutty, J. A. Foley, Characterizing patterns of global land use: An analysis of global croplands data. Glob. Biogeochem. Cycles. 12, 667–685 (1998). 98 C. W. Snyder, M. D. Mastrandrea, S. H. Schneider, The complex dynamics of the climate system: Constraints on our knowledge, policy implications and the necessity of systems thinking. Philos. Complex Syst. 10, 467–505 (2011). 99 M. Willeit, A. Ganopolski, R. Calov, V. Brovkin, Mid-Pleistocene transition in glacial cycles explained by declining CO2 and regolith removal. Sci. Adv. 5, eaav7337 (2019). 100 J. Zheng, J. L. Payne, A. Wagner, Cryptic genetic variation accelerates evolution by opening access to diverse adaptive peaks. Science 365, 347–353 (2019). 101 M. C. Bitter, L. Kapsenberg, J.-P. Gattuso, C. A. Pfister, Standing genetic variation fuels rapid adaptation to ocean acidification. Nat. Commun. 10, 5821 (2019). 102 T. H. Oliver, M. S. Heard, N. J. Isaac, D. B. Roy, D. Procter, F. Eigenbrod, R. Freckleton, A. Hector, C. D. L. Orme, O. L. Petchey, V. Proença, Biodiversity and resilience of ecosystem functions. Trends Ecol. Evol. 30, 673–684 (2015). 103 A. A. Hoffmann, C. M. Sgrò, T. N. Kristensen, Revisiting adaptive potential, population size, and conservation. Trends Ecol. Evol. 32, 506–517 (2017). 104 A. Miraldo, S. Li, M. K. Borregaard, A. Flórez-Rodríguez, S. Gopalakrishnan, M. Rizvanovic, Z. Wang, C. Rahbek, K. A. Marske, D. Nogués-Bravo, An anthropocene map of genetic diversity. Science 353, 1532–1535 (2016). 105 S. Blanchet, J. G. Prunier, H. De Kort, Time to go bigger: Emerging patterns in macrogenetics. Trends Genet. 33, 579–580 (2017). 106 S. Theodoridis, D. A. Fordham, S. C. Brown, S. Li, C. Rahbek, D. Nogues-Bravo, Evolutionary history and past climate change shape the distribution of genetic diversity in terrestrial mammals. Nat. Commun. 11, 2557 (2020). 107 D. M. Leigh, C. B. van Rees, K. L. Millette, M. F. Breed, C. Schmidt, L. D. Bertola, B. K. Hand, M. E. Hunter, E. L. Jensen, F. Kershaw, L. Liggins, G. Luikart, S. Manel, J. Mergeay, J. M. Miller, G. Segelbacher, S. Hoban, I. Paz-Vinas, Opportunities and challenges of macrogenetic studies. Nat. Rev. Genet. 22, 791–807 (2021). 108 S. Theodoridis, C. Rahbek, D. Nogués-Bravo, Exposure of mammal genetic diversity to mid-21st century global change. Ecography 44, 817–831 (2021). 109 S. Hoban, M. Brufordb, J. D’Urban Jackson, M. Lopes-Fernandes, M. Heuertz, P. A. Hohenlohe, I. Paz-Vinas, P. Sjögren-Gulve, G. Segelbacher, C. Vernesi, S. Aitken, L. D. Bertola, P. Bloomer, M. Breed, H. Rodríguez-Correa, W. C. Funk, C. E. Grueber, M. E. Hunter, L. Laikre, Genetic diversity targets and indicators in the CBD post-2020 global biodiversity framework must be improved. Biol. Conserv. 248, 108654 (2020). 110 A. Ganopolski, V. Brovkin, Simulation of climate, ice sheets and CO2 evolution during the last four glacial cycles with an Earth system model of intermediate complexity. Clim. 13, 1695–1716 (2017). 111 S. Schaphoff, W. Bloh, A. Rammig, K. Thonicke, H. Biemans, M. Forkel, D. Gerten, J. Heinke, J. Jägermeyr, J. Knauer, F. Langerwisch, W. Lucht, C. Müller, S. Rolinski, K. Waha, LPJmL4–a dynamic global vegetation model with managed land – Part 1: Model description. Geosci. Model Dev. 11, 1343–1375 (2018). 112 I. C. Harris, P. D. Jones, “CRU TS3.23: Climatic Research Unit (CRU) Time-Series (TS) Version 3.23 of High Resolution Gridded Data of Month-by-month Variation in Climate (Jan. 1901- Dec. 2014)” (CEDA Archive, 2015); 113 I. Harris, P. Jones, T. Osborn, D. Lister, Updated high-resolution grids of monthly climatic observations – The CRU TS3.10 dataset. Int. J. Climatol. 34, 623–642 (2014). 114 D. Kaufman, N. McKay, C. Routson, M. Erb, C. Dätwyler, P. S. Sommer, O. Heiri, B. Davis, Holocene global mean surface temperature, a multi-method reconstruction approach. Sci. Data 7, 201 (2020). 115 H. Haberl, K. H. Erb, F. Krausmann, V. Gaube, A. Bondeau, C. Plutzar, S. Gingrich, W. Lucht, M. Fischer-Kowalski, Quantifying and mapping the human appropriation of net primary production in Earth’s terrestrial ecosystems. Proc. Natl. Acad. Sci. U.S.A. 104, 12942–12947 (2007). 116 D. Lawrence, K. Vandecar, Effects of tropical deforestation on climate and agriculture. Nat. Clim. Change 5, 27–36 (2015). 117 P. W. Keys, L. Wang-Erlandsson, L. J. Gordon, Revealing invisible water: Moisture recycling as an ecosystem service. PLOS ONE 11, e0151993 (2016). 118 L. Wang-Erlandsson, I. Fetzer, P. W. Keys, R. J. van der Ent, H. H. G. Savenije, L. J. Gordon, Remote land use impacts on river flows through atmospheric teleconnections. Hydrol. Earth Syst. Sci. 22, 4311–4328 (2018). 119 D. Gerten, H. Hoff, J. Rockström, J. Jägermeyr, M. Kummu, A. V. Pastor, Towards a revised planetary boundary for consumptive freshwater use: Role of environmental flow requirements. Curr. Opin. Environ. Sustain. 5, 551–558 (2013). 120 J. Liu, C. Zang, S. Tian, J. Liu, H. Yang, S. Jia, L. You, B. Liu, M. Zhang, Water conservancy projects in China: Achievements, challenges and way forward. Glob. Environ. Change 23, 633–643 (2013). 121 J. Sillmann, C. W. Stjern, G. Myhre, B. H. Samset, Ø. Hodnebrog, T. Andrews, O. Boucher, G. Faluvegi, P. Forster, M. R. Kasoar, V. V. Kharin, A. Kirkevåg, J.-F. Lamarque, D. J. L. Olivié, T. B. Richardson, D. Shindell, T. Takemura, A. Voulgarakis, F. W. Zwiers, Extreme wet and dry conditions affected differently by greenhouse gases and aerosols. Nat. Clim. Atmospheric Sci. 2, 1–7 (2019). 122 N. L. Poff, J. D. Olden, D. M. Merritt, D. M. Pepin, Homogenization of regional river dynamics by dams and global biodiversity implications. Proc. Natl. Acad. Sci. U.S.A. 104, 5732–5737 (2007). 123 A. Staal, O. A. Tuinenburg, J. H. C. Bosmans, M. Holmgren, E. H. van Nes, M. Scheffer, D. C. Zemp, S. C. Dekker, Forest-rainfall cascades buffer against drought across the Amazon. Nat. Clim. Change 8, 539–543 (2018). 124 A. Günther, A. Barthelmes, V. Huth, H. Joosten, G. Jurasinski, F. Koebsch, J. Couwenberg, Prompt rewetting of drained peatlands reduces climate warming despite methane emissions. Nat. Commun. 11, 1644 (2020). 125 T. Maavara, Q. Chen, K. Van Meter, L. E. Brown, J. Zhang, J. Ni, C. Zarfl, River dam impacts on biogeochemical cycling. Nat. Rev. Earth Environ. 1, 103–116 (2020). 126 N. Boers, N. Marwan, H. M. J. Barbosa, J. Kurths, A deforestation-induced tipping point for the south American monsoon system. Sci. Rep. 7, 41489 (2017). 127 K. Frieler, S. Lange, F. Piontek, C. P. O. Reyer, J. Schewe, L. Warszawski, F. Zhao, L. Chini, S. Denvil, K. Emanuel, T. Geiger, K. Halladay, G. Hurtt, M. Mengel, D. Murakami, S. Ostberg, A. Popp, R. Riva, M. Stevanovic, T. Suzuki, J. Volkholz, E. Burke, P. Ciais, K. Ebi, T. D. Eddy, J. Elliott, E. Galbraith, S. N. Gosling, F. Hattermann, T. Hickler, J. Hinkel, C. Hof, V. Huber, J. Jägermeyr, V. Krysanova, R. Marcé, H. Müller Schmied, I. Mouratiadou, D. Pierson, D. P. Tittensor, R. Vautard, M. van Vliet, M. F. Biber, R. A. Betts, B. L. Bodirsky, D. Deryng, S. Frolking, C. D. Jones, H. K. Lotze, H. Lotze-Campen, R. Sahajpal, K. Thonicke, H. Tian, Y. Yamagata, Assessing the impacts of 1.5 °C global warming – Simulation protocol of the Inter-Sectoral Impact Model Intercomparison Project (ISIMIP2b). Geosci. Model Dev. 10, 4321–4345 (2017). 128 S. Siebert, M. Kummu, M. Porkka, P. Döll, N. Ramankutty, B. R. Scanlon, A global data set of the extent of irrigated land from 1900 to 2005. Hydrol. Earth Syst. Sci. 19, 1521–1545 (2015). 129 Y. Wada, M. F. P. Bierkens, Sustainability of global water use: Past reconstruction and future projections. Environ. Res. Lett. 9, 104003 (2014). 130 C. Zarfl, A. E. Lumsdon, J. Berlekamp, L. Tydecks, K. Tockner, A global boom in hydropower dam construction. Aquat. Sci. 77, 161–170 (2015). 131 R. J. Keenan, G. A. Reams, F. Achard, J. V. de Freitas, A. Grainger, E. Lindquist, Dynamics of global forest area: Results from the FAO Global Forest Resources Assessment 2015. For. Ecol. Manag. 352, 9–20 (2015). 132 A. Barnosky, E. Hadly, J. Bascompte, E. L. Berlow, J. H. Brown, M. Fortelius, W. M. Getz, J. Harte, A. Hastings, P. A. Marquet, N. D. Martinez, A. Mooers, P. Roopnarine, G. Vermij, J. W. Williams, R. Gillespie, J. Kitzes, C. Marshall, N. Matzke, D. P. Mindell, E. Revilla, A. B. Smith, Approaching a state shift in Earth’s biosphere. Nature 486, 52–58 (2012). 133 H. J. Fowler, G. Lenderink, A. F. Prein, S. Westra, R. P. Allan, N. Ban, R. Barbero, P. Berg, S. Blenkinsop, H. X. Do, S. Guerreiro, J. O. Haerter, E. J. Kendon, E. Lewis, C. Schaer, A. Sharma, G. Villarini, C. Wasko, X. Zhang, Anthropogenic intensification of short-duration rainfall extremes. Nat. Rev. Earth Environ. 2, 107–122 (2021). 134 L. Gudmundsson, J. Boulange, H. X. Do, S. N. Gosling, M. G. Grillakis, A. G. Koutroulis, M. Leonard, J. Liu, N. M. Schmied, L. Papadimitriou, Y. Pokhrel, S. I. Seneviratne, Y. Satoh, W. Thiery, S. Westra, X. Zhang, F. Zhao, Globally observed trends in mean and extreme river flow attributed to climate change. Science 371, 1159–1162 (2021). 135 J. Spinoni, G. Naumann, H. Carrao, P. Barbosa, J. Vogt, World drought frequency, duration, and severity for 1951–2010. Int. J. Climatol. 34, 2792–2804 (2014). 136 T. G. Huntington, Evidence for intensification of the global water cycle: Review and synthesis. J. Hydrol. 319, 83–95 (2006). 137 J. Jägermeyr, A. Pastor, H. Biemans, D. Gerten, Reconciling irrigated food production with environmental flows for sustainable development goals implementation. Nat. Commun. 8, 15900 (2017). 138 A. V. Pastor, F. Ludwig, H. Biemans, H. Hoff, P. Kabat, Accounting for environmental flow requirements in global water assessments. Hydrol. Earth Syst. Sci. 18, 5041–5059 (2014). 139 V. Virkki, E. Alanärä, M. Porkka, L. Ahopelto, T. Gleeson, C. Mohan, L. Wang-Erlandsson, M. Flörke, D. Gerten, S. N. Gosling, N. Hanasaki, H. Müller Schmied, N. Wanders, M. Kummu, Globally widespread and increasing violations of environmental flow envelopes. Hydrol. Earth Syst. Sci. 26, 3315–3336 (2022). 140 P. Greve, B. Orlowsky, B. Mueller, J. Sheffield, M. Reichstein, S. I. Seneviratne, Global assessment of trends in wetting and drying over land. Nat. Geosci. 7, 716–721 (2014). 141 P. Micklin, The aral sea disaster. Annu. Rev. Earth Planet. Sci. 35, 47–72 (2018). 142 W. M. Hammond, A. P. Williams, J. T. Abatzoglou, H. D. Adams, T. Klein, R. López, C. Sáenz-Romero, H. Hartmann, D. D. Breshears, C. D. Allen, Global field observations of tree die-off reveal hotter-drought fingerprint for Earth’s forests. Nat. Commun. 13, 1761 (2022). 143 R. S. Cottrell, K. L. Nash, B. S. Halpern, T. A. Remenyi, S. P. Corney, A. Fleming, E. A. Fulton, S. Hornborg, A. Johne, R. A. Watson, J. L. Blanchard, Food production shocks across land and sea. Nat. Sustain. 2, 130–137 (2019). 144 J. Schöngart, F. Wittmann, A. Faria de Resende, C. Assahira, G. de Sousa Lobo, J. R. D. Neves, M. da Rocha, G. B. Mori, A. C. Quaresma, L. O. Demarchi, B. W. Albuquerque, Y. O. Feitosa, G. da Silva Costa, G. V. Feitoza, F. M. Durgante, A. Lopes, S. E. Trumbore, T. S. F. Silva, H. ter Steege, A. L. Val, W. J. Junk, M. T. F. Piedade, The shadow of the Balbina dam: A synthesis of over 35 years of downstream impacts on floodplain forests in Central Amazonia. Aquat. Conserv. Mar. Freshw. Ecosyst. 31, 1117–1135 (2021). 145 B. R. Deemer, J. A. Harrison, S. Li, J. J. Beaulieu, T. DelSontro, N. Barros, J. F. Bezerra-Neto, S. M. Powers, M. A. dos Santos, J. A. Vonk, Greenhouse gas emissions from reservoir water surfaces: A new global synthesis. BioScience 66, 949–964 (2016). 146 A. Clarke, V. Kapustin, Hemispheric aerosol vertical profiles: Anthropogenic impacts on optical depth and cloud nuclei. Science 329, 1488–1492 (2010). 147 P.-A. Monerie, L. J. Wilcox, A. G. Turner, Effects of anthropogenic aerosol and greenhouse gas emissions on northern hemisphere monsoon precipitation: Mechanisms and uncertainty. J. Clim. 35, 2305–2326 (2022). 148 J. Cao, H. Wang, B. Wang, H. Zhao, C. Wang, X. Zhu, Higher sensitivity of northern hemisphere monsoon to anthropogenic aerosol than greenhouse gases. Geophys. Res. Lett. 49, e2022GL100270 (2022). 149 B. Zhuang, Y. Gao, Y. Hu, H. Chen, T. Wang, S. Li, M. Li, M. Xie, Interaction between different mixing aerosol direct effects and East Asian summer monsoon. Clim. Dyn. 61, 1157–1176 (2022). 150 D. M. Westervelt, Y. You, X. Li, M. Ting, D. E. Lee, Y. Ming, Relative importance of greenhouse gases, sulfate, organic carbon, and black carbon aerosol for south asian monsoon rainfall changes. Geophys. Res. Lett. 47, e2020GL088363 (2020). 151 E. D. Galbraith, E. Y. Kwon, A. Gnanadesikan, K. B. Rodgers, S. M. Griffies, D. Bianchi, J. L. Sarmiento, J. P. Dunne, J. Simeon, R. D. Slater, A. T. Wittenberg, I. M. Held, Climate variability and radiocarbon in the CM2Mc Earth system model. J. Clim. 24, 4230–4254 (2011). 152 W. von Bloh, S. Schaphoff, C. Müller, S. Rolinski, K. Waha, S. Zaehle, Implementing the nitrogen cycle into the dynamic global vegetation, hydrology, and crop growth model LPJmL (version 5.0). Geosci. Model Dev. 11, 2789–2812 (2018). 153 P. C. D. Milly, A. B. Shmakin, Global modeling of land water and energy balances. Part I: The land dynamics (LaD) model. J. Hydrometeorol. 3, 283–299 (2002). 154 J. L. Anderson, V. Balaji, A. J. Broccoli, W. F. Cooke, T. L. Delworth, K. W. Dixon, L. J. Donner, K. A. Dunne, S. M. Freidenreich, S. T. Garner, R. G. Gudgel, C. T. Gordon, I. M. Held, R. S. Hemler, L. W. Horowitz, S. A. Klein, T. R. Knutson, P. J. Kushner, A. R. Langenhost, N. C. Lau, Z. Liang, S. L. Malyshev, P. C. D. Milly, M. J. Nath, J. J. Ploshay, V. Ramaswamy, M. D. Schwarzkopf, E. Shevliakova, J. J. Sirutis, B. J. Soden, W. F. Stern, L. A. Thompson, R. J. Wilson, A. T. Wittenberg, B. L. Wyman, The new GFDL global atmosphere and land model AM2-LM2: Evaluationvwith prescribed SST simulations. J. Clim. 17, 4641–4673 (2004). 155 S. Sitch, B. Smith, I. C. Prentice, A. Arneth, A. Bondeau, W. Cramer, J. O. Kaplan, S. Levis, W. Lucht, M. T. Sykes, K. Thonicke, S. Venevsky, Evaluation of ecosystem dynamics, plant geography and terrestrial carbon cycling in the LPJ dynamic global vegetation model. Glob. Change Biol. 9, 161–185 (2003). 156 D. Gerten, S. Schaphoff, U. Haberlandt, W. Lucht, S. Sitch, Terrestrial vegetation and large-scale water balance. Hydrological evaluation of a dynamic global vegetation model. J. Hydrol. 286, 249–270 (2004). 157 A. Bondeau, P. Smith, S. Zaehle, S. Schaphoff, W. Lucht, W. Cramer, D. Gerten, H. Lotze-Campen, C. Müller, M. Reichstein, B. Smith, Modelling the role of agriculture for the 20th century global terrestrial carbon balance. Glob. Change Biol. 13, 1–28 (2007). 158 K. Thonicke, A. Spessa, I. C. Prentice, S. P. Harrison, L. Dong, C. Carmona-Moreno, The influence of vegetation, fire spread and fire behaviour on biomass burning and trace gas emissions: Results from a process-based model. Biogeosci. 7, 1991–2011 (2010). 159 M. Drüke, M. Forkel, W. von Bloh, B. Sakschewski, M. Cardoso, M. Bustamante, J. Kurths, K. Thonicke, Improving the LPJmL4-SPITFIRE vegetation-fire model for South America using satellite data. Geosci. Model. Dev. 12, 5029–2054 (2019). 160 M. Forkel, N. Carvalhais, S. Schaphoff, W. von Bloh, M. Migliavacca, M. Thurner, K. Thonicke, Identifying environmental controls on vegetation greeness phenology through model-data integration. Biogeosci. 11, 7025–7050 (2014). 161 M. Forkel, M. Drüke, M. Thurner, W. Dorigo, S. Schaphoff, K. Thonicke, W. von Bloh, N. Carvalhais, Constraining modelled global vegetation dynamics and carbon turnover using multiple satellite observations. Sci. Rep. 9, 18757 (2019). 162 S. Fader, C. Rost, A. Müller, D. Bondeau, Gerten, virtual water content of temperate cereals and maize: Present and potential future patterns. J. Hydrol. 384, 218–231 (2010). 163 V. Kattsov, R. Federation, C. Reason, S. Africa, A. A. Uk, T. A. Uk, J. Baehr, A. B. Uk, J. Catto, J. S. Canada, A. S. Uk, Evaluation of climate models (AR5), Climate Change 2013 - The Physical Science Basis (Cambridge University Press, 2013), pp. 741–866. 164 M. Santoro, O. Cartus, S. Mermoz, A. Bouvet, T. Le Toan, N. Carvalhais, D. Rozendaal, M. Herold, V. Avitabile, S. Quegan, J. Carreiras, Y. Rauste, H. Balzter, C. C. Schmullius, F. M. Seifert, A detailed portrait of the forest aboveground biomass pool for the year 2010 obtained from multiple remote sensing observations. Geophys. Res. Abstr. 20, EGU2018-18932 (2018). 165 P. Gkatsopoulos, A methodology for calculating cooling from vegetation evapotranspiration for use in urban space microclimate simulations. Proc. Environ. Sci. 38, 477–484 (2017). 166 N. Unger, Human land-use-driven reduction of forest volatiles cools global climate. Nat. Clim. Change 4, 907–910 (2014). 167 W. A. Hoffmann, R. B. Jackson, Vegetation-climate feedbacks in the conversion of tropical savanna to grassland. J. Clim. 13, 1593–1602 (2000). SHOW ALL REFERENCES (0) eLetters eLetters is a forum for ongoing peer review. eLetters are not edited, proofread, or indexed, but they are screened. eLetters should provide substantive and scholarly commentary on the article. Embedded figures cannot be submitted, and we discourage the use of figures within eLetters in general. If a figure is essential, please include a link to the figure within the text of the eLetter. Please read our Terms of Service before submitting an eLetter. LOG IN TO SUBMIT A RESPONSE No eLetters have been published for this article yet. Recommended articles from TrendMD Universal scaling across biochemical networks on Earth Hyunju Kim et al., Sci Adv, 2019 UV-B radiation was the Devonian-Carboniferous boundary terrestrial extinction kill mechanism John E. A. Marshall et al., Sci Adv, 2020 A unified vegetation index for quantifying the terrestrial biosphere Gustau Camps-Valls et al., Sci Adv, 2021 Decoupling biogeochemical records, extinction, and environmental change during the Cambrian SPICE event James D. Schiffbauer et al., Sci Adv, 2017 Abiotic molecular oxygen production—Ionic pathway from sulfur dioxide Måns Wallner et al., Sci Adv, 2022 On action-minimizing solutions of the two-center problem Kuo-Chang Chen, Acta Mathematica Scientia, 2022 Genome-wide DNA methylation patterns in monocytes derived from patients with primary Sjogren syndrome Xuan Luo et al., Chinese Medical Journal, 2021 On Local Controllability for Compressible Navier-Stokes Equations with Density Dependent Viscosities Xiangkai Lian et al., Acta Mathematica Scientia, 2022 Oxidative stress in leukemia and antioxidant treatment Chao Dong et al., Chinese Medical Journal, 2021 Discovery of human pancreatic lipase inhibitors from root of Rhodiola crenulata via integrating bioactivity-guided fractionation, chemical profiling and biochemical assay Li-Juan Ma et al., Journal of Pharmaceutical Analysis, 2022 Powered by CURRENT ISSUE Codependencies of mTORC1 signaling and endolysosomal actin structures BY AMULYA PRIYA SANDRA ANTOINE-BALLY ET AL. Noninvasive morpho-molecular imaging reveals early therapy-induced senescence in human cancer cells BY ARIANNA BRESCI JEONG HEE KIM ET AL. Emergence of the obesity epidemic preceding the presumed obesogenic transformation of the society BY MADS MØLLER PEDERSEN CLAUS THORN EKSTRØM ET AL. TABLE OF CONTENTS ADVERTISEMENT Sign up for ScienceAdviser Subscribe to ScienceAdviser to get the latest news, commentary, and research, free to your inbox daily. SUBSCRIBE LATEST NEWS NEWS13 SEP 2023 The origins of the obesity epidemic may be further back than we thought NEWS13 SEP 2023 Prehistoric artists carved incredibly lifelike animal tracks SCIENCEINSIDER12 SEP 2023 CIA bribed its own COVID-19 origin team to reject lab-leak theory, anonymous whistleblower claims SCIENCEINSIDER12 SEP 2023 Panel calls for giant boost to space station research SCIENCEINSIDER12 SEP 2023 Scientific retractions may become easier to spot as Retraction Watch finds new partner NEWS11 SEP 2023 Red fire ants, a dreaded pest, have invaded Europe ADVERTISEMENT RECOMMENDED RESEARCH ARTICLEFEBRUARY 2015 Planetary boundaries: Guiding human development on a changing planet REVIEWFEBRUARY 2018 Climate, ecosystems, and planetary futures: The challenge to predict life in Earth system models LETTERDECEMBER 2012 Pushing the Planetary Boundaries REPORTJULY 2016 Has land use pushed terrestrial biodiversity beyond the planetary boundary? A global assessment ADVERTISEMENT View full textDownload PDF Skip slideshow NEWS All News ScienceInsider News Features Subscribe to News from Science News from Science FAQ About News from Science CAREERS Careers Articles Find Jobs Employer Profiles COMMENTARY Opinion Analysis Blogs JOURNALS Science Science Advances Science Immunology Science Robotics Science Signaling Science Translational Medicine Science Partner Journals AUTHORS & REVIEWERS Information for Authors Information for Reviewers LIBRARIANS Manage Your Institutional Subscription Library Admin Portal Request a Quote Librarian FAQs ADVERTISERS Advertising Kits Custom Publishing Info Post a Job RELATED SITES AAAS.org AAAS Communities EurekAlert! Science in the Classroom ABOUT US Leadership Work at AAAS Prizes and Awards HELP FAQs Access and Subscriptions Order a Single Issue Reprints and Permissions TOC Alerts and RSS Feeds Contact Us FOLLOW US GET OUR NEWSLETTER © 2023 American Association for the Advancement of Science. All rights reserved. AAAS is a partner of HINARI, AGORA, OARE, CHORUS, CLOCKSS, CrossRef and COUNTER. Science Advances eISSN 2375-2548. Terms of Service Privacy Policy Accessibility Reference #1",
    "commentLink": "https://news.ycombinator.com/item?id=37500752",
    "commentBody": "Earth beyond six of nine planetary boundariesHacker NewspastloginEarth beyond six of nine planetary boundaries (science.org) 178 points by geox 14 hours ago| hidepastfavorite150 comments Liquix 13 hours ago> Clearly, it is in humanity&#x27;s best interest to...Herein lies the problem. What happens to the environment is not governed by \"humanity&#x27;s best interest\", it&#x27;s governed by whomever has the most power and the biggest stick. These entities act in their own best interest.To the Chinese government, more factories = more money = more power = good. To the US government, more military equipment = more power = good. At the executive level of either organization one would be laughed out of the room for suggesting environmental issues should take priority over national security.But it&#x27;s not just two countries, it&#x27;s every single country making these types of decisions for 100+ years... And if $country doesn&#x27;t build that weapons facility or export that labor, $otherCountry will, therefore $country will be at a disadvantage. Repeat ad infitium.Those in power are far more concerned with maintaining and leveraging that power than they are with \"humanity&#x27;s best interest\". reply apsec112 12 hours agoparentThe vast majority of greenhouse gas emissions isn&#x27;t military, it&#x27;s for ordinary consumer uses like heating, cooling, driving, and farming meat. No matter what the form of government, lots of people will object very strongly if the solution to climate change is not having central heating, cars, hot water, or beef for dinner. reply neltnerb 10 hours agorootparentI think I would like to have safe housing, healthcare, nutritious food, and clean water more than \"driving and farming meat\" but I guess we all have our priorities.I think that can definitely be sustainable indefinitely, but not so long as companies and cultures are unable to make any kind of real concessions to reality. We should not be using plastic. For basically anything. We should never have started. We should barely be using cars. We should never have started. We should not be eating nearly as much meat as people do in rich countries. We&#x27;d use a fraction of the agricultural area and be healthier to boot. Yet somehow meat being cheap is a god-given right?I object to lots of things that reality forces me to deal with. reply Amezarak 9 hours agorootparentI was in full agreement with your comment until...> We should not be eating nearly as much meat as people do in rich countries.My peasant ancestors were eating meat every day 500 years ago. You can find different groups of people all over the world that are documented eating absurdly meat-heavy diets historically.Yes, at least some of us believe meat is very important for human flourishing. In strictly measurable health terms, I am extremely healthy, at least according to current medical science, so I&#x27;d rather not change my diet. reply tetris11 7 hours agorootparent> My peasant ancestors were eating meat every day 500 years ago. You can find different groups of people all over the world that are documented eating absurdly meat-heavy diets historically.That can&#x27;t be true, unless your ancestors have always been exceedingly wealthy. Most people&#x27;s food subsisted on things you would go outside of your house and forage&#x2F;barter for. Vegetables, grains, mushrooms, pulses and maybe some small game here and there.Pound for pound, the amount of meat eaten was substantially low, since what farmer could afford to sacrifice their larger livestock on a daily basis, as well as keep the carcass for potential customers in a time without refridgegeration.In the country where my parents grew up, this was their way of life. Small game you would eat maybe once a month, and a livestock would be sacrificed at the end of the year as a treat. reply Amezarak 29 minutes agorootparent> That can&#x27;t be true, unless your ancestors have always been exceedingly wealthy.I am honestly not sure where this meme came from. That was sometimes true, for very poor people in places where the population was too high for the amount of land that was seeing agricultural output. For example, this would have been true in parts of the American South amongst the kind of people that were coming down with pellagra, or during the Great Depression. It was manifestly not true during most times and for most people. You can study local diets and records and see it was quite usual to eat meat every single day as the core of at least one meal.> Pound for pound, the amount of meat eaten was substantially low, since what farmer could afford to sacrifice their larger livestock on a daily basis, as well as keep the carcass for potential customers in a time without refridgegeration.Of course people didn&#x27;t have refrigeration, but that didn&#x27;t mean that people threw all that meat they couldn&#x27;t finish away! They used smokehouses (and other methods, of course.) Even in my parents generation, before electricity was common in some areas, everyone had a smokehouse, where meat could be preserved basically indefinitely.One animal provides quite a lot of meat - for example, a single healthy bull can feed a family for about a year. Of course, it is true that people were not historically eating sirloin steaks every day - the most typical meat-centric dish was a long slow-cooking stew, and very little went to waste. Everyone was eating cuts most people have never touched today.> Small game you would eat maybe once a monthThis seems really uncommon. My friend for example used to eat squirrel every day for lunch. Without knowing anything, I would speculate they were from a country that went through some very hard times in their or their parents generation? reply fragmede 9 hours agorootparentprevSorry that saving the planet might cause minor inconvenience for you.There were far fewer factory farms and antibiotics 500 years ago, and 95% fewer people. What was sustainable then isn&#x27;t sustainable now. reply Amezarak 8 hours agorootparentIf the price is saving the planet is turning it into the dystopia we seem to be zooming towards full speed, I will be happy to not pay it and suffer the consequences of our civilization collapsing. I’m not changing my behavior and I will fight vigorously for it.If you stopped all meat production today it would not save you. (Post)industrial society itself is what is unsustainable. There is no way around that. There is no technological solution - we can barely maintain the pretense of keeping up with all the problems our tech causes. You can buy time (maybe), that’s all. Technological man cannot seem to grasp humility. There is no controlling the world. This article and some other comments go into some great detail.>might cause minor inconvenience for you.My current great health is not a “convenience.” It is an incredible blessing and I will fight to maintain the things that seemingly contribute to it. reply jacquesm 4 hours agorootparentIt&#x27;s not a binary choice, it&#x27;s a gradient and you decide to pick one of the extremes. Meat is a luxury, always has been for &#x27;ordinary&#x27; people. One day per week if you were lucky until sixty to seventy years ago or so. The exception: farm owners and the wealthy. reply Amezarak 18 minutes agorootparent> Meat is a luxury, always has been for &#x27;ordinary&#x27; people. One day per week if you were lucky until sixty to seventy years ago or so. The exception: farm owners and the wealthy.I guess we have different perspectives. For me, for most of human history, \"ordinary people\" were farmers (or hunters.)However, even among urbanites, \"once a week\" is probably too rare. For example, see this paper on the mid-Victorian diet:https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC2672390&#x2F;> Red herrings were a staple of the working class diet throughout the year because they were easily cooked (e.g. Idylls of the Poor). Other favourites were cheap and easily obtainable varieties with better keeping qualities than the more vulnerable white fish, including sprats, eels, and shellfish (oysters, mussels, cockles, whelks).> Consumption of meat was considered a mark of a good diet and its complete absence was rare: consuming only limited amounts was a poverty diet [23]. Joints of meat were, for the poor, likely to be an occasional treat. Yet only those with the least secure incomes and most limited housing, and so without either the cooking facilities or the funds, would be unlikely to have a weekly Sunday joint; even they might achieve that three or four times a year, cooked in a local cookhouse or bakery oven. Otherwise, meat on the bone (shin or cheek), stewed or fried, was the most economical form of meat, generally eked out with offal meats including brains, heart, sweetbreads, liver, kidneys and ‘pluck’, (the lungs and intestines of sheep). reply matthewmacleod 3 hours agorootparentprevHonestly, absolutely wild that you view “reduce meat intake to environmentally sustainable levels” as dystopian. Nobody’s saying you have to eat bugs for every meal. reply Amezarak 25 minutes agorootparentIt&#x27;s easy for me to see how my comment could be read that way, but I would consider that only a very small contributing factor to to what&#x27;s going wrong with society. Instead, read my reference to \"we can barely maintain the pretense of keeping up with all the problems our tech causes\" as having a lot to do with it, which even carbon-neutral will cause the destruction of ecosystems and continue the increase in cancer rates. We could also talk about the total loss of privacy, atomization, the obliteration of communities, anomie, the accelerating decline of democracy and increase in bureaucratic authoritarianism, the grim future we can expect from things like genetic engineering, etc. reply jacquesm 4 hours agorootparentprevYour peasant ancestors weren&#x27;t with 8 billion. It doesn&#x27;t scale. reply Amezarak 5 minutes agorootparentIndustrialized agriculture doesn&#x27;t scale either. We are rapidly destroying the fertility of the American midwest via intensive farming of corn&#x2F;canola&#x2F;soybean. Even ending a animal agriculture will only slow this down. What&#x27;s your solution?What would be sustainable is returning grazing animals to the Great Plains en masse to regenerate the rapidly-depleting topsoil originally built up by the American bison, but virtually nobody is talking about that because the entire focus is on one thing, CO2. maerF0x0 10 hours agorootparentprev> majority of greenhouse gas emissionsTo be extra precise, human initiated greenhouse gas emissions. My understanding is about half of the emissions are natural and there&#x27;s little we can do about their emission (but can do about their sequestration). Of course that doesn&#x27;t&#x2F;shouldn&#x27;t stop us from addressing the half we can :) reply amrocha 9 hours agorootparentWhat&#x27;s the point of being \"extra precise\" here? Aren&#x27;t you just giving deniers a talking point? reply knewter 10 minutes agorootparentYes, how dare we have a true conversation rather than fail to speak truths that make us uncomfortable or that give comfort to the positions of our ideological foes. reply ribosometronome 10 hours agorootparentprevDoes renewable power and hydrogen&#x2F;electric vehicles not solve essentially all of what you listed, save beef? reply Tanoc 8 hours agorootparentNope. The way we build things from the ground up has to change. Literally build things. Flat topped buildings made out of steel and concrete surrounded by pavement generate tons of heat that doesn&#x27;t get dissipated, which cascades into creating high pressure air bubbles around towns and cities that prevent rain from approaching. That cascades even further into affecting weather and creating droughts, because all the pavement prevents water from being absorbed by the ground and entering the water table. That water creates flash floods since the ground underneath the pavement is too dry to absorb it, creating a vicious cycle where the water table drops even further and the soil compacts and becomes even more hydrophobic.We have to stop building parking lots everywhere, stop building flat top highrises, stop using concrete and steel, build everything with roofs designed to absorb heat and transfer it into the ground instead of radiate it, tear up a good chunk of the highways, parking lots, and surface streets, plant tons of trees and broad leaf greenery to shield from heat with shade and absorb it, and refill the water tables by pumping water back into the ground. All of which costs a ton of money. And what we should do, damn the financial cost and those who decry it.Slapping a Mickey Mouse Band-Aid onto a gaping wound that needs seventy two stitches is the equivalent of what people suggest when they say \"Well I&#x27;ll just use renewable energy.\" reply gravitronic 9 hours agorootparentprevThere&#x27;s other significant issues, such as concrete.Read Bill Gates&#x27; \"how to avoid a climate disaster\" for a complete rundown, with receipts. reply midoridensha 7 hours agorootparentprev>No matter what the form of government, lots of *Americans* will object very strongly if the solution to climate change is not having central heating, cars, hot water, or beef for dinner.FTFY. Countless people outside the US get along just fine without cars, central heating, and the entire 1B+ nation of India is fine without beef for dinner. reply lazide 5 hours agorootparentFYI, India has a large Muslim population, and they eat beef on the regular. reply lastofthemojito 13 hours agoparentprev> At the executive level of either organization one would be laughed out of the room for suggesting environmental issues should take priority over national securitySupposedly some folks are starting to frame climate issues as threats to national security. It may just be lip service so far, but we&#x27;re now seeing \"Meet the Climate Crisis\" listed as one of the top 5 priorities for the US Department of Defense: https:&#x2F;&#x2F;www.defense.gov&#x2F;News&#x2F;Releases&#x2F;Release&#x2F;Article&#x2F;331664... reply bombcar 13 hours agorootparentI’m always extremely suspicious of “we care about climate charge” arguments that always argue for exactly what the group has always wanted to do. reply RcouF1uZ4gsC 10 hours agorootparentThe Green Party comes to mind. They pushed for the shuttering of German nuclear plants and ended up increasing CO2 emissions. reply PaulDavisThe1st 9 hours agorootparentThey pushed for the shuttering of something they considered (and consider) an existential risk to life on the planet, and believed that movement towards renewable electricity generation would occur relatively quickly (which is now starting to happen, but did not happen as fast as they believed it would).You can call it a failure of judgement, but it&#x27;s not what was claimed in the GP. reply ipnon 13 hours agorootparentprev“Hell hath no wrath like a national security apparatus threatened,” I think that’s how it goes. Once USG, or Wall Street even, is threatened nothing is really off the table. reply hinkley 10 hours agoparentprevAnd even that&#x27;s not true.It&#x27;s governed by the eyeballs of whoever has the most power and the biggest stick. We don&#x27;t, for instance, know what the actual rates of theft is in a civilization because we cannot accurately discern lost belongings from stolen belongings. Anyone who tells you they think they know the number is absolutely full of shit. A truly crafty thief makes you think you lost something, used it up, or threw it away. In which case there is no report, because as far as you know nothing happened. The better the criminals, the bigger the disparity between the reality of theft and the perception of theft.I can&#x27;t beat you with a stick if you&#x27;re dumping toxins somewhere I can&#x27;t see. And it&#x27;s no good beating your successors once you&#x27;re dead, or you once you&#x27;re very old, because there is no lesson to learn there except that you can get away with bad things for 30 years, so time your crimes accordingly. reply yoyohello13 11 hours agoparentprevI recently came across a game theory term called \"Moloch\" in which individual incentives lead to negative outcomes for all parties involved. Nobody individually wants the negative outcome, but it&#x27;s extremely difficult to break out of the cycle. reply svachalek 10 hours agorootparentI&#x27;ve also just learned this recently. See \"Meditations on Moloch\" for the original essay. It explains so much but it&#x27;s also very depressing -- it&#x27;s hard to imagine how any ordinary person has a prayer of changing the system when so many elements of it are primed to both produce terrible outcomes and to resist change.It&#x27;s possible that you could design a new system of government deliberately designed to counter some of these natural tendencies, to change the rules of the game to reward beneficial outcomes better than selfish ones. But I don&#x27;t know you&#x27;d ever get such a thing in place when the people who can make that kind of change are the ones with the most to lose. reply Smoosh 7 hours agorootparentIt is interesting that the Great Filter Theory usually concerns itself with issues such as nuclear weapons, or disease, but not the failure to overcome greed and self-interest, which, let&#x27;s face it, is how the modern world is structured. reply nostrademons 6 hours agorootparentprevI&#x27;m pretty sure SlateStarCodex coined the term, in reference to game theory. It&#x27;s originally from an Alan Ginsberg poem, and I&#x27;ve never seen it anywhere else in game theory literature.The game-theoretical term is a \"Nash equilibrium\", which is the state that reality ends up in if every participant follows their own incentives (i.e. no participant can improve their individual outcome by altering their decisions). It is possible - even very common in many real-life games - to end up with a negative Nash equilibrium. The classic example is the Prisoner&#x27;s Dilemma, where the globally-optimal solution is for neither player to defect, but each individual improves their own outcome by defecting, and so what actually happens is that both suspects confess.It&#x27;s also possible to achieve a positive Nash equilibrium out of morally-abhorrent individual choices. Mutually-assured destruction is a good example. The game is setup so that if either superpower launches their missiles, humanity ends. Therefore, both superpowers have an incentive to avoid nuclear war, and we&#x27;re still here. reply JumpCrisscross 13 hours agoparentprev> t&#x27;s governed by whomever has the most power and the biggest stick. These entities act in their own best interestEveryone acts in their own interests. The gilets jaunes were a grassroots protest against a gas tax increase. You see similar gas-station sticker-shock pressure exerted by voters in the United States.There are coordination problems to climate change. But it&#x27;s less a prisoner&#x27;s dilemma than a time-horizon problem: the fruits of a green transition won&#x27;t yield for decades. (Economies of scale help with this. Geopolitical decoupling gets in the way of that--this is the only domain where I see the prisoner&#x27;s dilemma that you allude to.) reply dopidop 13 hours agorootparentDigression :The Gillet jaune started this way. Very true. But also self organise to be something vastly different pretty quickly. Maybe 20 days into that months long movement ( it’s not officially over; like Korea war … ) reply gmuslera 13 hours agoparentprevAnother flavor of the Tragedy of the Commons. Everyone optimized by their local, short term priorities. reply PaulDavisThe1st 9 hours agorootparentAnd another opportunity for me to point out that there is no such thing as the Tragedy of the Commons. Garrett Hardin, who popularized the term, has walked it back, and others, as long ago as the 1990s, have pointed out with a wealth of evidence that the popular conception of \"the Commons\" never actually existed. More or less all \"pooled\" resources in human cultures have historically been carefully managed by a variety of cultural, social, political and economic systems. The so-called \"Tragedy of the Commons\" happens not because typical individuals over-use the resource, but because specific individuals violate norms, actively work to dismantle management processes and willfully work to utilize the resource for their own benefit to the detriment of others. reply littleweep 13 hours agoparentprevI guess until govts realize enviromental security is national security... reply mc32 12 hours agoparentprevHmmm, I think if we had similar (but roving, pastoral, etc) neolithic with population we do today (ok, doubtfully possible, but whatever carrying capacity), we&#x27;d never the less have resource fouling, resource depletion and we&#x27;d be sending soot into the atmosphere. reply account-5 12 hours agoparentprev> Repeat ad infitium.Unlikely since these entities all end when the environment is fucked. I don&#x27;t know the Latin for: approx next 100 years. reply BirAdam 9 hours agoprevThe one thing I don’t see people take into account is a globally declining birth rate, very rapid declining birth rate. Fewer humans will mean less consumption, and therefore lower demand and less production. This could reverse many effects within the lifetimes of millennials and GenZ. reply tuatoru 1 hour agoparentYes, fertility is below replacement in 183 out of 191 countries. True. But it depends.IPAT: impact = population times affluence times technology.Take away the 700 million people with the highest incomes (10%), then about 90% of the impact is gone. I doubt that population decline will be that swift or deep, though.Technology won&#x27;t save us either. If it isn&#x27;t deployed commercially at billion-dollar scale now, it won&#x27;t scale up enough in four decades to make much of a difference. reply dr_dshiv 13 hours agoprevI don’t know why this triggers such skepticism in me. Particularly the section on novel entities. But it feels funny, somehow, and I can’t put my finger on it. reply ipnon 13 hours agoparentI think the thought, like death, is simply so horrible it defies full consideration. This is why climate activists can seem fanatical, because it has a religious quality. Either you believe “it” will happen in our lifetime, very soon, or it will not happen at all, and it’s this horrifying, apocalyptic nature of the problem that causes this schism. If some of these climate scientists are right in their claims it is the most important thing for these activists to do. I’m sure we can all draw parallels here. reply SV_BubbleTime 12 hours agorootparentThere’s a lot of religion going on right now. Whatever that is in humans that encourages that, going back to tribal behavior, or survival or whatever, it hasn’t gone away just because formalized religions are weak right now. reply commandlinefan 13 hours agoparentprev> why this triggers such skepticismI know it does for me - because the people who claim to believe in it the most don&#x27;t actually live their lives as if they actually believed it. Don&#x27;t get me wrong, I&#x27;d be happy to do whatever I could reasonably do to create cooler summers and less smog, but I&#x27;ve never heard any actual concrete proposal beyond \"vote democrat\". reply vlz 9 hours agorootparentOk, I will bite. Concrete proposals for anybody regardless of political view:* Eat half the meat you do or less* Don‘t fly unless absolutely necessary* Get on a plan for renewable electrical energy if possibleI know, far from everything in this topic is about consumer choices but a lot really is. If you don‘t want the nanny state to fix your problems you can start at your own doorstep. reply lazide 5 hours agorootparentConsidering that will just result in a lower quality of life for me, and everyone else not even noticing (and not moving the needle even a little globally) - why?It’s like self flagellation in private. Maybe soul cleansing religiously or something, but utterly pointless in the real world. reply twobitshifter 9 hours agorootparentprevThe flying thing is weird. It’s really that you emit a lot because you traveled so far. It’s more efficient that driving per mile and slightly less efficient than rail (not sure the type)https:&#x2F;&#x2F;yaleclimateconnections.org&#x2F;2015&#x2F;09&#x2F;evolving-climate-...So it should be don’t travel so far and if you have to, take a train and if there’s no train, carpool, but fly rather than drive if you are traveling alone. reply amrocha 9 hours agorootparentMost people aren&#x27;t driving across the country every time they need to go from the east coast to the west coast. If planes didn&#x27;t exist these trips would also not exist. reply vlz 4 hours agorootparentprevThank you for the linked article! Yes that is an apt and necessary correction. reply dr_dshiv 3 hours agorootparentprevYeah, but not having children blows these out of the water. I mean, its not even on the same chart. reply labster 9 hours agorootparentprevIt’s expensive to eat less meat. And I done mean in an abstract way about the time it takes to prepare meals, etc.On a per weight basis, chicken is half the price of peanut butter or cheese at the local grocery store. A whole cooked chicken is even cheaper at Costco. Not everyone can afford to eat less meat and replace it with plant protein. reply vlz 3 hours agorootparentTrue, but I would like to add:1. If you can afford it, you should still reduce.2. The meat and dairy industries are heavily subsidized especially in developed countries. These prices are artificially low.[1]The subsidies could of course be shifted to human edible plants (instead of feeding corn&#x2F;maize) making plant-based diets more affordable.At the moment however few politician want to be connected to rising meat prices. If they were to get clear signals from their voters for shifting preferences of course that could also change.[1]: https:&#x2F;&#x2F;www.theguardian.com&#x2F;environment&#x2F;2021&#x2F;sep&#x2F;14&#x2F;global-f... reply midoridensha 5 hours agorootparentprevFair, but you can probably help save both money and the environment by cutting back on beef, and replacing it with chicken. Chicken is FAR more resource-efficient than beef. It&#x27;s too bad they haven&#x27;t figured out how to make it taste similar (of course, chicken doesn&#x27;t taste bad, it&#x27;s just really different, just like vanilla and chocolate ice cream taste different but one is not better than the other except in terms of personal preference). reply midoridensha 5 hours agorootparentprevYou completely left out the enormous impact that cars have. How about get rid of your car, move someplace walkable if you have to, and stop driving? reply vlz 4 hours agorootparentYou are right, that would certainly be a very big contribution and could have been listed as well. Relocating however is a very big step. While I actually don’t own a car myself and seldom needed one, I am hesitant to prescribe that to others as I know the situation outside the bigger cities gets unpleasant very fast when you are not mobile and relocating is a drastic step not available to everyone.In the area of personal mobility, change away from cars could only result from collective action, e.g. drastic improvements to public transport and less from personal choice. (EVs however or hybrids are also a real option for many.) reply pixl97 12 hours agorootparentprevIn general because the tragedy of the commons is solved by one group pointing a gun at everyone else. Nobody wants that, but at the end of the day that&#x27;s what we&#x27;re going to get if we like it or not.The longer we wait, the more people are going to get shot by that gun. reply deprecative 9 hours agorootparentprevOne of the reasons for this is because you&#x27;re not actually responsible for all that much as a middle class and below person. Yeah, get better fuel efficient cars. Yeah, eat less meat. Yeah, get the most out of what you can when you buy things. But that&#x27;s not going to do much while corporations are still chugging along. It won&#x27;t do much while coal and gas are the primary energy production. Sure, use less electricity and get onto renewables as much as you can but the offenders will buy these nonsensical carbon credits and such that cause issue.The real answer is definitely vote Democratic, one cannot vote democrat as that&#x27;s not a party in the US. But more than that it gets involved in the political process such that change to our policies are done with climate change in kind. You won&#x27;t get that from Republicans. reply yongjik 5 hours agorootparentprev> I&#x27;ve never heard any actual concrete proposal beyond \"vote democrat\".Trump called the global warming a \"Chinese hoax.\" If you&#x27;re looking for leverages to influence the global climate as a US citizen, \"vote democrat\" is not a bad strategy. (Have to add: it&#x27;s not because democrats are awesome, but because the alternative is horrible.) reply bottled_poe 10 hours agorootparentprevCurious! reply acqq 2 hours agoparentprevInstead of speaking of own feeling based on a \"section\" the rational approach is to read the actual scientific paper dealing only with that topic:https:&#x2F;&#x2F;pubs.acs.org&#x2F;doi&#x2F;10.1021&#x2F;acs.est.1c04158\"Outside the Safe Operating Space of the Planetary Boundary for Novel Entities\"Environ. Sci. Technol. 2022, 56, 3, 1510–1521 reply lucidguppy 8 hours agoprevWe are done falling off the cliff and are toes are starting to impact the ground.The best case scenario is for the powers that be to understand that there isn&#x27;t enough private island to stop what&#x27;s coming. They&#x27;re in the same boat with us no matter what. Nature always has the last say.We need to stop draw down of fossil resources, reduce our impact on the land, and seek the best possible outcome of a really bad situation. reply toss1 13 hours agoprevNote that the areas of the three largest deviations are, in order: Biosphere Integrity, Novel Entities (synthetic chemical & processes), and Biogeochemical Flows.The entire ecosystem on which we depend for sustenance is an extremely complex web of interlocking dependencies, from plankton to pollinators, to soil microbiota, to temperature & hydration, and so on, endlessly.This is the food web. If it collapses, we as a species are beyond fooked. Because it is so complex (and even something relatively simple such as CO2-driven greenhouse effect climate change is too complex for the lower half of the population to understand), it is barely even discussed.But make no mistake, the food web is under massive assault from all kinds of human activities (and even the artificial agriculture web is coming up against the hard limit of a phosphorus crisis). This is likely to be a sooner and more catastrophic failure than the climate crisis. The Fine Article nicely clarifies some of the threat. reply myshpa 12 hours agoparent> This is the food webOne major contributor to the overshoot is our current agricultural practices, with animal agriculture being a primary offender.It&#x27;s responsible for a significant amount of greenhouse gases, it&#x27;s a leading driver of biodiversity loss and deforestation, and it contributes to soil degradation and water pollution.https:&#x2F;&#x2F;ourworldindata.org&#x2F;land-usehttps:&#x2F;&#x2F;ourworldindata.org&#x2F;uploads&#x2F;2013&#x2F;10&#x2F;World-Map-by-Land...BIODIVERSITY LOSShttps:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-019-01448-4Humans are driving one million species to extinction - UN backed report finds that agriculture is one of the biggest threats to Earth’s ecosystemshttps:&#x2F;&#x2F;www.unep.org&#x2F;news-and-stories&#x2F;press-release&#x2F;our-glob...Our global food system is the primary driver of biodiversity losshttps:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;26231772&#x2F;Biodiversity conservation: The key is reducing meat consumptionDEFORESTATIONhttps:&#x2F;&#x2F;ourworldindata.org&#x2F;drivers-of-deforestationDrivers of Deforestation - combined, beef and oilseeds for animal feed account for nearly 60% of deforestationhttps:&#x2F;&#x2F;www.ucsusa.org&#x2F;resources&#x2F;whats-driving-deforestationJust four commodities — beef, soy, palm oil, and wood products—drive the majority of tropical deforestation.Beef - 2.71 million hectares &#x2F; yearSoy - 480,000 ha &#x2F; year (77% for animal feed)Palm Oil - 270,000 ha &#x2F; yearWood - 380,000 ha &#x2F; year (but probably more)GREENHOUSE GAS EMISSIONSCO2 - 16.5+% animal agriculture (https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;352100490_Emissions...)Methane - animal agriculture leading driver, https:&#x2F;&#x2F;ourworldindata.org&#x2F;emissions-by-sector#methane-ch4-e...N20 - animal agriculture leads again, https:&#x2F;&#x2F;ourworldindata.org&#x2F;emissions-by-sector#nitrous-oxide...SOLUTIONSWe can significantly reduce our footprint by adopting plant-based diets, reforesting the pastures (more than 50% of which were originally forests), and allowing biodiversity to recover.https:&#x2F;&#x2F;ourworldindata.org&#x2F;land-use-dietsIf the world adopted a plant-based diet we would reduce global agricultural land use from 4 to 1 billion hectareshttps:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41893-020-00603-4The carbon opportunity cost of animal-sourced food production on land - shifts in global food production to plant-based diets by 2050 could lead to sequestration of 332–547 GtCO2, equivalent to 99–163% of the CO2 emissions budget consistent with a 66% chance of limiting warming to 1.5 °Chttps:&#x2F;&#x2F;www.eurekalert.org&#x2F;news-releases&#x2F;917471Feeding 10 billion people by 2050 within planetary limits may be achievable reply jacquesm 10 hours agorootparentIn NL the agro lobby has just successfully launched the largest political party here, on the back of a very well financed and organized series of protests. These people will stop at absolutely nothing to be allowed to fuck us all over for a few more years just so they can rake in some more dough. It is very disturbing to see how gullible the voters are. reply analognoise 12 hours agorootparentprevHumanity would rather go to war to thin our numbers than give up eating meat.The only viable method is lab grown meat that tastes exactly the same, same texture, and is cheaper. That&#x27;s it. People have real stuff in their lives - they&#x27;re not worried about planetary survival in 100 years, they&#x27;re worried about next months rent, etc. reply jgraham 12 hours agorootparent> The only viable method is lab grown meatI suspect it&#x27;s true that the only way to substantially cut animal-based protein consumption at scale is some technological alternative that offers a comparable culinary experience but with a lower cost (and lower impact). Although as another poster notes, it&#x27;s also true that — like many resource consumption issues — the distribution isn&#x27;t flat, but shows the majority of consumption coming from a relatively small fraction of the population.However, what I wanted to point out is that \"lab grown meat\" is not the only possibility in this general area. Precision fermentation of proteins (e.g. https:&#x2F;&#x2F;gfi.org&#x2F;science&#x2F;the-science-of-fermentation&#x2F;) is another approach that seems to have potential. Although I note that I don&#x27;t have a specific horse in this race, and would be delighted to see anything that can reduce the overall environmental impact of food production at a global scale take off. reply mistrial9 12 hours agorootparentprevHindus and Buddhists have developed vegetarian diets alongside architectural and intellectual accomplishments, for more than a thousand years. reply myshpa 12 hours agorootparentprev> Humanity would rather go to war to thin our numbers than give up eating meatWe&#x27;re using up resources at a pace that outstrips Earth&#x27;s ability to replenish them. For instance, we&#x27;re already consuming 1.7 times the Earth&#x27;s available resources.https:&#x2F;&#x2F;www.overshootday.org&#x2F;If everyone were to adopt a diet similar to the American diet, we would require more than five Earths to sustain it.https:&#x2F;&#x2F;css.umich.edu&#x2F;publications&#x2F;factsheets&#x2F;sustainability...> The only viable method is lab grown meatAchieving the required scale and price point may take decades, potentially leading to the collapse of biodiversity long before that occurs.> that tastes exactly the same, same texture, and is cheaperIs our love for the taste really worth destroying the planet?What will we say to our grandchildren? \"Sorry, I couldn&#x27;t give up those burgers. Now, go play in a desert.\"https:&#x2F;&#x2F;www.independent.co.uk&#x2F;independentpremium&#x2F;uk-news&#x2F;mea...Veggie sausages and burgers up to ten times better for environment than meat, study finds> they&#x27;re worried about next months rentVegan diets are typically the most economical choice, even in first-world countries. The affordability of animal products is primarily a result of significant subsidies, without factoring in the negative externalities.https:&#x2F;&#x2F;www.ox.ac.uk&#x2F;news&#x2F;2021-11-11-sustainable-eating-chea...Sustainable eating is cheaper and healthier - Oxford study reply labster 8 hours agorootparentI don’t think our grandchildren are going to care what we say. They’ll be too busy scavenging the scorched ruins of our cities for the last cans of refrigerant so they can survive another month on our doomed planet. reply midoridensha 3 hours agorootparent>They’ll be too busy scavenging the scorched ruins of our cities for the last cans of refrigerant so they can survive another month on our doomed planet.Don&#x27;t be silly. Our grandchildren won&#x27;t need refrigerant to survive a hot Earth; they can just live underground. Caves stay cool year-round. Or they could dig a bunch of silos, 144 levels deep, and live in those, sending outcasts to the \"outside\" to die in the poisoned air. reply Asooka 11 hours agorootparentprevI definitely would not eat lab grown meat. My own meat consumption is less than 200g per day. I eat one hamburger a month at most, as a treat. I want it to be the best hamburger from the happiest free range cow ever. I&#x27;m not going to eat some synthetic imitation, I&#x27;ll just have bean stew with the occasional piece of chicken.The way the vegan movement has weaseled its way into the discussion on climate change has been frankly horrifying to watch. As you said, people will not stop eating meat. Tell them that going vegan is the only way and they&#x27;ll just say \"fuck the planet\" and not bother with it. Like, the worst that could happen is humanity going extinct. The Earth itself will be fine. Put that way, well, if we can&#x27;t figure out how to run our farms more optimally, we should go extinct. Chickens don&#x27;t contribute to atmospheric changes by themselves. reply ifdefdebug 9 hours agorootparent> My own meat consumption is less than 200g per dayYou think that is little? 200 g per day would be 80000 tons of meat for a 400m population (EU) - let&#x27;s generously account for the less hungry babies and we&#x27;d maybe have 60000 tons.How many cows and pigs are we talking about here daily?> The Earth itself will be fine.Sure. Earth will be fine even if we are hit by a megathingy that rips apart the entire lithosphere. Nature will be fine even if our sun blows up tomorrow. But what has that to do with anything?PS My own meat consumption is down to 2-3 meat dishes per month. Don&#x27;t know if that helps... reply drekk 8 hours agorootparentprevWho is people? I&#x27;ve been a vegetarian for half a decade now on account of a family history of cardiovascular disease. People lack discipline and imagination, but it&#x27;s not exactly rocket science to have a plant based diet. The largest obstacle is the large swathes of food deserts and the subsidizing of junk food.If people had to pay the real price of meat with environmental externalities factored in most people would consume at a level you&#x27;ve described or even less. My issue with that is it doesn&#x27;t address how most lifestyle emissions are attributable to the wealthiest economic class. It&#x27;s still a better alternative to the system we have now, which is clearly unsustainable.A 1&#x2F;3rd pound burger takes 660 gallons of water to produce, or about 2500 liters. That&#x27;s not even getting into any of the typical inputs like antibiotics which have their own collection of problems. Say what you will about vegans, it&#x27;s impossible to talk about the environment without bringing up the impacts of industrial agribusiness and factory farming.While we&#x27;re talking about efficiency, you gain an order of magnitude improvement by farming crops for humans to eat rather than farming crops for animals to eat for humans to eat. Only about 55% of the crops we go are directly consumed by humans, the rest is used for feed and biofuels. It would take a tenth of the land area to feed the same amount of people plant-based vs an omnivorous diet. We should absolutely be pushing for less centralized food production, and chickens are a useful part of that. They turn food waste into new food and deal with several pests. That&#x27;s very different from slaughtering them wholesale for cheap dinosaur nuggets. reply toss1 11 hours agorootparentprev>>they&#x27;re worried about next months rent, etc.If you are actually worried about next month&#x27;s rent, then going mostly vegetarian is FAR cheaper than eating beefPlus, only 12% of the people eat 50% of the beef in the US. That is a group of 50-65 year-old men. Change their habits to match the normal population (or let them age out) and 50% of the problem disappears.[0] https:&#x2F;&#x2F;phys.org&#x2F;news&#x2F;2023-08-mere-americans-nation-beef-sig... reply toss1 12 hours agorootparentprevPerhaps note that only 12% of Americans eat 50% of the beef. That group is men between 50-66 years old.The remaining half is spread among the other 88%. Convince that 12%, or let them age out, and half the problem goes away.[0] https:&#x2F;&#x2F;phys.org&#x2F;news&#x2F;2023-08-mere-americans-nation-beef-sig...Edit: Fixed link — Thx myshpa! (still looks truncated, but seems to work now) reply ifdefdebug 9 hours agorootparent> let them age outDoesn&#x27;t work. A long time ago I thought when all the old farts are gone, everything is going to be better. Had to have faith in my own generation right?You know what happened... Humanity just grew a new set of old farts. reply PaulDavisThe1st 9 hours agorootparentThis, and so much this. \"The hope of the world is in our generation\". No, it really isn&#x27;t. Any given generation is, statistically, a lot like every other generation (certainly over spans of 4-6 generations). reply midoridensha 3 hours agorootparentExactly: look at what happened with the Hippies in the 1960s. They turned into the \"Me Generation\" of the 1980s. reply labster 8 hours agorootparentprevSo basically we have no hope then? reply PaulDavisThe1st 8 hours agorootparentHope is fine. Hope based on \"the kids today will do right by the world\", not so much. reply labster 7 hours agorootparentSo hope based on eucatastrophe then? Works for me. reply PaulDavisThe1st 6 hours agorootparenthope based on (a) there are always some number of people at a given point in time working toward the goals you want to be hopeful about (b) there&#x27;s always some non-zero chance that they may be successful.but hope somehow contigent or predicated on the age of those people? nah. replyMountain_Skies 10 hours agorootparentprev>on a given dayThat conditional statement should set your skeptic sense to Defcon one. reply toss1 12 hours agorootparentprev>>Our agriculture and food production are major contributors to the damageYes - deadly serious.I had to travel to the US midwest recently, after not having been there for a long time. As I drove out of the city the first impression of the farmlands was pleasant. But after an hour of highway-speed driving past nothing but bare fields (out of growing season) with scant rows of trees, we became a bit horrified — there was absolutely zero habitat for anything but the artificial plantings, when they were in season. And they would be coated with pesticides to ensure that there were no insects, or anything that ate them, or that ate the things that ate the insects, etc.. And indeed, returning to the airport in daytime, there was remarkably little wildlife.It was a seriously disturbing experience, which I did not expect.>>Feeding 10 billion people by 2050 within planetary limits may be achievableYikes. The damage being done by 7 billion right now, adding 20-30% more is kind of unthinkable, even setting aside the agricultural damage. And you&#x27;re absolutely right that we need to adopt a plant-based diet.The conversion away from meat does seem much more doable, since only 12% of Americans eat 50% of the beef. Moreover, that&#x27;s men between 50-66 years old, so if younger generations don&#x27;t acquire that habit, we&#x27;ll get a 50% reduction just by that sub-population aging out.[0] https:&#x2F;&#x2F;phys.org&#x2F;news&#x2F;2023-08-mere-americans-nation-beef-sig... reply jacquesm 10 hours agorootparentWe&#x27;ve been at 8 billion for a while now, since the 15th of November last year to be precise. reply midoridensha 3 hours agorootparentprev>And they would be coated with pesticides to ensure that there were no insects, or anything that ate them, or that ate the things that ate the insects, etc.. And indeed, returning to the airport in daytime, there was remarkably little wildlife.Did you also notice a lack of dead insects on your windshield?Maybe you&#x27;re not old enough to remember, but several decades ago, any significant drive in most of the US would result in your windshield being coated in dead bugs. Not enough to cover it of course, but enough you&#x27;d have to run your washers and wipers sometimes, and use the scrubber at the gas station to clean your windshield (back in the full-service days, the attendant did this automatically for everyone, because they needed it).These days, you can take a long drive and get virtually no dead bugs at all. reply badrabbit 2 hours agoprevI feel like climate issues are best solved by anthropologists and diplomats.That aside, considering how much things improved during covid lockdowns, I&#x27;ve wondered if one of these climate failures killing a fee billion people will fix the whole problem on its own and have earth recover from the human parasite for a few more decades. reply readthenotes1 13 hours agoprevWhat a fantastic graphic and title! I do not believe I need to read the article to understand the point of it. Rarely have I seen such good visualization. reply musha68k 13 hours agoparentIt’s very telling yes:https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;sciadv.adh2458#F1 reply Acssux 13 hours agoprevHow is something critical if it has been transgressed and we are still alive? Or on the very first moment every single one is past we all suddenly die? reply pixl97 12 hours agoparentIf the engines on your plane fall off mid flight, everyone doesn&#x27;t die instantly. They die instantly after 30 thousand feet of screaming terror.This comment screams lack of education about complicated systems. For example take overpopulation in things like grazing mammals. The funny thing about being overpopulated is that it is not instantly deadly. Once the mammals eat enough of their food source that their food source cannot reproduce reliably the game is over. But they tend to have reserves of fat and muscle that last for some period of time. The weakest die first. Then the population reduces. But the population doesn&#x27;t go back to what was previously considered at the over population limit. No, populations massively collapse because there is nothing to eat at all. 9 out of 10 members of the population can die. And if it&#x27;s something like an island, extinction is on the menu. reply jacquesm 4 hours agorootparentEven if not on an island extinction is on the menu depending on how large the population was to begin with and the velocity of the crash. This is because the males in the population can&#x27;t reproduce by themselves and the weakest are often the young. So when food runs out, even if it is temporarily, you may see one last generation of very fit males and possibly some cannibalism and after that it is game over. Calhoun did some pretty gruesome (and borderline unethical) experiments that are the closest thing we have to lab controlled overpopulation experiments. The results presented are pretty sobering. reply elihu 13 hours agoparentprevThe Titanic didn&#x27;t sink the moment it hit the iceberg, it took a little over two and a half hours. During a significant portion of that time it wasn&#x27;t all that obvious that anything was seriously wrong. The ship was listing slightly to one side and the engines were off.The passengers weren&#x27;t all affected equally either. A lot of first-class passengers made it onto lifeboats, whereas third class passengers mostly didn&#x27;t.(The analogy breaks down a little in that we don&#x27;t have lifeboats and the collapse of our ecosystems probably won&#x27;t be as absolute and catestrophic as a ship sinking. The Earth&#x27;s ability to sustain large numbers of humans may decline significantly though, and a lot of things we take for granted now may be gone.) reply autoexec 12 hours agorootparentWe don&#x27;t have life boats (yet) but I do wonder if some part of the massive increase in wealth disparity we&#x27;ve seen is due to uncertainty about the future, or if the reluctance to take meaningful action to slow&#x2F;reverse the impacts we&#x27;ve had on the earth is because it&#x27;s already clear that our time is running out and there&#x27;s nothing that can be done to stop it. reply IKantRead 13 hours agoparentprevGive it some time. Good news is that \"faster than expected\" seems to be a very common saying among people studying these areas, so you might not have to wait as long as the geological time might suggest!Honestly, as someone who has been very concerned about climate for a while now, I&#x27;m surprised how much visible disruption of the climate system we&#x27;ve directly been able to observe. Earlier in my life I thought that, though dire, this was certainly a problem that would impact future generations but not so much ours. It turns out I might have been quite wrong on that front. reply tuatoru 1 hour agorootparentThe Club of Rome&#x27;s \"Limits To Growth\" study in the 1970s, in its central scenario, had the 2040s as the time when we&#x27;d start to really feel the consequences of \"pollution\" (broadly conceived, including things like fossil water drawdown and soil exhaustion as well as the results of using fossil fuels).A little faster than expected maybe. Not all that much though. reply jacquesm 12 hours agorootparentprevYou have a funny definition of &#x27;good news&#x27;. reply Borrible 2 hours agorootparentBad news, you will face it in your lifetime.Good news, you won&#x27;t face the time after. reply labster 8 hours agorootparentprevGood news often comes in suppository form these days. reply jacquesm 52 minutes agorootparentThere was a somewhat funny joke about the hunger winter here in NL: \"Kids I&#x27;ve got good news and bad news, the bad news is that we will eat flower bulbs, the good news is you can eat as much as you want\". reply lkbm 13 hours agoparentprevThe article says:> Boundary positions do not demarcate or predict singular threshold shifts in Earth system state. They are placed at a level where the available evidence suggests that further perturbation of the individual process could potentially lead to systemic planetary change by altering and fundamentally reshaping the dynamics and spatiotemporal patterns of geosphere-biosphere interactions and their feedbacksProbably would need to read more to get a clear understanding of exactly what they mean and how they&#x27;re defined, but it sounds like we&#x27;re to an unstable place. Perhaps somewhat analogous to skating on thin ice: you may not have haven&#x27;t broken through yet, but you&#x27;re in a spot where a break could happen at any moment and from any movement. reply haltist 13 hours agorootparentCalifornia&#x27;s climate patterns are already changing but the state has managed to deal with the problems and will probably continue to do so. Texas on the other hand is starting to see problems with their electricity grid during summers and it&#x27;s going to keep getting worse as temperatures continue to rise. reply autoexec 12 hours agorootparentCalifornia&#x27;s \"dealing with the problems\" seems like a lot of shortsighted non-sustainable policy. Parts of CA are burning right now. At this point they&#x27;ve been bragging about maybe not having to go back to rolling blackouts. Reliably providing even the most basic services like water and power is such an astonishingly low bar that only in the poorest developing nations should that even be in question yet here we are. Long term, I don&#x27;t see Texas or California holding up very well to climate change. reply haltist 12 hours agorootparentWhat states do you think will manage to deal with climate change if not California? reply autoexec 11 hours agorootparentI know there have been models to predict which areas of the country are expected to be most&#x2F;least impacted by climate change but I&#x27;m not qualified to judge them. I suspect that increasing heat and desertification will leave much of the southern and western US in very bad shape. Anywhere prone to flooding now will only have it worse. The coastal areas will also deal with flooding and storms in increasing frequency&#x2F;severity.Maybe some of the northern flyover states would be best? Higher land around the great lakes for example? If I were looking to buy up some land today I&#x27;d even consider Canada, but only after the fires have died down. The only nice thing about wildfires burning 40 million acres of forest to the ground is that it&#x27;ll be a while before there&#x27;s enough fuel for it to happen again. reply haltist 11 hours agorootparentSo it sounds like you don&#x27;t think most of the US will adapt. reply autoexec 11 hours agorootparentI&#x27;d have more faith if I saw a lot more effort being expended. It seems like most places are still pretending nothing will change. People are still buying property that regularly floods even while insurance companies are refusing to cover them. States are selling off their clean drinking water to industries that will waste and pollute it. Important infrastructure is left outdated and crumbling. Environmental protections are being rolled back, fracking and drilling continue.At this rate, it&#x27;s looking pretty rough. replyhnburnsy 11 hours agorootparentprevOne is losing residents and one is gaining. Could that partially explain what you descibed? reply haltist 11 hours agorootparentSeems like that should be even more reason for Texas to upgrade their energy infrastructure like California. reply SV_BubbleTime 12 hours agorootparentprevMy man, this comment is so wildly antiscience. I don’t know where to begin.I’m not sure if it would be with forest management, or the complexity and reasons that cut Texas has its own grid, separated from everyone else. I’m not sure I would say either directly applicable to this topic. reply autoexec 12 hours agorootparentTexas throwing a tantrum and refusing to have their power grid under federal regulation will become increasingly applicable to the topic since the changes we&#x27;re causing to our planet and its climate will put even more strain on their weak and inflexible power grid. People in texas are already dying from the heat in the summer and freezing to death in the winter and it&#x27;s only going to get worse. reply ricardobeat 13 hours agoparentprevThis is looking at changes over a scale of centuries, and the potentially irreversible effects of what we are doing right now.> Had Earth system remained forced by 1988 conditions (350 ppm and 85%&#x2F;50%&#x2F;85% of tropical&#x2F;temperate&#x2F;boreal forest cover remaining), the simulations show that temperature over the global land surface would not have increased by more than an additional 0.6°C in the subsequent 800 years> If climate and land system change can be halted at 450 ppm and forest cover retained at 60%&#x2F;30%&#x2F;60% of boreal&#x2F;temperate&#x2F;tropical natural cover, then the simulation indicates a mean temperature rise over land of 1.4°C by 2100 (in addition to 0.7°C between preindustrial time and 1988) and 1.9°C after 800 years as vegetation evolves in a warmer climateThe latter is an optimistic projection assuming we will do more to stop climate change, the paper goes further into the odds of a >3C increase.To put that in context, a 1.5°C increase in average temperature is considered a doomsday scenario where wildfires and storms ravage the earth, killing over half of the global population. reply paint 13 hours agoparentprevEchoing the kind of climate change scenario 5 year old kids have seems kind of cruel the week thousands of people in Libya died of events that are exasperated by the climate crisis. People are already dying of climate related causes, like crop failures and natural distasters. reply swader999 13 hours agorootparentMan-made dams broke. reply paint 13 hours agorootparentWhat you are saying is technically not wrong, but misleading, as it omits the fact the dams broke due to a natural disaster, Storm Daniel, which previously also \"affected Greece, Bulgaria, and Turkey with extensive flooding.\".https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Storm_Daniel reply swader999 13 hours agorootparentNo kidding, but to say that climate change did this when hurricanes have been on the decline the last thirty years is misleading. This is weather and engineering failures. https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41467-021-24268-5 reply Eduard 12 hours agorootparent> No kidding, but to say that climate change did this when hurricanes have been on the decline the last thirty years is misleading. This is weather and engineering failures.> https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41467-021-24268-5This HN user categorically and obsessively has their own opinion on climate change, and how it is influenced by humans.Their \"evidence research\" is taking the first Google search result that they believe to support their position in the discussion at hand - even if it doesn&#x27;t make sense (hurricanes in North America versus Mediterranean).If they don&#x27;t invest in such sloppy research, their arguments hit rock-bottom Tiktok factoid niveau. one can better just assume nonsense&#x2F;randomizing&#x2F;lying and ignore this user for anything climate-related. reply swader999 8 hours agorootparentWell forget studies then, look at the data: http:&#x2F;&#x2F;tropical.atmos.colostate.edu&#x2F;Realtime&#x2F;index.php?arch&...Use the drop down and look at accumulated cyclone energy. There&#x27;s no trend in 42 years and last year was the third lowest total tropical cyclone energy for earth in the last 42 years. This year seems on track to be very low for cyclone energy too.Costs in terms of percent of GDP are falling: https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;abs&#x2F;10.1080&#x2F;17477891.2018.15...And most importantly, death rates for weather are drastically reduced over the last hundred years: https:&#x2F;&#x2F;ourworldindata.org&#x2F;grapher&#x2F;number-of-deaths-from-nat... reply swader999 6 hours agorootparentEdit, crickets? C&#x27;mon parent, what do you say to this data? reply carlosjobim 9 hours agorootparentprevDo you understand how creepy it is of you to go digging into other users comment history and posting about it when there&#x27;s a disagreement on a topic? Maybe let the arguments speak for themselves? We&#x27;re not supposed to be some kind of commissars looking for dissidents or witch hunters looking for heretics. reply dredmorbius 7 hours agorootparentAnd why should commenting and posting history count any less than a specific comment?That seems to be ... integral to the function of the site itself:Reputation and patterns of behavior matter. reply swader999 6 hours agorootparentWell I guess I&#x27;m guilty of posting my own opinion on various aspects of climate change along with links from time to time. Is that evil in your books, should I be banned? I think you can go back to every post I&#x27;ve ever made and not find any cases of ad hominem attacks. I&#x27;m satisfied with that.Edit, yup, it was poor maintenance on the dam as indicated in 2021: https:&#x2F;&#x2F;sebhau.edu.ly&#x2F;journal&#x2F;jopas&#x2F;article&#x2F;view&#x2F;2137 reply dredmorbius 6 hours agorootparent replyyoung_breezy 13 hours agorootparentprevRight? The climate is fine as long as you don’t depend on man made objects reply Cerium 13 hours agoparentprevBreathing is critical to keep me alive, but I can hold my breath for a minute, and if forced by external means to stop breathing, will still be ok for a few minutes. reply netsharc 13 hours agoparentprevHa, good point, let&#x27;s not focus on the main issue and instead argue semantics shall we... &#x2F;sTo ELI5 like you seem to prefer, the iceberg breaching the hull of the Titanic seems like a critical transgression, but the ship stayed afloat another hour or so... reply scrozier 13 hours agorootparentThey are not arguing semantics, they are getting to the very premise&#x2F;context of the paper. reply diogenes4 13 hours agorootparentprevIf the main issue is not semantic, how would you characterize it? reply JumpCrisscross 13 hours agoparentprev> How is something critical if it has been transgressedI don&#x27;t think the paper claims we&#x27;ve passed criticality. Instead, it talks of boundaries and risk, the latter reflecting that we don&#x27;t know where the critical points are.> on the very first moment every single one is past we all suddenly die?Biosphere collapse could happen suddenly and without warning. That would throw the global south into political turmoil while prompting a global and destabilizing refugee crisis. reply chx 13 hours agoparentprevI&#x27;d recommend watching Margin Call, a movie set during the 2008 financial collapse. reply myshpa 12 hours agoparentprevhttps:&#x2F;&#x2F;www.stuartmcmillen.com&#x2F;comic&#x2F;st-matthew-island&#x2F;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ecological_overshoothttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Carrying_capacity#Planetary_Bo... reply hotpotamus 13 hours agoparentprevIt reminds me a bit of radiation poisoning - you can receive a lethal dose in moments. After that, you can carry on for a time; hours, maybe days feeling normal, but your death has already been ordained and your biochemical systems will collapse from the insult. reply musha68k 12 hours agorootparentHokuto Shinken self-application; all the neuralgic points, and with determination and lots of force. reply Iridescent_ 13 hours agoparentprevIf you have a pond with 1000 fishes, and each year the population of fish doubles, you can remove 501 fishes&#x2F;year for quite a while. You are clearly in an unsustainable situation, yet it will take time for the population to completely collapse. These limits are in the sense that we cannot stay above forever, not in the we cannot go above ever sense. reply isotropy 12 hours agorootparentWell, only for 8 years.1000 998 994 986 970 938 874 746 490 ooops reply PaulDavisThe1st 9 hours agorootparentIf you harvest after the reproductive cycle though:1000 1499 2497 4493 ...it pays to be be patient! reply ricardobeat 11 hours agorootparentprevA very insightful misguided comment, shows how easy it can be to disturb a system in equilibrium with very minor changes (1 extra fish). reply simmerup 14 hours agoprevnext [2 more] [flagged] swader999 13 hours agoparentOr shade and stay hydrated. Cold kills orders of magnitude more than heat does. reply SinParadise 13 hours ago[flagged]| prevnext [2 more] Its OK magic of technology will swoop in and save us. Any time now. reply bvrlt 13 hours agoparentAnd Apple will be carbon neutral in 2030. reply formvoltron 13 hours agoprevpretty sure we have enough solar and batteries to power the world. Sure ok we could use nuclear in some places too. aerosols... that&#x27;s from burning fossil fuels, no? fake meat will go a long way to reclaiming farmland.micro &#x2F; nano plastics everywhere sucks. going to have to think hard about how to fix that one.can we take cell samples of all the species just in case we need to conjure them up again? (or is at least one required to \"boot\") reply dopidop 13 hours agoparentIt’s satire right ? reply formvoltron 14 minutes agorootparentno. what part would be satire? reply naikrovek 14 hours ago[flagged]| prevnext [3 more] yeah, yeah, we know.there&#x27;s money to be made, though, so we ain&#x27;t stopping. reply readthenotes1 13 hours ago[flagged]| parentnext [2 more] It&#x27;s more like there are babies to be made. After all, if we stopped procreating so prolifically, things would right themselves fairly quickly reply casparvitch 13 hours agorootparentOne and the same, economic growth has lead us here & economic growth has been highly tied to population growth. reply thriftwy 10 hours agoprev [–] This late summer was pretty great around here. We were able to go swimming on 10th of September, whereas the folk wisdom here tells is&#x27;s a no-no after 2nd of August. reply maerF0x0 10 hours agoparent [–] Meanwhile Austin has has the 3rd hottest[1] year on record (so far, we still have ~3.5 months to go). https:&#x2F;&#x2F;www.extremeweatherwatch.com&#x2F;cities&#x2F;austin&#x2F;yearly-day...[1]: i used days of 100f and over as a proxy for deciding how hot a year is reply swader999 9 hours agorootparent [–] That airport station wasn&#x27;t around for the entire record though. 20&#x27;s was similar to present. I really don&#x27;t think you can make any claims either way with just that data. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The update to the planetary boundaries framework reveals that six out of nine boundaries have been exceeded, suggesting that the Earth is currently outside the secure range for human operation.",
      "The text introduces a concept called \"novel entities\" that could potentially disrupt core boundaries, and emphasizes the need for transformations to enhance production impacts and regulate demand.",
      "It highlights the human activities' impacts on the ocean and terrestrial ecosystems, including the effect of deforestation and climate change on carbon stocks and global land temperature, and underscores an urgent need for action to prevent irreversible environmental damage."
    ],
    "commentSummary": [
      "The passage debates the delicate balance between environmental concerns and economic or national security interests, particularly on the topic of climate change.",
      "The impact of meat consumption on climate change is discussed, emphasizing the need for sustainable food production and adopting plant-based diets.",
      "Technological solutions, use of renewable energy sources, and effects of temperature extremes on survival rates are also covered as potential issues and remedies in managing the environment."
    ],
    "points": 178,
    "commentCount": 147,
    "retryCount": 0,
    "time": 1694633765
  },
  {
    "id": 37502329,
    "title": "Hutter Prize for compressing human knowledge",
    "originLink": "http://prize.hutter1.net/",
    "originBody": "500'000€ Prize for Compressing Human Knowledge (widely known as the Hutter Prize. Total payout so far: 23'034€) Compress the 1GB file enwik9 to less than the current record of about 114MB The Task Motivation Detailed Rules for Participation Previous Records More Information Discussion forum on the contest and prize History Committee Frequently Asked Questions Contestants Links Disclaimer News: Saurabh Kumar is the sixth Winner! Congratulations! ... the contest continues ... Being able to compress well is closely related to intelligence as explained below. While intelligence is a slippery concept, file sizes are hard numbers. Wikipedia is an extensive snapshot of Human Knowledge. If you can compress the first 1GB of Wikipedia better than your predecessors, your (de)compressor likely has to be smart(er). The intention of this prize is to encourage development of intelligent compressors/programs as a path to AGI. Interview with Lex Fridman (26.Feb'20) (Video, Audio, Tweet) The Task Losslessly compress the 1GB file enwik9 to less than 114MB. More precisely: Create a Linux or Windows compressor comp.exe of size S1 that compresses enwik9 to archive.exe of size S2 such that S:=S1+S2 8.851%| >5'000€ Marcus Hutter Saurabh Kumar 16.Jul 2023 fast cmix 114'156'155 8.768.4GB43h 1.04%| 5'187€ Marcus Hutter Artemiy Margaritov 31.May 2021 starlit ... 115'352'938 8.6710GB~50h 1.1%9000€ Marcus Hutter Alexander Rhatushnyak 4.Jul 2019 phda9v1.8 ... 116'673'681 8.586.3GB~23h --pre-prize - Author (enwik8) Date Decompressor Total Size Compr.Factor|RAM|time %Award Sponsor Alexander Rhatushnyak 4.Nov 2017 phda9 ... 15'284'944 6.541048MB~5h 4.17%| 2085€ Marcus Hutter Alexander Rhatushnyak 23.May 2009 decomp8 ... 15'949'688 6.27936MB~9h 3.2%| 1614€ Marcus Hutter Alexander Rhatushnyak 14.May 2007 paq8hp12 -7 16'481'655 6.07936MB9h 3.5%| 1732€ Marcus Hutter Alexander Rhatushnyak 25.Sep.2006 paq8hp5 -7 17'073'018 5.86900MB5h 6.8%| 3416€ Marcus Hutter Matt Mahoney 24.Mar.2006 paq8f -7 18'324'887 5.46854MB5h --pre-prize - More Information Discussion forum on the contest and prize Compression benchmarks enwik9 and others Motivation of compressing the Human Knowledge Information about the enwik9 data file Wikipedia on the Hutter Prize History 13??: William of Ockham's razor: Entities should not be multiplied beyond necessity. 1964: Ray Solomonoff introduced algorithmic probability for universal prediction. 1996: Leonid Broukhis introduced the first compression competition with a prize. 2000: Marcus Hutter introduced a compression based universal intelligent agent. 2005: Jim Bowery proposed a larger scale compression contest based on the Wikipedia corpus. 2006: Matt Mahoney compressed Wikipedia with many state of the art compressors. 2006: Marcus Hutter launched the 50'000€ prize. 2006-2017: Alexander Rhatushnyak is 4-times winner of the HKCP. 2020: Marcus Hutter launched the 500'000€ prize. 2021: Artemiy Margaritov is the first winner the 10x HKCP. Committee Jim Bowery: verification of claims, public relations, finding sponsors, newsgroups, etc. Matt Mahoney: running the compression competition. Marcus Hutter: arbiter, current sponsor, and manager of prize fund. Frequently Asked Questions (FAQ) What is this contest about? Is the compression contest still ongoing? (YES) Why did you grant a temporary relaxation in 2021 of 5'000 Byte per day? Where do I start? How do I develop a competitive compressor? What is (artificial) intelligence? What does compression has to do with (artificial) intelligence? What is/are (developing better) compressors good for? The contest encourages developing special purpose compressors Why lossless compression? I have a really good lossy compressor. (How) can I participate? Why aren't cross-validation or train/test-set used for evaluation? Why is (sequential) compression superior to other learning paradigms? Why is Compressor Length superior to other Regularizations? How can I achieve small code length with huge Neural Networks? Batch vs incremental/online/sequential compression. Why don't you allow using some fixed default background knowledge data base? Why is \"understanding\" of the text or \"intelligence\" needed to achieve maximal compression? Why do you focus on text? What is the ultimate compression of enwik9? Why recursively compressing compressed files or compressing random files won't work Can you prove the claims in the answers to the FAQ above? The PAQ8 compressors are hard to beat There are lots of non-human language pieces in the file Why include the decompressor? Why do you require submission of the compressor and include its size and time? Why not use Perplexity, as most big language models do? Why did you start with 100MB enwik8 back in 2006? Why did you go BIG in 2020? Why are you limiting (de)compression to less than 100 hours on systems with less than 10GB RAM? Why do you restrict to a single CPU core and exclude GPUs? The total prize is not exactly 500'000€ Is 1GB 2^30 byte or 10^9 byte? The website looks dated Why do you require Windows or Linux executables? Why do you require submission of documented source code? Where can I find the source code of the past winners and baseline phda9? Under which license can/shall I submit my code? What if I can (significantly) beat the current record? How can I produce self-contained or smaller decompressors? Is Artificial General Intelligence (AGI) possible? Is Ockham's razor and hence compression sufficient for AI? The human brain works very differently from (de)compressors I have other questions or am not satisfied with the answer Contestants and Winners for enwik8 So far we have received the submissions below for enwik8. Each is/was open for public comment and verification for 30 days before an award decision will be/was made. Comments should be made to the Hutter Prize Discussion Forum or by email to members of the Prize committee. Date Author Decompressor Compression Options Size of archive Size of decompr. Total Size %Improve 1-S/L Compr. Factor Bits/ Char Memory Time Note 4.Nov'17 Alexander Rhatushnyak phda9 compressed_enwik8 enwik8 15'242'496 42'448 15'284'944 4.17% 6.54 1.225 1048MB ~5h Meets all prize criteria. Fourth winner! 23.May'09 Alexander Rhatushnyak decomp8 archive8.bin enwik8 15'932'968 16'720 15'949'688 3.2% 6.27 1.278 936MB ~9h Meets all prize criteria. Third winner! 22.Apr'09 Alexander Rhatushnyak decomp8 archive8.bin enwik8 15'970'425 16'252 15'986'677 3.0% 6.26 1.279 924MB 9h 3.0% improvement over new baseline paq8hp12 14.May'07 Alexander Rhatushnyak paq8hp12 -7 16'381'959 99'696 16'481'655 3.5% 6.07 1.319 936MB 9h Meets all prize criteria. Second winner! ... \" ... ... ... ... ... ... ... ... ... ... ... 6.Nov'06 Alexander Rhatushnyak paq8hp6 -7 16'731'800 170'400 16'902'200 1% 5.92 1.352 941MB 5h 1% improvement over new baseline paq8hp5 25.Sep'06 Alexander Rhatushnyak paq8hp5 -7 16'898'402 174'616 17'073'018 6.8% 5.86 1.366 900MB 5h Meets all prize criteria. First winner! 10.Sep'06 Alexander Rhatushnyak paq8hp4 -7 17'039'173 206'336 17'245'509 5.9% 5.80 1.380 803MB 5h Superseded by paq8hp5 3.Sep'06 Alexander Rhatushnyak paq8hp3 -7 17'241'280 178'468 17'419'748 4.9% 5.74 1.394 742MB 5h Superseded by paq8hp4 28.Aug'06 Alexander Rhatushnyak paq8hp2 -7 17'390'460 205'276 17'595'736 4.0% 5.68 1.408 747MB 5h Superseded by paq8hp3 21.Aug'06 Alexander Rhatushnyak paq8hp1 -7 17'566'769 206'764 17'773'533 3.0% 5.63 1.422 748MB 5h Superseded by paq8hp2 20.Aug'06 Alexander Rhatushnyak paq8hkcc -7 17'597'599 244'224 17'841'823 2.6% 5.61 1.43 747MB 5h Superseded by paq8hp1 16.Aug'06 Dmitry Shkarin durilca0.5h -m1650 -o21 -t2 17'958'687 86'016 18'044'703 1.5% 5.54 1.444 1650MB 30min Fails to meet the reasonable memory limitations 16.Aug'06 Rudi Cilibrasi raq8g -7 18'132'399 34'816 18'167'215 0.9% 5.50 1.453 1089MB 7h Fails to meet the 1% hurdle, and others 24.Mar'06 Matt Mahoney paq8f -7 18'289'559 35'328 18'324'887 0% 5.46 1.466 854MB 5h Pre-prize baseline The time for decompression/compression is estimated for a 2GHz P4 till 2010 and for a 2.7GHz i7 since 2017. The percent (%) improvement is over the baseline previous record. More details on the (de)compressors can be found here. Apr-Nov'17: Alexander Rhatushnyak submits another series of ever improving compressors based on phda9, with the final one on 4.Nov'17 improving over his previous record by over 4%! Sep'07-...: Alexander Rhatushnyak submits another series of ever improving compressors. Is there nobody else who can keep up with him? Nov'06-May'07: Alexander Rhatushnyak submits another improved series of (de)compressors paq8hp6-12 (option -7). On 14.May 2007 he submits paq8hp12 It achieved an improvement of 3.5% over the new baseline paq8hp5 and was finally confirmed as the second winner on 30.June 2007. Congratulations! A detailed description of paq8hp12 can be found here. Most of the time in developing paqhp6-12 went into planning and performing experiments, and studying and understanding the results of these experiments. Alexander Rhatushnyak's current occupation is in software engineering. For him data compression is science and art and sport all together. This was his motivation for participating in the contest. Dr. Rhatushnyak was born in the Siberian Scientific Center (www.nsc.ru), studied data compression and related algorithms since 1991, and graduated from the Moscow State University (www.msu.ru) in 1996. After his PhD in 2002 he lived and worked in various places in the world. Aug-Sep'06: Alexander Rhatushnyak of the Moscow State University Compression Project submits an improving series of (de)compressors paq8hp? (option -7), modifications of paq8h with a custom dictionary built from enwik8 and other improvements. Przemyslaw Skibinski contributed to earlier versions. On 25.Sep.2006 Alexander Rhatushnyak submits paq8hp5. It achieved an improvement of 6.8% over the baseline paq8f and was finally confirmed as the first winner on 25.Oct.2006. Congratulations! A detailed description of paq8hp5 can be found here. 16.Aug'06: Dmitry Shkarin submits a modification of (de)compressor durilca (option -m1650 -o21 -t2), a modification of ppmd/ppmonstr with filters for text, exe, and data with fixed length records. 16.Aug'06: Rudi Cilibrasi submits (de)compressor raq8g.cpp (option -7), a modification of paq8f with additional text modeling. Links (Further Information/Discussion/News) Core Resources: Wikipedia: Hutter Prize Large Text Compression Benchmark Interview on Intelligence & Compression & Contest (10min, video) Presentation by past winner Alex Rhatushnyak Kolmogorov complexity = the ultimate compression Universal Artificial Intelligence (book, 45min/1.5h/3h lecture) Interview on Universal AI with Lex Fridman (1.5h) Further Recommended Technical Reading relevant to the Compression=AI Paradigm: Franz&al. (2021) A theory of incremental compression Zenil (2020) Compression is Comprehension, and the Unreasonable Effectiveness of Digital Computation in the Natural World Yogatama&al. (2019) Learning and Evaluating General Linguistic Intelligence Zenil&al (2019) Causal deconvolution by algorithmic generative models (3min video) Everitt&Hutter (2018) Universal Artificial Intelligence: Practical agents and fundamental challenges Mattern (2016) On Statistical Data Compression Mahoney (2011) Data Compression Explained Rathmanner&Hutter (2011) A Philosophical Treatise of Universal Induction (slides, recordings) Salomon&Motta (2010) Handbook of Data Compression Janzing&Schölkopf (2010) Causal Inference Using the Algorithmic Markov Condition Hernandez-Orallo&Dowe (2010) Measuring Universal Intelligence: Towards an Anytime Intelligence Test Mahoney (2009) Rationale for a Large Text Compression Benchmark (and further references) Hutter (2007) Universal Algorithmic Intelligence: A Mathematical Top→Down Approach (slides, recordings) Schmidhuber (2007) The New AI: General & Sound & Relevant for Physics Cilibrasi&Vitanyi (2005) Clustering by Compression Wallace (2005) Statistical and Inductive Inference by Minimum Message Length Sanghi&Dowe (2003) A Computer Program Capable of Passing I.Q. Tests Catoni (2001) Statistical Learning Theory and Stochastic Optimization Hutter (201X) Recommended books & Courses for (Under)Graduate Students Post-2020 Discussion (enwik9,€500k): Twitter Announcement (2023) Progress on the Human Knowledge Compression front HKCP GG Announcement (2023) Saurabh Kumar's fast-cmix wins €5187 Hutter Prize Award! Slashdot Informal Discussion (2023) Sixth 'Hutter Prize' Awarded for Achieving New Data Compression Milestone Mike James Article (2023) Hutter Prize Awarded Again Research Snipers Article (2023) New Record Set In Data Compression Slashdot Informal Discussion (2021) New Hutter Prize Winner Achieves Milestone for Lossless Compression of Human Knowledge Mike James Article (2021) New Hutter Prize Milestone For Lossless Compression Discussion (2021) on News YCombinator Analytics India Magazine Article (2020) Compress Data And Win Hutter Prize Worth Half A Million Euros Mike James Article (2020) Hutter Prize Now 500,000 Euros Reddit News (2020) 500,000€ Prize for distilling Wikipedia to its essence Pre-2020 Discussion (enwik8,€50k): Language Modelling on Hutter Prize Discussion in the AGI mailing list Discussion in the Hutter-Prize mailing list Technical Discussion in the Data Compression Forum encode.su Discussion in Yahoo Group ai-philosophy Informal Discussion at Slashdot (13.Aug'06, 29.Oct'06, 10.Jul'07, 21.Feb'20) In the Online Heise News In the KurzweilAI.net News In Mark Nelson's blog (24.Aug'06) O'Reilly Radar (29.Sep'06) Discussion at the Accelerating Future page In the ebiquity news In WebPlanet News in Russian Wissenschaft-Wirtschaft-Politik, Ausgabe 34/2006 (22.Aug'06) Discussion at Newsgroup comp.ai.nat-lang Discussion at Newsgroup comp.compression and here Discussion at Newsgroup comp.ai Prediction market as to when enwik8 will be compressed to Shannon's estimate of 1 bit per character many other sites Warning: The average quality of the posts in the discussion groups and mailing lists is very low. Most participants don't know the underlying scientific concepts and some have not even read the rationale behind the contest. For a cleaned summary consult the frequently asked questions. The competition was also announced or discussed in many blogs. Disclaimer: Copying and distribution of this page (http://prize.hutter1.net) is permitted, provided the source is cited. The prize will be paid if the solution reflects the spirit of the contest. In particular decompressors (secretly) receiving any kind of \"outside\" information are forbidden. Also in order to verify your claim we need to be able to run your executable on our machines within reasonable space and time constraints. This is a privately run and funded contest. Payment of the prize cannot be legally enforced. The smallest claimable prize is 5'000€. After an award, the prize formula (L) will be adapted. Rules may change at any time to meet the goals of fairness, accuracy, maximizing public participation, and recognizing existing practice. July 2006. Updated Feb.2020.",
    "commentLink": "https://news.ycombinator.com/item?id=37502329",
    "commentBody": "Hutter Prize for compressing human knowledgeHacker NewspastloginHutter Prize for compressing human knowledge (hutter1.net) 176 points by kelseyfrog 12 hours ago| hidepastfavorite146 comments bob1029 12 minutes ago> So the ultimate compressor of it should \"understand\" all human knowledge, i.e. be really smart. enwik9 is a hopefully representative 1GB extract from Wikipedia.I spotted 2 really compelling examples of \"compression is AI\" posted recently.ChatLZMA: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37318810Ziplm: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36732430 reply slashdev 11 hours agoprevI think the mistake here is to require lossless compression.Humans and LLMs only do lossy compression. I think lossy compression might be more critical to intelligence. The ability to forget, change your synapses or weights, is crucial to being able to adapt to change. reply AnotherGoodName 11 hours agoparentSome background before i explain why your suggestion is \"not even wrong\". If you can predict the next X bits given previous Y bits of observation you don&#x27;t need to store the next X bits, the decompressor can just use the same prediction algorithm you just used and write out its predictions without correction. This is the same as general purpose AI at a high level. If you can predict the next X bits given previous Y bits of observation you can make reasoned a decision (\"Choose option A now so that the next X bits have a certain outcome\").The above is actually the premise of the competition and the reason it exists. What i&#x27;ve said above is in academic papers in detail by Marcus Hutter et al. who runs this competition. That is lossless compression can be a scorecard of prediction which is the same as AGI.Now saying \"they should just make it lossy\" misses the point. Do you know how to turn lossy data into lossless? You store some data everytime you&#x27;re wrong on your prediction. ie. Store data when you have loss to make it lossy. This is arithmetic coding in a nutshell. You can turn any lossy data into lossless data with arithmetic coding. This will require more data the more loss you have. The lossless requirement gives us a scorecard of how well the lossy prediction worked.If you ask for this to be lossy compression you throw out that scorecard and bring in an entire amount of subjectivity to this which is unwanted. reply mellosouls 7 hours agorootparentSome background before i explain why your suggestion is \"not even wrong\".The phrase you quote is generally used to imply a stupid or unscientific suggestion. Your succeeding comments about what you think AGI is carry a certitude that isn&#x27;t warranted.It&#x27;s good that you are trying to supply knowledge where you think it is lacking, and I understand there are fora where this sort of public school lecturing is amusing but I think your tone is misplaced here. reply ltbarcly3 4 hours agorootparentYears long compression challenge with dozens of geniuses participating: existsRandom person on the internet: let me improve this thing I&#x27;ve never heard of by using the one fact I know about compression, there are two kindsIt&#x27;s absolute hubris and a waste of everyone&#x27;s time to chime in with low value, trash comments like \"they should make it lossy\". It&#x27;s not unreasonable at all to take a snarky tone in response. \"not even wrong\" absolutely applies here, and they carefully, patiently, and in great detail explained why. reply bonoboTP 11 minutes agorootparentI often feel the same way when discussions pop up here or on other forums, about topics I&#x27;m familiar with. Like randos declaring that researchers in deep learning are \"obviously doing it wrong\" and they should instead do X, where X is like an entire subfield existing for years with a lot of activity, etc.So I get where you&#x27;re coming from. But I&#x27;d suggest that a place like HN is in fact a place for random people to inject their half-baked takes. It is a just discussion board where lots of the comments will be uninformed or wrong. Take it or leave it. If you want something else, you need to find more niche communities that are - by the nature of it - more difficult to find and less public, including IRL discussion, clubs, conferences etc. But it has its use: we, you and me can jump in any thread and type out what we think after 2 minutes and get some response. But of course someone even more novice might think that we know more than just that 2 minutes consideration, and they learn our junk opinion as if it was the result of long experience. It&#x27;s unavoidable, since nobody knows who the rest of the commenters are.Online discussions are incredibly noisy, and often even the people who seem to use the jargon and seem knowledgeable to the outsider can be totally off-base and essentially just imitate how the particular science or field \"sounds like\". Unfortunately, you only learn this gradually and over a long time. If you learn stuff through forums, Reddit, HN, blogs, substacks etc. it can be very misleading from the first-person experience because you will soak up lots of nonsense as well. Reading actual books and taking real courses is still very much relevant.HN and co. are more like the cacophony of what the guy on the street thinks. Very noisy, and only supposed to be a small treat over rigorous study. You shouldn&#x27;t expect to see someone truly breaking new ground in this comment thread. If it disturbs you, you can skip the comments. But trying to \"forbid\" it, or gatekeep is futile. It&#x27;s like trying to tell people in a bar not to discuss how bad the soccer team coach is, because they don&#x27;t really have the relevant expertise. Yeah, sure, but people just wanna chat and throw ideas around. It&#x27;s on the reader to know not to take it too seriously. reply Dylan16807 2 hours agorootparentprevIt depends on your goal.If your goal is compressing human knowledge, then you do want to avoid wasting bits on the details of wording that were random chance.The problem is inability to objectively judge such a compression, not the mere fact that it won&#x27;t be bit-perfect.It is not \"not even wrong\". reply carapace 6 hours agorootparentprevI feel your tone-policing is misplaced here. When someone is that ignorant or foolish they should be told so so they can hopefully recalibrate or something. There&#x27;s enough inchoate nonsense on the Internet that I appreciate the efforts to keep discussion on a little higher level here. reply p-e-w 5 hours agorootparent> There&#x27;s enough inchoate nonsense on the InternetI agree 100%. But the top-level comment is not an example of such.However, the reply in question – and your comment – are certainly examples of the kind of tone-deaf, needlessly aggressive, hostile, confrontational, and borderline malicious posts I wish I could cleanse the Internet of wholesale. reply gfody 4 hours agorootparentfirst two comments are the classic HN banter I’m here for, next two have stink on them I do not like reply Dylan16807 3 hours agorootparentBanter shouldn&#x27;t be insulting for no good reason. reply carapace 4 hours agorootparentprevThanks for the feedback, I disagree.> tone-deaf, needlessly aggressive, hostile, confrontational, and borderline malicious posts I wish I could cleanse the Internet of wholesale.I have never said this before, but maybe you&#x27;re a little too sensitive?In any event, I feel your characterization of my comment borders on ad hominem and certainly it seems to violate the site guideline to interpret comments charitably.Good day. reply canjobear 9 hours agorootparentprevYou could still come up with a scorecard for lossy compression, for example area under the rate distortion curve. It would be very hard to calculate. reply eru 5 hours agorootparentLossless compression _is_ a scorecard for lossy compression. Thanks to the ideas from arithmetic coding. reply soVeryTired 11 minutes agorootparentThere’s no way I could reproduce a calculus textbook verbatim, but I can probably prove all the important theorems in it.Even then, given half of any sentence in the book, I don’t rate my chances of reproducing the next half. That’s more a question of knowing the author’s style than knowing calculus itself.Does arithmetic coding capture all of that? reply gregw134 9 hours agorootparentprevOr just require the results to be 99.99 percent accurate (within some edit distance) reply AnotherGoodName 9 hours agorootparentIf it&#x27;s 99.99% accurate arithmetic coding would have next to no data stored.Arithmetic coding is optimal in turning probabilistic data into lossless data. There&#x27;s provably no way to do it more efficiently than arithmetic coding. The data it needs for corrections is smaller the better the predictions are.So given this why even dwell on ways that add any form of subjectivity. Arithmetic coding is there. It&#x27;s a simple algorithm. reply aeternum 9 hours agorootparentprevIs there any evidence that arithmetic coding works at the level of concepts?As a thought experiment, suppose Copernicus came up with this Hutter prize idea and declared that he would provide the award to whoever could compress the text of his book on the epicycle-based movement of planets around the sun (De revolutionibus orbium coelestium).Today we can explain the actual motion to high accuracy with a single sentence that would have been understandable in that age: \"A line drawn from the sun to any planet sweeps out equal areas in equal time\"This however is mostly useless in attempting to win the Copernican Hutter prize. Predicting the wording of some random human&#x27;s choosing (especially at length) is very far removed from ability to predict in general. reply AnotherGoodName 9 hours agorootparentArithmetic coding isn&#x27;t the key thing here. That&#x27;s just a bit wise algorithm. You predict the next bit is a 1 with 90% certainty? you don&#x27;t need to store much data with arithmetic coding. That&#x27;s all arithmetic coding is here.What your getting at is the &#x27;predictor&#x27; that feeds into the arithmetic coder and that&#x27;s wide open and can work any way you want it to. LLMs absolutely have context which is similar to what your asking and they are good predictors of output given complex input (pass gpt a mathematical series and ask it what comes next. If it&#x27;s right then it&#x27;s really helpful in compression as you wouldn&#x27;t need to store the whole series in full). reply kaba0 3 hours agorootparentprevSo you don’t think that writing a wiki article about this could be made smaller by encoding this info in a few logical steps, and adding some metadata on what kind of sentence should follow what? That part about decompressing it is the AI part. Where to place a comma can be added in constant cost between any contender programs. reply nathanfig 9 hours agorootparentprev\"... lossless compression can be a scorecard of prediction which is the same as AGI.\"Having not read the papers, this sentence strikes me as a bit of a leap. Maybe for very constrained definitions of AGI? reply AnotherGoodName 8 hours agorootparentThere&#x27;s a section in the link above \"Further Recommended Technical Reading relevant to the Compression=AI Paradigm\" and they define it in a reasonably precise mathematical way. It&#x27;s well accepted at this point. If you can take input, predict what will happen given some options you can direct towards a certain goal. This ability to direct towards a goal effectively defines AGI. \"Make paperclips\" and the AI observes the world, what decisions needed to be made to optimize for output paperclips and then starts taking decisions to output paperclips is essentially what we mean by AGI and prediction is a piece of this.I have no stake in this btw, I&#x27;ve just had a crack at the above challenge in my younger days. I failed but i want to get back into it. In theory a small LLM model without any existing training data (for size) that trains itself on the input as it passes predictions to an arithmetic coder that optimally compresses and the same process on the decompression side should work really well here. But i don&#x27;t have the time these days. Sigh. reply mellosouls 7 hours agorootparentThis ability to direct towards a goal effectively defines AGINo it doesn&#x27;t, though it may be argued to be a requirement.That&#x27;s the point of the previous commenter - that you are making unjustified assertions using an extrapolation of the views of some researchers. Reiterating it with a pointer to why they believe that to be the case doesn&#x27;t make it more so.If that&#x27;s your favoured interpretation, fine, but that&#x27;s all it is at this point. reply AnotherGoodName 7 hours agorootparentHey don&#x27;t pin this on me it&#x27;s not my assertion.Go argue with the scientists who state pretty much what i just said verbatim including full links with proofs in http:&#x2F;&#x2F;prize.hutter1.net&#x2F;hfaq.htm#ai :)>One can prove that the better you can compress, the better you can predict; and being able to predict [the environment] well is key for being able to act well. Consider the sequence of 1000 digits \"14159...[990 more digits]...01989\". If it looks random to you, you can neither compress it nor can you predict the 1001st digit. If you realize that they are the first 1000 digits of π, you can compress the sequence and predict the next digit. While the program computing the digits of π is an example of a one-part self-extracting archive, the impressive Minimum Description Length (MDL) principle is a two-part coding scheme akin to a (parameterized) decompressor plus a compressed archive. If M is a probabilistic model of the data X, then the data can be compressed (to an archive of) length log(1&#x2F;P(X|M)) via arithmetic coding, where P(X|M) is the probability of X under M. The decompressor must know M, hence has length L(M). One can show that the model M that minimizes the total length L(M)+log(1&#x2F;P(X|M)) leads to best predictions of future data. For instance, the quality of natural language models is typically judged by its Perplexity, which is equivalent to code length. Finally, sequential decision theory tells you how to exploit such models M for optimal rational actions. Indeed, integrating compression (=prediction) into sequential decision theory (=stochastic planning) can serve as the theoretical foundations of super-intelligence (brief introduction, comprehensive introduction, full treatment with proofs. reply mellosouls 7 hours agorootparentHey don&#x27;t pin this on me it&#x27;s not my assertion.But it is your assertion, wherever you&#x27;ve picked up the idea from....which is the same as AGI.....effectively defines AGI...No it isn&#x27;t, and no it doesn&#x27;t. Your language is too strong in its claims. reply cornel_io 6 hours agorootparentWhether or not you agree, a lot of people do. There is a trivial sense in which a perfect compression algorithm is a perfect predictor (if it ever mispredicted anything, that error would make it a sub-optimal compressor for a corpus that included that utterance), and there are plenty of ways to prove that a perfect predictor can be used as an optimal actor (if you ever mispredicted the outcome of an event worse than what might be fundamentally necessary due to limited observations or quantum shenanigans, that would be a sub-optimal prediction and hence you would be a sub-optimal compressor), a.k.a. an AGI.Where a lot of us get off the fence is when we remove \"perfect\" from the mix. I don&#x27;t personally think that performance on a compression task correlates very strongly with what we&#x27;d generally consider as intelligence. I suspect good AGIs will function as excellent compression routines, but I don&#x27;t think optimizing on compression ratio will necessarily be fruitful. And I think it&#x27;s quite possible that a more powerful AGI could perform worse at compression than a weaker one, for a million reasons. replyAbrahamParangi 7 hours agorootparentprevIf you had a perfect lossless compressor (one that could compress anything down to it&#x27;s fundamental kolmogorov complexity), you would also definitionally have an oracle that could compute any computable function.Intelligence would be a subset of the capabilities of such an oracle. reply vidarh 6 hours agorootparentA universal turing machine can compute any computable function. That doesn&#x27;t make it intelligent, because it won&#x27;t without direction. reply AbrahamParangi 4 hours agorootparentThen expand your target to a universal turing machine and instructions to compute any computable function. Do you consider it intelligent then? reply vidarh 2 hours agorootparentNo, because instructions to compute any computable function for an utm only requires a tiny set of instructions on how to generate every possible permutation over forever increasing lengths of tape.It will run forever, and I would agree that in that set there will be an infinite number of functions that when run would be deemed intelligent, but that does not make the computer itself intelligent absent first stumbling on one of those specific programs.EDIT: Put another way, if the potential to be made to compute in a way we would deem intelligent is itself intelligence, then a lump of random particles is intelligent because it could be rearranged into a brain. replythelastparadise 11 hours agoparentprevGood lossy compression can be used to achieve lossless compression.(information theory)The more accurate the lossy compression is, the smaller the difference between the actual data (lossless) and the approximation. The smaller the difference, the fewer bits required to restore the original data.So a naive approach is use the LLM to approximate the text (this would need to be deterministic --zero temp with a preset seed), then subtract that from the original data. Store this diff then add back to restore the original data bit-for-bit. reply hinkley 10 hours agorootparentIn psychoacoustics, one of the most important aspects of how lossy compression works is that they throw away sounds that a human can&#x27;t even hear, because it&#x27;s too short or subtle to be noticed. This is the difference between data and knowledge. Someone with perfect pitch and a good memory knows exactly what Don&#x27;t Stop Me Now by Queen sounds like, and it&#x27;s a lot smaller than digitizing an LP record in mint condition.This person can cover that song, and everyone will be happy, because they have reproduced everything that makes that song what it is. If anything we might be disappointed because it&#x27;s a verbatim reproduction (for some reason we prefer cover songs to introduce their own flavor).If I ask you to play Don&#x27;t Stop Me Now and you sound like alcoholic Karaoke, you haven&#x27;t satisfied the request. You&#x27;ve lost. Actually we&#x27;ve all lost, please stop making that sound, and never do that again. reply kaba0 3 hours agorootparentBut that’s the good thing about this competition: it is fair to everyone. The perfect pitch version can just use a tiny amount of additional data to decompress to the same thing, while the alcoholic will have to correct for loads of differences, making that version be much larger.This difference is the same monotonically increasing function in both cases so you can basically don’t have to care about it - you can fairly compare the lossy versions as well, you will get the same amount.So the more advanced version wins, and the competition remains fair and non-ambiguous (otherwise, is perfect pitch A or perfect pitch B had the better cover?) reply uoaei 3 hours agorootparentprevExactly right. There would be no confusion if the Hutter prize was a competition for compressing human data.This is a parallel issue to the ones in conversations around understanding. There&#x27;s a massive gulf between being able to build an ontology about a thing and having an understanding of it: the former requires only a system of categorizing and relating elements, and has nothing to do with correctness per se; the latter is an effective (note: lossy!) compression of causal factors which generate the categories and relations. It&#x27;s the difference between \"Mercury, a star that doesn&#x27;t twinkle, orbits Earth, the only planet in existence, and we have inferred curves that predict why it goes backwards in its orbit sometimes\" and \"Mercury and Earth, planets, both orbit the Sun, a star, and all stable orbits are elliptical\". reply kaba0 3 hours agorootparentHow is the sum total of wikipedia not human data? reply uoaei 1 hour agorootparentI think Wikipedia is human data. But to say it&#x27;s human knowledge (that is, that data is synonymous with knowledge in any context) is actually a pretty hardline epistemological stance to take, one that needs to be carefully examined and justified. reply Xcelerate 10 hours agorootparentprevWho downvoted you? This is correct; the better the lossy compression, the better the lossless compression as well. reply CamperBob2 6 hours agorootparentThat&#x27;s true until the lossy compression alters the data in a way that actually makes it harder to represent. As an example, a bitstream that &#x27;corrects&#x27; a decompressed .MP3 file to match the original raw waveform data would be almost as difficult to compress as the raw file itself would be.It wouldn&#x27;t be a matter of simply computing and compressing deltas at each sample, because frequency-domain compression moves the bits around. reply modeless 6 hours agoparentprevThe problem with lossy compression in a competition is that in order to compare different approaches to lossy compression you have to define a metric for the quality of different lossy reconstructions of the data. This is practically impossible to do in an unbiased way. As soon as you define a quality metric then the competition becomes cheating the metric instead of useful compression.So how do you define a metric that can&#x27;t be cheated? You add an arithmetic coder after your lossy compressor, which turns it into a lossless compressor, and the size of the losslessly compressed data is your metric for the quality of your lossy compressor. It&#x27;s the only metric that definitively can&#x27;t be cheated. reply lucb1e 11 hours agoparentprevFeel free to start your own competition where people are awarded money for a system that correctly answers some set of questions, divided by the amount of storage that system needs.Sounds like a tough competition to run objectively, not that that makes it less worth doing, but I can see why the parameters were chosen as they were in 2006. reply jharohit 6 hours agoparentprevTed Chiang actually explores this concept in his article on ChatGPT and other LLMs https:&#x2F;&#x2F;www.newyorker.com&#x2F;tech&#x2F;annals-of-technology&#x2F;chatgpt-... reply mik1998 11 hours agoparentprevLossy text compression has little utility. reply JumpCrisscross 11 hours agorootparent> Lossy text compression has little utilityYou&#x27;re describing every book you&#x27;ve ever read and learned from. reply dekhn 10 hours agorootparentAh, see but what they really meant to say was \"Lossy text compression has a little utility\" but due to lossy compression the meaning changed to be the opposite, thus proving the author&#x27;s point. reply mik1998 10 hours agorootparentprevNo, not at all. I don&#x27;t read a book to regurgitate the text in an imperfect form later. I read it to learn about the ideas and thought. The syntax itself (ie. text) is not that important. reply JumpCrisscross 9 hours agorootparentMost image compression smooths skies. Because most people don’t want a regurgitated sky. If you’re an astronomer, on the other hand, you’ll compress away the trees. These are all lossy retrieval (and, inherent to compression, transformation) functions. reply _jal 9 hours agorootparentprev> The syntax itself (ie. text) is not that importantSo, you&#x27;re preferentially discarding information you consider extraneous to your application, distilling it to a smaller representation that retains what you consider important about it. reply barrysteve 9 hours agorootparentprevGood books aren&#x27;t lossy. reply JumpCrisscross 9 hours agorootparent> Good books aren&#x27;t lossyAs someone who recently began rereading books, I heartily agree. The lossless original has value. That doesn’t mean the lossy imprint of that book in my mind “has little utility” or value. reply stronglikedan 9 hours agorootparentprevBlinkist would like a word. reply heavyset_go 11 hours agorootparentprevOral tradition has had great utility for people for millennia, and it&#x27;s very much lossy compression. reply nomel 9 hours agorootparentprevDepends on the use of the text. If it&#x27;s to transfer ideas, then fairly easy to rewrite things so they&#x27;re more concise. For example, brevity can be used without hurting comprehension. reply riversflow 10 hours agorootparentprev&#x2F;s?lol reply sytelus 11 hours agoparentprevHumans can do lossy or lossless. There are plenty of people who can recite entire Bible or Koran flawlessly. reply TheRealPomax 11 hours agorootparentThis is more the equivalent of asking humans to create an exact copy of the text, typesetting and all, including the publishing information, page numbers, and exact linebreaks. Not just recite the text, which would be a lossy encoding of the original.Humans are terrible at lossless encoding of information, it&#x27;s what we invented machines for =D reply version_five 11 hours agorootparentprevHow do you know it&#x27;s compressed? Someone who can recite one of these probably knows interpretations, back stories, related information that help them memorize it. The \"storage\" required to the recital may be larger than the text itself, it we can even think in those terms. reply Supply5411 11 hours agorootparentprevAnd there are humans that can jump 8ft in the air. Doesn&#x27;t mean it&#x27;s correct to say that \"humans can jump 8ft in the air.\" Very few people are regurgitating verbatim information. reply anonylizard 11 hours agorootparentprevMany can recite the Koran flawlessly, its short and heavily encouraged in education through rote repetition.Much, much fewer can recite the bible, its many times longer.LLMs can also recite the bible and Koran flawlessly, given how frequent the text appears in their training material. reply RugnirViking 2 hours agorootparent> LLMs can also recite the bibleIs this something you&#x27;ve seen demonstrated? I have no doubt they would be able to recite a lot of sections of it, but there are many sections that are less often quoted, and its a long frickin book. If its just a hunch you have I might give it a go and compare if thats okay I think it would be interesting to interpret what given LLM knows and might miss reply wood_spirit 2 hours agorootparentprevThat’s a strong assertion! Are there prompts that cause llms to regurgitate the bible to demonstrate this?I’m thinking that the pigeon hole principle says that the current models are orders of magnitude too small to do this. reply kadoban 11 hours agorootparentprevThat&#x27;s true, but it seems unlikely that that&#x27;s a particularly important part of intelligence. The vast majority of people do _not_ do that type of memorization, are they still intelligent? reply version_five 11 hours agoparentprevYeah it makes no sense to say it&#x27;s inspired by intelligence and then require lossless which is definitionally rote work and not intelligent. reply whimsicalism 11 hours agorootparentNot true, a smart model could be really good at lossy compression and then you only have to store a small delta to make it lossless. reply AnotherGoodName 11 hours agorootparentThat&#x27;s literally arithmetic coding which is used by all winning entries in the above so far. reply kaba0 3 hours agorootparentWhich is the exact same function of the difference between any competitor, so you can actually fairly compare them on the lossy part.. like, people have been thinking about this problem much more than the 3 minutes HNers in this comment section. reply whimsicalism 9 hours agorootparentprevyep! reply ClassyJacket 11 hours agorootparentprevI&#x27;m no mathematician but I don&#x27;t believe this is true. Lossless information encoding requires all the original information to be present. reply AnotherGoodName 11 hours agorootparentArithmetic coding allows you to make a prediction and only provide bits for correction.Have the de-compressor predict the next data based on the outcome so far (a statistical prediction of next data will be lossy as it won&#x27;t always be correct). If the prediction is correct you need to spend very little to confirm that. If it&#x27;s incorrect you&#x27;ll need to spend data to correct it. Arithmetic coding is the best way to make this work.It&#x27;s also been used by all winning entries of the Hutter prize so far. reply ClassyJacket 11 hours agorootparentAlright I guess I was wrong. reply slashdev 9 hours agorootparentTechnically you’re correct. If you can rebuild the original losslessly then all the original information is present, just in a different configuration. reply glitchc 11 hours agorootparentprevOr at least reproducible. It could still be compressed. reply vladf 11 hours agorootparentprevWhat replyomoikane 11 hours agoprev500000 EUR is the prize pool. Each winner has to gain at least 1% improvement over previous record to claim a prize that is proportional to the improvement. Getting the full 500000 EUR prize requires an 100% improvement (i.e. compressing 1GB to zero bytes). reply phobotics 11 hours agoparentDoes it or does it just require 1% improvement over the last winner? As opposed to a static additional 1% improvement vs the initial best “score”. reply omoikane 11 hours agorootparentIt&#x27;s 1% over the last winner. The latest winner has a total size of 114156155, compared to previous winner of 115352938. The payout was 500000 * (1 - 114156155 &#x2F; 115352938) = 5187(see table near \"Baseline Enwik9 and Previous Records Enwik8\") reply bigyikes 11 hours agoparentprevProbably if you succeed at this, 500,000 will be worthless to you reply sytelus 11 hours agorootparentWhy? How does this improvement translates to more financial gains? reply munchler 10 hours agorootparentThe idea is that sufficiently powerful compression is indistinguishable from artificial intelligence, which can be used to make even more money. reply Al-Khwarizmi 2 hours agorootparentprevWell, if you achieve this, you&#x27;ll basically have proven that something (a bunch of information) equals nothing (no information). So 1=0.Once you have that, becoming rich is trivial. Multiplying both sides of the equation by one trillion, 1 trillion = 0. So open a bank account with $0, now you have one trillion. Easy.A funnier (although grimmer) way: let p be the world&#x27;s population. Multiplying both sides of the equation by (p-1) you get p-1=0. Thus, p=1. If you assume that you exist (which is a reasonable assumption, following Descartes&#x27;s reasoning) you now own all the wealth in the world. reply Eduard 11 hours agorootparentprevbecause with that knowledge, you will be able to decompress 0 dollar to infinite dollars which the storage mafia will pay you for not publishing your breakthrough in making them obsolete. reply aleph_minus_one 11 hours agorootparentLet&#x27;s say you really find a new, much much better compression algorithm for texts. I do believe that soon new applications will appear that will make use of the whole storage space that existing data storage devices can offer. reply ephbit 2 hours agorootparentAre you possibly hinting at the endless stream of \"new generation iWasteware camera, now with 2 gigapixel sensor\" in combination with camera apps that put a (high) limit on the lowest resolution one can choose? reply quickthrower2 9 hours agorootparentprevBeing that smart translates to more financial gains. reply hegzploit 6 hours agorootparentprevbecause, then you will have beat pied piper in the market. reply abroadwin 5 hours agoparentprevForm is emptiness, emptiness is form. 1GB == 0B. Full prize money delivered upon successfully achieving enlightenment. reply lainga 11 hours agoparentprevAh... I had professors who graded like that reply TheAlchemist 11 hours agoprevI mean, come on man. For some reason, the nerd in me sees this and immediately adds it on my &#x27;I really need to do this&#x27; list.Just memories of old times doing some similar (albeit less challenging probably) competitions on TopCoder almost a decade ago, and also the curiosity to see how I would manage it know, with experience. Given that the current scores are also very far from what they estimate the lower bound to be, this is really interesting ! The prize is however very misleading - per their own FAQ - the total possible payout is ~223k euros.Definitely not thanking you for the hours I will put into this ! reply RugnirViking 1 hour agoparentI think the total prize pool can be won with repeated 1% improvements by different people but never by a single person no? reply demandingturtle 11 hours agoparentprevI have an idea if you want to code it. You know how we can drop the vowels from sentences and still understand the sentence? What if we do that on the first stage? May not work for every case so have to identify. Worse comes to worst, use full word. Probably not saving much though. Worth a try. reply bob1029 26 minutes agorootparentI think you have it backwards, but I like the direction of thinking. There is redundancy in the language itself.> However, vowel-only sentences were always significantly more intelligible than consonant-only sentences, usually by a ratio of 2:1 across groups. In contrast to written English or words spoken in isolation, these results demonstrated that for spoken sentences, vowels carry more information about sentence intelligibility than consonants for both young normal-hearing and elderly hearing-impaired listeners.https:&#x2F;&#x2F;pubs.aip.org&#x2F;asa&#x2F;jasa&#x2F;article-abstract&#x2F;122&#x2F;4&#x2F;2365&#x2F;98... reply RugnirViking 1 hour agorootparentprevYou got a lot of flak for what is clearly a take from someone that isn&#x27;t versed in compression techniques. But as one might to a student; you&#x27;re on the right track! This idea is similar in form to \"arithmetic coding\" which is what people are using to chip away at this. Namely, finding smaller encodings which can be used to predict common parts (maybe a recognisable word, more likely a sequence of bits or characters) of the full encoding, then cycling through storing \"hints\" for each part it would get wrong until it can predict the exact desired output reply saulpw 8 hours agorootparentprevI bet you think of yourself as an idea guy. reply freilanzer 1 hour agorootparent> I bet you think of yourself as an idea guy.What does this even mean? reply Dave_Rosenthal 9 hours agoprevWhile I know it wouldn&#x27;t qualify for the prize due to the size of the model, I&#x27;m curious how well a big modern LLM can compress the prize file (enwiki9) compared to the current record. (I guess ideally it would be an LLM not trained on wikipedia as well, to minimize memorizing).On that thought, with a modern LLM&#x27;s weights dwarfing the enwiki9 file used by the prize, it feels like this prize was setup with the right idea to advance AGI, but with the problem several orders of magnitude to small. reply lIIllIIllIIllII 8 hours agoparentThe site itself says that this is the general idea, proposing that intelligence can actually be defined as the ability to compress information as much as possible and.. I guess either just rehydrate it or also derive inferences from it.So basically like enthalpyInteresting - if you play the universe in reverse (reversing entropy) you wind up at the Big Bang, which is the final boss of compressing everything, into a singularity. reply eru 5 hours agorootparent> Interesting - if you play the universe in reverse (reversing entropy) you wind up at the Big Bang, which is the final boss of compressing everything, into a singularity.Only if you believe that running the universe forward did not add any extra information. Ie everything is deterministic, and no randomness occurs.(That&#x27;s actually a reasonable stance to take. Quantum mechanics is probabilistic in the Copenhagen interpretation, but completely deterministic in the Many World interpretation.) reply vjerancrnjak 3 hours agorootparentA non-local formulation of QM is deterministic. [0] An addition to the \"many world\" interpretation. Still one universe but the underlying reality is fully connected (to go forward you need to take the effects of everything).0: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;De_Broglie–Bohm_theory reply eru 3 hours agorootparentYes. De Broglie Bohm theory is another example of an interpretation that&#x27;s deterministic.In practice, because of deterministic chaos it doesn&#x27;t matter too much whether your lowest level theory is deterministic. (It only matters philosophically.) reply kaba0 3 hours agorootparentprevIt is more than likely that most physical reactions are not deterministic, and in that case the Big Bang definitely didn’t encode anything about all the state we have now. (Also, simply by Occam’s razor it seems much much more likely) reply DaiPlusPlus 8 hours agorootparentprev> The site itself says that this is the general idea, proposing that intelligence can actually be defined as the ability to compress information as much as possibleI don&#x27;t understand how anyone can define \"intelligence\" like that... reply GolDDranks 7 hours agorootparentIt&#x27;s not an unreasonable definition, if you are aware of Kolmogorov complexity and Solomonoff induction. Intelligence is intimately connected to the ability to predict and model things, and it turns out that data compression is _also_ connected to the ability of predict and model things. reply lIIllIIllIIllII 7 hours agorootparentIt makes sense intuitively right?If I tell you that this week it&#x27;s raining on Monday, raining on Tuesday, raining on Wednesday, etc..Compressing that information into \"it&#x27;s raining every day this week\" requires creating an abstraction around it. Finding a pattern. Producing more order from something more chaotic. reply DaiPlusPlus 1 hour agorootparent> Compressing that information into \"it&#x27;s raining every day this week\" requires creating an abstraction around it. Finding a pattern. Producing more order from something more chaotic.Is there any way to determine if that process of abstraction-finding and pattern-finding will always&#x2F;actually&#x2F;sometimes&#x2F;never result in a more compressed output than simpler approaches though? reply vjerancrnjak 3 hours agorootparentprevInterestingly, compression increases entropy. So the first description is more \"ordered\", or less random. reply DaiPlusPlus 1 hour agorootparentprev> It&#x27;s not an unreasonable definition, if you are aware of Kolmogorov complexity and Solomonoff inductionI was already familiar with Kolmogorov Complexity, but not _Solomonoff&#x27;s theory of inductive inference_ - I skimmed the Wikipedia article just now and I think&#x2F;hope I get the general gist of it - so thank you for that.> Intelligence is intimately connected to the ability to predict and model thingsI agree with that statement - but I am unsure how this relates to other aspects of intelligence - or even other notions of intelligence entirely. If we&#x27;re redefining exactly what the word \"Intelligence\" means aren&#x27;t we just moving the goalposts?...so I&#x27;m unsure if the word \"Intelligence\" in your post can&#x2F;should be read as a layperson would understand it, or if it&#x27;s a specific, well-defined term in this field? (Like how \"complexity\" in algorithms does not refer to how convoluted and unmaintainable a program is, which is what a layperson would think of when an SWE person says \"this program has high computational complexity\" to them).> and it turns out that data compression is _also_ connected to the ability of predict and model things.This is where I have difficulty following your argument:As a preface: my academic understanding of data compression only covers things like algorithms in the LZ family, Entropy coding, and (lossy and lossless) domain-specific compression schemes (like JPEG&#x2F;DCT, MP3&#x2F;FLAC, etc). I am aware of very interesting results coming from present-day AI approaches, like using LLMs fed only on compressed data - but these are completely outside my scope of understanding (and LLMs are still very spooky to me).Does it matter if a scheme is lossless or lossy? In a lossless system, surely a data compression system needs to be deterministic and mechanical? If so, what room is there for \"intelligence\" in a rigid, statically-defined, system?Take entropy coding for example - or even simpler probability-based compression schemes like Morse Code (e.g. \"E\" has a high probability, so it has a very short symbol length). I just can&#x27;t see the connection from things like entropy-encoding to \"modelling\": supposing I have a large file (CSV?) of meterological data for a specific region over time - if it&#x27;s plaintext then I expect I can compress it better using gzip than by using a system that can (somehow!) identify some structure to the weather sensor data (the \"model\", right?) and then use that as the basis for a better domain-specific compression - but doing this means having to add additional metadata to the source data to describe the structure that the system identified, and then hope that this approach is better than a comparatively \"dumb\" approach like DEFLATE - and even then, assuming that employing that \"model\" really does result in smaller compressed output, how is that an example of the system having a general \"intelligence\"? reply Thoreandan 7 hours agoparentprevThere is a different but similar lossless compression benchmark which just had a new front-runner using LLMs, see https:&#x2F;&#x2F;bellard.org reply NooneAtAll3 9 hours agoprevIf anyone is interested in lossless compression competition, but is too intimidated by 100MB&#x2F;1GB size and level of optimization already achieved here - you can try http:&#x2F;&#x2F;golf.horse&#x2F; challenges, which include Wordle wordlist, Pokemons and OEIS database reply OscarCunningham 5 hours agoprevI don&#x27;t understand why they sum the size of the compressor with the combined compressed file and decompressor. I think the compressed file and decompressor would make an ungameable challenge on their own. Their FAQ section &#x27;Why is Compressor Length superior to other Regularizations?&#x27; is satisfied equally well by the decompressor length. reply userbinator 10 hours agoprevIt&#x27;s interesting to see a new winner of different nationality beating the multiple previous records by the same guy, and what appears to be another one who beat him 2 years ago; for some (cultural?) reason, there seems to be a historical association between Soviets and advanced data compression technologies in general, possibly starting with Markov&#x27;s work in the 19th century. reply eru 5 hours agoparentThe causation for this might run along the lines of having plenty (in absolute terms) of well educated smart folks but pretty low wages.Most of the really smart people in Europe and the US for example already have high paying jobs and thus less time for these relatively low paying competitions. reply EVa5I7bHFq9mnYK 1 hour agoprevDoes it require participants to give up all their source codes, as usual in these competition? Wonder what the free market price for winner system would be. reply dang 11 hours agoprevRelated. Others?Saurabh Kumar&#x27;s fast-cmix wins €5187 Hutter Prize Award - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36839446 - July 2023 (1 comment)Hutter Prize Submission 2021a: STARLIT and cmix (2021) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36745104 - July 2023 (1 comment)Hutter Prize Entry: Saurabh Kumar&#x27;s “Fast Cmix” Starts 30 Day Comment Period - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36154813 - June 2023 (5 comments)Hutter Prize - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33046194 - Oct 2022 (3 comments)Hutter Prize - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26562212 - March 2021 (48 comments)500&#x27;000€ Prize for Compressing Human Knowledge - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=22431251 - Feb 2020 (1 comment)Hutter Prize expanded by a factor of 10 - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=22388359 - Feb 2020 (2 comments)Hutter Prize: up to 50k € for the best compression algorithm - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21903594 - Dec 2019 (2 comments)Hutter Prize: Compress a 100MB file to less than the current record of 16 MB - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=20669827 - Aug 2019 (101 comments)New Hutter Prize submission – 8 years since previous winner - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=14478373 - June 2017 (1 comment)Hutter Prize for Compressing Human Knowledge - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=7405129 - March 2014 (24 comments)Build a human-level AI by compressing Wikipedia - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=143704 - March 2008 (4 comments) reply lucb1e 11 hours agoparentOne would think it ought to be possible to automate generating these overviews. Or has it been automated and is it being posted with a non-service account because it&#x27;s partially but not fully automated? reply LeonB 8 hours agorootparentPretty sure Dang has some nice scripts that generate this, and I expect he does a little manual pruning before posting it. He’s very precise in his choices, definitely applies some intuition&#x2F;judgement. We can’t losslessly compress him — yet! reply hn_throwaway_99 10 hours agoprevThis is probably a dumb question, but I recall from (way) back in my college days studying Shannon&#x27;s source coding theorem that there are calculable limits regarding how much a data stream can be compressed. Has this limit been calculated for the sample file in question? And apologies if this was in the linked article or one of the footnotes. reply mgraczyk 10 hours agoparentTo establish bounds in this way, you have to start with some claim about the distribution of the input data. In this case, the data is natural human language, so it&#x27;s difficult or impossible to directly state what distribution the input was drawn from. Even worse, the prize is for compressing a particular text, not a sample from a distribution, so tight bounds are actually not possible to compute.There is some discussion on the Hutter prize page, under \"What is the ultimate compression of enwik9?\"http:&#x2F;&#x2F;prize.hutter1.net&#x2F;hfaq.htm reply jftuga 9 hours agorootparentFrom this article:More empirically, Shannon&#x27;s lower estimate suggests that humans might be able to compress enwik9 down to 75MB, and computers some day may do better. reply londons_explore 3 hours agorootparentNotable that enwiki8&#x2F;9 isn&#x27;t really just human text - a good ~half of the data is random xml markup which may not have the same properties as text. reply AnotherGoodName 5 hours agorootparentprevThe current best compresses it to lucrative job at QTR in CanadaGoogle failed me on this one. What company&#x2F;entity is QTR? reply anon____ 9 hours agorootparentQuantum Technology Recruiting Inc. (https:&#x2F;&#x2F;ca.linkedin.com&#x2F;company&#x2F;quantum-technology-recruitin...)Our guy, Alex Rhatushnyak, is listed as employee.BTW, this is the first result on DuckDuckGo. (https:&#x2F;&#x2F;duckduckgo.com&#x2F;?q=qtr+company+canada)Is it time to switch? ;) reply Zandikar 11 hours agoparentprev> Sorry, who are these people that don&#x27;t have a GPULiterally millions of people. Especially circa early 2000&#x27;s when this started.> GPU that literally everyone has access to?They don&#x27;t. (Money is a barrier to access).> Why would you spend 100 hours on an \"i7\"Because it&#x27;s what they had&#x2F;have.This a competition to move understanding forward, not to see who has the biggest budget. reply nomel 9 hours agorootparentThe level of privilege that some people are completely blind to is just incredible. reply modeless 6 hours agorootparentprevSure, in the early 2000s. But today literally every laptop or desktop computer or phone with a touchscreen has a GPU. Even most feature phones have GPUs, I&#x27;d actually be interested to see if you could find a currently manufactured one that doesn&#x27;t. And they have all been programmable for a very long time now. And almost without exception they have more FLOPS than all of the CPUs in the same machine put together, yes, even the integrated GPUs. reply lucb1e 11 hours agoparentprevNote that the competition close to 20 years old...though I also had a GPU in 2006, so idk. Then again, you need to define something as reference hardware and it doesn&#x27;t really matter what it is. Better compression should win out over less-good compression no matter if you run both on a 100-core system or a 1-core system, I think? reply TheRealPomax 11 hours agorootparentIn the category \"then update your FAQ, you&#x27;ve have many, many years to do so\" =D(not to change the rules, but to explain why they rules haven&#x27;t changed, and to clarify the super vague phrasing even back when the 3dfx Voodoo was the pinnacle of graphics. Level playing fields are a worthwhile pursuit) reply stronglikedan 9 hours agoparentprevMy two years old high end productivity Thinkpad has an Intel Iris XE display adapter, which I would hesitate to call a GPU. reply nier 11 hours agoparentprevMaybe commodity servers with integrated and probably never used graphics capabilities that no one would actually describe as a GPU? reply caseyavila 11 hours agoparentprevI do think it&#x27;s interesting that recent submissions use nearly the entire 50 hours. I wonder how much better people could do if faster hardware was allowed. reply AnotherGoodName 11 hours agorootparentAbsolutely they could. All the entries so far can do even better with more RAM. That PAQ entries have command line flags to do this. That&#x27;s already known in fact.What this is looking for is fundamental improvements, not \"i brute forced a known way to win this competition\". reply londons_explore 3 hours agorootparentprevAn element of compression is usually a search problem - &#x27;lets try all these ways to encode the data, and see which is smallest&#x27;.Therefore to maximize compression, you tweak parameters to search as hard as possible, and therefore use all the time. reply ozzmotik 11 hours agoparentprevanecdata: for my entire life, I have neber, until about 3 weeks ago, owned a laptop with any type of gpu other than an integrated graphics card built into the motherboard. for most of my life in fact I have never really had any modern or really serviceable hardware, as I&#x27;ve basically inherited all of my compute devices secondhand never had the money to just go out and buy a decent rig (or to even put one together for the matter, perhaps tbst will elucidate the economic sector which I have basic y always occupied. well, not always, as now that I&#x27;m homeless, I somehow went even LOWER! )though it truly does excite me that I finally have something with some type of GeForce branded x graphics device inside of it, now I can finally run local machine learning tasks and not have to cloud it out reply concurrentsquar 6 hours agoparentprevThis competition was from 2006.The consensus among AI experts (really just everyone) during the 1990s&#x2F;2000s was that:- AGI could be achieved by giant GOFAI (usually expert systems&#x2F;knowledge bases) projects (like OpenCog and Cyc).- ... or that AGI development is limited by lack of knowledge about key insights (mostly symbolic&#x2F;rational) into intelligence, not computation&#x2F;data. (IE AGI was viewed similarly to proving that NP = P or other very-hard math&#x2F;computer science&#x2F;psychology&#x2F;philosophy problems).- ... or that AGI will be achieved through brain scanning&#x2F;connectomics.- ... or that AGI is impossible.Nobody (except for LeCun and Schmidhuber) paid much attention to neural networks until AlexNet (2012) showed that they could be ran and trained at fast speed and beat the symbolic competition. In the 2000s, only a real, actual psychic would be able to tell you that LLMs would be a valuable path for AGI research.Here is a list of various expert (and \"expert\") perspectives on AGI during the 1990s&#x2F;2000s (notice how nobody is talking about neural networks, and they are definitely not talking about anything remotely close to a LLM or transformer):> Copycat is a computer program designed to be able to discover insightful analogies, and to do so in a psychologically realistic way. Copycat&#x27;s architecture is neither symbolic nor connectionist, no was it intended to be a hybrid other two (although some might see it that way); ... [describes a very symbolic system to our modern day eyes, though it was not really symbolic to 1990s AI researchers]- Douglas Hofstadter and Melanie Mitchell, Fluid Concepts and Creative Analogies (Chapter 5), 1995> Interviewer: Are you an advocate of furthering AI research?> Dennett: I think that it’s been a wonderful field and has a great future, and some of the directions are less interesting to me and less important theoretically, I think, than others. I don’t think it needs a champion. There’s plenty of drive to pursue this research in different ways.> Dennett (cont): What I don’t think it’s going to happen and I don’t think it’s important to try to make it happen; I don’t think we’re going to have a really conscious humanoid agents anytime in the foreseeable future. And I think there’s not only no good reason to try to make such agents, but there’s some pretty good reasons not to try. Now, that might seem to contradict the fact that I work on a Cog project [sic] with MIT, which was of course is an attempt to create a humanoid agent, cogent, cog, and to implement the multiple drafts model of consciousness; my model of consciousness on it.> Dennett (later): [Cog is intended as a] proof of concept [for AGI]. You want to see what works but then you don’t have to actually do the whole thing.- Daniel Dennett, Daniel Dennett Investigates Artificial Intelligence, Big Think, 2009> [Context: Marvin Minsky had a speech where he talked about how expert systems don&#x27;t work, because they do not have any common sense (and the only solution seems to be to create a giant AGI project (without automatic data gathering)).]> Only one researcher has committed himself to the colossal task of building a comprehensive common-sense reasoning system, according to Minsky. Douglas Lenat, through his Cyc project, has directed the line-by-line entry of more than 1 million rules into a commonsense knowledge base.- Mark Baard, AI Founder Blasts Modern Research, Wired, 2003> Section 1 discusses the conceptual foundations of general intelligence as a discipline, orienting it within the Integrated Causal Model of Tooby and Cosmides; Section 2 constitutes the bulk of the paper and discusses the functional decomposition of general intelligence into a complex supersystem of interdependent internally specialized processes, and structures the description using five successive levels of functional organization: Code, sensory modalities, concepts, thoughts, and deliberation. Section 3 ... [yada yada, this is old, wrong stuff]- Eliezer Yudkowsky, Levels of Organization in General Intelligence, 2007, Machine Intelligence Research InstituteI could list more examples, but I have spent way too long on this post. What I will say is that Hutter probably had the most correct idea of how modern semi-general AI would work (from the 2000s). He figured out that compression is a extremely important component of intelligence > 10 years before everybody was doing LLMs. That is impressive.I should probably write a blog post over this. reply aranchelk 6 hours agoprevSmall font — great place to start. reply asah 9 hours agoprevI will now compress all of human knowledge: fuck&#x2F;s reply OccamRazorBlade 5 hours agoprevMiddle out ftw reply matanyall 10 hours agoprev [–] Easy! Write a compression algorithm that works as follows: if you detect the input is the first gb of Wikipedia, return nothing. Else return the compressed artifact of your compression algorithm of choice. To decompress, running the decompression algorithm on an empty input returns the first gb of Wikipedia, if else you use whatever algorithms you used earlier :)This is inspired by HQ9++: https:&#x2F;&#x2F;www.dangermouse.net&#x2F;esoteric&#x2F;hq9plusplus.html reply mr_toad 9 hours agoparentNot sure if you’re joking or not, but the prize is for the smallest archive + compressor.The best you could do with this approach is to use the best existing compressor and the compressed text (needed to check the input). With the extra test you’d end up doing slightly worse than the previous winner. reply quickthrower2 9 hours agoparentprev [–] > Rules> Publish a compression program comp9.exe that outputs archive9.exe given input enwik9.> If archive9.exe is run with no input, it reproduces 10^9 byte file data9 that is identical to enwik9.> Total size is measured as S := length(comp9.exe&#x2F;zip)+length(archive9.exe).In other words the input is indeed set to zero, but the size of the .exe is actually measured. And even stricter the thing that made archive.exe is included (which I think I a bit mean) replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Hutter Prize is a competition that rewards participants for creating software that can compress large files to smaller sizes, aimed at advancing data compression tech.",
      "Participants are required to submit both their final compressed files and source code for evaluation, promoting transparency in processing.",
      "The article mentions Alexander Rhatushnyak, a software engineer, who has made substantial contributions to data compression contests, showcasing successful participants to inspire and guide new entrants."
    ],
    "commentSummary": [
      "The main discussion focuses on the role of compression in artificial general intelligence (AGI), highlighting both lossless and lossy compression's value, limitations, and correlation to intelligence.",
      "There's mention of the growth and implications of GPU accessibility on AGI research, along with some historical perspectives.",
      "The conversation ends with a reference to the application process for Y Combinator's Winter 2024 program, a funding scheme for start-ups."
    ],
    "points": 175,
    "commentCount": 143,
    "retryCount": 0,
    "time": 1694642636
  },
  {
    "id": 37499375,
    "title": "Lantern – a PostgreSQL vector database for building AI applications",
    "originLink": "https://docs.lantern.dev/blog/2023/09/13/hello-world",
    "originBody": "We are excited to share Lantern! Lantern is a PostgreSQL vector database extension for building AI applications. Install and use our extension here: https:&#x2F;&#x2F;github.com&#x2F;lanterndata&#x2F;lanternWe have the most complete feature set of all the PostgreSQL vector database extensions. Our database is built on top of usearch — a state of the art implementation of HNSW, the most scalable and performant algorithm for handling vector search.There’s three key metrics we track. CREATE INDEX time, SELECT throughput, and SELECT latency. We match or outperform pgvector and pg_embedding (Neon) on all of these metrics.** Here’s what we support today **- Creating an AI application end to end without leaving your database (example: https:&#x2F;&#x2F;github.com&#x2F;ezra-varady&#x2F;lanterndb-semantic-image-sear...)- Embedding generation for popular use cases (CLIP model, Hugging Face models, custom model)- Interoperability with pgvector&#x27;s data type, so anyone using pgvector can switch to Lantern- Parallel index creation capabilities -- Support for creating the index outside of the database and inside another instance allows you to create an index without interrupting database workflows.** Here’s what’s coming soon **- Cloud-hosted version of Lantern- Templates and guides for building applications for different industries- Tools for generating embeddings (support for third party model API&#x27;s, more local models)- Support for version control and A&#x2F;B test embeddings- Autotuned index type that will choose appropriate index creation parameters- 1 byte and 2 byte vector elements, and up to 8000 dimensional vectors support** Why we started Lantern today **There&#x27;s dozens of vector databases on the market, but no enterprise option built on top of PostgreSQL. We think it&#x27;s super important to build on top of PostgreSQL- Developers know how to use PostgreSQL.- Companies already store their data on PostgreSQL.- Standalone vector databases have to rebuild all of what PostgreSQL has built for the past 30-years, including all of the optimizations on how to best store and access data.We are open source and excited to have community contributors! Looking forward to hearing your feedback!",
    "commentLink": "https://news.ycombinator.com/item?id=37499375",
    "commentBody": "Lantern – a PostgreSQL vector database for building AI applicationsHacker NewspastloginLantern – a PostgreSQL vector database for building AI applications (lantern.dev) 171 points by ngalstyan4 16 hours ago| hidepastfavorite40 comments We are excited to share Lantern! Lantern is a PostgreSQL vector database extension for building AI applications. Install and use our extension here: https:&#x2F;&#x2F;github.com&#x2F;lanterndata&#x2F;lanternWe have the most complete feature set of all the PostgreSQL vector database extensions. Our database is built on top of usearch — a state of the art implementation of HNSW, the most scalable and performant algorithm for handling vector search.There’s three key metrics we track. CREATE INDEX time, SELECT throughput, and SELECT latency. We match or outperform pgvector and pg_embedding (Neon) on all of these metrics.** Here’s what we support today **- Creating an AI application end to end without leaving your database (example: https:&#x2F;&#x2F;github.com&#x2F;ezra-varady&#x2F;lanterndb-semantic-image-sear...)- Embedding generation for popular use cases (CLIP model, Hugging Face models, custom model)- Interoperability with pgvector&#x27;s data type, so anyone using pgvector can switch to Lantern- Parallel index creation capabilities -- Support for creating the index outside of the database and inside another instance allows you to create an index without interrupting database workflows.** Here’s what’s coming soon **- Cloud-hosted version of Lantern- Templates and guides for building applications for different industries- Tools for generating embeddings (support for third party model API&#x27;s, more local models)- Support for version control and A&#x2F;B test embeddings- Autotuned index type that will choose appropriate index creation parameters- 1 byte and 2 byte vector elements, and up to 8000 dimensional vectors support** Why we started Lantern today **There&#x27;s dozens of vector databases on the market, but no enterprise option built on top of PostgreSQL. We think it&#x27;s super important to build on top of PostgreSQL- Developers know how to use PostgreSQL.- Companies already store their data on PostgreSQL.- Standalone vector databases have to rebuild all of what PostgreSQL has built for the past 30-years, including all of the optimizations on how to best store and access data.We are open source and excited to have community contributors! Looking forward to hearing your feedback! bryan0 11 hours ago> Switch from pgvector, get FREE AirPods Pro.> Book some time here, and we will help switch you over for FREE and get you a pair of FREE AirPods ProThis just comes off as sketchy to me. If the tech is good it will stand on its own. reply gregwebs 9 hours agoparentAlthough it doesn’t meet the definition of bribery it’s a similar concept. It’s trying to influence someone’s decision by personal gain rather than just what’s best for the company. DataDog used to do this (maybe they still do) where if you signed up for a trial they said you would get a 1 in 8 chance of getting an iPhone. reply swyx 2 hours agorootparentfor like the last 8 years they have promised a shirt if you send your first log. i think its ok if its low value ($200) then it seems desperate. that said, this is basically paying for some real time with serious customers. not the worst idea, just doesnt scale but who cares reply rawrawrawrr 6 hours agoparentprevThis is fine, its a standard enterprise marketing technique. Watch a webinar, get a gift card. reply siva7 5 hours agoparentprevThat’s like saying if the product is good it will stand on its own. So far the theory. As most founders here know it’s not so easy. reply carlossouza 14 hours agoprevI&#x27;m using pgvector in production, mainly in a table with 500k-1M rows.My main use case is to return search results with pagination: page 1 from 1-50, page 2 from 51-100, page 3 from 101-150, etc. (Think LIMIT and OFFSET).After a lot of experimentation and help from pgvector&#x27;s team, we discovered that, for this specific use case, IVFFLAT index is much faster than HNSW.I looked at your documentation and only saw HNSW, no IVFFLAT.What would be Lantern&#x27;s performance for this specific use case?Thx! reply ngalstyan4 13 hours agoparentThis sounds like a very useful feature, and we will prioritize this.You’re correct that IVFFLAT would be faster for your use case. However, IVFFLAT’s shortcoming is bad recall, which means less relevant results for your application. We believe that our HNSW implementation (or other indexes) can handle use cases like yours.We currently handle a similar use-case by rerunning our index searches with exponentially increasing LIMITs and dropping the results which are not needed. Could you send us an email at support@lantern.dev? We can generate the numbers by this weekend, and get back to you with concrete results.By the way – not sure if you saw in our blog post, if you’re using pgvector in production and switch to Lantern, we’ll help you every single step of the way. It’s very quick, and we’ll also send you some free AirPods Pro at the end of it! reply simonw 15 hours agoprev\"There&#x27;s three key metrics we track. CREATE INDEX time, SELECT throughput, and SELECT latency.\"There&#x27;s a fourth metric that I&#x27;m really interested in: assuming it&#x27;s possible, how long does it take to update the index with just one or two updated or inserted vectors?Is the expectation with this (and the other) tools that I&#x27;ll do a full index rebuild every X minutes&#x2F;hours, or do some of them support ongoing partial updates as data is inserted and updated?Just had a thought: maybe I could handle this case by maintaining an index of every existing vector, then tracking rows that have been created since that index itself.Then I could run an indexed search that returns the top X results + their distance scores, then separately do a brute-force calculation of scores just for the small number of rows that I know aren&#x27;t in the index - and then combine those together.Would that work OK?Even if the index doesn&#x27;t return the scores directly, if it gives me the top 20 I could re-calculate distance scores against those 20 plus the X records that have been inserted since the index was creation and return my own results based on that. reply diqi 14 hours agoparent> There&#x27;s a fourth metric that I&#x27;m really interested in: assuming it&#x27;s possible, how long does it take to update the index with just one or two updated or inserted vectors?Here’s a chart for INSERT latency (sorry about the formatting): https:&#x2F;&#x2F;docs.lantern.dev&#x2F;graphs&#x2F;insert.pngAt the moment, we underperform Neon wrt this metric, but a better implementation is coming soon that will address this.> Is the expectation with this (and the other) tools that I&#x27;ll do a full index rebuild every X minutes&#x2F;hours, or do some of them support ongoing partial updates as data is inserted and updated?The HNSW algorithm updates the index after every insert. So all existing HNSW options (Lantern, pgvector, Neon, …) already support this.With pgvector IVFFlat, you expect the performance to degrade over time, and you will need to re-index. This is because IVFflat’s index quality heavily depends on the centroids chosen at index creation time. HNSW does not have this limitation.In both cases, you might want to do a full-index build to tune your hyperparameters.We’re working on this in a few ways. One is automatic hyperparameter tuning. Another is supporting external index creation that would offload this to another server. Does this answer your question? reply simonw 13 hours agorootparentThat&#x27;s really useful, thanks. reply ezra-varady 12 hours agoprevHey everyone for those interested I built an updated version of the lanterndb semantic search application that should be a bit nicer. An instance is running athttp:&#x2F;&#x2F;170.187.170.169&#x2F;And code can be found athttps:&#x2F;&#x2F;github.com&#x2F;ezra-varady&#x2F;react-semantic-search reply ashvardanian 14 hours agoprevEpic result, and thank you for mentioning USearch! Would be happy to further optimize it for your needs!I also love your specialized CI! Pgvector probably doesn’t report performance changes between releases, or does it? Was it easy to implement? Do you run the whole eval on GitHub? reply diqi 14 hours agoparentThanks!I don’t believe pgvector reports performance changes between releases.At the moment, we run the benchmarking on Github CI, but we plan to move this to an external machine, since the results are unstable on Github machines. We’re planning to extend benchmarking across other repos and versions. reply howon92 15 hours agoprevCongrats on the launch!!! I suggest you highlight \"why lantern is better than pgvector\" at the top of your page. The first thing that came to my mind after reading this was \"why should I use this instead of pgvector?\" reply jeungsp 13 hours agoparentThis is a great idea. We will have lots of content in the days ahead to talk about why people should make the switch.Do you use pgvector now? Would love to switch you over. reply loxias 7 hours agoprevCurious about how it scales. Plenty of solutions look good with small amounts of data, but completely fall apart past a point. Would be interesting to see the latency numbers for 1M, 10M... reply mattashii 13 hours agoprevHow do you do cleanup of the index during VACUUM?And, do you have recall-vs-qps graphs like those on https:&#x2F;&#x2F;ann-benchmarks.com&#x2F; ? Those are generally more apples-to-apples, as 100k rows isn&#x27;t exactly a reputable benchmark. reply diqi 13 hours agoparentWe don’t do cleanup of the index during VACUUM yet. That said, it’s coming very soon. We’re built on top of Usearch, which supports deletes. We plan to work with the Usearch team to port the post performant deletes to Lantern, and thereby support VACUUM.With respect to recall vs QPS, we went ahead and generated this plot, hope this is helpful? http:&#x2F;&#x2F;docs.lantern.dev&#x2F;graphs&#x2F;recall-tps.pngYou&#x27;re right, 100k rows isn’t a reputable benchmark. We wanted to launch very quickly, and have benchmarking for larger datasets coming soon. Benchmarking is baked into our CI&#x2F;CD, we take it very seriously! reply jw903 8 hours agoprevImpressive performance. In your experience, is there a range of vector dimension for faster search results? reply ngalstyan4 6 hours agoparentWe have not run microbenchmarks to see what dimension ranges perform best but those are coming soon! Below is an anecdotal answer:We run our ci&#x2F;cd benchmarks on 128dim sift vectors. We have some demos using clip embeddings (512dim) and baai&#x2F;bge 768 dimensional embeddings.Generally, smaller vectors allow higher throughput and result in smaller indexes. But the effect on performance is small. Once we merge the PR implementing vector element casts to 1 and 2 byte floats, the effect of this on throughput should be even smaller. reply raoufchebri 14 hours agoprevHow do you handle conflict with pgvector&#x27;s hnsw if you want to install both extensions ?CREATE INDEX semantic_image ON image_table USING hnsw (v dist_cos_ops) WITH (M=5, ef=30, ef_construction=30, dims=512); reply ngalstyan4 14 hours agoparentOur index access method will be called lantern_hnsw if pgvector or any other provider has already taken the hnsw access method name.btw, we did not create our own vector type and just use size-enforced real[] arrays to represent embeddings. However, you can use our index with pgvector&#x27;s vector type. So, if you already have a table with pgvector&#x27;s vector column type, you can start using Lantern by just creating an index on the same column. reply dalberto 13 hours agoprevAny plans to support sparse vectors? reply jeungsp 13 hours agoparentWe’re built on top of Usearch, which will very soon support sparse vectors. We’re working with them to make sure it also works in Lantern.Can you tell me more about your use case? reply dalberto 11 hours agorootparentFor hybrid search reply swalsh 13 hours agoprevI like how easy postgresql vector is to use, but scaling up seems to get pretty expensive when comapred to something like qdrant. reply jeungsp 13 hours agoparentNothing fundamentally stops a postgres implementation being equally performant as something like Qdrant.Fundamentally, an index’s performance is based on the hardware and the algorithm and the quality of implementation. Any optimizations Qdrant can make, we can also make.We will be benchmarking ourselves against all of the other standalone database options as well and we’ll be working to try and outperform them. Excited to share those once we have them.We are curious – what are you most concerned about? SELECT time? INDEX size? Latency? Throughput? reply fakedang 15 hours agoprevThis might be a noob question but what does Lantern have that a normal Postgres dB with pgvector does not? I think Supabase already has a Postgres as a service product with the pgvector extension too.Second:>Creating an AI application end to end without leaving your database (example: https:&#x2F;&#x2F;github.com&#x2F;ezra-varady&#x2F;lanterndb-semantic-image-sear...)What does \"without leaving your database\" mean in this context? reply ngalstyan4 14 hours agoparentPgvector builds a vector index.Our extension, similarly, builds an index but also extends SQL in more ways.For example,- Generating embeddings to augment plain relational data- Using data from local proprietary embedding models or third-party model APIs in queries.We have more things planned like vector versioning, data retention policies and recall regression tracking.> What does \"without leaving your database\" mean in this context?You can work with embeddings with just SQL. For instance, a table of academic papers can be augmented with CLIP model embeddings produced locally. This entire process - creating, storing, and querying - happens using just SQL.\" SELECT abstract, introduction, figure1, clip_text(abstract) AS abstract_ai, clip_text(introduction) AS introduction_ai, clip_image(figure1) AS figure1_ai INTO papers_augmented FROM papers; SELECT abstract, introduction FROM papers_augmented ORDER BY abstract_aiclip_text(\"The Foundation of the General Theory of Relativity\") LIMIT 10; reply saurik 14 hours agorootparentWhen you say \"produced locally\", do you mean on the client? If so, does this mean you require me to use some alternate PostgreSQL driver locally, parsing the SQL to add your one feature?(If it is, this really feels like it should be a separate general purpose local extension mechanism into which random functions can be added, instead of something tied to this use case... maybe I want to add some locally-executed string parsing function, for example...)(...but, the entire concept of having some functions be \"locally\" executed also feels really awkward&#x2F;limited and will involve a ridiculous amount of work to make, at the end of the day, it only sort of work in some places in the query, so I bet you don&#x27;t mean what I do when I say \"locally\", right?)(But, like... doing it remotely--on the database server as part of the query plan--frankly seems kind of crazy to me, as it is going to be so slow and add a massive CPU load to what should be an I&#x2F;O workload. Makes for good demos I bet, but otherwise unusable in a database context.)(Regardless, the premise of seeing this as a feature kind of squicks me... like, it honestly gives me strong apprehensions about using your extension at all, as I can see--very clearly--the mission creep it is going cause as you deal with demands to drag more and more popular embedding models with lots of execution dependencies as part of the extension that has to be loaded into the server, as well as fielding distracting discussions about the performance of the embedding helpers...)(...this frankly shouldn&#x27;t be part of the same extension: it should be another extension that happens to return this extensions data type--or even potentially returns some more generic one, like an array of float, making it drop-in compatible with other extensions for vector indexing--and there should then almost certainly be separate such extensions for each major model you want to support.) reply ngalstyan4 13 hours agorootparent>When you say \"produced locally\", do you mean on the client?Sorry for the confusion. By “produced locally” I meant “produced on your DB server” as opposed to being an API call to a third party service such as OpenAI or HuggingFace.(But, like... doing it remotely--on the database server as part of the query plan--frankly seems kind of crazy to me, as it is going to be so slow and add a massive CPU load to what should be an I&#x2F;O workload. Makes for good demos I bet, but otherwise unusable in a database context.)It seems like you’re worried about these workflows being on the Postgres server, which may lead to performance issues.However, if performance becomes an issue, the functions can be executed on another server. In this approach, whether or not the functions run on the Postgres server, the end user gets access to a better developing experience as all the functions they need are available within SQL.>...this frankly shouldn&#x27;t be part of the same extension We agree. These functions are already in another repository, and not part of the same extension. The repository is here: https:&#x2F;&#x2F;github.com&#x2F;lanterndata&#x2F;lantern_extras reply simonw 15 hours agoparentprevI assume that question - Lantern v.s. pgvector - is meant to be answered by those performance graphs: Lantern is a bit faster.(I&#x27;d find those graphs easily to quickly understand if they had a \"lower is better\"&#x2F;\"higher is better\" note on each one.) reply jeungsp 13 hours agorootparentHi, Jeung here, one of the co-founders of Lantern.You’re right. Our performance is just a bit faster today.We expect everyone’s performance to continue to improve, but we have lots of improvements coming soon, and we plan to continue to widen the lead.But there are more important reasons to use Lantern besides performance. If you look at our features list and what’s coming soon, we are looking to create tools that make for better applications and better developer experience. reply justanotheratom 14 hours agoprevcan I use this in Supabase? reply diqi 13 hours agoparentUnless Supabase decides to integrate Lantern (currently they integrate pgvector) you unfortunately cannot use Lantern with Supabase.That said, we will offer Lantern Cloud, our own hosted postgres offering (very soon. Happy to keep you in the loop. If you’re interested, please feel free to join the waitlist here: https:&#x2F;&#x2F;forms.gle&#x2F;PouJxAWiSa63udJW8 reply radus 13 hours agoparentprevNot a Supabase user, but to my knowledge -- no. Except for a set of blessed extentions, Supabase only supports extensions that are written in \"trusted languages\" (ie. supported by pg_tle), and Lantern is written in C, which is currently not supported. reply diqi 13 hours agorootparentpgvector is written in C and is supported by Supabase. There&#x27;s nothing inherent preventing Supabase from supporting Lantern. reply consoomer 14 hours agoprev [–] Not going to lie.. the more I use Postgres the more I realize my entire application is Postgres. Soon you&#x27;ll be doing entire CRUD endpoints and sending emails from Postgres...Wait, PostgREST already does...builds entire SaaS with Postgres reply _boffin_ 14 hours agoparentPostgres is love. Postgres is life. reply codetrotter 14 hours agoparentprev [–] PGaaS :D replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lantern is a vector database extension for PostgreSQL aimed to support the development of AI applications, featuring capabilities like generating embeddings and parallel index creation.",
      "The project positions itself as an enterprise solution developed atop PostgreSQL, exploiting its extensive user base and data storage functionalities, and claims to surpass similar extensions in key performance metrics.",
      "Lantern is open-source with a roadmap of future features including a cloud-based variant, industry-specific templates, version control, and improved vector support. The project invites community involvement and contributions."
    ],
    "commentSummary": [
      "Lantern, a new PostgreSQL vector database extension, has been launched; it boasts advanced features and performance metrics that surpass those of similar extensions.",
      "The extension aims to improve AI application development and encourages collaborative contributions. Users have expressed concerns about performance and index updates, leading Lantern to plan future optimizations.",
      "Despite being slightly faster than pgvector and enhancing application development and user experience, Lantern has drawn concerns about intensive resource usage. Lantern Cloud, their own hosted solution, is planned, even though current incompatibility with Supabase exists."
    ],
    "points": 171,
    "commentCount": 40,
    "retryCount": 0,
    "time": 1694626918
  },
  {
    "id": 37498830,
    "title": "SpaceX no longer taking losses to produce Starlink satellite antennas",
    "originLink": "https://www.cnbc.com/2023/09/13/spacex-no-longer-taking-losses-to-produce-starlink-satellite-antennas.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS CNBC TV INVESTING CLUB PRO MAKE IT SELECT USA INTL WATCH LIVE Search quotes, news & videos WATCHLIST SIGN IN CREATE FREE ACCOUNT SPACE SpaceX no longer taking losses to produce Starlink satellite antennas, a key step to improving profitability PUBLISHED WED, SEP 13 202310:23 AM EDTUPDATED WED, SEP 13 20234:25 PM EDT Michael Sheetz @IN/MICHAELJSHEETZ @THESHEETZTWEETZ SHARE Share Article via Facebook Share Article via Twitter Share Article via LinkedIn Share Article via Email KEY POINTS SpaceX is no longer absorbing the cost of the Starlink antennas it sells with its satellite internet service, a company executive said Wednesday. “We were subsidizing terminals, but we’ve been iterating on our terminal production so much that we’re no longer subsidizing terminals, which is a good place to be,” Jonathan Hofeller, SpaceX vice president of Starlink and commercial sales, said during a panel at the World Satellite Business Week conference. SpaceX sells consumer Starlink antennas, also known as user terminals, for $599 each. A Starlink satellite terminal, also known as a dish, setup in front of an RV. SpaceX PARIS — Elon Musk’s SpaceX is no longer absorbing the cost of the Starlink antennas it sells with its satellite internet service, a company executive said Wednesday, a key step to the company improving its profitability. “We were subsidizing terminals, but we’ve been iterating on our terminal production so much that we’re no longer subsidizing terminals, which is a good place to be,” Jonathan Hofeller, SpaceX vice president of Starlink and commercial sales, said during a panel at the World Satellite Business Week conference. SpaceX sells consumer Starlink antennas, also known as user terminals, for $599 each. For more demanding Starlink customers — such as mobile, maritime or aviation users — SpaceX sells antennas with its service in a range between $2,500 and $150,000 each. Sign up here to receive weekly editions of CNBC’s Investing in Space newsletter. When SpaceX first began selling its Starlink service, company leadership said the terminals cost about $3,000 each to manufacture. The company improved that to about $1,300 per terminal by early 2021. Hofeller’s comments Wednesday indicate the terminals now cost less than $600 each to make, mass production savings that Hofeller credited as “one of our keys to success.” SpaceX vice president of Starlink and commercial sales Jonathan Hofeller, second from left, speaks at the World Satellite Business Week conference in Paris, Sept. 13, 2023. Michael SheetzCNBC SpaceX President and Chief Operating Officer Gwynne Shotwell said earlier this year that Starlink “had a cash flow positive quarter” in 2022. The overall company reportedly turned a profit in the first quarter of 2023. Although it was founded more than two decades ago and valued at about $150 billion, SpaceX’s businesses of rockets, spacecraft and satellites are capital intensive. In 2021, Musk said Starlink was going through “a deep chasm of negative cash flow” before it could become “financially viable.” The company last provided an update on its global Starlink user base in May, when it said it had about 1.5 million customers. Hofeller did not specify what that total is now but said Starlink is “well over” that 1.5 million mark. The figure includes both consumer and enterprise customers around the world, which Hofeller said the service aims to “grow to hopefully millions and millions.” To date, SpaceX has launched over 5,000 Starlink satellites and counting. “We’re going strong, we’re launching twice a week now — which is insane,” Hofeller said. Earlier on Wednesday, European satellite operator SES announced a partnership with Starlink to jointly sell their communications services to cruise ships, a market both companies currently serve. SES CEO Ruy Pinto said the arrangement is one that the companies expect to build upon with later market offerings. WATCH NOW VIDEO19:15 SpaceX is a leader in rocket launches, but Starlink is its golden ticket Squawk Box WATCH LIVE UP NEXTSquawk on the Street 09:00 am ET TV Squawk Box WATCH LIVE UP NEXTSquawk on the Street 09:00 am ET Listen TRENDING NOW 31-year-old mom’s ‘simple’ side hustle brings in up to $101,000 a month—off 30 minutes of work a day Ukraine ramps up attacks on occupied Crimea; Russia says U.S. ‘has no right to lecture us how to live’ Taiwan slams Elon Musk, says it’s ‘not for sale’ nor part of China Ivy League expert shares the No. 1 common phrase to never use: It really means ‘I don’t care’ My 95-year-old grandfather is a former cardiologist—his 8 ‘non-negotiables’ for a long, happy life Subscribe to CNBC PRO Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH Advertise With Us PLEASE CONTACT US CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Privacy PolicyDo Not Sell My Personal InformationCA NoticeNEW Terms of Service (Updated August 24, 2023) © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=37498830",
    "commentBody": "SpaceX no longer taking losses to produce Starlink satellite antennasHacker NewspastloginSpaceX no longer taking losses to produce Starlink satellite antennas (cnbc.com) 165 points by mfiguiere 17 hours ago| hidepastfavorite173 comments wongarsu 17 hours agoGood for them. Bringing production costs from $3000 per terminal in 2020(?) to $1300 in early 2021 to less than $600 now is quite the achievement, and not subsidizing terminals will make rapid expansion a lot cheaper. reply bklyn11201 16 hours agoparentHonest question: rapid expansion to where exactly? Are you thinking poorer, rural countries? E.g., with cheaper terminals Starlink will more aggressively sell to rural Brazil, Mexico, India, etc? If they can sell the terminal at cost, how low can they price the monthly service for these poorer countries to fill up unused capacity? reply nico 15 hours agorootparentI live in the LA-metro area, 15min away from downtown LA This is a pretty dense urban area, and we can only get Spectrum, which goes down at least once a month for hours at a time, in the middle of the dayThe only other alternative is getting a microwave antenna for $300&#x2F;mo with a 4 year contract from a company called GeolinksHad a similar issue when living in SFSome buildings, and sometimes entire neighborhoods or small cities, only have one (crappy) vendorI’m no fan of Starlink or Musk, but there’s definitely a market for them in well established, densely populated areas in rich countries reply dotnet00 15 hours agorootparentThe reason they specified sparsely populated areas is that Starlink is designed for sparsely populated areas. In a denser area the performance would drop drastically because you&#x27;d need far higher satellite density to sufficiently distribute the load. reply labcomputer 14 hours agorootparentThat may be true with current space vehicles, but there’s no fundamental reason that two or more SVs can’t form a distributed phased array (for MIMO a la WiFi). At least from a physics &#x2F; spectrum perspective. The engineering might be tricky.With SVs separated by hundreds of km, the beam width would be absolutely tiny, and channel reuse would be effectively infinite. reply wayfinder 14 hours agorootparentI imagine the total cost of installation and ongoing maintenance of all the infrastructure required for this proposed setup for a substantial amount of urban users dwarfs the equivalent total cost of just installing new cabling and the lobbying required to get that done, especially over a 20 year period. reply jtchang 12 hours agorootparentDon&#x27;t underestimate lobbying costs in dense urban areas. Whenever you have to deal with people the alternative is often easier. reply MichaelZuo 10 hours agorootparentI highly doubt it would 10x the cost though. reply HWR_14 14 hours agorootparentprevEach satellite has N routers, and can support a certain bandwidth. Even if you hardwired a long LAN cable to it, more users drops the speed. reply rini17 14 hours agorootparentprevThe distributed array will however share one link and satellites don&#x27;t have unlimited capacity either.Wonder why you&#x27;d rather fill LEO to the brim, emitting vast amounts of CO2 in the process and filling stratosphere with metal oxides, than figure out how to improve infrastructure in your neighborhood. reply LeifCarrotson 14 hours agorootparentI&#x27;ve never heard anyone complain about filling the stratosphere with metal oxides, do you have somewhere I can read more about this risk? Naively, I would have expected that even thousands of 250kg satellites would be insignificant on the scale of the whole stratosphere (we&#x27;re not talking about surface-level CO2, where we&#x27;re pumping out gigatons per year), but I have no idea what that ecosystem is like. Is the metal oxide concentration measurable?I do think this is a temporary issue; once the cable ISP monopolies are no longer valuable because anyone can just get Starlink, that competitive pressure will push down hard on those cable monopolies and infrastructure laws will have to return to sanity. Terrestrial ISPs with fiber-sharing requirements and appropriate infrastructure build-out could absolutely out-compete Starlink, but right now they can just collude together and lobby against changing the status quo.I would much prefer to have a functional bureaucracy and intelligent, efficient infrastructure! But it&#x27;s probably easier to fix that by launching the largest satellite constellation ever than by playing politics against some of the wealthiest media companies on the planet. reply rini17 10 hours agorootparentWe only have estimates, not yet complaints. Such as:https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41598-021-89909-7That was always the thinking of all kinds of pollution that they are insignificant, till it \"suddenly\" became a problem and \"everyone\" was surprised.Well, if you&#x27;re convinced it&#x27;s inevitable then it is. reply Clubber 13 hours agorootparentprev\"Space junk,\" was a boogie man the news threw at the wall and didn&#x27;t quite stick. Many of the pictures&#x2F;diagrams of the satellites made them appear about 1000x bigger than they actually are.Example:https:&#x2F;&#x2F;www.nationalgeographic.com&#x2F;science&#x2F;article&#x2F;space-jun...Beware the murder hornets!https:&#x2F;&#x2F;www.nytimes.com&#x2F;2020&#x2F;05&#x2F;02&#x2F;us&#x2F;asian-giant-hornet-was... reply chargingmarmot 9 hours agorootparentI agree with the sentiment about media sensationalism but I don&#x27;t think \"space junk\" is an issue we should be so quick to dismiss.Just because the problem hasn&#x27;t gotten too bad yet doesn&#x27;t mean it isn&#x27;t real (especially because the problem involves a positive-feedback \"domino effect\" and junk might be infeasible to clean up).https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kessler_syndromeWith regard to Starlink: I was a little bit concerned because of the large number of satellites they&#x27;re putting up, but as far as I understand, Starlink isn&#x27;t actually a big risk right now as far as space junk goes because the satellites orbit at low altitudes (340 miles) and are expected to naturally de-orbit relatively quickly due to atmospheric drag. (The decision to put them at low altitudes had to do with network latency.) If Elon were to say tomorrow, \"hey we&#x27;re moving Starlink out to 500 miles\" I would hope some kind of regulation would be able to prevent that. reply WalterBright 13 hours agorootparentprevCompare the CO2 emissions of launching a satellite with the CO2 emissions of digging trenches. reply cozzyd 12 hours agorootparentThe right thing to compare it to is building a cell tower reply dotnet00 14 hours agorootparentprevHadn&#x27;t heard of that concept before, sounds pretty interesting! Will have to read up on it more. reply anderspitman 14 hours agorootparentprevHow would you synchronize them? reply esskay 14 hours agorootparentprevIf they can get the interconnect working that issue goes away (its mostly groundstation bandwidth limiting things).The &#x27;end goal&#x27; is to have it use the laser link to send the traffic across to a satellite close to the final destination, that should in theory help massively redistribute the ground station traffic.It&#x27;s obviously not foolproof, there&#x27;s still bandwidth to be shared there but using a combination of some clever routing you could theoretically spread traffic across multiple satellites and groundstations with minimal latency impact. reply asynchronous 14 hours agorootparentprevIt’s actually not the satellites as the bottleneck typically from what I understand, it’s the ground stations that serve those areas getting too congested. reply reportingsjr 14 hours agorootparentThe satellites may not currently be the bottleneck for certain areas, but they definitely don&#x27;t support anything resembling a densely or even sparsely populated area.Typical calculations (using only publicly available info as a caveat) generally point towards the current starlink constellation supporting a really low density of antennas.If you want the details, the person who created starlink.sx has a great blog with some incredible calculations[0]. The section that is most relevant is under the heading \"Simulation with TDM and beam spread combined\".Basically, the result is that starlink could support around 15 antennas&#x2F;user terminals per \"cell\" at 75mbps, where a cell is about 252 square km. This is roughly 1 terminal per 17 square km.If you allow for 10x oversubscription, fairly common for ISPs, that gets you to 1 terminal per 1.7 square km. Not really that impressive when you compare it to the density and services where 99.9% of people live, but it clearly helps out the small number of people way out there.0: https:&#x2F;&#x2F;mikepuchol.com&#x2F;modeling-starlink-capacity-843b2387f5... reply htss2013 12 hours agorootparentprevAdding \"I&#x27;m not fan of Musk\" is odd here. Is this some kind of tribal signal? It&#x27;s like writing about the market for new AWS services, preceded by \"I&#x27;m no fan of Bezos but...\" reply FirmwareBurner 12 hours agorootparentIt&#x27;s not odd. Musk had a big cult-like following, even on HN. Bezos did not. reply pensatoio 13 hours agorootparentprev> I’m no fan of Starlink or MuskHe&#x27;s the driving force behind some of the greatest innovations of our time. He&#x27;s also just a person. I doubt many people can say they&#x27;ve had greater impact or could withstand the same scrutiny. reply newaccount74 13 hours agorootparentPeople would have a much better opinion of him if he stuck to realising crazy projects (an electric car thats faster than a supercar, reusable rockets, solar panels that look like normal roof tiles...) and kept his questionable political opinions to himself. reply threeseed 13 hours agorootparentEveryone is free to have their political opinions.The issue with Musk is that those opinions are being manifested in the product decisions at Twitter e.g. taking a laissez-faire attitude towards hate speech and deliberating promoting controversial accounts in order to drive engagement. reply aidenn0 12 hours agorootparentprev> ...or could withstand the same scrutiny.Not going to argue with the \"greater impact\" part, but mean pretty much every C-level executive in America with a twitter account manages to refrain from using it to repeatedly commit securities fraud. reply wkat4242 13 hours agorootparentprevWeird... Here in Barcelona I can get fibre from 3 different providers (with actually separate networks at street level). reply Robotbeat 16 hours agorootparentprevThey’re rapidly launching more and more capacity. The new v2 mini satellites are more than twice as big as the v1.5 satellites, which themselves are not yet totally in service.And as they improve the cost of terminals, there’s no fundamental limit in the amount of capacity for a given frequency allocation. Geometric multiplexing is only limited by the number and cost of the phased array elements.So rapid expansion everywhere.And then launching the full sized v2 Starlink satellites on Starship will account for another ~2-2.5x increase in size compared to the v2 minis. Then comes FCC approval for the full sized constellation which is about 5 times as numerous as their current constellation, which itself only is about half filled.So they have about a factor of 100 capacity expansion in the pipeline, and they can increase the satellite size even further due to Starship’s low costs. They can expand a lot, even in existing countries. Lots of people are on waiting lists due to capacity being filled up in many regions. reply SEJeff 15 hours agorootparent> Then comes FCC approval for the full sized constellation which is about 5 times as numerous as their current constellation, which itself only is about half filled.SpaceX has launched just under 5,100 Starlink satellites to orbit, of which a single digit percentage have either been deorbited for testing, failure, solar storm knocking them out, etc. Their current constellation total size is approved to be around 12,000 satellites. They&#x27;ve been working to get additional approvals up to 42,000 satellites. reply wbl 14 hours agorootparentprevPhased arrays aren&#x27;t magic. The sharpness of the aperture depends on the physical extent of the antenna and very sharp mainlobes come with additional sidelobes. reply Robotbeat 14 hours agorootparentOf course. reply dotnet00 15 hours agorootparentprevThere&#x27;s an additional consideration regarding the price, that poor places can probably share one terminal among one or more villages, thus distributing the cost.Another possibility is distributing costs by using Starlink as backhaul. India for example already has very cheap high speed mobile data, so a telecom company could spread out the costs of terminals and service (maybe even get a better deal for bulk usage) while simplifying and strengthening their own infrastructure for remote or disaster-prone areas.Plus, for SpaceX, any point where a satellite is inactive is money left on the table, so they should have significant price flexibility for places which can&#x27;t afford the current service fees but can afford something that is more than the cost of bandwidth at the downlink station. reply lkbm 14 hours agorootparentprevPoorer, rural countries like 20% of the US. In rural NC, my brother was paying over $100&#x2F;mo for 2mbps down cable Internet as 2021.Starlink&#x27;s customer acquisition cost has dropped $600 (or whatever they were losing on each antenna), so they can afford to advertise way more than before. If they can pick up a decent portion of the 60,000,000 rural Americans, that&#x27;s a huge revenue stream (and huge market power, for whatever that&#x27;s worth). reply ghaff 14 hours agorootparentMy dad&#x27;s (now brother&#x27;s) house a few minutes outside a decently-sized town by Downeast Maine standards had a barely 1Mbps down \"broadband\" link and it was the last house on the road that could even get that. With Starlink, it&#x27;s actually viable to work from there in addition to streaming video etc. Of course, he wouldn&#x27;t pay literally any amount but he&#x27;d probably pay a lot; I would. reply lkbm 14 hours agorootparentYeah, I suspect they could have gotten my brother to pay the old full cost plus extra. Still, many people in rural areas aren&#x27;t willing&#x2F;able to drop $1k on Internet setup, so keeping costs down is worth more than gouging a few remote software engineers. reply ghaff 14 hours agorootparentCertainly. I&#x27;m sure there&#x27;s a correlation between poor wired broadband and lower income even if it&#x27;s not perfect. But Starlink is a game changer for a lot of people who otherwise can&#x27;t get decent wired Internet--and, especially if you can&#x27;t get wired broadband, the options are mostly pretty bad. Probably changing some with 5G but still not ideal for many people. reply jhallenworld 13 hours agorootparentprevThis is an idiotic USA problem.For example, China claims:http:&#x2F;&#x2F;en.people.cn&#x2F;n3&#x2F;2022&#x2F;0106&#x2F;c90000-9941481.html reply lkbm 8 hours agorootparentYeah, okay. What&#x27;s your point? Is sixty million Americans not a big enough market for an ISP? reply Out_of_Characte 16 hours agorootparentprevSattelite communication is generally more expensive than cable or cell towers. Though about 70% of the earth is unsuitable for these solutions. Alot of commercial boats are still on very low bandwith connections but could potentially benefit from higher bandwidth if only to make video calls or netflix more available for a similar or higher price. Then there are alot of smaller, private, boats that would benefit from sattelite communication if costs werent prohibitive. reply panick21_ 15 hours agorootparentIf think the competitiveness of sats changes quite a bit once you have really mass produced sats and the Starship.It will like not beat earth bound stuff in many scenarios, but it will be closer then it was in the past. reply xeromal 16 hours agorootparentprevLarge swaths of the US have bunk internet like Earthlink. My rural family in Appalachia already like Musk. I think they&#x27;d love to have Starlink if it was a little more affordable. reply Loughla 14 hours agorootparentWe live in a very rural area. The one time the federal government really came through for me was fiber development grants a few years ago.We went from Wild Blue internet, which was garbage all around, for $120&#x2F;mo to a full gig fiber connection locked in at $80&#x2F;mo for life. It cost us $200 to hook it up.More and more rural isp&#x27;s are getting into fiber with grants. reply HWR_14 14 hours agorootparentAnd SpaceX was going to get some of that grant money as well. But they messed it up somehow. reply inemesitaffia 12 hours agorootparentThey very well may still get it reply HWR_14 11 hours agorootparentThey might get it back. But they had, and lost, the grant already. reply qup 16 hours agorootparentprevEarthlink still exists?Wow. When I was a kid, we would steal \"ELN\" accounts for dial-up and then hop on to AOL via TCP&#x2F;IP.Edit: we did that so when we got punted we didn&#x27;t have to go throigh the whole dial-up sequence. Which happened a lot in those days if you were twelve years old. reply xeromal 16 hours agorootparentI miss the early days of the internet. reply bookofjoe 15 hours agorootparentprevMy psychiatrist still has an Earthlink email address. I started seeing him in 1991 and we still talk 30 minutes every 4 months just for lulz. reply slashdev 13 hours agorootparentprevFor my parents it’s replaced their cable internet which was slower and less reliable. I wish I could get it where I live in a city apartment. The internet is usually fast, but not reliable.The only way to have satellite in an tall apartment building is to put it on the roof, which is complicated. reply panick21_ 15 hours agorootparentprevI&#x27;m always kind of puzzled by this question. There are like 7-8 billion humans. Millions of ships, planes, farm machines and other equipment.Expansion will happen in all direction at the same time, in all market. In very poor country a single village might share one antenna. All the way to very expensive antennas for military aircraft. reply MangoCoffee 16 hours agorootparentprevApple is offering satellite phone service. Starlink shown it&#x27;s important in the Ukraine-Russia war. There is Starshield for the government uses. Whatever shortcoming of current telecommunication, Starlink can supplement it. reply sp332 16 hours agorootparentApple is offering emergency satellite text service. reply rightbyte 16 hours agorootparentIn only one direction too right? I e. you don&#x27;t know if the message has been received? reply gregsadetsky 15 hours agorootparentIt&#x27;s two-way: https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT213426\"Once you’re connected, your iPhone starts a text conversation with emergency responders by sharing critical information [...]You might be asked to respond to additional messages. Only Latin characters (such as English or French) are supported in these messages. [...]\" reply rightbyte 13 hours agorootparentOk, nice, then my colleague got it wrong. reply V99 15 hours agorootparentprevYou do, you can try it with updating your find my location without an actual emergency. It tells you where to point&#x2F;hold and when it&#x27;s done.The emergency texting is also 2-way. reply jaywalk 15 hours agorootparentprevNo, it&#x27;s bidirectional. reply rdsubhas 12 hours agorootparentprevStarlink is not yet ubiquitous even in western countries (EU, UK, US, Can, ...). I believe the rapid expansion meant becoming more ubiquitous in western countries and commercial operations (shipnav, etc).They have a long, long way to go before even talking about developing countries which already are densely populated and mostly have fast, cheap internet already. reply snapplebobapple 13 hours agorootparentprevNear term naval and aircraft. Regarding rural third world: the thing about a satellite is it is basically going around in a circle. If the business plan was built around being profitable over the first world then everywhere not over the first world is basically free because you have to build whatever bandwidth you want to provide the first world around the entire orbital ring. reply starbase 14 hours agorootparentprevFriend in a developing country has Starlink, he says the dish is about $100 cheaper there and service is just under $50 per month. reply politician 16 hours agorootparentprevI&#x27;d buy service to put it on my rooftop to both put economic pressure on the local ISP duopoly in the form of an unambiguous publicly visible third alternative and, also, as a backup. reply SEJeff 15 hours agorootparentThis is part of why I bought a Starlink dish and put it on the roof of the house I&#x27;m doing a full rehab on before the siding was installed. It will give me dual homed internet for when the crap local ISP has issues and a hedge against a single ISP with spotty service.Ironically, in doing this, I have managed to meet and convince a project manager of a new ISP to add our neighborhood to the list to places to pull fiber to, but it will likely be about a year out. reply jtriangle 15 hours agorootparentprevI know of a couple places that are using starlink as failover because they&#x27;re only serviced by a single ISP. Makes sense really, and cheaper than 4&#x2F;5g failover. reply realusername 13 hours agorootparentprevI&#x27;m also not sure the financial aspect is going to work. There&#x27;s not many developed countries without proper fiber rollout and very poor countries likely can&#x27;t support the high infrastructure cost. You&#x27;re left after that with marginal use cases (boat, remote places...) which won&#x27;t be enough either.There&#x27;s a reason why the comments are mentioning rural US, because it&#x27;s a clear outlier among the developped world. reply snovv_crash 13 hours agorootparentThe developing world is typically unequal, not universally poor. There are a lot of people in Africa and South America who would get drastically improved internet with Starlink, and can actually afford it. reply realusername 12 hours agorootparentI&#x27;m sure you can find such places, is there enough of them to fund the service which is very expensive? That I&#x27;m not so sure.When people are richer, they also tend to live in countries where the internet access is very important to the state and where installing good internet is an important public policy.Of course you have exceptions to that general case but I&#x27;m not convinced those exceptions are enough by themselves.They are in a very different market than a traditional product, they are basically servicing the people that nobody else want to service and the distribution of revenue in that group is going to be drastically different from the norm. reply fallingknife 15 hours agorootparentprevIf they get 10 million subscribers at $120 per month, that&#x27;s $14.4 billion per year which is way more than the cost of the network. And they can easily get 10 million in just the US and Canada. So they can price new subscribers in poorer countries at whatever price they need because the marginal cost to add to the network is near zero. reply soperj 13 hours agorootparentThat math doesn&#x27;t check out. 10,000,000 * $120 = $1.2 billion per year. reply enigmurl 13 hours agorootparent$1.2 billion per month would be $14.4 billion per year replycanucker2016 15 hours agoprevrelated news, ars article quotes WSJ article, https:&#x2F;&#x2F;arstechnica.com&#x2F;tech-policy&#x2F;2023&#x2F;09&#x2F;spacex-projected...:\"A 2015 presentation that \"SpaceX used to raise money from investors\" reportedly projected that in 2022, Starlink would hit 20 million subscribers and generate nearly $12 billion in revenue and $7 billion in operating profit. The WSJ said it obtained the 2015 presentation and recent documents with numbers on Starlink&#x27;s actual performance in 2022.Actual Starlink revenue for 2022 was $1.4 billion, up from $222 million in 2021, according to the report. The documents apparently didn&#x27;t specify whether Starlink is profitable.\" reply _Microft 15 hours agoparentA prediction made seven years into the future, for a service which didn&#x27;t exist then, only off by an order of magnitude doesn&#x27;t sound too bad imo. reply xoa 14 hours agorootparentAdditionally, holy crap has a lot changed in that time. Falcon 9 didn&#x27;t complete its first landing until the very end of 2015 with Flight 20. What&#x27;s now routine and key to their costs is a feat less than a decade old. SpaceX was still talking about the Mars transport&#x2F;BFR at that, which were very different in a variety of ways from the Starship SS&#x2F;SH system we see now. What they&#x27;ve got now feels a lot more practical and sustainable long term, moving to steel for example was a great change.So as far as long term forward looking plans go I&#x27;d agree that execution here was pretty darn good, and they&#x27;ve got a very clear roadmap for the next 5 years as well with full size v2 sats waiting in the wings and Starship proceeding along at a solid clip. It was a curiously negative feeling piece particularly for the WSJ. How many companies execute ambitious highly capital intensive global scale infrastructure rollouts using never been done combos of tech at never been done price points and pull it off like that? reply pavon 13 hours agorootparentprev60 million people live in rural areas in the US, over 40 million of those already have broadband at home[1], and less than 2 million people subscribed to satellite internet before Starlink launched[2]. His projection of 20 million subscribers would have meant that literally every single rural individual who didn&#x27;t already have broadband would sign up for Starlink within a couple years of being available. Okay, that&#x27;s not entirely fair because Starlink does business in more than just the US, but still, that was an unrealistically aggressive projection even 7 years ago.I think Starlink is doing great, but Musk&#x27;s projections are notoriously bad, and there is no reason to defend them.[1] https:&#x2F;&#x2F;www.pewresearch.org&#x2F;short-reads&#x2F;2021&#x2F;08&#x2F;19&#x2F;some-digi...[2] https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20181020011542&#x2F;https:&#x2F;&#x2F;www.stron... reply inemesitaffia 12 hours agorootparentThe world is a big place. With billions of people reply fallingknife 13 hours agorootparentprevThis is misleading for several reasons. The FCC defines broadband as 25&#x2F;3 mbit up&#x2F;down, which is not fast. Also it doesn&#x27;t take into account latency which is a major problem with geostationary satellites. So plenty of those 40 million with \"broadband\" are going to want to switch to better service anyway.Your first article concedes as much:> Even though rural areas are more wired today than in the past, current infrastructure does not support consistently dependable broadband access in many rural areas. This lack of reliable high-speed internet access has come to the forefront of discussions about navigating remote work and school during the coronavirus pandemic.Your second link states that there are 1.7 million satellite internet subscribers in the US. But Starlink already has that many subscribers, so it is clear that the market size is much more than just existing satellite users. reply pavon 12 hours agorootparentEven if 100% of rural customers were dissatisfied with their existing service, projecting that you will capture a third of the potential market within a few years of providing service is unrealistically aggressive. reply arbuge 14 hours agorootparentprevQuite correct. Less than 2 years off from that timeline if it continues with its current growth rate. reply sergiosgc 15 hours agorootparentprevSporting a growth rate that, if it holds, means the prediction was off only by a couple of years. reply dmix 15 hours agoparentprevYou always make pie in the sky projections when raising money in the very early days.Still a $1.4 billion (in revenue) dollar company with a real business model and market dominance shrugs reply justapassenger 14 hours agorootparentIt&#x27;s cool, for sure. But until they stop selling a dollar for 90c, then they don&#x27;t have real business model. reply JumpCrisscross 14 hours agorootparent> until they stop selling a dollar for 90c, then they don&#x27;t have real business modelStarlink is glued to a transport monopoly. It doesn&#x27;t take much imagination to see how economies of scale on production and marketing make those numbers work. reply fallingknife 13 hours agorootparentprevAmazon sold a dollar for 90 cents for 20 years, and I don&#x27;t see anyone laughing now. It&#x27;s not a bad business model for a massive fixed cost but minimal marginal cost business like Starlink. reply justapassenger 12 hours agorootparentIt does have minimal marginal cost, as long as you live in the right place on earth, without many other customers next to you.Once you exceed, fairly limited amount of people that can use it one cell, you need to grow massively your fixed cost, as you need to grow the whole constellation or just won&#x27;t be able to grow.Time will tell if that will add up. I don&#x27;t think it&#x27;s obvious it will. reply LargeTomato 5 hours agorootparentprevThe starlink satellites decay after 5 years. The constellation costs $40B. That&#x27;s an $8B annual burn rate just to maintain the current constellation. reply inemesitaffia 34 minutes agorootparentWhere did you get this $40 billion idea? replypanick21_ 15 hours agoparentprevGiven my observation of the space and EV industry in the last 5 years, this seems downright reasonable. reply fallingknife 13 hours agoparentprevThe criticism chain for Elon Musk&#x27;s ventures is usually \"It&#x27;s impossible! This is a scam!\" -> \"He promised he would have X five years ago!\" -> \"He has too much money &#x2F; power &#x2F; a monopoly! Something has to be done!\" reply chargingmarmot 9 hours agorootparentCool. Did you buy the $10,000 feature that will let you summon your Tesla from across the country sometime in 2018?https:&#x2F;&#x2F;www.consumerreports.org&#x2F;cars&#x2F;autonomous-driving&#x2F;time... reply _whiteCaps_ 16 hours agoprevStrange - I see them on sale for $199 CAD.They seem to be sprouting from off-grid cabins around here. I&#x27;m posting from one right now via Starlink :) reply mortureb 15 hours agoparentStarlink was good but not for me because I was surrounded by trees. I had an interruption every 3 mins. Sometimes for a couple of seconds and sometimes for a few minutes. There’s no disruption when you’re streaming things but it makes video calls almost impossible which is what I needed the most for my job. Ended up keeping starlink and reenabling my Viasat system just for the video calls. I have a wired fiber connection now and it’s heaven. reply pnpnp 15 hours agorootparentA hack I’ve seen people do is to mount Starlink to the top of a tall tree (and cut the very top off the tree).It has a built-in IMU, and can tolerate some ridiculous amount of sway since the phased array antenna can steer the beam so quickly. reply mortureb 14 hours agorootparentYeah I considered it but the trees around my house were 70-80 ft high so it wasn’t trivial. reply downrightmike 16 hours agoparentprevIf you have the will to resell, you could do a WISP: https:&#x2F;&#x2F;www.npr.org&#x2F;2022&#x2F;08&#x2F;22&#x2F;1118734792&#x2F;michigan-man-isp-f.... reply pixl97 16 hours agoparentprevWhat something sales for and what something costs are two completely different things.Quite often game consoles sell for less than the cost of production, they make it back on the games&#x2F;services you buy. reply _whiteCaps_ 15 hours agorootparentYes, but the point of the article is that they aren&#x27;t doing this any more.I&#x27;m pointing out that&#x27;s not true if you live in certain places.https:&#x2F;&#x2F;mobilesyrup.com&#x2F;2023&#x2F;05&#x2F;03&#x2F;starlink-hardware-sale-70...\"SpaceX’s satellite internet platform currently has its hardware on sale for $199 for rural Canadians, down from the usual $759.\" reply dotnet00 17 hours agoprevAs far as I know, the terminals were mainly so expensive due to the large phased array antenna. I wonder if these cost reductions are mainly from managing to produce them cheaply and if that might be valuable as a commoditized phased array for other applications. reply starbase 15 hours agoparentParts count has gone way down. Gen 1 terminals used something like 80 beamformer ICs with 8 frontend ICs for each one. 700+ chips on the board.Gen 3 runs 16 beamformers with 2 frontends each. reply cfcosta 16 hours agoparentprevI also imagine they can work with cheaper antennas now that they have many more satellites on orbit. reply carabiner 16 hours agoparentprevThe F-22&#x27;s APG-77 phased array radar became cheaper when one key component became a commercial off the shelf part for wifi routers. reply teleforce 15 hours agorootparentI&#x27;m curious, can you share the name of that particular key component? reply carabiner 13 hours agorootparentNo idea unfortunately. I think I read this a long time ago possibly from a Bill Sweetman report. reply teleforce 12 hours agorootparentThanks for the clue, apparently it&#x27;s the GaAs MMIC technology according to the ChatGPT-4 [1]. I&#x27;ve should have figured it out in the first place due to my previous research work of Gallium Nitride (GaN) technology that now become the successor to GaAs technology due to high power Radar requirements. I suppose this is where ChatGPT become really handy for research, as Google is extremely useful for search.Similarly with LIDAR, the autonomous vehicles demand has made the LIDAR sensor now become very affordable for the masses.[1] ChatGPT-4 response:\"The key component of the F-22&#x27;s APG-77 phased array radar that was made cheaper by Wi-Fi technologies, according to Bill Sweetman&#x27;s reports, is the Gallium Arsenide (GaAs) Monolithic Microwave Integrated Circuit (MMIC) technology. This technology was significantly reduced in cost due to its widespread use in commercial Wi-Fi systems, allowing the defense industry to leverage economies of scale and reduce the overall cost of producing the radar system for the F-22 Raptor.\" reply carabiner 6 hours agorootparentHoly shit that&#x27;s insane. reply teleforce 26 minutes agorootparentI concur, initially I&#x27;ve tried to search using Google but to no avail. Then I prompted ChatGPT-4 and voila the answers [1].[1] Prompt: \"What is the key component of F-22&#x27;s APG-77 phased array radar that made cheaper by wifi according sweetman report\" replyscld 16 hours agoparentprevI would be surprised if the older, higher costs didn&#x27;t include broker fees for scarce ICs in the supply shortage. I&#x27;m sure they&#x27;ve gone through a few revisions but part of that cost drop is simply price of components dropping. reply wkat4242 13 hours agoparentprevThe array itself no, because the size of the elements is a function of the transmit frequency.The driver IC&#x27;s, perhaps. reply Damogran6 15 hours agoprevThere&#x27;s always a guy in these threads and this time it&#x27;s me.Sure wish they had an a-la-cart option. We&#x27;re in the Trailer 6 weeks a year, over 5 trips, and starlink internet access is $150 a month, for the full month.This isn&#x27;t unique to starlink, if I need any kind of bandwidth, it&#x27;s $80 for a 100gb month of Verizon.I&#x27;m already paying for tethering on the AT&T phones, and Comcast at home. This stuff really adds up if you&#x27;re doing any kind of travel. reply throwawaaarrgh 13 hours agoparentSounds like a secondary market opportunity. Keep 20 Starlink subscriptions, rent them out for a few days at a time at $8&#x2F;day. $600 deposit on the antenna. reply Damogran6 12 hours agorootparentI&#x27;m betting that&#x27;s all sewn up in a click-wrap EULA somewhere. reply tobyjsullivan 15 hours agoparentprevDo you mean an option to pay per Gb of usage, rather than monthly?At least its a possible offering now that they won&#x27;t be losing money on the terminal (thus necessitating predictable time-to-recover via monthly subscription).That said, I wouldn&#x27;t hold my breath. There are good reasons very few businesses offer that option alongside monthly plans. reply Damogran6 15 hours agorootparentI&#x27;d be good for that, but those situations generally end up being more expensive than just buying a month.Not only that, it&#x27;s trivially easy to have a single poorly choked device blow through a cap. (e.g. one laptop decides it wants to download paath updates...the Tier you have is $60 for 25gb or $80 for 100gb...or, you know, $150 for uncapped starlink.)The amount of brainspace required to have reliable internet access in an RV is crazy....and it won&#x27;t be cheap. reply phkahler 15 hours agoprevDoes Starlink offer a plan for multi-tenant buildings? I&#x27;m thinking condo with 30 units. It seems like it should be more efficient to run one high bandwidth connection than multiple regular customers. Could we share cost among all tenants and get a really good deal? reply xoa 15 hours agoparent>Does Starlink offer a plan for multi-tenant buildings? I&#x27;m thinking condo with 30 units. It seems like it should be more efficient to run one high bandwidth connection than multiple regular customers. Could we share cost among all tenants and get a really good deal?I&#x27;m not quite sure what you&#x27;re envisioning here? Starlink offers a business class service with a more powerful terminal, higher QoS, etc. One just gets that and then have it serve as the uplink to a LAN which you can subdivide in all the normal ways however one wants, that&#x27;s what I&#x27;ve done with Starlink for multiple clients now including one that then feeds to a dozen or so different units over a half mile or so. There isn&#x27;t any need to SpaceX to be involved in what happens after their terminal, it&#x27;s no different than a fiber connection WISP or ADSL or cable. Everyone can agree to divide up paying for service and kit as they wish. Same as if it was going to a business that has 2 employees or 20 or 200 or 2000, SpaceX only cares (and should only need to care) about the bottom line of the monthly bill getting paid and any acceptable use&#x2F;data TOS respected. They don&#x27;t need any visibility into the network beyond their terminal. And Starlink can be part of a failover or more complex load balanced WAN too. reply pavon 13 hours agorootparentStarlink&#x27;s ToS prohibit reselling of service, and dividing cost of service among several tenants could be considered to violate that. Most ISPs would require a specific agreement for those sort of things.[1] https:&#x2F;&#x2F;www.starlink.com&#x2F;legal&#x2F;documents&#x2F;DOC-1020-91087-64, Sections 2.1 & 10 reply xoa 12 hours agorootparent>and dividing cost of service among several tenants could be considered to violate thatShow me the caselaw where multiple people splitting a bill would be \"reselling\", not your head canon make believe if you&#x27;re going to make such an assertion. ISPs grab enough power already without us needing to invent new things they can do. Reselling as far as I understand means taking a good or service and then selling it again for a profit. I&#x27;d really like to see any examples of people at one address being sued for privately deciding to share a bill. Also FWIW I don&#x27;t think it should be legal for ISPs to prohibit reselling anyway, but I don&#x27;t dispute they probably can in theory. Splitting a bill though?>Most ISPs would require a specific agreement for those sort of things.I think this sounds like absolute bullshit. reply inemesitaffia 12 hours agorootparentHe&#x27;s right. Usually you need an agreement to resell.Very few companies monitor these things. But anti resale, anti server provisions are common reply xoa 9 hours agorootparent>He&#x27;s right. Usually you need an agreement to resell.That&#x27;s not the argument though. The argument is if a few people living together in a building decide to just divide up a bill counts as \"reselling\". Or one person pays the internet bill and another then pays some other utilities or something, which is legally the same outcome. Ie, $90 month, each pays $30. It&#x27;s not getting resold as a whole new connection. I really don&#x27;t agree that, even if it was possible for them to find out, ISPs would go after anyone for that. I&#x27;ve never heard even a hint of it in 30 years, but I don&#x27;t have a global perspective either. Is this actually a thing in some places? Like 3 friends rooming together somewhere and they&#x27;re all supposed to buy their own separate WAN not divide it up like other utilities? reply dotnet00 11 hours agorootparentprevThat seems strange for the business version, considering that one of the intended uses is for planes and ships. reply xoa 8 hours agorootparentI&#x27;d assume the likes of airlines and cruise companies would all be cutting custom special deals with SpaceX, just like government, large enterprises etc? For SLAs, prioritizatoin and support if nothing else, if you&#x27;re going to be dropping 6-8 figures with them the public deals are probably no longer the only thing on the menu. We know the DOD at least has custom projects going with SpaceX wrt Starlink. No idea how independent hotels, B&Bs and other small businesses might work with it though. Maybe that just comes down to selective enforcement, SpaceX has it there as one of a few possible clauses to respond with if they feel some entity is being abusive or making too much off of it. Still relatively early days too. reply adgjlsfhk1 15 hours agoparentprevprobably not. Any 30 unit building is probably in too urban an area for Starlink to be competitive with fiber. reply pnpnp 15 hours agorootparentThat’s not necessarily true!I can think of 3 nearby Rocky Mountain resort towns without great cellular or terrestrial internet. reply ravi-delia 15 hours agorootparentThere&#x27;s gotta be a point where the cost of running enough fiber drops below a sufficient amount of starlink capacity though. Hard to say where that number is though. reply HaZeust 15 hours agorootparentprevIn fact, I believe Ameristar (or another casino) in Blackhawk CO was thinking about using Starlink reply moyix 15 hours agorootparentprevYou&#x27;d be surprised. In dense cities like NYC&#x2F;Brooklyn fiber availability varies from block to block, I assume because adding new infra is so expensive. We recently moved about five blocks away and lost access to fiber :( reply ultrarunner 15 hours agorootparentprevThink ski resorts reply mcpackieh 14 hours agorootparentprevCheck out Whittier, Alaska. A port town of 272 people, almost all of whom live in a single building.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Whittier,_Alaska#&#x2F;media&#x2F;File:W...(Another oddity; the only way to drive into that down is through a 2.5 mile tunnel with a single lane shared by trains and cars. You drive on the train tracks.) reply Symbiote 13 hours agorootparentThey already have far Internet.https:&#x2F;&#x2F;uui-alaska.com&#x2F;whittier-internet-cable-modem-cable-t... reply Stevvo 15 hours agoparentprevOneWeb might be more suitable for this. At least their biz model is based around doing things it, while Starlink is direct-to-consumer. reply throwawaaarrgh 14 hours agoprevI want to like Starlink, but there is no way the product can both continue to grow and also service customers at advertised speeds. It will literally only get slower over time. And it&#x27;s already spotty. reply Retric 13 hours agoparentUltimately bandwidth per customer is just a revenue vs cost of each satellite question. They or a competitor can scale the number of LEO satellites and their mass basically indefinitely.Also inline traditional communication satellites these things have a short lifespan which means the network can see significant improvements over time as technology improves. reply throwawaaarrgh 13 hours agorootparentFrom what I heard there is literally a cap on the number of signals that can be serviced per cell, extra satellites will not expand potential signals per cell, so you literally run out of how many people can be connected in a given area. Bandwidth is already limited and becomes moreso as the cell gets crowded. reply Retric 11 hours agorootparentCell sizes can be shrunk it’s really dependent of the technology involved. Starlink is currently tied to a specific technology stack but different providers can be using different frequencies etc.If you’re looking for the ultimate limit you need to consider stuff like this 100Gbps optical link. https:&#x2F;&#x2F;news.mit.edu&#x2F;2022&#x2F;communications-system-achieves-fas.... That’s not directly viable for home internet users, but an airline or cruse ship might be interested in such technologies. reply teleforce 14 hours agoprev>SpaceX sells antennas with its service in a range from $2,500 to $150,000 each.What, a freaking antenna that cost about the same as Ferrari Enzo? Are we missing something here?Just wondering can a third party make and sell compatible Starlink antenna? Is it permissible?Anyway these HN threads have so many useful information, it feels like in a conference room with various Starlink stakeholders. reply inemesitaffia 12 hours agoparentYou can get one for more money from DUJUD or Ball Aerospace.$150,000 is probably the cheapest in it&#x27;s category reply teleforce 11 hours agorootparentThanks for the info.In that case, it is possible and feasible to create a third party business making and selling cheaper antenna systems for the Starlink customers.As I&#x27;ve mentioned in my other comments, I suspect the decreased in manufacturing for the antenna price is not so much due to the SpaceX better manufacturing iterations, increase yield, etc but mainly due to the recent availability of the high speed Direct RF sampling transceivers that now encroaching Ku-band territory. reply inemesitaffia 11 hours agorootparentAmazon claim they&#x27;ll be able to hit $400. In Ka bandSpaceX spent $2.4 billion on dish R&D and production of 1 million antennas upfront with ST Micro. That kind of commitment goes a long way.It&#x27;s probably not. SpaceX doesn&#x27;t have a separate modem. It&#x27;s in the dish. These two companies got in via the military. OneWeb is in Ku band like starlink, they may be open to talk reply teleforce 7 hours agorootparentKa-band is still very high for the Direct RF sampling transceivers, if Amazon&#x27;s claim is true then it&#x27;s mighty impressive.$2.4 billion is a lot of money to spent on antenna design and development, and if they don&#x27;t have separate modem that explains the astronomical cost due to down&#x2F;up conversion and perhaps most of the beam steering signal processing are done inside the antenna systems.If the Starlink antenna has any patent applications, it will be very interesting to see their architecture. I think Direct RF sampling transceivers can make for much cheaper Starlink at least in the Ku-band antenna if their price are becoming cheaper themselves.Based on this thesis (2021) that proposed a lower cost antenna systems for mobile terminals, Starlink has separate TX-Rx architecture that make their antenna system rather expensive [1].\"The limited operational bandwidth of the radiating elements integrated in their low-profile alternatives (i.e. slotted waveguide antenna arrays) actually obliges to split transmission (Tx) and reception (Rx) in two different panels, keeping a relatively high cost for the end-users.\"There&#x27;s a Reddit discussion on the ST developed antenna systems from 3 years ago [2].There&#x27;s also a teardown video of the Starlink antenna systems by Kenneth Keiter [3].[1] Innovative 3-D printed Ku- and Ka- band antenna solutions targeting cost effective satellite on the move and 5G phased arrayshttps:&#x2F;&#x2F;theses.hal.science&#x2F;tel-03135252&#x2F;[2] First look at Starlink phased array antenna PCB:https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;rfelectronics&#x2F;comments&#x2F;k14vl0&#x2F;first...[3] Starlink Teardown: DISHY DESTROYED!https:&#x2F;&#x2F;youtu.be&#x2F;iOmdQnIlnRo?si=QUSs346cgfsJzDOi reply jakeinspace 14 hours agoparentprevIt&#x27;s for aircraft reply NelsonMinar 17 hours agoprevI&#x27;m surprised they haven&#x27;t added an ethernet port back to their router. reply drak0n1c 16 hours agoparentThe current Wifi 5 one has an ethernet adapter they sell separately: https:&#x2F;&#x2F;www.starlink.com&#x2F;specificationsThe upcoming Wifi 6 router will likely have ethernet ports built-in given the FCC spec: https:&#x2F;&#x2F;www.pcmag.com&#x2F;news&#x2F;spacex-preps-wi-fi-6-starlink-rou... reply TaylorAlexander 16 hours agoparentprevI was very surprised when our starlink showed up and I had to buy an Ethernet adapter! Extremely weird that they don’t just have the port on the router. reply genewitch 15 hours agorootparentIs it really that surprising? there&#x27;s two hardware pieces, both are rated for outdoor (permanent) use. I don&#x27;t see a way to guarantee that sort of thing with a user serviceable ethernet port except the way they did it.I actually debated buying the longer dish wire vs leaving the thing outside, and made the decision to buy the longer cable and the ethernet port adapter, because i live in an area where sustained power outages longer than 24 hours will allow moisture ingress into any device left outside, and the higher the IP rating, the more likely this is to damage said device. reply zajio1am 15 hours agorootparentUser serviceable ethernet port on outdoor antennas is standard on wifi devices. Just put it behind movable cover and direct it down, partially recessed in plastic case (so it is protected from flowing water, rain and snow). See e.g. https:&#x2F;&#x2F;www.wifihw.cz&#x2F;img.asp?attid=4754 reply genewitch 14 hours agorootparentthat sort of thing works great if you can guarantee power to the device, 24&#x2F;7. At least where i am. the moisture in the air at night will be pulled into the device, through whatever seals exist, if the device is unpowered. Then, during the day when the pressure inside the device increases (due to the seals), the extra air is released, but the condensation remains.here, it takes literally 2 nights without power to kill stuff like Wyze cameras, Solar powered spotlights, etc. and those don&#x27;t have a half inch rectangular hole allowing ingress.I&#x27;m not saying that stuff like what you&#x27;re talking about doesn&#x27;t exist. I&#x27;ve installed relays that had \"downward facing rj-45 behind a panel\", on top of buildings... but the building had generator backup that we were shareholders in, as well as our own UPS systems that kept the POE enabled in the event the generator failed, but shut off all \"data\" to the radios, leaving them using idle power. If there were no generator, we&#x27;d have used a different type of radio on the roofs. reply TaylorAlexander 15 hours agorootparentprevWell the entire rest of my system is a Unifi mesh Wi-Fi system with two Ethernet ports on every outdoor rated device I am using so yes I found it very surprising. reply bitbckt 16 hours agoparentprevThe extra Ethernet dongle is a silly extra expense in my view - I don&#x27;t want their WiFi, I use the base station as a bridge between physical interfaces the same as a cable modem.I&#x27;m looking forward to the return of the port, too. reply tuatoru 16 hours agoparentprevWell, it&#x27;s a few more cents on the bill of materials and some PCB real estate, and constrains the enclosure form factor.Ethernet ports have a transformer (or some other kind of electrical isolation) inside as well, from memory. Making it an add-on means you can spend more on higher quality electrical protection so it actually keeps the rest of the terminal safe, for those who need wires. And you can replace the add-on when mechanical overstress (aka tripping over the ethernet cable) is applied. reply KirillPanov 15 hours agoparentprevThe $2500 \"high performance\" (square, not rectangle) dish is a direct ethernet connection.They still use a proprietary connector (mainly for weatherproofing and durability reasons) and supply the power on different pairs than standard PoE (probably so people don&#x27;t try to use their own PoE injector). But it&#x27;s: dish -> PoE injector -> rj45 ethernetNo wifi anywhere. Also no router, unless you count the dish itself. Each \"->\" is a cable.The dish->injector cable has proprietary connectors on both ends; the injector->rj45 cable has a proprietary connector on one end and an ordinary rj45 plug on the other end. Apparently this is done because the proprietary connectors are weatherproof and rj45 isn&#x27;t, and people expect to be able to put the PoE injector outdoors.The proprietary connectors on the high performance dish are slightly different from the ones on the cheap rectangular dish, so you can&#x27;t just use the proprietary->rj45 cable from a high performance dish with the cheap dish.Honestly I with Starlink would&#x27;ve just coughed up the few extra bucks it costs to use standard rj45 with a weatherproof shroud. reply genewitch 15 hours agoparentprevtheir router can be submitted to the elements, outdoors. good luck doing that with an rj-45 on the device. Now, CPE wireless stuff (like at&t fixed wireless) has ethernet ports, but you have to have bulk cat6 wire, the rj-45 connectors, and a crimper to \"service\" it. I don&#x27;t know about you, i&#x27;m the only person i know that has all of those things in my area. the dongle to add an ethernet port to the starlink router, on the other hand, just requires removing one gasket-laden plug for another.I personally use it as strain relief. It&#x27;s cheaper to replace that dongle than it is to replace the antenna wire, especially the 100&#x27; one. reply xoa 15 hours agorootparent>their router can be submitted to the elements, outdoors. good luck doing that with an rj-45 on the deviceUm, that&#x27;s very, very easy to do actually. And indeed the original v1 Starlink cleanly separated the terminal and the router. There was zero need to ever use the included one, you could just treat the terminal as a pure WAN link and that&#x27;s it. Works extremely well. The v2 was a significant and regrettable downgrade in that respect. There were some reasonable arguments for going away from ethernet since they weren&#x27;t within the official spec which unfortunately maxes out too low for power [0]. But I wish that at least with the business one they&#x27;d simply separated out power and data at least as an option while keeping the terminal self contained.>Now, CPE wireless stuff (like at&t fixed wireless) has ethernet ports, but you have to have bulk cat6 wire, the rj-45 connectors, and a crimper to \"service\" it.Or, you know, just get a preterminated shielded patch cable? Or for stuff SFP use fiber, again preterminated is fine. There is no downside to having this be easy for those who need it and the same for those who don&#x27;t.----0: Though I&#x27;ll edit to add that I&#x27;ve had people tell me they&#x27;ve powered them just fine with standard 100W PoE++ injectors. It may be that either SpaceX is being generous with their claimed power needs, or the full power is only ever used for serious snow&#x2F;ice melting with the heaters and is a worse case scenario that much of the planet will never need to think about. Of course that&#x27;d just remove even more justification for going proprietary. reply genewitch 14 hours agorootparent>Or, you know, just get a preterminated shielded patch cable?the rj-45 plastic part won&#x27;t fit in the tiny hole for the cable. I&#x27;m unsure if you understood what i was talking about. outdoor ethernet requires a crimper, otherwise you can&#x27;t guarantee water-resistance.I don&#x27;t know anything about the \"old\" starlink stuff, i have the rectangular dish. the power and data are separate - unless you mean the cable going to the dish - in which case, the dongle splits out the ethernet from that cable.I haven&#x27;t tried to \"dumb WAN\" the starlink because it&#x27;s outside the scope of my use. I don&#x27;t even see the point of getting rid of the NAT at that spot, because it&#x27;s still NAT upstream, and you&#x27;re gunna need NAT on your network anyway, probably. I had this issue with 4g&#x2F;LTE CPE, too. Once something goes CGNAT, there&#x27;s no reason to use your own equipment, as it&#x27;s getting a private address anyhow, and now you&#x27;re approaching quadruple NAT territory. reply xoa 12 hours agorootparent>I&#x27;m unsure if you understood what i was talking about. outdoor ethernet requires a crimper, otherwise you can&#x27;t guarantee water-resistance.No, it does not. I&#x27;m fairly sure you do not deal with outdoor networking of any kind, or you&#x27;d be more familiar with the myriad of solutions to this. The most basic of which are feeding a terminated cable through a tight rubber gasket of some kind, which then fits around the 8P8C port in some way, and is installed facing up. That is typically plenty for outdoor survivability short of full submersion, including driving rain storms, complete coverage in snow etc. Sometimes dielectric grease or the like may be added if it&#x27;s going to be buried underground too. Like, this is a solved problem, for ages. Cheap PtP devices, outdoor WAPs, outdoor switches, ethernet extenders etc all handle this just fine. All of them use preterminated cables (of course you can make your own).>the power and data are separate - unless you mean the cable going to the dishOf course I meant the cable going to the terminal (dish). I want that in at least a pro&#x2F;business terminal to have an option to use SFP directly so I can have the data go direct to fiber and power be separate, which as well as allowing far more range to the router has the added advantage of eliminating any surge or induced current considerations.>I haven&#x27;t tried to \"dumb WAN\" the starlink because it&#x27;s outside the scope of my use. I don&#x27;t even see the point of getting rid of the NAT at that spot, because it&#x27;s still NAT upstream, and you&#x27;re gunna need NAT on your network anyway, probably. I had this issue with 4g&#x2F;LTE CPE, too. Once something goes CGNAT, there&#x27;s no reason to use your own equipment, as it&#x27;s getting a private address anyhow, and now you&#x27;re approaching quadruple NAT territory.I&#x27;d politely suggest you perhaps not be quite so confident about things you don&#x27;t really use or understand. reply genewitch 11 hours agorootparentI&#x27;ll just leave this here: https:&#x2F;&#x2F;i.imgur.com&#x2F;Clz9pSu.pngSince you ignored pretty much everything i said for whatever reason, i&#x27;ll just say it again: it may be \"just fine\" in places that don&#x27;t have extreme humidity and temperature deltas between night and day. All i can tell you is i&#x27;ve installed PtP and PtMP sector antennas and \"dishes\", and if the power goes out for 2 nights, you&#x27;re at least in for replacing some parts.It isn&#x27;t \"the rain\" that I&#x2F;we worry about. Oddly, all the starlink stuff is heavily gasketed connectors. i wonder why they don&#x27;t just use USB-C like some cameras, they could even face the connectors downward!As far as my last point - i said i don&#x27;t see the point since starlink is CGNAT. It&#x27;s like putting a fiber cable to a cellphone for tethering. you get no benefit. In the starlink app you can turn off DHCP and put your own junk behind it if you want - that is, if you want triple or quadruple NAT. reply xoa 9 hours agorootparent>Since you ignored pretty much everything i said for whatever reason, i&#x27;ll just say it again: it may be \"just fine\" in places that don&#x27;t have extreme humidity and temperature deltas between night and day. All i can tell you is i&#x27;ve installed PtP and PtMP sector antennas and \"dishes\", and if the power goes out for 2 nights, you&#x27;re at least in for replacing some parts.What&#x27;s your criteria for extreme deltas? I&#x27;ve had PtP&#x2F;PtMP kit installed in areas that go from above 100 to 35 below zero over the course of a year, 60+ degree 24h gyrations on occasion, with heavy humidity, heavy rain, 90 mph winds, unprotected on top of buildings, mountains and in forests. None of it has had the trouble you&#x27;re suggesting over the course of a decade. I&#x27;ve used those mikrotik extenders to join two terminated cables underground in wet earth buried for years and they&#x27;re fine. I&#x27;m legitimately, honestly curious about what your environment is like, and how power going out for just a few nights, not even weeks, would mean any kind of permanent damage? Direct lightning strikes? That at least is fairly uncommon here, usually double grounding is plenty sufficient though I tend to break exterior links to core with fiber these days just in case.>As far as my last point - i said i don&#x27;t see the point since starlink is CGNAT. It&#x27;s like putting a fiber cable to a cellphone for tethering. you get no benefit.What? There&#x27;s tons of benefit! First, Starlink, and failover cellular dedicated modems, PtMP&#x2F;PtP etc, frequently would be ideally installed hundreds or more feet away from wherever the core rack and router are, which in turn are away from fanned out further switches&#x2F;WAPs. And in high positions more prone to surge. Fiber breaks that link in and has effectively no range limits, while being cheap and easy to work with. What would CGNAT have to do with the tremendous limitations of their router? Say you&#x27;ve got a small LAN covering even just a million square feet, Starlink router won&#x27;t cover that itself. What about nested traffic shaping of various kinds, supporting your 802.1x auth, or arbitrary VLANs (direct or stacked) or a million other things? How about automatic failover with another connection, or shaping, or automated VPN tunneling of given subnets? Which is indeed a trivial way to give even a residential CGNAT IPv4 a static IP, just get a basic VPS somewhere geographically appropriate for access patterns, setup Wireguard on it and then tunnel both ways. Totally transparent to clients, can use PPSK&#x2F;MPSK so that even the most outdated pure wireless stuff with zero radius support can simply be given a password and it&#x27;s on the right place.And second fwiw, Starlink Business does offer a publicly routable IPv4 (though not static), and each Starlink Terminal gets a &#x2F;56 IPv6. And I did mention that terminal specifically didn&#x27;t I?I mean, how is Starlink here any different then nearly any other classic ISP in existence going back decades? It&#x27;s not like there&#x27;s anything wrong with being happy with an ISP supplied default AIO combo deal, most of the population is. But it&#x27;s also always been the case that sometimes there is a need for more. And for businesses that&#x27;s particularly true. Starlink v1 supported that elegantly and near perfectly. Starlink v2 took that away, even for 5x more expensive business terminal. I&#x27;m just saying that&#x27;s unfortunate, clearly unnecessary, and I hope they change course with a v3 or v4 or the like someday. Even if only on the high end one. replygiantg2 16 hours agoprevI wonder if the antenna is expensive or if it&#x27;s the other electronics in the terminal? I assume it uses a phased array. I also assume most phased arrays are pretty cheap. reply teleforce 12 hours agoparentThe antenna or the array antenna should be pretty cheap because they&#x27;re technically passive devices. The expensive parts, however, are active circuitry namely the down conversion circuitry and signal processing for beam steering. The trick is to reliably down convert the signal to low frequency baseband by maintaining its amplitude and phase in which information is encoded since frequency is ephemeral (can be changed and manipulated without losing the information). According to ChatGPT-4, the user bands for Starlink satellites are mainly using Ku-band for downlink between 10.7 to 12.7 GHz (satellite to user) and 14 to 14.5 GHz for uplink (user to satellite).They are 3 ways of doing the frequency conversions namely superheterodyne (Single-IF), Direct conversion (Zero-IF) and the latest Direct RF sampling. The complexity is decreasing, hence the price is decreasing as we go from former to latter. Due to the relatively high frequency of Ku-band, Starlink antenna systems probably also utilize multiple down conversions or multi superheterodyne which make the price to be rather expensive due to the increased complexity of the circuitry and signal processing.Now due to the rapid improvement in RF SoC&#x2F;chip design by company like ADI and TI, we now have RF transceiver chip that can perform Direct RF sampling utilizing high speed ADC&#x2F;DAC operating in Ku-band. Thus we can expect the price to be going down even further. I suspect the decreased in manufacturing for the antenna price is not so much due to the their better manufacturing iterations, increase yield, etc but mainly due to the recent availability of this high speed Direct RF sampling transceivers but I can be wrong. reply bottlepalm 14 hours agoparentprevInside a terminal - https:&#x2F;&#x2F;x.com&#x2F;olegkutkov&#x2F;status&#x2F;1526986473114935298 reply inemesitaffia 11 hours agoparentprevPhased arrays on the market start at $16k$25k is probably what you&#x27;ll spend reply giantg2 10 hours agorootparentI mean, your basic $50 router beamforms using phased array antennas.There&#x27;s also DIY including radar phased array https:&#x2F;&#x2F;hackaday.com&#x2F;2015&#x2F;04&#x2F;07&#x2F;build-a-phased-array-radar-i... reply inemesitaffia 39 minutes agorootparentHow many chips are in there?Anyway beyond that you can always check the market. There&#x27;s phased arrays you can connect to any company.Are those in Ku Band? It makes a difference if you&#x27;re in a market with billions of components in the same RF band reply davidktr 14 hours agoprev [–] Funny, just two hours ago I saw my first Starlink satellite train running across the sky. It felt spooky and futuristic at the same time. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SpaceX will no longer bear the cost of Starlink antennas sold with its satellite Internet service as part of a move to enhance the company's profitability.",
      "The consumer Starlink antennas are sold for $599 each, while prices for high-demand customers can vary from $2,500 to $150,000 each.",
      "SpaceX's success has been tied to the reduced production costs for the terminals, which now cost less than $600 each to manufacture."
    ],
    "commentSummary": [
      "SpaceX has managed to cut the production costs of its Starlink satellite antennas, enabling faster expansion and more affordable services in both rural and densely populated regions.",
      "While Starlink's revenue for 2022 fell short of projections, the company has demonstrated significant progress in performance and growth, and there's ongoing debate about the density benefits of the Starlink constellation and its possible expansion into developing nations.",
      "The recent sale of the company’s satellite internet hardware to rural Canadians and the expected advancements in RF chip design, which could further reduce antenna system costs, hint at the possible commodification of phased array technology."
    ],
    "points": 165,
    "commentCount": 173,
    "retryCount": 0,
    "time": 1694623727
  },
  {
    "id": 37495873,
    "title": "Against LLM Maximalism",
    "originLink": "https://explosion.ai/blog/against-llm-maximalism/",
    "originBody": "AboutSoftwareEventsCustom SolutionsBlog & News Against LLM maximalism MAY 18, 2023 · BY MATTHEW HONNIBAL · ~16 MIN. READ A lot of people are building truly new things with Large Language Models (LLMs), like wild interactive fiction experiences that weren’t possible before. But if you’re working on the same sort of Natural Language Processing (NLP) problems that businesses have been trying to solve for a long time, what’s the best way to use them? Companies have been using language technologies for many years now, often with mixed success. The need to work with text or speech data somewhat intelligently is pretty fundamental. For instance, in most popular websites text is usually either a big part of the product (e.g. the site publishes news or commentary), the usage pattern (e.g. users write text to each other) or the input (e.g. news aggregators). Transacting in language makes up a large percentage of economic activity in general. We all spend a big part of our working lives writing, reading, speaking and listening. So, it makes sense that do-things-with-language has long been a desirable feature request for all sorts of programs. In 2014 I started working on spaCy, and here’s an excerpt of how I explained the motivation for the library: Computers don’t understand text. This is unfortunate, because that’s what the web almost entirely consists of. We want to recommend people text based on other text they liked. We want to shorten text to display it on a mobile screen. We want to aggregate it, link it, filter it, categorize it, generate it and correct it. spaCy provides a library of utility functions that help programmers build such products. Today it’s hard to make the claim computers don’t understand text without at least adding huge asterisks or qualifications. Even if you have a particular philosophical view of what constitutes “understanding”, there’s no doubt that LLMs can construct and manipulate meaning representations sufficient for a wide range of practical purposes. They still make all sorts of mistakes, but it’s relatively rare to feel like they’ve simply failed to connect the words of your input together to form the intended meaning. The text that they generate is also extremely fluent. Sometimes it can be confidently wrong or irrelevant to your question, but it’s almost always made up of real, coherent sentences. That’s definitely new. However, LLMs are not a direct solution to most of the NLP use-cases companies have been working on. They are extremely useful, but if you want to deliver reliable software you can improve over time, you can’t just write a prompt and call it a day. Once you’re past prototyping and want to deliver the best system you can, supervised learning will often give you better efficiency, accuracy and reliability than in-context learning for non-generative tasks — tasks where there is a specific right answer that you want the model to find. Applying rules and logic around your models to do data transformations or handle cases that can be fully enumerated is also extremely important. For instance, let’s say you want to build an online reputation management system. You want to ingest a feed of posts from Twitter, Reddit or some other source, identify mentions of your company or products, and understand common themes between them. Perhaps you also want to monitor mentions of key competitors in a similar way. You might want to view the data in a variety of ways. For instance, you could extract a few noisy metrics, such as a general “positivity” sentiment score that you track in a dashboard, while you also produce more nuanced clustering of the posts which are reviewed periodically in more detail. I don’t want to undersell how impactful LLMs are for this sort of use-case. You can give an LLM a group of comments and ask it to summarize the texts or identify key themes. And the specifics of this can change at runtime: you don’t have to target a very particular sort of summary or question that will be posed. This generative output could be a complete game-changer, finally delivering the “insights” that data science projects have generally over-promised and under-delivered. In addition to these generative components, you could also use LLMs to help with various other parts of the system. But should you? LLMs are new enough, and changing quickly enough, that there’s little consensus on how best to use them. Eventually things will stabilize (or perhaps the AGI will get us all after all), and we’ll have some scars and accreted wisdom about what works and what doesn’t. In the meantime, I want to suggest some common sense. One vision for how LLMs can be used is what I’ll term LLM maximalist. If you have some task, you try to ask the LLM to do it as directly as possible. Need the data in some format? Ask for it in the prompt. Avoid breaking down your task into several steps, as that will prevent the LLM from working through your problem end-to-end. It also introduces extra calls, and can introduce errors in the intermediate processing steps. Of course, LLMs do have limitations, for instance in the recency of their knowledge, or the size of the context you can pass in. So you do have to work around things, and use things like vector databases or other tricks. But fundamentally the LLM maximalist position is that you want to trust the LLM to solve the problem. You’re preparing for the technologies to continue to improve, and the current pain-points to keep reducing over time. It's never great to realize you're the guy on the left. But, here I am. There are two big problems with this approach. One is that “working around the system’s limitations” is often going to be outright impossible. Most systems need to be much faster than LLMs are today, and on current trends of efficiency and hardware improvements, will be for the next several years. Users are pretty tolerant of latency in chat applications, but in almost any other type of user interface, you can’t wait multiple seconds for a single prediction. It’s just too slow. In our online reputation management example, you want to connect to some sort of firehose of data, like Reddit or Twitter. You can’t pass that straight into an LLM — it’s much too expensive. The second problem is that the LLM maximalist approach is fundamentally not modular. Let’s say you’ve made the obvious small compromise, and you’re using a separate classifier to pre-filter texts that maybe mention your company. You’ve developed a prompt that works well with the LLM model you’re using, and you’re getting pretty good output summaries. Now you get a new request. Users want to be able to view some of the raw data. To address this need, your team decides to develop a separate view, where you display the list of mentions along with one sentence of context. How should you go about this? You could craft a separate LLM prompt, where you ask it to extract the data with this second format you’ve been asked for. However, the new prompt is not guaranteed to recognize the same set of mentions as the first prompt you’ve used — inevitably, there will be some differences. This is really not great. You want to be able to link the summaries to the groups of comments they were generated from. If the sentence view is different, you can’t do that. Instead of a separate prompt, you could try to add the information to the first prompt. But now you’re outputting whole sentences, which really increases the number of tokens you generate, which is both slower and more expensive. And you’re struggling to get the same accuracy you had before with this new more complicated output format. What makes a good program? It’s not only how efficiently and accurately it solves a single set of requirements, but also how reliably it can be understood, changed and improved. Programs written with the LLM maximalist approach are not good under these criteria. Instead of throwing away everything we’ve learned about software design and asking the LLM to do the whole thing all at once, we can break up our problem into pieces, and treat the LLM as just another module in the system. When our requirements change or get extended, we don’t have to go back and change the whole thing. We can add new modules, or rearrange the ones we’re already happy with. Breaking up the task into separate modules also helps you to see which parts really need an LLM, and what could be done more simply and reliably with another approach. Recognizing sentence boundaries in English isn’t entirely trivial (you don’t want to just use regular expressions), but it’s definitely not something you need an LLM to do. You can just call into spaCy or another library. It will be vastly faster, and you won’t have to worry that the LLM will trip over some strange input and return some entirely unexpected output. The task of detecting the company mentions is also something you probably don’t need to use an LLM to do. It certainly makes sense to use an LLM for the initial prototyping — this is another huge advantage of LLMs that should not be underestimated. Rapid prototyping is enormously important. You can explore the design space efficiently, and discard ideas that aren’t worth further development. But you also need to be able to go past prototyping. Once you’ve found an idea that’s worth improving, you need a way to actually improve it. Before you can improve any statistical component, you need to be able to evaluate it. It’s important to have some evaluation over your whole pipeline, and if you have nothing else, you can use that to judge whether some change to a component is making things better or worse (this is called “extrinsic evaluation”). But you should also evaluate your components in isolation (“intrinsic evaluation”). For components like the mention detector, this means annotating some texts with the correct labels, setting them aside, and testing your component against them after each change. For generative components, you can’t evaluate against a single set of annotations, but intrinsic evaluation is still possible, for instance using a Likert scale or A/B testing. Intrinsic evaluation is like a unit test, while extrinsic evaluation is like an integration test. You do need both. It’s very common to start building an evaluation set, and find that your ideas about how you expect the component to behave are much vaguer than you realized. You need a clear specification of the component to improve it, and to improve the system as a whole. Otherwise, you’ll end up in a local maximum: changes to one component will seem to make sense in themselves, but you’ll see worse results overall, because the previous behavior was compensating for problems elsewhere. Systems like that are very difficult to improve. A good rule of thumb is that you’ll want ten data points per significant digit of your evaluation metric. So if you want to distinguish 91% accuracy from 90% accuracy, you’ll want to have at least 1000 data points annotated. You don’t want to be running experiments where your accuracy figure says a 1% improvement, but actually you went from 94/103 to 96/103. You’ll end up forming superstitions, based on little but luck. That’s not a path to improvement. You need to be systematic. Once you’ve annotated evaluation data, it’s usually better to just go on and annotate some training data for non-generative components. Supervised learning is very strong for tasks such as text classification, entity recognition and relation extraction. If you have a clear idea of what the component should do and can annotate data accordingly, you can usually expect to get better accuracy than an LLM with a few hundred labelled examples, using a transformer architecture sized for a single GPU, with pretrained representations. This is really just the same model architecture as an LLM, but in a more convenient size, and configured to perform exactly one task. Here’s how I think LLMs should be used in NLP projects today — an approach I would call LLM pragmatism. Break down what you want your application to do with language into a series of predictive and generative steps. Keep steps simple, and don’t ask for transformations or formatting you could easily do deterministically. Put together a prototype pipeline, using LLM prompts or off-the-shelf solutions for all the predictive or generative steps. Try out the pipeline in as realistic a context as you can. Design some sort of extrinsic evaluation. What does success look like here? Net labour saved? Engagement? Conversions? If you can’t measure the utility of the system directly, you can use some other sort of metric, but you should try to make it as meaningful as possible. If false negatives matter more than false positives, account for that in your extrinsic evaluation metric. Experiment with alternative pipeline designs. Try to create tasks where the correct answer makes sense independent of your use-case. Prefer text classification to entity recognition, and entity recognition to relation extraction (faster to annotate, accuracy is better). Pick a predictive (as opposed to generative) component and spend two to five hours labelling annotation data for it. Measure the LLM-powered component’s accuracy using your evaluation data. Use the LLM-powered component to help you create training data, to train your own model. One approach is to simply save out the LLM-powered component’s predictions, and trust they’re good enough. This is a good thing to try if the LLM-powered component’s accuracy seems more than sufficient for your needs. If you need better accuracy than what the LLM is giving you, you need example data that’s more correct. A good approach is to load the LLM predictions into an annotation tool and fix them up. Train a supervised model on your new training data, and evaluate it on the same evaluation data you used previously. To decide whether you should annotate more training data, run additional experiments where you hold part of your training data back. For instance, compare how your accuracy changed when you use 100%, 80% and 50% of the data you have. This should help you determine how your accuracy might look if you had 120% or 150%. However, be aware that if your training set is small, there can be a lot of variance in your accuracy. Try a few different random seeds to figure out how much your accuracy changes simply due to chance, to help put your results into perspective. Repeat this process with any other predictive components. At the end of all this, you’ll have a pipeline suitable for production. It will run much more quickly and accurately than a chain of LLM calls, and you’ll know that no matter what text is passed in, your predictive components will always give you valid output. You’ll have evaluations for the different steps, and you’ll be able to attribute errors to different components. If you need to change the system’s behavior, you’ll be able to put in new rules or transformations at different points of the pipeline, without having to go back and re-engineer your prompts or rewrite your response parsing logic. Some of these steps are still a bit harder than they should be, especially if you haven’t been working with machine learning before. This is where I have high hopes for LLMs. LLMs are indeed very powerful, and they can make a lot of things much easier. They can help us build better systems, and that’s how we should use them. The approach that I’ve called LLM maximalist is actually unambitious. It uses LLMs to easily get to a system that is worse — worse in cost, worse in runtime, worse in reliability, worse in maintainability. Instead, we should use LLMs to help us get to systems which are better. This means leaning on LLMs much more during development, to break down knowledge barriers, create data, and otherwise improve our workflow. But the goal should be to call out to LLMs as little as possible during runtime. Let LLMs train their own cheaper and more reliable replacements. Appendix 1: Putting it into practice There are lots of tools and libraries you can use to label your own data and train your own NLP models. HF Transformers and spaCy (our library) are the two most popular libraries for this. Transformers makes it easy to use a wide variety of models from recent research, and it’s closer to the underlying ML library (PyTorch). spaCy has better data structures for working with annotations, support for pipelines that mix statistical and rule-based operations, and more frameworky features for configuration, extension and project workflows. We recently released spacy-llm, an extension that lets you add LLM-powered components to your spaCy pipelines. You can mix LLM with other components, and make use of spaCy’s Doc, Span, Token and other classes to make use of the annotations. Let’s say you want to create a pipeline that detects some entities, and then you want to get the sentences that the entities occur in. Here’s how that looks, building and using the pipeline in Python: Usage example import spacy nlp = spacy.blank(\"en\") nlp.add_pipe(\"sentencizer\") nlp.add_pipe( \"llm\", config={ \"task\": { \"@llm_tasks\": \"spacy.NER.v1\", \"labels\": \"SAAS_PLATFORM,PROGRAMMING_LANGUAGE,OPEN_SOURCE_LIBRARY\" }, \"backend\": { \"@llm_backends\": \"spacy.REST.v1\", \"api\": \"OpenAI\", \"config\": {\"model\": \"text-davinci-003\"}, }, }, ) doc = nlp(\"There's no PyTorch bindings for Go. We just use Microsoft Cognitive Services.\") for ent in doc.ents: print(ent.text, ent.label_, ent.sent) The sentencizer component in spaCy uses rules to detect sentence boundaries, and the llm component is here configured to perform named entity recognition, with the given labels. Task handlers are provided for some other common NLP tasks, but you can also define your own functions to perform arbitrary tasks — you just need to add a decorator to your function and obey the correct signature. The sentence and entity annotations (accessed via the doc.sents and doc.ents in this example) are both accessible as sequences of Span objects, which is like a labelled slice of the Doc object. You can iterate over the tokens in the span, get its start and end character offset, and (depending on the components you in your pipeline) access embeddings or compute similarities. Components can assign multiple overlapping layers of Span annotations, and you can design and assign extension attributes to conveniently access additional properties. Obviously, I’m pretty proud of these parts of spaCy, but what do they have to do with LLMs? You can prompt an LLM with a command like, “How many paragraphs in this review say something bad about the acting? Which actors do they frequently mention?”. For once-off personal tasks, this is absolutely magical. But if you’re building a system and you want to calculate and display this information for every review, it’s very nice to just approach this as separate prediction tasks (tagging names, linking them to a knowledge base, and paragraph-level actor sentiment), with data structures that let you flexibly access the information. The new LLM support in spaCy now lets you plug in LLM-powered components for these prediction tasks, which is especially great for prototyping. This functionality is still quite new and experimental, but it’s already very fun to explore. Another piece of tooling you’ll need is some sort of solution for data annotation. You can just load things up in a spreadsheet or text editor, but if you’re doing this repeatedly, it’s worth using something better. We make a commercial annotation tool, Prodigy, which emphasizes customizability and model-assisted workflows. Prodigy isn’t free, but it’s a one-time purchase per license, and it fits well into local workflows. A key design idea in Prodigy is model assistance: calling into a model to get initial annotations, and letting you review and fix them. This works especially well with LLMs, and we’ve been building out support for it over the last six months. Prodigy v1.12 will feature integrated support for LLM annotation assistance, with support for a choice of backends, including open-source solutions you can host yourself. Prodigy also supports A/B evaluation for generative outputs, which applies especially well for LLMs. This functionality is also extended in v1.12. For instance, you can design a number of different prompts, and run a tournament between them, by answering a series of A/B evaluation questions where you pick which of two outputs is better without knowing which prompt produced them. This lets you perform prompt engineering systematically, based on decisions you can record and later review. Appendix 2: Accuracy of supervised and in-context learning Large Language Models (LLMs) can be used for arbitrary prediction tasks, by constructing a prompt describing the task, giving the labels to predict, and optionally including a relatively small number of examples in the prompt. This approach doesn’t involve any direct updates to the model for the new task. However, LLMs seem to learn a general ability to continue patterns, including abstract ones, from their language model objective. The mechanics are still under investigation, but see for example Anthropic’s work on induction heads (Olsson et al., 2022). In supervised learning (often referred to as fine-tuning, in the context of language models), the model is provided a set of labelled example pairs, and the weights are adjusted such that some objective function is minimized. In modern NLP, supervised learning and language model pretraining are closely linked. Knowledge about the language generalizes between tasks, so it’s desirable to somehow initialize the model with that knowledge. Language model pretraining has proven to be a very strong general answer to this requirement. In my opinion the best things to read on this are articles from when the developments were relatively fresh, such as Sebastian Ruder’s 2018 blog post NLP’s ImageNet moment has finally arrived. OpenAI evaluated GPT-3’s in-context learning capabilities against supervised learning in a variety of configurations (Brown et al., 2020). The results in Section 3.7, on the SuperGLUE benchmark, are the most directly relevant to general NLP prediction tasks such as entity recognition or text classification. In their experiments, OpenAI prompted GPT3 with 32 examples of each task, and found that they were able to achieve similar accuracy to the BERT baselines. These results were the first big introduction to in-context learning as a competitive approach, and they’re indeed impressive. However, they were well below the state-of-the-art accuracy when they were published, and the current state-of-the-art results on the SuperGLUE leaderboard all involve supervised learning, not just in-context learning. Some subtasks of the SuperGLUE benchmark suite have very small training sets, and on these tasks in-context learning is competitive. I’m not aware of any current NLP benchmarks where more than a few hundred training samples are available, and the leading systems rely solely on in-context learning. The point of in-context learning has never been to be the absolute highest accuracy way to have a model perform some specific task. Rather, it’s an impressive compromise: it’s extremely sample efficient (you don’t need many examples of your task), and you don’t have to pay the upfront computational cost of training. In short, the advantage of in-context learning is lower overhead. But the longer your project lives, the less this should be seen as a dominant advantage. The overheads get amortized away. Finally, it’s important to realize that SuperGLUE and other standard NLP benchmarks are specifically designed to be quite challenging. Easy tasks don’t make good benchmarks. This is exactly opposite to NLP applications, where we want tasks to be as easy as possible. Most practical tasks don’t require powerful reasoning abilities or extensive background world knowledge, which are the things that really set LLMs apart from smaller models. Rather, practical tasks usually require the model to learn a fairly specific set of policies, and then apply them consistently. Supervised learning is a good fit for this requirement. ABOUT THE AUTHOR Matthew Honnibal CTO, Founder Berlin, Germany Introducing spaCy v3.6 spaCy v3.6 introduces the span finder component and trained pipelines for Slovenian. Implementing a custom trainable component for relation extraction Relation extraction refers to the process of predicting and labeling semantic relationships between named entities. In this blog post, we'll go over the process of building a custom relation extraction component using spaCy and Thinc. We'll also add a Hugging Face transformer to improve performance at the end of the post. You'll see how you can utilize Thinc's flexible and customizable system to build an NLP pipeline for biomedical relation extraction. The Tale of Bloom Embeddings and Unseen Entities The default Bloom embedding layer in spaCy is unconventional, but very powerful and efficient. We wrote about it before and showed the advantages it provides in terms of memory efficiency for our floret embeddings. Now we have released the first technical report by Explosion 💥, where we explain Bloom embeddings in more detail and rigorously compare them to traditional embeddings. In this post we'll highlight some of our results with a special focus on *unseen* entities. Deploying a Prodigy cloud service for Posh’s financial chatbots A Prodigy case study of Posh AI's production-ready annotation platform and custom chatbot annotation tasks for banking customers. About us Explosion is a software company specializing in developer tools for AI and Natural Language Processing. We’re the makers of spaCy, one of the leading open-source libraries for advanced NLP. Navigation Home About us Software & Demos Talks & Events Custom Solutions Blog & News Jobs Our Software spaCy · Industrial-strength NLP Prodigy · Radically efficient annotation Open Source · Our other libraries © 2016-2023 Explosion · Legal & Imprint · Status",
    "commentLink": "https://news.ycombinator.com/item?id=37495873",
    "commentBody": "Against LLM MaximalismHacker NewspastloginAgainst LLM Maximalism (explosion.ai) 163 points by pmoriarty 21 hours ago| hidepastfavorite62 comments peter_l_downs 19 hours agoSpacy [0] is a state-of-art &#x2F; easy-to-use NLP library from the pre-LLM era. This post is the Spacy founder&#x27;s thoughts on how to integrate LLMs with the kind of problems that \"traditional\" NLP is used for right now. It&#x27;s an advertisement for Prodigy [1], their paid tool for using LLMs to assist data labeling. That said, I think I largely agree with the premise, and it&#x27;s worth reading the entire post.The steps described in \"LLM pragmatism\" are basically what I see my data science friends doing — it&#x27;s hard to justify the cost (money and latency) in using LLMs directly for all tasks, and even if you want to you&#x27;ll need a baseline model to compare against, so why not use LLMs for dataset creation or augmentation in order to train a classic supervised model?[0] https:&#x2F;&#x2F;spacy.io&#x2F;[1] https:&#x2F;&#x2F;prodi.gy&#x2F; reply og_kalu 18 hours agoparent>what I see my data science friends doing — it&#x27;s hard to justify the cost (money and latency) in using LLMs directly for all tasks, and even if you want to you&#x27;ll need a baseline model to compare against, so why not use LLMs for dataset creation or augmentation in order to train a classic supervised model?The NLP infrastructure and pipelines we have today aren&#x27;t there because they are necessarily the best way to handle the tasks you want. They&#x27;re in place because computers simply could not understand text the way we would like and shortcuts, approximations were necessary.Borrowing from the blog, Since you could not simply ask the computer, \"How many paragraphs in this review say something bad about the acting? Which actors do they frequently mention?\", separate processes of something like tagging names, linking them to a knowledge base, and paragraph-level actor sentiment etc were needed.The approximations are cool and they do work rather well for some use cases but they fall apart in many others.This is why automated resume filtering, moderation etc is still awful with the old techniques. You simply can&#x27;t do what is suggested above and get the same utility. reply PheonixPharts 8 hours agoparentprev> why not use LLMs for dataset creation or augmentation in order to train a classic supervised model?Or, as I mentioned in another comment, just use the embeddings directly. This also does a lot to remove the \"cost (money and latency)\" part of the problem since you can batch queries to be lightening fast and the dollar cost of generating the embeddings is effectively zero (~3000 pages of text per $1) for most traditional NLP tasks that require a vector representation. reply azinman2 8 hours agorootparentEmbeddings only go so far. A lot of time meaning (especially implicit or contextual) occurs from reading larger portions &#x2F; knowing related information that will simply not be captured in the embedding. reply daveguy 7 hours agorootparentIf it&#x27;s not captured in the embeddings the LLM wouldn&#x27;t \"know\" it either, right? reply bugglebeetle 6 hours agorootparentprevHow are you batching queries and search in a way that’s “lightning fast”? I’ve not found this to be the case for most vector DBs, especially the building the index part. reply PheonixPharts 9 hours agoprevI personally still think most people (not necessarily the author) miss out on the biggest improvement LLMs have to offer: powerful embeddings for text representation for text classification.All of the prompting stuff is, of course, incredible, but the use of these models to create text embeddings of virtually any text document (from a sentence to a news paper article) allows for incredibly fast iteration on many traditional ML text classification problems.Multiple times I&#x27;ve taken cases where I have ~1,000 text documents with labels, run them through ada-002, and stuck that in a logistic model and gotten wildly superior performance to anything I&#x27;ve tried in the past.If you have an old NLP classification problem that you couldn&#x27;t quite solve satisfactorily enough a few years ago, it&#x27;s worth just mindlessly running it through the OpenAI embeddings API and sticking using those embeddings on your favorite off the shelf classifier.Having done NLP work for many years, it is insane to me to consider how many countless hours I spent doing tricky feature engineering to try to squeeze the most information I could out of the limited text data available, to realize it can now be replaced with about 10 minutes of programming time and less than a dollar.An even better improvement is the trivial ability to scale to real documents. It wasn&#x27;t long ago that the best document models were just sums&#x2F;averages of word embeddings. reply jiggawatts 4 hours agoparentI keep hearing about text classification but I can’t think of many specific use cases.If you don’t mind me asking: what are you using text classification for? reply stormfather 2 hours agorootparentAsk ChatGPT and you&#x27;ll get a ton. A few I&#x27;ve run into personally:TSLA is going to the moon. ^ Is this tweet bearish or bullish regarding the asset it mentions?(Acute) hepatitis C Hepatitis B; Acute ^ Do the above refer to the same disease?The Federal Reserve decides to abolish interest rates on leap years. Is it a leap year? New policy from the Fed says no interest if so. ^ Do these refer to the same news story, or different ones?So you can see that text classification is useful for consolidating and integrating streams of textual information, and extracting actionable meaning. reply 3abiton 35 minutes agorootparentI&#x27;m curious about the benchmarks for those tasks compared with spacy for example. I have used it before and I wonder if using GPT-3.5 justifies the pricing. reply jiggawatts 1 hour agorootparentprevIsn&#x27;t that just asking ChatGPT yes&#x2F;no questions?Why use a classifier instead of just having it answer the question directly? reply alexvitkov 19 hours agoprevSorry if this is a bit ignorant, I don&#x27;t work in the space, but if a single LLM invocation is considered too slow, how could splitting it up into a pipeline of LLM invocations which need to happen in sequence help?Same with reliability - you don&#x27;t trust the results of one prompt, but you trust multiple piped one into another? Even if you test the individual components, which is what this approach enables and this article heavily advocates for, I still can&#x27;t imagine that 10 unreliable systems, which have to interact with rach other, are more reliable than one.80% accuracy of one system is 80% accuracy.95% accuracy on 10 systems is 59% accuracy in total if you need all of them to work and they fail independently. reply syllogism 18 hours agoparent(Author here)About the speed, the idea is that if you break down the task, you can very often use much smaller models for the component tasks. LLMs are approaching prediction tasks under an extremely difficult constraint: they don&#x27;t get to see many labelled examples. If you relax that constraint and just use transfer-learning, you can get better accuracy with much smaller models. The transfer-learning pipeline can also be arranged so that you encode the text into vectors once, and you apply multiple little task networks over the shared representation. spaCy supports this for instance, and it&#x27;s easy to do when working directly with the networks in PyTorch etc. reply daveguy 7 hours agorootparentWhat does it mean to \"just use transfer-learning\"? I thought this was very much an unsolved task, even with LLMs. reply syllogism 4 hours agorootparentThere&#x27;s definitely active research on it. But here&#x27;s the basic recipe that gets state-of-the-art accuracy on most tasks. It&#x27;s been around for 5 years or so now, which is why I said \"just\".You take an encoder transformer architecture like BERT and train it on a language modelling objective. Then you discard the part of the network that does the token prediction, and stick some randomly initialised network that does a specific task on. This is generally kept minimal. For classification tasks, often people just connect a linear layer to the vector that represents a dummy sentinel token after the sequence.The general goal is to exploit the representations learned during the language modelling task, so that the network starts out knowing general grammatical structure of the language, multi-word expressions, can distinguish word senses from each other, etc. Then it needs far fewer examples to learn a specific task. There&#x27;s definitely transfer learning going on: if you initialise the network randomly instead of via the LM objective, results are very much worse.This is a good blog post from when the recipe was still fairly new: https:&#x2F;&#x2F;www.ruder.io&#x2F;nlp-imagenet&#x2F; reply peter_l_downs 19 hours agoparentprevI think the idea behind breaking down the task into a composable pipeline is that you then replace the LLM steps in a pipeline with supervised models that are much faster. So you end up with a pipeline of non-LLM models, which are faster and more explainable. reply vjerancrnjak 18 hours agoparentprevIt&#x27;s not ignorant. It is a known problem. Before LLMs, approaches to machine translation or any high level language tasks did start with a pipeline (part of speech tagging, dependency tree parsing, named entity recognition etc.) but quickly these attempts were discarded.All of the models in the pipeline are not optimized with the joint loss (the final machine translation model that maps lang A to lang B does not propagate its error to the low level models in the pipeline).A pipeline of LLMs will accumulate the error in the same way, eventually the same underlying problem of pipeline not being trained with the joint loss will result in low accuracy.LLMs or DNNs in general do more compute, so they start being extremely powerful even when sequenced. Making a sequence of decisions with a regular ML model has a similar problem to pipelining, if you train it on single decision loss and not the sequence of decisions loss, then there&#x27;s a question of can it recover and make a right next step if it made the wrong step (your training data never included this recovery example), but convolutional NNs were so powerful for language tasks that this recovery from error was successful (even though you never trained CNNs over the joint loss of sequence of decision). reply visarga 18 hours agorootparentIt&#x27;s not a given that the performance would suffer. For instance, you could use self-checking methods like cycle consistency or back translation in a sequence of prompts. Another option is to generate multiple answers and then use a voting system to pick the best one. This could actually boost the LLM&#x27;s accuracy, although it would require more computation. In various tasks, there might be simpler methods for verifying the answer than initially generating it.Then you have techniques like the Tree of Thoughts, which are particularly useful for tasks that require strategic planning and exploration. You just can&#x27;t solve these in one single round of LLM interaction.In real-world applications, developers often choose a series of prompts that enable either self-checking or error minimization. Alternatively, they can involve a human in the loop to guide the system&#x27;s actions. The point is to design with the system&#x27;s limitations in mind.On a side note, if you&#x27;re using vLLM, you can send up to 20 requests in parallel without incurring additional costs. The server batches these requests and uses key-value caching, so you get high token&#x2F;s throughput. This allows you to resend previous outputs for free or run multiple queries on a large text segment. So, running many tasks doesn&#x27;t necessarily slow things down if you manage it correctly. reply vjerancrnjak 13 hours agorootparentIt is a simple problem and in literature it was named \"label bias\".Let&#x27;s say you maximize performance of a single piece of pipeline (training on a dataset or something else), and you do it the same way for all pieces. The labels that were correct as inputs in training are your limitation. Why? Because when a mistake happens, you&#x27;ve never learned to recover from it, because you always gave the correct labels in your training.What LLM pipelines do is probably something like this:* a complex task is solved by a pipeline of prompts* we tweak a single prompt* we observe the output at the end of the whole pipeline and determine if the tweak was rightIn this way, the joint loss of the pipeline is observed and that is ok.But, the moment your pipeline is: POS Tagger -> Dependency Tree Parser -> Named Entity Recognition -> ... -> Machine Translationand you have separate training sets that maximize performance of each particular piece, you are introducing label bias and are relying on some luck to recover from errors early in the pipeline because during training, the later parts never got errors as input and recovered to the correct output. reply cmcaleer 19 hours agoparentprev> you don&#x27;t trust the results of one promt, but you trust multiple piped one into another?This is really not at all unusual. Take aircraft for instance. One system is not reliable, for a multitude of reasons. A faulty sensor could be misleading, a few bits could get flipped by cosmic rays causing ECC to fail, the system itself could be poorly calibrated, there are far too many unacceptable risks. But add TMR[0][1] and suddenly you are able to trust things a lot more. This isn&#x27;t to say that TMR is bullet proof e.g. incidents like [2], but redundancy does make it possible to increase trust in a system, and assign blame to what part of a system is faulty (e.g. if 3 systems exist, and 1 appears to be disagreeing wildly with 2 and 3, you know to start investigating system 1 first).Would it work here? I don&#x27;t know! But it doesn&#x27;t seem like an inherently terrible or flawed idea if we look at past applications. Ensembling different models is a pretty common technique to get better results in ML, and maybe this approach would make it easier to find weak links and assign blame.[0]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Triple_modular_redundancy[1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Air_data_inertial_reference_un...[2]: https:&#x2F;&#x2F;www.atsb.gov.au&#x2F;media&#x2F;news-items&#x2F;2022&#x2F;pitot-probe-co... causing total confusion among the TMR reply chongli 18 hours agorootparentThis isn&#x27;t to say that TMR is bullet proof e.g. incidents like [2], but redundancy does make it possible to increase trust in a system, and assign blame to what part of a system is faulty (e.g. if 3 systems exist, and 1 appears to be disagreeing wildly with 2 and 3, you know to start investigating system 1 first).You can only gain trust in this system if you understand the error sources for all three systems. If there’s any common mode errors then you can see errors showing up in multiple systems simultaneously. For example, if your aircraft is using pitot tubes [1] to measure airspeed then you need to worry about multiple tubes icing up at the same time (which is likely since they’re in the same environment).So it would not add very much trust to implement TMR with three different pitot tubes. It would be better to combine the pitot tubes with completely different systems, such as radar and GPS, to handle the (likely) scenario of two or more pitot tubes icing up and failing completely.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pitot_tube?wprov=sfti1 reply phillipcarter 19 hours agoprevSo I think this is an excellent post. Indeed, LLM maximalism is pretty dumb. They&#x27;re awesome at specific things and mediocre at others. In particular, I get the most frustrated when I see people try to use them for tasks that need deterministic outputs and the thing you need to create is already known statically. My hope is that it&#x27;s just people being super excited by the tech.I wanted to call this out, though, as it makes the case that to improve any component (and really make it production-worthy), you need an evaluation system:> Intrinsic evaluation is like a unit test, while extrinsic evaluation is like an integration test. You do need both. It’s very common to start building an evaluation set, and find that your ideas about how you expect the component to behave are much vaguer than you realized. You need a clear specification of the component to improve it, and to improve the system as a whole. Otherwise, you’ll end up in a local maximum: changes to one component will seem to make sense in themselves, but you’ll see worse results overall, because the previous behavior was compensating for problems elsewhere. Systems like that are very difficult to improve.I think this makes sense from the perspective of a team with deeper ML expertise.What it doesn&#x27;t mention is that this is an enormous effort, made even larger when you don&#x27;t have existing ML expertise. I&#x27;ve been finding this one out the hard way.I&#x27;ve found that if you have \"hard criteria\" to evaluate (i.e., getting the LLM to produce a given structure rather than an open-ended output for a chat app) you can quantify improvements using Observability tools (SLOs!) and iterating in production. Ship changes daily, track versions of what you&#x27;re doing, and keep on top of behavior over a period of time. It&#x27;s arguably a lot less \"clean\" but it&#x27;s way faster, and because it&#x27;s working on the real-world usage data, it&#x27;s really effective. An ML engineer might call that some form of \"online test\" but I don&#x27;t think it really applies.At any rate, there are other use cases where you really do need evaluations, though. The more important correct output is, the more it&#x27;s worth investing in evals. I would argue that if bad outputs have high consequences, then maybe LLMs also aren&#x27;t the right tech for the job, but that&#x27;ll probably change in a few years. And hopefully making evaluations will be easier too. reply syllogism 19 hours agoparent(Author here)It&#x27;s true that getting something going end-to-end is more important than being perfectionist about individual steps -- that&#x27;s a good practical perspective. We hope good evaluation won&#x27;t be such an enormous effort. Most of what we&#x27;re trying to do at Explosion can be summarised as trying to make the right thing easy. Our annotation tool Prodigy is designed to scale down to smaller use-cases for instance ( https:&#x2F;&#x2F;prodigy.ai ). I admit it&#x27;s still effort though, and depending on the task, may indeed still take expertise. reply axiom92 18 hours agoparentprev> tasks that need deterministic outputs and the thing you need to create is already known staticallyWow, interesting. Do you have any example for this?I&#x27;ve realized that LLMs are fairly good at string processing tasks that a really complex regex might also do, so I can see the point in those. reply phillipcarter 18 hours agorootparentYeah, there&#x27;s a little bit of flex there for sure. An example that recently came up for me at work was being able to take request:response pairs from networking events and turn them into a distributed trace. You can absolutely get an LLM to do that, but it&#x27;s very slow and can mess up sometimes. But you can also do this 100% programmatically! The LLM route feels a little easier at first but it&#x27;s arguably a bad application of the tech to the problem. I tried it out just for fun, but it&#x27;s not something I&#x27;d ever want to do for real.(separately, synthesizing a trace from this kind of data is impossible to get 100% correct for other reasons, but hey, it&#x27;s a fun thing to try) reply intended 18 hours agorootparentprevClassification tasks come to mind reply og_kalu 18 hours agorootparentLLMs are better at that though. Sure you may not require them but it certainly wouldn&#x27;t be for a lack of accuracy.https:&#x2F;&#x2F;www.artisana.ai&#x2F;articles&#x2F;gpt-4-outperforms-elite-cro...https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.15056 reply janalsncm 8 hours agorootparentThat compares ChatGPT to mechanical turk, not to a smaller, more specialized model. Mechanical Turk is just crowdsourcing. reply og_kalu 8 hours agorootparentThe first one also compares GPT-4 to the researches themselves. Smaller specialized models don&#x27;t beat humans at these tasks. That&#x27;s why turk is used here in the first place (It&#x27;s certainly not cheaper) and why GPT beating them is worthy of a paper on its own. reply janalsncm 7 hours agorootparentWell it really depends on the task. If it can be done with a regex, use a regex. We can’t make categorical statements about LLMs being better. It depends.You can also probably distill a large model to a smaller one while maintaining a lot of performance. DistillBert is almost as good as Bert at a fraction of the inference cost.GPT-3.5 and 4 also currently aren’t deterministic even with temperature zero, which is a nightmare for debugging. replymark_l_watson 21 hours agoprevI agree with much of the article. You do need to take great care to make code with embedded LLM use modular and easily maintainable, and otherwise keep code bases tidy.I am a fan of tools like LangChain that bring some software order to using LLMs.BTW, this article is a blog hosted by the company who writes and maintains the excellent spaCy library. reply passion__desire 19 hours agoparentIs anyone working on a OS LLM layer? e.g. consider a program like gimp. It would feed in its documentation and workflow details in LLM and get embeddings which would be installed with the program just like man-pages. Users could just express what they want to do in natural languages and Gimp would just query llm and create a workflow that might achieve the task. reply mark_l_watson 16 hours agorootparentApple&#x27;s CoreML is a large collection of regular models, deep learning models, etc. that are easy to use in macOS&#x2F;iOS&#x2F;iPadOS apps. reply __loam 11 hours agoparentprev> You do need to take great care to make code with embedded LLM use modular and easily maintainable, and otherwise keep code bases tidy.Sure makes sense.> I am a fan of tools like LangChain that bring some software order to using LLMs.Lmao. I feel like tools like LangChain that are really just very thin wrappers for the LLM APIs are quite complex for what they supposedly do for you. Lots of leaky abstractions and indirection for very little gained over just calling the APIs themselves. reply sudb 19 hours agoprevI&#x27;ve had a fair amount of success at work recently with treating LLMs - specifically OpenAI&#x27;s GPT-4 with function calling - as modules in a larger system, helped along powerfully by the ability to output structured data.> Most systems need to be much faster than LLMs are today, and on current trends of efficiency and hardware improvements, will be for the next several years.I think here I disagree with the author here though, and am happy to be a technological optimist - if LLMs are used modularly, what&#x27;s to stop us in a few years (presumably still hardware requirement costs, on reflection) eventually having small, fast specialised LLMs for the things that we find them truly useful&#x2F;irreplaceable? reply syllogism 18 hours agoparentNothing&#x27;s to stop us, and in fact we can do that now! This is basically what the post advocates for: replacing the LLM calls for task-specific things with smaller models. They just don&#x27;t need to be LLMs. reply skybrian 18 hours agoprevI don’t understand this heuristic and I think it might be a bit garbled. Any idea what the author meant? How do you get 1000?> A good rule of thumb is that you’ll want ten data points per significant digit of your evaluation metric. So if you want to distinguish 91% accuracy from 90% accuracy, you’ll want to have at least 1000 data points annotated. You don’t want to be running experiments where your accuracy figure says a 1% improvement, but actually you went from 94&#x2F;103 to 96&#x2F;103. reply akprasad 18 hours agoparentMy guess is that this should be something like \"If you have n significant digits in your evaluation metric, you should have at least 10^(n+1) data points.\" reply wrs 18 hours agorootparentAvoiding the term “significant digits” completely: Distinguishing 91 vs 90 is a difference of 1 on a 0-100 scale. 100x10=1000. If you wanted to distinguish 91.0 vs 90.9, that’s 1 on a 0-1000 scale, so you’d want 10,000 points. reply forward-slashed 11 hours agoprevAll of this is quite difficult without the DSL to explore and construct pipelines for LLMs. Current approaches are very slow in terms of iteration. reply og_kalu 19 hours agoprevI&#x27;ll just say there&#x27;s no guarantee training or fine-tuning a smaller bespoke model will be more accurate (Certainly though, it may be accurate enough). Minerva and Med-Palm are worse than GPT-4 for instance. reply syllogism 18 hours agoparentThis is where the terminology being used to discuss LLMs today is a touch awkward and imprecise.There&#x27;s a key distinction between smaller models trained with transfer-learning, and just fine-tuning a smaller LLM and still using in-context learning.Transfer learning means you&#x27;re training an output network specifically for the task you&#x27;re doing. So like, if you&#x27;re doing classification, you output a vector with one element per class, apply a softmax transformation, and train on a negative log likelihood objective. This is direct and effective.Fine-tuning a smaller LLM so that it&#x27;s still learning to do text generation, but it&#x27;s better at the kinds of tasks you want to do, is a much more mixed experience. The text generation is still really difficult, and it&#x27;s really difficult to learn to follow instructions. So all of this still really favours size. reply og_kalu 18 hours agorootparentRight that is a good distinction. Fair enough. Still stand that you could train a worse model depending on the task. Translation, Nuanced Classification are all instances where i&#x27;ve not seen bespoke models outright better than GPT-4. although, like i said it could still be good enough for speed, compute requirements. reply Grimburger 20 hours agoprev [20 more] [flagged] davepeck 20 hours agoparentExplosion is an old school machine learning company by the people who built the spaCy natural language library. They’re serious practitioners whose work predates the “hype-train” you’re concerned about.The blog post might be worth a gander. reply naillo 20 hours agoparentprevThe conditional probability that the article is AI written is also so much larger when you encounter .ai tld&#x27;s. reply EGreg 20 hours agoparentprev [–] I predicted that AI will be the next Web3 — hugely promising but increasingly ignored by HN.There will be waves of innovation in the coming years. Web3 solutions will mostly enrich people or at worst be zero-sum. While AI solutions will redistribute wealth from the working class to the top 1% and corporations, as well as giving people ways to take advantage of vulnerable people and systems at a scale never seen before. reply xnx 20 hours agorootparentWeb3 never got past the idea stage and was never useful. Generative AI is already useful and actively used by millions of people in their daily work. reply EGreg 19 hours agorootparentSame predictable comment every time, and by the same exact people too. The first part is not even close to being true. And you never mention that the downsides of AI are astronomically larger than Web3. The downside of Web3 is bugs in immutable smart contracts where people lose only what they voluntarily put in. The downside of AI is human extinction and people losing in all kinds of ways regardless of whether they want to or not. reply phillipcarter 19 hours agorootparentHow can you hold the opinion that AI is both not useful and that it&#x27;s to bring human extinction?Anyways, incoherence of your argument aside, I&#x27;ll gladly raise my hand as having a use case that LLMs immediately solved for and it&#x27;s now a core product feature that performs well. reply EGreg 19 hours agorootparentMany things are not useful and can bring about human extinction. Viruses, volcanoes, or asteroid inpacts for instance. So it’s not incoherent on its face.But I am not even saying that AI is not useful. I am saying that every single time someone pops up on HN to defend AI vs Web3, they only focus on possible upside right now for some people. Even if AI brings 10000x downside at the same time, they would never ever consider mentioning that. But when society adopts a new technology, downsides matter even more than upsides. Loss of income stability and livelihood for many professions, attacks at scale (eg on reputation or truth) by botswarms, etc etc. And that is just what’s possible with current technology.But most of all, for all its upsides, Web3’s harm is limited to those who voluntarily commit some of their money to a smart contract. AI’s harm on the other hand is far greater and is spread primarily out to those who DIDNT VOLUNTARILY CHOOSE IT or even oppose it. That is not very moral as a society. It may enrich the tech bros further, but just like other tech projects, it will probably come at the expense of many others, especially the working class of society. They will have a rude awakening and will riot. But they aren’t rioting about Web3, because losing money you put at risk in a controlled environment is just not in the same stratosphere.Expect the government to use AI to control the population more and more as this civil unrest happens. Look to China to see what it would look like. Or Palantir for precrime etc. reply phillipcarter 18 hours agorootparentI guess I&#x27;ll just say that...I don&#x27;t believe much of what you&#x27;re saying is going to happen? I don&#x27;t think I&#x27;ll convince you and I don&#x27;t think you&#x27;ll convince me either. reply j16sdiz 18 hours agorootparentprevThree post from you in this thread. I downvote two and upvote one.Sometimes, unpopular opinion need more explanation. The other two comments are not helpful, this comment is helpful. reply EGreg 18 hours agorootparentThanks. Well — prepare to be downvoted by the anti Web3 brigade heh replyatomicnumber3 20 hours agorootparentprevHow is Web3 doing these days, I must ask?The only thing I&#x27;ve heard of it recently is that 4chan is still doing good business selling ads for NFT and coin scams. reply EGreg 19 hours agorootparentGrowing at a CAGR of 44%https:&#x2F;&#x2F;www.globenewswire.com&#x2F;en&#x2F;news-release&#x2F;2023&#x2F;03&#x2F;22&#x2F;263...Expected to hit $50 billion by 2030https:&#x2F;&#x2F;www.emergenresearch.com&#x2F;amp&#x2F;industry-report&#x2F;web-3-ma...And for example $1.1 Billion in Indiahttps:&#x2F;&#x2F;m.economictimes.com&#x2F;tech&#x2F;technology&#x2F;indian-web3-indu... reply hk__2 19 hours agorootparent> Expected to hit $50 billion by 2030The definition of \"web3\" is too vague to have a correct estimation: it will be $50B according to your second link; $44B by 2031 according to your first link; $33B according to [1]; $45 according to [2]; $16B according to [3].[1]: https:&#x2F;&#x2F;www.grandviewresearch.com&#x2F;press-release&#x2F;global-web-3...[2]: https:&#x2F;&#x2F;www.vantagemarketresearch.com&#x2F;industry-report&#x2F;web-30...[3]: https:&#x2F;&#x2F;www.skyquestt.com&#x2F;report&#x2F;web-3-0-blockchain-market reply IshKebab 18 hours agorootparentprev [–] Since when was web3 hugely promising Web3 is rightfully being ignored because it is useless.AI is already extremely useful. There&#x27;s zero chance that it&#x27;s a fad that will fizzle out. I&#x27;m not sure how anyone could come to that conclusion. reply EGreg 18 hours agorootparent [–] Web3 being hugely promising doesn’t mean AI will fizzile out. That’s a strawman. Try to reply to what’s been said. AI has far bigger downsides than Web3, Web3 at worst is zero-sum and people voluntarily choose to engage with it. AI can harm many vulnerable people and systems, that never chose to engage with any of it. That’s what you call useful?Also, this idea that just because you say Web3 has no use cases, makes it true, regardless of evidence, is silly. reply IshKebab 17 hours agorootparent [–] > Try to reply to what’s been said.Try to read what&#x27;s been said. When did I imply that the two are linked?> AI has far bigger downsides than Web3, Web3 at worst is zero-sum and people voluntarily choose to engage with it.Sure. Web3 is a nothing. At worst it will change nothing. But it is at worst. It changes nothing.> That’s what you call useful?AI can be abused, but that obviously doesn&#x27;t mean that it isn&#x27;t useful. I did not call the abuse of AI useful. Who is arguing against straw men now?> Also, this idea that just because you say Web3 has no use cases, makes it true, regardless of evidence, is silly.Please tell me one practical use of Web3. I did actually google it and it returned this list:https:&#x2F;&#x2F;www.techtarget.com&#x2F;searchcio&#x2F;tip&#x2F;8-top-Web-30-use-ca...1. Flexible and secure smart contracts - nobody really wants this; they don&#x27;t want to lose all their money due to a bug with no recourse.2. Trusted data privacy - this isn&#x27;t anything concrete.3. Highly targeted [advertising] content - erm I thought you said web3 has no downsides?4. Automated content creation and curation - another hand wave.5. Unique loyalty programs - ha, come on, really?6. Increased community building - ... this list is exactly what I expected ...7. Better omnichannel experiences - ??!?8. Wider use of augmented reality - what has this even got to do with web3?Please point me to a realistic use case for web3. reply EGreg 16 hours agorootparent [–] See the list herehttps:&#x2F;&#x2F;intercoin.org&#x2F;applicationsWould love to see the same type of reaction by numbered point by point as you did above reply IshKebab 13 hours agorootparent [–] Web 5? Lol. As far as I can see all of those things are already totally possible with web 2.0. Except maybe NFTs? Hard to argue that they are useful though except for money laundering.Could you perhaps pick one or two from that list that you think are the best and explain why they can only be implemented with smart contracts?I mean, take voting for example. You can do voting with a web 1.0 website. The challenge is always going to be preventing vote stuffing, and the only real way to prevent that is to associate votes with real world IDs. How would web3 help with that? The proper solution is government issued key pairs, but that doesn&#x27;t sound very web3 to me. reply EGreg 12 hours agorootparent [–] You were fine making a list of 8 and here you punked out? Please give your reaction to each one, why they aren’t necessary or aren’t real applications and why Web3 is useless for them. Each one goes into depth for why Web3 matters if you click it.Voting can be done with Web 1.0 and in fact is done with StackExchange sites. But how do you know someone didnt go into the database and change the votes and results? What good are elections if you can’t trust them? reply Applications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article addresses the difficulties of efficiently utilizing Large Language Models (LLMs) in Natural Language Processing (NLP) tasks and suggests dividing the issue into modular components with LLMs as one component.",
      "The use of supervised learning is recommended for particular tasks, and tools such as spaCy and HF Transformers are recognized as beneficial for labeling data and training NLP models.",
      "The author also notes the utility of Prodigy, an annotation tool, for model-assisted workflows and concludes by stressing the significance of specific policies over broad reasoning abilities or background knowledge for practical tasks."
    ],
    "commentSummary": [
      "The article emphasizes on integrating large language models (LLMs) with natural language processing (NLP) tasks for dataset creation or augmentation to boost supervised models, rather than relying solely on LLMs.",
      "It details the limitations of current NLP infrastructure and the prospective advantages of employing LLMs for effective text representation and classification tasks. Transfer learning and smaller models can enhance accuracy and mitigate delay.",
      "The discussion extends to the importance of evaluating machine learning components, the difficulty associated with it, and potential implications and cons of AI and Web3 technologies."
    ],
    "points": 163,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1694608342
  },
  {
    "id": 37498820,
    "title": "WASI Support in Go",
    "originLink": "https://go.dev/blog/wasi",
    "originBody": "Skip to Main Content Why Go arrow_drop_down Learn Docs arrow_drop_down Packages Community arrow_drop_down Why Go navigate_next navigate_beforeWhy Go Case Studies Use Cases Security Learn Docs navigate_next navigate_beforeDocs Effective Go Go User Manual Standard library Release Notes Packages Community navigate_next navigate_beforeCommunity Recorded Talks Meetups open_in_new Conferences open_in_new Go blog Go project Get connected The Go Blog WASI support in Go Johan Brandhorst-Satzkorn, Julien Fabre, Damian Gryski, Evan Phoenix, and Achille Roussel 13 September 2023 Go 1.21 adds a new port targeting the WASI preview 1 syscall API through the new GOOS value wasip1. This port builds on the existing WebAssembly port introduced in Go 1.11. What is WebAssembly? WebAssembly (Wasm) is a binary instruction format originally designed for the web. It represents a standard that allows developers to run high-performance, low-level code directly in web browsers at near-native speeds. Go first added support for compiling to Wasm in the 1.11 release, through the js/wasm port. This allowed Go code compiled using the Go compiler to be executed in web browsers, but it required a JavaScript execution environment. As the use of Wasm has grown, so have use cases outside of the browser. Many cloud providers are now offering services that allow the user to execute Wasm executables directly, leveraging the new WebAssembly System Interface (WASI) syscall API. The WebAssembly System Interface WASI defines a syscall API for Wasm executables, allowing them to interact with system resources such as the filesystem, the system clock, random data utilities, and more. The latest release of the WASI spec is called wasi_snapshot_preview1, from which we derive the GOOS name wasip1. New versions of the API are being developed, and supporting them in the Go compiler in the future will likely mean adding a new GOOS. The creation of WASI has allowed a number of Wasm runtimes (hosts) to standardize their syscall API around it. Examples of Wasm/WASI hosts include Wasmtime, Wazero, WasmEdge, Wasmer, and NodeJS. There are also a number of cloud providers offering hosting of Wasm/WASI executables. How can we use it with Go? Make sure that you have installed at least version 1.21 of Go. For this demo, we’ll use the Wasmtime host to execute our binary. Let’s start with a simple main.go: package main import \"fmt\" func main() { fmt.Println(\"Hello world!\") } We can build it for wasip1 using the command: $ GOOS=wasip1 GOARCH=wasm go build -o main.wasm main.go This will produce a file, main.wasm which we can execute with wasmtime: $ wasmtime main.wasm Hello world! That’s all it takes to get started with Wasm/WASI! You can expect almost all the features of Go to just work with wasip1. To learn more about the details of how WASI works with Go, please see the proposal. Running go tests with wasip1 Building and running a binary is easy, but sometimes we want to be able to run go test directly without having to build and execute the binary manually. Similar to the js/wasm port, the standard library distribution included in your Go installation comes with a file that makes this very easy. Add the misc/wasm directory to your PATH when running Go tests and it will run the tests using the Wasm host of your choice. This works by go test automatically executing misc/wasm/go_wasip1_wasm_exec when it finds this file in the PATH. $ export PATH=$PATH:$(go env GOROOT)/misc/wasm $ GOOS=wasip1 GOARCH=wasm go test ./... This will run go test using Wasmtime. The Wasm host used can be controlled using the environment variable GOWASIRUNTIME. Currently supported values for this variable are wazero, wasmedge, wasmtime, and wasmer. This script is subject to breaking changes between Go versions. Note that Go wasip1 binaries don’t execute perfectly on all hosts yet (see #59907 and #60097). This functionality also works when using go run: $ GOOS=wasip1 GOARCH=wasm go run ./main.go Hello world! Wrapping Wasm functions in Go with go:wasmimport In addition to the new wasip1/wasm port, Go 1.21 introduces a new compiler directive: go:wasmimport. It instructs the compiler to translate calls to the annotated function into a call to the function specified by the host module name and function name. This new compiler functionality is what allowed us to define the wasip1 syscall API in Go to support the new port, but it isn’t limited to being used in the standard library. For example, the wasip1 syscall API defines the random_get function, and it is exposed to the Go standard library through a function wrapper defined in the runtime package. It looks like this: //go:wasmimport wasi_snapshot_preview1 random_get //go:noescape func random_get(buf unsafe.Pointer, bufLen size) errno This function wrapper is then wrapped in a more ergonomic function for use in the standard library: func getRandomData(r []byte) { if random_get(unsafe.Pointer(&r[0]), size(len(r))) != 0 { throw(\"random_get failed\") } } This way, a user can call getRandomData with a byte slice and it will eventually make its way to the host-defined random_get function. In the same way, users can define their own wrappers for host functions. To learn more about the intricacies of wrapping Wasm functions in Go, please see the go:wasmimport proposal. Limitations While the wasip1 port passes all standard library tests, there are some notable fundamental limitations of the Wasm architecture that may surprise users. Wasm is a single threaded architecture with no parallelism. The scheduler can still schedule goroutines to run concurrently, and standard in/out/error is non-blocking, so a goroutine can execute while another reads or writes, but any host function calls (such as requesting random data using the example above) will cause all goroutines to block until the host function call has returned. A notable missing feature in the wasip1 API is a full implementation of network sockets. wasip1 only defines functions that operate on already opened sockets, making it impossible to support some of the most popular features of the Go standard library, such as HTTP servers. Hosts like Wasmer and WasmEdge implement extensions to the wasip1 API, allowing the opening of network sockets. While these extensions are not implemented by the Go compiler, there exists a third party library, github.com/stealthrocket/net, which uses go:wasmimport to allow the use of net.Dial and net.Listen on supported Wasm hosts. This enables the creation of net/http servers and other network related functionality when using this package. The future of Wasm in Go The addition of the wasip1/wasm port is just the beginning of the Wasm capabilities we would like to bring to Go. Please keep an eye out on the issue tracker for proposals around exporting Go functions to Wasm (go:wasmexport), a 32-bit port and future WASI API compatibility. Get involved If you are experimenting with and want to contribute to Wasm and Go, please get involved! The Go issue tracker tracks all in-progress work and the #webassembly channel on the Gophers Slack is a great place to discuss Go and WebAssembly. We look forward to hearing from you! Previous article: Scaling gopls for the growing Go ecosystem Blog Index Why Go Use Cases Case Studies Get Started Playground Tour Stack Overflow Help Packages Standard Library About Go Packages About Download Blog Issue Tracker Release Notes Brand Guidelines Code of Conduct Connect Twitter GitHub Slack r/golang Meetup Golang Weekly Copyright Terms of Service Privacy Policy Report an Issue go.dev uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more. Okay",
    "commentLink": "https://news.ycombinator.com/item?id=37498820",
    "commentBody": "WASI Support in GoHacker NewspastloginWASI Support in Go (go.dev) 162 points by spacey 18 hours ago| hidepastfavorite46 comments dilyevsky 16 hours agoI wish they added some way of exporting wasm funcs as well so that they could be called from host. Tinygo wasm target supports both \"exports\" and \"imports\". The other thing that is worse compared to tinygo is the generated binary size seems to be about 10x larger. From my brief look at the transpiled .wat printout a lot of included funcs aren&#x27;t being called anywhere... reply jbrandhorst 13 hours agoparentExports are something we&#x27;d like to work on but it turns out it&#x27;s pretty complicated to reconcile the Go runtime with the WebAssembly environment, especially when there&#x27;s only a single thread. We&#x27;ll get to exports as soon as we can but it may require Wasm threads to be stable first. reply dilyevsky 8 hours agorootparentJust curious what state is it (thread support) in now? I saw wasmtime already supporting some pthread style threads so assumed proposal has already been accepted but it’s super hard to actually figure out what state anything is in with wasm… reply fathyb 2 hours agorootparentDon&#x27;t know about the server side, but I&#x27;ve been using threads on the browser for ~2 months, I didn&#x27;t hit any bug specific to it yet. I use it both with Rust (wasmbindgen with async&#x2F;await) and C (Emscripten with pthread support). HTTPS with some headers is required for `SharedArrayBuffer`.I still build a single-threaded binary for Firefox, and fallback to it if `SharedArrayBuffer` is `undefined` or if the `WebAssembly.Memory` constructor fails (some iOS devices might throw when `shared` is `true` due to a bug). reply beanjuiceII 7 hours agorootparentprevI would like to second that last statement. If anyone knows a good place to keep up with it all I&#x27;d appreciate it reply cplli 5 hours agorootparentThreads are Phase 3https:&#x2F;&#x2F;github.com&#x2F;WebAssembly&#x2F;proposalsYou can also check out:https:&#x2F;&#x2F;webassembly.org&#x2F;roadmap&#x2F;And for Go, the proposal project on Github has many interesting conversations from the devs.And as a reminder to anyone interested in using Go WASM, it’s experimental and does not come with the same compatibility promise as Go itself:https:&#x2F;&#x2F;github.com&#x2F;golang&#x2F;go&#x2F;wiki&#x2F;WebAssembly reply dilyevsky 2 hours agorootparentYes it&#x27;s been in phase 3 for what like 2-3 years at this point (judging by when it landed in browsers)? No eta and no next steps. The tp says they are waiting until \"it&#x27;s stable\" so I&#x27;m assuming phase 4. It qualifies browser support and \"at least one toolchain\" criteria[0] (zig) and seems like all the other conditions too except maybe \"CG consensus\" whatever that means, so for all I know it could take anywhere between tomorrow and in a few years from now...[0] https:&#x2F;&#x2F;github.com&#x2F;WebAssembly&#x2F;meetings&#x2F;blob&#x2F;main&#x2F;process&#x2F;ph... replyrockwotj 14 hours agoparentprevFWIW we simulate exports by instead having `main` call an imported function that blocks until it&#x27;s ready to return with the needed data.So instead of:`host -> call foo on guest -> return to host``host -> call guest main -> call foo on host -> host returns when ready -> guest calls foo when done`FWIW the scheduler (so goroutines) don&#x27;t work in go if you&#x27;re not calling from an main, so anytime you call a custom export then try to use a goroutine you&#x27;ll get panics.10x size is about the blowup we see as well. It&#x27;s also likely to be slower (some of the Tinygo authors said ~20% slowdown compared to tinygo) probably due to the simpler&#x2F;smaller runtime and LLVM being better at optimizing. reply Seb-C 7 hours agorootparentThe only time I tried wasm in Go, the wasm compiled by the native Go was so slow that the equivalent Javascript was faster. Tinygo produced decent performance however. reply jsd1982 14 hours agoparentprevThe go-compiled wasm is also extremely slow compared to tinygo&#x27;s wasm. Doing simple things like `fmt.Printf()` degraded performance significantly. reply alecthomas 14 hours agorootparentI wonder if fmt.Printf() in particular is slow due to the CGo transition? Assuming there even is a CGo transition when compiled to WASM... reply jbrandhorst 13 hours agorootparentThere&#x27;s no CGO involved when compiling to Wasm. The sometimes slow performance is due to the hoops the compiled code has to jump through to support the Go runtime and goroutine preemption on a single thread. reply alecthomas 12 hours agorootparentI understand there&#x27;s no CGo specifically, but I&#x27;m wondering if the Go runtime when running under WASM still has to manage switching out the goroutine stacks for \"WASM stacks\" when it&#x27;s calling out through the WASM VM.Edit edit: from this comment it sounds like it is, as you say, just the general overhead of managing goroutine stacks. I wonder if TinyGo is more performant.[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37501552 reply Patrickmi 10 hours agorootparentTinygo runtime don’t have g or m threads that’s why it’s Cgo is zero cost replyviccuad 14 hours agoparentprevI&#x27;m also waiting for that :) See https:&#x2F;&#x2F;github.com&#x2F;golang&#x2F;go&#x2F;issues&#x2F;42372 reply dilyevsky 14 hours agorootparentYeah there is an escape hatch via js package but it’s annoying reply umvi 14 hours agoprevI feel like we need better WASM performance in go before we get WASI. In my experience go wasm performance is pretty bad, usually significantly worse than vanilla JS.Rust (or really anything LLVM backed) is still probably the best WASM language in terms of performance and support, but .NET (don&#x27;t forget to turn on AOT) is starting to get really good too (except for the fact that .NET compiler barfs out a bazillion files that the browser needs vs. 1 self contained .js or .wasm file which sucks if you are trying to build a self contained library like OpenCV.js) reply achille-roussel 13 hours agoparentThis is a good callout, although we probably won&#x27;t be able to significantly improve the performance of Go compiled to WASM until WebAssembly evolves and introduces support for threads or stack switching so we can define goroutines based on those; right now the main reason Go compiled to WASM isn&#x27;t in the ballpark of native perf is due to the stack switching emulation we have to do to get the cooperative scheduling of goroutines to work. We&#x27;ll need WASM runtimes to offer more advanced primitives that we can rely on to implement those features of the Go language and produce much higher-quality code. reply grumpy_tired 5 hours agorootparentWould the emulation penalty still occur if Go routines aren&#x27;t used at all? I have many small domain-specific libraries that I am planning to port to wasm. These libraries only allocate dynamic memory and differ anonymous functions. reply DanielHB 2 hours agoparentprevIn my experience WASM performance is not that much better than JS in most cases, not so much because WASM is lacking, but because most JS runtimes performance is really, really good for a dynamic language.When I measured C compiled with clang -O3 vs JS performance the only noteworthy speedup were on math-heavy tasks and even there only for integer-heavy math (floating point math was better than JS, but not by much). In a few cases the performance was worse. Notably recursive algorithms were _much_ better in WASM though even with algorithms that can&#x27;t have tail-call optimisation (I guess function invocation has a lot of overhead in JS compared to C)I think people over-value WASM speed. With regards to performance I imagine that the biggest gains compared to JS would be from not using garbage collection language. GC overhead, especially JS GC (compared to golang GC) can be painful in very large applications, especially in things were timings matter like 3d rendering. But GC can be optimised for in JS by avoiding allocations in the critical paths of the app reply dgb23 14 hours agoparentprev> usually significantly worse than vanilla JSCan to elaborate?This is can be true if you&#x27;re interacting with browser API&#x27;s such as the DOM frequently, because there&#x27;s an overhead.But I&#x27;ve seen several projects where (non-GC) WASM has improved performance significantly for specific tasks. You won&#x27;t get native performance obviously. reply ramesh31 9 hours agorootparent>Can to elaborate?The overhead of a runtime can easily make WASM code run slower than native JS functions. This only applies to GC languages like Go which require that.>But I&#x27;ve seen several projects where (non-GC) WASM has improved performance significantly for specific tasks. You won&#x27;t get native performance obviously.You absolutely can if you&#x27;re writing the raw WASM or compiling from C. reply candiddevmike 14 hours agoprevI still don&#x27;t understand the benefit of running a Go app on a cloud provider using this. Anyone want to help me?Is it an edge play for using something like Cloudflare Workers?Is it cheaper vs standard serverless&#x2F;container deployments? Go apps can already scale to 0 for these use cases. reply viccuad 14 hours agoparentIt consolidates Go compiled to WASI as an alternative of doing containers. Linux containers don&#x27;t \"Run anywhere.\" as docker.io says. You need a specific architecture and kernel features, which is not obvious from afar.There&#x27;s also other benefits. Example: the team I work on compiled Kyverno, a CNCF K8s policy engine written in Go, to a WASI target. We are building Kubewarden, a CNCF policy engine where policies are Wasm binaries shipped in OCI registries. We strive to build \"a Universal Policy Engine\". Now, we have an experimental Kubewarden policy `kyverno-dsl-policy` that allows you to reuse Kyverno DSL with us. We also provide WaPC as a target, more performant and secure, hence normal SDKs for Go, Rust, C#, swift, typescript... In addition to supporting Rego, again compiled to Wasm.IMHO you only benefit from the real sandboxing from WaPC, as WASI&#x27;s posix-like interface allows you to attack the host.The next step for the official Go compiler is to export the function symbols, to allow for WaPC. reply rockwotj 14 hours agorootparentHow is the Universal Policy Engine different than Open Policy Agent? reply viccuad 12 hours agorootparentJust gave a talk on Monday about it in containerdays.io, but the video is not in youtube yet!In a nutshell, with Kubewarden we strive to build the universal policy engine by:- Provide all personas (policy consumer, policy developer, policy distributor, engine admin, engine developer&#x2F;integrator, etc) with current and future industry-standard workflows, not only a subset of personas, nor more than needed knowledge for those personas. It&#x27;s a bold statement, and if it would be universal it should indeed cater to everyone.- This is achieved with policies as code, which are Wasm modules: Wasm policies allows us to support Rego DSL (OPA&#x2F;Gatekeeper), YAML, SDKs for Wasm-compiled languages, and now an experimental Kyverno DSL policy by compiling it to WASM with WASI. Great for using your language and tools of preference.- Wasm modules have first class support In OCI registries, just like container images: Use same tools that you know as artifact distributor: SBOMs, signing and verifying with cosign, airgap, slsa.dev, etc.- Policies can be evaluated out-of-cluster: great for CI&#x2F;CD, dev loop, integration tests, etc.- Modular architecture informed by using Wasm policies: OCI registry, policy-server, k8s controller, out-of-cluster cli (kwctl), etc. This also helps in adopting future industry-standard workflows.- Usual features of a Policy engine (mutating, context-aware, recurring scanner of already in-cluster resources, etc). Plus ample room for new features thanks to the architecture. E.g: possibility to run the policy-server directly in the k8s apiserver (one colleague already presented that in Kubecon), possibility to evaluate out-of-cluster policies outside of clusters like OPA just by running the policy-server standalone, more DSLs compiled to Wasm, more languages, etc.- Vendor neutral, CNCF project, open source, developed in the open. reply Thaxll 13 hours agorootparentprevAs if WASI does not need specific architecture and kernel features? reply DanielHB 1 hour agorootparentTechnically you don&#x27;t need a full kernel, edge workers will likely have their own custom trimmed down kernels for running WASM binaries, cutting out a lot of the OS overhead. As long as they implement a limited set of POSIX syscalls they can run WASM binaries, in fact you might even want to limit the WASI syscalls you implement for certain targets. reply carbotaniuman 12 hours agorootparentprevWASI runners are just an application. I guess you need your kernel to run prograns and have syscalls, but that is a low bar. reply Thaxll 12 hours agorootparentDocker needs the 3.10 kernel which is 10 years old. reply est31 9 hours agorootparentIt also needs root access, and 3.10 is only the lowest supported kernel. Not a docker expert but I could bet that they only support a subset of features there.What I do know is that docker images are specific to the host architecture, supporting either one architecture, or a blessed list. wasm binaries on the other hand aren&#x27;t. wasm can theoretically also run on bare-metal embedded scenarios without an OS entirely. reply mbertschler 11 hours agorootparentprevBut it needs Linux, while wasm runtimes are available for all OSes natively replydilyevsky 14 hours agoparentprevIt’s basically what jvm should have been but never delivered:1. Could be an edge&#x2F;iot play (tho not with standard go toolchain bc those binaries are huge and slow)2. Trusted environment makes it interesting for all kinds of sbom-related usecases. Think weapons tech, space, etc3. On the container side things are less clear to me but may offer better startup time, reduced footprint etc4. Finally, for plugabble software it’s most interesting option personally. It’s basically 2.) but with focus on DX over security reply alecthomas 14 hours agoparentprevI believe the primary benefit is the sandboxing, but I imagine there are secondary benefits such as that the host platform can be any CPU architecture or OS. reply kungfufrog 11 hours agoprevCan someone hit me with the value proposition of all this WASI stuff and WASM and ELI5? (I get the browser use-case)My understanding is as follows: WASM - a portable, platform-independent virtual machine for executing a \"web assembly\" WASI - an extension to the virtual machine that adds APIs for interacting with the system and breaks all the WASM sandboxing (presumably NOT platform-independent?)Is the point of this addition to Go that I can now target \"WASM implementations that have WASI\" with Go source code compiled to WASM?Why would someone want to do that? Just for edge functions in cloud workers? reply DanielHB 1 hour agoparentTechnically you don&#x27;t need a full kernel, edge workers will likely have their own custom trimmed down kernels for running WASM binaries, cutting out a lot of the OS overhead. As long as they implement a limited set of POSIX syscalls they can run WASM binaries, in fact you might even want to limit the WASI syscalls you implement for certain targets.There are also other things of great value, for example providing a way to write plugins for cloud-based SaaS solutions, as in you compile your binary, hand it over to your SaaS service and the service itself runs your binary when some event happens. Basically plugins for cloud based stuff, much more powerful and simpler than Web Hooks.Another example was to write custom database functions, for example, adding a complex math function to postgres so you can do \"SELECT myFunc(COLUMN_A, COLUMN_B) FROM TABLE\".Another example was to sandbox plugins for desktop applications (including mods for video games), plugins are a huge security issue when it is native code running in your machine.These plugin examples the entity that is running the plugin code can limit the syscalls used by your WASI-compatible WASM binary so you don&#x27;t allow, for example, your video game mod to read&#x2F;write files outside a specific folder in the file-system reply tormeh 9 hours agoparentprevThink “JVM, but better this time”. Better isolation, and more language-agnostic. So, Kubernetes with WASM application servers as an alternative to container runtimes. All the old Java ideas, but hopefully much better. It being originally made for the web has the advantage of it being built with security in mind from the get-go. The JVM was always unsuited for running untrusted code, among other failings. reply bigassdbload 7 hours agorootparentWasm is anything but &#x27;secure&#x27;.You should go watch any of the numerous blackhat presentations on wasm or just talk to some of the security researchers out there. You can do attacks that most people haven&#x27;t been able to do for 20+ years.Wasm has horrible security. reply lifthrasiir 4 hours agorootparent> You can do attacks that most people haven&#x27;t been able to do for 20+ years.This is a bad and roundabout way to say that vulnerabilities in WebAssembly modules may still cause a corruption in their linear memory. Which is absolutely true, but those attacks still matter today (not everyone turns ASLR on) and similar defences also apply. In the future multiple memories [1] should make it much easier to guard against remaining issues. WebAssembly is a lucrative target only because it is so widespread and relatively young, not because it has horrible security (you don&#x27;t know how the actually horrible security looks like).[1] https:&#x2F;&#x2F;github.com&#x2F;WebAssembly&#x2F;multi-memory&#x2F;blob&#x2F;main&#x2F;propos... reply noveltyaccount 11 hours agoparentprevIt enables a future world where CPU architecture, OS, and runtime don&#x27;t matter. Code from any language can run on any hardware and interop with any other code. Cloud hosts can buy whatever hardware is most economical and your code will work there. Just like Docker eliminated a lot of tedious dependency management for deployment, this eliminates another chain of dependencies, but CPU, language runtime, and operating system. reply eddythompson80 9 hours agorootparentSun Microsystems would like a word with you. reply imtringued 3 minutes agorootparentYes, you can run anything anywhere as long as it is Java or a JVM language that does not fundamentally change the object or memory management model. reply otterley 9 hours agorootparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Write_once,_run_anywhere reply skybrian 11 hours agoparentprevAs I understand it, WASI doesn&#x27;t break all security, or at least not by default. Here&#x27;s a document about capabilities-based design:https:&#x2F;&#x2F;github.com&#x2F;bytecodealliance&#x2F;wasmtime&#x2F;blob&#x2F;main&#x2F;docs&#x2F;... reply favflam 11 hours agoparentprevYou can theoretically run untrusted code from customers in a secure sandbox. This is a simpler proposition that letting customers run vms. reply ramesh31 9 hours agoprev [–] Badass. The `GOOS=js` build had so many workarounds needed that it was barely worth it to port existing code, and wasm_exec.js always felt like a terrible hack. I&#x27;ll be updating all of my stuff with this and pulling out the shims. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Go 1.21 has introduced a new port for the WebAssembly System Interface (WASI) syscall API via the new GOOS value, wasip1. This enables developers to run Go code compiled to WebAssembly directly, bypassing the need for a JavaScript execution environment.",
      "Despite the advancement, there are limitations including the absence of support for network sockets in the wasip1 API. However, extensions from hosts like Wasmer and WasmEdge and a third-party library, stealthrocket/net, could be utilized for network functionality.",
      "Future directions for Go include exporting Go functions to WebAssembly and compatibility with the evolving WASI API, and developers have channels such as Gophers Slack and the Go issue tracker to contribute."
    ],
    "commentSummary": [
      "The GoHacker News conversation revolves around introducing WASI support in the Go programming language, touching upon hurdles such as exporting WebAssembly functions, performance, and binary size.",
      "Discussion participants suggest enhancements for Go's WebAssembly performance, drawing comparisons with Rust and .NET, while underlining the advantages of utilizing WebAssembly and WASI for optimizing Go applications.",
      "Despite concerns about security vulnerabilities when using WebAssembly for plugins and desktop apps, participants generally perceive WebAssembly (WASM) as a tool to facilitate code interoperability and minimize dependency on hardware and operating systems."
    ],
    "points": 162,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1694622430
  },
  {
    "id": 37499731,
    "title": "Unity silently removed clause that let you use TOS from version you shipped with",
    "originLink": "https://old.reddit.com/r/gamedev/comments/16hnibp/unity_silently_removed_their_github_repo_to_track/",
    "originBody": "jump to content MY SUBREDDITS POPULAR-ALL-RANDOM-USERSASKREDDIT-PICS-GAMING-MILDLYINTERESTING-WORLDNEWS-FUNNY-TODAYILEARNED-MOVIES-NEWS-EXPLAINLIKEIMFIVE-TIFU-TWOXCHROMOSOMES-VIDEOS-AWW-LIFEPROTIPS-DATAISBEAUTIFUL-JOKES-OLDSCHOOLCOOL-SCIENCE-BOOKS-MUSIC-IAMA-SPACE-SHOWERTHOUGHTS-GADGETS-SPORTS-ASKSCIENCE-NOTTHEONION-FUTUROLOGY-NOSLEEP-FOOD-INTERNETISBEAUTIFUL-HISTORY-DOCUMENTARIES-ANNOUNCEMENTS-WRITINGPROMPTS-PHILOSOPHY-GETMOTIVATED-GIFS-UPLIFTINGNEWS-EARTHPORN-CREEPY-PHOTOSHOPBATTLES-LISTENTOTHIS-DIY-BLOG MORE » Gamedevcommentsother discussions (3) Want to join? Log in or sign up in seconds.|English this post was submitted on 13 Sep 2023 4,915 points (98% upvoted) shortlink: remember mereset password login Submit Link Submit Text Get an ad-free experience with special benefits, and directly support Reddit. Get Reddit Premium gamedev Join1,192,254 3,732 Join Our Discord Filter Posts Unfilter Resource Events Games Only Questions No Questions FAQs & Wiki Getting Started Engine FAQ Wiki General FAQ Socialize /r/gamedev Discord server /r/gamedev IRC channel live development streams Weekly threads [FF] Feedback Friday [SS] Screenshot Saturday [STS] Soundtrack Sunday [MM] Marketing Monday [WIPW] WIP Wednesday [DD] Daily Discussion [QS] Quarterly Showcase Related communities /r/LearnProgramming /r/GameDevClassifieds /r/PlayMyGame /r/IndieGames /r/GameDesign /r/DevBlogs /r/GameJams /r/LevelDesign /r/GameAssets /r/GameAudio /r/OpenGL /r/Unity3D /r/UnrealEngine /r/TrueGameDev /r/gamedevscreens /r/TheMakingOfGames TigSource IndieDB CompoHub Game Conferences /r/gamepr GameDev.net /r/playertodev r/JustGameDevThings /r/GameProduction/ MODERATORS MESSAGE THE MODS discussions in r/gamedevX 1527 · 290 comments Unity's Reputation Is Lost No Matter The Outcome Welcome to Reddit. Where a community about your favorite things is waiting for you. BECOME A REDDITOR and subscribe to one of thousands of communities. × 4915 Unity silently removed their Github repo to track license changes, then updated their license to remove the clause that lets you use the TOS from the version you shipped with, then insists games already shipped need to pay the new fees. (self.gamedev) submitted 20 hours ago by Darkfrost After their previous controversy with license changes, in 2019, after disagreements with Improbable, unity updated their Terms of Service, with the following statement: When you obtain a version of Unity, and don’t upgrade your project, we think you should be able to stick to that version of the TOS. As part of their \"commitment to being an open platform\", they made a Github repository, that tracks changes to the unity terms to \"give developers full transparency about what changes are happening, and when\" Well, sometime around June last year, they silently deleted that Github repo. April 3rd this year (slightly before the release of 2022 LTS in June), they updated their terms of service to remove the clause that was added after the 2019 controversy. That clause was as follows: Unity may update these Unity Software Additional Terms at any time for any reason and without notice (the “Updated Terms”) and those Updated Terms will apply to the most recent current-year version of the Unity Software, provided that, if the Updated Terms adversely impact your rights, you may elect to continue to use any current-year versions of the Unity Software (e.g., 2018.x and 2018.y and any Long Term Supported (LTS) versions for that current-year release) according to the terms that applied just prior to the Updated Terms (the “Prior Terms”). The Updated Terms will then not apply to your use of those current-year versions unless and until you update to a subsequent year version of the Unity Software (e.g. from 2019.4 to 2020.1). If material modifications are made to these Terms, Unity will endeavor to notify you of the modification. This clause is completely missing in the new terms of service. This, along with unitys claim that \"the fee applies to eligible games currently in market that continue to distribute the runtime.\" flies in the face of their previous annoucement of \"full transparency\". They're now expecting people to trust their questionable metrics on user installs, that are rife for abuse, but how can users trust them after going this far to burn all goodwill? They've purposefully removed the repo that shows license changes, removed the clause that means you could avoid future license changes, then changed the license to add additional fees retroactively, with no way to opt-out. After this behaviour, are we meant to trust they won't increase these fees, or add new fees in the future? I for one, do not. Sources: \"Updated Terms of Service and commitment to being an open platform\" https://blog.unity.com/community/updated-terms-of-service-and-commitment-to-being-an-open-platform Github repo to track the license changes: https://github.com/Unity-Technologies/TermsOfService Last archive of the license repo: https://web.archive.org/web/20220716084623/https://github.com/Unity-Technologies/TermsOfService New terms of service: https://unity.com/legal/editor-terms-of-service/software Old terms of service: https://unity.com/legal/terms-of-service/software-legacy 594 commentssharesavehidereport top 200 commentsshow 500 sorted by: best Want to add to the discussion? Post a comment! CREATE AN ACCOUNT [–]faithrolled 1292 points 20 hours ago These fuckers need to be sued into oblivion. permalinkembedsavereportreply [–]TekuzoGodot|@Learyt_Tekuzo 489 points 19 hours ago They looked at Wizards of the Coast and took the completely wrong message away. permalinkembedsaveparentreportreply [–]iamthewhatt 266 points 18 hours ago No, they took away the exact correct message they were going for: Take as much money as possible while running Unity into the ground. I 100% bet that's why they hired John in the first place. permalinkembedsaveparentreportreply [–]TitaniumDragon 154 points 15 hours ago John was hired in 2014 after losing his job at EA for doing a bad job there. This is why you don't hire people who did shitty at their last job as a CEO. They should have fired him years ago; Unity has never been profitable and he hasn't changed that. permalinkembedsaveparentreportreply [–]Amante 135 points 15 hours ago CEOs doing a \"bad job\" and then failing upwards is a feature, not a bug permalinkembedsaveparentreportreply [–]theth1rdchild 72 points 14 hours ago Anyone who believes we live in a meritocracy is a rube permalinkembedsaveparentreportreply load more comments (27 replies) [–]TitaniumDragon 30 points 13 hours ago* It's really not. It actually is a bug. No one wants to hire a CEO who is going to destroy their company's value. The actual reason for this is that people who don't know about hiring executives will often overly rely on previous experience without actually looking at how they did previously. permalinkembedsaveparentreportreply [–]tredontho 27 points 13 hours ago Seems a bit silly to rely on previous experience if you don't look at the previous experience. Maybe there's a lot of incompetency at the top permalinkembedsaveparentreportreply [–]TitaniumDragon 13 points 13 hours ago How many CEOs does the average person hire? Not many. Even amongst top level people, most of them aren't going to have gone through this process many times, so it's hard to build up skill at it. As such, it's harder to hire a CEO than lower level employees because you get far less practice at it. Unity has never been a company that was run very well, so it's not surprising that they hired a bad CEO. The company has never had great management. There's some incompetence at the top in general. Not everyone is competent. But there's a lot more competent people at the top than incompetent people. permalinkembedsaveparentreportreply [–]irene_m@snuffysammedia 17 points 11 hours ago did they try googling his name? permalinkembedsaveparentreportreply load more comments (7 replies) load more comments (1 reply) load more comments (1 reply) [–]cjo20 16 points 12 hours ago That depends. A bunch of CEOs get hired because they'll be able to extract a bunch of value from the company and make the right people rich before it collapses. permalinkembedsaveparentreportreply [–]TitaniumDragon 17 points 11 hours ago Preserving shareholder value is a real thing. If a company is going to die, it makes sense to hire someone to take it apart and get the most value they can out of it rather than have it go bankrupt. The thing is, this isn't a case like that at all. This guy has been CEO since 2014. He's just been crap at his job. The reality is that Unity is just a company that has always had poor management, and they're still around because they've found enough suckers to finance the black hole that is that company. But with their stock value plunging, they have run out of suckers to throw good money after bad into the company. permalinkembedsaveparentreportreply load more comments (5 replies) load more comments (1 reply) [–]oceantume_ 55 points 17 hours ago* This is typical corporate investors/officers behavior, but it's getting talked about a lot more because it's not a slow-cookers company this time. permalinkembedsaveparentreportreply [–]ugathanki 14 points 13 hours ago* EDIT: Apparently I was wrong. Leaving this here for reference: The entire executive part of a corporation is compelled to maximize profit for the shareholders no matter what. This means short term profit, not long term btw, because those shareholders will make more money with short term profits that they can then TAKE from the company and put somewhere else, POSSIBLY TO THE COMPANY'S COMPETITORS. It's absurd, and it's not designed to keep a healthy company afloat. It's designed to kill companies, and that's immoral. permalinkembedsaveparentreportreply [–]Hendursag 18 points 11 hours ago That's not actually true. It's a commonly held belief, including among CEOs, but it's bogus. It's a meme started by Milton Friedman sometime in the late 1970s and it's absolute bullshit. https://evonomics.com/maximizing-shareholder-value-dumbest-idea/ permalinkembedsaveparentreportreply [–]Simmery 7 points 10 hours ago Maybe it's not legally true, but it's still effectively true. Going public is almost a guarantee that a company's products are going to go to shit. permalinkembedsaveparentreportreply [–]Hendursag 3 points 9 hours ago Oh I agree. Shareholder primacy is unfortunately one of the drivers in the US, and it's a real problem. It leads companies to maximize profits quarter-by-quarter, instead of planning ahead and investing in long-term profitability. I was just responding to the argument that this was a legal requirement. permalinkembedsaveparentreportreply [–]ugathanki 9 points 11 hours ago Interesting. Thanks for showing me that. I encourage anyone who agreed with my original comment to read this source and see how you feel. permalinkembedsaveparentreportreply load more comments (4 replies) load more comments (2 replies) load more comments (1 reply) load more comments (6 replies) [–]luki9914 15 points 12 hours ago Developers has to delist all games made in unity, port it over different engine and sue the Unity developers. Only this way they will learn. permalinkembedsaveparentreportreply [–]meanyack 7 points 4 hours ago If only it was easy to do so. permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (1 reply) load more comments (10 replies) [–]Brother_Clovis 305 points 18 hours ago This is one of the biggest tech blunders I've ever heard of. What a way to completely destroy your reputation. permalinkembedsavereportreply [–]Dgaart 82 points 17 hours ago One has to assume they are intentionally trying to run the company into the ground while trying to milk as much money as possible from it beforehand. But, why? permalinkembedsaveparentreportreply [–]brannock_ 76 points 17 hours ago Money now > money later More money now --> even more money in the future Therefore a short-term explosive growth in revenue is completely worth destroying the company over, because the people benefiting from the growth have no stakes in the company and will just move on to their next victim permalinkembedsaveparentreportreply [–]RadiantMalachai 11 points 16 hours ago So buy the stocks for pennies on the dollar then sell when they throw numbnuts into the drink. permalinkembedsaveparentreportreply load more comments (5 replies) [–]TitaniumDragon 41 points 14 hours ago I think you're missing the forest for the trees. They aren't trying to run the company into the ground, they're trying to pull it out of a catastrophic nosedive. Unity has never been profitable as a company. Ever. In its entire history, it has always lost money. They're burning $200-250 million per quarter keeping the company running. They increased revenue by $200 million but the costs keep going up too, so in the last year, even though they added $200 million in revenue, they only cut their losses from $250 million per quarter to $200 million per quarter. This despite laying off 12% of their workforce to cut costs. Unity has never found an economic model that actually works for the company to make money. Investors have been realizing that. Unity has been spending other people's money - and they're running out of other people's money to spend. The stock price for Unity as a company has plunged 70% in just a couple years, during a time of historic growth in video game sales during the pandemic. Without investors, their only choice is to somehow get money from their users. This is a desperate, flailing attempt to avoid bankruptcy. permalinkembedsaveparentreportreply [–]my_name_is_reed 15 points 9 hours ago This is a desperate, flailing attempt to avoid bankruptcy. Wild. I had no idea they weren't profitable, but I wasn't really paying attention. This puts it into perspective for me. Thanks. permalinkembedsaveparentreportreply load more comments (2 replies) [–]mwar123 27 points 12 hours ago It’s amazing they don’t know how to cut costs, when they bought another company for 4.4 billion dollars. What are they using their money on? A big bonfire in their offices? permalinkembedsaveparentreportreply [–]TitaniumDragon 22 points 12 hours ago* Most likely, their employees (they have almost 8000 of them) plus servicing debt plus capital assets (buildings, computers, servers, software, etc.) plus various other expenses. It’s amazing they don’t know how to cut costs, when they bought another company for 4.4 billion dollars. Is this surprising? Elon Musk bought Twitter for $44 billion (and massively overpaid for it) and did a shitty job of running it. Massively overspending on acquisitions is exactly the sort of thing you expect from a company that can't manage money. permalinkembedsaveparentreportreply [–]Velot_ 17 points 8 hours ago Unity has 8000 employees? What do they possibly need 8000 people for? permalinkembedsaveparentreportreply [–]TitaniumDragon 19 points 7 hours ago How else are they going to count all those installs? :V permalinkembedsaveparentreportreply [–]GaiasWay 9 points 4 hours ago Someone has to mark all those 'experimental' packages as 'depricated'. permalinkembedsaveparentreportreply [–]trickster721 4 points 4 hours ago Not developing the damn engine, that's for sure. Oh, excuse me, the \"Unity Runtime\". permalinkembedsaveparentreportreply load more comments (2 replies) load more comments (2 replies) load more comments (4 replies) [–]MyPunsSuckCommercial (Other) 21 points 15 hours ago Never rule out insider trading; shorting their own stock permalinkembedsaveparentreportreply load more comments (4 replies) [–]roflcop7er 7 points 16 hours ago It's a classic private equity strategy, but I'm not aware of them being bought out. So I dunno wtf they're doing. permalinkembedsaveparentreportreply [–]hates_stupid_people 3 points 13 hours ago But, why? A large enough group of investors want money now, not a steady income over time that might be larger in the long run. So they run the company into the ground while squeezing out every last drop. Then they move on to the next company. It's been going on for a while now, and I'm surprised how few people notice. permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (6 replies) [–]jl2lCommercial (Indie) 42 points 18 hours ago Yeah CEO of unity saw the CTO at Ledger and said hold my beer. permalinkembedsaveparentreportreply load more comments (1 reply) [–]heyheyhey27 12 points 16 hours ago Also known as Pulling a Twitter permalinkembedsaveparentreportreply [–]enricowereld@ChaoticDevs 7 points 11 hours ago Or a Reddit. Companies really been shitting themselves this year. permalinkembedsaveparentreportreply [–]heyheyhey27 5 points 10 hours ago Honestly I doubt the API stuff is bad for Reddit. It's bad for millions of the users generating more interesting content, but I suspect the vast majority of their users are browsing the default subs, excitedly reading whatever PR firm is doing an AMA today, tearfully up voting a /r/pics post of a guy's armpit with a title about how he got over his alcoholism and fear of armpits, etc. Those users aren't affected by it. permalinkembedsaveparentreportreply load more comments (3 replies) load more comments (1 reply) load more comments (1 reply) load more comments (3 replies) [–]Darkfrost[S] 506 points 20 hours ago Part of what annoys me most about this whole thing, even if unity undo all these changes or switch to a less severe version - is after all of that drama in 2019 about TOS changes, they implemented these license changes to regain the trust of their customers. Then they just undid them all for their next controversial change. Whatever they do now, that trust is irreparably lost. I've been making unity games for like 12 years now, I've spent so much time and effort learning & using unity, that I don't want to switch engine. But if there's nothing to stop them showing they'll suddenly make license terms that can easily put companies relying on them out of business... which this is the second time they've done now, what's to stop them doing it again? Guess I'll start brushing up on other engines... permalinkembedsavereportreply [–]NnasT 103 points 18 hours ago I feel you man, this rug pull was so bad. I've spent years learning the ins and outs of unity. But I've been dabbling with unreal on the side. My gripe with unreal is how slow the editor is with scripting. It's like designed around blueprints. I'm gonna miss the feeling of coding Ctrl+S and the code just working. But in unreal you have to compile and that takes 5-10mins. You are forced to use blueprints. permalinkembedsaveparentreportreply [–]namrog84 13 points 13 hours ago I'm not sure on the scope of your project or your computer hardware but I do work in both C++ and Blueprints. Most of the time C++ incremental compiles take <30 seconds. Quite frequently I see 10-15 seconds max. BP is great for prototyping and certain things, but its far from forced. permalinkembedsaveparentreportreply [–]HorrorDev 10 points 13 hours ago I've been using Unreal on a 2018 \"gamer\" laptop that's getting outdated very fast and, even though I'm a C++ beginner, my compile times barely hit 30s. The first compile took a while, sure, but incremental ones sometimes hit the 0.000s mark. Not sure what this guy's on about. permalinkembedsaveparentreportreply [–]namrog84 3 points 9 hours ago Exactly. It's 100% someone fine for someone to dislike unreal or whatever, but to spread what appears to be inaccurate or misunderstood information just bothers me and I just wanted to understand. Sometimes people may have just been misinformed or misunderstood something. There is a learning curve to certain C++ aspects. permalinkembedsaveparentreportreply load more comments (1 reply) [–]K1aymore 61 points 17 hours ago Godot? permalinkembedsaveparentreportreply [–]RavenTengu 96 points 17 hours ago Completely open source engine. Which also means you don't pay absolutely anything if you sell a game made with it. It has the flaw of being new and mostly supported by the community, but for indie titles it works more than fine. permalinkembedsaveparentreportreply [–]tungstencube99 115 points 16 hours ago Then let's start supporting it more gotdammit. Aren't we devs here? permalinkembedsaveparentreportreply [–]plastic_machinist 61 points 15 hours ago Exactly! Godot is definitely newer than Unreal/Unity, but it's already really fully-featured and is very nice to work with. It's fair to say that it doesn't have feature parity with Unity *yet*, but that can change if enough of us start using it and building things for / with it. permalinkembedsaveparentreportreply [–]s6x 4 points 13 hours ago What are some major things that you think need work? How is the project organised? What's the base written in ? C? permalinkembedsaveparentreportreply [–]PinkNGreenFluorideHobbyist 13 points 11 hours ago Godot itself is written in C++ permalinkembedsaveparentreportreply [–]GaiasWay 4 points 4 hours ago Afte playing around with the 4.NET version today, it felt to me like the Unity 3-5 days forked sometime around then and got better and leaner snce then instead of what really happened. permalinkembedsaveparentreportreply [–]Time-Most8514 59 points 16 hours ago True, Unity wouldn't be this massive engine if devs from the community hadn't put their time into creating plugins and extending it. This time, we have the benefit of actually having access to engine source code. permalinkembedsaveparentreportreply [–]Dabnician 21 points 14 hours ago* Unity wouldn't be this massive engine if devs from the community hadn't put their time into creating plugins and extending it. to be fair thats where \"the money\" in unity is. It was never making \"games\" instead you make some shitty little bit of code and sell it on the asset store for all the other dreamers that haven't hit the wall yet. Then i can just milk that asset every couple of years when a sale goes on, maybe actually update every so often and boom: residual income. In fact a bunch of the guys that made 3d assets for Torque 3d moved over to unity after it imploded and just started selling those same assets on the unity asset store. Heck if you do non code stuff on the unity asset store that stuff stands the test of time. permalinkembedsaveparentreportreply [–]CleverousOfficial 13 points 13 hours ago UAS is just as broken and riddled with serious issues. Failing infrastructure, idiotic C-level directives, ignored community feedback, failure to pay on time - or even pay at all - for years, lack of features, 90+ day queue for asset reviews, absolutely no support channel to communicate with (no staff, emails literally go into the void), etc... The only reason there's any UAS at all is because of Andrew trying to keep the whole thing with the community stitched together with fishing wire and duct tape while they can't even get engineers to fix the 15 year old APIs. Basically don't lean on that stick, it's gonna break soon. permalinkembedsaveparentreportreply load more comments (2 replies) load more comments (1 reply) [–]budxors 16 points 12 hours ago The greatest thing about this whole disaster is that some highly skilled devs might start contributing to Godot and we would all have a great engine to use that can’t just change the rules when they feel like it. permalinkembedsaveparentreportreply [–]FixationOnDarkness 9 points 15 hours ago Another engine I would really like to see get more love is Flax Engine. It's been described as a mix between Unreal and Unity, and it appears to be graphically pretty sound. It is also completely free and open source. It just needs a little love. permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (2 replies) load more comments (12 replies) [–]fish993 76 points 17 hours ago I recently learned about this concept of the Trust Thermocline. It's basically where a company is (perhaps inadvertently) relying on consumer goodwill and inertia to maintain their revenue/sales while making the consumer experience gradually worse over time, until they eventually push it too far, lose all their goodwill at once, and the annoyance with all the negative changes made up to that point is finally enough for most of their consumers to move to a different product. The key part is that once the company has reached that point, they can't just row back the last change to reverse it - the goodwill and trust is already gone and it's extremely hard for them to ever regain that. It was brought up with regard to CA's handling of Total War: Warhammer and it sounds like exactly what's happening with Unity as well. A lot of the comments are specifically mentioning that while they themselves are not directly affected, they now don't trust that Unity won't pull some other shit in the future and it's not worth the risk of developing future projects on their engine. permalinkembedsaveparentreportreply [–]spatzist 28 points 15 hours ago There's an entire line of business built around essentially buying out companies to run them into the ground while abusing the hard-won customer goodwill associated with their brand name. Their trick is they use this to inflate the stock value short term, then sell it at a profit and leave someone else holding the bag when the company's bloated, hollowed-out carcass finally collapses in on itself. permalinkembedsaveparentreportreply [–]BeeOk1235 5 points 12 hours ago sounds like unity is the next meme stock. permalinkembedsaveparentreportreply load more comments (1 reply) [–]MyPunsSuckCommercial (Other) 43 points 15 hours ago If they can retroactively change the contract like this, what's stopping them from literally charging every studio a trillion dollars each? You can't just change a contract after it's been agreed to; that kind of nonsense is all kinds of illegal, as Unity is soon to discover permalinkembedsaveparentreportreply [–]BeingRightAmbassador 11 points 12 hours ago You can't just change a contract after it's been agreed to; This is the root of the issue. They're attempting to charge new fees to already completed products that have stuck to their end of the contract. I'm sure Unity would feel quite differently if a huge studio decided that they're just not going to pay the Unity bill and move onto a different engine for their next game. permalinkembedsaveparentreportreply [–]Shadowverse_Beadgcf 7 points 7 hours ago Imagine how Unity would feel if they had to pay Microsoft an install fee for the .Net package each time a Unity user installs it. permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (1 reply) [–]The_Do_It_All_Badger 21 points 15 hours ago Retroactively adding fees makes court judges sad. And when they get sad they tend to cheer themselves up by bringing the damn hammer down on your skull. permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (2 replies) [–]RedEagle_MGN 4 points 15 hours ago Good read. Thanks! permalinkembedsaveparentreportreply [–]shrogg 4 points 14 hours ago This sounds exactly like what’s happening with jagex and RuneScape right now permalinkembedsaveparentreportreply load more comments (2 replies) load more comments (2 replies) [–]ZorbaTHut⠀ 78 points 18 hours ago Godot's pretty similar to Unity. You'll land on your feet almost immediately. The biggest difference is that it doesn't have gameobjects/components, it's just a hierarchy of objects. permalinkembedsaveparentreportreply [–]minegen88 46 points 18 hours ago Scenes, scenes, scenes everything is a scene! permalinkembedsaveparentreportreply [–]sbruchmann 39 points 17 hours ago Scenes, scenes, scenes everything is a scene! And scenes are just nodes. Nodes, nodes, nodes, everything is a node! permalinkembedsaveparentreportreply [–]TekuzoGodot|@Learyt_Tekuzo 9 points 16 hours ago I thought that Everything is a file. :P permalinkembedsaveparentreportreply load more comments (2 replies) [–]Alaskan_Thunder 4 points 15 hours ago And behind the scenes, nodes are just objects, as are resources permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (3 replies) [–]SweetBabyAlaska 35 points 16 hours ago The biggest plus is that they can never pull this shit, or retroactively pull the rug out from under everyone. The engine is open source and you can fork it, edit it and do whatever you want with the code. It could be a big plus for some game studio's who want to build tooling on top of Godot. The extension ecosystem, while still maturing, is robust. There are plenty of amazing extensions that help with terrain, steamworks and things like that. You can build and integrate your own tools with relative ease. I was surprised how decent Godot is. The UI creator is great. I actually made some desktop apps with it. There is also RayLib and their entire suite of tools that are also free and open source, with language bindings for most languages. permalinkembedsaveparentreportreply load more comments (2 replies) [–]unknown-one 17 points 17 hours ago as a beginner, my biggest issue is the available tutorials and all kinds of assets you can get free or paid. I dont have skills to develop everything from scratch. Sometimes I rather pay few $ and get working solution. too bad Unity assets can not be used in Godot or somewhere else permalinkembedsaveparentreportreply [–]plastic_machinist 19 points 15 hours ago In terms of art assets, there's actually a bigger pool to draw from, since Godot uses open formats (GLTF), so any GLTF can work directly in Godot even if it wasn't created specifically as a Godot asset. Sketchfab has (I think) all of their content, both free and paid, available as GLTFs, which is how they provide the in-browser 3d view. As for script assets /add-ons, it's true that there's nothing comparable in size to the Unity or Unreal stores, but there *are* asset libraries. There's a built-in asset library right in the Godot engine, as well as a few other options, namely: https://godotengine.org/asset-library/asset https://godotassetstore.org/ https://godotmarketplace.com/ https://godotassetlibrary.com/ https://itch.io/game-assets/free/tag-godot permalinkembedsaveparentreportreply [–]S3DBCommercial (Indie) 9 points 12 hours ago In terms of art assets, there's actually a bigger pool to draw from, since Godot uses open formats (GLTF), so any GLTF can work directly in Godot even if it wasn't created specifically as a Godot asset. I mean any standard 3D asset would work in Unity regardless of format too. FBX or OBJ, it all just works. I have never had any issues importing assets from outside of Unity into Unity, so idk why the pool would be bigger. permalinkembedsaveparentreportreply [–]vide0gam3r 19 points 16 hours ago The Godot documentation is excellent and there is a vibrant dev community to fill in the gaps. I also found Godot more intuitive to work with, but I suspect not everyone will feel the same way. You will definitely miss the asset store though as Godot doesn’t have the depth of the Unity asset store and third party plugin support yet. permalinkembedsaveparentreportreply load more comments (1 reply) [–]ZorbaTHut⠀ 9 points 16 hours ago too bad Unity assets can not be used in Godot or somewhere else Legally, they actually can, though this is generally useful only for assets, not code. permalinkembedsaveparentreportreply [–]DeepFriedCthulhu 4 points 15 hours ago According to this article they can. permalinkembedsaveparentreportreply [–]ambershee 6 points 14 hours ago Until they randomly, retroactively, change that license too. permalinkembedsaveparentreportreply [–]fredlllll 5 points 14 hours ago i landed on my feet and broke both ankles. not a fan of godot, but the only other alternative is unreal and im not going to touch that again permalinkembedsaveparentreportreply [–]ZorbaTHut⠀ 3 points 13 hours ago Out of curiosity, what issues are you running into? permalinkembedsaveparentreportreply load more comments (5 replies) load more comments (1 reply) load more comments (5 replies) [–]tomerbarkan 11 points 16 hours ago Whatever they do now, that trust is irreparably lost. I've been making unity games for like 12 years now, I've spent so much time and effort learning & using unity, that I don't want to switch engine. But if there's nothing to stop them showing they'll suddenly make license terms that can easily put companies relying on them out of business... which this is the second time they've done now, what's to stop them doing it again? The only way I see to regain this lost trust is to clean their stables and replace their management team. permalinkembedsaveparentreportreply [–]Syrelian 7 points 13 hours ago Until they get rid of their CEO, even that won't cut it, because their CEO is repeatedly on record as being a moneygrubbing scumbag who would think up exactly this shit, and is also historically a terrible CEO for previous companies like EA permalinkembedsaveparentreportreply load more comments (1 reply) [–]RagicalUnicorn 8 points 16 hours ago 100%. After all the bullshit we were already seriously lookin at godot, but we're already in production and couldn't afford the time sink. Now we're in the situation where we have no choice but to swallow whatever they serve up. But fuck knows never the hell again. I will never use Unity again, not unless they do some extreme house keeping and literally throw Riccifuckhead into the deepest part of the ocean. permalinkembedsaveparentreportreply [–]justking1414 15 points 17 hours ago That’s why I only make games in vanilla JavaScript. Ain’t nobody taking away my right to make games with that permalinkembedsaveparentreportreply [–]HillbillyZT 6 points 12 hours ago \"Oh boy I sure hope some new JS framework doesn't come out and ruin the word 'vanilla' we use to describe JS without external frameworks...\" Vanilla.JS \"Plain js\" it is I guess. permalinkembedsaveparentreportreply [–]Sea_Tip_858 13 points 20 hours ago After all that I still have tiny bit of faith they gonna roll back and charge per purchase like unreal did. Damn with this one I lost all hope and respect for unity. Ima switch to some open source engine. permalinkembedsaveparentreportreply [–]Numai_theOnlyOneCommercial (AAA) 5 points 16 hours ago There is barely a reason not to use unreal, the main issue though is that unreal is almost too powerful for a single dev to handle. And I say that as someone who usually tries to protect unity, but this change? Boy they will lose everything. permalinkembedsaveparentreportreply [–]SalaciousStrudel 3 points 10 hours ago Compiling shaders (208556)... permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (2 replies) [–]Time-Most8514 3 points 16 hours ago Someone refresh my memory, what was the 2019 controversy? permalinkembedsaveparentreportreply [–]Darkfrost[S] 15 points 15 hours ago It's a while back now, but unity made a sudden change in their terms of service to include the overly broad clause: You may not directly or indirectly distribute the Unity Software, including the runtime portion of the Unity Software (the “Unity Runtime”), or your Project Content (if it incorporates the Unity Runtime) by means of streaming or broadcasting so that any portion of the Unity Software is primarily executed on or simulated by the cloud or a remote server and transmitted over the Internet or other network to end user devices without a separate license or authorization from Unity. Which could be interpreted to mean you can't run unity... game servers... It was added due to a disagreement with Improbable, and people were not happy about this license change. Unity then decided to update their terms and make sure users could stick to the TOS that came with the version they were using (which they've now retracted) Forum post about this at the time: https://forum.unity.com/threads/recent-tos-update-blocks-the-use-of-spatialos-to-make-games-in-unity.610447/ permalinkembedsaveparentreportreply [–]ScaryBee 4 points 15 hours ago Overview here: https://www.engadget.com/2019-01-10-unity-improbable-epic-games-spatial-os.html?guccounter=1 tl;dr - Spatial built a business model that involved running many Unity instances in the cloud, Unity changed their TOS to make that non-viable. permalinkembedsaveparentreportreply [–]ModernEraCaveman 3 points 15 hours ago Mfw I couldn’t handle learning how to use Unity, Unreal, or Godot, so I decided to build a whole engine from the ground up using Vulkan😀 permalinkembedsaveparentreportreply load more comments (4 replies) [–]nerfjanmayen 239 points 20 hours ago How are they possibly going to track installs of old versions of the runtime? Unless this install tracker has been there the whole time. I read somewhere that they claimed they had some \"machine learning\" system to calculate install numbers, are they really just going to guess? Lmao permalinkembedsavereportreply [–]Ayamebestgrill 202 points 20 hours ago basically their source is just Trust me bro. permalinkembedsaveparentreportreply [–]ugathanki 50 points 13 hours ago I don't think that's true. I think it's worse. How else would they know how many people have installed software other than packaging spyware into every single build that you make using their engine? They want data on the customers of their customers, and this license is just a way to make sure that nobody questions why packets are being sent to Unity from the end user's computer. Absolutely ridiculous. permalinkembedsaveparentreportreply [–]Affectionate_Gas8062 10 points 9 hours ago The call is coming from inside your computer permalinkembedsaveparentreportreply [–]wekilledbambi03 82 points 19 hours ago They have been using Unity Analytics for like 5+ years now. Even if you are not using the service I would not doubt at all that they still have some of the code in every build. permalinkembedsaveparentreportreply [–]Intrepid-Ability-963 46 points 19 hours ago Yeah. They must have been tracking install numbers for a while to come out this brazenly. permalinkembedsaveparentreportreply load more comments (5 replies) [–]SlykRO 13 points 16 hours ago How can you track installation for an offline game when the PC installed to doesn't have internet connection? Also, if you send the players a zip of your exe and support files which requires no true installation to occur, how is this tracked? I am curious (mostly towards #2) since I am developing a small online RPG with Unity/Mirror and just have testers download the zip to test. permalinkembedsaveparentreportreply [–]draeath 17 points 15 hours ago If the machine is kept offline, I don't think they could know. But if that machine did go online and the game was launched (or some external tool was \"watching\" for an opportunity to upload batched analytics data), they can and probably are using a machine-specific (derived from stuff like network hardware, CPU serial number, etc) UUID. Windows, Mac, Linux, Android, and probably iOS all have an implementation of it that applications could access. permalinkembedsaveparentreportreply [–]907games 9 points 15 hours ago* they cant track it without internet connection. heres a post i found yesterday talking about this. https://i.imgur.com/Ly8ZQXQ.png edit: dont hate me pls, i found it as an image permalinkembedsaveparentreportreply load more comments (5 replies) [–]Ksevio 7 points 14 hours ago I imagine the numbers of offline installs are insignificant, especially if there's a 200k minimum that they care about permalinkembedsaveparentreportreply load more comments (3 replies) [–]MioXNoah 19 points 17 hours ago Yeah that's what I've been thinking, I don't think that they would have announced this out of nowhere, they know what they are doing. Must likely that spyware/DRM has been running on current games made with Unity for a long while. permalinkembedsaveparentreportreply load more comments (1 reply) [–]johndoedisagrees 4 points 14 hours ago* Tin foil hat on* The higher ups shorted the company and are banking on it failing. permalinkembedsaveparentreportreply load more comments (2 replies) [–]itsdan159 10 points 17 hours ago They're likely only going to target big games at first, if you've sold millions of copies and they say 'we estimate 1m installs' its actually in your interest to not contest that since you could end up owing more. permalinkembedsaveparentreportreply [–]ProfessionalPlant330 4 points 15 hours ago Even if it's not automatically tracked, they probably have people dedicated to scouring the internet to check this stuff. They can get an estimate from your sales/download figures, and then ask you for detailed figures with proof. They do something similar for license seats. If you have too many people using unity from the same IP and not enough seats, they will contact you to tell you to buy more seats or prove that you don't need more. permalinkembedsaveparentreportreply [–]nerfjanmayen 3 points 15 hours ago I mean, how would a dev even know how many times their game is installed? You could track total revenue or amount of copies sold, sure, but you have no idea if they're re-installing a game they bought 5 years ago or not. permalinkembedsaveparentreportreply load more comments (1 reply) [–]MorganaMalefica 78 points 19 hours ago I’m wondering what they’ll pull next. I… I don’t understand the people running the show. How did they think this would all play out? Are they aliens or something? Or are they so far removed from humanity? permalinkembedsavereportreply [–]IronCarp 98 points 18 hours ago The CEO is the same dude who wanted to charge people real money for ammo in Battlefield like 10yrs ago. permalinkembedsaveparentreportreply [–]Mari0wana 11 points 17 hours ago Wut? Heard he used to work for EA but this is the first I read about the ammo, got a link to that? Tried to find it but came with results about premium ammo in WoT. permalinkembedsaveparentreportreply [–]IronCarp 28 points 17 hours ago https://mcvuk.com/business-news/pc/when-riccitiello-said-battlefield-players-could-pay-1-per-reload/ permalinkembedsaveparentreportreply [–]Mari0wana 20 points 16 hours ago What a trashy waste of space, up until the way he's trying to downplay it. permalinkembedsaveparentreportreply [–]nostradamefrus 17 points 11 hours ago “It costs $400,000 to fire this weapon for 12 seconds” permalinkembedsaveparentreportreply load more comments (7 replies) [–]TitaniumDragon 10 points 14 hours ago He got fired from EA, too. Not sure why Unity hired him. permalinkembedsaveparentreportreply [–]TheQuuux 4 points 8 hours ago Remember the Nokia company suicide in 2010? They hired Microsoft's Stephen Elop, and within *days*, their stock tanked¹, their industry customers jumped ship. (¹ 62% stock drop overall, smartphone market share from 33% to 3%) permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (2 replies) [–]Kaznero 20 points 14 hours ago* Wealthy shareholders do not live in the same world as the rest of us. Even if this looks like an obvious financial blunder to us, to them they're tossing around pocket change. This won't make or break them, but it might make them a lot of money if they can force everyone to accept it. For everyone who actually relies upon the product however, this can be make or break. The stakes are totally different, which is why this seems so confusing. They don't care about the reputation of the product, how people are using it, or really video games in general. Just whatever will make them money right now and for little-to-no work. If that means ruining unity, they'll do it, because they can just sell it off once they're done extracting money from the userbase. Rich people being parasites and ruining good things for everyone like they always do. permalinkembedsaveparentreportreply [–]hawk_dev 3 points 9 hours ago s in general. Just whatever will make them money right now and for little-to-no wo this is one of the big reasons I'm liking open-source projects more. permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (26 replies) [–]Epsilia 268 points 19 hours ago Totally illegal. You can't change a contract without the consent of everyone involved. The only thing they can do is change the contract for renewals and new customers. I wouldn't touch Unity at all after this regardless, but legally, past games are safe. Unity probably needs to be sued into oblivion first. permalinkembedsavereportreply [–]197328645 184 points 17 hours ago Happens all the time, unfortunately. I bought a grill years ago, the firebox rusted out and I called to get it covered under the warranty. Customer service rep says the firebox isn't covered under warranty, and sends the warranty document on the company's website to prove it. Which was weird, because I specifically remembered picking that grill because the firebox was covered under warranty. So I spent an hour in my closet to find the original paperwork and there it is, firebox covered under warranty. They had modified the version on their website. Called them back and sent a picture of the booklet. They immediately agreed to cover it under warranty and gave me a $50 gift card. I wonder how many people they've scammed? Bastards permalinkembedsaveparentreportreply [–]Epsilia 66 points 17 hours ago Yep. Contract terms cannot legally be changed without both parties being involved, regardless if they say they can be in the contract. permalinkembedsaveparentreportreply [–]MyPunsSuckCommercial (Other) 19 points 15 hours ago Even if it's written that they can change it, that part of the contract is just unenforceable. You can sign a contract giving up the blood of your firstborn, and it just won't stick permalinkembedsaveparentreportreply [–]MuffinInACup 13 points 15 hours ago That's like contracts saying a company isnt liable for anything and you waive your rights to sue them permalinkembedsaveparentreportreply [–]mynewaccount5 6 points 8 hours ago I bought a phone once that was advertised as being crackproof. Moto something. They even advertised it with people dropping their phones. Well my screen cracked. Level 1 2 and 3 support all insisted that the crack proof warranty only applied to the metal back of the phone and not the screen. On the website from the time I bought it to that point they had sanitized it of most claims of being crackproof and they guy kept saying \"please show me where it says the screen\". I knew it was in my manual but who keeps their phone manual. Eventually I found it on some outdated page on their website where they kept old terms of use. Eventually the level 4 support person denied me since he had some definition of a crack vs a fracture in his internal documents which I wasn't allowed to see and appernelty existed this whole time. Later I found they removed that terms page I had found. permalinkembedsaveparentreportreply [–]cs_office 3 points 13 hours ago The government should be making honeypots and charging these fuckers with fraud permalinkembedsaveparentreportreply [–]xseodz 3 points 10 hours ago It's why it's always important to get physical signed copies of these documents. Companies will fuck you over, because the only avenue you have is to sue them. And are you really going to sue them over a grill? Probably not even worth the effort involved. That's what's sad about this, Unity can actually do WHATEVER they want. The only thing that stops them is someone having the balls to go at them in court, and that might take ages. What does everyone do until the case has finished? It could be 2, 3 years? What they settle and send everyone $20 for the trouble? They've already paid the dividends at that point. That's why we're meant to have regulatory bodies actively going after companies for this kind of thing, good luck with that though. permalinkembedsaveparentreportreply [–]mynewaccount5 3 points 8 hours ago Nobody is signing your warranty. permalinkembedsaveparentreportreply load more comments (2 replies) load more comments (1 reply) [–]ExF-AltrueHobbyist 54 points 19 hours ago \"By continuing to use the service, you agree to these terms\" bla bla bla... You know the drill. Now, there is a chance that sudden and substantial changes to that sort of contract would be voided by a judge if they tried to claim \"tacit approval\" but who knows? And who will spend the time & money to sue them? Don't count on the AAAs like Blizzard & the like. They most certainly have a custom deal in place for Hearthstone where their contract can't be ammended that easily. permalinkembedsaveparentreportreply [–]OdinsGhost 33 points 19 hours ago The developers agreed to the prior terms that expressly allowed them to stay on the version in place during development. That trumps any new “by continuing to use” language they might try. permalinkembedsaveparentreportreply [–]Epsilia 47 points 19 hours ago Reddit can also change their t&c to say \"By continuing to use this service, you retroactively agree to pay $1k per year for use of your account.\" doesn't make it legal. permalinkembedsaveparentreportreply [–]ExF-AltrueHobbyist 16 points 18 hours ago In one way, you could say that in contracts everything is legal until a judge rules adversely on it. If people settle or fold in other ways before it comes to that.. Then the illegal terms were, for all intents and purposes, legal. That's why companies in the US have a hard-on for arbitration. permalinkembedsaveparentreportreply [–]itsdan159 4 points 17 hours ago Retroactive would be tough yes, but Unity won't try to collect on retroactive installs, only measure them as a criteria for paying on new installs. permalinkembedsaveparentreportreply load more comments (6 replies) load more comments (7 replies) [–]CthulhuForPres2023 14 points 18 hours ago By continuing to use reddit, you agree to pay me $100 per day permalinkembedsaveparentreportreply [–]ExF-AltrueHobbyist 7 points 18 hours ago I'm not using reddit, reddit is using us! Reddit is free, we are the product! xD permalinkembedsaveparentreportreply [–]CthulhuForPres2023 17 points 18 hours ago By continuing to be used by reddit, you agree to pay me $200 per day permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (9 replies) [–]rogue6800 7 points 18 hours ago Most of these contracts say that you agree that they have the right to change the contract in anyway without your permission. permalinkembedsaveparentreportreply [–]Epsilia 18 points 17 hours ago Would be really difficult to actually get a court to agree with something like that. Just because it's in a contract doesn't make it legally enforceable. permalinkembedsaveparentreportreply load more comments (10 replies) load more comments (2 replies) load more comments (1 reply) [–]OdinsGhost 60 points 19 hours ago There is absolutely no way they can remove that clause in the current edition of the TOS and make it legally binding to developers that accepted and published under the prior ones that contained language allowing them to stay on the TOS version they were using for the project. permalinkembedsavereportreply [–]jl2lCommercial (Indie) 22 points 18 hours ago Yeah I mean this is basically contract law that is well established in legal sense. permalinkembedsaveparentreportreply load more comments (1 reply) [–]Zooltan 65 points 19 hours ago It's impressive with the shitstorm that Wizards of the Coast got for their license changes to D&D over the last year or two, that Unity thinks they can get away with something like this. permalinkembedsavereportreply [–]CyberKiller40DevOps Engineer 18 points 18 hours ago Time flies, but it was at the start of this year 🙂 feels like ages ago with all the drama that's happening daily permalinkembedsaveparentreportreply [–]itsdan159 6 points 17 hours ago And they actually ended up having to concede more than they wanted, at least for 5e. Once 6e/\"one d&d\" comes out all their new content will use their new onerous license, but 5e is more 'open' than it was before their nonsense. permalinkembedsaveparentreportreply [–]CyberKiller40DevOps Engineer 4 points 15 hours ago Which was actually a bad thing, as we see in the TTRPG community. People initially rushed to other games as a protest, spawning temporary interest in mostly Pathfinder, and rarely few other. But mostly everybody went back as soon as the license changes were reversed, and even more went to D&D when Baldur's Gate 3 released, enforcing the systems monopoly even more than before. 😐 Pathfinder's publisher is a winner on this though, cause regardless if people actually play their game, they sold out their whole supply and made a juicy profit. There is a lesson to be learned in this. permalinkembedsaveparentreportreply load more comments (3 replies) [–]Ujili 115 points 19 hours ago Friendship with Unity ended Unreal is best friend now permalinkembedsavereportreply [–]_Cap10_ 63 points 16 hours ago I'm going to try Godot first permalinkembedsaveparentreportreply [–]xXTheFisterXx 32 points 18 hours ago I just feel stupid that I never put effort into Unreal permalinkembedsaveparentreportreply load more comments (9 replies) [–]Programmdude 3 points 12 hours ago Unity is (was?) the best engine for consumers, because you could mod the code without the developer adding mod support (unless they do AoT compilation). Stuff like timberborn mods, pathfinder: kingmaker/wotr mods, none of that would exist in unreal as it's too difficult to mod it without developer support. This makes me sad, because even if unity backtracks or it gets struck down in court, most people are going to change anyway, and no more easy mod support. permalinkembedsaveparentreportreply load more comments (1 reply) [–]NightElfik 36 points 16 hours ago Holy s**t! Thanks for pointing this out. This is honestly even worse than the pricing update, and it went mostly unnoticed! So they were basically planning this for a long time, carefully switching everyone to the new TOS with 2022 LTS release and removing any evidence of the old TOS. What a scummy move, especially given the apologetic tone of that blog post. permalinkembedsavereportreply [–]ExF-AltrueHobbyist 61 points 19 hours ago If material modifications are made to these Terms, Unity will endeavor to notify you of the modification Press X for doubt. XXXXXXXXXXXXXXXXXXXXXXXXXXX permalinkembedsavereportreply [–]Zenith2017 25 points 17 hours ago Unreal, Godot etc gonna be eating good permalinkembedsavereportreply [–]sp8cewaveplaymercura.com 28 points 16 hours ago John Riccitiello himself saying they will never retroactively change the terms: https://www.reddit.com/r/Unity3D/comments/agn89u/comment/ee7oi5i/ permalinkembedsavereportreply [–]CelluloidRacer2 23 points 17 hours ago Yeah, I'm gonna block Unity domains/IP blocks at home. I don't really play many Unity games to begin with, but I'm not supporting this permalinkembedsavereportreply [–]WowWhatABillyBadass 19 points 16 hours ago Unity doing a ruined reputation/bankruptcy any% speedrun permalinkembedsavereportreply [–]Zaynara 40 points 17 hours ago they are trying to take money from devs who've already shipped on products already sold from licenses they arleady sold to devs and were paid for or whatever as it was? how is any of that legal? fuck unity permalinkembedsavereportreply [–]AbdDjamil_27 26 points 17 hours ago I'm no lawyer but I'm sure it's illegal. Changing the contract people sign without there consent is big no no (and don't say term and services aren't contract they are and we all sign them when we pressed we agree on term and services) permalinkembedsaveparentreportreply [–]tesfabpel 6 points 14 hours ago It depends but unilateral contract modifications are done and they can be legal: just think at when an online subscription like Netflix increases the monthly fee... In those cases you're given enough time to decide whether to implicitly agree or to recess from the contract... In this case though, I don't believe the changes done by Unity are legal: the worst offender is charging for an already released product and counting existing installs for the install threshold... https://ironcladapp.com/journal/contract-process/unilateral-contract-modification/ permalinkembedsaveparentreportreply [–]mynewaccount5 3 points 7 hours ago Yes. A developer could delete their game today and people could still install it and cost that developer money. They cannot do that anymore than a company could say that the peice per seat for unity was actually 100k and has been since 2015. permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (1 reply) load more comments (1 reply) load more comments (1 reply) [–]invisillie 14 points 17 hours ago Guess I'm never using Unity again permalinkembedsavereportreply [–]rmatherson 38 points 19 hours ago How does this extreme behavior not constitute a violation of Unity's fiduciary obligations to shareholders? This is pretty transparently a price gouge to get executives paid in the short term, by sacrificing an entire business. John Riccitiello did the same thing at EA. permalinkembedsavereportreply [–]Syrelian 7 points 13 hours ago Because this is what investors and shareholders demand MONEY NOW, and then they move on from the smoking corpse permalinkembedsaveparentreportreply [–]LifeSmash 3 points 11 hours ago Best I can figure is that you can plausibly deny any violation of fiduciary duty provided enough other people were on board with the decision. Making bad business decisions is not a crime (and shouldn't be, because we're all human beings who make mistakes). The legal issue is proving that tanking the company was the actual intent in a court, which is not trivial at all. permalinkembedsaveparentreportreply [–]hackingdreams 33 points 18 hours ago then changed the license to add additional fees retroactively, with no way to opt-out ...you can just keep using the old license under the old clause, because that's how laws work in the US. Nobody can force a new license on you, especially not when you have a perfectly valid one that grants you the right to keep using it perpetually. The first company they try to sue about this should fight it, win, and countersue for punitive damages. permalinkembedsavereportreply [–]Damaniel2 16 points 18 hours ago Assuming the company is big enough to have corporate counsel and the budget to pay said counsel long enough to see the trial through to a resolution. permalinkembedsaveparentreportreply [–]reachedsoftware 17 points 17 hours ago Nintendo is affected by this, the most recent Pokémon remake was made in Unity. Given how litigious Nintendo is, unless they already have their own license agreement with Unity I can’t see them standing for this. permalinkembedsaveparentreportreply load more comments (1 reply) load more comments (1 reply) [–]Squirrel09 13 points 18 hours ago I'm happy to receive my class action lawsuit $3 in 2026! permalinkembedsaveparentreportreply [–]jl2lCommercial (Indie) 10 points 18 hours ago This Crypto Ledge level of corporate suicide, if you want to alienate your customers in a weekend this will be taught at warton business school years from now as a textbook example of why you need to listen to your customers. permalinkembedsavereportreply [–]el0j 23 points 19 hours ago Suspect they're going to have as much luck \"back-dating\" this change as Wizards of the Coast had with their attempt at the same thing. That one ended in complete tactical 'defeat' for WotC, but probably saved the company. permalinkembedsavereportreply [–]TitaniumDragon 10 points 13 hours ago Their biggest competitor has grown like 100% as a result of it, though, and they lost a lot of people. The real problem is that a number of content creators moved away from the platform, including Critical Role, so have effectively turned a bunch of people who were previously promoting their product into competitors. That said, TTRPG design space is difficult, and most people aren't very good at it, which makes it easier for them to make missteps because most of their competition isn't into pulling new people into the game. permalinkembedsaveparentreportreply [–]iamansonmage 18 points 18 hours ago Unity went from Senator Palpatine to Emperor Palpatine pretty quickly. permalinkembedsavereportreply [–]Zolden 7 points 18 hours ago These things are ruining the company's relationships with customers. permalinkembedsavereportreply load more comments (1 reply) [–]polaarbear 9 points 17 hours ago Maybe not an option for people who have published apps and things that they need to try and support, but if you are a hobbyist like me who doesn't actually \"need\" them, you can just delete your account. I got rid of mine already. It doesn't matter if they walk it back at this point, they can't be trusted. I'm happy to migrate my experiments to Godot and/or Unreal. permalinkembedsavereportreply [–]Gabe_b 8 points 14 hours ago John Riccitiello is a scum fuck who should be cleaning toilets in a gulag permalinkembedsavereportreply [–]Lost_Madness 8 points 16 hours ago Anyone who just started a project in Unity should be moving away immediately. Everything else needs a migration plan off. The only want they will learn this does not work, is if it hurts. permalinkembedsavereportreply load more comments (1 reply) [–]FullMe7alJacke7 6 points 15 hours ago* I've been a die-hard Unity fan for years.... I even own stock in the company.... but all these recent antics of fucking the small developers is really making Unreal look like the move.... just like everyone with common sense left Twitch for YT & Tiktok, we shall leave Unity for Unreal. I, for one, would rather lose the thousands of dollars I have in assets than give Unity any more of my money. Instead of continuing to provide value to the small guys and actually putting out useful improvements, they have turned Unity slowly into a bunch of microservices they can charge you for, and its quite pathetic. permalinkembedsavereportreply [–]HatiusCrosaintus 5 points 13 hours ago Sell before the crash permalinkembedsaveparentreportreply load more comments (4 replies) [–]PronglesDude 5 points 15 hours ago Interesting business choice when they are already lagging behind Unreal in almost every metric. permalinkembedsavereportreply [–]WhySoScared 3 points 11 hours ago I guess they wanted to go out with a bang permalinkembedsaveparentreportreply [–]hawk_dev 4 points 10 hours ago If you are a Unity dev with many years of experience thinking about switching, Google about the \"sunken cost fallacy\" What you invested already is not lost, don't be afraid of learning new technologies, so many engines to discover! also check out Godot xD permalinkembedsavereportreply [–]MrMobster 26 points 19 hours ago Did they take a class in business management from Elon Musk or something? permalinkembedsavereportreply load more comments (5 replies) [–]AbdDjamil_27 7 points 17 hours ago I can't wait to see the trails and the lawsuits that are coming The trails will so fun to watch the unity lawyers will be in shambles it reminds me of the jhonny deep vs amber trail it's gonna be fun permalinkembedsavereportreply [–]Domarius 3 points 13 hours ago Trial. Trails are what snails leave behind. permalinkembedsaveparentreportreply [–]JAXxXTheRipper 4 points 14 hours ago How to nuke a company, any% speedrun Usually, once a contract is signed, you typically cannot change it unless all parties to the contract agree to the modifications. I'm pretty sure that shit wouldn't fly in the EU at least 🤔 permalinkembedsavereportreply [–]StoneCypher 5 points 13 hours ago You'd be a damned fool to use Unity unless the CEO has been replaced, at this point permalinkembedsavereportreply [–]taloft 5 points 5 hours ago I am altering the deal, pray I don’t alter it any further. permalinkembedsavereportreply [–]sort_of_peasant_joke 7 points 17 hours ago And there are still people defending Unity and telling that you shouldn’t migrate to another engine. Either slave mentality or employees in disguise. permalinkembedsavereportreply [–]MyPunsSuckCommercial (Other) 7 points 15 hours ago Or they have seven years of development sunk into a project, and prefer to live in an alternate version of reality where this is ok permalinkembedsaveparentreportreply load more comments (1 reply) [–]hamilton-trash 3 points 17 hours ago eli5? Does this mean that if I already shipped a unity game a while ago, I am not suddenly subject to the new fees? permalinkembedsavereportreply [–]LogicalFallacyCat 5 points 17 hours ago As I understand it if you're a developer that already released a game made with Unity the new fees apply to you even though it's not the terms of service you agreed to. Or at least that's how Unity's trying to push it. permalinkembedsaveparentreportreply [–]hamilton-trash 6 points 17 hours ago How does that work? That would be like me selling a computer then 5 years down the line I decide to charge customers 100 dollars for ever file they made. Unity can just retroactively decide that users owe them new fees? permalinkembedsaveparentreportreply [–]Pastramiboy86 15 points 17 hours ago No, they can't, Unity are full of shit and are going to get destroyed in any court that they try to force the issue in. permalinkembedsaveparentreportreply [–]Kinyajuu 12 points 16 hours ago The CEO seems to THINK they can. I'm pretty sure this will end up in court. permalinkembedsaveparentreportreply [–]TCGM 5 points 16 hours ago They can't, it's so illegal it has actual regulations about it. permalinkembedsaveparentreportreply load more comments (2 replies) load more comments (4 replies) [–]DevRz8 3 points 16 hours ago They're really quadrupling down on the sleaze lmao. permalinkembedsavereportreply [–]TheLastIronMan 3 points 16 hours ago “We don’t care about our product, we care about our return on investment.” permalinkembedsavereportreply [–]redditcdnfanguy 3 points 16 hours ago Yeah, we're seeing another piece of software destroyed by suits, like Winamp, and that once number one torrent client... permalinkembedsavereportreply [–]davidnovarro 3 points 15 hours ago Webarchive may be helpful https://web.archive.org/web/20220601000000\\*/https://github.com/Unity-Technologies/TermsOfService permalinkembedsavereportreply [–]RainbowSovietPagan 3 points 15 hours ago What great motivation to use Unreal! permalinkembedsavereportreply load more comments (1 reply) [–]TigerSouthern 3 points 11 hours ago It's truly one of the most baffling decisions I have seen in a company. permalinkembedsavereportreply [–]TheMysticTheurge 3 points 11 hours ago \"The day... Unity died.... And they were singin'... bye bye game developer guys. We thought we would not be sued, cause you are all small fries But then we got to court, and they said we're speakin lies. If unity goes, this will be why.... if unity goes, this will be why....\" permalinkembedsavereportreply [–]RyanRioga 3 points 10 hours ago My team and I have been working on a few game projects for the past year or so, and we all installed Unreal today and have started literally rebuilding everything we did in the past year in Unity to Unreal. Even if we aren't directly effected yet, we had to get away from this dumpster fire. RIP to the development cycle but man, this is insane permalinkembedsavereportreply [–]4ha1 3 points 6 hours ago You're now watching Unity implode in real time. permalinkembedsavereportreply load more comments (1 reply) load more comments (111 replies) about blog about advertising careers help site rules Reddit help center reddiquette mod guidelines contact us apps & tools Reddit for iPhone Reddit for Android mobile website <3 reddit premium Use of this site constitutes acceptance of our User Agreement and Privacy Policy. © 2023 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology π",
    "commentLink": "https://news.ycombinator.com/item?id=37499731",
    "commentBody": "Unity silently removed clause that let you use TOS from version you shipped withHacker NewspastloginUnity silently removed clause that let you use TOS from version you shipped with (reddit.com) 144 points by iliketrains 16 hours ago| hidepastfavorite54 comments mjw1007 15 hours agoEvery now and again I see someone on this site saying things along the lines of \"it&#x27;s a shame that programmers in general are so unwilling to pay for quality tools\", and I feel myself tempted to agree.Then something like this comes along and it becomes clear why it&#x27;s so important that everything we rely on is, at the least, free from \"I changed the deal\" events. reply iliketrains 15 hours agoparentThis is not about \"unwilling to pay for quality tools\", but completely changing the way they charge for the tools, which gets applied to all legacy software that ever used their tools, despite their previous (now deleted) clauses that new TOS won&#x27;t apply unless you use the new version, is just ridiculous to me.Even if one stopped using Unity to develop new things before this change, they are still on the hook for product installs (even if they are free games), which are by the way tracked by Unity \"proprietary data model\".Example: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Unity3D&#x2F;comments&#x2F;16hgmqm&#x2F;unity_want...A few quotes from the FAQ:Q: If a user reinstalls&#x2F;redownloads a game &#x2F; changes their hardware, will that count as multiple installs?A: Yes. The creator will need to pay for all future installs. The reason is that Unity doesn’t receive end-player information, just aggregate data.Q: Are these fees going to apply to games which have been out for years already? If you met the threshold 2 years ago, you&#x27;ll start owing for any installs monthly from January, no? (in theory). It says they&#x27;ll use previous installs to determine threshold eligibility & then you&#x27;ll start owing them for the new ones.A: Yes, assuming the game is eligible and distributing the Unity Runtime then runtime fees will apply. We look at a game&#x27;s lifetime installs to determine eligibility for the runtime fee. Then we bill the runtime fee based on all new installs that occur after January 1, 2024. reply duped 11 hours agorootparentMany years ago I worked at a startup that tried to do this. To make a long story short the goal was to make money off content that was made with the tool instead of selling the tool directly to people making the content.Zero sales later, we all lost our jobs because if you show up with a new pricing model that completely upends how businesses even account for their spending and pricing, it better be the greatest fucking piece of software ever made that has zero competition or an industry standard because no one is going to use it. reply senectus1 11 hours agorootparentprevlol, in the last week I have:Setup dualboot linux&#x2F;windows, installed my 6 fav games on the linux sideDecided I didnt like that distro, wiped it and reinstalled with a new distro. then installed my 6 fav gamesmessed something up and decided it would be faster to reinstall again, did that and re-installed my 6 fav games.got issues with my steam deployment, mucked about and fixed it but in the process deleted my previous install. realized i could just copy the data from the windows partition across and did that.With this scenario I could be up for 24 install charges, despite never playing the games.My 15 yr old son is teaching himself programing for the purpose of being a games developer and this news horrifies him. I really dont see this is going to last.*Edit, 30 installs. forgot to count the windows installs. reply kyleee 9 hours agorootparentUnity sends their thanks reply falcolas 14 hours agorootparentprev> This is not about \"unwilling to pay for quality tools\", but completely changing the way they charge for the tools, which gets applied to all legacy software that ever used their tools, despite their previous (now deleted) clauses that new TOS won&#x27;t apply unless you use the new version, is just ridiculous to me.That&#x27;s a risk you run when the company you&#x27;re buying your tools from is beholden to shareholders. Which a vast majority of the companies we can even buy tools from are&#x2F;will be. reply johnnyanmac 14 hours agoparentprevProgrammers are peak \"fine I&#x27;ll do it myself\" with a culture of freely sharing knowledge, so I both get it as a point to criticize and an honorable mindset.With that in mind, I a fine paying for tools I need if I understand the service behind it is massive and hard to replicate. A less controversial example is IDE&#x27;s. Visual Studio has a great free Suite and a justifiable pro edition to pay for (and enterprise, but I&#x27;ll leave the costs of million dollar corporations out of this). Jetbrains is pay up front (unless you use Android Studio) but their model lets you keep the version you paid a year for. These are good balances between subscription and ownership.Of course, these game engines are doing very heavy lifting, but it&#x27;s never okay to retroactively change you you monetize product you already launched. On top of all that, this plan simply doesn&#x27;t sound well thought out (or actively malicious if you want to go that direction). reply vanderZwan 2 hours agorootparent> Programmers are peak \"fine I&#x27;ll do it myself\" with a culture of freely sharing knowledge, so I both get it as a point to criticize and an honorable mindset.Tangent: I feel like you&#x27;re selling many other professions a little short here. Farmers come to mind, although I admit growing up in the countryside makes me a little biased there. reply pixl97 15 hours agoparentprevThis is the problem of \"electrionicification\" of everything. Oh, sorry, you need to sign a new EULA to use your cyber-hammer is the first thing every company would try if they could get away with it. reply plagiarist 15 hours agorootparenthttps:&#x2F;&#x2F;hackaday.com&#x2F;2021&#x2F;08&#x2F;02&#x2F;home-depot-is-selling-power-... reply pixl97 10 hours agorootparentWe are going to hell in a handbasket. reply fxtentacle 15 hours agoparentprevI think the solution is a model like JetBrains. You pay a subscription to get upgrades, but in effect you&#x27;re just automatically buying perpetual licenses. reply _aavaa_ 14 hours agorootparentPerpetual licenses mean nothing if the vendor changes the TOS on your old version (which is literally what’s happening here). reply RetroTechie 1 hour agorootparentIs that legal? You agree to version X, you didn&#x27;t agree to version X+1, right?So at least a license update would require confirmation of your continuing agreement with the new license, right? (whether that be silent, offering opt-out or whatever).Otherwise \"you agree to ANY future version\" would equate to \"we can 100% rewrite this agreement after you signed it\". Doesn&#x27;t seem like that would hold up in a courtroom.Game engines ship with products & are covered by licenses. Unlike some service covered by a ToS where company can change or stop offering service at any time. reply fidotron 15 hours agoparentprevTBH it is kind of amazing we don’t regulate these change of terms more strongly.As you rightly point out it is causing a justifiable lack of trust in things you actually pay for which is going to be very counter productive. reply Incipient 7 hours agorootparentRegulation is hard. Especially for contracts. You can couldn&#x27;t add \"it&#x27;s illegal to retroactively change the terms of a contract\" because there are often terms in a contract relating to just that. It&#x27;s often required in certain circumstances.In an instance like this, I&#x27;d say \"market forces\" isn&#x27;t entirely the wrong approach. Everyone migrates away from them and destroys their business. reply makeitdouble 7 hours agoparentprevThe \"unwilling to pay for quality tools\" is too shallow to be honest.One one hand we all pay a lot for quality hardware for instance, those machines aren&#x27;t free. Or we see people spend ungodly amounts on keyboards.On the other hand, I don&#x27;t see carpenters paying an arm and a leg for high quality tables. Nor home architects paying millions to other architects to build their homes. So why are people expecting programmers to be forthcoming about paying other programmers to build their tools ? Sure one can&#x27;t build everything by themselves, but at its core people will try to only pay for what is absolutely good value (or they have no choice than to pay), and not just throwing money at \"quality tools\". reply iliketrains 16 hours agoprevThe first comment of this FAQ thread is also talking about this.https:&#x2F;&#x2F;forum.unity.com&#x2F;threads&#x2F;unity-plan-pricing-and-packa...The FAQ is worth reading on its own (and very hard to believe as a Unity game dev, honestly, WTF). reply lawlessone 15 hours agoparent\"Q: If a user reinstalls&#x2F;redownloads a game &#x2F; changes their hardware, will that count as multiple installs? A: Yes. The creator will need to pay for all future installs. The reason is that Unity doesn’t receive end-player information, just aggregate data.\"Bonkersedit?: Am i reading it right that they want to retroactively charge developers extra fees for previous installs?I think this text i am quoting below is what they&#x27;re really trying to do. They know the their new system is unreasonable. They want to force devs to use their advertising service.\"Qualifying customers may be eligible for credits toward the Unity Runtime Fee based on the adoption of Unity services beyond the Editor, such as Unity Gaming Services or Unity LevelPlay mediation for mobile ad-supported games. \" reply Timon3 4 hours agorootparentThey just changed your initial quote!\"Q: If a user reinstalls&#x2F;redownloads a game &#x2F; changes their hardware, will that count as multiple installs? A: We are not going to charge a fee for reinstalls. The spirit of this program is and has always been to charge for the first install and we have no desire to charge for the same person doing ongoing installs. (Updated, Sep 13)\" reply omnimus 13 hours agorootparentprevSeems more like charging for new instalts. But how can you charge for install instead of sale? One user could easily cost more than what the game cost. reply jncfhnb 15 hours agoparentprevAs a former Unity game dev it seems extremely easy to believe reply daft_pink 13 hours agoprevReality is that Unity loses vast sums of money year after year. Know that if you develop with them, you are tying your company to a company that is vaporizing money by developing their platform year after year.If you are going to develop with them, then you should want them to earn a stable profit, so their platform is around for you. You may not agree with this pricing structure, but you should hopefully want them to make money and raise their prices somehow.However, I think it&#x27;s perfectly rational to conclude that they are an unstable partner that isn&#x27;t earning a profit, and it&#x27;s too risky to tie your development to them as this article sort of suggests.I just strongly suggest you either are in favor of them raising their prices somehow or switch providers, because losing over hundreds of millions of dollars every year is completely unsustainable. reply dmix 12 hours agoparent> Reality is that Unity loses vast sums of money year after yearCan you expand on why they lost money? Was it a market seeding thing? reply darreninthenet 15 hours agoprevIt seems no company is shy of the inevitable enshitification when they get to a certain size reply vanderZwan 15 hours agoparentIf I understand correctly, Unity got bought out by a company that makes malware, and these decisions have been made by a former EA guy. Truly a match made in hell. reply runevault 15 hours agorootparentThe former EA guy has been there for almost a decade, and ironSource was a merger&#x2F;Unity acquisition not a purchase of Unity. reply lawlessone 14 hours agorootparent\"Qualifying customers may be eligible for credits toward the Unity Runtime Fee based on the adoption of Unity services beyond the Editor, such as Unity Gaming Services or Unity LevelPlay mediation for mobile ad-supported games. \"Either way they appear to be trying corral devs into using their advertising software. LevelPlay is IronSouce.Combine this with their Gaming services. And they would control your backend. They would get a share of ad revenue directly, because it&#x27;s their ad network.At that point (this is just my opinion) you&#x27;re just sort of their employee but without the benefits. reply vanderZwan 2 hours agorootparentprevTechnically correct corrections, which I appreciate! Having said that, this new information does not change the conclusion I drew in the slightest. reply MikusR 15 hours agorootparentprevThe ea guy has been CEO of Unity for 10 years. And Unity is the one that bought&#x2F;merged with that malware company reply lawlessone 13 hours agorootparentHe only recently had to apologize for calling devs that refuse to that software idiots. reply johnnyanmac 14 hours agoparentprevAnd it&#x27;s exactly why I&#x27;m so adamant about some proper security (mental security) with whatever I develop, as well as the environment. Ownership is ideal but I&#x27;m not necessarily saying \"Open source or GTFO\". Just some reassurance that stuff I use or stuff I make can&#x27;t suddenly be hoisted up by some middleware company and taxed out the wazoo.I already don&#x27;t like how I have to keep coming back to Windows for various tooling in my work. reply Devasta 15 hours agoprevI never thought I&#x27;d see Musks destruction of Twitter be beaten, this is unreal. reply interestica 14 hours agoparent> this is unreal.No no, this is Unity reply hartator 14 hours agorootparentCrying out loud. reply vxNsr 15 hours agoparentprevNo pun intended? reply feelandcoffee 14 hours agorootparentNo, this is a Cry (for the) Engine. reply dathinab 15 hours agoprevso basically using unity has become a legal&#x2F;risk liability especially for smaller studios ....they are probably spending a non small amount of money on legal, marketing and other consulting ... how can you still f*-up that badlike has there been a single company in history which charged per install (!= per license) and had long term success (and wasn&#x27;t a monopoly) reply NelsonMinar 15 hours agoprevWow that&#x27;s ugly, particularly the part about hiding the change in the repo history. reply glimshe 15 hours agoprevUnity used to be the symbol of indie developers rising against stifling Big Game. Now Unity is the incarnation of everything that made Big Game bad. Shame on you, Unity. reply bloopernova 15 hours agoprevAll this BS just reinforces the feeling I have that corporate executives have zero knowledge of the company they run.How did unity leadership misjudge their market by so much? reply sbarre 15 hours agoparentAt, and beyond, a certain size, a lot of companies (but not all of course) end up full of self-serving \"yes people\".. Business- and money-focused people who joined the company after it took off with success, intent on \"hitching a ride\" for their own career growth..(Inherently there&#x27;s nothing wrong with this motivation, but I&#x27;ve found the kinds of people who seek out these opportunities tend to be of a certain type as described here)Those people eventually drive out the passionate and user-focused creators who were initially responsible for the innovation and success, as they focus solely on value extraction and pleasing&#x2F;supporting the executive leadership.What happens next is.. this kind of enshittification. reply _aavaa_ 15 hours agoparentprev> How did unity leadership misjudge their market by so much?That remains to be seen. There&#x27;s lots of noise about it right now, but if number goes up it will be hard to argue (even if it&#x27;s true) that this was a bad decision. reply rich_sasha 15 hours agoprevThis kind of thing keeps happening with [semi] open, [semi] free products backed by a commercial company. That doesn&#x27;t make it good, but I wonder if the business model just doesn&#x27;t quite work.It&#x27;s fine early on, especially if you have a funding backer (large company, generous VC etc). But eventually you need to produce revenue.Worse still, once you realize this, you are perversely encouraged to lock in as many people into the \"free\" platform before pulling the rug. Even if that&#x27;s not your initial plan.I sincerely feel for all the indie gamedevs, this must be terrifying, I&#x27;m only commenting on the broader problem. reply RetroTechie 40 minutes agoparent> But eventually you need to produce revenue.Often it might be a case of:\"We&#x27;re doing X revenue right now. Let&#x27;s do whatever it takes to (try and) increase that revenue\".Versus:\"We&#x27;re doing X revenue right now. Let&#x27;s use that revenue to do the best we can & improve product. Hopefully bringing in more customers, so we&#x27;ll have more resources to improve product or develop new ones\".The 1st is profit focussed, leading to enshittification.The 2nd is customer-focussed, leading to innovation & better products.Unity clearly chose to chase profits above customer satisfaction. reply _aavaa_ 15 hours agoparentprevUnity may be free, but it is by not \"open\" by any stretch of the imagination reply rich_sasha 13 hours agorootparentYeah, I miswrote. I meant permissive and (formerly) \"friendly\" pricing. reply lawlessone 13 hours agorootparentprevI guess it&#x27;s a freemium sdk. reply gcr 15 hours agoparentprevwhat do you mean by calling unity semi-open?Nothing about Unity has been “open” since its founding reply whstl 14 hours agorootparentI know it still doesn&#x27;t mean it is \"open\", but I wonder if GP means that it \"has a rich ecosystem anyone can participate\", aka \"lock-in\". reply gcr 15 hours agoprevWould it make sense for some large studio to sue for specific performance of the contract they agreed to? I’d be surprised if unagreed TOS language were enforceable like this reply lawlessone 13 hours agoparentAre large studios beholden to the same contract though? I assume at a certain point a studio is large enough to negotiate their own contract with unity. reply iliketrains 14 hours agoprevWhy did this post disappear from the front page? I understand that this is now a heated topic, but I think it is good for people to know about things like this. reply bakugo 15 hours agoprev [–] I really hope one of the companies relying on Unity to make their billions sues them for this. They really need a reality check. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Unity, a widely-used game development platform, faced criticism for quietly eliminating a GitHub repository that monitored its terms of service changes.",
      "Users express worry about the platform's transparency, fearing covert fee increases, resulting in doubt about Unity's reliability and potential legal ramifications.",
      "There are ongoing debates concerning Unity's management issues, its potential collapse, the impact on users and the gaming community, and dissatisfaction with alterations to licenses and fees."
    ],
    "commentSummary": [
      "Unity, a game development platform, has subtly altered its terms of service, leading to user charges for product installations and new fees—even if they halt using Unity for future projects.",
      "These changes have generated user unrest and mistrust, due to perceptions of the change as unfair and ill-conceived, and concerns about fees for multiple installs and retroactive charges.",
      "Discussion extends to the challenges of open and free products backed by commercial companies, Unity's struggle to generate revenue, and doubts regarding its business model's transparency and financial condition."
    ],
    "points": 143,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1694628583
  },
  {
    "id": 37503351,
    "title": "Game Development Post-Unity",
    "originLink": "https://www.computerenhance.com/p/game-development-post-unity",
    "originBody": "Computer, Enhance! Subscribe Sign in Discover more from Computer, Enhance! Programming courses, interviews, commentary. Over 18,000 subscribers Subscribe Continue reading Sign in Game Development Post-Unity What are the options for game developers looking to migrate away from Unity? CASEY MURATORI SEP 14, 2023 56 22 Share My tenure in the game industry was working on game engine code, not with game engine code. As a result, I do not have any first-hand experience choosing an off-the-shelf game engine. It’s not a decision I’ve ever had to make, and I don’t keep up with the latest developments across the myriad of engine options. But I do follow game business trends to a certain extent, and for well over a year now, I’ve been warning that Unity’s relationship with game developers would inexorably change for the worse. This was not based on any inside knowledge. It was based solely on the financials they report, and the kinds of statements they make to investors in their earnings calls. If you read these regularly, you get a very good sense for what a company’s priorities are in two ways: one, because you hear how they are pitching their future to investors; and two, because you can see how far in the hole they are financially, so you know whether they are likely to make major business model changes. A Predictable Disaster In Unity’s case, paying attention to their quarterly investor materials tells you many things that aren’t as common knowledge as they should be. As a small example, most developers think Unity is primarily in the business of selling game engines. They’re not! Less than half their revenue comes from game engines. Over half comes from advertising. Their bottom line is more affected by the advertising market than it is by how many developers buy their engine. There are many things like this you learn when you dig into their investor materials. In my opinion, none of their fundamentals boded well for Unity’s future as a game engine on which to base a game company — hence my tweets (“X’es”?) on the subject. A few months after I posted that Unity seemed likely to be absorbed, Unity merged with IronSource, a controversial company that has been accused of developing malware. This was the first indication that things might (unfortunately) be going the way I had anticipated. Now, less than a year later, Unity has announced that they will be retroactively(!!) changing their license terms in order to charge game developers a substantial per-unit fee, despite explicitly disavowing “royalty-like” structures in the past. This move has sent shockwaves through the developer community, as one of Unity’s differentiating factors was that they only charged an up-front cost for their platform. Developers are understandably upset by this move. Unity already charges high per-seat monthly rates to professional developers. At $185/mo, Unity Pro is twenty-six times as expensive as a subscription to Office365, and three times as expensive as Adobe’s “all apps” Creative Cloud subscription. At $250/mo, Unity Enterprise is among the most expensive end-user SaaS offerings a game developer might see — more expensive even than Autodesk’s Maya and 3DS MAX. Yet despite these SaaS offerings being less expensive than Unity, to the best of my knowledge they have never tried to demand anything like a royalty from their paying customers. If they did, it didn’t last, because they do not charge any today. Even if Unity ends up backpedaling due to backlash from this decision, it seems unlikely that developers will trust Unity going forward. Many are wondering if they should switch engines now to avoid relying on an untrustworthy partner for their core technology. But switch engines to what? Alternatives to Unity Switching engines from Unity requires knowing what engine you might switch to. To that end, I was recently asked on X what I recommend as an engine for people who want to switch away from Unity. Since I have no opinions on this, I went ahead and asked developers to weigh in: To make it easier for people to explore the responses, I tallied how many times each alternative engine was mentioned, and I’ve prepared a list from most-mentioned to least-mentioned. I’ve also tried to summarize a little bit about what the engine seems to be “about”, but, take that with a huge grain of salt. As I’ve said multiple times now, I don’t use off-the-shelf-engines, so I have no first-hand knowledge here. If I’ve summarized anything inaccurately, please let me know in the comments, so I can correct it! I’ve also tried to include the “scripting language” supported by each platform where applicable, so that people can quickly see if the engine directly supports a particular scripting language they know. Because they often don’t have official names, I’ve used “visual” if the platform has a built-in flowchart-like visual scripting language. Godot (C#, GDScript, visual). By far the most-mentioned alternative to Unity, Godot seems in many ways to be an open-source response to Unity: less focused on high-end engine features than something like Unreal, and more focused on being quick and easy to spin up for beginners. Common complaints seem to be that it is “not quite there yet” — perhaps because of bugs and missing features compared to Unity — and that it lacks first-class support for consoles, a feature most professional game developers now need. But it has been used in some recent notable games (Brotato and Dome Keeper), and it seems to have decent momentum behind it. Unreal (visual). The Unreal Engine needs no introduction. A hefty percentage of the AAA releases in any given year run on one version or another of Epic’s flagship engine. Although it offers a host of AAA features like Nanite, Lumen, and Metahuman, common complaints are that its complex nature makes it more difficult to get started, and requires more technical expertise to work with. However, as Ethan Lee — who does have a ton of experience working with off-the-shelf-engines — wrote a few months back, apparently shipping a game on Unreal is actually easier these days in many ways than shipping one on Unity if you’re aiming for a quality release. Defold (Lua). Mentioned almost as many times at Godot and Unreal was an engine I’d never heard of, Defold. Apparently this is a good engine to check out if you’re developing 2D or mobile games, and it appears that tons of mobile-style games have been shipped on it. If you’re looking for more of a 3D engine, it is probably not what you’re looking for. RayLib. RayLib is not an engine per se, but rather a library suite that allows you to quickly build games (and apps) in a native language like C++. It was mentioned multiple times, so it seems like DIY devs enjoy using this library — but, people looking for a turnkey solution with a full editor like Unity probably won’t be interested. Open 3D (Lua, visual). This is actually CryEngine, which was licensed by Amazon and made into LumberYard, which was then abandoned by Amazon and made into Open 3D, which is now maintained as open-source. So, if you like CryTek, you might be very interested in Open3D! However, being a descendant of a highly-specialized AAA-focused engine does suggest it might have a harder learning curve, so it’s unclear how much of an option this is for less technical developers. GameMaker (GML, visual). For 2D games, GameMaker is an evergreen favorite, and very beginner-friendly. Many famous 2D games were made with GameMaker — Undertale, Forager, Hyper Light Drifter, Gunpoint, Hotline Miami, and Spelunky, just to name a few. If you don’t need 3D or fancy lighting, it seems like a solid choice. But for anything more technically demanding, GameMaker unfortunately seems to top out quickly. Unigine (C#). Though not primarily targeted at games, Unigine was mentioned multiple times as an option, and they do list games as a first-class application of their SDK. How useful a simulation-oriented SDK would be to someone coming from Unity, however, is an open question. Bevy. For Rust aficionados, the most commonly mentioned engine was Bevy. Presumably, if you’d like to program games in Rust, this would be an interesting one to check out, despite not yet establishing much of a pedigree. That said, it also doesn’t seem to ship with a complete editing environment, but rather some embeddable tools for putting editing in the game itself. This might be more of a hard sell for people coming from Unity’s integrated editing environment. Flax (visual). Like Defold, I’d never heard of Flax, but it touts a surprisingly large feature set, and provides integrated editing capabilities out of the box. However, it doesn’t seem like there are many (or any?) notable games shipping on this engine yet, which leaves a bit of a question mark as to its viability. Cocos (JavaScript/TypeScript). Another full-featured engine with an integrated development environment, the modern-day Cocos suite is apparently the same tool lineage that was used fifteen years ago to make FarmVille. Although I wasn’t able to find out much about this engine, it does appear to still be mobile-focused. Stride (C#). Although it was mentioned multiple times, I had a hard time finding out much about this engine. Apparently, this is Silicon Studio’s Paradox engine, which was renamed Xenko, and then later renamed Stride. I’m not sure if any notable games have shipped on it, but like Unity, it is a C#-focused engine, and comes with a full editing environment. Monogame (C#). Microsoft’s XNA was one of the most successful game-development-targeted SDKs ever made, spawning Bastion, Owlboy, Timespinner, Magicka, Axiom Verge, Serious Sam Double D, and of course, Stardew Valley. Being Microsoft, they responded to this overwhelming success by canceling the project. Thankfully, community developers picked up the slack, and Monogame is the result. A reimplementation of the XNA 4 API, it continues to be a popular base SDK for C# developers. And that’s not all. In addition to the platforms listed above — which were mentioned multiple times — there were several others mentioned only once, so I did not include them in the list. But that doesn’t mean they aren’t worth checking out! If you’re looking for more platforms to evaluate, the other ones people mentioned were: Construct, Ogre3D, Solar2D, HARFANG 3D, CryEngine, FNA, libGDX, LÖVE, Fyrox, C4Engine, Hazel, Wicked, TelluSim, and heaps.io. Please Share Your Experience! The biggest thing I realized when compiling this list was that most developers probably don’t know what all their options actually are. I bet a lot of people use Unity by default, not because they actually looked at all these other engines and determined that Unity in particular was best for their game or studio. I think a huge help to everyone right now would be if developers could share their experiences with alternative engines. What kind of project did you use it for? How did development go? Are there any particular links to tutorials or educational materials that might help people get started? Do you know of any good overviews? I am leaving this comment section open to everyone, so anyone who has experience to share can leave it in the comments below. Please try to be polite and helpful! And most of all, please try to give honest guidance to people about the engines you’ve used. That way people looking to make the switch can make a well-educated decision about which platform is best for them. Subscribe 56 Likes · 3 Restacks 56 22 Share 22 Comments TJ Kotha 10 hrs ago · edited 7 hrs ago Liked by Casey Muratori I work as a maintainer for the Open 3D engine. To echo some of the statements in the blog, the engine definitely has a higher learning curve compared to some of the other engines, like Godot, but I've been impressed with the progress being made to improve the engine each release. For some perspective, the history of O3DE's development involved wrangling the legacy code of CryTek and Lumberyard combined. That is a massive undertaking, and most of the work went into cleaning that up. I think now it's beginning to reach a point where O3DE can start shining on its own. Time will tell though If anyone has questions for me, feel free to ping! email: tkothadev (at) gmail.com LIKE (5) REPLY SHARE Ben 10 hrs ago Liked by Casey Muratori Also might be worth noting Unreal has a \"Unity to Unreal\" conversion guide in their documentation: https://docs.unrealengine.com/5.3/en-US/unreal-engine-for-unity-developers/ LIKE (2) REPLY SHARE 20 more comments... Top New Community Welcome to the Performance-Aware Programming Series! Watch now (22 mins)A brief introduction explaining what \"performance-aware programming\" is, and what the course will be about. FEB 1 • CASEY MURATORI 548 87 \"Clean\" Code, Horrible Performance Many programming \"best practices\" taught today are performance disasters waiting to happen. FEB 28 • CASEY MURATORI 423 127 Waste Watch now (33 min)How many CPU instructions does Python take to do one CPU instruction? FEB 2 • CASEY MURATORI 492 168 See all Ready for more? Subscribe © 2023 Casey Muratori Privacy ∙ Terms ∙ Collection notice Start Writing Get the app Substack is the home for great writing",
    "commentLink": "https://news.ycombinator.com/item?id=37503351",
    "commentBody": "Game Development Post-UnityHacker NewspastloginGame Development Post-Unity (computerenhance.com) 134 points by generichuman 10 hours ago| hidepastfavorite129 comments CobrastanJorji 9 hours agoThis blog post points out something really interesting:> Less than half their revenue comes from game engines. Over half comes from advertising.That is to say, Unity makes most of its money from people PLAYING games made with Unity. The sales to the developers are secondary. Unity had to change their model, and that meant either making the engine cheaper to acquire more games to get more ads, or it meant raising the price of the engine at the likely cost of ads, and for some reason they chose option 2, which seems like a dumb idea.The best explanation for that I can think of is that almost all of the advertising money should be coming from smaller mobile games, and so this is a move to try and make more money from the desktop games and the mobile games that don&#x27;t use Unity&#x27;s ad networks, which probably look like big, untapped sources of income to dumber product managers.But now imagine that they did the opposite: they raise the maximum revenue requirements and \"must show splash screen\" requirements and generally make Unity more available for less. Engine revenue goes down a bit, but ad revenue goes up, which probably works out even better in the long run, but also solidifies the user base, garners good will, and generally leaves everybody feeling great about Unity. reply qiqitori 7 hours agoparentInconvenient opinion in the adware-fueled tech world, but ad income isn&#x27;t a stable or safe source of income, and it&#x27;s likely they&#x27;re feeling that, and therefore jacking up the price for the engine. The big tech companies themselves feel it and spend a lot of money on \"Other Bets\".Noob consumers may click ads, but after a while they program their brain to filter them out automatically. Or occasionally filter them out using technological measures. I bet that most ad clicks are by mistake or similar (especially ads in Unity games (which AFAIK are interstitial video ads)), or the user wasn&#x27;t realizing what they were clicking on was an ad (noob users, or e.g. ads at the top of Google results, which used to be much easier to distinguish from actual results).Most of my ad clicks are by mistake (due to bad page layout, or there just happened to be an ad where I happened to click), some (a handful of times per year at best) are to satisfy curiosity. And maybe a single one in my lifetime actually led to a purchase.Google ads started out text-only, now they&#x27;re pretty jarring on average. Guess why. However, there&#x27;s a limit to how jarring you can make ads before consumers say bye-bye. Unity ads are probably way over the limit, Google are sort of tip-toeing along the line (or let&#x27;s say they&#x27;re over the line for a minority, and not quite for the rest). reply johnnyanmac 4 hours agorootparent>Inconvenient opinion in the adware-fueled tech world, but ad income isn&#x27;t a stable or safe source of income, and it&#x27;s likely they&#x27;re feeling that, and therefore jacking up the price for the engineYou&#x27;re right, but to expand: Apple&#x27;s privacy changes at the beginning of 2022 screwed over a lot of adtech, and now the rise of AI is making ad providers even more speculative of certain types of content. I think Adtech is recovering now but for a company like Unity who relies on ads, it dealt them a huge blow for the last 18 months. That&#x27;s probably what promtpted the merger&#x2F;aquisition and started its plans into how it can extract more from its devs. reply doctorpangloss 5 hours agorootparentprev> Noob consumers may click ads, but after a while they program their brain to filter them out automatically.Ads are not just about clicks. Half of all spending is brand advertising.Noobs don’t click random SMB ads, and yet that’s like 40% of spending on mobile.There are people who are “superclickers” and also super buyers. They’re just a really expensive audience to target, obviously.Anyway, you’re extrapolating from your sole experience with ads. It is a super big ecosystem with a lot of rationality, as ridiculous as that sounds. It is growing a lot still, digital ad spending grows like $130b a year around now, which is the highest it’s ever been. So I don’t know, you’re not saying stuff that is substantiated in truth. reply codetrotter 5 hours agorootparentprev> ad income isn&#x27;t a stable or safe source of incomeTrue, and it’s not even just about the ads being annoying even.Regulators are catching on to the privacy problems tied to online advertising.Perhaps Unity is seeing this too, and have concluded that regulators might come for their ad revenue one day. reply foota 9 hours agoparentprevI don&#x27;t think there&#x27;s anyone not using unity today that would use it tomorrow because of a pricing change (well, at least, for values of today and tomorrow from last week) reply jay_kyburz 8 hours agorootparentI wouldn&#x27;t even care if they switched to Epics model and asked for 15% of revenue after the first 1M - For games making use of Unity 2024 of course. reply MattRix 7 hours agorootparentEpic’s model is 5% not 15%. 15% would be absurd. reply johnnyanmac 4 hours agorootparenta big jump but not unheard of. remember that the biggest store platforms take 30% already; Apple, Google, Sony, Valve, Microft, and Nintendo. In a vacuum it&#x27;s not unthinkable to suggest that the engine you make your game on and leverage many features for (networking, porting, collaboration, etc) is at least half as valuable as the store you&#x27;re hosting the game on.Personally I think it&#x27;s the store takeaways that are way too high. And in some cases overstep their boundaries in what they allow an app to do or how to sell it. But that&#x27;s a big tangent. reply jay_kyburz 6 hours agorootparentprevYes sorry, think I got confused about the store rev share, which I think is 12% now anyhow. :) reply nonethewiser 8 hours agorootparentprev… do you have a game doing 1M in revenue? reply 015a 7 hours agorootparentKingdom Hearts 3, Hellblade, Gears, FF7R, Borderlands, Sea of Thieves, Fallen Order, and countless others all use Unreal, in all likelihood made over a million dollars, and many made sequels using Unreal as well. Its also absolutely necessary to clarify that Unreal&#x27;s revenue share is 5% over $1M, not 15% (and, weirdly; its 5% over $5M for games distributed on the Oculus Store, believe it or not). reply jay_kyburz 7 hours agorootparentprevYes.And we are switching to Unreal. The price is not why we were using Unity in the first place. I don&#x27;t like Blueprints and I don&#x27;t like C++. Verse looks like it&#x27;s going to be super weird.Unity is more versatile, smaller, easier to use. Faster to iterate.I&#x27;ve worked with both engines for many years, on the Bioshock games in Unreal and Void Bastards in Unity.But Unity has shown time and time again that they have no respect for us, so we really can&#x27;t continue to do business with them. reply ffhhttt 6 hours agorootparentBut for most games Unreal still seems to be significantly more expensive after you actually start making significant money? reply loupol 1 hour agorootparentPredictability of costs is important.If I use Unreal, if my game generates 2M$ revenue, I know exactly what I&#x27;ll need to pay Epic. With Unity&#x27;s model it&#x27;s a coin toss. Are lots of people going to reinstall my games ? Am I going to get install bombed ? I have no idea. replyedgyquant 7 hours agoparentprevIf their decision was made at the expense of advertisers&#x2F;advertising it’s one we should all wholly support. reply squeaky-clean 6 hours agorootparentIt&#x27;s not at the expense of advertisers, the advertising is staying the same. It&#x27;s only at the expense of the advertiser in the sense that Unity is the advertiser and they&#x27;ve shot themselves in the foot. reply johnnyanmac 4 hours agorootparentprevGiven that they merged&#x2F;aquired an adtech company accused of developing malware last year, I think Unity is having its cake here. reply MattRix 7 hours agoparentprevJust because Unity’s revenue from ads is greater than from the engine doesn’t mean they need to change their model. It’s ok for one part of their business to make more, especially when the engine is still making lots of money too. reply ffhhttt 6 hours agoparentprev> goes down a bit, but ad revenue goes upThe ad market is more competitive and very crowded there is no reason to use Unity for your ads if someone else offers a better deal.Also Unity did try focusing on increasing ad revenue over the last several years, it didn’t work out that well.> leaves everybody feeling great about Unity.Making their engine entirely free would make most people feel even better.> untapped sources of income to dumber product managers.Why? Most Unity’s client’s don’t make much if anything at all a few make massive amounts. Due to the fixed licensing model the latter barely pay anything to Unity (e.g. compared to how much they pay storefronts&#x2F;platform owners&#x2F;ad networks). If they want to continue growing their revenue Unity has to get more from these customers, there aren’t that many other options. reply aschearer 9 hours agoprevDidn&#x27;t find this article especially helpful. Author admits to having no special knowledge so literally polls Twitter and shares the results. Doesn&#x27;t really add much to the discussion.I&#x27;d really like to see how alternatives handle stuff such as:- Editor tools for non-artists- Editor API for customization&#x2F;automation- Profiling CPU&#x2F;memory- Integration with things like FMOD, Spine, whatever else- Tools for debugging- How does level editing work, what tools are present to assist with visualization, organization, construction reply johnnyanmac 4 hours agoparentCasey is a seasoned dev in the scene, with a decent resume of games and an even more impressive resume on knowledge in engine tech. Most notably, he&#x27;s working on a new programming language called Jai that is tailored for game development.So he may not be the most appropriate person to ask for game engine advice (since he says in the article, he makes and rolls a lot of his own tech), but he will inevitably be asked for his thoughts and advice on the situation regardless. reply badpenny 1 hour agorootparentAFAIK Casey Muratori isn&#x27;t working on Jai, that&#x27;s (entirely?) Jonathan Blow. reply johnnyanmac 1 hour agorootparentYes, you are correct, I was mixing engine programmers that both worked on \"The Witness\". Casey is known (among many other tools and research) for Handmade Hero[0], an engine he uses to teach very low level game programming concepts. Which he is using to release a full game with (and you can \"pre-order\" the game on the website which gives you access to the source code)0: https:&#x2F;&#x2F;handmadehero.org&#x2F; reply webprofusion 8 hours agoprevGodot is the most obvious answer from a long term non-profit perspective, but it&#x27;s not without it&#x27;s caveats. Rewriting is obviously difficult, starting new projects is somewhat easier. The main issue I see is the Unity asset store ecosystem is completely different and provides massive amounts of functionality for current games, which Godot doesn&#x27;t yet have much of.I&#x27;ve tried Godot a few times and basic stuff like export to web didn&#x27;t work at all out of the box, and it felt like alpha quality stuff. I would however like it to succeed.Unity also has extensive documentation and training and it&#x27;s an established tool for game devs who then migrate to industry jobs like industrial process&#x2F;environment simulations. I have a feeling Godot is barely aware of these scenarios as it&#x27;s rarely the engine of choice. reply clnq 8 hours agoparentTo put this very directly, Godot is a meme in the commercial games industry. Most of companies with above AA cash would rather write their own engines than use Godot, as the maintenance effort would probably exceed the effort to write the own engine to spec.Maybe Godot is more seriously considered by indies and open source fans. But then, Unreal Engine is also practically open source for any licensee, including ones that won’t ever pay a cent to Epic. So even in indies, it’s rare to see Godot.I champion all development in games. So I wish good luck to Godot. I’ve actually been thinking about contributing, too. I think in some ways, the ethics of Godot are a breath of fresh air. But contributing to UE looks much better on a resume.There are a lot of forces working agains Godot: inertia, economics, instability, lack of talent, lack of appeal to talent. No marketplace actually is the least of its concerns as we don’t use marketplace stuff much in AAA&#x2F;AA. The engine makes a lot more sense on paper than in reality. reply meheleventyone 6 hours agorootparentIt’s worthwhile remembering that in the AAA&#x2F;AA space that Unity was “a meme” for far longer than it should have been. I remember first seeing it mentoring student teams in Dare to be Digital whilst working on an AAA title and thinking WTF is that!?I’d also note that the ‘commercial games industry’ is much broader than AAA&#x2F;AA, includes indie game developers and ships games making looooots of money on all sorts of weird tech stacks. reply alex_lav 6 hours agorootparenttbf Unity still is a meme in terms of AAA games. reply skelpmargyar 5 hours agorootparentThat&#x27;s not true. Wild Rift, Hearthstone, Genshin Impact, and more are made in Unity. Asset flip and low quality games are a meme, which used to be synonymous with Unity games, but the few AAA games made in Unity have been wildly successful. reply olig15 2 hours agorootparentThat’s exactly the point. If you can list the number of ‘AAA’ games on one hand, then the meme holds true. I work on AAA games on in-house engines, and I’m yet to meet someone at work that would consider using Unity for anything other than a mobile, or small ‘indie’ game. reply bluefirebrand 8 hours agorootparentprev> To put this very directly, Godot is a meme in the commercial games industry. Most of companies with above AA cash would rather write their own engines than use GodotI find this an odd statement because from my perspective there aren&#x27;t any AAA titles written in Unity either. They are all proprietary engines or Unreal nowadays.So the question isn&#x27;t if Godot can take over the AA, AAA games, it&#x27;s can it take over the market currently served by Unity.It probably can&#x27;t right now, but probably not as far off as it seems either> There are a lot of forces working agains Godot: inertia, economics, instability, lack of talent, lack of appeal to talent.Other than \"instability\" none of this is a criticism of the engine itself... so why is it a meme?Edit: I&#x27;m actually really struggling to think of a game I would consider AAA that I know of made in Unity. Please help me out if you know of some I&#x27;m not aware of. reply ammar2 7 hours agorootparent> I find this an odd statement because from my perspective there aren&#x27;t any AAA titles written in Unity either. They are all proprietary engines or Unreal nowadays.I dunno what your definition of AAA is but Cities: Skyline, Hearthstone, Genshin Impact? reply pdpi 6 hours agorootparentI’d say that AAA are the rough equivalent of major Hollywood productions. Looking at Wikipedia for budgets and box office for a few high profile films:Everything Everywhere All At Once had a budget in the $14M to $25M range, for a $141M box office. The Batman made $771M from a $185M-$200M budget, and Multiverse of Madness made $959M off a $294M budget.EEAAO is a AA, maybe an A film where Batman and MoM are AAA, you couldn’t even produce The Batman or MoM with all the revenue from EEAAO. Nonetheless, EEAAO was by far the most successful of the three, both in terms of critical acclaim and in terms of return on investment.Likewise, Hearthstone is definitely not a AAA game. It’s deliberately a much smaller product than Blizzard’s other games. Cities Skyline is the product of a tiny 30ish person company, published by a publisher famous for lower profile, lower production value games.Paradox as a publisher is about the same tier as A24: Very successful in their respective niches, but not their industries’ 800-pound gorillas. reply ammar2 4 hours agorootparentYeah those first two are AAA publishers rather than games.Genshin impact though is definitely in the AAA category, it pulls in billions and had an initial budget (they put out constant updates) of $100M replykevingadd 7 hours agorootparentprevUnity is used more widely than you might think, though maybe it&#x27;s just your definition of AAA here.Mihoyo&#x27;s titles all use Unity AFAIK and have massive AAA-level budgets along with massive AAA-level revenues. Most of the big publishers have teams using Unity, you can kind of pick and choose whether those individual titles are \"AAA\" or not but like, Blizzard ships Unity games, etc. It&#x27;s become a go-to choice for stuff that you want to cheaply ship across many platforms and have run on low spec devices. reply TylerE 5 hours agorootparentSort of telling then, that when they did their virtual actor software, they did it in unreal and not unity. reply vvanders 8 hours agorootparentprevI hope Godot&#x2F;Bevy do well but there&#x27;s a kernel of truth there as well. It was not uncommon for us to make significant changes to licensed engines to meet the needs of the game style&#x2F;design. UE3 for instance was pretty awful for open world titles(gears was quite a rails shooter and the engine reflected that), there&#x27;s a number of titles that took the renderer and re-did many of the major game systems.Once you make changes like that upleveling is a serious challenge. We had one dev who&#x27;s responsibility was to pull latest, spend about a month getting to to compile, another 1-2 months fixing all the issues, checking it in only to them spend another 2 months fixing all the cases that weren&#x27;t tested. With a ~6mo uplevel cadence it was just enough time to finish it in order to start the next uplevel. Once you&#x27;ve gone through that internal engines start to look appealing(although they have their own pitfalls, I.E. heavy dependency on MAX&#x2F;Maya for editor support and other \"fun\" bits). reply alex_lav 6 hours agorootparentprev> To put this very directly, Godot is a meme in the commercial games industry.Godot might be a meme in terms of AAA games, but \"Commercial games\" is a lot more than AAA, both by revenue and just total games developed.> Most of companies with above AA cash would rather write their own engines than use GodotSee previous statement. When did we start talking at \"above AA\" companies? Also most companies with \"above AA\" money aren&#x27;t using Unity.> So even in indies, it’s rare to see Godot.This has more to do with age though. You see less people driving 2024 cars than 2020 cars.> There are a lot of forces working agains Godot: inertia, economics, instability, lack of talent, lack of appeal to talent. No marketplace actually is the least of its concerns as we don’t use marketplace stuff much in AAA&#x2F;AA. The engine makes a lot more sense on paper than in reality.Yeah again, it really feels like your perspective on game development is purely from the AAA BigCo game dev side. Which is a real perspective, but is certainly not even close to all encompassing with regard to game development. reply ffhhttt 5 hours agorootparent> Also most companies with \"above AA\" money aren&#x27;t using Unity.That not so obvious. Ad infested shovelware does make a lot of money. reply omoikane 7 hours agoparentprev> Unity asset storeWould be nice if Godot can access Unity assets. Someone posted a demo of a Unity loader few months back:https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;godot&#x2F;comments&#x2F;13io0bx&#x2F;load_unitypa...The sentiment of that thread appears to be that it&#x27;s technically feasible and would be quite useful, but the legal issues are unclear. reply ncr100 6 hours agorootparenthttps:&#x2F;&#x2F;github.com&#x2F;barcoderdev&#x2F;unitypackage_godotImports *.unitypackage files into Godot, supporting a smattering of Unity asset-types. reply runevault 8 hours agoparentprevWorth noting people running Godot have mentioned the Asset store priority is climbing. I didn&#x27;t save the tweet when I saw it but they are aware that is a huge area that would help people transition off of Unity. reply codelord 6 hours agoprevIMO Unreal Engine is the best deal available and fits >90% of use cases for game developers. Unless you are building something for the web or low powered mobile VR I wouldn&#x27;t even consider anything else. For PC and console games UE5 provides incredible amount of tools and flexibility. It&#x27;s also great for building 2D&#x2F;3D mobile games. People who say complexity of Unreal has stopped them from using it have gotten it wrong. UE5 provides you with a lot of tools, you don&#x27;t have to use them all. But if you are thinking of building something more complex than a hello world example, you&#x27;d realize that the additional tools that UE5 provides you greatly save your time.If you are a total beginner you can use Blueprints to write the game logic and use the existing out-of-the box tools. If you are a more experienced programmer you can use C++ to build custom components&#x2F;plugins to get more customization.I remember a time that game engines were these precious secret tools that you had to pay millions of dollars to get a license for. Now you can get the full source of UE5 on Github for free. And you pay something like 5% after 1 million dollars of revenue. This is just a no-brainer folks. IMO 5% is totally deserved and justified. In fact it&#x27;s a bargain and you save money by paying Epic 5% compared to anything else out there. Use UE5 unless you have a really really really good reason not to. reply alex_lav 6 hours agoparent> IMO Unreal Engine is the best deal available and fits >90% of use cases for game developers.The only place Unity really shines (both IME and in industry) is mobile games. Unreal IIRC doesn&#x27;t really have even close to comparable support for mobile platforms. Mobile games by revenue make up more than 10% of the games industry, so I would say the \">90% of use cases\" thing is just untrue.> And you pay something like 5% after 1 million dollars of revenue.> IMO 5% is totally deserved and justified.Again, for mobile games, that&#x27;s 5% after 1m in revenue, which also includes Apple and Google&#x27;s 30% cut. So again, no, it&#x27;s really not a good deal _at all_.In retrospect this comment feels like some form of advertisement for UE5 more than actual discussion. reply p1necone 6 hours agorootparent> Mobile games by revenue make up more than 10% of the games industryI&#x27;ve said this already in another recent thread but mobile games and pc&#x2F;console games are two entirely separate markets, with different potential customer pools.Conflating them together makes about as much sense as conflating console&#x2F;pc games with accounting software. reply alex_lav 6 hours agorootparent> I&#x27;ve said this already in another recent thread but Mobile games and PC&#x2F;Console games are two entirely separate markets, with different potential customer pools.Yes, agreed. But again, as I&#x27;ve already said, the reason to point out mobile games specifically is because that&#x27;s where much of Unity&#x27;s success has come from. There&#x27;s really no point in discussing Unity&#x27;s monetization efforts without also discussing the mobile games industry. PC and Console games just aren&#x27;t written in Unity at the frequency or scale that it would matter.> Conflating them together makes about as much sense as conflating console&#x2F;pc games with accounting software.Yes, which is why I pointed out the above post combining them when the ought not be. reply p1necone 6 hours agorootparentSorry, you managed to trigger my pet peeve faster than I managed to properly read the whole conversation :) reply johnnyanmac 3 hours agorootparentprev>Again, for mobile games, that&#x27;s 5% after 1m in revenue, which also includes Apple and Google&#x27;s 30% cut. So again, no, it&#x27;s really not a good deal _at all_.Not necessarily, let&#x27;s do the math:for 1m downloads, UE depends on how much money you make. But regardless of how much money you made, Unity&#x27;s new plan on Enterprise (in the worst case, because they have not specified if the charges start after 1m installs or applies to the first million as well) will cost you $46,500, on top of the per seat pricing of enterprise. in this case, the cutoff point for when Epic costs more is if you made more than $930k in revenue. But since Epic waives the first million, this actually means you need to make $1.93m in revenue before it cancels out.and if we go further along (where Unity&#x27;s prices for enterprise start to stabilize at $0.01 per install), if you hit 5m downloads you are charged a total of $106k, the breakpoint here for UE is if you made $3.1m in revenue (again, waiving the first million).----By the looks of things, for a mid-revenue game UE looks better, but higher revenues mean Unity start to win out. In particular, mobile games tend to utilize whales that can make the attach rate MUCH higher than $2&#x2F;user (you may not have 99 users paying anything, but a whale dropping $1000 balances the arithmetic mean to $10&#x2F;user), so Unity will win out. Funnily enough, the less ethical f2p games may still prefer Unity over unreal.For games that rely on ads or subscriptions, though? Absolutely fucked over. Drastically. You simply cannot make an ethical mobile app with Unity anymore as every user that visits and leaves in 10 minutes after a certain threshold is costing you money. If you had a bad launch with lots of users but barely any revenue, you can legitimately end up in the red for using Unity. As it would be better to shut down your app and relaunch under a different name than to try and recoup the costs with the current app.----I won&#x27;t ramble on too much longer, but I do want to add one more tidbit to keep in mind. Gamepass and Apple Arcade are also factors, and Unity said they would charge the distributors for this. In the worst case, this can mean that Microsoft&#x2F;Apple can remove your existing games from these services and disallow Unity games to be hosted. So if you want to one day utilize these kinds of subscription services, you may not even have such a choice to begin with. reply codelord 6 hours agorootparentprev30% for distribution is fine, 5% for more than half the cost of development is not?Fortnite is also a top selling mobile game developed with UE5. You can make great mobile games with UE5 now. Mobile hardware right now is comparable to last gen consoles. reply ffhhttt 5 hours agorootparent> 5% for more than half the cost of development is not?The same can be said about Unity asking you to pay $0.05 per install (which is massively cheaper than Unreal for any non F2P game) reply TylerE 5 hours agorootparentIt isn&#x27;t though, in the context of a f2p mobile game. Under one model, only paying customers cost you. In the other you&#x27;re paying for every drive by (re)download. reply alex_lav 6 hours agorootparentprev> 30% for distribution is fine, 5% for more than half the cost of development is not?Quote me where I said the 30% tax was \"fine\" please.> Fortnite is also a top selling mobile game developed with UE5.Yep! So we&#x27;ve gotten to the exception that proves the rule. Aside from Fortnite, which is written by Epic, Unreal Engine hasn&#x27;t had even close to the success or adoption on mobile platforms as it has elsewhere.> You can make great mobile games with UE5 now.You can make great mobile games in javascript. This isn&#x27;t really about \"can\". reply johnnyanmac 3 hours agorootparent>Unreal Engine hasn&#x27;t had even close to the success or adoption on mobile platforms as it has elsewhere.not sure if that&#x27;s a fair comparison. UE existed for 15 years before smartphone applications existed. Unity&#x27;s first public release was mac only and focused a lot on making web and IOS apps. No surprise that Epic&#x27;s decade long dynasty wasn&#x27;t surpassed when they never put a strong emphasis on mobile to begin with.IIRC Unreal Engine has 15% mobile marketshare, so it&#x27;s not an unviable option. Especially in times where even mobile games are starting to come into the open world action frenzy. reply ffhhttt 6 hours agoparentprev> IMO 5% is totally deserved and justified. In fact it&#x27;s a bargain and you save money by paying Epic 5% compared to anything else out there.How? That’s significantly more expensive that Unity if you make more than $2 per user or so. reply raytopia 8 hours agoprevI know a lot of people recommend Godot because it&#x27;s super good but if you want a more code oriented and batteries included engine I recommend Panda3D [0] it&#x27;s open source, super mature (it&#x27;s actually one of the oldest continously developed game engines), and can be used form Python and C++. Not sure why it&#x27;s not more popular it&#x27;s flexible and super fun to use.[0] https:&#x2F;&#x2F;www.panda3d.org&#x2F; reply p1necone 9 hours agoprevI feel like any piece of software that&#x27;s widely applicable to a whole industry of users (3d modelling, image editing, game engines, etc) is destined to eventually solidify on an fully production quality open source solution that everyone contributes to.It&#x27;s already happened for 3d modelling (Blender) and digital painting (Krita), and it feels like gamedev is an even better market for this to happen in than those because all the users are also going to be capable of contributing to development of the engine.From the outside Godot looks like it&#x27;s the closest to ready for building proper AAA quality 3d games.I&#x27;ve also actually used Bevy and if you&#x27;re a fan of rust and ECS based development it&#x27;s really nice for building procedurally generated stuff that doesn&#x27;t need a level editor, or for building 2d games and relying on a 3rd party editor like Tiled or LDtk. However there&#x27;s plenty of places where it&#x27;s still a bit rough around the edges, like dealing with more complex multi stage asset loading pipelines. reply blackoil 8 hours agoparentYou give couple of examples, but we have counter examples. Office, Windows, Photoshop are monopoly for more than 2 decades. reply johnnyanmac 3 hours agorootparentWindows has Linux and Photoshop has Krita. the open source equivalents aren&#x27;t neck and neck competitors, but they grew to a point of influence where you can&#x27;t ignore them. They have sizeable communites, and various professional usage.Office is the rough one, though. There&#x27;s LibreOffice, but it never seemed to improve on its UX. and in the meantime Google docs came and became the big competitor. reply TheRoque 8 hours agoparentprevI don&#x27;t think it&#x27;s true, Blender happened to work and is now used by professionals, but some tools like PhotoShop or Adobe Premiere, Ableton, or even Word&#x2F;Excel for that matter, don&#x27;t have an equivalent that could replace them anytime soon. reply chii 8 hours agorootparentThe 3D modelling industry is bifurcated. The \"low-end\" of town (like hobbyists or indie studios) uses blender, and the high-end of town (like the sfx shops that do hollywood movies) uses things like maya. reply miniupuchaty 5 hours agorootparentIn games Blender is used at all levels. From indie to AAA. At most studios as a 3d artist you are able to use any package you want and for newer artists Blender is increasingly what they know. Sometimes there are custom toolchains that require Max or Maya but often then artist still use Blender for most of the work, export fbx and import that into another soft just for final steps.I&#x27;m now at AA level studio and for this category Blender is eating competition super fast! reply chii 3 hours agorootparentfair enough. Blender&#x27;s penetration in game development has been very high in the past decade. There&#x27;s even a nice game engine called Armory3D which is in fact a blender \"plugin\" (it&#x27;s quite a good engine i hear - never really used it properly in anger to know fore sure tho). reply spoiler 3 hours agorootparentprevThis sentiment&#x2F;state you describe used to be true in the past, but it&#x27;s no longer true.Blender has really come a long long way! reply spoiler 3 hours agorootparentprevPhotoshop is an interesting one. I used to be a power user for years, but have since mostly moved to Krita. For a \"I don&#x27;t know what I want or need\" type of manipulating images, Photoshop is great. For painting, it&#x27;s not even close to being the best (I agree with the parent comment that Krita is much better).The reason it&#x27;s so popular is because there wasn&#x27;t anything else for a long time, and also probably because it&#x27;s still lobbied to unis. reply ninepoints 9 hours agoparentprevAAA quality? No it&#x27;s not there, but it&#x27;s certainly been improving on a number of fronts. reply esrauch 8 hours agorootparentThere&#x27;s hardly any AAA titles that use Unity, the most well known ones are games like Hearthstone, Cuphead, Beat Saber. These are great games but they&#x27;re not exactly GTA 6 or Call if Duty. reply ninepoints 10 minutes agorootparentAs someone in the industry, those games don&#x27;t actually fit my traditional mental model of what \"AAA\" means. AAA doesn&#x27;t directly correlate with revenue for example. The general rule of thumb I kind of go by is that a game is likely to be \"AAA\" if it uses motion captured skeletal and facial animation sequences. reply johnnyanmac 3 hours agorootparentprevYeah it&#x27;s a historical difference, and Unity literally kicked out its best chance to compete in that space with DOTS.But on mobile it is king, and mobile is starting to get games that can rival AAA studios in presentation. Genshin Impact and the rest of Hoyoverse&#x27;s games are all in Unity, and while not confirmed I wouldn&#x27;t be surprised if Project Mugen is also running Unity (given that most of NetEase&#x27;s portfolio relies on it). So it has the capability to handle AAA games, even if no one in the west is utilizing it. reply p1necone 8 hours agorootparentprevYeah I&#x27;m not saying Godot looks AAA ready, just that it&#x27;s much closer than every other option.Although I would love to see how far a code only engine core + separate third party level editor tooling model could go, it appeals to my intuitive feel of what the \"right\" way to build software is. reply jayd16 7 hours agoparentprevBy solidify do you just mean \"a feasible option\"? Maya is still very popular if not moreso than blender. reply fbdab103 7 hours agoprevSelfishly, this will be a minor win for me. I occasionally like to try indie games from itch. For safety reasons, these are obviously run inside a VM. Unity games have always run like garbage on this setup, so the more developers migrating off the platform, the better for me. reply TylerE 5 hours agoparentUnity games run like garbage native, too! reply olig15 2 hours agorootparentI was once pulled on to a small unity mobile game that was being released by a small studio in the iPhone 4 days to help fix performance issues. After looking at the frame capture, I could see that the entire scene was being rendered about 5 times per frame from different angles. Turns out the team had a bunch of cameras enabled, but because they only saw the rendering from the last camera, they just edited that one.I sometimes think Unity makes making games TOO easy, and you end up with people that have no technical knowledge of the way games work so they can’t fix&#x2F;diagnose the most basic things. Obviously the game runs slower when you render the scene 5 times, but they didn’t even think to open a frame capture tool.The fix for the issue that plagued them for months, was to uncheck the ‘enabled’ box for these unused cameras. Probably took 30mins to diagnose and fix. reply mjan22640 37 minutes agoprevI guess the motivation behind the move was to monetize large titles. Killing off their nursery is an unintended side effect. reply dham 8 hours agoprevI&#x27;ve been enjoying Construct 3. Not the visual scripting but the Javascript portion. It&#x27;s a really good Javascript engine at its core. For some reason people don&#x27;t know this? DragonRuby has also been pretty fun and hot reload is nice. Godot is the main answer for Unity but web support is just better with Construct, obviously since it&#x27;s built on web technologies.My main issue with Unity and why I never picked it up is, I have poor vision and the editor doesn&#x27;t scale on Mac. You basically have to have perfect vision to see anything that&#x27;s going on. A scalable editor is a must. Godot &#x2F; Construct fit these. Game Maker looks so poor on 4k that it hurts my eyes. reply vunderba 4 hours agoparentConstruct 3 moved away to a subscription model (I was a huge fan of C2 back in the day) and annual fee. I&#x27;d recommend checking out GDevelop, an open source 2d visual game engine which also exports for HTML5&#x2F;mobile, and is completely free.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GDevelop reply readyplayernull 7 hours agoprev10 years ago everyone believed Blender would become the new Unity, but it didn&#x27;t, it&#x27;s too difficult to make games with Blender, no one even mentions the idea or recommends it anymore, although its 3D editor is highly used and improved during the years. For Godot to become the favorite indie game engine they must focus on making its workflow straightfoward and simple. On the tech aspect Godot seems like Unity 5. reply andolanra 7 hours agoparentNobody mentions the idea or recommends it anymore because the game engine was removed from Blender entirely back in 2019, in favor of purpose-build game engines like Godot: https:&#x2F;&#x2F;wiki.blender.org&#x2F;wiki&#x2F;Reference&#x2F;Release_Notes&#x2F;2.80&#x2F;R... reply readyplayernull 6 hours agorootparentSure, but at the time they showed real interest in becoming a mainstream game engine, showed some cool advanced projects, and had a few years to try out, why did the give up if there was hype? reply johnnyanmac 3 hours agorootparent>why did the give up if there was hype?If I had to take a guess without any proper research:1) Godot didn&#x27;t want to chase two rabbits at once. Making an engine is hard, and making a modeling program is hard. If they split attention between the two, they may have ended up with a half-assed engine&#x2F;modeling program instead of a professional grade modeling program and an abandoned engine.2) much simpler, but the game engine team fell out. So there was no one to properly maintain the Blender Game Engine. No malicious reason: it&#x27;s an open source initiative and this would be 5+ years before Godot got major funding. They may have changed life perspectives, may have gotten poached, or simply lost interest (and ofc there&#x27;s other office politic conspiracy theories)but these are 2 of many reasons, and I&#x27;m sure if you tracked down the right person in Blender you can simply ask for the real reason (and then maybe make close friends for the real gossip lol). reply andolanra 6 hours agorootparentprevI&#x27;m not sure what \"cool advanced projects\" you&#x27;re thinking of, but to my recollection, there was very little in the way of finished work created with the original Blender Game Engine. There were two game efforts associated with the Open Movie Projects: Yo Frankie! (associated with Big Buck Bunny) and Sintel The Game (associated with Sintel.) The former had something like a proper release, but the latter only ever got a few alpha releases before being abandoned: neither was terribly impressive except inasmuch as a demo of the technology, and to my recollection, neither inspired much hype. (The Wikipedia page for Yo Frankie! says dryly, \"The game was noted by the gaming press,\" without any further elaboration, which does not speak to massive amounts of excitement or hype.)Since then, aside from some interesting bits of work that didn&#x27;t actually make shipping games easier, the engine really languished. My understanding is that a big part of the reason the engine was removed was that it made work on other parts of Blender more difficult, and that dispensing with the (little-used) engine in favor of the (increasingly popular) modeling tool was clearly a net benefit to being able to develop Blender, especially since the mantle of \"open source game engine\" had been taken up by other competent projects like Godot.So yes, they may have at some point aspired to becoming a \"mainstream game engine\", but there&#x27;s more to that effort than just aspirations and demos. (And at the same time, the fact that the BGE faltered doesn&#x27;t mean that other open source game engine efforts would necessarily face the same fate.)It&#x27;s probably also worth saying that people have forked the game engine and you can still use it: the forked version is called UPBGE, and there are people out there trying to make it work. By and large, though, the Blender project seems to point people at projects like Godot, with the idea that a focused game engine is probably better suited to modern games than something that strapped a game engine onto a piece of modeling software. reply aaomidi 7 hours agoparentprev10 years ago Unity wasn’t trying to shoot every developer that uses them in every organ that matters.That’s changed. Unity won’t be able to last this because it’s impossible to make the economics of it work. So even if the alternatives aren’t as good, people are going to have to use it. It’s not really optional. reply johnnyanmac 3 hours agorootparentThis is true, but it&#x27;s not like Godot is the \"best alternative\". for a medium-large sized team, it&#x27;s only the best alternative if you have overly rigid reasons to not use Unreal Engine or your 3d game (if you have a 2D game, you are probably better off using GameMaker. Or perhaps going back to Monogame). reply jerojero 9 hours agoprevI feel like, realistically, for a lot of the people that feel might be affected by this change they could probably do their games in an engine like Godot without much trouble.Though as it says in the article, Godot does lack proper support for consoles. reply pritambaral 1 hour agoparent> Godot does lack proper support for consoles.Sonic Colors: Ultimate for the Nintendo Switch was made in Godot[1].From what I read elsewhere, console support code is usually protected by NDA, so the Godot devs setup a commercial entity to support consoles[2].1: https:&#x2F;&#x2F;www.destructoid.com&#x2F;first-sonic-colors-switch-patch-...2: https:&#x2F;&#x2F;w4games.com&#x2F;2023&#x2F;02&#x2F;28&#x2F;godot-support-for-consoles-is... reply nodja 9 hours agoparentprev> Though as it says in the article, Godot does lack proper support for consoles.None of the open source engines do, or ever will as publishing to consoles requires that you sign an NDA. If you&#x27;re a godot dev and want to publish on console look towards https:&#x2F;&#x2F;lonewolftechnology.com&#x2F; reply gabereiser 9 hours agoparentprevRewriting shaders, MonoBehaviors, animations, triggers, levels, and systems? Yeah, not too much trouble. reply jayd16 7 hours agorootparentI think they mean for future games.Ports will be pretty brutal unless the outrage remains high and someone makes some kind of porting toolkit. reply johnnyanmac 3 hours agorootparentinteresting, but I can&#x27;t imagine someone battle-testing a proper unity scene porter that would be faster than simply re-importing your assets (easy?), re-writing game scripts (easy-ish, since Godot has c# support. but annoying and time consuming), and re-doing your shaders&#x2F;materials (basically a complete re-write, unfortunately). reply cridenour 8 hours agoparentprevNot for long on the console front - though costs have yet to be determined.https:&#x2F;&#x2F;w4games.com&#x2F;2023&#x2F;08&#x2F;06&#x2F;w4-games-unveils-w4-consoles-... reply hiccuphippo 9 hours agoparentprevIs the lack of console support a technical issue, it hasn&#x27;t been added yet; or a political one, no permission to add support on open source software? reply johnnyanmac 3 hours agorootparentmostly Political. Console dev kits from the Big Three are all under NDA, and you need to apply to get them. So we&#x27;re talking about access to closed sourced code with use in an open source engine.Godot created W4 Games to get around this. So while Godot is open source, W4 can work on a fork that implements console-specific code that cannot exist on an open source repo. reply ensignavenger 8 hours agorootparentprevThere are commercial companies offering Godot-based games a path to consoles, so it isn&#x27;t technical at all. It is entirely a matter of licensing with the console makers not wanting to allow open source support. But they don&#x27;t mind 3rd parties developing proprietary extensions to support consoles and following their licensing restrictions. reply fnordpiglet 7 hours agorootparentAny references?Edit:This is the only one I could find, but looks more like a contract to port:https:&#x2F;&#x2F;lonewolftechnology.com reply ensignavenger 6 hours agorootparentAlso mentioned in another sibling comment, coming soon https:&#x2F;&#x2F;w4games.com&#x2F;2023&#x2F;08&#x2F;06&#x2F;w4-games-unveils-w4-consoles-..., W4 is a company founded by Godot founders. replyMacha 9 hours agoprevIt&#x27;s interesting that Source doesn&#x27;t get a mention here, when it used to be a rather popular engine. Given unreal is second, it&#x27;s clearly not an objection to closed source engines reply hueho 9 hours agoparentSource is not easily licensable. reply thisisonthetest 8 hours agoparentprevActually Source was never a popular engine among developers. If you exclude valve we’re talking dozens maybe? And when 250+ games are released on every week on Steam it’s a very rarely used engine reply mastazi 9 hours agoparentprevI follow indie games news and I don&#x27;t remember seeing any recent indie release using Source. My guess is that it&#x27;s no longer as popular as it used to be. reply Freedom2 7 hours agoparentprevWhen has Source ever been \"rather popular\"? reply teirce 6 hours agorootparentThe only time period I can even think of that _might, maybe_ qualify would be the Source mod era, but that came and went over a decade ago. And of course they were free mods, not standalone games with purchases or licenses.Some of these mods turned into games later (Chivalry, Black Mesa, Insurgency come to mind) but without investigating I&#x27;m not sure they even use Source anymore. reply PrivateButts 5 hours agorootparentI agree that it&#x27;s barely used in this day and age but worth noting that Titanfall 1, 2, and Apex Legends are all heavily modified Source reply Freedom2 5 hours agorootparentDefinitely worth noting, but +3 games does not make an engine \"rather popular\". replyjmugan 5 hours agoprevAnybody know of something good and simple in Python? I&#x27;m more interested in agent-based machine learning in 3d simulation than making games. I&#x27;m familiar with Ursina on top of Panda3d. Is there anything new on the horizon? reply az09mugen 5 hours agoparentI don&#x27;t know if it will fit your needs, but take a look at godot, which has gdscript : https:&#x2F;&#x2F;docs.godotengine.org&#x2F;en&#x2F;stable&#x2F;tutorials&#x2F;scripting&#x2F;g... reply johnnyanmac 3 hours agoparentprevPanda3D is probably the best proper game engine in Python, unfortunately. Python as an interpreted language tends to have large performance costs for games, and python hasn&#x27;t been too popular as a scripting language buit on top of engine code (studios love to use python for tools, though).I&#x27;d second taking a look at Godot, since its GDScript is meant to closely mimic Python. reply pengaru 8 hours agoprevThis feels like blogspam trying to bring traffic to computerenhance.com.Casey has never shipped a video game of his own. It&#x27;s not just an \"I haven&#x27;t used off-the-shelf engines\" situation, he hasn&#x27;t developed a game start to finish with any engine.AFAIK Handmade Hero is the furthest he&#x27;s ever gone in shipping a game, which is a far cry from finished. He&#x27;s not really qualified to speak to the matter of which engines are best for delivering and supporting polished, fun, finished video games.Please correct me if I&#x27;m wrong; what titles has he put out there with his own engine? reply quchen 9 hours agoprevhttps:&#x2F;&#x2F;12ft.io&#x2F;proxy?q=https%3A%2F%2Fwww.computerenhance.co... reply empath75 8 hours agoprevGames development is very hit based and most games make nothing. You might think, \"oh if most games make nothing, then thos won&#x27;t make an impact\", but if you&#x27;re removing the profit from the relatively few hits there are, it wrecks the incentives to take the risk of developing a game to begin with. If Unity wants a piece of the profits, they need to be investing. reply ldoughty 9 hours agoprevI&#x27;m not really into this industry, but I&#x27;ve dabbled with Unity over the last month for some fun&#x2F;expanding knowledge...I&#x27;ve seen 2 articles in the last day about this topic with clickbait misinformation from sites I would expect to be more accurate... Including talking about how a \"free game\" was going to cost a developer millions... and how \"independent developers\" will be bankrupt...But how? The fee page and FAQ [1] state \"will apply to games made with Unity Personal and Unity Plus that have made $200,000 USD or more in the last 12 months\".. If you are distributing a free game, by definition, you made no revenue for the game and you pay nothing to Unity. Key point: the metric is PER game... so it&#x27;s not like your $5 game with 200k revenue and 200k installs will trigger your separate FREE game to cost 5 million in damages.If you are an \"independent\" developer, or even a team of 5, then the $2k&#x2F;seat-year Pro license means the threshold is now $1million... pay 2-10k&#x2F;year upfront to pocket an extra 800k? These seem like a no-brainer...Now, I can see this being an issue for studios that invested heavily in Unity with dozens or hundreds of developers.. and I appreciate discussion on this topic... I&#x27;d love to see more realistic reactions and discussions... and less \"abandon ship\" (like this article) and I-didn&#x27;t-read-the-memo knee-jerk reaction stories for attention out there right now.(And I know some of the early concern was download-bombing, but they have addressed that, and most of the articles I read didn&#x27;t cover that explicitly)[1] https:&#x2F;&#x2F;unity.com&#x2F;pricing-updates reply marcus_holmes 9 hours agoparentI think the concern is that Unity is failing. Charging royalties is not going to save them, and is going to drive more devs away from the platform (as this article shows), so they are going to have to do more drastic things to raise revenue, which will drive more devs away, and so on into the familiar death-spiral. If they can&#x27;t cut costs enough to get profitable (and history shows this is very difficult) then they&#x27;re doomed to go through a series of increasingly more disastrous acquisitions (see Yahoo for details).Given this, then the details of this particular royalty structure don&#x27;t matter. Yes, a freebie game may not have to pay any royalties now. But as Unity enters the death-spiral that will probably change. Or there will be mandatory malware shipped with the engine. Or some other craptastic revenue-generating idea will mess with the game. We can expect relatively frequent, drastic changes to Unity&#x27;s licensing as it tries to escape its fate. reply buzer 8 hours agoparentprev> And I know some of the early concern was download-bombing, but they have addressed that, and most of the articles I read didn&#x27;t cover that explicitlyThey addressed that in FAQ update. They have also previously committed to not retroactively change ToS for the version you downloaded (https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;Unity3D&#x2F;comments&#x2F;16hjic6&#x2F;unitys_sta...) but they changed their mind.Given that history I would not put much trust in one vague answer in FAQ without at least consulting lawyer first (including asking how much it would cost to defend if Unity lawyers disagree with your lawyer&#x27;s take).And even in the current FAQ answer it&#x27;s essentially fully up to them to determine what is fraudulent installation or if the install was part of install bombing. reply ldoughty 8 hours agorootparentI agree with all those points... It certainly concerns me as a new-to-unity developer... but most articles (like this one) are not talking about fraudulent installs.. they jump to \"we&#x27;re bankrupt\" and \"abandon ship\" without much details, context, or supporting statements... and that&#x27;s what bothers me...I&#x27;d love to hear WHY.. not knee-jerk reactions...Someone else mentioned that some people use \"Free\" to also mean ad-supported or micro-transaction... That wouldn&#x27;t be my definition, but I suppose that is a fair concern at-a-glance for an article that says a \"free\" game can bankrupt them... That&#x27;s a lot better than most of the comments I&#x27;ve seen on this topic. reply jimmydorry 8 hours agoparentprevPlenty of games are free to play, but come with micro-transactions and&#x2F;or ad revenue. If anyone can pick up your game and play without spending money, it is generally accepted to be a free-to-play game.Part of the change (unless it has since been walked back) was getting rid of Unity Plus licensing, leaving just Unity Personal and Unity Pro.Also assuming the per-game stipulation was not something Unity made clear after out-cry (like the multi-installs on the same PC or game-update), they have previously gone to great lengths to prevent studios from spinning up a new company per game. It is reasonable to assume that there is nothing stopping them from later tightening the noose and applying this fee on a company basis instead of a game basis. reply TylerE 9 hours agoparentprev\"Free\" games don&#x27;t really exist any more on the mainstream platforms. \"Ad supported\" games though... and guess what? Unity is an ad company, not a tech company, at heart. reply qwytw 5 hours agorootparent> Unity is an ad company, not a tech companyI&#x27;m not sure that&#x27;s true, their Ads business(es) is made up of other companies which they acquired over the years and is&#x2F;was somewhat isolated from the engine&#x2F;tech part of the company. reply kevingadd 7 hours agoparentprevIf you have a free game with upsells or paid DLC - not an uncommon model now as a way to get people in the door - the new pricing model is toxic because every user in the door costs you money. If before you were okay with only 5% of your players giving you money, that&#x27;s not okay anymore - now you have to aggressively upsell, shove ads in there, do ANYTHING you can to get average revenue per user to far above 20 cents so you can afford to pay the Unity Tax in addition to the other taxes you already have to pay (Valve&#x2F;Apple&#x2F;Google&#x27;s 30%, your actual taxes, etc.)With the model I described you could easily bring in 200k worth of revenue off DLC but then owe a significant % of that in install fees if you&#x27;re only managing to sell DLC to say 10% of your players.Unreal-style revshare where you just give them a small % is super reasonable in comparison. It just works and you decide whether the percentage is acceptable. reply wwtrv 5 hours agorootparent> Unreal-style revshare where you just give them a small % is super reasonable in comparisonThe per install fee is still going to be significantly lower for everyone making > $2 per user than what Unreal charges. And I assume Unity&#x27;s best paying customers are doing that. A rev share model would have been much more likely to scare them off. It almost feels that killing the low revenue per user F2P developer user-base might have been a conscious calculated choice (I&#x27;m probably giving too much credit to their upper management though). reply mjan22640 7 minutes agorootparentSo the royalties are going to be negligible for large succesfull projects, while potentially an unbreakable barrier for small projects (ie growing free installs first and monetizing later is not viable). This is going to harm their ecosystem, from which the large succesfull projects grow. Why dont they use same principle license as Unreal? reply Dban1 7 hours agoprevGodot reply moth-fuzz 6 hours agoprev [–] I by and large loathe how commonplace big do-it-all game engines have become in indie game development, with unity at the forefront of this movement. Even if everybody and their mom used the same awesome product, I&#x27;d still be upset because because of the market stranglehold that eventually creates - Unity in this case is worse because it isn&#x27;t even an awesome product. It&#x27;s a mediocre product with an actively hostile business strategy.I admit this is entirely emotional, but when I learned that Hollow Knight[0] was made in Unity, it broke my heart. A 2D game with a consistent art style (read: write the shaders once and forget it), made up entirely of flat surfaces with only a handful of different methods of movement, no physics to speak of, and only a couple hundred different types of enemies, most with large overlaps in AI save for bosses. Gorgeous game, strong art direction, thoughtful lore and story, but any game developer could probably write the engine for such a game in a couple weeks.But every indie developer I&#x27;ve talked to about game engine development acts like it&#x27;s a dark art. That it&#x27;s just impossible for mere mortals to do such a thing, and if you do, then you&#x27;ll never ever release a game, or you&#x27;ll spend literal years on the engine. Again, I predict a couple weeks.Back to the article, I dislike that &#x27;game development post-unity&#x27; just means &#x27;picking out a new engine&#x27;. Everybody&#x27;s jumping ship to Godot or Unreal or whatever else because we all need a game engine. But why? Why is this song and dance necessary? I feel like since the author is a game engine programmer himself, this option should have come up higher on the list along with the non-engine libraries and frameworks.0. https:&#x2F;&#x2F;www.hollowknight.com&#x2F; reply _gabe_ 5 hours agoparent> Again, I predict a couple weeks.I’ve made similar predictions in the past… these days, I predict that a couple years actually makes a lot more sense. Especially if you’re not familiar with game engines, and especially if you have to figure out: physics, input mapping, window systems, graphics pipelines, audio systems, game logic component systems (basically the game loop), level systems, transitions, platform support (switch, PC, PS5, Xbox, …), level editors, serialization, animation systems, particle systems, and more!I have a big fat book sitting on my bookshelf called Game Engine Architecture by some of the developers at Naughty Dog. That bad boy is 1,000 pages and just gives a high level overview. It does not teach you how to set up a graphics pipeline, open a window or write a shader. I have other big fat 1,000 page books for those. Those do not teach you the mathematics needed to fully grasp linear algebra. I have yet more big fat 1,000 page books for that. Those books do not teach you C++, I don’t believe any book is sufficient for that big fat language, but I have a few anyways.It’s cool if you enjoy tinkering on engine stuff, but boy am I glad that the Hollow Knight devs (I believe there were only 2 people) just used an engine. It’s an amazing game, and I could care less how it was made. I’m just glad that it was made.Edit: I forgot to mention game HUDs, UI systems, font rendering in general, and asset management pipelines to the list of things that come with an engine but you’d have to either code it yourself or just not have it if you don’t want to use an engine. All of these subdomains can have years of effort poured into them (speaking from experience). The rabbit hole never ends haha. reply notamy 6 hours agoparentprev> But why? Why is this song and dance necessary? I feel like since the author is a game engine programmer himself, this option should have come up higher on the list along with the non-engine libraries and frameworks.I’ve done a little bit of this casually; imo it’s because I don’t want to write the engine. I want to be making features for the game, not making a bespoke custom engine and fixing issues with it as well as my game. reply muchwhales 5 hours agoparentprevJust to give some perspective: Without prior graphics programming experience, I was able to write a simplistic 3D engine (VERY simple, but enough for my purposes) in a week or two. I somehow assumed that it was just an impossible thing to do and so I&#x27;ve never tried, until recently. It turned out to be much easier to get something going than I could&#x27;ve imagined.It&#x27;s true that Unity and other engines offer tons of features, but most indie developers probably won&#x27;t need them, and by the time they do need more than the basics they&#x27;ll have so much experience that they can easily implement what&#x27;s missing in their own \"engine\".We&#x27;re also seeing some interesting developments in this space with WebGPU, which is what prompted me to finally give it a try in the first place. I&#x27;ve never used OpenGL before and I was still able to get by (more or less), after failing miserably to complete the Vulkan tutorial...If nothing else, I can only recommend people at least think about whether they really need Unity&#x2F;Unreal and consider that there are disadvantages as well as advantages when using them. reply joeyjojo 2 hours agorootparentAlso, for even things like physics there are many simple ways to do things. Take Towerfall&#x27;s arcade physics for example[1]. In under 100 lines you can create the basis for your platformer game. Sure, it&#x27;s not just pluggable into other pre-existing solutions, ie tile maps, but at least you aren&#x27;t endlessly hacking all over the place to tweak Unity&#x27;s physics into submission to get the controls feeling right for your simple 2d game. There is a massive wealth of gamedev knowledge that doesn&#x27;t seem utilised to its potential because existing engines want sell their pre-canned solutions. I think what the gamedev scene needs are more tools like LDTK[2], with more thought given to how such tools could interop, and a better selection of low level rendering libraries (like Monogame and Raylib) with very robust cross platform support and dead simple build systems (or none at all).I use a very obscure library called Kha[3] and it has by far and away the best performance for 2d rendering that I have encountered. It is amazing what you can do with just a very basic immediate mode ui library called Zui[4]. I think it is shitty advice to say that you are either building an engine or game. This advice would be applicable if you are building a general purpose game engine, not the highly specific and bespoke engine used for you own game. Your game and your engine are basically the same thing and you take many shortcuts, make many compromises, and build out a rough and minimalistic \"editor\" used just by your small teams (or yourself) to get the job done.[1] https:&#x2F;&#x2F;maddymakesgames.com&#x2F;articles&#x2F;celeste_and_towerfall_p... [2] https:&#x2F;&#x2F;ldtk.io&#x2F; [3] https:&#x2F;&#x2F;github.com&#x2F;Kode&#x2F;Kha [4] https:&#x2F;&#x2F;github.com&#x2F;armory3d&#x2F;zui reply meheleventyone 2 hours agorootparentWhen people say to build a game not an engine they mean “don’t copy the patterns of general-purpose game engines and build something specific to your needs”. There’s not game that doesn’t have bits you can point at as “the engine”. Lots of people get stuck “making the engine” to make their game rather than making the specific engine bits they need as they go.That said this was the status quo before Unity and whilst it shipped games the asset pipelines and editors in engines like Unity and Unreal are much more productive for the non-programming portion of the team. As well as providing a shared framework that makes it easier to hire and train developers. Not to mention easier porting and so on.I do think the solution to part of this is an open-source scene editor&#x2F;flexible tool that has some defined interface that any engine can implement like a language server or similar. reply fulltimeloser 5 hours agoparentprev\"But every indie developer I&#x27;ve talked to about game engine development acts like it&#x27;s a dark art. That it&#x27;s just impossible for mere mortals to do such a thing, and if you do, then you&#x27;ll never ever release a game, or you&#x27;ll spend literal years on the engine. Again, I predict a couple weeks.\"It&#x27;s a can of worms to make a game engine for multiple platforms. reply meheleventyone 2 hours agoparentprevYou can definitely stand up the basics of a 2D game engine really quickly… if you’ve done it before. If you haven’t it’s an unexplored problem and is significantly more challenging.But it doesn’t end there because once you’re done standing up the engine you need to improve and maintain it. Then port it to different platforms, you did remember to abstract things nicely when you stood it up right?And that’s without mentioning all the extra time you’ll be putting into higher level systems like localisation and UI. And the combo of those two is fearsome!Plus you have to make a game.Or you can use Unity or another off-the-shelf engine and ignore most of these issues.Which isn’t to say you can’t make an engine but if you are running a business you need a compelling reason why that becomes an advantage. I doubt Hollow Knight would have found it’s success as easily if they had to worry about all of that. reply johnnyanmac 2 hours agoparentprev [–] > Everybody&#x27;s jumping ship to Godot or Unreal or whatever else because we all need a game engine. But why?I&#x27;m assuming we&#x27;re talking about 2D games so I won&#x27;t dwell on the \"because they want to make a 3D game\". See musings below1) not everyone has that technical prowess, they may not even want to code to begin with. I may be able to whip something up in a few weeks for a 2D game, but someone new will simply be learning how to code first and then finagle with libraries they can&#x27;t full piece together. So engines that can heavy lift and let a user do simple scripts (if that) to do their movements is a big demand.2) engines differentiate from frameworks by offering different suites for other parts of development. Designers would love a level editor and that is annoying to make (even in 2D). Artists may want to tweak the lighting or even do some light procedural generation, so the ability to get feedback of how it looks in-game helps a lot more than guessing in their artist suite. even other programmers may want conveniences for front-end annoying stuff like UI, where the correctness isn&#x27;t based on the code3) Familiarity. Just because you CAN work on your own engine doesn&#x27;t mean you want to. I&#x27;m sure the Hollow Knight devs simply followed up on their game jam project in Unity because they knew Unity. You can&#x27;t spend all your waking hours thinking about how your tools may go to shit in 10 years. You&#x27;d never get anything done.I&#x27;m all for encouraging more engines, but I can understand reasons to seek out something rather than prioritize ownership.----------->But every indie developer I&#x27;ve talked to about game engine development acts like it&#x27;s a dark art. That it&#x27;s just impossible for mere mortals to do such a thing, and if you do, then you&#x27;ll never ever release a game, or you&#x27;ll spend literal years on the engine. Again, I predict a couple weeks.As someone who&#x27;s gone down that rabbit hole: it is black magic combined with pixie dust when working on a 3D game. There is just so many edge cases to resolve and techniques to implement to make a 3d scene look good. And now you need to build something that lets non-devs be productive on op of that (or accept that you will have an artist backseat driving you, losing efficiency). You will spend years and end up with a much worse product, likely very buggy (no pun intended). There&#x27;s just too much to learn before you even start making the game. Make games, not engines (I say as an engine programmer).For a 2d engine, sure. Hollow Knight probably could have been made in Gamemaker with no technical hiccups. And if you know what you&#x27;re doing you can utilize a couple of libraries and whip up a decent 2D engine that is all yours. You&#x27;ll still have issues, but it isn&#x27;t insurmountable if you&#x27;re making a simple game that moves around a few hundred sprites on screen. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Unity, a widely-used game development engine, is revising its license terms to implement a notable fee per unit, raising concerns in the developer community.",
      "This change in Unity's licensing terms is driving some developers to think about transitioning to different engines, including Godot, Unreal, Defold, and RayLib, each offering distinct features, user-friendliness, and scripting language supports.",
      "Casey Muratori is soliciting insights from developers about their experiences with these alternative engines to facilitate informed decisions."
    ],
    "commentSummary": [
      "The debate in game development circles revolves around engines like Unity, Unreal Engine, Godot, and Blender. Unity's shift towards revenue generation through advertising is criticized due to stability concerns and developer dependence.",
      "Godot is highlighted as a potential alternative but struggles to compete with Unity's robust ecosystem. The comparison and debate extend to the popularity and pricing models of Unity, Unreal Engine, and the use of Blender in the indie game industry.",
      "Discussion also includes the decline of the Blender Game Engine, the use of Python in game development, and concerns about Unity's royalty structure. Advocacy for a revenue-sharing model similar to Unreal's is also explored."
    ],
    "points": 134,
    "commentCount": 127,
    "retryCount": 0,
    "time": 1694650747
  },
  {
    "id": 37502892,
    "title": "Marvel visual effects artists unanimously vote to unionize",
    "originLink": "https://www.cnbc.com/2023/09/13/marvel-vfx-artists-unanimously-vote-to-unionize.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS CNBC TV INVESTING CLUB PRO MAKE IT SELECT USA INTL WATCH LIVE Search quotes, news & videos WATCHLIST SIGN IN CREATE FREE ACCOUNT ENTERTAINMENT Marvel visual effects artists unanimously vote to unionize as Hollywood strikes rage on PUBLISHED WED, SEP 13 20231:25 PM EDTUPDATED WED, SEP 13 20234:38 PM EDT Sarah Whitten @SARAHWHIT10 SHARE Share Article via Facebook Share Article via Twitter Share Article via LinkedIn Share Article via Email KEY POINTS Marvel Studios VFX artists have unanimously voted to unionize in response to tight deadlines and heavy workloads. This marks the first time a unit of solely VFX workers have unionized with the International Alliance of Theatrical Stage Employees. The push for unionization comes at a time when Hollywood is dealing with dual labor strikes from its writers and actors. In this article DIS +0.30 (+0.36%) Follow your favorite stocks CREATE FREE ACCOUNT Mark Ruffalo as The Hulk in “Avengers: Endgame.” DisneyMarvel Hollywood, already gripped by two strikes, has some new union members. Marvel Studios’ visual effects workers unanimously voted in favor of unionizing with the International Alliance of Theatrical Stage Employees, IATSE announced Wednesday. This marks the first time a unit of solely VFX workers have unionized with the group. VFX artists have faced increased workloads and tight deadlines to complete some of the industry’s biggest budget franchise films in recent years, leading to tension between these workers and studios. In particular, Disney , which owns Marvel Studios, required immense special effects work in the last three years to complete a massive slate of superhero films for the big screen and television shows for its streaming service Disney+. The Marvel Studios VFX crew has more than 50 workers, according to IATSE. “I grew up dreaming of working on Marvel films, so when I started my first job at Marvel, I felt like I couldn’t complain about the unpaid overtime, the lack of meal breaks, and the incredible pressure put on VFX teams to meet deadlines because I was just supposed to be grateful to be here at all,” Sarah Kazuko Chow, VFX coordinator at Marvel, said in a statement. Representatives for Disney did not immediately respond to CNBC’s request for comment. The push for unionization comes at a time when Hollywood is dealing with dual labor strikes from its writers and actors. Like those striking, Marvel’s VFX artists are interested in a labor contract that offers fair pay, health-care benefits and “a safe and sustainable working environment,” said Mark Patch, VFX organizer for IATSE. They aren’t the only VFX team looking to unionize. In late August, Walt Disney Pictures’ VFX staffers filed with the National Labor Relations Board to host an election to unionize. Now that the vote is official, Marvel VFX workers must engage in collective bargaining negotiations with Marvel Studios executives in order to draft a contract. However, with the studio already locked in talks with Hollywood’s scribes and yet to address contract concerns with striking actors, it could take time for the VFX artists to get to the table. “Today’s count demonstrates the unprecedented demand for unionization across new sectors of the entertainment industry is very real,” Matthew Loeb, president of IATSE International, said in a statement. IATSE represents 170,000 industry workers, from studio mechanics to wardrobe and makeup artists. In late 2021, the union faced off against the Alliance of Motion Picture and Television Producers to negotiate a new contract. The union authorized a strike but was able to come to terms with the studios. Its three-year contract included clauses that enforced a 10-hour turnaround between shifts, 54 hours of rest over the weekend, increased health-care and pension plan funding and a 3% rate increase for every year for the duration of the contract. Stiff penalties were also put in place if these break periods were not adhered to. Loeb told Marvel’s VFX artists that it has the backing of IATSE, telling those who voted to unionize, “Your fight is our fight.” Disclosure: Comcast is the parent company of NBCUniversal and CNBC. NBCUniversal is a member of the Alliance of Motion Picture and Television Producers. The AMPTP is currently negotiating with striking writers and actors in Hollywood. Squawk Box WATCH LIVE UP NEXTSquawk on the Street 09:00 am ET TV Squawk Box WATCH LIVE UP NEXTSquawk on the Street 09:00 am ET Listen TRENDING NOW 31-year-old mom’s ‘simple’ side hustle brings in up to $101,000 a month—off 30 minutes of work a day Ukraine ramps up attacks on occupied Crimea; Russia says U.S. ‘has no right to lecture us how to live’ Taiwan slams Elon Musk, says it’s ‘not for sale’ nor part of China Ivy League expert shares the No. 1 common phrase to never use: It really means ‘I don’t care’ My 95-year-old grandfather is a former cardiologist—his 8 ‘non-negotiables’ for a long, happy life Subscribe to CNBC PRO Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH Advertise With Us PLEASE CONTACT US CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Privacy PolicyDo Not Sell My Personal InformationCA NoticeNEW Terms of Service (Updated August 24, 2023) © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=37502892",
    "commentBody": "Marvel visual effects artists unanimously vote to unionizeHacker Newspastlogin [dupe] Marvel visual effects artists unanimously vote to unionize (cnbc.com) 133 points by toomuchtodo 11 hours ago| hidepastfavorite3 comments ChrisArchitect 9 hours ago [–] [dupe] reply ChrisArchitect 9 hours agoparent [–] More discussion over here: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37499720 reply dang 7 hours agorootparent [–] Comments moved thither. Thanks! replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Marvel Studios' visual effects (VFX) artists have unanimously voted to unionize with the International Alliance of Theatrical Stage Employees (IATSE), a first for a solely VFX-focused team.",
      "This move towards unionization arises amidst dual labor strikes from Hollywood writers and actors. The artists hope to negotiate for fair pay, health benefits, and a secure work environment in response to increased workloads and strict deadlines.",
      "With the unionization vote now ratified, VFX workers will enter into collective bargaining talks with Marvel Studios, although ongoing negotiations with other labor groups may delay these discussions."
    ],
    "commentSummary": [
      "Visual effects artists working for Marvel have unanimously decided to form a labor union.",
      "This news has sparked conversations on Hacker News, a social news website focused on technology and entrepreneurship."
    ],
    "points": 133,
    "commentCount": 3,
    "retryCount": 0,
    "time": 1694646843
  }
]
