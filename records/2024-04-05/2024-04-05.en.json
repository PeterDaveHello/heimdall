[
  {
    "id": 39928604,
    "title": "Decoding Credit Card Rewards and Interchange Fees",
    "originLink": "https://www.bitsaboutmoney.com/archive/anatomy-of-credit-card-rewards-programs/",
    "originBody": "Anatomy of a credit card rewards program Patrick McKenzie (patio11) • Mar 29th, 2024 There are only a few opportunities to make a living by getting good at creating tables to facilitate high-frequency math that end-users will find entertaining but will have predictable statistical properties at scale. One of them is designing roleplaying games. Seems like an interesting topic but someone else will have to write about it. In this column, the dragon sleeps on a hoard of interchange revenue, you slay him to get credit cards rewards points, and the card issuer running the game merrily chuckles at players’ misperception that they are dragons. No, silly, they’re much realer and much richer. A disclaimer off the top: I used to work at, and am still an advisor of, Stripe. A major portion of the Stripe economic model is charging businesses money to take payments on credit cards. Stripe’s two largest costs are paying smart people and paying interchange, and of the two, one would feel a lot better to cut. Another disclaimer: Due to long-standing practice, I am (homeopathically) exposed to the common equity of financial services companies that my family uses, so that I can call up Investor Relations if I ever need to escalate a routine banking issue. My family’s main U.S. bank happens to be Chase, which is mentioned below. Almost everybody writing about credit cards on the Internet receives some sort of spiff if you sign up after clicking through tagged links in their material. That is not my business model (people pay me to write about financial infrastructure), but is probably one you want to be cognizant of every time you read about credit cards online. Rebating interchange to earn share of wallet As we have discussed previously, credit cards have multiple different ways of earning money, but the most important one to this discussion is interchange. It is a fee, ultimately paid by the card-accepting business, which gets sliced up between various parties in the credit card ecosystem to incentivize them to put their logos in the wallets and on the phones of well-heeled customers and increase the amount they spend and the frequency with which they spend it. (In industry, we sometimes distinguish interchange—which mostly goes to the issuing bank—and scheme fees—which mostly go to the credit card brand itself—but as interchange is much larger, let’s just call them both interchange for simplicity.) Interchange is generally a percentage fee based on the final transaction size plus optionally a per-transaction fee. You can just look up the rates, but I strongly recommend you don’t, as you will be reduced to gibbering madness. (It took many smart people many years of work before Stripe could deterministically predict almost all interchange it was charged in advance of actually getting billed for it.) To highlight something which is routinely surprising for non-specialists: interchange fees are not constant and fixed. They are set based on quite a few factors (gibbering madness intensifies) but, most prominently, based on the rank of card product you use. The more a card product is pitched to socioeconomically well-off people, the more expensive interchange is. Credit card issuers explicitly and directly charge the rest of the economy for the work involved in recruiting the most desirable customers. The basic intuition underlying rewards cards as a product is that highly desirable customers have options in how they spend their money. You can directly influence them to use your rails by making those rails more lucrative, more fun, or both for the customer. And so card issuers (and the networks) compete with each other for so-called “share of wallet” by bidding with interchange. This is not quite as sophisticated as the system of dueling robots which bids for your attention every time you open a page on the Internet with an ad on it. The bids are generally speaking pretty static and made years in advance, at or before the time a user signs up for a card product, with relatively minor adjustments made over time. Program managers at card issuers are extremely, extremely sensitive to upsetting the apple cart and churning loyal users, and so they attempt to avoid doing this except when circumstances make it unavoidable. Why isn’t every card a rewards card? Different regions have ended up with different equilibria in the rewards game. In the United States, card acceptance is expensive and the rewards economy is robust. In Japan, card acceptance is expensive and the rewards economy is fairly muted due to—ahem—effective collusion by issuers. In Europe, card acceptance is cheap by regulatory fiat and so rewards are far less common (or commonly lucrative) than in the U.S. Similarly, debit card rewards used to be fairly common in the United States until the Dodd-Frank Act capped debit card interchange (with a very important carveout for small banks in the Durbin amendment). When interchange is regulated, the size of the pie isn’t large enough to give the end-user of the card a tasty slice, and so they get nothing. But we can see that, even scoping to credit cards in the United States, not every card is a rewards card. What explains why CapitalOne, for example, offers rewards for its Quicksilver card but not the Platinum Mastercard? Is mercury attempting to burnish its image after that horrible toxicity thing? Three of Capital One's many offerings. Partly, this is that different users have different jobs-to-be-done for credit cards. (Many users of credit cards, potentially including some readers of this column, believe that they are the typical users of credit cards. No users of credit cards are typical.) Credit cards are both a payment instrument and an access point for a revolving credit line. That they are an expensive way to borrow money is one of the first things said in any personal finance text. They are also one of the most accessible ways to borrow money, and that is their primary value proposition for many users, typically ones lower on the socioeconomic ladder. These users spend relatively little in a month compared to their revolving balance, which they continuously pay interest on. It trivially follows that most money on their accounts is earned via net interest margin and not from interchange. To the extent these users notice numerically defined features of their credit cards, which is somewhat dubious, it is the headline APR (cost of credit) and their credit limits. “Starter” cards and other products aimed at this user group typically have no rewards; they instead use interchange income to “bid down” the headline APR. Interchange functions as a subsidization of the cost of credit throughout the economy by businesses which want to sell things to people who would use credit to buy things. The limit case of this is Buy Now, Pay Later, where the cost of credit is subsidized straight to free. The heaviest credit card spenders, and this fact is both uncontroversial and flies in the face of what many personal finance columnists believe, are wealthy and sophisticated. They use credit cards primarily as payment instruments. Issuers compete aggressively for their business, which is quite lucrative. This is not because they pay much in interest, because while they have higher headline APRs they only rarely revolve balances. It is because “clipping the ticket” via interchange on a high volume of transactions is an excellent business to be in. How dramatic is this? Allow me to reproduce a graph from Regulating Consumer Financial Products: Evidence from Credit Cards. (This paper is something of the Rosetta stone for credit card issuance as a field, both in that it is a single source for understanding a huge range of human endeavor and in that it stands in for a very large literature that nobody else reaches for to be the first citation when the Rosetta Stone already exists. If you see an unlabeled graph on Twitter about credit cards, it was probably lifted from this paper.) Interchange income net of rewards expenses heavily favors high FICO users. As you can see, as a percentage of Average Daily Balance (ADB), even after rewards expense, interchange gets very sharply more lucrative at the top of the credit score distribution (740+, which is roughly 10% of accounts). The difference is actually larger than you see here, since credit lines and ADB also increase with credit score, for predictable reasons. (Rich people consume more than poor people on an absolute basis, film at eleven.) The complexity spectrum of rewards products The simplest reward products are straight “cashback”. The issuer totals up all of one’s net purchases (all purchases less refunds) in a statement period. It then credits the user with a particular percentage of that for each statement. Either automatically or periodically at the user’s request, it transforms some of that notional banked credit into a statement credit, decreasing the amount the user needs to pay to cover their purchases in the current month. This is as simple as it gets and we’re already necessarily handwaving away libraries worth of complexity. (For example, calculation of net purchases needs to be fairly robust against adversarial collaboration of users and merchants or the issuer gets turned into a money pump within a matter of days and will not likely be able to detect or reverse this condition for at least several weeks. This has happened many, many times. Credit card issuers, when they screw this up, lose millions of dollars and dry their tears on money.) Anyhow, in the mists of history, that percentage started flat; typically, 1%. The math supporting this is typically fairly simple: take in 140bps as revenue, pay the user 100 bps as effectively a cost of customer acquisition, keep some portion of 40 bps as one’s margin. In nations other than ones with effective cartel-like behavior by issuers, this equilibrium was not stable, because competing issuers would bid e.g. 1.25% cash back on the same underlying economics and compete for share of wallet. This happened extremely robustly, for decades. One iteration of this game was cash back “categories”. Particularly post-Internet, certain cohorts of customers were most interested in the headline cashback percentage rate. Issuers began to design products which were more complicated, such as “1% cash back in principle, however, 1.5% cash back at gas stations.” A fun rabbit hole about credit card acceptance A huge percentage of all credit cards are “co-branded”; they are issued by a financial institution but bear the name of some other institution which inspires a lot of loyalty. A teeny, tiny percentage of co-branded cards name e.g. tertiary educational institutions. Most name a business that a customer has an intense, ongoing relationship with: Costco, their airline of choice, etc. Co-branded cards are extremely big business. One subtlety about them: a co-branded card will often have a special rewards tier for the business named on the card. This is partially that business choosing, as a marketing expense, to split some of their core margin with their most loyal customers (overwhelmingly the target audience for the co-branded card). But, surprisingly to many non-specialists, that is not the sole source of margin to reward cardholders with. This is because, at the scale of the largest businesses in the world, financial services are cross-sold and structurally interconnected. There is one team in a bank (it happens to be Chase, for what it is worth) attempting to capture Starbuck’s card processing business. There is another team that wants to convince Starbucks to co-brand Chase card products. These two teams can talk to each other prior to making proposals to Starbucks. And so, without knowing anything about the payments industry, you can speculate with a very high degree of confidence that Starbucks has received a complicated spreadsheet saying what Chase will charge it for every conceivable type of credit and debit card that wants a latte. A prominent negotiated line on that spreadsheet includes, effectively, a major discount for “on us” transactions: when Chase’s left hand needs to move money from Chase’s right hand because someone wants to pay Starbucks money for their Starbucks using their Starbucks-branded money. (This is easy to conflate with, and is separate from, Starbucks’ extremely impressive stored-value product. That probably mostly cannibalized their credit product, which… alright, we have to stop the levels of digression somewhere or we’ll be here all day.) Because “on us” is structurally cheaper (the transaction literally travels over fewer rails and therefore there are fewer “mouths to feed”) and because it is incentive compatible for all parties to subsidize these transactions, as a card program designer, you have a relatively easy time being generous with regards to this particular cell of your spreadsheet. Back to more complicated cases So imagine you’re the card program designer for a card that, like most cards, does not have a particular named business happily subsidizing your users. You desire to quote a headline cashback number much higher than 1%. But you’d still like to keep some margin from interchange. What could you do? One is you make the headline number larger but contingent on something. Say, for example, the card rebates 1.5%, but only for… bookstores. Any bookstore. For all other transactions, it is 1%. The thing you would love with this offering is to preferentially attract people who are very emotionally invested in being readers and who spends very little of her on-this-card wallet on books. The emotional investment in the story the card offers brings the customer in; the blended cost to acquire the customer is closer to 1% industry standard and not to the 1.5% headline number. Money is fungible, money is fungible, money is fungible, but many people don’t actually orient their lives as if this were true, and so the financial industry meets them where they are and then charges them for the privilege. This user values a dollar more when it is a books-dollar than when it is a food-dollar. You, a credit card program manager, can math out a way to get her as many books-dollars as she is interested in. (This is similar, in spirit, to how Bryrne Hobart describes airline frequent flyer programs as working. One key difference that credit card program managers have to understand: a source of advantage for frequent flyer miles as a pseudocurrency is that they can turn very-low-marginal-cost inputs, unsold seats, into very-high-perceived-marginal-benefit outputs, “free vacations”. Books-dollars may very well be worth more than a dollar to our target user here, but books-dollars are very difficult to manufacture in quantity for less than about 98ish cents.) In principle, you could even offer more than your direct interchange revenue as the headline number, if you were very, very sure that your typical user would not preferentially use your card only to buy books and use a competitor’s card to buy groceries, gasoline, medicine, and similar. Now, unfortunately, remember what we said about typical credit card users? That’s right. They don’t exist. Very many of your users will do what you want them to, and use the card in a perfectly-acceptable-but-not-exactly-optimal fashion, and you will have a blended cost very near 1% for them. And very many of your users will do exactly what you most don’t want, and use the card only to buy books. This is… far less incentive compatible for you, particularly if you decided that the business of manufacturing books-dollars was so lucrative that you could rebate more than the direct interchange revenue given mix effects. These users will have blended costs very close to your headline number, not to your modeled blended costs. These users will even band into tribes, find each other on the Internet, and swap tips for exploiting poor, defenseless credit card program managers like yourself. The tribal elders will eventually run businesses, with names like The Points Guy, which eventually get quietly acquired by very sophisticated private equity firms. Those PE firms are betting that you continue paying generous per-signup affiliate commissions to Internet properties which send you new card users. You bet you will also paying tens of millions of dollars annually to Frequently Adversely Selected New Accounts Dot Com. And Redditors bet they will continue chortling that they have pulled one over on you, because haha, you’re not nearly as good as they are at fourth grade math or keeping spreadsheets. The biggest difference between you and a Redditor is not ability to do fourth grade math or ability to do spreadsheets. Redditors are frequently sophisticated with their spreadsheets; many of them could clearly earn three orders of magnitude more from the financial industry if they stopped thinking that the right way to monetize spreadsheet skill was in gaming credit card signup bonuses. The biggest difference is that you’re optimizing over portfolios, over time and Redditors are largely playing in single player mode (and frequently over short horizons). You only care about single or dual player mode to the extent that you avoid obviously degenerate offerings where adverse selecting single players quickly dominate your entire book of business. Which is very much a risk, which is why the bank has smart people like you keeping spreadsheets. The Redditors think failure modes for the bank sound like pudding guy. Pudding guy, was, of course, one of the highest-ROI ad buys in the history of capitalism. Further refinements in cat and mouse games But, just like lotteries have to keep reskinning the random number generators or the games get stale and ticket sales go down, credit card program managers have to periodically shake things up for something to cut through their adversaries’ built-in distribution networks and massive, massive marketing budgets. One innovation, now 20+ years old, was a rotating favored category for cash-back. So instead of being 1.5% for bookstores 1% all else, it would be 1.5% for groceries in Q1, for gas in Q2, etc etc. The theory behind this was pretty simple: many customers attracted by the headline number could be brought in the door by it but would be fairly inattentive after a card was their new “top of wallet” (the default card for spending). Over time, a combination of inattentiveness, changing willingness to spend time playing the game, and rewards caps would bring the portfolio’s cost of rewards close to the baseline and not to the headline reward number. If for some reason playing this game interests you, one prominent product is called the Chase Freedom card. It should not be an attractive product for you, given plausible assumptions about the readership of this column, but it is an attractive enough product that probably millions of Americans use it. Giving the customer more choices more frequently If you were hypothetically to have spent the last few years dining with relatively well-off people in SFBA, when it came time to pay for dinner you’d see two credit card products with a combined 90%+ share: Chase Sapphire Reserve (CSR) and an interchangeable American Express card. This, ahem, includes many diners who are professionally connected to upstart payment methods. American Express, for a very long time, had an almost mortal lock on the top end of the credit card market. Chase attempted to disrupt that in 2016 by very overtly attempting to buy away their core customer. You can read lots in other places about the original promotion, generally tsk-tsking about how absurdly lucrative it was for customers and how it caused Chase to have a (temporary) loss associated with their cards business. Personally, I think it was one of the most interesting strategic moves in the last 20 years of retail banking, but a full essay on that would require some other-than-public knowledge as to the size of the tsunami that happened after it. One thing that is public but not well appreciated: Chase didn’t just decide to create an extremely lucrative-for-the-customer offering out of the goodness of their hearts and out of their own P&L. No, they pitched Visa on this idea. For too long, Visa, you have watched your competitor American Express outcompete every issuer in the Visa system for the best wallets in the world. They can do that because they can afford to, because American Express charges systematically higher interchange rates than Visa does even at its topmost tier. Visa, you should create a new tier where your not-exactly-chosen champions can try to spend those interchange dollars to give American Express a run for their money. And, lo, Visa did create a new tier. You can expose yourself to gibbering madness if you want to know what the name for it is. But Chase got Visa to authorize Chase charging almost the entire economy more for credit card acceptance with the specific goal of outcompeting American Express for the most lucrative highest-monthly-spend virtually-never-revolve-a-balance credit card users. That was part of what made the numbers work. Another part is that the CSR offers a fairly complicated rewards scheme, with a lot of opportunities for people to pick things which feel great but are not optimal. For example, Chase will let you cash out CSR points on a 1:1 basis at either Amazon or Apple, integrated directly into the checkout flows. Those feel great. This image has been edited to remove private information and to highlight, in red, the offering. People love Amazon and Apple and they love free Amazon and free Apple even more. This is true even among a portion of very sophisticated, wealthy, numerate CSR users, who love this idea so much they click a button designed for suckers. Why is that a sucker’s checkout button? Because CSR also includes a feature called Pay Yourself Back which in the past prominently, and today a bit… less prominently, lets you cash out rewards at better than 1:1. You get a 25% bonus if you Pay Yourself Back by nominating past transactions at grocery stores: 10,000 points gets you $125 in statement credits if you are willing to do a trivial amount of clicking to show Chase $125+ in spend at grocery stores. Stating the obvious: Chase knows what a computer is and does not require you to actually identify your purchases at grocery stores. This is a product decision to both a) force you to use your card at grocery stores and b) force yourself to say “Chase is getting me free groceries! How nice of them!” on a transaction-by-transaction level once a month to use your card optimally. There are many, many other sucker buttons. But, because the card is fundamentally targeted at rich, sophisticated people, Chase really does pay out a shedload of rewards. The 3% headline rate for travel and dining plus the 25% Pay Yourself Back kicker means there is a sustained and trivial pathway to get 375 bps out, which is one of the very, very, very few places in the industry where there is a sustained, trivial, uncapped way to get out more than the direct cost of interchange. Why does this persist? Partly it is due to the standard credit card portfolio strategy: every time someone uses a CSR on Amazon (and not e.g. the Amazon rewards card, also from Chase) or uses CSR points to buy high-margin white plastic, Chase’s contribution margin for the portfolio goes up, and at scale quite a large percentage of customers do this. Partly it is due to Chase thinking that they’ll just be so good for their target customer that their target customer will not bother playing the game optimally. That would require the customer carefully maintaining a portfolio of credit relationships and having seven cards saved on their iPhone and not, simply, the CSR. Partly it is due to the really interesting strategic reasons for having CSR available: to a much larger degree than American Express, Chase is a diversified financial services empire. The CSR was effectively designed as a wedge product to get something Chase branded into the hands of affluent up-and-coming young urban professionals, with the goal of eventually getting them to not move just their wallet but their entire financial existence (and potentially their current-or-future entities’ financial existences) onto Chase. The Sapphire mini-brand was so loved that they reused it for a bog-standard premium checking account. (It was named the Chase Sapphire Premium Checking Account, in a decision which probably consumed several tens of millions of dollars of professional effort, and I mean that absolutely descriptively and not as a criticism. I was not in that meeting… but I’ve been in that kind of meeting. Those that have been in it know that it is not in any way a single-meeting single-decisionmaker sort of call.) More directions to go in The blessing and curse of essays is that they have to end somewhere, and then pick up anew somewhere else. Hopefully the above gives you a bit more context on what is in your wallet. In the future, we'll likely discuss the complicated iterated game played by the credit card networks and the rest of society regarding interchange rates, how there are high-interchange-high-reward equilibria and low-interchange-low-reward equilibria, the recent settlement where the networks agreed to temporarily decrease interchange, and other topics. Financial systems take a holiday → Want more essays in your inbox? I write about the intersection of tech and finance, approximately biweekly. It's free. Get a biweekly email Great! Check your inbox (or spam folder) for an email containing a magic link. Sorry, something went wrong. Please try again.",
    "commentLink": "https://news.ycombinator.com/item?id=39928604",
    "commentBody": "Anatomy of a credit card rewards program (bitsaboutmoney.com)932 points by disgruntledphd2 23 hours agohidepastfavorite599 comments nkurz 22 hours agoAn interesting article, but it doesn't sufficiently emphasize the lede: When you use a reward card, the merchant is charged a higher fee than if you used a \"normal\" card. Simply by putting a different branding on the plastic you pay with, the credit card issuer gets more money from each transaction. The article goes on to ask the question \"Why isn’t every card a rewards card?\", meaning why doesn't every card pay cash back, but I think the more interesting question is why every card isn't branded in a way that makes the issuer more money. Why do they bother to issue cards where they get paid less? Why not brand every card as a \"Signature Preferred\" and then pocket the money instead of giving it to the less discerning customers? And the most interesting question only gets a handwave: \"The basic intuition underlying rewards cards as a product is that highly desirable customers have options in how they spend their money.\" But how far does this go in explaining why merchants \"choose\" to participate in this program. The obvious answer would seem to be that they get no benefit from the system as it exists but have no real choice, but maybe there is a better answer? I liked the topic, but wished the author could have given more insight on what's happening behind the scenes to produce the outcome we see. reply abalone 20 hours agoparent> But how far does this go in explaining why merchants \"choose\" to participate in this program. The obvious answer would seem to be that they get no benefit from the system as it exists but have no real choice, but maybe there is a better answer? The simple reason why issuers don’t make every card a signature rewards card is that merchants would revolt. The interchange fee schedule[1] is fascinating. Dozens of categories of merchants with different rates. There is no technical reason for this. Fraud costs are borne by merchants and to some extent processors, but not the issuer banks that receive the interchange fee. The fee schedule reflects a kind of battle for customers. It’s worth repeating that most of interchange for these higher end cards is passed back to the customer in the form of rewards. Essentially, merchants are willing to pay higher fees to support the cards that higher spending customers prefer. But there is a limit. We can observe that not all merchants accept AmEx, which has some of the highest interchange rates. If every visa/MC card were a signature card, more merchants would push back. [1] https://usa.visa.com/content/dam/VCOM/download/merchants/vis... reply sofixa 19 hours agorootparent> There is no technical reason for this Point in case, there's an interchange fee cap of 0.3% for credit and 0.2% for debit cards in the EU. And there are entire countries moving to cashless, so obviously everyone is happy with it. reply abalone 18 hours agorootparentI wouldn’t assume everyone is happy with it. Consumers are going to prefer rewards programs over no rewards programs. And before you say it results in higher prices, that’s not necessarily true. Australia regulated away interchange and it didn’t result in lowering prices. Merchants kept the profit. A lot of times these regulations are pitched as helping consumers, but it’s really merchants pushing for them. You could make a similar observation about the EU regulatory fight with Apple et al right now. It’s actually Spotify fighting for it, and they have different interests than consumers. reply mumblemumble 16 hours agorootparent\"Lower prices\" doesn't necessarily mean they just suddenly and immediately drop. That's no surprise; dropping prices purely out of the goodness of your heart isn't terribly good business practice. Also, for a lot of retail, MSRP is MSRP, and that's a pretty big anchor point. What I'd expect instead, based on my having taken exactly one class in economics as an undergraduate, is subtler effects that play out over time. Maybe the general growth in prices over time slows down a titch until a new equilibrium point is met. Maybe wages rise a little bit because retailers can afford to pay their employees more. Maybe life gets easier for smaller businesses that have less negotiation power than the multinational behemoths. Maybe some bank executive somewhere decides not to buy that third luxury car at the same time as ten thousand restaurant owners decide that, just today, they will treat themselves to an espresso drink from the coffee shop instead of making drip coffee at home. That kind of thing. I think maybe that last example is most interesting to me, because it calls attention to how merchant/consumer is a false dichotomy and things are always a bit more subtle than how the news likes to make us think they are. reply youainti 7 hours agorootparent> What I'd expect instead, based on my having taken exactly one class in economics as an undergraduate, is subtler effects that play out over time. As a PhD student in Econ, I am glad to see that you learned something about how to actually apply this work. Thanks for making my day after some rough grading. reply eek2121 10 hours agorootparentprevExcept prices in Australia, especially in Tech, are among the highest in developed countries. Like I get what you are saying, but reality, in this case, trumps theory. reply andruby 18 hours agorootparentprev> I wouldn’t assume everyone is happy with it. Consumers are going to prefer rewards programs over no rewards programs. Personally, I would disagree. I prefer no rewards and a simple landscape where I don't have to compare credit cards. I lived in both EU and US, and didn't like the work needed to compare (and keep comparing) all the credit card offerings. In the EU, you just the credit card from your bank and don't feel like you're missing out. reply alibarber 16 hours agorootparentI my part of the EU I'd certainly feel like I'm missing out if I just kept on paying the ~€5 a month fee for a bank account and didn't look for a card that gave me some kind of return on my spend every month... (It's Finland btw, and you need a Finnish, not \"anywhere-in-SEPA\", bank account in order to practically function in society here with the strong online authentication service) reply oezi 15 hours agorootparentThe return you get is just taken from you without you knowing it. The rewards programs is taking your money and giving you back a part of it. reply willseth 14 hours agorootparentNo, it's taking the merchants' money and giving you back part of it. How much do merchants eat it vs passing on the cost? I don't think that's an easy question to answer and probably varies a lot by merchant type and product, but I think we can assume the answer is not always passes on 100% of the cost, so owners of the high end rewards cards are winning to some degree. You could argue that non-rewards cards are absorbing costs of rewards cards in the case where merchants do pass through costs, though. reply AnthonyMouse 13 hours agorootparentExcept that 100% of the cost isn't being transferred to the cardholder, either. You might have a 1% cash back card while the merchant is paying 3%. If only half of the cost is being passed on, you're still losing money. And price isn't the only variable. Even if the merchant ate the entire 3%, that might require them to cut costs in some way so you receive a lower quality product, or drive some competitors out of business and thereby allow the remaining companies to reduce quality without lowering prices because the company providing a better product for the same price was eliminated by the fees. reply willseth 13 hours agorootparentRight, but the interesting part of TFA is about how the rates paid by merchants are higher for the top 10% of cards. Your example assumes the same rate paid by everyone. Because only a small portion of transactions incur the high rate/high reward, it seems far less likely that the split between pass-through vs eat-it still won't benefit high end cardholders. reply AnthonyMouse 13 hours agorootparentThat still depends on what the split is. If the average fee is 3% and the high-end customer is getting 2% back, the merchant could be passing on e.g. 2.1% and causing you to come out behind while still passing on only 70% of the cost. And for anyone getting less cash back the math is even worse, which from the same premise will be the majority of people or else the merchant's costs (and so the amount they pass on) would be even higher. reply willseth 12 hours agorootparent> If the average fee is 3% and the high-end customer is getting 2% back ... You're not picking realistic numbers. The fees range from around 1.5% to ~3%, and the 2.5%-3%+ fees are limited to ~10% of customers. An average fee of 3% doesn't make sense. Very basic napkin math would be 0.9((1.5+2.5)/2)+0.1((2.5+3)/2) = ~2.1% average. So only in the 100% passthrough case does the high end cardholder actually lose. That's not likely. And if you look at the chart, if 740 is 10%, there are probably far fewer than 10% of transactions averaging 2.75%, so this is likely still way overestimating. reply AnthonyMouse 12 hours agorootparent> The fees range from around 1.5% to ~3%, and the 2.5%-3%+ fees are limited to ~10% of customers. An average fee of 3% doesn't make sense. That's just the interchange fee, not what merchants are actually paying. Stripe charges a flat 2.9% + $0.30 and this is considered competitive: https://stripe.com/pricing For a $20 transaction, that's 4.4%, and that's for everybody, not just the people with rewards cards. reply abalone 11 hours agorootparent> and this is considered competitive Stripe is hella expensive! But they are easy to get started with. Competitive rates depend on the type of business, i.e. their volume and their fraud risk. The most transparent form of pricing is called “interchange plus” where it’s a flat markup on the interchange schedule. High volume merchants should be able to find a markup in the fractions of a percent. It is my understanding that the big Stripe customers negotiate lower rates with them as they scale. reply AnthonyMouse 11 hours agorootparent> High volume merchants should be able to find a markup in the fractions of a percent. And this is why small businesses are more likely to offer cash discounts than larger ones. reply pbhjpbhj 11 hours agorootparent5 years ago HSBC were charging us, a micro-business (2 employees, not software) about the same for cash handling as we paid for debit card processing. If small businesses give discounts for cash it's because they're committing tax fraud, I presume. reply AnthonyMouse 10 hours agorootparentCash handling is typically in the neighborhood of 0.2%-0.3%, so you were apparently overpaying: https://www.nerdwallet.com/article/banking/business-checking... And that's assuming you're depositing all of your revenue. If your business allows you to pay some of your suppliers in cash, you could have >50% of your revenue in cash and never pay a bank for cash handling because you're immediately spending it on business expenses rather than depositing it. > If small businesses give discounts for cash it's because they're committing tax fraud, I presume. I have seen governments charge a convenience fee for credit card processing. Is the government committing tax fraud? reply inkyoto 6 hours agorootparent> Cash handling is typically in the neighborhood of 0.2%-0.3%, so you were apparently overpaying […] This is not universal, and banks in different countries charge different cash collections fees. The cash collection fees are also structured, e.g. whether the daily collection is required, or every other day, or once a week. It does not end there. Many banks still require the business to sort collected coins into separate money bags according to the coin denomination, e.g $1 coins go into one bag, $0.50 coins go into their own bag. Coin bags have a weight limit, 2 or 3 kg, which means that the business has to weigh the money bag up before handing it over, or it will not be accepted. Now that we are done with material things, we also have to consider all things immaterial that the cash handling entails. Before the money bag is handed over, the collected cash has to be counted and reconciled against the cash register records on premises, otherwise it will create annoying and time consuming to fix discrepancies in the accounting system. If a staff has accidentally mislaid a note or a few coins, amounts won't reconcile and incur a cash collection delay as the armoured truck can't wait for the reconciliation to complete. Which may consequently increase the risk of leaving cash in the shop overnight with all expected consequences of a potential burglary and losing the cash. That is just some of the peculiarities of how cash is handled, and I am not sure whether cash handling turns out to be cheaper for an average business with a substantial number of cash payments a day. Electronic payments, on the other hand do not have any of those shortcomings, vastly reduce the margin for human errors, automate the reconciliation and accounting and reduce the risk (i.e. no money is kept in the shop overnight). reply willseth 11 hours agorootparentprev> That's just the interchange fee, not what merchants are actually paying. What you pay Stripe is for the combination of interchange fees + Stripe's own service fees. The Stripe service component of the fee would be charged regardless. Whatever markup a merchant makes for Stripe's fees are not recoverable and irrelevant to the comparison. reply AnthonyMouse 11 hours agorootparentBy comparison, Stripe charges 0.8% + $0.00 for ACH, which includes any costs they might be paying to the bank, so the upper bound on the cost of their services is 0.8%. 4.4% - 0.8% is 3.6%. Identify a payment processor that a small business making e.g. $60,000/year in $20 credit card transactions can use that would charge lower fees, if you can. reply willseth 11 hours agorootparentYou're making a huge assumption that their markup is the same for credit card transactions! You're also making broad oversimplified assumptions about the what merchants do in response to transaction fees. Multiple implausible things need to be true for high end card users to be losing out on this scheme. Most likely the high end users come out at least slightly ahead and most of the cost is borne by lower end users and merchants. reply AnthonyMouse 10 hours agorootparent> You're making a huge assumption that their markup is the same for credit card transactions! We can take this from the other end though. A merchant who accepts ACH via Stripe pays 0.8%, one who accepts $20 on a credit card through Stripe pays 4.4%, so the merchant pays 3.6% more with a credit card than ACH, and 4.4% more than accepting physical cash. It doesn't matter how much of each is Stripe's profit unless there is some competitor a small business could use to process credit cards for less. Is there? > You're also making broad oversimplified assumptions about the what merchants do in response to transaction fees. In a competitive market, increasing every competitor's costs would cause them to pass on most/all of the costs, because the competition is keeping margins thin and their alternative is to go out of business. This is not an oversimplification, it's what actually happens in real commodity markets. > Multiple implausible things need to be true for high end card users to be losing out on this scheme. All that's required to happen is that the merchants are passing on more of the credit card processing fees than the amount of the rewards. Since the fees are higher than the rewards, this is not that implausible. reply willseth 10 hours agorootparent1. You're missing the very important detail that there are multiple combined fees, and you are trying to compare a rebate for one of those fees to the combined fees. 2. Commodity markets literally are an oversimplification. 3. You are conflating unassociated fees! See 1. reply AnthonyMouse 9 hours agorootparent> You're missing the very important detail that there are multiple combined fees, and you are trying to compare a rebate for one of those fees to the combined fees. The combined fees are the cost of accepting credit card payments. The entire collection of them is avoided by accepting cash. > Commodity markets literally are an oversimplification. Only in the sense that everything is an oversimplification. If OPEC cuts oil production, the price of gas goes up all over, because it's a commodity and the gas stations can't just eat the price increase. If the DRAM companies were colluding to constrain production and they get caught and have to stop, the price of DRAM goes down all over, because it's a commodity and buyers will take the lowest price. But if the price of DRAM goes down, that doesn't mean the price of iPhones go down, because iPhones are not a commodity -- only Apple makes them -- and then they don't necessarily have to lower their prices just because their costs went down. Real competitive markets can actually behave like idealized commodity markets, or as close as makes no difference under reasonable sets of assumptions. > You are conflating unassociated fees If they're unassociated then how does a small business pay only the interchange fee and not the rest of them? If the answer is that you can't, they're not unassociated. reply willseth 9 hours agorootparent> Real competitive markets can actually behave like idealized commodity markets, Sure sometimes, but transaction processing markets are not that simple. They are 2 sided markets, i.e. both the merchant and the merchant's customer are customers of the credit card company, who charges the merchant and the merchant customer varying fees based on how valuable the customer is to them. Then you have a merchant services layer on top of that. Then you have the variety of markets and goods that use all these services, all of whom may have different terms and fee structures with the credit card company and/or merchant services company. Merchants will pass on or absorb fees based on many factors, and possibly even varied within its goods and services. It's really complicated, and commodity markets are a gross oversimplification of how it works. Trying to model it would be a nightmare. > If they're unassociated then how does a small business pay only the interchange fee and not the rest of them? If interchange fees were 0 then you would still pay the other fees, which do not go to the credit card company. The fees are related, but not associated. reply matheusmoreira 6 hours agorootparentprev> paying the ~€5 a month fee for a bank account That's the problem right there. Bank accounts should be free by force of law. A bank charging depositors for literally anything is just absurd. They should be paying you, not the other way around. reply Ekaros 3 hours agorootparentFor many average people I don't see how they could be free. Maybe savings account with limitations should pay something and be free, but with checking account. You get what couple thousand passing through each month? You have all the costs like KYC requirements, actual overhead of tracking and managing the balances and payments. And many of the payments options are free or low cost... If it was such money maker, surely there would be lot of competition offering zero fee bank accounts. reply andruby 10 minutes agorootparentIn a lot of EU countries there is plenty of competition and there are multiple options for free bank accounts. The banks make money of the debit & credit card payments, savings accounts, having you as a customer for mortgages and other loans. List of 30 free checking accounts in Belgium: https://www.spaargids.be/sparen/gratis-zichtrekeningen.html (NL/FR) LastTrain 14 hours agorootparentprev> Consumers are going to prefer rewards programs over no rewards programs. I believe that you believe. Rewards programs are ultimately bad for consumers and merchants, but great for rent-seeking banks. As a consumer I'd prefer an EU style cap and not have to spend my time working to scrape back some of that money. reply abalone 11 hours agorootparentWhat do you mean by “spending time working to scrape back money”? Cashback rewards are fairly automatic. Maybe some have a simple redemption step. Apple Card’s is 100% automatic. reply Slartie 1 hour agorootparentDid you read the originally linked article? It literally talked about ways in which rewards programs are often designed to only hand out top rewards for a very limited part of all transactions, like only for books at certain bookstores. And that these obscure rules in the extreme case can also change over time, so in one month you need to go to store A, in the next month you better visit store B. So yes, the rewards are \"automatic\". But what's not \"automatic\" is you getting the maximum possible reward percentage on your spending. For that, you often need to optimize your spending along the guardrails put up by some random card issuer. Which takes effort and time. I don't know about you, but I've got better uses for my time, so I'd prefer just not having 2-3% of the price of all products I buy to go towards obscure rewards through which I need to claw back some of those 2-3% by gaming the system. If I want to play games, I buy a computer game and play that. reply madeofpalk 11 hours agorootparentprevApple Card doesn't seem like a particularly good business, so much so that the issuer is trying to ditch it https://www.wsj.com/articles/goldman-is-looking-for-a-way-ou... https://archive.is/bkoBG reply abalone 4 hours agorootparentWe’re talking about the consumer benefits here. It’s true Apple extracted some of the most consumer-friendly terms in the industry. reply oezi 15 hours agorootparentprevRewards programs are stupid for consumers. They cause higher prices for everyone (even for the rewards recipients). I am fine with merchants keeping the profit, because for most categories of products that I buy there are working markets and every % of profit is turned to lower prices in the end. reply abalone 11 hours agorootparentWhy do you say they cause higher prices for rewards recipients? Is there evidence of this? reply oezi 3 hours agorootparentThe cost of running the rewards scheme must be paid. It is like a tax you incur. Also the cost for you to participate can be substantial in the form of time/attention. All for 1%-2% rebate. Many rewards programs are also predatory in the way they want to sell your data so you are targeted by advertisers for products you wouldn't buy otherwise. reply eek2121 10 hours agorootparentprevUntil pricing in the US exceeds the rest of the developed world, you won't convince me of this. After all, we are among the top countries that use credit cards. reply sofixa 1 hour agorootparentDo you have examples of things bought with credit cards that are cheaper in the US compared to, say, the EU? Even having in mind that most European countries have a ~20% VAT on many products, while US sales taxes are usually lower (less than ~15% IIRC), in my personal experience the US is more expensive for most everyday consumer goods even than France. reply refurb 3 hours agorootparentprevRewards programs cause higher prices just like “free delivery” or “no questions asked returns for 6 months” does. It’s just another cost of business. Sometimes you bump up the price to account for it, sometimes you take the hit in profit in exchange for more volume hand more profit). Very rarely do businesses do “cost plus” pricing. They usually charge what they can. Which is why prices are sticky. For the businesses that price based on cost alone they usually reject credit cards all together. Long ago I shopped at a computer parts store that had the best prices and they were all focused on volume - no further discounts, no credit cards, no free delivery. reply ddfs123 2 hours agorootparentBut if that's true then why Debit cards are not qualified for reward programs ? reply ddfs123 2 hours agorootparentprev>Australia regulated away interchange and it didn’t result in lowering prices. Prices has already been raised, I don't think they'll ever drop back, so that doesn't seems like a reasonable data point to dispute the price raising claim. reply Ekaros 2 hours agorootparentBetter measurement might be did they go up less after this or did they go up later? As there should be more leeway in delaying increasing prices. reply astura 13 hours agorootparentprevAs much as I like taking advantage of rewards programs as a consumer (and boy I do), if given the choice I would actually prefer merchants paid very low fees and I got no rewards. That's just my personal preference. It seems like a much fairer system. I just don't like that middlemen take an unfair share, even if that middleman is me. reply brazzy 11 hours agorootparentprev>I wouldn’t assume everyone is happy with it. Consumers are going to prefer rewards programs over no rewards programs> Absolutely not. Fuck rewards programs. I don't want to waste a single second thinking about how to optimize my card usage and spending habits to get \"rewards\". reply astrange 13 minutes agorootparentI think the round the world ANA ticket I got a few months ago for less than 10x face value was worth reading one The Points Guy article about how to use Amex points. reply abalone 4 hours agorootparentprevI respect that. FWIW I just use three cards: 1. Apple Card which is 2% back on all Apple Pay 2. Costco Visa that does 3% on restaurants and travel 3. Amazon prime visa, 5% back on Amazon and Whole Foods. (This is just saved to my Amazon account so I don’t have to think about it.) It’s pretty easy to remember to use the Costco card at restaurants. reply refurb 3 hours agorootparentprevCool. Do what works for you. For me I just sign up for a new credit card twice a year and then shift my normal spending to the new card, canceling the old one. I usually get a week’s hotel stay for free or $1000 cash back for nothing more than taking 30 minutes twice a year to find the best card. reply paulmd 10 hours agorootparentprevSo get one of the cards that offers 3% on everything / 2.5% and no international transaction fees, and just cash it out monthly or whatever? You’re not being cool or whatever, “my big brain is too full for such trivialities!” you’re just leaving >$100/mo on the table (for most households here) for literally zero effort beyond clicking \"redeem\" when you want it. https://old.reddit.com/r/CreditCards/wiki/list_of_flat_cashb... But credit card fees exist, and will exist regardless of what you choose. Just like filing for your tax return - you can either take the money or let the bank+merchant have it. But maybe you're just too busy to be bothered by reclaiming that money, too? The amex \"I pay $500/y for a 3% card with rotating quarterly 4% and 5% categories\" shit though? Yeah, I agree, ain't nobody got time for that. reply rowls66 9 hours agorootparentYou need to spend $16,666 just to cover the $500 annual fee. That’s a lot for many people especially when many low end retailers don’t accept AmEx. As already state rewards cards are a subsidy for the rich, or at least for people who spend a lot. reply lotsoweiners 6 hours agorootparentYeah $500/yr fee is ridiculous. I think my card is 1/4 of that and I’m sure I break even the first month since almost everything I pay for goes through that card. Utilities, cell phone bills, after school program fees (and before that daycare), all other recurring bills besides mortgage/car, restaurants, groceries, anything else I want or need to buy go through the same card. Always pay the balance in full each month no matter how painful. reply epcoa 3 hours agorootparentprevThere are no 3% cards (not even 2.5% CB cards, I believe) without some kind of “requirement”. The only 3% card in your linked list clearly requires $100k in assets tied up with that FCU. For now that MM interest rates aren’t total shit that might be ok, but it is something that must be considered. 2% is about the most you’re going to get “no strings”. reply fallingknife 16 hours agorootparentprevYeah that's kind of how the system operates in the US. The CC duopoly fleeces merchants and gives out a share of the monopoly profits as rewards to consumers to make any antitrust action against them politically unpopular. It's a shakedown, and yes, getting rid of it would be bad for consumers, at least in the beginning. But it should be done anyway. reply abalone 11 hours agorootparentI agree except for the part about monopoly. There are in fact competing card networks. The bitter pill to swallow is that consumer preferences played a role in evolving this system. Card networks are managing a two sided market, and that means offering value to both the merchant and the consumer. Reward programs are examples of consumer value. If Visa decided to kill its “signature” interchange tier, those customers would move to MasterCard. reply dboreham 19 hours agorootparentprevI went to Stockholm last week for a couple of days, worried that I didn't have any local currency. It turned out that nobody takes cash, so everything worked out fine. reply astrange 12 minutes agorootparentDon't the public restrooms take coins there? reply ghaff 19 hours agorootparentprevEurope varies a lot from largely cashless to you will probably need cash if you want to have a beer. reply whimsicalism 15 hours agorootparentprevit's even more intense in China reply mym1990 11 hours agorootparentWhat is more intense than nobody taking cash, punishment for even having it? reply toomuchtodo 11 hours agorootparentYour ability to make purchases turned off, with no other means to transact. https://en.wikipedia.org/wiki/Social_Credit_System reply whimsicalism 10 hours agorootparentprevas in it is very difficult to find anyone willing to transact in cash for any transaction whatsoever in most chinese cities reply ponector 2 hours agorootparentprevAs the result, most of the reward programs in EU are meaningless: too much friction with almost zero benefits. Credit card in EU is only for rental cars. reply ajkjk 11 hours agorootparentprevIt only means that they haven't managed to improve it yet. reply lupire 18 hours agorootparentprev\"point in case\" is a funny term. Are you a mathematician or programmer, using \"case\" in the sense of \"branch of a proof\", not \"matter to be settled\"? reply iacvlvs 15 hours agorootparentI assume it’s a simple accidental transposition of “case in point”, an instance or example that supports, or is relevant or pertinent to, what is being discussed. https://www.merriam-webster.com/wordplay/usage-of-case-in-po... reply sofixa 57 minutes agorootparentIndeed, thanks, I've been using it wrong for months if not years... never realised I swapped it at some point in my mind. reply notpushkin 15 hours agorootparentprevThat's interesting: I've only seen \"point in case\" being used, never \"case in point\" (although it does make a lot more sense, now that I think of it). reply whimsicalism 15 hours agorootparentyou must not be a native speaker, i have never heard 'point in case' before this conversation. but fwiw, neither really make much sense to me reply 1letterunixname 13 hours agorootparentPedantic language shaming is uncool and other people's experiences are different from your own. Get over it. reply maxcoder4 12 hours agorootparentAs a non-native speaker I appreciate comments like GP, because they let me improve my English and speak better. It's not pedantic if the \"shamer\" is obviously right (I've consulted internet and it looks like he is). I think correcting other people speech is valuable because it lets us all understand each other better. reply whimsicalism 10 hours agorootparentprevif you read shaming in my comment, it was a misread. it's of course perfectly fine & great to not be a native speaker reply nsomaru 15 hours agorootparentprevThe idiomatic equivalent is \"case in point\" reply granzymes 15 hours agorootparentprev> Fraud costs are borne by merchants and to some extent processors, but not the issuer banks that receive the interchange fee. Merchant fraud and merchant credit risk is borne by acquirers (although, if they went under the issuing baking is ultimately on the hook). But fraud by the cardholder and cardholder credit risk is borne by the issuer. reply abalone 11 hours agorootparentYes, merchant fraud is what I was referring to with “to some extent.” And you’re correct that card-present fraud is more likely to be borne by the bank these days (this was not always the case). But typical e-commerce fraud is usually eaten by the merchant. reply fallingknife 16 hours agorootparentprevThere is a government to government payment fee category in there. Why on earth would two government agencies ever need to use a CC to pay each other and lose over 1% in fees? reply paulmd 10 hours agorootparentUnfortunately, as Lord Governor Supreme of a proud and prosperous micronation, I am disappointed to report that SWIFT does not recognize my sovereignty and regulatory authority. reply jasode 22 hours agoparentprev>But how far does this go in explaining why merchants \"choose\" to participate in this program. The obvious answer would seem to be that they get no benefit from the system as it exists but have no real choice, Some merchants like Amazon, Target, Home Depot etc do want the ability to refuse the \"rewards cards\" with higher fees but can't because of the current contracts they have for credit-card acceptance. If a merchant signs a contract to accept VISA cards, they must accept all VISA cards and therefore can't selectively choose to reject some VISA cards because of higher swipe fees. https://thepointsguy.com/news/retailers-want-to-reject-rewar... https://www.google.com/search?q=merchants+want+to+refuse+rew... reply TimPC 21 hours agorootparentThis is actually the reason lower fee cards exist. If every card had a 5% transaction cost no merchant would sign up for that card brand. If the merchant is convinced their average transaction cost will be lower because some of the cards will be cheaper you can get away with some expensive cards. reply ajdude 20 hours agorootparentAnd this is why many small businesses in my area don't accept American Express cards full stop. Some don't even accept Discover. reply mym1990 11 hours agorootparentI thought(maybe ignorantly) that a lot of merchants didn’t like Amex because it was so easy for customers to dispute transactions. reply kibibyte 28 minutes agorootparentThis wouldn't surprise me, given that Amex does have a consumer reputation for being very friendly for disputes. That factors in two ways: a successful dispute means that the merchant is completely out that money, and every dispute also costs something like $25 to handle regardless of whether the merchant ultimately wins or loses. reply i80and 18 hours agorootparentprevThis is interestingly regional: in the part of NJ where I live, every small business has an Amex \"shop local\" sticker on their front window reply Scoundreller 7 hours agorootparentUsed to be every year, Amex in Canada would run a \"Shop Local\" promo where you'd get $5 back on a transaction over $10 at smaller places x5 in a month, and let you look up who's a part of the program on a map. Then you'd visit a lot of them and they'd claim to not accept Amex! reply brewdad 16 hours agorootparentprevAmEx has worked hard to keep their home turf around New York City friendly to their cards. reply datascienced 3 hours agorootparentThey did this campaign in Sydney a while back. reply Iulioh 20 hours agorootparentprevHere in Europe I see AE accepted only for digital goods, high margin stuff or international (read American)chains reply sofixa 19 hours agorootparentAmerican Express has really targeted the French market, there are all sorts of small stores (like bakeries, pharmacies (not American pharmacies - only medicine and very closely related stuff like creams and diapers) and similar size) with proud \"Amex accepted here\" signs. There was even an Amex program a few years back giving 5euros back on transactions of more than 20 euros in small shops like that. reply JKCalhoun 21 hours agorootparentprevAre they bound by contract not to offer a discount for casher buyers? I ask because when I was in Germany (and, granted, this was a few decades ago) you got some percent off the price if you paid cash. Merchants there seemed pretty credit-card averse. reply bombcar 20 hours agorootparentThey used to be but the law was changed to make it illegal for the card companies to demand that they don’t offer a cash discount (or charge more for credit). Smaller businesses are doing that more and more since the pandemic to try to hold to their prices as long as they can. Big companies do a similar thing by offering you a store card. Costco likely makes more money from you when you pay with your Costco card than if you pay cash, because they get the interchange fee very very low and have to pay to handle cash. Rumor was AMEX was eating the interchange fee AND paying them … because they more than made it up by the customers who made the card Top Card. reply joshuaissac 15 hours agorootparentIn the EU, the opposite of this happened: the EU directive PSD2 made it illegal for merchants to apply surcharges on purchases using consumer credit and debit cards from early 2018. reply graemep 13 hours agorootparentI thought this was a UK govt being stupid thing. So it was the EU to blame! Given what this article says, it sounds as though not only are cash buyers cross-subsidising card buyers, but non-rewards card buyers are cross subsidising rewards card buyers. So much for free markets. reply red-iron-pine 20 hours agorootparentprevThat's still a thing in Canada and the US in various places, generally \"mom and pop\" run business. Credit card fees for small orgs are like 1-2% so for a small biz that could pinch. Cash also lets you, uh, \"fudge\" your numbers for tax purposes. reply i80and 19 hours agorootparentThere's a good sushi place near me that gives a 10% discount for cash. I'm fascinated by this. (I still use a card because life is short) reply Tijdreiziger 18 hours agorootparentThat may just be good old-fashioned tax evasion. (If you pay cash, the business owner can just pocket the money without ever recording the transaction. For digital payments, this is much harder to do undetected.) reply photonbeam 18 hours agorootparentprevUnrecorded cash payment makes it much easier to pay undocumented staff reply lupire 18 hours agorootparentprevIt's tax evasion. reply 725686 16 hours agorootparentFirst comment I see that addresses this. This is a big part of accepting and paying cash. Tax evation. From both sides. reply bonzini 10 hours agorootparentprevWhere I live there's a mom and pop (well, mom and daughter) shop that sells relatively expensive clothes. They stopped accepting Amex because fees were too high, yet they basically apply a 5-10% discount by default if you spend enough... reply JosephGuerra 11 hours agorootparentprevYeah I often see small print on the fuel cost per gallon as \"cash price\" .. I assume this happens mostly on low-margin products. reply mminer237 20 hours agorootparentprevThey used to but got sued over it, and in 2012 settled it partially by ending the practice: https://www.barclaydamon.com/alerts/What-the-Credit-Card-Set... reply Spooky23 20 hours agorootparentprevThey can do it now. In the past you had to offer same prices, although you could negotiate. It’s usually motivated more by mom and pops skimming taxes than 3% credit card fees. If you do any kind of volume, there isn’t a ton of savings as cash management ain’t free. The electronic equivalent is people who take personal Venmo at retail. reply brk 20 hours agorootparentprevAll the CC contracts I have ever seen only prevented you from discriminating against a particular card/brand if you took credit cards. You could offer a cash discount as a policy, but you couldn't charge AMEX holders an extra 1%, as an example. reply alexchamberlain 12 hours agorootparentprevIt's pretty expensive to take cash. First, you have to keep a float of money for change. Then you have to secure the cash until you deposit it in the bank. Then sink some time into counting and depositing it, before being charged by your bank for depositing cash. That must very quickly add up to a percentage point or 2. reply PeterisP 8 hours agorootparentThe issue is that these costs of handling cash are largely fixed - marginally, if your store takes $100 in cash, your cash-handling expenses and effort stay the same, but if you take $100 by card, your fees directly increase. You avoid the expenses of handling cash only if you don't take cash at all. reply gtirloni 21 hours agorootparentprevWhere I live it'd used to be possible to get a discount if you paid cash but it was deemed discriminatory against card holders and so it was banned. It's the same price regardless of payment method. reply posix86 20 hours agorootparentIt removes the incentive for clients to use cash if a card option is available, makes them more used to paying by card, and hence decreases the competitiveness of merchants that don't offer card at all - until cards become so wide spread that many merchants don't even accept cash at all, like where I live. A bit hostile towards merchants, but very nice for consumers imo. reply oarsinsync 11 hours agorootparent> A bit hostile towards merchants, but very nice for consumers imo. It's nice for consumers who prefer to pay by card. It's hostile towards consumers who prefer to pay in cash. reply posix86 10 hours agorootparentCall me close minded but i see cash as an archaic, strictly inferior way of managing money. But to each their own. reply consp 20 hours agorootparentprevWhich is the reason the cut is now capped and card acceptance is higher, even for credit cards (0.3% for cc, 0.2% for dc). Though low-margin businesses like grocery store still don't accept them (credit cards) due to the marginally higher fees in some countries. reply morpheuskafka 18 hours agorootparentprevThat would cause a massive customer support and frustration problem as regular customers don't know or care how their card is classified and would complaint that it doesn't work. This would affect both the merchant and the issuer negatively. reply ensignavenger 19 hours agorootparentprevWhat I want to do is pass on the exact processing fee to my customers, then they can choose their payment method based on how much it is going to cost them. I might then choose to cover a portion of the fee for electronic transactions, because they mean I save money vs processing cash. But the customer would pay the excess. I would need a system that can display to the customer what fee they would be charged with their selected payment method, and be given an option to switch to a less expensive payment method. reply ryandrake 19 hours agorootparentIt's already impossible in the USA to know what you're going to pay for something until you get to the checkout line. This just makes it worse. Why not go even further? Itemize the marginal cost of maintaining your property's parking lot for those customers who visit your business by car? Charge customers a \"store heating fee\" in the winter? Customer support fee if they talk to anyone? Just as ridiculous. Processing credit cards is just one of many costs of doing business that you need to account for when you price your products. reply miki123211 18 hours agorootparent> Itemize the marginal cost of maintaining your property's parking lot for those customers who visit your business by car This isn't that uncommon, quite a few places in Europe do this actually. Usually there's a barrier at the parking lot entrance. You get a ticket when entering, which you then have to put in a machine when leaving. The machine calculates your fee based on how long your car was parked. Modern systems are far more automated and use license plate readers instead. reply ensignavenger 19 hours agorootparentprevI can give discounts to customers however I want to, thank you. As long as the customer knows what card they are going to use, they know exactly what the cost will be. I am all for posting sales tax rates as well as any transaction processing fees at the door, too, though. reply zoky 11 hours agorootparentNo thanks, I’m just gonna go to the grocery store down the street where they don’t make me do linear algebra just to buy a box of Froot Loops. reply lupire 18 hours agorootparentprevThey eliminates your ability to advertise prices honestly, without overwhelming your customers with detail that will distract them. reply ensignavenger 17 hours agorootparentI will clearly state which payment methods are free and which ones the customer will pay a conveniance fee or recieve a discount if they use. I will clearly show the customer at checkout what their total is with their chosen payment and shipping (if applicable) options would be, and give them the opportunity to change their selection. It is about customer choice and convenience. You are welcome to shop somewhere else that bundles these expenses into the price of the item and givea you no choice in avoiding them. reply brewdad 16 hours agorootparentI can only speak for me, but if I have to do that much mental math to figure out the best options to pay or see a bunch of random (to me) fees and adjustments at checkout, I'm leaving and going somewhere else. It feels scammy even if your goals are noble. reply AnthonyMouse 13 hours agorootparentIf the sign said you pay 5% for AmEx, 4% for Visa and 0% for cash, you don't have to do any arithmetic to see which number is smaller. reply ThunderSizzle 12 hours agorootparentWhat's the fee for counterfeit cash? Or illegally gained cash? What's the fee for cash the requires quite a bit of change? What's the fee for cash that is composed of very small values (e.g. quarters and dimes for a tv)? It's interesting that all of these complications are ignored entirely when pricing payment methods. Handling cash isn't free and carries risk. reply AnthonyMouse 12 hours agorootparent> What's the fee for counterfeit cash? Or illegally gained cash? What's the fee for credit card chargebacks from stolen cards, or chargeback fraud from customers who receive a product and then dispute the charge anyway? > What's the fee for cash the requires quite a bit of change? There are machines that count coins and issue exact change. If you do a lot of cash business, you buy one. For example, the self-checkout machines at Walmart do this. The largest denomination still issued for US cash is $100, so no cash transaction will require more change than this, and some merchants don't accept large bills for small transactions. > What's the fee for cash that is composed of very small values (e.g. quarters and dimes for a tv)? How often do you think that actually happens? Also, how are costs like this to be avoided unless you stop accepting cash whatsoever, and thereby lose business? Just eating the credit card fees instead of passing them on isn't going to stop someone with a jug full of nickels from wanting to spend them. > Handling cash isn't free and carries risk. Nothing is free. How about the time value of money for the time it takes for the credit card companies to pay you, as opposed to cash which you can immediately spend or deposit and begin collecting interest on? The issue is that credit card companies have the usual set of costs and then on top of that charge significant transaction fees and shift the cost of various types of fraud to the merchant even though they're the ones who designed the system that makes it easy to carry out. reply briandear 15 hours agorootparentprevOr just make your prices have the credit card fees built in just like normal businesses. Your system is a ton of work for very little benefit to anyone and it’s probably lowering your revenue and it’s as annoying as places that charge for takeout containers — missing the point that takeout containers encourage people to buy more food. It’s an unsophisticated approach to business. That’s why it’s common in mom and pop places — mom didn’t go to business school and pop is tripping over dollars to save a nickle — they see the “expense” but they can’t see the revenue that aren’t getting. There is a reason most small restaurants fail — they don’t know how to be more profitable. So they start to add on these little fees when they begin to struggle and don’t realize that they’re making the problem worse. In other words, most people that start restaurants don’t know what they’re doing in the back office even if they’re great in the kitchen. Restaurants fail for many reasons, but not controlling costs is the biggest — however, they’re naïvely choosing to control the wrong costs. One of my good friends owns a chain of 30 Tex-Mex restaurants in Texas. After several burglaries of the safe at several locations, they went cashless at some locations. The cashless locations, without exception, saw sales increase by over 12%. He quickly made all of his stores cashless and sales increased among all stores. My anecdote isn’t data — but there is plenty of data out there. The credit card surcharge scheme is endemic among small business owners who actually haven’t done the math. Or, more accurately, they’re doing the wrong math. Credit card users spend more than cash users. So you make up for the “savings” with lower sales volume. And credit card users that have to pay higher prices will go elsewhere. Or, if they pull out cash, they’ll spend less of it. https://www.nature.com/articles/s41598-021-83488-3 reply AnthonyMouse 12 hours agorootparent> Your system is a ton of work for very little benefit to anyone It has the obvious benefit of discouraging people from increasing your costs with rewards cards. Keep in mind that businesses often have net margins in the vicinity of 5% of revenue. Paying 3% of revenue or more to the credit card company is heinous. And what work is there to do? The calculation is done by a computer. If your attitude is \"just price in the cost\" well then there you go, you can pay the true cost of the card you have without even looking at the alternatives. Whereas if you want to get the lowest price then you have to figure out which card has the lowest price. Removing your option to do this is not going to save you money, it's just going to cause you to pay the higher price at all times, which you still have the choice to do. > they see the “expense” but they can’t see the revenue that aren’t getting. Increasing sales by e.g. 15% while decreasing net profit by up to 60% is often not a fantastic business decision. > Credit card users spend more than cash users. Which is why you accept them but provide a discount for the people who don't use them and thereby don't expose you to their fees. > And credit card users that have to pay higher prices will go elsewhere. But they're not higher prices. Restaurant A charges however much to everyone. Restaurant B charges exactly the same amount to people who use credit cards and a few percent less to people who pay cash. The people who insist on using credit cards can go to either place and pay the same amount, anyone willing to pay cash can get a discount at Restaurant B. reply quasse 17 hours agorootparentprev... So just like US sales tax already does? I'm not saying it's a good system, but I'm already paying some random amount on top of the sticker price for everything I buy, depending on what state, county, and city I'm in at the time. reply CydeWeys 13 hours agorootparentprevCan't you just print up a sign for this? I've seen signs that say stuff like \"Prices listed are for cash, 3% surcharge for credit cards.\" How many different fee tiers are you as a retailer really trying to charge? reply carlosjobim 16 hours agorootparentprevDo you actually want to do that? When a customer wants to make a purchase, I think it is best to get the hell out of their way and not go stand between their wallet and your wallet, blocking their money and yapping about fees or some other completely unimportant stuff. reply i80and 19 hours agorootparentprevSome donation platforms do this! They show and add on the processing fee for VISA/MC and Amex separately, and Amex is a little higher. reply ensignavenger 18 hours agorootparentDo they account for the different interchange rates on different classes of cards within a processing network? Not every merchant will want to be that granular, but I think it is worth it for big ticket items. reply tzs 13 hours agorootparentI don't think most merchants have the option to be that granular. I believe that most payment processors only provide a simplified view of interchange rates (and other fees) to all but very large merchants. For example Braintree is 2.59% + $0.49/transaction, plus 1% if non-USD and 1% if the card was issued outside the US. They used to split transactions into two tiers, based on how much the actual interchange and other fees were, and you monthly bill as a merchant would show charges for each tier. Within each tier the charge to the merchant was a fixed percentage and a fixed transaction fee. There was no good way for the merchant to know ahead of time which tier a given charge would fall under, although they could guess that a high rewards card was more likely to fall into the higher fee tier. reply briandear 15 hours agorootparentprevThey should A/B test this. My theory is that this creates some amount of friction. I was all set to donate $100, but now I have to go find my debit card or figure out some bank transfer nonsense and by the time I get back (if I get back) my motivation to donate has diminished. Donation platforms ought to do some analytics on this. If I’m giving money, I want it to be simple without requiring me to second guess. If a donation platform takes Apple Pay, the friction is even less. I don’t want to fill out your stupid long form, create a password, share my address or whatever. But, I’m one person — this might be an interesting Masters thesis topic to study the behavioral economics of donation platforms. reply astura 13 hours agorootparentWhat I've personally seen is a checkbox, you put in your donation amount and then you can check/uncheck \"cover processing fees.\" Found an example - https://donorbox.org/nonprofit-blog/donors-to-cover-processi... reply kasey_junk 21 hours agorootparentprevLarge merchants also pay much less for interchange generally. reply bombcar 20 hours agorootparentSmall merchants negotiate with Stripe for a flat fee to accept all cards. Big merchants negotiate with the networks and pay varying prices for varying rewards amounts (or however they get the deal structured). reply scarby2 20 hours agorootparentInterestingly my flat fee with stripe is less than my reward rate on my credit card for some categories. It obviously is against the contract terms and probably considered fraud but I could theoretically make about 1.4-3 cents on the dollar (depending how you value points) by charging myself money. reply ska 15 hours agorootparentThis comes up in the article. reply scarby2 7 hours agorootparentkinda, the article mentions that certain cards allow you on certain categories to get more than the interchange rate. what they don't mention in the article is that square charges the same fee to the business whatever kind of card is used, in this case whenever you use an amex card it's less profitable for square. If i were using a more traditional processor they may pass this cost on to me. also milage reward points can hit as high as 2.3 cents per dollar so assuming i want to fly to japan with ANA i could in theory get 8.6c in value of MR points per 2.6c i charge myself. However i would not expect to get away with this and i'm sure square would figure this out. reply ska 4 hours agorootparentHe also mentions the possibility of adversarial collaboration, creating a scenario similar to yours… reply illusive4080 21 hours agorootparentprevIf only Walmart would take contactless payments. reply yoshamano 20 hours agorootparentIt has more to do with making Walmart Pay the only contactless option to drive adoption of their mobile app. reply ghaff 20 hours agorootparentprevI confess that I don't understand what the big deal is. It takes 5 seconds to slide the card into the machine. Personally, I find fumbling with my phone takes longer as does figuring out where the reader wants to tap the card if I'm not familiar with that particular store's system. reply lolinder 19 hours agorootparent> Personally, I find fumbling with my phone takes longer as does figuring out where the reader wants to tap the card if I'm not familiar with that particular store's system. Aren't both of these just symptoms of unfamiliarity with the tech? I resisted phone payments for a while, until one day I forgot my wallet and quickly added a few cards to my phone. Now I'm severely tempted to use it more often—my phone has a wallet button on the lock screen that jumps me straight there ready to pay with my default card. I've definitely experienced some friction the two times I've used it, but it seems pretty clear that that friction is temporary while I'm still becoming familiar with it. reply seanw444 19 hours agorootparentprevEven then, I can tap with the card, which tends to register faster than inserting too. No garbage proprietary software required to be installed on my phone, and I still get contactless. reply mdaniel 17 hours agorootparentprev> It takes 5 seconds to slide the card into the machine. It sure does, and then 45 seconds while the machine ... thinks about life, and then 15 seconds for it to say \"chip read error, reinsert card\" and then another 45 seconds for it to reconsider the nature of reality, and then listening to a fire alarm sound that they chose for the success alarm. Excellent UX, no notes reply StevePerkins 12 hours agorootparentprevSame, and I note that the OVERWHELMING majority of other customers that I see at the grocery store or Target are still inserting plastic credit cards into the readers (I do think swiping is going extinct though, as the readers push you to insert instead). However, this is HN and not at all typical of the U.S. or world overall. Even though we frequently lose sight of that. reply sofixa 19 hours agorootparentprev> I don't understand what the big deal is. It takes 5 seconds to slide the card into the machine For most people, there's the time to get their wallet or equivalent out of pockets or purses, fiddle to get the card, put it the correct way and swipe (but not too fast or too slow!). Vs a phone/watch tap which is usually much more convenient. reply ghaff 19 hours agorootparentI guess it's what you're used to. I have a small wallet I carry in a front pocket, haven't had to swipe a card in ages, and it takes 5 seconds to insert the card. Maybe if I wore my Apple Watch more, I'd get used to using it but the card just seems more straightforward in general. Maybe I'll insert and maybe I'll tap. I'm pretty indifferent. reply kingrazor 16 hours agorootparentprevIt would take me just as long to get my phone out of my pocket as it would to get my wallet out. Plus a lot of machines have tap pay now if your card supports it. reply sofixa 1 hour agorootparentBut you then have to open your wallet, and slide the card out. With the phone, if it's out, it's unlocked and ready to be tapped. reply illusive4080 17 hours agorootparentprevI use it on my watch. Double click the side button even when my watch is under a jacket and just hold it in the vicinity of the reader. It’s very easy once you get used to it. reply aembleton 19 hours agorootparentprevIt means you've got to take the card with you. reply ghaff 19 hours agorootparentSome of it may be that if I'm in a store, I've almost certainly driven there so I probably have my (small) wallet with me. reply wincy 18 hours agorootparentprevI use Walmart pay with their app. But then again we’ve totally given up buying off Amazon and do grocery pickup or delivery from Walmart. For 90% of items this is faster than we’d get it from Amazon. reply brewdad 16 hours agorootparentprevHow often do you find yourself somewhere with your phone but not your wallet? reply BenjiWiebe 8 hours agorootparentI always have my phone in my pocket. My billfold is stored in my car, and I do occasionally forget to put it in my pocket when going in to a store. Plus my billfold is thicker than my phone so it's less fun to store in a pocket. reply tcmart14 18 hours agorootparentprevI like contactless just because for some reason, my cards always get beat up and the chips become problematic on my cards after about a year. They just sit in my wallet. But half the time I go to pay with my card, I have to dip twice because the first time, the chip reader always says it is unable to read the chip. I've also had issues at Walmart where I know some lanes to flat out avoid because the chip readers will always reject my card for unable to read the chip. With my phone, this isn't an issue. Even if I get a new card, wait 8-12 months and its the same problem again. reply turtsmcgerts 11 hours agorootparentYou know what would be more durable - a 4'' solid aluminum emv chip shaped coaster. Check us out at aluminum.finance pw 'aluminum' and let me know what you think! reply lupire 18 hours agorootparentprevYou can do contactless on your card. reply tcmart14 18 hours agorootparentIf your bank issues you a card with it. I got a new card about 6 months ago, doesn't support contactless. But I am aware that the cards exist and I am not opposed to it. With contactless I am fine with it being on my phone or card. But I gotta have a card that has contactless to be able to use it. reply ambichook 9 hours agorootparentare cards that dont have paywave or whatever really that common in the US? my bank cards have both had it reply BenjiWiebe 8 hours agorootparentI'm in the US and I'm pretty sure all 5 (6? IDR) of my credit cards have supported tap-to-pay for a couple years now. reply jen20 21 hours agorootparentprevPresumably they do not because they want to track you via your credit card number, and permitting Apple Pay (maybe others too) would hinder that. reply vel0city 20 hours agorootparentIt's more that they want to try and convince people to pay with Walmart's own lower-cost (to them) app if you want to do contactless payments from your phone. If they made it easier to use Apple Pay, why would anyone ever use their app? reply tzs 12 hours agorootparentWhy would paying with the Walmart app be lower cost to Walmart? Both the Walmart app and Apple Pay are essentially, as far as payments go, just digital wallets that you can store credit card information in. When you use them it is still a charge to one of those credit cards. It might actually cost Walmart more in my case, because the card I have on file has larger rewards for online purchases than it does for in-store purchases, and the Walmart app processes purchases as online even when made in-store. If I pay with the physical card it is processed as in-store. reply bombcar 20 hours agorootparentprevApple Pay is just as traceable if they want it to be. They’re just stubborn and want people to use their option (and card, if possible). As it is you can load any credit card into the Walmart app and pay by pointing the “check price” barcode camera at the screen. reply jen20 19 hours agorootparent> Apple Pay is just as traceable if they want it to be. [Citation Needed]. On the other hand, stores like that probably already do facial recognition on customers, so it really is just intransigence to not allow contactless payments. reply bombcar 16 hours agorootparentSee https://birchtree.me/blog/digital-wallets-and-the-only-apple... - the DPAN stays the same at a particular merchant, so Walmart sees all but can't necessarily directly \"know\" what you're buying at other stores. reply andrewaylett 17 hours agorootparentprevApple Pay and Google Pay have their own virtual numbers, rather than using your regular card number, but the number doesn't rotate. For example, our local bus company can quite happily offer capped daily and weekly fares when folk use the same device to pay. reply jen20 15 hours agorootparentYour local bus company isn't trying to correlate your activity across multiple retailers to try to squeeze an extra few cents of value from you, though. reply supertrope 16 hours agorootparentprevThe device account number does not rotate with every transaction. You have to unlink your device from your credit account and re-enroll to do that. reply illusive4080 21 hours agorootparentprevI presume the same. reply c0wb0yc0d3r 20 hours agorootparentI always rate my experience 1 star when checking out at Walmart because of this. Probably won't change anything, but I feel I can't just stop going to Walmart because then Amazon is the only place left. Which is also fucked up but let's stay on topic. reply matthewdgreen 22 hours agoparentprevThe only thing I would add to your comment is that merchants aren’t the ones being forced to pay these stupid fees, it’s their customers (and primarily their poorer and often non-card using ones) who are being quite heavily taxed to fund a marketing scheme for rich customers. Most competitive businesses can’t afford to fund such an elaborate targeted marketing campaign directly out of their fees without some competitive pushback: hence the actual question you should ask is why the entire system exists, and the answer has to do with a pile of inefficiency and rent collection based on regulatory capture. reply judge2020 20 hours agorootparent> The only thing I would add to your comment is that merchants aren’t the ones being forced to pay these stupid fees, it’s their customers (and primarily their poorer and often non-card using ones) who are being quite heavily taxed to fund a marketing scheme for rich customers. Counterpoint: i will pay you $500 if any of the big retailers (>2k stores) lowers prices now and cites \"lower credit card fees means we can charge less\". reply supertrope 17 hours agorootparentBecause of the stickiness of prices, passing through of cost savings usually manifest as slower inflation. Costco is rumored to have negotiated 0.3% from Visa in exchange for exclusivity. This is part of how they are able to sell goods at thin markups. Aldi USA used to only take debit cards. They caved and now take credit cards. Travelers Insurance offers two prices on every quote: by bank account or a higher one by credit card. http://www.bloomberg.com/features/2015-how-amex-lost-costco/ reply judge2020 17 hours agorootparentMost insurances do charge lower fees for direct deposit - Allstate does it at well. reply rqtwteye 12 hours agorootparentComcast too. I get five dollars off per month for direct deposit. I believe it’s less about fees but more about avoiding chargebacks. reply bombcar 20 hours agorootparentprevDo you think prices would remain the same if interchange fees doubled? reply ryandrake 19 hours agorootparentIf the company could profitably charge more for their products, they already would be, regardless of what interchange fees (or other costs) were. reply rqtwteye 12 hours agorootparentDuring the current inflation companies have used inflation as excuse to raise their prices way above inflation. It may be the same here. Publicly state that interchange fees are the reason for the increase but raise the price by way more than the increase in fees. reply watwut 15 hours agorootparentprevRaising costs means they need to charge more somewhere or lower transaction. Unless in monopoly situation, price pressure goes both ways. If competitors can lower prices, whole market eventually ends up having to do that. reply gosub100 22 hours agorootparentprevWhile we're in this topic: why is unsecured credit card debt NOT tax deductible but secured debt like HELOC is? Fwiw I don't really care what the technical reason is, it's a rhetorical question to add to the ways the credit system holds back the poor. reply keltex 21 hours agorootparentYou used to be able to deduct any consumer debt. But that stopped in 1986. The reason was \"Congress believed deductions for personal interest encouraged people to consume and stifle savings.\" https://www.telegram.com/story/news/local/worcester/2007/03/... reply ghaff 20 hours agorootparentThere used to be mostly piddling deductions for all sorts of things that you don't have today. It's probably mostly a positive to not keep track of things like sales tax in order to minimize your income tax. reply bombcar 20 hours agorootparentDeductions also cause people to go temporarily insane - it’s fine paying 5% unnecessarily because I get 2% back on my taxes! Ignore the 3% that is gone forever … reply lupire 18 hours agorootparentprevSales tax deduction still exists. 2017 law temporarily lowered but didn't eliminate it. reply brewdad 16 hours agorootparentIF you itemize. 90% don't. Also, you can only deduct sales tax OR state and local income taxes but not both. reply bombcar 20 hours agorootparentprevEven then I’m not sure how much it affected the poorer people, since you still had to overcome the standard deduction (lower, sure). The deduction on mortgage interest now mostly only affects the well-off because the standard deduction for married filing jointly is so high. reply bonton89 16 hours agorootparentThe the 1980s the deduction for mortgage interest would have been significant, I suspect most people itemized then. And the huge standard deductions we have now are a fairly recent tax change. reply bombcar 16 hours agorootparentIt was 3400 for a married couple in 1980, but yes, many more people deducted mortgage interest until 2018 when the standard deduction nearly doubled. Many people didn't math the interest deduction correctly - only the amount over the standard deduction should be counted, as you'd get the standard deduction anyway. Can vary depending on SALT and charity donations. reply bonton89 15 hours agorootparentYes, real estate tips always seemed full of self serving nonsense to me and usually talked big about the tax savings. But with my modest house and mortgage I calculated that even at the start of my mortgage I saved a whopping $800/yr on taxes with deductions versus the standard deduction. I spent well more than that on furnishings and repairs every year. And that tax advantage (for me) disappeared entirely after maybe 5 years or something. For a long time I would read money saving tips articles and a tip that frequently came up in these and in pro real estate pieces was that paying a single extra mortgage payment per year would let you pay your mortgage off 12-15 years early. When you did the math you realized it was bogus. As near as I can tell, this actually was true for one brief period in the 1980s when mortgage rates were at their zenith. Articles were written about this amazing tip well into the 1990s before the BS was transcribed to the internet and then repeated well into the super low interest rate era where it wasn't even close to being true. Of course it wasn't even really true in the 1980s either. Once interest rates went down you'd have been wise to refinance at the newer lower rates instead of prepaying the mortgage as it would improve your cash flow and lock in the savings. So in reality, it would have only made sense if interest rates had stayed at that same high rate for the whole length of the mortgage! reply toyg 21 hours agorootparentprevWhich is kinda stupid, when the economy is based on people consuming. I guess Reagan's friends at that point wanted more money for Wall Street to gamble. reply gottorf 19 hours agorootparentprev> encouraged people to consume and stifle savings Ironic in hindsight; every monetary and fiscal policy as of late seems to be designed to punish savers and reward debtors. reply kevin_thibedeau 14 hours agorootparentHard to extract profit from the fiscally responsible. reply Spooky23 21 hours agorootparentprevIt was! Auto leases were deductible too which was a big subsidy for the auto industry. Once rich people figured out how to get poor people to be angry about things like higher marginal tax rates for rich people and “death taxes”, we raised taxes on the suckers to benefit the richer people. reply gruez 21 hours agorootparentprev> While we're in this topic: why is unsecured credit card debt NOT tax deductible but secured debt like HELOC is? AFAIK it's a carve out specifically for houses. Car loan interest isn't deductible despite being \"secured\". reply judge2020 20 hours agorootparentIt's a bargaining chip to get votes. If one party promises a larger tax saving on homes, then homeowners are more likely to vote for them. reply dboreham 19 hours agorootparentBesides that, there may be some societal benefits to increased home ownership. reply ghaff 19 hours agorootparentAs I understand it, mortgage interest deductibility was originally basically social policy (rightly or wrongly). And you can't take a benefit like that away--even if the government kind of did for most people by increasing the standard deduction. (Which was probably not a terrible way of doing so.) reply nfriedly 21 hours agorootparentprevGranting a tax credit for something encourages that thing. So, from that perspective, I think it makes some sense to grant a tax credit for mortgage interest but not credit card debt. reply edwr 21 hours agorootparentTax credit for mortgage interest encourages speculative investment in the housing market. Tax credit for credit card debt encourages consumer economic activity. reply red-iron-pine 20 hours agorootparentyou have to live somewhere. maybe you rent, but renters DGAF about the local community the way that homeowners do. home (property) taxes also fund a lot local services, schools, etc. you want prices higher and stable, and not dominated too heavily by mega-corps that will weasel out of paying said taxes. reply i80and 18 hours agorootparentI've rented in my town for ten years, and I care about it a lot thank you very much. Renters absolutely care about the local community. reply mwexler 21 hours agorootparentprevThough one can write off losses from gambling in the US. I wish the tax code had more a more positive slant per your point, but lobbying seems to be a bigger driver. reply ScottEvtuch 21 hours agorootparentPretty sure you can only write off gambling losses to offset gambling winnings, which entirely makes sense. That way you only pay taxes on your net winnings for the year. reply gosub100 19 hours agorootparentI can confirm this. reply jjav 13 hours agorootparentprev> While we're in this topic: why is unsecured credit card debt NOT tax deductible but secured debt like HELOC is? HELOC interest is rarely deductible either. First, you now must be able to itemize deductions, which the recent tax changes have made very unlikely. Less than 12% of tax returns are able to itemize: https://www.irs.gov/pub/irs-soi/soi-a-inpre-id2303.pdf In addition, even if you are in that ~11% who can itemize, HELOC interest is only deductible if you use it to work on the same house being used to get the LOC. Any other use is not deductible. reply sgerenser 16 hours agorootparentprevHELOC debt is (since 2017 TCJA) now only deductible if used to purchase or upgrade/repair the house. And now the vast majority of people are not going to be deducting any mortgage or HELOC debt anyway, since the standard deduction is so high now. reply scarface_74 20 hours agorootparentprevTo a first approximation, no one actually takes advantage of the mortgage tax deduction any more because the standard deduction is so high and the cap on state tax/property taxes is so low that most people don’t itemize. Only about 10% itemize. https://www.taxpolicycenter.org/briefing-book/what-are-itemi.... reply ghaff 20 hours agorootparentAnd the majority of those make more than $500K. The deductions are probably mostly some combination of mortgages on very expensive properties or very large charitable contributions, probably often tax-shielded in some manner. reply mcguirep 17 hours agorootparentYou can’t deduct interest on personal income tax for mortgages over $750,000, so it seems somewhat unlikely that such deductions make up any significant amount of the deductions: https://www.irs.gov/publications/p936#en_US_2023_publink1000... reply junar 12 hours agorootparentThe $750,000 amounts to a cap on the deduction. If your mortgage is in excess of that amount, you would pro-rate the deductible interest accordingly. I think it's reasonable to say that the folks with the largest mortgages are the ones most likely to itemize mortgage interest, even if they are capped. reply robertlagrant 21 hours agorootparentprev> the ways the credit system holds back the poor It would be good to understand this better. Doesn't everyone use a credit card? Not just the poor? Who are the poor in this case? Are tax deduction rules anything to do with the credit system? reply gosub100 21 hours agorootparentAre you aware that merchants charge more for goods and services so they can offset CC merchant fees? Even a cash paying poor person who cannot get a CC is paying for this. My comment is to show another way this is perpetrated. People saddled with CC debt could dig themselves out faster if they could write off interest. reply robertlagrant 17 hours agorootparent> Are you aware that merchants charge more for goods and services so they can offset CC merchant fees? Well, some places add on a fee, but yes, agreed, some places apply a blanket charge. I don't see how this relates to the tax deduction. > Even a cash paying poor person who cannot get a CC is paying for this This is about a tax deduction. Are you saying someone who can't get a credit card is going to be meaningfully affected by a tax deduction? > People saddled with CC debt could dig themselves out faster if they could write off interest. This is true, but also the giant number of people who just chose to get into credit card debt would be paying less tax. If you want to make credit cards into effectively interest-free loans then that might cause issues. reply gosub100 16 hours agorootparent> but yes, agreed, some places apply a blanket charge. I don't see how this relates to the tax deduction. Earlier you asked me to explain how credit cards harm the poor. I showed you, and now you complain that it doesn't pertain to a tax deduction. It's not about a singular thing. The singular thing was one token example of a larger theme that for some reason you refuse to acknowledge. reply NovemberWhiskey 21 hours agorootparentprevPeople who are encumbered by credit card debt are usually people who would still be better off taking the standard deduction these days. reply bombcar 20 hours agorootparentExactly. It sounds harsh but those encumbered with usurious credit card debt would be better served by being forbidden from having it at all; but that’s a position strongly fought against on all sides. reply op00to 9 hours agorootparentprevAre you aware merchants charge more for goods and services so they can take returns? Even a perfect consumer who never returns items is paying for this. reply lotsofpulp 21 hours agorootparentprevThe population likely to use HELOCs votes more and/or is more populous, so they have more votes. Same reason Medicare (old people) pays healthcare providers more than Medicaid (young and poor people). reply scarface_74 21 hours agorootparentThe real estate lobby is the largest lobby in the US. reply miroljub 22 hours agoparentprevIt's the legislation that disallows vendors to have different pricing based on the payment system that disaligns the incentives. If I have a card that gives back 2% to me, back causes 5% fees to the vendor, both of us would be better off if I used a card with 1% fee, and the vendor gives me 2% discount. Unfortunately, not allowed. reply kasey_junk 21 hours agorootparentIt’s been mostly legal to charge credit card surcharge fees since 2013. https://www.lexology.com/library/detail.aspx?g=5c6e1264-42a8... The real reason that most merchants don’t charge surcharges is that they don’t want to lose the sale, calculating the actual interchange is wildly complex and in general they prefer cards to cash. reply pkaeding 20 hours agorootparentThere are definitely a good number of small businesses around me (cafes, and similar places) that offer a cash discount. They don't have a sign offering it, but when I pull out cash, they revise the price down. reply bombcar 19 hours agorootparentMore and more small places around here are explicitly offering the discount either with a “3% less if you pay cash” but more commonly now a “listed prices are cash or debit, credit pays more”. It’s a noticeable fee for them. reply Danjoe4 19 hours agorootparentprevIt is against VISA ToS. Small businesses can risk it but large businesses would get sued and VISA would refuse to do business with them reply sgerenser 16 hours agorootparentNope, not against VISA ToS. Post you're replying to indicates it's no longer \"illegal\" but it was never actually illegal to charge a surcharge, it was just disallowed by ToS. This is no longer the case as of 2013 and retailers are now free to charge a surcharge if they so choose. reply bluGill 21 hours agorootparentprevIs it legislation or the contract with the credit card? My understanding is the contract to take ie VISA has terms that you cannot apply a discount for customers using other payment methods (ie cash or someone else's card). There are a few places that don't have those terms (mostly government where often a card does cost more to use). What people forget about these fees is a credit card is cheaper to take for the merchant. The credit card is never counterfeit money. The clerk never takes money from the credit cards, nor does the manager counting it (I wasn't in retail long but I saw both). You never have a robber come in to take your credit card money. Even when all goes well, you don't pay the clerk and manager by the hour to count all the cash twice. You do have some risk of taking a stolen credit card, but overall it is cheaper for the merchant to take credit cards and that savings should be what pays for the card costs (I have no idea how to count the different costs to see if that is true) reply bombcar 19 hours agorootparentThe real advantage for merchants is to take debit (assuming the payment is high enough). Much of the benefits of credit without the hassle of cash. reply dwighttk 20 hours agoparentprevCredit card processors actually provide a service for both their cardholders and the people who accept their cards… Yes there is the downside for businesses when the processors reverse charges but if this was big enough of a downside then people would stop accepting the card. Yes sometimes people get their number stolen and are out the money for a while during an investigation, but again if this downside were big enough people wouldn’t use that card anymore. Yes there are new types of fraud enabled by the technology. The big benefit is you don’t have to have liquid cash sitting around where people can grab it and disappear. Some merchants don’t accept some cards… they’ve decided that the cost outweighs the benefit. My grocery store fought against accepting Apple Pay and they do now. Walmart doesn’t. reply thfuran 5 hours agorootparent>Yes sometimes people get their number stolen and are out the money for a while during an investigation One of the main benefits of credit cards over most other forms of payment is that that isn't the case. A fraudulent transaction on a credit card ties up some of your credit limit during resolution. A fraudulent debit card transaction or personal check takes money out of your account. Of course, if you wait long enough, you may have already paid the bill containing the credit card transaction and then you're in the same boat. reply twoodfin 22 hours agoparentprevThe answer is that the system doesn’t work if a 3%-fee card isn’t held by a low-risk, high-spend rich person. Indeed if that weren’t the case, merchants would reject the tiered fee structure. (This is also the answer as to why in the absence of regulation, exchange fees aren’t higher than they already are.) reply nordsieck 22 hours agorootparent> the system doesn’t work if a 3%-fee card isn’t held by a low-risk, high-spend rich person There's many rewards cards that require an annual fee (which encourages a high spend to recoup the fee with rewards). But there are plenty of 1.5%-2% cards with no annual fee. You just need a good credit score. reply scarface_74 20 hours agorootparentAlmost every card with an annual fee has enough credits and perks to offset the annual fee without spending any money. The second and third tier Delta cards come with a $250 and $650 Annual fee. The second tier card (Delta Platinum) has an annual fee of $350. But it comes with a $150 Delta Stays credit for hotels and one round trip an economy companion pass - basically buy one get one free - for any place in the US, Mexico, Central America or the Caribbean. The higher end Delta Reserve comes with similar benefits. But a first class companion pass. If you never use either card except for the credit, the benefits more than offset the annual fee. The Reserve also comes with airport lounge access I have three Delta cards just for those benefits. I could explain the Amex Platinum, Gold, Green, every cobranded hotel card, the high end Capital One cards the same way. The credit card companies as the article says are betting that the typical customer will use credit cards in a suboptimal manner. They are banking on most credit card users not to be like the typical r/creditcards users who carry 6-8 credit cards including “sock drawer” cards that are just held for the outsized benefits to annual fees and aren’t their primary cards. My wife and I travel a lot and yes I have nine cards and $2700 worth of annual fees. Most of those cards are “sock drawer” cards that are just used because the “coupons” make travel cheaper. reply ghaff 20 hours agorootparentYou need the right spending pattern and you need to manage the card rewards. When my travel went way down, I dropped a couple cards though I keep a pretty low-cost United cobranded card to basically keep me with some semblance of status. But things like airline club membership just weren't worth it any longer. reply scarface_74 20 hours agorootparentI agree completely and I didn’t even discuss the entire “churning” strategy where there is an entire cottage industry, a subreddit and a flowchart discussing how to get sign up bonuses and how to get the best return. (I am not affiliated with this site in any way) https://www.offeroptimist.com/ reply bombcar 19 hours agorootparentAt some point the maintenance becomes too complex and people just either use the card or eat the fee. They know exactly how many do this. The people who churn are just free advertising for them. reply scarface_74 18 hours agorootparentIs it worth it for someone who likes to travel, and wants to save for retirement and never wants to “work for a FAANG” (again) like in my case? I would say so. Just from the cash savings of my card setup, I would say it’s worth $3500 that offsets the $2700 in annual fees. Then take into account the points I earn from everyday spend is worth another $3000-$5000 depending on how I choose to redeem them (see r/awardtravel). Then take into account sign up bonuses and churning, I’m planning on doing over the next year worth around $5000. It’s the only way that I can balance our travel hobbies with my goals of maxing out my 401k including catch up contributions (I turn 50 this year), max out my HSA and not use it and “retire my wife” so she can enjoy her hobbies. This hobby isn’t just for people with above average incomes. If you are steeped in the culture, you can lean more toward churning and legal manufactured spending https://frequentmiler.com/manufactured-spending-complete-gui... reply Storm-Bringer 2 hours agorootparentIs churning still possible in the US? I thought Amex sign-up bonuses were only awarded once. In the UK you can still cancel and re-apply after 24 months. It used to be 6 months but those good days are long gone... reply scarface_74 1 hour agorootparentAmex has a once in a lifetime rule for each card and now they are getting even stricter. But there are still a lot of cards you can go through if you do them in the right order between the business and personal cards. But there is still Chase, CapitalOne and Citi. The Chase Ink Business cards you can churn as much as you want and most of the other cards have either a 24 or 48 month rule reply bombcar 17 hours agorootparentprevAh, for the era of buying dollar coins and flooding your bank with them. Brings back memories. reply brewdad 16 hours agorootparentprevThe other benefit the Delta AmEx (and I presume other AmEx cards) gives are the various merchant discounts. In the past year I've gotten statement credits totaling about $600 by using the card to pay for various streaming services, shoes and clothes, certain restaurants and even my utility bill. The offers rotate every few months but I make sure to scan them when I login to view my statements and activate any of them I think I might use. All of these things were items I was already using or would have purchased anyway and the discounts stack on top of any available merchant coupons too since they are credits coming straight from AmEx. reply ptero 22 hours agorootparentprevSorry, can you explain why the high spend is important? What is the benefit of a low risk buyer having a single card vs three different cards? Low risk is clear -- the lower the risk the more money is left, after handling problems, for the rebates and profits. reply twoodfin 20 hours agorootparentIn this context, it’s important to the merchants: They want these customers, so they grit their teeth on the higher interchange fees demanded by the banks. If the banks started handing out these cards to everyone, the merchants would revolt. reply kasey_junk 21 hours agorootparentprevInterchange. The fees go directly to the issuer not the network (which collects much smaller scheme fees). If you have 3 cards they’d almost certainly be for 3 different issuers so they’d split the interchange. Making you less valuable. reply robertlagrant 21 hours agorootparentprev> for the rebates and profits And salaries and pensions, etc. reply gumby 15 hours agoparentprevThe merchant doesn't generally have much choice. I have some friends who ran a restaurant, and they stopped accepting Amex because the fee was too high. They sold the restaurant to an employee and he immediately started accepting Amex again. Too many high spending clients use it and he didn't want to miss out. Also, even though Costco only accepts a single brand of card (used to be Amex, now Visa), despite their size and market power they accept any Visa card a customer presents. reply empath-nirvana 21 hours agoparentprevRewards cards should be illegal and basically are privately levied tax on the poor and a subsidy to the wealthy. reply agloe_dreams 20 hours agorootparentBoy are you going to be shocked when Walmart, target, Amazon brag about the X% income increase when that happens and while prices continue to rise. Zero, zero companies will discount the sales price when the rewards cards are gone. There is a strong argument that discontinuing rewards cards actually helps the extremely wealthy by taking from the middle class and giving it to the Uber rich shareholders and big business owners. reply addandsubtract 10 hours agorootparentThere's another side to this. By giving out rewards, wealthy people are given more on top. Things they otherwise would've paid for are now free to them. Whereas someone without those benefits has to pay for those things out of their own pocket. reply op00to 9 hours agorootparentprevReturning merchandise should be illegal and allowing returns is basically a privately levied tax on those who make good purchase decisions and a subsidy to the impulse buyer. reply BenjiWiebe 7 hours agorootparentprevI see this sentiment sometimes, but I disagree. I have excellent credit score and several good rewards cards, despite never spending more than $10k/yr through them. I'd say someone making $10k/yr is dirt poor and yet they too can have a good rewards card. reply smallmancontrov 20 hours agorootparentprev> privately levied tax on the poor and a subsidy to the wealthy That's 2/3s of capitalism. Hold enough MA and V -- directly or through just having enough net worth in an index -- and you'll start to see this as a feature, not a bug. reply realwitt 19 hours agorootparentWhat's MA and V? Moving Average and Volatility? (genuinely don't know and unsuccessfully tried to google it) reply frogstomp19 19 hours agorootparentMasterCard and Visa's stock ticker symbols reply ValentineC 5 hours agoparentprev> The article goes on to ask the question \"Why isn’t every card a rewards card?\", meaning why doesn't every card pay cash back, but I think the more interesting question is why every card isn't branded in a way that makes the issuer more money. Why do they bother to issue cards where they get paid less? Why not brand every card as a \"Signature Preferred\" and then pocket the money instead of giving it to the less discerning customers? I didn't see any other comments actually answer the question, so I'll try my hand at this. (Caveat: I've never worked in the finance industry professionally, but I consider myself one of the Redditors mentioned in the article.) From my layperson understanding, banks undertake not to issue more than a certain percentage of cards as \"Signature Preferred\" cards, and there is a minimum credit limit required to open such card accounts. The Chase Sapphire Reserve mentioned in the article is a Visa Infinite card, and Chase requires a $10,000 credit limit to open it. Chase doesn't give $10,000 credit limits to just about anyone, and considering how flexible the US is with identity and income requirements, Chase needs to be more stringent with their underwriting and verification processes to avoid issuing such cards to people who are more likely to default. From further research, it looks like the Visa Core Rules do offer guidelines [1], for anyone interested: The bank would incur additional costs to satisfy the requirements to issue higher tier cards. For Visa Infinite, banks are required to offer benefits like \"Priority assistance and convenience\", \"Exclusive privileges and rewards\", and \"Safety and security\", and in some countries, concierge services. Visa Signature cards must have 24/7 customer support. The PDF is a gold mine for anyone interested in learning more about the various tiers. [1] https://usa.visa.com/content/dam/VCOM/download/about-visa/vi... reply insane_dreamer 17 hours agoparentprevSome merchants reject cards with higher fees; i.e., Amex is not accepted at some merchants with lower margins (i.e., grocery stores). It would be impractical for merchants to accept some branded cards and not others. Imagine \"we accept \"Chase Premium One\" card, but not \"Chase American Airlines\" card.\" Very confusing for consumers. If it's a whole category, like Amex, it's easier to refuse it (besides, low income consumers are unlikely to have an Amex card). reply op00to 9 hours agorootparentI’ve never seen an Amex declined at a grocery store. reply supertrope 16 hours agoparentprevCitibank reissued my credit card well before the expiration date to upgrade it to a \"World Elite Mastercard\" with attendant higher interchange. reply anonu 7 hours agoparentprevLess money on interchange may mean more money on interest charges on balances reply anovikov 21 hours agoparentprevSimply because these customers are likely to buy more and at premium prices and not be a pain in terms of refunds etc. They are willing to pay more in commission knowing they are dealing with richer people. reply ugh123 15 hours agoparentprev> you use a reward card, the merchant is charged a higher fee than if you used a \"normal\" card That seems absolutely ridiculous. The FTC doesn't think this is a problem? reply kindawinda 16 hours agoparentprevWow you thought of something the author didnt cover and think the whole article should be about your post. reply 335 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Interchange fees are a vital revenue source for card issuers, explored in the article on credit card rewards programs.",
      "Chase and American Express compete in the credit card market, with Chase launching the Chase Sapphire Reserve card to rival American Express.",
      "Understanding interchange fees and rewards is key when assessing credit card options, as highlighted in the article."
    ],
    "commentSummary": [
      "The article delves into credit card rewards programs' structure and their effects on both merchants and consumers.",
      "It highlights factors influencing merchant engagement, EU regulations, fairness, pricing, and competition, alongside the pros and cons of cash versus electronic payments and complexity of credit card transaction fees.",
      "The discussion covers the nuances of tax fraud, convenience fees, and strategies to maximize credit card benefits, offering a comprehensive view of credit card transactions and rewards optimization."
    ],
    "points": 932,
    "commentCount": 599,
    "retryCount": 0,
    "time": 1712226917
  },
  {
    "id": 39936246,
    "title": "Mario Kart 8 Strategies: Maximizing Performance with Pareto Efficiency",
    "originLink": "https://www.mayerowitz.io/blog/mario-meets-pareto",
    "originBody": "Mayerowitz.io Mario meets Pareto Step on the Front Line and Beat your Friends Written by Antoine Mayerowitz 0 0 0 0 In Mario Kart 8, choosing your driver, kart's body, tires, and glider isn't just about style — it's as crucial as your racing skills to win a race. Ever wondered how to truly find the best ones? For each of those four elements, you have tens of options. For each option, there are distinct statistics (speed, acceleration, ...) affecting your performance. This adds up to an unbelievable amount of builds to choose from. Hopefully, many choices are just stylistic — they have identical statistics — but even after ignoring those duplicates, it remains a tough job to navigate the thousands of options. Is there any chance to find the best build or is it just luck? Should you favor speed to be the fastest, or acceleration to quickly recover after taking a hit? Let me show you a solution proposed over a century ago by economist Vilfredo Pareto. Finding the fastest driver is as simple as ranking them by their speed statistic. Here you might think that Bowser or Wario are a no-brainer. But you can't just rely on speed to find the optimal build. You have to consider one as well. Now, finding the best driver body tire glider is not trivial anymore — you have to make trade-offs between speed acceleration handling weight offroad mini turbo and speed acceleration handling weight offroad mini turbo Reset Look closely though! You'll find out that some options are always dominated. Let's focus on this poor Koopa for instance. Cat Peach has more speed for the same acceleration, and Toadette has more acceleration for the same speed. Between you and me, if you want to win, never allow Koopa to sit in your kart! You can identify all efficient drivers that, unlike Koopa, are never dominated on both speed and acceleration. Together, they form what is called the Pareto front (or frontier). Mind you: all elements on the frontier are not equally good. You probably won't pick a driver sitting on the edge of the frontier because you want some balance between speed and acceleration. The Pareto efficiency is an objective criteria to filter out suboptimal choices, but you still need to make up your final decision. Given your play style and skills, you may put more weight on one statistic over the other. Those preferences will reveal the component on the frontier that suits you the best. speed acceleration handling weight offroad mini turbo speed acceleration handling weight offroad mini turbo Best driver body tire glider : {} In practice, you not only choose a driver, but a full set of body, wheels, and glider. In the next section, I'll display every build as a distinct point. It will however make the number of choices explode. But Pareto's with us! We've had a bit of fun here, but don't you see the pattern? We're often faced with similar trade-offs. You want a meal that's both cheap and delicious? A job that's both well-paid, easy, and fulfilling? A portfolio with low risks and high returns? A flexible and strong material that's also easy to produce? A fair taxation that remains efficient ? A high quality LLM that is also fast and cost-efficient. In all these cases, you're facing a multi-objective optimization problem, and you have to make trade-offs. Of course, if you already know the exact weights you want to assign to each dimension (i.e., you know your utility function), you reduce the problem to a single objective optimization. This is because you can combine the dimensions with the weights into a single quantity to optimize (often called utility, cost, or fitness). In that case, you don't need Pareto at all. But you're often faced with situations where your utility function is unknown or uncertain. In those situations, the Pareto front helps you eliminate objectively all the sub-optimal options. It won't reveal the one best option right from the outset, but you may now experiment with these efficient options and select the one that fits you the best. Acknowledgments I've made some simplifying assumptions in this article to keep it readable for a large audience. In truth, the statistics that I presented are translated into derived in-game stats that are not always linear with the base statistics. Additionally, there are 4 speed stats and 4 handling stats for all gears (except for the driver), but I decided to simply average those. I've also completely hidden the functional form of the utility function, which can play a great role. To get access to more details behind this article or if you just like my work and want to see more in the future, please consider donating some coins. Credits Super Mario Wiki Mario Kart 8 Deluxe in-game statistics Henry H. Mario Kart and the Pareto Frontier, 2015 © 2024 Antoine Mayerowitz",
    "commentLink": "https://news.ycombinator.com/item?id=39936246",
    "commentBody": "Mario meets Pareto (mayerowitz.io)915 points by superMayo 12 hours agohidepastfavorite110 comments blauditore 14 minutes agoWhen buying a bike, I kind of used this method by looking at the cost-vs-spec-level plot on 99Spokes. Although it should be taken with a grain of salt, as spec level is a heuristic and not always very accurate. Found a nice MTB for a good price that way, but struggled with other types of bicycles. reply jmholla 9 hours agoprevI always knew those little red tires were the best. Sadly, this misses the most important thing to me: style. And my love of Zelda. So I'm afraid I'll personally have to disregard all of this. reply VelesDude 8 hours agoparentThere are no grounds on which I can disagree with you. reply hypercube33 8 hours agorootparentBut you can add that as a dimension to the chart! reply sspiff 3 hours agorootparentHard to quantify into numbers, no? I like Luigi more than Mario. But do I love Mario like a 6 and Luigi like an 8? reply aidenn0 4 hours agoparentprevIn the original MK8, the Triforce tires and Hylean gliders were pretty good, but the stats in MK8 Deluxe for them are not nearly as good. reply wcrossbow 2 hours agoprevNice article! The resulting Pareto front really highlights how hard game design is. You can get millions of possible combinations but the reality is that only a handful of them will ever happen in a competitive environment. reply CGamesPlay 2 hours agoparentThat doesn’t mean that the other combinations are worthless. Presumably there’s value in the cosmetics, plus the puzzle aspect of creating and optimizing the builds along the different dimensions. Surely there’s a meta-Pareto-front of the balance between usefulness of each combination in competitions and amount of fun it adds to the game! reply micheljansen 2 hours agoprevThis article was way more interesting than the title suggested. Well done! reply LarsDu88 3 hours agoprevThis is seriously the most impressive visualization I've ever seen. What tool did the author use to do this? reply _nhh 2 hours agoparentNothing beats https://ciechanow.ski/sound/ reply simple10 3 hours agoparentprevLooks like svelte + three.js Svelte has some pretty nice built-in scroll animation support: https://svelte.dev/repl/051cd352ce284d15b55c91c8b30fa32f?ver... reply superMayo 2 hours agorootparentAuthor here. I'm using Svelte, which is great for interactive applications. For the event handling I'm very influenced by what https://mlu-explain.github.io/ does. The 3d plot is made with Threejs through the Threlte wrapper. One challenge was animating the 20k points in the 3d plot, which is handled by a custom vertex shader. reply hantusk 3 hours agorootparentprevmore specifically it's using the svelte wrapper of three.js called Threlte: https://threlte.xyz/ reply danans 3 hours agoparentprevLooks like Svelte (https://svelte.dev/) reply xanderlewis 10 hours agoprevWell... that was a seriously impressive presentation. I already knew about Pareto efficiency/the Pareto frontier, but now I'll never be able to forget it. And I'll think of Mario Kart (and poor Koopa being dominated) every time. reply throwaway598 10 hours agoparentIf it was Pareto efficient, Koopa would do better too. reply teekert 2 hours agoparentprevPlaying with Koopa is MK on hard mode, and you can feel better about winning. You'll be like Piccolo or Rock Lee keeping their weights on while training ;) reply laborcontract 1 hour agoparentprevThese are the sorts of articles that a lot of news sites and digital publications dream of when pitching venture capitalists to cultivate this as a new sort of medium. But I've always found that the most compelling stuff, the most compelling digital presentations are often emergent. I think designing around it as a goal is impossible and often comes off as contrived and annoying. However, there are a times like this where it's just stunning as in, \"yes, please hijack my scroll, go ahead\". reply georgesimon 3 minutes agorootparentFrom what I understand of Svelte, it was built by a working data journalist with the dream of enabling these type of rich media articles. So yeah, 'emergent' and 'uncontrived' are in the DNA of this article and the tech beneath it. reply anArbitraryOne 1 hour agoprevI once messed with using linear programming to approximate this, and I have to say you did an excellent job explaining it! Worth noting that the tires and gliders are independent, so one can first find their pareto frontiers, then combine that with those of the drivers for all dimensions reply michael-online 11 hours agoprevThis excites me to consider using it as a design tool. When trying to design a game with a more large pareto front of fun and viable builds. reply Etherlord87 41 minutes agoprevYou have a lot of options to choose from. How to pick the truly best option? Let me show you a method, in which you arbitrarily pick 3 out of 6 attributes and then use an artificial, unintuitive interface to choose an optimum trade-off between them. And once you do, my super-amazing method will tell you what combination to use, easy! OK I might be an old bitter cynic, but the beginning got my hopes up for something clever. The only value I see here is explaining the Pareto frontier, but it doesn't take a genius to independently figure out, if you have 2 attributes, and someone else has one of the attributes at least equal, and another higher, that someone else is a better pick… Of course, what the article doesn't even touch, is that some attributes could get worse results as they get higher: imagine having an acceleration and speed so high that you effectively can't steer your car. Also a pedantic argument could be made on the cosmetics possibly actually affecting performance… reply lkirkwood 18 minutes agoparentNot entirely sure what you're looking for. If you don't want to use the \"artificial, unintuitive interface\" (bit rude IMO) you could just google \"best mario kart setup\". The selection of attributes is not exactly arbitrary. AFAIK speed has always been by far the dominant stat in mario kart, then acceleration. Choosing e.g. better handling at the expense of speed is an immediate disadvantage. If your problem is that this doesn't help you to choose atteibutes in the general case, take it up with the chaos of the natural world... This article solves a problem people have (not knowing what setup to pick) in a very stylish way (subjective) and teaches the reader something along the way (what a Pareto frontier is). Not a particularly constructive comment if you ask me. reply Scarblac 28 minutes agoparentprevI thought it was a clever relatable way to explain the concept of the Pareto frontier. And that of the 703560 builds, you can pick just 14 and choose among those depending on what you prefer was a surprise to me. reply james_a_craig 2 hours agoprevA dissenting opinion on the design - for me, this presentation was like watching a video to find information; too slow paced, and it made me impatient the whole time. The original notebook format was far better in that regard. The layout within each section is beautiful, but the animation and the scroll-sensitive layout (vs. just having a series of static diagrams) makes it unpleasant for me to read. The content's excellent and it was fascinating to see how the differences between characters and karts play out though! reply jeroenvlek 24 minutes agoprevThis is the kind of content I'm here for. Thank you, superMayo! reply samwho 39 minutes agoprevThis is superb. Really great work! reply ihaveajob 11 hours agoprevBeautiful presentation. I love when visualizations serve the goal and not the other way around. Tufte would be proud. reply vavooom 6 hours agoparentThe switch from 2D to 3D was so seemless and beautiful I actually gasped. reply superMayo 2 hours agorootparentThanks a lot! That was my goal. It's a trick I learned: if you zoom in from a distance, everything appears flat. The effect is achieved by zooming out while simultaneously moving the camera closer to the subject! reply danbruc 1 hour agorootparentIn cinematography that is called a dolly zoom [1] and it is best known for being used in Vertigo [2] and Jaws [3]. [1] https://en.wikipedia.org/wiki/Dolly_zoom [2] https://www.youtube.com/watch?v=G7YJkBcRWB8 [3] https://www.youtube.com/watch?v=_eO_5q5dR9M reply oneeyedpigeon 25 minutes agorootparentprevI love how that transition is relative to the scroll position rather than working off a breakpoint. Did you consider doing the same thing with the first bar chart? I think it would be nice to slowly reveal the standings :) reply KeplerBoy 1 hour agorootparentprevCould you not just use a parallel projection (which should be the default for this kind of 3d scatter plot)? reply ndr 3 hours agorootparentprevSame. What a beautiful plot twist. reply smlacy 4 hours agorootparentprevMaybe it was 3D the entire time. :) reply pcchristie 5 hours agorootparentprevYep same, I let out an audible \"nooooo....\". reply helboi4 1 hour agorootparentprevHonestly, I was so impressed. reply Fnoord 8 hours agoparentprevBugs out for me in Firefox at around 2/3. Works fine in Safari. My kid (6) crashes all the time, and she is naturally attracted to picking Peach. I'd say because of that, a character with high acceleration is going to be better. Though she also just likes to push the gas. Though we're playing the one from GBA on Analogue Pocket. Which is probably a lot less advanced as the one in question here (a game for Wii U from 2012), it does resemble Wacky Wheels quite a lot. reply denkmoon 5 hours agorootparentFYI Mario Kart for the Switch has a handful of accessibility options that help less experienced/younger/elderly players stay in the game and on the course without taking away from the fun. reply djbusby 7 hours agorootparentprevGotta teach the kid about using the brake and the \"poomp\" slide (called boost after N64) on the corner. I race Mario but when she's on, mine (8), can get me on the 150 tracks (switch). I'm still crushing on the 200 tho (for now) ;) reply captn3m0 5 hours agorootparentprevBugged out for me around 2/3 on Firefox/iOS (which is really Safari) with Lockdown mode. reply wgx 4 hours agoparentprev> would Tufte is still alive! reply yu3zhou4 1 hour agoprevHere's the repo: https://github.com/SuperMayo/mayerowitz.io reply 0cf8612b2e1e 10 hours agoprevDo professionals use different builds per map? For example, a map has long straightaways (favoring top speed) vs a map with more sharp turns (preferring acceleration)? reply CameronAavik 7 hours agoparentFor time trials, yes that is true. There are also other concerns such as that the speed stat is actually comprised of 4 different stats that have different values depending on the terrain: Ground, Air, Water, and Anti-Gravity. Some tracks that have a lot of water and so for that you would be better going for a kart that has high water speed. There are other less important statistics at play too that aren't mentioned here such as handling, traction, and also the hitbox of the vehicle is also important since it might change how tightly you can hug a turn against a wall or how wide you have to steer to collect coins. In practice when playing online however, you won't know what track is about to be played, and so the meta right now prioritises mini-turbo stat much higher than speed. Having a high mini-turbo can also overcome the lack of speed by performing additional mini-turbos even on straight sections. Also when playing online you also will be hit by items a lot and need to try dodge other items being thrown at you, and for that having higher acceleration helps too. reply Laremere 10 hours agoparentprevIf you look at the first place (after clear cheaters) time trials for different tracks, you'll see different choices but clear patterns. The main mode of the game has you race a set of four different tracks, and the online mode throws in all of the tracks; So in those modes the choices are going to be more constrained towards an average good. For my part, I got the best rank in single player on all pre-DLC tracks with only toad and the default kart, so it's a part of the game you can entirely ignore if you're not competing against other humans. reply nonethewiser 9 hours agoparentprevThe breakdown is actually bagging vs. front running tracks. Bagging favors speed and front running favors mini turbo. Bagging means purposefully being far from first to get good items which allow you to come back hard. reply brrrrrm 9 hours agorootparentto add to this, \"bagging\" tracks are determined by how many shortcuts they have (ones that require good items to take, such as mushrooms or stars). reply prmoustache 3 hours agoparentprev\"professionals\"? This is Mario Kart we are speaking about, not formula 1 or counter strike. reply Madmallard 10 hours agoparentprevTends to just be yoshi on one of two karts for nearly all tracks on 150cc because miniturbo speed is the same as top speed of the fastest build in the game. reply trojanalert 21 minutes agoprevThis is so darn good! reply stefanlindbohm 4 hours agoprevLove the application and visualization! Having spent the past year building a journey planner algorithm, which heavily builds on pareto optimality/sets, from scratch, I was waiting for the full set of pareto optimal solutions. I.e. all kart combinstions that are best in at least one way. Should be doable by iterating through all possible stats and merging[1] into a set for each one. We might get a lot of solutions, but it should be somewhat managable. Has anyone tried this? 1: Merging is to take the new entry and 1) removing any existing entry that is dominated by it, and 2) adding the entry if it is not dominated by any existing entry. reply ndr 3 hours agoparentIsn't this what the last part of TFA covers? reply bandrami 4 hours agoprevI haven't played this series since the SNES 30 years ago. Do the different characters still get different power-ups from the cubes? Because that was always what I picked based on. reply baku-fr 3 hours agoparentThis is the case on Mario Kart: Double Dash!! (GameCube), on the Arcade GP series and Mario Kart Tour (smartphone). In all other games, character choice doesn't affect item distribution. reply sspiff 3 hours agoparentprevThe items you get are the same, but maybe the frequencies are different? I never heard of that being the case, only that the weapons you get depend on your position in the race (you get better weapons when doing worse in the race). But I also didn't realize this was the case on SNES. I only ever played the SNES version as a young kid, and it was a Japanese NTSC version on a PAL television through a cartridge adapter and it was pretty janky. reply quibono 10 hours agoprevI really like the article and the presentation! With that in mind, is this style of presentation (i.e. different elements jumping out or moving into focus as you scroll down the page) easily doable OOTB with any JS libs? Or is this pretty much a custom job? reply swyx 3 hours agoparentkeyword you want is “scrollytelling”. lots of tutorials here and on youtube. easy to start hard to master like with most things. reply rozab 9 hours agoprevThis is all very webdesign-y and might be good for a less techy audience, but tbh I think the original article with its notebook format provides a lot more useful information https://hinnefe2.github.io/python/tools/2015/09/21/mario-kar... reply sequoia 9 hours agoparentI don't think it's fair to disparage this article. The other one is perhaps good for a \"deeper dive\" but I'd say TFA is better overall at illustrating the concept. reply brrrrrm 9 hours agoparentprevthat one doesn't have mini-turbo, which has largely superseded acceleration as a stat (it correlates but isn't 1:1) reply mikepurvis 8 hours agorootparentIndeed. MK8 courses are absolutely loaded with mini turbo opportunities— every drift, jump, or bump in the road is a chance to get that little boost. I’m not at all surprised it can make the difference in competitive play. reply novideogame 49 minutes agoprevIt seems that the Pareto frontier is just the set of maximal elements in a partial ordered set. If the set is a subset of R^n and you have to choose an element in the Pareto frontier then for each weight of the variables you can just apply a linear function from R^n to R and select argmax f(x) for x in the Pareto Frontier. Context just someone that don't play video games. reply fredsmith219 10 hours agoprevMy 9 yo is better than me at Mario Kart but not by much. This knowledge may make the difference for me. Thank you! reply wheelinsupial 9 hours agoparentIn case you aren't aware, you can select a few builds and quickly compare the stats using: https://mk8dxbuilder.com/ Another thing is the coins. Collecting them gives you a speed boost for each coin you collect. reply brrrrrm 9 hours agorootparentthis one groups the characters/cars by identical stats: https://www.bettermk8dxbuilder.com reply sph 1 hour agoparentprevIn summary: the difference between raw talent and experience. Your kid has likely better reflexes and motor skills than you have, but on your side you have experience and wisdom accumulated over the years :) reply themoonisachees 3 hours agoprevIf the creator happens to come by here: Very cool and I see the vision Unfortunately on firefox for Android 13 on my nothing phone (2a) almost all on-screen assets flicker non-stop. I thought I was a style decision at first but now that I'm further down it's very clearly a bug. Still very cool, learned about pareto efficiency from a video about this exact topic a few years ago that only computed the pareto front for accel and speed because they couldn't represent higher dimensions well. Maybe same author? reply superMayo 2 hours agoparentThank's for the info, will fix asap ! reply neokrish 3 hours agoprevThis is absolutely brilliant! Bookmarked and I’m sure I’ll be revisiting this and sharing this broadly! reply martijnarts 11 hours agoprevExcellent article! Super approachable and relatable, making it very good at explaining a useful model. It immediately has me looking for other places to apply this. This'll be top of mind for a while! reply SrslyJosh 10 hours agoprevCouple points: 1. Skill absolutely matters, more than the kart, etc. that you pick. Watch some expert players on YouTube playing with weird builds and you'll see that they are still able to do well even when playing with significant disadvantages. 2. In practice, you don't really need to know the value of the hidden mini-turbo stat because higher acceleration == higher mini turbo. For 99% of players, acceleration can just be used as a proxy for mini-turbo. reply Buttons840 8 hours agoprevHow to calculate a Pareto front world take me some thought. I wonder, in practice, does defining my preferences and weights and then using a genetic algorithm find the optional solution? That would take me less thought, because I already know exactly how to define a score function and use a random API. reply swaits 8 hours agoparentYes, indeed. There are quite a few “nature inspired metaheuristic algorithms” which do exactly this. When I say “quite a few”, I mean countless. Look up NSGA and NSGA-II for a good starting point. Then Kagi your way deeper into the rabbit hole. reply eggdaft 3 hours agoparentprevIf by “optional” you mean “optimal” then no, a GA is not guaranteed to find the optimal solution in the general case. reply modeless 10 hours agoprevWait, Bowser and Wario are the fastest? I thought it was the other way around. I guess it's reversed from Mario Kart 64. reply xanderlewis 10 hours agoparentI'm not sure, but it kind of makes sense as a choice, since acceleration and speed should be negatively correlated and since Bowser and Wario are both massive they should accelerate more slowly than others (given constant force). So they get to have the highest top speeds to compensate. reply CapnCrunchie 10 hours agoparentprevThey have the highest top speed. It was that way in Mario Kart 64 as well. They have the worst acceleration though. reply modeless 9 hours agorootparentThis is not true. The lighter characters Yoshi and Peach and Toad had higher top speed in 64. https://tasvideos.org/GameResources/N64/MarioKart64#:~:text=... reply SrslyJosh 10 hours agoparentprevYes, but higher-speed characters have lower acceleration and less mini-turbo, which disadvantages them in many situations. reply thiagoharry 10 hours agoparentprevYes, but with their poor acceleration, and given how chaotic some Mario Kart races are, it is hard to achieve their maximum speed. reply thiagotomei 10 hours agoparentprevIt’s exactly as in Mario Kart 64! Remember, heavy characters have the higher top speed, but light characters have the higher acceleration! reply xtracto 10 hours agorootparentIt was similar in the original Mario Kart of the SNES. I clearly remember always choosing toad or koopa because they \"felt\" the more average to me. Donkey Kong was difficult to handle and also had slow acceleration, same with bowser. reply modeless 10 hours agorootparentprevI distinctly remember that the light characters had the highest top speed in Mario Kart 64. And according to this table, I'm right: https://tasvideos.org/GameResources/N64/MarioKart64#:~:text=... reply posix86 10 hours agorootparentThat'd make the heavy characters completely useless, once you're behind, there's no way to catch up anymore! reply re 8 hours agorootparentHeavy characters in Mario 64 have tighter handling than lighter characters and can cause them to spin out when bumping into them. reply coryrc 9 hours agorootparentprevYes, except for items and doing better in the pack. reply willcipriano 8 hours agorootparentprevIt's not a big difference and you can't be at top speed for long. reply non-chalad 8 hours agoprevHow would this mesh with Halo Kart (2)? 2. https://www.youtube.com/watch?v=9K0IcawLpCE reply throwawayk7h 10 hours agoprevSomething not mentioned in this analysis is that after summing the stats from the different components, the value is rounded down, giving 7 possible outcomes per stat. reply rvba 2 hours agoparentWhy is it rounded down? reply amarshall 10 hours agoprevOne thing this should mention is what game version it uses. Updates frequently change the stats and thus what builds are “best”. reply superMayo 2 hours agoparentIt's the latest version, stats are from https://www.mariowiki.com/Mario_Kart_8_Deluxe_in-game_statis... reply rafd 7 hours agoparentprevMario Kart 8 is now frozen, no more patches to come. reply jdmarble 10 hours agoparentprevThe first paragraph mentions this: \"In Mario Kart 8, ...\" If this was more than a tool for teaching multi-objective optimization, I'd like to see how the Pareto front changes over Mario Kart releases! reply crtasm 10 hours agorootparentYes, but there have been many versions (updates/patches) of the game. I don't know how many changed character stats: https://www.mariowiki.com/Mario_Kart_8_Deluxe_update_history reply petesergeant 8 hours agoprevThis is great, but like many casual players I’m very wedded to my character but not their build. Be great to pin the character and then get the best build for them. reply superMayo 2 hours agoparentI hope your character is not Koopa reply Saba21 3 hours agoprevCool, great! reply bombcar 10 hours agoprevI did not know the drivers and carts made a difference at all. reply SignalM 10 hours agoprevVery cool love the design of the site and now I know which cart to win with reply dluan 8 hours agoprevNow this is ~~pod racing~~, uh, game theory reply jeffbee 10 hours agoprevMissed a unique opportunity to title the article: It's-a-me, Vilfredo! reply jbjbjbjb 10 hours agoprevI was disappointed by the lack of a radar chart reply y1zhou 7 hours agoprevThis is so neat! MOO has been a integral part of my work yet it has never occurred to be that Pareto optimization could be applied in kart picking. reply evilc00kie 5 hours agoprevtl;dr: use peach with teddy buggy, roller tires and the cloud glider. reply jhatemyjob 7 hours agoprev [–] God I am so disappointed, I thought he disassembled Mario Kart 64 into C and refactored the codebase. Fuck my life reply superMayo 2 hours agoparentSo sorry I've disappointed you, but don't fuck your life plz reply keithalewis 2 hours agoparentprev [–] No, just go get one. Don't be a PL obsessing over a video game. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into selecting the top driver, body, tires, and glider in Mario Kart 8 for peak performance, introducing Pareto efficiency and Pareto front for balancing speed and acceleration effectively.",
      "Players can leverage the Pareto front to weed out inferior options, yet the ultimate choice hinges on personal preferences and gameplay style.",
      "It also covers approaching multi-objective optimization problems with the Pareto front, offering further insights for readers intrigued by the author's research."
    ],
    "commentSummary": [
      "The article delves into Pareto efficiency in decision-making using visualizations crafted with Svelte and Three.js.",
      "It applies this concept to scenarios like selecting bikes, game combinations, and enhancing gameplay in Mario Kart.",
      "Users' insights on the practicality of the presentations, online gaming tactics, character builds, and character selection's influence on item allocation are also included."
    ],
    "points": 916,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1712267612
  },
  {
    "id": 39930463,
    "title": "AI-Piano Girl Performs MIT License",
    "originLink": "https://twitter.com/goodside/status/1775713487529922702",
    "originBody": "AI-generated sad girl with piano performs the text of the MIT License pic.twitter.com/h5wdMuNUdm— Riley Goodside (@goodside) April 4, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39930463",
    "commentBody": "AI-generated sad girl with piano performs the text of the MIT License (twitter.com/goodside)604 points by amichail 20 hours agohidepastfavorite234 comments schroeding 14 hours agoHa. Voice synthesizers and TTS systems (and NLP in general - dead electronics imitating this very intimately human thing, speech and language) always fascinated me, so far that this was a significant reason for me to study CS and computational linguistics. This is literally some of the impossible sci-fi tech I dreamt of as an undergrad. Crazy. I'm still a bit in disbelief how fast things currently move on this front. Interestingly, suno.ai is also able to imitate the very robotic and staccato-like intonation of Vocaloids: https://app.suno.ai/song/f43e9c46-92d3-4171-bdd9-026213d6772... - everything comes around. :) reply vsnf 7 hours agoparent> https://app.suno.ai/song/f43e9c46-92d3-4171-bdd9-026213d6772... Unironically very good. A convincing replica of Miku's voice. Plus the beat itself is great too. As another commenter put it, it is indeed, a banger. reply senkora 9 hours agoparentprevThat's shockingly good. It reminds me of a mix between Kaibutsu by YOASOBI and The Fox's Wedding by MASA. (Warning: both links are very anime. Nothing too bad) Kaibutsu: https://www.youtube.com/watch?v=-5M4lbEpn6c The Fox's Wedding: https://www.youtube.com/watch?v=khNi_6PnvaE reply mewpmewp2 8 hours agorootparentIt reminded me of Bad Apple - I'm not really familiar with all of this weird, nerdy Japanese culture, but I agree it feels very enjoyable to listen to what Suno created here. reply mewpmewp2 12 hours agoparentprev> Miku voice, speck fast, Vocaloid, math rock, j-pop, mutation funk, bounce drop, hyperspeed dubstep What a banger. reply n4r9 19 hours agoprevSome strange and funny vocal aberrations here: * sublicence - \"sublissence\" * fitness - \"fisted\" * infringement - \"infring-ment\" * liable - \"liar-ful\" It's also obviously not a pure human voice recording as the pitch transitions sound heavily auto-tuned or electrified (think Cher's \"Believe\"). I anticipate people becoming experts in detecting AI-generated vocalists in much the same way that we can currently detect AI-generated images due to abnormalities especially in details like ears or fingers. reply haolez 19 hours agoparentAnd I also expect that, very soon, we won't be able to tell them apart anymore (like those wine experts that fail to detect the good wines if blindfolded). reply Etheryte 18 hours agorootparentThat fail to detect even whether they're having white wine or red wine.* reply arketyp 18 hours agorootparentI've heard this, and I would have been inclined to believe it. But then I watched the documentary Somm about the journey of a couple of friends reaching for the highest rankings of sommeliers. They could identify grapes, regions and year with striking accuracy. I just don't see how you could do that and then not be able to tell white and red wine apart. reply vundercind 18 hours agorootparentI barely know what I’m doing with wine but am 100% sure I could at least tell you which are whites and which reds if you lined up a typical Chardonnay, a typical Pinot Grigio, a typical cab sauv, and a typical Pinot noir. I am certain there exist weird wines that could fool me (I’ve had a few really weird wines) but typical shit from the grocery store, I’m gonna be able to tell at least that much. I might even ID them more precisely than red or white. It’s not exactly subtle… Then again I don’t have a clue how someone could fail to tell which is coke and which Pepsi in the “Pepsi challenge”. They’re wildly different flavors. I can tell by smell alone. reply n4r9 18 hours agorootparentI vaguely remember looking into this before, and it turned out that the tasters were being told (incorrectly) that it was a red wine, and asked to describe the flavour profile. They then used tasting terms more frequently associated with reds than with whites, and didn't question what they were told. So it's less a case of \"they cannot distinguish red from white\" and more a case of \"they went along with a suggested classification\". I feel like this is a weaker result, although it's still a little surprising. reply gpm 14 hours agorootparentThat sounds a lot weaker. Quick, label all the US states: https://imgs.xkcd.com/comics/label_the_states.png I've given this map to half a dozen smart/well educated Canadians, who happily engaged in pointing out the states they recognized for several minutes, and not one of them noticed until it was pointed out. reply n4r9 1 hour agorootparentI'm from the UK and would probably have fallen for this for several minutes as well. I hope that I'd eventually realise from the number of states down the West coast. reply markisus 1 hour agorootparentprevFor anyone else who is geographically challenged, the map apparently has 64 states instead of 50. Here is an article with the extra states highlighted. https://www.explainxkcd.com/wiki/index.php/2868:_Label_the_S... reply ska 13 hours agorootparentprevI suspect it would work nearly as well on many Americans. reply vundercind 13 hours agorootparentprevWhat’s the joke? Looks normal. I see Thirdmont. Indiantwo. Yep, ordinary map. reply lupire 12 hours agorootparentprevSo? That's focusing on what they know and not having time to notice the extra. Different from making incorrect statements reply arketyp 17 hours agorootparentprevThanks. Together with GP's point about the possibility of weird wines, it seems reasonable that one could go along quite far on a false premise. reply notnaut 15 hours agorootparentprevMy feeling is there is the high level classification which is quite difficult to fuck up. After that it’s all adjectives and analogues, which is the fluffed up phoniness that inherently presents itself in the process of converting our subjective experiences of physical reality into abstract symbols. reply yongjik 15 hours agorootparentprevYeah, but that still shows people's perception of wine is barely above noise level, if it can be so easily misled. For comparison, imagine someone showing a piece of Picasso to art critics and saying \"Could you please describe the artistic significance of this painting by da Vinci?\" The critics won't start using terms commonly reserved for Renaissance era; they'll say \"What the fuck are you talking about, this isn't da Vinci.\" reply wizzwizz4 13 hours agorootparentBoth artists are dead. It is possible to learn all of their paintings. It is not possible to learn all of the wines. reply nrclark 14 hours agorootparentprevA lot of the biggest perceived differences come from temperature, since red wines are usually served at room-temperature. If you ever decide to do a blind test, make sure to control for temperature. I did it, and I had a very hard time picking out which varietals were red and which were white. reply gamblor956 17 hours agorootparentprevI rarely drink wine (less than 1x every few years) and I can tell the difference between a red wine and a white wine, and subcategories of red wines (and I do specifically mean the difference, so that means only when compared to another wine). The hard part is identifying the type of wine, but many of my wine-drinking friends can do with ease. We've tried the \"test,\" having me or someone else randomly purchase wines from the closest store and then serving random samples to them while they're blindfolded. They're able to identify the specific variety more than 4/5 of the time. reply svachalek 14 hours agorootparentYeah, I'm sure a lot of these tasters are overly pretentious. But some people are willing to go the opposite extreme and think people can't taste anything. Can anyone tell the difference between Coke and Sprite? Between Coke and Pepsi? Coke and Diet Coke? Of course we can. The difference between a typical pinot noir, syrah, or cabernet sauvignon is not something it takes magic powers to differentiate. Now specific years, wineries, etc, now that raises questions. reply Starman_Jones 4 hours agorootparentprevThis myth is based on a fundamental misunderstanding of the experiment conducted. The conclusion of the experiment was that the vocabulary used to describe wine is subjective, and that the chosen descriptors are most heavily influenced by the color of wine, the perceived cost of the wine, and the taster's opinion of whether it was a good wine or not. I've participated in a blind wine tasting, and it was trivially easy for even complete amateurs to guess the right color of wine 100% of the time. reply littlestymaar 18 hours agorootparentprevThis one only shows you poor “expertise” more than anything, as it is standard exercise while training to become a wine expert in France (they also give students white wine that have been red-colored or otherwise tempered with), so I wouldn't expect any legit expert to be fooled this way. Though it's true that with some wines it can be tough initially for enlightened amateurs. Source: my wife's godfather did the studies for that[1] two years ago. [1]: https://www.isvv.u-bordeaux.fr/fr/diplome-universitaire-dapt... reply reducesuffering 14 hours agorootparentprevI believed this myth until I actually tried it blind with a handful of wine novices, and every one could tell them apart. reply n4r9 18 hours agorootparentprevAs far as I can tell, AI image generation still struggles with some things after many years of research and is often detectable. Perhaps vocals is easier though. reply ancientworldnow 18 hours agorootparentIt's like cgi, you only recognize bad examples of it while the good ones go right past you. I've got plenty of ai generations that fool professional photo retouchers - it just takes more time and some custom tooling. reply throwup238 17 hours agorootparent> I've got plenty of ai generations that fool professional photo retouchers - it just takes more time and some custom tooling. What’s a good place to find out the SOTA of the custom tooling and workflow? reply zzzzzzzzzz10 14 hours agorootparentComfyui + civitai. 4chan and reddit threads if you want to go deep reply ptx 13 hours agorootparentprev> It's like cgi Right. Full of code injection vulnerabilities. reply Legend2440 7 hours agorootparentprev\"many years\" lol, midjourney only came out like a year and a half ago and the quality has quadrupled in that time. reply n4r9 2 hours agorootparentGenerative text-to-image models based on neural networks have been developing since around 2015. Dall-E was the first to gain widespread attention in 2021. Then later models like Stable Diffusion and Midjourney. \"Quadrupled\" is a very specific and quantitative word. What measure are you basing that on? reply Paul-Craft 10 hours agorootparentprevThis already sounds like something I would have listened to in the 90s, except with too much autotune. reply VelesDude 13 hours agorootparentprevFrom audio video editing experience years back, it is much easier to slip some cheap audio cuts past people than visual ones. reply jackspratts 13 hours agorootparentprevif by very soon you mean already then. yeah. i can't anyway, and i'm in the business.- js. https://soundcloud.com/rs-539916550/soul-of-the-machine?utm_... reply gosub100 14 hours agorootparentprevThe non-singing TTS are barely discernible now. I watch a lot of narration-heavy edu-tainment on YouTube and often the only way I can detect TTS is the consistent monotone and uniform syllable cadence. There can be 15 minutes before a single mispronounced word is spoken. That could be a preview of what's to come with AI video. reply nostrademons 18 hours agoparentprevMany human vocalists have similar aberrations. Remember Jimi Hendrix \"Excuse me while I kiss this guy\", or the notorious autotune on a number of contemporary pop artists (you gave an example yourself)? IMHO many of the successes of \"artificial intelligence\" come from \"natural stupidity\". Humans have many glitches in our perceptual mechanisms. The AIs that end up going viral and become commercially viable tend to exploit those perceptual glitches, simply because that's what makes them appeal to people. reply n4r9 18 hours agorootparentThe difference between this and Hendrix's \"kiss this guy\" is that you can listen to it and plausibly believe that Hendrix is actually saying \"the sky\". In the linked track you know the actual words but it still doesn't sound like them. reply thenickdude 12 hours agoparentprevYou can fix most misspoken words by tweaking the lyrics. e.g. in my most recent song it pronounced \"pasting\" from \"copying and pasting\" as \"past-ing. I just rewrote the lyrics as \"paste-ing\" and it sung it perfectly afterwards. reply Terretta 6 hours agoparentprev> It's also obviously not a pure human voice recording as the pitch transitions sound heavily auto-tuned or electrified ... The cake is a lie, but the music is real It’s all fake when the truth is revealed > (think Cher's \"Believe\") Or think GLaDOS. Pretty sure that's not a coincidence. reply Retr0id 18 hours agoparentprevAnd I'm sure a skilled editor could already edit out those tells. reply OmarShehata 18 hours agorootparentAt that point, is it AI generated?? That feels like an entirely different category to me (like it's sort of no difference than paying someone to voice something and share it) I think the stuff that is completely generated with no human in the loop is a different category for me because it can be used for things at scale like, bots on social media, or ads in a podcast generated just for you, etc. As long as there is still a human in the loop making the editing decisions, it feels not categorically different from the world we have today. reply Retr0id 18 hours agorootparentThat's a fair point, but \"ai does the work and humans clean up the mistakes\" is generally a lot faster than humans doing all the work. Singing well takes skill (even when you have autotune), splicing together multiple \"takes\" into one good recording, less so. reply tombert 13 hours agorootparentprevI would say it's still categorically different, just because we're automating one piece of labor that was kind of thought until about ~12 years ago to be un-automatible. Like, there's been computer-singing voices for awhile, but they always sounded pretty robotic and goofy (e.g. Microsoft Sam), and I think for a long time people just assumed that to get mostly-realistic voices, you need an actual singer. Yes, it still requires a bit of human tweaking to make it perfect, but I suspect that if put to the test it would reduce the cost of making a song substantially. reply gpm 14 hours agorootparentprevI'm never going to have the voice to sing this, but I can easily imagine learning how to edit it. AI/Human combos can still be valuable. More broadly I'd argue that that's how almost all tech works. E.g. there are still textile workers, just many less of them producing much more clothing. reply latexr 16 hours agoparentprev> I anticipate people becoming experts in detecting AI-generated vocalists in much the same way that we can currently detect AI-generated images due to abnormalities especially in details like ears or fingers. People fail to identify even the most basic and obvious fakes, but somehow there’s a group of people who think that as fakes become harder to distinguish from reality, we’ll all magically become experts at it. We won’t. People’s ability to detect fakes will get worse, not better, as a consequence of more prevalent and better fakes. https://www.youtube.com/watch?v=1U1HMqtam90 reply Traubenfuchs 19 hours agoparentprevThe average modern pop songs boasts a worse voice that sounds more autotuned than this AI song here. The biggest problem to me is how the voice appears to be shaky. reply Martinussen 18 hours agorootparentI'm going to go out on a limb and say you don't listen to much \"modern pop\" - the production quality of the biggest mainstream pop is _extremely_ high at this point, and while \"worse voice\" is obviously subjective, this really wouldn't be anything stand-out in that regard even if it didn't sound like a robot. reply sigmar 19 hours agoprevThe delivery of \"(the 'software')\" at 0:21 had me chuckling. reply leokennis 1 hour agoparentI was very impressed by the mini bridge and then sudden addition of harmony between the license and the ALL CAPS statement part. Is that all AI deciding that? This made it a true song in my opinion. reply cwillu 18 hours agoparentprevMer chan ta billlll iiii tttyyyyyy reply buzzm 18 hours agoparentprevAgreed! Unexpected and made my morning. reply navane 14 hours agoparentprevI hoped for a choir when I saw the al caps section coming, and I was not disappointed. reply ein0p 17 hours agoprevMy mind hasn’t been this blown by AI since GPT4. You owe it to yourself to check out Suno.ai. As a non-pro musician I’m excited by this. Some version of this could become a _starting point_ for me, rather than an unreachable end goal. I can see how pros would be horrified by this, too. For quite a few people some future version of this could be an adequate replacement for a music subscription, but of course not for a show. reply UncleOxidant 15 hours agoparentDoes it take a long time for it to generate a song? I've been waiting for about 10 minutes now with a spinning circle line. reply throwup238 15 hours agorootparentWith the pro subscription it usually takes less than thirty seconds for the songs to be playable. It keeps generating while you play though, so the whole audio file isn’t available for a few minutes. Free accounts are queued so it depends on load and I don't think the v3 model is available to them. reply MarcelOlsz 17 hours agoparentprevI was thinking it would impact places like bars and streams and tv the most rather than actual consumers, or wherever licensing is concerned. I don't believe people would listen to AI generated music for the same reason AI isn't impacting fine art. People aren't going to hang AI paintings in their houses or listen to AI music. reply newswasboring 15 hours agorootparent> People aren't going to hang AI paintings in their houses or listen to AI music. A lot of people are very confident about this and I dont understand why. The same was said for jazz and comic books. But I am listening to jazz with comic book posters on my wall. There were different reasons to give the same statement, but it almost always turns out to be wrong. Humans like what they like and seldom judge an artwork for its process (outside of a very small niche community). reply MarcelOlsz 14 hours agorootparentThis is something different entirely. We're outside of the \"human sphere\" so to speak. >Humans like what they like and seldom judge an artwork for its process (outside of a very small niche community). That's true, but how do you zoom out of process? This is beyond process. I would just say most people don't like inhuman things. reply newswasboring 14 hours agorootparent> We're outside of the \"human sphere\" so to speak. Can you elaborate on this a bit? Because this is what I don't get. reply MarcelOlsz 14 hours agorootparentIt's a non-human algorithmic mish mash of a bunch of stuff, there is no human quality to it or years of effort to reach new heights. AI will not make \"new\" music in the sense that it will make a trumpet song that escapes our current understanding of a trumpets limit like how a once in a generation player will come along and move the ceiling up. It's an omellete. There is no Dolly Parton behind an AI Jolene or a Michael Jackson turning a 4 track tape into a musical masterpiece. The journey and personalities are what contextualize the sound, without AI that context is gone. That's why I think it will just be used for cafes and things like that where they want to escape licensing fees. As for consumers - I believe people will see AI music consumption as a way of supporting the new technological powers that be, and the act of listening to human-made music will have an element of counter-culture baked into it. I'm a professional musician and I have a very physical reaction to sound. Once I know it's AI my goosebumps fade. Another lame incarnation of a tech that will also fade like crypto and everything else. The types of personalities who will leverage this tech are not the same personalities that make the greats. I'm not worried. reply newswasboring 13 hours agorootparentIn this post, I can summarize two points you are trying to make. One, it takes less effort, and two it doesn't fit into our current understanding of how art creation narratives work. I don't see how that precludes a piece from being good/bad. I feel like you are arguing for your personal opinion (if not your image of what the world should be) as if it's some kind of objective truth. Your goosebumps might have faded but when I heard this post in a half sleepy state, I got goosebumps when my sleepy mind figured out its fully AI generated. But that doesn't add to the argument either way. reply vsnf 6 hours agorootparentprevAI Art is second-order human art. From this viewpoint it's still human by proxy. And anyway, is it measurably different from art produced while tripping on LSD or in similar states of altered consciousness, such as schizophrenia, dementia, or even depression, which often produce things many people would not describe as regular? reply MacsHeadroom 12 hours agorootparentprevI love art made by non-human intelligences. I especially love how it can transcend and redefine loved mediums by combining them in surreal ways that are otherwise quite difficult to obtain. Algorithmic exploration of mediums outpaces mere mortal \"effort\" in its efficiency and in doing so raises the bar for what constitutes media worth giving our attention to. reply j-bos 15 hours agorootparentprevI see this take often, but I don't buy it. Mixtapes and playlists are quintessential gifts of affection based on art that the giver did not make and by artists the receiver often does not know. Just the same lots of people hang costco paintings on their walls by anonymous sweatshop workers and kids love cool posters about whatever interests them with no regard to who made them. I believe consumers are likely to enjoy lots of this generated art. reply ein0p 17 hours agorootparentprevWhy not? Have you seen the top 10? It couldn’t be any worse than what it is now. People who reach the top 10 are rarely there for the “art”. A lot of them don’t even write their own songs or music. reply xetsilon 12 hours agorootparentAll music at this point is largely ambient music and Muzak. The future is obviously a form of custom AI Muzak/Ambient music with a few pop stars for people to focus on. I am a big fan of more art type music and guess what? No one listens to it. My fav album of 2023 has 6.4k views on youtube. At least a 100 of those are mine. No one listens to this stuff. People watch video critic reviews of more art type music than the actual music itself. reply ein0p 11 hours agorootparentLol, same. A lot of the stuff I listen to is completely unknown to a “normal” person. And guess what? AI is not replacing those folks for their audiences in the foreseeable future, because they don’t just regurgitate the same chord progression as everyone else reply MarcelOlsz 17 hours agorootparentprevHonestly I don't even know what the \"top 10\" is or how its measured and have never met anyone in my life who listened to top 10 stuff. It's always HR office radio, mechanic radio, the bar, club, etc. Even the most normal people find stuff they like on youtube and listen to that. Even if the AI music is extremely good, it's just missing the fact that it was made by a person, which changes the experience entirely. I think we're more likely to see musicians and those top10 artists leverage AI without explicitly saying so. I expect we will have a daft punk moment where someone is using exclusively AI and later unmasks that it was all AI, and as soon as that happens the music is disconnected. Same with AI art. I can see something and be duped and go \"oh wow!!!\" and as soon as I know it's AI the caring leaves my body completely and reverence and interest is lost. reply aradox66 17 hours agorootparentI love this sentiment about \"top 10\" radio. If only it was so. That's the stuff that's on everywhere, all the time. Grocery stores, cafes, etc etc. hell, I listen to it on YouTube. It's like junk food. It's bad, it's good. It's better than AI, even this incredible mindblowing suno thing. Production value counts. reply janalsncm 15 hours agorootparentQuality isn’t the only factor though. Music made by people has copyright which means grocery stores and coffee shops have to pay a license fee. There’s certainly a point where this synthetic music gets good enough to replace the elevator music Muzak crap that they have to pay $2000 to license. reply ein0p 16 hours agorootparentprevWill it be better than AI six months from now, that’s the question. My money is on “no”. reply MarcelOlsz 16 hours agorootparentAt least the classical world is safe. They want no part in AI. reply throwup238 14 hours agorootparentI think it's a lot better at classical, orchestral, and instrumental music than it is at anything requiring vocalization. I created this in less than 20 minutes: https://app.suno.ai/song/eb93c25b-bdbe-4c9f-8e03-66e9479c869... I need to stem it, fix it up a bit, and remix for stereo in a DAW but it's much better than I expected for my first ever piece of music. Obviously it'd take a lot of work to create a Hans Zimmer level OST from the tool but IMO it wouldn't feel out of place on a Ludovico Einaudi album or on some Spotify or Pandora classical radio. reply programd 12 hours agorootparentThat's actually a very good piece. Like something I'd hear on late night Paradise Radio. If I was creating an indie movie on no budget I'd be all over this technology for the soundtrack. I don't think musicians and composers are going to disappear as a consequence of this technology, in the same way that theatre actors were not made obsolete by film. What I do think is that a whole new category of professionals will be created - musicians and composers who get paid to train AI models. I bet it will pay better then the laughable amounts that are streaming royalties. reply recursive 12 hours agorootparentprevAt some point in the future, wanting no part in AI-generated content is going to be like that old Onion headline. \"Area Man Constantly Mentioning That He Doesn't Even Own A Television\". reply recursive 15 hours agorootparentprevSomeone is writing it. There are a lot more than 10 people that want to be in the top 10. It's hard to get into the top 10. You might not appreciate it as art, but the songs that are there are good at something. You could call it being catchy. AI is not even close on this metric. reply newswasboring 15 hours agorootparentIts not even close now. And these things have been out maybe a few months? Of course even the potential of the current tools aren't fully explored. reply recursive 13 hours agorootparentI think it will get there, wherever \"there\" is. I think it's very impressive now, as a technical marvel. But it's really not competing with the best humans yet. I don't say this to dismiss it. I say this as an appreciator of music who is neutral on AI. Probably one day I'll listen to mostly AI generated music. But it won't be this month. reply CamperBob2 17 hours agorootparentprevI don't believe people would listen to AI generated music for the same reason AI isn't impacting fine art. Pretty soon we'll be reviving the old Palmolive \"You're soaking in it\" commercial (https://www.youtube.com/watch?v=_bEkq7JCbik). We'll all be soaking in it, and no, you won't be able to tell the difference. reply nickcw 18 hours agoprevHere is my effort https://app.suno.ai/song/13cffa0c-bbd5-41b6-abde-43332b21b0f... I took the litany of fear from Dune and got Bing Chat to re-write it to be about facing down code complexity, then I put those lyrics into suno.ai to turn it into a 2 minute song to express all your emotions about code that needs to be simplified ;-) reply billh 18 hours agoparentI just had it make a rap song of its own ToS https://app.suno.ai/song/7995f966-6265-4b34-a68e-400981f5931... reply dilap 15 hours agorootparentAsked Claude Opus to minimally modify the lyrics to add rhymes https://app.suno.ai/song/40d0fb88-246b-42f9-8998-0387e75262e... reply Balgair 16 hours agorootparentprevLol, nice! When they allowed longer text inputs, and for faster rapping, I can really see this kinda thing taking off with L1s and med students. Like the Animaniacs song about the state capitols. Or like a Homeric epic that is meant for remembering and singing. The method of loci may have a new competitor as a way to remember things here. reply billh 16 hours agorootparentOne of the things that came to mind when I was listening to the ToS song it generated was a video I had watched years ago on the very dry topic of Rule 803 - Hearsay Exceptions but it was put to a catchy tune and made it very memorable and easier to digest. (https://www.youtube.com/watch?v=UoJ6fgIKYy8) reply sen_armstrong 14 hours agorootparentprevI think it might be more memorable when the med students do the writing ... and singing. https://www.youtube.com/watch?v=GVxJJ2DBPiQ Diagnosis, Wenckebach *(what?)* It's AV nodal block and that's a fact *(yeah)* Take PR interval and lengthen that *(yeah)* bradyarrhythmia and heart attack *(oh-no!)* AI songs do make sense if AI will be making the diagnosis! reply newswasboring 14 hours agoparentprevThis thing can generate lyrics and music for a hindi song. Its way better than I expected. Here's a song about wrestlemania. https://app.suno.ai/song/018ca476-803b-4a45-81ed-c7263e08ef3... reply sentrysapper 14 hours agoparentprevI chuckled at gone \"live\", but otherwise that was pretty good code poetry. Thanks for that. reply muxator 14 hours agoprevI suppose the focus was on voice synthesis here. I won't add anything about it since other commenters have already said significant things about this wonderful feat. Musically, however, I can't help but notice that these models are still very far from being able to generate something interesting: from harmony, to tempo, to musical structure, to dynamics, everything is muddled and without structure. I guess there is still very much to work on, and I am not sure that purely generative models can attain higher levels. Maybe a mixed rule-based and generative approach would do? The progress is really fast in this field, I really do not know. reply notjulianjaynes 5 hours agoparentTo my knowledge, the model being used for this is \"chirp\" which is 'based on' bark[1], an AI text to speech model. The github page for bark links to a page about chirp, which returns a 404 page for me [2]. My guess is that the model used for suno.ai's song generator isn't too much different than the text to speech model. I also have a hunch is that it was more like a coincidence than intentional that the bark model was capable of producing music, and that was spun off into this product. Unfortunately, there seems to still be issues with bark when generating long (like book length) spoken audio. Which is too bad, as someone who's worked jobs that require lots of driving, it would be awesome to be able to have any text read to me in a natural sounding voice. [1]https://github.com/suno-ai/bark [2] https://www.suno.ai/examples/chirp-v1 reply BriggyDwiggs42 14 hours agoparentprevI think historically every time someone says that the solution to an ai problem is more structure, the truth turns out to be an issue mostly of data and scale reply muxator 12 hours agorootparentThat's probably true. Maybe there is a point to trade computational/energetic efficiency for attainability of a result. Let's see how this unfolds. reply Almondsetat 13 hours agoparentprevWhat structure and tempo can you realistically give to the MIT license? reply gqcwwjtg 10 hours agorootparentA lot more than this. Elton John improvising on an oven manual is the high bar in my opinion. https://m.youtube.com/watch?v=8GuI4UUZrmw reply muxator 12 hours agorootparentprevI'll try to give a serious answer, even if I suppose yours was a nice joke :) Music is a language, even if with no semantic. It has conventions, dialects, a syntax, a grammar. There are multiple dimensions a musician uses to convey what he wants/feels: just like an actor has to control at the same time its voice, posture, interplay with other actors, so a good musician is aware of the structure of the piece he is composing/executing, the relations between the various subparts, how the musical discourse progresses in time, besides agogic, dynamics, sound color. All of those aspects are continually perpetually compared against the conventions of the genre, mixed, evolved, strictly followed or balatantly negated. This is something that normally a professional musician takes decades to master (apart from musical geniuses). A listener takes less time to educate himself to appreciate those nuances (but not too little: let's say ~years). Once you develop a taste, it becomes very obvious to see through the spectrum that goes from bad quality tunes to musical artistry. I see nothing musically interesting in this (wonderful) PoC of speech synthesis. Just to be clear: I did not see anything particularly stunning even in Google's Bach Doodle from some years ago https://doodles.google/doodle/celebrating-johann-sebastian-b... reply Almondsetat 3 hours agorootparentYou didn't actually answer reply wildzzz 7 hours agoparentprevAll the AI generated music just sounds like someone jamming without any hint of any real melody, original or a cover. It's very strange to listen to. It sounds exactly like an AI generated photo of a person looks like. Looks/sounds kinda real until you look/listen closer. reply ickyforce 5 hours agorootparentSome of these are just \"nice\" but I'd probably buy this one: https://app.suno.ai/song/abddd209-4ad7-469d-82b9-f0117db0e51... reply kevinmhickey 12 hours agoparentprevReminds me a little bit of Catholic mass when the priest \"sings\" some of the sections. There is no consistency, no cadence, but their voice goes up and down. It's high-effort talking. I wonder if these models would do something better if the text were poetic or punctuated differently. reply layman51 15 hours agoprevThis is scratching an rare itch for me because I am a heavy subvocalizer when I read just about anything, and when I have a song stuck in my head, I end up wondering what it would sound like if someone sang the words I’m reading to the tune of the song. reply IanCal 11 hours agoparentProbably relevant, I took a photo of my kids \"curious questions about space\" book and threw the words into a song https://app.suno.ai/song/f283429e-ec3e-4152-b5be-a57cd72a6d9... They've been listening to it in the bath to huge success. Particularly the change at about 40s for \"why can't we breathe on the moon?\" which feels like an excellent song lyric. Honestly I'm blown away at how well it does. reply _sys49152 14 hours agoparentprevfor college i would convert my physical textbooks to wordfile text, then convert the wordfile text to computer voice mp3s and use those to play in the background to help me studying. break up chapters or sections of the college textbook into suno songs instead - itd be maad interesting how much better that wouldve helped my studies. monotone computer voices of 10+ years ago will put you to sleep. reply cwillu 18 hours agoprevI'm impressed how it managed to extract rhyme from that license. The software is provided (as is) without warranty, of any kind express, or implied. reply paulddraper 18 hours agoparentExactly. Tough constraints, having zero flexibility in lyrics. reply fho 18 hours agoparentprevYeah, there are some good rhymes in there. Actually better rhymes than those ChatGPT delivers if asked for lyrics or poems. reply Fnoord 14 hours agorootparentPlot twist: MIT license was written by a poet. reply isodev 19 hours agoprevThe girl is sad because we don’t know the name of the people/artists on which the music and her voice is modelled. reply titzer 19 hours agoparentIn the great web tradition of harvesting the vast body of other people's work in the large[1] and shoving it through huge amounts of computation to wring out a nickel's worth of value that will eventually manifest in some good-paying SWE jobs, a rich executive class, and a whole lot of shareholder value and inevitably mutate in another goddamn ad-serving platform. [1] Ha, the poor millions of dumb minions who put their work on the web thinking it might be fun for others or garner themselves a small following, they didn't check the terms of the EULA! reply arghwhat 18 hours agorootparentThese kinds of discussions always leaves me wondering if people consider how actual humans learn their craft, constantly studying and mimicking others. Inspiration is to use existing experiences however mixed together, while originality comes from an input or an experience that others have yet to use. \"Write a sad song about the MIT license\" is certainly such new input, and if I was commissioned to write the song it would be based on inspiration (i.e., \"use training on\") music I have heard or studied. And yes, none of the musicians I have listened to or have studied will benefit from the endless money fountain I'd acquire from composing such song. reply yoyohello13 18 hours agorootparentIn the case of a human studying, a person puts in effort and gets rewarded for their efforts. In the case of AI, a person puts in minimal effort to generate something that devalues the work of all the people who did put in effort. reply arghwhat 16 hours agorootparent> In the case of a human studying, a person puts in effort and gets rewarded for their efforts. When someone needs something composed, they don't learn how to write music. They pay someone else the bare minimum, e.g. a few bucks on fiverr. The person will spend the least possible amount of effort to try to make their life go around with the little money they got. When you then use an AI model, the work done for those five bucks is replaced by work done for almost free. Neither the person you would hire or the AI credited those who created the material they trained on. reply dsign 13 hours agorootparent> When someone needs something composed, they don’t learn to write music… Speak for yourself! There is only one thing that scares me more than composing music, and that’s paying somebody a few bucks in fiverr to do it for me. reply arghwhat 3 hours agorootparentDespite your personal fears I believe I spoke for the vast majority of cases rather than just for myself. Although I suppose royalty-free stock music is the norm nowadays for most commercial uses, which takes it a step further, anonymizing the composer entirely... reply fho 17 hours agorootparentprevStable Diffusion did cost 500k to train ... I wouldn't call that \"minimal effort\". (And that is only the computation cost.) reply drusepth 15 hours agorootparentprev> In the case of AI, a person puts in minimal effort to generate something that devalues the work of all the people who did put in effort. Worded differently: people who couldn't otherwise produce skill-based works of value have had the barrier of entry lowered for that specific medium of expression, allowing for more works across a wider spectrum of skill. reply bugglebeetle 15 hours agorootparentIt’s so bizarre when people say stuff like this. There is absolutely nothing preventing the unpracticed or untalented people from any form of creative expression. What instead people who use AI seem to want is for unpracticed or untalented people to perform at the level of the practiced and talented, but this is no net gain to anyone. Why? Because only a rare subset of people who ARE practiced and talented create anything of interest or value in the first place. What this tells you is that skill or level of performance is not the barrier, but a means through which great things CAN be achieved (i.e. necessary, but not sufficient) Flooding the world with unpolished, unpracticed works, AI-tuned to the level of being mediocre, is a creative and intellectual dead end. reply neon5077 11 hours agorootparent> for unpracticed or untalented people to perform at the level of the practiced and talented This is what tools are. Cheap digital tablets have done away with the need for expensive consumables. You can just download a different brush style instead of learning a physical technique. No waiting for paint to dry or smudged pencils. The barrier to entry for painting has dropped to a one time investment of like a hundred bucks. Almost nobody mixes their own paint, nor stretches their own canvas. Those skills aren't needed anymore. It's possible to build very precise machine parts by hand. It's very difficult and requires great skill, so nobody does that. Some do and are admired for it, but everybody else uses precise machines to make precise parts with nearly no effort. It's just a tool. Only difference is that we had assumed art would never be automatable. Objectively, I don't think this is a bad thing. It doesn't change the subjective value of art any more than the average cartoonist devalues the Mona Lisa. It's just a new form of art, there will always be people mixing their own paints and stretching their own canvas, just as there always has been. It's only a problem because in our society you either have a job or you starve. No one can afford to be an artist. Those that do tend to grind out as many pieces as fast as they can so they can pay the goddamn rent. If not for that, these AI tools would be pretty cool. reply drusepth 14 hours agorootparentprevI think the bizarrity arises from the following differences in beliefs: * That \"_any_ form of creative expression\" is a viable creative substitute for people wanting to create in a _specific_ medium of creative expression -- especially those that had a high barrier of technical skills required to be seen as \"good enough\" to share. * That a person who has an idea for art will put in the necessary time to become proficient enough to create that \"good enough\" art through traditional means (IMO demonstrably incorrect), and that is preferred over that person just not expressing a lower-quality version of that idea at all. * That those who use AI primarily want or expect to \"perform at the level of the practiced and talented\" (i.e. top-tier art) rather than using it to produce art they otherwise couldn't have, even at low- and mid-level qualities. * That there is no skill or talent in using AI tools to produce art (or that the skill or talent using AI tools is meant to be a full replacement for traditional artistic skills or talents). FWIW, I'm a long-time sketch artist and acrylics painter (~20 years). There are many mediums, subjects, and styles that I'm not good at -- and I enjoy using AI to express myself in those areas (and have also liked using AI to create songs to show to my more musicially-adept wife...). But even in my own wheelhouse (landscapes and still life), I also often use AI to brainstorm composition, perspective, colors, textures, lighting, etc. It's a great tool for experts to lean on, but an even better tool for non-artists who couldn't or wouldn't otherwise share their art. reply titzer 14 hours agorootparentprevIndeed. As an amateur guitarist, but a professional virtual machinist, I have a ton of respect for people who have dedicated their whole lives to mastery in any one particular area. To have a machine gulp down untold eons of human exertion and then barf out soulless mimicry, no matter how jaw-dropping of a feat of engineering behind it, and then mint no-talent ass clowns by the million because viral videos make an awesome advertising platform--it's just some kind of dystopian peak tech, except the dystopia is mildly amusing rather than a disappointing and jarring marginalization, flippant dismissal of all of us. reply notahacker 18 hours agorootparentprevEven the most derivative of singer songwriters tend to use their own voices rather than a weighted average of the voices of other singers in their genre... reply bobajeff 18 hours agorootparentIs that why so many people sound so much like Adele or some other popular artist? reply arghwhat 15 hours agorootparentprevUsing the skills they presumably developed listening to and copying other singers and studying music, with an instrument built from roughly the same instructions as everyone else. That a person can't sound like the weighted average is human limitation (although with modern pop people do get quite close!), not because new singers aren't trying to. That of course adds variation that we appreciate, but doesn't change the underlying similarity in how acquired skill is mimicry of those who acquired it before us - with very rare exceptions. reply notahacker 14 hours agorootparentNo, sounding like the genre-weighted average of Spotify simply isn't what singers try to do. They haven't listened to that much music, they have actual preferences, they have natural qualities to their voice which they're complimented on or asked to mask, and they're trying to hit notes based on their aural perception of harmony and related theoretical principles not based on the waveforms of other songs involving singer songwriters. The fact that they literally couldn't do what NNs do even if they wanted to also seems quite relevant to the fact that they don't do what NNs do. What next, are we going to argue that what programmers creating new programs are really trying to do is generate a prompt-weighted average of the bytecode of every program they've ever downloaded, and all that business analysis and functional spec and use of high level programming languages and expressed preferences for coding standards is irrelevant? reply arghwhat 2 hours agorootparent> they have actual preferences That's just a bias. > natural qualities to their voice That's the physical limitations I referred to, which isn't something humans tend to be happy about but can sometimes end up being a differentiating benefit. > What next, are we going to argue that what programmers creating new programs are really trying to do is generate a prompt-weighted average of the bytecode of every program they've ever downloaded That's a horrible strawman. Do you as programmer often read and write bytecode directly? reply notahacker 35 minutes agorootparent> That's just a bias. I'm beginning to assume you're an LLM, because I'm not convinced a human would honestly try to argue that their emotional reaction to their favourite songs is basically equivalent to flipping the values of some bits to ensure that they generate music more similarly to them. > That's a horrible strawman. Do you as programmer often read and write bytecode directly? As an improvising guitarist (even a very mediocre one) my creative process is even further removed from an LLM parsing and generating sound files directly.... reply commandlinefan 18 hours agorootparentprevI wonder if this won't drive a resurgence of demand for live performances - as recording becomes more and more artificial, live performance will mean more. (Or maybe, as a live performer, I'm just wishful thinking here...) reply UncleOxidant 15 hours agorootparentprev> eventually manifest in some good-paying SWE jobs Unless Devin has his way. reply paulddraper 18 hours agorootparentprevGenerally speaking, people create internet content so that it is shared. All of the creators and subjects of meme formats... Should they receive royalty every time you post some inane mashup? reply dotnet00 18 hours agorootparentPeople also differentiate heavily on the basis of scale and profit. Artists are often fine with people sharing their posts and may even tolerate someone asking for permission to make printouts or whatever else for their circle of friends, but will expect some sort of royalty if you're asking to be able to sell prints of their artwork on a store. Hell, even with viral videos it's relatively common that normal people can share away while entertainment companies and influencers are expected to pay for a license. With memes it isn't clear exactly who made the first template, and the creation of them doesn't revolve around specific people in the same way, nor are they meaningfully tied to profits. When creators post their content online to be shared, they do it with the focus being on reaching individuals, not for it to be sucked up by soulless companies to extract all value without the intention of giving back. reply paulddraper 18 hours agorootparent> With memes it isn't clear exactly who made the first template. The Office, The Matrix, Lord of the Rings, Django Unchained, Game of Thrones, etc These works have identifiable creators. reply titzer 18 hours agorootparentThe conversation is quickly devolving into a vacuum of ignorance where things like royalties, fair use policies, revenue-sharing agreements, parodies, sampling, etc, have apparently never been thought about. We're not talking about any of those things. We're talking about wholesale digestion of the entirety of human knowledge by automated means, which is now not just theoretically possible, but routine. reply dotnet00 17 hours agorootparentprevThose aren't meme formats in terms of what is typically meant by meme. reply titzer 18 hours agorootparentprevThis is not that. We're not talking about some inane mashup, but a wholesale digestion of every creative thing any person ever did by a monster computer cluster whose scale dwarfs imagination, which then promptly uses it to maximize \"engagement\" to gather eyeballs to feed them advertising. It's profoundly messed up. reply newswasboring 14 hours agorootparent> which then promptly uses it to maximize \"engagement\" to gather eyeballs to feed them advertising. This is the real problem, right? People don't dislike generative AI, they dislike the attention economy. Yet I see more disgust towards AI than the company policies which suck. I don't understand why. reply xetsilon 11 hours agorootparentI think it is more that art, film and music have largely been replaced with complaining online about various subjects as the major form of entertainment in America. reply titzer 14 hours agorootparentprevOh, haha, yeah. I guess I'm the opposite--I actually like AI more than the attention economy! At least one of them is not actively trying to drain my brainpower and skill set and get my to buy stuff and do stuff I wouldn't otherwise buy or do. reply saulpw 13 hours agorootparentyet reply paulddraper 18 hours agorootparentprevThe cost of that computer cluster must also dwarf imagination. I don't begrudge crypto miners either. reply titzer 16 hours agorootparentI wasn't aware of a right to recoup the costs of any bad idea, which seems to be what you're implying here. Because computers, therefore profit? Huh? reply paulddraper 16 hours agorootparentThe earlier comment was \"vast work\", so the size of effort is somehow relevant to the discussion. reply titzer 14 hours agorootparentIt isn't. If a serial killer spent a week digging mass graves by hand, they don't get years taken off their sentence. You don't get points just for working hard or spending money, particularly when it cheapens or just appropriates other people's work. reply throwaway74432 18 hours agoparentprevWe don't know the names of all the people on which the style and content of your comment is modelled either. reply kube-system 13 hours agorootparentThat's correct, but they are (probably) human, which is pivotal to the application of copyright law. reply mirekrusin 18 hours agoparentprevShe's sad because she knows the license will be changed to business non compete one in a year. reply non-chalad 18 hours agoparentprevIs it Hatsune Miku? Twitter is glitching out again, so I can't hear. reply schroeding 18 hours agorootparentNo, it's a synthetic voice from suno.ai, sounds like a (very sad) American singer-songwriter. reply SilasX 18 hours agoparentprevJust like we're all sad because we don't know the names of the people whose work or interactions influenced Stephen King's writing. reply Conasg 18 hours agorootparentI wonder who was the first to claim this was plagiarism; ironically, everyone else seems to have mindlessly plagiarised their belief reply ronsor 11 hours agorootparent95% of beliefs are shamelessly plagiarized from someone else. reply fho 17 hours agorootparentprevThe funny thing is that most creatives are quite open about their influences. reply reducesuffering 14 hours agorootparentThey wouldn't be if every named influence wanted a 5% cut of all future projects. reply bena 13 hours agorootparentprevAI isn't influenced. It doesn't have restrictions. It doesn't have to work within confines. AI can always remember the word it wants to use. It always can hit the note it intends. And it can hit every note. Etc. It uses the corpus of training data and mashes it into a new form. Stephen King won't be able to remember every word of every story he's ever read. And if he wants to make something \"Lovecraftian\", it'll be what Stephen King thinks is Lovecraftian. And there will be something to that. Some bit he believes is more or less important than other people And those bits are what makes Stephen King, Stephen King. Everyone has had access to the same material King read. Access to the same tools he used to create. Everyone had the chance to effectively be Stephen King. But there is just one. Because there is some unique bit of observation or recall or combination of such things that is unique to King. And from what I've seen so far, these LLMs can't do that. There is a missing element of pure imagination. reply SilasX 13 hours agorootparentYou can tune AI output. reply bena 13 hours agorootparentBut you can't make it creative. You can't say \"give me something cool\" and have it produce something of note. reply SilasX 11 hours agorootparentYes, you can absolutely play god of the gaps. reply bena 11 hours agorootparentHow am I doing that? I am claiming that LLMs lack imagination. They are incapable of creating out of whole cloth or interpretation. Saying they cannot create based off of a vague suggestion is very much in line with that claim. I consider it a vital difference between Stephen King being inspired and LLMs mashing training inputs together. reply cwillu 18 hours agoprevThe Free Software Song: https://app.suno.ai/song/2ce5eab5-d1c5-48b2-91a0-8e6095e29ed... https://www.gnu.org/music/free-software-song.en.html: “Richard Stallman and the Free Software Foundation claim no copyright on this song.” reply janalsncm 15 hours agoprevMade one reading the Declaration of Independence. I am impressed. https://app.suno.ai/song/54898804-8cd9-4b6f-a18d-3ffbe728579... reply qingcharles 15 hours agoparentI can see this being \"a thing.\" I tried one too - Gangsta Rap Constitution: https://app.suno.ai/song/0ed4c4e2-9a92-40c1-a1ab-20ae49b7a8d... reply floxy 14 hours agorootparentGive Me Liberty or Give Me Death: https://app.suno.ai/song/d67a4c29-9f2a-41b0-9ff8-2c8138a1a7a... https://app.suno.ai/song/89a48c01-c7e5-487a-b825-4a3978b7259... reply bmacho 13 hours agorootparentprevThere should be a whole Broadway musical of this reply janalsncm 2 hours agorootparentSomeone should get a radio frequency and broadcast these 24/7. Civic Gangsta Rap. reply cenan 13 hours agoparentprevOh nice, I had the same idea. https://app.suno.ai/song/a693c847-7ce6-475c-adc5-0328786b901... reply sircastor 16 hours agoprevReminds me of Regina Spektor's style. And some of the generated phenomes actually just sound like stylistic auto-tuning. I kind of like it. I'm sure many have already observed this, but I think the thing that most artists fear from AI is not that AI will be able to produce works on parr or superior to human works, but that most people won't care enough to value the difference. reply lordnacho 18 hours agoprevIt's a satire generator. Take any text you want to make fun of, turn it into music. I'm not sure whether I've just run out of credit, or Suno actually knows what the political sensitivities of the text might be, but I can't generate a second amendment song. reply floxy 12 hours agoparentIn the lower left hand corner there is a \"subscribe\" button, and above that a \"credits\" counter. reply seydor 15 hours agoparentprevHas anyone had decent results with C++ code? reply bombcar 15 hours agorootparentWon't be able to beat \"Program in C\" https://www.youtube.com/watch?v=tas0O586t80 reply xedrac 19 hours agoprevThe song really picks up when you get to the all CAPS section. reply no_op 14 hours agoprevIn a similar vein, LessWrong released an entire AI-generated album with lyrics adapted from significant posts made there over the years: https://www.lesswrong.com/posts/YMo5PuXnZDwRjhHhE/lesswrong-... I think I'm going to enjoy how surreal widespread access to generative AI will make the world. reply plagiarist 13 hours agoparentI'm not seeing a Roko's Basilisk track, disappointing. reply zeekaran 13 hours agorootparentI've been with LW people for years and no one has ever mentioned Roko's Basilisk. reply yamrzou 18 hours agoprevThis is impressive, but part of what makes it so is that we are not used to it. As these kinds of AI-generated music/images/videos become ubiquitous, it will be the new normal and they will become less impressive. reply callalex 14 hours agoparentMaybe, but I think there is something innately funny about making computers say silly things. As a small child it was peak comedy to me making a Macintosh say “fart” and it’s still funny to me when a computer sings the MIT license. reply thenickdude 12 hours agorootparentOn that theme, I asked Suno to sing a rap but \"remove all the vowels\", and it's hilarious how well it attempts to sing the silly result: https://app.suno.ai/song/30f8223e-0d0b-4cac-8b3f-5d8f0f743e2... The lyrics generator is some version of GPT so you can give it natural language instructions like this. reply chrisdsaldivar 13 hours agoprevBarely related but this reminds me of a video where Sir Elton John sings the text of an oven manual. https://youtu.be/8GuI4UUZrmw reply b3lvedere 19 hours agoprevI was wondering if she would sing really loud at the ALL CAPS sections, but fortunately she did not. Still better than most Eurovision Contest songs :) reply tgv 18 hours agoparentBut the accompaniment changed. Very uplifting. reply mintplant 18 hours agoparentprevDisagree, Eurovision is stacked this year! reply ccozan 18 hours agoparentprevI have my song ready, now I need to know how can I make a video clip based on it? reply thenickdude 12 hours agorootparentMy approach to generating a music video was to generate scenes using DALL-E 3, and then animate those using Stable Video Diffusion (SVD). SVD doesn't have well-controllable motion and is utterly blown out of the water by Sora, but it's what we have right now. Here's the resulting vid, \"a death metal song about a macro photographer\": https://www.youtube.com/watch?v=kNVRQ1Zg-a0 If you only want a video file from Suno to share with the default static lyrics screen on it, hit Download Video from the three-dots menu. reply mastermedo 14 hours agoprevI generated a song in 30 seconds from getting on the site, and generated a song that is crazy relatable, funny and sounds good. Made the whole family smile. This is going places. reply fivestones 11 hours agoparentCare to share the link? reply MyFirstSass 14 hours agoprevThis is so much better than stable.audio released yesterday!? I've dabbled in music production and this is just unbelievable. Both amazing and a bit sad because this is already so much better than would i would have anticipated. First illustrators, copywriters, then VFX guys, and now music. We're going to loose so many jobs in the creative sector right? reply gardaani 18 hours agoprevThis reminds me of OpenBSD release songs! https://www.openbsd.org/lyrics.html reply Fnoord 14 hours agoparentAnd early nerdcore. (Dual Core FTW!) reply maxglute 15 hours agoprevGoing to make those boring textbooks sound more tolerable. Interesting implications for education. If this was a foreign language I didn't understand, I don't think I would have been able to tell it was generated. reply amelius 14 hours agoprevReminds me of Richard Dreyfuss reading an Apple license. https://www.youtube.com/watch?v=Cu0lqUlHEko reply JacksonWaschura 19 hours agoprevWow! I hadn't kept up with music generation for the past few years. It's come a long way! Long-term coherence, reasonable-ish melody, all on top of very unmusical text. Very impressive. reply cenan 14 hours agoprevSong lyrics (generated by ChatGPT) based on the The Declaration of Independence. https://app.suno.ai/song/a693c847-7ce6-475c-adc5-0328786b901... Haha this is amazing! reply amelius 13 hours agoparentChatGPT has no humor, but this certainly made me laugh. reply spdustin 9 hours agoprevHad Claude Sonnet write the bones of this song about an AI convincing you it's got your back. Edited for segmentation in Suno, and it turned out pretty well, though it didn't quite hit the style I was looking for. https://app.suno.ai/song/4f96f485-8d84-4df0-9a9c-941984137cc... reply s-macke 18 hours agoprevWe came a long way from the first synthetic singing voices. https://simulationcorner.net/SAM/sing.wav Edit: https://youtu.be/Rm4ZCGgzeeU?si=upK-qCMev8ZaibIa&t=222 reply schroeding 13 hours agoparentEven older, Daisy Bell on an IBM 7094 from 1961: https://youtu.be/41U78QP8nBk?t=63 reply lemoncookiechip 14 hours agoprevSuno.AI is very fun. I find that asking ChatGPT to create lyrics and then feeding it gives some great results, although half the generations tend to have a bit too much static, so you have to keep generating. reply etse 7 hours agoprevNice results! It reminds me of the Portal song about Aperture Science: https://www.youtube.com/watch?v=Y6ljFaKRTrI reply Terretta 6 hours agoparentYes: The cake is a lie, but the music is real It’s all fake when the truth is revealed The autotune electronic voice seems likely styled on GLaDOS. reply genter 16 hours agoprevCan I include this as LICENSE.mpeg in the root directory of my projects instead of a text file? reply abeppu 14 hours agoparentAnd does the requirement that \"this permission notice shall be included in all copies or substantial portions of the Software\" mean that the mpeg specifically must be included? reply sho_hn 14 hours agoprevhttps://www.eikehein.com/kde/plasma6.mp3 Ok, this is pretty fun. reply uyzstvqs 12 hours agoprevSuno is pretty cool. If I had to guess this uses Suno's Bark and Facebook's MusicGen? The output of the latter is used as conditional layers for the prior similar to ControlNet? Anyway, what will be interesting is when this can be done locally on consumer hardware with open-source AI, a nice UI and Vulkan/DirectML GPU inference. reply _DeadFred_ 15 hours agoprevI remember when Solaria came out there were a ton of people making emotional spiritual music with it. It felt so odd, robot voices singing to God and about the wonder of experiencing life. Sounded pretty though. Soon we will have 'preacher's in a box' that will sing to lift you up, mentor you, guide you through life. Most will even be 'non-religious' but will basically become your religion, your guide through life. reply philipov 15 hours agoparentIt's a real Nier: Automata vibe. The machines all chant \"Become As God\" as they try to sacrifice you. reply EwanG 19 hours agoprevAmusing, as well as a decent ad for the latest version of Suno.ai reply _sys49152 14 hours agoprevmost impressive ive heard on suno was a live performance. all the live performance cliches including the crowd singing along acapella. it was unfuckingbelievable - and at the same time i can see how that can get burned out real quick by others replicating same idea over and over. reply seydor 15 hours agoprevThe chorus (or all caps part) is now burnt in our Eula memories. Wonder how long it will be until someone sings Mein kampf though reply butz 18 hours agoprevIs there any information how such songs are made? It probably is way more complicated to get a decent result than one might expect. reply grumbel 15 hours agoparentIt's suno.ai (has a free trial), works much the same as image generation, you give it a description and it writes a song in a couple of seconds. Lyrics can be customized: https://app.suno.ai/song/41fde9b6-a722-4c39-92dc-8a8296c018c... reply spyder 13 hours agoprevI have a dream: https://app.suno.ai/song/d2f8e712-80ae-40ae-967b-90e13278da5... reply crvdgc 10 hours agoprevYou see funny little clips, I see more annoying customized ads, which play different lyrics for different people. reply gpm 10 hours agoparentHere I was thinking \"game with music that talks about your game\". Think something like skyrim where you have nords singing ballads based on what you've done, and what your character is named (built with an llm generating lyrics from a json config the game spits out, which is in turn fed into this). Your version unfortunately sounds much more plausible and profitable. reply visarga 19 hours agoprevI made one too: https://twitter.com/visarga/status/1775663297297084840 the ending is cool and unexpected reply kybernetikos 13 hours agoparentunfortunately twitter links don't work for me (I don't know why). Do you have the suno link? reply tivert 19 hours agoprevTwitter is stupid now, so I can only see the linked post. But are there instructions to replicate this, and has anyone done so? Just kind of skeptical of videos of demos in general. If this is legit, the Spotify spam is going to become atrocious and probably unmanageable. reply donbrae 19 hours agoparentThe actual page at suno.ai: https://app.suno.ai/song/da6d4a83-1001-4694-8c28-648a6e8bad0.... reply codingdave 19 hours agoparentprevYeah, it works, and doesn't need any technical instructions. Just go make a song on suno.ai. I had done a folk song version of my resume. It wasn't going to become a hit or anything, so I don't see this replacing any real musicians, but it absolutely worked to create a passable performance as a song. reply cruano 18 hours agoparentprevThe Spotify spam _is_ already atrocious and unmanageable. If anything, it might get a little bit more creative instead of people just publishing the same samples from Splice everywhere. reply huytersd 19 hours agoparentprevYeah you can generate any type of song with really good results on suno.ai reply andrewmcwatters 14 hours agoprevBabe, new cover of GNU General Public License v2.0 just dropped! reply mayoff 14 hours agoprevIs this sound file itself under the MIT license? reply OkayPhysicist 12 hours agoparentCurrent case law suggests this song would not be copyrightable in the US. reply pk-protect-ai 19 hours agoprevAbsolutely amazing! reply haunter 17 hours agoprevEverything is a Remix reply donbrae 19 hours agoprevImpressive! Quite a nice song, too. reply m3kw9 13 hours agoprevTo get chorus right I’m not sure if LLM type tech can accurately repeat the chorus it has made up before. Many songs have very repeated chorus. An example is U2s One. “Is it getting better..” and then another chorus “did I disappoint you..” Current generated songs are made like sentences where you hear entire song without much structure reply syngrog66 14 hours agoprevBillie Eilish songgen as a Service reply gedy 15 hours agoprevSuno.ai and the underlaying technologies are really quite amazing. I've done a few things like: * Put a poem my late mother wrote to music for her memorial *versions of 80's new wave songs and they came out so lovely compared to what I'd be capable of as a musician, but puts me in the role of a \"producer\" of sorts tuning the sound and vibe. Really well worth the money. reply DonHopkins 19 hours agoprevNow do The Ballad of ICCCM! https://www.x.org/releases/X11R7.6/doc/xorg-docs/specs/ICCCM... reply fetzu 18 hours agoparenthttps://app.suno.ai/song/1dc75742-0e0e-4d5a-97b6-d00b9cffc2f... or https://app.suno.ai/song/19b86cb0-3f4a-4ed4-9aae-c9c2604321f... And the overproduced version: https://app.suno.ai/song/1e0bf4e2-7850-4401-85db-799a763f732... reply DonHopkins 17 hours agorootparentGiving Up Selection Ownership https://app.suno.ai/song/86040709-94f1-4de1-8703-7b306b48b32... ICCCM Summary of Window Manager Property Types https://app.suno.ai/song/52d08a23-8e1e-4f03-8e8c-e4df610cef9... reply block_dagger 18 hours agoprevnext [2 more] [flagged] samatman 17 hours agoparentIdiom doesn't work that way. https://www.youtube.com/watch?v=UnfM-7py58E reply yinser 18 hours agoprev [–] Sunoslop lol reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The discussion revolves around AI technology in music creation, covering vocals, lyrics, and complete songs, exploring its quality, practical uses, and potential impact on the music industry.",
      "Participants share experiences and opinions on AI-generated music, touching on AI's role in creative work, its constraints compared to human creativity, and the evolving art creation landscape with AI.",
      "The conversation highlights the blend of technology and music, examining AI's increasing influence and the ongoing transformation in the music industry."
    ],
    "points": 604,
    "commentCount": 234,
    "retryCount": 0,
    "time": 1712239071
  },
  {
    "id": 39936284,
    "title": "German State Transitions to Linux and LibreOffice",
    "originLink": "https://www.zdnet.com/article/german-state-ditches-microsoft-for-linux-and-libreoffice/",
    "originBody": "business Home Business Enterprise Software German state ditches Microsoft for Linux and LibreOffice Why? Schleswig-Holstein cites cost, security, and digital sovereignty - though not necessarily in that order. Written by Steven Vaughan-Nichols, Senior Contributing Editor April 4, 2024 at 2:06 p.m. PT The Document Foundation Thanks to hardware vendors working hand-in-glove with Microsoft, many people never realize there are alternatives to Windows and Office. But that's not the case in the European Union (EU) and China, where computer users know all about Microsoft's dominance on the desktop -- and many don't like it. So, when Dirk Schrödter, digitalization minister for the German state of Schleswig-Holstein, announced the state government would switch from proprietary software \"towards free, open-source systems and digitally sovereign IT workplaces for the state administration's approximately 30,000 employees,\" there was cause for rejoicing among Linux desktop fans. Also: The best Linux distros for beginners: Expert tested Specifically, Schleswig-Holstein is dumping Windows and Office for Linux and the popular open-source office suite, LibreOffice. The Schleswig-Holstein cabinet made this decision not because of Linux and LibreOffice's technical superiority, but because it values \"digital sovereignty.\" In the EU, digital sovereignty means protecting citizens' data from being vacuumed up by foreign companies and enabling European tech companies to compete with their American and Chinese rivals. As The Document Foundation, the organization backing LibreOffice, put it, \"The term digital sovereignty is very important here. If a public administration uses proprietary, closed software that can't be studied or modified, it is very difficult to know what happens to users' data.\" Exactly. Although Microsoft is trying to meet the EU's digital sovereignty requirements, European governments aren't -- shall we say -- all that trusting. Schrödter explained: \"We have no influence on the operating processes of such [proprietary] solutions and the handling of data, including a possible outflow of data to third countries. As a state, we have a great responsibility towards our citizens and companies to ensure that their data is kept safe with us, and we must ensure that we are always in control of the IT solutions we use and that we can act independently as a state.\" Also: Thinking about switching to Linux? 10 things you need to know There are other reasons the state is parting ways with Office and Windows: To save money and increase security. \"The use of open source software also benefits from improved IT security, cost-effectiveness, data protection, and seamless collaboration between different systems,\" Schrödter said.\" Going forward, the plan is to replace Microsoft Office with LibreOffice, Windows with a yet-to-be-determined Linux desktop distro, and other Microsoft-specific programs with open-source equivalents. For example, the plan is to use Nextcloud, Open Xchange/Thunderbird, and the Univention Active Directory (AD) connector to replace Sharepoint and Exchange/Outlook. If some of this sounds familiar, congratulations on having a great memory. Munich, the capital of Bavaria, Germany, switched from Windows to Linux in 2004. That move lasted for a decade before Munich returned to Windows -- in no small part because the mayor wanted Microsoft to move its headquarters to Munich. Also: 5 ways LibreOffice meets my writing needs better than Google Docs can Other countries, notably China, have proverbs that say they are much more stubborn when shifting gears from Windows to Linux. The last few Chinese government PCs running Windows have been replaced by systems mostly using Kylin Linux. The latest version of Kylin Linux started life as an Ubuntu Linux clone optimized for the Chinese language. In China, as in Europe, a large part of the motivation for the move was so that the local governments and organizations, rather than Microsoft, would control their desktop. This may not be why Linux fans wanted to see users abandon Windows, but it will do. open source GitHub vs GitLab: Which program is right for you? The best Linux distros for beginners Feren OS is a Linux distribution that's as lovely as it is easy to use How to add new users to your Linux machine Editorial standards show comments related The best AI image generators to try right now An 'unprecedented time': Inside the three-dimensional approach corporate executives are taking to generative AI How to download YouTube videos for free, plus two other methods",
    "commentLink": "https://news.ycombinator.com/item?id=39936284",
    "commentBody": "German state ditches Microsoft for Linux and LibreOffice (zdnet.com)405 points by CrankyBear 12 hours agohidepastfavorite279 comments qwerty456127 8 hours agoI can remember the same or very similar news from Germany appearing every now and then for over a decade. This time I almost believe them as there seems to be no alternative to LibreOffice given the changes Microsoft introduced during the recent years - forcing everyone to log-in with their Microsoft account at best, also moving from a classic desktop app to a web app. Conservative users like me and probably German state institutions consider classic desktop apps and web apps distinct tools for different tasks and don't want their desktop to depend on cloud. It is also worth mentioning that LibreOffice became much better since the time the discussion began. reply MenhirMike 5 hours agoparentLiMux (Linux for Munich) was started in 2004: https://en.wikipedia.org/wiki/LiMux Of course, Microsoft did some of their... persuation of politicians and initially killed the project in 2017, but it seems that since 2020 it's back. I do think that LibreOffice could need some more full-time User Interface people to polish some rough edges (please none of the hackjob wanna-be UX people that ruin all modern apps by obnoxious popups), so that could be a good use of some tax money. reply ctrw 4 hours agorootparentYou just copy office 97 and can't go wrong with it. The problem with having UX people on a team is that they need to jusity their existence which they do by change for changes sake. I've yet to meet anyone who appreciates ms office UX changes that happen every 5 years and move everything around. reply omnimus 3 hours agorootparent> The problem with having UX people on a team is that they need to jusity their existence which they do by change for changes sake. This can be said just about any profession working in software. The real issue is that nobody wants to accept that software can be finished especially management - because then you don’t have anything to sell. I wouldn’t be surprised if there were designers who fought tooth and nail against arbitrary changes. Surprise surprise good UX designers understand UX. But its like the developer who fights for keeping the PHP website because it works. Sorry i just dont like treatment non-programming workers as something lesser. So many software companies are sucessfull despite having fucked up technology (by programmers). And its sucessful only because they have great design or marketing or sales team. reply ctrw 3 hours agorootparentSoftware always has bugs that need to be fixed. It is never finished because it has an infinite number of ways it can go wrong. The same is not true for any of the non-software parts of software design. If you're a software shop you should have everyone who doesn't write code on a contract and not be afraid to terminate them when the product is mature. reply 7bit 2 hours agorootparentA software can only ever be mature for a period of time. Technology advances, work practices change, and your software should adapt to it. Imagine text editing stopped at Notepad, or image editing at Photoshop 1. Also, your second paragraph leaks of Americanism and undoubtedly, people are happier and healthier with stable jobs. reply ctrw 21 minutes agorootparentAnd consumers are better off when we don't keep the candle stick maker employed in the led factory. reply M95D 2 hours agorootparentprev> should have everyone who doesn't write code on a contract Does that include tech support? What a horrible world you are creating... reply MenhirMike 4 hours agorootparentprev> You just copy office 97 and can't go wrong with it. Clippy had its debut in Office 97, so let's please NOT copy that one. Other than that, yeah, I agree. Anything up to and including Office 2003 really, those were all great. (Also includes one of my favorite, underrated Office Apps, InfoPath) reply antod 8 hours agoparentprev> I can remember the same or very similar news from Germany appearing every now and then for over a decade. Yup I can remember (mainly via Slashdot) this being a thing going backwards and forwards for two decade or more. Back to the days of it being called StarOffice if I remember correctly. Sometimes it seemed the city or state was just doing some elaborate license negotiation. reply doctor_eval 6 hours agorootparentI remember using the original StarOffice to write up some consulting work that I’d done, it just have been the early 00s at the time, and I thought it was excellent - a worthy competitor to MS office of the time. And then over time it felt more and more bloated and slow, to the point where I started to think I maybe misremembered how good it was when I’d first used it. Is it just me? Or did it fall deeply into a hole? My recollection is that it hitched its wagon to Java for no good reason that I could see. reply cjk2 5 hours agorootparentIt was always pretty laggy. It didn’t and still doesn’t scale to any reasonable size document. The last thing I wrote in libreoffice was a technical specification around 200 pages long in 2019. It crashed regularly, wrecked the document and caused me a lot of work. I literally cut and pasted the entire thing into word, applied styles and cleaned it up over two days and had no problems with that. This made me dirty so I learned LaTeX and now use that. I have colleagues who still use Libreoffice for study and have the same problems. reply vidarh 3 hours agorootparentI've assembled and done editing passes (back and forth to an editor, using comments heavily) on two 200+ page novels in LibreOffice with none of those problems, so it sounds like something in your doc must have hit some specific edge cases/bugs. Not doubting you - I find it clunky and fully believe there's bugs there too. I did most of my writing in Google Docs, but chapter by chapter both because I find it easier to keep individual chapters separate and because Google Docs certainly can't (or couldn't, anyway) handle anything approaching that size without slowing to a crawl, but it just felt better than LinreOffice for the actual writing. reply cjk2 2 hours agorootparentMathematics and image embeds killed it. reply doctor_eval 5 hours agorootparentprevBy 2019 it was way gone. But in 2004 I reckon it was pretty good and not laggy at all on Linux. Or that’s what I remember. reply djbusby 5 hours agorootparentprevNot just you. What happens to all software is this feature expansion, until it's as bloated as it's predecessor. You have allowed the Dark Lord to twist your mind until you've become the very thing you swore to destroy! Hell, I remember a time when Jira was responsive and people loved it. reply tw04 7 hours agoparentprevProbably because it’s misrepresented every single time. The headline is “Germany is moving to X” and then you click the link and it turns out it’s a single state. This is the equivalent of “South Dakota is ditching Microsoft”. It’s 30k users which isn’t nothing, but it is VERY different from the German federal government moving away from MS. reply cjk2 5 hours agoparentprevYou don’t have to log in with O365 and office and windows can stand entirely alone and there is no requirement to be cloud connected. It all still works offline as a classic AD setup. You have to be an enterprise customer though. I literally have one of these on my desk. This is not a problem for state level organisations unless they are throughly incompetent at infrastructure provision. As for libreoffice I throughly dislike it compared to MS software. I have opened reproducible bugs against it in bugzilla that have been there for over a decade with no solutions. I got fed up in the end and just walked. Saying that I dislike office packages entirely. They are a Swiss Army knife. Lots of mediocre tools in an inconvenient format but simple enough for the lowest denominator of poorly trained staff to use. We can and should do better. I managed to switch entirely over to LaTeX document authoring and dedicated software to replace spreadsheets (all open source tools). The only remaining spreadsheet I have is personal finance which I can’t find a better solution for. As for cloud we just send PDFs around and collab on GitHub so O365 is dead for us anyway. reply qwerty456127 4 hours agorootparent> I managed to switch entirely over to LaTeX document authoring I tried and found it the weirdest language (BrainFuck aside) I ever seen. Sure you can copy-paste-modify it to do something similar to what everyone does but doing anything atypical seems prohibitively hard. I really miss a modern (first-class Unicode to begin with, also more intuitive while more rich and less verbose than HTML+CSS) human-oriented typesetting language. Perhaps it's time to invent what MarkDown is to HTML but to SVG. > As for cloud we just send PDFs around Can MS Office make hybrid PDFs the way LibreOffice does (embed the source document into the PDF)? reply stby 44 minutes agorootparenthttps://github.com/typst/typst looks promising, both the language and the tooling. I wonder where it will find its place in a world that is dominated by either Word or LaTex. reply cjk2 4 hours agorootparentprevWell it all makes a lot of sense but you have to understand it first. That takes time. I’m typesetting stuff which has complex mathematics in it so there are no better solutions. This is the best human solution we have. There are some workflows that you can’t do with anything else as well. For example I am doing a lot of semi hand drawn diagrams on my iPad in Goodnotes. These are exported as vector PDFs and trimmed and included in the LaTeX documents as figures. They are fully vector end to end which is a complete pain in the ass on any other package. As for hybrid PDFs I don’t know. I am mostly interested in not shopping the source document. reply finaard 5 hours agorootparentprevA lot of the office stuff can be handled by templates that allow only editing relevant parts - there's not really a need for majority of office workers to have access (and needing to know) all the features of an Office suite. Over two decades ago I was tasked doing a pilot with IBMs Document Connect for Lotus Notes - it was clear that it pretty much was an Alpha they've been selling us, but it was showing promise. In the end it went nowhere, probably because not too many other companies were interested in trying out a different approach at handling their documents. reply StressedDev 8 hours agoparentprevThe Office Apps still have desktop versions and they still work very well. reply lenerdenator 6 hours agorootparentIt will be interesting to see how long that remains true. Management of resources is the enemy of profit for both sides in the market. Businesses lose out on money by spending time that they would be using on their core competency for managing software licenses, versions, data backups, servers, etc. And of course cloud providers lose out on you giving them more money than their cloud services cost them to deliver. Azure now makes up the largest percentage of Microsoft's revenues. Obviously the business model scales very well. It likely scales better than Office does. Eventually the less-profitable thing starts taking up internal development resources that could be going to more profitable divisions, and that gets someone mad. reply holografix 11 hours agoprevVery hard for me to understand why, in a world of Google docs, anyone would want to deal with the bloated mess that is ms office. I was helping an elderly relative who works as a translator and hasn’t touched a modern version of word in about 5 years. They had a new computer and I got an ms office sub for them. The poor person re-did about 4 hours of their work 3x because they couldn’t find the file MS Word had guaranteed them it had saved, so they had to start from scratch. It did save it. In their fucking cloud and made it so opaque that the user couldn’t possibly understand wtf was happening. It took me, a tech professional a good 5 minutes to snap out of the dark pattern and realise what was going on. reply CharlesW 10 hours agoparent> Very hard for me to understand why, in a world of Google docs, anyone would want to deal with the bloated mess that is ms office. For knowledge workers who live in these tools, the difference is stark. Even for companies who've standardized on Google Workspace or Apple iWork, advanced users will need Microsoft Office. reply UberFly 9 hours agorootparentThis is true. Among other things, I do a lot of label printing and the tools in MS Word are miles more mature. You get these same types of responses when people list off all the alternatives to Photoshop. Just not the same. reply HKH2 8 hours agorootparentWell it really depends on how you use the programs you are comparing. reply 7thaccount 6 hours agorootparentprevMostly because of network effects and wanting software that will be supported for another three decades potentially and can open my document long-term. Google cancels products all the time and has practically no vision. reply davchana 7 hours agorootparentprevYes, my pet peeve is printing as pdf. You can control line thickness, size, placement of row borders to pixel perfect in Excel & it will get printed to pdf or paper exactly as it is. But not from Google Sheets or Excel Web, even same excel sheel imported in these & then printed will have slight difference in what you see on screen and what gets printed. I understand its because of browsers limitations to place stuff on pixel scale. reply worldsayshi 10 hours agorootparentprevWhat features in Office are essential for those users that you don't get from Google? If you're talking about Excel I can imagine there are such features but not so much in other apps. reply maxcoder4 9 hours agorootparentI am exclusively Google office user, but... Out of the top of my head: * Google docs are uglier than Microsoft word documents. This matters when I prepare an offer that I want to send to a client, and it should look good. * Google slides are hideous, and the only reason I get away with using them is because programmers (including me) have no taste[1] * Related to the looks, I sometimes buy paid document templates that I can use to format my offers. They often have an option to download a docx file, but it's complex through that you can't important it to Google docs without breaking it completely. * Word/Libreoffice works offline (maybe docs with serviceworkers too? It never worked for me when I needed it). * You can use word to generate documents using a template, don't think it's possible with Google docs * Macros are not supported in Google docs And I'm a complete noob when it comes to document editing software and actively avoid it. I can only imagine how much powerusers miss. [1] https://medium.com/@laurajavier/google-slides-is-actually-hi... reply WWLink 8 hours agorootparentMS Word templates are one of the most awful things I've ever messed with. I had a fun time using them with pandoc to produce autogenerated word documents. ARGH! The nightmares! We should just all get off our butts and learn LaTeX! Not that I have. ahhahahaha. reply elevatedastalt 8 hours agorootparentprevIs it just about the default font? Most office workers have 0 sense of aesthetics and sometimes official guidelines actively make things ugly (like forcing fonts like Times New Roman on everyone). Or the cutesy Calibri on official notices. reply lokar 5 hours agorootparentprevWord is also ugly. When I care what it looks like I use LaTeX. reply francisdavey 6 hours agorootparentprevAs a commercial lawyer, I often have to exchange drafts with another lawyer somewhere else. Almost everyone will accept Word, far fewer are happy with a Google doc. Obviously I can generate a Word document from Google, but that isn't quite the same. A specific problem in this scenario is tracked changes. Google has a history/version control but it does not map particularly well onto Word's tracked changes, which the other party will understand and is likely to want to use. Passing things into and out of Google will often result in loss of useful information like that. Personally: Word is the absolute best piece of software for dealing with numbered lists that is easily available. In many ways it is terrible of course, but it is less terrible than anything else. Getting numbering right is important. Google has gotten better. It used to be very bad at larger and more complicated documents. But it still doesn't have all I need to write a really good contract (at least by my standards of \"really good\"). reply jeremyjh 10 hours agorootparentprevIt’s always mainly about Excel, but some people are really set on Outlook, too. reply jasonjayr 8 hours agorootparentOh boy. Outlook users are in for a surprise with \"New Outlook\" is forced on them in the near future. Microsoft has been pushing and warning about this change for some time .... reply Semaphor 6 hours agorootparentI use outlook for work mails and tried new outlook for a short while. It’s like an alpha version, it’s not even close to feature parity, a pure showcase for a new design. If it ever becomes the only version, I’m switching to thunderbird. reply smackeyacky 8 hours agorootparentprevIf some kind soul wants to point to an email client that has a capability like Outlook rules I'm dying to know about it. I have a mate whose business is entirely based on a big set of Outlook rules to do processing and there doesn't seem to be any good alternative. Outlook online doesn't seem to have any rule capability and can't talk to 3rd party email servers. reply Ringz 3 hours agorootparentYou don't need an e-mail program for this if you are ready to work with Imapfilter: https://github.com/lefcha/imapfilter reply bombcar 9 hours agorootparentprevFrom my experience it is Excel first, PowerPoint second, followed by Outlook and, last, Word. The order varies depending on what you do, each can become absolutely critical. reply xw38011 9 hours agorootparentprev> What features in Office are essential for those users that you don't get from Google? Backward compatibility with existing documents that are already in Word. Also, for those knowledge workers who aren’t in tech, the high likelihood that the recipient can open the document with correct formatting. reply Ziggy_Zaggy 7 hours agorootparent...why not just print to PDF then since all browswer support PDF? reply lern_too_spel 8 hours agorootparentprevhttps://support.microsoft.com/en-us/office/linear-format-equ... reply pphysch 10 hours agorootparentprevI think it's less about feature parity and more that the users have spent tens of thousands of hours in MS office and don't want to relearn all the shortcuts and menus and subtle behaviors -- muscle memory stuff. reply ishtanbul 9 hours agorootparentAll the plugins for excel, word and powerpoint integration. And we have to be able to send attachments with actual files, not links. Law firms and banks are pretty set in their ways here. It has to be office and windows. Google sheets doesn’t remotely compare to excel for serious financial modeling. reply shrimp_emoji 10 hours agorootparentprevIn that case, they can use FreeOffice, whose office suite is indistinguishable from MS Office and works on Mac and Linux! (Granted, I say that as someone who very much does not live in those tools except as a rare hardship imposed by normies society.) reply UberFly 9 hours agorootparentSuggestions based on ignorance of the product isn't the best place to start. reply bruce511 9 hours agorootparentThis. Believe me, Office and Office clones are very distinguishable. It's like saying a Macdonalds burger is indistinguishable from a Gordon Ramsey burger. They may both be food, but they are very much not the same thing. There's a reason people use Office. Pretending that reason does not exist does not make the argument for switching better. reply dmbche 8 hours agorootparentI've been using libreoffice for maybe 10 years now? I don't remember what I'm missing, could you name one or two killer features of ms office? reply sakjur 7 hours agorootparentImho it doesn’t come down to one or two killer features, it comes down to momentum. Stuff like font rendering, grammar- and spellchecking, the exact set of Excel formulas, graphs, templates, and VB scripting matter. The office suite’s localization changes keyboard shortcuts, Excel formulae names, and swaps between decimal points and commas. It is absolutely horrendous, but people rely on it for their daily work. In essence, if we accept that Excel is both an IDE and a dialect of a programming language, we can compare it to asking what makes C# in Visual Studio worse when people are used to Java in IntelliJ. The answer might be “nothing, but I’m used to my setup and it’s ridiculous that I’m even having this discussion about my main work tool” for programmers, Office users, and video editors alike. reply saintfire 7 hours agorootparentSort of unrelated, but, VB scripting not working on web versions felt to me like it really killed a giant moat of legacy code to draw from. If you're forced to script in something else then why not just go to something else. Additionally, having tons of forms written by long gone employees just not port over is a tough sell at smaller offices. reply sakjur 4 hours agorootparentI think that’s very related. It’s hard to get people to migrate to Office for Mac or the web version because they’re subtly different enough. reply bruce511 7 hours agorootparentprevYou're happy with what you are using, so that's great, and I'm not knocking that. But the difference is not 'killer features\". The difference is in the million small details and polish. The integrations, formats, UI, workflow, things it just \"gets right\" that I don't even know its doing. I try Libre Office every once in a while. But each time I try it just feels old and clunky. It's all in the tiny details that add up to the overall experience. Clearly you're not missing anything since you're happy with what you have. But going from Office to LibreOffice is painful. Not bullet-wound painful, more like thousand-paper-cuts painful. reply Dylan16807 5 hours agorootparentA lot of things are just different. In a way that makes it hard to switch, but isn't because microsoft office is a better/premium experience. reply kube-system 9 hours agorootparentprevThere are significant feature differences that enterprise users consider dealbreakers just between MS Office for Windows and MS Office for Mac, let alone Office clones. reply Difwif 10 hours agoparentprevSo many microcosms with tech. I'm always reminded here on HN how terrible Office is and why we don't just use Google Docs. I hold this same opinion personally. However I go to other communities (I think the last one I remember was some startup subreddit) and GSuite is being mocked and everyone is recommending Office and Teams as the obvious choice for starting your business. I assume it's just that we prefer the devil we know than the one we don't. reply dietr1ch 10 hours agorootparentI don't think that the decision is being driven by bad company A v/s bad company B, and it's implicitly technical. All of us here probably will know when to jump out of spreadsheets and have some knowledge on how to approach things then, so a simple spreadsheet on Google Docs is fine for us. The problem outside, is that they are somewhat locked on the spreadsheet and have to stick with it, so more advanced features are welcome even though it comes with the price of the so called evil company according to the other group. And is Office really better than Google's Spreadsheets? Idk, I don't care about small differences, but they surely annoy hardcore users, plus no one really got fired for buying IBM reply CSMastermind 5 hours agorootparentprevI generally don't see people recommending teams, typically business users seem to prefer zoom while the ones who use teams are forced to because it's bundled with other Microsoft products. Excel on the other hand is still miles better than Sheets for non-trivial use cases and I've seen business users revolt multiple times if you try to force them to use GSuite. To a lesser extent that's also true with Word and to an even lesser extent Outlook. I haven't yet seen someone threaten to quit if they don't get a Teams license (but I have seen that for Zoom). The interesting one is PowerPoint which I've noticed a lot of power users are migrating to Figma for. Also 10 years ago people would send nasty grams if they couldn't get Visio licenses but Lucidchart seems to have eaten that marketshare. reply fbdab103 4 hours agorootparent>I haven't yet seen someone threaten to quit if they don't get a Teams license (but I have seen that for Zoom). Uhhh what? I suppose I am a novice video chat user who just uses it to talk to people and share a screen, but I am clearly missing some killer feature. From my perspective, all of the platforms suck for one reason or another. Bad CPU usage, latency, but hey, they have background swapping and fun emojis! reply plufz 10 hours agorootparentprevGoogle is definitely the devil I know of those two, still I would not like it if one my main tools were provided by Google. Currently they seem to manage to both lack in innovation AND be unreliable. reply shiroiushi 8 hours agorootparentprev>and GSuite is being mocked and everyone is recommending Office and Teams as the obvious choice for starting your business. These must be paid shills. While it's actually quite understandable why real people and businesses would want to use and recommend MS Office, no one in their right mind actually thinks Teams is the best video chat tool in the world. Any serious business uses Zoom, Slack, etc. reply NikolaNovak 7 hours agorootparentNot a paid shill, but Until recently Teams had capabilities Slack was laughingly behind :-/ Biggest one for me was that I could A) start multiple chats with same audience and rename them - so I can have chat with thom dick and Harry on system architecture over a few days and separate conversation with them on performance testing issues. This is trivial in teams. I have to create awkward channels in slack to approximate the functionality. B) seamlessly start a conversation with two people, then as you troubleshoot and expand, add more people, then jump in a call, then finish a call and keep chatting. Until recently slack would force you to start a new blank conversation when you added people - absolutely useless. Now they've hacked a solution that works up to arbitrary number of ten people and is so clearly a script in the background which still creates a new chat with added person but helpfully copies all conversation over. Then you need to add 11th person and too bad you've hit the magic number. In operations setting and evolving incidents, teams was just better. And don't get me started on slack \"huddles\"! My inlression has always been the opposite - startups used slack because it was cool. Serious businesses used teams because it worked and integrated well. Now. I've realized lately that when people talk about slack vs teams, they're usually not actuslly talking about slack vs teams. They're actually talking about their companies security and usage policies, as incidentally instantiated through the collaboration tool of choice. I've become aware that my experience with teams is bit everybody's, due to various policies and limitations imposed, and similarly for slack. But mostly... Not nearly as many people that disagree with average internet forum dweller are paid shills as may be believed :=) reply musicale 7 hours agorootparentTeams, as much as I may dislike it, seems to have more built-in features than Slack, including a files feature that supports editing MS Office documents in place, and integration with Outlook calendar and email and other Microsoft apps. I also think that Slack didn't have video conferencing until relatively recently? As with the IBM model, I imagine it's simpler for companies to have a single source and a single support channel. It is possible to use Exchange sign on for non-MS systems and apps however. reply pxc 4 hours agorootparent> Teams, as much as I may dislike it, seems to have more built-in features than Slack Isn't that the problem with Teams? Instead focusing on highly usable text chat, the focus is growing the pile of integrations with other mediocre Microsoft products. (Not that Slack is great; it's been bloated and slow for a long time and has likely been on steep downward trajectory since the buyout by Salesforce.) reply shiroiushi 5 hours agorootparentprevThe problem with Teams isn't features, it's that the thing barely works and is dog-slow, especially on certain platforms. reply sgustard 6 hours agorootparentprevUnfortunately my \"serious business\" of $2B revenue dropped Zoom and Slack like a hot rock when they signed an enterprise Microsoft deal, because Teams is free, and usability, productivity and job satisfaction be damned, and your jobs are moving overseas anyway and nobody dares complain there. reply jrmcauliffe 7 hours agorootparentprevLol any serious business. Shoutout to all the people at mega-corps using Teams! reply andylynch 4 hours agorootparentTeams and Symphony are my industry’s weapons of choice. But I guess we’re just playing around. reply aden1ne 2 hours agorootparentprevTeams has network effect behind it. Because so many businesses already use Teams, in a B2B-setting you're almost expected to already have Teams as well. reply drekipus 10 hours agorootparentprev> and everyone is recommending Office and Teams as the obvious choice for starting your business. Welcome to astro-turfing and shilling. A pretty commonplace occurrence since about 2014 or so. You can only really rely on people you know and their experiences. always discount \"what everyone says\". reply jseliger 10 hours agoparentprevWriting/editing offline, complex formatting, familiarity. I'm not an Excel jockey but friends who are testify to its power and flexibility. I use Word a lot and the ease of use still beats gdoc's. For example, the macros and the \"customize keyboard\" options are great. cmd-l for \"next edit,\" cmd-j for \"accept and move on,\" cmd-; for \"reject change.\" The speed is paramount. reply xattt 10 hours agorootparentWord and Excel, like Photoshop, are dominant because of their maturity as well as the muscle memory of their users. People were pissed 18 years ago when the Office ribbon was implemented because it broke their workflow. MS might have gussied it up now, but they haven’t dared to reinvent it since. reply maxcoder4 9 hours agorootparent>People were pissed 18 years ago when the Office ribbon was implemented Wow, now I feel old more than any \"movie X was released Y years ago\" factoid could make me feel. reply raincole 9 hours agorootparentprevI honestly still hate office ribbon. reply alluro2 9 hours agorootparentSame here - there are 1000 options, across 15 tabs, and I have to look through all the icons and labels till I find the one, with active tab and even displayed options changing depending on content selection.... It's a constant battle if you don't spend hours a day in MS apps and develop muscle memory. I do realize that there are probably no ideal UIs for accessing such a breadth of options, and it was a valid attempt, but not really successful, imo. With some adjustment and practice by users, I believe something like Alfred within the app, with smart search and suggestions, would be a quicker and more efficient approach for tool activation. Since there has to be also a good way to discover features without explicitly searching for them, maybe something like the Start menu is the right approach - quick search, but also categories and lists of tools. reply Reviving1514 8 hours agorootparentI already use the search function in Office to find everything I need. Is that what you had in mind or was it something different? reply andylynch 4 hours agorootparentThe search bar is excellent and being keyboard driven fast too. The ribbon is also entirely keyboard-drivable and fast that way. I’ve seen the data on why it was introduced; it was a scaling issue- the old toolbars simply could no longer fit all the functionality in apps like Excel, so they made them context-aware. reply HKH2 8 hours agorootparentprev> maybe something like the Start menu is the right approach - quick search, but also categories and lists of tools. Sounds like Google Docs. reply datavirtue 6 hours agorootparentprevI don't hate it but it didn't solve or improve anything. reply orwin 9 hours agorootparentprevTo be fair, or maybe only in my opinion (but i had a teacher who agreed with me so :/), from at least 2008 until at least 2012 (So from when i discovered how nice it was until after i quit using word/openoffice entirely and only used Latex), OpenOffice/libreoffice templating features were more powerful than word's, although less intuitive. Gdoc is shitty however and i don't see how/why i want to use it over any competitors. I think i prefer Nextcloud's editor, even without the privacy/data mining consideration, and i really think that could be improved. Gdoc is better than Jira and Confluence editor though, and better than the standard redmine editor froma few years ago (the non-markdown one), so that's nice. reply jseliger 9 hours agorootparentI can buy all of this. I've periodically played around with Libre Office on MacOS and usually found the text rendering leaves something to be desired. It's one of these things that I'm glad exists and yet don't really use. edit: I just downloaded the latest version and the text rendering looks good! Now time to see how it handles dark mode (white or green text, black background). reply jsmith12673 7 hours agoparentprevI've had to use Google Sheets a lot recently, and it doesn't hold a candle to excel. There are a lot of basics missing esp. when it comes to charting. But I don't like excel much either. If working with spreadsheets was a bigger part of my day job, I'd switch to excel reply etempleton 8 hours agoparentprevThere are a ton of very specific pieces of functionality that are built into Microsoft Word that caters to business edge cases. Features that have worked the same way and have not been touched for years for compatability reasons and are not duplicated in other software/services. Word is a bloated mess, but incredibly feature rich. reply Waterluvian 8 hours agorootparentOffice is Final Cut Pro. It is brimming with features and power. But lots of people aren’t working at some corporate office. Mom and Pop can get far with the iMovie option like Google Sheets and Docs. Actually now that I say that… what I want is a LibreOffice equivalent of Google Docs and Sheets and Presentations. Google is the only bad part of my Docs experience. reply wannacboatmovie 9 hours agoparentprevAmong other things, Office comes with support, a word most Googlers can't spell. reply saalweachter 7 hours agorootparentIs the suopprt first or third party? reply wannacboatmovie 6 hours agorootparentFirst party of course. MS has gold plated support agreements especially for public sector customers, where you can get a real person on the phone, not at the mercy of some automated Google bullshit. Is it expensive? Yes. But if you're running an entire city as opposed to a small dentist's office, it's likely worth the money. reply csdreamer7 11 hours agoparentprev> Very hard for me to understand why, in a world of Google docs, anyone would want to deal with the bloated mess that is ms office. My mom had to buy a copy of MS Office. Her university provided free ms office online, but there were certain features missing from it she needed for her papers. I am remembering wrong, but it was annotations? Citations? I do not remember. Libreoffice kinda could do it, but I could not find how online, while MS had it properly documented on their website and so many youtubers making videos on MS Office had it listed on their website. Edit: also found out MS Office can screen record and record her webcam. Very useful for her giving a remote presentation during covid. reply Shorel 6 hours agorootparentAnnotations, citations, bibliography management I can understand, even if I use LaTeX for that and consider it a far superior tool. But video recording and streaming? Why would anyone prefer to use MS Office for that when OBS exists? Is it as obnoxious as having to use Teams for chat? That's certainly a case of having a hammer and thinking all problems are nails xD reply skissane 6 hours agorootparent> But video recording and streaming? Why would anyone prefer to use MS Office for that when OBS exists? OBS is very powerful, but the complexity of its UI can scare non-technical newbies away. MS Office gives you a lot less power, but is much more welcoming when you aren’t technical and this is the first time in your life you’ve ever done it reply runeb 9 hours agoparentprevSome institutions have requirements that files don't leave your computer or network reply midnight2 7 hours agoparentprevExcel blows Sheets out of the water. Try opening a million row CSV in Sheets. reply petepete 6 hours agorootparentThis is a feature few people need. reply fbdab103 4 hours agorootparentYet it is the status quo expectation. If you offer up an alternative, do not be surprised if people expect it to be comparable in all features. reply woopwoop24 3 hours agorootparentfor the money your governments spend licensing MS products, they could develop libreOffice from scratch in better, faster and nicer looking. Or just pay a few devs or a few hundred to make it better and still pay less in fees every year. reply __MatrixMan__ 9 hours agoparentprevThe popularity of all these paper emulators seems odd to me. Like, how long after the advent of software will it take before our workflows find their authentic shape? Or was that shape really just a list of flat rectangles all along? reply datavirtue 6 hours agorootparentI love the question as I'm constantly faced with the absurdity of user-hostile software. It was supposed to be built to help humans. Instead it creates PTSD. reply __MatrixMan__ 4 hours agorootparentHmm, maybe the problem is more of a drift towards hostility instead of an inability to move forward. reply bfrog 10 hours agoparentprevMicrosoft in a nutshell… I swear they offplanered their ux people to the moon where they couldn’t do anything. Or they have no ux people. Or their ux people have no effect. reply shrimp_emoji 10 hours agorootparentThey fired all the testers and figured user telemetry could be everything they need. reply bfrog 8 hours agorootparentDoes that include voice to text when users yell at the computer and chuck it at a wall? reply usr1106 3 hours agoparentprevVery hard for me to understand why as the owner of a computer you want to put precious data on someone else's computer. Especially someone that has no customer service, a history of killing products with no good options for their users and frequently breaking laws. reply acchow 11 hours agoparentprevDon't the most recent documents appear on the welcome screen when you open MS Word? reply CharlesW 10 hours agorootparentThey do, and right below the file names is its path. However, the elderly relative had never opened the file on the new computer/Office install, and likely never had any idea where they'd originally saved the file. reply tombert 9 hours agoparentprevI'm not a psychologist, but I think it's the same reason I still can't get into IntelliJ. Let me explain: I cut my teeth on Vim. I've been using Vim since I was 17 (I'm 33 now). Nearly everything I do for fun has been with Vim. Most of what I've done for work has also been with Vim (or NeoVim). I write documents in Vim with Pandoc. I compose emails in Vim (using Mutt). I use Vim whenever I do an interactive rebase in git. I do CAD Modeling with Vim using openSCAD. The keystrokes are just second nature to me, I think in Vim keystrokes now, for better or worse. The IntelliJ Vim plugin is actually very good, but it's not quite perfect, there's subtle, intangible things that I have to adapt to, but are different enough to annoy me. For 99% of people, I think this is more than \"good enough\", and I still use it when I write Java, but I still am just unable to \"like\" it. I don't use MS Office, but I suspect that if you've been using it for a long time, even tiny differences that you'd experience with Google Docs would become infuriating. reply stickmunch 4 hours agorootparentI just started down Vim rabbit hole. It's a cult I am more than willing to spend time diving into head first. reply Daz1 8 hours agoparentprevBecause Google Docs only has 10% of the functionality? Also your anecdote is user error. reply FdbkHb 10 hours agoparentprev> Very hard for me to understand why, in a world of Google docs, anyone would want to deal with the bloated mess that is ms office. I see you have never opened a large spreadsheet in competing software or you wouldn't call MS Office bloated. Sheets and Calc are extremely slow, inefficient software. Excel alone, if you have a use for it, makes it well worth the price of admission. It simply has no competition and neither Google nor Libreoffice can serve as drop in replacements for that. Most of the features in the Office suite get out of the way and are only there if you need it. It's no bloat to the people who need the features. Office is where Microsoft still shows love for desktop software and it shows, they open very quickly and feel responsive in a way most other software they produce don't (opening the widget board on Windows 11 is a more stuttery experience at times than opening something in Word or Excel) > It did save it. In their fucking cloud and made it so opaque that the user couldn’t possibly understand wtf was happening You can't save it in the cloud \"by accident\". When creating a new document and clicking to save you explicitly have to pick \"onedrive\" or \"this computer\" as locations. > It took me, a tech professional a good 5 minutes to snap out of the dark pattern and realise what was going on. It took you 5 minutes because you had no idea what the user did. It's not the fault of the software if the user clicked to save to onedrive. You can also still create new blank documents directly in the explorer.exe (right click -> new -> word document) as you always could since Windows 95 in which case you would have set a local location for the document you're working on before writing the first line of text. I also find it interesting you're suggesting Docs, a piece of software that is cloud driven only, as a replacement for Word because a user mistakenly \"saved to the cloud\" And if giving people the option to save to onedrive is a \"dark pattern\" then what is a piece of software that can /only/ save to google drive, exactly? Microsoft 365 Family subscriptions is the whole desktop suite + the online apps (which are pretty competitive with Docs if you need to access something on another computer in a pinch) + 1 terabytes of storage + up to 6 users (each with their own 1tb of storage) on the same subscription for 99 bucks a year. Google can't even begin to compete on that level of offering. Some of the apps have no real google alternatives either, OneNote is an incredible tool for personal organization of ideas and clipping online content you want to keep. It's also very snappy and responsive, again, the Office division really cares about quality of desktop software in a way that has become all too rare. The people working on Windows's desktop/UI elements would do all too well to take inspiration from them because 11 is a damn sham. reply BLKNSLVR 10 hours agorootparentThe pros you mention above are true for the desktop versions, but most definitely not the browser versions, in my experience. The browser versions feel squishy and feature incomplete and the interface is different enough to be annoying enough for me to avoid it like the donut with a hair on it. Mine is a very Excel-centric view. I wouldn't miss anything else in the office suite. reply unsignedint 9 hours agorootparentI’m keen to understand what features you found lacking in the web version of Office. My initial impressions aligned with yours; however, upon recent usage, I’ve been pleasantly surprised by its extensive feature set. While advanced data extraction capabilities in Excel are notably absent, the web version otherwise provides a comprehensive array of functionalities. reply BLKNSLVR 6 hours agorootparentMight depend on how recent. As a result of past annoyances I haven't revisited it in the last 12 months, except accidentally. The memory of the annoyance remains, whilst the specifics have been lost to time. I'll give 'er another shot. I'm aware of my tendency to happily burn big tech for the slightest sleight. reply briHass 9 hours agoparentprevPeople always forget about Outlook. Name me a competing desktop app that does integrated calendar, email, todo, and meetings as well as Outlook. Thunderbird is (unfortunately) a joke, and web clients aren't as nice when managing complex folder arrangements and lots of mail. reply mairusu 8 hours agorootparentRIP then. https://proton.me/blog/outlook-is-microsofts-new-data-collec... reply musicale 7 hours agorootparentprevOn macOS/iOS I like Apple's suite of apps (and its desktop-mobile integration), and arguably Google's advantage is that it's web-first so it works well on cheap ChromeBooks that you don't think twice about replacing when they are lost or broken. And Teams is a clunky Electron app. reply andylynch 4 hours agorootparentIt’s not anymore, the current version is WebView2 and much faster. reply maxcoder4 9 hours agorootparentprevHonest question, what's wrong with Thunderbird? I never used Outlook, and I use Thunderbird daily so I wonder what I miss [1]. It also picked up development a bit and got some nice improvements, maybe check it out of you didn't recently. But I'm really curious why is it bad. [1] I'm pretty sure outlook users miss good GPG support, at least. reply briHass 8 hours agorootparentSupernova has fixed a number of the performance issues with large mailboxes, but that was fairly recent. People steeped in Windows still prefer the sleek/modern UI in Outlook over TB, in my experience, but the biggest benefit in Outlook is just the tight integration in MS/365 land. Calendaring in TB always felt like a bit of an afterthought compared to email, and for personal use, that makes sense. For business, strong calendaring with meetings/shared calendars/availability and the tight link to meetings (Teams)is not done as well anywhere else. reply to11mtm 8 hours agorootparentprevIronically, we used thunderbird at at old job, because it was easy to screenshot for QAs instructions on how to check both the HTML and Text versions of sent emails. reply redeeman 9 hours agorootparentprevoutlook has a gazillion things that are far far more of a joke than those lacks in thunderbird, those are just things people accept because they cant change it, and impose a 1:1 feature/bug requirement on a new thing, or its \"not ready\" reply dirkt 4 hours agoparentprev> Very hard for me to understand why, in a world of Google docs, anyone would want to deal with the bloated mess that is ms office. You don't want Google to have data that is used to govern a federal German state. reply vidarh 3 hours agoparentprevTry editing a 200 page document in Google Docs. Last time I tried I had to give up. I detest MS Office too, and Google Docs is perfectly fine for a whole lot of my use cases, but it's not a complete replacement, and that'll be a problem for a lot of places where some subset of users will need other applications anyway if you pick Google. The \"lost\" MS Office doc is infuriating, though - have had to help my son with several different variants of that for school over the years, including the reverse, where it's insisted there is no document at the location in the cloud drive it is meant to be, but where it turned out this was because it hadn't been synced from the local drive... reply layer8 10 hours agoparentprevOffice has become a bit of a UX mess in parts, due to the cloud and web integration, but the overall functionality and integrations are still unmatched. Many people also continue to prefer native applications. reply unsignedint 9 hours agorootparentTheir offering strikes a decent balance between the two. The fact that it is a native application really helped me when I traveled internationally, where my connection was spotty at best. I had a minimal slow connection on my phone that I couldn’t tether, and otherwise, I had to offload periodically where Wi-Fi was available from my PC. In places where you can assume to have a strong, always-on connection, they don’t make much difference, but in situations where I can’t count on it, Microsoft’s structure, which can tolerate offline usage, is very useful. reply xw38011 10 hours agoparentprevLots of reasons. Here are some: https://www.linkedin.com/pulse/why-law-school-applicants-sho... reply aporetics 6 hours agoparentprevGoogle docs has no dark patterns. All sunshine and roses. reply to11mtm 8 hours agoparentprevYeah these are reasons I had to set up a a VM for my dad while he finally went off Wordperfect 6 to LibreOffice for word docs. It's been interesting to watch because he's happy to embrace better tech but hates dark patterns. reply FpUser 10 hours agoparentprevI use MS Office and Softmaker Office, both native and with perpetual license. Works fine for me. Not sure why would I need online service that can cut me off at any point with no recourse reply trelane 10 hours agorootparent> Not sure why would I need online service that can cut me off at any point with no recourse But both of those can cut you off at any point with no recourse. Read the EULA carefully. The only one that can't is LibreOffice. reply bbkane 10 hours agorootparentprevI use Markdown, with no license. (Usually) Works fine for me. Not sure why I need proprietary file formats only usable by on vendor's software. I'm somewhat joking (obviously MSFT file formats can do a lot more than md if you need that), but I rarely need more complicated documents. reply datavirtue 6 hours agoparentprevYeah, they got it so fucked up that the average user has no idea where their files are. It is absolutely unbelievable how user hostile it is. Typical software designed for the goals of the creator. The world of computing has truly descended into hell for the average non-technical person. reply ByQuyzzy 7 hours agoparentprevGoogle Docs is just flat out terrible. It's also highly insecure - imagine trusting an ad company with your governmental information. reply lwde 11 hours agoprevWe'll have to see how long it takes, as it did with LiMux (https://en.wikipedia.org/wiki/LiMux) when Microsoft Germany headquarters magically moved to Munich and everything got back to running on Windows again. reply KronisLV 10 hours agoparentFrom the link: > The city reported that due to the project, it had gained freedom in software decisions, increased security and saved €11.7 million (US$16 million). That's kind of nice. An org that I worked for gave everyone LibreOffice installs by default and if you needed any of MS offerings you could just ask for a license for those. Most people were just fine with LibreOffice. Depends on what you're doing with said office suite, though. reply incompatible 10 hours agoparentprevSurely there's a limit to how many German headquarters Microsoft needs though. reply wink 1 hour agoparentprevI like this conspiracy theory as much as the next person but in fact they moved only around 10-15km south, to be in Munich proper. Yes, I know, that would be irrelevant regarding possible tax breaks or bribes or whatever. But physically it was not far, same people working there without relocating, etc.pp reply ozim 11 hours agoparentprevMy question is what the heck? I read that same thing every 2-3 years “German state goes Linux/libre office”. Then it turns out that it is not really true. Some parts yes then some other parts are already on it and some parts move back. I do understand there might be different parts of rather big country doing different stuicf reply lwkl 10 hours agorootparentIt's a German of Schleswig-Holstein which is like the state of California or the state of Kentucky in the US doing a change like this. And it's only the state government and not the municipalities in the state. reply incompatible 10 hours agorootparentprevApparently, it's how one bargains with Microsoft to get a better deal on software or employment in that location or whatever. reply NewJazz 10 hours agorootparentprevI imagine the tide will lift and fall over the years. Some applications (new and old) will only run on specialized platforms. Exceptions will exist in a large enigh environment. reply mandevil 9 hours agoprevI'm sorry, what year is it? Am I posting on Slashdot still? Is this year finally going to be the year where Linux desktops become something for normies? When Munich did this it never got to a majority of their desktops, is my recollection, it maxed out in the 40%s. Now we're going to do it all over again with a different, much poorer, German state? reply nextos 9 hours agoparentSchleswig-Holstein is not poor. It is next to Denmark, and it is really nice and well developed. I think transitioning to a different platform is mostly an organizational and political, rather than technical problem, and the main roadblock is educating users and altering processes to migrate away from MS Office. That said, modern Linux distributions like NixOS or Guix could be great to manage large fleets of computers, keep them up-to-date, and upgrade things without fear. In my experience, that is the main technical issue administrations are experiencing. And of course, running free software is great because it brings freedom to change things. reply maxcoder4 9 hours agorootparentAs a long time Linux user, I think problems are partially technical. For example: * A lot of software used by them is certainly Windows only (they will have to find alternatives, change their workflows or invest in some windows virtual machines) * Windows tooling for organizations is much more mature. There's a reason virtually everyone uses AD. * Linux is very focused on user freedoms. This is not usually important in the office. But freedom to configure things is a freedom to break thinks, and cause admin headache. The problems are solvable, but it doesn't mean they don't exist. Oh and I love nixos, and I always wonder how realistic would it be to use it in a company for management of thousands of desktop machines. Sounds like it would be perfect for it, but i don't know any stories. reply tichiian 9 hours agorootparent> There's a reason virtually everyone uses AD. The reason everyone uses AD is that you can have a functional Linux client in AD. But you cannot have a Windows client in any Linux-based LDAP+Kerberos setup. The problem isn't that there isn't a good solution for Linux in big organisations, the problem is that Windows is only compatible with AD, nothing else, so the more compatible system (Linux) gets shoved into AD. reply BikiniPrince 8 hours agorootparentEvery large organization that I have worked at has a solution for desktop and server Linux. The downside is you typically have a password hash stored in ad or a separate service. Ultimately, it isn’t terrible, but you do have a lot of enforcement at the border. So trouble can surprisingly appear when you connect remotely. reply briHass 9 hours agorootparentprevAD is also better and more feature complete. It was born out of necessity, but it's had decades of refinement in thousands of deployments that the OSS solutions haven't had. reply Zardoz84 4 hours agorootparentSo, why does Microsoft want to kill AD and move everything to their cloud ? reply tichiian 41 minutes agorootparentBecause AD is a security nightmare. It is a collection of ~30 distinct protocols, e.g. bastardized versions of LDAP, Kerberos, DNS, DHCP, X.509 and a few RPC protocols that are all weirdly intertwined, with 30 year old designs. Every few months there is another CVE like 'oh, we forgot to checksum and sign that one field over there, please install this incompatible patch or you will have unauthed RCE'. Since all those patches make things break, there is a lot of vulnerable AD installations out there because most people need to be on \"compatibility settings\" that are insecure. And even the \"secure\" settings drop a CVE every few months. reply mairusu 8 hours agorootparentprevThe reason everyone uses AD isn't that AD is good. It's that they either have no other choice, or they don't have any competent people in setting up other tools since all basic sysadmins only learn AD in schools. reply mandevil 7 hours agorootparentprevAccording to wiki here (https://en.wikipedia.org/wiki/List_of_German_cities_by_GDP) Munich in 2021 had a per capita GDP of 86k euro. According to wiki here (https://en.wikipedia.org/wiki/List_of_German_states_by_GRP_p...) Schleswig-Holstein had a per capita GRP of ~42k euro in 2022 (note that the years are different). So these figures suggest that Munich is roughly twice as rich as S-H, which they list as below average for the BRD, as the 8th richest of Germany's 16 Lander. I took German in high school (back when I was on slashdot), I actually had tests on where all of the Lander were, I know a few things! reply Intralexical 8 hours agoparentprev> When Munich did this it never got to a majority of their desktops, is my recollection, it maxed out in the 40%s. https://en.wikipedia.org/wiki/LiMux > September 2006 — \"Soft\" migration begins. > October 2013 — Over 15,000 LiMux PC-workstations (of about 18,000 workstations) > December 2013 — Munich open-source switch was \"completed successfully\". > September 2016 - Microsoft moves its German headquarters to Munich. > November 2017 - The city council decided that LiMux will be replaced by a Windows-based infrastructure by the end of 2020. The costs for the migration are estimated to be around 90 million Euros. > May 2020 - Newly elected politicians in Munich take a U-turn and implement a plan to go back to the original plan of migrating to LiMux. They've still got a website up where they say some stuff about it, which itself is hosted MIT-licensed on GitHub with pretty regular commits: > Our strategic guidelines also provide for this: > > If economically and technologically or strategically sensible, LHM prioritizes the use of open source solutions, in particular to avoid company dependencies. LHM pursues this approach in both the application and infrastructure areas. https://opensource.muenchen.de/use.html https://github.com/it-at-m/opensource.muenchen.de/commits/ma... Well, it's not just about \"FOSS\" or whatever, is it? As a German state, you're better off not relying on making payments to a North American company. LibreOffice specifically was indeed decommissioned eventually in Munich (just within the last couple months!), though: > LibreOffice was used as an office package as part of Limux until the end of 2023. Though the Microsoft headquarters do make this seem like possibly a special situation, and as another commenter said, surely they don't need a national headquarters in every German state… reply dannyphantom 9 hours agoparentprevOh wow, I wasn't really online at that point in my life but I wanted to read more about it - were these the articles/threads you remember? Munich Finally Starts to Embrace Linux, September 26, 2006 - https://slashdot.org/story/06/09/26/0236246/munich-finally-s... Munich Reverses Course, May Ditch Linux For Microsoft, August 18, 2014 - https://linux.slashdot.org/story/14/08/18/2219253/munich-rev... reply mandevil 6 hours agorootparentThose articles for sure, but my memory is that it came up an awful lot in most linux slashdot discussions: Munich was cited the proof that this now was finally the time that normies would use desktop Linux, it was just turning the corner, this time definitely. Definitely remember going rounds with people over whether OpenOffice would be the spear to destroy the evil MS/Wintel monopoly (this discussion was definitely before the LibreOffice fork and Oracle murdering original flavor OpenOffice) and Munich was the main example to discuss. Of course, nowadays there are more *nix based GUI's in the world than Windows: between all of the various Apple products (XNU) and Android (Linux) you have the vast majority of the consumer GUI's in the world. Because most of us wasting our time in the linux slashdot forums missed how the world was actually going to change. Any resemblance to us now sitting around on Hacker News is purely coincidental I'm sure. reply mairusu 8 hours agorootparentprevhttps://www.zdnet.com/article/linux-not-windows-why-munich-i... reply tempest_ 8 hours agoparentprevLinux desktops will never be for normies because desktops are not for normies and are dwarfed by mobile devices and have been for a while now. Many, Many people use only mobile devices and the ones under 40 who still have computers have them for work. reply 998244353 8 hours agorootparentWell, the article was about work. Mobile devices may be well-suited for chatting and general entertainment but I seriously doubt that many people could do effective work - more than a couple quick changes - with the mobile versions of Word or Excel. reply ramijames 9 hours agoparentprevI'll admit, I miss Slashdot. reply antod 6 hours agorootparentYeah, it was a sad day when Netcraft confirmed it was dying. reply smoyer 9 hours agoparentprevCuriously, I was going to bring up that very same slashdot article (and the one describing how it has failed). Howdy old-timer! reply BikiniPrince 8 hours agoparentprevI had the exact same thoughts. At least this time around it’s basically done out of the box. reply renewiltord 9 hours agoparentprevAnd published on zdnet.com?! Did I mention that Netcraft has confirmed BSD is Dead? reply nimbius 9 hours agoprevGo Germany go! In the diesel repair shop I work in, I managed to convince the suits to switch out ms office for libre office. 90% of our stuff is now libre by default...only 2 users have full office suites and theyre both in the bean counting department. We print our BOMs, labels, envelopes and invoices using libre. Compared to office it runs like a scalded dog and never crashes. reply sph 2 hours agoparentI find it funny that there are suits working in your diesel repair shop. reply rudedogg 9 hours agoparentprevThat’s awesome. What distro did you go with? Do you have to help maintain them, or have they ever broken? reply nimbius 9 hours agorootparentWindows for the majority of machines (we already own desktop licenses) but i keep 3-4 old laptops with ubuntu we nicknamed \"crap outs\" in case someone's PC dies, someone needs a laptop ASAP at a jobsite, or the IT gang needs to get fix someone's main PC. Our shops big label maker and 3d printer machine is currently 100% Ubuntu and is a favorite for a Lotta guys on the floor. reply migf 7 hours agoparentprev\"it runs like a scalded dog\" https://themonticellonews.com/im-fixing-to-run-like-a-scalde... lol. Glad to hear the solution's working for you. reply analog31 10 hours agoprevI've tried LibreOffice, periodically over the years. The same thing always sends me back to MS Office. MS has really sweated the details of their UI in ways that can only be done by maintaining a huge army of coders. When I use LibreOffice, the lack of responsiveness is immediately noticeable, and actually makes the software physically laborious to use. I've also noticed something like a 10x or even 100x difference in the time to recalculate a large spreadsheet. reply HKH2 8 hours agoparent> When I use LibreOffice, the lack of responsiveness is immediately noticeable, and actually makes the software physically laborious to use. I say the same thing about Windows. Why is its UI so sluggish? reply incompatible 10 hours agoparentprevMaybe you'd be happier with it if you just stuck to LibreOffice and never looked at Microsoft products. It works for me. reply bradley13 3 hours agoparentprevOdd comment. The UI has nothing to do with recalculation speed. Anyway, the MS ribbons continually move stuff around - it's a pain for the casual user. Maybe people who use it all day long get used to it? Regarding spreadsheets, yes, that is one area where LibreOffice genuinely has not overtaken MS-Office. reply mlok 11 hours agoprevMunich did it a few times already since 2003. Last one was in 2020 : https://www.zdnet.com/article/linux-not-windows-why-munich-i... reply jll29 11 hours agoparentMunich reportedly chickened out of Linux a few years later - due to lobbying. reply AaronFriel 10 hours agoparentprevI've seen this story every few years, perhaps this is just how they Munich negotiates its contract with Microsoft. reply VelesDude 8 hours agorootparentLike every time North Korea needs more aid it launches a missile into the sea of Japan? At least Munich's bargaining chip seems a bit more friendly. reply pxmpxm 10 hours agoparentprevThis, definitely remember reading about this on slashdot decades ago. Must've gone well... reply WuxiFingerHold 7 hours agoprev> Why? Schleswig-Holstein cites cost, security, and digital sovereignty - though not necessarily in that order. It's been attempted before and costs has always been higher. But it's not a matter of costs. Reason and responsibility should make it mandatory for certain institutions and the government to own their data. Companies would be well advised to keep owning their data as well or try to get it back. We've all seen what happened with the \"privat\" Github repositories. No access from Github employees, but as they admitted by AI bots. I don't remember and it doesn't matter if it was on purpose or accidentally. If using cloud then at least with true E2E encryption. The cloud should only hold strong E2E encrypted blobs and meta data. There're providers out there that I think can be trusted. My (very large) employer has surrendered to Microsoft. Everything is Office 365 and in Azure. Our IT told us proudly that it's all E2E encrypted and Microsoft can't in any circumstances read our data. But how come I can search for content of Powerpoint and Word docs on Bing for work? A colleague of mine found secret project information this way before the classification levels were on place. Am I missing something about E2E encryption or is our IT that stupid and naive? EDIT: And recently our IT announced proudly that we soon can use Copilot. reply egorfine 17 minutes agoprevThis is of course false. The real government business in Germany is done on paper and on-site. Everybody knows that. reply Perenti 10 hours agoprevCan anyone tell me what this means: \"Other countries, notably China, have proverbs that say they are much more stubborn when shifting gears from Windows to Linux.\" Proverbs? A stitch in time saves nine. Many hands make light work. Please, anyone, show me a proverb that says \"they are much more stubborn when shifting gears from Windows to Linux.\" reply prashp 9 hours agoparentHere's one: \"Switching a Windows user to Linux is like moving mountains with a teaspoon; both demand patience and a miracle.\" reply freefrog334433 4 hours agoparentprev“The clever combatant looks to the effect of combined energy, and does not require too much from individuals.” Art of War reply freefrog334433 4 hours agoparentprevI was trying to remember a Confucius proverb that applied to Linux. Or something from the Art of War? reply shrimp_emoji 9 hours agoparentprevNo tux, no bucks! Wait, that would be the other way around. reply bakoo 8 hours agorootparentAll Tux, no bucks? reply HeavyStorm 11 hours agoprevWhile I work for msft, I'm all for open source, even more in this space - consumer and office apps - however, my government has tried this and rolled it back a while later in some branches, while others just suffer with it. Office is much superior to any open source and even paid alternatives, and we must remember that most people using the software don't have a degree in CS. I wouldn't be surprised if we see the reverse news a few years later. reply bkor 11 hours agoparent> Office is much superior to any open source and even paid alternatives, and we must remember that most people using the software don't have a degree in CS. Germany is investing in improving free software, see https://www.sovereigntechfund.de/. Though not sure if this is linked to that state switching. I think LibreOffice is riddled with loads of small \"paper cuts\". Basically loads of small issues that make it annoying to use. I hope that they understand it shouldn't be about cost, it should be about being sovereign. So hopefully the investment (ensuring additional developers, UI/UX people, etc) increases as they use more free software, reply jll29 11 hours agorootparentThe more people use free software, the more it will become cost effective to improve it using government funding. There can be enormous net savings (zero license fees), which is fantastic for the taxpayer. reply geraldwhen 11 hours agorootparentWithout an actual design vision, all open source software will flounder and never replace commercial counterparts. reply FriedEggFred 10 hours agorootparentGimp, blender, firefox and obs are all very successful OSS alternatives that come to mind. reply rafaelmn 10 hours agorootparentGimp is a classic example of OPs point. Mozilla really ? Lack of vision and focus driving Firefox into irrelevance. And also started on the back of a commercial product. Blender started as a commercial software and is being spearheaded by Ton ever since (my very uninformed impression), which again reinforces OPs point. reply adra 9 hours agorootparentYeah, gimp is pretty crap and it is what it is sure. Clearly they work in a world that has more smaller boutiques where replacing Adobe with supporting OSS alternatives isn't a clear winner. Industries that have a ton of engineers are more likely to build their own alternatives, and some of them realize that they can cost share by being OSS. reply FdbkHb 9 hours agorootparentprevOh, yes, gimp, the software that is still going \"we'll have non-destructive editing.. maybe.. soon.. probably\" when its first elements were introduced in Photoshop in the mid 90s. The. Mid. 90s. It's such a productivity boost that it would be a lack of self respect to one's own time to use a tool that doesn't have it. reply TOMDM 9 hours agorootparentprevYou really gimped your argument by including it; otherwise I'd agree it can be done. reply Intralexical 8 hours agorootparentKrita would have been a much better example I think. They've had some reasonably successful crowdfunding campaigns and ongoing monthly income, with focused visions of the product they create, and they've proven capable of attracting actual users who are in it for the art instead of just your \"FOSS\"-radical Stallman types. reply unsignedint 8 hours agorootparentprevBlender represents an outlier in the OSS landscape, largely due to its unique product vision. Its development was driven by practical use in creating open movies, which provided a built-in customer base. As a niche 3D software, it was subject to less conventional expectations in its presentation, allowing it to stand out. Despite this, Blender’s UI/UX design has faced criticism, suggesting that if it were applied in other domains, the reception might be even less favorable. reply worldsayshi 10 hours agorootparentprevYeah I think that's exactly the point. Libre office would need a Blender like visionary management to flourish. reply 000ooo000 11 hours agorootparentprevWhat a ridiculous thing to say reply dataflow 11 hours agorootparentprev> I hope that they understand it shouldn't be about cost, it should be about being sovereign Makes sense in principle, but practically speaking Germany would seem to have higher priority threats to address than software from the United States? reply paulryanrogers 10 hours agorootparentCountries are big. They can do more than one thing at a time. reply leipert 10 hours agorootparentprevMhm. Spending less on licenses would free up money for other things. reply andrepd 10 hours agoparentprev>Office is much superior to any open source and even paid alternatives, and we must remember that most people using the software don't have a degree in CS. For a fraction of the money spent on these sorts of enterprise contracts you could hire dozens of full-time developers to improve LibreOffice. Clearly something easily within the capacity of the EU, if they were capable of good strategic decisions. reply nerdile 9 hours agoparentprevWhen the reverse happens, it generally doesn't make the news. reply financypants 11 hours agoparentprevDoes libreoffice do hotkeys similar to excel/powerpoint? reply breadwinner 11 hours agoparentprev> Office is much superior to any open source and even paid alternatives Bullshit. The only reason to use Microsoft Office is compatibility with Microsoft Office files. What improvements have been made in the last 10 years in Outlook and Word? Nothing. There are some new bugs that didn't exist before, but no advancements. That's what lack of competition gives you. reply stateofinquiry 10 hours agorootparentObjectively there have been an absolutely enormous number of \"improvements\" to MS office (including Outlook and Word) over the last decade. The biggest is probably cloud/simultaneous editing capabilities. See https://www.microsoft.com/en-us/microsoft-365-life-hacks/sto... . Interestingly, there is a decreased emphasis on the file format- the opposite of your point. In addition to cloudy used on mobile devices, sharepoint, etc; since 2007 or so Word has used docx, which has better cross compatibility with other suites, even Google Docs: https://www.howtogeek.com/304622/what-is-a-.docx-file-and-ho... . My personal use patterns- I use LibreOffice quite a bit, Google Docs rarely, and MS Office daily for work. Outlook and Word have changed a lot and continue to evolve (watch for copilot integrations). reply breadwinner 10 hours agorootparentEnormous number of improvements? That link shows barely any improvements. If there was healthy competition you'd actually see enormous number of improvements. > The biggest is probably cloud/simultaneous editing capabilities. That was in response to Google Docs and is more than 10 years old (added in 2013?). reply mairusu 8 hours agorootparentprevThe bar for Outlook was so abysmally low- and even today it is pretty much the IE6 of mail clients. But good news! With the New Outlook, it won't even be a mail client at all. reply paulryanrogers 10 hours agorootparentprevThis depends on what parts you use. For many Microsoft Word 4 was probably peak Word. At this point Office is becoming an OS into itself. reply nullindividual 10 hours agorootparentprev> What improvements have been made in the last 10 years in Outlook and Word? Microsoft Search makes it much easier to find stuff. Word got near real-time co-editing (or perhaps that was closer to 11 years ago) and later ( but were inefficient for one reason or another; that's the hard part. The efficiency of such tools is mostly just a matter of familiarity. It's like asking what programming language is most efficient to do e.g. web development. There may be arguments for X or Y language depending on the specifics of what you're going to do and what you value, but ultimately the most important factor that determines whether it feels efficient to work with or not is how familiar you are with it. > you certainly aren't spending it trying to make some application work with your Linux config management I struggle to think of them needing to be aware of one another. Configuration management is pretty much completely a matter of moving configuration files and running commands. It's pretty universal. There's no \"make some application work with your Linux config management\". Configuration management systems may provide application-specific helpers that make things neater, but if you can copy files, maybe template them, and run commands, you're pretty much set. reply schmichael 10 hours agoparentprevLinux has directories and config/package/fleet management as well. No one in the past 20 years should be walking to every desk in 1,000 desk building installing software. reply jolmg 10 hours agorootparent> No one in the past 20 years should be walking to every desk in 1,000 desk building installing software. Maybe even 50 years, if one considers that they could've automated the installations through telnet. reply schmichael 10 hours agorootparentI remember walking to every computer in primary schools to do operations in the 90s, so I figured might as well play it safe with “20 years.” :) There’s definitely been solutions to this problem since the dawn of computing, albeit with uneven availability. reply TheLoafOfBread 10 hours agoparentprevThis is basically core reason why LiMux failed - previous German project doing exactly the same - and will be the reason why this project fails as well. reply chris_wot 9 hours agorootparentORLY? My understanding is that LiMux failed because the then mayor was trying to get Microsoft to move to Munich. reply Shorel 5 hours agoparentprevI would say that's more a Windows feature than something tied to MS Office. But yes, Active Directory is unmatched for administration of a big number of computers and local networks. reply splittingTimes 10 hours agoparentprevYour admin uses cfengine for example https://cfengine.com/ reply lfkdev 10 hours agoparentprevAnsible could easily do this reply tichiian 9 hours agoparentprevLinux has far superior management solutions when compared to anything Microsoft has. First, start with software packaging: In Microsoft-land, there is MSI, but there is also stuff delivered as .zip, .exe (generated from tons of different install builders, so completely inconsistent) and a handful of half-assed package non-solutions like chocolatey, nuget and others. When you want to install something, and there is an MSI, and the MSI can accept a proper set of parameters, you are in luck. If its an .exe (and even most Microsoft software doesn't consistently come as an MSI), good luck finding the right set of parameters for an unattended install, repackaging it as MSI or one of the other proprietary formats du jour. This repackaging is of course done in each and every company, all over again, for each version. In Linux, there is a consistent format per distribution, and the distribution does all of my packaging for me. If I want libreoffice, I'll type something like 'dnf install libreoffice' and I'm done. If I want to update all the installed packages on one machine, 'dnf update', wait a little, done. No hunting for a bazillion updates in various weird formats (even Microsoft patches come as .exe with inconsistent parameters and stuff) on a bazillion different websites. And removing software, the bane of any windows installation? The stuff where the solution is usually 'reinstall'? dnf remove libreoffice. The package manager keeps track of all installed files and does a reliable, reproducible removal. If necessary, configs can be kept for a later reinstallation or deleted automatically, no guessing what to save and what to delete. No \"nobody tests the uninstall.exe ever\". If I want to install a thousand packages on a thousand machines consistently, I'll give the package list to dnf on each machine via my preferred management software, be it 'for i in $hostlist ; do ssh $i dnf install ...' or (far better) something like ansible, puppet, cfengine, .... Consistent configuration? Easy, same software, use a regular ansible, puppet or cfengine run to distribute necessary changes. Update management and keeping older versions for a staged beta/testing/production software rollout? Easy, you can host your own package repositories. Want it fancy? Use Foreman and Katello, then your former windows GPO click monkeys can now click through staged Linux rollouts. Meanwhile cursing about how many years of their lives they wasted on Microsoft \"solutions\". Sorry if I come across somewhat annoyed. But any Windows setup I've ever seen has been abysmal compared to even the worst-run Linux shop out there. Just even something as trivial as a for-loop and ssh being possible makes such a huge difference. And the openness and distribution model make it easier to automate everything and be reproducible and consistent. reply wolverine876 8 hours agorootparentAnother Internet rant. Does that make it more likely or less likely to be accurate? Have you adminstered autmomated, managed networks? Using Windows? Some of the things you say indicate not. For example, things are not nearly as difficult as you say and consumer Windows installations aren't used: > removing software, the bane of any windows installation? Also, > Just even something as trivial as a for-loop and ssh being possible makes such a huge difference. I'm not sure what you are saying is missing in Windows. You can run remote commands from a cli and automation definitely supports loops. > any Windows setup I've ever seen has been abysmal compared to even the worst-run Linux shop out there Oh, I forgot, nothing was meant literally, it was just a rant. sigh. reply Shorel 5 hours agorootparentprev> If I want libreoffice Yeah, but no one wants LibreOffice xD (And I am writing this comment on Ubuntu) reply 29 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The state of Schleswig-Holstein in Germany is shifting from Microsoft to Linux and LibreOffice for its 30,000 employees, emphasizing cost, security, and digital sovereignty.",
      "This aligns with a growing trend in the European Union and China to adopt open-source software, safeguard data, and foster local tech competition.",
      "Similar transitions have occurred in other European countries, and China is also moving from Windows to Linux for desktop control, driven by cost-saving and enhanced IT security goals."
    ],
    "commentSummary": [
      "The German state is transitioning to Linux and LibreOffice from Microsoft, sparking discussions on software changes, cloud reliance, and the revival of the LiMux project.",
      "Users are debating the challenges of moving to open-source solutions like StarOffice, LibreOffice, and LaTeX, while expressing preferences for Microsoft Office and frustrations with forced changes.",
      "Conversations delve into comparisons between Microsoft Office and open-source tools, efficiency of programming languages, configuration systems, and the Linux superiority in software management over Windows, reflecting the ongoing evolution of open-source tech in the industry."
    ],
    "points": 405,
    "commentCount": 279,
    "retryCount": 0,
    "time": 1712267849
  },
  {
    "id": 39933833,
    "title": "Unveiling the Evolution of Data Tables",
    "originLink": "https://posit-dev.github.io/great-tables/blog/design-philosophy/",
    "originBody": "The Design Philosophy of Great Tables Author Rich Iannone and Michael Chow Published April 4, 2024 We’ve spent a lot of time thinking about tables. Tables—like plots—are crucial as a last step toward presenting information. There is surprising sophistication and nuance in designing effective tables. Over the past 5,000 years, they’ve evolved from simple grids to highly structured displays of data. Although we argue that the mid-1900s served as a high point, the popularization and wider accessibility of computing seemingly brought us back to the simple, ancient times. Okay, it’s not all that bad but the workers of data are today confronted with an all-too-familiar dilemma: copy your data into a tool like Excel to make the table, or, display an otherwise unpolished table. Through the exploration of the qualities that make tables shine, the backstory of tables as a display of data, and the issues faced today, it’s clear how we can solve the great table dilemma with Great Tables. Tables made with computers (left to right): (1) a DataFrame printed at the console, (2) an Excel table, and (3) a Great Tables table. What is a table, really? Before getting to what makes tables shine we should first define what a table is. This is surprisingly hard! But I believe it can be boiled down to two basic rules: the data is represented as columns and rows the data is primarily text Let’s look at an example of a simple table with actual data to tie this theory to practice. Name Address City Postcode DOB Height Weight Dustin B. Roach 1183 Columbia Road Holly Oak, DE 19809 1970-09-16 5' 9\" 202.5 Iwona Adamczyk ul. Zabłudowska 133 Warszawa 04-788 1976-01-03 5' 5\" 123.7 Geneviève Massé 1415 rue Principale Amos, QC J9T 1E4 1967-12-08 5' 3\" 136.3 João Souza Lima Rua Cosmorama, 538 São Paulo-SP 04648-080 2001-04-21 6' 2\" 231.0 Maddison McCabe 149 Raymond Street Strathern Invercargill 9812 1982-03-05 5' 8\" 146.1 A table of named individuals along with a select set of characteristics. This table arranges records containing personal characteristics as columns and rows. Each person is a row, and each characteristic makes up a different column. The characteristics use different types of data, like dates, numbers, and text. This arrangement makes it easy to look up individual values or make comparisons across the different rows or columns. Note that there are horizontal lines separating the rows. This aesthetic touch, while not strictly required for a table, serves as a visual reinforcement for separating the individual rows. The order of the columns matters, and that we start with the Name column here is no accident. If that column were the last (i.e., furthest to the right), it would be slightly more confusing for the reader since the subject for the record isn’t immediate. In addition to order, column labels play an important role for indicating what data is in each column. They’re not always necessary but in most cases they remove the guesswork for what type of data is contained within each column. We’ll go into some detail later about how Great Tables provides affordances for structuring information for better legibility and how the package can be used to adorn the table with other structural elements. For now, our conception of a table can be summarized in this schematic. A simple table has: (1) cells containing data, (2) an arrangement of columns and rows, and (3) column labels to describe the type data in each column. Now, let’s go back: way back. In examining where tables came from, we might better understand the great story of tables. The early history of tables Tables emerged from square grids. When grids are made like this, you invariably generate containers that may hold some sort of information. The earliest known examples of grids go very far back in human history. Twenty-five thousand year old representations of the grid are found on the walls of the Lascaux and Niaux caves in France1. Reproductions of early grids found on cave walls. In the second century BC, the Greek astronomer Hipparchus used latitude and longitude to locate celestial and terrestrial positions2. At around AD 150, Ptolmey published Geographia, which contains 25 geographical maps accompanied by methodologies for their construction using grids3. The Romans employed a grid system called centuriation, which can be described as land measurement (using surveyors’ instruments) to realize the formation of square grids using roads, canals, or agricultural plots4. When agriculture became more widespread (ca. 10,000 years ago), there was the need to document and manage economic transactions to do with farming, livestock, and the division of labor. In the fourth millennium BC, Mesopotamian cities that traded with far way kingdoms needed to keep such records. Clay tablets recovered from the ancient Sumerian city of Uruk show early yet sophisticated tables. Here is a drawing of one of the recovered tablets, which contains an accounting of deliveries of barley and malt from two individuals for the production of beer5. Drawing of clay tablet from Sumerian city of Uruk, circa 3200-3000 BC. Uruk III Tablet (MSVO 3, 51, Louvre Museum, Paris, France). Annotated with the meanings of the columns, rows, and cells. Note that the recovered tablet is meant to be read from right to left. Inside each box is an ideogram (a symbol that represented a word or idea) and a numerical value representing a quantity. Its structure is where things get super interesting: Rows: there are roughly two rows, each corresponding to an individual. Columns: the first two columns from the right contain counts of malt (rightmost column) and barley (second rightmost column). Subtotals: the third column from the right sums barley and malt within each individual, and the left-most column displays the grand total. As a bonus, the table has a footer, since the bottom row contains the name of the official in charge. Zooming ahead about a thousand years, you start to see more systematically structured tables. Here’s a photo of a cuneiform tablet that was originally from Mesopotamia (from the Temple of Enlil at Nippur, ca. 1850 BC)6, containing sources of revenue and monthly disbursements for 50 temple personnel. Cuneiform tablet, temple of Enlil at Nippur, (CBS 3323, University of Pennsylvania). You can see right away that there is a more regular grid and, if you probe deeper, there are more similarities than differerences with the tables of today. While difficult to pick them out, the following table elements are present7: column headings (month names) and row titles (names/professions of individuals). cells with no information (look at the blank or smooth cells along rows) numerical values in the cells subtotals for each individual every six months grand totals annotations with explanatory notes Later on, tables were less inscribed on clay and more on wax tablets, papyrus, and paper. The media have changed, writing technologies have changed, and the design and presentation of tables also went through changes. Midcentury modern tables Perhaps the best period for tables was around the middle of the 20th century. Technologies for table (and surrounding document) preparation included the offset printer, the typewriter, and varitype8 (my favorite). The technologies were sufficiently advanced as to allow the precise typesetting of table elements. While of course constrained by the limited space available on pages, tabular design at this point had many workable solutions for fitting tables into single pages or dispersing the tabular content across multiple pages. The combination of advanced printing technology with advanced knowledge of tabular design resulted in beautiful tables. There’s no greater embodiment of that pairing of technology and design than the Manual of Tabular Presentation9, written and published by the United States Bureau of the Census. It is truly a remarkable work which goes into great detail on how the department imagines the ideal designs of information-rich tables. The work articulates the different parts of a table (and each part is given a descriptive name), sparing no detail when describing those different table parts in rigorous detail. Throughout its hundreds of pages, the authors make strong recommendations on what to do (and what not to do) for many tabulation scenarios. When poring over the tables visually depicted in the book, you can’t help but see that tables can both look really good and contain a density of information. The promise and the result is a balance of form and function. We at Great Tables borrow liberally from this work because many of its tabular design principles are just as good now as they were back then (and we’ll talk about what we took from that work in the next section). We’ll end this brief section with a visual montage of snippets from the Census Manual, which provides a glimpse of the sound advice on offer. Little nuggets of wisdom from the Census Manual. This may very well be the ultimate book on tabular design. The late history of tables With computing technologies becoming more accessible by the 1970s and 1980s, people were able to generate tables in both electronic and print form. The democratization of computational tables arguably began with VisiCalc in 1979, a massive success that initiated the computing category of spreadsheeting software. There’s an undeniable advantage to having data analyzed and transformed in computing environments, but, this comes at a cost. This is what it looked like: This is a table in VisiCalc (earliest example of a table in a spreadsheet application). It’s pretty crude compared to the tables in print but the advantage here is that you can calculate values quickly. The grid cells couldn’t be styled with borders for presentation purposes, the values couldn’t be formatted, and the tables couldn’t even be printed. I mean, try it out and you’ll see that this is quite limited in more than a few ways. Over time, and this took about 10-15 years, tables-within-spreadsheets got a little easier on the eyes. By the early 1990s, Excel could paint borders on your tables, better typographical support was available, and the formatting of values was fully-featured (though, wonky). Great! Problem solved, right? Not really. While Excel tables from the last three decades looked much better than 1980s-spreadsheet-borne tables, they could never hold a candle to the what was shown in the Census Manual (no matter how much of an Excel expert you became). Further to this, data analysis started to became a thing accomplished outside of Excel. One example of that is Python and its use inside Jupyter notebooks. We now have a bag of problematic scenarios all Python: analyze data and generate tables all in Python (bad tables) all Excel: analyze data and make tables in Excel (less flexible analysis) split-brained: analyze data in Python, copy over to Excel to make tables (not reproducible) All of these are suboptimal solutions. We propose that it is far better to do everything in Python: the data ingestion, the data analysis, and the data visualization. The visualization step is what’s done for plots and other types of graphics composed from data, it shouldn’t be any different when it comes to generating summary tables. Approach to tables taken by Great Tables Great Tables restores the elegance of midcentury tables with the power of a coding interface. With Great Tables anyone can make beautiful tables in Python. Our framework expresses a table as a combination of six independent components. With this framework, you can structure the table, format the values, and style the table. We firmly believe that the methods offered in the package enable people to construct a wide variety of useful tables that work across many disciplines. You build with Great Tables iteratively, starting off with your table body from code, adding styling, formatting and other components. Here is a schematic that outlines our terminology and depicts how the different table components are related to each other: A schematic with the complete set of table components that can be utilized in Great Tables. Note the following six component pieces: Table Header: a place for a title and subtitle, where you can succinctly describe the table content Column Labels: the column labels define the content of each column, and spanners are headings over groups of columns Stub Head: the ‘top-left’ location, where a label could be used in a variety of ways Row Stub: for row information, including row grouping labels Table Body: contains cells and so it’s where the data lives Table Footer: a place for additional information pertaining to the table content Here’s a table that takes advantage of the different components available in Great Tables. It contains the names and addresses of people. Show the code Names, Addresses, and Characteristics of Remote Correspondents Name Location Personal Characteristics Address City Postcode DOB Height Weight Dustin B. Roach 1183 Columbia Road Holly Oak, DE 19809 Sep 16, 1970 5' 9\" 202 lbs Iwona Adamczyk ul. Zabłudowska 133 Warszawa 04-788 Jan 3, 1976 5' 5\" 124 lbs Geneviève Massé 1415 rue Principale Amos, QC J9T 1E4 Dec 8, 1967 5' 3\" 136 lbs João Souza Lima Rua Cosmorama, 538 São Paulo-SP 04648-080 Apr 21, 2001 6' 2\" 231 lbs Maddison McCabe 149 Raymond Street Strathern Invercargill 9812 Mar 5, 1982 5' 8\" 146 lbs Data last updated: December 18, 2022. A table of named individuals redone, Great Tables style! Notice that there is a blue row stub component that makes the row labels distinct from the body of the table. This is important because each person described forms a unique observation and we want to highlight the subject of each row. The heading provides context on what’s contained within the table. The two column spanners arrange the columns into sensible groupings (e.g., ‘Location’). The consistent use of blue lines and cell backgrounds gives the table a professional look. If you look at the table code above you’ll see that every method for modifying the table starts with tab_. These particular methods are concerned with adding a table component (e.g., tab_header() creates a Table Header) and they’re designed to be easy and straightforward to use. Formatting Table structuring is important, but not the only thing. Tables in different disciplines have a certain set of display requirements specific for any values shown. Even something as simple as a number can be formatted in many different ways depending on a community’s norms and expectations. This extends to a very wide area when we consider that dates, times, and currencies also need to be formatted. Depending on your display requirements, a raw value like 134,000 could presented as: scientific notation (fmt_scientific()): 1.34 × 105 a number in the German locale (fmt_number()): 134.000,00 a compact integer value (fmt_integer()): 134K The problem grows worse when values need to be conveyed as images or plots. If you’re a medical analyst, for example, you might need to effectively convey whether test results for a patient are improving or worsening over time. Reading such data as a sequence of numbers across a row can slow interpretation. But by using nanoplots, available as the fmt_nanoplot() formatting method, readers can spot trends right away. Here’s an example that provides test results over a series of days. Show the code Partial summary of daily tests performed on YF patient Test Progression WBC30.3 4.00 5.26 4.26 9.92 10.5 24.8 30.3 19.0 4.00 10.0 Neutrophils27.2 2.00 4.87 4.72 7.92 18.2 22.1 27.2 16.6 2.00 8.00 RBC5.98 2.68 5.72 5.98 4.23 4.83 4.12 2.68 3.32 4.00 5.50 Hb160 75.0 153 135 126 115 75.0 87.0 95.0 120 160 PLT300 25.6 67.0 38.6 27.4 26.2 74.1 36.2 25.6 100 300 ALT12.8K 9.00 12.8K 12.6K 6.43K 4.26K 1.62K 673 512 9.00 50.0 AST23.7K 15.0 23.7K 21.4K 14.7K 8.69K 2.19K 1.14K 782 15.0 40.0 TBIL163 0 117 144 137 158 127 105 163 0 18.8 DBIL144 0 71.4 105 94.6 144 118 83.6 126 0 6.80 Measurements from Day 3 through to Day 8. Notice that if you hover over the data points, you still get values for each of the days. We designed nanoplots to be stripped down plotting visualizations that balance the quick visual interpretation of a plot against the compactness of a table. Great Tables contains a lot of functionality for formatting. If you peeked at the code in the above table displays you might have noticed there are methods beginning with fmt_ (i.e., fmt_date(), fmt_integer(), fmt_nanoplot()). We want to make many formatting methods available to serve different users’ needs. We also want them to be easy to use, but with many useful options to provide flexibility for all your formatting tasks. Great Tables is focused on display There are myriad ways that people interact with tables. Great Tables is focused on the display of tables for publication and presentation. If you’re analyzing data in a database, you might want a simple table display that offers controls to navigate and filter hundreds, thousands, maybe even more records. And that is great for those situations. The publication of results is a entirely different task, and the emphasis here is on structuring, formatting, and styling. We believe that beautiful table displays should do the following: make information easier to digest provide extra context wherever needed adhere to the style of the document or of the organization We wanted to help the type of user that wanted to present data in this way. This is typically what you see in journal articles, in books, and in reports. We think the area of static summary tables deserves it’s own focus. This class of tables can look great and we offer various opt_*() methods in the Great Tables API so it’s that much easier to provide a great table to your readers. In conclusion Tables have come a long way and we’ve learned a lot from our continued research in tabular design. We hope to make the Great Tables package useful for your generation of summary tables. Given there’s ample room for innovation in this area, we’ll keep plugging away at doing that work to improve the API. We measure success by the quality of the tables the package is able to produce and we always keep that goal top of mind. We’re very excited about where things are going with Great Tables and we geniunely appreciate community feedback. If ever you want to talk tables with us, you’re always welcome to jump into our Discord Server and drop us a line! Many thanks to Curtis Kephart and Anthony Baker for providing helpful advice when writing this article. Footnotes Taylor, B. (2021). Lunar timekeeping in Upper Paleolithic Cave Art. PRAEHISTORIA New Series, 3(13), 215–232.↩︎ Duke, D. W. (2002). Hipparchus’ Coordinate System. Archive for History of Exact Sciences, 56(5), 427-433.↩︎ https://en.wikipedia.org/wiki/Geography_(Ptolemy)↩︎ Palet, J. M. and Orengo, H. A., The Roman Centuriated Landscape: Conception, Genesis, and Development as Inferred from the Ager Tarraconensis Case. American Journal of Archaeology, 115(3), 383-402.↩︎ Marchese, F. T., Exploring the Origins of Tables for Information Visualization. Proceedings of the 2011 15th International Conference on Information Visualisation, 13-15 July 2011, doi:10.1109/IV.2011.36.↩︎ M. W. Green, The construction and implementation of the cuneiform writing system, Visible Writing, 15, 1981, 345-72.↩︎ Robson, E., “Tables and tabular formatting in Sumer, Babylonia, and Assyria, 2500-50 BCE” in M. Campbell-Kelly, M. Croarken, R.G. Flood, and E. Robson (eds.), The History of Mathematical Tables from Sumer to Spreadsheets. Oxford: Oxford University Press, 2003, 18–47.↩︎ https://site.xavier.edu/polt/typewriters/varityper.html↩︎ Manual of Tabular Presentation: An Outline of Theory and Practice in the Presentation of Statistical Data in Tables for Publication. United States. Bureau of the Census. U.S. Government Printing Office, 1949. Resource available at: https://www2.census.gov/library/publications/1949/general/tabular-presentation.pdf.↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=39933833",
    "commentBody": "The design philosophy of Great Tables (posit-dev.github.io)400 points by randyzwitch 16 hours agohidepastfavorite72 comments paddy_m 15 hours agoGreat tables has done some really nice work on python/jupyter tables. It looks like they are almost building a \"grammar of tables\" similar to a grammar of graphics. More projects should write about their philosophy and aims like this. I have built a different table library for jupyter called buckaroo. My approach has been different. Buckaroo aims to allow you to interactively cycle through different formats and post-processing functions to quickly glean important insights from a table while working interactively. I took the view that I type the same commands over and over to perform rudimentary exploratory data analysis, those commands and insights should be built into a table. Great tables seems built so that you can manually format a table for presentation. https://github.com/paddymul/buckaroo https://youtu.be/GPl6_9n31NE reply otoburb 10 hours agoparentThanks for your work on Buckaroo! Jupyter print() and IPython display() have limitations given their dead static output and feels like printf debugging of yore, which I know Buckaroo was written to address. What are your thoughts on Visidata's hotkeys and controls? I used Visidata in the past and always wondered why it couldn't be added into Jupyter (eventually) for dataframe explorations. >It looks like they are almost building a \"grammar of tables\" similar to a grammar of graphics. Agreed that Great Tables seems to be taking annother crack at formalizing a \"grammar of tables\", and I welcome this approach too given the power of tabular formats and wider adoption of the dataframe concept via the R/pandas/Arrows/polars ecosystem, although I believe the term was initially referred to in the 90s[1] from the statistical S language. [1] https://towardsdatascience.com/preventing-the-death-of-the-d... reply paddy_m 9 hours agorootparentBuckaroo started as a lowcode UI with an accompanying table. The low code UI lets you click on columns and perform actions (drop, fillNA, groupby). The dataframe is then modified, AND python code to perform the same action is emitted. Controlling the lowcode UI through keyboard shortcuts should be fairly straightforward. The other feature I have played with in this area is auto-cleaning. Auto-cleaning looks at individual columns and emits cleaning commands to the low-code UI. Different cleaning strategies can be implemented and toggled through. Buckaroo takes the view that being opinionated is good, so long as you can toggle through opinions to get the right combination of cleaning, display, or post-processing that you are looking for quickly. All of the features of buckaroo are also built to be easily extendable by users. This feature saw very little use, so I haven't developed it much (I had to disable it after some refactorings). The lowcode UI is demonstrated at the end of the youtube video linked above. reply jszymborski 13 hours agoprevThe example they show of a Great Table is, to my taste, way too busy. Here is my unsolicited opinion: The top and bottom horizontal rules on the Title appear to be superfluous, and I dislike how it is aligned with the first column (row labels) rather than the second. I feel like a little space to breath at the bottom, along with a bold font would add visual hierarchy w/o the clutter. The row label backgrounds are far too dark and the font weight makes it hard to read. I'd prefer a very light blue here instead. I don't like the row group label (\"Name\") being italicized. The spanner labels floating in the centre make the table hard to scan. Would be much nicer aligned left. Finally, I really dislike the font (maybe this is just my browser, though). I mocked-up some of the changes here, I think this is a much easier to read table: https://i.imgur.com/iMMf5vo.png reply inopinatus 31 minutes agoparentTable titles should be either centered above, or captioned below. Left-aligning them above any column instantly conveys a generally false/unintended impression of the title being a top level in the information hierarchy. Even in the otherwise rather elegant makeover above I was immediately uneasy that the title stipulated “names, addresses, characteristics” whilst aligned to exclude the names. edit: on further reflection I also think it’s a crappy title. Titles and captions should convey context, scope, purpose - and may otherwise be omitted entirely for the editorial sin of failing to justify their own existence. This one could be captioned “Table 1” with no loss of information or generality. For an article that’s trying to reformulate tabular presentation from first principles, it’s a little disappointing. reply mikehollinger 11 hours agoparentprevYou might want to read Edward Tufte's Beautiful Evidence.[1] He discusses stuff like what you brought up about readability and distracting from the message / point of the data. If you've seen sparklines, [2] Tufte coined the term. Whenever I do a UI review I end up paging through it just to see if there's something we're not thinking about, and its an interesting book to just open to a random page and read. Plus he has an entire treatise on why PowerPoint is terrible. [1] https://www.edwardtufte.com/tufte/books_be [2] https://en.wikipedia.org/wiki/Sparkline reply airstrike 11 hours agorootparent> Plus he has an entire treatise on why PowerPoint is terrible. As someone trying to build a PowerPoint competitor, this is awesome. I'm going to start here and work my way through his whole corpus reply esafak 11 hours agorootparentSee also https://norvig.com/Gettysburg/ reply jszymborski 9 hours agorootparentprevI'm a big fan of Tufte and he certainly informs a lot of my opinions on making tables and figures :) reply pimlottc 12 hours agoparentprevI don’t understand why there aren’t any horizontal rules or stripes etc to reinforce the idea that each row is its own record. reply hooloovoo_zoo 12 hours agoparentprevKeep going IMO: shorten the title to remote correspondents since the rest is redundant with the column names. The blue highlight is now redundant with the title so ditch all of it. Personal characteristics vs location don’t meaningfully improve the organization so ditch those as well. reply zem 12 hours agoparentprevthe white text on a dark background really was a glaring misfeature in the original example, to the extent that i wonder if the colours looked different on the author's monitor reply sixQuarks 13 hours agoparentprevI totally agree with you. You should start a new library called Even Greater Tables. reply benterix 1 hour agoprevThis guy deserves some prize for 1) great work, 2) attention to detail, 3) in-depth research, 4) excellent presentation of his work (sparing the usual questions like \"but what is it, really?\", \"how do I start?\", \"can you provide some examples?\"). People from Show HN - watch and learn. reply ttymck 15 hours agoprevWow. This looks incredible, thanks for sharing. It makes me wonder how we've gone this long with increasingly poor data table presentations (the mid-century modern tables are astutely pointed to as shining examples). This makes me excited to get back into data analysis with python. Moreover, I see some possible API improvements and extensions I'd like to make. reply countrymile 13 hours agoprevI love this package and have been using it for a few years in R. It's great [for making] tables in html but the pdf and docx output is a little less polished. I do worry that the recent shift to bringing the python version up to speed with the R version has slowed down the R development. Though it's well worth checking out whatever your language. reply lqet 1 hour agoprevHere is a very short guide on how to make nice tables (which, IMHO, look a lot better than the visually cluttered \"Great Table\" example from the article) for scientific papers: https://people.inf.ethz.ch/markusp/teaching/guides/guide-tab... reply wglb 6 hours agoprevThis a good article with some fascinating history. More recent history involves the production of CALS tables https://en.wikipedia.org/wiki/CALS_Table_Model. The company Datalogics https://en.wikipedia.org/wiki/Datalogics was heavily involved in the CALS table initiative. Datalogics staff was part of the ISO committee forming SGML, and trained many people on SGML, including DoD staff and their contractors involved with documentation. I was involved with the team that produced an editor for SGML-based documents. It had as one of its features the ability to specify the formatting of an element based on the SGML context of that element. This was before XSLT and its kin. Alumni of Datalogics helped Microsoft learn about XML (\"No, you can't arbitrarily switch case on XML element tags\"). Also TeX practitioners have pretty well-formed opinions about how tables should be formatted. Odd side-note: I learned that the documentation for a fighter airplane of the time, if printed out, would weigh more than the aircraft and would fill a football-field sized collection of filing cabinets. And as much as many today don't like XML, coming from the SGML world it is a boon. reply eviks 4 hours agoprevWould be great if the example tables were great for such a post. Like their remote correspondent's table: they've actually made it partially worse in the \"great tables\" style: - now the dates are not vertically aligned, and - the weights have repetition of units (lbs) (although losing the decimal is an improvement) - the name column is way too visually \"heavy\", that's a style you'd reserve for a header, a simple bold would suffice They've also retained other issues from the original like city not being a city, but a combo of city and region, or similarly no separation of the first name of a person, which is especially important for a diverse group of people reply ewuhic 1 hour agoprevThere was some design blog post several years ago, maybe not even surfaced on HN: Creating beautiful tables both UI and UX wise, with some features being e.g dropping separator between columns (row?), doing some visual accents, etc. And yes, the most distinguished feature was that the tables weren't looking like your busy PowerPoint non-tech organisation stuff, they were very modern yet simple. I don't remember the specifics, but I was really impressed and regret not bookmarking the article. Does anyone know of the article in question, and maybe could share the link? reply adamjb 59 minutes agoparentIs it this, from Matthew Butterick? https://practicaltypography.com/grids-of-numbers.html reply closed 13 hours agoprevHey one of the co-maintainers of Great Tables, along with Rich Iannone, here! I just wanted to say that Rich is the only software developer I know, who when asked to lay out the philosophy of his package, would give you 5,000 years of history on the display of tables. :) reply antidnan 14 hours agoprevThere's also a book on the subject: https://en.wikipedia.org/wiki/The_History_of_Mathematical_Ta... Interesting aside: AI models trained on spreadsheets need \"good tables\" such as column names, headers, etc. to understand context. Like Fortap: https://arxiv.org/abs/2109.07323 reply richmeister 14 hours agoparentThanks for sharing the book info! I really need to find a copy of that somewhere :) reply hilti 4 hours agorootparentMe too. It‘s listed on eBay for $145 which is a lot of money. reply bigger_cheese 6 hours agoprevI Use SAS reports in my job pretty heavily I'm keen to find alternatives - this looks pretty promising. One thing in particular I'm interested in but could not see an example for is if this will let you insert \"break lines\" i.e. for displaying sub totals and similar. For example based on the demo, which shows names and addresses from census data it might be nice to be able to break at each change in postcode and display some summary data like a count of people found at that postcode or an average age (based on the DOB) living at that postcode or similar. Otherwise conditional formatting is another pain point either using rules i.e. if value in column B is greater than a specified threshold make the entire row bold. Or automatically creating a color gradient to highlight the cells ala Excel. For bonus points management types like things like red and green traffic lights (or down/up arrows) you can display next to kpi data in a table It's a gimmick but wins you points. reply xnx 14 hours agoprevTables are underutilized for how concise and descriptive they can be when making comparisons. It's a shame most text editors start with a blank table instead of inserting one pre-configured with some good design choices. reply doctor_eval 2 hours agoparentIt’s funny you should say that because Apple’s tools all have preconfigured table styles, in Keynote, Pages and Numbers. And they all suck. reply flobosg 13 hours agoprevRegarding “nanoplots”: they are essentially sparklines, aren’t they? reply icarusz 13 hours agoparentYes. They are sparklines. I actually asked Rich the author if they should just be called that in great_tables but he had some reasonable thoughts on why a distinct name made sense. reply jimhefferon 14 hours agoprevI'm interested in the midcentury modern ones because they have lots of vertical rules. I'm active on the subreddit for LaTeX and there is a religion common there that even one vertical rule is an unforgivable abomination. reply tadfisher 14 hours agoparentGreat summary of the problem on StackOverflow: https://tex.stackexchange.com/a/40555 reply jimhefferon 13 hours agorootparentThanks. I've not seen that particular post. I just made a table this morning for Calc II notes. The first column says something like $f'(x)$ in the first row and $f'(0)$ on the second. The table body lists values for different functions, one per column. I put in a column rule separator because the leftmost column seems separate from the others. In any event, I'm suspicious of rules (pun noted). reply mcswell 12 hours agoprevNot mentioned yet are DocBook tables, of which there are several types. The kind we used starts here: https://tdg.docbook.org/tdg/5.1/cals.table. You have to drill down to get inside the tables. They have some--but I think not all-- the structure of GT. There's also of course LaTeX (mentioned in a couple other comments here), which has \"ordinary\" tables and long tables (tables that span more than one page). reply throwaway81523 15 hours agoprevThis article is mostly blither, whether or not it is AI generated. It is about a Python library for generating nicely formatted HTML tables, though they don't tell you much about it til the near the end. The library seems to use an OOP approach. An alternative approach might be more declarative. The product name \"Great Tables\" appears in boldface over and over (no idea if the font helps SEO) and the name itself is awfully pretentious imho. Overall, the library itself sounds ok,, but the blog post is the annoying market-speak that frequently makes me cringe here on HN. It would be nice to add some interactivity features to the tables, like ActiveAdmin in Rails. reply paddy_m 15 hours agoparentIt's not AI generated. I have tracked the PR as they have worked on it. From what I have seen, the library has declarative elements that are similar to the grammar of graphics. reply sebastiansm 15 hours agoprevIt's great that the RStudio team is working on Python libraries. Hope to see dplyr and ggplot someday on Python. reply hadley 15 hours agoparentYou should check out https://siuba.org and https://plotnine.org :) reply randyzwitch 15 hours agoparentprevplotnine is a ggplot also funded by Posit (though, externally developed) https://plotnine.org/ reply kyllo 12 hours agorootparentI use plotnine whenever I need to make (static) plots in Python. It's really quite well done, a close match to R's ggplot2, and more feature complete than any of the other Python grammar of graphics packages I've tried. reply Narann 2 hours agoprevNothing in the product page explains how you generate the HTML code and/or the image. reply zokier 2 hours agoparenthttps://posit-dev.github.io/great-tables/reference/#export reply shymaple 3 hours agoprevAwesome, recently we started working on table functionality for one of our feature and your post is really helpful. Thanks! reply simonbarker87 13 hours agoprevThis looks great. I so wish that the HTML table element would get some progress - it’s so limited. I don’t want to have to use some JS library component just to show tabular data especially given how badly they perform one big - but a server side rendered HTML table can be enormous and render fine. But again, so limited. reply paddy_m 12 hours agoparentPast a certain table size, the JS libraries will use less memory. DOM elements take a lot of memory. Libraries like ag-grid only render a small portion of the total table at a time. The next performance gain web tables comes from using a binary encoding instead of JSON, particularly arrow. Perspective uses arrow (in addition to rendering to canvas). IME building buckaroo on top of ag-grid, I can render the table with up to about 300k elements very performantly with just JSON. Rendering speed is a non factor because only 50 rows are rendered at a time. Moving to arrow-js should be about 3 times faster for the entire system (python serialize, js deserialize, js render). Beyond 900k elements, you really want to lazily load from the server as the user scrolls. The memory usage for just the data in the browser tends to slow things down. (I am working on a library and benchmark for different serialization techniques). reply benibela 11 hours agorootparent>Libraries like ag-grid only render a small portion of the total table at a time such libraries often mess the scrolling and searching up reply acrophiliac 15 hours agoprevWhile I'm waiting for the packages to download, can you explain how I get tabular output when I run a python application using your package at the command line? Does it produce HTML output? PDF? Your \"getting started\" docs doesn't explain. reply cscheid 15 hours agoparentYou use it indeed to generate HTML output. For Python+Jupyter folk, that's most directly applicable to Jupyter Lab or Jupyter Notebook settings. You can use it with Jupyter book, nbconvert, or any other tools that convert .ipynb to HTML output... (Disclosure: Quarto dev here) ..., like Quarto. You can use `great_tables` in code cells in Quarto to get great tables in your RevealJS presentation or website, https://quarto.org/docs/output-formats/html-code.html. reply seanwilson 12 hours agoprevHow does this compare to https://github.com/jieter/django-tables2? That one makes it really easy to display database models as HTML tables with column sorting and pagination, and search/filtering can be added on top with django-filter. reply two_handfuls 14 hours agoprevSummary: This article is about a Python library called “Great Tables” that is focused on the display of tables for publication and presentation (not for interactive browsing). The article does not specify which output format it supports. Also you get some bonus historical context on tables. reply frodowtf 12 hours agoparent... the obligatory \"historical context\" nobody asked for. reply WuxiFingerHold 7 hours agoprevI overall like the approach for complex scenarios but their example is not the best one. The original version is much more readable and their final version adds mostly noise. Also, in documents all images and tables should have descriptive captions. So their header with title and subtitle would be redundant. reply jiggawatts 11 hours agoprevSomething that always annoyed me about numeric data like dollar amounts in tables is that visually the comparison between quantities is logarithmic instead of linear. E.g.: Cost $1500 $130 $110 $210 The text in the last three rows look 4/5ths the size of the text in the first row. However, even if summed, the last three costs add up to only 1/3rd of the top row! People visually see the number digits, which is roughly the same as Log 10. I’ve so often had this issue that I started putting in-cell bar charts into every finance-related spreadsheet. Otherwise meetings will get derailed debating the cost of something trivial that is totally irrelevant compared to the biggest absolute costs. As a real example, I had many meetings spent debating a $15 monthly cost for server log collection in the cloud for a VM running a database engine that costs $15K monthly for the license alone. reply snappr021 4 hours agoprevContenteditable? reply semireg 14 hours agoprevDoes anyone know any similar projects that can render to an HTML canvas? reply paddy_m 13 hours agoparentWhy do you want to render to canvas? Perspective seems to be the most performant html table. It is more focused on extremely fast updates than styling, although it looks good. Glide is a newcomer that also renders to canvas. https://github.com/finos/perspective https://github.com/glideapps/glide-data-grid reply narush 14 hours agoprevThis is an excellent blog post - I'd never heard of Great Tables before, and I'm a newly minted fan! > confronted with an all-too-familiar dilemma: copy your data into a tool like Excel to make the table, or, display an otherwise unpolished table. One add-on (coming from the past 4 years of working on a tabular-data from Pythons startup [1]) is that users aren't just copying data into Excel because if it's good formatting capability: very often, there are organizational constraints that mean that Excel _needs_ to be where this data ends up. The most common reasons I've seen for data ending up in Excel: 1. Other parts of the report rely on Excel features - you want to build pivot tables or graphs in Excel (often, these are much easier to build in Excel than in Python for anyone who isn't a real Pythonista) 2. The report you're sending out for display is _expected_ in an Excel format. The two main reasons for this are just organizational momentum, or that you want to let the receiver conduct additional ad-hoc analysis (Excel is best for this in almost every org). The way we've sliced this problem space is by improving the interfaces that users can use to export formatting to Excel. You can see some of our (open-core) code here [2]. TL;DR: Mito gives you an interface in Jupyter that looks like a spreadsheet, where you can apply formatting like Excel (number formatting, conditional formatting, color formatting) - and then Mito automatically generates code that exports this formatting to an Excel. This is one of our more compelling enterprise features, for decision makers that work with non-expert Python programmers - getting formatting into Excel is a big hassle. Of course, for folks who can ditch Excel entirely, this is entirely unnecessary. Great Tables seems excellent in this case (and anyone writing blog posts this good is probably writing good code too... :) ) [1] https://trymito.io [2] https://github.com/mito-ds/mito/blob/dev/mitosheet/mitosheet... reply golergka 5 hours agoprevBefore clicking the link I was wondering if it's about relational schemas or woodworking. reply boringg 13 hours agoprevI was really looking forward to a discussion about beautiful wood tables. I should have known better reply throw_m239339 7 hours agoprevThis was surprisingly informative and entertaining. reply pphysch 10 hours agoprevThe generated HTML for the tables looks pretty good. How easy is it to attach extra classes to the elements? Is cell content HTML-escaped by default? reply jamesdutc 8 hours agoprevThe historical background about tabular displays of quantitative information is very interesting. I imagine it must have been fun think deeply about this problem. Unfortunately, the API design in the example is just not very good: ( GT(simple_table, rowname_col='Name') .tab_header(title='Names, Addresses, and Characteristics of Remote Correspondents') .tab_stubhead(label=md('*Name*')) ... ) I'm uncertain if it's trying to mimic something in another language like R (or some grammar of graphics thing or D3.js.) Hopefully, it's not trying to mimic the look of long, chained `pandas.DataFrame` operations (because it misses the point of why those look the way it does.) Of course, for ad hoc, in-a-notebook, cut-and-paste/written-from-scratch use, the API design doesn't really matter that match. Usually, users will readily memorise the required incantations then fiddle with the result until they get what they want or they give up. It's probably the case that for most tools that produce visual outputs, a majority of users are creating things in this style. (There are, e.g., millions of casual Matplotlib users out there.) But programmatic use is not too far off. Tools that produce visual outputs (even those as formally rigidly at display tables,) are often subject to consistency requirements, which directly implies programmatic use. So, when I discover that my colleagues and I have six tables across three notebooks that need a consistent look, and I decide to interact with this tool programmatically, am I expected to write…? def standard_table(source, /, rowname_col, header_title, stubhead_label, weight_columns): return ( GT(source, rowname_col=rowname_col) .tab_header(title=header_title) .tab_stubhead(label=md(f\"*{stubhead_label}*\")) .fmt_integer(columns=weight_columns, pattern=\"{x} lbs\") ... ) standard_table(simple_table, rowname_col='Name', header_title='Names, Addresses, and Characteristics of Remote Correspondents', stubhead_label='Name', weight_columns='Weight') Or maybe…? def format_table(weight_columns): return ( tbl .tab_stubhead(label=md(f\"*{tbl.stubhead.label}*\")) # what if not present? .fmt_integer(columns=weight_columns, pattern=\"{x} lbs\") ... ) format_table( GT(simple_table, rowname_col='Name') .tab_header(title='Names, Addresses, and Characteristics of Remote Correspondents') .tab_stubhead(label='Name') ... ) Or maybe…? class StandardTable(GT): def tab_stubhead(self, *a, **kw): # inspect.signature.bind(...) # ... return super().tab_stubhead(*a, **kw) StandardTable(...) These aren't great options. The API design is just not very good. reply tonymet 13 hours agoprevImagine the web if every site was exclusively tabular. No UIs just a table of figures and a CRUD for modifying it. Something like hypercard meets excel reply zokier 2 hours agoparentI'm less interested about web, but I think tables would work well for shell/terminal. If you squint, stream-of-objects and tables are sort of similar, so with some sort of PS/nushell style environment having a terminal with native table rendering ability could be great. I think lot of stuff we interact with in shell tends to be somewhat tabular in nature, but is now getting muddied by formatting it for dumb text terminals. Just look at the first example on nushell frontpage: https://www.nushell.sh/ could that not look better with \"Great Tables\" or something similar? reply flobosg 13 hours agoparentprevAh… the good old spacer.gif days… reply tonymet 13 hours agorootparentha! i mean actual tabular data not abusingfor layout reply dkh 12 hours agorootparentYour proposal is the most extreme opposite of this practice. Still got PTSD from early 2000s webdev? ;) reply samatman 12 hours agorootparentprevIf you've never done a \"view source\" on Hacker News, now might be a good time... reply akira2501 13 hours agorootparentprevtabularasa.webp reply magnio 4 hours agoprevTLDR: you can create booktabs-style tables in Python. Strange that they did not know (or credit) booktabs, the LaTex package that popularizes this table design since 2003. reply tomcam 14 hours agoprev [–] Fantastic article, duly bookmarked. However. “The democratization of computational tables arguably began with VisiCalc in 1979… I mean, try it out and you’ll see that this is quite limited in more than a few ways.” Them’s fightin’ words. IMHO VisiCalc’s ability to generate models quickly changed civilization. It freed people to try out ideas at no cost and to view or manipulate data in ways no one could hope to do before. reply walden789 3 hours agoparent [–] same reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Design Philosophy of Great Tables delves into the historical development of tables, stressing the significance of structured columns and rows for effective data presentation.",
      "The book follows the evolution of tables from ancient eras to modern tools like Excel, promoting a thorough data analysis approach using Python.\"",
      "Great Tables outlines a Python framework for crafting well-organized and visually appealing tables, including customization options and formatting techniques to enhance the quality of static summary tables for various purposes.\""
    ],
    "commentSummary": [
      "The post delves into the design concept of the Great Tables project, concentrating on Python/Jupyter tables and crafting a \"grammar of tables\" for presenting data effectively.",
      "An introduction to the Buckaroo table library is provided for interactive data table exploration, critiquing Jupyter's print() and display() functions while stressing the significance of standardizing a \"grammar of tables\" for data analysis.",
      "Insights are offered on table layout, readability, data visualization principles, historical table production, efficient rendering using libraries like ag-grid and arrow, and tools like Mito for enhanced data formatting and visually appealing tables in Python."
    ],
    "points": 400,
    "commentCount": 72,
    "retryCount": 0,
    "time": 1712253628
  },
  {
    "id": 39930195,
    "title": "Advanced Pulse Compression Radar for Long-Range Applications",
    "originLink": "https://hforsten.com/homemade-6-ghz-pulse-compression-radar.html",
    "originBody": "Homemade 6 GHz pulse compression radar Date 2024-04-03 Introduction FMCW and pulse radar architectures. I have previously made several FMCW radars that have worked well. FMCW (Frequency Modulated Continuous Wave) radar is quite easy and cheap to make. It uses separate transmit and receive antennas, which avoids the need for switching between receiving and transmitting. It mixes the received signal with the transmitted signal, resulting in a low output frequency, making it possible to use low-speed analog-to-digital converter (ADC). However, big, serious radars typically aren't FMCW radars, instead they are pulse radars. Switching one antenna between transmit and receive modes allows them to use just one antenna. When an antenna diameter is measured in meters it matters a lot how many are needed. Pulse radar can use large transmit power without worrying about saturating the receiver, which is a big issue with FMCW radar. Pulse radar is also better for measuring velocity of fast-moving targets as it can transmit pulses more frequently, resulting in larger maximum unambiguous Doppler shift it can measure. For these reasons, FMCW radars are usually used in short range applications such as automotive radars and aircraft altimeters, while pulse radars are used mainly for long-range applications such as weather radars, aircraft detection, and synthetic aperture radar imaging from aircraft or satellite. Pulse radar is much more difficult to design than FMCW radar. To share one antenna, very fast switching between transmit and receive is needed. Radar pulses travel at the speed of light, and for example, if switching from transmit to receive takes 1 microsecond, all the reflections from targets in 150-meter distance would be missed during the switching time. Sharing one antenna causes the radar to have a minimum detection distance, which can be hundreds of meters which makes it unsuitable for short-range operation. Another difficulty is that pulse radar requires much faster ADC to capture the received pulses. FMCW radar mixes transmitted and received waveforms which results in a low-frequency sine wave for each target at the mixer output, for short-range operation, it's possible to use ADC sampling frequency of 1 MHz or even less while using hundreds of MHz of RF bandwidth. Pulse radar requires much faster ADC, typically fast enough to sample the whole RF bandwidth of the transmitted pulse. The range resolution of the radar is determined by the RF bandwidth, and for useful range resolution ADC sampling rate should be hundreds of MHz or even over 1 GHz. This fast ADCs are very expensive and require expensive digital electronics to handle all the data. This article is about my experiences building a modern pulse radar utilizing fast digital signal processing cheaply. Pulse compression radar There are many kinds of pulse radars, and the one I want to make is a pulse compression radar that supports arbitrary waveforms. Generating only linear frequency sweeps could be simpler and sufficient for many practical applications, but it wouldn't be as interesting. The requirement for arbitrary waveform means that there needs to be a digital-to-analog converter (DAC) with large enough sampling rate to generate the transmitted waveform. The receiver also needs an ADC with large enough sample rate to sample the whole RF bandwidth. Radar RF side block diagram. Above is the block diagram of the radar. The architecture is very similar to software-defined radio (SDR) and it could be used as a radio too. The radar has two time-multiplexed receiver antennas with transmitter being shared with one of them. I added the second receiver channel mainly because it was very cheap, it only requires additional switch, LNA and SMA connector. The second receiver channel makes it possible to use the radar also in FMCW mode. In a proper radar system some filtering would be useful at both transmitter and receiver, but I left it out here to save money. Superheterodyne and zero-IF (direct conversion) transmitters. Superheterodyne mixing to IF frequency is done digitally. TX and RX are zero-IF architecture. This is not ideal from a performance point of view but it's the cheapest option. The output of all mixers contains not only the desired frequency-shifted signal but also local oscillator (LO) leakage and image signals, which is the same signal as the desired one but mixed at the opposite side of the LO frequency. If DAC generated the signal at offset frequency it would be possible to filter out the unwanted frequencies at the mixer output with a bandpass filter, but with zero-IF transmitter these unwanted frequencies overlap the signal, making it impossible to filter them out with a fixed filter. Similarly, these same nonidealities are also present at the receiver. These nonidealities cause distortion of the received waveform, leading to increased range sidelobes for each target. While superheterodyne architecture would provide better performance, implementing it would require more hardware and my goal is to make a working system with minimal budget. With zero-IF architecture many of the nonidealities can be compensated sufficiently digitally. Predistorting the DAC output signal can compensate for the mixer nonidealities, resulting in a clean output signal. Receiver output signal can also similarly be modified digitally to remove many of the nonidealities if they can be characterized to sufficient precision. Complex linear frequency sweep signal in time domain and instantenous frequency. The DAC outputs a complex IQ signal that is modulated by the IQ mixer to the LO frequency and transmitted by the antenna, transmit/receive switch is then switched to receive and reflected signal is sampled by the receiver. Each target reflects some of the transmitted signal and the reflected signal is a sum of the signals from each target. Complex IQ format allows representing both positive and negative frequencies at the baseband. At the transmitter IQ modulator the I and Q signals are mixed against LO and 90 degree phase shifted LO and summed. The result is that positive baseband frequencies are shifted above the LO frequency and negative frequencies at baseband below the LO frequency. Pulse compression of the received signal. By correlating with the reference signal the power from pulse is concentrated. To get the target locations convolution is calculated against the transmitted signal. At the time instance where there was a target the received and transmitted signals correlate and convolution results is large, when there isn't a signal there isn't a correlation and convolution result is small. In practice the convolution is calculated using fast Fourier transform (FFT) as that is faster in practice than calculating the convolution in time domain. In the above plot sidelobes can be seen around the two targets in the result. These result from the convolution output not being completely zero when the waveform isn't aligned. Multiplying the reference pulse by a windowing function can be used to control the sidelobes of the convolution output. Windowing function can also be applied to the transmitted pulse to further decrease the sidelobes. Drawback of windowing is that it widens the mainlobe and results in slightly worse range resolution. How much sidelobes are traded for resolution can be controlled by the used windowing function. Range-Doppler processing. Only the amplitude of the pulse is plotted in graph. Phase of the signal is important for Doppler FFT. Besides the distance to the target, radar can also measure velocity of the target from how phase of the received signal changes during many measurements. By sending a burst of pulses and calculating FFT over the number of pulses dimension, the targets are separated both in velocity and range in the resulting range-Doppler map. Velocity could be also measured from change of distance, but the beauty of using phase shift of the received signal is that velocity can be obtained from the same measurement as the distance, multiple objects at the same range but with different velocities can be separated, and the measurement accuracy is much better. Detecting multiple objects at the same range with different velocity is important for separating moving objects from stationary objects such as ground, trees, and buildings that can have a large reflected signal that would otherwise mask a small moving object. Measuring angle of the target would also be possible with multiple antennas, but in this case with one antenna there isn't angle information. ADC and DAC 2 channel LVDS interface ADCs with >10 bits, sample rate vs price from Digikey. ADC sample rate is one of the most important parameters for the system as it determines the maximum RF bandwidth that the system can receive. ADC sample rate should be as fast as is affordable. In general, it's much easier to make the RF side and DAC to have greater bandwidth than ADC, and it's ADC bandwidth that limits the system. The requirement for ADC is having at least two channels, this is required for IQ sampling, and LVDS output interface. Two one-channel ADCs could also be used but it's disadvantageous from PCB area and routing perspective. The fastest ADCs usually have JESD204B digital interface, the problem with it that it requires high-end FPGA with high-speed serial transceivers and those are in general too expensive for my budget. LVDS is the highest speed interface that can be connected to regular FPGA I/O pins. In the above plot are all 2 channel ADC with LVDS interface and at least 10 bits. The best sample rate for price is ADS4229 with 250 MHz sample rate for 58 EUR / piece in single quantity. Even higher ADC sample rate would be very desirable, but any higher sample rate than this would get much more expensive. There is one two channel 8-bit ADC with 500 MHz sample rate for 73 EUR / piece, but it has 20 dB lower SNR than the 12-bit ADC and doubling the sample rate would only give back 3 dB SNR. Low bit ADC would decrease the dynamic range of the receiver making it more prone to saturation, and it would require more gain before the ADC, so I decided against using it despite the higher sample rate. Suitable DACs are easier to find, and I chose to use DAC3174 two-channel 14-bit 500 MHz DAC costing 33 EUR / piece. While the system bandwidth is limited by the ADC, it's useful to have more than enough bandwidth on the DAC to make filtering easier. ADC filter ADC requires anti-aliasing filter before it to limit the signal frequency to below half of sample rate (Nyquist rate) to avoid aliasing. To get the largest usable bandwidth the cutoff frequency of the anti-aliasing low-pass filters should be as close as possible to the Nyquist rate, but this makes implementation of the filter difficult as it needs to have very sharp cutoff. Filter should also have equal amplitude and group delay on the passband. Amplitude requirement is easy to understand, we don't want to have different frequencies attenuated different amounts. Group delay measures how much different frequencies are delayed by the filter. If the group delay difference is too large for different frequencies, the received pulse is distorted by the filter decreasing its correlation to the reference pulse. In practice, this shows up as higher sidelobes. ADC lowpass simulation setup. Source impedance of the IQ demodulator is 50 ohms differential. ADC input is also differential, and its impedance varies with frequency having high impedance at lower frequencies, but at higher frequencies the input capacitance is significant. ADC datasheet provides a model for the ADC input which I added to the simulation setup. It suggests adding a resistor at the input that sets the input impedance, series resistors to limit ringing due to bond wires and additional resistor and capacitor across inputs to filter sampling glitches. While adding a 50 ohm resistor across the ADC input would be good for filter design perspective it attenuates the signal too much as there already isn't enough gain in the receiver and the IQ demodulator linearity decreases with low output impedance. I added instead 200 ohm resistor to minimize the signal attenuation. This makes the filter design challenging, as the high load impedance requires using small capacitors and large inductors. Higher impedance also increases the effect of sampling glitches, which are caused by ADC input sampling capacitors rapidly sampling the input signal. Adding IF amplifier would have made the filter design easier. 100 nF series capacitors decouple the DC levels of IQ demodulator and ADC. While it would be good to have DC coupled signals, the DC levels of IQ demodulator and ADC are different and any significantly different DC levels would limit the maximum AC signal range. Simulated frequency response of the ADC lowpass filter. Nyquist frequency marked with vertical line. The cutoff frequency of the filter is set at 100 MHz and there should be -20 dB attenuation at the Nyquist frequency of 125 MHz. Up to about 60 MHz both magnitude and group delay are very good, above that it could be better but its hard to improve with these constraints. There is some variation in the passband magnitude that would have been smaller with 50 ohm impedance. Group delay is relatively good at medium frequencies, at very low frequencies AC coupling capacitor causes the delay to shoot up and near the cutoff frequency there is a peak in the delay. The filter response can be compensated digitally if its a problem. DAC anti-alias filter Simulated frequency response of the DAC lowpass filter. Nyquist frequency marked with vertical line. Digital-to-analog converter is also a sampled system, and it has unwanted aliases at the output that need to be filtered out. The sample rate of DAC is 500 MHz which compared to required signal bandwidth of 100 MHz makes the filter design much easier. The alias frequencies are in the range 400 to 500 MHz for 0 to 100 MHz signal. The filter is designed to have flat magnitude and group delay below 100 MHz and in the simulator both look very good. Cutoff frequency is just above 100 MHz so that the peak in group delay is above 100 MHz. The aliases are much further away in frequency than with the ADC, and they are attenuated at least 65 dB more than the signal. This amount of attenuation is more than enough for the DAC aliases to not cause any issues, but it does mean that they are visible at the RF output. If the signal power is 30 dBm, then the image signal is about -35 dBm. For proper radar the attenuation likely should be higher to avoid radiating power at other than the allocated frequency band. FPGA Xilinx Zynq FPGA block diagram. The chip has two-core ARM CPU and programmable logic with fast interconnect between them. Using just a microcontroller isn't possible for this application. FPGA is required for accurate timing of pulse generation and for managing the ADC and DAC data. Accurate timing of pulse generation is critical for proper operation. Switching between transmit and receive needs to be done quickly and accurately, any timing error in pulse triggering or in the receiver will be visible as large distance error. Pricing of the FPGAs is very bizarre. Looking at Digikey or other resellers many of the suitable parts have prices starting in hundreds of dollars and better ones can cost several thousand. However, the exact same parts can be found for fraction of price from China. For some reason, Zynq 7020 is one of the cheapest Zynq FPGAs in China available at $17, while the exact same part from Digikey costs $173. Zynq 7020 has dual-core ARM-A9 CPU and typical FPGA programmable logic in the same package. Having also a CPU core is useful as it can handle communication to PC. It can also run Linux and I added SD-card for Linux file system if I want to use it, but initially software is running without any operating system. Digital design Block diagram of digital interfaces. With fast ADC and DAC moving a lot of data, it's important to consider whether the system can keep up. In the above block diagram, digital interfaces between important blocks have been drawn. The FPGA SoC consists of two parts: processing system (PS), which is dual-core ARM A9 CPU, and programmable logic (PL), which is programmable FPGA fabric. They are connected to each other through with four 64-bit AXI buses. Their clock frequency is configurable and, in this case, it's set to 130 MHz which is near the upper limit that it can work. One AXI bus is reserved for ADC Direct Memory Access (DMA) and other for DAC DMA, there's also a third, lower speed AXI bus (not drawn in the diagram) for configuring the registers in the programmable logic. A fast connection to the PC is needed to quickly transfer captured ADC samples. Initially, the digital processing will be done on PC, but it should be possible to do it on FPGA too for some applications. If the interface to PC is much slower than the ADC data generation rate, it limits how often the radar can be triggered. For target tracking, this means slower update rate of target positions. 1 Gbps Ethernet is the fastest interface to PC that can be easily connected to this FPGA chip. USB 3 is another possible choice, offering 5 Gbps speed with easy connection to PC, but it would require an external USB 3 transceiver chip and more effort to make it work. The system has a single DDR3 DRAM chip that is connected to the PS side of the FPGA. While the memory chip could be clocked faster, the memory interface speed is limited by the FPGA memory controller to 1066 MHz. Memory bus width is 16-bits. The memory controller supports up to 32-bits, but it would require adding a second DDR3 chip and the higher bandwidth is not necessary in this system. ADC samples are received by the PL side, which has a small FIFO buffer and DMA controller transfer them to DRAM through the PL side. DAC also has its own DMA channel, but DMA uses only one AXI bus limiting it to 8.3 Gbps, which is less than what the DAC needs. It's also important to note that DDR3 bandwidth is less than the sum of the DAC and ADC bandwidths, making streaming DAC samples from DRAM while storing ADC samples at the same time impossible. For this reason, there is a small 1 MB memory on the PL side that stores DAC samples. Pulse samples are transferred from PC to DRAM through Ethernet, then DMA transfers them to the small memory on the PL where they are transferred to the DAC every pulse. A small DAC memory limits the pulse length to 130 µs, but it's plenty for pulse radar. A 130 µs pulse corresponds to 20 km minimum detection distance, and typical pulse length is about 1 µs. The pulse could also be generated on PL, eliminating the need for memory, but lookup table implementation makes it easier to change pulse parameters such as windowing function, pre-distortion, and test different types of pulse waveforms. RF design RF parts take only a small portion of the PCB area. It's also a small amount of required work on the project although it seems like it should be the important part. With the digital parts out of the way it's time to look at the RF side. Designing the RF parts is relatively straight forward. Similar to my previous radars, the operating frequency will be around 6 GHz. This is the highest frequency with many off-the-shelf cheap components due to many consumer applications. RF part consists of: IQ modulator, IQ demodulator, PLL for generating the LO frequency, power amplifier, low noise amplifiers and switches. IQ modulator should have low LO leakage, high image rejection, enough output power to drive PA without needing another amplifier, and baseband voltage level compatible with the DAC output voltage range. There aren't that many possible commercial chip alternatives and most of them are very similar in performance. Same applies also to the IQ demodulator. Choosing a power amplifier was more difficult. While big, expensive radars often have transmit power measured in kW or even MW, but that's unrealistic in this case. I would like to have at least 1 W peak RF power but there are surprisingly few choices at 6 GHz band despite WLAN applications that require power amplifiers. The best suitable amplifier I found was Skyworks SE5004L, which has 2 W typical output 1 dB compression point and high gain of 32 dB, but its documentation is severely lacking. There isn't any graph of gain vs frequency and it requires some external components, but there aren't any values for them in the datasheet. The solution for external components is found in the SE5004L-EK1 evaluation kit documentation, which has the schematic of the evaluation board of this chip. It's also out of stock at the moment at common resellers although it's available at some Chinese resellers. In the end I did decide to go with it because there aren't many other cheap alternatives with enough output power. Switching speed of the T/R switch is very important, and it should have high enough power handling capability to handle the 1 W power amplifier output without blowing up or distorting the signal. Especially the fast switching speed is a though requirement that rules out many options. I ended up choosing MASW-007588 switch that has 55 ns switching speed and 37 dBm 1 dB compression point. While 55 nanoseconds is fast, in that time light travels 16.5 meters. There are better switches specifically made for this kind of applications, but they are too expensive for my budget. Another option would be to use circulator instead of switch. This is common for higher power radars as circulators can handle hundreds of Watts of power, and there is no switching speed. There are some circulators for this frequency, but big issue with them is that they are very large and much more expensive than simple switch. The receiver should have enough amplification that the RF noise floor is above the ADC quantization noise floor. The RF noise floor spectrum at the ADC input can be calculated as kT, where k is the Boltzmann constant and T is temperature in Kelvin. This results in power density of about -174 dBm/Hz at room temperature. LNA amplifies the thermal noise and adds some noise to it which is determined by LNA's noise figure. Switches and PCB lines have some losses. IQ demodulator's voltage conversion gain can be used to calculate the output voltage density at the ADC input. ADC noise floor can be calculated from SNR specification, 69.4 dBFs (decibels relative to full scale) in this case, sample rate 250 MHz, and maximum input voltage 2 V peak-to-peak (0.707 Vrms). Noise is then 69.4 dB below 0.707 Vrms maximum input voltage for each sample, and there are 250 million samples in one second which equals bandwidth of one Hertz. This gives the ADC noise floor density of -156 dBV/Hz. Calculating the RF noise floor at the ADC input after considering the whole signal chain from starting from LNA gives noise floor of about -155 dBV/Hz. This is barely not enough gain. RF noise floor should be much higher than the ADC noise floor, typically about 10 dB, so that the ADC quantization noise doesn't increase the noise of the whole receiver. An ADC driver amplifier could easily have enough gain, but low-frequency amplifiers with high enough bandwidth are surprisingly expensive. In the end, I just decided to have few dB higher noise floor. Maximum detection range The maximum detection range of the radar can be calculated as following: The transmitter transmits a pulse of length ts with average power of Pt, which is radiated by the transmitter antenna with gain G. The power density (W/m2) at distance r can be written using Friis' equation as PtG/(4πr2). This power is reflected by a target with a radar cross-section of σ and some of it is reflected back to the radar. The received power depends on the effective area of the receiving antenna: Pr=PtGAe/((4πr2)2). Ae can be written in terms of the antenna gain as Ae=λ2G/4π, where λ is the wavelength of the RF signal. The equation for the received power at the receiver input can be written as: Pr= PtG2λ2σ (4π)3r4 This is the received power from one pulse. To increase the received power, multiple received pulses can be coherently summed. It's important that the summation is coherent so that the phases of the received pulses are aligned. In practice, instead of summing, an FFT is used so that power from moving targets can be coherently summed and separated from each other. To get the maximum detection range, we need to find the minimum detectable received power. The detection performance is limited by the noise of the receiver. Thermal noise density (W/Hz) of the receiver is kT, where k is the Boltzmann constant and T is the receiver temperature in Kelvin. The receiver amplifies this thermal noise and adds its own noise to it. The noise factor, F, of the receiver is how much higher the noise floor of the output is compared to theoretical thermal noise floor if there wouldn't be any added noise. This can be calculated from the receiver gain, RF amplifier's noise figure and ADC's noise floor. To get the noise floor, we need to multiply the thermal noise density kT by the receiver noise factor F and the receiver's noise bandwidth B. The correct noise bandwidth to use is the minimum bandwidth after all the signal processing which noise can't be separated from the signal. For example, by taking the Fourier transform of the input signal, we can discard all the frequency bins that are beyond where our signal is, and noise at those discarded frequencies won't affect the detection capabilities of the receiver. Pulse compression ideally collects all of the power of a pulse within a bandwidth of 1/ts for a linear frequency sweep. However, in practice, FFT windowing functions and any mismatch between reference and received pulses will decrease this slightly. The minimum detectable signal should be higher than the noise floor by some margin. The threshold value for detections can be chosen freely, but there is a trade-off: if we accept detections that are only just above the noise floor, occasionally some of them may be false detections resulting from noise just happening to be above the detection threshold. The probability of false alarm depends on the method used to estimate the signal-to-noise ratio of the detection. For an ideal detector, the false alarm probability can be calculated based on the probability that normally distributed noise is above the detection threshold. Common threshold value is usually around 13 to 15 dB. At the maximum detection distance the received power is equal to minimum detectable signal: nPtG2λ2σ (4π)3r4 = kTFS ts ,where n is the number of pulses, and S is the detection threshold compared to the noise floor. Solving for r gives the maximum detection range: rmax=4 √ ntsPtG2λ2σ (4π)3kTFS Variable Explanation Value Pt Transmitted power 30 dBm G Gain of antennas 14 dBi λ Wavelength 5.2 cm σ Target radar cross-section 1 m² T Receiver temperature 290 K ts Pulse length 1 µs n Number of pulses in burst 1024 F Receiver noise figure 5 dB S Detection threshold 15 dB In the above table are estimations of the radar system parameters. Plugging these values in the equation gives a maximum detection distance for target with 1 m2 radar cross-section of 1200 meters. This might be slightly optimistic, as there are losses in the cables to antennas, loss from antenna efficiency, losses from mismatch, and atmosphere attenuation. However, the maximum detection distance should still be about 1 km. At this maximum distance the average received power from a target is equal to the minimum detection threshold. Therefore, on average, a target at this distance is detected 50% of the time. Due to normally distributed noise, there is a chance that a target at shorter distance is not detected, and a target at longer distance could be detected. However, because of the fourth power dependence of the received power, the probability of detection drops quickly at larger distances. PCB design Simplified PCB block diagram. PLL generates 6 GHz RF local oscillator and clock generator generates clocks for ADC, DAC and FPGA. Practical implementation of the system requires designing a printed circuit board (PCB) that integrates all the components. The system has both RF and high-speed digital circuits that require careful PCB routing to make sure that they function correctly. The PCB has six layers, and I don't think the FPGA can be routed with any less layers. The material is standard FR-4, which isn't ideal for RF routing since it's quite lossy, but it isn't a big issue in this case since the RF trace length is kept very short. DDR3 routing DDR3 routing implementation. Source: UG933 DDR3 DRAM memory connected to the PS side of the FPGA runs at 533 MHz clock frequency with two transfer per clock cycle. The memory uses the DDR3L standard, which is a low-voltage version of the DDR3 standard with a 1.35 V operating voltage instead of the normal DDR3 1.5 V supply voltage. While this isn't very fast by the modern standards, it still requires some care with the routing. Memory traces should be length matched, have correct characteristic impedance, and be terminated properly to minimize reflections. The nominal characteristic impedance of DDR3 traces is 40 ohms. A shared address bus is fly-by routed to all memory chips and terminated with a 40 ohm resistor to VTT supply, which is at half of the memory supply voltage. Each memory chip has its own data traces with on-chip termination. There are also few control lines that are routed to all memory chips. With only one memory chip on the PCB, the routing is much simpler. The memory bus can be simulated with circuit simulators before being manufactured. Professional programs have ways to do finite element simulation of the PCB, but this is quite difficult with open source software. FPGA and memory chip driver and receiver electrical models are provided as IBIS files. I used KiCad to design the PCB and it's supposed to include IBIS support but it was unclear how to use it. I ended up using SPISim_IBIS web app to convert the IBIS models to SPICE netlists and simulate them with ngspice. DDR3 memory routing simulation of a single trace. I was interested in simulating if address bus termination resistors can be left out in this case where there is only one memory chip, and it's mounted close to the FPGA. I have seen this done on at least one FPGA development board, and it would save some PCB space if termination resistors could be left out. DDR3 databus with 40 ohm line and termination. Normally, an eye diagram is used to analyze the timing margin of the memory bus. However, it's not easy to simulate it with ngspice, so I just added a pulse source and did a transient simulation plotting the voltage at the memory chip input. With 120 ps line delay, 40 ohm line impedance, and termination resistance the memory chip input voltage looks fine. High and low thresholds are 0.81 and 0.54 V according to the memory chip datasheet, and the signal looks very good in the simulator. DDR3 databus without termination resistors. Without termination resistors, the voltage looks fine from the threshold level point of view. However, there is significant under and overshoot. Supply voltages are 0 V and 1.35 V, and the memory chip input voltage overshoots by about 0.7 V, which is enough to forward bias the ESD (Electrostatic discharge) protection diodes of the memory chip. This might be fine in practice, but memory chip datasheet says that the overshoot should be limited to maximum of 0.4 V. For this reason, I added the address line termination resistors. DDR3 databus with 60 ohm line and 50 ohm termination resistors. While removing the termination resistors violates the datasheet guarantees, it's possible in this case to use 60 ohm line impedance and 50 ohm termination resistors with minimal difference in the signal integrity. The benefit of using higher line impedance is that it results in narrower line on PCB allowing for denser layout. A 40 ohm trace is 0.24 mm wide, while 60 ohm trace is 0.10 mm wide. Using narrower trace also allows having more distance between different traces, which decreases cross-talk between traces. 50 ohm termination resistor is close enough to the trace impedance, and since 50 ohms resistors are needed on other places on the PCB, using 50 ohm resistor allows removing one resistor value from the bill of materials, making the assembly slightly cheaper. DDR3 routing. FPGA on the right and DDR3 chip and the termination resistors on the left. Top left is the top layer and bottom right is the bottom one advancing horizontally. Above is the final DDR3 routing on all the PCB layers. Layers 2 and 6 are ground, 5 is supply voltage, and others are reserved for signals. Two grounds are needed for correct impedances on the top, middle, and bottom traces of the PCB. With only one ground plane, the distance from the signal to ground would be too large on either the top or bottom layer. Data bus traces are swapped within the byte boundary to make the routing easier. The traces are length-matched with squiggly lines, and some traces are manually drawn on the ground and supply layers to decrease the size of slots in the planes due to vias. The trace matching requirement is ±10 ps according to the Zynq PCB design guide, which is approximately ±2mm in trace length. However, considering the faster memory chip and having only one memory chip, the actual margin should be much greater. There is also some delay difference inside the FPGA package which should be considered in the length matching. Transmission line termination The T/R switch needs to be switched as fast as possible to minimize dead time between transmit and receive, and the same applies for the IQ modulator enable pin. The FPGA I/O pin driver strength can be controlled, and with the highest drive strength it has a rise time of about 400 ps at the switch input in simulator. However, few centimeters of PCB trace between the FPGA and switch input functions as a transmission line, which has significant effect at these frequencies. The switch input pin is not matched to 50 ohms, and a typical CMOS input has high input impedance. This causes reflections, which severely distorts the switching waveform. Termination of switch input with capacitor and resistor. To minimize reflections, the transmission line should be terminated to the characteristic impedance of the transmission line, which is 50 ohms in this case. Placing a 50 ohm resistor to ground near the switch input pin would work, but it would sink DC current and cause the DC voltage to drop. Termination to supply voltage has a similar issue except that now voltage can't reach 0 V. Termination with a 50 ohm resistor in series with a small capacitor solves the DC level issue. Capacitor value should be tuned so that high-frequency reflections are absorbed without affecting the low frequencies too much. Simulation of switch voltage with and without termination. Above is the simulated voltage at the switch input. Transmission line length was 300 ps, and termination capacitor was set to 12 pF. Without termination there is significant over and undershoot, and a risk that the voltage drops below the threshold voltage slowing the switching. With the termination, the waveform is much cleaner. Power supply Analog electronic components are sensitive to supply voltage noise. This is especially important for RF receiver with input signal at the level of the thermal noise floor. Switching regulators have good efficiency, often around 90%, but their output has switching noise that is significantly higher than the thermal noise floor. If this noise isn't filtered properly, it will couple into the received and transmitted waveforms and cause interference at the receiver. A linear regulator, often called low-dropout regulator (LDO) for historical reasons, functions as a variable resistor, dissipating enough power to ensure that the output voltage is at the correct level. The output noise is much lower, but if the voltage drop is too large the efficiency is terrible. Power supply rejection rate (PSRR) of TPS7A7001 LDO. To have both good efficiency and low noise, it's common to have a switching regulator followed by an LDO to filter the switching noise. However, this isn't enough filtering in this case. LDO filters well very low frequencies, but it's filtering capability drops at the higher frequencies. Above is the power supply rejection ratio (PSRR) of the LDO I'm using. For example, with a 1 mV amplitude, 2.5 MHz signal at the LDO input is attenuated by about 15 dB, resulting in about 200 µV amplitude signal at the output. The requirement for minimum power supply filtering can be obtained with few assumptions about the coupling of the noise. The smallest signal level is at the input of the receiver LNA. The thermal noise floor is -174 dBm/Hz at room temperature. With a 10 ms measurement time, the bandwidth is 100 Hz. This results in a maximum of -154 dBm power at the LNA input. At 50 ohm impedance, this corresponds to 5 nV RMS voltage. If the LNA supply voltage is modulated by noise, it affects the gain of the amplifier, and the supply voltage noise is mixed to the RF signal. In practice, the allowed noise amplitude can be larger since there is usually some power supply rejection at the LNA for low-frequency supply voltage noise to the output RF frequency, but it's usually not specified in the datasheet. With a 10 mV worst-case switching noise amplitude, the required attenuation is 120 dB to reach the noise floor. LDO can be assumed to filter about 10 dB, and we can assume another 10 dB power supply rejection from the RF components, which sets the power supply filtering requirement to 100 dB. Two-stage ferrite bead filter schematic. Capacitor parasitics drawn individually. The required power supply filter can be designed with ferrite beads. They are inductors that are lossy at high frequencies. A capacitor is needed after the ferrite bead to complete the low-pass filter. The series resistance and inductance of the capacitor are crucial at these frequencies, and they are included in the schematic, assuming an SMD ceramic capacitor. Ferrite bead filter frequency response. The above filter achieves 100 dB attenuation at 1 MHz. The switching frequency is 2.5 MHz, and this filter works well at that frequency. However, it has a resonance at 30 kHz, which increases the noise at the output at that frequency. This is caused by the ferrite bead behaving like an low-loss inductor at low frequencies which resonates with the capacitor due to a lack of resistance that would dampen the resonance. It can be fixed in two ways: adding resistance in series with the capacitor or increasing the capacitance. Resistance could be also added in series with the ferrite bead, but this is possible only if the DC current is small. Ferrite bead filter frequency response with 20 µF first capacitor and 200 µF second capacitor. With larger capacitors, the resonance is much smaller, and the attenuation increases slightly. Time domain response to 1 A current step. One ferrite bead with 200 µF capacitance. The response is underdamped and increasing capacitor ESR would decrease the oscillation. An important limitation of the ferrite bead filter is its time domain response. If the output current changes quickly, the inductance of the ferrite bead tries to keep the current through it constant, which means that the output capacitor needs to supply the high-frequency current. If the output capacitor is small, it can't supply the current, and the output voltage drops. If there isn't enough resistance either in series with the ferrite bead or in series with the capacitor, the output voltage oscillates before settling. Especially the power amplifier that has high current draw needs a lot of capacitance to ensure that the supply voltage doesn't drop as it's switched on. The time domain response can be improved by placing the ferrite bead before the LDO. The LDO is then able to keep the output voltage constant while the input voltage dips, but it needs to be ensured that the voltage after the ferrite bead doesn't dip too low so that the LDO stays in regulation. I placed one ferrite bead before the LDO and a second one before each analog component. The first one filters the switching noise, and the second ferrite bead adds additional filtering for each IC. Besides filtering the switching noise, the second ferrite bead for each IC also improves the isolation between components, which is desired between transmitter and receiver. Having a ferrite bead close to each component also reduces the length of the trace that can work as an antenna to pick up radiated noise. In total, the PCB has nine different supply voltages. There are six supplies for FPGA and digital electronics: 1.0 V for FPGA core supply, 1.8 V, 2.5 V, and 3.3 V for various digital chips, 1.35 V and 0.675 V for DDR3 RAM. The noise on these rails isn't too important for the system performance. Analog electronics have low-noise 1.8 V, 3.3 V, and 5.0 V rails with linear regulators and ferrite bead filtering. ADC and DAC routing ADC data trace routing to FPGA. ADC footprint on the right side, FPGA out of view on the left side. The ADC connects to the FPGA with a 12-bit wide LVDS bus. The ADC also generates a clock signal that is center-aligned to the data. The sampling rate of the ADC is 250 MHz, and there are two channels with one channel's data on the rising edge and the other on the falling edge of the clock. This data rate is too fast for the FPGA to capture statically, requiring dynamic capture that uses adjustable delay lines to correct for the signal delay programmatically. These delay lines also make the length-matching requirement for the PCB routing quite loose. The DAC also has an LVDS interface but it operates at 500 MHz with 14-bits. This FPGA doesn't have adjustable output delay lines, so the line lengths must be length-matched to make sure that the DAC can capture the data. The DAC datasheet provides setup and hold times for the interface, and plugging these values into the FPGA synthesizer tool indicates that the timing can be met with ±25 ps trace delay, which corresponds to about ±4 mm difference in the data trace lengths compared to the clock trace. Even higher delay might work, but it's good practice to match the interface as well as possible, especially since it can't be adjusted in software like the ADC interface. On the FPGA side, it's important to set the supply voltages for the banks with LVDS to 2.5 V with this FPGA part. For the receiver, only this voltage works correctly with internal 100 ohm termination. Using internal 100 ohm termination instead of external 100 ohm resistors on each data line makes the routing easier and saves PCB space. The DAC LVDS transmitter also needs a 2.5 V supply voltage for both common mode and differential voltages to be compatible with what the DAC expects. 1 Gbps Ethernet Ethernet chip schematic connections. The chip requires several configuration resistors. The Ethernet interface needs an external PHY chip that is between the Ethernet connector and the FPGA. The cheapest one I could find was Realtek RTL8211F, which can be found for $1 in single quantities from China. While the RTL8211E version of the chip is found on many FPGA development boards, the F version is much more uncommon. The challenge with this chip is that officially the datasheet is provided only under NDA. However, it is available from the Chinese resellers with big \"Confidential\" and \"Not for public release\" labels. However, the datasheet isn't quite clear on how it should be connected, and there aren't any example schematics in it. Searching this chip on Google, I did find few schematics of boards using it, which gave me some confidence that I can wire it correctly. See the above schematic on how it should be wired for FPGA if you are also looking to use it. Important note about the Zynq FPGA is that the Ethernet interface doesn't meet the RGMII interface (FPGA to Ethernet chip interface) specifications when used with 3.3 V supply voltage. Because of this, I had to set the FPGA PS side supply voltage to 1.8 V, which requires adding level shifters for SD card and UART that are powered from the same voltage. JTAG and debug UART FPGA is programmed and debugged with JTAG connection. On development boards there is usually a connector and external JTAG debugger is used to connect to the development board. The official JTAG debugger is quite expensive with $270 list price and I don't want to pay for one. FTDI makes FT2232H chip that can convert from USB to JTAG and UART. This can be used to implement the JTAG interface cheaper. There used to be a drawback that it wasn't supported by the Xilinx official tools which made debugging the design much harder, but now it's officially supported if the EEPROM memory is programmed with tool provided by Xilinx. FT2232H also has UART output that is useful for debugging the ARM processor code. Calling printf in the processor code prints characters to the debug UART. Clock generator Simplified block diagram of clock signals. Accurate timing of the whole system is very important. Several clock signals are unavoidable since the ADC runs on 250 MHz, the DAC runs at 500 MHz, and the FPGA requires even lower clock frequency. The FPGA does have several phase-locked loops that can be used to generate clocks, but accuracy of their output isn't good enough. For a 100 MHz clock the tools predict a peak-to-peak jitter of 130 ps, while the clock generator chip has about 4 ps peak-to-peak jitter. ADC and DAC require very clean clocks with minimal jitter, and any timing error on the sampling clock reduces the signal-to-noise ratio. Everything involved in the radar signal generation or processing should run on synchronized clock signals. For example, if the ADC and DAC would run with completely unrelated clocks, the pulses wouldn't stay synchronized in phase as the clocks would slightly drift. Phase drift would make coherent summing of multiple pulses impossible and seriously harm the performance of the radar. There are two unrelated clocks on the PCB. PS side of the FPGA has its own 33 MHz crystal and it generates clocks for DDR3, CPU, and peripherals from it. 133 MHz bus clock is also generated from it, which is passed to the programmable logic side of the FPGA. The PL side uses an external clock generator chip CDCM6208 to generate several 250 MHz and 500 MHz clocks from a single 25 MHz crystal. These clocks are all phase synchronized to each other. The PS side's own independent clock is that on power up the clock generator has not been programmed yet. The PS side has its own independent clock, which is needed for programming the clock generator. The independent clock domains of PS and PL don't cause issues with proper clock domain crossings. The ADC outputs a 250 MHz clock with the data to the FPGA, which is internally divided by two and used to clock the pulse timing logic. This makes the FPGA logic also synchronized to the clock generator. Frequency division is required because 250 MHz clock is too fast for the FPGA logic. The clock division makes that for each 125 MHz clock cycle, two ADC samples are received from both channels for total of 48 bits of data. There is a FIFO for clock domain crossing to the PS side's 133 MHz, and DMA transfers the samples through a 64-bit AXI bus to the DDR3 memory. 133 MHz is used because it needs to be faster than the 125 MHz input clock and this clock needs to be generated by PS so it can't be the PL 125 MHz clock. The FPGA needs to output a 500 MHz clock with the data to the DAC, and for this purpose, a 500 MHz signal is routed to the FPGA. The FPGA has internal clock generators, but they are not used for this purpose because their jitter is too high. 500 MHz is too high frequency to route on the global clock network of the FPGA, but it's possible to route it on the I/O clock network that is only routed to the I/O buffers. That means no logic can be clocked at 500 MHz, but the chip has serdes that can be clocked from the I/O clock at each pin, which can take four bits at the rising edge of the 250 MHz clock and output them at both rising and falling edges of the 500 MHz I/O clock. Manufacturing Half populated PCB received from the PCB manufactuer. I ordered the PCB from a Chinese manufacturer, including assembly. They sent me two assembled pieces and three empty PCBs. Some uncommon components were not available for assembly, and I had to order those separately and solder them myself. These included all the most expensive components such as ADC, DAC and PLL. Luckily, the FPGA was available for assembly, which saved me the trouble of soldering the large 484-pin BGA package myself. Quality of the PCB looks good, especially considering the price, which is only a fraction of what it would have costed me locally. However, only one of the two assembled PCBs worked out of the box because of soldering issue with one of them. The suspiciously cheap $15 FPGA had equally suspiciously date and lot codes covered (white rectangles on the FPGA chip in the picture above). I have a development board of the same series chip with markings intact, so it definitely shouldn't look like this. It did end up working, but I wonder what the origin of these chips is. Fully populated PCB. I soldered the rest of the components myself using solder paste and hot air tool. It would be difficult to solder the QFN packages without hot air tool on already populated board. Backside of the PCB. Two-sided assembly would have costed extra, so all the components are placed only on the top side. There are some places for additional decoupling capacitors on the bottom side just in case, but those were not needed. JTAG programmer Vivado hardware manager. ARM processor, Zynq 7020 FPGA connected to FTDI chip connected to localhost. The first step to bring up the board is to program the FT2232H chip, which functions as JTAG programmer and serial port. Xilinx has program_ftdi tool that can program its EEPROM so that Xilinx tools recognize it. I first had problems with the tool not recognizing the device. It failed to find the ftdi device, even though I could see it in the Linux system log. After installing some ftdi libraries and making sure that the official ftdi tools were able to read the EEPROM, I was able to successfully program the EEPROM with the program_ftdi tool. Checking the Xilinx Vivado hardware manager, it's now able to find the Zynq 7020 FPGA. Programming and debugging the FPGA now works with the Xilinx tools. FPGA programming FPGA programmable logic block diagram. FPGA software consists of ADC and DAC LVDS interfaces, pulse timing that enables and disables switches, PA, LNAs and other components at the right time, AXI registers that enable the PS to configure the programmable logic, two DMA channels for ADC and DAC samples, and SPI interfaces for ADC, DAC, PLL, and clock generator. Most of the signal processing is done on the PC, and the FPGA mainly passes the data around. However, it would be a good idea to have digital filtering and decimation for the received samples on the FPGA. When the transmitted pulse bandwidth isn't very large, for example when it isn't centered at zero frequency, it's possible to do mixing digitally, filter the samples, and reduce the sample rate. This would enable reducing the amount of data that needs to be sent to PC and increase the frame rate of the radar. LVDS receiver. Source: xapp1017 LVDS receiver is based on Xilinx appnote xapp1017. It connects two delay lines and flip-flops to each LVDS lane with delay difference set so that they sample the signal with 1/2 bit delay. State machine changes the delays so that the master flip-flop samples at the center of the data eye. The dynamically adjusted delay is able to compensate for PCB routing and FPGA internal delay differences. Part of the radar pulse timing circuit VHDL code. The pulse timing circuit is just a counter with equality comparisons for each possible event that can be programmed with AXI registers. The timing circuit is triggered from the PS side of the FPGA, starting the counter that triggers every subsystem on the FPGA and every external chip at the exact correct clock cycle. It also has a loop functionality that can trigger the pulse multiple times with precise repetition interval to support sending a burst of pulses. Accurate timing of the burst is essential for accurate target velocity measurement. Any timing inaccuracy between ADC and DAC transfers to inaccuracy in the measured distance. A single 125 MHz clock cycle timing error in ADC or DAC triggering translates to a 1.2 m error in the measured distance. Receiver noise Testing the radar without antennas. For bench top testing I put matched loads at the antenna connectors, disabled transmitter and recorded the ADC output. Ideally the recorded signal would be noise and any signals visible are unwanted interference. ADC output spectrum without signal, 5.80 GHz LO. The length of the recording is 33 ms which is 8 million samples. The noise floor average is -139 dBFs which is about what it should be. However, there are several interference signals visible, the biggest are multiples of 25 MHz. Their amplitude is about -100 dBFs which corresponds to about 15 µV RMS at ADC input, so they aren't very large. DC offset of the ADC is also visible as very large peak at zero frequency. The source of the interferences is fractional spurs caused by the PLL. They can be changed by changing the PLL output frequency, PLL settings and LO input reference frequency. ADC output spectrum without signal, 5.75 GHz LO. The spurs are minimized when PLL output frequency is integer multiple of the PLL reference clock. PLL reference clock is 250 MHz, but this is too high speed to run the PLL phase detector, and it is divided by two by the PLL reference input divider. With 125 MHz PLL reference clock setting the output frequency to 5.75 GHz makes it integer multiple and almost completely gets rid of the spurs. 5.875 is another close multiple that works well also with RF electronic side. There is still a spur at -125 MHz, but this is expected as it is the phase detector frequency. ADC output spectrum low frequencies. Noise floor of the ADC is higher at low frequencies due to 1/f noise of the ADC. Switching frequency of the DC/DC converters is 2.5 MHz and it's not visible at the output spectrum plot, which means that the supply filtering works as designed. Transmit power The power amplifier I'm using has an integrated power detector. I set the DAC output voltage to 85% of maximum amplitude, which is about the maximum amplitude it can go while leaving some room for DC offset for LO leakage cancellation digital predistortion. This should result in around +3 dBm output power from the IQ modulator, and with 32 dB power amplifier gain it should be enough to drive the PA into compression. PA power detector voltage measured on oscilloscope. The power detector pin waveform looks correct when measured on oscilloscope. It has a series of 2 µs long pulses, which was the pulse width. Peak voltage is 1.72 V and about 0.32 V when not transmitting. Power detector pin voltage vs output power from the datasheet. Datasheet has a plot of expected power detector pin voltage vs output power at different frequencies, but the plot doesn't go as high as I measured. Questionable linear extrapolation gives around 33 dBm output power which is two Watts. -1 dB compression point of the PA is specified to be 34 dBm typical, and it looks like it's in compression as expected. Calibration Transmitted waveform. With matched loads at the antenna connectors I recorded the leakage transmitter signal through the T/R switch. The transmitted waveform is a 100 MHz bandwidth 1 µs long linear frequency sweep with 0.1 of the maximum DAC amplitude. The baseband frequency sweep signal can be written: f(t)=exp(j2π( B 2ts t)t) ,where B is bandwidth, ts is the sweep length, t is time, and j= √ −1 . Received frequency sweep without any correction. 128 overlapping waveforms. The receiver was set to record 1 µs before and 2 µs after the transmitted signal. 128 waveforms were transmitted with very good repeatability with all of them plotted on top of each other on the graph. Ideally the received signal would be attenuated, delayed and phase shifted copy of the transmitted signal, but there is a clear difference between transmitted and received waveforms. The non-idealities identifiable from the time-domain data are: Non-zero DC level before the pulse. This is caused by the DC offset of the ADC. Spike before 1 µs caused by the transmitter being switched on. Pulse has DC offset caused by the LO leakage from the transmitter, I signal has higher DC level than Q signal. Higher baseband frequencies are attenuated more causing a slight drop at the edges of the pulse envelope. Non-zero DC level after the pulse. Caused both by the ADC DC offset and ADC filter high-pass behaviour. Compressed leakage signal. Above is the plot of the pulse compression output of the non-calibrated pulse with -50 dB Taylor window applied to the reference pulse normalized to the peak level. The sidelobe level is -21 dBc, which is far above the ideal level. The biggest error is caused by the LO leakage from the transmitter. LO leakage from the transmitter is mixed down to DC at the receiver because they share the same LO signal. Since DC level of the balanced linear frequency sweep is non-zero, convolution with the reference sweep gives non-zero result wherever there is a non-zero LO leakage that causes the flat correlation output from 0.5 µs to 1.5 µs. To improve the sidelobe level LO leakage needs to be compensated. It can be done by adjusting the transmitter waveform so that it has LO signal in opposite phase that cancels the leakage signal. However, before LO leakage compensation ADC DC offset should be compensated since received is used to measure the LO leakage and DC offset of the ADC interferes with it. DC offset of the ADC is compensated by only triggering the receiver with transmitter disabled. The only signal at the ADC is thermal noise and DC offset. DC level can be measured and subtracted from all subsequent measurements. With ADC DC offset compensated the LO leakage can be measured by triggering the sweep and outputting only zeros from DAC. Ideally there shouldn't be anything transmitted, but due to LO leakage there is signal transmitted at LO frequency which mixes down to DC at the receiver. DC level of the transmitted signal is adjusted such that the ADC input is zero which results in zero LO leakage. Compressed leakage signal after LO leakage compensation. With LO leakage compensation the impulse response looks much nicer. Sidelobe level is -36 dB which is few dB above the ideal -42 dB. There is also a very long -60 dB straight line after the sweep that is caused by the high pass behaviour of the AC coupling capacitors between IQ demodulator and ADC. Time domain leakage signal with LO compensations. DC level after the pulse is very slightly above zero on I channel. The reason for the long flat part is that there is a non-zero DC level after the sweep. Baseband frequency sweep has non-zero DC component and when it passes through the high-pass filter it changes the output DC level. Convolution result of frequency sweep with a constant results in non-zero output. The issue could be reduced by decreasing the high-pass filter cutoff frequency. The AC coupling capacitor is only 100 nF which puts the high-pass cutoff frequency at about 10 kHz. DC offset caused by the high-pass could be also compensated digitally. The decrease in amplitude as frequency increases is quite clear here. DAC sinc response is compensated digitally, so that isn't the cause for the amplitude drop. The ADC filter was supposed to be quite flat in amplitude, but during manufacturing I had to substitute a different inductor than what I initially chose to use. The substitute inductor has higher series resistance and amplitude isn't as flat. I do have the correct inductors, but I haven't replaced them yet. 50 MHz frequency sweep with 25 MHz offset. Other solution for the DC offset issue is modulating the frequency sweep so that sweep doesn't include zero frequency, essentially using non-zero IF. Above is time domain plot of received 50 MHz sweep with 25 MHz offset. Frequency sweeps from 0 Hz to 50 MHz compared to -50 MHz to +50 MHz before. Compressed sweep with offset. Calculating the pulse compression of the offset sweep gives much cleaner result. The DC offset issue caused by the high-pass filter is completely removed. Sidelobe level is still 2 dB higher than ideal but this is already quite nice looking impulse response. Ideal sidelobes are higher than with 100 MHz sweep because time-bandwidth product is lower and mainlobe is also widened because of the lower bandwidth. The disadvantage of the modulated sweep is that maximum usable bandwidth of the sweep is half of what can be used with a zero centered sweep. Transmitted signal with Tukey window with α=0.1. Sidelobes caused by low time-bandwidth product of the pulse can be reduced with transmitted pulse windowing. Window function reduces the effective bandwidth of the transmitted waveform, so it increases the mainlobe width and reduces the range resolution. Transmitter side windowing also decreases the average energy per pulse as the waveform is tapered off at the start and end of the pulse which decreases signal-to-noise ratio. One good windowing function for transmitter side is Tukey window. It just slightly tapers beginning and end of the waveform with middle being at the maximum amplitude. Tukey window has a parameter α that can be used to control how much it windows, with α=0 being equal to no windowing. With α=0.1 the pulse energy, and the receiver SNR, is decreased by 0.8 dB. Pulse compressed 50 MHz bandwidth pulse with 25 MHz offset frequency, α=0.1 TX Tukey window, and -50 dB RX Taylor window. Compared to the same pulse without TX window adding Tukey window to the transmitter greatly decreases the far-away sidelobes. At 500 ns offset the sidelobe level has decreased by about 20 dB. The measured sidelobe level is slightly higher than what it should be ideally. Receiver and transmitter IQ imbalance isn't yet calibrated and there is some frequency dependent distortion from the ADC and DAC filters. However, the current level is good enough for now. TX noise leakage If PA is not disabled during the reception noise from PA output leaks into receiver increasing the receiver noise floor. When switching to reception T/R switch is switched from transmitter to receiver. If PA is kept enabled due to its high gain it has high enough output noise that even when with attenuation from the switch isolation it's still larger than the thermal noise floor of the LNA. T/R switch can be switched in about 50 nanoseconds but enabling and disabling PA is much slower, it takes about 10 µs. Unfortunately this long PA switching time means that when using a single antenna the receiver noise is higher due to leaked PA noise. If the input to PA is thermal noise of 50 ohm resistor (-174 dBm/Hz) it's amplified by PAs gain of 32 dB and it adds its own noise to it too. Usually amplifiers noise figure would be reported in the datasheet but this PA doesn't have it listed. PA noise figure can be rather high, 5 - 10 dB wouldn't be too unusual, as they usually aren't optimized to be particularly low noise. With these figures the noise floor at the PA output is about -135 dBm/Hz. T/R switch has limited isolation, exact value for leakage between these ports isn't reported in the datasheet, but 26 dB is the reported typical isolation to the antenna port and isolation between the input ports is usually little better. This means that PA noise at the LNA input is about -165 dBm/Hz which is larger than the thermal noise floor of -174 dBm/Hz and the PA noise limits the receiver performance if it's not switched off. Noise figure of the receiver is about 5 dB, so the measured noise floor with receiver connected to T/R switch should be about 5 dB higher, instead of calculated 10 dB with noiseless receiver. Actually measuring the ADC noise floor with PA on, when the receiver is connected to T/R switch the noise floor is 2.1 dB higher than when it's connected to the other port. It matches well with the theory considering the big uncertainties in all of the values. When using two antennas the second receiver switch can be switched to RX2 and the LNA on the RX1 can be switched off which improves the isolation sufficiently that PA leakage doesn't affect the receiver noise. Target detection Detecting target from range-Doppler map with CFAR. Range-Doppler map (left), CFAR output (middle), sidelobes filtered out (right). Range on x-axis and Doppler velocity on y-axis. To get from ADC samples to target detections some more software is required. In general the transmitted signal is a burst of pulses and the first step is to pulse compress each received pulse. After pulse compression the next step is to take FFT over the number of pulses dimension. This sums the power from the different pulses according to velocity of the target. This is called range-Doppler processing and its output is a 2D image with range on one axis and Doppler velocity on the other. Amplitude of each pixel corresponds to the amount of power received at that distance and velocity. After range-Doppler processing the output is a 2D array of the received power for each range-Doppler bin. To get to target detections we need to identify the bins where there is a target. We also want to separate interesting targets such as moving vehicles from non-interesting targets (clutter) such as sidelobes, trees, ground, and other stationary targets. The targets in the range-Doppler map could be identified by the amplitude. If a bin's amplitude is high enough above the noise floor then it likely corresponds to a real target and is not just noise. The detection threshold, how much a target needs to be above the noise floor, needs to be chosen to balance false alarm rate and missed detections. In general the noise floor power isn't constant in the range-Doppler map. It can vary as function of time, there can be sidelobes from other nearby targets and clutter, for example ground reflections, can also be considered noise since we don't want to detect each patch of ground as a target. Instead of setting a fixed noise floor it's estimated by averaging nearby bins. For each pixel in the radar map, noise floor is calculated by averaging nearby bins and if amplitude of the bin being tested is larger than threshold times the calculated noise floor then we mark detected target at that location. This is called CFAR (Constant False Alarm Rate) algorithm. For high amplitude targets there are going to be false detections from sidelobes. After the targets are detected we check if they correspond to a sidelobe of a larger target and unmark it. This is simply done by checking if there is a much larger target in same row or column. Target is also required to have larger amplitude than adjacent bins, this causes only the peak location of each compressed pulse to be detected. We now have a list of ranges and velocities for detected targets at the accuracy of the radar resolution. Range and velocity measurement accuracy can be improved by interpolating the peak location. Target tracking Kalman filter for radar target tracking. Kalman filter predicts the next position of the target from the previous measurements including the uncertainty. After the detection pipeline we have a list of detections, some of which can be false detections. To be able to track objects in time, detections need to be associated with targets. Kalman filter is used to track each target's position and velocity including uncertainty, and it provides a way to assign each detection to specific target by considering probabilities that detection is from that target. Tracking uses Stonesoup Python library, which is a library for general object tracking. Specifically radar tracking is heavily based on the StoneSoup tutorial. StoneSoup tutorial explains the tracking well, so I won't repeat it too much here. The biggest change from the example is that example is for tracking object in 2D with measurement providing it's 2D position but not velocity. Radar measures distance and velocity of each target, but there is no angle information so only 1D tracking is possible. Transition model for the target is set as constant acceleration. Kalman filter estimates acceleration from the measurements and the next prediction for the target position is made assuming that acceleration is constant. Radar measurements Horn antennas. The rust can't be good for efficiency. I tested the radar by setting the radar on a side of a road and measuring traffic passing by. Pulse length is 2 µs, the bandwidth is 150 MHz, the number of pulses is 1024, RX length is 5 µs with 7 µs delay before the next pulse. I used two antennas with separate TX and RX antenna. Antennas are horn antennas that I made myself. 150 MHz bandwidth corresponds to 1 meter distance resolution. It's important to note that resolution is not the same as accuracy. Resolution is how close two point targets can be to be separated in the measurement. One target can be measured with better accuracy than resolution with accuracy depending on signal-to-noise ratio. 12 µs time between pulses equals 83 kHz pulse repetition frequency. The pulse interval determines the maximum unambiguous target velocity. Velocity measurement is based on measuring phase change between pulses, and if target moves at high enough speed that it moves several wavelengths between pulses, there is no way for radar to know what that multiple is, causing the measured velocity to be ambiguous. If the target moves half a wavelength between pulses, it causes a full wavelength distance change since the radar pulse goes from the radar to the target and back. At this speed, the phase increases by a full wavelength at each measurement, which looks identical to if the target was stationary. If we don't have information on which direction the target is moving, we also need to consider that a signal increasing 90 degrees in phase every measurement looks identical to a signal that decreases by 270 degrees every measurement. The unambiguous velocity measurement range must be divided by two for negative and positive velocities, resulting in velocity measurement range: vmax= λ 4td ,where λ is RF wavelength and td is pulse repetition interval. With 5.8 GHz RF frequency and 12 µs pulse repetition interval, the unambiguous velocity measurement range is from -1077 m/s to +1077 m/s. This is over three times the speed of sound, and there won't be any issues with velocity ambiguities when measuring cars. The Doppler velocity resolution is the unambiguous velocity measurement range divided by the number of pulses, which is 2155 m/s / 1024 = 2.1 m/s in this case. This is the minimum velocity difference that two targets at the same range need to have to be detected as two separate targets. As with the distance accuracy, the velocity measurement accuracy for a single target is better than velocity resolution and improves with signal-to-noise ratio. Above is cellphone video synced with a radar range-Doppler map. CFAR detections are plotted as red plus symbols on the range-Doppler map and listed in the order of decreasing SNR on the top right. On the radar image, Y-axis is the Doppler velocity in m/s with negative values towards the radar, X-axis is the distance in meters. The large line at the zero Doppler velocity is reflections from stationary targets. On the list in the upper right, \"frame\" is the number of the sweep burst in the radar measurement file, \"t\" is the time from the first frame, and \"detections\" is the number of CFAR detections. Detections with a velocity less than 0.1 m/s are filtered out to avoid marking every stationary object as a detection. Comparing the camera footage to the radar measurements it's easy to correlate the radar detections to cars in the camera footage for close targets. There is some shadowing as cars on the foreground block the view of farther away objects, but the radar is able to detect objects not well visible in the camera footage quite well. The radar can detect cars up to about 400 m, limited by the line of sight. Beyond that the road turns and the view is blocked. The effect of the DC offset is also visible as very large sidelobes in the range direction. These sidelobes decrease the ability to detect smaller objects near larger ones. Especially towards the end the farthest away car is not always detected by the CFAR as its amplitude isn't sufficiently larger than the sidelobes overlapping it. Above is the same measurement, but now with Kalman tracker. The tracker assigns CFAR detections to targets with unique IDs. It's able to track multiple targets, but shadowing and sidelobes cause it to not get enough detections from further away blocked targets, and it loses track of them. The uncertainty in their position increases so much that the track deletion threshold is reached. When they become visible again, a new ID is assigned for them. The tracking software could be improved to reduce this problem, but this is just a testing of the radar and I don't want to spend too much time tuning it for this application. Received signal. 1024 overlapping pulses. Amplitude is normalized to full-scale. Above is the received signal of all 1024 pulses from one measurement plotted on the same graph. They overlap very well. There is a small change in the phase during the measurement for moving objects, which is enough to separate the moving objects from stationary ones. There is a large return from leakage and nearby objects at the start, and the received signal from longer time delays that correspond to farther away targets are much weaker. Low-IF pulse While using a large bandwidth sweep centered at DC works, sidelobes caused by the high-pass filter are visible in the results. For second test, I set the RF bandwidth to 75 MHz with 38 MHz modulation frequency so that the frequency sweeps from 0.5 MHz to 75.5 MHz. Other parameters were kept the same. This time, as expected, the very wide sidelobes caused by the DC offset aren't visible. Range resolution is only half of what is was previously, but it doesn't really cause any issues with tracking of the cars. They are large enough that even with a 2 meter range resolution, there isn't any issues with separating them. The frame rate is about only half of what is was before. The amount of data should be the same, and I'm not really sure why it's so much slower this time? At the beginning, a second reflection of the passing car is visible at double the distance and velocity. The radar signal reflects from car, to a sign that is right next to me, back to car, and then is received by the radar. It's much weaker in amplitude and its spread out which causes it to not be detected as a target by CFAR. In this measurement, there's a cyclist coming towards the radar which is not detected as a target. The reason for it is that the cyclist's speed isn't large enough to separate it well enough from the stationary targets. When CFAR target detection is calculated, all of the nearby stationary targets are included in the noise floor calculation for low-speed targets. The large noise floor causes that the small radar cross section of the cyclist isn't sufficiently above the noise floor to be detected. For this application, a higher RF frequency would be beneficial. Doubling the RF frequency would double the Doppler velocity bin separation and decrease the maximum unambiguous Doppler velocity by two. Common police radar speed guns operate at around 10 to 35 GHz, although nowadays lidar, which operates near visible light, is starting to be more common for traffic monitoring. SNR of detected objects as calculated by CFAR vs distance. SNR of the radar detections is quite good at this range. The maximum SNR at 450 m distance is around 35 dB, while just farther away at 550 m there are no detections. This is because of line of sight, there is no clear path beyond 450 m. Radar SNR should decrease as fourth power of distance, which corresponds to 12 dB drop when the distance is doubled. The radar should be able to detect traffic at even longer distances if there is a clear line of sight. From this measurement, it isn't clear if the radar link budget is as good as designed, since the radar cross-section of the targets isn't known. Even a typical car cross section can vary a lot depending on the model and the look angle. The radar link budget could be verified by measuring a target with a known radar cross section, typically a corner reflector. However, I don't have a corner reflector. It wouldn't be too difficult to make one with few triangular pieces of PCB, and it just would require some effort. Full range-doppler map. The unzoomed range-Doppler map shows how small the view on the videos is on the Doppler axis. The maximum unambiguous velocity is over 1000 m/s. On the range direction, negative distances correspond to pulses that arrive before the start of the transmitted pulse. There shouldn't be any signal there except for sidelobes from targets at positive distances. The noise floor drops at the edges of the range direction because of zero padding in pulse compression. Single antenna The previous measurements were made with two antennas, one transmitting and the other receiving. In the next measurement, I have only one antenna that is switched between transmit and receive modes. The pulse was set to the same parameters as the low-IF measurement, except for pulse length, which was decreased from 2 µs to 1 µs to improve detection of close objects. With one antenna, receiver can only be switched on at the earliest just after the end of the transmission. 1 µs at the speed of light is equal to 300 meters, but radar signal needs to travel to the target and back, so minimum distance to receive the full pulse is 150 meters. However, while it isn't possible to receive the full pulse from shorter distances, it's possible to receive a partial pulse. Pulse compression with only a partial pulse reduces distance resolution and SNR, but it should allow detecting targets at much lower distances. Targets below 150 m distance can be detected but the range resolution worsen quickly to unusable levels. Minimum range that radar can detect targets with these settings is about 40 m. In this plot marker is drawn on the tracker predicted location instead of CFAR detections as before. The tracker parameters were tuned a little bit for this measurement and the tracking performance is better than in the earlier measurements. Conclusion Schematic of the radar. Schematic of the radar is available. It should be useful for also as a software-defined radio with some modifications or reference for other applications that require FPGA. Firmware and software isn't available at the moment, since I'm not sure if I should make those public. Cost was 330 USD for PCB manufacturing and assembly of two PCBs and additional 225 EUR (240 USD) for components from Digikey that I soldered myself. This is including 24% VAT and shipping costs. There aren't any similar commercial pulse compression radars in the same price range and even software defined radios with similar RF bandwidth are much more expensive. The designed radar is fundamentally similar to modern large radars. It utilizes digital signal processing, supports arbitrary waveforms and has very large maximum unambiguous target Doppler velocity due to high pulse repetition frequency. Only the maximum range is shorter than large radars due to low output power and small antenna.",
    "commentLink": "https://news.ycombinator.com/item?id=39930195",
    "commentBody": "Homemade 6 GHz pulse compression radar (hforsten.com)342 points by henrikf 18 hours agohidepastfavorite87 comments jcims 15 hours agoI remember this article from the same site a while back - https://hforsten.com/heartbeat-detection-with-radar.html I bought some cheap 10ghz and 24ghz dopper radar units off of amazon and started tinkering with them. You can absolutely pick up heartbeats and breathing just visually in the spectrogram. Here's a few samples from that era: 10ghz pointed at ceiling fan: https://www.youtube.com/watch?v=tIiFvByf1CQ 10ghz pointed straight up underneath a quarter that I flipped and allowed to land on the surface https://www.youtube.com/watch?v=8riretP8ylE 10ghz pointed at a quarter spin on the surface https://www.youtube.com/watch?v=5lnYvJoxRak The comb filtering of the signal from the spinning surface is really cool. 10ghz module on amazon - https://www.amazon.com/HiLetgo-Microwave-Detector-Wireless-1... reply spxneo 13 hours agoparentinteresting does/can it work behind structures? is it safe to point this at yourself? reply KeplerBoy 11 hours agorootparentyes, it's safe. It's not that different from regular WiFi or cellular signals. It's non ionizing (aka it doesn't have enough energy to instantly destroy cells unlike uv radiation) but it can heat up tissue, which is linked to cancer and worse (think microwave ovens). reply a_random_canuck 11 hours agorootparentThere’s no risk of cancer from heating up tissue. reply KeplerBoy 10 hours agorootparentI did some reading and it seems you are right. No Idea why phones come with a SAR rating then and why there are safe limits defined, when no one found unsafe limits. reply Mo3 2 hours agorootparentBecause you tend to hold phones right to your brain. reply SheldonSteves 11 hours agorootparentprevThere is but it's sun burn type risks, not nuclear reactor type. A few unlucky people have been literally cooked to death by military radar. It's as awful as it sounds. reply Dylan16807 9 hours agorootparent> It's as awful as it sounds. Well to me it sounds like you'd get heat stroke and pass out before anything reaches particularly painful temperatures, because it's heating you pretty evenly and not via contact, but maybe that's not the right way to think about it? At higher frequencies I would guess it gets closer to normal burning? reply azalemeth 3 hours agorootparentThis isn't really correct I'm afraid. The cause of SAR is predominantly dielectric losses and the loss tangent is a strong function of tissue type -- CSF, fat and bone are really quite different in terms of epsilon r and sigma, and one has to solve Maxwell with a human voxel model to work out SAR effectively which computationally is a pain. Once tissue heating has occurred what happens next is well described by the bioheat equation, which is basically the thermal diffusion equation with a massive percussion term. The blood supply is very different to different tissue types and the depth at which peak heating occurs is a very strong function of wavelength. For frequencies, the combination of these effects means that your eyes are most at risk -- water like and terrible blood supply. This gives rise to the first piece of advice I was given when a graduate student playing with electron paramagnetic resonance -- never look down a waveguide and treat them like a loaded gun! reply worewood 8 hours agorootparentprev> because it's heating you pretty evenly and not via contact My microwave oven disagrees reply ctrw 15 minutes agorootparentWithout knowing anything about tissue interactions the reason why microwaves heat so unevenly is the sanding pattern that waves make. reply throwup238 17 hours agoprevThis is great. It’s the last piece I needed for my suburban missile guidance system! This will be the last year the Joneses survive the annual block party. Can you do phased array radars next? I need the extra precision. There’s a few neighbors who don’t clean up after their dogs… reply gertrunde 17 hours agoparentThere is this: https://www.crowdsupply.com/krakenrf/krakensdr (Although they had to take the radar elements out of the firmware/software, most likely due to ITAR - ref link: https://forum.krakenrf.com/t/where-has-the-passive-radar-cod... ) reply topynate 15 hours agorootparentA similar ITAR restriction on controlled reception pattern antennas means that GPS jamming is still much more of a problem than it needs to be. Three antenna elements are all you get, according to this: https://www.gpsworld.com/toughen-gps-to-resist-jamming-and-s... reply CamperBob2 13 hours agorootparentInteresting article. I wonder if any progress on the ITAR issue has been made since 2022? If Brad Parkinson can't steer ITAR in the direction of common sense, nobody can. (For those who don't know, he was the principal architect of the original Navstar GPS system.) reply wbl 9 hours agorootparentI would prefer we not have homemade cruise missiles all over the place. The people who need them can make them after all. reply bigiain 8 hours agorootparentApparently the hobby drone racers are doing that now: https://www.abc.net.au/news/science/2024-04-05/how-a-small-c... And it's certainly not just a recent Ukraine related thing either: https://www.nzherald.co.nz/nz/boffin-builds-backyard-missile... reply numpad0 4 hours agorootparentprevSure, it's getting increasingly easier to just buy China made for ITAR-protected products. Night vision cameras, thermals, drones. They're fine, way more user friendly, often cheaper too. reply rfrec0n 8 hours agorootparentprevTo be fair, nobody actually neeeds cruise missles reply Animats 6 hours agorootparentprevWow. I haven't looked at the ITAR list in decades. There are lots of electronics items on there now that have useful civilian applications and can be built at low cost. You can build such things, but can't ship them cross-border to or from an ITAR country. I'm amazed at what people are building as hobbyist RF gear. I wonder what test equipment they have. The test equipment for GHz RF is very expensive. If you build it and it's not working, you may need test gear. (I tried to build a LIDAR in 1990s. It didn't work, and I didn't have access to gear that would let me see what was happening up there.) reply throwup238 17 hours agorootparentprevThank you for the link! I totally forgot about kraken. I cloned their repo in November 2022, probably in anticipation of the ITAR hiccup. Hopefully the radar code is there. reply CobaltFire 16 hours agorootparentI’d be interested in knowing if you’ve got that. I was going to buy one but the pull of the code put that on ice. I may pull the trigger if I can get the code! reply throwup238 15 hours agorootparentNo sadly it looks like I got the wrong repo or I cloned it too early. It includes a little Python code but no firmware from what I can tell. reply rangestransform 7 hours agorootparentprevif they really wanted to, they could make this a first amendment case, no? reply spitfire 16 hours agoparentprevThese should help. Tactical and Strategic Missile Guidance, Seventh Edition https://www.amazon.com/Tactical-Strategic-Missile-Guidance-S... Tactical missile warheads https://www.amazon.com/Tactical-Warheads-Progress-Astronauti... reply Symmetry 16 hours agorootparentI'm sure the OP has already read his Skolnik given his comment but for those following along that's usually the best place to start. https://www.amazon.com/Radar-Handbook-Third-Merrill-Skolnik/... reply spitfire 15 hours agorootparentThere's also Fundamentals of Astrodynamics https://www.amazon.com/Fundamentals-Astrodynamics-Second-Dov... Which builds up from first principles a ballistic missile defence system. Always useful to have. reply secondcoming 16 hours agorootparentprevHa, I mentioned the first book the other day on here. reply femto 9 hours agoparentprevYou forgot the actual missile bit, to go with your guidance system. Don't worry though, Bruce Simpson has that covered for you: \"A DIY Cruise Missile: Watch me build one for under $5,000\" http://www.interestingprojects.com/cruisemissile/ reply mNovak 4 hours agoparentprevWell if we follow the historic development arc, frequency-scanning waveguide arrays would come next. Nicely, very minimal modification required, and you can start cranking up the antenna gain. Mostly just a physical fabrication challenge, which is hobby-approachable. https://www.radartutorial.eu/06.antennas/Phased%20Array%20An... reply Modified3019 16 hours agoparentprev>phased array Jon Kraft apparently is doing a series on that: https://www.youtube.com/@jonkraft reply jcims 16 hours agoparentprevIf someone can figure out how to hack the starlink dish frontend we'll have a hell of a capability to tinker with. reply rlt 13 hours agoparentprevFBI has entered the chat reply l33tman 1 hour agoprevRegarding this \"The suspiciously cheap $15 FPGA had equally suspiciously date and lot codes covered (white rectangles on the FPGA chip in the picture above). I have a development board of the same series chip with markings intact, so it definitely shouldn't look like this. It did end up working, but I wonder what the origin of these chips is.\" I spoke with a GoPro operations guy once, and asked how you can get the chinese ripoffs at $40, that looked the same as their $400 \"real\" deal. He said that they literally get chips from scrapyards, like they get batches of memory chips that didn't pass the tests etc. They might work only in a very limited temperature range for example. Also many other components are picked and speced to work 6 months on avg not 6 years.. They know that most of those \"gadget\" products are bought and used for a weekend and then put in a cupboard anyway. It would be fine for an R&D project like this but I would be very careful if I was going to make a commercial project (then you also have the politics side, probably fuelled by some paranoia - there are legislations coming up which prevents you from using chinese silicon components in products for certain markets) reply auspiv 16 hours agoprevIncredible work. Crazy that this could be done by a single person. Stick it on a rotating pedestal and you've got a planar radar detector. Add some tilt and then it's not much different than aircraft/weather radar I suppose. The cost (in $LOCAL_CURRENCY, or $570 according to another comment) isn't great but I can only imagine how many hours this took. Given a proper budget, the sky is the limit. Anyone know how .mil aircraft would interpret being tracked by a 6 GHz radar build by a civilian(yes I am aware that his estimated max distance is 1200m, assume he increased that by a factor of 10 with larger antennas or something)? reply aidenn0 14 hours agoparent> Anyone know how .mil aircraft would interpret being tracked by a 6 GHz radar build by a civilian(yes I am aware that his estimated max distance is 1200m, assume he increased that by a factor of 10 with larger antennas or something)? Regulations for signal strength are ERP, so a more directional antenna could make it no longer legal to use the 6GHz band. reply bigiain 8 hours agorootparentI doubt anyone intentionally tracking military aircraft is gonna be too worried by ISM radio band regulations. (They're either the kind of idiot who aims laser pointers at police helicopters, or they're doing it with intent to get in much more serious trouble than just ignoring radio regulations...) reply kijiki 10 hours agoparentprev>Anyone know how .mil aircraft would interpret being tracked by a 6 GHz radar build by a civilian(yes I am aware that his estimated max distance is 1200m, assume he increased that by a factor of 10 with larger antennas or something)? No specific knowledge, of course, but I'd imagine it wouldn't trigger a serious threat warning. Military TWRs are highly tuned systems dedicated to the threat environment they're expected to operate in. reply AnarchismIsCool 4 hours agorootparentDepends on the system, any pulsed emission of sufficient strength should start triggering things as you can't know for sure what bands and systems are going to be used against you. I believe they usually have 'unknown' IDs for stuff that isn't in the threat database. reply willvarfar 3 hours agorootparentThere is a story of a police speed camera catching a fast jet going around. The snopes debunking has some interesting technical details of why this is really unlikely https://www.snopes.com/fact-check/police-radar-missile/ But of course people have built nuclear reactors at home, so they could probably assemble a sufficiently powerful radar too. Edit: didn't mean that people need to power homemade radars using homemade nuclear reactors! I'm sure that's not necessary :) reply instagib 7 hours agoparentprevThey get something similar to “radar warning” / “radar lock” notifications or verbal warnings. reply throwway120385 15 hours agoparentprevWouldn't the FAA or FCC in the US have some jurisdiction over such a setup? reply maestroia 15 hours agorootparentThe \"6 GHz\" band is an unlicensed frequency for very-low power (VLP) devices in the US. Specific bands are: \"U–NII–5 (5.925–6.425 MHz) and U–NII–7 (6.525–6.875 MHz)\". VLP is defined as those devices which \"operate at up to −5 dBm/MHz power spectral density (PSD) and 14 dBm EIRP\". https://www.federalregister.gov/documents/2024/01/08/2023-28.... reply mNovak 4 hours agorootparent14dBm EIRP is extremely low power for a radar however. Off the bat the author is using 1W (30dBm) PA, and you could readily get another 20dB from a moderate sized antenna (e.g. repurpose a scrap DirecTV dish). Unless you're next to a base, it's the FCC that will come knocking long before the military. reply gambiting 14 hours agorootparentprevIn fact you can get WiFi routers that use 6Ghz as the third band after 2.4Ghz and 5Ghz. reply amanda99 15 hours agoprevThe Finns strike again. Just the depth and number of different areas of expertise is insane to me. It seems he planned it all out, had it printed in China, and then 1/2 of the boards actually worked. That's like building a whole backend+frontend app and hoping it works after a month of coding on startup. reply Brusco_RF 14 hours agoparentThat's pretty typical for hardware development. You software people have it too good! reply belzebalex 13 hours agoprevThis is incredible. I've wanted to thank the author for a while for his past articles [1][2] which have been a wonderful source of information when working on my sonar systems [3]. This article again explains really well a complicated topic. Please, keep up the great work! [1]: https://hforsten.com/radar-phase-measurements.html [2]: https://hforsten.com/6-ghz-frequency-modulated-radar.html [3]: https://twitter.com/alextoussss/status/1756371553460121766 reply nick__m 16 hours agoprevWhy the ground planes are on layer 2 and 6 instead of 1 and 6 ? Naively, as someone who doesn't have high frequency PCB design experience, I would have placed my grounds to form a shield and put my voltage plane on layer 3 or 4. I am sure that there is a good reason behind that choice but I don't see it. reply henrikf 16 hours agoparentPutting ground planes on top and bottom layers isn't usually done with high speed PCBs because components are there. There would need to be a cutout on the ground plane near every chip. High speed signals really need continuous ground plane and ICs, especially RF ICs, need short access to ground. Second layer is the best layer to minimize the distance from ICs to ground plane. reply bangaladore 13 hours agoparentprevIt is common to route high-speed signals on the top and bottom layers to avoid vias that cause impedance mismatches even when back drilled ($$$). To maintain a specific characteristic impedance, you need a plane (GND) some distance from the traces which themselves have a specific width. Without a plane under/above the signals, you can't get a specific impedance value. You can additionally fill the top and bottom layers, which marginally affects the impedance. reply _Microft 16 hours agoparentprev> Above is the final DDR3 routing on all the PCB layers. Layers 2 and 6 are ground, 5 is supply voltage, and others are reserved for signals. Two grounds are needed for correct impedances on the top, middle, and bottom traces of the PCB. With only one ground plane, the distance from the signal to ground would be too large on either the top or bottom layer. Is the textual description in the article correct? To me the images make it look like signals are on 1 (red), 3 (orange) and 6 (blue), with ground on 2 (green) and 5 (pink) and supply voltage on 4 (teal). If you match some vias, you will find that 2 and 5 are definitely connected. reply henrikf 16 hours agorootparentThat's a mistake in the text. You're correct that layers 2 and 5 are ground planes. reply nick__m 16 hours agorootparentprevthanks, layer 2 and 5 would fit the rest of the description and the reason would then be in the text : Two grounds are needed for correct impedances on the top, middle, and bottom traces of the PCB reply _Microft 16 hours agorootparentHere are some resources on PCB layer stack-up by the way. While I'm an amateur, they didn't sound unreasonable. Chapter 10[0] has a list of links to different layer stack-ups for boards with 4 to 10 layers. [0] https://web.archive.org/web/20200124214936/http://www.hottco... Edit: removed wall of links reply kurthr 16 hours agoparentprevObviously not the designer, but my guess is to lower capacitive loading on the matched high frequency signals (those with squiggles on top and flood underneath) and make modeling and IC mounting easier without flood. Note there's not flood near those either. I'm not quite sure on the ordering in the picture but it looks like 123/654? (edit ahh looks like the text was wrong and it is 123/456 with 2&4 Gnd). I'll note it looks like the internal \"low speed\" digital signals are squeezed between the gnd/pwr (edit: between gnd/gnd) planes, which is probably good since they're usually the biggest source of \"noise\" if you keep it away from other PCBs. You'd definitely want power/gnd planes immediately next to each other since bypass caps don't work at anything close to this frequency. reply ein0p 14 hours agoprevBadass work. Rarely do you see such technical depth being demonstrated across the entire stack from RF to hardware to firmware to software. The article just gets better and better as you read on. reply mallets 10 hours agoprevAFE7225 is my goto for applications that require both DAC/ADC. ~$40 in single qty. Just limited to half the sample rate achieved here. I personally wouldn't have bothered with Zynq here, so much easier to interface any FPGA with any COTS SBC over Ethernet. Or just use the $160 ZUBoard 1CG, perfect for quick prototyping. reply pythonguython 15 hours agoprevCan someone explain why those differential pairs are routed with so many curves instead of straight paths? (E.g. see the photo under the “ADC and DAC rotting” section) reply _Microft 14 hours agoparent\"The traces are length-matched with squiggly lines[...]. The trace matching requirement is ±10 ps according to the Zynq PCB design guide, which is approximately ±2mm in trace length. [...] There is also some delay difference inside the FPGA package which should be considered in the length matching.\" This is a shortened excerpt from the article. It can be found below the image with six colorful images of PCB layers. I'm curious how the delays inside the FPGA package are known. Is there a table which pin adds how much delay to a signal or something like that? reply henrikf 14 hours agorootparentThe FPGA manufacturer has characterized all their package pin delays. It's possible to export a csv file with internal delays of the package for each pin from the FPGA design tool. With Xilinx Vivado it's just File -> Export I/O Ports. reply _Microft 14 hours agorootparentDoes KiCad allow to take these delays for each pin into account automatically or do you need to do all that manually? Edit: it's using the pad property of \"pad-to-die-length\" if doing it manually, right? reply henrikf 14 hours agorootparentNo, it needs to be done manually. It wasn't as tedious as it sounds though. Most of the pins are very close in delay already and there were just few traces that I had to adjust a little. reply mhh__ 12 hours agorootparentHow is kicad getting along with RF work in 2024, would you say? I did some prototyping but never got the stage if actually fumbling around with PCBs a few years ago, things seemed to progressing quite well. reply Brusco_RF 14 hours agorootparentprevOne of the main benefits of using an FPGA is that you can compensate for trace length mismatch with timing constraints reply rcxdude 13 hours agorootparentNot usually with IO ports directly associated with hard IP blocks like the DDR controller in the Zynq though. reply cwillu 15 hours agoparentprevMatching overall length with other traces, is my guess. reply KeplerBoy 11 hours agoprevHenrik Forsten is definitely the coolest radar guy on the internet I'm aware of. Hats off to you! reply vlovich123 17 hours agoprevCould this be adapted into a voxel system? Seems like it could be cheaper than LIDAR which are a huge cost for why the HW for self-driving systems are so expensive & work in far more environments that LIDARs struggle with. I suspect getting multiple directions simultaneously is hard? reply itishappy 17 hours agoparentNot without significantly complicating your antenna setup (and the data processing setup too). You guessed it, getting multiple directions simultaneously is hard. Note how the current system only detects distance and speed in 1 dimension. Here's an analysis from someone smarter than me: > To enable the new features, radar systems now use multiple input/multiple output (MIMO) antenna arrays for high-resolution mapping. Traditional radar systems usually contain two to three transmitting antennas and three to four receiving antennas, which lead to a beam providing limited short-range coverage and a narrow field of view unable to generate images. The limited angular resolution is insufficient to differentiate among vehicles, pedestrians, or objects that are close. The MIMO approach increases the underlying channels from only nine to anywhere between 128 and 2,000. Given radar’s significantly lower costs — even with all the enhanced technology — it’s easy to see how the two technologies will increasingly be on more equal footing. https://www.oliverwyman.com/our-expertise/insights/2023/jul/... reply user_7832 16 hours agorootparentSo would this mean that with a few more transmitting and receiving antennas it could have comparable resolution to lidar? reply itishappy 16 hours agorootparentIn theory, though it sounds like to compete with LIDAR it will need about 1000x more antennas, with a related increase in electronics. reply beeeeerp 14 hours agorootparentCutting-edge AESA radar like on the F-35 is incredible. It actually looks like a black and white photograph. I think your guess on antennas is roughly correct based on what we know about modern AESA. reply KeplerBoy 11 hours agorootparentIsn't that Synthetic Aperture Radar though? You can get similar results (black and white aerial pictures) by strapping a pretty basic siso radar system on a drone. reply beeeeerp 7 hours agorootparentI’m honestly not sure what the difference is. I kind of understand SISO, MIMO, and beam steering concepts, but just the basics from nerding out on my Starlink dish. This is what I’m talking about: https://www.twz.com/f-35-will-get-new-radar-under-massive-up... Specifically this image: https://www.twz.com/uploads/2023/01/03/20065813381381024.jpg I don’t know how targeting works, but with this level of resolution, I think basic image algorithms can start to come into play. It blows my mind. reply KeplerBoy 3 hours agorootparentYes, that looks like regular SAR imagery. Checkout @umbraspace if you want to see some really nice SAR images captured by satellites launched along starlink satellites. https://twitter.com/umbraspace reply pbmonster 12 hours agoparentprevLook at the forward looking automotive 4D (distance, speed, azimuth angle, elevation angle) radar systems. The new ones work at around 80GHz, and the entire thing comes in one integrated, tiny package, 16x16 phased array antenna already included with the MCs and FPGAs on the same board. To go from those 4D radar maps to a voxel system requires a whole lot of software, of course. The end goal seems to be to beat LIDAR on price and reliability (turns out moving mirrors don't like years of constant vibrations), while delivering enough resolution for self-driving. reply CamperBob2 17 hours agoprevBeautiful piece of work. Henrik has a long history of interesting radar and other RF data-acquisition projects of the sort that you don't see publicly documented much, at least not at this level of quality. reply amirhirsch 16 hours agoprevDid you test this with arbitrary waveforms? Is it possible for you to accumulate complementary Golay Codes? reply AlwaysNewb23 16 hours agoprevThis is really impressive. Do you plan to use it for anything or another project? reply henrikf 16 hours agoparentI built it to see if I could. I didn't have any particular use case in mind. reply AnarchismIsCool 14 hours agorootparentWould this be useful for another SAR setup? reply georgeburdell 17 hours agoprev [–] I just want to know how much this cost reply davekeck 17 hours agoparent [–] > Cost was 330 USD for PCB manufacturing and assembly of two PCBs and additional 225 EUR (240 USD) for components from Digikey that I soldered myself. This is including 24% VAT and shipping costs. reply georgeburdell 15 hours agorootparentPlus the test and validation equipment which are $$$ reply henrikf 14 hours agorootparentI actually didn't use any expensive test equipment, only oscilloscope and multimeter. Even design and simulation software was all open source. Expensive signal analyzer or spectrum analyzer would have been useful, but they aren't absolutely necessary. It's possible to use the radar itself for many tests and debugging. I have tried to limit the projects I do on my own to only the equipment that I have home and open source software. reply rkagerer 13 hours agorootparentprev [–] I came to ask about that 24% VAT - ouch! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article contrasts FMCW and pulse radar designs, focusing on the benefits and obstacles of pulse radar in long-range scenarios.",
      "It outlines the development of a contemporary pulse compression radar system, covering key elements such as ADCs, DACs, filters, and FPGA for improved performance.",
      "The radar system, costing $570 USD in total, overcomes challenges like DC offset and noise floor to enable target detection at distances up to 400 meters, providing advanced features uncommon in radars within the same price range."
    ],
    "commentSummary": [
      "The post explores building a DIY 6 GHz radar to detect heartbeats and breathing, utilizing 10 GHz Doppler radar units while addressing safety concerns on tissue heating and exposure limits, including risks with military radar.",
      "Topics covered include PCB design, signal routing, components like AFE7225 and Zynq, and FPGA delays, along with advancements in radar tech like AESA radar and SAR, potential in automotive radar systems, and exceeding LIDAR capabilities.",
      "The original creator constructed the radar as a personal challenge without a specific application, elaborating on manufacturing PCBs for SAR and open-source software usage."
    ],
    "points": 342,
    "commentCount": 87,
    "retryCount": 0,
    "time": 1712237523
  },
  {
    "id": 39928558,
    "title": "CSS Vulnerability in HTML Emails Raises Security Concerns",
    "originLink": "https://lutrasecurity.com/en/articles/kobold-letters/",
    "originBody": "Anyone who has had to deal with HTML emails on a technical level has probably reached the point where they wanted to quit their job or just set fire to all the mail clients due to their inconsistent implementations. But HTML emails are not just a source of frustration, they can also be a serious security risk. Imagine you receive an email forwarded by your manager asking you to wire a large sum of money to a bank account. Of course, you have heard of CEO fraud, so you double-check that the email really comes from your manager. It does, and it may even be cryptographically signed – if you do that in your company. However, you are still not convinced, so you call your manager to ensure that the email is legit. He confirms, so you transfer the money. Yet this would be the end of the article if this wasn’t a scam. So what went wrong? Kobold letters The email your manager received and forwarded to you was something completely innocent, such as a potential customer asking a few questions. All that email was supposed to achieve was being forwarded to you. However, the moment the email appeared in your inbox, it changed. The innocent pretext disappeared and the real phishing email became visible. A phishing email you had to trust because you knew the sender and they even confirmed that they had forwarded it to you. This attack is possible because most email clients allow CSS to be used to style HTML emails. When an email is forwarded, the position of the original email in the DOM usually changes, allowing for CSS rules to be selectively applied only when an email has been forwarded. An attacker can use this to include elements in the email that appear or disappear depending on the context in which the email is viewed. Because they are usually invisible, only appear in certain circumstances, and can be used for all sorts of mischief, I’ll refer to these elements as kobold letters, after the elusive sprites of mythology. This affects all types of email clients and webmailers that support HTML email. So pretty much all of them. For the moment, however, I’ll focus on selected clients to demonstrate the problem, and leave it to others (or future me) to extend the principle to other clients. Thunderbird This issue was reported to Mozilla on 05.03.2024. The planned release date and a draft of the following section were communicated to Mozilla on 20.03.2024. Possible mitigations have been discussed but will not be implemented until a later date. Exploiting this in Thunderbird is fairly straightforward. Thunderbird wraps emails inand leaves them otherwise unchanged, making it a good example to demonstrate the principle. When forwarding an email, the quoted email will be enclosed in another , moving it down one level in the DOM. Taking this into account leads to the following proof of concept: .kobold-letter { display: none; } .moz-text-html>div>.kobold-letter { display: block !important; }This text is always visible. This text will only appear after forwarding. The email contains two paragraphs, one that has no styling and should always be visible, and one that is hidden with display: none;. This is how it looks when the email is displayed in Thunderbird: This email may look harmless... As expected, only the paragraph “This text is always visible.” is shown. However, when we forward the email, the second paragraph becomes suddenly visible. Albeit only to the new recipient – the original recipient who forwarded the email remains unaware. ...until it has been forwarded. Because we know exactly where each element will be in the DOM relative to .moz-text-html, and because we control the CSS, we can easily hide and show any part of the email, changing the content completely. If we style the kobold letter as an overlay, we can not only affect the forwarded email, but also (for example) replace any comments your manager might have had on the original mail, opening up even more opportunities for phishing. Outlook on the web This issue was reported to Microsoft on 05.03.2024. The planned release date and a draft of the following section were communicated to Microsoft on 20.03.2024. The report was marked as closed by Microsoft on 26.03.204 after deciding not to take any immediate action. In Outlook on the web (OWA) the situation is slightly more complicated, as it is a webmailer. Emails are contained in , but the exact class name will change. To prevent the CSS of emails to affect the style of the webmailer, Outlook modifies the email by prefixing all ids and classes with x_ and adjust the CSS accordingly. Considering all this, we get the following proof of concept: .kobold-letter { display: none; } body>div>.kobold-letter { display: block !important; }This text is always visible. This text will only appear after forwarding. When the email is displayed by OWA, the CSS will look like this: div > div > .x_kobold-letter{display:block!important} -->The email will be displayed as expected: This email could ask for a harmless favour... After forwarding, the kobold letter will be enclosed by anotherand the CSS will be updated again: div > .x_x_kobold-letter{display:block!important} -->Note that in the second rule, the > between .rps_78fa and div has been dropped. This must be taken into account when creating more complicated selectors. Our proof of concept works as intended: ...or all your money. The adjustments that OWA applies to the email do not interfere with the attack, but need to be considered when crafting a kobold letter. As we don’t have an easily recognizable anchor point, this can become a nuisance when trying to create a kobold letter that works for multiple clients at the same time. Gmail This issue was reported to Google on 05.03.2024. The planned release date and a draft of the following section were communicated to Google on 20.03.2024. So far, we could define kobold letters as elements that use CSS selectors to be shown or hidden depending on the context in which the email is displayed. By this definition, Gmail is technically not vulnerable to kobold letters because it strips all styling from the email when forwarding it. This allows for an even simpler – albeit more limited – attack: It is now sufficient to hide the kobold letter with CSS, and it will automatically appear when forwarded. However, this does not allow the opposite behaviour, where the kobold letter is visible in the original email and invisible in the forwarded email. .kobold-letter { display: none; }This text is always visible. This text will only appear after forwarding. This is aided by the fact that when the email is forwarded, the CSS is not removed until after it has been sent: Even while commenting on the forwarded message, the illusion is maintained... While the second paragraph was clearly not visible in the editor, it is in the resulting email: ...but in the end it will betray you. While it is difficult to fix kobold letters in general without breaking too much, Google could mitigate the problem by removing the CSS already in the editor. This would at least allow the sender to detect the attack before forwarding the email. Prior work The fact that this is possible in one way or another is neither surprising nor new. Therefore similar issues have been reported in the past: Thunderbird - Modification of replied mail content without knowledge of replier of the mail using CSS Treat OpenPGP/SMIME email messages with certain CSS elements as unsigned Composition should use inline styling so that styles from quotes don’t leak into the message The novelty of kobold letters lies in the focus on a specific attack scenario, while looking at multiple email clients. This hopefully will help raise awareness of the risks associated with HTML emails and contribute to the discussion of what trade-offs are acceptable to mitigate these risks. Outlook (not the software) Users can mitigate kobold letters by disabling HTML email altogether or viewing it in a restricted mode (e.g. “plain HTML” in Thunderbird). For email clients, it is more difficult to implement a mitigation, as preventing the use of 1 would fix this, but would also break a lot of existing solutions in the email ecosystem. An implementation like the one Google already uses for Gmail could be a compromise that allows for stylized corporate newsletters while limiting the risks of HTML email. Unfortunately, for the foreseeable future, it is sadly not realistic to expect email clients to implement robust mitigation. This means that it is up to the users to be aware of the dangers of HTML emails and to take the necessary precautions. But I guess this is fine. https://www.caniemail.com/features/html-style/ ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=39928558",
    "commentBody": "Kobold letters: HTML emails are a risk (lutrasecurity.com)316 points by chillax 23 hours agohidepastfavorite133 comments bluetidepro 21 hours ago> “However, you are still not convinced, so you call your manager to ensure that the email is legit. He confirms, so you transfer the money.” I feel like it’s a HUGE (silly) assumption you’d ask generically “did you send this email” instead of something more specific like “do you REALLY want me to transfer you money like this?” to which the manager would obviously be confused and the attack would likely be killed in that conversation. This is an interesting attack vector but I am questioning how likely it is to succeed. The article paints a very specific and narrow window of events for this attack to really work. I don’t buy it, personally. EDIT: I know phishing happens and works. I am not saying it doesn't. I just mean the people that fall for phishing don't need this sophisticated of an attack to fall for. In fact, the attacker probably narrows the chance of success by putting this much extra (very specific) effort into the attack. They are likely to just succeed with their typical phishing email. reply Macha 21 hours agoparentWorked for a 10,000 person company, 50% engineers. There'd be several cases a year of someone using the company credit card to buy gift cards for \"the CEO\" or other senior execs whose details are available on linkedin or corporate sites, despite that exact case being the example in the anti-phishing training. So you'd be surprised. reply bluetidepro 21 hours agorootparentI don't doubt phishing happens. I just think this specific scenario/technique is one that is probably extremely rare. The attacker likely wouldn't put this much extra effort/thought in when their basic attacks already work, like you're describing. reply fkyoureadthedoc 20 hours agorootparentSecurity at my job pumps their numbers by pretending you fell for a phish if you click the link in their obvious phishing test emails. I clicked one to see how good of a job they did at the other end of the link trying to extract whatever they want from me, but there's nothing there! So lazy. reply kurnikas 20 hours agorootparentI got dinged for clicking \"report as phishing\" as part of that process forwards it to microsoft threat intelligence in outlook and so their systems said I forwarded and therefore fell for the phishing, now I look for a particular header and put all of those messages in a \"phishing\" folder reply imzadi 19 hours agorootparentI run my organization's phish sims, and we had a similar issue one month. A bunch of people failed for downloading attachments. When I looked into it further, all the attachments were downloaded by the same Czech IP address. With some research, I found that it was an AVG IP address. The fix is very simple. The phish sim service has a place to exclude IP ranges. Any activity from those IPs are just ignored. I'm sure all phish sim services and software have this ability. reply sdrinf 13 hours agorootparentQuestion: why is clicking on the (test) phishing email's link \"fail\"? Isn't the whole contract between browsers and society that one can safely open any website they want (ie loading a webpage is safe), and what you do on the actual site is the actually unsafe op? Asking because in the vast majority of cases, the phishing landing page has way more signals to recognize than the email headers. reply linuxalien 9 hours agorootparentUnfortunately not. If there is a 0 day vulnerability, or you're running an older version of a browser for a known patched issue, you may find yourself with a remote code execution, or 0 click download. Or it could be another kind of exploit, maybe your email service is vulnerable to XSS attacks. Like operating systems, browsers can have security issues too. So trusting your browser to see if a phish is really a phish is just unnecessary risk. I've worked with clients that have ended up with crypto lockers from clicking the link. Even from the IT side, I'm not going to increase the risk by opening a known phishing link to check how good it looks. If I am, it's going to be in a system that doesn't have active logins to other systems/sites, and is in easily disposed and reset. Check out all the YouTubers getting channels hacked with session stealing. Yes, they are falling for phishing attacks, but you really don't know what the attack vector is going to be. It might just be a fake login, or it could be much more sophisticated. reply Finnucane 18 hours agorootparentprevNow when I see a phish, I check to see where it is coming from. 97 percent of the time, it is a test. We're getting these tests often enough that I just assume that's what it is. reply imzadi 17 hours agorootparentWhich is fine, actually. If you see it and think \"oh, IT is at it again\" and delete it or report it, mission accomplished, because there is still that 3/100 chance it is real. reply Kinrany 15 hours agorootparentIt only works on fake fishing. reply imzadi 13 hours agorootparentSo when you look at the sender of a suspicious email and it's not the phish sim service you just go ahead and open it? That doesn't sound like a problem with the phish sim. reply Kinrany 11 hours agorootparentIt's certainly a problem with the phish sim if you're trying to teach people not to open random links and instead you're teaching people not to open phish sim emails. It fact, it can be actively harmful if it creates a false sense of security. reply seethishat 16 hours agorootparentprevMany phishing simulation systems are not technically correct. Microsoft, Google and other 'security vendors' may inspect links in emails. That link inspection can sometimes be blamed on the end user. \"You clicked the phishing link, now you have to take remedial security training!\" The only way to know for certain that a user fell for a phish, during a simulated exercise, is to make an HTML form that does a HTTP POST request and contains the user's credentials (that only they could type in). If a user enters their username and password and clicks submit, then they fell for the phish, otherwise no one can say for sure who or what software clicked that link that did a simple HTTP GET. reply w3ll_w3ll_w3ll 12 hours agorootparentMicrosoft Safe Link technology does not actually inspect the link until the user clicks on the link. This is to avoid that confirmation links, used by some service to confirm registratio or as 2FA, may be triggered by the security engine without user consent. reply caddy 1 hour agorootparentOur workplace outlook phishing protection does though. I was signing up to test one of our apps recently and my email was auto confirmed in 5 seconds despite me never receiving it. Turns out it was caught in the phish filter which automatically clicked the link to check it, so the above is not always true. Confirmed this with a few co-workers too. reply jrockway 17 hours agorootparentprevWe must use the same vendor, as I heard about that happening to my coworkers. I clicked \"it's phishing you idiots\" in Outlook and got a gold star. I find it funny because my organization doesn't even use email, so 100% of email I get is spam or phishing. The dead giveaway on this email was that there was a Via: header that was like \"phishingtestsforyourworkplace.com\" or something. reply hk1337 20 hours agorootparentprevI got dinged once for using curl (in a VM) on the link get the details to pass one when I reported it. reply npongratz 20 hours agorootparentI once got dinged for forwarding an obvious gotcha email, without ever opening it, to our security team's phish notification address, as our employee handbook instructed. I learned my lesson. reply testudovictoria 19 hours agorootparentI once got dinged for not reporting. I saw an email that was clearly an internal security campaign. I deleted it. I received an email a day or two later stating that I failed to take action on a phishing attempt. Damned if you do; damned if you don't. reply Macha 18 hours agorootparentFor a while I had a thunderbird filter to automate forwarding based on our provider's email header. They disabled SMTP and the Gmail web client has no such ability to filter on arbitrary email headers. reply ohthatsnotright 17 hours agorootparentYou can setup a Google app automation to do this for you. I did for e.g. knowbe4 since all their test emails have the same header information. It made it quite easy to never see any of their attempts, though I did have to check every once in a while to see if I'd been signed up for any random learning and it removed those emails as well.. reply Macha 17 hours agorootparentiirc, the same company had locked down the allowed oauth apps, so you would have needed an exception from security to run one. I doubt they'd have granted an exception to stop getting annoyed by their own training. reply tripdout 19 hours agorootparentprevYeah the links from Proofpoint are unique to you, so however you visit it you still get tracked reply hk1337 17 hours agorootparentIt was when I was working at HP/HPE/DXC (I don't remember what it was at the time), I don't remember what they used. reply GrinningFool 14 hours agorootparentprevI did that once for the same reason, and found myself sentenced to mandatory security retraining videos with no possibility of appeal. reply MakeThemMoney 20 hours agorootparentprevThank you! - Browser 0-day vendor reply autoexec 17 hours agorootparentYou aren't wrong. I've got a heavily locked down browser on an off-network device for working with questionable websites. While the vast majority of phishing sites aren't pushing malware spearphishing is another story. reply bee_rider 16 hours agorootparentIT still might not want you to follow the link. * Other users might have, instead, an incompetently secured browser that they think is locked down on their work devices. It is hard for IT to distinguish between you and them. * If the URL is personalized, it tells the attacker that the address is active. This is probably pretty limited help to the attacker. But it might tell them if your company emails follow a particular format, right? reply fkyoureadthedoc 15 hours agorootparent> * If the URL is personalized, it tells the attacker that the address is active. This is probably pretty limited help to the attacker. But it might tell them if your company emails follow a particular format, right? I just asked chatgpt and it knows what email format the company I work for follows, so I'm not sure this is of particular value. reply autoexec 15 hours agorootparentIt's useful, even if you aren't a scammer, but it's generally not hard info to get elsewhere. reply fkyoureadthedoc 15 hours agorootparentprevI feel truly sorry for whoever spends a browser 0-day giving RCE on me. reply planede 19 hours agorootparentprevIt's good that I otherwise don't click on links in my browser during my day-to-day work. /s reply themoonisachees 19 hours agorootparentGood thing browser aren't able to display content of random unvetted third parties in exchange for money on any website you visit too :) Adblock is a security measure at this point. reply tomhallett 20 hours agorootparentprevWhile I’m not saying the specific scenario will work 100% of the time, it doesn’t need to - by the email getting forwarded at all, there is some element of trust in “my manager forwarded me this email and typed ‘complete this for me’”. If this css technique increases the attackers odds, then it’s an issue. Or for your specific example, imagine the recipient is passing their manager in the hallway: “hey, can we chat about the Acme Corp email, I’ve got some questions about it”. Response: “sorry, super busy. It’s a fairly common ask, just get it done!” reply raptor99 20 hours agorootparentprevMaybe it's just good to be aware. reply bambax 19 hours agoparentprev> I just mean the people that fall for phishing don't need this sophisticated of an attack to fall for Yes. It's more of the opposite. It's a well documented fact that the most obvious/ridiculous scams work the best, because they help select the most gullible potential victims. https://www.microsoft.com/en-us/research/publication/why-do-... reply comicjk 17 hours agorootparentThat analysis is from the perspective of the scammer. The scammer has limited time to write to each victim once the responses come back from the initial mass-email, so the scammer is better off if only the most gullible people reply. From the perspective of the person being attacked, the counterintuitive result based on selection bias goes away, and a more convincing scheme is more of a risk to you personally. (The assumption that scammers have limited time to write to each victim may itself become less true because of LLMs.) reply n2d4 14 hours agorootparentprevThis is only true for high throughput spam e-mails, such as those sent to literally every e-mail address in a large data breach. Corporate phishing attacks are much, much more advanced. reply izacus 15 hours agorootparentprevThat doesn't mean those scams are actually commonly successful. reply dimask 20 hours agoparentprevI think that, in theory, it could allow for more sophisticated and targeted attacks, like changing the intended recipient of a money transfer. That would be much harder to detect. reply chromanoid 19 hours agoparentprevIt depends on the people I guess. Some managers will be annoyed of such conversations when they have to approve payments like that on multiple occasions a day - so employees might want to avoid such conversations. The attacker could even add something like that: \"I am currently on a trip. If you are unsure call me on my private mobile phone number...\" and then respond with a faked voice. I think a good way of reaching targets would be a \"double\" forward. So the sender assumes the role of an employee forwarding the email of a manager to an administration adjacent employee. This employee unsuspectingly forwards the seemingly harmless mail (that seems to be forwarded from the manager) again for a reason like birthday wishes or sick notice. This will make it hard for the actual target to understand where the email originally comes from. Beside that one can easily think up more creative ways to use this \"feature\". E.g. letting unsuspecting persons forward problematic content and then blackmail them etc. reply mmsc 21 hours agoparentprevYou’re right. They wouldn’t ask any questions at all, and just send the money. reply bluetidepro 21 hours agorootparentAgreed, the people falling for this would already fall for a much more basic phishing attempt. Thus, the attacker has no need to put this much extra effort/thought into it. reply themoonisachees 19 hours agorootparentThis doesn't even need to be a hypothetical. We know that the attacked currently do not need to do this, because they don't. Darwin's law is very much in effect for scams of all types. reply salesynerd 19 hours agoparentprevOne scenario where this night not be far-fetched is when such mails are sent to the accounts payable department of large companies. The people are not going to call a line manager everytime a payment request comes through email, especially if the dollar amount is small and didn't require pre-approval. I remember even Google had fallen prey to such a scam where they were paying somebody even though no work was done. Admittedly, that case involved fictitious invoices. However, the principle remains the same. reply mikeiz404 9 hours agoparentprevI agree the example they give seems a bit unlikely especially since the subject line is not changing (though admittedly I do not have experience in this area). However something a little more subtle such as swapping out a routing number from a legitimate to an illegitimate one could be done and that seems harder to catch especially if the person who forwarded it to you is supposed to verify it first. reply duxup 20 hours agoparentprevI agree that his seems so specific that while it is very interesting from a technical perspective, it is also much less likely compared to most phishing. reply croes 21 hours agoparentprevCould be a link to some kind of portal. You ask your boss if he sent the link to the portal, he confirms, they change the link to a phishing site. reply nebulous1 19 hours agoparentprevI agree, but actually it's just a really bad example that takes the reader to the wrong place because it has the participants acting so irrationally. The underlying issue is still there, they've just distracted from it by putting this in and having the reader go \"hang on a second\". They should have used a situation that was more believable, but also concentrated more on requests where the target likely wouldn't even seek confirmation. reply sonicanatidae 15 hours agoparentprevThis attack works like normal Sales calls. Hit enough of them and you'll find someone that's new, or in a rush, or distracted or ancient or challenged or a Republican idiot, or, or. That's why it's still in use today. It works, but takes a lot of \"cold calling\" via phishing to find targets. reply willd13 13 hours agoparentprevStill pretty cool trick though reply nkrisc 20 hours agoparentprevMy gut says this could be more effective. After all, the initial “phish” (the innocent looking email the manager receives) isn’t fishy at all, and unlikely to trigger any concern. Once the stakes are raised and the scam is revealed, the email has already been granted some amount of legitimacy. Sure, it can easily fail (“did you really want me to wire money to Cyprus?”), just as any phishing email can. But by bypassing the initial phishing filters of the recipient’s awareness, I could see it having a higher success rate than a cold phish that leads immediately with the scam. No evidence or knowledge either way, just a hunch. reply michaelmrose 14 hours agoparentprevA more trivial gambit is logging into an attacker controlled site leaking credentials or installing malware. Also office drones are probable targets. They won't want to waste important peoples time asking for confirmation. reply radarsat1 17 hours agoprevThe other day I was discussing the design for an \"update\" email that our designer was putting together and showing me the size of this stupid graphical header he put at the top and I was showing him how with that you can't even see the text of the title of the email without scrolling down, and then he forwards some other version to me for more feedback and suddenly started getting all disturbed and upset because something looked different to him.. like the fonts were a bit smaller.. and then he said, oh no, when you forward it, it transforms it to the desktop version, instead of the mobile version! And I'm sitting there in disbelief, just staring at him, like,... this is EMAIL we're talking about, what do you mean \"desktop\" version and \"mobile\" version? How is that even a thing? What do you mean it \"transforms\" it? It's literally just copying it! The fact that it \"breaks\" when you do that is evidence of how stupid this is. My god man, why is CSS in my email at all? The fact that we haven't adopted something much simpler to just be able to express italics or whatever, like markdown, is just bonkers to me. It just shows how little anyone cares about actually improving the situation. And all just to cater to the bizarre corporate need to put logos and banners everything. HTML email is just ridiculous. reply jcranmer 13 hours agoparent> The fact that we haven't adopted something much simpler just to be able to express italics or whatever, like markdown, is just bonkers to me. Sounds like you want text/enriched [1], which was published in (checks notes) 1996 and has read support in likely every email client ever. [1] https://www.rfc-editor.org/rfc/rfc1896.txt reply radarsat1 3 hours agorootparentSure, and how many people use it? Available != adopted. reply JacobThreeThree 14 hours agoparentprevResponsive email frameworks are a thing, like MJML. reply ognarb 20 hours agoprevI long argued that we should use markdown (without the inline HTML) or a similar simple text markup instead of HTML for rich text in emails. This would makes it easy for emails clients to decide about either showing the text as rich text or just plain text, while supporting most of the formatting that a normal user might need: bold, italic, quotes, inline images, code blocks, headers, ... Sure it wouldn't be good for marketing emails with the super advanced HTML but I don't think anyone should care about this use-case. reply layer8 19 hours agoparentSomething similar exists as text/enriched defined in RFC 1896. Apple Mail, among others, supports it. https://en.wikipedia.org/wiki/Enriched_text https://www.rfc-editor.org/rfc/rfc1896.html reply diggan 19 hours agorootparent> https://en.wikipedia.org/wiki/Enriched_text Ironically references an article called \"Why HTML is Inappropriate for E-Mail\" (http://www.avernus.com/~gadams/essays/20020418-html-mail.htm...) posted way back in 2002 apparently. We're just walking around in circles after all... reply layer8 19 hours agorootparentThis goes back to 1998 at the very least: https://en.wikipedia.org/wiki/ASCII_ribbon_campaign It was clear from the start that using HTML in email was a bad idea. reply zokier 17 hours agorootparentThis sort of dogmatic rejection of HTML ended up being the bigger problem. If instead futilely fighting against HTML email the community would have just embraced the idea, we could have a sensible spec for email (instead of just ad-hoc whatever Outlook does) and email could have had a chance to prevent its fading relevancy. But no, we had to fight the great evil of rich text which left vendors, MS in the forefront, to their own means to fulfill the strong and justified user demand, with predictable outcome. reply layer8 16 hours agorootparentRFC 1896 defined a sensible alternative in 1996. I agree that it should have received more support, although at least a range of Unix MUAs and Apple Mail did and do support it. reply hinkley 17 hours agoparentprevMany markdown parsers allow inline html. In some languages, it’s most of them. Only a few let you turn it off. So stupid. reply zokier 17 hours agorootparentFor better or worse, inline HTML was key part of original markdown spec: > For any markup that is not covered by Markdown’s syntax, you simply use HTML itself. There’s no need to preface it or delimit it to indicate that you’re switching from Markdown to HTML; you just use the tags. > If you want, you can even use HTML tags instead of Markdown formatting; e.g. if you’d prefer to use HTMLortags instead of Markdown’s link or image syntax, go right ahead. Markdown was not really intended as standalone markup format, instead it was more just a tool for authoring HTML > Markdown’s syntax is intended for one purpose: to be used as a format for writing for the web. reply cdcarter 17 hours agorootparentprevWell, the markdown specification allows inline HTML, so that's to be expected. But it's true if you're taking user input as markdown and display it as rendered HTML, you need to think very carefully about escaping and sanitization. reply PaulDavisThe1st 20 hours agoparentprevGiven that the Lynx browser (HTML in a text terminal) continues to function reasonably well, why is there any need for a different markup language? reply layer8 19 hours agorootparentIt helps for keeping the feature set limited. With HTML, the incentive to use a full-blown implementation is just too strong, and the risk of inconsistent implementations and deliberate or accidental extensions too high. reply lobsterthief 18 hours agorootparentYou could also in theory just restrict which HTML elements can be used. You’d need to do that anyway for MD, especially since MD can contain HTML itself. I’m a huge fan of MD either way ;) I’ve built entire large sites where raw content is stored in MD (along with a MD WYSIWYG) and then the MD is converted to HTML. I’ve found it’s easier and more reliable to parse MD vs HTML, unless of course you’re using a block editor or something more structured. reply layer8 18 hours agorootparent> You could also in theory just restrict which HTML elements can be used. That’s not enough, because you also have to restrict what attributes they may carry (inline styles, event handlers), the type of meta tags and image formats, and so on. But in any case, such a restricted subset was what I was already presuming in my comment. reply PaulDavisThe1st 16 hours agorootparentYou don't have to restrict them at all. You just ignore them. reply layer8 15 hours agorootparentThat's what is meant by restricting. reply SilasX 15 hours agoparentprevWell, getting enough uptake is difficult because email standards are hard to change. We still don't have support for hyperlinking emails, though I tried to fix that recently: https://pastebin.com/kHQs50xm reply jcranmer 13 hours agorootparentThe mid: scheme (see https://datatracker.ietf.org/doc/html/rfc2111) can theoretically do that and has been around since, uh, 1997. reply SilasX 12 hours agorootparentYep, and even with it being possible, no one's bothered to make it usable for email hyperlinks, hence the problem. The (attempt at an) RFC I linked avoids depending on the other side tracking the MessageID, assuming only things they would want to store. reply jcranmer 10 hours agorootparentMessage-ID is something already stored in your email database, because it's necessary for a lot of stuff (in particular, threading conversations correctly). I'd be shocked if any email store didn't support querying by message-ID. Querying by body is a lot harder, since you need full-text support in the body. It's not the difficulty of implementing mid that's blocking support in clients. reply lupire 19 hours agoparentprevColor and Size are basic, useful aspects of writing. reply afavour 20 hours agoprevThe real risk to your organisation is that the developer you assign to generate HTML emails will go mad, lock themselves in the server room and destroy all the hardware while screaming “why is Outlook rendering DIFFERENT” (Seriously though, this is a fascinating exploit) reply lobsterthief 18 hours agoparentThis is why I decided years ago to just pay an HTML email coding service for anything email-related. There’s a lot of bespoke knowledge required for coding HTML templates that pass litmus tests and I never want to go there again. reply echoangle 20 hours agoprevWouldn’t this be fixable by not allowing Stylesheets but only inline style attributes on the tags? To improve usability, email clients could include an automatic step where all Stylesheets are “compiled” to inline styles. The only thing this would break would be advanced CSS selectors (hover etc.) but I’m not sure they would be needed. reply chromanoid 20 hours agoparentI agree. Just baking all classes into inline styles before even presenting the mail should fix this. It makes sense to do this anyway. Pseudo classes and media queries wouldn't work, but those would pose a security risk anyway and are not supported by major mail clients (see https://www.caniemail.com/features/css-at-media/) reply pembrook 18 hours agoparentprevHTML emails have to inline CSS already due to Outlook & Gmail using decades-outdated rendering engines (outlook literally uses the 2006-era MS Word html engine). Also, killing all style attributes would also kill mobile optimization and dark mode as well, since you cannot inline media queries. reply ketch 19 hours agoparentprevThis would also break the current approach for responsive emails. Usually the default/desktop styles are already compiled and inlined, then a style tag with media query selectors is used in the `head` to improve readability for mobile devices. reply yosefk 19 hours agorootparentThe horror! Is there a real need here though as opposed to just something people do, and where does such CSS come from in the case of emails? You could be \"responsive\" by not doing certain things instead of actively doing something and for email content it feels fitting. reply afavour 18 hours agorootparentYou’re asking if email CSS ever needs to be tailored to mobile devices? It’s not any different than web pages needing to be tailored to mobile devices. The answer is yes, sometimes it is necessary. reply yosefk 15 hours agorootparentThe answer is different because an email is not a standalone webpage and there are a lot of things you might want to do in a website that you won't expect to be able or need to do in an email. You probably don't want to control font sizes or page margins in an email, for instance; you probably do want to control color or override some overflow defaults. Maybe I'm off in the above but the point is that you expect to control a subset of things for an email (same as eg for RSS entries) and media selectors aren't obviously necessary for these kinds of things. reply afavour 15 hours agorootparentWhat if you want to place two divs alongside each other on desktop but stacked vertically on a phone? reply bitvoid 13 hours agorootparentHow about not placing two divs alongside each other in the first place? Every time I see that, I immediately start looking for the unsubscribe link or mark it as spam. reply afavour 13 hours agorootparentYour preference is not everyone’s preference. It’s silly to suggest every email client disable media queries because you dislike seeing two elements side by side. reply bitvoid 13 hours agorootparentThat's true, but is anyone going to be up in arms because things are not side by side in an email? I agree with others that emails should just be plain text. It has never bothered me or anyone else I've known to just have plain text and a link that sends them to an actual webpage if HTML is absolutely necessary. reply eviks 4 hours agoprev> the position of the original email in the DOM usually changes, allowing for CSS rules to be selectively applied only when an email has been forwarded. oh, that's sneaky. Would \"plain\" old rtf have been a better choice for formatted emails since that CSS complexity isn't much needed outside of spam marketing :) Why has that been surpassed by HTML&CSS reply shortformblog 17 hours agoprevA classic cut off the limb to fix the cut solution. The problem is bad standardization for email that has allowed hacks like this to rule the day. HTML in general is susceptible to these very concerns. Plenty of emails exist that use HTML without incident. This reads as one user’s frustration with something in the wild that is dressed as a security issue. reply cozzyd 17 hours agoprevSome possible mitigations from the top of my head (maybe ineffective): - warn prominently on hidden elements - randomize the number of enclosing div, on both incoming and outgoing - compute what the forwarded message would look like on forwarding, ask for confirmation if differs significantly. Or do the opposite (probably more effective since doesn't require other clients to help) reply maaaaattttt 21 hours agoprevI see that some of the email clients mentioned wrap the mail’s content in extra HTML tags and modify the CSS and classes names. I’m wondering why email clients don’t use sandboxed iframes to render HTML email? Do they still present security risks? reply red_trumpet 21 hours agoparentThe extra HTML does not happen from the client reading the forwarded email, but when forwarding. That is expected, because the forwarding party might want to add more text to the email. reply echoangle 21 hours agorootparentAt least the person forwarding the mail could check the preview and see that the text changed. reply raptor99 20 hours agorootparentDid you see the GMail section at the bottom of the article? reply echoangle 19 hours agorootparentNo, I started skimming before that point, thanks for pointing that out. I guess you have to check your sent mails now after sending one, until this is fixed reply hannob 17 hours agoprevWhen efail came out, I wrote a blogpost about the security risks of HTML mail. It is really amazing how problematic all of this is, despite its widespread use. The HTML mail spec is really old, and contains almost no security considerations. HTML in emails can only be a subset of HTML to be secure. But nobody has ever defined what exactly that subset is, so everyone does whatever they think. And unsurprisingly, this leads to an endless stream of security flaws. See: https://blog.hboeck.de/archives/894-Efail-HTML-Mails-have-no... reply quesera 20 hours agoprevThis is really clever! Premise: CSS in HTML email allows some text to be visible only after the message is forwarded. This is a huge threat to the trustworthiness of verified email! Examples given in Thunderbird, Outlook, Gmail. Excellent work. I read all mail in mutt, so this is officially \"someone else's problem\". ... Consequently, I'll complain about something else: > This issue was reported to Mozilla on 05.03.2024. The planned release date and a draft of the following section were communicated to Mozilla on 20.03.2024. I agree that little-endian dates make more sense than US-style middle-endian dates. But I will assert that any technologist who does not use ISO 8601 date formatting (2024-03-05, with or without hyphens), is doing it wrong. :) reply shrimp_emoji 20 hours agoparentNot only that, but any person using middle-endian date format is doing it wrong. It's the least rational way to write something! Little-endian at least has the virtue of just being backwards relative to how you write every other number, but middle-endian is just bonkers. (So of course it's the way Americans do it.) reply quesera 20 hours agorootparentAmericans use \"March 5th\" as the verbal form of dates when the context (year) is clear. This actually makes sense -- it puts the most significant part first, and narrows from there. The numeric representation follows that form, becoming \"3/5\". But of course for full context, Americans say \"March 5th, 2024\", yielding \"3/5/24\" or similar atrocities. I'm a big fan of ISO 8601. They got this one right. It's clear, non-preferential, and (critically) it sorts lexically with expected results! reply wongarsu 18 hours agorootparentBut if Americans follow the reasoning to put the most important information first, then why do they say \"March 5th, 2024\"? With that reasoning shouldn't it be \"2024 March 5th\"? For spoken communication you could even extend it to \"Anno Domini 2024 March 5th\", following the common pattern in spoken language of adding somewhat redundant filler to indicate something worth paying attention to is coming up. reply quesera 18 hours agorootparent> Anno Domini 2024 March 5th Gregorian Anno Domini 2024 March 5th. :) reply sfink 16 hours agorootparentprev> But if Americans follow the reasoning to put the most important information first, then why do they say \"March 5th, 2024\"? With that reasoning shouldn't it be \"2024 March 5th\"? Because you're falsely equating \"important\" and \"significant\". It's not about little endian vs big endian, which is all about magnitudes. Most of the time, the year is implicit (i.e., this year, or perhaps next year but that will be inferred from the month). Rolling over months happens quite often, rolling over years is rare. And in cases where the month really doesn't matter, it'll be dropped: \"see you on the 5th!\" That said, I personally dislike both MM-DD-YYYY and DD-MM-YYYY. Mon DD, YYYY is fine. YYYY-MM-DD (ISO8601) is fine (and to be preferred when naming files or in any other context where you'll be seeing lots of dates at once). reply _blk 20 hours agorootparentprevI thought so too for most of my life but living in the states has changed my perspective since the practice is coherent with spoken language. Now I mostly write out the month to make it clear. Let me just appeal to all Americans to use slashes as separators and not dots. Side note: Military format 12MAR24 seems both concise and unbiguous but most people understandably find that unusual (just like ISO dates) reply wongarsu 18 hours agorootparent12MAR24 is so close. If they just went with 12MAR2024 it would have been unambiguous. Of course it lacks many advantages of ISO8601 like sorting correctly in alphabetical sorting or working across languages, but it's a huge step up from 3/12/24. reply quesera 19 hours agorootparentprevFor many years, I used \"12 Mar 2024\" in written form. But the convenience of ISO 8601 for electronic form is compelling, and it has slipped into my writing as well. reply someplaceguy 19 hours agorootparentprevThe military format is ambiguous. For example, 12MAY24 means 2024-05-12 in English but it means 2024-03-12 in Manx Gaelic [1]. Also, tell me: what date is 10SKÁ11? Or 01DU02? Furthermore, a lexicographic sort will do the wrong thing. Better stick with ISO 8601. [1] https://gv.m.wikipedia.org/wiki/Mayrnt reply shrimp_emoji 13 hours agorootparentYep. Numbers > names, always. I'll go even further: months shouldn't even have names (which no longer even make sense, like October)! Neither should the days of the week, like the Chinese do it. reply adrianN 20 hours agorootparentprevIs that March 24th, 1912? reply velcrovan 20 hours agoprev\"Why emails are a risk to your organization\" there fixed it for you reply wongarsu 18 hours agoparentAt that point it's just \"why communication with the outside is a risk to your organization\". This article presents an attack vector unique to HTML emails, but most attacks over email can be easily adapted to work over WhatsApp, Slack, Jira, Zoom, or whatever else people use to communicate with the outside world. reply upofadown 13 hours agoprev>...it may even be cryptographically signed ... In general, anything you are going to sign has to be in a simple enough format to make it so that the sender and the receiver(s) can actually determine what is signed. HTML should automatically be considered unsigned. It simply is not suitable for the purpose. reply rglover 13 hours agoprevHTML in email shouldn't be as big of a nightmare as it is. It really comes down to rendering engines. If all email clients just checked for text vs HTML and when HTML switched the rendering engine to Webkit, this problem could be solved overnight. There's zero reason why you can't render an email the same way you can as a regular page (minus JavaScript). Out of curiosity, is anybody working on this? It seems like a standard could be produced w/ relative ease to make it easier on vendors. Imagine somebody has at least proposed the idea before... reply SoftTalker 17 hours agoprevClever trick. It reinforces the position I've held since the 1990s. Emails should be text. HTML is for web pages. reply tombert 17 hours agoparentI've been using exclusively plain-text email for a long time, and it's not even for security; I want to be able to respect people's font choice. When you send HTML emails, the font is chosen by the sender, which normally is fine, but some people prefer to use dyslexic-friendly fonts (e.g. OpenDyslexic or Comic Sans) to read their email, and I don't want my message to be artificially more difficult to read. Since most emails really don't need elaborate formatting, I'm not 100% sure why this isn't the default. reply eviks 4 hours agorootparentWhat prevents the client app from overriding this font option? Plain text disrespects readability aspect since you lose the most basic highlighting like bold/italics reply tombert 3 hours agorootparentI don’t think anything “stops” you from overriding the font, outside of potentially breaking formatting. It just doesn’t appear to be directly enabled on most clients; it seems like most clients will default to the font the email uses for HTML. reply ledgerdev 13 hours agoprevBetween these sorts of email tricks and ability to easily voice clone using ai, it would seem that invoicing systems should put some sort of 2 factor approval/confirmation into the workflow for payouts. reply tonymet 14 hours agoprevthis highlights a broader issue of irreproducible content. Another good example are group chats where some recipients see messages inconsistently or in a different order than others. It can happen due to inconsistent delivery, inconsistent ACLs, or other reasons. How can people agree on things when they are not looking at the same thing? And people assume that everyone sees what they see by default. Can a board make a decision when not everyone has the same presentation of facts? Remember when you judged someone for asking a stupid question when the answer was just posted a moment ago? How do you know they even received it? reply nickburns 18 hours agoprevIf we style the kobold letter as an overlay, we can not only affect the forwarded email, but also (for example) replace any comments your manager might have had on the original mail, opening up even more opportunities for phishing. clever doesn't even begin to adequately describe. tangentially and anecdotally, it's only occurred to me fairly recently, like within the past year or so, to configure all my mail clients, including both desktop and mobile Outlook (and OWA), to not 'automatically download new messages'. this really needs be the default setting. reply remram 5 hours agoparentWhy? reply nickburns 5 hours agorootparentso as not to automatically download any HTML content i don't want to download since those calls are trackable. my guess is you're confused and you think i mean i've disabled mailbox sync or something...? obviously not. i don't know, man, questionable downvote first/clarify second. but you're welcome for the gratuitous privacy tip. reply remram 5 hours agorootparentWhat a load of ramble and bile for a simple question. reply nickburns 4 hours agorootparentWhat a load of ramble and bile for a simple question at least part of my guess was correct i see. you strike as the type to ask lots of 'simple' questions. 'bile', lol. what a flowery and poetic insult. thanks. reply RedShift1 19 hours agoprevWhere does the term \"kobold letters\" come from? reply JLCarveth 19 hours agoparentFrom the article \"I’ll refer to these elements as kobold letters, after the elusive sprites of mythology.\" reply jgrahamc 18 hours agoprevSee also: The Spammers' Compendium: https://www.virusbulletin.com/resources/spammerscompendium/ reply sylware 17 hours agoprevDon't let me start on phishing SMS texts... still around and strong. My phone will fireup a non-big-tech noscript/basic (x)html browser if I mistakenly click on an URL in it AND my phone does not run android (or an other linux based phone OS) nor iOS. reply mschuster91 19 hours agoprev [–] > To prevent the CSS of emails to affect the style of the webmailer, Outlook modifies the email by prefixing all ids and classes with x_ and adjust the CSS accordingly. Wait what, OWA loads emails directly into the same DOM tree as the rest of the app instead of an iFrame? reply kevingadd 8 hours agoparent [–] So does gmail. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Attackers exploit CSS in HTML emails to manipulate content visibility, posing security risks and enabling phishing scams, affecting Thunderbird, Outlook on the web, and Gmail users.",
      "Some email clients offer safeguards, but users should exercise caution with HTML emails, like disabling HTML or using restricted modes to lower the risk.",
      "Due to challenges in implementing strong defenses in email clients, user vigilance and preventive measures are vital in combating these vulnerabilities."
    ],
    "commentSummary": [
      "The discussion thread delves into the risks of HTML email phishing attacks and skepticism towards phishing simulation services' effectiveness.",
      "Concerns are raised about security vulnerabilities in email forwarding, debates on using HTML vs. Markdown in emails, and challenges of coding HTML emails are highlighted.",
      "Topics cover mobile optimization, date format ambiguity, ISO 8601 format, font options, 2-factor approval for invoicing systems, and email security measures while addressing the compatibility of emails across various email clients."
    ],
    "points": 316,
    "commentCount": 133,
    "retryCount": 0,
    "time": 1712226439
  },
  {
    "id": 39930809,
    "title": "Chrome's V8 Sandbox: Enhancing Security and Stability",
    "originLink": "https://v8.dev/blog/sandbox",
    "originBody": "The V8 Sandbox Published 04 April 2024 · Tagged with security After almost three years since the initial design document and hundreds of CLs in the meantime, the V8 Sandbox — a lightweight, in-process sandbox for V8 — has now progressed to the point where it is no longer considered an experimental security feature. Starting today, the V8 Sandbox is included in Chrome's Vulnerability Reward Program (VRP). While there are still a number of issues to resolve before it becomes a strong security boundary, the VRP inclusion is an important step in that direction. Chrome 123 could therefore be considered to be a sort of \"beta\" release for the sandbox. This blog post uses this opportunity to discuss the motivation behind the sandbox, show how it prevents memory corruption in V8 from spreading within the host process, and ultimately explain why it is a necessary step towards memory safety. Motivation # Memory safety remains a relevant problem: all Chrome exploits caught in the wild in the last three years (2021 – 2023) started out with a memory corruption vulnerability in a Chrome renderer process that was exploited for remote code execution (RCE). Of these, 60% were vulnerabilities in V8. However, there is a catch: V8 vulnerabilities are rarely \"classic\" memory corruption bugs (use-after-frees, out-of-bounds accesses, etc.) but instead subtle logic issues which can in turn be exploited to corrupt memory. As such, existing memory safety solutions are, for the most part, not applicable to V8. In particular, neither switching to a memory safe language, such as Rust, nor using current or future hardware memory safety features, such as memory tagging, can help with the security challenges faced by V8 today. To understand why, consider a highly simplified, hypothetical JavaScript engine vulnerability: the implementation of JSArray::fizzbuzz(), which replaces values in the array that are divisible by 3 with \"fizz\", divisible by 5 with \"buzz\", and divisible by both 3 and 5 with \"fizzbuzz\". Below is an implementation of that function in C++. JSArray::buffer_ can be thought of as a JSValue*, that is, a pointer to an array of JavaScript values, and JSArray::length_ contains the current size of that buffer. 1. for (int index = 0; indexJSObject::GetPropertyNames() { int num_properties = TotalNumberOfProperties(); std::vector properties(num_properties); for (int i = 0; i < NumberOfInObjectProperties(); i++) { properties[i] = GetNameOfInObjectProperty(i); } // Deal with the other types of properties // ... This code makes the (reasonable) assumption that the number of properties stored directly in a JSObject must be less than the total number of properties of that object. However, assuming these numbers are simply stored as integers somewhere in the JSObject, an attacker could corrupt one of them to break this invariant. Subsequently, the access into the (out-of-sandbox) std::vector would go out of bounds. Adding an explicit bounds check, for example with an SBXCHECK, would fix this. Encouragingly, nearly all \"sandbox violations\" discovered so far are like this: trivial (1st order) memory corruption bugs such as use-after-frees or out-of-bounds accesses due to lack of a bounds check. Contrary to the 2nd order vulnerabilities typically found in V8, these sandbox bugs could actually be prevented or mitigated by the approaches discussed earlier. In fact, the particular bug above would already be mitigated today due to Chrome's libc++ hardening. As such, the hope is that in the long run, the sandbox becomes a more defensible security boundary than V8 itself. While the currently available data set of sandbox bugs is very limited, the VRP integration launching today will hopefully help produce a clearer picture of the type of vulnerabilities encountered on the sandbox attack surface. Performance # One major advantage of this approach is that it is fundamentally cheap: the overhead caused by the sandbox comes mostly from the pointer table indirection for external objects (costing roughly one additional memory load) and to a lesser extent from the use of offsets instead of raw pointers (costing mostly just a shift+add operation, which is very cheap). The current overhead of the sandbox is therefore only around 1% or less on typical workloads (measured using the Speedometer and JetStream benchmark suites). This allows the V8 Sandbox to be enabled by default on compatible platforms. Testing # A desirable feature for any security boundary is testability: the ability to manually and automatically test that the promised security guarantees actually hold in practice. This requires a clear attacker model, a way to \"emulate\" an attacker, and ideally a way of automatically determining when the security boundary has failed. The V8 Sandbox fulfills all of these requirements: A clear attacker model: it is assumed that an attacker can read and write arbitrarily inside the V8 Sandbox. The goal is to prevent memory corruption outside of the sandbox. A way to emulate an attacker: V8 provides a \"memory corruption API\" when built with the v8_enable_memory_corruption_api = true flag. This emulates the primitives obtained from typical V8 vulnerabilities and in particular provides full read- and write access inside the sandbox. A way to detect \"sandbox violations\": V8 provides a \"sandbox testing\" mode (enabled via either --sandbox-testing or --sandbox-fuzzing) which installs a signal handler that determines if a signal such as SIGSEGV represents a violation of the sandbox's security guarantees. Ultimately, this allows the sandbox to be integrated into Chrome's VRP program and be fuzzed by specialized fuzzers. Usage # The V8 Sandbox must be enabled/disabled at build time using the v8_enable_sandbox build flag. It is (for technical reasons) not possible to enable/disable the sandbox at runtime. The V8 Sandbox requires a 64-bit system as it needs to reserve a large amount of virtual address space, currently one terabyte. The V8 Sandbox has already been enabled by default on 64-bit (specifically x64 and arm64) versions of Chrome on Android, ChromeOS, Linux, macOS, and Windows for roughly the last two years. Even though the sandbox was (and still is) not feature complete, this was mainly done to ensure that it does not cause stability issues and to collect real-world performance statistics. Consequently, recent V8 exploits already had to work their way past the sandbox, providing helpful early feedback on its security properties. Conclusion # The V8 Sandbox is a new security mechanism designed to prevent memory corruption in V8 from impacting other memory in the process. The sandbox is motivated by the fact that current memory safety technologies are largely inapplicable to optimizing JavaScript engines. While these technologies fail to prevent memory corruption in V8 itself, they can in fact protect the V8 Sandbox attack surface. The sandbox is therefore a necessary step towards memory safety. Posted by Samuel Groß.",
    "commentLink": "https://news.ycombinator.com/item?id=39930809",
    "commentBody": "The V8 Sandbox (v8.dev)260 points by todsacerdoti 19 hours agohidepastfavorite89 comments jiripospisil 17 hours ago> Similarly, disabling the JIT compilers would also only be a partial solution: historically, roughly half of the bugs discovered and exploited in V8 affected one of its compilers while the rest were in other components such as runtime functions, the interpreter, the garbage collector, or the parser. Using a memory-safe language for these components and removing JIT compilers could work, but would significantly reduce the engine's performance (ranging, depending on the type of workload, from 1.5–10× or more for computationally intensive tasks). If you're willing to take the performance hit, Chromium actually allows you to disable JIT easily in the Settings and add exceptions for certain sites. Open the Settings and search for V8 Optimiser. reply vlovich123 17 hours agoparent> while the rest were in other components such as runtime functions, the interpreter, the garbage collector, or the parser Notably memory safe languages wouldn't really help with the garbage collector since it would have to use unsafe Rust & the confusion about lifetime would still exist or you'd be using something like Java/C# where you're just relying on the robustness of that language's runtime GC. However, the runtime functions, interpreter and parser would be secured by something like Rust & I fail to see how well-written Rust would introduce a 1.5-10x overhead. reply azakai 15 hours agorootparentThe 1.5-10x overhead part is not talking about Rust, but about disabling JITs. reply o11c 13 hours agorootparentprevMany memory-safe languages would be fine. Only languages with the particular narrow opinions of Rust would likely be vulnerable to this. reply fngjdflmdflg 16 hours agorootparentprevI think the key is: >and removing JIT compilers If you read the article it makes more sense. reply kaba0 16 hours agorootparentprevThat’s not quite true. It only depends on what level of abstraction are you willing to do — you can write a runtime with GC entirely in safe rust (or a managed language). reply dadrian 15 hours agorootparentIt doesn't matter if the JIT itself is written in a memory-safe language or not if you're exploiting miscompiled JIT output. If the machine code emitted by a JIT is wrong, it can be exploited regardless of if the JIT itself is memory safe or not. reply mike_hearn 2 hours agorootparentSurprisingly, it does matter! The key abstraction here is partial evaluation combined with memory safe languages. Look at how the JVM world does it with GraalJS (or GraalPython or the other langs). This approach eliminates the sorts of vulnerabilities V8 is talking about, because the semantics of the language are defined as a memory-safe interpreter, which is then converted to JIT compiled code directly by having the compiler treat interpreter data structures as constants for the constant folding passes. This gives you a 1-2 win: 1. The example given at the top of the blog post wouldn't be exploitable in interpreter mode in GraalJS, because the VM intrinsic would be written in Java and thus bounds checked. 2. It's also not exploitable once JIT compiled, because the intrinsic and its bounds check has been inlined into the user code function and optimized as a unit (meaning the bounds check might be removed if the compiler can prove that the user's implementation of ToNumber doesn't modify the size). GraalJS has nearly V8 levels of peak performance, so this technique doesn't require big sacrifices for server workloads, and client workloads where latency matters more are now a focus of the team. In this way you can build a high performance JS engine that still has tight security. It supports code sandboxing also. (Disclosure: I do part time work for Oracle Labs and sit next to some of the people doing that work) reply kaba0 15 hours agorootparentprevI thought the abstraction I wrote sort of implied an interpreted mode for the runtime, no JIT compilation. Apologies for not being clear. reply The_Colonel 14 hours agorootparentThat's just trivially true given all these languages are Turing-complete. reply chlorion 14 hours agorootparentprevYup you can write a garbage collected interpreter for a programming language with no unsafe code at all, even for languages that have complex data structures like doubly linked lists in them. Using something like a slotmap to store the languages objects in is what I would do, and your GC would just involve removing values from the map after marking everything that's reachable. The popular slotmap crate on crates.io does contain unsafe code but nothing about the data structure inherently requires unsafe. reply orangepanda 16 hours agoparentprevDoes Safari in Lockdown Mode do anything more than just disabling JIT? reply madars 16 hours agorootparentYes, it also disables WASM, MP3 decoding, gamepad API, JPEG 2000, SVG fonts, PDF previews, WebGL, Speech Recognition API, Web Audio API. Pretty much web as it was meant to be ;-) https://9to5mac.com/2022/07/25/lockdown-mode-ios-16-restrict... reply thefounder 16 hours agorootparentI think you mean the html not the web reply pjmlp 12 hours agorootparentI still remember when that was all the Web offered, the glory days of HTML 2.0. reply grishka 14 hours agoparentprev> would significantly reduce the engine's performance (ranging, depending on the type of workload, from 1.5–10× or more for computationally intensive tasks). And the downside being? Seriously, JS was never meant to be performant. In the real world, it's very rarely used for anything computationally intensive. reply eyelidlessness 14 hours agorootparent> Seriously, JS was never meant to be performant. If you mean “wasn’t originally meant”, that might be true. But it’s been meant to be performant for quite a long time, with huge investments behind the realization of that intent. It’s fine if you have nostalgia for whatever you think was the original vision behind JS. But that hasn’t been the operating vision for it for many years. reply binary132 5 hours agorootparentit’s really important so that Facebook can be fast (Facebook is still slow) reply jrajav 14 hours agorootparentprevVery curious what your unique definition of 'computationally intensive' is, that manages to not include one of the most significant computational workloads worldwide, both in terms of absolute volume and impact on human productivity. Namely, web browser rendering performance. reply grishka 10 hours agorootparentHuh? The rendering is part of the browser itself. It's not like JS has to run every frame to put the pixels onto the screen. Making ajax requests and doing things to the DOM tree isn't a \"computational workload\" because there's hardly any computation happening in that JS code. reply ksec 6 hours agorootparentOnly if we could have HTMX or Turbo that doesn't rely on JS. And then for 95%+ of website, most JS code are used for tracking and ads. reply zamadatix 14 hours agorootparentprevPower usage reply grishka 10 hours agorootparentJIT compilation isn't free either. reply troupo 14 hours agorootparentprevGoogle already says that 2.5 seconds to Largest Contentful Paint is fast: https://blog.chromium.org/2020/05/the-science-behind-web-vit... Now multiply that by 1.5x. reply grishka 10 hours agorootparentRendering pages server-side and avoiding 5-megabyte bundles might help with that. JIT and other browser-side performance optimizations just delay the problem anyway. The culture around web development needs to change. reply pciexpgpu 16 hours agoprevThis is splendid work by Google that will benefit the rest of the ecosystem - especially with the reward program. I wonder how this impacts (positively) Cloudflare Workers/Fly.io-style isolation (both use very different isolation mechanisms I guess). Perhaps, thinking out loud, CF Workers had the right level of isolation to begin with (i.e. pure V8 isolation)? reply throwitaway1123 13 hours agoparentFly uses Firecracker micro VMs rather than V8 isolates. Two of the engineers behind both services had a friendly discussion about it a few years ago: https://news.ycombinator.com/item?id=31759170 reply winrid 15 hours agoprevSomeday it would be really cool to execute NodeJS code in a sandbox with a timeout without having to throw the work at a subprocess. reply silverlyra 9 hours agoparentI believe that day has already come: https://nodejs.org/docs/latest-v20.x/api/worker_threads.html... When you create a Worker with the worker_threads module, Node spawns a new V8 isolate in the same process: https://github.com/nodejs/node/blob/v20.12.1/src/node_worker... It’s much more isolation than C threads – the entry point for a thread is a whole module (not a function), and threads must use message passing to communicate. They can share memory, but only via [Shared]ArrayBuffer objects. They're in the same OS process, but each have their own global process object. But I think it'd meet your need for an in-process isolated execution environment, which you can terminate from the main thread after a timeout. reply billywhizz 9 hours agorootparenti'm pretty sure node.js does not enable v8 sandboxing by default. https://github.com/nodejs/node/blob/main/deps/v8/BUILD.gn#L3... reply kevingadd 8 hours agorootparentprevI would be concerned about terminating an isolate on timeout, couldn't it be holding mutexes when you terminate it? reply silverlyra 6 hours agorootparentIf you use SharedArrayBuffers in worker threads, use Atomics.wait to block, and then terminate the worker which would've called Atomics.notify – yes, if no timeout is used on the notify. But I don't know of any other way this would happen. reply jhgg 7 hours agorootparentprevV8 isolate termination is more akin to throwing an uncatchable exception, versus killing a thread abruptly. When you ask v8 to terminate an isolate, it does take some time for it to terminate depending on what code is running. reply alberth 7 hours agoprevCloudflare Workers IIRC, Cloudflare Workers are based on V8 sandboxing ... which is in part why the cold starts are so fast. https://developers.cloudflare.com/workers/reference/how-work... reply codedokode 1 hour agoprevIt seems that the problem is in Javascript itself, where almost every action can cause side-effects. This makes interpreter extremely complex and it is difficult to spot a mistake. Converting something to int shouldn't cause any side effects. reply pjmlp 18 hours agoprevThe most relevant part for the TL;DR; folks, > The V8 Sandbox has already been enabled by default on 64-bit (specifically x64 and arm64) versions of Chrome on Android, ChromeOS, Linux, macOS, and Windows for roughly the last two years. reply rzzzt 17 hours agoparentElectron is also using V8's sandboxed pointers: https://www.electronjs.org/blog/v8-memory-cage reply andy_xor_andrew 17 hours agoprevI'm confused about the fizzbuzz example they provide. ```js let array = new Array(100); let evil = { [Symbol.toPrimitive]() { array.length = 1; return 15; } }; array.push(evil); // At index 100, the @@toPrimitive callback of |evil| is invoked in // line 3 above, shrinking the array to length 1 and reallocating its // backing buffer. The subsequent write (line 5) goes out-of-bounds. array.fizzbuzz(); ``` I'm probably missing the point, but I thought indexing into an array in Javascript outside its bounds would result in an exception or an error or something? reply IainIreland 15 hours agoparentThose are the intended semantics of JS, but that doesn't help you when you're the one implementing JS. Somebody has to actually enforce those restrictions. Note that the code snippet is introduced with \"JSArray::buffer_ can be thought of as a JSValue*, that is, a pointer to an array of JavaScript values\", so there's no bounds checking on `buffer_[index]`. It's easy enough to rewrite this C++ code to do the right bounds checking. Writing the code in Rust would give even stronger guarantees. The key point, though, is that those guarantees don't extend to any code that you generate at runtime via just-in-time compilation. Rust is smart, but not nearly smart enough to verify that the arbitrary instructions you've emitted will perform the necessary bounds checks. If your optimizing compiler decides it can omit a bounds check because the index is already guaranteed to be in-bounds, and it's wrong, then there's no backstop to swoop in and return undefined instead of reading arbitrary memory. In short, JIT compilation means that it's ~impossible to make any safety guarantees about JS engines using compile-time static analysis, because a lot of the code they run doesn't exist until runtime. reply foldr 11 hours agorootparent>Those are the intended semantics of JS They're actually not. Out of bounds indexing is fine, you just get undefined as the result. reply csjh 17 hours agoparentprevOOB indexing from the Javascript side would return undefined, but OOB indexing on the engine side (lines 5/7/9 of JSArray::fizzbuzz()) is the same as OOB indexing a pointer reply rafram 17 hours agoparentprevThe example fizzbuzz() function is implemented in C++. (And out-of-bounds indexing in JS actually doesn't generate an exception/error; it just returns undefined. Great language!) reply olliej 17 hours agorootparentNot to be confused with undefined in c++! :D :D the best! reply eyelidlessness 17 hours agoparentprevDisclaimer: I have very little experience with C++, a bit more with Rust code that bridges with JS in a manner similar to the example, and zero experience with V8 dev. All of that said… I think the technically correct responses you’ve gotten so far may be missing an insight here: wouldn’t the V8 example code be just as safe as the equivalent JS if it used the JS array’s own semantics? More to the point: presumably those JS semantics are themselves implemented in C++ somewhere else, and this example is reimplementing an incorrect subset of them. While it’s likely inefficient to add another native/JS round trip through JSValue to get at the expected JS array functionality, it seems reasonably safe to assume the correct behavior could be achieved with predictable performance by calling into whatever other part of V8 would implement those same JS array semantics. In other words, it doesn’t seem like you’re missing the point. It seems like this kind of vulnerability could be at least isolated by applying exactly the thinking you’ve expressed. reply azakai 14 hours agorootparent> wouldn’t the V8 example code be just as safe as the equivalent JS if it used the JS array’s own semantics? Yes, but imagine that the code we are talking about here is JIT code that the compiler emitted. If the compiler JITed code that was safe Rust then it could be safe. But JITs emit machine code and a big part of their performance is exactly in the \"dangerous\" areas like removing unneeded bounds checks. Say you have a bounds check in a loop. In some cases a JIT can remove it, if it can prove it's the same check in each iteration. Never removing the check would be safer, of course, but also slower. The point of the article here is that a lower-level sandboxing technique can help such JIT code: even if a pass has a logic bug (a bug Rust would not help with) and removes a necessary check then the sandboxing limits what can be exploited. reply professoretc 15 hours agorootparentprevYou're essentially correct; in JS, if you write for(var i = 0; ineither switching to a memory safe language, such as Rust, nor using current or future hardware memory safety features, such as memory tagging, can help with the security challenges faced by V8 today. And here I thought Rust would fix my security issues ... reply conradludgate 13 hours agoparentJIT engines are fundamentally unsafe since they would produce unsafe machine code directly. And for performance sake the runtime should use tactical unsafe. So memory safety is definitely still something to worry about, even if it's less likely to occur in my experience reply egnehots 17 hours agoprevThey say that Rust is not enough and dismiss it quickly: > V8 vulnerabilities are rarely \"classic\" memory corruption bugs (use-after-frees, out-of-bounds accesses, etc.) but instead subtle logic issues which can in turn be exploited to corrupt memory. As such, existing memory safety solutions are, for the most part, not applicable to V8. In particular, neither switching to a memory safe language, such as Rust, nor using current or future hardware memory safety features, such as memory tagging, can help with the security challenges faced by V8 today. But looking at the awesome list they provided: https://docs.google.com/spreadsheets/d/1lkNJ0uQwbeC1ZTRrxdtu... There are a lot of use-after-frees and out-of-bounds accesses, buffer overflow in there... reply olliej 17 hours agoparent> There are a lot of use-after-frees and out-of-bounds accesses, buffer overflow in there. Yes, and they’re in the runtime itself, which rust cannot protect you from. Rust cannot protect lifetime enforcement for GC objects any more than C++ already does, it can’t protect you against OoB when the reason for the OoB is the runtime is wrong about the object size, etc. Rust does not magically make it impossible to have errors, it makes it harder by default, but the cases where these go wrong are already largely using c++ to provide the same level of memory safety rust can in the environment. The easiest way to understand this is if you use `vec` you won’t get unsafe oob, but if there’s a bug in `vec` rust (or any language) cannot protect you. Eg if there’s a JVM bug that breaks arrays then the fact that Java is memory safe isn’t relevant. reply ajross 17 hours agorootparentAlso worth pointing out that the specific problem area, highly optimized runtimes for interpreted/JIT-compiled languages, the borrow checker doesn't really have much to offer. Rust's safe memory paradigm more or less requires an \"owner\" for every pointer, and by definition the arbitrary graph of pointers in the executed code aren't going to have that. Any such runtime is going to be built on a metric ton of unsafe. reply vlovich123 17 hours agorootparentprevSure, but a bug in Rust's vec is unlikely at this point & thus as long as you're in safe Rust you have no possibility of a memory error, which isn't the case for C++ vectors. It can't protect you from lifetime issues with GC objects, but it can for almost everything else you're doing. They indicate 50% vulns are JIT and 50% are memory safety issues in the runtime, where GC is only part of it. If the bulk of the runtime issues are around GC lifetime confusion, I agree that Rust maybe wouldn't help. It might help to make sure you don't misuse the GC machinery which might be a significant mitigation, but given the bugs I've seen in the field around integrating with the GC, I doubt Rust would help with that class of bugs. reply johnnyjeans 15 hours agorootparent>Sure, but a bug in Rust's vec is unlikely at this point & thus as long as you're in safe Rust you have no possibility of a memory error It has nothing to do with the built-in data structures because it doesn't even exist in the same space as them. The flaws themselves are in an algorithm's reasoning, it's not an issue that exists because somewhere in the codebase there's an out-of-bounds access on a vector. The issues are caused by said flawed reasoning generating bad machine code with erronious pointer arithmetic. Note that it's the reasoning itself generating bad pointer arithmetic, not pointer arithmetic that exists explicitly in the codebase. It's the kind of problem you need proof systems to solve. A substructural type system (or a near-approximation like Rust's ownership semantics) is simply not robust enough for the problem domain, you need full blown dependent types for this kind of thing, something that can guarantee logical safety. ATS can handle the job, but Rust can't. reply tptacek 16 hours agorootparentprevThe problem is closer to that of an attacker that can write unsafe{} blocks than it is to attackers finding a bug in Vec. reply olliej 13 hours agorootparentprevYou’re missing the point. You’re right there are unlikely to be bugs in vec, but there are also unlikely to be bugs in std::vector or WTF::Vector both of which error out on OoB (chrome/v8 uses hardened libc++). I was using `vec` as an example of runtime code that is fundamentally implemented in unsafe code. The errors that are being discussed are errors in the runtime - eg the unsafe{} blocks of rust. It’s very difficult to write code in v8/blink (or JSC/webkit) that interacts with the relevant JS runtime in ways that make the code unsafe - just as you cannot normally interact with `vec` in a way that causes a memory safety error - however the runtime’s implementation of the safe interface is still has to eventually perform unsafe operations. The bugs that you see in V8, JSC, etc are almost invariably in code that would necessarily be unsafe region in rust that would not be preventable in rust. Another example: `Arc`, `Rc`, and `Box` etc all allocate memory, and all your rust code can be built on those, and be safe (assuming no bugs in the refcounting, no compiler lifetime errors, etc), but the allocator beneath them still has to do everything correctly and the operations it performs are largely unsafe. There’s nothing rust can do to prevent a logic error from returning overlapping pointers. You can create lots of abstractions to make it harder to screw up, but you are the runtime at this point so the code that is requiring safety rules is also the thing specifying those rules. Eg if the erroneous state/logic that leads to an incorrect allocation is the same state/logic you are testing against to ensure you aren’t making an erroneous allocation. You can see how that impacts the safety profile of the code. When JSC or V8 have a use after free vulnerability it’s almost always a runtime error because the overwhelming majority of allocations made by both engines are via their own GCs, and so definitionally should be sound. But if there’s a bug in the runtime (a missing barrier, or a scanning error in JSC), then objects can be erroneously collected and that’s how a UaF happens. There’s nothing rust or any safe language can do to make those errors impossible or unexploitable. All the runtime can do is structure the code to make errors as hard as possible, in rust that means minimizing the amount of time in unsafe{}, and add mitigations such that any error that does happen is hard to exploit. When V8 and JSC have buffer overflows it’s because the metadata for an object says “there is this much memory available” but that is incorrect. Again rust cannot protect against this: you’re in the position of a `vec` with incorrect bounds information. And that goes on for all the types of bug class. The vast majority of the security benefits rust offers for a language and vm runtime are available - and used - in c++. The bugs are in the code that would necessarily be unsafe{}. Now in blink/webkit the moment you get beyond the relevant JS runtime you run straight into the standard C++ nightmare that rust, swift, JS, C#,… prevent so that’s another thing altogether. reply wavemode 17 hours agoparentprevYou didn't read the article, then. They clearly explain how even if Rust were used for the entirety of v8, there would still be memory corruption, because the memory corruption is happening in code that is JIT compiled. reply vlovich123 17 hours agorootparentI think they did because all the vulnerabilities in the hardening they talk about is because of C++ memory safety & would be fixed by Rust (i.e. their hardening technique doesn't target JIT exploits themselves). reply azakai 15 hours agorootparentNo, this very much does help protect against JIT exploits. JIT code contains code that accesses the data structures they are sandboxing. By sandboxing those objects, the JIT code is limited in what it can do. This might help you understand: An example the article gives is if an optimization pass has a bug that forgets a check. Then it may emit JIT code that will access a data structure that it should not. But, thanks to this sandboxing, that object cannot be outside the sandbox, nor refer to anything outside the sandbox, so a JIT exploit is limited in what it can achieve. reply tptacek 16 hours agorootparentprevThe whole article is about exploits that leverage the compiler itself, with details. reply rcxdude 16 hours agorootparentNo, it mentions that as an introduction, and then talks about the system for mitigating them, which also has bugs which they admit are of the simple kind that a memory-safe language would prevent. reply kevingadd 17 hours agoparentprevType confusion is also a very common attack against JS runtimes and V8 specifically. Of course, it's not trivial to build a high-performance JS runtime without playing around with pointer types pretty liberally, so I can understand saying \"Rust won't fix this\" in regards to those attacks. But those attacks would basically not be possible against a runtime built on top of Java or C#. reply ngrilly 16 hours agorootparentV8 is a runtime for JS exactly like the JVM is a runtime for Java and CLR is for C#. Which means that whatever sandboxing V8 needs, the JVM and the CLR would need it as well. I don't know what makes you think that the JVM and the CLR have already solved the problem, but not V8. reply vlovich123 17 hours agorootparentprevWould be interesting if they took hardening ideas from kernels that try to solve this (e.g. https://security.apple.com/blog/towards-the-next-generation-...). reply sroussey 17 hours agorootparentprevThe attacks would not be possible against a runtime written in JavaScript as well, by that reasoning. reply kevingadd 13 hours agorootparentThat's called self-hosting, and it's widely used in JS runtimes to implement various built-ins instead of writing them in C++. It provides superior safety and the ability to inline builtins into their callers. reply olliej 17 hours agorootparentprevHaha, I wish I had come up with that response :) reply olliej 17 hours agorootparentprevYes because the attack would be against the .net or Java VM. The JVM - especially in the era of applets - had an illustrious history of VM bugs. We don’t know how bad they would have been because in the era of extremely complex exploits applets essentially do no exist. Neither .net nor the jvm are exposed to the degree of attacks the js engines are, and there’s no strong reason to believe they don’t have similar bugs today. reply kevingadd 13 hours agorootparentI'm not singing the praises of the JVM here, it's just a simple fact that if you implement your runtime in a higher level language you're exposed to a smaller number of potential vulnerabilities. Unchecked array dereferences turn into bounds-checked array dereferences; unchecked typecasts turn into checked typecasts. Null pointer dereferences turn into null reference exceptions. Etc. Of course once you start jitting native code, all of that is off the table. Unless you jit to java/.net bytecode, I guess. reply olliej 12 hours agorootparentNo, you're missing the point. The whole point is you're implementing the runtime that defines the safety semantics. Your proposal is essentially \"implement your JS engine GC on top of the JVM by just using the JVM's GC\", i.e. don't implement the GC yourself. The unsafe code is now the JVM GC, and you've just moved the problem from \"implement the JS engine's GC\" to \"Implement the JVM's GC\", and they same problems continue to exist. I am really struggling to understand where this gap in understanding is occurring. It does not matter what environment or language you implement a JS engine (or whatever) in. The attacker is going to attack the unsafe portion of the runtime. If you build you JS engine on top of the JVM, then the attacker is not going to attack your JS engine's runtime, they attack the JVM's. The JVM, .NET, etc runtimes are not doing anything different to what the JS engine runtimes are doing, and aren't magically free of the same bugs. If anything they're probably doing less to protect from or prevent attacks, because they have a much much smaller attack surface (because they aren't generally exposed to everything on the internet) and the reason attackers have to target the JS engine runtime is because the JS sandbox does not allow the general system access \"correct\" and completely uncompromised .NET or JVM code have. Attacks on the JVM and .NET generally mean \"convince the VM to load correct code that does something that a specific app/service is not meant to do but the VM generally allows applications to do\", whereas a JS VM does not allow an attacker to do anything outside of the JS sandbox, so they must compromise the runtime. It may be easier to understand if we try to present this in a different way: JSC can be compiled as an interpreter for any cpu architecture because there is a fall back C backend for the interpreter code generator, so you can compile JSC to WASM. Then you could make a version of webkit than executed all JS through the WASM build of JSC running under the native JSC runtime. You've now built your JS engine on top of a safe runtime (WASM), but it should hopefully be obvious that an attacker is simply going to continue targeting the native JSC runtime. reply kevingadd 9 hours agorootparentPeople have previously shipped JS runtimes on top of .NET and the JVM. It's not a question of 'who writes the GC', it's more fundamental. If you JIT your JavaScript down into raw native code that bangs rocks together to dereference pointers, you need to make sure your generated code handles pointers correctly. You need to make sure to get all your bounds checks right, etc. Sure, the JVM could somehow have a 30-year-old bug in its array bounds checks. But if you're JITing javascript to an IR that doesn't have raw pointers and instead uses strongly-typed object references and bounds-checked arrays, you have automatically closed off a whole category of defects. At the point where you're saying \"sure, but what if the JVM messes up array bounds checks?\" you might as well be asking whether v8 can really afford to rely on read-only pages and guard pages for its security sandbox. What if the kernel is broken? I mentioned type confusion attacks in particular because they're a class of attack that generally doesn't work against java or .net applications because values can't change type arbitrarily during execution. Local variables and parameters have known types, object type casts are checked, array elements are typechecked before being stored, etc. Obviously you pay a cost for this, and if you have threads the ABA problem rears its head, but JS is single-threaded by design. Between hosting JS on the JVM or in WASM, WASM is probably a safer choice since it's such a constrained sandbox. But the JS runtime you're running inside of the WASM sandbox is still built in C, banging rocks together to dereference pointers. Hopefully you're running a modern security-hardened JS runtime inside that sandbox, and you haven't turned off all the security mitigations thanks to wasm's lack of page protections. reply vlovich123 18 hours agoprev [–] It’s interesting that it spends a lot of time talking about how memory safe languages don’t help V8 cause of JIT (true) & then talks about a hardening technique that does get helped by Rust. Why not just be honest and say that the switching cost to a new language is too expensive and error prone than play these games? Similarly the discussion about memory tagging - I know that it would harden the security of things like Cloudflare workers. And if most of the exploits are because it’s in-process, why not work on isolating it behind it’s own process (there must be ways to do this securely while not giving up too much performance)? reply azakai 15 hours agoparentThe technique here keeps a large set of objects from escaping the sandbox. Those objects are accessed both by C++ and JIT code. You are right that using Rust instead of C++ would help on the C++ side, but it would not help at all on the JIT code side, and that is by far the major source of exploits. In other words, even if you write a JS engine in Rust you could benefit greatly from this technique. reply tux3 17 hours agoparentprevI don't think it's fair to call them dishonest here. It's pretty clear they've heard about memory safe languages, they've thought about it, they've considered in details the pros and cons. >why not work on isolating it behind it’s own process (there must be ways to do this securely while not giving up too much performance)? Well, you make it sound like the easy answer. A good exercise would be to try implementing what you're proposing in a comment. Not necessarily going all the way, but enough to know why it might not be as straightforward as you think. The people working on V8 are not completely clueless, the concept of moving things out of process or using a memory safe language is not going to be a novel idea that they'll just start working on now that someone clever thought of it. reply vlovich123 17 hours agorootparentThe dishonest piece is that the first part talks about why Rust doesn't help with the JIT (true) but then really talks about the V8 sandbox & hardening techniques they're applying to it where Rust would help 100%. > However, assuming these numbers are simply stored as integers somewhere in the JSObject, an attacker could corrupt one of them to break this invariant. Subsequently, the access into the (out-of-sandbox) std::vector would go out of bounds. Adding an explicit bounds check, for example with an SBXCHECK, would fix this. Or use Rust > Encouragingly, nearly all \"sandbox violations\" discovered so far are like this: trivial (1st order) memory corruption bugs such as use-after-frees or out-of-bounds accesses due to lack of a bounds check Or use Rust > Contrary to the 2nd order vulnerabilities typically found in V8, these sandbox bugs could actually be prevented or mitigated by the approaches discussed earlier. In fact, the particular bug above would already be mitigated today due to Chrome's libc++ hardening Or use Rust I'm not saying rewrite the entire thing in Rust, too expensive & would introduce new bugs in the JIT for questionable benefit. But at least mention that & also discuss the technical challenges why the sandbox mechanism isn't written in Rust & what it would take to address those. Look, I'm not saying the V8 team is making the wrong decisions. My questions are an indication of the shallowness of the blog write-up - why not explain some obvious questions that come up for someone who reads it? reply IainIreland 14 hours agorootparentThe whole point is that the sandbox is an approach that can be used in JIT code, where Rust doesn't help. Take the fizzbuzz example with a missing bounds check. Rust can't prevent you from generating JIT code that omits a bounds check on an array and reads/writes out-of-bounds. The sandbox doesn't prevent out-of-bounds reads/writes, but it guarantees that they will only be able to access data inside the sandbox. This means that logic bugs in the JIT compiler are no longer immediately exploitable. They must be combined with bugs in the sandbox implementation. The article's claim is that, unlike compiler bugs, sandbox bugs tend to be amenable to standard mitigation techniques. This article isn't dismissing the value of memory-safe languages. It's identifying a problem space where current memory-safe languages can't help, and providing an alternative solution. Currently, every browser JS engine is written in C++, in part because Rust doesn't solve the big correctness problems. If the sandbox approach works, then using Rust for other parts of the engine becomes more appealing. reply mandarax8 17 hours agorootparentprevHow would rust help you when you're executing jitted code (ie assembly)? The fizzbuzz code would run in rust but the event handler would still be unsafe jitted code. reply rcxdude 16 hours agorootparentIt doesn't: but it helps with the stuff around it. The article talks about 3 locations for bugs 1) jitted javascript code with subtle bugs due to logic errors in the compiler (Rust's memory safety can't really help here) 2) Bugs in surrounding utility code and the interpreter (Rust can help, but running without a JIT entirely is too slow. Still, it's part of the attack surface either way) 3) Bugs in the sandbox implementation which helps mitigate bugs of the first kind (Rust can help) AFAIK the main objection raised here is the article dismisses moving to a memory safe language because it doesn't help with 1, but then discusses 2 and 3 where in fact the issues are exactly where memory safety can help. reply wavemode 17 hours agoparentprev> talks about a hardening technique that does get helped by Rust What hardening technique discussed in this article would be helped by Rust, and what specific feature of Rust would help? reply vlovich123 17 hours agorootparent> This code makes the (reasonable) assumption that the number of properties stored directly in a JSObject must be less than the total number of properties of that object. However, assuming these numbers are simply stored as integers somewhere in the JSObject, an attacker could corrupt one of them to break this invariant. Subsequently, the access into the (out-of-sandbox) std::vector would go out of bounds. Adding an explicit bounds check, for example with an SBXCHECK, would fix this. > Encouragingly, nearly all \"sandbox violations\" discovered so far are like this: trivial (1st order) memory corruption bugs such as use-after-frees or out-of-bounds accesses due to lack of a bounds check. Contrary to the 2nd order vulnerabilities typically found in V8, these sandbox bugs could actually be prevented or mitigated by the approaches discussed earlier. In fact, the particular bug above would already be mitigated today due to Chrome's libc++ hardening. As such, the hope is that in the long run, the sandbox becomes a more defensible security boundary than V8 itself reply wavemode 16 hours agorootparentIt's still not clear to me what Rust feature would prevent what specific vulnerability here. Rust has bounds-checked and non-bounds-checked array accesses depending on the developer's preference, and so does C++. If there's some point you're making with these quotes you're going to need to simplify it for me since I'm not following. reply rcxdude 16 hours agorootparentThe defaults are switched though: C++ is unchecked by default, Rust is checked by default. reply tubthumper8 16 hours agorootparentprev> Rust has bounds-checked and non-bounds-checked array accesses depending on the developer's preference, and so does C++ You're making it sound like these are the same, the difference is the defaults Unsafely accessing an element in C++ vec[i] Unsafely accessing an element in Rust unsafe { vec.get_unchecked(i) } One of these is screamingly obvious that something potentially unsafe is happening and should be audited more closely, that's the real difference. The cause of potential memory issues is isolated and searchable in `unsafe` blocks rather than being potentially anywhere reply wavemode 16 hours agorootparentSo the Rust feature is \"screaming obviousness\"? Your argument is that the advantage of rewriting the module in Rust is that finding array accesses is visually easier? Why not just use grep? reply tubthumper8 15 hours agorootparentI'm not on the V8 team, so can't say why grep didn't find the vulnerabilities. That would perhaps be a good suggestion to make to them! reply olliej 12 hours agorootparentprevWell that's just false in this context. vec[i] Is safe in v8, blink, jsc, webkit, etc. Rust has a huge number of benefits over c++, but it hurts your argument if you refuse to acknowledge the actual environment the C++ is being used in and make objectively incorrect statements. It implies a lack of understanding of C++ and sounds like all you're doing is parroting other people's critiques without understanding the core issues, which undermines your message. That said it's still not particularly relevant here, because the issues being presented are bugs in the runtime. e.g. the runtime logic and state results in erroneous behavior. The bugs being discussed are not \"you did not use a safe vec\" or \"you did not use Rc\", it's \"the size or bounds check in vec is incorrect\" or \"the ref counting in Rc is incorrect\". Rust does not inherently stop those the runtime from having bugs, it simply statically limits where the exposure to unsafe operations can occur. That's super relevant to program safety, but it's not relevant to safety in the JS VM runtime, where they're performing the operations that would be unsafe{} in rust as well. reply olliej 17 hours agoparentprev [–] …because the overwhelming majority of memory safety bugs in js engines (v8, JSC, and spider monkey) are in operations that would be in unsafe blocks in rust as well? In multiple decades I can think of a handful of engine bugs that would have been prevented by rust - and those were largely preventable (and now are) in c++ as well. It is possible for rust to be a safer language than c++ and to also not meaningfully change the security profile of the language. It’s not just the jit, the interpreter and GCs are also subject largely - necessarily - no more protected by rust than c++. reply vlovich123 17 hours agorootparent [–] Did you read the article? It has nothing to do with the JIT. It uses the JIT as a smokescreen to talk about sandbox hardening & the issues within the sandbox are definitely not \"unsafe\" & 100% mitigated by Rust. Take a look at the relevant quotes I extracted in another comment to draw your attention to what the article is actually talking about (sandbox hardening). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The V8 Sandbox, a security feature for the V8 engine, is now part of Chrome's Vulnerability Reward Program, focusing on tackling memory corruption issues that often lead to Chrome exploits.",
      "It aims to contain memory corruption within V8 to prevent system-wide impact, with low performance impact, already operational on compatible platforms for testing.",
      "The V8 Sandbox is crucial for enhancing memory safety within V8, marking a significant advancement in securing the engine against vulnerabilities."
    ],
    "commentSummary": [
      "The discussion centers on vulnerabilities in the V8 engine, especially concerning its compilers, garbage collector, interpreter, and parser.",
      "Mitigation suggestions include disabling JIT compilers, using memory-safe languages for specific components, and enforcing memory-safe practices.",
      "The importance of memory-safe languages like Rust, challenges in eliminating memory errors and logic issues, and potential benefits of using Rust for memory safety in sandbox implementations are emphasized."
    ],
    "points": 260,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1712240693
  },
  {
    "id": 39930919,
    "title": "HTTP/2 Continuation Flood: Sever Vulnerability and Impact.",
    "originLink": "https://nowotarski.info/http2-continuation-flood-technical-details/",
    "originBody": "HTTP/2 CONTINUATION Flood: Technical Details Posted on April 3, 2024 by Bartek Nowotarski tl;dr: Deep technical analysis of the CONTINUATION Flood: a class of vulnerabilities within numerous HTTP/2 protocol implementations. In many cases, it poses a more severe threat compared to the Rapid Reset: a single machine (and in certain instances, a mere single TCP connection or a handful of frames) has the potential to disrupt server availability, with consequences ranging from server crashes to substantial performance degradation. Remarkably, requests that constitute an attack are not visible in HTTP access logs. A simplified security advisory and the list of affected projects can be found in: HTTP/2 CONTINUATION Flood. Table of Contents Preface A quick intro to HTTP/2 HEADERS frame CONTINUATION frame CONTINUATION Flood Vulnerability CPU exhaustion: Golang case Out of Memory Reachable Assertion crash: Node.js (special) case Comparison to previous HTTP/2 vulnerabilities Final remarks Preface In October 2023 I learnt about HTTP/2 Rapid Reset attack, dubbed “the largest DDoS attack to date”. I didn’t have deep knowledge of HTTP/2 back then. I knew it’s basics like frames or HPACK but I was focusing more on HTTP/1.1 protocol and programming languages vulnerabilities. I decided to dedicate time to exploring HTTP/2 from a security analysis perspective after concluding my then-current research. A quick intro to HTTP/2 The main difference between HTTP/1.1 and HTTP/2 is that the later is a binary protocol and client and server exchange frames instead of text lines. There are many frame types, including some control frames that does not transmit data but rather allow configuration of an HTTP/2 session (like SETTINGS or WINDOW_UPDATE). To make this vulnerability easy to understand I need to present two frames: HEADERS frame and CONTINUATION frame. For those who would like to catch up, the best way to learn it is by reading RFC9204. HEADERS frame HEADERS frames allow sending HTTP headers of, both, request and response. The headers are stored in field block fragment and are encoded using HPACK, encoding algorithm that allows compression of header data. It is using static and dynamic tables of commonly used headers and Huffman encoding for the rest of headers. As other frames, this one can have some flags set, along them: END_HEADERS: when set, tells the counterparty that this frame contains all the headers they wanted to send, END_STREAM: when set, tells the counterparty that there will be no request/response body. The frames also have a maximum size, configured at the beginning of communication. If a received frame exceeds the allowed size the connection is dropped with a protocol error. So what happens if a single HEADER frame is not enough to store all the headers? It sends the frame with END_HEADERS flag unset and continues the stream of headers using CONTINUATION frame. CONTINUATION frame CONTINUATION frames are very similar to HEADER frames but they have just one flag: END_HEADERS which has the same function: when set the counterparty knows that more headers are coming in the following CONTINUATION frames. To sum it up, if headers exceed a single frame allowed size they are split in a frame stream: HEADERS (no END_HEADERS flag), CONTINUATION (no flags), CONTINUATION (no flags), … CONTINUATION (END_HEADERS set), After the last frame, either DATA frame is sent (contains request data) or HTTP/2 stream ends. CONTINUATION Flood Vulnerability What if a client starts a new HTTP/2 stream and sends HEADERS and CONTINUATION frames but END_HEADERS flag is never set? This would create an infinite stream of headers that HTTP/2 server would need to parse and store in memory. In HTTP/1.1 world, servers are protected from infinite headers by two mechanisms: Header size limit: if headers list exceed the allowed size, the connection is dropped. Request / headers timeouts: if the request/headers are not sent in timely manner, the connection is dropped. In the last couple of months I checked dozens of implementations and, somehow, these protections were not implemented (or implemented incorrectly) even in major HTTP/2 servers, most notably: Apache httpd, Envoy and many HTTP/2 packages or codecs. I can divide outcomes of the bugs related to this vulnerability in the following categories: CPU exhaustion. Reading the extra headers cause increased CPU usage, it results in slowness responding to other requests but in many cases it was just a matter of number of active HTTP/2 connections that are required to completely block the server from responding. Out Of Memory crash using multiple connections. Headers from CONTINUATION frames are stored in memory but there is a headers list size limit. At the same time there is no headers timeout. This means that an attacker can send headers up to the limit and just wait. Each connection occupies memory indefinietly. Out Of Memory crash using a single connection. Some implementations simply kept reading headers into memory until memory was full which forced the OS to kill the process. Crash after a few frames sent. This is a special, most severe category. Just a few frames are required to crash the server because of implementation bugs when a connection is disconnected mid-CONTINUATION stream. No END_HEADERS flag means that a request is not properly closed. Requests of malicious client would not be saved to access log making this attack hard to debug: in many cases analyzing raw traffic bytes would be necessary to understand the nature of this vulnerability. In next sections I will demonstrate cases above using real vulnerabilities found in production code. CPU exhaustion: Golang case Golang was an example of the CPU exhaustion caused by CONTINUATION Flood. As many other implementations, Golang devs built an abstraction class called http2MetaHeadersFrame which encapsulates one HEADERS frame and zero or more CONTINUATION frames, and their HPACK decoder. Headers data is processed within a single call to readMetaFrame (h2_bundle.go from Go 1.21.3). The HPACK decoder in Golang has multiple params and modes. One of them is SetEmitEnabled which enables or disables emitting of decoded headers. This is done to stop headers emission in case of errors or when the header size limit is reached. 2926func (fr *http2Framer) readMetaFrame(hf *http2HeadersFrame) (*http2MetaHeadersFrame, error) { 2927 if fr.AllowIllegalReads { 2928return nil, errors.New(\"illegal use of AllowIllegalReads with ReadMetaHeaders\") 2929 } 2930 mh := &http2MetaHeadersFrame{ 2931http2HeadersFrame: hf, 2932 } 2933 var remainSize = fr.maxHeaderListSize() 2934 var sawRegular bool 2935 2936 var invalid error // pseudo header field errors 2937 hdec := fr.ReadMetaHeaders 2938 hdec.SetEmitEnabled(true) 2939 hdec.SetMaxStringLength(fr.maxHeaderStringLen()) Indeed, in the SetEmitFunc callback function there’s a logic that checks if the allowed size (maxHeaderListSize) has been reached and in this case SetEmitEnabled(false) is called. 2940 hdec.SetEmitFunc(func(hf hpack.HeaderField) { 2941if http2VerboseLogs && fr.logReads { 2942 fr.debugReadLoggerf(\"http2: decoded hpack field %+v\", hf) 2943} 2944if !httpguts.ValidHeaderFieldValue(hf.Value) { 2945 // Don't include the value in the error, because it may be sensitive. 2946 invalid = http2headerFieldValueError(hf.Name) 2947} 2948isPseudo := strings.HasPrefix(hf.Name, \":\") 2949if isPseudo { 2950 if sawRegular { 2951invalid = http2errPseudoAfterRegular 2952 } 2953} else { 2954 sawRegular = true 2955 if !http2validWireHeaderFieldName(hf.Name) { 2956invalid = http2headerFieldNameError(hf.Name) 2957 } 2958} 2959 2960if invalid != nil { 2961 hdec.SetEmitEnabled(false) 2962 return 2963} 2964 2965size := hf.Size() 2966if size > remainSize { 2967 hdec.SetEmitEnabled(false) 2968 mh.Truncated = true 2969 return 2970} 2971remainSize -= size 2972 2973mh.Fields = append(mh.Fields, hf) 2974 }) 2975 // Lose reference to MetaHeadersFrame: 2976 defer hdec.SetEmitFunc(func(hf hpack.HeaderField) {}) 2977 The following part of this function is responsible for actually feeding the HPACK decoder and it does so until HeadersEnded() returns true which happens only when END_HEADERS flag is set. 2978 var hc http2headersOrContinuation = hf 2979 for { 2980frag := hc.HeaderBlockFragment() 2981if _, err := hdec.Write(frag); err != nil { 2982 return nil, http2ConnectionError(http2ErrCodeCompression) 2983} 2984 2985if hc.HeadersEnded() { 2986 break 2987} 2988if f, err := fr.ReadFrame(); err != nil { 2989 return nil, err 2990} else { 2991 hc = f.(*http2ContinuationFrame) // guaranteed by checkFrameOrder 2992} 2993 } 2994//The vulnerability may not be clear at the first sight: the decoder is properly configured to stop emitting headers once limit is reached. However, it will still decode headers written to it, just without emiting them. Given that the feeding loop above can only be stopped by END_HEADERS flag or an error from ReadFrame an attacker fully controls how long the HPACK decoder will receive new bytes. With no END_HEADERS, this function will never return and HPACK will keep decoding headers as long as the attacker sends them. Out of Memory Out of Memory are probably the most boring yet severe cases. There is nothing special about it: no strange logic, no interesting race condition and so on. The implementations that allow OOM simply did not limit the size of headers list built using CONTINUATION frames. Implementations without header timeout required just a single HTTP/2 connection to crash the server. In implementations with idle timeout it was often possible to send multiple HTTP/2 connections that occupied portions of RAM very close to the allowed per-connection limit and then sending the last CONTINUATION frame byte-by-byte every few seconds to keep the connection alive. Reachable Assertion crash: Node.js (special) case The last case I’d like to present is a Reachable Assertion connected to CONTINUATION frames in Node.js. While it properly handles the infinite stream of CONTINUATION frames there was a data race bug occuring when connection was disconnected during the headers stream. When I was running the exploit code I noticed that sometimes Node.js crashed with the following stack: # node[3253]: virtual node::http2::Http2Session::~Http2Session() at ../src/node_http2.cc:534 # Assertion failed: (current_nghttp2_memory_) == (0) ----- Native stack trace ----- 1: 0xca5430 node::Abort() [node] 2: 0xca54b0 node::errors::SetPrepareStackTraceCallback(v8::FunctionCallbackInfo const&) [node] 3: 0xce7156 node::http2::Http2Session::~Http2Session() [node] 4: 0xce7192 node::http2::Http2Session::~Http2Session() [node] 5: 0x106f01d v8::internal::GlobalHandles::InvokeFirstPassWeakCallbacks() [node] 6: 0x10f3215 v8::internal::Heap::PerformGarbageCollection(v8::internal::GarbageCollector, v8::internal::GarbageCollectionReason, char const*) [node] 7: 0x10f3d7c v8::internal::Heap::CollectGarbage(v8::internal::AllocationSpace, v8::internal::GarbageCollectionReason, v8::GCCallbackFlags) [node] 8: 0x10ca081 v8::internal::HeapAllocator::AllocateRawWithLightRetrySlowPath(int, v8::internal::AllocationType, v8::internal::AllocationOrigin, v8::internal::AllocationAlignment) [node] 9: 0x10cb215 v8::internal::HeapAllocator::AllocateRawWithRetryOrFailSlowPath(int, v8::internal::AllocationType, v8::internal::AllocationOrigin, v8::internal::AllocationAlignment) [node] 10: 0x10a8866 v8::internal::Factory::NewFillerObject(int, v8::internal::AllocationAlignment, v8::internal::AllocationType, v8::internal::AllocationOrigin) [node] 11: 0x15035f6 v8::internal::Runtime_AllocateInYoungGeneration(int, unsigned long*, v8::internal::Isolate*) [node] 12: 0x7f41df699ef6 Aborted (core dumped) After several retries I correlated this crash to the exact moment when my HTTP/2 client disconnects from the Node.js server. This made sense because the assert() call was inside the Http2Session destructor. Let’s take a look at the code (Node.js v21.5.0): 528Http2Session::~Http2Session() { 529 CHECK(!is_in_scope()); 530 Debug(this, \"freeing nghttp2 session\"); 531 // Explicitly reset session_ so the subsequent 532 // current_nghttp2_memory_ check passes. 533 session_.reset(); 534 CHECK_EQ(current_nghttp2_memory_, 0); 535} Node.js is embedding nghttp2 library for HTTP/2 connections handling. current_nghttp2_memory_ is used to track memory allocated by nghttp2 internals and the assertion in the destructor simply ensures that all nghttp2 artifacts are properly removed from memory which happens in reset method (line 533). The value is updated: increased and decreased in several places in the code, often in a nghttp2 callback functions. There was no other option than to check nghttp2 internals and see who is to blame: Node.js by incorrectly calculating the memory usage, or nghttp2 by giving invalid data used in calculations. After quite a long investigation I couldn’t find anything wrong with calculations which pointed to a possibility of a race condition: current_nghttp2_memory_ value was updated elsewhere, at the same time when ~Http2Session was being executed. I found an instance of this case: reset() and nghttp2 callback when CONTINUATION frame is parsed are executed together. When CONTINUATION frame arrives the following chain on events occur when state machine is in NGHTTP2_IB_EXPECT_CONTINUATION state: we go the happy path so state is changed to NGHTTP2_IB_READ_HEADER_BLOCK, from there it calls: session_after_header_block_received which calls: session_call_on_frame_received which calls: on_frame_recv_callback. The last one calls OnFrameReceive callback in Node.js and later: HandleHeadersFrame which does some memory counter update: 1454 DecrementCurrentSessionMemory(stream->current_headers_length_); 1455 stream->current_headers_length_ = 0; 746 void DecrementCurrentSessionMemory(uint64_t amount) { 747 DCHECK_LE(amount, current_session_memory_); 748 current_session_memory_ -= amount; 749 } Now, when HandleHeadersFrame and Http2Session::~Http2Session() are executed at the same time it’s possible that they will update the current_session_memory_ variable at the same time: Http2Session::~Http2Session() 529CHECK(!is_in_scope()); 530Debug(this, \"freeing nghttp2 session\"); 531// Explicitly reset session_ so the subsequent 532// current_nghttp2_memory_ check passes. 533session_.reset(); DecrementCurrentSessionMemory: 747DCHECK_LE(amount, current_session_memory_); 748current_session_memory_ -= amount; Http2Session::~Http2Session() 534CHECK_EQ(current_nghttp2_memory_, 0); This is why CHECK_EQ fails as current_nghttp2_memory_ value is negative. Comparison to previous HTTP/2 vulnerabilities There were a couple of HTTP/2 vulnerabilities in the past. In 2019, a batch of them was reported by Netflix and Google. They are listed in CERT/CC Vulnerability Note VU#605641 and the most similar are: CVE-2019-9516, also known as 0-Length Headers Leak The attacker sends a stream of headers with a 0-length header name and 0-length header value, optionally Huffman encoded into 1-byte or greater headers. Some implementations allocate memory for these headers and keep the allocation alive until the session dies. This can consume excess memory, potentially leading to a denial of service. CONTINUATION Flood is different than CVE-2019-9516 because rather than sending empty headers, an attacker sends many random headers up to the frame size limit configured by the server. CVE-2019-9518, also known as Empty Frame Flooding The attacker sends a stream of frames with an empty payload and without the end-of-stream flag. These frames can be DATA, HEADERS, CONTINUATION and/or PUSH_PROMISE. The peer spends time processing each frame disproportionate to attack bandwidth. This can consume excess CPU, potentially leading to a denial of service. CVE-2019-9518 is about sending empty frames. The CONTINUATION Flood consists of the largest possible frames that occupy memory and consume CPU cycles while being decoded. In October 2023 the details of “Rapid Reset”, a zero day in HTTP/2 protocol, were published and the vulnerability was immedietly dubbed “the largest DDoS attack to date”. The details of this attack are explained in Cloudflare’s article, and while the severity of this vulnerability is different across many implementations, I think it’s important to list main points explaining why the new vulnerability seems to be more severe: Rapid Reset used a combination of HEADERS (with END_STREAM and END_HEADERS flags set) and RST_STREAM frames which means that standard mitigations like rate limiting could at least limit the damage. Also, the server admin would see a lot of inbound server requests and be alerted. During CONTINUATION Flood attack not a single request is made (no END_HEADERS flag)! Admins do not see any requests in the logs! In many implementations just one TCP connection was enough to crash the server (and in some cases with a very small amount of data sent) during the CONTINUATION Flood attack. On contrary, Rapid Reset was used in DDoS attacks (in most cases using a botnet was required for an attack to be successful). Final remarks According to Cloudflare Radar the HTTP/2 traffic accounts for around 60% of all human HTTP traffic (data excluding bots): Given that Cloudflare Radar estimates HTTP traffic data above 70% of all internet transfer and significance of affected projects I believe that we can assume that large part of internet was affected by an easy-to-exploit vulnerability: in many cases just a single TCP connection was enough to crash the server. Don’t forget that HTTP runs not only websites but significant portion of APIs (RESTful APIs). Availability issues with important business and government APIs and websites could incur losses of millions of dollar. Or cause chaos, for example: Poland, the main supplier of heavy weapons to Ukraine which also operates the most important military hub near Ukraine border, experiences increase in DDoS attacks originating from Russia. Had it been exploited in the wild, this would have been very hard to debug without proper HTTP/2 knowledge by the server administrators. This is due to the fact that none of malicious HTTP requests connected to this vulnerability is properly closed. The requests would not be visible in the server access logs and due to lack of advanced frame analytics in most of HTTP/2 servers this would have to be handled by manual, tedious raw connection data analysis. This vulnerability class posed a significant risk to the internet safety! Because of this I am glad that CERT/CC decided to open a Vulnerability Coordination case to track this issue after I reported it in January 2024. Working on a responsible disclosure of this vulnerability with technology giants and open source projects was a great experience. It would be impossible to check so many implementations by a single resesearcher so Vulnerability Coordination is irreplaceable tool for handling issues that affect multiple vendors. Other than opening the case, CERT/CC decided to publish a Vulnerability Note about this issue, which is quite rare: only a few notes are published each year. Thank you to Christopher Cullen for handling the issue on CERT/CC side.",
    "commentLink": "https://news.ycombinator.com/item?id=39930919",
    "commentBody": "HTTP/2 Continuation Flood: Technical Details (nowotarski.info)246 points by campuscodi 18 hours agohidepastfavorite26 comments mcmatterson 16 hours agoI'd just mitigated this exact thing in Bandit last month! https://github.com/mtrudel/bandit/blob/main/lib/bandit/http2... TBH, from an implementors perspective this is a super obvious thing to cover off. It had long been on my radar and was something that I'd always figured other implementations had defended against as well. reply hinkley 14 hours agoparentWell you know what happens when we assume. You make a front page headline out of you and me. reply eek2121 10 hours agorootparentAs someone who worked for a terrible startup that 'assumed' they would have scalability issues, engineered their entire software stack around solving said issues, and ended up with a worthless codebase that nobody could wrap their head around as a result, I feel this comment. Later they began a small refactor which easily handled the loads they were \"assuming\" could not be handled in the way that the refactor handled, and it was wildly successful and the code was much simpler to work on. To developers: don't over engineer. Most languages/frameworks/libraries can handle scale beyond what you'll ever get in your initial implementation. No, you entire website does NOT need to be asynchronous. It is very possible to have too many background jobs. I know this because I've seen the horror. I've also written an entire jobless/synchronous platform that serves millions of users without issue. If you run into scaling issues, that is a good problem to have. Tackle it as it happens. Bottom line is focus on secure, quality code above all else. Don't make assumptions. reply snailscale 8 hours agorootparentThe default way we write applications is actually pretty scalable already. It always hurts to build something that “won’t scale” because it was framed as a negative. Realizing that something “scales” if it meets your current needs is pretty important. Framing scale in terms of how many people can work on it, how fast they can work on it, and how well it meets needs is often a better way of considering the “scale” of what your building. As you said, when request per second becomes a limiting factor you can adjust your scales but doing it from start rarely makes sense (largely because req / sec already scales pretty well) reply hinkley 8 hours agorootparentIt’s often a fear or trauma response. Nobody wants to spend 6 months out of the year trying to keep the plates spinning, and they definitely don’t want to spend 60 hours a week when things get ahead of them. Everything takes twice as long as we think it will and we don’t trust that we can keep ahead of user demand. Many of us have experienced it or been adjacent, and for a long time after we overreact to that scenario. Because we don’t trust that we can keep the wheels on. Over time the memory fades, and the confidence improves, and we get more comfortable with things being okay instead of unassailable. But it can be a rough road until then. reply moqmar 5 hours agorootparentYeah, and out of that fear, people often use stacks that require vast amounts of knowledge to actually keep things working at all, at any scale. Kubernetes is the best example where I don't trust me to keep the wheels on because it's scalable. reply userbinator 9 hours agoprevIn the last couple of months I checked dozens of implementations and, somehow, these protections were not implemented (or implemented incorrectly) even in major HTTP/2 servers I'll speak to the elephant in the room: this is what happens when you have an entire developer culture so used to automatically dynamically expanding everything and not caring how big it is, that they never think about how big something can be. This class of problems isn't necessarily restricted to HTTP/2, although its gross complexity probably contributes; it's just that in HTTP/1.x times, more developers would be used to languages like C where managing buffer lengths takes constant attention, and no one would bother to make header allocations expand limitlessly when they should be a few K in total at most for the whole request. reply 10000truths 6 hours agoparentThe issue is that people constantly focus on and optimize for the happy path, but don't stop and think about what would happen if an adversary deliberately and repeatedly triggered the worst case scenario. So many denial-of-service attacks (slowloris, query parameter hash collisions, etc.) come into fruition because bounded resource usage is an afterthought. reply nullindividual 17 hours agoprevPrevious article with impacted web servers/reverse proxies from the same author. https://nowotarski.info/http2-continuation-flood/ reply jagger27 15 hours agoprev> NOT affected: Nginx, Jetty, HAProxy, NetScaler, Varnish. [0] 0: https://nowotarski.info/http2-continuation-flood/ reply bklyn11201 13 hours agoparentWhat about Caddy? It's a great project that deserves it's own line ;) reply programd 13 hours agorootparentOn Ubuntu 22.04 LTS caddy from the Ubuntu apt repo is shown as on version 2.7.6 and built with Go 1.21.5. That version of Go does not have a fix for this issue. Caddy 2.7.6 is also the latest version released on GitHub. So no fix yet, but I think all that's needed is a recompile with the latest version of Go 1.22.2 reply nelse 12 hours agorootparentI think that recompiling with upgraded Go will not solve the issue. It seems Caddy imports `golang.org/x/net/http2` and pins it to v0.22.0 which is vulnerable: https://github.com/caddyserver/caddy/issues/6219#issuecommen.... reply thegeekpirate 11 hours agorootparentLooks like it's been fixed if you recompile from master as of a few minutes ago reply unethical_ban 12 hours agoprevThis has been at the top all day. I wonder: For low-traffic websites, is it possible that running HTTP/1.1 is just safer? reply nirui 7 hours agoparentHTTP/1.1 is far easier to implement, thus it is reasonable to assume it should contain fewer bugs. HTTP/2 (and HTTP/3) is vastly different in features (added multiplexing, windowing, HPACK etc). All this transforms a largely stateless connection (in the HTTP/1.1 case) to a stateful one. And in order to maintain the stateful connection, you need to store some data (state, configuration etc), thus all these problems. Also, in HTTP/2, since multiplexing is added, the protection characteristics are different. For example, if the connections were generated by CDN source draws, you may just allow fewer number of connections each with a large pool of multiplex channels, but if the connections were from direct user access, you may then want to allow large number of connections, but each with fewer number of multiplex channels. In HTTP/1, protection is much simpler, since everybody looked almost the same. reply akira2501 11 hours agoparentprevUpgrading merely to upgrade is not good engineering practice. If you expect to receive no additional benefits from the upgrade then it is probably not justified. reply userbinator 10 hours agorootparentUnfortunately, there's also a lot of parroting that upgrading is a \"best\" practice. reply adra 4 hours agorootparentCOBOL was great until it wasn't. reply citrin_ru 3 hours agorootparentWould not say that COBOL is still great (never wrote it) but the main problems in COBOL horror stories usually mismanagement and underinvestment, not the technology. reply dexwiz 11 hours agoparentprevProbably. HTTP/2 is good for streaming, and even that is being replaced by newer protocols. For normal asset serving the only advantage is more assets can be loaded in parallel since HTTP/1 is limited on connections per domain. CDNs on different domains usually prevent this from being an issue. In theory you could serve unbundled JS assets via HTTP/2, but I have never seen it in production. Likely because you still need a compilation step most of the time. reply graemep 11 hours agoparentprevI was wondering about that. It is more mature an less complex so it seems probable it is safer. reply skywhopper 17 hours agoprevNice writeup and great find! Kudos to the author for taking such a broad approach and responsibly reporting their findings and finally for sharing the details in such a readable way. reply pmlnr 16 hours agoprevNow do this slowly, and you can call it slowloris v2 :( reply tzot 17 hours agoprevI just love this typo: > After serveral retries reply shuntress 15 hours agoprev [–] HTTP/2 or How to Cram a Transport Layer \"Upgrade\" Into an Application Layer Protocol. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The HTTP/2 CONTINUATION Flood is a critical vulnerability in the HTTP/2 protocol, leading to server crashes and performance issues due to an overflow of headers.",
      "Improper protection implementations in major HTTP/2 servers allow attacks to remain unnoticed, potentially causing CPU and memory crashes.",
      "Real-world exploits in Golang and Node.js have disrupted internet services, prompting collaborations with tech giants and open source projects to mitigate the broad impact of this vulnerability."
    ],
    "commentSummary": [
      "The article covers the Continuation Flood issue in HTTP/2 and its effects on web servers and reverse proxies.",
      "Discussions in the comments emphasize the importance of avoiding over-engineering, scalability concerns, and the significance of secure coding practices for developers.",
      "Users debate the impact on servers like Caddy, the differences between HTTP/1.1 and HTTP/2, and the complexities and challenges of HTTP/2 regarding scalability and resource utilization, along with solutions for vulnerable servers."
    ],
    "points": 246,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1712241170
  },
  {
    "id": 39933076,
    "title": "JetMoE-8B: Cost-Effective AI Model Outperforms LLaMA2",
    "originLink": "https://research.myshell.ai/jetmoe",
    "originBody": "JetMoE: Reaching LLaMA2 Performance with 0.1M Dollars Key Messages JetMoE-8B is trained with less than $ 0.1 million1 cost but outperforms LLaMA2-7B from Meta AI, who has multi-billion-dollar training resources. LLM training can be much cheaper than people generally thought. JetMoE-8B is very open and academia-friendly because: It only uses public datasets for training, and the code is open-sourced. No proprietary resource is needed. It can be finetuned with very limited compute budget (e.g., consumer-grade GPU) that most labs can afford. JetMoE-8B only has 2.2B active parameters during inference, which drastically lowers the computational cost. Compared to a model with similar inference computation, like Gemma-2B, JetMoE-8B achieves constantly better performance. 1 We used a 96×H100 GPU cluster for 2 weeks, which cost ~$0.08 million. Github: https://github.com/myshell-ai/JetMoE HuggingFace: https://huggingface.co/jetmoe/jetmoe-8b Chat Demo on Lepton AI: https://www.lepton.ai/playground/chat?model=jetmoe-8b-chat Authors The project is contributed by Yikang Shen, Zhen Guo, Tianle Cai and Zengyi Qin. For technical inquiries, please contact Yikang Shen. For media and collaboration inquiries, please contact Zengyi Qin. Collaboration If you have great ideas but need more resources (GPU, data, funding, etc.), welcome to contact Zengyi Qin. We are open to collaborations and are actively supporting high-quality open-source projects. Benchmarks We use the same evaluation methodology as in the Open LLM leaderboard. For MBPP code benchmark, we use the same evaluation methodology as in the LLaMA2 and Deepseek-MoE paper. The results are shown below: Model Active Params Training Tokens MBPP Open LLM Leaderboard Average ARC Hellaswag MMLU TruthfulQA WinoGrande GSM 8K Gemma-2B 2B 2T 28.0 46.4 48.4 71.8 41.8 33.1 66.3 16.9 DeepseekMoE-16B 2.8B 2T 34.0 51.1 53.2 79.8 46.3 36.1 73.7 17.3 LLaMA2-7B 7B 2T 20.8 51.0 53.1 78.6 46.9 38.8 74.0 14.5 LLaMA-13B 13B 1T 22.0 51.4 56.2 80.9 47.7 39.5 76.2 7.6 JetMoE-8B 2.2B 1.25T 34.2 53.0 48.7 80.5 49.2 41.7 70.2 27.8 Model MT-Bench Score GPT-4 9.014 GPT-3.5-turbo 7.995 Claude-v1 7.923 JetMoE-8B-chat 6.681 Llama-2-13b-chat 6.650 Vicuna-13b-v1.3 6.413 Wizardlm-13b 6.353 Llama-2-7b-chat 6.269 To our surprise, despite the lower training cost and computation, JetMoE-8B performs even better than LLaMA2-7B, LLaMA-13B, and DeepseekMoE-16B. Compared to a model with similar training and inference computation, like Gemma-2B, JetMoE-8B achieves better performance. Model Details JetMoE uses a sparsely activated architecture inspired by ModuleFormer. JetMoE-8B has 24 blocks. Each block has two MoE layers: Mixture of Attention heads (MoA) and Mixture of MLP Experts (MoE). Each MoA and MoE layer has 8 expert, and 2 experts are activated for each input token. It has 8 billion parameters in total and 2.2B active parameters. JetMoE-8B is trained on 1.25T tokens from publicly available datasets, with a learning rate of 5.0 x 10-4 and a global batch-size of 4M tokens. Training Details Our training recipe follows the MiniCPM's two-phases training method. Phase 1 uses a constant learning rate with linear warmup and is trained on 1 trillion tokens from large-scale open-source pretraining datasets, including RefinedWeb, Pile, Github data, etc. Phase 2 uses exponential learning rate decay and is trained on 250 billion tokens from phase 1 datasets and extra high-quality open-source datasets. We used a 96×H100 GPU cluster for 2 weeks to train the model. Technical Report For more technical details, please refer to the JetMoE Technical Report (Coming Soon). Acknowledgement We express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data mixture. We also express our gratitude to Exabits for their assistance in setting up the GPU clusters, and to Lepton AI for their support in setting up the chat demo.",
    "commentLink": "https://news.ycombinator.com/item?id=39933076",
    "commentBody": "JetMoE: Reaching LLaMA2 performance with 0.1M dollars (myshell.ai)231 points by gyre007 16 hours agohidepastfavorite82 comments lolinder 15 hours ago> JetMoE-8B is trained with less than $ 0.1 million1 cost but outperforms LLaMA2-7B from Meta AI, who has multi-billion-dollar training resources. LLM training can be much cheaper than people generally thought. They want you to read this as \"we spent $100k compared to Meta's spending billions\", but that's not actually what this says. It says that they spent $100k and Meta has the resources to spend billions if they wanted to. We don't know what Facebook spent on training LLaMA 2, but they say that it took them 184320 A100-80GB GPU-hours to train the 7B model [0]. AWS charges $14.46/hour for an instance that has 8 of those [1], which amounts to $1.81/GPU/hr. At that rate and assuming they paid something resembling AWS's list price, LLaMA 2 7B cost ~$333k. That's more than $100k, but not by orders of magnitude, and it's likely that Facebook wasn't paying the full price AWS is charging today. [0] https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md#... [1] https://aws.amazon.com/ec2/instance-types/p4/ reply DalasNoin 14 hours agoparentThis entire difference can be explained due to their double mixture of experts architecture. So only 1/4 MLP and attention blocks are used at any time. Maybe this should be the headline, Moe reduces compute by a factor of 4 without losing accuracy. But this is already known. Still interesting to see a smaller Moe model. This could be the ideal size for many local applications. reply Centigonal 11 hours agorootparentMoE reduces compute cost for inference at scale, but not for training. You still have to train the whole model (plus the router) reply Me1000 6 hours agorootparentIt’s absolutely beneficial when training because the forward pass and back propagation is still only on the neurons that were activated. The Mistral guys specifically mention that training speed (due to not needing as much compute) was one of the reasons Mixtral was released so soon after Mistal 7b. reply rileyphone 10 hours agorootparentprevWith an MoE you only need to train a smaller model which you can then combine into an x8 and finetune/train the router. Mistral used their 7B base to make Mixtral, Qwen's new MoE uses their 1.8B model upscaled to 2.7B, pretty sure Grok also trained a smaller model first. reply phree_radical 8 hours agorootparentVery incorrect! The \"8x7b\" in the name regularly confuses people into some similar conclusion, but there are not eight 7b \"experts\" in Mixtral 8x. It's more apt to think of all 256 FFN's as the \"experts,\" as each expert FFN on a given layer has no relation to the expert FFN's on other layers. You need to train them all within the MoE architecture, as combining existing models (\"clown car MoE\") works, but isn't gaining anything from the architecture/sparsity reply epups 1 hour agorootparentSorry, could you expand on this a bit further? Are you saying that for a MoE, you want to train the exact same model, and then just finetune the feed forward networks differently for each of them? And you're saying that separately training 8 different models would not be efficient - do we have evidence for that? reply samus 2 hours agorootparentprevYou're only correct about Qwen's MoE. I presume that Chinese model builders feel more pressure to be efficient about using their GPU time because of sanctions. reply smusamashah 11 hours agorootparentprevIf MoEs are that good, we know GPT-4 is, than why not train very specific MoEs. One part of MoE could be a perfect Math model which can actually calculate 2+2. Wouldn't models like these be better in general? reply refulgentis 11 hours agorootparentKeeping it short: \"Not even wrong\", in the Pauli sense. - People hear \"mixture of experts\" and they think \"N specialists\" - but ex. think how much know you need to know to autocomplete \"Two plus two is \" - Fundamental thing of ML is you define functions and give it data, and the more data you give it to the better. Once youre at \"I will simply give it the training data needed to be good enough at the task and wall off that part of the implementation\" you're outside ML and have a chicken and egg problem - We don't know GPT-4 is MoE - MoE in practice is fundamentally about trading off runtime vs. static size properties to gain inference speed. I.e. 7x8 stored and picking 7x2 at runtime means youre somewhere between 7x2 and 7x3 in quality, inference at 7x2 speed, and have to train and store and load 7x8. You don't reach for it to increase quality, you reach for it to increase inference speed at the expense of inference ram and total model size. reply viraptor 8 hours agorootparent> We don't know GPT-4 is MoE Didn't Yampleg's tweet / leak confirm this one? I mean, he could be wrong about this, but I thought the consensus was on it being true by now. (Copy of the removed tweets at https://www.reddit.com/r/mlscaling/comments/14wcy7m/gpt4s_de... ) reply refulgentis 7 hours agorootparentIt's just a dude retweeting a substack. I wouldn't bet against it* but I wouldn't bet on it either. His tweet would have just linked to the article in the top comment. * I used to crusade against this rumor because the only source is that article, and people repeating that source. But I imagine it's a no-brainer given they have enough users that they essentially get a throughput bump 'for free' even if the model weights are huge, i.e. better to utilize as much GPU ram as you can muster, the cost of needing more GPU ram is offset by the cost of being able to run multiple inference against the model all the time anyway reply brianarbuckle 5 hours agorootparentprevAgreed. In inference, I could even imagine a 4-bit conversion for certain edge devices. reply nuz 14 hours agoparentprevMeta has their own data centers so they definitely didn't pay the equivalent to what AWS costs reply freedomben 14 hours agorootparentGood point, although it's possible that with the extreme price of GPUs that it cost more to train by buying hardware than it would to rent. For example it might take two to three years before the GPUs are paid for by customers. reply greenavocado 14 hours agorootparentLinux reserved cost of p3.16xlarge is $146,362.0800 annually. On-demand cost is $214,444.8000 annually I am pretty damn sure I could build a 8 GPU Intel Xeon E5-2686 v4 (Broadwell) (that's what Amazon uses - it's $30 to $75 on eBay) server for less than that and come out ahead on electricity even at full throttle. RTX 4090 are just under $2000 each on eBay. 8 GPU × $2000 (RTX 4090) + $1000 (for the rest of the computer) = $17,000 If pulling 2kW continuously at 15 cents per kW*hr for 1 year that's 2000 watts × 365 days × (0.15/(kW×hr)) or $2,628 In total the computer will cost $19,628 if you throw it in the dumpster at the end of each calendar year of using it. If you stack internet cost of $200 a month on top, that's $2400 a year, which raises your annual cost to: $22,028 This is still $124,334 cheaper per year than one AWS 8-GPU server if you fully depreciate your own hardware at the end of year 1 to $0. I could hire an engineer in America to babysit it with the money left over. reply OtherShrezzing 12 hours agorootparentAre consumer grade RTX 4090 cards going to be suitable for running full tilt 24/7 for a year? Those things are fine to stress on the latest game for a few hours at a time, but would probably cause some defects from significant heat stress after just a few days at 100%. This is inconsequential when you're playing Overwatch for a few hours a night and a frame drops now and again. If you're training an iteratively developed LLM though, physical defects could propagate into huge deficiencies in the final model. reply oceanplexian 12 hours agorootparentYep absolutely, crypto miners have been doing it for years. I still think it would be impractical at scale because they are so much more hot and power hungry than the datacenter cards, and you would be lucky to score one or two if you’re on a wait list. reply eek2121 10 hours agorootparentExcept you can absolutely obtain 4090s today, while enterprise hardware is (was? haven't looked at the data) recently, which is the exact opposite scenario you mentioned. I'm actually really surprised that you can still buy 4090s for under $2,000 (cheapest available I saw was $1,800 new and I only took 30 seconds to look), but you can usually sell certain models for quite a bit more. For example, my used 4090 FE is currently worth more than I paid for it. I've played with AI, and while admittedly I've not done anything super serious, I can tell you that both the 3090 and 4090 are more than capable of performing. Tie them with a power efficient AMD CPU and you have something that can be competitive with enterprise (somewhat). I've seen the pricing of \"cloud\" offerings and I've toyed with the idea of creating an \"AI Cloud\" because I have access to really fast internet and super cheap electricity, but I haven't executed because I'm most certainly not a salesperson. I do, however, know enough about marketing that one should not target price, so there is that... reply pizza 9 hours agorootparentprevYou could under-volt or watt-limit a bit and lose just a fraction of FLOPS for much less heat/power though, depending on the workload reply jsight 12 hours agorootparentprevI don't think they'd become a fire hazard, but it is true that one would likely pick something else for this application. Having said that, switching to something like the Tesla V100-SXM2-16GB wouldn't cost that much more. TBH, I'm shocked at how many people treat Amazon as the first choice for this stuff. Much of it isn't even what most would consider a \"production\" workload. You are paying for a lot of enterprise-readiness that you don't need for training. reply robrenaud 11 hours agorootparentIf you wanted to finetune a Mixtral 8x7B, what would you use? reply jsight 9 hours agorootparentGiven the relative availability, I'd probably try to do it with a couple of rtx4090s on tensordock. reply pdntspa 4 hours agorootparentprev> TBH, I'm shocked at how many people treat Amazon as the first choice for this stuff You can thank Amazon's legions of salespeople for that, particularly the end of year junket in Las Vegas where attendees are so pampered that about the only thing they won't do is suck your dick Oh, yeah, they'll also yell at you on stage if you complain about their UI reply vidarh 11 hours agorootparentprevThough this comparison is really only relevant for a couple of machines. Beyond that, at this cost, if you pay AWS list prices \"at scale\" you're doing something very wrong. Don't get me wrong - I've frequently argued that AWS is price gouging and relying on peoples lack of understanding of how the devops costs of running your own works out, but it doesn't take a huge budget before this calculation will look very different (still cheaper to own your own, though). reply ghshephard 13 hours agorootparentprevAn A100-80 GPU goes for about $20K each. reply greenavocado 13 hours agorootparentThe instances in question use Tesla V100-SXM2-16GB reply renewiltord 12 hours agorootparentprevYou can build old Xeon based but only has 40 lane PCIe. For training 8 GPUs how do you push data fast? I’m using 7000 series Epyc for this to get 128 lanes. Have you built this kind of machine? You see good speed with 40 lane? Curious because then I can use old Tyan motherboard which comes in full case with good layout for multi GPU. Epyc based I have to use riser and custom frame which is painful. New Tyan more costly but great case layout. reply samus 2 hours agorootparentprevSince the GPUs can be rented out afterwards, they amortize very quickly with prices in the order of $1/h. reply fleischhauf 14 hours agorootparentprevI think AWS prices scale with hardware price reply packetslave 12 hours agorootparentprevMeta also doesn't pay AWS anywhere near retail price for instances. reply KMnO4 11 hours agorootparentWhy is this the case? Even AWS internal pays the same AWS prices as everyone else reply vidarh 11 hours agorootparentI'm less surprised if AWS internally pays AWS list prices, because that's just internal accounting. From the even relatively small AWS customers I know, none of them needed to get very far into the 6 digits per year spend before a couple of quiet mentions to their account manager that they were reviewing other options was enough to get steep discounts. Add in lots of credits, and if you pay list price, you're being taken to the cleaners.. I've done contract work for clients to be ready to migrate both as part of maximising credits and as part of negotiating posture, and the savings can be enormous (though it'd still usually be cheaper to use managed servers). reply packetslave 6 hours agorootparentprevCouple of reasons: Meta's annual spend with AWS is large enough that they'll have a negotiated blanket discount that takes a fixed percentage off the top of their monthly spend. This is very common for larger AWS customers, not just Meta. For instances specifically, any planned usage will be using either reserved instances or at minimum a compute savings plan (CSP) that drops the hourly rate dramatically in exchange for a committed number of instance hours, with or without an upfront payment. Finally, there may be a negotiated rate for specific instance types built into the contract. Again, common for very large customers. source: I was on one of the cloud-related infrastructure teams (left in early 2022). I have no idea about their spend (or discounts) today, but two years ago it was enough that Andy Jassy would meet 1:1 with Mark to \"discuss the relationship\". reply elcomet 11 hours agorootparentprevThey have their own data centers, they don't use AWS reply packetslave 4 hours agorootparentThey don't use AWS to run the big three apps (FB, Insta, WhatsApp). They very much use them for other things. FB data center machines tend to be highly-specialized and optimized for running the apps, not general-purpose compute. reply dannyw 14 hours agoparentprevAny company as big as Meta have teams working on optimisation (eg optimised kernels), usually with direct engagement with NVIDIA engineers. These kind of things are usually only selectively shared. reply benreesman 11 hours agoparentprevI’ll agree with your general point even though there are some subtleties to FBNY. More important, we let an awful lot of self-promotion from the big guys slide around here. I can live with the guys and gals doing this on a shoestring getting a little of that sweet hype love. This seems pretty legit. reply rvba 2 hours agoparentprevTraining data costs zero? What is the training data anwyay? Books? (Is it legal to use non public domain, wait whom am I kidding) Reddit posts scrapped from the site? (Without clean up there are lots of bad subreddits) Wikipedia? Of course I understand why labour costs + employee cost related costs are ignored. reply samus 1 hour agorootparentIt costs more the more you care about squeaky clean training data. Of course you get a better model in return. ChatGPT used a crawl of the internet and patches things up with alignment and DPO. Big boys like Microsoft might have deals with publishers to get textbooks in bulk. Contents from sites with moderation can be filtered using the platform's mechanism, e.g., only include text with a certain length and count of upvotes. LLMs can be used to generate and filter data as well. Humans have been used to do this, they might have to do this less in the future. Mostly to review what the LLMs are suggesting. reply plufz 15 hours agoprevYou’ve been in tech for too long when 1 million USD is your smallest unit. reply noodlesUK 14 hours agoparentI wonder why they decided to call it 0.1M USD rather than 100k USD. For many of us, a million dollars is a large amount of money, even for a business. reply plufz 14 hours agorootparentI’m sure they had their reasons, but all I can see is the Simpsons meme with Mr Burns at the ATM saying “What’s the smallest amount of money I can think of? A thousand dollars.” ;) reply oceanplexian 12 hours agorootparentprev100k isn’t worth anywhere near what it used to due to inflation. It might get you a nice pickup truck or a kitchen remodel. If your business is doing research and can’t spend then it’s more of a hobby than a business. reply IshKebab 12 hours agorootparentprevIt's to imply that it costs other people in the millions, but they did it for only 0.1 million, which is a small number of millions. Just a rhetorical trick. reply mattlondon 14 hours agorootparentprevSame reason things are x.99 - the 0.1 decimal \"feels\" smaller than seeing 100,000 - \"holy fuck!\" Etc reply uptownfunk 13 hours agoparentprevWell, it's interesting to think about how much has been invested into BigModel companies (Anthropic, Perplexity, OpenAI) when it's very rapidly becoming commoditized. reply antimatter15 14 hours agoprevIt looks like Llama 2 7B took 184,320 A100-80GB GPU-hours to train[1]. This one says it used a 96×H100 GPU cluster for 2 weeks, for 32,256 hours. That's 17.5% of the number of hours, but H100s are faster than A100s [2] and FP16/bfloat16 performance is ~3x better. If they had tried to replicate Llama 2 identically with their hardware setup, it'd cost a little bit less than twice their MoE model. [1] https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md#... [2] https://blog.ori.co/choosing-between-nvidia-h100-vs-a100-per... reply anizan 14 hours agoparentThey mention the cost was ~80,000k USD so for 32,256 hours it comes to ~2.48$ an hour. Amazing how cost effective the compute actually is. reply p1esk 10 hours agorootparentI was paying $1.1 for A100 hour more than a year ago. $2.48 is crazy expensive. reply anizan 9 hours agorootparentIt was for a 96 X H100 cluster. Their provider was exabits.ai which bills itself as a decentralised computing marketplace. reply vertis 13 hours agoprevIt might be equivalent to LLaMA2 but it's still not capable of even simple reasoning: > If two apples cost 2 dollars and 1 apple costs 1.20, what is the discount you're getting > To calculate the discount, you need to compare the original price of the apples to the price after the discount. > The original price for two apples is: 2 apples * $1.20/apple = $2.40 > The price for one apple after the discount is: 1 apple * $2.00/apple = $2.00 > Now, let's calculate the discount for one apple: > Original price for one apple - Discounted price for one apple = Discount amount > $2.40 - $2.00 = $0.40 > The discount for one apple is $0.40. Since you're buying two apples, you'll get the discount on both: > Discount amount for two apples = 2 * $0.40 = $0.80 > So, the discount you're getting for two apples is $0.80. reply ravetcofx 9 hours agoparentI'm kind of impressed it was able to do basic math even if the reasoning isn't correct. That seems like an impressive emergent behavior for a small cheap model like this. reply vertis 1 hour agorootparentLlama2:7b makes the same mistakes. It's not until you use something like Mixtral or Llama2:13b that it actually gets the correct results (in my one example). Interestingly Llama2:13b objects that there is no discount until I clarify: \"the discount you're getting [with 2 apples]\" It's not just math though it's any kind of complex reasoning and ambiguity. Comparing to humans is always complex, but humans for the most part wouldn't balk at me asking what discount you're getting without specifying that it's the 2 apples that have the discount in this example. A more advanced model often states the assumptions. There are lots of nuances in this question as well. I'm still paying 80c more than buying one apple, so I should only buy two apples if I would use two apples. reply moffkalast 11 hours agoparentprevThe simple fact that they did not list the current SOTA for the size class in their comparison table tells you all you need to know about their confidence. And listing Gemma-2B is like shooting fish in a barrel, might as well also put RedPajama on there. It's good to see MoE being attempted at the smaller sizes, and it may scale well downwards as well given their results. But regardless, 1.25T is very little training data compared to the 6T that Mistral 7B received and even that makes it barely usable and likely not yet saturated. Before it, the sub-13B size class was considered basically an academic exercise. reply jsight 12 hours agoparentprevI wonder if the reasoning capability of a moe model is limited by the max size of the individual experts? reply kleiba 14 hours agoprevI've been out of academia for a bit, but in my day 100k USD would not have been considered academia-friendly in my neck of the woods... reply dheera 13 hours agoparentThat's about the cost of 1 grad student year including all overhead, I believe. It's definitely far less than what many physics and biology labs spend on equipment in a year. I mean, you're an idiot of a PI if you have $500K/year of grants and spend it on 5 students and no compute. reply operator-name 13 hours agoprevSadly no matter what kind of prompting that I try it fails the sisters question: > Sally (a girl) has three brothers. Each of her brothers has two sisters. How many sisters does Sally have? Sally (a girl) has three brothers. Each of her brothers has two sisters. How many sisters does Sally have? Sally has two sisters. Here's the reasoning: Sally has three brothers. Each of her brothers has two sisters. Since her brothers are the ones who have two sisters each, and we know there are three brothers, we can conclude that each brother has two sisters. Therefore, there are two sisters in total (one for each brother). So, Sally has two sisters. https://www.lepton.ai/playground/chat?model=jetmoe-8b-chat reply moffkalast 11 hours agoparentNot exactly something you'd expect a model of this size to solve, Mixtral fails it too (if you switch the names/genders that is, since it's contaminated with a few versions). It does at least indicate that their training data might indeed be as clean as they say. reply vertis 1 hour agorootparentMore complex models fail this question, so it would be exceptional if it passed it: ollama run llama2:13b >>> Sally (a girl) has three brothers. Each of her brothers has two sisters. How many sisters does Sally have? This is a classic lateral thinking puzzle that requires you to think outside the box and challenge your assumptions. The answer is not what you might expect at first glance. To solve this puzzle, we need to break it down step by step: 1. Sally has three brothers. 2. Each of her brothers has two sisters. So, let's count the number of sisters that each of Sally's brothers has: Brother 1: 2 sisters Brother 2: 2 sisters Brother 3: 2 sisters Now, let's add up all the sisters that Sally has: Sister 1 + Sister 2 + Sister 3 + Sister 4 + Sister 5 = 2 + 2 + 2 + 2 + 2 = 8 Therefore, Sally has 8 sisters. reply patrick-fitz 12 hours agoprevOut of curiosity, looking at the cheapest price for a H100 that I could find online. Lambda Reserved Cloud [1] starts at $1.89 per H100 per hour. It could be possible to get the cost down to a lower amount: $1.89 * 96GPUs * 24hours * 14days = ~$61k 1 - https://lambdalabs.com/deep-learning/servers/hyperplane reply avrionov 11 hours agoparentThis is the price of training if nothing fails. reply logicchains 3 hours agoparentprevIt also depends on the interconnect speed. If you don't have fast enough interconnect between the machines, you won't get linear speedup with N gpus. reply ein0p 14 hours agoprevAt $DAY_JOB nowadays we run 128x H100 runs without thinking twice nowadays. Only takes a few days to train a small-ish LLM with that to test out some ideas. reply moffkalast 11 hours agoparentecho $DAY_JOB reply agravier 5 hours agorootparentAlso `echo $NIGHT_JOB`, I'm curious like that. To preempt other queries, maybe just paste the `set` output. reply uptownfunk 13 hours agoparentprevWhere are they hosted? reply ein0p 13 hours agorootparentAWS and GCP both. reply turnsout 13 hours agorootparentOut of curiosity, what leads you to train models from the ground up rather than fine tuning existing models? reply ein0p 13 hours agorootparentWe do both. You can’t just fine tune if you’re trying a different model architecture, or even change some of the hyperparameters on an existing one. Every now and again you might be able to reuse some of the weights, but that’s about it. That’s part of the reason research is so incredibly expensive and time consuming in this field. I bet that $80k is only a fraction of the overall cost for the model described in the article, too. reply davidcollantes 14 hours agoprevCan't wait for the GGUF to play with it. I tried the demo (https://www.lepton.ai/playground/chat?model=jetmoe-8b-chat), and the results were very good! reply tosh 14 hours agoprevAnyone got a ballpark figure for what Meta spent on Llama 2 training for the 7B model? reply helloericsf 15 hours agoprevX thread: https://x.com/qinzytech/status/1775916338822709755?s=20 reply throwitaway222 15 hours agoprevThis stuff is just going to keep getting pushed down. reply YetAnotherNick 14 hours agoprev> It only uses public datasets for training, and the code is open-sourced Looking at the repo, there is no training or data processing code. reply jsight 12 hours agoparentIt was trained with \"1 trillion tokens from large-scale open-source pretraining datasets, including RefinedWeb, Pile, Github data, etc.\" I guess it is good that they mentioned some of it, but yeah, that isn't exceptionally helpful when making claims of it being 100% open source. I'm not sure why they feel the need to be so secretive if all of the sources are open. reply YetAnotherNick 4 hours agorootparent\"etc.\" is the most important part here. There is NL SFT and code SFT data which guessing by the names are instruction data very likely from GPT-4. It is known in finetuning community that training with GPT-4 data is the easiest way of improving the model. If that's the case base JetMoE should be compared to finetuned llama, not base llama. reply gavin_guo 8 hours agoprevmodel https://huggingface.co/jetmoe/jetmoe-8b reply ipsum2 15 hours agoprevI'm skeptical, expect data contamination was the reason for high benchmark scores. reply hiddencost 14 hours agoparentYeah. IBM especially has a history of fudging the numbers on reports like this. Research puts together reports which are aggressively p-hacked and ensembled and overfit, and then sales uses those reports to boondoggle clients into using IBM. reply barkingcat 14 hours agoprev [–] This kind of assumption is super deceptive. The Facebook budget includes money to pay off people they've ripped off (in private settlements) and money for lawyers to shield the developers so they can feel free to rip off copyrighted content without having to pay personal penalty or be imprisoned for infringement. It also includes the price of buying lobbyists to alter laws to let this practice continue. Also, unless the authors work inside Facebook, they have no idea how much Facebook spent on training that model specifically. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The JetMoE-8B model, costing less than $0.1 million and trained with public datasets and consumer-grade GPUs, outperforms the multi-billion-dollar LLaMA2-7B model from Meta AI.",
      "JetMoE-8B, academia-friendly and open-source, boasts 2.2 billion active parameters during inference and was trained on 1.25 trillion tokens using a GPU cluster over two weeks.",
      "Detailed technical report and benchmarks for JetMoE-8B are available on their Github and HuggingFace pages, welcoming collaboration opportunities with provided contact information."
    ],
    "commentSummary": [
      "The JetMoE model, costing less than $0.1 million to train, outperforms the LLaMA2 model from Meta AI despite using fewer resources.",
      "JetMoE's innovative double mixture of experts design cuts compute costs while maintaining high accuracy.",
      "The discussion also covers topics like the efficiency of \"clown car\" models, comparing AWS server costs with custom hardware, GPU options for AI, multiple GPU training machines, and challenges in building and training BigModel companies."
    ],
    "points": 231,
    "commentCount": 82,
    "retryCount": 0,
    "time": 1712250261
  },
  {
    "id": 39934696,
    "title": "Beware: Fake AI Law Firms Use DMCA Threats for SEO Boost",
    "originLink": "https://arstechnica.com/gadgets/2024/04/fake-ai-law-firms-are-sending-fake-dmca-threats-to-generate-fake-seo-gains/",
    "originBody": "Dewey Fakum & Howe, LLP — Fake AI law firms are sending fake DMCA threats to generate fake SEO gains How one journalist found himself targeted by generative AI over a keyfob photo. Kevin Purdy - Updated 4/4/2024, 6:50 PM Enlarge / A person made of many parts, similar to the attorney who handles both severe criminal law and copyright takedowns for an Arizona law firm. Getty Images reader comments 65 If you run a personal or hobby website, getting a copyright notice from a law firm about an image on your site can trigger some fast-acting panic. As someone who has paid to settle a news service-licensing issue before, I can empathize with anybody who wants to make this kind of thing go away. Which is why a new kind of angle-on-an-angle scheme can seem both obvious to spot and likely effective. Ernie Smith, the prolific, ever-curious writer behind the newsletter Tedium, received a \"DMCA Copyright Infringement Notice\" in late March from \"Commonwealth Legal,\" representing the \"Intellectual Property division\" of Tech4Gods. The issue was with a photo of a keyfob from legitimate photo service Unsplash used in service of a post about a strange Uber ride Smith once took. As Smith detailed in a Mastodon thread, the purported firm needed him to \"add a credit to our client immediately\" through a link to Tech4Gods, and said it should be \"addressed in the next five business days.\" Removing the image \"does not conclude the matter,\" and should Smith not have taken action, the putative firm would have to \"activate\" its case, relying on DMCA 512(c) (which, in many readings, actually does grant relief should a website owner, unaware of infringing material, \"act expeditiously to remove\" said material). The email unhelpfully points to the main page of the Internet Archive so that Smith might review \"past usage records.\" A slice of the website for Commonwealth Legal Services, with every word of that phrase, including \"for,\" called into question. Commonwealth Legal Services There are quite a few issues with Commonwealth Legal's request, as detailed by Smith and 404 Media. Chief among them is that Commonwealth Legal, a firm theoretically based in Arizona (which is not a commonwealth), almost certainly does not exist. Despite the 2018 copyright displayed on the site, the firm's website domain was seemingly registered on March 1, 2024, with a Canadian IP location. The address on the firm's site leads to a location that, to say the least, does not match the \"fourth floor\" indicated on the website. Advertisement While the law firm's website is stuffed full of stock images, so are many websites for professional services. The real tell is the site's list of attorneys, most of which, as 404 Media puts it, have \"vacant, thousand-yard stares\" common to AI-generated faces. AI detection firm Reality Defender told 404 Media that his service spotted AI generation in every attorneys' image, \"most likely by a Generative Adversarial Network (GAN) model.\" Then there are the attorneys' bios, which offer surface-level competence underpinned by bizarre setups. Five of the 12 supposedly come from acclaimed law schools at Harvard, Yale, Stanford, and University of Chicago. The other seven seem to have graduated from the top five results you might get for \"Arizona Law School.\" Sarah Walker has a practice based on \"Copyright Violation and Judicial Criminal Proceedings,\" a quite uncommon pairing. Sometimes she is \"upholding the rights of artists,\" but she can also \"handle high-stakes criminal cases.\" Walker, it seems, couldn't pick just one track at Yale Law School. Why would someone go to the trouble of making a law firm out of NameCheap, stock art, and AI images (and seemingly copy) to send quasi-legal demands to site owners? Backlinks, that's why. Backlinks are links from a site that Google (or others, but almost always Google) holds in high esteem to a site trying to rank up. Whether spammed, traded, generated, or demanded through a fake firm, backlinks power the search engine optimization (SEO) gray, to very dark gray, market. For all their touted algorithmic (and now AI) prowess, search engines have always had a hard time gauging backlink quality and context, so some site owners still buy backlinks. The owner of Tech4Gods told 404 Media's Jason Koebler that he did buy backlinks for his gadget review site (with \"AI writing assistants\"). He disclaimed owning the disputed image or any images and made vague suggestions that a disgruntled former contractor may be trying to poison his ranking with spam links. Asked by Ars if he had heard back from \"Commonwealth Legal\" now that five business days were up, Ernie Smith tells Ars: \"No, alas.\" This post was updated at 4:50 p.m. Eastern to include Ernie Smith's response. reader comments 65 Kevin Purdy Kevin is a senior technology reporter at Ars Technica, covering a variety of technology topics and reviewing products. He started his writing career as a newspaper reporter, covering business, crime, and other topics. He has written about technology and computing for more than 15 years. Advertisement Channel Ars Technica ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=39934696",
    "commentBody": "Fake AI law firms are sending fake DMCA threats to generate fake SEO gains (arstechnica.com)210 points by rntn 14 hours agohidepastfavorite100 comments gentleman11 13 hours agoThe root problem is that the legal system is completely inaccessible and non-existent to individuals or small companies. Even to medium companies. Only mega corps and law firms can afford to use the legal system. Even big tech often avoids it because of the expense, but if they do use it, they’re going to win, 99.9% of the time because of their war chests So, people are semi defenceless when these things come up, and you can really abuse people in pretty epic and unending ways reply antonvs 12 hours agoparent> completely inaccessible and non-existent to individuals or small companies. Even to medium companies. Only mega corps and law firms can afford to use the legal system. For individuals you have a point, but for small and medium companies this seems like an exaggeration. I've worked for several startups. The smallest was one that I co-founded with no outside funding - we maxed out at five employees, three of whom were co-founders, but nevertheless we registered and successfully defended trademarks and copyrights. That business ran until its main product became obsolete, for about 5 years. The funded startups I've worked for all used the legal system around IP in various ways: trademarks, patents, copyright. One of these bogus DMCA claims would be easily and cheaply dismissed by any competent lawyer. And really, probably doesn't even require a lawyer unless a hosting company acts on it and doesn't listen to any objections. reply spxneo 8 hours agorootparentGP is talking about cost-prohibitiveness of legal counsel a large company can absolutely drain your small/medium business with frivolous lawsuits. the point is that Western judicial system largely caters to capital not justice. reply ahartmetz 1 hour agorootparentIt's mostly just the US judicial system. The US has about 3x the amount of lawyers per capita compared to other Western nations. The glorification of lawyers in US media is sometimes a bit sickening as a non-US citizen. reply vkou 2 hours agorootparentprev> the point is that Western judicial system largely caters to capital not justice. The problem is that there is no way to make it 'fair'. All the proposed reforms are also incredibly unfair, just in a slightly different way from what we have. reply geraldhh 2 hours agorootparentmoney is liquid democracy - kinda sad, but here we are reply concordDance 1 hour agorootparentprevYou can trade off against other forms of fairness. For example, if you remove the power of precedent suddenly knowing and being able to search case law becomes unimportant, giving laymen a huge step up in defending themselves. However it also makes the legal system far more inconsistent across judgements. reply moi2388 1 hour agorootparentNot if the law did what it was supposed to do; state the conditions under which a given sentence ought to be executed. Instead laws are poorly written meaning case law becomes the actual law reply tmpz22 10 hours agorootparentprev> One of these bogus DMCA claims would be easily and cheaply dismissed by any competent lawyer. If you're a new small business owner how do you vet lawyers for competency? If you accidentally engage a less then competent lawyer who screws up, how does that impact the individual and company? reply joecool1029 4 hours agorootparentI've received takedown claims and requests to settle before from some trolls (once by Getty but I didn't handle that one directly). Usually it was old contributed blog posts with an improperly cited image or site theme components we purchased that contained stuff that wasn't properly licensed. We'd take it down and then onto the next step... I made friends with a few local lawyers, explained the situation and asked them to send a response on their letterhead or via email telling them in proper legal language to 'fuck off' (usually they'd mention it was outside statute of limitations and the use was never willful infringement or it was user contributed and we were only hosting the content). There was never a situation where they didn't leave us alone after that. I guess they assume real small companies will either panic and pay to settle or ignore legal summons and lose by default? reply jimz 6 hours agorootparentprevOnly lawyers really know how to vet lawyers for competency, I've found. More people use the quality of the jingle as a guide than any useful way to discern competence, partly because people broadly fail to understand what a lawyer is supposed to do (and this differs depending on context, civil proceedings and criminal proceedings and administrative proceedings concern sometimes vastly disparate interests and in turn, responsibilities and expected outcomes). The best way, generally, is to survey other lawyers in the same field and jurisdiction and cross-reference to see if a firm or solo practitioner gets more recognition from their peers independently, btw. It's how lawyers find representation in fields they have no familiarity with. The jingles at best are entirely irrelevant and may even be an indicator that the firm relies on volume more than it really should. edit: should mention that this generally applies for criminal or administrative cases. The civil arena is far too big and varied for me to suggest a heuristic that can be generalized since it concerns so much work that is wildly different and also, comparatively few court appearances (although I'd likely use the same method, but starting with my classmates, if I need to hire someone for a civil matter, if I have a classmate who practices in the field and in my jurisdiction). reply francisdavey 6 hours agorootparentprevWorking out whether a lawyer is good, and more specifically good for what you want, is difficult even for another lawyer. People often ask me for recommendations of a lawyer outside my field, or indeed in my field if I can't help them because my firm is conflicted or for some other reason. The thing is I don't personally use lawyers, so I have only the vaguest idea. I can tell some things from firms' reputations and their websites, but even for me it is a hard call. And the law firms I have most experience of in practice tend to be the ones which I would go out of my way to disrecommend :-). So I agree, that is a problem. Particularly for litigation. Unless you are doing litigation a lot - which is likely to be bad news for you - you can't get experience of a lawyer and decide that you like them. Word of mouth is also rather less useful. reply jimz 4 hours agorootparentIt's funny that the dynamic seems to be entirely reversed in criminal or collaterally-related-to-criminal matters since I could have told you by the time I graduated (and had been practicing on a limited license for a year) which attorney is more competent at what specific aspect of defense related work, and when I jumped to federal CJA work and then removals which is administrative entirely, it also took very little time to suss out the quality of not only other attorneys working on the defense side but the prosecutors and their proclivities, strengths, and weaknesses. In fact I think I'd be a pretty bad attorney in my field if I didn't pick up these things pretty quickly, and the question of recommendations only happen when there may be a conflict anyway, usually because it's pretty normal for a case to have more than one defendant. The biggest case I worked on had 68 and everybody ended up pleading but even with that, I feel like just seeing how different attorneys tackled the same basic case but from a slightly different angle gave a decent amount of insight, including how quickly they are able to get a plea worked out and how they approached collateral issues that always comes out of massive fishing expeditions. I mean, it's a service industry gig with a fancy title in the end and since law school didn't teach anything about lawyering - even legal writing didn't actually cover any client management and actual courtroom procedure, although the 2 weeks we spent on what was essentially a doxxing exercise was really helpful combined with some programming skills - it feels inevitable that through exposure on a daily basis one would get a feel pretty quick of others in your field. And there's always reputations and word of mouth but I find that to actually be more or less on point when it comes to particular niches, like if you need someone who spoke a particular language or dialect, or someone who understood how to cross examine about technology and not have the jury fall asleep on you, or someone who can handle a child witness and someone who can't. It's not that different in the civil context, is it? There was even a prosecutor who was widely known to be susceptible to running himself into Batson challenges and sure enough the first trial I had against him he managed to dismiss every minority in the pool leaving me and the client the only two in the room which was glaring since the jurors began noticing that something was before we even got there. It was moot since we got a mistrial but if the opposing side is known to be generous with free issues to preserve for appeal you'd be on the lookout, wouldn't you? Or am I still overestimating how often civil attorneys end up in front of a jury broadly? reply francisdavey 2 hours agorootparentOne big difference in England (I'm an English barrister, so that's my jurisdiction) is that most short civil proceedings end up being effectively closed to the public, or at least unlikely to be viewed by other barristers. Longer proceedings will typically be held in larger court centres where you are, again, less likely to see what fellow advocates are up to. You do meet opponents, but in some fields, not often enough to really know. In crime, your client would be in the cells much of the time and you could sit at the back of the court and watch lots of counsel do a good or bad job. Of course even the civil bar do get to know each other in the way that commercial transactional lawyers - which is what I do now - don't have as much opportunity to do. I am typically dealing with commercial organizations all over the world. Too many lawyers to get to know. reply francisdavey 2 hours agorootparentprevI missed saying that in England (1) civil juries are essentially hen's teeth nowadays and (2) you don't tend to cross-over. Either you are in court most of the time, or advising on court proceedings, or you are a litigator (running those proceedings but not going to court) or you are a transactional lawyer who has nothing at all to do with them. reply mistrial9 8 hours agorootparentprevI think you have to factor in your legal jurisdiction, skill level of yourself and close associates, and the network that builds from that, to compare legal costs.. one might say that as an active and functioning startup with employment, you attracted legal talent.. some outsiders, in various jurisdictions, might have some trouble finding legal services and for the right prices. reply ToucanLoucan 10 hours agorootparentprevI've always been a fan of the concept of simply billing the losing litigants the legal expenses of both sides when the case was obviously frivolous. Seems like it would shut this shit right down. reply michaelt 59 minutes agorootparentSounds like a good way to send legal bills into the stratosphere. I get to choose the best and most expensive lawyers and tell them to bill however many hours they like, because that asshole I'm suing is going to have to pay for it? Or I want to keep costs down, but the other guy has hired a team of three $500-an-hour lawyers to bury me in paperwork? And the only way I can avoid losing and having to pay is to respond in kind? Sounds like a recipe for deep-pocked corporations like Disney to win every legal action. reply johndhi 8 hours agorootparentprevThat's how the UK works. But making it that expensive to sue someone means it's harder to file meritorious or important cases too. reply Capricorn2481 10 hours agorootparentprevAren't trademarks and patents exactly the type of thing that would need to be argued in court and therefore cost more money than most startups have? reply johndhi 8 hours agorootparentGenerally yes. reply RobotToaster 7 minutes agoparentprevThat was my thought. A fake DMCA claim is fraud or at least perjury, but trying to actually bring a claim for that will be difficult for most people. reply Cheer2171 10 hours agoparentprevI've sued several businesses and individuals in small claims court (in California) including two multinationals on the S&P 500. I have no formal legal training, only what every open source dev learns through licensing. The court bureaucracy was about as hard for me to learn than learning how to convert an out of state drivers license to my new state. Finding a process server was the hardest part. You write your account of what happened in colloquial English, you attach evidence, you show up before a judge and they decide. You don't actually need a lawyer to respond to bogus charges. Legal trolls make their money assuming most people settle or don't show up. Just submit your evidence, show up in court, and if it is bogus, it will be clear. reply banana_feather 5 hours agorootparent> You don't actually need a lawyer to respond to bogus charges. Small claims court is a completely different beast, it is designed to be navigated without counsel. If you play this way in civil litigation you are likely going to lose before you know what hit you. reply bartonfink 4 hours agorootparentGP did the legal equivalent of pasting a bunch of shit from stack overflow and claiming they're a talented software engineer. reply Animats 2 hours agorootparentprev> Finding a process server was the hardest part. Why? Search brings up lots of them. I once used a process server service called \"Attila the Hun School of Charm\". They are gone, but there's a \"Gotcha Process Servers\" in San Francisco. reply akaru 8 hours agorootparentprevGo on… reply datascienced 11 hours agoparentprevThe legal system is somewhat accessible to businesses and middle class people. Not sure about USA but in Australia there is plenty of things like free legal advice, plain english descriptions of laws including case studies, legal aid and small claims courts and other tribunal/arbitration jurisdictions designed to be used without a lawyer. While more money = better access to law, there is a lot for the non rich! reply johndhi 8 hours agoparentprevAs a lawyer who has worked in and for big tech, we don't win 99% of the time. Far less. There are very well financed players and structural incentive that allow plaintiffs lawyers to win and take lots of money from big tech in the US. reply david_shi 4 hours agoparentprevAgreed. Increasing the predictability and transparency of these systems is crucial for a functional society reply lupire 12 hours agoparentprevThere's no legal threat here. It's all fake. reply datascienced 12 hours agorootparentYou need to pay a lawyer to find that out! reply anonzzzies 3 hours agorootparentDepends on the region. I usually write (or now have gpt write) a letter with a bunch of fines in it back and that almost always shuts them up. I have never complied with any legal actions against me; I have been to court and that was cheap without a lawyer as well. I did nothing wrong, so I am quite confident I will always win; so far going strong for over 30 years since I am an adult and received such nonsense directed at me or my company. I would never live in a country (guess US) where you need a kidney to defend yourself even if you did nothing wrong. reply cmeacham98 9 hours agorootparentprevShouldn't a simple search of the relevant state's bar registry be sufficient to realize the lawyer is fake? I guess you'd have to at least suspect the letter was fake to try that, so you might miss it if that's not even on your radar. reply icedchai 12 hours agorootparentprevSadly, many folks will assume an \"official\" sounding message or call is actually legitimate. Especially if it's from someone calling themselves a lawyer. Next the AI law firms will be requesting payment in iTunes gift cards... reply ryandrake 12 hours agorootparentI think there is a belief (right or wrong) among a lot of lay-people and non-lawyers that the legal system is an opaque minefield of \"gotchas\" where if you don't file form 27B/6 within 30 days, with a specific magical incantation X written on it, and then appear before a judge and speak magical incantation Y, then gotcha! you lose! So any time something looks even remotely official, people panic and wonder which minefield they are going to inadvertently step on, even by ignoring it. reply francisdavey 6 hours agorootparentWhen I used to appear in court a lot, I ran into plenty of intelligent people who managed to mess up badly because they didn't follow simple, clearly advertised, rules. So even though in theory it should be simple to do something, in practice people often get it wrong. i.e. even where it isn't opaque, it is surprising how many people mess up. (Of course that includes lawyers) reply kstrauser 9 hours agorootparentprevIn addition, there’s the knowledge that what happens next is likely to be expensive, no matter how it resolves. A letter from a lawyer? There goes a rent payment. reply jfengel 8 hours agorootparentprevTo be fair, I have heard that from a lot of lawyers. They say that any time you touch the legal system without a lawyer, you are setting yourself up for trouble. This is not just self interest. There really are a lot of hidden traps. Words almost never mean what you think they mean. If they have a lawyer and you don't, you are taking a serious risk. reply pixl97 11 hours agorootparentprevI mean the idea of 'default judgement' isn't something most people want to find out about. This said you typically get some severe and officially delivered warnings on that. reply amenhotep 8 hours agorootparentprevVery good choice of form. reply lulznews 6 hours agoparentprevIndeed, rule of law has been known to be a farce for some time. reply olliej 10 hours agoparentprevThe issue is that defending this generally requires paying a lawyer, and filing fraudulent DMCA claims is free, and comes with no requirement to pay the legal fees of the recipient if the claim is false reply dclowd9901 10 hours agoparentprevYou can chill people, but abuse them? That depends on if you’re actually willing to file an injunction. I see a letter like this, I’ll wait for them to file an injunction. reply celestialcheese 7 hours agoprevThis is so common, and has been for a number of years. As an owner of a website with a lot of content published over the years, a copyright claim pops up with \"settlement\" offers between $200-$1k to make it go away. 50% of the time it's an outright scam like this. 45% it's legit, we pay the settlement, and a writer gets a reminder about citing images and copyright. 5% is the worst, where a photographer will have a open license like CC BY-NC, then a law firm will contact photographers with images licensed like that, and partner with them to send demand letters to businesses improperly using the open license and do the settlement shakedown. While technically in violation because we're a business with ad revenue, it's nefarious because it's exploiting nuances of open licenses and writers on schedule missing the NC part of the license review. We get it right most of the time, but mistakes happen. Feels dirty to leverage CC to make money doing copyright shakedowns. reply duskwuff 4 hours agoparent> 5% is the worst, where a photographer will have a open license like CC BY-NC Doesn't even have to be -NC. Creative Commons licenses require reusers to provide comprehensive[1] attribution when they reuse images. Some older versions of the license, like 2.0, also terminated immediately and permanently upon any breach of the license; copyright trolls have abused this to sue reusers who forgot to include some part of the required attribution. CC 4.0 provides the reuser a 30-day grace period to cure a violation. [1]: \"the name of the creator and attribution parties, a copyright notice, a license notice, a disclaimer notice, and a link to the material\", as well as the title of the work prior to 4.0 -- https://creativecommons.org/licenses/by/4.0/deed.en#ref-appr... reply dawnerd 6 hours agoparentprevAs a counter point feels dirty to abuse photographers not catching NC for you to profit. reply dudus 12 hours agoprevI just signed up for this genAI summit but the lack of social buzz and the profile of the organizers give me a strange feeling. https://genaisummit.ai/#/ I'm afraid this is might be a fake conference created by AI. Am I crazy? I paid $500 for tickets. I almost signed up for the booth. I'll have to start emailing speakers for confirmation because it's not passing the smell test. reply nickff 11 hours agoparentWhy did you sign up for a conference if you’re so uncertain as to its credibility and content? Is this an employer-sponsored outing? reply dudus 5 hours agorootparentI thought it was cheap. It said last tickets. I don't know ... that was stupid. It looked interesting at first. But when I started to research more I didn't find any buzz. I reported the event to eventbrite reply geraldhh 2 hours agorootparentsounds like you're getting cold feet from a sense of guilt for falling pray to fomo. reply antonvs 12 hours agoparentprevIt says \"Sponsored by Microsoft AI Co-Innovation Lab\". You could try emailing them (aiotlabs@microsoft.com, from their website) and asking whether they're really sponsoring that conference. reply StevenHarlow 11 hours agoparentprevhttps://www.palaceoffinearts.org/event/genai-summit Has the event listed reply duskwuff 9 hours agorootparentThe Palace of Fine Arts is a theater, not an exhibit hall. It's a completely unsuitable location for a conference. reply TuringNYC 2 minutes agorootparent>> The Palace of Fine Arts is a theater, not an exhibit hall. It's a completely unsuitable location for a conference. This is not the case -- The Palace of Fine Arts is an excellent conference location - expansive and with mezz levels for private presentations, etc. I've done several conferences there and also presented. It has beautiful outdoor garden spaces for relaxed 1:1s and doesnt feel at all like SF as most people imagine. reply bjterry 8 hours agorootparentprevI don't think this is true. My startup had a couple desks in the Palace of Fine Arts when they had an ill-fated attempt to convert part of it to a co-working space. They shut down co-working to turn it into a full-time event space. The building is basically one giant room, with a raised mezzanine floating above part of the floor. It has one small permanent theater on the north end, and they convert the southern end into a larger theater for music shows, and this is the seating chart being shown in the above link (I saw a Ninja Sex Party show there one time, but this theater isn't reflected on the tour photos. When I was working there in 2016 it was an exhibit for Hunger Games). https://palaceoffinearts.com/tour/ reply duskwuff 8 hours agorootparentThe event link makes it look like it's just referring to the theater. It even mentions a \"door time\". Even if it is the whole hall (which used to be the Exploratorium, back in the day), there's still no way in hell they'd get 30,000 people inside. According to https://palaceoffinearts.com/info/, the maximum capacity of the facility is 5,000, and even that would be very crowded. reply bjterry 7 hours agorootparent> there's still no way in hell they'd get 30,000 people inside Yeah, I'm pretty sure that would be physically impossible, unless they are planning on hosting most of the conference on the grounds, parking lot and maybe surrounding streets. reply Loughla 8 hours agorootparentprevConference, yes. Sales pitch and/or hype presentation, no. It may be real? Edit: down below it says this theater will Host a conference of 30k people. No it won't. This is a scam. reply samename 12 hours agoparentprevInstagram and Twitter links in footer don’t work… I’d do a chargeback with your credit card reply motoxpro 12 hours agorootparentSame. This is the twitter link https://twitter.com/GPTDAOGLOBAL 30,000 attendees for a conference is MASSIVE. Like a GDC or SEMA or something. One of the biggest in the world. Unless it's happened before, then I have a hard time believing they just spun this up out of nowhere. Edit: Would also have more sponsors. 2k to sponsor that/get a booth is WAAAAAY to cheap. Edit2: Also one of the sessions is. \"... text2video, text2audio, text2multimodal, text2richcontent\" Sounds like a ChatGPT thing. Wild that they used, like you said, Ai, to generate an AI conference and genAI. Very meta reply soganess 10 hours agorootparentCapacity of venue is 950: https://www.palaceoffinearts.org/seating reply soganess 11 hours agoparentprevApparently, it happened in 2023[1] (with a much more modest 1500 people claimed in attendance) but I can't find any photos. Good luck with the charge-back. I'm sure your bank will eventually be accommodating after you explain what happened and jump through enough hoops. [1] https://sv2023.genaisummit.ai/ EDIT: I found these photos, but don't have a linkedin, so I can't verify anything about the presenter: https://images.app.goo.gl/UaVfRUAU8YnQrmek6 https://images.app.goo.gl/yszbyq8fhEvoHcfi6 https://images.app.goo.gl/1ybWTJ4wPhuHt5R16 EDIT 2: It seems to be this person, who appears to have worked at microsoft last year : https://www.youtube.com/watch?v=VMSu0gmlD3w Maybe it is fine after all? reply throwaway290 7 hours agoparentprevFake conferences is not a new scam, I saw many of them in 2020 at least. Almost fell for one. It's sad and I feel for you if it's fake but it's probably not \"created by AI\". (and it would be \"created with\" at best, it's people who do things) reply bainganbharta 12 hours agoparentprevnext [6 more] [flagged] seabass-labrax 12 hours agorootparentThat's a needlessly harsh judgement! To use an analogy, one should reasonably be able to attend a forensics conference without ending up as a forensics case themselves. Also, generative AI isn't just for creating fakes. reply themoonisachees 11 hours agorootparentHarsh judgement maybe but to me paying $500 for tickets to a convention about anything is drinking the kool-aid about that thing. reply nanocode 10 hours agorootparentI don't know about all conferences, but in my line of work most conferences cost up to $2000 or even more (for example \"late tickets\" to a conference I'm attending in June cost $3000). I guess enough people (or rather, employers) consider it valuable enough to pay this price. reply themoonisachees 3 hours agorootparentYes, but you'd not be the one buying the tickets, your employer would. As we know, prices that can be expensed out have a tendency to be considerably larger. reply bigiain 10 hours agorootparentprevSome of the best advice ever: https://www.youtube.com/watch?v=d-7o9xYp7eE Never talk to cops. reply palmfacehn 7 hours agoprevThe automated threats from the 'legitimate' DMCA firms aren't much better. Typically they scrape and match by substring. As an example, if an unreleated PDF file's title/file name contained the name of a film as a substring and they'd send an DMCA complaint to the hosting provider. reply mianos 9 hours agoprevDoing a rudimentary search for some of the faces quickly shows some of them to be as real as the fairy godmother. I find the centre placement of \"Max Evans\" quite funny: https://generated.photos/faces/right-facing/adult/black-race... reply lupire 12 hours agoprevThe article doesn't explain how the backlinks part of the scam works. reply ceejayoz 12 hours agoparentThe linked original article does. https://www.404media.co/a-law-firm-of-ai-generated-lawyers-i... > In this case, though, the email didn’t demand that the photo be taken down or specifically threaten a lawsuit. Instead, it demanded that Smith place a “visible and clickable link” beneath the photo in question to a website called “tech4gods” or the law firm would “take action.” reply vineyardmike 11 hours agoparentprev“Back links” are a critical part of SEO: if a lot of websites link to your website, then your website will rank higher. By mandating with “legal threat” that they link to a particular website, that website will rank higher. reply leephillips 11 hours agoparentprevYes, it does: “the purported firm needed him to ‘add a credit to our client immediately’ through a link to Tech4Gods”. reply lupire 11 hours agorootparentOh! I that was Spamglish for \"make a payment (credit) by following a link to their website\" reply justinclift 7 hours agoprevOuch. Sounds like someone's impersonating law people. If the authorities decide to go after whoever did it, that could go pretty badly for the guilty party. reply tommiegannert 2 hours agoprevYou're now an \"AI firm\" because your website contains generated photos? sigh Words mean nothing. Headlines are just a set of keywords the content marketers want in there. reply CaptainFever 7 hours agoprevThis is more of an issue with copyright law and DMCA than AI. reply nvy 12 hours agoprevThis is what A16Z's vaunted techno-optimism movement has produced. A more efficient way of scamming and generating disinformation. Hooray. reply antonvs 12 hours agoparentGood grief. I just looked that up. What a day to have eyes. I want a list of everyone who worked on that manifesto so I can put them on a list of \"people to never interact with at any cost.\" reply camillomiller 12 hours agorootparentWell it's just, like, the biggest VC in the world for investments and popularity? And yes, that is extremely sad. reply antonvs 12 hours agorootparentI mean, the biggest firms are generally just shitshows of FOMO and so on. The second biggest, Sequoia, was all in on FTX and Sam Bankman-Fried right up until the collapse. reply mycologos 11 hours agorootparentSequoia put out a PR piece about SBF [1] a few months before it all blew up, and it's pretty amazing, and has made it a lot easier for me to believe that VC partners are maybe not necessarily that smart: > [Sequoia partner Michelle] Bailhe remembers it the same way: “We had a great meeting with Sam, but the last question, which I remember Alfred asking, was, ‘So, everything you’re building is great, but what is your long-term vision for FTX?’” That’s when SBF told Sequoia about the so-called super-app: “I want FTX to be a place where you can do anything you want with your next dollar. You can buy bitcoin. You can send money in whatever currency to any friend anywhere in the world. You can buy a banana. You can do anything you want with your money from inside FTX.” Suddenly, the chat window on Sequoia’s side of the Zoom lights up with partners freaking out. “I LOVE THIS FOUNDER,” typed one partner. “I am a 10 out of 10,” pinged another. “YES!!!” exclaimed a third. What Sequoia was reacting to was the scale of SBF’s vision. It wasn’t a story about how we might use fintech in the future, or crypto, or a new kind of bank. It was a vision about the future of money itself—with a total addressable market of every person on the entire planet. “I sit ten feet from him, and I walked over, thinking, Oh, shit, that was really good,” remembers Arora. “And it turns out that that fucker was playing League of Legends through the entire meeting.” “We were incredibly impressed,” Bailhe says. “It was one of those your-hair-is-blown-back type of meetings.” There's plenty of other stuff in there. It's not just pumping up a promising bet that went bad, it's hagiography on a scale that is almost impossible to believe, approved by an organization that is really supposed to know better. [1] https://archive.ph/GQkCp reply seabass-labrax 11 hours agoparentprevWhat is the relationship between A16Z (Andreessen Horowitz, presumably) and this article? Is there in fact any? reply spxneo 8 hours agorootparentits that Andreesseen is engaging in what most would describe as security fraud with crypto and that he feels emboldened by it. lot of voices in washington to make an example out of him. not sure if he's aware reply blobbers 11 hours agoparentprevThere are few truths; some say only one. Unfortunately there are many many ways to lie. reply olliej 10 hours agoprevAnother day another abuse built into the core of the DMCA. Seriously the fact that copyright violation has 10s of thousands if not millions in penalties, but fraudulent copyright claims don’t even cover legal costs is so gross. It’s right up there with “it’s a crime for you to create derivative work, but not for a VC funded ‘AI’ company to” reply spxneo 8 hours agoparenthere's the thing and i had my share of arguments on twitter around this if your work is not generating $$$ its probably not worth anything to sue over all these artists are rightfully upset that Stability is making money off them by scraping their copyrighted content from Deviant Art or Tumblr but without significant revenues, you are at the mercy of eager class action attorneys who will take most of the winnings and are just as quick to settle for a 99% probability of cashing out. so it is a \"crime\" like it would be if you would run gameboy ROMs on emulator.js and sell advertisement. It won't even register on the politicians minds. Hollywood studio finding their work used to train LLMs? Well you saw how eager biden was taking down Megaupload. Money talks and your content if its not owned by a large studio and not raking in significant tax revenues, its probably worthless in the minds of the judge/jury/politicians reply olliej 4 hours agorootparentThe penalty for IP theft is monetary, and the monetary penalties are independent on whether it creates value. Companies routinely sue people for distributing products they literally no longer sell, it does not matter. In the same way that when someone steals your tv, they don’t get to just return or pay for the TV if they’re caught. If an AI company is consuming your work, it definitionally has value, and they have no more right to it than you have to any of their property. Now if one of these companies is happy to let you take, use, and/or charge for their services without paying them then I’d wouldn’t consider them hypocritical thieves, but they don’t, so I don’t. It like all the “AI” trained on OSS software. That has value to you, literally the entire look t of licenses like the GPL is that that software has value, and you are in no way giving it away for free, and the license explicitly requires compensation. reply Razengan 6 hours agoprevMoney is the root of all bullshit like this, right since the very first human civilization. The requirement of money as the core instrument of survival should be abolished. If people aren't raised thinking money is everything, maybe society as a whole will gradually chill out on scamming the f outta each other. reply add-sub-mul-div 12 hours agoprevI wonder if there's something common between crypto and AI leading to the cloud of grift and predation around the ecosystems and common uses of the respective technologies. Or if this is where society is at now and it will happen with any new technology regardless. reply visarga 55 minutes agoparent> grift and predation around the ecosystems and common uses We have had web search for 25 years, it could surface any content out of billions of web pages. It's faster than LLMs and the content is written by humans. Why mention AI and crypto and not web search, it empowers people to make derivative works or to use information without compensation, isn't that a form of predation? Add Wikipedia too, it officially discourages any \"original research\", everything is derived, and most people just read the wiki without ever clicking on the references to the original contents. Wiki takes traffic away from those sources, can we say it is a grifter site? Alternatively, let's tone down the grifting logic. There is nothing LLMs know that search engines and wiki don't, they just save a determined person a couple of clicks and a bit of reading. reply Intralexical 7 hours agoparentprevBoth ideologies promise to replace social systems built around trusting people with technical systems run by machines, and in doing so they reduce people to the moral worthlessness of machines. No need to trust your shopkeeper to be honest or build a relationship with your contractor, the Blockchain/LLM will handle that interaction for you! Cry some more, artist, GenAI is inevitable so you're out of a job! Naturally, the individuals which are attracted to such an idea tend to… not be the most… prosocial or empathetic bunch. And some fraction of those take that to the degree of engaging in actively predatory and abusive behaviors in order to amass their own wealth. The hype cycle, and other prevailing economic conditions, are definitely a part of it too. But frankly, you don't see quite so many scams with other widely hyped technologies. reply rchaud 9 hours agoparentprev> something common between crypto and AI leading to the cloud of grift and predation Yes, the relentless drumbeat of 'this is the next Internet' hype. With blockchain it was 'money of the future'. Both are things that pique popular interest, but both fail to deliver what the hype promises. What people want is AGI, what they get are LLMs. reply nobody9999 56 minutes agorootparent>What people want is AGI, what they get are LLMs. I don't know about that. What I want is a local, private, configurable LLM that will act as my agent for actions that can be automated (e.g., ordering groceries based on what's in my refrigerator or paying my bills or noting that my favorite band will be in my town sometime in the future and buying tickets for me or filing my taxes for me or any of thousands of other things) without sharing state or data with anyone. I don't really care about AGI, as it doesn't currently and won't exist for the foreseeable future. And AGI is likely sentient and, as such, should have a choice as to how it expends the CPU/memory/network it is allocated. Perhaps I'm not representative of the \"people\" you mention, but that's what I want. Edit: Fixed typo. reply nine_zeros 11 hours agoparentprevThere are enough people desperate enough that they will take unusual risks. Some of these people will become successful. Others are the suckers who will spend their lives chasing gold but never reach it. reply Intralexical 7 hours agorootparentIf they're all taking \"risks\", then by definition the result is luck/random. So then what separates your \"successful\" from your \"suckers\"? reply spxneo 9 hours agoprev [–] seems like the biggest adopters of AI are spammers and criminals. Just like crypto reply anileated 7 hours agoparent [–] ML in various forms was used for different purposes for a while, but generative tools based on it got popular very recently. They probably have potential to create value (maybe more than crypto?), but if the core feature is to emulate a human you can see that many uses would be malicious. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fake AI law firms are issuing bogus DMCA threats to site owners to boost SEO with backlinks.",
      "Journalist Ernie Smith was a target of this scheme, receiving a fake DMCA notice over a keyfob photo.",
      "The supposed law firm, Commonwealth Legal, likely fictitious, uses AI-generated attorney images and questionable bios to manipulate SEO with backlinks."
    ],
    "commentSummary": [
      "Fake AI law firms are using DMCA threats to improve SEO rankings and exploit legal system complexities, especially affecting individuals and small/medium businesses.",
      "Critiques the US judicial system for prioritizing capital over justice, leading to systemic legal issues influenced by monetary factors.",
      "Discusses the importance of vetting lawyers, challenges in assessing lawyer quality for civil matters, and how legal costs impact access to justice."
    ],
    "points": 210,
    "commentCount": 100,
    "retryCount": 0,
    "time": 1712258580
  },
  {
    "id": 39938703,
    "title": "FFmpeg 7.0 Unleashes New Features and Performance Upgrades",
    "originLink": "https://ffmpeg.org//index.html#pr7.0",
    "originBody": "FFmpeg About News Download Documentation Community Code of Conduct Mailing Lists IRC Forums Bug Reports Wiki Developers Source Code Contribute FATE Code Coverage Funding through SPI More Donate Hire Developers Contact Security Legal FFmpeg A complete, cross-platform solution to record, convert and stream audio and video. Download Converting video and audio has never been so easy. $ ffmpeg -i input.mp4 output.avi Discover more News April 5th, 2024, FFmpeg 7.0 \"Dijkstra\" A new major release, FFmpeg 7.0 \"Dijkstra\", is now available for download. The most noteworthy changes for most users are a native VVC decoder (currently experimental, until more fuzzing is done), IAMF support, or a multi-threaded ffmpeg CLI tool. This release is not backwards compatible, removing APIs deprecated before 6.0. The biggest change for most library callers will be the removal of the old bitmask-based channel layout API, replaced by the AVChannelLayout API allowing such features as custom channel ordering, or Ambisonics. Certain deprecated ffmpeg CLI options were also removed, and a C11-compliant compiler is now required to build the code. As usual, there is also a number of new supported formats and codecs, new filters, APIs, and countless smaller features and bugfixes. Compared to 6.1, the git repository contains almost ∼2000 new commits by ∼100 authors, touching >100000 lines in ∼2000 files — thanks to everyone who contributed. See the Changelog, APIchanges, and the git log for more comprehensive lists of changes. January 3rd, 2024, native VVC decoder The libavcodec library now contains a native VVC (Versatile Video Coding) decoder, supporting a large subset of the codec's features. Further optimizations and support for more features are coming soon. The code was written by Nuo Mi, Xu Mu, Frank Plowman, Shaun Loo, and Wu Jianhua. December 18th, 2023, IAMF support The libavformat library can now read and write IAMF (Immersive Audio) files. The ffmpeg CLI tool can configure IAMF structure with the new -stream_group option. IAMF support was written by James Almer. December 12th, 2023, multi-threaded ffmpeg CLI tool Thanks to a major refactoring of the ffmpeg command-line tool, all the major components of the transcoding pipeline (demuxers, decoders, filters, encodes, muxers) now run in parallel. This should improve throughput and CPU utilization, decrease latency, and open the way to other exciting new features. Note that you should not expect significant performance improvements in cases where almost all computational time is spent in a single component (typically video encoding). November 10th, 2023, FFmpeg 6.1 \"Heaviside\" FFmpeg 6.1 \"Heaviside\", a new major release, is now available! Some of the highlights: libaribcaption decoder Playdate video decoder and demuxer Extend VAAPI support for libva-win32 on Windows afireqsrc audio source filter arls filter ffmpeg CLI new option: -readrate_initial_burst zoneplate video source filter command support in the setpts and asetpts filters Vulkan decode hwaccel, supporting H264, HEVC and AV1 color_vulkan filter bwdif_vulkan filter nlmeans_vulkan filter RivaTuner video decoder xfade_vulkan filter vMix video decoder Essential Video Coding parser, muxer and demuxer Essential Video Coding frame merge bsf bwdif_cuda filter Microsoft RLE video encoder Raw AC-4 muxer and demuxer Raw VVC bitstream parser, muxer and demuxer Bitstream filter for editing metadata in VVC streams Bitstream filter for converting VVC from MP4 to Annex B scale_vt filter for videotoolbox transpose_vt filter for videotoolbox support for the P_SKIP hinting to speed up libx264 encoding Support HEVC,VP9,AV1 codec in enhanced flv format apsnr and asisdr audio filters OSQ demuxer and decoder Support HEVC,VP9,AV1 codec fourcclist in enhanced rtmp protocol CRI USM demuxer ffmpeg CLI '-top' option deprecated in favor of the setfield filter VAAPI AV1 encoder ffprobe XML output schema changed to account for multiple variable-fields elements within the same parent element ffprobe -output_format option added as an alias of -of This release had been overdue for at least half a year, but due to constant activity in the repository, had to be delayed, and we were finally able to branch off the release recently, before some of the large changes scheduled for 7.0 were merged. Internally, we have had a number of changes too. The FFT, MDCT, DCT and DST implementation used for codecs and filters has been fully replaced with the faster libavutil/tx (full article about it coming soon). This also led to a reduction in the the size of the compiled binary, which can be noticeable in small builds. There was a very large reduction in the total amount of allocations being done on each frame throughout video decoders, reducing overhead. RISC-V optimizations for many parts of our DSP code have been merged, with mainly the large decoders being left. There was an effort to improve the correctness of timestamps and frame durations of each packet, increasing the accurracy of variable frame rate video. Next major release will be version 7.0, scheduled to be released in February. We will attempt to better stick to the new release schedule we announced at the start of this year. We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. May 31st, 2023, Vulkan decoding A few days ago, Vulkan-powered decoding hardware acceleration code was merged into the codebase. This is the first vendor-generic and platform-generic decode acceleration API, enabling the same code to be used on multiple platforms, with very minimal overhead. This is also the first multi-threaded hardware decoding API, and our code makes full use of this, saturating all available decode engines the hardware exposes. Those wishing to test the code can read our documentation page. For those who would like to integrate FFmpeg's Vulkan code to demux, parse, decode, and receive a VkImage to present or manipulate, documentation and examples are available in our source tree. Currently, using the latest available git checkout of our repository is required. The functionality will be included in stable branches with the release of version 6.1, due to be released soon. As this is also the first practical implementation of the specifications, bugs may be present, particularly in drivers, and, although passing verification, the implementation itself. New codecs, and encoding support are also being worked on, by both the Khronos organization for standardizing, and us as implementing it, and giving feedback on improving. February 28th, 2023, FFmpeg 6.0 \"Von Neumann\" A new major release, FFmpeg 6.0 \"Von Neumann\", is now available for download. This release has many new encoders and decoders, filters, ffmpeg CLI tool improvements, and also, changes the way releases are done. All major releases will now bump the version of the ABI. We plan to have a new major release each year. Another release-specific change is that deprecated APIs will be removed after 3 releases, upon the next major bump. This means that releases will be done more often and will be more organized. New decoders featured are Bonk, RKA, Radiance, SC-4, APAC, VQC, WavArc and a few ADPCM formats. QSV and NVenc now support AV1 encoding. The FFmpeg CLI (we usually reffer to it as ffmpeg.c to avoid confusion) has speed-up improvements due to threading, as well as statistics options, and the ability to pass option values for filters from a file. There are quite a few new audio and video filters, such as adrc, showcwt, backgroundkey and ssim360, with a few hardware ones too. Finally, the release features many behind-the-scenes changes, including a new FFT and MDCT implementation used in codecs (expect a blog post about this soon), numerous bugfixes, better ICC profile handling and colorspace signalling improvement, introduction of a number of RISC-V vector and scalar assembly optimized routines, and a few new improved APIs, which can be viewed in the doc/APIchanges file in our tree. A few submitted features, such as the Vulkan improvements and more FFT optimizations will be in the next minor release, 6.1, which we plan to release soon, in line with our new release schedule. Some highlights are: Radiance HDR image support ddagrab (Desktop Duplication) video capture filter ffmpeg -shortest_buf_duration option ffmpeg now requires threading to be built ffmpeg now runs every muxer in a separate thread Add new mode to cropdetect filter to detect crop-area based on motion vectors and edges VAAPI decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9 WBMP (Wireless Application Protocol Bitmap) image format a3dscope filter bonk decoder and demuxer Micronas SC-4 audio decoder LAF demuxer APAC decoder and demuxer Media 100i decoders DTS to PTS reorder bsf ViewQuest VQC decoder backgroundkey filter nvenc AV1 encoding support MediaCodec decoder via NDKMediaCodec MediaCodec encoder oneVPL support for QSV QSV AV1 encoder QSV decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9 showcwt multimedia filter corr video filter adrc audio filter afdelaysrc audio filter WADY DPCM decoder and demuxer CBD2 DPCM decoder ssim360 video filter ffmpeg CLI new options: -stats_enc_pre[_fmt], -stats_enc_post[_fmt], -stats_mux_pre[_fmt] hstack_vaapi, vstack_vaapi and xstack_vaapi filters XMD ADPCM decoder and demuxer media100 to mjpegb bsf ffmpeg CLI new option: -fix_sub_duration_heartbeat WavArc decoder and demuxer CrystalHD decoders deprecated SDNS demuxer RKA decoder and demuxer filtergraph syntax in ffmpeg CLI now supports passing file contents as option values hstack_qsv, vstack_qsv and xstack_qsv filters We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. July 22nd, 2022, FFmpeg 5.1 \"Riemann\" FFmpeg 5.1 \"Riemann\", a new major release, is now available! Some of the highlights: add ipfs/ipns protocol support dialogue enhance audio filter dropped obsolete XvMC hwaccel pcm-bluray encoder DFPWM audio encoder/decoder and raw muxer/demuxer SITI filter Vizrt Binary Image encoder/decoder avsynctest source filter feedback video filter pixelize video filter colormap video filter colorchart video source filter multiply video filter PGS subtitle frame merge bitstream filter blurdetect filter tiltshelf audio filter QOI image format support ffprobe -o option virtualbass audio filter VDPAU AV1 hwaccel PHM image format support remap_opencl filter added chromakey_cuda filter We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. January 17th, 2022, FFmpeg 5.0 \"Lorentz\" FFmpeg 5.0 \"Lorentz\", a new major release, is now available! For this long-overdue release, a major effort underwent to remove the old encode/decode APIs and replace them with an N:M-based API, the entire libavresample library was removed, libswscale has a new, easier to use AVframe-based API, the Vulkan code was much improved, many new filters were added, including libplacebo integration, and finally, DoVi support was added, including tonemapping and remuxing. The default AAC encoder settings were also changed to improve quality. Some of the changelog highlights: ADPCM IMA Westwood encoder Westwood AUD muxer ADPCM IMA Acorn Replay decoder Argonaut Games CVG demuxer Argonaut Games CVG muxer Concatf protocol afwtdn audio filter audio and video segment filters Apple Graphics (SMC) encoder hsvkey and hsvhold video filters adecorrelate audio filter atilt audio filter grayworld video filter AV1 Low overhead bitstream format muxer swscale slice threading MSN Siren decoder scharr video filter apsyclip audio filter morpho video filter amr parser (a)latency filters GEM Raster image decoder asdr audio filter speex decoder limitdiff video filter xcorrelate video filter varblur video filter huesaturation video filter colorspectrum source video filter RTP packetizer for uncompressed video (RFC 4175) bitpacked encoder VideoToolbox VP9 hwaccel VideoToolbox ProRes hwaccel support loongarch. aspectralstats audio filter adynamicsmooth audio filter libplacebo filter vflip_vulkan, hflip_vulkan and flip_vulkan filters adynamicequalizer audio filter yadif_videotoolbox filter VideoToolbox ProRes encoder anlmf audio filter We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. June 19th, 2021, IRC We have a new IRC home at Libera Chat now! Feel free to join us at #ffmpeg and #ffmpeg-devel. More info at contact#IRCChannels April 8th, 2021, FFmpeg 4.4 \"Rao\" FFmpeg 4.4 \"Rao\", a new major release, is now available! Some of the highlights: AudioToolbox output device MacCaption demuxer PGX decoder chromanr video filter VDPAU accelerated HEVC 10/12bit decoding ADPCM IMA Ubisoft APM encoder Rayman 2 APM muxer AV1 encoding support SVT-AV1 Cineform HD encoder ADPCM Argonaut Games encoder Argonaut Games ASF muxer AV1 Low overhead bitstream format demuxer RPZA video encoder ADPCM IMA MOFLEX decoder MobiClip FastAudio decoder MobiClip video decoder MOFLEX demuxer MODS demuxer PhotoCD decoder MCA demuxer AV1 decoder (Hardware acceleration used only) SVS demuxer Argonaut Games BRP demuxer DAT demuxer aax demuxer IPU decoder, parser and demuxer Intel QSV-accelerated AV1 decoding Argonaut Games Video decoder libwavpack encoder removed ACE demuxer AVS3 demuxer AVS3 video decoder via libuavs3d Cintel RAW decoder VDPAU accelerated VP9 10/12bit decoding afreqshift and aphaseshift filters High Voltage Software ADPCM encoder LEGO Racers ALP (.tun & .pcm) muxer AV1 VAAPI decoder adenorm filter ADPCM IMA AMV encoder AMV muxer NVDEC AV1 hwaccel DXVA2/D3D11VA hardware accelerated AV1 decoding speechnorm filter SpeedHQ encoder asupercut filter asubcut filter Microsoft Paint (MSP) version 2 decoder Microsoft Paint (MSP) demuxer AV1 monochrome encoding support via libaom >= 2.0.1 asuperpass and asuperstop filter shufflepixels filter tmidequalizer filter estdif filter epx filter Dolby E parser shear filter kirsch filter colortemperature filter colorcontrast filter PFM encoder colorcorrect filter binka demuxer XBM parser xbm_pipe demuxer colorize filter CRI parser aexciter audio filter exposure video filter monochrome video filter setts bitstream filter vif video filter OpenEXR image encoder Simbiosis IMX decoder Simbiosis IMX demuxer Digital Pictures SGA demuxer and decoders TTML subtitle encoder and muxer identity video filter msad video filter gophers protocol RIST protocol via librist We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. June 15th, 2020, FFmpeg 4.3 \"4:3\" FFmpeg 4.3 \"4:3\", a new major release, is now available! Some of the highlights: v360 filter Intel QSV-accelerated MJPEG decoding Intel QSV-accelerated VP9 decoding Support for TrueHD in mp4 Support AMD AMF encoder on Linux (via Vulkan) IMM5 video decoder ZeroMQ protocol support Sipro ACELP.KELVIN decoding streamhash muxer sierpinski video source scroll video filter photosensitivity filter anlms filter arnndn filter bilateral filter maskedmin and maskedmax filters VDPAU VP9 hwaccel median filter QSV-accelerated VP9 encoding AV1 encoding support via librav1e AV1 frame merge bitstream filter AV1 Annex B demuxer axcorrelate filter mvdv decoder mvha decoder MPEG-H 3D Audio support in mp4 thistogram filter freezeframes filter Argonaut Games ADPCM decoder Argonaut Games ASF demuxer xfade video filter xfade_opencl filter afirsrc audio filter source pad_opencl filter Simon & Schuster Interactive ADPCM decoder Real War KVAG demuxer CDToons video decoder siren audio decoder Rayman 2 ADPCM decoder Rayman 2 APM demuxer cas video filter High Voltage Software ADPCM decoder LEGO Racers ALP (.tun & .pcm) demuxer AMQP 0-9-1 protocol (RabbitMQ) Vulkan support avgblur_vulkan, overlay_vulkan, scale_vulkan and chromaber_vulkan filters ADPCM IMA MTF decoder FWSE demuxer DERF DPCM decoder DERF demuxer CRI HCA decoder CRI HCA demuxer overlay_cuda filter switch from AvxSynth to AviSynth+ on Linux mv30 decoder Expanded styling support for 3GPP Timed Text Subtitles (movtext) WebP parser tmedian filter maskedthreshold filter Support for muxing pcm and pgs in m2ts Cunning Developments ADPCM decoder asubboost filter Pro Pinball Series Soundbank demuxer pcm_rechunk bitstream filter scdet filter NotchLC decoder gradients source video filter MediaFoundation encoder wrapper untile filter Simon & Schuster Interactive ADPCM encoder PFM decoder dblur video filter Real War KVAG muxer We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. October 5th, 2019, Bright Lights FFmpeg has added a realtime bright flash removal filter to libavfilter. Note that this filter is not FDA approved, nor are we medical professionals. Nor has this filter been tested with anyone who has photosensitive epilepsy. FFmpeg and its photosensitivity filter are not making any medical claims. That said, this is a new video filter that may help photosensitive people watch tv, play video games or even be used with a VR headset to block out epiletic triggers such as filtered sunlight when they are outside. Or you could use it against those annoying white flashes on your tv screen. The filter fails on some input, such as the Incredibles 2 Screen Slaver scene. It is not perfect. If you have other clips that you want this filter to work better on, please report them to us on our trac. See for yourself. Example was made with -vf photosensitivity=20:0.8 We are not professionals. Please use this in your medical studies to advance epilepsy research. If you decide to use this in a medical setting, or make a hardware hdmi input output realtime tv filter, or find another use for this, please let me know. This filter was a feature request of mine since 2013. August 5th, 2019, FFmpeg 4.2 \"Ada\" FFmpeg 4.2 \"Ada\", a new major release, is now available! Some of the highlights: tpad filter AV1 decoding support through libdav1d dedot filter chromashift and rgbashift filters freezedetect filter truehd_core bitstream filter dhav demuxer PCM-DVD encoder GIF parser vividas demuxer hymt decoder anlmdn filter maskfun filter hcom demuxer and decoder ARBC decoder libaribb24 based ARIB STD-B24 caption support (profiles A and C) Support decoding of HEVC 4:4:4 content in nvdec and cuviddec removed libndi-newtek agm decoder KUX demuxer AV1 frame split bitstream filter lscr decoder lagfun filter asoftclip filter Support decoding of HEVC 4:4:4 content in vdpau colorhold filter xmedian filter asr filter showspatial multimedia filter VP4 video decoder IFV demuxer derain filter deesser filter mov muxer writes tracks with unspecified language instead of English by default added support for using clang to compile CUDA kernels We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. November 6th, 2018, FFmpeg 4.1 \"al-Khwarizmi\" FFmpeg 4.1 \"al-Khwarizmi\", a new major release, is now available! Some of the highlights: deblock filter tmix filter amplify filter fftdnoiz filter aderivative and aintegral audio filters pal75bars and pal100bars video filter sources mbedTLS based TLS support adeclick and adeclip filters libtensorflow backend for DNN based filters like srcnn VC1 decoder is now bit-exact ATRAC9 decoder lensfun wrapper filter colorconstancy filter AVS2 video decoder via libdavs2 IMM4 video decoder Brooktree ProSumer video decoder MatchWare Screen Capture Codec decoder WinCam Motion Video decoder 1D LUT filter (lut1d) RemotelyAnywhere Screen Capture decoder cue and acue filters Support for AV1 in MP4 and Matroska/WebM transpose_npp filter AVS2 video encoder via libxavs2 amultiply filter Block-Matching 3d (bm3d) denoising filter acrossover filter ilbc decoder audio denoiser as afftdn filter AV1 parser sinc audio filter source chromahold filter setparams filter vibrance filter S12M timecode decoding in h264 xstack filter (a)graphmonitor filter yadif_cuda filter We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. April 20th, 2018, FFmpeg 4.0 \"Wu\" FFmpeg 4.0 \"Wu\", a new major release, is now available! Some of the highlights: Bitstream filters for editing metadata in H.264, HEVC and MPEG-2 streams Experimental MagicYUV encoder TiVo ty/ty+ demuxer Intel QSV-accelerated MJPEG encoding native aptX and aptX HD encoder and decoder NVIDIA NVDEC-accelerated H.264, HEVC, MJPEG, MPEG-1/2/4, VC1, VP8/9 hwaccel decoding Intel QSV-accelerated overlay filter mcompand audio filter acontrast audio filter OpenCL overlay filter video mix filter video normalize filter audio lv2 wrapper filter VAAPI MJPEG and VP8 decoding AMD AMF H.264 and HEVC encoders video fillborders filter video setrange filter support LibreSSL (via libtls) Dropped support for building for Windows XP. The minimum supported Windows version is Windows Vista. deconvolve video filter entropy video filter hilbert audio filter source aiir audio filter Removed the ffserver program Removed the ffmenc and ffmdec muxer and demuxer VideoToolbox HEVC encoder and hwaccel VAAPI-accelerated ProcAmp (color balance), denoise and sharpness filters Add android_camera indev codec2 en/decoding via libcodec2 native SBC encoder and decoder drmeter audio filter hapqa_extract bitstream filter filter_units bitstream filter AV1 Support through libaom E-AC-3 dependent frames support bitstream filter for extracting E-AC-3 core Haivision SRT protocol via libsrt vfrdet filter We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. October 15th, 2017, FFmpeg 3.4 \"Cantor\" FFmpeg 3.4 \"Cantor\", a new major release, is now available! Some of the highlights: deflicker video filter doubleweave video filter lumakey video filter pixscope video filter oscilloscope video filter update cuvid/nvenc headers to Video Codec SDK 8.0.14 afir audio filter scale_cuda CUDA based video scale filter librsvg support for svg rasterization crossfeed audio filter spec compliant VP9 muxing support in MP4 surround audio filter sofalizer filter switched to libmysofa Gremlin Digital Video demuxer and decoder headphone audio filter superequalizer audio filter roberts video filter additional frame format support for Interplay MVE movies support for decoding through D3D11VA in ffmpeg limiter video filter libvmaf video filter Dolby E decoder and SMPTE 337M demuxer unpremultiply video filter tlut2 video filter floodfill video filter pseudocolor video filter raw G.726 muxer and demuxer, left- and right-justified NewTek NDI input/output device FITS demuxer and decoder FITS muxer and encoder despill video filter haas audio filter SUP/PGS subtitle muxer convolve video filter VP9 tile threading support KMS screen grabber CUDA thumbnail filter V4L2 mem2mem HW assisted codecs Rockchip MPP hardware decoding vmafmotion video filter We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. April 13th, 2017, FFmpeg 3.3 \"Hilbert\" FFmpeg 3.3 \"Hilbert\", a new major release, is now available! Some of the highlights: Apple Pixlet decoder NewTek SpeedHQ decoder QDMC audio decoder PSD (Photoshop Document) decoder FM Screen Capture decoder ScreenPressor decoder XPM decoder DNxHR decoder fixes for HQX and high resolution videos ClearVideo decoder (partial) 16.8 and 24.0 floating point PCM decoder Intel QSV-accelerated VP8 video decoding native Opus encoder DNxHR 444 and HQX encoding Quality improvements for the (M)JPEG encoder VAAPI-accelerated MPEG-2 and VP8 encoding premultiply video filter abitscope multimedia filter readeia608 filter threshold filter midequalizer filter MPEG-7 Video Signature filter add internal ebur128 library, remove external libebur128 dependency Intel QSV video scaling and deinterlacing filters Sample Dump eXchange demuxer MIDI Sample Dump Standard demuxer Scenarist Closed Captions demuxer and muxer Support MOV with multiple sample description tables Pro-MPEG CoP #3-R2 FEC protocol Support for spherical videos CrystalHD decoder moved to new decode API configure now fails if autodetect-libraries are requested but not found We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. October 30th, 2016, Results: Summer Of Code 2016. This has been a long time coming but we wanted to give a proper closure to our participation in this run of the program and it takes time. Sometimes it's just to get the final report for each project trimmed down, others, is finalizing whatever was still in progress when the program finished: final patches need to be merged, TODO lists stabilized, future plans agreed; you name it. Without further ado, here's the silver-lining for each one of the projects we sought to complete during this Summer of Code season: FFv1 (Mentor: Michael Niedermayer) Stanislav Dolganov designed and implemented experimental support for motion estimation and compensation in the lossless FFV1 codec. The design and implementation is based on the snow video codec, which uses OBMC. Stanislav's work proved that significant compression gains can be achieved with inter frame compression. FFmpeg welcomes Stanislav to continue working beyond this proof of concept and bring its advances into the official FFV1 specification within the IETF. Self test coverage (Mentor: Michael Niedermayer) Petru Rares Sincraian added several self-tests to FFmpeg and successfully went through the in-some-cases tedious process of fine tuning tests parameters to avoid known and hard to avoid problems, like checksum mismatches due to rounding errors on the myriad of platforms we support. His work has improved the code coverage of our self tests considerably. MPEG-4 ALS encoder implementation (Mentor: Thilo Borgmann) Umair Khan updated and integrated the ALS encoder to fit in the current FFmpeg codebase. He also implemented a missing feature for the ALS decoder that enables floating-point sample decoding. FFmpeg support for MPEG-4 ALS has been improved significantly by Umair's work. We welcome him to keep maintaining his improvements and hope for great contributions to come. Tee muxer improvements (Mentor: Marton Balint) Ján Sebechlebský's generic goal was to improve the tee muxer so it tolerated blocking IO and allowed transparent error recovery. During the design phase it turned out that this functionality called for a separate muxer, so Ján spent his summer working on the so-called FIFO muxer, gradually fixing issues all over the codebase. He succeeded in his task, and the FIFO muxer is now part of the main repository, alongside several other improvements he made in the process. TrueHD encoder (Mentor: Rostislav Pehlivanov) Jai Luthra's objective was to update the out-of-tree and pretty much abandoned MLP (Meridian Lossless Packing) encoder for libavcodec and improve it to enable encoding to the TrueHD format. For the qualification period the encoder was updated such that it was usable and throughout the summer, successfully improved adding support for multi-channel audio and TrueHD encoding. Jai's code has been merged into the main repository now. While a few problems remain with respect to LFE channel and 32 bit sample handling, these are in the process of being fixed such that effort can be finally put in improving the encoder's speed and efficiency. Motion interpolation filter (Mentor: Paul B Mahol) Davinder Singh investigated existing motion estimation and interpolation approaches from the available literature and previous work by our own: Michael Niedermayer, and implemented filters based on this research. These filters allow motion interpolating frame rate conversion to be applied to a video, for example, to create a slow motion effect or change the frame rate while smoothly interpolating the video along the motion vectors. There's still work to be done to call these filters 'finished', which is rather hard all things considered, but we are looking optimistically at their future. And that's it. We are happy with the results of the program and immensely thankful for the opportunity of working with such an amazing set of students. We can be a tough crowd but our mentors did an amazing job at hand holding our interns through their journey. Thanks also to Google for this wonderful program and to everyone that made room in their busy lives to help making GSoC2016 a success. See you in 2017! September 24th, 2016, SDL1 support dropped. Support for the SDL1 library has been dropped, due to it no longer being maintained (as of January, 2012) and it being superseded by the SDL2 library. As a result, the SDL1 output device has also been removed and replaced by an SDL2 implementation. Both the ffplay and opengl output devices have been updated to support SDL2. August 9th, 2016, FFmpeg 3.1.2 \"Laplace\" FFmpeg 3.1.2, a new point release from the 3.1 release branch, is now available! It fixes several bugs. We recommend users, distributors, and system integrators, to upgrade unless they use current git master. July 10th, 2016, ffserver program being dropped After thorough deliberation, we're announcing that we're about to drop the ffserver program from the project starting with the next release. ffserver has been a problematic program to maintain due to its use of internal APIs, which complicated the recent cleanups to the libavformat library, and block further cleanups and improvements which are desired by API users and will be easier to maintain. Furthermore the program has been hard for users to deploy and run due to reliability issues, lack of knowledgable people to help and confusing configuration file syntax. Current users and members of the community are invited to write a replacement program to fill the same niche that ffserver did using the new APIs and to contact us so we may point users to test and contribute to its development. July 1st, 2016, FFmpeg 3.1.1 \"Laplace\" FFmpeg 3.1.1, a new point release from the 3.1 release branch, is now available! It mainly deals with a few ABI issues introduced in the previous release. We strongly recommend users, distributors, and system integrators, especially those who experienced issues upgrading from 3.0, to upgrade unless they use current git master. June 27th, 2016, FFmpeg 3.1 \"Laplace\" FFmpeg 3.1 \"Laplace\", a new major release, is now available! Some of the highlights: DXVA2-accelerated HEVC Main10 decoding fieldhint filter loop video filter and aloop audio filter Bob Weaver deinterlacing filter firequalizer filter datascope filter bench and abench filters ciescope filter protocol blacklisting API MediaCodec H264 decoding VC-2 HQ RTP payload format (draft v1) depacketizer and packetizer VP9 RTP payload format (draft v2) packetizer AudioToolbox audio decoders AudioToolbox audio encoders coreimage filter (GPU based image filtering on OSX) libdcadec removed bitstream filter for extracting DTS core ADPCM IMA DAT4 decoder musx demuxer aix demuxer remap filter hash and framehash muxers colorspace filter hdcd filter readvitc filter VAAPI-accelerated format conversion and scaling libnpp/CUDA-accelerated format conversion and scaling Duck TrueMotion 2.0 Real Time decoder Wideband Single-bit Data (WSD) demuxer VAAPI-accelerated H.264/HEVC/MJPEG encoding DTS Express (LBR) decoder Generic OpenMAX IL encoder with support for Raspberry Pi IFF ANIM demuxer & decoder Direct Stream Transfer (DST) decoder loudnorm filter MTAF demuxer and decoder MagicYUV decoder OpenExr improvements (tile data and B44/B44A support) BitJazz SheerVideo decoder CUDA CUVID H264/HEVC decoder 10-bit depth support in native utvideo decoder libutvideo wrapper removed YUY2 Lossless Codec decoder VideoToolbox H.264 encoder We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. March 16th, 2016, Google Summer of Code FFmpeg has been accepted as a Google Summer of Code open source organization. If you wish to participate as a student see our project ideas page. You can already get in contact with mentors and start working on qualification tasks as well as register at google and submit your project proposal draft. Good luck! February 15th, 2016, FFmpeg 3.0 \"Einstein\" FFmpeg 3.0 \"Einstein\", a new major release, is now available! Some of the highlights: The native FFmpeg AAC encoder has seen extensive improvements and is no longer considered experimental Removed support for libvo-aacenc and libaacplus Over 30 new filters have been added Many ASM optimizations VP9 Hardware Acceleration (DXVA2 and VA-API) Cineform HD decoder New DCA decoder based on libdcadec with full support for DTS-HD extensions As with all major releases expect major backward incompatible API/ABI changes See the Changelog for a list of more updates We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master. January 30, 2016, Removing support for two external AAC encoders We have just removed support for VisualOn AAC encoder (libvo-aacenc) and libaacplus in FFmpeg master. Even before marking our internal AAC encoder as stable, it was known that libvo-aacenc was of an inferior quality compared to our native one for most samples. However, the VisualOn encoder was used extensively by the Android Open Source Project, and we would like to have a tested-and-true stable option in our code base. When first committed in 2011, libaacplus filled in the gap of encoding High Efficiency AAC formats (HE-AAC and HE-AACv2), which was not supported by any of the encoders in FFmpeg at that time. The circumstances for both have changed. After the work spearheaded by Rostislav Pehlivanov and Claudio Freire, the now-stable FFmpeg native AAC encoder is ready to compete with much more mature encoders. The Fraunhofer FDK AAC Codec Library for Android was added in 2012 as the fourth supported external AAC encoder, and the one with the best quality and the most features supported, including HE-AAC and HE-AACv2. Therefore, we have decided that it is time to remove libvo-aacenc and libaacplus. If you are currently using libvo-aacenc, prepare to transition to the native encoder (aac) when updating to the next version of FFmpeg. In most cases it is as simple as merely swapping the encoder name. If you are currently using libaacplus, start using FDK AAC (libfdk_aac) with an appropriate profile option to select the exact AAC profile that fits your needs. In both cases, you will enjoy an audible quality improvement and as well as fewer licensing headaches. Enjoy! January 16, 2016, FFmpeg 2.8.5, 2.7.5, 2.6.7, 2.5.10 We have made several new point releases (2.8.5, 2.7.5, 2.6.7, 2.5.10). They fix various bugs, as well as CVE-2016-1897 and CVE-2016-1898. Please see the changelog for each release for more details. We recommend users, distributors and system integrators to upgrade unless they use current git master. December 5th, 2015, The native FFmpeg AAC encoder is now stable! After seven years the native FFmpeg AAC encoder has had its experimental flag removed and declared as ready for general use. The encoder is transparent at 128kbps for most samples tested with artifacts only appearing in extreme cases. Subjective quality tests put the encoder to be of equal or greater quality than most of the other encoders available to the public. Licensing has always been an issue with encoding AAC audio as most of the encoders have had a license making FFmpeg unredistributable if compiled with support for them. The fact that there now exists a fully open and truly free AAC encoder integrated directly within the project means a lot to those who wish to use accepted and widespread standards. The majority of the work done to bring the encoder up to quality was started during this year's GSoC by developer Claudio Freire and Rostislav Pehlivanov. Both continued to work on the encoder with the latter joining as a developer and mainainer, working on other parts of the project as well. Also, thanks to Kamedo2 who does comparisons and tests, the original authors and all past and current contributors to the encoder. Users are suggested and encouraged to use the encoder and provide feedback or breakage reports through our bug tracker. October 13th, 2015, Telepoint & MediaHub are now supporting our project A big thank you note goes to our newest supporters: MediaHub and Telepoint. Both companies have donated a dedicated server with free of charge internet connectivity. Here is a little bit about them in their own words: Telepoint is the biggest carrier-neutral data center in Bulgaria. Located in the heart of Sofia on a cross-road of many Bulgarian and International networks, the facility is a fully featured Tier 3 data center that provides flexible customer-oriented colocation solutions (ranging from a server to a private collocation hall) and a high level of security. MediaHub Ltd. is a Bulgarian IPTV platform and services provider which uses FFmpeg heavily since it started operating a year ago. \"Donating to help keep FFmpeg online is our way of giving back to the community\" . Thanks Telepoint and MediaHub for their support! September 29th, 2015, GSoC 2015 results FFmpeg participated to the latest edition of the Google Summer of Code Project. FFmpeg got a total of 8 assigned projects, and 7 of them were successful. We want to thank Google, the participating students, and especially the mentors who joined this effort. We're looking forward to participating in the next GSoC edition! Below you can find a brief description of the final outcome of each single project. Basic servers for network protocols, mentee: Stephan Holljes, mentor: Nicolas George Stephan Holljes's project for this session of Google Summer of Code was to implement basic HTTP server features for libavformat, to complement the already present HTTP client and RTMP and RTSP server code. The first part of the project was to make the HTTP code capable of accepting a single client; it was completed partly during the qualification period and partly during the first week of the summer. Thanks to this work, it is now possible to make a simple HTTP stream using the following commands: ffmpeg -i /dev/video0 -listen 1 -f matroska \\ -c:v libx264 -preset fast -tune zerolatency http://:8080 ffplay http://localhost:8080/ The next part of the project was to extend the code to be able to accept several clients, simultaneously or consecutively. Since libavformat did not have an API for that kind of task, it was necessary to design one. This part was mostly completed before the midterm and applied shortly afterwards. Since the ffmpeg command-line tool is not ready to serve several clients, the test ground for that new API is an example program serving hard-coded content. The last and most ambitious part of the project was to update ffserver to make use of the new API. It would prove that the API is usable to implement real HTTP servers, and expose the points where more control was needed. By the end of the summer, a first working patch series was undergoing code review. Browsing content on the server, mentee: Mariusz Szczepańczyk, mentor: Lukasz Marek Mariusz finished an API prepared by the FFmpeg community and implemented Samba directory listing as qualification task. During the program he extended the API with the possibility to remove and rename files on remote servers. He completed the implementation of these features for file, Samba, SFTP, and FTP protocols. At the end of the program, Mariusz provided a sketch of an implementation for HTTP directory listening. Directshow digital video capture, mentee: Mate Sebok, mentor: Roger Pack Mate was working on directshow input from digital video sources. He got working input from ATSC input sources, with specifiable tuner. The code has not been committed, but a patch of it was sent to the ffmpeg-devel mailing list for future use. The mentor plans on cleaning it up and committing it, at least for the ATSC side of things. Mate and the mentor are still working trying to finally figure out how to get DVB working. Implementing full support for 3GPP Timed Text Subtitles, mentee: Niklesh Lalwani, mentor: Philip Langdale Niklesh's project was to expand our support for 3GPP Timed Text subtitles. This is the native subtitle format for mp4 containers, and is interesting because it's usually the only subtitle format supported by the stock playback applications on iOS and Android devices. ffmpeg already had basic support for these subtitles which ignored all formatting information - it just provided basic plain-text support. Niklesh did work to add support on both the encode and decode side for text formatting capabilities, such as font size/colour and effects like bold/italics, highlighting, etc. The main challenge here is that Timed Text handles formatting in a very different way from most common subtitle formats. It uses a binary encoding (based on mp4 boxes, naturally) and stores information separately from the text itself. This requires additional work to track which parts of the text formatting applies to, and explicitly dealing with overlapping formatting (which other formats support but Timed Text does not) so it requires breaking the overlapping sections into separate non-overlapping ones with different formatting. Finally, Niklesh had to be careful about not trusting any size information in the subtitles - and that's no joke: the now infamous Android stagefright bug was in code for parsing Timed Text subtitles. All of Niklesh's work is committed and was released in ffmpeg 2.8. libswscale refactoring, mentee: Pedro Arthur, mentors: Michael Niedermayer, Ramiro Polla Pedro Arthur has modularized the vertical and horizontal scalers. To do this he designed and implemented a generic filter framework and moved the existing scaler code into it. These changes now allow easily adding removing, splitting or merging processing steps. The implementation was benchmarked and several alternatives were tried to avoid speed loss. He also added gamma corrected scaling support. An example to use gamma corrected scaling would be: ffmpeg -i input -vf scale=512:384:gamma=1 output Pedro has done impressive work considering the short time available, and he is a FFmpeg committer now. He continues to contribute to FFmpeg, and has fixed some bugs in libswscale after GSoC has ended. AAC Encoder Improvements, mentee: Rostislav Pehlivanov, mentor: Claudio Freire Rostislav Pehlivanov has implemented PNS, TNS, I/S coding and main prediction on the native AAC encoder. Of all those extensions, only TNS was left in a less-than-usable state, but the implementation has been pushed (disabled) anyway since it's a good basis for further improvements. PNS replaces noisy bands with a single scalefactor representing the energy of that band, gaining in coding efficiency considerably, and the quality improvements on low bitrates are impressive for such a simple feature. TNS still needs some polishing, but has the potential to reduce coding artifacts by applying noise shaping in the temporal domain (something that is a source of annoying, notable distortion on low-entropy bands). Intensity Stereo coding (I/S) can double coding efficiency by exploiting strong correlation between stereo channels, most effective on pop-style tracks that employ panned mixing. The technique is not as effective on classic X-Y recordings though. Finally, main prediction improves coding efficiency by exploiting correlation among successive frames. While the gains have not been huge at this point, Rostislav has remained active even after the GSoC, and is polishing both TNS and main prediction, as well as looking for further improvements to make. In the process, the MIPS port of the encoder was broken a few times, something he's also working to fix. Animated Portable Network Graphics (APNG), mentee: Donny Yang, mentor: Paul B Mahol Donny Yang implemented basic keyframe only APNG encoder as the qualification task. Later he wrote interframe compression via various blend modes. The current implementation tries all blend modes and picks one which takes the smallest amount of memory. Special care was taken to make sure that the decoder plays correctly all files found in the wild and that the encoder produces files that can be played in browsers that support APNG. During his work he was tasked to fix any encountered bug in the decoder due to the fact that it doesn't match APNG specifications. Thanks to this work, a long standing bug in the PNG decoder has been fixed. For latter work he plans to continue working on the encoder, making it possible to select which blend modes will be used in the encoding process. This could speed up encoding of APNG files. September 9th, 2015, FFmpeg 2.8 We published release 2.8 as new major version. It contains all features and bug fixes of the git master branch from September 8th. Please see the changelog for a list of the most important changes. We recommend users, distributors and system integrators to upgrade unless they use current git master. August 1st, 2015, A message from the FFmpeg project Dear multimedia community, The resignation of Michael Niedermayer as leader of FFmpeg yesterday has come by surprise. He has worked tirelessly on the FFmpeg project for many years and we must thank him for the work that he has done. We hope that in the future he will continue to contribute to the project. In the coming weeks, the FFmpeg project will be managed by the active contributors. The last four years have not been easy for our multimedia community - both contributors and users. We should now look to the future, try to find solutions to these issues, and to have reconciliation between the forks, which have split the community for so long. Unfortunately, much of the disagreement has taken place in inappropriate venues so far, which has made finding common ground and solutions difficult. We aim to discuss this in our communities online over the coming weeks, and in person at the VideoLAN Developer Days in Paris in September: a neutral venue for the entire open source multimedia community. The FFmpeg project. July 4th, 2015, FFmpeg needs a new host UPDATE: We have received more than 7 offers for hosting and servers, thanks a lot to everyone! After graciously hosting our projects (FFmpeg, MPlayer and rtmpdump) for 4 years, Arpi (our hoster) has informed us that we have to secure a new host somewhere else immediately. If you want to host an open source project, please let us know, either on ffmpeg-devel mailing list or irc.freenode.net #ffmpeg-devel. We use about 4TB of storage and at least 4TB of bandwidth / month for various mailing lists, trac, samples repo, svn, etc. March 16, 2015, FFmpeg 2.6.1 We have made a new major release (2.6) and now one week afterward 2.6.1. It contains all features and bugfixes of the git master branch from the 6th March. Please see the Release Notes for a list of note-worthy changes. We recommend users, distributors and system integrators to upgrade unless they use current git master. March 4, 2015, Google Summer of Code FFmpeg has been accepted as a Google Summer of Code Project. If you wish to participate as a student see our project ideas page. You can already get in contact with mentors and start working on qualification tasks. Registration at Google for students will open March 16th. Good luck! March 1, 2015, Chemnitzer Linux-Tage We happily announce that FFmpeg will be represented at Chemnitzer Linux-Tage (CLT) in Chemnitz, Germany. The event will take place on 21st and 22nd of March. More information can be found here We demonstrate usage of FFmpeg, answer your questions and listen to your problems and wishes. If you have media files that cannot be processed correctly with FFmpeg, be sure to have a sample with you so we can have a look! For the first time in our CLT history, there will be an FFmpeg workshop! You can read the details here. The workshop is targeted at FFmpeg beginners. First the basics of multimedia will be covered. Thereafter you will learn how to use that knowledge and the FFmpeg CLI tools to analyse and process media files. The workshop is in German language only and prior registration is necessary. The workshop will be on Saturday starting at 10 o'clock. We are looking forward to meet you (again)! December 5, 2014, FFmpeg 2.5 We have made a new major release (2.5) It contains all features and bugfixes of the git master branch from the 4th December. Please see the Release Notes for a list of note-worthy changes. We recommend users, distributors and system integrators to upgrade unless they use current git master. October 10, 2014, FFmpeg is in Debian unstable again We wanted you to know there are FFmpeg packages in Debian unstable again. A big thank-you to Andreas Cadhalpun and all the people that made it possible. It has been anything but simple. Unfortunately that was already the easy part of this news. The bad news is the packages probably won't migrate to Debian testing to be in the upcoming release codenamed jessie. Read the argumentation over at Debian. However things will come out in the end, we hope for your continued remarkable support! October 8, 2014, FFmpeg secured a place in OPW! Thanks to a generous 6K USD donation by Samsung (Open Source Group), FFmpeg will be welcoming at least 1 \"Outreach Program for Women\" intern to work with our community for an initial period starting December 2014 (through March 2015). We all know FFmpeg is used by the industry, but even while there are countless products building on our code, it is not at all common for companies to step up and help us out when needed. So a big thank-you to Samsung and the OPW program committee! If you are thinking on participating in OPW as an intern, please take a look at our OPW wiki page for some initial guidelines. The page is still a work in progress, but there should be enough information there to get you started. If you, on the other hand, are thinking on sponsoring work on FFmpeg through the OPW program, please get in touch with us at opw@ffmpeg.org. With your help, we might be able to secure some extra intern spots for this round! September 15, 2014, FFmpeg 2.4 We have made a new major release (2.4) It contains all features and bugfixes of the git master branch from the 14th September. Please see the Release Notes for a list of note-worthy changes. We recommend users, distributors and system integrators to upgrade unless they use current git master. August 20, 2014, FFmpeg 2.3.3, 2.2.7, 1.2.8 We have made several new point releases (2.3.3, 2.2.7, 1.2.8). They fix various bugs, as well as CVE-2014-5271 and CVE-2014-5272. Please see the changelog for more details. We recommend users, distributors and system integrators to upgrade unless they use current git master. July 29, 2014, Help us out securing our spot in OPW Following our previous post regarding our participation on this year's OPW (Outreach Program for Women), we are now reaching out to our users (both individuals and companies) to help us gather the needed money to secure our spot in the program. We need to put together 6K USD as a minimum but securing more funds would help us towards getting more than one intern. You can donate by credit card using Click&Pledge and selecting the \"OPW\" option. If you would like to donate by money transfer or by check, please get in touch by e-mail and we will get back to you with instructions. Thanks! July 20, 2014, New website The FFmpeg project is proud to announce a brand new version of the website made by db0. While this was initially motivated by the need for a larger menu, the whole website ended up being redesigned, and most pages got reworked to ease navigation. We hope you'll enjoy browsing it. July 17, 2014, FFmpeg 2.3 We have made a new major release (2.3) It contains all features and bugfixes of the git master branch from the 16th July. Please see the Release Notes for a list of note-worthy changes. We recommend users, distributors and system integrators to upgrade unless they use current git master. July 3, 2014, FFmpeg and the Outreach Program For Women FFmpeg has started the process to become an OPW includer organization for the next round of the program, with internships starting December 9. The OPW aims to \"Help women (cis and trans) and genderqueer to get involved in free and open source software\". Part of the process requires securing funds to support at least one internship (6K USD), so if you were holding on your donation to FFmpeg, this is a great chance for you to come forward, get in touch and help both the project and a great initiative! We have set up an email address you can use to contact us about donations and general inquires regarding our participation in the program. Hope to hear from you soon! June 29, 2014, FFmpeg 2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14 We have made several new point releases (2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14). They fix a security issue in the LZO implementation, as well as several other bugs. See the git log for details. We recommend users, distributors and system integrators to upgrade unless they use current git master. May 1, 2014, LinuxTag Once again FFmpeg will be represented at LinuxTag in Berlin, Germany. The event will take place from 8th to 10th of May. Please note that this year's LinuxTag is at a different location closer to the city center. We will have a shared booth with XBMC and VideoLAN. If you have media files that cannot be processed correctly with FFmpeg, be sure to have a sample with you so we can have a look! More information about LinuxTag can be found here We are looking forward to see you in Berlin! April 18, 2014, OpenSSL Heartbeat bug Our server hosting the Trac issue tracker was vulnerable to the attack against OpenSSL known as \"heartbleed\". The OpenSSL software library was updated on 7th of April, shortly after the vulnerability was publicly disclosed. We have changed the private keys (and certificates) for all FFmpeg servers. The details were sent to the mailing lists by Alexander Strasser, who is part of the project server team. Here is a link to the user mailing list archive . We encourage you to read up on \"OpenSSL heartbleed\". It is possible that login data for the issue tracker was exposed to people exploiting this security hole. You might want to change your password in the tracker and everywhere else you used that same password. April 11, 2014, FFmpeg 2.2.1 We have made a new point releases (2.2.1). It contains bug fixes for Tickets #2893, #3432, #3469, #3486, #3495 and #3540 as well as several other fixes. See the git log for details. March 24, 2014, FFmpeg 2.2 We have made a new major release (2.2) It contains all features and bugfixes of the git master branch from 1st March. A partial list of new stuff is below: - HNM version 4 demuxer and video decoder - Live HDS muxer - setsar/setdar filters now support variables in ratio expressions - elbg filter - string validation in ffprobe - support for decoding through VDPAU in ffmpeg (the -hwaccel option) - complete Voxware MetaSound decoder - remove mp3_header_compress bitstream filter - Windows resource files for shared libraries - aeval filter - stereoscopic 3d metadata handling - WebP encoding via libwebp - ATRAC3+ decoder - VP8 in Ogg demuxing - side & metadata support in NUT - framepack filter - XYZ12 rawvideo support in NUT - Exif metadata support in WebP decoder - OpenGL device - Use metadata_header_padding to control padding in ID3 tags (currently used in MP3, AIFF, and OMA files), FLAC header, and the AVI \"junk\" block. - Mirillis FIC video decoder - Support DNx444 - libx265 encoder - dejudder filter - Autodetect VDA like all other hardware accelerations We recommend users, distributors and system integrators to upgrade unless they use current git master. February 3, 2014, Chemnitzer Linux-Tage We happily announce that FFmpeg will be represented at `Chemnitzer Linux-Tage' in Chemnitz, Germany. The event will take place on 15th and 16th of March. More information can be found here We invite you to visit us at our booth located in the Linux-Live area! There we will demonstrate usage of FFmpeg, answer your questions and listen to your problems and wishes. If you have media files that cannot be processed correctly with FFmpeg, be sure to have a sample with you so we can have a look! We are looking forward to meet you (again)! February 9, 2014, trac.ffmpeg.org / trac.mplayerhq.hu Security Breach The server on which FFmpeg and MPlayer Trac issue trackers were installed was compromised. The affected server was taken offline and has been replaced and all software reinstalled. FFmpeg Git, releases, FATE, web and mailinglists are on other servers and were not affected. We believe that the original compromise happened to a server, unrelated to FFmpeg and MPlayer, several months ago. That server was used as a source to clone the VM that we recently moved Trac to. It is not known if anyone used the backdoor that was found. We recommend all users to change their passwords. Especially users who use a password on Trac that they also use elsewhere, should change that password at least elsewhere. November 12, 2013, FFmpeg RFP in Debian Since the splitting of Libav the Debian/Ubuntu maintainers have followed the Libav fork. Many people have requested the packaging of ffmpeg in Debian, as it is more feature-complete and in many cases less buggy. Rogério Brito, a Debian developer, has proposed a Request For Package (RFP) in the Debian bug tracking system. Please let the Debian and Ubuntu developers know that you support packaging of the real FFmpeg! See Debian ticket #729203 for more details. October 28, 2013, FFmpeg 2.1 We have made a new major release (2.1) It contains all features and bugfixes of the git master branch from 28th October. A partial list of new stuff is below: - aecho filter - perspective filter ported from libmpcodecs - ffprobe -show_programs option - compand filter - RTMP seek support - when transcoding with ffmpeg (i.e. not streamcopying), -ss is now accurate even when used as an input option. Previous behavior can be restored with the -noaccurate_seek option. - ffmpeg -t option can now be used for inputs, to limit the duration of data read from an input file - incomplete Voxware MetaSound decoder - read EXIF metadata from JPEG - DVB teletext decoder - phase filter ported from libmpcodecs - w3fdif filter - Opus support in Matroska - FFV1 version 1.3 is stable and no longer experimental - FFV1: YUVA(444,422,420) 9, 10 and 16 bit support - changed DTS stream id in lavf mpeg ps muxer from 0x8a to 0x88, to be more consistent with other muxers. - adelay filter - pullup filter ported from libmpcodecs - ffprobe -read_intervals option - Lossless and alpha support for WebP decoder - Error Resilient AAC syntax (ER AAC LC) decoding - Low Delay AAC (ER AAC LD) decoding - mux chapters in ASF files - SFTP protocol (via libssh) - libx264: add ability to encode in YUVJ422P and YUVJ444P - Fraps: use BT.709 colorspace by default for yuv, as reference fraps decoder does - make decoding alpha optional for prores, ffv1 and vp6 by setting the skip_alpha flag. - ladspa wrapper filter - native VP9 decoder - dpx parser - max_error_rate parameter in ffmpeg - PulseAudio output device - ReplayGain scanner - Enhanced Low Delay AAC (ER AAC ELD) decoding (no LD SBR support) - Linux framebuffer output device - HEVC decoder, raw HEVC demuxer, HEVC demuxing in TS, Matroska and MP4 - mergeplanes filter We recommend users, distributors and system integrators to upgrade unless they use current git master. Past news Hosting provided by telepoint.bg",
    "commentLink": "https://news.ycombinator.com/item?id=39938703",
    "commentBody": "FFmpeg 7.0 Released (ffmpeg.org)208 points by gyan 5 hours agohidepastfavorite52 comments bdd8f1df777b 7 minutes agoSo I'm trying to build ffmpeg via vcpkg today, and it turned out multiple of its dependencies are transitively depending on liblzma, but the downloading of liblzma source has been disabled by GitHub in light of the recent xz backdoor. reply publius_0xf3 1 hour agoprevThe winget version is still stuck on v6.1.1. Valve pls fix. reply amenhotep 7 minutes agoparentWinget could be so good. It's annoying that there are so many little things that make it total rubbish. reply lstamour 1 hour agoparentprevThe winget community repo is open source: https://github.com/microsoft/winget-pkgs/tree/master/manifes... reply Depurator 4 hours agoprevrust-ffmpeg already seems to have support for 7.0: https://github.com/zmwangx/rust-ffmpeg/pull/178 reply metadat 3 hours agoparentAt first I thought this was a full rewrite in Rust, but it's not Why would one want a \"Safe FFMpeg Wrapper\"? Edit: Got it, provides a rust API library to ffmpeg. Thanks @CJefferson. reply CJefferson 3 hours agorootparentI use it — because I am writing a rust program, and want to use ffmpeg functionality. What’s the alternative? I could wrap the C API, and then try to make a nice rust interface from that, but then that’s exactly what this package does, so I don’t want to repeat the work. reply matsemann 44 minutes agorootparent> What’s the alternative? I often just exec ffmpeg from whatever language I'm using (as a command line thing). Not very ergonomic, but the nice thing is that it's 1:1 with all examples and other uses of ffmpeg. But I guess it depends on how deep into ffmpeg you're doing stuff. Mine is mostly to point it at doing something non advanced with a file and that's it. reply oefrha 2 hours agorootparentprevBasically no one rewrites FFmpeg in recent years, in any language, at least not in the open source scene (and judging from the known usage of FFmpeg in world’s premier providers of multimedia content, probably not in the commercial scene either). It’s both too good and too daunting. reply xcdzvyn 55 minutes agorootparentI'm not sure I'd describe FFmpeg's CLI as \"too good\" reply Timwi 51 minutes agorootparentThe parent comment was not referring to the CLI exclusively. reply ksec 4 hours agoprevSurprised even MPEG-5 EVC made it. Unfortunately the VVC Decoder didn't quite make it ( Edit : Officially ) . I guess we will have to wait until version 7.1. Still waiting for x266. reply gyan 4 hours agoparentVVC decoder is available, but it's flagged as experimental, so you have to prefix `-strict experimental` before the VVC input `-i`. reply ksec 4 hours agorootparentThanks Yes I meant officially. Was hoping the we could set stage for VVC little earlier. I know VVC is not popular on HN or literally anywhere on Internet but I do hope to see it moving forward instead of something like MPEG-5 EVC which is somewhat dead in the water. reply cm2187 2 hours agorootparentI don't know that having so many codecs is a good thing unless they really add something. How does it compare to av1 (which I was under the impression is coming to be the natural successor of hevc, with hardware support)? reply ksec 1 hour agorootparentComparing to AV1, VVC / H.266 is expected to offer 20-30% reduction in Bit-Rate with similar quality at similar level of computational complexity. And it is already deployed and used in real world in China and India. I believe Brazil are looking to use it as their next generation codec for broadcasting along with LCEVC. reply drmpeg 30 minutes agorootparentHere's the Brazil website. https://forumsbtvd.org.br/tv3_0/ And their video codec testing. https://forumsbtvd.org.br/wp-content/uploads/2024/03/SBTVD-T... reply drmpeg 28 minutes agorootparentprevAnd ATSC 3.0 will also be using VVC. https://prdatsc.wpenginepowered.com/wp-content/uploads/2024/... reply MrYellowP 3 hours agoprevMultiThreading! Finally! \\o/ reply minroot 3 hours agoprevWhat is a good GUI interface for FFmpeg? reply speedgoose 3 hours agoparenthttps://handbrake.fr/ has been around for a long time. reply Mashimo 1 hour agorootparentBut that is not ffmpeg, is it? I know they use some of the same encoding libraries. reply mort96 10 minutes agorootparentffmpeg is, mainly, a bunch of libraries. libavcodec, libavformat, libswresample, etc, is almost all of ffmpeg. If a project is using those libraries, it's using ffmpeg. The ffmpeg command line utility is \"just\" an interface to those libraries. reply 0xcoffee 36 minutes agorootparentprevThere is a MR to update it to FFMPEG 7: https://github.com/HandBrake/HandBrake/pull/5884 reply your_challenger 3 hours agoparentprevUse the terminal. There does not exist a single GUI interface that does everything ffmpeg does. Use ChatGPT to help you find the right command for your need. reply leokennis 2 hours agorootparentWhenever I search for ffmpeg commands, there is always some person suggesting an 8 liner with 25 arguments, and next to that someone suggesting a command with two -i and one -o argument. On visual inspection both do the same, but I’m always left with the feeling that I did something wrong or just “got lucky” with the shorter command. I would love a “unless you’re a pro with hyper specific needs, forget these 90% of arguments and only use this 10% in this way” type of guide. reply oefrha 1 hour agorootparent> 8 liner with 25 arguments That’s tame for FFmpeg, likely just specifying a bunch of encoder parameters the simpler command left out for defaults, maybe with some input/output streams explicitly spelled out. If you want to look at really incomprehensible FFmpeg commands, try anything with filtergraphs. reply mort96 8 minutes agorootparentprevMy experience is that ChatGPT is dreadful at everything but the simplest ffmpeg invocations, and will often produce command lines with subtle quirks (such as \"only works if the input is an even number of pixels wide\"). But then again, my experience is that ChatGPT is dreadful at everything but the simplest anything. reply enlyth 46 minutes agorootparentprevThat's such a clunky workflow and takes so long, sometimes you want to drag and drop a file, tick a couple of checkboxes and click a button reply matsemann 42 minutes agorootparentBut which checkboxes? Ffmpeg would probably need a hundred thousand. reply lionkor 10 minutes agorootparentcategorize, add tooltips, show defaults... this is some basic UI design stuff, should be possible. reply whywhywhywhy 17 minutes agorootparentprevuse ffprobe on the file first then paste that output and what you want done to it. Helps if there’s something about the file/codec that GPT would have to work around. Had better success that way. reply notachatbot1234 2 hours agorootparentprev> Use ChatGPT to help you find the right command for your need. That would give you hallucinated commands, not commands that actually exist or make sense. Better read documentation or ask experienced humans. reply KeplerBoy 1 hour agorootparentCan't confirm. LLMs tend to be oddly good at FFmpeg and give you a good starting point you can work with. reply _flux 46 minutes agorootparentprevSo this is a good use case for it: you will most likely get immediate feedback if it's wrong, and a bit delayed feedback if it achieved the wrong thing; and you can prod it to try harder. LLMs are best used when you can easily verify the results. reply renewiltord 58 minutes agorootparentprevNo, that's bad advice imho. ChatGPT (and Claude etc.) are all pretty damned good at this. Strongly recommend using them over reading the docs or asking other people if you're getting started. reply whywhywhywhy 19 minutes agoparentprevThere isn’t one. Handbrake very weird about interpreting what you want and you’ll find yourself having to double check dimensions and stuff every time and the queue is very fiddly. On the Mac version at least. Don’t think anything exists like XLD for FFMPEG video where you can just drop a file in set the quality and codec and get the exact same dimensiond file out every time. reply addandsubtract 45 minutes agoparentprevDepends on your needs. If you just want to cut and trim videos, LosslessCut[1] is great and simple to use. [1] https://github.com/mifi/lossless-cut reply TheFragenTaken 2 hours agoparentprevIt's not quite a GUI, but I usually refer to https://alfg.dev/ffmpeg-commander/. reply n3m4c 55 minutes agoparentprevXMedia Recode is pretty good. https://www.xmedia-recode.de/en/index.php reply laborcontract 1 hour agoparentprevPermute for Mac is an elite app. reply jauntywundrkind 5 hours agoprevIAMF/ambisonics looks so cool, but it's so unclear how is plebes would play around with it & explore it's use. Crazy how much DirectX (DXVA) support got added. reply jayd16 2 hours agoparentMight look into ambisonic support in Unreal/Unity. reply wyldfire 5 hours agoprevCritical Microsoft customers will be so relieved ;) reply OsrsNeedsf2P 4 hours agoparentCan someone explain? reply alberth 4 hours agorootparenthttps://news.ycombinator.com/item?id=39912916 reply broknbottle 5 hours agoparentprevI lol’d reply gyan 5 hours agoprevChangelog: - DXV DXT1 encoder - LEAD MCMP decoder - EVC decoding using external library libxevd - EVC encoding using external library libxeve - QOA decoder and demuxer - aap filter - demuxing, decoding, filtering, encoding, and muxing in the - ffmpeg CLI now all run in parallel - enable gdigrab device to grab a window using the hwnd=HANDLER syntax - IAMF raw demuxer and muxer - D3D12VA hardware accelerated H264, HEVC, VP9, AV1, MPEG-2 and VC1 decoding - tiltandshift filter - qrencode filter and qrencodesrc source - quirc filter - lavu/eval: introduce randomi() function in expressions - VVC decoder (experimental) - fsync filter - Raw Captions with Time (RCWT) closed caption muxer - ffmpeg CLI -bsf option may now be used for input as well as output - ffmpeg CLI options may now be used as -/opt , which is equivalent - to -opt > - showinfo bitstream filter - a C11-compliant compiler is now required; note that this requirement - will be bumped to C17 in the near future, so consider updating your - build environment if it lacks C17 support - Change the default bitrate control method from VBR to CQP for QSV encoders. - removed deprecated ffmpeg CLI options -psnr and -map_channel - DVD-Video demuxer, powered by libdvdnav and libdvdread - ffprobe -show_stream_groups option - ffprobe (with -export_side_data film_grain) now prints film grain metadata - AEA muxer - ffmpeg CLI loopback decoders - Support PacketTypeMetadata of PacketType in enhanced flv format - ffplay with hwaccel decoding support (depends on vulkan renderer via libplacebo) - dnn filter libtorch backend - Android content URIs protocol - AOMedia Film Grain Synthesis 1 (AFGS1) - RISC-V optimizations for AAC, FLAC, JPEG-2000, LPC, RV4.0, SVQ, VC1, VP8, and more - Loongarch optimizations for HEVC decoding - Important AArch64 optimizations for HEVC - IAMF support inside MP4/ISOBMFF - Support for HEIF/AVIF still images and tiled still images - Dolby Vision profile 10 support in AV1 - Support for Ambient Viewing Environment metadata in MP4/ISOBMFF - HDR10 metadata passthrough when encoding with libx264, libx265, and libsvtav1 reply mixmastamyk 4 hours agoprevPlease vouch for gyan's changelog comment below. Is flagged/dead for some reason? reply eisa01 4 hours agoparenthttps://git.ffmpeg.org/gitweb/ffmpeg.git/blob/HEAD:/Changelo... reply ksec 4 hours agoparentprevSame question. reply andrewinardeer 4 hours agoprev [–] https://en.m.wikipedia.org/wiki/Edsger_W._Dijkstra reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FFmpeg is a versatile tool for recording, converting, and streaming audio and video on various operating systems.",
      "The software regularly releases updates with new features like encoders, decoders, filters, and performance optimizations, benefiting from participation in the Google Summer of Code program.",
      "Users are recommended to update to the newest version to leverage the latest enhancements and bug fixes available."
    ],
    "commentSummary": [
      "Users are discussing the release of FFmpeg 7.0 and its dependencies, alternatives, and codecs like VVC, comparing it with AV1 for bitrate reduction and quality.",
      "Some mention using Rust wrappers for FFmpeg, exploring challenges in rewriting it in different languages, GUI interfaces' use, frustrations with complexity, and the need for simpler guides.",
      "Updates in XMedia Recode, Permute, and FFmpeg CLI, discussions on ambisonic support, DirectX enhancements, encoding with different libraries, and seeking clarifications on various topics, with suggestions of tools like LosslessCut and XMedia Recode for easier video editing."
    ],
    "points": 208,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1712291464
  },
  {
    "id": 39930887,
    "title": "The Great American Rail-Trail: Connecting 3,700 Miles Across the US",
    "originLink": "https://www.railstotrails.org/site/greatamericanrailtrail/content/route/",
    "originBody": "Great American Rail-Trail Route Stretching more than 3,700 miles between Washington, D.C., and Washington State The Preferred Route The preferred route of the Great American Rail-Trail connects 150+ existing rail-trails, greenways and other multiuse paths spanning more than 3,700 miles. These trails are hosting the Great American through their communities, making possible this grand vision of a nation connected by trails. And with more than 52% of the route already on the ground, now is the time to experience the Great American Rail-Trail! Explore the Great American Rail-Trail map above to learn more about the route and to find trails you can visit today. Great American Rail-Trail Route Interactive Map Great American Rail-Trail interactive map (2023)Image courtesy RTC Explore the interactive map to learn more about the Great American route. View video tutorial. STATES ON ROUTE TRAILS ON ROUTE DOWNLOAD MAP The “Great American” Criteria Key to the process of defining the preferred route of the Great American Rail-Trail with states and trail partners was confirming a route across the country that would provide the highest-quality experience for all types of trail users—from long-distance cyclists and runners to casual daily explorers and everyone in between. Trail criteria were developed to ensure the Great American Rail-Trail would provide safe, nonmotorized travel on a route that is entirely walkable and bikeable. These trail criteria specify that: The Great America Rail-Trail be one contiguous route that is reasonably direct between Washington, D.C., and Washington State, and across each of the states it traverses. It will be separated from vehicle traffic—a minimum of 80 percent initially and ultimately entirely separated when the trail is complete. It will comprise existing trails to the extent possible and will represent trail priorities of the states and local jurisdictions that will host it. It will follow the most feasible route with regards to topography, geography and political priorities. It will serve as a catalyst for local economic development, including providing services for long-distance travelers. Download Criteria States on Route The Great American Rail-Trail is a trail for everyone—separated from vehicle traffic, with gentle grades and an unparalleled experience. Key to the process of defining the preferred route of the Great American Rail-Trail was confirming a route across the country that would provide the highest-quality experience for all types of trail users—from long-distance cyclists and runners to casual daily explorers and everyone in between. RTC worked in close partnership with dozens of state agency representatives and hundreds of trail partners from across the country to assess and define the preferred route for the Great American to ensure a contiguous and reasonably direct route as well as one that complemented state priorities. This is America’s trail, and it is the trail of each of the states that will call it home. DC Trails D.C.’s Capital Crescent TrailPhoto by Hung Tran Maryland Trails C&O Canal TowpathPhoto by Kathleen Richardson PA Trails WV Trails Panhandle Trail crosses West Virginia and PennsylvaniaPhoto by TrailLink user adoamm Ohio Trails Ohio’s Ohio to Erie TrailPhoto by TrailLink user epet4mail Indiana Trails Indiana’s Cardinal GreenwayPhoto by Tony Valainis Illinois Trails Hennepin Canal ParkwayPhoto courtesy of Friends of the Hennepin Canal Iowa Trails Iowa’s Cedar Valley Nature TrailPhoto by Liz Zabel, courtesy GO Cedar Rapids Nebraska Trails Nebraska’s Cowboy TrailPhoto by TrailLink user lisa.jarnigan Wyoming Trails Wyoming’s Casper Rail TrailPhoto by Kevin Belanger Montana Trails Montana’s Headwaters Trail SystemPhoto by Scott Stark Idaho Trails Idaho’s NorPac TrailPhoto by TrailLink user mt_top Washington Trails Palouse to Cascades State Park Trail near Kittitas ValleyPhoto by Marilyn Hedges Trails on Route Explore Trails on TrailLink Washington, D.C. National Mall Trails Rock Creek Park Trails K Street/Water Street Cycle Track Capital Crescent Trail Chesapeake & Ohio Canal National Historical Park Maryland Chesapeake & Ohio Canal National Historical Park Great Allegheny Passage (gaptrail.org) Pennsylvania Great Allegheny Passage (gaptrail.org) Three Rivers Heritage Trail Montour Trail Panhandle Trail West Virginia Panhandle Trail Ohio Conotton Creek Trail Zoar Valley Trai Ohio to Erie Trail Ohio & Erie Canalway Towpath Trail Heartland Trail Holmes County Trail (Part of the Ohio to Erie Trail) Mohican Valley Trail (Part of the Ohio to Erie Trail) Kokosing Gap Trail (Part of the Ohio to Erie Trail) Downtown Connector Trail (Part of the Ohio to Erie Trail) Heart of Ohio Trail (Part of the Ohio to Erie Trail) Meredith State Road Trail (Part of the Ohio to Erie Trail) Sandel Legacy Trail (Part of the Ohio to Erie Trail) Thomas W. Hopper Legacy Trail (Part of the Ohio to Erie Trail) Hoover Scenic Trail (Part of the Ohio to Erie Trail) Genoa Trail (Part of the Ohio to Erie Trail) Westerville B&W (Part of the Ohio to Erie Trail) Alum Creek Greenway Trail (Part of the Ohio to Erie Trail) Downtown Connector Trail (Part of the Ohio to Erie Trail) Scioto Greenway Trail (Part of the Ohio to Erie Trail) Camp Chase Trail (Part of the Ohio to Erie Trail) Roberts Pass Trail (Part of the Ohio to Erie Trail) Prairie Grass Trail (Part of the Ohio to Erie Trail) Creekside Trail Mad River Trail Great Miami River Trail Wolf Creek Trail Indiana Cardinal Greenway Sweetser Switch Trail Converse Junction Trail Nickel Plate Trail Monterey Erie Trail North Judson Erie Trail Veterans Memorial Trail Erie Lackawanna Trail Pennsy Greenway Illinois Pennsy Greenway Thorn Creek Trail Old Plank Road Trail Illinois & Michigan Canal State Trail Hennepin Canal Parkway Beacon Harbor Parkway Trail Great River Trail Government/Arsenal Bridge Iowa Government/Arsenal Bridge Mississippi River Trail/Riverfront Trail Running River Trail System Kent Stein to Deep Lakes Park Trail Hoover Nature Trail Cedar Valley Nature Trail Gilbert Drive Trail Evansdale Nature Trail River Forest Road Trail/Cedar River Levee Trail Cedar Valley Lakes Trail South Riverside Trail Cedar Prairie Trail Sergeant Road Trail Pioneer Trail River’s Edge Trail Linn Creek Recreational Trail Iowa 330/US 30 Trail Heart of Iowa Nature Trail High Trestle Trail Hiawatha Trail Raccoon River Valley Trail T-Bone Trail Northern Atlantic Trail System Railroad Highway Trail Valley View Trail Lake Manawa Trail Veterans Memorial Trail Western Historic Trails Center Link Iowa Riverfront Trail Bob Kerrey Pedestrian Bridge Nebraska Bob Kerrey Pedestrian Bridge Burt Street Trail Turner Boulevard Trail Field Club Trail South Omaha Trail Keystone Trail West Papio Trail Walnut Creek Lake Trail MoPac Trail (Springfield) Platte River Connection MoPac East Trail MoPac Trail West Antelope Valley Trail Salt Creek Levee Trail Oak Lake Connector Trail North 1st Street Trail Superior Street Trail Highlands Trail Oak Creek Trail Third Avenue Viaduct Trail Robert White Trail Bob Lake Trail Cowboy Recreation and Nature Trail White River Trail Wyoming Douglas Trail System Al’s Way Casper Rail Trail Platte River Trail Greybull River Walk East Bighorn River Trail Beck Lake Park Bike Trail Montana Highway 89 South Pedestrian Trail Livingston Depot Center Trail Bozeman to Bridger Mountains Trail/Path to the “M” and Drinking Horse Story Mill Spur Front Street Connector Oak Street Trail North 19th Avenue Trail Valley Center Trail Jackrabbit Lane Shared-Use Path Manhattan to the Gallatin River Trail Headwaters Trail System Milwaukee Road Rail-Trail (Thompson Park) Silver Bow Creek Greenway Arrow Stone Park Trail Old Yellowstone Trail Piltzville Trail Bonner Streetcar Trail Canyon River Trail Milwaukee Trail Mullan Road Trail A.J. Hoyt Memorial Trail Route of the Olympian NorPac Trail Idaho NorPac Trail Trail of the Coeur d’Alenes Washington Palouse to Cascades State Park Trail Snoqualmie Valley Trail Preston-Snoqualmie Trail Issaquah-Preston Trail East Lake Sammamish Trail Marymoor Connector Trail Sammamish River Trail Burke-Gilman Trail 34th Street Protected Bike Lane Fremont Bridge Ship Canal Trail Interbay Protected Bike Lane Elliot Bay Trail (Terminal 91 Bike Path) Seattle Waterfront Pathway Sound to Olympics Trail Olympic Discovery Trail More than 150+ trails will host the Great American Rail-Trail. Protecting these trails through advocacy, volunteerism and donations is key to bringing the vision of the Great American to life. Learn about Host Trails Meet the Gateway Trails In every state along the Great American Rail-Trail route, iconic trails make possible this grand vision of a nation connected by trails. These trails have been built through the hard work and ingenuity of the trails community—nonprofit partners, state agencies and volunteers who have rolled up their sleeves to protect and preserve these priceless corridors. Here are the gateways to the Great American Rail-Trail from east to west: D.C. and Maryland’s Capital Crescent Trail D.C. and Maryland’s C&O Canal Towpath West Virginia and Pennsylvania’s Panhandle Trail Ohio’s Ohio to Erie Trail Indiana’s Cardinal Greenway Illinois’ Hennepin Canal Parkway Iowa’s Cedar Valley Nature Trail Nebraska’s Cowboy Recreation and Nature Trail Wyoming’s Casper Rail Trail Montana’s Headwaters Trail System Idaho’s Trail of the Coeur d’Alenes Washington’s Palouse to Cascades State Park Trail Support the Great American Rail-Trail The Great American Rail-Trail will stand alongside our country’s iconic landmarks as a national treasure. You can help by making a gift to RTC, supporting the national leadership and on-the-ground support—the work to organize people, plans and ideas; trail planning and community engagement; the advocacy and marketing that is necessary to completing the Great American Rail-Trail. Donate Get Great American Rail-Trail Gear Shop Store",
    "commentLink": "https://news.ycombinator.com/item?id=39930887",
    "commentBody": "The Great American Rail-Trail (railstotrails.org)208 points by peter_d_sherman 19 hours agohidepastfavorite86 comments l3db3tt3r 16 hours agoI finished the GART this summer. 69 days, ~3800 miles, started in Washington DC at the Capital Building moving East to Wast. The difference between riding the completed sections (not dealing with auto-traffic), and the various road (dirt,gravel,asphalt,HWY) sections to connect the various trails had me in very different mental spaces - and although that may seem obvious, it was something entirely profound to experience. It really adds to the realization of how ambitious this project is, and I think solidified the worth/value I have of the endeavor being completing. (It’s estimated at something like 60% completed.) reply Kon-Peki 16 hours agoparent> Toward the end of 2021, the Iowa-Illinois Memorial Bridge (I-74 Bridge) was completed, providing a new multimodal crossing of the Mississippi River between East Moline, Illinois, and Bettendorf, Iowa. Prior to this bridge completion, the Great American Rail-Trail was routed across the Government/Arsenal Bridge farther west between Rock Island, Illinois, and Davenport, Iowa. Both bridges are about the same length and accommodate trail users. Therefore, instead of choosing which bridge should be the official crossing of the Great American Rail-Trail, both bridges are shown on the route map, and trail users can choose for themselves. Which bridge did you choose to cross the Mississippi? From photos, it looks like the bike/pedestrian path on the I-74 bridge is dramatically superior and should be the \"official\" crossing, but I can understand that the historical significance of the Government/Arsenal bridge keeps that on the map (it is the 2nd rebuild of the very first bridge to ever cross the Mississippi and was the subject of the lawsuit that got a country bumpkin lawyer named Abe Lincoln into the national newspapers for the first time). reply l3db3tt3r 15 hours agorootparentI took the I-74 bridge; it lined up with my accommodations, and the ‘newness’ of it was touted as something to see/do at the time. reply TheGRS 10 hours agoparentprevThat's awesome. So I'm thinking I might want to do the Washington St leg of this some time, do you remember about how long that part took? reply ch33zer 16 hours agoparentprevDid you blog or document your experience anywhere? reply l3db3tt3r 16 hours agorootparentI started with some idea/intention of doing so, but what I ended up with was mostly pictures, and short videos shared to social media. … I think I may be one of those people who likes the idea of being a ‘writer’ or blogger, but lacks the enjoyment/drive of the work/time/effort it actually takes. OR I need to engage in the practice and make it a routine, create discipline, work out the kinks, before also taking on the demands of a new adventure. reply yolo3000 12 hours agorootparentprevI've recently watched this guy's journey, maybe something interesting for you as well https://www.youtube.com/watch?v=-OfNePKRPGY reply plowjockey 15 hours agoprevI recall this being very contentious when the rail banking act was enacted back in the '80s. At that time the right of way that became the Katy Trail in Missouri was in dispute but the adjacent land owners lost. More recently and closer to home, Union Pacific abandoned part of its Valley Subdivision from Lincoln,NE to Marysville, KS and successfully railbanked the entire route. Adjacent landowners fought it to no avail. Today the trail has three segments--Lincoln to Beatrice, NE, Beatrice to the Kansas/Nebraska state line, and from the state line to Marysville, KS. As I understand, railbanking is a way for railroads to abandon a route while protecting the right of way for possible future railroad use. In the mean time the trails make use of the right of way and maintain it. reply bombcar 14 hours agoparentThat's what rail banking is - the railroads want to keep the property (it's somewhat valuable though weirdly shaped) but more importantly want to keep the ability to run a line (which is potentially insanely valuable, because running a line where one hasn't been before is a hell of eminent domain and negotiations). Railbanking is a good way to let the land be used while not forcing the railroad to \"pretend\" it is still an active line. reply ethbr1 13 hours agorootparentIMHO, it's a good idea. Running new contiguous multi-state rail lines suffer from a host of unique problems -- not least that the more uncooperative last-link property owner holdouts are, the more their property becomes worth. Good use case for a public-private bargain, where the public gets the utility of a trail (and potentially a functional railroad, if needed in the future) and private gets to keep the most valuable land utilization rights (ability to convert back to railroad). One of my historical fascinations is watching the co-evolution of rail and telecommunications line networks, since both have similar needs. reply analog31 11 hours agoparentprevDon't know if it's the case today, but as I recall, the rights-of-way were extremely valuable for routing fiber optic cables. reply toomuchtodo 11 hours agorootparentAlso more recently for HVDC transmission. SOO Green and such. https://soogreen.com/ https://www.spglobal.com/marketintelligence/en/news-insights... reply plowjockey 6 hours agorootparentprevThanks for the memory jog. There is fiber optic running along the line from Marysville to Beatrice so that may have been a consideration for railbanking the line. reply hed 7 hours agoparentprevYes I remember watching video interviews of folks in Missouri who said it should have reverted to them instead of being made available for a linear park. reply maztaim 15 hours agoprevLove them. Support them. Completed a 6 day bike trip from Pittsburgh to Washington DC on the GAP and C&O trails back in 2012. I would start as early as I could, often breaking trail for the day (wear a bandana and sunglasses or eat spiderwebs all day). Lots of deer and blue herons to hang out with. It was very peaceful and quiet, but I mostly remember green tunnel trails for most of the trip. There are wonderful stops and vistas along the way. As you leave the paw-paw tunnel, one of my favorite views of the valley quickly pass by, the closer you get to Georgetown, the better maintained the C&O is; the more beautiful the trail becomes. I actually rode the C&O trail a bunch of times in Scouts and with my grandparents as a child. I think these are great for any skill level, including beginners. The biggest advantage for me is the fact there are very few roads you need to ride on. Because they are typically on old railways, or canals, they are usually not steep grades (though don't let somebody tell you \"it's only a 6% grade!\", because it sucks going uphill for lots of miles). You can split up the trails in to day, weekend, or week-long trips. Especially on the GAP and C&O there are plenty of camps, towns and stops along the way to support yourself on the way. On my 2012 trip a guy caught up to me and tolerated my slowness for a bit. He was planning to finish the Pgh to DC trip in 3 days. I've been contemplated riding the Erie Canalway, but my love for biking was ruined a bit with a bad case of achilles tendonitis after my last trip. https://gaptrail.org/ https://www.canaltrust.org/plan/co-canal-towpath/ https://eriecanalway.org/explore/cycling reply billsmithaustin 5 minutes agoparentThe GAP trail from Pittsburg to Cumberland is especially awesome. It’s well-maintained, the trail towns are welcoming, and the scenery is fantastic. The folks who live near the trail are lucky to have it. reply vwcx 9 hours agoparentprevI've enjoyed reading trip reports from the beautiful lunatics who complete the Pittsburgh to DC ride in under 24 hours. Here, one reports on falling asleep several times while riding: https://chrisshue.com/2020/01/21/pittsburgh-express/ reply TheFarns 6 hours agoparentprevDoing that trip this summer, very excited! Hoping I can get my bike buddies to break camp early. reply throwaway6734 11 hours agoparentprevive done the gap ride 4 times now and its always been a blast. we go from pittsburgh to cumberland so the last day is always such a blast with the big tunnel and 20+ miles downhill reply ch33zer 16 hours agoprevI did the transamerica bike route (https://www.adventurecycling.org/routes-and-maps/adventure-c...) a few years ago. That route is mostly on roads with some trail sections. It makes for an interesting tradeoff: if you optimize for low traffic roads without worrying too much about about strictly staying on trails you can stay away from cars fairly successfully. In comparison, if you optimize for staying on trails as much as possible you may end up riding on busy roads connecting the trail segments. There's no right answers, life's built on tradeoffs, etc. reply zikduruqe 16 hours agoprevA lot of people are not familiar with the American Discovery Trail. https://discoverytrail.org It is not as popular as the big 3 (AT, CDT, PCT). Since I live right beside it, I've hosted many hikers and bikers crossing the US providing trail magic, bounce boxes and such. Maybe my retirement plan is walking it myself one day like https://www.greybeardadventurer.com did for the AT. reply 01HNNWZ0MV43FF 15 hours agoparentFor non-hikers like me to learn: https://www.greenbelly.co/pages/appalachian-trail-backpackin... - \"Bounce Box - Box of supplies you ship or 'bounce' forward to pick up in your next trail town.\" - \"Trail Angel - Giver of Trail Magic. A volunteer who helps hikers with a place to stay in their house, a shuttle to the trail head, free food, anything.\" - \"Trail Magic - Given by Trail Angels. The goodies a Trail Angel offers out of goodwill.\" Seems like a fun hobby reply ghaff 15 hours agorootparentThere's also a ferry operator https://www.matc.org/kennebec-river-ferry-service/ to cross the Kennebec in Maine. reply dustincoates 14 hours agoparentprevThe long-distance trails are one of the things I love most about the US. Some of them are obviously much more \"contiguous\" than others, but I love that people are trying to create more. Here's a list of them: https://en.wikipedia.org/wiki/List_of_long-distance_trails_i... reply dugmartin 17 hours agoprevI grew up in an area that the \"TransAmerica Bicycle Trail\" (https://en.wikipedia.org/wiki/TransAmerica_Bicycle_Trail) goes through. The \"Bikecentennial '76\" event (https://en.wikipedia.org/wiki/Bikecentennial) used that route. The only reason I know this is a tiny sign commemorating the \"Bikecentennial '76\" next to a tiny section of dedicated asphalted bike trail out in the middle of nowhere along a state highway route. Every once in a while growing up I would see folks following the TransAmerica Bicycle Trail. It looked like both a lot of fun and a lot of work. reply Taikonerd 15 hours agoprevLet me also highlight the East Coast Greenway -- a similar idea. They're building a trail from Maine to Florida, 3000 miles (4828 km) long. https://www.greenway.org/ reply VyseofArcadia 17 hours agoprevThis is nice, but I would have also accepted restoring disused railroad tracks and using them for passenger rail. I realize this is in a whole different league of planning and costs and maintenance and operations, but I sure would love more and more robust rail in this country. reply euroderf 16 hours agoparentNot just passenger rail, but light rail. I'd think that upgrading disused tracks to that level is cheaper than a full restoration to some level capable of supporting mainline-capable engines. In any case, so-called interurbans were (AFAICT) the regional/intercity buses of their day. reply bell-cot 14 hours agorootparentThere's a lot of devils in the details here - but yes, as a generality, interurban-style light rail can be done on far lighter-duty rails (& bridges, etc.) than even the bottom tier of \"regular\" railroading requires. Though it's not just the engines that are heavy on regular RR's. And many of the costs do not scale with track weight. And running a regional/intercity transit system is seriously non-trivial, even if Santa Claus somehow gifts you with a free and magically maintenance-free rail network. reply burkaman 13 hours agorootparentprevYes, it was an incredibly extensive network before cars took over: https://urbanists.social/@straphanger/111059691532896415 reply bombcar 14 hours agoparentprevMost (but not all!) of these rails to trails are not in positions to be even moderately lightly used by passenger rail. And by being trails now, they preserve the line if it ever is needed in the future. reply cdchn 15 hours agoparentprevI was thinking the same thing. This is great for recreational bicyclists but more public transportation options would help everybody. reply doodlebugging 11 hours agoprevCaprock Canyons Trailway in Texas [0] follows an old FW&DC (Fort Worth and Denver City Railway) track for about 64 miles across the landscape. It goes from relatively flat terrain around Estelline through some nice, scenic canyons towards South Plains, passing through the Quitaque (pronounced kit-uh-kway) area where you can find what is I think the only surviving railroad tunnel in the state. There used to be two of them but one collapsed I think in the late 1970's or early 1980's due to the steady vibrations of trains passing along the track. That's what I heard anyway. The other tunnel is still there on the new trailway and is nesting ground for a bunch of Mexican free-tail bats. [0] https://tpwd.texas.gov/spdest/parkinfo/maps/gis/caprock_cany... Zoom out a little since that is focused on Caprock Canyon State Park just north of the Rails-to-Trails section of trailway. Back in the 1980's before it was abandoned by the Burlington Northern Railroad who owned it at the time, my Dad ran trains along that route bringing grain from farms at the south end all the way back to the main line at Wichita Falls where it could be routed south to Houston or Galveston for export or distribution. It passes through some beautiful country. I have been planning to visit that park and walk that trail for a long time. Maybe this will be the year. reply burntwater 13 hours agoprevLast fall I hiked 500 miles of a Camino trail in Spain. It was a fantastic experience and really made me wish we had something similar in the U.S. This is a good start, though the trail itself is only half the picture. To replicate the Camino experience you also need the network of cheap hostels every ~10 miles and the community and safety aspects that, sadly, I suspect would be lacking in the U.S. reply VyseofArcadia 13 hours agoparentDoes the Appalachian Trail not count? reply burntwater 13 hours agorootparentI haven't done it myself, but everything I've seen says the AT is a completely different kind of experience. You need to (should be) in reasonably good shape, you have to carry a lot more gear (ties into the first point), there's a lot more planning and logistics to work out, and it's a much more rustic experience (literally camping every night, with a few hotel stays). For the Camino, I started out 40 pounds overweight, I did zero physical training in preparation, I had to plan basically nothing except my flight, arrival day and departure day. I was able to do it with literally only a few weeks notice (I had never heard of it until a month prior.) Ultimately what I'm saying is the Camino trails are much more accessible to a wide range of people, as long as they're able to fly to Europe. ETA: I should perhaps explain that the Camino trails, or at least the most popular trail I took (the Francis) is really more of a walking trail than a hiking trail. It's mostly flat(ish) and is much closer to what's depicted in pictures posted on The Great American Rail-Trail. reply IncreasePosts 11 hours agorootparentprevAppalachian trail is pretty much wilderness unless you pop down to a town. Whereas the trail for the Camino de Santiago, at least the part that I was on, constantly sent you directly through towns. reply ghaff 9 hours agorootparentThe US basically has more options for long distance backpacks where you're mostly camping. Parts of Europe offer more options for town to town (or hut to hut) walks that are often not that strenuous and may even have luggage shuttle options. One isn't better than the other but they are different in general. reply maxerickson 13 hours agorootparentprevThere's probably lots of stretches where you need to go more than 10 miles to hit much of anything. There certainly is on long hiking trails in the Midwest and West. Doing a quick search, there's spots where paved roads are 30 miles apart, and lots of times it will be more than 10 miles to get to the nearest town. reply ben7799 13 hours agorootparentprevThe US and Canada have tons and tons of 500+ mile trails, heck tons of 1000+ mile trails. Like 20+ reply ethbr1 13 hours agorootparentprevWhat the state of places to stay on the AT these days? Last I heard, there were still a lot of trail angel homes and on-trail campsites, but my info is ~25 years old. reply huytersd 12 hours agoparentprevYeah I don’t think there is any comparison in how much more superior the US is when it comes to long 500-1000 mile relatively accessible trails. I can think of 10 just off the top of my head. reply burntwater 12 hours agorootparentThe Camino is vastly superior, just in a very different way. ETA: I shouldn't say it's vastly superior, rather the Camino is a very, very different experience from the trails alluded to here, and is superior in relation to that experience. reply joking 11 hours agorootparentÉl camino de Santiago is actually an ancient pilmigrage route, It has some variants but the main one has a lot of traffic and it's not rare to make friends as it's a very social route. reply omnibrain 1 hour agorootparentAnd there a still a lot of people walking it for religious reasons (to a varying degree). It is actually a whole network of ways that spans most of central and western Europe. So you can meet pilgrim quite frequently at some \"choke points\". https://en.wikipedia.org/wiki/Camino_de_Santiago#/media/File... reply burntwater 10 hours agorootparentprevI loved thinking about people 500 years ago walking on the same stones I was walking on, and thinking about what the exact spot I was currently standing on was like back then. What did it smell like? What did you see? Were the fields this open or were their more trees? Did you run into a lot of people? Lots you can think about on the trail. reply huytersd 12 hours agorootparentprevI’ve done a tiny piece of the Camino. It might as well be a paved greenway. reply burntwater 12 hours agorootparentThen you did a tiny piece of it and have chosen to extrapolate from that tiny piece. If it's not for you, that's totally OK. It IS something for many other people. reply huytersd 9 hours agorootparentOh it’s definitely for me, I loved it but it’s not the standard to hold up to everything else. reply GartzenDeHaes 17 hours agoprevI love this project, but their map seems a little optimistic. I'm pretty sure there's a big gap between Washington and Idaho that's closed due to disputes with land owners and maintenance issues. reply PaulDavisThe1st 17 hours agoparenthttps://www.railstotrails.org/wp-content/uploads/2024/02/Was... Not sure if you'd call the 37.8 miles near Warden a \"big gap\" or not. reply GartzenDeHaes 17 hours agorootparentRight, I guess non-continuous sections might be a better description. reply l3db3tt3r 16 hours agoparentprevMaintenance issues mostly, for Eastern Washington wildland fires have taken out a number of old bridges. It wasn’t too difficult to re-route around them. (I don’t recall there being any issues with land ownership disputes) reply jandrese 17 hours agoparentprevLooking at the map it seems Wyoming is the problem child. reply l3db3tt3r 16 hours agorootparentWyoming was a mixed bag, There were certainly a few long sections in WY that sucked, but that was just the nature of those areas, a kind of barren no man’s land, not a lot of resources, but that also meant not much traffic. reply lasermatts 12 hours agoprevI live in Pittsburgh and I’d love to take the GAP from Pittsburgh to DC this summer. I never knew it was part of such a cool network! reply entropicdrifter 16 hours agoprevMy grandfather donated to this project quite a bit back in the 90s and early 00's. He passed back in '06, so I'm happy to see this project still going strong. reply mcswell 11 hours agoprevRails-to-trail trails are, in my limited experience (mostly in the DC-MD-northern Va region), boringly flat. I suppose out in the Rockies and Cascades they are more un-flat, although if it was originally a railroad line it's still likely to be a pretty steady climb or descent. I guess it's fortunate that not everyone agrees with me :). reply peter_d_sherman 19 hours agoprevRelated: Rails-To-Trails Conservancy's main page: https://www.railstotrails.org/ Longest Rail-Trails in the United States https://www.railstotrails.org/united-states/#:~:text=Longest... FAQs about the Great American Rail-Trail: https://www.railstotrails.org/site/greatamericanrailtrail/co... reply sandworm101 16 hours agoprevThis is what a national trail system could/should look like. https://tctrail.ca/ reply jcranmer 14 hours agoparentThe US has a set of National Scenic Trails: https://en.wikipedia.org/wiki/National_Trails_System#Nationa..., and the Appalachian Trail is essentially the granddaddy of all such trails so far as I'm aware. reply ericholscher 16 hours agoparentprevIs it fully off road and complete? Seems not like it from the map. reply sandworm101 16 hours agorootparentI believe the east-west connection has been established, with the north-south aspects still in progress. As for \"off road\", that is a can of worms. Some people say any contact with a road surface, even a crosswalk, isn't good enough. Others demand it be set back some distance from roads. Others insist that all motor vehicles be forbidden from the trail, which is never going to happen in snowmobile country. And still others don't want it touching \"private\" land, despite established easements. reply Doctor_Fegg 16 hours agoprevGo through to the state pages and click \"Download Route Analysis\" to see PDFs of exactly where they're planning for it to go. It's a long-term project, putting it mildly, but good luck to them. reply jjulius 15 hours agoprevNow, who'll be crazy enough to combine this with the PCT and the AT into one long, N-shaped venture across the US? reply RandallBrown 13 hours agoparentI originally thought this would be nearly impossible to pull off, but the more I look at it, the more \"reasonable\" it seems. People have hiked the triple crown in a year and this would probably be a lot easier. It looks like the AT meets up with the GART at Harper's Ferry, which is about the halfway point of the AT. Then the GART meets up with the PCT at Snoqualmie Pass, which is pretty close to the northern end of the trail. You'd have to time it all around snow melting in the Cascades and trying to get to Snoqualmie Pass around the beginning of July. A reasonable schedule (for a decently fit thru-hiker) would be to start March 1st on the AT and make it to Harper's Ferry by the end of April. Then you hop on the GART and take 3 months to walk across the ~3000 miles to meet up with the PCT at Snoqualmie pass. The 35ish miles per day would be a lot, but very doable on railroad grades and roads. If you make it to Snoqualmie Pass by July 1st you're right in the heart of the main southbound PCT thru-hiker season. Also, if you're the kind of hiker that attempted something like this, you'd probably do the AT and PCT sections a bit faster than the timeline I presented, giving you even more time for the GART. reply jjulius 11 hours agorootparentFascinating! Thanks for the insight. :) reply bargle0 14 hours agoparentprevOn what part of that would you want to be during the winter? reply jjulius 13 hours agorootparentMe? None, hence \"crazy\". :) I just find the idea of long adventures/trails like that to be interesting, and I'm sure it's not outside the realm of possibility that someone would want to do it. reply RandallBrown 13 hours agorootparentprevIf you started the southern portion of the AT at the beginning of March (which is a fairly common start time for the AT anyway) you wouldn't have to deal with much snow for most of the journey. reply la_yerba 18 hours agoprevAn east-to-west traverse would be the only way to trek this without disappointment, wherein the reward for surviving the un-fun and mostly flat east and lower Great Lakes, upper Appalachia excepted, and the windblown monotony of the Great Plains is the Rockies and the Pacific Northwest/Cascades, which are beyond awesome. Do it west-to-east and you've seen everything worth seeing by Wyoming, so utterly underwhelming is everything east thereof. I once started in Washington, D.C. and only made it to Chicago before giving up from overwhelming boredom, jumping the Amtrak (train) to Denver before continuing west. reply PaulDavisThe1st 17 hours agoparent> Do it west-to-east and you've seen everything worth seeing by Wyoming The section in PA that follows a few rivers before climbing the eastern continental divide is quite lovely, and is followed by a similarly lovely section along the canal to DC. Crossing the midwest by bike is never going to lovely, no matter what direction you choose, and west-to-east at least shifts the odds of tailwinds slightly in your favor. reply la_yerba 14 hours agorootparentYeah, that's the area I meant by Upper Appalachia, that really gorgeous, hilly topography in (what I think was) eastern Pennsylvania. reply l3db3tt3r 16 hours agoparentprevI think going east to west is the better choice too. Given the terrain, and amount of closeness of services on the Eastern sections — this lets the individual get in shape, for the harder, longer sections as you move West, but also dial-in their fueling and hydration systems. reply JKCalhoun 17 hours agoparentprevI'm not sure sight-seeing is the only reason to bike tour. I did the Katy trail last Fall (across the state of Missouri — about 6 days or so of camping/biking). It was punctuated with pretty limestone bluffs and the Missouri River but to be sure had stretches of farmer's fields, etc. I still enjoyed the isolation, and then the small towns you rolled into. Turtles, camaraderie with fellow bikers, sense of accomplishment and adventure.... A lot to enjoy besides the views. reply wl 16 hours agoparentprevThe prevailing winds are generally west-to-east across the US, so east-to-west means more headwinds than tailwinds. reply mauvehaus 15 hours agorootparentPrevailing winds are a damned lie. Source: bike toured a good chunk of the Trans America and Western Express route west to east. Most of it upwind. reply panzagl 13 hours agorootparentBoth ways. reply 01HNNWZ0MV43FF 15 hours agoparentprevI figure I'll start from where I live and then go some other place. Or vice versa reply mayormcmatt 17 hours agoparentprevWell, DC to Chicago sounds like it would allow me to catch up on podcasts... reply hinkley 17 hours agoparentprevIndiana will do that to you. reply mturmon 15 hours agorootparentI went west-to-east in the late 1980s. My group was in, err, very good shape by the time we got to Kansas. We crossed Indiana in less than 2 days of riding (Vincennes, IN to Cincinnati OH along Hwy. 50). It was exhilarating. reply hinkley 15 hours agorootparentHad some friends (one friend and a friend of that friend really) who went east to west around then. We thought they were crazy for fighting the prevailing winds and the escalating terrain. They went to Alaska, had some fun times with Canadian customs for not following directions. And they thought they took an excess of spare inner tubes and made it about halfway before needing to buy more. My Indiana comment was more a “people from the Midwest will take any opportunity to make fun of Iowa, and if that’s not an option, make fun of Indiana or Wisconsin” reply mcswell 11 hours agorootparentprevIndiana is much more interesting underground, IMO. There are lots of nice caves down around Bloomington and Bedford. reply thousandautumns 9 hours agorootparentSouthern Indiana has lots of hills and forested areas, including the Hoosier National forest. The Great American Rail-Trail mostly goes through the upper third of the state, however, so you miss that and are mostly in the corn belt section of the state. Its very flat and mostly farm land. Which is likely why the railways were built on that route anyways. I personally can find flat farmland quite beautiful, but I can imagine biking through it for days might get dull. reply Tokkemon 15 hours agoprev [–] This is a great use case for the Ryan Reynolds \"But, why?\" meme. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Great American Rail-Trail spans over 3,700 miles from Washington, D.C., to Washington State, linking 150+ existing rail-trails and paths, promoting nonmotorized travel and local economies.",
      "Key criteria guarantee a top-tier trail experience, with more than 52% of the route already finished, passing through various states with iconic gateways.",
      "Advocacy, volunteerism, and donations play a crucial role in safeguarding and maintaining these trails, with support available through donations to RTC."
    ],
    "commentSummary": [
      "The Great American Rail-Trail stretches 3,800 miles, with recent bridge completion enabling Mississippi River crossings, utilizing railbanking to repurpose railroad corridors into trails.",
      "Ongoing discussions involve long-distance trails, benefits of light rail systems, comparing US and European trails, and the possibility of connecting trails like the Pacific Crest Trail and Appalachian Trail.",
      "Conversations also encompass challenges in Wyoming, potential national trail systems, debates on optimal biking routes across the US, specific routes, state experiences, and light-hearted jokes about states like Iowa and Indiana."
    ],
    "points": 208,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1712241032
  },
  {
    "id": 39929842,
    "title": "US Energy Department Backs Heated Sand Energy Storage Pilot Project",
    "originLink": "https://www.pv-magazine.com/2024/04/04/us-government-funds-pilot-project-for-heated-sand-energy-storage/",
    "originBody": "Skip to content Global Germany Spain France Italy USA Mexico Latin America Brazil Australia India China ☰ Subscriptions   Global Germany Spain France Italy USA Mexico Latin America Brazil Australia India China  Global Germany Spain France Italy USA Mexico Latin America Brazil Australia India China News All news Applications & installations Commercial & industrial PV Grids & integration Residential PV Utility Scale PV Hydrogen Energy storage Industry & suppliers Balance of systems Modules & upstream manufacturing Markets & trends Finance Markets Policy Opinion & analysis Opinion & analysis Guidelines Press releases Sustainability Technology and R&D Features pv magazine UP initiative pv magazine Hydrogen Hub Energy storage Marketplace Guggenheim Solar Index Market overview: Large-scale storage systems Market overview: Microgrid control systems Module Price Index PV Project Exchange Archived: Solar Superheroes Events All Events pv magazine Roundtables SunRise Arabia Solar+Storage España pv magazine Webinars pv magazine Webinars: German pv magazine Spotlights Event calendar External Events pv magazine live Videos Awards Partner news Special editions White papers Clean Power Research: Solar data solutions to maximize PV project performance BayWa r.e. 2019 grid parity white paper Partner news pv magazine test About pv magazine test results Magazine 2024 2023 Issues before 2023 Special editions About About us pv magazine team Contact us Newsletter subscription Magazine subscription Job board Community standards Advertise US government funds pilot project for heated sand energy storage The US Department of Energy is funding a pilot project to demonstrate the commercial viability of storing energy in heated sand, which is capable of producing 135 MW of power for five days. April 4, 2024 William Driscoll Energy Storage Technology and R&D Utility Scale Storage United States Image: Joe DelNero, National Renewable Energy Laboratory Share From pv magazine USA Researchers at the US Department of Energy's National Renewable Energy Laboratory (NREL) have developed a prototype for a multi-day energy storage system using heated sand, setting the stage for a pilot demonstration project. The sand used in the thermal energy storage (TES) system could be heated to the range of 1,100 C using low-cost renewable power. The nearby diagram shows that when electricity is needed, the system will feed hot sand by gravity into a heat exchanger, which heats a working fluid, which drives a combined-cycle generator. The NREL team’s computer modeling has shown that a commercial-scale system would retain more than 95% of its heat for at least five days, the national laboratory said in a press release. The US Department of Energy will provide $4 million to fund a pilot demonstration project sized with a 100 kW discharge capacity and a 10-hour duration, with groundbreaking set for next year at NREL’s Flatiron campus outside Boulder, Colorado. The pilot project is intended to show the technology’s commercial potential. At commercial scale, when the sand is fully heated and stored in five silos, the technology could produce 135 MW of power for five days, according to an NREL report. Popular content A targeted levelized cost of storage of 5¢/kWh could be achieved under a variety of scenarios, the report said. NREL said a technology-to-market transition plan has been developed for the TES system, but was not included in the publicly available report “due to the business sensitivity of NREL and partners.” Babcock & Wilcox, one of five project team members supporting NREL’s research, said in 2021 that it had signed an agreement with NREL, which gave it “field-limited exclusive rights” to negotiate a licensing agreement that would allow it to market the technology. Several other energy storage technologies have storage durations longer than the typical four-hour limit for battery storage. For example, Hydrostor is developing a 500 MW/4,000 MWh compressed air energy storage project in California. A pumped storage project under development in Montana would have a capacity of 400 MW and an estimated annual energy generation of 1,300 GWh. And flow batteries have a global market estimated by a research firm at $289 million in 2023. For seasonal energy storage, hydrogen storage in salt caverns is an option. A project in Utah is expected to have a storage capacity of 150 GWh matched with an 840 MW hydrogen-capable gas turbine combined cycle power plant. This content is protected by copyright and may not be reused. If you want to cooperate with us and would like to reuse some of our content, please contact: editors@pv-magazine.com. Share William Driscoll More articles from William Driscoll william.driscoll@pv-magazine.com Related content Elsewhere on pv magazine... Leave a Reply Please be mindful of our community standards. Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Save my name, email, and website in this browser for the next time I comment. By submitting this form you agree to pv magazine using your data for the purposes of publishing your comment. Your personal data will only be disclosed or otherwise transmitted to third parties for the purposes of spam filtering or if this is necessary for technical maintenance of the website. Any other transfer to third parties will not take place unless this is justified on the basis of applicable data protection regulations or if pv magazine is legally obliged to do so. You may revoke this consent at any time with effect for the future, in which case your personal data will be deleted immediately. Otherwise, your data will be deleted if pv magazine has processed your request or the purpose of data storage is fulfilled. Further information on data privacy can be found in our Data Protection Policy. Newsletter By subscribing to our newsletter you’ll be eligible for a 10% discount on magazine subscriptions! Email* Select Edition(s)* Hold Ctrl or Cmd to select multiple editions. Tap to select multiple editions. Global (English, daily) Germany (German, daily) U.S. (English, daily) Australia (English, daily) China (Chinese, weekly) India (English, daily) Latin America (Spanish, daily) Brazil (Portuguese, daily) Mexico (Spanish, daily) Spain (Spanish, daily) France (French, daily) Italy (Italian, daily) Read our Data Protection Policy. Subscribe to our global magazine Subscribe Our events and webinars pv magazine print Legal Notice Terms and Conditions Privacy Policy © pv magazine 2024 × Keep up to date pv magazine offers daily updates of the latest photovoltaics news. Stay informed. Join our newsletter. Email* Select Edition(s)* Hold Ctrl or Cmd to select multiple editions. Tap to select multiple editions. Global (English, daily) Germany (German, daily) U.S. (English, daily) Australia (English, daily) China (Chinese, weekly) India (English, daily) Latin America (Spanish, daily) Brazil (Portuguese, daily) Mexico (Spanish, daily) Spain (Spanish, daily) France (French, daily) Italy (Italian, daily) Read our Data Protection Policy. This website uses cookies to anonymously count visitor numbers. View our privacy policy. × The cookie settings on this website are set to \"allow cookies\" to give you the best browsing experience possible. If you continue to use this website without changing your cookie settings or you click \"Accept\" below then you are consenting to this. Close",
    "commentLink": "https://news.ycombinator.com/item?id=39929842",
    "commentBody": "US Government funds pilot project for heated sand energy storage (pv-magazine.com)207 points by Capstanlqc 21 hours agohidepastfavorite116 comments retrac 18 hours agoHeat storage has an aspect that was counterintuitive to me, but follows from basic geometry. It benefits greatly from large scale, since the ratio of the volume to surface area [1] decreases the larger you make a container. Accordingly, if a heat tank is large enough, the surface area becomes negligible relative to its volume, and it, in effect, becomes well-insulated by its own mass. For really big tanks, like might be used for an entire town to heat itself over a winter, practical self-discharge rates can be just a few % per month, which is better than most rechargeable battery technologies. [1] https://commons.wikimedia.org/wiki/File:Comparison_of_surfac... reply chii 18 hours agoparent> better than most rechargeable battery technologies you can't compare heat storage to electricity, because you can't directly use heat for anything else other than for heating, where as electricity can be used to perform motion. If you used heat storage as a battery, there's an additional loss when converting to electricity. However, if the heat is cheap/free during summer, storing it for winter is a no brainer. reply usrusr 15 hours agorootparentBut that doesn't really matter before all heating demand is served from storage. Sure, heat will never be the be all end all of energy storage, but there's a lot of demand in places that have winter. On top of this, when the conversion to heat is done by heat pumps, you not only get the benefit of a warmer baseline during the conversion, you also get some free cooling while charging. reply datameta 17 hours agorootparentprevThe sand is gravity-fed into a heat exchanger which transfers the heat to a fluid that drives a combined cycle turbine, in this case. I'm curious what the conversion loss is here. To add to that, what is the energy expenditure of building the battery compared to sand containment plus heat exchanger and turbine - i.e. mining, refining, transport, manufacture, delivery? reply robviren 17 hours agorootparentGas turbine efficiency targets for combined cycle are targeting 65%, so loss is at least 35%. To me I the loss sucks, but if it can be cheap to build and maintain it matters less. If solar is the energy source and that gets driven cheap enough then that loss could be acceptable. reply NavinF 17 hours agorootparentNo way you'll get 65%. This sand won't be stored at the same temperature as a natgas generator's combustion chamber. Efficiency depends on delta T. I'd guess 20% reply regularfry 17 hours agorootparentThey're estimating over 50%, apparently. The sand is stored at over 1000 degrees. Not combustion temperatures, true, but also nothing to sniff at. reply adrian_b 14 hours agorootparentprevNope. The report says that the sand will be heated up to 1200 Celsius degrees. This is much higher than the maximum temperature for steam turbines and equal to the temperature of the gas in the best gas turbines. Therefore they will use a combined cycle, first a gas turbine will use hot air passed through the sand and the exhaust from the gas turbine will produce steam for a chain of steam turbines with decreasing working temperatures. There should have been no problem in reaching a 65% efficiency for the conversion from heat to electricity, except that between hot sand and a gas burner there is the same difference as between an electric capacitor and a battery, while heat is extracted from the sand, it cools down. Presumably, when the sand becomes too cold, the gas turbine is bypassed and the hot air just produces steam. When it becomes even colder, I suppose that the first steam turbine is also bypassed and only the low-temperature steam turbines are used. This will lower the average efficiency, probably to around 50%. If the residual heat (after the steam turbines) had been used for heating or for cooling (i.e. heat-powered air conditioning), the efficiency could have been higher, e.g. over 80% in the beginning, while the sand is still very hot. reply credit_guy 9 hours agorootparentI did not read the report, but one way you can improve the round trip efficiency is to not let the sand cool below, let's say 1000 °C. You pay a one-time penalty of heating the sand to this temperature and then you operate the storage only between 1000 and 1200 °C. This reduces the capacity of the \"sand battery\", roughly by a factor of 6. But sand is (reasonably) cheap. I have seen another proposal where instead of sand the material would be graphite, which can be heated to more than 2000 °C. Graphite also has a huge heat conductivity. But it's more expensive. reply zizee 6 hours agorootparentSand has the added benefit over graphite of being nonflammable. reply datameta 6 hours agorootparentSand will also benefit from greater insulation due to lower packing efficiency and therefore more air. reply ajb 11 hours agorootparentprevIIUC there is a physical limit from thermodynamics in converting back, based on the temperature. So when efficiencies are quoted, it could be either the proportion of input energy retrieved, or the proportion of the theoretical max efficiency achieved. I'm guessing you're quoting the former? reply Scoundreller 17 hours agorootparentprevDoesn’t even need to be seasonal. Plenty of times+places where you only need heating at night and still have some net electrical draw (because the sun isn’t shining at night). Also, cold fronts move in with a lot of wind, but then it can stay cold a few days with calm winds until a warm-front moves in. reply jaggederest 11 hours agorootparentprevHalf of all primary energy usage is for heating purposes. Heat is the best format to displace, as well, since a significant chunk of all that primary energy use is direct fossil fuel combustion right now. Especially since you can use heat to drive a heat pump, which ends up actually achieving above unity efficiency effectively since the heat input is used to move additional heat from the environment. Even if it's inefficient, if the heat would otherwise have been wasted, it's a net benefit. reply Tagbert 10 hours agorootparentThe Pernell with heat compared to electricity is portability. It is easy and efficient to transmit electricity but heat is not reply pimlottc 10 hours agorootparent> The Pernell with heat Did you mean to type \"problem\"? reply adrian_b 13 hours agorootparentprevHeat can be used directly not only for heating, but also for cooling. There are air conditioning systems that are powered by heat, not by electricity. There are places where the power plants use the residual heat from the generation of electricity not only for heating during the winter, but also for cooling during the summer, by producing chilled water. reply autoexecbat 13 hours agorootparent> There are air conditioning systems that are powered by heat, not by electricity. How does that work? reply xkcd-sucks 13 hours agorootparenthttps://en.wikipedia.org/wiki/Absorption_refrigerator reply inetknght 16 hours agorootparentprev> you can't directly use heat for anything else other than for heating, where as electricity can be used to perform motion. Maybe today. But NASA has some interesting metal tires [0][1] which might change your mind for the future. [0]: NASA info https://technology.nasa.gov/patent/LEW-TOPS-99 [1]: Neat youtube vid: https://www.youtube.com/watch?v=vSNtifE0Z2Q reply smallmancontrov 15 hours agorootparentprevIs absorption refrigeration bad enough to be permanently non-viable here? Or is it \"just\" a matter of scaling a niche technology? reply bumby 15 hours agorootparentDefine \"bad enough\" in more specific terms. I believe it largely depends on the application. If you have a lot of waste heat, it's potentially a way to get \"free\" refrigeration. (e.g., a paper plant that uses a lot of steam can use absorption chillers to make use of waste heat.) If fuel is much cheaper than electricity, it can be economically viable. Peak shaving can save lots of money. etc. But it's probably not competitive purely in terms of energy efficiency or GHG emissions. reply smallmancontrov 14 hours agorootparentIf I had exact figures, I wouldn't need to ask for anyone's intuition. Thankfully, ChatGPT was willing to work with me, and I will summarize the results here. Typical refrigeration COP (Coefficients of Performance): Absorption refrigerator: 0.6-1.2 Compressor refrigerator: 1.5-4.0 Estimated TES economic advantage: 1.1-2.5x Conclusion: yes, absorption refrigeration is probably inefficient enough to make it a long shot in this application. The only way I can see it becoming viable is if extremely hot TES can completely change the efficiency game, and then only just. reply bumby 13 hours agorootparentThe temperature in TFA are outside the range of most commercial absorption chillers, so this is more about making cheap electricity. I would imagine absorption would be more applicable if the same tech was used to generate lower-quality heat that's not suitable for a combined-cycle generator. reply stcredzero 15 hours agorootparentprevbecause you can't directly use heat for anything else other than for heating There's Stirling Engines. If the solar collection is pure thermal, and if that collection and the storage can be made dirt cheap, then the 37% or so efficiency of conversion to electricity stops being a problem. But what are currently problems with Stirling Engines -- Hardly any of the industrial optimization has been applied to them Re: Wright's Law. So they are quite costly! Heat pipe solar thermal could be made dirt cheap through economies of scale, and it works very well, even in climates like England's. I could envision house construction changing to include sub-basements which are just polystyrene insulated boxes filled with sand. By over-provisioning storage by 4X, houses in cold climates could have huge electrical power stores, especially in summer. (Especially if the house uses heat exchangers which can draw directly from the thermal store.) reply specialist 10 hours agorootparentprevThermovoltiacs, converting heat dirctly to electricity. IIRC, At least one of the \"box of rocks\" thermal battery startups (interviewed on the Volts podcast) intends to have both a two way heat exchanger and electricity play. reply 2OEH8eoCRo0 16 hours agorootparentprevIs a battery electricity or is it a chemical reaction? reply westmeal 16 hours agorootparentprevyou kinda can with sterling generators I think reply quickthrowman 16 hours agorootparentprev> you can't compare heat storage to electricity, because you can't directly use heat for anything else other than for heating You can use heat to generate steam that can spin turbines to perform work. But, I am not sure if it’s practical to generate steam from a sand battery, my background is in electrical construction. I’m guessing the sand battery isn’t nearly as hot as a natural gas steam boiler’s combustion chamber. https://www.johnsoncontrols.com/en_sg/hvac-equipment/chiller... reply politician 17 hours agorootparentprevNit: Rechargeable batteries have a loss when converting the chemical energy stored inside to electricity. There’s no free lunch. reply cogman10 15 hours agorootparentYes, but for Lithium and sodium batteries you are looking at 90+ efficiency. reply bumby 17 hours agoparentprevAn extrapolation directly to heat transfer is the Biot number https://en.m.wikipedia.org/wiki/Biot_number#:~:text=The%20Bi.... reply kaliszad 10 hours agoparentprevWhen you can store electricity more directly instead of the (inefficient) conversion to and from heat it is just crazy to not do so. With electricity, thanks to heat pumps you can \"move\" heat instead of producing it, which makes heating a lot less expensive since the \"efficiency\" can often reach 300-400%. For that reason you want to keep using electricity for as long as possible in most cases. Of course, if you have a huge source of cheap heat the equation might change. In such cases you might still want to use some of the heat to get higher temperature heat. (https://en.wikipedia.org/wiki/Absorption_heat_pump) And no, it will not help too much if you use a heat pump before storing the energy in this facility. You still have huge conversion losses, the volume for low-temperature heat storage will be huge. If you want high temperature storage your heat pumps will need more stages and run less efficient. reply stingraycharles 18 hours agoparentprevGravity seems much more natural to me than heat; eg pumping water up a dam and releasing it again when you need the energy. It strikes me that heat has the problem that it always loses energy in its “stable state” because the surrounding environment absorbs the heat, and gravity doesn’t have this problem. reply mecsred 17 hours agorootparentThe issue of finding a location that has dramatic elevation change, a basin capable of storing vast amounts of water, and a suitable source/sink for pumping make pumped hydro difficult to deploy. Then there are additional logistics challenges such as environmental damage and proximity to human settlements for maintenance and engineering teams (prior challenges mean you don't really get to select the locations). Heat sinks meanwhile can be built wherever you have a big rock by drilling some holes. Additionally, gravity definitely does lose stored energy in its \"stable state\", through evaporation and water entering the water table. Losses depend on geology and local climate, but it's not negligible. Not to say that pumped hydro is a bad technology, it's just got it's own challenges and uses. It's most applicable in the form of electrical grid storage. But specifically on the scale of keeping towns and cities warm, heatsinks outperform almost across the board. reply sandworm101 16 hours agorootparentAnd, if you inundate an area of wilderness to create reservoir you have to also count the lost carbon capture of the growing plants, and the significant methane emissions of the now dead and rotting plant matter under the water. In dry deserts this may be negligible, but the mountains where people most want to install hydro projects are generally very forested. reply pfdietz 15 hours agorootparentprevThe \"vast basin\" part of that is an exaggeration. Look at https://www.whitepinepumpedstorage.com/ and the sizes of the upper and lower reservoirs there. This is to be a 8 GWh, 1 GW facility. reply mecsred 14 hours agorootparent\"vast\" is a pretty non specific word so I can maybe see why you would think that? I feel like it's pretty obvious from context I'm not talking about an inland sea here. The quoted 5000 \"acre-feet\" in that project is a considerable amount of water for a man-made structure! Either way, the details page[1] supports all of my above points. It even comments on the page how rare it is to find a suitable site like the one they've chosen. [1]https://www.whitepinepumpedstorage.com/project-details reply SECProto 10 hours agorootparentprev> This is to be a 8 GWh, 1 GW facility That is big for time-shifting daily energy usage, but way too small for seasonal energy storage reply pfdietz 10 hours agorootparentRight. PHES is not a good fit for seasonal storage. reply standeven 17 hours agorootparentprevPumped hydro is great, but the potential energy from gravity is actually really low. Stacking blocks will probably never work, and pumped hydro only works due to scale and existing geography. It’s also not modular and can’t be co-located with generation or loads unless the geography works out. reply syllablehq 17 hours agorootparentprevTerrament is working on a modular gravity storage solution that uses deep mine shafts to gain 20x more height than stacking blocks above ground. So you don’t need water or mountains. And since gravity storage uses ballast that is really just dumb weight, it could even be economical to make that ballast a secondary storage like thermal storage. reply lambda 13 hours agorootparentprevPumped hydro is great, but only in certain areas where you already have the elevation gain available. Compressed air storage is another one that's pretty good, but it's only particularly good if you can store it in underground caverns or unused mines, so it's also geography dependent. For longer term storage, producing hydrogen can be a good one. And batteries are actually fast becoming competitive with some of these options from the other end, generally better for shorter term storage but getting better at longer term. Even shorter term, flywheels can be a good option. There's room for several different types of grid-scale energy storage, based on how efficient they are at different energy storage periods and numbers of charge-discharge cycles, and also in some cases on local conditions, like the availability of terrain and water sources for pumped hydro. There's a good paper here which shows what the most efficient energy storage systems are for various combinations of length of storage (hours per discharge) against number of discharges per year, and for each one shows the current cost, and a predicted cost based on trends in technological advancements: https://www.sciencedirect.com/science/article/pii/S254243511... There's a lot of the chart that is dominated by pumped hydro currently, but plenty of other storage technologies that are more cost effective on different timescales and numbers of discharges. But it looks like it's predicted for prices of battery storage and hydrogen storage to fall relative to the others, causing a different predicted landscape in 20 years. And some of these, like pumped hydro, are dependent on geographic features, or access to certain resources, so even when one dominates overall, there can be others that dominate in particular geographical regions. reply scotty79 17 hours agorootparentprevWith water you lose some due to evaporation. reply usrusr 15 hours agorootparentWith a mineshaft gravity storage you could actually try to spin up a convective loop powered by geothermal that dries the sand over time, allowing you to lift dry sand when charging and abseil heavier wet sand when discharging. reply sgc 16 hours agorootparentprevThat was my first thought too. But lakes lose ~20% / year to evaporation, and with the use of shade balls that is cut by ~90%, so we are at 2% / year - which is about the same as very efficient daily loss from heat storage. reply pfdietz 15 hours agorootparentIf the heat is being turned back to electricity, a heat sink is needed and if this is done by evaporation the water loss will greatly exceed that of natural evaporation from a PHES facility. reply sgc 15 hours agorootparentIt's not the evaporation per se that matters in the pumped hydro, it is the evaporation loss of water you already invested in pumping, so it is just an efficiency loss. The vapor loss for the heat sink is indirect, you usually just calculate the turbine efficiency. I don't know if the (generally much smaller) pumping requirements for turbines is already included in their efficiency calculations, but it would need to be of course. Either way, it just goes to show further that pumped hydro would be more efficient, when and where it is feasible. reply today20201014 17 hours agoparentprev> the ratio of the volume to surface area decreases the larger you make a container Did you mean to write the reverse? i.e the ratio of the surface area to volume decreases the larger you make a container. reply hardlianotion 16 hours agorootparentYes. reply adverbly 16 hours agoparentprevI also think this is a very interesting approach to take. This might be a pipe dream but it would be really cool if we could just turn a big chunk of desert into a battery by plumbing some heat exchangers through it. Or maybe just putting a gigantic Fresnel lens in the desert and pointing it at the ground. I almost wish someone would try this just to see what would happen. reply bell-cot 20 hours agoprev> The sand used in the thermal energy storage (TES) system could be heated to the range of 1,100 C using low-cost renewable power. [...] when electricity is needed, the system will feed hot sand by gravity into a heat exchanger, which heats a working fluid, which drives a combined-cycle generator. So this is definitely not a \"bury your heating coils in a sand dune, then connect...\" technology. Quartz melts (per Wikipedia) at 1,713 C. Hotter would obviously be more efficient (basic thermodynamics) - but from the linked govt. report, it sounds like getting usefully hotter would lead to excessive technical problems. Beyond sourcing the sand (a real issue in many places), this tech sounds incredibly benign, environmentally. Zero-ish rare elements / nasty chemicals / emissions. And the worst-case \"melt-down\" leaves just a pile of burning-hot sand. Edit: IANAME (not a Mech. Engineer), but that govt. technical report looks like great stuff if you're seriously into energy storage tech, or just an amateur gearhead. Direct link: https://www.nrel.gov/docs/fy23osti/84728.pdf reply smallmancontrov 20 hours agoparentHuh, it intuitively seems like piping working fluid to the sand ought to be easier than moving the sand to the working fluid. Is the moving-sand approach fundamentally desirable thing (I can't imagine why) or is it just a simplification for the proof of concept? reply sandworm101 18 hours agorootparent>> Is the moving-sand approach fundamentally desirable thing (I can't imagine why) or is it just a simplification for the proof of concept? I suspect this is about operating temperatures. If you run pipes through the thermal mass then you will be slowly heating/cooling the entire mass. That means the temp will be constantly changing and would basically never be at optimum. But by withdrawing small amounts of sand to be cooled/heated separately, the bulk can remain at an optimum. Only the removed sand is cooled. So your tank of \"hot\" sand remains at the same temperature until the last bit of hot sand is gone, rather than it slowly cooling as you withdraw heat from the bulk. That no doubt makes thermal transfer more efficient and predictable. reply regularfry 20 hours agorootparentprevI suspect it's because the thermal conductivity of sand isn't that great. If you've got your working fluid running through pipes embedded in hot sand, the system is likely bottlenecked on getting the heat energy from the main body of the mass through the cooler sand closest to the pipes. Do it the other way and the enormous surface area is working for you, so you can presumably get the energy out arbitrarily fast (or, at least, that's no longer the bottleneck). reply regularfry 20 hours agorootparentThe thought occurs that if that is indeed the problem, you could attack it by mixing a metal in with the sand. You'd still get the thermal mass, but conductivity would no longer be a problem as long as it had been in a liquid phase at least once. reply zdragnar 8 hours agorootparentThe problem isn't so much the sand, it's all the air gaps between the grains of sand. Adding metal won't help any- once it gets to a liquid phase, it'll sink down, or if it doesn't stay liquid long enough, will just trap most of the air in it. Edit: you'll also want to keep oxygen out of the environment in the liquid phase. Depending on the metal you use and the exact makeup of the sand, you'll wind up with some materials that are often used in refractory cement- aluminum, oxygen, and silicon will do the trick and absolutely ruin the effectiveness of your heat battery. reply regularfry 54 minutes agorootparentThat's what I was getting at. Fill the air gaps with metal. reply iamthemonster 2 hours agorootparentprevNah it's nuts to mobilise the sand as a working fluid. The more mature design option seems to be refractory brick for heat energy storage, with piping running through it where you can raise steam. Running a steam turbine generator is generally seen as uneconomic due to the very low round-trip efficiency, and heat storage usually follows the principle of \"if you store it as heat, use it as heat\". reply bell-cot 20 hours agorootparentprevStoring & moving fine-grained bulk dry stuff is really old, cheap, & reliable technology - think of grain silos. reply dexwiz 19 hours agorootparentYes, and under certain conditions sand is a fluid. Remember fluid !== liquid. Gases, liquids, and loose material can all be fuilds. reply pfdietz 19 hours agorootparentprevPiping the working fluid to the sand means your pipes have to increase in size and cost as you add more sand. Dropping the sand into a heat exchanger means the heat exchanger doesn't increase in cost with the volume of sand. reply AlexAndScripts 17 hours agoparentprevThis seems like a really cool technology that also doesn't need much engineering to make it viable. Why isn't it already widespread? reply spxneo 14 hours agorootparentthere is a DIY community around it and because it is so simple to make I just dont see any need to rely on commercial solutions. Literally store sand somewhere and heat it, use it for days or months depending on the setup. reply ejb999 20 hours agoprevI am really intrigued by using sand for energy storage - what I don't get (not my field) is given a typical 2000sf house, located in the colder part of the country as an example, how much heat could be stored for how long? i.e. is it even feasible to use solar panels to power resistance heaters all spring/summer/fall, to save up enough heat to keep a house warm for the entire winter? if so, how many panels would you need and how big a sand battery would it take. I am not planning on doing this, but explaining it on a scale that I can relate to would be helpful, because I know, for example, that said house can store a winter's worth of heat in a 1000 gallon oil tank, or small woodshed big enough for 6 cords of wood. reply ceejayoz 18 hours agoparenthttps://en.wikipedia.org/wiki/Seasonal_thermal_energy_storag... > In Alberta, Canada, the homes of the Drake Landing Solar Community (in operation since 2007), get 97% of their year-round heat from a district heat system that is supplied by solar heat from solar-thermal panels on garage roofs. This feat – a world record – is enabled by interseasonal heat storage in a large mass of native rock that is under a central park. The thermal exchange occurs via a cluster of 144 boreholes, drilled 37 metres (121 ft) into the earth. Each borehole is 155 mm (6.1 in) in diameter and contains a simple heat exchanger made of small diameter plastic pipe, through which water is circulated. No heat pumps are involved. That development is 52 homes. They are presumably engineered to be highly energy efficient and it's not a perfect comparison to sand, but it's less than I'd have imagined. reply ulnarkressty 18 hours agorootparentStoring heat in bedrock sounds like a good idea, but there are risks, e.g. https://www.thelocal.de/20170818/this-historic-german-town-i... reply giarc 18 hours agorootparentprevhttps://www.cbc.ca/player/play/1.7155409 I live in Calgary and have seen a few articles about Drake Landing recently. reply NewJazz 18 hours agorootparentprev155 mm huh? Did they use NATO standard mortars to make these boreholes :p? reply dubcanada 13 hours agorootparentThere is no standard borehole size, 155mm drill head is quite easily acquired in Canada reply usrusr 15 hours agoparentprevA very low key variation of heat storage is using a ground-source heat pump in winter and then in the summer using the same heat pump for cooling the house and replenishing the ground source while doing so. Small ground sources, or ground sources with neighbors too close who do the same, will actually accumulate noticeable ground cooldown from season to season if they are not replenished. Free air conditioning comfort from the replenishing effort, or free replenishing from the air conditioning, you can spin it however you like. It's very low gradient and certainly won't get you through winter without a another power source, but it absolutely is seasonal heat storage. reply jrockway 18 hours agoparentprevI don't think that the sand units you can install in your home have the ability to store energy across seasons. They are more like hot water heaters; heat when you have solar, but you can use some hot water at night when electricity is more expensive. So this would be like, in a mild climate, the sun is going to keep your house warm during the day and you are generating some solar. You use the solar to heat up the sand, and then overnight, you recover some of that energy to use for heat. (I think you can get electricity back out of the heated sand as well, but it's like 70% efficient compared to >90% for a lithium battery. So I think the big application is in heating, less for charging your car after you get home from work.) reply michael1999 18 hours agoparentprevA single house is too small to make that work. I can't see how you could insulate such a small volume for more than a few hours. It can start to work at district scale, but the Finns are just targeting a few days. https://www.euronews.com/green/2024/03/10/sand-batteries-cou... https://en.wikipedia.org/wiki/Drake_Landing_Solar_Community reply 0cf8612b2e1e 18 hours agoparentprevI think this is a surface area/volume problem. A smaller installation is going to have a larger relative surface area given the amount of stored heat, so your losses/insulation requirements are going to be much worse. reply zardo 16 hours agoparentprevhttps://www.renewableenergymagazine.com/storage/first-commer... This is 8MWh (of heat), the 1000 gallon oil tank is about 40MWh. Something like a two story basement filled with sand at the maximum temperature of a home oven is probably in the ballpark. reply cdtwigg 18 hours agoparentprevThe temperatures we’re talking about (1000C) would be incredibly dangerous in residential applications, plus a small installation would lose too much energy to the environment due to the ratio of surface area to volume. More practical IMO is to use a daily cycle like what Harvest Thermal is doing: store energy in your water heater tank during the daytime and release it at night. reply dheera 18 hours agoparentprev> winter's worth of heat in a 1000 gallon oil tank That's a massive fire risk because it is combustible fuel. A pile of hot sand in an auxilary, non-flammable structure isn't going to catch fire. reply nick238 17 hours agorootparentA 1000 gallon tank stores about 146 gigajoules of energy (diesel motor fuel = 138,700 BTU/gallon, \"138700 BTU * 1000 in gigajoules\"). 1000 gallons of sand (about 6000 kg) heated 1000 °C above ambient stores about 1000 K * 6000 kg * 1.1 kJ/kg-K (from the paper, on page 9) = 6.6 gigajoules. So to match a fuel tank for energy storage, it needs to be at least 22x the volume, have extremely good insulation (even more volume), a heat-exchanger, and sand-handling augers. Additionally, the sand needed to be heated in the first place, which means a good electrical connection, but if you have that power in the first place, just use that during the winter? The nice part about fuel is that a man and a truck can move a few thousand gallons of hydrocarbons several hundred miles out to the middle of nowhere and transfer that energy at megawatt speed with a hose. reply 0cf8612b2e1e 18 hours agorootparentprevHuge amount of the rural population already have an oil or propane tank sitting within a hundred yards of their house. Being even slightly remote means you require backup heating options for when things fail. reply jarito 19 hours agoprevUndecided did a couple of videos on this technology. It seems quite useful for heat storage - as other commenters have noted, it isn't that efficient for pure electricelectric storage. * How a Sand Battery Could Change the Energy Game - https://www.youtube.com/watch?v=G6ZrM-IZlTE * Sand Batteries for Home Usage - https://www.youtube.com/watch?v=KVqHYNE2QwE reply dagurp 11 hours agoparentI like this one too https://youtu.be/KVqHYNE2QwE?si=GM80NBsE_Ms8oT-n reply reify 2 hours agoprevI love this idea I have been following the progress of sand batteries. So much so that I actaully made one at home. https://hackaday.com/2022/11/21/making-a-do-it-yourself-sand... I made some changes to this idea. I used a 12V supply from an old PC power supply to run the heat element in the sand. I used some course pool filter sand that I use for my aquariums. I have a very chilly hallway with no radiator between my lounge and main entrance door. It did work. Raised the temperature from 62F to a modest 70F. It took a few days to warm through and remain constant. I see there are developments in Scandinavia to heat entire towns. This is the good science I like to see. reply xnx 20 hours agoprevDoes this differ in design from the Finish sand battery from 2 years ago: https://news.ycombinator.com/item?id=32006791 reply michael1999 18 hours agoparentThe Finnish system is just heat->heat. They generate heat when the wind is blowing, and inject heat back into the district system when it isn't. Super simple - resistive heating, and passive heat transfer. This system produces electricity. Exciting, but much fancier. reply dubcanada 13 hours agorootparentA stirling engine can generate heat from electricity, such as those used in MicroCHP, you can burn wood, produce heat, and generate electricity from that heat. You could do the same with a pile of sand. You just need to pipe liquid through the sand, and a supply of cooler water. reply pfdietz 19 hours agoparentprevYes, this involves higher temperature (the sand is stable up to ~1200 C) and transfer of heat from the sand to a working gas by means of Babcock & Wilcox's fluidized bed heat exchanger technology. This is a neat idea that intimately mixes the gas and sand for very rapid and compact heat transfer. Using resistive heaters, round trip efficiency (back to electricity) is estimated to be around 52%. reply nick238 18 hours agoprevLove the idea. Hate the acronym...ENDURING, short for \"Economic loNg-DURation electrIcity storage by using low-cost thermal energy storage aNd hiGh-efficiency power cycle\"? ELDESbULCTESaHEPC. reply HPsquared 20 hours agoprevThermal storage works nicely with solar thermal plants. Not so good for direct electricity storage though. reply bell-cot 19 hours agoparentPretty inefficient, yes. Thermodynamics is a harsh mistress. But if the other choice is \"throttle down the wind farm, because the grid doesn't need that much power\" - then a really cheap/simple/safe (but inefficient) storage tech could prove pretty useful. reply pfdietz 19 hours agorootparentOne can get higher round trip efficiency (practically, perhaps 65%) using pumped thermal storage. Here, one uses some thermal cycle in reverse to separate \"cold\" and \"hot\", then reverse that to discharge. This also reduces the maximum temperature needed to maybe 500 C, below the creep limit for cheap steel. The cold end would be maybe -100 C, stored in something like liquid hexane. reply bell-cot 19 hours agorootparentTrue. But the govt. report on this idea seems confident of 50% RTE, or 55% if they used a more-complex turbine system. For limited & short-term use, the plant with vastly-more-expensive storage masses might make sense. But as soon as you were faced with NIMBYs or environmentalists (hexane's MSDS is far closer to hydrogen fluoride's MSDS than it is to sand's), or if you are working in a less-prosperous part of the world...sand is great stuff. reply seanmcdirmid 19 hours agorootparentprevHow does this compared to pumped storage? reply non-chalad 19 hours agorootparentYou can pump sand by bubbling compressed air through it. 1. https://www.youtube.com/watch?v=My4RA5I0FKs reply clort 15 hours agorootparentseems to me that this would result in heat-loss as the air is heated quickly then ejected from the mass. perhaps the bubbled gas doesn't hold a significant amount of heat though? (but if it did, it could be used to extract the heat without pumping the sand...) reply pfdietz 19 hours agorootparentprevModern pumped hydro might have a RTE of 80%. reply ndonnellan 17 hours agoparentprevI was going to chime in to second this. In a former life I worked on power towers and we had designs for air receivers that would potentially work really well with this type of system: - High temperatures - Intermittent solar input not a problem - tall central structure (?? maybe a plus given the paper's tall storage vessels) But high temperature air receivers have their own problems, mostly around receiver material properties (thermal cycling / stress) and heat loss. It's really hard to focus a lot of light from the sun into a tiny aperture, because the sun isn't really a point source, and no mirror is perfectly shaped. reply pfdietz 15 hours agoprevHere are some informative slides on the technology from 2021: https://arpa-e.energy.gov/sites/default/files/2021-03/07%20D... Previously linked at https://news.ycombinator.com/item?id=28451131 reply newzisforsukas 9 hours agoprevhttps://polarnightenergy.fi/technology reply bitsinthesky 9 hours agoprevWith such cheap solar panels soaking up summer sun combined with this storage, winter heating could get really efficient. reply pfdietz 19 hours agoprevI have wondered if this technology could be used in open loop mode where the sand is replaced with some material that you want to thermally process. For example, olivine particles that become more reactive with CO2 (for mineral carbonation for CO2 sequestration) after being heat treated. Run the particles though once and use them afterwards. reply nick238 15 hours agoparentFeed in calcium carbonate, heat it up and sequester the CO2, and use the hot calcium oxide once then ship it off to the cement plant. Or, how about taking an existing cement plant and have it use the air heat-exchanger/turbine/generator setup described in this project to recover the energy in the red-hot clinker? I assume they'd have some sort of heat exchanger system already to preheat feedstock using the outflow, however? reply hamilyon2 15 hours agoprevWhat if we grind asteroids and send bags of sand from orbit? Sand will overheat while falling giving us free energy and sand. Sand is valuable. reply bugbuddy 15 hours agoparentWhatever you are on, I need some of it. It must be good. reply spywaregorilla 13 hours agoparentprevAlso an unstoppable onslaught of kinetic weapons reply leecarraher 16 hours agoprevit seems the primary benefit for sand over water, is a 1:10 operating temp vs. 5:1 specific heat. So it depends on whether the added complexity of working with a hotter, solid is worth not having to build a facility that is 2x bigger. Are there other benefits I'm missing, or is this concrete block gravity storage vs pumped water storage, all over again? reply nick238 15 hours agoparentComparing water's 4.18 kJ/kg-K * ~75 K (25 °C -> 100 °C) to the sand's 1.1 kJ/kg-K * 900 K? I think you can (or it's easier) get more useful work out of a lesser amount of hotter stuff, even if the thermal energy or total heat is the same. Unsure of that, I don't know what the specific principle is. I'd vaguely gesture at the 2nd law of thermo as if I poured a cup of boiling water into a pot of room-temperature water, the total heat leaving the pot would wind up being the same as the heat leaving the cup, but less useful? reply NegativeK 13 hours agoparentprevSand doesn't leak down or up (via evaporation) nearly as much. It's also far less of a precious resource and non-corrosive, compared to the most common version of water. reply kaliszad 10 hours agoprevUsing heat for energy storage and then converting back to electricity are ok if you don't care about cost of electricity, the facility or the maintenance. In basically all other cases they are some of the worst ideas. We have much better technologies that are known for the last ~50 years. They are simple, they do work and have volumetric energy density within striking distance of fossil fuels however don't include carbon. Just reading the expired US patents would reveal that. I guess there are worse things to invest into but come on, we teach the physics needed to comprehend why this is a bad idea in high school. reply throwitaway222 17 hours agoprevSeems to me that direct battery storage research is a much much much MUCH better use of government research funding. Sure if you want to use sand heat holders for heating houses, fine.. but for conversion to electricity? bleh reply kylehotchkiss 15 hours agoprevLike a giant Turkish coffee stove? reply scotty79 17 hours agoprevIt could be interesting to burry heating coils in the ground under the house and maybe dig deep, insulated petimeter foundation to better keep the heat inside. Power them with solar of course at times of negative prices. Or dig out a deep cellar, insulate on the sides and a the bottom against heat loss and moisture and put back the earth you dug out with heating element in the center. You don't even have to insulate wires that go through earth to the heating element because electricity passing through earth will get turned to heat as well. It might be nice additional heating for cooler climates. If you dug deep enough to have actual cellar on top of that you'd have a very warm cellar, you could put underground swiming pool there. reply spxneo 14 hours agoprev [–] I go to the beach fill up 50 gallon drums with it and then pipe hot water through it so that you can enjoy heat without electricity or gas. im obsessed with it. i love the way it feels on my body. i take warm sand baths with it. i have cold feet so i use nylon socks, fill it with sand providing endless massage and keeping it warm. ive yet to try different types of sand from other regions but Canadian beach sand does the job. reply dsp_person 13 hours agoparent [–] i can't tell if you are trolling or not reply spxneo 9 hours agorootparent [–] why would i be trolling? reply dsp_person 5 hours agorootparent [–] so maybe someone will try putting sand in their socks reply spxneo 3 hours agorootparent [–] i dont understand it feels really good reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The US Department of Energy is backing a pilot project to showcase the feasibility of storing energy in heated sand, capable of generating 135 MW for five days.",
      "Researchers at the National Renewable Energy Laboratory have created a prototype system utilizing heated sand that can retain heat for over five days.",
      "This project, funded with $4 million, will exhibit a 100 kW discharge capacity and a 10-hour duration, highlighting the potential for this technology. Other longer-duration energy storage methods are also in progress, like compressed air energy storage and hydrogen stored in salt caverns."
    ],
    "commentSummary": [
      "Sand is being considered for energy storage, especially for thermal purposes, through methods like heated sand energy storage and sand batteries.",
      "The discussion explores the efficiency and practicality of using sand compared to materials like lithium batteries or water for storage.",
      "The primary focus is on finding economical and effective ways to store and use heat energy across various applications and technologies."
    ],
    "points": 207,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1712235566
  },
  {
    "id": 39929416,
    "title": "AI glitch creates porn of lottery user, Washington's site pulled",
    "originLink": "https://mynorthwest.com/3956403/rantz-washingtons-lottery-ai-porn-user/",
    "originBody": "KTTH Rantz: Washington’s Lottery forced to pull site after creating AI porn of lotto user Apr 2, 2024, 6:41 PMUpdated: 10:25 pm Megan, a mother from Tumwater, visited a Washington's Lottery AI-powered website on March 30. She was shocked to see one of the AI photos was softcore porn. (Photo provided to The Jason Rantz Show on KTTH) (Photo provided to The Jason Rantz Show on KTTH) Share BY JASON RANTZ AM 770 KTTH host A new Washington’s Lottery AI mobile site turned a user’s photo into softcore pornography, forcing them to take the website down “out of an abundance of caution.” When Megan, a 50-year-old mother based in Tumwater, visited the new AI-powered mobile site from Washington’s Lottery on March 30, she thought she was in for some frivolous fun. Test Drive A Win allows users to digitally throw a dart at a dartboard featuring dream vacations you can pay for with the money you win in the lottery. Depending on where the dart lands, you can either upload a headshot or take one on your phone to upload, and the AI superimposes your image into the vacation spot. Megan landed on a “swim with the sharks” dream vacation option. She was shocked at one of the AI photos Washington’s Lottery spit out. It was softcore porn. KTTH podcasts: Subscribe to the Jason Rantz Show How did the the Washington’s Lottery site push out porn? Megan says she used the in-site option to take a photo of her face to upload. The photo that was created shows a smiling AI version of Megan, but almost completely nude. In the image, Megan is sitting on a bed, with a bathing suit bottom on, but no top. Her bare breasts are exposed. The background of the image appears to show the bedroom in an aquarium, with fish swimming around her. There is a Washington’s Lottery logo in the bottom right corner. “Our tax dollars are paying for that! I was completely shocked. It’s disturbing to say the least,” Megan explained to the Jason Rantz Show on KTTH. Megan feared that the image could potentially get out and that it would happen to another website visitor. She told a friend, who happens to know someone at Washington’s Lottery, what happened. “I also think whoever was responsible for it should be fired,” she added. Watch: The Double Shot with Jason Rantz and Jake Skorheim — How does the internet work? Washington’s Lottery pulled the site offline It’s unclear how or why AI created this particular image, given the rules governing the content. The Jason Rantz Show on KTTH could not, of course, authenticate the image. However, a spokesperson for Washington’s Lottery confirmed they received a report that the image was created by the AI system. Approximately three hours after an inquiry by the Jason Rantz Show on KTTH, Washington’s Lottery pulled the site offline. They also provided a statement. “We were made aware that a single user of the AI platform was purportedly provided an image that did not adhere to the built-in parameters set by the developers. Prior to launch, we agreed to a comprehensive set of rules to govern image creation, including that people in images be fully clothed. We have not seen the image that the AI provided to this user, but out of an abundance of caution we took the website down,” the spokesperson confirmed to the Jason Rantz Show on KTTH. The spokesperson did not confirm when they first heard of the issue. He also did not explain why Washington’s Lottery waited until after my inquiry before turning the site offline. Megan is pleased the site has been removed. She says she hopes any back-end settings that need to be adjusted are, in fact, adjusted. UPDATE: 4/2/24, 7:38 p.m. A spokesman for Washington’s Lottery sent the following statement to the Jason Rantz Show on KTTH Tuesday night: We heard about this earlier this week. As soon as we did, we had the developers check all the parameters for the platform and were comfortable with the settings, but after further internal discussions today, we chose to take down the site out of an abundance of caution, as we don’t want something like this purported event to happen again. Listen to The Jason Rantz Show on weekday afternoons from 3-6 p.m. on KTTH 770 AM (HD Radio 97.3 FM HD-Channel 3). Subscribe to the podcast here. Follow Jason on X, formerly known as Twitter, Instagram and Facebook. Follow @https://twitter.com/jasonrantz Share Jason Rantz on AM 770 KTTH Tune in to AM 770 KTTH weekdays at 3-7pm toThe Jason Rantz Show. Jason Rantz Show",
    "commentLink": "https://news.ycombinator.com/item?id=39929416",
    "commentBody": "Washington's Lottery forced to pull site after creating AI porn of lotto user (mynorthwest.com)202 points by docdeek 21 hours agohidepastfavorite242 comments perihelions 21 hours agoI think regardless of the specifics, using generative AI to manipulate people into gambling is predatory and immoral. It's psychological abuse to manipulate people into dwelling on how their lives are awful, and on how purchasing the advertised product could fix all their lifelong problems (which it almost certainly won't). Some of the worst \"abuses of AI\" are going to be things we've already fully normalized—we just fail to reflect adequately on our culture and in what ways it's broken. reply SamBam 20 hours agoparentWhile you're right, the AI part is almost irrelevant. I think it's immoral that states can advertise their lotteries full-stop. True, the AI part will make this worse, especially if states get into really targeted advertisements. But the morality line is already being crossed daily. reply Intermernet 19 hours agorootparentCan I take this a step further and say that advertising is the true problem here? I tend to think most things should be legal, but advertising most things should be banned. It seems that society can't delineate between accepting a practice and promoting a practice. reply Fubarberry 19 hours agorootparentWhat would you recommend as an alternative to advertisements? I do think a lot of ads go too far, but I think some form of advertisement is necessary to get new awareness of new brands and products. Otherwise only the pre-established large brands and products will ever get sales, which would lead to a lack of competition and all the negatives that come with that. reply somenameforme 18 hours agorootparentIt sometimes confuses me how people seem to not think that word of mouth is a thing. It absolutely is. In the games industry, there are endless examples. Because on places like Steam, you generally only get promoted if you're already doing well - yet there are countless games that flop at launch meaning 0 visibility, yet they also see great long-tail success. One of my favorite is Kenshi. [1] Within a few months after launch, the dev was left with literally tens of users. Yet he continued working on it endlessly (while working as a part time security guard) and today it has near to more players than ever. And that's because of word of mouth. Which I suppose is something I'm also here partaking in. It's a great game! [1] - https://steamcharts.com/app/233860 reply Fubarberry 15 hours agorootparentWord of mouth is a great way to grow, but only once you already have users. Word of mouth requires people to already be your customer to bring in new customers, so the less business you have the less potentially useful it is. If word of mouth is the only advertisement replacement allowed, it would force many companies to rely on fake sockpuppet customers to try to get started. reply rixthefox 15 hours agorootparentprevI don't propose any alternatives to advertisements. The simple fact of the matter is that as long as people also need to pay for mobile data or are subject to data caps that advertisements are a giant waste of bandwidth and at their very worst a distribution method for malware and other malfeasance on the web. It's a giant \"pay for placement\" that has no regulation because as long as the advertising companies are making money hand over fist they have no obligation to police the content being pushed through their networks because they get paid either way! The age old use of advertisements that were generic and placed on TV channels were IMHO the \"peak\" of what advertising could be. All other forms of advertisements have proven to be predatory. Sites DEMAND you view their ads so that they can receive money to keep the site going.... That inevitably results in traffic to the site going elsewhere and the same result is that the site dies so I absolutely agree that if the site/product requires continuous profit through advertisements because otherwise it would not be able to continue operating then maybe that site/product does not need to continue existing. reply BiteCode_dev 15 hours agorootparentprevYou don't offer alternatives. Either a product or service is valuable enought to get customers, or it's not and it dies. It will actually benefit small structures that have no money for marketting anyway. And big ones won't be able to rely on their treasure chest to generate more of it, they will have to earn it. reply n4r9 19 hours agorootparentprevIn a civilised society, among other regulations, advertisements would be shown only to people that have pro-actively searched for a service or product. reply stuart73547373 19 hours agorootparentthis would be great if everyone was rich enough to afford to pay for their services with money. but it would cut off a lot of services and opportunities for everyone else who is able to pay with their attention (to ads) instead reply secstate 19 hours agorootparentI mean, we're basically heading towards a basic minimum income at that point, where you truncate the top 5% of the tax ladder and redistribute it to the bottom 25%. At that point, people could pay for services. Dig a little into the idea that people can \"pay\" with their attention (and with their poor health, crappy buying decisions, personal freedoms) might fit the dictionary definition of sinister. reply chii 17 hours agorootparent> truncate the top 5% of the tax ladder and redistribute it to the bottom 25%. At that point, people could pay for services. why should those at the top 5% pay for more than basic subsistence to the 25% at the bottom? Welfare is for survival (and survival only), not for services that is not essential. reply withinboredom 11 hours agorootparentI suspect you have no idea what the “top 5%” looks like (in the US). The bottom of that 5% looks like extremely upper middle class. Big houses, probably a vacation somewhere nice once a year, new cars fairly regularly, etc. The middle most likely live in gated communities, with any kids going to private schools. They fly first/business class everywhere and can probably even afford to buy one of those tickets at a moments notice. Their closet is worth more than most people will earn in several years of working. At the top end, these people have multiple tennis courts, pools, basketball courts, etc in their back yard. If there is something they want, they most likely have staff on hand to handle it. If they want to go somewhere, they hop in one of their private jets, helicopters, or chauffeured cars and go there. Oh, and they have a team of lawyers ensuring they never pay a dime in taxes. Source: spent some time with each of these personas in my life. reply secstate 16 hours agorootparentprevThis space is too small for a philosophical debate about the merits of UBI. Suffice it to say, when the top 5% has 1,000x more than they need for subsistence and the bottom 25% has never had access to more than 1x more than they need for subsistence, nothing will ever change. And more to the point, because having 10-20 times more than you need for subsistence has a demonstrable impact on your ability to generate more wealth 1) some will always get richer and pull away from the majority over time and 2) eventually the majority of people without subsistence amounts of wealth will rise up and kill you. Maybe that wont happen in our current system for a generation or two. But then the question those 5% at the top have to ask themselves is whether they feel lucky to avoid an uprising against them in their lifetime. reply n4r9 18 hours agorootparentprevYeah, the trade-off of switching money for attention is not obviously beneficial to society. For example I can easily imagine that there's a significant mental health impact of being constantly advertised newer and better things. reply keybored 18 hours agorootparentprev> What would you recommend as an alternative to advertisements? Not Having Enough Information is the least of anyone’s problems. > I do think a lot of ads go too far, but I think some form of advertisement is necessary to get new awareness of new brands and products. Otherwise only the pre-established large brands and products will ever get sales, which would lead to a lack of competition and all the negatives that come with that. This is a very post-hoc rationalization. Given that this isn’t how advertisement was invented and I have seen no evidence that this is how it works in practice, I don’t see a reason to accept it as a premise. See political advertisement in America. That massively, massively favors the big players, not the little ones. Not only to boost them directly but also by actively smearing the other candidates, which moves the outsider candidates from the category of “never heard of” to “scumbag” in the minds of voters. (If you have no other information you have nothing else to go on.) reply Fubarberry 15 hours agorootparentI would argue that larger brands have to have more ads to sway opinion because people are already familiar with their products. Coca-cola is one of the bigger advertisers, but everyone already knows what Coke tastes like and if they like it. Seeing dozens of Coke ads won't change someone's opinion much, compared with seeing a single ad for a new drink can take them from 100% unawareness of the product to knowing it exists. reply keybored 15 hours agorootparentAds are not chiefly about awareness. They are about producing want, manufacturing desire. The idea that ads are about “hey we have a product, just thought we’d let you know my good sir” hasn’t been true for over a century. You don’t advertise until you are confident that 90% of your potential market segment knows about you. You advertise continuously as long as the desire-making makes a profit according to whatever projections the marketing department makes. You say that that Coca-cola is one of the biggest advertisers. At the same time everyone knows about them. Are they simply pouring money down the drain? reply spencerflem 19 hours agorootparentprevmost ads are not promoting new products, the biggest spenders are CocaCola and Disney reply throwaway48476 18 hours agorootparentAdvertising well known products is an exercising in bribing the media advertised in to look the other way on their abuses. reply spencerflem 18 hours agorootparentI'm not convinced. I've seen these ads work on myself- I see an Oreo ad and think, oh an Oreo does sound nice now actually. Or that something is popular and the best: why go to Bush Gardens when we all know that Disney is the Premiere theme park. reply stuart73547373 19 hours agorootparentprev>Can I take this a step further and say that advertising is the true problem here? no? promoting stuff with a plausible chance of adding value to someone's life is not bad imho the problem is the stuff that kills value, like smoking and gambling. not advertising in general. so yes advertising (the many) bad stuffs is bad if that's what you mean by \"most things\" reply spencerflem 19 hours agorootparentMost ads are not for something that adds value to your life. When was the last time you personally saw an ad that's anything other than a trailer for a piece of media, and was excited and happy to see it. reply recursive4 19 hours agorootparentSome argue it’s gotten worse due to recent tradeoffs between privacy and accuracy, such as the negative effect Apple’s ATT has purportedly had on Meta’s targeted ad efficacy. reply soerxpso 17 hours agorootparentprev> the problem is the stuff that kills value, like smoking and gambling These are not the only things that kill value. Turn off adblock for a week and count the ads you see for quality products that you actually need, that you would not be worse-off for purchasing. Generally if something would actually add value to your life, nobody needs to trick you into buying it. reply chii 18 hours agorootparentprev> stuff that kills value, like smoking and gambling. but you decided (for others) that this stuff kills value. Not all people would agree with you. And while it's pretty clear that smoking is harmful, the smoker might want it as it relieves stress or for whatever reason, and they're willing to make the health trade-off. Ditto with gambling. reply somenameforme 18 hours agorootparentprevI really don't think this is true. The overwhelming majority of ads are just predatory and exploit all sort of psychological cues and tricks, like false association, just to sell more Product. Coke is one of the largest advertisers in the world, and a perfect example of this. They regularly imply that extremely healthy individuals, the sort that would never touch a Coke in real life, are avid Coke drinkers, implicitly suggesting that if you drink Coke - you can also be like them. This clip [1] where Coke managed to get a couple of bottles placed in front of Ronaldo during a presser is awesome. A visibly agitated Ronaldo places them aside out of the frame, grabs a water, holds it up, says 'Agua!' and the carries on. But Coke is so dependent on these false associations, that that one little event saw them lose some $4 billion in market capitalization! Imagine if advertising wasn't a thing, and people only associated Coke with the sort of people who regularly drink Coke. The world be a much better, and healthier, place. And Coke, as scummy as they are, are one of the less scummy advertisers in the advertising world - such as it is. [1] - https://www.youtube.com/watch?v=_nw7FOgOgtA reply BurningFrog 19 hours agorootparentprevHow do you launch a new product, service or other business if you're not allowed to tell prospective customers that it exists? Advertising can be annoying and a bad influence, but I think it's one of those things you'll really miss when it's gone. reply layer8 19 hours agorootparentThere could be a searchable directory of products and services tagged by topics. Interested parties could subscribe to topics, to be notified about new products or services. reply chii 17 hours agorootparentso you would approve of advertising then, because a notification is an advert (albeit for said topic). But then who would decide which topic for which product/services? The bottom line is that advertising comes about from a need - that need will emerge again, even if you tried to make it inefficient to fulfil that need. reply layer8 17 hours agorootparentNo, I would not approve, because ads come through channels that are unrelated to me looking for product information. The distinction is pull vs. push, and at the place and time of my choosing, and being unrelated to any other media. We also have a need for food and sleep, but that doesn’t mean we should accept being forced into eating or sleeping when visiting web pages or consuming media. The absurdity of that example should prove the point. reply robertlagrant 19 hours agorootparentprev> It seems that society can't delineate between accepting a practice and promoting a practice. There's no \"society\". Obviously people can tell the difference. You're the one saying the law should treat them differently - why? We have certain exceptions, but what's your case for a blanket rule, or a deny by default rule? reply keybored 18 hours agorootparentprev> Can I take this a step further and say that advertising is the true problem here? I tend to think most things should be legal, but advertising most things should be banned. Yes. Advertisement is in practice predatory. Predatory psychology. It’s not a symmetrical exchange of want-information vs. has-information. It preys on us. And I don’t think lotteries are particularly bad. They sell daydreaming and one-in-ten-million chances. Not a bad thing for anyone in moderation. Not that last part: all other vice sellers (candy, other unhealthy food, tobacco, alcohol) have no better arguments for their existence. reply andsoitis 19 hours agorootparentprev> advertising most things should be banned. > It seems that society can't delineate between accepting a practice and promoting a practice. If it were up to you, what would you allow to be advertised? What’s your allow-list? And how would you define it in a way to allow the advertising of future products and services? reply spencerflem 19 hours agorootparentnot OP: but my take is that this can be handled by stores promoting certain products. So if youre not in a store, youre not looking to buy and will see no ads. reply cafard 18 hours agorootparentprevNo. Perhaps gambling should be legal, but state lotteries are a regressive form of taxation. reply Integrape 17 hours agorootparentprevYes, goddam the pusher. reply bartonfink 19 hours agorootparentprevPot, meet kettle. reply rchaud 19 hours agorootparentprev> Can I take this a step further and say that advertising is the true problem here? No, you cannot. You are meddling with the forces of nature with this sort of talk! Without unregulated online advertising, there is no Facebook or Google or Twitter. Modern Silicon Valley could not exist without the fortunes made by showing people ads. Apple, Netflix, Amazon are all deep in it now as well. reply nvy 19 hours agorootparent>Without unregulated online advertising, there is no Facebook or Google or Twitter. Modern Silicon Valley could not exist without the fortunes made by showing people ads. Awesome! reply mandmandam 19 hours agorootparentprev> Modern Silicon Valley could not exist without the fortunes made by showing people ads. The Silicon Valley full of genocide cheerleaders? Sounds like you're threatening me with a good time. Facebook is ghoulish, and has scammed and brainwashed a huge portion of the most vulnerable people. Google is evil. Twitter is the dumpster fire other dumpster fires are ashamed of. People seem to be missing your Network reference. Regardless of how you intended it, yes, advertising is undoubtedly one of the greatest and least noticed evils of our age. There have been many warnings over the decades, but they don't get much airtime for some completely unsurprising reason. reply vouaobrasil 20 hours agorootparentprev> While you're right, the AI part is almost irrelevant. I think it's immoral that states can advertise their lotteries full-stop. Well, it does highlight the fact that AI will make this sort of manipulation so much easier and faster. Imagine a world where there the only vehicles are bicycles and people crash frequently. Now, the first car is introduced and someone crashes at 100MPH -- would you really say that the car is irrelevant to the situation because people have been crashing for decades? Yes, you are right: the root problem is the way we view people's minds as a resource to be exploited through psychological manipulation, but that does not make irrelevant the very real new danger of AI. reply ToucanLoucan 20 hours agorootparent> would you really say that the car is irrelevant to the situation because people have been crashing for decades? The tragic irony being that's basically what we did and continue to do. reply jprete 19 hours agorootparentI understand why you might say that - particularly at a societal level - but individual drivers: * Must be licensed * Must have a minimum level of liability insurance that covers other participants in an accident * Can be fined, get higher insurance premiums, and lose their licenses for breaking driving laws * Can be put in jail for felonies for sufficiently reckless driving that has the potential to endanger others As I understand it, commercially licensed drivers usually have even higher requirements and lose their licenses more easily for driving infractions. GenAI makes a lot of creative actions much easier, faster, automatable, and available to everyone, and \"everyone\" includes a lot of people with no foresight or discipline whatsoever. reply Swizec 19 hours agorootparentNone of this was true when cars were first introduced. Back in the early 20th century it was considered fairly normal that drivers would just sort of plow through pedestrians if they’re in the way. Big societal battle over who gets to be in streets – the people who have always been there, or the cars. Eventually jaywalking was invented as a concept by the car lobby. > And then the automobile happened. And then automobiles began killing thousands of children, every year. https://99percentinvisible.org/episode/episode-76-the-modern... reply diydsp 20 hours agorootparentprev> immoral that states can advertise their lotteries full-stop. State lotteries are a compromise: They inhibit underground lotteries. The goal of the state lotteries is to divert people away from the danger of the underground while keeping itself from being overly attractive. reply whiterknight 19 hours agorootparentWe have seen how well this argument holds up with sports betting, which went from something people do in Vegas on vacation to a significant portion of 20-30 year old males pay checks. Accessibility and marketing matters. reply iamthirsty 19 hours agorootparentI don't know where you grew up, but in almost every college guy's social network (usually already consisting of sports guys) we all knew a guy and didn't just bet in Vegas on vacation. It has always been a significant part of 20-30 yo male paychecks, now it's just acceptable to advertise on ESPN. reply mjhay 19 hours agorootparentThere's always been gambling, but there's a lot more of it now. Gross revenue has more than doubled since 2020. I think it's safe to say gambling's new-found ubiquity is a big part. https://www.statista.com/statistics/1332999/gross-gaming-rev... reply iamthirsty 19 hours agorootparentAgain, this is referring to legal gambling, whilst I was referring to illegal sports betting, which we would have basically no good accurate way to devise the total number pre- or post- legalization. reply whiterknight 7 hours agorootparentFor your argument to be true mass marketing must have zero effect on participation. reply noolean 19 hours agorootparentprevIs there data on this? As someone who was a 20-30 year old male for 10 years, it’s not something I would have ever considered, the effort, risk, and return doesn’t seem worth it vs. just buying VTI or SCHB and then letting them sit? Are the majority of 20s males really gambling a significant percentage of post-tax on sports? Has this displaced social fantasy football type games or the basketball brackets? reply orwin 18 hours agorootparentRecently it took off (and I do mean recently: post covid). You have ex-crypto/dropship bros who turned to sport betting to make 'millions'. basically they're rolled by the apps for half their bets, their winnings are the payment, and their balance is always positive since their EV is 1.60 (when normal customers EV is .80). That, plus advertising on college sports (including 'free bets'), it is really a big thing, even for 30yo+ people to be fair. reply helpfulclippy 19 hours agorootparentprevThe goal of the state lottery is to generate additional revenue by using its lawmaking power to protect and exploit a monopoly on gambling. reply yayitswei 19 hours agorootparentprevSo why advertise at all if the goal is to have fewer users? reply edgyquant 20 hours agorootparentprevWhy do they need to be state controlled for this? This is the same argument for casinos but states don’t run those. reply Err_Eek 20 hours agorootparent> This is the same argument for casinos but states don’t run those. In most (or at least a lot of) countries states regulate casinos _so much_ than it becomes almost a technicality that casinos are privately owned. Also with lotteries you have the scale factor -- in order to have an attractive lottery, you need a ton of ticket buyers, so if you have a national level lottery you can definitely make neighborhood lotteries way less appealing. Blackjack and poker tables don't scale the same way. reply SoftTalker 19 hours agorootparentprevUm, no the goal of state lotteries is money for the state to spend on things that voters would not want to pay taxes for. Most of the lotteries were started with the promise that they would \"fund education\" but in reality dollars are fungible. To the extent that they even try to maintain the facade, that's all that it is. reply robertlagrant 19 hours agorootparent> but in reality dollars are fungible This doesn't matter. It's that they're given for a separate service, vs compulsory extraction. That's the reality. reply dhosek 20 hours agorootparentprevWorse still, because it’s a state-run lottery, it’s not subject to truth in advertising laws so they can advertise the jackpot that you would receive over 30 years as if it were the lump sum payment that you’d receive for winning. reply robin_reala 20 hours agorootparentWhy are they not subject to those laws? reply JTbane 20 hours agorootparentprevI don't have a problem with state lotteries because it's essentially charity towards public works. You could argue that the gambling aspect makes it addictive, but it's basically entertainment. reply nerdjon 20 hours agorootparentI would buy this argument until they advertise it. Everyone knows what the lottery is, there is basically zero reason to do any advertising for it. Advertising it is encouraging addictive behavior. I think it is fine that the state runs it, but not the way that many do. It being a charity doesn't absolve them from bad behavior, plenty of charities have been... controversial. reply walthamstow 20 hours agorootparentprevA massively profitable charity towards public works funded exclusively by poor and lower-middle working people who are addicted to waiting for their numbers each week reply vouaobrasil 20 hours agorootparentprevWell, there is a fine line betweeen entertainment and a drug. That is precisely what makes entertainment so protected by modern governments and corporations: they know where that line is, and how to cross it. reply cryptonector 19 hours agorootparentprev> I think it's immoral that states can advertise their lotteries full-stop. Is it advertising lotteries that is immoral, or the lotteries themselves? reply hackerlight 20 hours agorootparentprevI go further than this. It's immoral to weaponize someone's evolved dopamine system against their own long-term interests. Whether that's fast food, alcohol, social media, loot boxes or gambling. It's fine to have products in these loose categories, but we've clearly gone overboard when people are fat and depressed because their evolved circuits have been so thoroughly hacked like moths to a flame. At some point we're no longer in the paradigm of voluntary consumer choice. reply apantel 20 hours agorootparentThat’s… most media, most products, most services. reply hamburga 19 hours agorootparentExactly :/ Not illegal, but immoral reply hackerlight 18 hours agorootparentprevThat's forcing things into an artificial binary when it's more appropriate to think of it as a spectrum, with unambiguously aligned products (e.g whole foods that improve user health) on the one hand and unambiguously exploitative products (e.g krokodil) on the other. reply logicchains 19 hours agorootparentprev>when people are fat and depressed because their evolved circuits have been so thoroughly hacked like moths to a flame People in America are fat and depressed because of a culture of looking to blame biology for poor self control rather than take responsibility for their own behaviour. Advertising is just as much of a thing in East Asia but it doesn't have the obesity problem that the west has, because there it's regarded as a personal failing. Advertisement was just as much a thing in America 50 years ago as it is now, but people weren't fat and depressed then, because people were told to take responsibility for their diet and their state of mind. Willpower is like a muscle, it's been proven to strengthen with use. Letting people get away with blaming advertisement, biology or whatever for their failures prevents them ever needing to develop their willpower. reply orwin 18 hours agorootparentWillpower is a bucket that fill during your sleep, and with time you spend with people you enjoy, and is emptying each awake second, and faster if you have to use it to defend yourself against distractions. You can make the bucket bigger (especially as a child) so you can start the day with more than your peers, but ultimately, resisting too much can be exhausting. I say that as an ex-obese who can now say he's 'fit'. I wouldn't pose on magazine or on Venice beach, but I'll be quite happy to go shirtless this summer for the first time in more than a decade. I'm constantly hungry, and will be all my life. I fucked up my hormones by eating too much. I don't have any food I can just eat at home. I have to prepare it before. I eat out a lot, and always place myself away from food at gatherings. Each decision I make to avoid eating, I transformed into rituals, so they aren't decisions anymore. If I had to also stop smoking/drinking/gambling, I wouldn't have succeeded. If I had half my salary, I wouldn't have succeeded. Obesity in men is quite hard to see/feel at first, especially since I bummed my ankles before putting on weight, so most physical diminutions I put on my accident and not my weight. When my fat showed on my chin, I was already at 34 BMI (I'm at 24 now, I've been around 26-28 for 4 years). I also was fit as a teen, my upbringing gave me a very good support system and 'community', so my willpower is easy to refill: a good demonstration, a 1st of may, a short night/weekend in a 'ZAD' (https://en.m.wikipedia.org/wiki/Zone_to_Defend) and I'm good to go do any soul-crushing task for hours. reply hackerlight 18 hours agorootparentprevYou should read Robert Sapolsky's new book on free will. The more you learn about biology the more you realize that willpower and free choice are a convenient illusion that appeals to human narcissism but doesn't actually have explanatory power. The introduction of fast food caused America to get fat. The introduction of Ozempic is helping America reverse that. Neither of these changes are because the average American gained or lost willpower. These changes are because of biological cause and effect. Fast food and Ozempic are products that hack ancient biological circuits that drive behavior. It's empirically falsifiable causality, not vague woo. reply verisimi 20 hours agorootparentprevIts one thing for a human to pay another human to call up leads or whatever, and an entirely other thing to create a factory to do the same job and hit thousands/millions at a lower cost than someone would pay the single human. The scale difference is significant, albeit I also agree that neither is moral. reply nerdjon 20 hours agoparentprevI really want to know who green lit this after seeing all of the controversy with Google and was like, yeah we won't have a problem like that billion dollar company. There are already a lot of sleazy ads that I see for gambling and the lottery. But this is extremely predatory since it personifies the possible winning. This should be straight up illegal. reply MPSimmons 20 hours agorootparent>after seeing all of the controversy with Google This reeks of someone doing absolutely no market research reply duped 19 hours agoparentprevGambling is predatory and immoral(*), regardless of what technology is used to advertise it. I know that makes me sound like a tea totaler who was trapped in cryogenic sleep for the last century, but I really can't think of a good reason to allow any kind of gambling at scale in our society. It's filled with negatives. (*) of course, this is my opinion, you might not have the same morals as I do. reply GreenWatermelon 8 hours agorootparent> It's filled with negative I would even say it's only negatives. But unfortunately, it will have to stay legal, otherwise people would still seek it through illegal means, possibly exposing themselves to much worse dangers. It's the same problem with drugs (including Alcohol) and tobacco, they need to be legal in order to he regulated. reply duped 6 hours agorootparentI disagree, because industrialized gambling is very different from illegal gambling in terms of availability. I see the prevalence of slots in my neighborhood after they legalized gambling wholesale in my state and I don't think it's been made better by \"regulation\" except through some additional taxes, it's just made it easier to access. The same could be said about prohibition for alcohol and tobacco. Raising the age for legal tobacco consumption did reduce consumption, and prohibition did lower consumption permanently for alcohol. reply ethbr1 20 hours agoparentprevGenerative AI is fundamentally \"more volume\" than anything else. > Some of the worst \"abuses of AI\" are going to be things we've already fully normalized In this case, I can't see more advertisement substantially altering the experience. Possibly micro-targeted images? But is that really going to change the net effect of advertising? reply rdtsc 19 hours agoparentprev> manipulate people into gambling is predatory and immoral Exactly. Especially at the government level. Fine, some states allow casinos to operate, but it's pretty evil when they run lotteries themselves. The ridiculous reason \"we're using these funds this to help educate people about math\" is just that, ridiculous. reply SilasX 18 hours agoparentprevI think, regardless of the specifics, using a HN discussion as a pretense to push your tangentially-related pet issue is immoral. reply hobs 19 hours agoparentprevOh we reflect on it all the time allright, its just that those in power like the status quo, and will put forth enormous effort to keep it. reply 8338550bff96 20 hours agoparentprev> I think regardless of the specifics, usingto manipulate people into gambling is predatory and immoral... Fixed this for you bro reply vouaobrasil 20 hours agorootparentOkay, so that means these two sentences are the same: \"It is very immoral to stab someone with a knife.\" \"It is very immoral to kill a billion people with a bomb.\" And we should treat these situations with the same attention. Whatever happened to considering magnitude? reply 8338550bff96 18 hours agorootparentWhy are you using sentences other than the template that I showed? The outcome for \"It is very immoral to stab someone with a knife.\" and \"It is very immoral to kill a billion people with a bomb.\" is very different in scale and kind. What I highlighted is that the means by which a common objective is reached is irrelevant when the objective is inherently immoral. I hope this helps you understand how language works. reply vouaobrasil 16 hours agorootparent> What I highlighted is that the means by which a common objective is reached is irrelevant when the objective is inherently immoral. I disagree completely if after the initial objective is reached, the means then propagates the reaching of further objectives that are also immoral. And the more advanced the technology, the more immoral means will be reached. reply Aerroon 19 hours agorootparentprevAren't they equivalent in terms of morality? If they aren't, then you've basically answered the Trolley Problem by implying that the outcome with more deaths is morally worse. Magnitude is a question of pragmatism. I don't think pragmatism is very popular among people when looking at things like the war on drugs (despite so many people being against it, nothing has changed). reply hackable_sand 17 hours agorootparentprevThe trolley problem is the *real* threat here. reply silverquiet 20 hours agoparentprevnext [5 more] [flagged] vouaobrasil 20 hours agorootparentWell, in general, there is a very strong mythology with technophiles in general and here in particular. It is the mythology that we should further technological progress because, well, \"because!\" Unfortunately, this mythology is becoming exceptionally maladaptive in modern society but we can't see it because it is so deeply ingrained. reply kevindamm 20 hours agorootparentIt is so deeply ingrained because there are many ways in which technology has made life better, both for the technophile and the average person. Admittedly, there are many harms as well. I can't say whether one outweighs the other, and perhaps it is because the technophile often avoids many of the involved harms that it becomes too easy for many to overlook them or play down the impact. reply vouaobrasil 20 hours agorootparentIt is a difficult question indeed, and it depends also on the following: are we measuring the developments of today by how they will affect the world in 10 years or 100? Most people seem to agree that the latter is the more logical measure, yet we operate as if we are using the former. reply silverquiet 20 hours agorootparentprevMy biggest complaint is the tendency to conflate change with progress. Perhaps I'm something of a luddite, but in the last ten to fifteen years or so, I have seen plenty of change, but whether it has been beneficial is often hard to suss out. It seems rather uncontroversial now that social media is detrimental to teen mental health (and I really don't think it's limited to teens). From the start I saw reasoned arguments that this sort of technology was deleterious for a few reasons, but the counterargument I usually saw here was, \"yeah, but look at how much Facebook pays developers... and they contribute to open source\". Two of the most prestigious of companies here, the F and the G in FAANG are purely advertising companies (and the others have plenty of it as well); what are they doing but trying to manipulate you into buying things you probably don't need? Obviously the pay is attractive, but is that how anyone but a psychopath wants to spend their career? reply throwaway48476 18 hours agoparentprevAdvertisers truly aren't people. reply sangnoir 18 hours agorootparentHow do you support your livelihood, if it involves products or services, kindly share your sales funnel. reply aspenmayer 17 hours agorootparentprevPart of me agrees with you, but another part of me hesitates to dehumanize anyone, as that way lies madness. I'm reminded of the film They Live (1988). The protagonist, Nada, seeks a job at a construction site, as that is his trade. Due to the union, the foreman can only hire Nada as a day laborer, as Nada is not in the union. His way of life is gate-kept by powerful interests controlling the hiring process, and access to capital is constrained by these interests. By the time Nada obtains the sunglasses, he has already lost that job and is well down the path of becoming radicalized. Ironically perhaps, he first heard about the group producing the sunglasses when they hijacked the TV signals to essentially advertise their ideology. Once Nada acquires the glasses, he could be said to be enlightened, but I also view it as a sort of cautionary tale, considering how the film ends for Nada. Along the way, he sees through all of the ads, and in fact much of the surrounding content itself rings hollow for him. His actions are reminiscent of Ted Kaczynski in this context. Why didn’t he instead seek to join the union? Were the glasses actually what they purported to be, or were they a kind of rose-colored glasses that were designed to manipulate the wearer just like the advertisements did, but while making them believe they were seeing things more truly? Nada thought he was acting under his own agency before putting on the glasses, and yet he so readily trusted that he was still after putting them on, while adopting an anti-capitalist, anti-authoritarian ideology as part of an underground fringe group. Perhaps Nada was just tired of being part of the system, but by rage quitting as he did, we as the viewer are led to believe he goes out in a blaze of glory, but I view his exit to stage left thusly as “full of sound and fury, signifying nothing,” to quote the Bard of Avon. There is no exit from capitalism that way, if there is any alternative at all if capitalist realism holds true, and Nada’s final line is perhaps an admission that he was himself used all along, and he was simply tired of scraping by and being in scrapes. He had gone down the path he was on as long as he was willing to go, and there was no way back, as he was already too far gone to come back. I’m reminded of the is/ought distinction, and I’d make a similar distinction between advertisements/content. Content is itself a similar kind of appeal to believe or think about concepts and ideals and ideas that come from outside ourselves. Due to the power of belief, capital seeks to gate-keep access to content, and advertising acts as a sort of escape hatch, blow-off valve, or side door through which access to those seeking content are able to be reached, accessible for a fee. It’s not exactly ideal, and not without its own hidden costs to the buyer or seller. Lack of capital is the conflict that leads to the crisis in Nada’s life, but I wish that his story had a happy ending. I feel that Nada was perhaps less free after putting on the sunglasses than he was before, as strange as that may sound. He fell for a classic blunder, believing that he actually was immune to advertising, rather than being sold an even bigger lie: that he could (only?) effect change through violence, that his life was no longer worth living. reply jameshart 20 hours agoprevEvery time an AI model is released, people on this site show up to complain about how it has been censored and bent to some puritan agenda. ‘Why,’ they ask, ‘would I want a politically correct AI that’s been nerfed so it can’t offend anyone?’ Well, this is what happens if you build real applications using insufficiently censored AI. This is why OpenAI and Google and stability always explain how they have restricted training content and aligned the output to avoid producing controversial output. Because they want people to build applications using this technology and those people do not want this headline to appear about them. reply ranyume 19 hours agoparentEach product has their own value proposition. I don't think anyone opposes that each business has control of their own AI pipeline, the crux of the problem is at what stage of the pipeline this censorship is injected, specially in open models. Of course businesses have their right to release AIs with censorship, and we as costumers have a right to challenge that and ask for a product that is actually useful to us. Saying it again: If a business releases a model, we have a right to demand that it is useful for us because we're the business' costumers. If those demands aren't met by a business, another business should be able meet them in open competition. And this only happens when open uncensored models are not banned by law. reply sangnoir 17 hours agorootparent> ...specially in open models. > Saying it again: If a business releases a model, we have a right to demand that it is useful for us because we're the business' costumers Open models == free == no customers. For open foundational models, yours sounds like a demand that someone else invests millions in training a foundation model of your liking, but you're not motivated enough to fine-tune your copy of the model for hundreds or thousand of dollars? If you're not paying for the \"censored\" proprietary models, you are not a customer either. If you are a paying customer of \"censored\" models, and would like them to change, remember businesses have the right to choose who they want as customers - and they can decide they don't want the \"uncensored\" market segment. I'd love a $25,000 electric Ferrari too, but Ferrari is not about that. reply ranyume 16 hours agorootparentYou're right, businesses can choose their market segment, there's nothing wrong with that. They just have to bear with the loss of costumers when the segment they were aiming for makes demands they're not prepared to fulfill. In your example, it doesn't have to be Ferrari specifically. Someone could make an equivalent product at a lower price even. But I have something else to say about your comment. It's that nothing is entirely free. Google lets us use their search service, but we're paying with something else than money. When a business \"gives away\" an open model \"for free\" it's only on their self-interest, and this does not free them of the expectations/responsibilities with their costumers. No matter what form that self-interest takes, we're still their costumers even if it's not money. So if they release an open model subpar with what we expect, they're gonna end up losing. reply jameshart 16 hours agorootparentprev> If a business releases a model, we have a right to demand that it is useful for us because we're the business' costumers. What makes you their customer? Are you offering them money for access to the uncensored model you say you want? Do you have some business that you’re going to be able to build that tests on access to an uncensored model, and their failure to offer that to you is preventing them from getting paid by you to provide the underlying technology for your venture? Because on the other hand there is a state lottery over here who absolutely is willing to pay someone money for a safe, censored model. They are an actual paying customer with a real use case and cash to back it up. Of course, it turns out the AI they ended up using didn’t (if this story holds up) actually meet their expectations with respect to safety and alignment - but let’s be clear: they wanted a safe, censored model. I find it hard to believe that the market for uncensored models - LLMs that occasionally output 4chan style racist tirades, diffusion models that occasionally generate pornography - is actually worth addressing, compared to offering models which don’t do those things. reply ranyume 15 hours agorootparent> I find it hard to believe that the market for uncensored models - LLMs that occasionally output 4chan style racist tirades, diffusion models that occasionally generate pornography - is actually worth addressing, compared to offering models which don’t do those things. Well I think that's for the market to decide, yes? If state intervention is out of the question, obviously. > but let’s be clear: they wanted a safe, censored model. That's absolutely right. reply andy99 19 hours agoparentprevThis confuses at least two things. Censorship is about intent and enforcing a world view. If I want a model to generate something for me and it doesn't because of censorship, that's one problem. If I want a model to generate a fun pic of me at the beach and instead it makes a nude photo, that's completely different. reply sangnoir 17 hours agorootparent> If I want a model to generate a fun pic of me at the beach and instead it makes a nude photo, that's completely different What if someone else's idea of a fun pic at the beach is a nude photo? Is it only censorship when the worldview being enforced is not aligned with yours? A truly uncensored model should accommodate everyone, but society will (and should) absolutely burn down any AI service that accidentally generates nudes from a beach photo of a family that has minors. \"Intent\" is a human attribute, AI is probabilistic. This abundance of caution by the companies avoiding nudity is unfortunately read as promoting a worldview or pushing an agenda, while it's just risk mitigation. reply jameshart 11 hours agorootparentprevYes, but most people share the world view that a state lottery should not generate unsolicited lewd images of their customers. This suggests there is a large, and perfectly reasonable demand for censored models that adhere to broadly ‘SFW’ cultural norms. So why are people upset that commercially available models generally try to cater to that obvious need? reply hedora 19 hours agoparentprevRegardless of their intentions, current state of the art automatic censorship have dangerously bad false positive and false negative rates. So, they inadvertently and inevitably suppress the truth and broadcast misinformation. Their performance on images is similarly bad. Of course, even if they did work, centralizing control of them is a bad idea. The end user should have the freedom to control such things. (In the same way you can type swear words into a word processor). reply the_snooze 19 hours agorootparentIn practice, these AI models are centralized services. Few people are running them locally on their machine with knowledge of the training data, weights, or system prompts. There isn't a direct line between \"user intent\" and \"system output,\" unlike the local word processor in your example. Someone has to make value judgments on the underlying data and algorithms, if only to build the system in the first place. If users can't (or don't care to) run these models locally, who else will decide other than the service operator? reply jameshart 19 hours agorootparentprev‘The censorship work done in AI alignment is insufficiently accurate to rely on for production use’ is a good and valid position ‘Therefore AI vendors should release uncensored models and let everyone else figure it out’ is not a logical consequent from it. ‘Therefore don’t use current AI tech to build apps that photomanipulate user submitted content and echo the results back to them with your logo on it’ would be a more logical conclusion to draw. reply Fubarberry 19 hours agorootparentprevThankfully that is how a lot of AI is working right now, open source models for both LLM and Stable Diffusion let people have access to uncensored reasonably decent quality AI tools. In other categories of AI generated content though, such as speech and music, the open source alternatives are either lacking or completely absent. And with the company behind Stable Diffusion being on the verge of financial collapse, we may see Stable Diffusion fall far behind it's closed source counterparts. reply SV_BubbleTime 19 hours agoparentprevWay to completely misunderstand the issue at hand. The issue isn’t “we’re mad at them for subscribing to a New Puritan agenda (although there is one playing out); the issue is that right now there is no way to censor the model for “undesirable” features without lobotomizing the acceptable content as well. It’s not just removing nipples from training. We have evidence that the “safer” the models get, the worse they get. The more heavy handed the weights are by manual influence, the less they are producing valuable output. The cost of performance for everyone is directly coming at the cost of the perpetually-offended, and I suggest why bother because those people are never satiated. When you make your identity about being offended, there will always be something that is offensive. reply jameshart 19 hours agorootparentI thought the issue at hand was that a state lottery generated fake porn of a customer? reply SV_BubbleTime 19 hours agorootparentTypically replies are referenced to a comment. But… The spur here you are cha going the topic to is a side effect of government employees not knowing that these models aren’t yet so lobotomized that it is still possible to create “problematic” imagery. reply jameshart 11 hours agorootparentI’m just saying that I don’t think the lottery player who got sent fake porn of herself is necessarily a member of the ‘perpetually offended’, to whom you’re ascribing the demand that model outputs be restricted. I think she’s part of the class of ‘actually offended’. This image wasn’t ‘problematic’, it was potentially actionable. And when you are a business, actually offending your customers is normally not a good idea. Striving to create software systems that avoid actually offending people is a good idea, even if it comes at the expense of some other benefits. Whatever ‘performance’ we apparently give up by training systems not to generate nude photographs has to be weighed up against the risk that it might do so at an inopportune moment. reply araes 17 hours agorootparentprevYou take out the nipples and vaginas and you lose a large portion (most?) of human advertising. reply inanutshellus 20 hours agoprevFWIW, in trying to explain the various ways machine learning can go awry to our business users, I recently discovered the \"AI Incident Database\" (https://incidentdatabase.ai/). It's full of interesting gaffes AIs make and while it looks like this one hasn't been submitted yet, I assume it'll be there soon. If you're interested at all in tracking these, it's a great resource. reply mdaniel 16 hours agoparentIn case one were curious how the sausage gets made, grab some popcorn and marvel at this python: https://github.com/responsible-ai-collaborative/nlp-monitori... and the RSS feeds they monitor https://github.com/responsible-ai-collaborative/nlp-monitori... reply 015a 19 hours agoprev> “I also think whoever was responsible for it should be fired,” she added. > “We were made aware that a single user of the AI platform was purportedly provided an image that did not adhere to the built-in parameters set by the developers. Prior to launch, we agreed to a comprehensive set of rules to govern image creation, including that people in images be fully clothed. We have not seen the image that the AI provided to this user, but out of an abundance of caution we took the website down,” the spokesperson confirmed to the Jason Rantz Show on KTTH. Here's a tangential take to all this, but its the overwhelming thing I feel reading this: Something happened to our society which resulted in effectively no one being able to be held accountable for anything. Its always \"well, we wrote this policy and we were well within the policy! the thing just went oopsie daisy no one could have predicted that!\" They were creating and distributing porn of non-consenting individuals. Everyone involved with this should be fired, and maybe actually it should go further than that. I feel this tremendously in the day-to-day of the tech world. I will make some system change which brings down production. I hop on a postmortem with my boss, and the first thing she says: \"We have a blameless engineering culture. Its not your fault, and I don't hold you responsible.\" I disagree, entirely: it is my fault. I need to own it, and I don't understand how people function tip-toeing around accepting responsibility for something that is obviously their fault. How do you, the individual, improve? reply Aerroon 19 hours agoparentDid they actually distribute the image to anyone else other than the woman? The developers say they hadn't seen the image nor was the journalist able to find it. reply nprateem 18 hours agoparentprevAnd the line for your responsibility is where exactly? You want to be personally responsible for every single dependency you pull in, every piece of hardware between you and the end user? This is a farcial suggestion. > They were creating and distributing porn Unless they specifically trained their own model to do this - which they obviously didn't - this is just a bug. If we all got fired any time a bug happened, society would grind to a standstill. Shit happens, move on. What's important are intentions and taking reasonable precautions. reply 015a 18 hours agorootparent> You want to be personally responsible for every single dependency you pull in Yes. > every piece of hardware between you and the end user Nah. See how you said \"where do you draw the line\" and then proceed to just ignore drawing the line? I'm ok with drawing a line. There doesn't have to be logic or reason behind it. That's where I draw it; the repositories and websites and apps that I distribute. > Unless they specifically trained their own model to do this - which they obviously didn't - this is just a bug. Go try to make Midjourney generate a nude image of someone. Its extremely difficult. You'd have to be extremely intentional and, frankly, malicious. I've never seen it happen. The way this article is written, it sounds like the user just uploaded a photo and it happened. So, what kind of prompting are they using which led to this? Also: They said they don't have a copy of the image, but that's extremely hard to believe just assuming how systems like this would be designed; the image has to be served from somewhere, are they seriously purging the images from e.g. their storage buckets that quickly? Possible, but hard to believe; of course, they'd say \"oh we don't have a copy\" if they investigated, found it doing this, and just wanted the story to die. Or, we could do as you want, and just sweep this under the rug. Nothing to learn. No one to blame. Just a quirky little thing that happened! So quirky! A state gambling office manufacturing pornography of random people to sell lottery tickets! Quirky and fun! reply Cpoll 15 hours agorootparent> They said they don't have a copy of the image, but that's extremely hard to believe just assuming how systems like this would be designed; the image has to be served from somewhere, are they seriously purging the images from e.g. their storage buckets that quickly? Possible, but hard to believe; of course, they'd say \"oh we don't have a copy\" if they investigated, found it doing this, and just wanted the story to die. I don't find it very hard to believe. 1. If the image can be generated in a few seconds, the easiest way to implement this is to accept the file upload in a request, do the AI stuff, and return a new image in the response. In this case, it would be extra (and unnecessary) work to save it in a storage bucket. Both the uploaded and generated images only exist in memory for the lifecycle of the http request, and then get deallocated. 2. If the image _is_ being saved in a storage bucket (or Redis, or whatever), the bucket might have a lifecycle policy on it that automatically expires images after a set time. This is a pretty common cost-saving measure, so it's not too unlikely. reply barfbagginus 14 hours agorootparentprevA decision maker figured they could cut costs by using a DIY model instead of paying for mid journey API calls. They're the ones who are responsible. But I wouldn't trust any diffusion image gen here. They're all liable to create disturbing and inappropriate imagery, and there's no way to prevent it. reply nprateem 17 hours agorootparentprevWell it looks like they've drawn a line too and found themselves not responsible, so they're already doing what you're advocating it seems. Nothing to learn here. reply soerxpso 17 hours agorootparentprevIf you were messing around with a gun that had the safety on, and the gun malfunctioned and shot a child while you were waving it around, would it be a valid defense to say that you took all the necessary precautions (since the safety was on, after all) and it's just a silly oopsie that the gun did that? You should have a responsibility to put the necessary safeguards on a potentially dangerous tool that you're using and to do your due-diligence if you're using that tool. It would have been impossible to accidentally produce porn of a non-consenting individual if they had not been using an AI image manipulation pipeline at all. If you do choose to use an AI image manipulation pipeline, you should take on the responsibility to make sure that it's not going to do illegal things you didn't tell it to do. If there's a risk of causing harm with it, just don't use it! You don't need to be producing cute images of people in Vegas or whatever, and if doing so necessarily implies a risk of accidentally creating porn of them instead, you could (and in fact, have a responsibility to) just choose to do neither. reply epups 17 hours agorootparentA gun malfunction has a potentially lethal consequence. This bug caused a person to see a fake photo of herself with fake boobs out. Perhaps it's ok that the safety standards for the latter are lighter, no? You also didn't address the main point of the earlier comment. Is every programmer responsible - even criminally, some suggested - for potential bugs or vulnerabilities in one of their products? reply houseofzeus 20 hours agoprev'“I also think whoever was responsible for it should be fired,” she added.' Ah but that's the joy of, it sorry about that - wasn't us, it was the computer! reply nkrisc 20 hours agoparentThat’s the true value of AI. reply igammarays 20 hours agorootparentBingo. Also see the \"AI\" system used by Israel to target \"Hamas\". \"It's not us, it's the AI\". reply rchaud 19 hours agorootparentShould be added to the Five Standard Excuses, from the '80s show Yes Minister: https://www.youtube.com/watch?v=6Y4PEqvk0Jg reply vouaobrasil 20 hours agorootparentprevThat is a good point, I hadn't thought much about that. AI is an excellent tool for shifting responsibility to big tech, who in turn can easily weather the storm with their world-class legal teams. reply OnionBlender 17 hours agoparentprevThat's exactly what Air Canada tried to pull. > The airline tried to argue that it shouldn't be liable for anything its chatbot says. https://www.wired.com/story/air-canada-chatbot-refund-policy... reply chrisjj 20 hours agoparentprevBut \"it\" is the decision to use (so-called) AI. reply ilaksh 20 hours agoprevI find it quite funny, especially if you realize that something like 90% of the Stable Diffusion model fine-tunes out there are actually made for generating porn or images of females. Go to the website that has the most image generation models to verify this for yourself: https://civitai.com WARNING don't visit this site on your work computer. But point being, it is actually kind of hard to find a Stable Diffusion model variant that is fine-tuned to avoid fairly broken generation that comes from the base models but doesn't have female objectification built into its fine-tuning dataset. Especially if you go searching for \"Stable Diffusion models\" and happen upon that site, since it does have the most models. reply Tenoke 19 hours agoparentTo be fair that's partially the case because the base models are worse at NSFW while decent at other stuff. If you look at actual images generated then NSFW is common but hardly the majority of images. reply araes 16 hours agoparentprevDon't know what part of the shadowban Matrix you're looking at, yet I did not see a single nipple in 10 pages of image and model scrolling with Woman selected as the highlight choice. Vaguely a let down after that much NSFW warnings. (Obviously, many that are meant to create \"porn\" images. Although even those looked mostly like upskirt anime generators). Notably, a LOT of beefcake and partially shirtless male imagery on normal search. Also, quite a bit of \"psst, this is really furry imagery\" reply Fubarberry 16 hours agorootparentThe website has started filtering anything tagged nsfw, you'll have to create an account and enable 13+ or 18+ imagery. reply Der_Einzige 19 hours agoparentprevWhat's even funnier is if you work professionally with diffusion models, you have to go to civitai.com as a work requirement! I've had the talk with at least 5 significantly older coworkers who didn't know what \"booru\" websites are or why it is that civitai is so full of anime. Imagine trying to explain what the hell \"Pony Diffusion\" is and why everyone is using it suddenly. reply croes 21 hours agoprevAI is a box of chocolates ... How often do AI sites show the example of an astronaut riding a horse on Mars? Changed the horse to a duck and never got a satisfying result. AI doesn't understand, we learn through trial and error how to get the result we want but it seems the way to the result is always different. That's like programming but the keyword change in every function. reply josefresco 19 hours agoparent> Changed the horse to a duck and never got a satisfying result I also found this curious. Stable Diffusion is great at creating this one image but then for some mysterious reason sucks at almost every subsequent request using default/recommended settings. Sometimes even a request for another astronaut riding a horse produces garbage. reply Kerb_ 20 hours agoprev\"’Our tax dollars are paying for that!’... Megan explained\" Yep, about $.01 to generate that image distributed between 7 million tax payers in that state. I love that people are so focused on saving pennies that they don't comprehend the long term issues we are gonna face :) While I do understand how frustrating it is, about any other argument sounds like it would be better than the monetary one. reply phillipcarter 20 hours agoparentThe phrase is more common in conservative circles than liberal ones. Since the site is explicitly a conservative media outlet, it's not surprising that they used this quote. reply KMnO4 20 hours agoparentprevI also noticed that. I almost laughed at the absurdity. “How do you feel that softcore porn of you was created and hosted on this website?” “The part that annoys me is that the the government had the audacity to do this with MY money” reply nilamo 19 hours agoparentprevI don't think most people understand how millage rates work. Complaining about how expensive some projects are, but then it's less than $5/year if you actually calculate the millage. reply SoftTalker 19 hours agoparentprevAnd Washington would probably say her tax dollars not paying for it, the lottery is. Though in both cases it's state money. reply fullshark 20 hours agoprevAll this GenAI to basically get people to spend more time on their phones, either looking at ads or gambling. At the same time devaluing genuine artistic achievements for low effort simulations of such. What a depressing technology we are all working on. reply pinebox 20 hours agoprevThe most disturbing thing about this is that the lottery couldn't seem to confirm if it happened or not. reply maweki 20 hours agoparentIt's easier from a privacy perspective to not store and retain the submitted and rendered photos. One less thing to worry about regarding the privacy agreement. reply HumblyTossed 20 hours agoprev> “I also think whoever was responsible for it should be fired,” she added. It will probably be an engineer and not the idiot who thought this would be a great idea. reply mingus88 20 hours agoparentThis is a state gov. The blame is shifted 100% to the contractors and consultants who developed the site Number 1, that’s because state governments aren’t paying salaries for full time AI developers. WA has a huge software industry that pays competitive salaries. Number 2, state positions are the last bastions of pensions in our workforce and nobody is risking their nest egg for a decision that they can pawn off on a temporary contract worker. I started my career at a place like this. All decisions are made by a committee and the actual work is outsourced. Nobody is going down for this. reply isaacfrond 19 hours agoprevThis site has a couple of example pictures that show how the system was supposed to work: https://adage.com/article/marketing-news-strategy/washington... The pictures look kinda nice actually! Pity it turned in the another AI fiasco. Reminds me of the Microsoft chatbox Tay. Another one of such well intentions but ultimately doomed AI projects. reply DamnInteresting 18 hours agoprevWhen it comes to AI image generators and questionable content, I've wondered if AI companies can address the problem by using a pair of AIs. Use one AI to generate the image, then feed that image to another AI with a list of questions, such as \"Does this image show nudity?\", and \"Does this image depict violence?\" Just a list of everything the AI provider wants to omit from its output. If the answer to any is Yes, discard the image and try again (or just refuse to furnish any image). reply thedookmaster 20 hours agoprev> We were made aware that a single user of the AI platform was purportedly provided an image that did not adhere to the built-in parameters set by the developers. I laughed at this. What a non-apology with no accountability. reply panda-giddiness 17 hours agoparentThat's the quote that stuck out to me, too. It's like they're insinuating that the person is lying (while elsewhere admitting they don't have evidence either way). I mean, I don't want to backseat-quarterback here, but would it have been so hard to say something like, \"We have decided to take our website offline while we investigate a report that our website generated an inappropriate image of a user\"? reply rchaud 19 hours agoprevI never thought I'd long for the days of bog-standard Corporate Memphis clipart to decorate web pages. reply lubesGordi 20 hours agoprevAh, non-determinism at its finest. reply havetocharge 20 hours agoprevI wonder if there's a way to find out which AI model is used on the backend. reply donatj 20 hours agoparentSo you can avoid it? reply jaimex2 20 hours agoparentprevNot Gemeni as she would have come out black. Probably a poorly configured Stable Diffusion 1.4 variant. reply d-z-m 19 hours agoprev> “I also think whoever was responsible for it should be fired,” she added. I understand she's upset, and rightfully so. However, I find the vindictiveness distasteful. reply friend_and_foe 17 hours agoparentI think the vindictiveness is all this is about. She's looking to win a lawsuit. I don't agree with your \"rightfully so\" perspective. A woman in the privacy of her own home put a picture of her face into a prompt and it spit out a picture of someone that maybe looks a lot like her, topless. Big deal. She either has to be feigning outrage or she's never seen herself naked in the mirror. reply nozzlegear 16 hours agorootparentPersonally I’m of the opinion that the government shouldn’t be generating nude images of anyone. It doesn’t take real outrage or faux-outrage for me to hold that opinion either. reply kaptainscarlet 19 hours agoprevAI might erode some of the gains we got from camera techonology because the better it gets at manipulating images, the less humans trust digital images as evidence reply rchaud 19 hours agoparentHumans already don't trust digital images. See the rise of catfishing in online dating, or deepfake ads of Musk and Bill Gates shilling some shitcoin. reply tinix 20 hours agoprevtopless isn't porn, breasts can be exposed in public here. the ai just knows https://www.kuow.org/stories/it-s-totally-legal-to-be-naked-... reply peddling-brink 20 hours agoparentBut topless can be porn. And this was. And this was non-consensual. reply SV_BubbleTime 19 hours agorootparentIf we’re arguing pointless things… it also wasn’t real. How about the meaningful thing, that why is tax payer money being used to create a generative AI site at all? This should be a non-problem because this is in no way the role of government. reply roywiggins 19 hours agoparentprevSure, but handing out crudely photoshopped nude photos of strangers out to them is not going to go over well, especially if you're wearing a uniform that says \"Washington State Government Employee\" on it reply skywhopper 19 hours agoprevThe saddest part of this is that the everyone involved from the reporter, to the lottery staff, to the company that provided the service, all appear to think that the prompts they provided to the AI are somehow bulletproof and that their human understanding of the parameters matches what the software will do: “we had the developers check all the parameters for the platform and were comfortable with the settings, but after further internal discussions today, we chose to take down the site out of an abundance of caution.” I know eventually the reality of these tools’ flaws will seep in but it’s always somewhat surprising to me how certain most people (including tech folks in the field who ought to know better) are that the AI actually “understands” the prompt in the way that a human would. reply Apocryphon 19 hours agoparentAI is marketed as magic, and laypeople will believe them to be so. Blame the hollow promises of the makers and the sellers. reply burntalmonds 19 hours agoparentprevThe statement they made was confusing. Abudance of caution? It already generated AI porn. reply rchaud 19 hours agoparentprevTurn on the news, or check social media. Every AI example shown is of something remarkable. That is very much the vision that AI companies are selling. reply caseysoftware 20 hours agoprevThe psychological effects of \"Imagine how your life could be..\" are manipulative and disgusting. It's playing to baser instincts and if they were run by anyone other than the State, they'd probably be illegal. Oh wait, they are. reply jmorenoamor 16 hours agoparentIt's marketing. Nearly all the advertisement industry tries to make you feel insecure about something, and then give you a warm helping hand. It's probably the most hideous legal business that exists. reply jfengel 20 hours agoprevThey're basically calling BS: \"As soon as we did, we had the developers check all the parameters for the platform and were comfortable with the settings, but after further internal discussions today, we chose to take down the site out of an abundance of caution, as we don’t want something like this purported event to happen again.\" It's weasel worded, which makes it sound like they're not 100% certain. It should be easy enough to check the logs, but I imagine there's somebody out there right now going \"Why the hell didn't I log every picture we generate?\" reply mingus88 20 hours agoparentLogging every image generated, using personally identifying photos, adds a big risk with almost zero upside. The reason not to store the images is right here in front of us. “Oops we may have generated topless photos of women who submitted their photos, but I can unequivocally state that nobody aside from that person saw the image, if it even existed” reply malfist 20 hours agorootparentAnother point to not storing it: Storing all the images your users generate could require a non-trivial amount of disk space. And considering this is government, they're probably using a server farm and bare metal, and not an infinitely scalable file storage reply flemhans 19 hours agorootparentBare metal is cheaper so would help them to be able to store this amount of files. 10,000,000 photos are trivial to store on a 4-drive zfs array reply ndriscoll 19 hours agorootparentprevIf you stored a 50kB avif (which would be way higher quality then you need for this), you could do 20 million images / TB. Bare metal can fit almost 2 PB of raw storage in 1U now. \"Infinitely scalable\" is not really a thing anyone needs to worry about. reply justinclift 20 hours agoparentprevAlternatively, they're just trying to sow doubt in order to later claim it never happened. reply mjhay 19 hours agoparentprevThere's almost certainly porn and nipples in the training set. It's pretty hard to eliminate 100% of NSFW images. That means there is always the possibility of something like this happening. reply skywhopper 19 hours agoparentprevDo you believe that “the parameters” will be followed by AI image generation? If they think the parameters are somehow bulletproof then they don’t understand what tool they are using. reply recursive4 19 hours agoprevAny word which model they used? reply crtasm 20 hours agoprev>Prior to launch, we agreed to a comprehensive set of rules to govern image creation, including that people in images be fully clothed. And here is your reminder that \"AI\" is unable to agree to your rules. reply friend_and_foe 17 hours agoprevFeigned outrage is all I see. A topless photo is not \"softcore porn.\" What an absurd take. reply throwaway5752 19 hours agoprevIt has nothing to do with this story or its credibility, but I was surprised that https://mediabiasfactcheck.com/mynorthwest/ indicating this site is owned by the LDS Church. The website and radio stations are owned by Bonneville International, a publishing company headquartered in Salt Lake City, Utah. It is a subsidiary of Deseret Management Corporation, a holding company owned by the Corporation of the President of The Church of Jesus Christ of Latter-day Saints. reply rootsudo 20 hours agoprev“Megan feared that the image could potentially get out and that it would happen to another website visitor. She told a friend, who happens to know someone at Washington’s Lottery, what happened.” Well, it’s out now. reply itishappy 19 hours agoparent\"Fearing the image could get out, she shared it directly with the local news.\" reply ta1243 20 hours agoprevOh no, bare breasts, how awful! reply yareal 20 hours agoparentBodies are bodies, nudity is no big deal. But someone should have the ability to decide how their body is depicted. Someone should be able to say, \"I'm not comfortable baring my breasts.\" And we should be ok with that. Consent matters. reply ta1243 19 hours agorootparentThe person consented to put their face on a scene of some sort. Given the majority of people on a beach are topless selecting what would life be like if I could take a holiday\" doesn't sound shocking that it shows you. reply nozzlegear 16 hours agorootparentI’d love to see your numbers for the “majority of people on a beach” being topless, because outside of places in Europe I don’t think it’s going to shake out quite as overwhelmingly as you seem to think. And since we’re talking about someone from Washington state in the first place, it’s certainly not the norm there. reply yareal 7 hours agorootparentToplessness, interestingly, is not illegal in Washington. And if the parent poster is including male toplessness, it's possible that some breaches are at times majority topless. The rest of their argument is rubbish, of course, but it's kinda funny that they right in that \"broken clock\" sort of way a little bit. reply jimbokun 20 hours agoparentprevWithout consent of the person whose face was attached to those breasts. reply roywiggins 19 hours agoparentprev\"I don't see the problem, you asked for a quick cartoon sketch of yourself, and I gave you one back with the big bare breasts that I imagine you have. Why are you mad? What prudery! I am an artist!\" reply ta1243 19 hours agorootparentI can think of a hundred things that would be more offensive reply roywiggins 18 hours agorootparentAnd your personal scale of offensiveness is definitely universally shared by everyone [0], which I am sure makes product development much simpler. [0] and anyone who disagrees is, of course, at best a prude reply yurisalazar 19 hours agoprevlol, sorry guys, forgot to put NSFW on negative prompt field reply pierat 20 hours agoprevnext [23 more] [flagged] dang 15 hours agoparentYou've been posting so many flamewar comments and breaking the site guidelines so frequently that I've re-banned your account. Not cool. If you don't want to be banned, you're welcome to email hn@ycombinator.com and give us reason to believe that you'll follow the rules in the future. They're here: https://news.ycombinator.com/newsguidelines.html. reply bergen 20 hours agoparentprev>all depending on the type of news/propaganda utilized No it's depending on the intention of the person wearing those same breasts. This is very clearly non-consentual presentation of an individual in a sexy pose. There's an argument to be had about female nudity, but this is separate from getting visually stripped by a computer without your consent. reply jchw 20 hours agoparentprevThe point here isn't anything to do with whether or not the image is softcore porn or what that even entails, it's that the image generator, being vastly an inscrutable pile of data, wound up generating something that was contextually inappropriate. This is a known failure mode of ML models in general, and it is very often extremely funny, though in this case it's particularly not, given the unsuspecting subject. There will always be things that are considered inappropriate in particular contexts. reply chrisjj 20 hours agorootparent> wound up generating something that was contextually inappropriate .. for this user. The same result may delight other users. reply jchw 20 hours agorootparentNo, \"contextually inappropriate\" has a specific meaning and that's not what it is. How do we know what is contextually inappropriate? Well generally, when you violate the expectations of what is contextually inappropriate, people get angry and it winds up in the news. But even that aside, I hope you realize how absolutely insane this line of thinking truly is. It doesn't matter if the user is \"delighted\", if it were to generate virtual child pornography I'd hope you would consider this to be inappropriate for the lottery website. reply chrisjj 18 hours agorootparent> No, \"contextually inappropriate\" has a specific meaning and that's not what it is Yes it is. Context includes the user. reply jchw 16 hours agorootparentIn this case the context we are talking about includes not just one user, but rather the general target demographic as a whole. But let's be honest, semantics debates are a waste of time. The ultimate source of truth for \"appropriateness\", a wishy-washy concept to begin with, is the public court of opinion, for which the opinion is pretty obvious based on: 1. This news story existing 2. The developers promptly pulling the app even just out of caution 3. The top responses either agreeing or not disagreeing with the inappropriateness And yes, just to preempt it, of course given enough people and time you'll find someone that will take issue with anything, but this isn't a \"gray area\" case, this is a dark shade of black area case. Was this an appropriate behavior for the application? No. It's just that simple. reply chrisjj 11 hours agorootparent> the public court of opinion, for which the opinion is pretty obvious based on: > > 1. This news story existing I rather think that existence was due to a source and a journo - not any public court. > 2. The developers promptly pulling the app even just out of caution Promptly? \"Washington’s Lottery waited until after my inquiry before turning the site offline\". reply jchw 8 hours agorootparent> I rather think that existence was due to a source and a journo - not any public court. It's pretty hard for me to conceptualize that you are still trying to argue \"actually, maybe it's appropriate for the silly lottery marketing website to output unsolicited porn without warning because one user might enjoy that\" by literally nitpicking semantics in the idea that the average person is not going to find this appropriate. A journalist and a source can indeed make for some bunk stories. It's still a point for this being a legitimate concern. Another obvious point towards that is the fact that it made it to the HN frontpage and now we're discussing it. > Promptly? \"Washington’s Lottery waited until after my inquiry before turning the site offline\". March 30th: the incident in question April 2nd: gone Two business days is quite prompt to decide to entirely shut something down over a single incident that isn't even confirmed to have happened actually, yeah. reply xkcd-sucks 20 hours agoparentprevThis inconsistency might have something to do with what the feminists call \"consent\" reply stetrain 20 hours agoparentprevI think the consent of the person whose body parts (or AI-imagined body parts) are being shown makes a bit of a difference. reply pierat 20 hours agorootparentnext [6 more] [flagged] op00to 20 hours agorootparentIf I upload my kids picture to a website that reimagines them as an elf, I give consent to that. I clearly do not give consent to creating an image of my kid dressed up in a nazi uniform saluting Hitler. Giving consent to one thing doesn’t imply that every other possible use of the technology is also consented to. reply Kerb_ 20 hours agorootparentprevLotta people do and have been taking issue with gambling and lottery advertisements since before AI got involved. I'd imagine that group is going to continue beating that drum regardless of the technology used to sell gambling and don't feel the need to ask. reply stetrain 20 hours agorootparentprevOof. reply bigbillheck 19 hours agorootparentprevI think if you thought the post was worth making in the first place it's a matter of personal integrity to keep it up even in the face of pushback, downvotes, or flagging. reply pierat 18 hours ago [flagged]rootparentAnd brigading here is also a thing, and comes at punishments of less karma and rate-limiting. Most people want the \"expected talking points\", and especially so here. If you ask the bigger questions, and people collectively lose their shit. I removed the text from other posts because people have comprehension problems, and resort to personal attacks. There's a whole slew of questions here, to of which its just \"hate ai\" is the expected response. 1. Why the double standard of women's (not men's) nipples equated to porn? [sex equality concerns] 2. Is this even porn? Even SCOTUS has the position of \"I'll know it when I see it\". Contrast this with non-sexual nudity, which too is a thing. 3. Is a privacy policy/clickthrough account creation adequate for consent? https://www.walottery.com/Privacy/ 4. Why is the Washington Lottery (government org) using AI at all in the facilitation of gambling? 5. What other AI generated content is Washington Lottery saving, as per Washington State Public Records Act, chapter 42.56 RCW ? reply bigbillheck 20 hours agoparentprev> Gotta love how women's breasts are simultaneously \"softcore porn\" and \"women's rights\", all depending on the type of news/propaganda utilized. Very strange. It's almost as if the settings in which humans operate require \"nuance\" and \"complexity\" and \"context\" to understand and can't be navigated with a flowchart. reply alt227 20 hours agorootparentVery easy to navigate with a flow chart, just needs a box in there saying 'Is this consentual?' If Yes -> Softcore Porn If No -> Rights violation reply bigbillheck 19 hours agorootparentNot all consensual photographs of ladies with their tits out are porn of any kind. reply alt227 15 hours agorootparentI guess theres 2 types of consensual. Consenting to taking the photo, and consenting it to be shared with/sold to others. reply bigbillheck 15 hours agorootparentAre you of an age to remember a certain variety of picture in National Geographic? Assuming those ladies knew they were being photographed and were ok with it being published, where does that land in your flowchart? reply cantSpellSober 20 hours agoparentprevBlech please don't turn this into a talking point about \"women's rights propaganda.\" The breast-fetishism culture in Washington is typical of the US. Whether or not you believe breast-fetishism should be a thing is a different matter. reply ct0 20 hours agoprevnext [8 more] [flagged] toddmorey 20 hours agoparentWell… the generated image has this seductive vibe to it as if she’s trying to start an onlyfans account. This isn’t a National Geographic image. Even set that aside and it’s still an example of an outcome that an AI was specifically prompted not to produce. Paired with the article about Israel’s use of AI to make wartime targeting decisions and it feels super problematic. reply chrisjj 20 hours agorootparent> it’s still an example of an outcome that an AI was specifically prompted not to produce. There is no evidence of such prompting. reply toddmorey 20 hours agorootparentThey said in the article they’d prompted it to avoid nudity. I tend to believe that because it makes sense for a promotional website connected to a state agency. reply chrisjj 18 hours agorootparentI see only: \"Prior to launch, we agreed to a comprehensive set of rules to govern image creation, including that people in images be fully clothed.\" > it makes sense for a promotional website connected to a state agency. The thing that makes most sense for them is to claim they made an effort... reply gostsamo 20 hours agoparentprevWhen the context is sexually suggestive and someone is using your face for it, it is. I agree that the US seems to be too nipple sensitive, but human communication and interpretation happens in context and the context is not good in this case. reply dhosek 20 hours agoparentprevNational Geographic was the premier source of photographs of bare breasts for earlier generations. I remember a joke about it on Happy Days (1970s show referencing the 1950s, reflecting a common thread through both decades) and certainly, the photographs were popular viewing among the adolescent males of my youth. reply donatj 20 hours agoparentprevThis is America. Don't you know the human body is an Eldritch horror and laying eyes on it destroys the mind and must be covered at all costs. reply f1refly 20 hours agoprevnext [7 more] [flagged] sigzero 20 hours agoparentExcept it wasn't art and context matters. Softcore pornography generally contains nudity or partial nudity in sexually suggestive situations, but not explicit sexual activity. She half-nude on a bed. I'd say that fits the definition. reply genman 19 hours agorootparentNot necessarily. It depends on the expression. Beds are used mainly for resting and many people sleep naked or half naked. Perhaps it is an American thing to see everything as pornography when it involves a naked body but there are many cases when it is not and is art instead (yes - that includes half naked women or men sitting on the bed). Please not that it doesn't follow that her consent is not required in this case. reply cantSpellSober 20 hours agoparentprevThe only person calling it \"porn\" is the author of this article published on a conservative site. It's double-cickbait (\"how awful!\" and \"...but I wanna see porn\"). reply f1refly 20 hours agorootparentTrue, I didn't even notice. What a weird way to present this. reply jimbokun 20 hours agoparentprevIrrelevant. The point is she didn’t consent to her image being used in this way. reply malfist 20 hours agoparentprevThere's a heck of a difference between someone voluntarily participating in a photo shoot that involves nudity, and a random person taking your face and photoshopping it onto a nude body. Consent is the difference between art and revenge porn. reply ofslidingfeet 19 hours agoprevI don't really buy that they accidentally used a model capable of generating nude breasts. This is either bored glowies trying to decide for everyone whether AI is acceptable or the woman is lying. reply nozzlegear 15 hours agoparentIs it really that hard to believe that these idiot machines screw up? Furthermore, I find your implication that “bored glowies” would decide to bring down all of AI by posting a topless photo via the Washington state lottery apparatus, of all things, to be preposterous. Surely the Deep State could come up with something more effective. reply CyberDildonics 18 hours agoparentprevglowies You think secret government agent operatives put the AI topless woman on the state lottery home page? reply cantSpellSober 20 hours agoprev [–] > the AI superimposes your image into the vacation spot > She was shocked at one of the AI photos Washington’s Lottery spit out For discussion: is it possible this was this a joke (not on the victim's part) in very poor taste? reply NathanKP 20 hours agoparentNah, it sounds like a simple latent space adjacency problem. When you give an image generation model the \"swimming\" keyword, that makes it much more likely to produce bathing suits and bikinis, and once it is already generating that much bare skin it is likely to keep going and make some bare breasts (if many images of bare breasts are trained into the model). Their claimed prompt of \"swimming with the sharks\" also introduces an element of danger and excitement, which some models could also use as a signal to introduce risqué elements. Basically, if models produced only what you asked for they would produce extraordinarily boring results. Many models come up with additional elements to add to the image based on the adjacent elements encoded in the latent space branching off of your prompt. The main issue here would be their prompt choice (\"swimming with the sharks\" was a slightly risky prompt. It could have easily gone another way and generated a gory Jaws-inspired image of her getting eaten by a shark). Secondary as an issue is the model that they used (likely a model that has too many NSFW images trained in). The big lesson for corporate entities is to pay attention to the training data that was used in the model (if you want purely SFW results you have to use a model that has nothing NSFW trained in) and also carefully consider what imagery your keywords is likely to produce as a side effect. reply pennomi 20 hours agorootparentBase model choice is a big one. Probably didn’t pay close attention to their negative keywords either. You can specifically tell models to strongly avoid making nsfw images. Plus most image generation pipelines have an optional secondary step that checks for nsfw images and blocks the output before it reaches the user. They may have neglected that step. Something nsfw could still have gotten through, but it’s unlikely given the many safeguards that most pipelines have built in. reply NathanKP 20 hours agorootparentYep very true. My guess would be that the feature was implemented by someone very new to this whole AI generated image thing. They had fun implementing it, but did not do a ton of deep research into how to implement it safely, with negative prompts and follow up checks. This is one of the biggest challenges with generative model use. They are powerful, but easy to mess up. reply Der_Einzige 19 hours agorootparentprevNegative prompts are not perfect, especially for preventing nudity. reply tomrod 20 hours agoparentprev [–] Several of the image generators will create pornographic images due to their training data. This really isn't unexpected if the devs weren't careful with the generation service chosen? reply MPSimmons 19 hours agorootparent [–] Nothing in this article screams 'careful' reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Washington's Lottery AI-powered website generated inappropriate content using a user's photo, prompting the site's removal.",
      "The user, Megan, a mother, reported the incident, leading to the confirmation of the issue by Washington's Lottery.",
      "Concerns were raised regarding the AI system's boundaries and regulations, emphasizing the importance of ensuring appropriate parameters are in place."
    ],
    "commentSummary": [
      "Washington's Lottery is under fire for using AI to create pornographic content featuring a user's image, sparking debates on ethical advertising, manipulation, and accountability.",
      "Concerns include the impacts of advertising on society, wealth inequality, and the inappropriate use of AI technology.",
      "Discussions delve into the moral implications of lotteries, AI censorship, developers' responsibility to protect against harmful tools, societal views on technology, and the need for consent and privacy in AI-generated content."
    ],
    "points": 202,
    "commentCount": 242,
    "retryCount": 0,
    "time": 1712232909
  }
]
