[
  {
    "id": 38806270,
    "title": "Switch to Firefox in 2024 for Privacy & Web Development",
    "originLink": "https://roytanck.com/2023/12/23/in-2024-please-switch-to-firefox/",
    "originBody": "In 2024, please switch to Firefox Roy Tanck · Posted on December 23, 2023 This December, if there’s one tech New Year’s resolution I’d encourage you to have, it’s switching to the only remaining ethical web browser, Firefox. According to recent posts on social media, Firefox’s market share is slipping. We should not let that happen. There are two main reasons why switching is important. “Red Panda” by Mathias Appel is marked with CC0 1.0. 1. Privacy Firefox is the only major browser not built by a company that makes money from advertising and/or selling your personal data. There’s been a lot of talk about websites tracking users using cookies, fingerprinting and other nefarious technologies that hurt your privacy. But owning the browser puts Google, Apple and Microsoft in a position where they don’t even need those tricks. We need to use browsers that are independent, and right now that means Firefox. 2. Browser engine monopoly Wikipedia lists four browser engines as being “active”. Browser engines are the bits that take a web page’s code and display it on your screen. Ideally, they conform to the official W3C standards, and display all elements as it describes. If that’s the case, web developers can easily write sites that work on all browsers. No proprietary vendor lock-in nonsense, just glorious open standards at work. It’s happened before In the early 2000’s, Internet Explorer had a massive 95% market share. This meant that many sites were only developed for use with IE. They’d use experimental features that IE supported, in favor of things from the official HTML standard. This was a very bad situation, which hindered the development of the World Wide Web. Currenty, Chrome, Safari and Edge all use variations of the closely related Webkit and Blink engines. If we want to avoid another browser engine monopoly, we need to support Firefox, and its “Gecko” engine. Firefox is actually really good If Firefox would be a bad browser, I would not recommend you to switch. It’s fast, has a nice user interface, and feels every bit as modern and elegant as its competition. I’ve been using it as my main browser for a couple of years now, on Linux, Windows, MacOS and Android. As a web developer, I usually have at least three browsers open, but when I go look something up on the web, I pick Firefox. So please, help save the web by using the best browser out there. It’s an easy thing to do, and it makes a big difference.",
    "commentLink": "https://news.ycombinator.com/item?id=38806270",
    "commentBody": "In 2024, please switch to FirefoxHacker NewspastloginIn 2024, please switch to Firefox (roytanck.com) 1563 points by Vinnl 18 hours ago| hidepastfavorite766 comments jcalvinowens 17 hours agoI switched back to firefox last year and haven&#x27;t looked back. I still use chrome on another laptop sometimes, the performance difference from my human perspective is literally zero.These days, it&#x27;s much more common for me to encounter a website that works in firefox but not chrome than the other way around. I actually switched for good when I had to use firefox to file my taxes, because the IRS free self-file site was hopelessly broken on chrome. reply krastanov 17 hours agoparentFirefox out of the box indeed does not cause broken websites. However, the demographic of this forum probably will use Firefox with \"Multi-Account Containers\", \"Temporary Containers\", \"uBlock Origin\", and a few more. These are amazing for privacy and productivity, but will occasionally cause broken websites.Source: I am a Firefox-first user who occasionally uses plugin-less Chrome because the aforementioned plugins (and \"ClearURLs\", \"Consent-o-matic\", and a few others) occasionally break websites. reply kube-system 16 hours agorootparent> Firefox out of the box indeed does not cause broken websites.Firefox doesn&#x27;t cause broken websites. Websites developers that target only Chrome cause websites to inadvertently become broken in Firefox. reply hyperthesis 15 hours agorootparenthttps:&#x2F;&#x2F;wikipedia.org&#x2F;wiki&#x2F;Embrace,_extend,_and_extinguishBe evil reply JohnTHaller 17 hours agorootparentprevFirefox doesn&#x27;t. But publishers de-prioritizing or being hostile to Firefox does. I keep Chrome up for VirusTotal to scan PortableApps.com releases. In Firefox, it&#x27;ll throw broken ReCaptcha prompts over and over and over after a certain number of scans per day (pick the thing, next, pick the thing, next x10, etc). And that&#x27;s with all extensions disabled. Possibly related: VirusTotal and ReCaptcha are owned by Google. reply goku12 16 hours agorootparent> But publishers de-prioritizing or being hostile to Firefox does.Honestly, we should take the same stand against them. Those who are hostile towards Firefox should be publicly named and shamed for sheer incompetence and&#x2F;or malice. reply babypuncher 16 hours agorootparentprevWhen these sites break I prefer to spam their support or just not use them. If they won&#x27;t support firefox then they don&#x27;t deserve my patronage. reply ferbivore 16 hours agorootparentIt&#x27;s not patronage if you don&#x27;t pay. Google would rather not have you as a user if it costs them any amount of time to support you. The relationship here is adversarial on both sides and pretending otherwise doesn&#x27;t help anyone. reply EE84M3i 7 hours agorootparentFor those that don&#x27;t know, Virustotal saves all samples and provides them to researchers. reply babypuncher 14 hours agorootparentprevif firefox users keep flooding their support line with problems then that means firefox users are costing them money. eventually it will be cheaper to build software that actually works. reply bradly 17 hours agorootparentprev> Firefox out of the box indeed does not cause broken websites.There are definitely websites that don&#x27;t work with Firefox out of the box. Just one example that annoys me is https:&#x2F;&#x2F;mtgarena-support.wizards.com. \"Firefox users: Firefox&#x27;s Enhanced Tracking Protection may interfere with Sign In. Temporarily disable it in Firefox Privacy Settings to load the sign in screen.\" reply lcnPylGDnU4H9OF 16 hours agorootparentAlternatively read as: “We are actively hostile to user-chosen browser privacy settings such that we develop our application to coach our users to turn these settings off as a necessary means of accessing their account”. I guess that’s what they call a death spiral given that the behavior discourages me from attending any would-have-been-DCI events. reply Propelloni 16 hours agorootparentprevAre you sure that your tracking protection is set to \"Standard\"? You&#x27;d have to change it manually to stricter protection.I know of several sites that break if you go beyond Standard, but none if you don&#x27;t. reply nijave 16 hours agorootparentprevYeah, I find more broken websites out of the box with Firefox than Chrome (even disabling extensions&#x2F;ad blockers)I use Chrome at work (corporate provisioned device) and Firefox at home so both get a good amount of usage reply hedora 16 hours agorootparentprevThere is a filter list for ublock origin that bypasses such things (and cookie consent popups).That’s not “working out of the box”, but leads to a much less broken experience than chrome (especially with the manifest v3 BS). reply realusername 16 hours agorootparentprevIt&#x27;s probably third party cookies but not for long, Chrome will also remove them next year so they will have to do something about it. reply 01nate 16 hours agorootparentprevI have had sites that are definitely broken in Firefox even after a stock install with every last toggle&#x2F;extension&#x2F;script blocker turned off. It&#x27;s fairly rare, but there are a slowly growing list of sites that don&#x27;t behave properly under Firefox but work in Chromium. reply croes 15 hours agorootparentThe question is if the problem is Firefox or the webpage.Remember when Google killed the Edge render engine per YouTube problems? reply rebolek 15 hours agorootparentprevI always read that some sites don&#x27;t work with Firefox but the writers never mentions which sites are broken.So I&#x27;m curious, which sites are broken in vanilla FF? reply gsej 13 hours agorootparentI use a home video appliance called camect which is accessed through a web interface which explicitly only works in chromium browsers. Oddly, I often find that the bbc news page doesn&#x27;t load images on the first attempt in Firefox, but always does in Chrome. reply linux_is_nice 15 hours agorootparentprevOne of them happens to be the educational site https:&#x2F;&#x2F;www.deltamath.com&#x2F; reply gerdesj 15 hours agorootparentOK so what is broken? The homepage comes up fine and a few clicks seem fine. Do I have to sign up to see issues? reply D13Fd 15 hours agorootparentprevI only use Firefox, and I very rarely notice any kind of broken sites. I remember one bank site last year had a minor issue. reply jerojero 17 hours agorootparentprevI have chrome installed. In the rare occasion that Firefox does not do a good job I just switch to chrome for that website and then go back.This is usually the case if there might be forms or something like that. It&#x27;s minor enough that it doesn&#x27;t bother me.Now, on the other hand, tab management is so much better with chrome and I&#x27;ve considered using Chrome for that reason alone. At work I do use chrome because I usually have 10-15 websites open at a time (for example I like to keep one tab group per ticket and every related item to it there).I do use simple tab groups on Firefox but it&#x27;s not good enough at least to replace my workflow at work. reply andrecarini 16 hours agorootparentHave you tried Tree Style Tabs [1]?[1]: https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;tree-style-ta... reply jessebeard 15 hours agorootparentHey! So I finally felt inspire and made a demo of my Firefox userchrome.css and Tree Style Tabs customization and CSS on my Github here [1]. It makes it so that the Tree style tabs expand and contract over the page, showing just the favicon and number of sub-tabs when contracted, along with a few other things, like reducing border sizes and adding better indication for sound in a tab. It&#x27;s pretty nifty I think; I hope someone finds it to their liking. :)https:&#x2F;&#x2F;github.com&#x2F;jessebeard&#x2F;firefox_settings reply lcof 15 hours agorootparentprevI wish tab groups to be implemented natively. Or that extensions can manage the tab bar instead of using a sidebar. That’s the best experience we can get so far, but it does not feel right to me. reply quantumf 16 hours agorootparentprevThis extension alone is sufficient justification to shift to Firefox. reply wkat4242 15 hours agorootparentprevMulti-account-containers doesn&#x27;t really break websites in my opinion. It&#x27;s just like running a separate browser (in fact it&#x27;s simply a firefox profile under the hood).What can go wrong is if you&#x27;ve set a certain site to always open in a certain container and another site redirects you to this site. This can happen with Identity Providers like Okta, ADFS etc. They will then open in a different container (the assigned one) and lose context. Especially microsoft services have an annoying habit of redirecting through 25 different URLs on every sign-in. But if configured correctly it&#x27;s a godsend, you can use this tool to sign into multiple MS tenants at the same time, something with chrome and not even Edge can do right now (switching teams between multiple tenants is a nightmare).But I don&#x27;t think it&#x27;s the multi accounts containers at fault here, it&#x27;s the user. Just don&#x27;t do that :P reply squidbeak 17 hours agorootparentprevI don&#x27;t mind breaking websites, if I can fix them on my own terms. reply jszymborski 15 hours agorootparentprevI have two profiles for firefox dorkily called \"hax0r mode\" and \"normie mode\", each with different theming so I can tell the diff. I try to do as much as I can in hax0r mode, which has uBlock O, NoScript, auto cookie delete and a few other privacy settings like no 3rd party cookies. Sites usually start pretty broken before I tweak NoScript for them, which I&#x27;m OK with.Occasionally, sites are obstinate and I need to use Normie mode (e.g. for Maps)Normie mode just has uBlock O, containers. I really have zero problems with sites breaking here. reply closewith 17 hours agorootparentprevNah, there&#x27;s plenty of examples of sites breaking on Firefox. For example, the recent degraded performance on Youtube linked to Firefox User Agent strings. reply culi 17 hours agorootparentYeah but that&#x27;s not on Mozilla in any way. That&#x27;s just Google&#x27;s anti-competitive practices. Firefox refuses to jump on Google&#x27;s attempts to ban adblockers with Manifest V3 so Google wants to punish them reply mvdtnz 16 hours agorootparentI feel like it doesn&#x27;t matter how many examples of Firefox not working properly you are faced with, you will simply respond \"well that&#x27;s not Mozilla&#x27;s fault, that on the website developer\" for each and every one. At the end of the day it&#x27;s the Firefox user who is faced with the problem. reply ncallaway 15 hours agorootparentI do feel that there’s a difference in kind between: “a website was built with Chrome in mind, and has problems rendering in Firefox”, and “a website was built specifically to degrade in Firefox”.If no engineering time was spent on Firefox, and it’s broken in Firefox, that’s a Firefox problem.If active engineering time was spent on _deliberately breaking_ Firefox, then yes, I don’t think that’s a Firefox problem. I think it’s a website problem at that point. reply r3d0c 15 hours agorootparentwhich means other than a small handful of power users most people will continue using chrome, and websites will continue prefering to put it first, continuing the cycleim curious to see what effect ublock not working as well on chrome as it will on firefox will have to the demographics, if there&#x27;s no shift then that&#x27;s a hurdle that has to be overcome by either firefox by some sort of engineering, google, or via legislation reply behnamoh 17 hours agorootparentprevWhat matters for the end user is the experience. If using FF will lead to a poor experience on certain websites, then why recommend it?Do people honestly believe that if they keep recommending FF, people will magically switch to FF and Google will be forced to stop its anti-competitive practices? reply culi 16 hours agorootparent> What matters for the end user is the experience. If using FF will lead to a poor experience on certain websites, then why recommend it?Because holding that against Firefox is exactly what Google is counting on. And the more you recommend it the harder it is for Google to continue its anti-competitive practices.The more you fall a fool for Google&#x27;s (or Microsoft a decade ago) practices, the worse the experience for everyone is in the future.Anyways, there&#x27;s simple extensions that will sidestep Chrome&#x27;s bs. In addition, you&#x27;ll soon to be able to get an adblocked experience that you can only get on Firefox. That means less network traffic, faster loading websites, and better user privacy reply HankB99 16 hours agorootparentSpeaking of adblock experience with Firefox (and uBlock origin), Google has managed to slip ads into my Youtube experience. So far I can skip them after watching the first 5 seconds so it&#x27;s not too bad.Before that they had a popup that would timeout after 15s (?). At that point I tried disabling uBlock on Youtube but found the ads stacked up to much longer, so the 15s delay was more acceptable.As expected, this will probably continue to be a cat and mouse game. reply Brian_K_White 16 hours agorootparentprevImmediately giving every bully whatever they want with no resistance is certainly one way to navigate life. reply munk-a 16 hours agorootparentprevYea - because Google is just outright acting evil in a number of ways. A good example is the fact that background blur isn&#x27;t supported in google meet in FF and that audio translation is similarly blocked in FF. These are just arbitrary ways that Google is degrading the FF experience because of their commanding market share. reply runarberg 16 hours agorootparentI use Google meet with Firefox on Ubuntu. Background blur works for me, as well as auto-caption. Haven’t tried auto-translation though. reply butterfi 16 hours agorootparentprevIf you want an experience that includes privacy violations, knock yourself out. Personally, as an end user, that is exactly what I don’t want, which is why I use Firefox reply dspillett 15 hours agorootparentprevWhat good does not recommending it achieve? Maintaining the status quo which as well discussed elsewhere in these threads is hardly generally desirable?My move back to FF has been slow (as mentioned already too) but I&#x27;ve been recommending it to others, who don&#x27;t have my self-inflicted blocker, for some time. Maybe some will stop listening if I keep mentioning it, but people online I&#x27;ll never meet in person are hardly a great loss in my life. The sort of people who are going to take such issue in RealLife™ are likely those just paying me attention in exchange largely for free tech support (the matter isn&#x27;t likely to come up in other contexts) and they can do one anyway too. reply danaris 16 hours agorootparentprevWell, there&#x27;s also the fact that Google is currently being prosecuted for antitrust violations on both sides of the Atlantic...Do you honestly believe that it&#x27;s OK for Google to just keep being anti-competitive? Or that this is a completely inevitable and unfixable state of affairs?We can, should, and will hold Google accountable for its monopolistic conduct, and this is absolutely part of that. reply charcircuit 16 hours agorootparentprevManifest v3 does not ban adblockers, nor does the Chrome web store reply 01nate 16 hours agorootparentIt doesn&#x27;t ban a blockers outright but does severely hamper them. It severely limits how many filters that can exist within the plugin, and also prevents plugins from updating block lists themselves and forces those updated lists to go through the plugin store.Both of those will seriously hamper a more advanced adblock like UBlock Origin reply charcircuit 15 hours agorootparent>It severely limits how many filters that can exist within the pluginThe limits are 30,000 static rules and 30,000 dynamic rules. Running tens of thousands of regexs for each request can lead to a performance impact. Allowing for even higher limits may result in people having a worse experience from the browser becoming slower. The API was designed such that these limits can be increased in the future as available computation and user needs change over time. Getting extension developers to design their extensions in a way where they have to think about not slowing down the browser too much I think is a good thing and I would not call these current limits severe.>also prevents plugins from updating block lists themselvesdeclarativeNetRequest lets rules be added and removed dynamically by the extension.>forces those updated lists to go through the plugin storeThe Chrome team has said that configuration can be updated outside of a store update. What the Chrome web store does not want are extensions that download and run code. This policy does not related to mv3. reply roblh 16 hours agorootparentprevAnd Netflix STILL refusing to play above 720p on Firefox if you’re running Linux, which I’m sure many here are. reply hellotomyrars 15 hours agorootparentTo be fair, this is not a Firefox issue.Unless you use Edge on Windows you still have the same limitation (or the windows store Netflix app). reply realusername 16 hours agorootparentprevI&#x27;ll just keep pirating personally, I don&#x27;t see why I should be treated worse if I&#x27;m paying. reply bee_rider 16 hours agorootparentprevOne solution is not to use poorly coded websites.It is unfortunate that Firefox doesn’t do more to help us avoid sites programmed by devs who are too incompetent to follow standards, really. reply jcfrei 16 hours agorootparentprevThere&#x27;s some more you should do to increase privacy: Disable Firefox sending each keystroke into the address bar to all the numerous search engines (includes google). Best to just enable the separate bar for search and disable search suggestions entirely. reply yetanother12345 14 hours agorootparentprevIn general, browsers do not cause broken websitesThis, again, can be restated as incompetent developers cause broken websites (!)(\"... do not attribute to bad faith that which can be explained by stupidity\") reply ijhuygft776 15 hours agorootparentprevI also had problems recently with an online notary service... they forced me to use Chrome...Not the same problem, but I wonder when faxes will disappear... For example, Progressive Insurance wanted me fax, mail pictures or bring them in person... email didn&#x27;t work... Of course they didn&#x27;t have a safe way to transfer them digitally but I would not care if everyone in the world saw those emails.The useless requirements that they set just proves that they don&#x27;t understand technology. reply yjftsjthsd-h 16 hours agorootparentprev> Source: I am a Firefox-first user who occasionally uses plugin-less Chrome because the aforementioned plugins (and \"ClearURLs\", \"Consent-o-matic\", and a few others) occasionally break websites.I have good luck just using a private window, since that has no extensions by default. Bonus: It&#x27;s really fast; ctrl-l ctrl-c ctrl-shift-p ctrl-c enter reply jhardy54 16 hours agorootparent> ctrl-l ctrl-c ctrl-shift-p ctrl-v enterFTFYEDIT: Mine was wrong too. Thanks to ipython for the correction. reply ipython 15 hours agorootparentWouldn’t it be Ctrl-v not ctrl-p? To paste instead of print a blank page? reply jhardy54 14 hours agorootparentYep! My mistake, nice catch. reply LtdJorge 13 hours agorootparentprevZero broken websites here, and I use those and more extensions. I also use FF on Android and iPadOS (although that&#x27;s a Safari reskin, it provides some niceties on top like send tab to device). reply n3storm 16 hours agorootparentprevShouldn&#x27;t we say \"Some sites break user browsing experience\"? reply SoftTalker 16 hours agorootparentprevI use Firefox with uBlock Origin. I don&#x27;t use containers but I do use profiles.Yes some sites are \"broken\" with uBlock Origin. I don&#x27;t find it to be many, however. I run into paywalls much more often than I do problems with my browser settings. reply acchow 16 hours agorootparentprevIt’s probably the ublock origin breaking websites and not the multi-account containers, right? reply tekmate 15 hours agorootparentprevi also use ublock matrix so i get to play \"which third party hosts are needed to make this page work\" for every new website reply spac 13 hours agorootparentprevyou might want to try about:profilescreate a new profile and use that for testing reply DavideNL 12 hours agorootparent> ”who occasionally uses plugin-less Chrome”Exactly, simply use a second “plugin-less” FireFox profile.And if you really need a Chromium browser, Brave is a better choice than Google Chrome (for privacy reasons…) reply ijhuygft776 15 hours agorootparentprevconsent-o-matic lets a lot of them through.... I need to find an alternative that works better reply dheera 15 hours agorootparentprevNot sure about the others but uBlock Origin allows you to disable it for specific websites and remember the setting. reply hutzlibu 17 hours agorootparentprev\"Firefox out of the box indeed does not cause broken websites\" .. for you.I regulary ran across something that does not work or has a broken style. And I know how to turn UO off.And why should that be a surprise? FF has way less manpower than Chrome. (And even fired lots of engineers, to raise the CEO bonus)Chrome leads the way and probably the vast majority developes for chrome and with chrome dev tools. So most of the time FF works, but not always. And performance is just worse, but not noticable on a desktop and on mobile it is offset by the working adblock. Meaning perceived performance is usually better, because ads are blocked, unlike in chrome mobile. reply beeboobaa 16 hours agorootparentThis has absolutely not been my experience. Must be a different extension you installed. reply hutzlibu 16 hours agorootparentOr I just visit different websites? reply 01nate 15 hours agorootparentNot the worst idea to avoid a website for breaking in FF (or take a chance to touch grass), but unfortunately when you can&#x27;t pay your credit card bill or need something for your work and it only works in Chromium it can&#x27;t be avoided. reply lcof 15 hours agorootparentprevThen we can compare brave and firefox in terms of speed. I don’t see any noticeable performance difference (I use both on linux and macos) reply hutzlibu 15 hours agorootparentThen ... maybe have a look at some numbers:https:&#x2F;&#x2F;www.phoronix.com&#x2F;news&#x2F;Firefox-Chrome-109-BenchmarksAnd sadly it can be worse, with my quite complex app, that I absolutely did not tailor for chrome, but gone out of my way to also support FF - the result is that chrome is just 3x faster. reply lcof 14 hours agorootparentThank you, never looked at these. The benchmarks show a big margin, but still, I use both daily and do not notice such a gap reply rytor718 16 hours agorootparentprevYeah im still a bit surprised that Github doesnt work in firefox for me. It wont load a repository page, its just blank with nothing but the navigation on the page. This happens after turning off all plugins. dont know if github has made it so only chrome works but thats a pretty major site for firefox to not work with. reply callalex 15 hours agorootparentThat’s something weird with your setup, GitHub works just fine on Firefox for everyone else. reply MrDrMcCoy 12 hours agorootparentprevMight have something stuck in your profile that&#x27;s causing that. Wiping your profile can usually fix that, even if you re-sync your preferences. reply geysersam 15 hours agorootparentprevGitHub works with Firefox. I use that combination every day. reply voidfunc 16 hours agorootparentprev> However, the demographic of this forum probably will use Firefox with \"Multi-Account Containers\", \"Temporary Containers\"Never even heard of these... hmm reply jjice 17 hours agoparentprevI&#x27;ve been using FireFox for about 4.5 years now, but I have to have Chrome installed for a few reasons unfortunately.- Some websites still will just not work in FireFox. It&#x27;s not super common anymore, but if I sense something is fishy, I pop open the console and see some strange error and swap over to Chrome. Things will just work then. All extensions disabled even. I even ran into this on Vanguard&#x27;s website, albeit for some obscure forms.- When I worked at a company that had a larger web app presence, I would have to test in Chrome. That&#x27;s a given, but my Chrome counterparts did not do the same with FireFox. I would fairly regularly (few times a quarter) find things that were completely broked on FireFox.All that said, I don&#x27;t really care about my choice in browser very much, but I&#x27;d rather support Mozilla over Google still. Especially since they&#x27;re the only non-Chromium and v8 engine out there aside from Safari, which is also owned by a massive for profit company. I&#x27;d like to help support a more open web, even if it&#x27;s just a little bit. reply vehemenz 16 hours agorootparent> Especially since they&#x27;re the only non-Chromium and v8 engine out there aside from Safari, which is also owned by a massive for profit company.You know, it&#x27;s not necessarily a bad thing that another enormous company is competing with Chrome. It might be less than ideal than Firefox having Safari&#x27;s share, but it still eats at Google&#x27;s monopoly more effectively. reply bradly 16 hours agorootparentprev> Some websites still will just not work in FireFoxI run into this as well, but I just use Safari as my backup browser and that usually is good enough. The only thing I still need to use Chrome for is my Nest thermostat. reply szundi 17 hours agoparentprevFirefox became better, Chrome became worse. Every week one time I have to start Chrome for something. reply sparrish 16 hours agorootparentI&#x27;ve been using Chrome for years and I never have to start Firefox for something. reply bee_rider 16 hours agorootparentSame, but other way around. Don’t even have Chrome installed. I don’t know what websites people are using, but I’m glad to not need them. The idea that a site could fail to render on any reasonably common browser seems pretty absurd. reply trolan 15 hours agorootparentThe few times Firefox hasn&#x27;t worked is on incredibly niche and outdated sites like a local kennel to board my dog or my university parking system.Edge works fine in those incredibly rare instances since I can&#x27;t get it off my computer anyway. I think I have one reply dylanz 17 hours agoparentprevI unfortunately switched back to Chrome last week after having used Firefox for years due to not being able to use sites I frequent. I constantly ran into issues with Heroku, GCP (go figure), and a few financial sites I&#x27;d log into regularly. reply tass 17 hours agorootparentI know it’s mentioned elsewhere, but this does it for me:* Try the same site in a private window (assuming plugins are disabled when in private)* If on dev or nightly, try those sites on the regular release.I haven’t bumped into anything in GCP that fails to work on ff, though likely don’t use the console as extensively. reply Tagbert 16 hours agorootparentprevWhat do those sites say about the problem when you report the error to them? Do any of them acknowledge their error? reply gitaarik 15 hours agorootparentprevdid you check if you have any funky plugins enabled? reply dspillett 16 hours agoparentprevI&#x27;m fully switching to FF at home now. I&#x27;d half done it but had a large collection of tabs open in Chrome which kept pulling me back as I couldn&#x27;t be bothered with reassessing them all (a fair I should have closed) and recording the ones I still wanted to keep elsewhere. Chrome gave me the final push the other day by completely forgetting most of those open items during an update.I&#x27;d not encountered anything broken in Chrome that was fixed by FF though.I&#x27;ll still be primarily Chrome in DayJob though, as most of our clients&#x27; users are (with some on Edge, a few using FF, and a couple of idiots still not off IE though we don&#x27;t officially support that) so that makes sense even though I very rarely touch anything front-end these days. reply r3d0c 15 hours agorootparentone-tab or supatabs extensions reply DrBazza 16 hours agoparentprevChrome is the new IE6 reply gitaarik 15 hours agorootparentWe need an updated version of this video: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9sKppwrLBY8 reply buffalobuffalo 17 hours agoparentprevI just switched back to firefox yesterday. Not a long sample period, but so far I completely agree, less painful than chrome. reply scruple 15 hours agoparentprevI&#x27;ve been using Firefox for regular browsing for years now, but I use Chrome for streaming videos (Firefox streaming quality is noticably worse) and LibreWolf for YouTube (blocks ads). It&#x27;s annoying to have 3 browsers open basically all of the time. reply mr_spothawk 15 hours agorootparentI&#x27;m slowly migrating a lot of my browsing out of Safari and into LibreWolf, and using the opportunity to document accounts&#x2F;passwords that i want to keep.If i were willing to get an iPhone, then i&#x27;d be quite happy with Safari (i don&#x27;t have Chrome installed on my Mac), but I want the ability to have my bookmarks available on multiple phones & computers... so firefox profiles (in LibreWolf) is the mechanism i&#x27;ve decided to use for that (for now) reply behnamoh 17 hours agoparentprev>\\ Hot take:I tried FF for a long time but finally switched to Brave. Yes, I&#x27;ll be downvoted for saying this but it&#x27;s objectively one of my top 3 favorite browsers rn.1. Brave2. Edge3. SafariI like each of them for different reasons. Brave (after disabling annoying features such as crypto and VPN) is awesome and its iOS app is the only one which can play videos in the background, has dark mode, and syncs really well with the desktop app.Edge is so tempting esp. with recent Microsoft Copilot which makes it so useful (I can summarize pages right in the browser, organize my tabs by telling so to the Copilot, etc.)Safari is not a good browser per se and lacks many features and plugins, but it&#x27;s minimal and doesn&#x27;t drain the battery like Brave and FF.I really wanted to like FF but it&#x27;s just not cutting it anymore. reply trts 17 hours agorootparentBrave is really good and always surprising me with features while largely staying lean and out of my way.They recently added a chatbot that runs locally they call Leo based on llama2. It&#x27;s pretty impressive that you can perform LLM tasks on the current page without the use of any 3rd party service. And of course you can pay them for the souped up version. https:&#x2F;&#x2F;brave.com&#x2F;leo-release&#x2F;Feels like I am alone in thinking the crypto features of Brave are cool. And not because I think that industry generally isn&#x27;t full of slime. But micropayments for content has been a good, latent idea for a generation, and here they&#x27;ve simply built it as a default feature.I still use FF and Brave equally because of my experience with the first browser wars and my mistrust of Chrome, having become the new IE. reply nzrf 7 hours agorootparentThis also where I landed with Brave as it seems to be working great for all of devices. Runs smoothly and I don’t really have a problem full with many sites on aggressive mode. I’ve had the sync’ing feature go flaky a few times, but over all good experience.My big gripe with FF honestly is the lack for PWA ( progressive web apps) If they resurrect that effort I’d give it a shot. I’m not really interested in running another browser for that feature.Also the brave privacy settings are remarkably better “by default” across all my devices. I’m sure Firefox can be configured to be hardened I just have other ways I’d like to spend my time. Heck I’d even pay for better option for all my devices. reply edgan 16 hours agorootparentprevI have tried Brave multiple times. I find its desktop version as bad or worse than Firefox for things not working or bugginess.I have used Brave more on Android. I have bounced between Firefox, Brave, and custom builds of Chromium for years. I am currently on Firefox. reply behnamoh 10 hours agorootparentBrave opens 99.9% of websites for me and I&#x27;m using \"Aggressive\" mode. reply gnicholas 15 hours agorootparentprevBrave is my primary also, but Orion is creeping up. It’s still a little buggy, but nested tree tabs is very nice. reply colordrops 15 hours agorootparentprevI use FF on my desktop and Brave on mobile. I tried FF on mobile and it has too many issues, unlike its desktop counterpart. And there are no good maintained de-googled chromium alternatives on Android other than Brave. reply jonathankoren 16 hours agorootparentprevBrave and Edge are just chromium.Bro. You’re still using chrome. reply monooso 16 hours agorootparentChromium !== Chrome. reply gitaarik 15 hours agorootparentThat is true, but the Chrome&#x2F;Chromium ecosystem is largely driven by Google. And Google makes use of this power position to push through web standards that benefit them, but not the users. Therefore I choose to use Firefox, to support a more open browser ecosystem. reply mattl 15 hours agorootparentprevHow much do they really differ? Chromium browsers for the most part are just going to appear the same to site owners. reply 01nate 15 hours agorootparentIt&#x27;ll all render the same using Chrome&#x2F;Blink, but forks might take out tracking by Google (and potentially add other tracking), add adblock outside of plugins, or re-add support for Manifest V2 to name a few. Chromium forks can actually be pretty different. reply asadotzler 9 hours agorootparentChromium is over 20 million lines of code. No Chromium fork is meaningfully different. Come back with that BS when Blink has 25% of its contributions coming from one of these Chrome skins. reply jonathankoren 15 hours agorootparentprevIt’s literally a technological monoculture controlled by one company that can’t be trusted.Ironically, focusing on the window chrome does not matter for a healthy web, let alone a healthy open standard. reply mikepurvis 16 hours agoparentprevI&#x27;m a FF-first user but I definitely have had to keep Chrome around for a few things— my investment banking doesn&#x27;t load in FF nor do some parts of Office 365. reply cycomanic 15 hours agorootparentWhich parts of office365? I don&#x27;t have any issues using it on FF. That said I&#x27;m not a heavy user (mainly outlook, word, sometimes PowerPoint). reply mikepurvis 3 hours agorootparentTeams maybe? Tbh I&#x27;m not totally sure as I&#x27;m new to office so I don&#x27;t spend a lot of time trying to make it work when it&#x27;s being hinky. reply toddmorey 15 hours agoparentprevI love & support Firefox but feel nervous about going all in on it when Mozilla appears to be pivoting away from investing in it. reply dheera 15 hours agoparentprevWhat would be really cool is a Firefox plugin that allowed you to replace the entire browser tab with a Chromium renderer on a specific website if you so wish, and remember that setting. That way there would really be no need to install Chrome for a few one-off websites.Considering both Firefox and Chromium are open-source it should be entirely possible. reply cmrdporcupine 17 hours agoparentprevThe only site I&#x27;ve found behaves terribly in Firefox is LinkedIn. Weird pauses on page load that lock the whole browser (not just the tab) for like 5-10 seconds. No idea what they&#x27;re doing to make this happen, but it&#x27;s odd.Which is, well, fine, because LinkedIn is mostly a dumpster anyways. reply pmontra 17 hours agorootparentThis prompted me to check LinkedIn after months of having it parked in a tab and it worked with no problems.When I have problems with a site it&#x27;s usually because I&#x27;m blocking most JavaScript with uMatrix and I have to find the correct combination of scripts to make the site work for me without having to give away my soul to the gods of tracking.As a software developer, it&#x27;s been years since a customer told me that the sites I develop on Firefox don&#x27;t work on Chrome or Safari. I don&#x27;t even bother to check anymore. I couldn&#x27;t check with Safari anyway and they are OK with that. The point is that if it works in Firefox it works everywhere. Of course we&#x27;re not using any Chrome-only API but we never had to use one of them as far as I can remember. reply Valord 16 hours agorootparentAlso checked LinkedIn, no issues in FF on a mid-2014 mbp. reply whalesalad 17 hours agorootparentprevBigquery studio is an absolute dog in Firefox even on S-tier desktop hardware. It works better in chrome. Go figure. reply boringuser2 17 hours agorootparentprevAny site that is on the edge of performance (often due to bad engineering, which you can blame on time constraints) will perform vastly better in Chromium. reply toyg 17 hours agorootparentSocial networks really have no business being \"on the edge of performance\", FFS. reply munk-a 16 hours agorootparentLook, Google and Facebook are just mom and pop businesses - they can&#x27;t afford to support all these fancy browsers!(I absolutely agree - especially when it&#x27;s google turning off features when you&#x27;re using FF... it feels blatantly anticompetitive). reply boringuser2 16 hours agorootparentprevOkay, well, they&#x27;re using React and most React engineers aren&#x27;t very good so it ends up being a clusterfuck that most people don&#x27;t even notice given how well Chromium is optimized. reply cmrdporcupine 17 hours agorootparentprevI don&#x27;t much care, TBH. Having worked in the Chromium codebase before, I know what an absolutely mammoth amount of engineering hours goes into that.V8 on its own is a technological miracle.But all funded by a firehose of crazy privacy invading ad revenue.So. I&#x27;ll live with the odd pause and a bit of battery drain. I gave Google 10 years of my life as an employee in exchange for $$, I&#x27;m not interested in giving them the rest of my life for free. reply boringuser2 16 hours agorootparentI get it, it&#x27;s a nice story, but here&#x27;s a nice little life-hack:What if you could take the benefits of the spending and jettison the side-effects?It&#x27;s called Brave, they have their own ad blocker in the source code written in C, so it can&#x27;t be hamstrung. reply RunSet 9 hours agorootparentIf you&#x27;re trying to go \"chrome but better\" than Brave is a move in the wrong direction, given its bundled cryptocurrency and ad platform.Perhaps you haven&#x27;t heard of Chromium. If so, that may be because Chromium doesn&#x27;t spend as much on marketing as Brave.https:&#x2F;&#x2F;www.chromium.org&#x2F;getting-involved&#x2F;download-chromium&#x2F; reply cmrdporcupine 16 hours agorootparentprevCrypto crap, no thanks. reply boringuser2 12 hours agorootparentThis isn&#x27;t an intelligent statement. I have used Brave for years and literally never see anything related to \"crypto\". replyscotty79 10 hours agoparentprevSame here. Chrome was just too buggy for me. This year was when I finally made the switch.What helped me was that I switched my phone browser to DuckDuckGo browser. This kind of opened my horizons. reply ncgl 17 hours agoparentprevThis is a funny comment - why&#x27;d you leave Firefox to have to come back to it? reply voakbasda 17 hours agorootparentIf GP is anything like me, they used Firefox before Chrome was released. The Mozilla&#x2F;Netscape suite that spawned Firefox is older than Google itself.For a time, Firefox performed worse than a rabid dog. Chrome ate their lunch and gained market share fast. I and many of my colleagues switched around that time. reply goku12 16 hours agorootparent> Chrome ate their lunch and gained market share fast.That isn&#x27;t the main reason they gained market share fast. It&#x27;s sabotage: https:&#x2F;&#x2F;threadreaderapp.com&#x2F;thread&#x2F;1116871245021220875.html reply jcalvinowens 17 hours agorootparentprevThis exactly, I should&#x27;ve added that context reply shermantanktop 16 hours agorootparentprevThat’s what I did. reply mrweasel 16 hours agorootparentprevIn my case: Because Opera stopped using Presto and switched to Chromium. That does says a bit about when I switched.I used Firefox pretty extensively, then switch to Chrome when Firefox fell behind on speed, but the developer tools absolutely sucks in Chrome, so I tried Opera which had a great feature set, speed and wonderful developer tools. It was a pretty sad day when Opera dropped Presto, and more so when they where bought by some Chinese company. reply nsagent 17 hours agoprevAs someone in the Apple ecosystem, Firefox is not a very compelling alternative. I&#x27;ve certainly tried: Firefox Focus was my main iOS browser for a few years (I know it&#x27;s Webkit-based) and I&#x27;ve tried switching on desktop as well.On desktop, various Firefox updates would change my settings subtly and nag me about features I had no interest in. On top of that, it was simply a slower browser, both with DOM manipulation and Javascript.For example, when I made my latest research project (https:&#x2F;&#x2F;pl.aiwright.dev) Firefox would routinely struggle with the large complex graphs (over 100k nodes, though only a few hundred nodes attached to the DOM at any given time). Safari handles it with no issues whatsoever. I&#x27;m sure there are likely workarounds to speed things up on Firefox, but I don&#x27;t have the time or energy for something like that since it&#x27;s a research project, not an end-user facing product.Maybe you can make the argument that switching to Firefox will incentivize improvements, but so far that hasn&#x27;t been my experience. I&#x27;ve now moved away from Firefox to things like Orion (which is still quite buggy, but useful enough). Safari seems to be getting better over time, with things like containers now and extensions like Userscripts [2].[1]: https:&#x2F;&#x2F;pl.aiwright.dev[2]: https:&#x2F;&#x2F;github.com&#x2F;quoid&#x2F;userscripts reply wharvle 17 hours agoparent> On desktop, various Firefox updates would change my settings subtly and nag me about features I had no interest inIf Firefox suited my needs 100% as well as Safari, this is why I’d still not use it. God damn is it noisy. You’re a browser, I have no interest in engaging with you more than necessary, whatever-I-was-trying-to-do is what I care about. Please stop acting like a kid who’s not getting enough attention. It’s off-putting. reply pivo 16 hours agorootparent> God damn is it noisyWhat is this noise you&#x27;re referring to? I&#x27;m 90% Firefox and 10% Safari and I don&#x27;t understand this comment. reply mvdtnz 15 hours agorootparentI know what he&#x27;s talking about. Firefox has constant nag screens. I just restarted Firefox and got two separate nags[1]. I am certain that within the next week it will nag me for at least one more thing. Maybe it&#x27;ll want me to try Pocket, or sign in with a Mozilla account, or set up sync, or the absolute worst offender - when I launch the browser it will say \"heyyyy I gotta update and I treat this like it&#x27;s 1998 so you&#x27;ll need to wait, and then I&#x27;ll need to restart\". Firefox is VERY naggy.[1] https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;dLr6E0C reply pivo 15 hours agorootparentOk, but obviously in this case you can check the \"Don&#x27;t show me this message again\" checkbox and, like me, you&#x27;ll have had to endure the burden of responding to this nag just once in years of using Firefox.I may be forgetting about nags that don&#x27;t allow themselves to be disabled, but if so, they&#x27;re infrequent enough that they seem insignificant and obviously non-memorable.Edit: I should add, when I had to have Chrome installed I really didn&#x27;t like the fact that it updated itself automatically via a daemon process. I don&#x27;t think having a constantly running background process for a browser, especially one from a giant advertising company, is a good alternative. reply mvdtnz 15 hours agorootparent> in this case you can check the \"Don&#x27;t show me this message again\" checkboxI had literally just installed Firefox minutes ago (inspired by this thread). During the install it asked me if I wanted to make it the default browser, and I had said no. This is what we mean by nagging. Firefox needs to know when to shut up and get out of the way.> you&#x27;ll have had to endure the burden of responding to this nag just once in years of using Firefox.Twice. For this one particular nag. Next time Mozilla releases some shitty VPN or AI program, it will nag me again. If I don&#x27;t log in to a Mozilla account, it will nag me to. If I don&#x27;t try Pocket, I will be nagged to try it. reply cycomanic 13 hours agorootparentThat&#x27;s literally a dialog that every browser gives you after installing&#x2F;on start if not permantly disabled (and there is a convenient button, there), and you are calling out FF? Also regarding pocket, as an almost exclusive FF user I didn&#x27;t know what pocket was until some years ago when it was in the news, similarly Mozilla VPN I found out about through the news, not a single nag. This compared to lots of \"the web works best in Chrome...\" nags I got when visiting google websites over the years.Let&#x27;s not even talk about Apple and their dark patterns (the whole green bubble messages as one example). reply mvdtnz 10 hours agorootparentThis is after installing and after I had already said \"no\" during the install. reply Capricorn2481 15 hours agorootparentprevAnd Chrome does this too. They want you to know about new features, and they&#x27;re easy to dismiss.The only reason Safari doesn&#x27;t is because they aren&#x27;t developing it because it&#x27;s too broken already. So much doesn&#x27;t work with Safari. reply pivo 13 hours agorootparentprevWell, Firefox hasn&#x27;t nagged me in a long time and you&#x27;ve just installed it, so why not give it a try for a while?> shitty VPNThe Mozilla VPN is a rebranded Mullvad VPN, AFIK which I understand is very good.Even if there are a few more nags than Chrome or whatever, I guess, I&#x27;m happy knowing that I&#x27;m making it a bit harder for Google to know what I&#x27;m doing online. reply Springtime 7 hours agorootparentprevFirefox for many versions now has no way to disable update notification messages, which will repeatedly appear as a popup from the addressbar on every launch, unless one creates a new file with particular content.Even if one wants a particular version for testing. Chromium doesn&#x27;t do this ime. And this isn&#x27;t to say it&#x27;s a reason not to use FF but it is a counter-example to simply being able to dismiss something without it being annoying. reply causal 15 hours agorootparentprevThis is asking you to make it the default browser, like every other browser does. How is that noisier? reply MrDrMcCoy 12 hours agorootparentprevI have found it quite easy to turn those features and their accompanying nagging off. I do it precisely once when I set up on a new OS install and never see them again after updates. Never understood these stories. reply nonbirithm 15 hours agorootparentprevI don&#x27;t know if it&#x27;s the same \"noise\", but for a period of months I used an extension that replaced the New Tab page. When you do this with Firefox it pops up a notification the first new tab you open each session stating \"your new tab has changed, keep settings?\" I mean, I installed the extension to change the new tab page, so presumably I wouldn&#x27;t need to be asked a second time.The noisy part was this notification steals focus so you can&#x27;t type into the search bar immediately, you have to hit escape or click out to gain focus again. And the popup kept appearing until I realized you had to specifically press \"yes, keep changes\" on the notification to get it to stop (usually I canceled out of it reflexively to do actually important things). If you just hit Escape or tried to use the URL bar it would come back next session and steal focus again.It sounds like something minor but the idea of stealing focus to re-confirm a change you already confirmed is a mild source of headaches, and not necessary in my view. Not to mention, this process would repeat itself for every Firefox installation synced with, since the extension counts as a fresh install each time. In a world where switching browsers is trivial, I think minor annoyances like those are best removed. reply mrweasel 16 hours agorootparentprevI kinda do. Safari feels very much like a simplistic browser, but a damn good one, except when it&#x27;s not. Firefox tried to do a lot, tons of features baked in, Safari outsources a lot to the operating system and as a result the interface and interaction becomes simpler, but less flexible.The primary reason I don&#x27;t use it 100%, but 90&#x2F;10 like you, is due to extensions and those few sites that doesn&#x27;t work in Firefox. reply pivo 15 hours agorootparent> Safari outsources a lot to the operating system and as a result the interface and interaction becomes simpler, but less flexible.Yes, definitely agree with that. But I think of \"noise\" as stuff that interrupts your normal interaction with web sites and in that regard I don&#x27;t think Firefox is particularly noisy.There are definitely a lot of features, settings and extensions with Firefox and while I don&#x27;t use very many of them I do really appreciate the ones that I do use. reply jwells89 16 hours agorootparentprevAnd much of the “noisiness” stems from tooltips, popups, etc explaining changes and additions. I can see where some users might want those but it should be possible to disable those entirely, or at least make them more “quiet” (e.g. a notification icon with red news count pip on the new tab screen where changes can be seen at the user&#x27;s leisure). reply lolinder 15 hours agorootparentprevThere was this, a couple months ago:Firefox displayed a pop-up ad for Mozilla VPN over an unrelated page (382 comments) https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36077360I tabbed away from Firefox for a bit, and when I tabbed back I had a full screen popup ad for Mozilla VPN overlayed on top of the webpage I had been using. reply unethical_ban 16 hours agorootparentprevOnce every three months, one gets an upgrade tab showing release notes. Upon occasion (and I mean, occasion, not regularly) they may try a new feature like Sync, save for later, etc.Once I get the browser installed and running, what you describe matches 0% of my experience with it. reply temp0826 16 hours agorootparentprevIt&#x27;s been a long time since I&#x27;ve had a fresh install of ff (I do recall some notification bars in the window you have to close out), but as a very longtime user, I have no idea what you&#x27;re talking about...example? reply wharvle 15 hours agorootparentThey pop new tabs, little “hint” boxes, et c, all the time. It’s the kind of thing that’s easy to become blind to as a power user (but is annoying if you do notice it), but that murders UX for lots of folks. reply temp0826 13 hours agorootparentUnless you mean the random tabs when an extension e.g. tampermonkey gets updated and shows a change log I don&#x27;t think I&#x27;ve seen what you mentioned. Definitely nothing \"all the time\". reply Semaphor 14 hours agorootparentprevBeen using it since quantum, it happens after some updates, recently there was some modal about some feature I didn’t care about, and I had to click twice to close it. That was only a few weeks ago. reply dvngnt_ 15 hours agorootparentprevchrome has to be worse. now there&#x27;s that ad privacy option that you have to remember to change reply echelon 15 hours agorootparentprevGoogle and Apple get to engage you frequently all the time for free. You have no choice in the matter. Every surface is theirs. reply wharvle 15 hours agorootparentFirefox doesn’t need to get me to “engage” when I’m already in their browser. reply echelon 15 hours agorootparentFirefox needs to make money. reply wharvle 15 hours agorootparentThat’s why I have to change the default search engine.Most of what they’re nagging me about won’t even make them money. It’s marketing-changelog pages nobody reads. They could save some money by keeping text change logs where people who care can find them, and dropping those altogether. reply Dalewyn 15 hours agorootparentprevIndeed, their CEO wants her fat paycheck.[1][1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mitchell_Baker#Negative_salary... reply echelon 13 hours agorootparentShe needs to go. 100% of the money should go to engineering and (actually skilled and focused) product people. replyjwells89 16 hours agoparentprevFirefox is not my primary browser on macOS for similar reasons. Given how similar iOS is to macOS I wouldn’t expect the situation to be much different with a hypothetical Gecko-based version of Firefox for iOS — the performance and efficiency isn’t quite there compared to WebKit-based stuff.There’s platform-agnostic issues too, however. Firefox is my primary browser on Windows and Linux, but it has a pile of UI papercuts which I’ve kinda-fixed with userchrome.css hacks, and the result is underwhelming (as is the possibility of these hacks periodically breaking with updates). It’s enough of a frustration that forking Firefox to properly fix them would be tempting if keeping up with the firehose of security patches from mainline weren’t so daunting.My biggest wish is for Gecko to re-gain embedding support on desktop platforms so I can build my own browser around it, making keeping it secure as simple as updating dependencies. reply andrewstuart 16 hours agoparentprevI use Firefox on macOS as my primary browser it’s great no complaints at all. reply n8henrie 8 hours agorootparentSame here, using basically nothing but FF for maybe 6 years or so.I somehow was lugging around up to 2,400 tabs for much of last year and it handled it surprisingly well. Got down to 30 at one point, now back up to nearly 400 :&#x2F;Very rarely a website won&#x27;t load, I just change the user agent to Chrome and refresh and it almost always works fine. reply schimmy_changa 16 hours agoparentprevsimilarly, on mac this bug is a showstopper for me, so I&#x27;m using Brave currently: https:&#x2F;&#x2F;bugzilla.mozilla.org&#x2F;show_bug.cgi?id=1149826(the bug is that text replacement does not work in FF on mac, something that saves me so much time daily) reply verwalt 16 hours agorootparentMy main Problem is that fn+E won&#x27;t open the emoji picker.I have to go to another app, insert an emoji there and copy it over.I tried switching to Firefox like 3 times this year, but there&#x27;s always something putting me off. I really really want to though. reply sixstringtheory 6 hours agorootparentIf this is on macOS, what about ⌘ ^ SPC to get the emoji picker? reply ianleighton 4 hours agorootparentprevYes. This bug makes the UX on mac just feel broken and frustrating reminds you every time you do it. Tried to bear it for a year but I use text replacements so heavily that I had to switch off eventually. Firefox: for the love of Jobs, fix it! reply oreilles 16 hours agorootparentprevOn macOS, the most annoying thing for me is the inability to use the native login to authenticate to apple web services. I have to type in my apple ID, my password, and then the code sent to one of my devices every time. On chrome and safari, I can just use my fingerprint or laptop password and done. reply butterfi 16 hours agoparentprevI had to stop using Safari for development when I discovered the cache wasn’t clearing, even when I asked it to. That was a few years ago, and frankly, I’ve never looked back. reply SpaghettiCthulu 17 hours agoparentprevI don&#x27;t know what browser you&#x27;ve tried, but it certainly is not Firefox Quantum. That browser in my experience is on par with Chrome&#x27;s performance. reply nsagent 17 hours agorootparentLegitimately curious. Considering Firefox Quantum was released in 2017, is it even possible that my M1 is running any other version of Firefox? Doesn&#x27;t seem likely considering the copy on the Mozilla site [1] for Firefox Quantum:> Firefox Quantum was a revolution in Firefox development. In 2017, we created a new, lightning fast browser that constantly improves. Firefox Quantum is the Firefox Browser.[1]: https:&#x2F;&#x2F;www.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;browsers&#x2F;quantum&#x2F; reply xvector 16 hours agorootparentprevFirefox has shipped Quantum in mainline for years. It may be on Chrome&#x27;s level (I never tested), but regardless, it&#x27;s nowhere near as performant as Safari.Try running a very compute intensive script (that chokes your CPU&#x2F;GPU). In my experience Firefox becomes unusable, Safari is still buttery smooth.Now maybe Safari simply performs better, or maybe that&#x27;s some OS level prioritization unfairly advantaging Safari, but to me as an end user, that&#x27;s irrelevant. reply verwalt 16 hours agorootparentI seem to be the only one, but I swipe to go back one website, and Safari for some reason shows the old website but freezes for 1-2 seconds. reply bee_rider 15 hours agoparentprevUsing Safari on Apple’s stuff seems fine. What really matters is not supporting the chrome-alike push toward monopoly. reply tomxor 16 hours agoparentprev> I&#x27;ve certainly tried: Firefox Focus was my main iOS browser for a few years (I know it&#x27;s Webkit-based)TL;DR The truth is, no one has ever used Firefox or Chrome on iOS.You say you know it&#x27;s webkit based... but the entire engine for any browser on iOS is Safari, not merely webkit based. Apple doesn&#x27;t have a policy of \"your browser must be derived from webkit\" they have a policy of forcing browsers to use the literal same engine built into iOS that Safari is using - i.e you are just using Safari with a different UI, there is only one browser engine on iOS.This is an annoying detail to have to repetitively explain to people, and Apple benefits from this blurring of the lines in the army of users defending them for browser diversity. Honestly, if i had an iOS device I would use the Safari app - and as a web dev and a user I fucking hate Safari, but what&#x27;s the point in using a different UI, they are all the Safari engine, the main browsing experience is by definition the exact same. reply the_gipsy 15 hours agorootparentExactly. I use Safari and I hate it, but Firefox is just a reskin with a worse WebView. reply tgv 16 hours agoparentprevWeird, I have very little problems with Firefox on macos. I had performance issues with Chrome for big tables. Firefox was sluggish, but Chrome took 30s to render it (about 3000 rows, 300 columns). I ended up writing something that displays a small view of the table using absolute positioned divs which depend on the scroll bar. Now it works smoothly on all browsers (including some older tablet). It was not a simple workaround, though.Orion is also webkit, but buggy, isn&#x27;t it? I don&#x27;t care for it. reply kzrdude 16 hours agoparentprevFirefox proper is available for Mac but remember that on iOS the \"firefox\" is not using its own browser engine and there are not any add-ons available. reply Last5Digits 12 hours agoparentprevSomewhat off topic, but thank you for putting into code what has been floating around in my imagination since high school. I haven&#x27;t tested your project yet, but I really do hope that AI assisted roleplaying becomes a mainstay in the game development world. I got a taste when toying around with AI dungeon, and if this isn&#x27;t the the next step in meaningful interactive storytelling, then I don&#x27;t know what is. reply PakG1 16 hours agoparentprevI&#x27;ve had one too many times where Firefox crashed and I lost all my tabs. Then had to go through weird steps to recover them. Too much annoyance. Just use Safari now, happy as a clam. reply xvector 17 hours agoparentprevSafari is much smoother on my Mac. I was recently running a very compute intensive script. Firefox was dropping frames so badly that the UI became nearly unusable. In contrast, Safari was still a buttery smooth 120fps.Combine this with the fact that I still get E2EE sync and better integration with my OS, Firefox is no longer tempting to me. And I used to be a Firefox power user for years.For me, the enhanced plugin ecosystem and additional control that Firefox offers simply isn&#x27;t worth a slower browser that isn&#x27;t as well integrated. reply nilespotter 17 hours agoparentprevI use Librewolf and Orion on MacOS, Orion on iOS&#x2F;iPadOS (which is Safari I suppose, but the shell is awesome) reply capl 16 hours agorootparentI’ve tried using Orion but I found it very slow and does not work fully as it should with a single chrome plugin I tried. Back to Safari! reply thefourthchime 17 hours agoparentprevSame, it&#x27;s simply easier to use Safari on my phone and Mac. The performance is fantastic, syncing is perfect, and switching between the two devices is effortless.If I used windows or Linux I would probably give it a try again. reply core-utility 16 hours agorootparentI also love having the Apple Pay and automatic SMS OTP integration. Those got me to start using Safari again from FF, and the only thing I&#x27;m really missing is uBlock Origin (Adguard is doing okay in its place) reply TheCleric 17 hours agorootparentprevNot to mention on iOS your choices are really between Safari and Safari with a different skin. reply behnamoh 17 hours agorootparentNo, on iOS the choice is between Safari + Plugins (official Safari) and Safari w&#x2F;o Plugins (Chrome, Brave) reply ajdude 16 hours agorootparentI have had a lot of success with Orion on iOS. I can use both Firefox and chrome extensions, all on my iPhone. It seems like even YouTube works better in Orion than in Safari. reply europeanNyan 17 hours agorootparentprevBrave on iOS is a, more or less, perfect Youtube Player. I haven&#x27;t seen ads in months and it just works while also supporting the download of videos and creation of local playlists. reply behnamoh 17 hours agorootparentOh yes, I totally agree (and I hinted on this in my other comment here). Brave on iOS is a blessing. reply redcobra762 16 hours agorootparentprevThe YouTube app is also a perfect YouTube player, so I’m confused… reply latexr 16 hours agorootparentI haven’t used the official YouTube app, but I seriously doubt it’s ad-free and that it lets you download videos. reply redcobra762 16 hours agorootparentIt does both, actually! reply latexr 15 hours agorootparentAre you saying that, by default and without any account, the official YouTube iOS app does not display ads? Then why are there so many alternative frontends which have ad blocking as a feature and Reddit is littered with threads asking how to block ads in the app? reply redcobra762 14 hours agorootparentNope, the question was whether or not the YouTube app played video without ads and allowed downloads. It does both. reply europeanNyan 11 hours agorootparentUnder the assumption that you have an account and have paid for Premium, right? reply redcobra762 11 hours agorootparentYes, with both the app performs as described. Nobody has thus far come in the night to snatch me for watching unapproved content, and the $8&#x2F;mo seems like the least I could do to support the product and the creators on the platform. reply europeanNyan 11 hours agorootparentAh, so now you&#x27;re packing being a student on top of it all. So, let&#x27;s reiterate: Brave vs. App + Account + Subscription + Student subsidy. Anything else? reply redcobra762 10 hours agorootparentYes, App + Registration to + Pay for the content I consume as a + student who is literally assigned videos to watch (TED, SciShow). Seems fair to me.Or should I be stealing this content and leave everyone involved poorer for it? reply ranting-moth 17 hours agoprevIf you keep feeding the Google monster you soon won&#x27;t be able to browse the internet without a 3rd party attesting that your computer is worth browsing that site.https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2023&#x2F;07&#x2F;googles-web-integrit... reply ramblenode 15 hours agoparent\"Users often depend on websites trusting the client environment they run in. This trust may assume that the client environment is honest about certain aspects of itself, keeps user data and intellectual property secure, and is transparent about whether or not a human is using it.\"Double-plus-good rights management! reply 38 15 hours agorootparentThis reads like someone hopelessly out of touch with actual users.Most users don&#x27;t give a shit if their client is \"honest\", or if it&#x27;s respects intellectual property. These are concerns of web admins and media companies. Users just want something to load websites. reply Vinnl 15 hours agorootparentNo no, it&#x27;s not the users that care if their client is honest, it&#x27;s the websites. But users want to use those websites, and therefore whatever is in the website&#x27;s interest is in the user&#x27;s interest.There&#x27;s a lot you can justify with a creative thought process. reply surajrmal 14 hours agorootparentprevI think it depends. When I use an ATM, I want to make sure it&#x27;s the official bank ATM and won&#x27;t steal my information. Also, spam is a tax that websites must pay and we as users are indirectly paying for this tax regardless of whether we intend to or not. reply 38 12 hours agorootparent> When I use an ATM, I want to make sure it&#x27;s the official bank ATM and won&#x27;t steal my information.sure, but in that situation, the \"client\" is you, and the \"server\" is the ATM. as the client, its not your job to worry or care if you are being \"honest\" with the \"server\". your concern is only getting the money. its the banks job to secure the ATM from bad actors, not yours. reply Klonoar 17 hours agoparentprevThis was reworked to be a more limited proposal specific for Android Webviews, IIRC. Fairly recently (last month)? reply goku12 16 hours agorootparentAfter all the intense backlash they faced, they made it a &#x27;limited&#x27; webview feature rather that dropping it entirely. Now that it&#x27;s away from a standardization body, what&#x27;s to prevent it from being developed unimpeded by public opposition? What&#x27;s to stop them from expanding it to browsers once the &#x27;feature&#x27; is ready? After all, this is exactly the pattern we saw with FLoC, &#x27;privacy&#x27; sandbox and the Topics API. reply EvanAnderson 15 hours agorootparentprevIt will come back again and again, and each time there will be less public outcry. It&#x27;ll end up being normalized and eventually accepted. General purpose computers give the unwashed masses too much power. reply __MatrixMan__ 16 hours agorootparentprevYes, but it&#x27;s the sort of creepy that they can&#x27;t just undo by saying \"nevermind\". reply Drakim 17 hours agorootparentprevAnd after that&#x27;s normalized, then Google will enhance your user experience by bringing \"Android Webview security\" to Chrome on android, you know, it makes you really secure, it&#x27;s really to help you keep safe.A few years down the road, a surprising amount of companies insist you can only use their product on those secure smartphone browsers because of it&#x27;s enhanced security, so Google helps you out by adding a special \"Android Secure Mode\" to desktop Chrome. reply graphe 16 hours agorootparentUnreasonable and unsubstantiated expectations.Web sites want you to visit them, they have no reason to barrier you. Some sites I use still have http and if a site wanted you to visit it in a specific way they&#x27;d use an app. If the model is to make web sites less accessible for profit it would need a compelling reason to visit it in spite of the barriers. It will never happen. reply goku12 16 hours agorootparentNothing unreasonable or unsubstantiated. This is exactly what happened with app geolocking, privacy sandbox&#x2F;topics, SafetyNet&#x2F;Play Integrity API, etc. All of these are supposed to improve security and privacy and yet none of them are under the control of the user. Clearly implying that the user is the biggest security&#x2F;privacy threat to them. reply graphe 16 hours agorootparentWhich sites require those? How would that allow them to make more profit?I literally said if they want people to visit anywhere they use a site and if not they lock down the experience with an app, and you said they lock down apps as &#x27;proof&#x27; that they&#x27;d lock down web sites because somehow they are equal. Apps have never been about freedom. Starbucks doesn&#x27;t want user choice and privacy when they ask you to download their app.And I&#x27;m yet to see what business model it would work for. I&#x27;m going with &#x27;none&#x27;. reply goku12 16 hours agorootparent> Which sites require those? How would that allow them to make more profit?Practically every banking site (or more importantly banking apps). And a lot of weird cases like bus&#x2F;train timings app, mobile operator apps, etc. You don&#x27;t see that a lot with websites yet because the web isn&#x27;t so severely constrained as mobile apps are. But the moment they appear, it will go the other way. One good example of this is AMP - which thankfully fizzled out for other reasons.> And I&#x27;m yet to see what business model it would work for. I&#x27;m going with &#x27;none&#x27;.You can go with whatever you feel like. But the real world experience corroborates what the other commenter said. And one good reason for this is the corporate security culture. &#x27;Our app isn&#x27;t secure if it doesn&#x27;t use the PIntegrity&#x27; type of argument. They&#x27;ll all fall for it even if it&#x27;s detrimental to their users. reply graphe 16 hours agorootparentMaking a website less accessible doesn&#x27;t make any sense. You&#x27;ve given an example of apps like before and you&#x27;ve yet to substantiate any points you made, maybe bank logins have a reason to be secure but that forum you go to doesn&#x27;t, and wouldn&#x27;t do this.If they wanted to make it less accessible they could easily do that by forcing you to use newer browser versions which some boilerplate sites with frameworks do, from lack of expertise. No \"safety\" required. I&#x27;m not going off feeling, I&#x27;m going off facts. It will NEVER happen. reply Drakim 14 hours agorootparentNetflix will not deliver the highest resolution video unless you have a DRM supporting browser.Website operators don&#x27;t need to outright block you, they can just start putting certain features behind \"the wall\". replyEvanAnderson 15 hours agorootparentprevPublishers, already pushing back against ad blockers and now suing because their sites were scraped and incorporated into LLM weights, would love to have clients \"attest\" to the \"humanity\" of the user and \"integrity\" (read: no ad blockers) of the browser. It&#x27;s not hard to imagine that, if given access to the feature, they&#x27;d jump on it as soon as it ways feasible and make the user experience for non-attesting browsers progressively worse to force the change. reply graphe 15 hours agorootparentYour point is that struggling publishers will stay relevant, gain subscribers and afloat&#x2F;make more money by implementing ad blockers, worst user experience and safety checks to make their sites less accessible. I&#x27;m sure it&#x27;ll happen any day now. reply EvanAnderson 14 hours agorootparentAbsolutely, yes. They will be empowered by tools they don&#x27;t yet have to make it feasible to slowly \"boil the frog\". Remote attestation is just such a tool. reply graphe 14 hours agorootparentThe frogs already moved onto 4chan, twitter, TikTok, reddit, or YouTube for news. Even here at HN everyone uses archive. Publishers are dead. Nobody checks fox&#x2F;cnn for the latest breaking news or needs to hear some anchor&#x2F;journalist tell them what their handlers told them to say. reply Boltgolt 16 hours agorootparentprevWebsites want all the real visitors they can get, webapps are not quite as concerned with that. I remember the Microsoft Silverlight days reply dudul 14 hours agoparentprevI admire how they barely try to hide the fact that it&#x27;s just a way to bombard you even more with ads. They don&#x27;t even care to pretend at this point. reply 93po 9 hours agoparentprevI am fine not browsing websites that require this bullshit and fully embrace the small selection of niche communities that will be Internet 2.0 reply LargeTomato 4 hours agorootparentFirst they required attestation on Facebook but I did not speak up because I do not use Facebook.Then they required attestation for Amazon but I did not speak up because I do not use Amazon.And finally they required attestation for Uber Eats, but there was nobody left to speak up for me. reply thejosh 17 hours agoprevI switched to Firefox earlier this year, having used Chrome since 2008&#x2F;2009, previously having used Firefox. Mostly out of laziness.Can&#x27;t believe how good Firefox is now, and how great it is on Android (addons, config).Being able to control anything on the browser is fantastic. Want to increase a certain UI elements font size just a tad? Sure! Want to tweak every aspect of the UI? sure! Want a myriad of config options, sure go ahead! Love the concept of Nightly, has worked great for me. reply atrettel 17 hours agoparentOne UI feature that I love that Firefox lets you change is the scrollbars. I forget the precise settings, but I set them up so that they are always present and always big enough for me to see how far down a page I am (yes, like I&#x27;m using Windows 95!). That might only be something that matters to a few people, but the fact that it is customizable to that level is one of Firefox&#x27;s greatest strengths. reply plugin-baby 16 hours agorootparentThis sounds great, but for web developers I feel it’s a risky change if you want to see your sites as your users do. reply lolinder 8 hours agorootparentWeb developers should be in the habit of checking against many different configurations anyway. I&#x27;ve lost track of the number of times that I&#x27;ve stumbled on a scrollable area that wasn&#x27;t meant to be scrollable, which would have been caught if the developer had opened the site in the browser with always on scroll bars. reply atrettel 15 hours agorootparentprevYou raise a good point. Fortunately, I&#x27;m not a web developer but I will keep your point in mind in case I do any web development later.From a web development standpoint, the scrollbars do seem to be an ignored feature for a lot of web pages. Adding scrollbars actually fixes a lot of issues with some sites. For example, some sites often have a frame inside of another frame, and this reveals the scrollbars for both frames. If I were to use my mouse wheel, I might scroll down inner frame or the outer frame, and I wouldn&#x27;t know which one would be activated until I try, but with the scrollbars on, I always know which one is which. This was a surprise benefit to this change. reply prossercj 13 hours agoparentprevAt the risk of losing credibility in this community, I&#x27;m going to voice an opinion which I think belongs to the silent majority: I do not want to customize anything. I want to read the news and check my bank account with as little drama as possible. More customizable settings always means more things that can break. If the best feature of Firefox is that it has lots of things to configure, that&#x27;s a negative for me.But this is coming from somebody who really would like to embrace better privacy...sigh...Holding out hope for the DuckDuckGo browser on Windows. The current beta is decent, but extensions are yet to come, which means no ad blocking at the moment. That makes it essentially unusable with today&#x27;s internet reply soundnote 12 hours agorootparentBrave. Seriously. Turn off a few things at first boot, it won&#x27;t bother you after and is just nice and clean:https:&#x2F;&#x2F;i.imgur.com&#x2F;tuMGc3c.png reply aAaaArrRgH 47 minutes agorootparentStill based on Chromium though, so the Manifest v3 drama about Google trying to sabotage adblockers still applies. reply cwillu 17 hours agoparentprevUnless you&#x27;re on mobile, in which case you&#x27;d find it to confusing, so they removed the option, unless you&#x27;re on nightly, with all the issues that entails. reply sp332 15 hours agorootparentIt took a long time, but the situation improved a couple weeks ago. https:&#x2F;&#x2F;blog.mozilla.org&#x2F;addons&#x2F;2023&#x2F;11&#x2F;28&#x2F;open-extensions-o... reply cwillu 5 hours agorootparentGreat, it&#x27;s still too bad about all the QoL options only accessible via about:config though. reply torstenvl 17 hours agoprevI use Firefox as my main browser. I love Firefox and I want it to succeed.However... a person who uses Firefox as their primary browser must generally still keep Chrome installed. A person who uses Chrome as their primary browser has no need to keep Firefox installed.Firefox should endeavor to fix that.First, Firefox should honor age-old tradition, swallow its pride, and spoof 99% of the Chrome user agent string. That&#x27;s how upstart web browsers have fought against sites breaking things for them for decades.Second, Firefox should endeavor to be as compatible as possible with Chrome with regard to the DOM and JS, being sneaky if it has to. I think O365 webmail is the first good place to start, since it&#x27;s widely deployed. USG websites as well, like DFAS MyPay or Marine A-PES do not work well on it. Obviously, it&#x27;s difficult for Firefox devs to test on these sites, but that&#x27;s all the more reason to take bug reports seriously. reply gitaarik 15 hours agoparentYeah, and apparently Google deliberately makes their sites work worse in Firefox. So for Firefox to by default spoof their user agent to be Chrome for those sites would be great. I actually installed a user agent spoofer plugin for this, User-Agent Switcher. reply sagarpatil 5 hours agoparentprevTill the time they fix it, there&#x27;s this add-on which fixes the issue: https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;android&#x2F;addon&#x2F;google-search... reply deafpolygon 17 hours agoparentprev> First, Firefox should honor age-old tradition, swallow its pride, and spoof 99% of the Chrome user agent string.This leads to the larger issue of not knowing who your visitors actually are. If 75% of all Firefox installations spoof Chrome, then you won&#x27;t know that they are actually FireFox users. This can drive adoption rates down as they stop caring about making things work on Firefox. reply andrewaylett 15 hours agorootparentIt&#x27;s already the case that companies using \"Real User Metrics\" will miss many Firefox users -- client-based monitoring is indistinguishable from client-based tracking (intent notwithstanding) with the result that if I need to disable three separate sets of protections (ETP, Privacy Badger, uBlock Origin) before my browser will report my useage of my employer&#x27;s website.Not everyone using Firefox will be quite so determined as I am, I&#x27;m sure. But it&#x27;s still an issue that I try to raise with people who want to use browser statistics for anything.It&#x27;s also worth noting that Firefox does spoof its user-agent in some circumstances, if you visit `about:compat` then you can see a list of sites that have user agent tweaks applied to them. See also: webcompat.com reply torstenvl 16 hours agorootparentprevNo. Fragmentation is not a feature of the web, and delusions about developers caring about Firefox and its ~2% of web traffic are not helpful. The way to make sites work on Firefox is to convince them to stop serving broken shit to Firefox. This is the way the web has always worked. reply genevra 15 hours agoparentprevYeah, hard to recommend a browser that breaks on certain sites for subtle reasons to family members who aren&#x27;t as tech oriented for instance. reply jmm5 12 hours agoparentprevMS Office webmail works well for me in Firefox under Ubuntu. reply 516 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author suggests switching to Firefox as a web browser for two main reasons in 2024: privacy and preventing a browser engine monopoly.",
      "Firefox is the only major browser not associated with a company that profits from advertising and selling personal data.",
      "The author warns against a single browser engine dominating the market, as it may hinder web development. They highlight Firefox's speed, user interface, and effectiveness, making it a valuable option for users, and encourage supporting Firefox to preserve the web."
    ],
    "commentSummary": [
      "The discussion revolves around various topics regarding web browsers, with a primary focus on Firefox and Chrome.",
      "Users explore topics such as compatibility of websites with Firefox, pros and cons of different browsers, privacy concerns, plugin issues, and the dominance of Chrome.",
      "The conversation underscores users' frustrations, preferences, and concerns regarding different web browsers and their features."
    ],
    "points": 1563,
    "commentCount": 766,
    "retryCount": 0,
    "time": 1703865438
  },
  {
    "id": 38809145,
    "title": "Turn your folder into a website with Blot",
    "originLink": "https://blot.im/how",
    "originBody": "Log in Sign up How to use Blot Files and posts Sync your folder Dropbox Git Google Drive Pages Drafts Metadata Moving to Blot Set up your site Domain Analytics Comments Redirects Link format Example sites Questions Pricing About Notes Status News Contact Getting started Blot turns a folder into a website. Files in the folder be­come posts on your website. Files and posts Text and Markdown.txt .md .rtf Images.png .jpeg .gif Word Documents.docx .odt Google Docs.gdoc Bookmarks.webloc .url HTML.html Org Mode.org Sync your folder DropboxUse a folder in your Dropbox GitUse a git repository Google DriveUse a folder in your Drive Prevent a file becoming a post Files and folders whose name starts with an underscore do not become posts or pages. You can link to or embed them in posts. _Banana.jpg _Fruits› Banana.jpg Files in your folder are public The files in your site’s folder are public. This is useful if you want to embed an image, a video or an audio file in a post. You can also use this to share files with your readers. For example, given the folder below, your readers can download Archive.zip if they visit /Files/Archive.zip on your site. Your site Name Files Archive.zip index.html Posts Last updated 2 hours ago Edit this page",
    "commentLink": "https://news.ycombinator.com/item?id=38809145",
    "commentBody": "Blot turns a folder into a websiteHacker NewspastloginBlot turns a folder into a website (blot.im) 367 points by angrymouse 14 hours ago| hidepastfavorite93 comments _0vzt 12 hours agoI announced Blot on Hacker News almost 10 years ago. Thank you all for helping to get it started. It was a nice surprise to see it posted again here today.The goal of Blot is to bring the benefits of the static site generator to people who haven&#x27;t heard of static site generators reply Sephr 10 hours agoparentPlease increase your pricing transparency. I could not easily figure out the price of your hosted service without having to use a search engine.Blot[1] (open source software) turns a folder into a website, and blot.im offers a hosted Blot service for $5&#x2F;mo.1. https:&#x2F;&#x2F;github.com&#x2F;davidmerfield&#x2F;blot reply _0vzt 2 hours agorootparentFair point – I have just added:https:&#x2F;&#x2F;blot.im&#x2F;pricing reply lolinder 9 hours agorootparentprevTo be fair, it&#x27;s very clearly fronted on the Sign Up form, which has a prominent button in the top right corner. A dedicated Pricing page&#x2F;section would be nice simply because people often look for it, but it&#x27;s not like they&#x27;re trying to be sneaky or use dark patterns. reply Sephr 8 hours agorootparentYeah, this is just a usability issue for sure. Unfortunately for me, I didn&#x27;t want to click \"sign up\" until after I could locate the pricing, leading me into a catch-22 situation. reply pmontra 1 hour agorootparentI second this. Why would I want to click a sign up link without knowing the price in advance? No price no sale. Every service has a pricing page.Anyway, it seems that they fixed it because there is a Pricing link in the menu now. Well done. reply cortesoft 6 hours agorootparentprevDid you think it would sign you up without confirmation or something? It seems strange to me to not want to hit sign up to see how much it would cost. reply badsectoracula 5 hours agorootparentPersonally it never even occurred to me that the \"sign up\" button would show pricing - in fact it wasn&#x27;t even apparent that this was a paid product. Usually services have a separate \"pricing\" link somewhere at the top that explain things.I pretty much never click on \"sign up\" buttons unless i have already been convinced the service is something i want to sign up for. reply swells34 3 hours agorootparentprevI think of a \"sign up\" button as something I click after I&#x27;ve chosen to use that product. The button click signals my intent to do so. Having to click it to see pricing is something that would just not happen for me, because I&#x27;d never intend to sign up without seeing pricing. reply voidfunc 3 hours agorootparentprevI agree sign-up is not how I would look for pricing. Sign-up is something I click when I&#x27;m ready to commit to using but until then I&#x27;m looking for information and I expect something like a Pricing page.If I can&#x27;t find the pricing I&#x27;m never going to click \"sign up\" reply hiddencost 5 hours agorootparentprevHubSpot popularized collecting emails before providing pricing. A lesson from corporate sales in other industries. (Urgh) reply badsectoracula 5 hours agorootparentprevI second this, in fact from the site i didn&#x27;t even knew this was something you&#x27;d pay for or that it was open source. reply slashink 12 hours agoparentprevBeen a happy customer since 2018! Thank you so much for making it. reply anigbrowl 12 hours agoparentprevJust what I needed! reply dang 13 hours agoprevRelated. Others?Blot is a blogging platform with no interface. It turns a folder into a website - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32041158 - July 2022 (9 comments)Blot – a blogging platform with no interface - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=17314858 - June 2018 (120 comments)Blot – blogging from a Dropbox folder - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=10078031 - Aug 2015 (17 comments)Blot, a static blog powered by Dropbox - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=8183498 - Aug 2014 (36 comments) reply starkparker 13 hours agoprevExpress application that converts files to HTML with pandoc and serves the results, with a dashboard.The TODO file in the repo[1] is fascinating.1: https:&#x2F;&#x2F;github.com&#x2F;davidmerfield&#x2F;Blot&#x2F;blob&#x2F;39d9583395c190534... reply noduerme 2 hours agoparentFascinating as in should have been added to .gitignore? reply _0vzt 1 hour agorootparentIt&#x27;s intentionally public. The news page is generated from todo.txt and my git commit messages:https:&#x2F;&#x2F;blot.im&#x2F;news reply low_tech_love 6 hours agoparentprevInteresting, he has links to his inbox in the file…! reply Ringz 9 hours agoparentprev> The TODO file in the repo[1] is fascinating.I had no idea! reply eternityforest 8 minutes agoprevHow does this compare to Zrok&#x27;s integrated server? I&#x27;ve been very impressed with zrok even though I haven&#x27;t done anything with it in production yet. reply RistrettoMike 6 hours agoprevI recently migrated my own static Hugo blog onto Blot, and I just about couldn’t be happier with it.I’m not versed in web development, but Blot’s developer (David) seems to have a great goal in mind & similar enough priorities to what I wanted that it was a great fit. I finally got to set up the photography site I’d been planning, too.It’s http:&#x2F;&#x2F;ristrettoshots.com&#x2F; if anyone was curious what one take on a Blot photo site would look like. reply btucker 13 hours agoprevThis makes me think of the early&#x2F;mid-2000s & https:&#x2F;&#x2F;blosxom.sourceforge.net. Blosxom had this delightful concept of file extensions as \"flavours.\" For example, you could have a \".rss\" flavour that would present that hierarchy of your site as an RSS feed if you added \".rss\" to the URL. Brilliant! reply vidarh 10 hours agoparentThis used to be fairly common. Reddit is another site. A company I worked at.aroind the same time also had .xml, .rss, .atom. .xml would serve up the raw xml our middleware generated, which was normally \"rendered\" via xsl (what can I say to redeem myself for that?) server side. It was great for both debugging (you could browse the site in \"xml mode\") and to provide an API.I still like the url approach - being able to browse until you have the view you need, and then just copy the URL and change format in order to find the right API call can be very nice. The challenge, of course, is that you need to be very cautious about which urls you guarantee will be stable, or you&#x27;ll be locked into a site structure you might regret. reply djbusby 6 hours agorootparent> xsl (what can I say to redeem myself for that?)Why? XSL is awesome even if a little arcane now.Time makes fools of us all. reply vidarh 2 hours agorootparentTry to format dates in a generic way with XSL.It&#x27;s doable. It&#x27;s also a massive pain.The big problem was that the easy way out is that your XML ends up being changed to be \"XSL-friendly\", which means a ton of concessions that effectively encodes knowledge of the expected presentation no matter how much you want to keep it largely semantic.Small presentation changes far too often result in changes to the XML to accommodate weaknesses in XSL.I still like the idea. But not the use of XSL to achieve it. Unfortunately, we don&#x27;t have any great alternatives that aren&#x27;t horrible in all kinds of different ways. reply freetonik 14 hours agoprevI briefly contributed to Blot (its code is Public Domain [1]). David keeps working on Blot constantly, and it&#x27;s pretty cool to see the progress changelog with direct mapping to git commits [2].1. https:&#x2F;&#x2F;github.com&#x2F;davidmerfield&#x2F;Blot2. https:&#x2F;&#x2F;blot.im&#x2F;news reply _0vzt 12 hours agoparentThanks for contributing – Rakhim created the &#x27;questions&#x27; forum:https:&#x2F;&#x2F;blot.im&#x2F;questions reply dangwu 13 hours agoprevWe&#x27;ve come full circle reply Nition 12 hours agoparentSoon we may even be able to put a website into a folder. reply samstave 12 hours agorootparentCTRL+SHIFT+N New Folder&#x2F;websiteEDIt: this is a good thing. reply otachack 13 hours agoparentprevI laughed, thanks :DIf there&#x27;s anything to learn about humanity it&#x27;s that we apply this technique in many ways. reply quickthrower2 8 hours agoparentprevNow how do I serve my micro service from here? Just drop in a js, py or rb file :-)What if I drop in a tf file? reply djbusby 6 hours agorootparentWTF is \"tf\"? reply porridgeraisin 4 hours agorootparentTerraform i assume reply thyrox 13 hours agoparentprevIf I remember correctly earliest version of Apache also did this (though it used S&#x2F;FTP instead of dropbox and .html instead of .md) reply margalabargala 13 hours agorootparentCurrent versions of Apache also do this. reply AlienRobot 11 hours agoparentprevThe internet is made of tubes. And tubes are made of circles.2024 is the year of PHP. reply 8n4vidtmkvmk 7 hours agorootparentJokes on you. I&#x27;ve been using PHP since.... 2001. Shit, that&#x27;s a long time. reply anyoneamous 8 hours agorootparentprevWell, I&#x27;m off to learn about Apache Tomcat so I can be ready for 2025. reply ravenstine 11 hours agoprev1. Old solution becomes new again2. Folks clamor that we actually had things right the first time3. Hype dies down4. Blog posts complain that the solution \"just doesn&#x27;t scale\" and that the complete opposite approach (or some hybrid) is better5. GOTO 1 reply every 9 hours agoprevI do a low-tech version[1] of this using tree-1.8.0...[1] https:&#x2F;&#x2F;every.sdf.org&#x2F; reply james-bcn 14 hours agoprevA similar thing: http:&#x2F;&#x2F;tiiny.host&#x2F;I use them and I&#x27;m a big fan. reply freetonik 14 hours agoparentIt seems a bit different. Blot keeps pulling changes from Dropbox&#x2F;Google Drive&#x2F;etc, so you don&#x27;t have to upload the folder manually. reply pazimzadeh 13 hours agorootparenthttps:&#x2F;&#x2F;site44.com is kind of similar, except that all files in a given folder do not become public reply sodapopcan 11 hours agoparentprevWhoa, “Remove ${HOST}’s banner”. That brings back memories. reply muhammadusman 12 hours agoprevWow, I’m surprised I’ve never never heard of this and I’ve been working in web dev for 10+ years. I love this idea and I have some things I want to put out there without much management on my part. This will be perfect reply arepb 14 hours agoprevLongtime fan of Blot and the founder&#x27;s (David) work. reply secondbreakfast 12 hours agoprevI used Blot for about 5 years for Second Breakfast. Its ease of use got me started blogging. Very cool app&#x2F;service, highly recommend.I had it strung up with RSS to Mailchimp to auto-send new posts to a mailing list. Recently just switched to Ghost to make that more integrated, we&#x27;ll see how it goes! reply nonrandomstring 13 hours agoprevHaven&#x27;t tried blot, but I discovered that with a basic stylesheet \"tree -H\" can be super useful. reply johnchristopher 13 hours agoparentShow ? reply nonrandomstring 12 hours agorootparentJust man tree if it&#x27;s on your system. I used it for temporary jobs, like where I needed to give some students a quick website of a folder of source files. Not much to it. -H baseHREF Turn on HTML output, including HTTP references. Useful for ftp sites. baseHREF gives the base ftp location when using HTML output. That is, the local directory may be `&#x2F;local&#x2F;ftp&#x2F;pub&#x27;, but it must be referenced as `ftp:&#x2F;&#x2F;hostname.organization.domain&#x2F;pub&#x27; (baseHREF should be `ftp:&#x2F;&#x2F;hostname.organization.domain&#x27;). Hint: don&#x27;t use ANSI lines with this option, and don&#x27;t give more than one directory in the directory list. If you wish to use colors via CSS style-sheet, use the -C option in addition to this option to force color output. -T title Sets the title and H1 header string in HTML output mode.and that&#x27;s about it! reply johnchristopher 11 hours agorootparentOooooh, sorry, sorry. I was thinking more about the CSS !But even so I didn&#x27;t check out `-H`, I thought it was just the help flag and immediately thought the comment was a bit lacking, my bad. reply d_philla 13 hours agoprevhttps:&#x2F;&#x2F;spinup.dev is a similar thing I made a few years ago, with free analytics out-of-the-box for each deploy. Syncing changes is a feature I&#x27;d like to add, but time is tight for side-projects at the moment. reply vsri 12 hours agoprevI&#x27;ve been using Blot for years (for two websites), and I can&#x27;t say enough good things about it. Happy to see it featured here! reply khobragade 4 hours agoprevI sort of have a bad taste about this project. I&#x27;ll use proper support channels and won&#x27;t rant here, but what happened was that my card was being declined trying to sign up. Thought I could self-host on a linode somewhere given the code is in the public domain, but there is no documentation :&#x2F; reply _0vzt 2 hours agoparentIs it possible that you are in India and affected by recurring billing regulations? Email us and we&#x27;ll get you up and running:https:&#x2F;&#x2F;blot.im&#x2F;contact reply puttycat 10 hours agoprevCan someone explain this to us 35+ year olds? What am I missing? reply all2 9 hours agoparentBasically you provide the app a folder of files and that app transforms the files into web pages and then (maybe?) serves them. reply danesparza 10 hours agoparentprevI&#x27;m 48 and I get this (although I prefer Hugo and Github pages).What part is confusing to you? I&#x27;m happy to help. reply hkon 10 hours agoparentprevIt&#x27;s like if dropbox automatically exposed your files on the web. reply Dylan16807 9 hours agorootparentI think you mean when, not if.The public folder feature was really nice. reply Brajeshwar 4 hours agorootparentprevDrop had that features. I was warned multiple times when I started hosting my file download away from my website host to Dropbox. reply manx 13 hours agoprevLove the simplicity! Looks like this shouldn&#x27;t need a server, but instead could just be a static site generator. Am I missing something? reply aendruk 12 hours agoparentLooks to be principally a turnkey service that shelters consumers from the details of web servers and static site generators. reply lpointal 12 hours agoparentprevHow do you want to serve pages without an http[s] server ? reply myself248 11 hours agorootparentThe next innovation I expect to see on HN is a service that, on a set schedule, gathers articles about a set of topics that you follow, prints them out onto paper, and delivers them to you as a bound volume for easy perusal.This has a number of advantages for privacy (there&#x27;s no way for the publishers to know how much time you spent reading each story), offline-first availability (dead-tree is the ultimate), and sharing (you can hand someone the entire volume rather than just a link to it, and they get the whole contents, all offline).It really sounds like it could be the hot new thing, if only some forward-thinking VC would invest in it. reply berkes 9 hours agorootparentYou are joking, but I honestly would like it to get a personalized weekly collection of articles, a wiki lemma, a comic or two and some reviews delivered as PDF or on paper.The crux being that it&#x27;s personalized. It could use my Pocket, RSS reader, reddit&#x2F;hn voting habits, even my bookmarks db as inspiration. I don&#x27;t know or care, as long as it manages to deliver me a week&#x27;s worth every week. Filled with content that I&#x27;m going to like 90% of the time, i&#x27;d love it and pay for it. I&#x27;d even accept ads every few pages. reply jll29 8 hours agorootparentprevYou can get a subset of the English Wikipedia printed and bound, that gives you even more privacy, as you don&#x27;t need to share the topics you are interested in with any electronic service.Hint: Using a public (book) library also means your reading behavior is tracked by intelligence agencies, unfortunately, so you need to own the books you may want to read. reply 8n4vidtmkvmk 7 hours agorootparentprevI&#x27;m guessing they mean non-static server. I.e. just nginx without node or php or ruby or python should suffice to serve the static files. reply rcarmo 9 hours agoprevThis is pretty much how my website worked back when running Dropbox as a headless service on Linux was “easy” and bloat-free. Glad to see it as a service. reply pseudosavant 12 hours agoprevCool idea. This is a project I’ve done that feels in a similar vein. https:&#x2F;&#x2F;github.com&#x2F;pseudosavant&#x2F;player.html reply _hzw 13 hours agoprevDoes anyone remember scriptogr.am? It was a similar idea, but it&#x27;s long gone now.https:&#x2F;&#x2F;hn.algolia.com&#x2F;?q=scriptogr.am reply 4justinwilliams 5 hours agoprevAKA www.creedthoughts.gov.wwwcreedthoughts reply LanzVonL 11 hours agoprevS-so does Apache? reply khobragade 4 hours agoparentApache only listens to us nerds reply galaxyLogic 10 hours agoprev> Files and folders whose name starts with an underscore do not become posts or pages. You can link to or embed them in posts.So you cannot link to \"posts\"? Only to files whose names starts with underscore? reply algas 5 hours agoparentYou can link to posts. I imagine this feature is mainly to have images to use in blog posts that do not become standalone pages on their own. reply RistrettoMike 6 hours agoparentprevYou can specify URLs for specific posts&#x2F;pages via tags in the post&#x2F;page file & then link to those URLs directly on other pages. reply iamcreasy 13 hours agoprevHow do projects like this routes traffic? Similar to ngrok? reply superkuh 8 hours agoprevYou know what else turns a folder into a website? Installing nginx from your distro repositories. Then every folder beneath the www dir is a website. I&#x27;m not joking around. Files in folders is really the best way to make websites. reply throwaway14356 12 hours agoprevif json and xml files become database tables we can get rid of everything else reply ulrischa 13 hours agoprevWould be cool if it could be self hosted reply starkparker 13 hours agoparentIt&#x27;s not trivial and mostly undocumented, but not terrible. https:&#x2F;&#x2F;github.com&#x2F;davidmerfield&#x2F;blot reply mikae1 13 hours agoparentprevYeshttps:&#x2F;&#x2F;forum.cloudron.io&#x2F;topic&#x2F;10417&#x2F;blot-im&#x2F;3 reply marginalia_nu 13 hours agoparentprevIsn&#x27;t that literally just a web server? reply starkparker 13 hours agorootparentOnly if the web server transparently converts non-HTML sources (Markdown, Word docs, Google docs, LaTeX, image galleries) into HTML on the fly. reply myself248 11 hours agorootparentWeb browsers can render more than just HTML, can&#x27;t they? Plain text at the very least, many do PDF as well.That function really seems like it should be in the browser anyway. The server serves, the renderer renders. reply PaulDavisThe1st 10 hours agorootparentif the server side file format has something semantically similar to #include then your lovely simple binary model fails (without browsers having the same) reply marginalia_nu 8 hours agorootparentHTML has this, yet browsers seem to cope...? reply marginalia_nu 12 hours agorootparentprevEspecially older servers have quite a lot of those types of capabilities, rendering directories to fancy indexes, processing input files in various ways.Like they&#x27;d even spawn arbitary processes for you (w&#x2F; CGI). reply scoofy 13 hours agoprevAn interesting idea! reply mikae1 13 hours agoprev [–] If there only was a Docker image... replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Blot is a platform that transforms a folder into a website, turning the files within the folder into individual posts on the site.",
      "Users can sync their folders with popular cloud storage services like Dropbox, Git, and Google Drive.",
      "Users have control over which files become posts and can also restrict certain files from being published."
    ],
    "commentSummary": [
      "The discussion centers around Blot, a website-making tool that enables users to transform folders into websites.",
      "Users on Hacker News appreciate Blot but criticize its lack of pricing transparency.",
      "The conversation covers topics such as using file extensions as \"flavors\" for different website versions, comparisons with other blogging platforms, and the ease of use of Blot."
    ],
    "points": 367,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1703878002
  },
  {
    "id": 38804135,
    "title": "Gentoo Linux now offers binary packages for easier installation and wider software availability",
    "originLink": "https://www.gentoo.org/news/2023/12/29/Gentoo-binary.html",
    "originBody": "Get Gentoo! gentoo.org sites gentoo.org Wiki Bugs Packages Forums Planet Archives Devmanual Gitweb Infra status Toggle navigation Home Get started Downloads Inside Gentoo Support Get involved Donate Toggle secondary navigation Debug News Gentoo goes Binary! Dec 29, 2023 You probably all know Gentoo Linux as your favourite source-based distribution. Did you know that our package manager, Portage, already for years also has support for binary packages, and that source- and binary-based package installations can be freely mixed? To speed up working with slow hardware and for overall convenience, we’re now also offering binary packages for download and direct installation! For most architectures, this is limited to the core system and weekly updates - not so for amd64 and arm64 however. There we’ve got a stunning >20 GByte of packages on our mirrors, from LibreOffice to KDE Plasma and from Gnome to Docker. Gentoo stable, updated daily. Enjoy! And read on for more details! Questions & Answers How can I set up my existing Gentoo installation to use these packages? Quick setup instructions for the most common cases can be found in our wiki. In short, you need to create a configuration file in /etc/portage/binrepos.conf/. In addition, we have a rather neat binary package guide on our Wiki that goes into much more detail. What do I have to do with a new stage / new installation? New stages already contain the suitable /etc/portage/binrepos.conf/gentoobinhost.conf. You are good to go from the start, although you may want to replace the src-uri setting in there with an URI pointing to the corresponding directory on a local mirror. $ emerge -uDNavg @world What compile settings, use flags, … do the ‘‘normal’’ amd64 packages use? The binary packages under amd64/binpackages/17.1/x86-64 are compiled using CFLAGS=\"-march=x86-64 -mtune=generic -O2 -pipe\" and will work with any amd64 / x86-64 machine. The available useflag settings and versions correspond to the stable packages of the amd64/17.1/nomultilib (i.e., openrc), amd64/17.1/desktop/plasma/systemd, and amd64/17.1/desktop/gnome/systemd profiles. This should provide fairly large coverage. What compile settings, use flags, … do the ‘‘normal’’ arm64 packages use? The binary packages under arm64/binpackages/17.0/arm64 are compiled using CFLAGS=\"-O2 -pipe\" and will work with any arm64 / AArch64 machine. The available useflag settings and versions correspond to the stable packages of the arm64/17.0 (i.e., openrc), arm64/17.0/desktop/plasma/systemd, and arm64/17.0/desktop/gnome/systemd profiles. But hey, that’s not optimized for my CPU! Tough luck. You can still compile packages yourself just as before! What settings do the packages for other architectures and ABIs use? The binary package hosting is wired up with the stage builds. Which means, for about every stage there is a binary package hosting which covers (only) the stage contents and settings. There are no further plans to expand coverage for now. But hey, this includes the compiler (gcc or clang) and the whole build toolchain! Are the packages cryptographically signed? Yes, with the same key as the stages. Are the cryptographic signatures verified before installation? Yes, with one limitation (in the default setting). Portage knows two binary package formats, XPAK (old) and GPKG (new). Only GPKG supports cryptographic signing. Until recently, XPAK was the default setting (and it may still be the default on your installation since this is not changed during upgrade, but only at new installation). The new, official Gentoo binary packages are all in GPKG format. GPKG packages have their signature verified, and if this fails, installation is refused. To avoid breaking compatibility with old binary packages, by default XPAK packages (which do not have signatures) can still be installed however. If you want to require verified signatures (which is something we strongly recommend), set FEATURES=\"binpkg-request-signature\" in make.conf. Then, obviously, you can also only use GPKG packages. I get an error that signatures cannot be verified. Try running the Gentoo Trust Tool getuto as root. $ getuto This should set up the required key ring with the Gentoo Release Engineering keys for Portage. If you have FEATURES=\"binpkg-request-signature\" enabled in make.conf, then getuto is called automatically before every binary package download operation, to make sure that key updates and revocations are imported. I’ve made binary packages myself and portage refuses to use them now! Well, you found the side effect of FEATURES=\"binpkg-request-signature\". For your self-made packages you will need to set up a signing key and have that key trusted by the anchor in /etc/portage/gnupg. The binary package guide on our Wiki will be helpful here. My download is slow. Then pretty please use a local mirror instead of downloading from University of Oregon. You can just edit the URI in your /etc/portage/binrepos.conf. And yes, that’s safe, because of the cryptographic signature. My Portage still wants to compile from source. If you use useflag combinations deviating from the profile default, then you can’t and won’t use the packages. Portage will happily mix and match though and combine binary packages with locally compiled ones. Gentoo still remains a source-based distribution, and we are not aiming for a full binary-only installation without any compilation at all. Can I use the packages on a merged-usr system? Yes. (If anything breaks, then this is a bug and should be reported.) Can I use the packages with other (older or newer) profile versions? No. That’s why the src-uri path contains, e.g., “17.1”. When there’s a new profile version, we’ll also provide new, separate package directories. Any plans to offer binary packages of ~amd64 ? Not yet. This would mean a ton of rebuilds… If we offer it one day, it’ll be at a separate URI for technical reasons. The advice for now is to stick to stable as much as possible, and locally add in package.accept_keywords whatever packages from testing you want to use. This means you can still use a large amount of binary packages, and just compile the rest yourself. I have found a problem, with portage or a specific package! Then please ask for advice (on IRC, the forums, or a mailing list) and/or file a bug! Binary package support has been tested for some time, but with many more people using it edge cases will certainly occur, and quality bug reports are always appreciated! Any pretty pictures? Of course! Here’s the amount of binary package data in GByte for each architecture… Questions or comments? Please feel free to contact us. Home Debug News Get Started About Gentoo Philosophy Screenshots FAQ Downloads Mirrors Signatures Inside Gentoo Developers Projects Artwork GLEPs Gentoo Foundation Sponsors Stores Contact Support Consulting Documentation Package database Repository news items Security USE flags rsync mirrors Get Involved IRC channels Forums Mailing lists Contribute Become a developer Get the code Privacy Policy © 2001-2023 Gentoo Authors Gentoo is a trademark of the Gentoo Foundation, Inc. The contents of this document, unless otherwise expressly stated, are licensed under the CC-BY-SA-4.0 license. The Gentoo Name and Logo Usage Guidelines apply. Version 9303336",
    "commentLink": "https://news.ycombinator.com/item?id=38804135",
    "commentBody": "Gentoo goes BinaryHacker NewspastloginGentoo goes Binary (gentoo.org) 319 points by akhuettel 21 hours ago| hidepastfavorite257 comments connorgutman 17 hours agoDisclaimer: I am a die-hard Gentoo fan.The appeal of Gentoo is not compiling everything from source, it&#x27;s having the freedom to install anything you want on nearly any hardware all with stellar documentation and minimal roadblocks. Want to run Enlightenment with OpenRC and NetworkManager on a laptop from 2008? Install Gentoo! Want ZFS as root on a smart refrigerator? Install Gentoo! Want a vanilla Gnome + SystemD install on a brand new laptop? Install Gentoo! The decision to ship binary packages only gives users *more* choices while other distributions have been actively removing one&#x27;s freedom to choose. Debian, the so-called \"universal operating system\" just dropped 32-bit x86 support. You can install an alternative init system on Debian if you&#x27;d like, but it&#x27;s a bit of a PITA. Meanwhile Gentoo allows you to pick between 17+ different stage 3 tarballs and 35 eselect profiles. Personally, I enjoy compiling everything from source and the flexibility that comes with it. On modern hardware it&#x27;s painless. If you disagree, great! Go install a shiny new binary. The selling point of Gentoo has never been portage. It&#x27;s always been the flexibility and the community. reply cbmuser 15 minutes agoparent> Debian, the so-called \"universal operating system\" just dropped 32-bit x86 support.I would argue that Debian has support for at least as many architectures as Gentoo:> https:&#x2F;&#x2F;buildd.debian.org&#x2F;status&#x2F;package.php?p=base-files&su...I know that because I am maintaining most of the obscure architectures in Debian with regular installation snapshots:> https:&#x2F;&#x2F;cdimage.debian.org&#x2F;cdimage&#x2F;ports&#x2F;snapshots&#x2F;Debian even has a Hurd port for both i386 and amd64. reply Sesse__ 14 hours agoparentprev> Debian, the so-called \"universal operating system\" just dropped 32-bit x86 support.This is imprecise at best. There was a meeting where the release team concluded that most likely, there will be no _installer_ and _kernel_ support for 32-bit x86 at some unspecified point in the future. (At the current point in time, both are still delivered and fully supported.) In particular, you can still run multi-arch 32-bit&#x2F;64-bit, allowing you to run 32-bit x86 software. reply connorgutman 12 hours agorootparentNot really the point I was trying to make. Gentoo still fully supports obscure architectures such as IA64 and PowerPC 32-bit while Debian dropped both back in 2018 and 2020. Hell, not only does Gentoo still support them but the handbooks are still updated regularly. When that unspecified point in the future comes knocking for 32-bit x86 Gentoo will continue to be a rock for fun devices like my Lenovo x60 with libreboot. :-) reply cbmuser 11 minutes agorootparent> Gentoo still fully supports obscure architectures such as IA64 and PowerPC 32-bit while Debian dropped both back in 2018 and 2020.We do that in Debian, too. FWIW, I am the one that eventually greenlit the removal of the ia64 port in the kernel because I was actually the one who took care of most of the issues in the ia64 port.I also regularly debug and report (and sometimes fix) regressions in various upstream projects regarding targets such as 32-bit PowerPC. It&#x27;s not enough to solely focus on downstream work, upstream work is as important if not even more important. reply pabs3 10 hours agorootparentprevThe Linux kernel just deleted IA64, so that support won&#x27;t last for long, unless Gentoo keeps ancient kernel versions? Debian still has unofficial ia64 and powerpc ports too. reply airhangerf15 3 hours agorootparentIt doesn&#x27;t, but the kernel also isn&#x27;t package managed, just the source. You can keep around the ebuilds for older kernels in an overlay, or someone might maintain an enthusiast overlay just for IA64 kernel support. reply cbmuser 1 hour agorootparentWhat do you mean \"it doesn&#x27;t\"? I&#x27;m maintaining Debian Ports and I build installer images for all old and obscure targets. We even have support for SuperH which Gentoo dropped at some point.> https:&#x2F;&#x2F;buildd.debian.org&#x2F;status&#x2F;package.php?p=base-files&su...> https:&#x2F;&#x2F;cdimage.debian.org&#x2F;cdimage&#x2F;ports&#x2F;snapshots&#x2F;Installer images for LoongArch are in the works as well. reply gary_0 12 hours agorootparentprevDoes Intel or anyone still sell 32-bit x86 chips for non-embedded use? A quick Google was inconclusive, although it seems that while Intel sells low-power x86 chips under the \"Pentium Silver\" brand, they all seem to be 64-bit now. So eventually more and more old x86 boxes will be decommissioned, and 32-bit x86 will be dead as a desktop platform, no?Which might mean people who want to run old proprietary 32-bit games&#x2F;software are going to be annoyed, but that&#x27;s not necessarily the distro&#x2F;OS&#x27;s problem; it could be argued that the user should install a 32-bit translation&#x2F;emulation layer to run that stuff, ala DOSBox. reply connorgutman 11 hours agorootparentAbsolutely not... but hear me out. NetBSD plans on supporting 32-bit x86 until \"long after 2038 (http:&#x2F;&#x2F;www.netbsd.org&#x2F;about&#x2F;).\" Legacy hardware matters for many reasons. Outside of the fact that I still love my Lenovo x60s and use it as a distraction-free writing machine, much of the world still depends upon 32-bit architecture. We can&#x27;t be replacing computers for all 8 billion people on Earth every 10-20 years. Hell, think about all the government bureaucrats still running MS DOS! :-) reply shotnothing 1 hour agorootparent> Hell, think about all the government bureaucrats still running MS DOS! :-)That&#x27;s not a good thing though, perhaps aggressively depreciating 32 bit will light a fire under their asses reply Sesse__ 12 hours agorootparentprevI believe the last x86-compatible chips without 64-bit support were launched around 2006 or so (Intel Core; Core 2 had 64-bit support), a couple of years after Opteron initially came out.It&#x27;s not really clear how long the Linux kernel itself will continue supporting such CPUs. The architecture certainly isn&#x27;t something anyone really cares about (e.g. Meltdown wasn&#x27;t patched for several months after it went public and 64-bit x86 got fixed). reply tedunangst 10 hours agorootparentThere were Atom CPUs that weren&#x27;t supposed to be 64-bit for a while after that. reply pzmarzly 2 hours agorootparentYup, I bought a brand new 32-bit Intel Atom tablet from Toshiba in 2014, gave it away but the person I gave it to is still using it. It has 32-bit UEFI, which is also loads of fun. I once installed Debian on it, but I couldn&#x27;t get touchscreen to work, so went back to Windows 8. reply Dalewyn 10 hours agorootparentprevOne of Linux&#x27;s oldest arguments for using it is that it can be used to revitalize old hardware that other, newer software might not properly utilize.If one were to rephrase it in today&#x27;s language, Linux serve(d) to reduce e-waste.So, yeah, it kind of is the distro&#x27;s problem if they are moving away and against that mantra now. reply bdavbdav 10 hours agorootparentThere’s a crossover point though where the effort to support it is larger than the supply of newer hardware coming through. With the last x86 deaktop chips coming out in ~2006, I would bet there are very few viable use cases left. That’s 6 3-year cycles past. reply withinboredom 9 hours agorootparentI have seen many Windows 98 machines in the last ten years. I saw one just last week connected to a giant robot in a factory. So, if you are looking for use-cases for 32-bit, there&#x27;s the fact that some giant machinery relies on computers that will never be updated.Last I heard, even the last BlockBuster in the US is still using old PC&#x27;s to run their DOS software. They harvest parts from old computers sold on e-bay. So, if you have any old computers laying around, they might be worth something these days. reply gary_0 7 hours agorootparentYou wouldn&#x27;t really want to mess with the OS in cases like that, though. If you did, a lot of the OS-included drivers would probably not work due to removal or bit-rot, or 3rd-party drivers might not work with a much newer OS. The only reason you might want to upgrade is if the machine is Internet-connected, in which case you&#x27;re probably just going to need a whole new box (if you need the old software, you&#x27;d probably resort to a VM), otherwise the new OS will run too slow (especially with things like new crypto&#x2F;security). Or you can keep the old box but put it behind nginx or whatever.Point of sale or industrial control computers are kind of just an offshoot of \"embedded\"; not something you need (or want) to run a browser or a modern GUI[0] on. You stick it in a cabinet or a kiosk and hopefully you don&#x27;t have to think about it again.[0] sadly also a browser reply Dalewyn 9 hours agorootparentprevI have a number of old computers lying around the house in closets, some 32-bit only. Linux not running on them anymore is actually pretty annoying since, as I said, that used to be one of its oldest mantras.Outside of the enterprise crowd, Linux&#x27;s next biggest audience are the tinkerers and hobbyists with too much free time and ancient hardware well past their Use By date. What a time it is that we can legitimately say it&#x27;s better to run some version of Windows instead of Linux on them now. reply Sesse__ 1 hour agorootparentThe hobbyist Linux market generally tinkers with things like Raspberry Pis now, not ancient 32-bit x86 machines. (This is a natural consequence of the fact that sources for cheap hardware have changed a lot over the last twenty years.) replygtirloni 17 hours agoparentprevI feel like the Gentoo userbase has been stolen in part by Arch these days. reply saghm 16 hours agorootparentThe one place Arch still is fairly opinionated compared to Gentoo is being all-in on systemd. That&#x27;s not to say you _can&#x27;t_ remove&#x2F;replace it, and obviously there are Arch-like distros that give other options, but my sense is that GP&#x27;s comment about Gentoo&#x27;s draw being completely agnostic to whatever configuration you want makes Arch having even a single strong opinion feel pretty different to users who care about that. I think you&#x27;re probably right that most people probably don&#x27;t need that, but then again, even as an Arch user, I think most people probably don&#x27;t really need the amount of customization that Arch lets you have; I just happen to like it. reply paldepind2 14 hours agorootparentI wouldn&#x27;t say systemd is the \"one place\" where Arch has an opinion. Quite the opposite, Arch is just as opinionated as the average distro. On Void Linux you can choose between musl and glibc, on Arch you&#x27;re forced to use glibc. On Gentoo you can choose your init system, on Arch you&#x27;re forced to use systems. On Debian you can have `&#x2F;bin&#x2F;sh` be any POSIX shell, on Arch you&#x27;re forced to have it be Bash. On nixOS you can choose between rolling and normal release, on Arch there&#x27;s only a rolling release. I can&#x27;t really think of anything you can customize on Arch that you can&#x27;t configure on most distros, but perhaps I&#x27;m missing something. reply starttoaster 11 hours agorootparentMy experience using Arch has very much been \"it&#x27;s just like all the other distros, except now you have at least 4 package managers to go through if you want to install any particular thing and you will almost certainly need to install utilities that convert packages from other package managers over to PKGBUILD.\" Other than that, it&#x27;s felt very similar to just using something like Debian. Which is fine, and Debian is fine, but the praise other people give it as \"the modder&#x27;s distro,\" or something like it, seems a little over zealous. reply gryn 6 hours agorootparentYour experience sounds weird. Reading it remind me of people who come from programming paradigm X try paradigm Y and only use methods from X only to conclude that Y sucks.Why would you use 4 package managers to do 1 thing ? Most people who use arch use 1 and it does the job. All of them are built around pacman + smth for AUR packages. Personally I use pacui. Why would you convert package from other distro, if another distro has it then it most certainly exists in the AUR, never once had to touch another distro&#x27;s packaging.I don&#x27;t know about the \"the modder&#x27;s distro\" part but I personally had much less headaches with it than I had with Ubuntu,fedora,debian, and CentOS.The only thing that sucks for me (regardless of distro) is Nvidia updates. reply shotnothing 1 hour agorootparenti think you made a point for why being opinionated is good, which is not really the debate here replyconnorgutman 16 hours agorootparentprevDefinitely a lot of crossover. However, there are several things that make Arch unusable for me. The #1 thing is that Arch only officially supports AMD64. Additionally, while I find Arch’s documentation better than most, it’s hard to understate how amazing the Gentoo wiki is. Lastly, I prefer portage overlays (like Guru) over the AUR. Bigger !== better. Again, the beauty of Linux is choice. Nothing but love for my Arch neighbors! reply sramsay 15 hours agorootparentprevAlso a die-hard Gentoo fan (daily driver for 20+ years).I tried Arch and what astounded me the most is that the docs didn&#x27;t seem as good as Gentoo&#x27;s. That surprised me because I end up looking at Arch&#x27;s docs all the time and it&#x27;s all of a very high quality.But the Gentoo Handbook is really a masterpiece. I&#x27;ve never read a clearer explanation of how to go from unformatted disks to a working system. reply BrandoElFollito 17 hours agorootparentprevI used to use Gentoo 20+ years ago; installing it was eye-opening. It helped me enormously to understand how Linux works.I then moved to debian&#x2F;ubuntu and recently switched my main server to Arch. And you are right - I got back this feeling of having all my 10 fingers in the system and living on the bleeding edge.Now: this was probably not the brightest choice, as this server rund my docker containers and basically nothing more so it should have been Debian(a fire-and-forget OS) reply moepstar 16 hours agorootparent> I used to use Gentoo 20+ years ago; installing it was eye-opening. It helped me enormously to understand how Linux works.Same!Starting with a Stage 1 install was what got me (with some guidance of a friend and the perfect Gentoo Wiki) into Linux circa 2004 (IIRC).Now, some 20 years later, i much prefer to have not to deal with something as... dare i say.. fragile... anymore.Yes, much of the breakage i dealt with was probably self-inflicted but always a good learning experience - but most of the time not at the most convenient time :( reply voidfunc 16 hours agorootparentThe Gentoo to Arch pipeline is pretty real.I did Stage 1&#x27;s as well around the same time but eventually got tired of doing that every so often or fighting Portage and switched to Arch.Then I got tired of dealing with Arch... and ended up on Fedora.I always liked Gentoo and Arch, but I don&#x27;t have the energy to put up maintaining them anymore. reply oniony 11 hours agorootparentArch is literally the easiest thing to maintain. I&#x27;ve been running it for over fifteen years, update maybe once a year at most and 99% of the time it&#x27;s a case of running pacdiff and updating a handful of config files. reply nixgeek 3 hours agorootparentIn 1998, Debian was a great option for SPARC and Alpha, and that’s really where I cut my teeth with Linux.Around 2004, I was building Gentoo up from stage1 just like many other commenters.For 2024 and personal stuff I’m back on Debian and it’s safe enough I pull security updates daily - entirely automated and no review on my part. It essentially never causes a problem. Updating once a year sounds terrifying to me given the rate of vulnerability discoveries of late.As ever your mileage may vary and your systems aren’t my systems, but Debian truly is a wonderful OSS project and as of August 2023 it celebrated its 30th anniversary!Gentoo going binary feels weird :) I’ve still got a soft spot for Gentoo and what it enabled me to do 15 years ago when I had more time for personal hacks. reply lovecg 14 hours agorootparentprevI did the hardcore path of Linux From Scratch and then discovered Gentoo which was a (very welcome) step down for customizability :) Highly recommend LFS for anyone who has a couple of hours per day to waste (i.e. young with no responsibilities) reply MrDrMcCoy 12 hours agorootparentprev> Now: this was probably not the brightest choice, as this server rund my docker containers and basically nothing more so it should have been Debian(a fire-and-forget OS)I dunno, there&#x27;s a fair amount to be said for having an up-to-date kernel, systemd, and Docker. :) reply BrandoElFollito 47 minutes agorootparentIt depends on the kind of \"up-to-datism\", at least for me. Security - yes. Features - for docker yes, the rest not really.The main problem is that if I provide a specific repo for docker (to keep a fresh version) it can request some dependencies and bam! I have a Debian-Turend-Arch system to maintain :) reply colordrops 15 hours agorootparentprevAnd NixOS. The fact that Nix has had a binary cache was one of the reasons I chose it over Gentoo. reply Tommstein 10 hours agorootparentprevExactly my case. Used to use Gentoo, got tired of every other update breaking the system, was told on IRC that if I didn&#x27;t like \"free\" updates (because apparently my time and serenity have no value) perpetually breaking the system I could go use something else, and that&#x27;s exactly what I did. Arch is solid. reply phamilton 17 hours agorootparentprevIt never recovered from the loss of gentoo-wiki reply ok123456 14 hours agorootparentprevI use arch btw reply pantantrant 15 hours agoparentprevI can&#x27;t install Gentoo, specifically, the part where I have to install a BIOS or UEFI grub2 bootloader. No I did not use a musl and &#x2F; or clang profile. With this being my first impression, unless Gentoo or a downstream fork comes with a installer, I won&#x27;t consider it.I&#x27;m going to install Solus OS &#x2F; Solus Linux soon, had a good first impression with it&#x27;s package manager, which claims to be reproducible but not like nix or guix are. reply MrDrMcCoy 12 hours agorootparentI have a work laptop that&#x27;s pretty cursed when it comes to booting anything that isn&#x27;t Windows or Fedora. I only recently got Arch to boot, and then only by using UKI and skipping the bootloader by directly registering it in UEFI. Maybe Gentoo has instructions for doing the same thing? I think it&#x27;s actually simpler this way, it&#x27;s just far less obvious and very new. reply cogman10 15 hours agorootparentprevWhat part is failing? Do you see the grub bootloader or is it failing before that?If you do systemd (which is what I&#x27;m using), then you might look at systemd.boot for UEFI. That&#x27;s currently what I&#x27;m using.I ask, because one issue I ran into was messing up the kernel config. Pulling the livecd config in and using that as the base config worked for me. reply Barrin92 16 hours agoparentprev>while other distributions have been actively removing one&#x27;s freedom to chooseI think that&#x27;s a bit unfair. Everyone wants to support everything, but at the end of the day if you want to guarantee security and maintenance and do the job of a distribution you have limited resources on what you can look after. Something always has to give.On any Linux distribution anyone can install what they want, but what you can genuinely claim to support is always limited by the amount of maintainers you have. reply connorgutman 16 hours agorootparentPerhaps I was being overly harsh. That being said, there is a certain philosophy of non-intervention which gives Gentoo a sense of freedom that other distributions lack. It’s not only about diverse support, it’s about allowing the user to choose upfront. Arch does this as well (mostly). It’s easy to choose your own bootloader, WM&#x2F;DE, audio server, and etc. I just think that Gentoo takes this philosophy one step further. If you want to install an alternative init system on most distributions you can… but you have to uninstall SystemD and all its weeds first. Gentoo’s solution is simple. Give the user a choice on EVERYTHING upfront. reply thaumasiotes 14 hours agorootparent> there is a certain philosophy of non-intervention which gives Gentoo a sense of freedom that other distributions lack.While I was untangling update dependencies one day I had it uninstall libc. There was a big warning advising me not to do that, but it was allowed.Everything broke immediately, of course, but I recovered the system and the dependency problem really had been fixed, too!This gave me a permanent positive impression of Gentoo. It&#x27;s better to be allowed to do things than not. reply pxc 8 hours agorootparentDebian and Ubuntu will let you do this, too. Pop_OS may have patched it out downstream, adding an additional barrier before you can uninstall &#x27;essential&#x27; packages since Linus Sebastian unintentionally blew away X11 (in the face of a very dire warning, which he ignored) on camera.Idr how this is handled on RPM-based distros, but on Debian it is part of the standard procedure for crossgrades.(More impressive than this sort of flexibility, imo, is how NixOS and GuixSD sidestep this problem altogether.) reply connorgutman 11 hours agorootparentprevLove this example! In general the way Gentoo handles masked packages also highlights this point. The system will kindly warn you if something is wrong, but it will never stop you from doing what you want. --autounmask also makes it dead simple. reply 1vuio0pswjnm7 12 hours agoparentprev\"Personally, I enjoy compiling everything from source and the flexibility that comes with it.\"Is this referring only to ports (Portage) or does this mean enjoy compiling kernel and userland. reply connorgutman 11 hours agorootparentDepends upon the device. I typically opt for compiling the Linux kernel myself but certain devices (like my Lenovo x60s) would absolutely melt if I compiled it from scratch. Thankfully, Gentoo offers gentoo-kernel-bin. Meanwhile my brand new GPD Win 4 Pro has a stupid nvme that requires 6.6.5+ to boot making compiling the only option (until a couple weeks ago at least). reply goku12 20 hours agoprevGentoo&#x27;s big attraction for me is Portage. It goes beyond just providing a build environment and dependency management. Ebuilds (Gentoo packages) are supported by great tooling and Eclasses that handle a lot of corner cases in builds. Developing Ebuilds feel like doing a real software project, and is great for anyone who wants to experiment with packages that are not in the official repository. Coincidentally, I just published a tool to manage unprivileged chroots for testing ebuilds.This development will make Gentoo more accessible for a lot of people. But I guess this isn&#x27;t for me. My build configuration (like CFLAGS) are never going to match the official binaries and so they will never get used. reply TrueDuality 18 hours agoparentI agree on customizing the package flags, and features. When using Gentoo in production it became an important part of our security posture to omit the features and integrations with unused software.That being said we&#x27;ve always had a build host dedicated to producing binaries, but the actual support for binaries in Gentoo hasn&#x27;t been great. Unsigned serving over HTTP or NFS of compiled artifacts is about all you get. I&#x27;m really pumped to see that the new package format adds in cryptographic verification that really should have been there all along even for internal only serving. reply zymhan 13 hours agorootparent> omit the features and integrations with unused software.That&#x27;s one of the most compelling cases I&#x27;ve heard for running something like Gentoo in prod.There are so many plugins, connectors, protocols, and often the old neglected ones turn into attack vectors. reply darkwater 12 hours agorootparent> There are so many plugins, connectors, protocols, and often the old neglected ones turn into attack vectors.Practically speaking this is probably true but theoretically a distribution&#x27;s job should be to somehow guarantee that a specific package built their way gets the security fixes for the way they built it.This is anyway tangential to the fact that in security \"less is more\". reply mid-kid 20 hours agoparentprevFor me this is great news for less powerful devices I use that I don&#x27;t feel like setting up a binrepo for (especially when it involves cross-compilation), but still want to reuse my portage config and custom ebuilds for. reply adrian_b 19 hours agorootparentFor less powerful devices it has always been possible to install Gentoo in a chroot directory on a powerful computer, using a configuration appropriate for the less powerful device, compile and install in the chroot environment any packages, and then just copy all the files from the chroot directory, through Ethernet or using a USB memory, over the root HDD&#x2F;SSD of the less powerful device.I have used Gentoo in this way on many small computers with various kinds of Atom CPUs and with a small DRAM memory and small and slow SSD&#x2F;HDD.With multiple computers of a compatible kind, the files from such a chrooted installation compiled on a powerful computer can be copied over all of them and everything will work fine. If the chrooted installation is preserved, it can be updated later with other software packages and all the changes can be propagated with rsync on all other computers.Linux is not like Windows, which will complain when run on a different computer than that on which it was installed initially. reply mid-kid 19 hours agorootparentI know this, I mentioned this as \"setting up a binrepo\" in my comment, as I think the binrepo approach makes more sense (esp. with regards to easy updates).It&#x27;s just an unwieldy amount of extra overhead, disk space, and time, which I&#x27;d rather avoid, especially for devices I&#x27;m not fully committed to maintaining over a long period. I&#x27;ve tried what you mention, it&#x27;s just never convenient enough to be worth the pain. reply bee_rider 19 hours agorootparentprevNext you’ll tell me there are optimization settings other than -mtune=native reply klodolph 19 hours agorootparentWell, duh. There’s also -funroll-loops. reply bee_rider 19 hours agorootparentMuch better than -boringroll-loops. reply abulman 18 hours agorootparentJust don&#x27;t use -O2 and -fe unless you want to end up with Rust. reply adrian_b 19 hours agorootparentprevI am not sure what you mean, but in the chrooted environment where you compile for a distinct machine you obviously use configuration options, including compiler flags, appropriate for the target computer, not for the host computer, so \"-mtune=native\" cannot be used.My point is that you do not need to setup a binrepo or any other complication like this.You can install easily Gentoo on a very weak computer, by performing the installation on a typical desktop computer, which may run a different Linux distribution, not necessarily also Gentoo, and then just copying the files.The Gentoo manual has always included information on how to install Gentoo inside a chrooted environment. reply trelane 18 hours agorootparent> not for the host computer, so \"mtune=native\" cannot be used.Right, I am sure they intended it to be absurd in an amusing way. reply bee_rider 18 hours agorootparentIndeed I did.Although, I do somewhat think that working out good optimization flags for cross-platform compiles is a moderately unusual skill, even among people who compile things regularly. Hopefully I caveated that sufficiently, I’m not saying it is a hyper-advanced dark art or anything.I have in the past set up code on a cluster to just compile on one node for the first run with -mtune=native because I’m lazy! reply trelane 19 hours agorootparentprevYou always have to remember to fun roll the loops, though. :)https:&#x2F;&#x2F;forums.gentoo.org&#x2F;viewtopic-t-245041-start-25.html reply Maken 19 hours agorootparentprevNever forget -Ofast. reply goku12 19 hours agorootparentprevThere are definitely going to be interesting use cases like yours. I&#x27;m in no way against this development. The more the merrier!But to be honest, I haven&#x27;t looked at binrepos so far. Perhaps your reply is a good reason to. reply yx827ha 18 hours agoparentprev> Coincidentally, I just published a tool to manage unprivileged chroots for testing ebuilds.You should check out what ChromeOS is doing. They are using bazel to execute ebuilds inside an ephemeral chroot: https:&#x2F;&#x2F;chromium.googlesource.com&#x2F;chromiumos&#x2F;bazel&#x2F;+&#x2F;refs&#x2F;he...This way it&#x27;s guaranteed that no undeclared dependencies get used. reply goku12 17 hours agorootparentThis tool [1] does the exact same thing - except that you nuke the chroot when you&#x27;re done. And the reason is the same - to find all necessary dependencies. I had a small script that eventually became a Rust program. Then I kept adding features to it until it became what it is now. That&#x27;s the reason why I never really got to explore the alternatives. Anyways, it&#x27;s usable and nearly done.Thanks for suggestion though. I&#x27;ll take a look at it.[1] https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;genpac[2] https:&#x2F;&#x2F;wiki.gentoo.org&#x2F;wiki&#x2F;Chroot_for_package_testing reply trws 14 hours agoparentprevYeah, my CFLAGS won’t either. But I have to say I’m tempted to script something up so I can override all the plasma-related packages to use the common ones, or something similar, so I build everything where I care about speed myself and let the gui stuff be binary packages. Would save a whole lot of build time, and I’m not sure it would be much of a loss. reply matoro 17 hours agoparentprevCan you please share the tool in question? I have been desperately looking for something like this for my sandbox project. reply goku12 17 hours agorootparentHere you go [1], [2]. It&#x27;s not completely ready yet - but it&#x27;s usable. It should be OK if you plan to just modify or reuse parts of it. It currently supports btrfs backend. Plain directory backend and packaging of the tool are not done yet - but shouldn&#x27;t be too hard. I was keeping it for tomorrow. Meanwhile, you can use asciidoctor to convert the docs if you need to refer it.[1] https:&#x2F;&#x2F;git.sr.ht&#x2F;~gokuldas&#x2F;genpac[2] https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;genpac reply 0xbadcafebee 18 hours agoprevSay what you will about Gentoo as a concept, it was a lot of fun when I was 17 learning more about software packaging, distributed compilation, and the ins and outs of compile-time optimization, not to mention Linux kernel optimization. And their community had pretty nice docs from what I recall. I think a few of my patches are still knocking around in some releases.I finally realized all the tweaking and optimization and bleeding-edge software wasn&#x27;t worth it once I discovered my Slackware boxes ran as fast as the Gentoo ones. Maybe there&#x27;s a few very specific applications out there that benefit from all the tweaking and custom compiles; perhaps a render farm or crypto miner? But my games got the same FPS on either distro. reply fl0ki 16 hours agoparentCPU-specific optimization has waxed and waned in importance over the years. It used to be that a lowest common denominator compilation would not even include MMX or SSE, which could make a huge difference for some kinds of CPU-bound algorithms. However, it was always possible to do a runtime CPU feature detection and run the optimal version, so compile time feature selection was not the only option.Then AMD Opteron came out and got everyone on a new baseline: if you compiled for amd64 then that meant a specific CPU again, there were no newer instructions yet. Now AMD64 has several levels[1] which can start to matter for certain kinds of operations, such as SIMD. A more mundane one I often run into is that v2 added popcnt which is great for things like bitsets in isolation, but in overall program performance I measure almost no difference on any of my projects between v1 and v3.When it comes to games, it&#x27;s more than likely your games were binary-only and already compiled for the lowest common denominator and maybe used runtime feature selection, and even then, they were probably GPU-bound anyway.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;X86-64#Microarchitecture_level... reply pleo__ 17 hours agoprevI ran Gentoo for a long time in the early 2000&#x27;s. I learned nearly everything I know about Linux machines in general from that experience!What was interesting about the USE flags was learning that a given package even HAD a particular integration with some other library &#x2F; package. Realizing that the SQLite3 binary doesn&#x27;t work the same when you don&#x27;t have readline support linked to it lead me to understand what readline was as a whole. That happened over and over again for a lot of the \"invisible\" libraries that are always included on every other Linux system.Absolutely invaluable learning tool at the right time in my life for sure. reply iamgopal 21 hours agoprevYears ago, somehow, I wasted hours and days of computer and my time, compiling and fine tuning my gentoo system, god knows why, when next day I anyway format it to install newly arrived Ubuntu cd. reply Brian_K_White 18 hours agoparentEveryone please realize that just because you did something and no longer do that thing does not mean it was wrong to have done that thing.All of us who at one point compiled our own kernels and now no longer do, are the killers that we are partly because we did things like that at least for a bit. It only makes sense not to now, after having done it.It&#x27;s not true to suggest (or to read these stories as a new bystander and come away with the idea that) \"if I were smarter I never would have wasted time on that\" reply twosdai 17 hours agorootparentI hear your point that the act or process of doing the learning is good, even if the end result is that you shouldn&#x27;t do the thing again. Such as learning assembly but then only doing web development for a profession where you don&#x27;t need to know assembly.However, I think that the statement below might be better with a bit of nuance.> It&#x27;s not true to suggest (or to read these stories as a new bystander and come away with the idea that) \"if I were smarter I never would have wasted time on that\"I would say its \"not true always,\" in some cases doing the action really wasn&#x27;t worth the time.Related to this, I believe the sentiment people have about regretting wasting time on some endevour, is a misalignment of what their intention was to begin with.For example, if someone wanted to compile their own kernal because they wanted to learn and understand more about their computer its unlikely that they would walk away from that experience with regret. However if they wanted to compile their own kernal because they believed that in doing so they would make 10x more money in the long run (through learning so much), and that goal failed to materialize. They would likely tell others to not waste their time learning to compile their own kernal.Not trying to be pedantic, or argumentative, I aggree with your point deeply, however I wanted to discuss it a bit further. Let me know your thoughts. reply hylaride 19 hours agoparentprevSame, but I learned so much while doing it. Eventually I got tired and moved to an arch and got most of the same without always fighting broken packages. But I still use the knowledge I gained dealing with random low-level issues when they crop up. reply zamadatix 18 hours agorootparentI had a similar experience with the Gentoos&#x2F;Arches of the world. I&#x27;d never use Gentoo or a Gentoo-like as my primary OS for anything, likely for the rest of my life, but it still ended up being one of the most valuable operating systems for me to spend some time on. reply bombcar 20 hours agoparentprevGentoo’s biggest attraction for me was always the USE flags - being able to turn off the X integration of mpg123 where CentOS demanded an entire X install to get the command line player.The flags were just icing on that cake. reply teekert 20 hours agoparentprevYou and me both. But, we learned a lot. Nowadays I feel like Linux is my super power, OS, VM, Container, Nix-shell, WSL2. It’s all Linux. And you can drop me on any command line (even a BSD one to some extent) and I will feel at home and can solve problems. I’d like to think that’s where my happy time with Gentoo led to. reply oblio 20 hours agorootparentI agree with you, but we&#x27;re also lucky (or unlucky?) that the variety of out there&#x27;s dropped a ton.If you can, try to get access to OpenVMS or Cisco IOS, it&#x27;s an entirely different world in terms of user experience. reply jstummbillig 20 hours agorootparentprevAnd no matter if its true (and it might well be!) the overarching tendency to look for reasons to explain time spent with Gentoo should probably tell us something.At best, we are at least a bit confused about it all. reply teekert 12 hours agorootparentI was confusing because you can’t install it without understanding stuff like fstab, grub, user creation etc. It sets you up to be a sysadmin, it requires you to be a sysadmin. Ubuntu on the other hand, looks and acts more like an iPad than Windows. reply bee_rider 19 hours agorootparentprevI never used Gentoo, but the time I spend screwing around in Arch was more educational than the video games I would have been playing, anyway. reply teekert 18 hours agorootparentprevI had fun. reply mad182 20 hours agoparentprevI too used to obsess over customizing my OS. Now I just install Debian, a handful of programs I use daily and that&#x27;s it. I can recreate my setup on another machine in 20 minutes. reply subtra3t 20 hours agorootparentWhat handful of programs do you use daily, if you don&#x27;t mind sharing? reply mad182 20 hours agorootparentNothing interesting. I spend like 99% of my computer time in the web browser, ide and terminal.Chrome, git, ssh, docker, netbeans reply bitcharmer 19 hours agorootparentOh wow, I did not expect to see NetBeans here. What do you use it for and why NetBeans? reply mad182 19 hours agorootparentGeneral web development. PHP, javascript, css. I used to do some java projects as well, but not lately.I know it&#x27;s not mainstream to use NetBeans these days, but I don&#x27;t care, I&#x27;m just used to it and it gets the job done. Maybe I&#x27;m just getting old. reply bitcharmer 1 hour agorootparentThanks for answering. I&#x27;m not NetBeans fan myself but there&#x27;s absolutely nothing wrong with using the tools you like that get the job done. replycalvinmorrison 19 hours agorootparentprevThey talk about the shell as an IDE. My entire desktop is a 14 year experiment in tuning productivity. My ~&#x2F;bin&#x2F; folder has around 100 scripts and maybe 20 are little scripts i wrote in conjunction with i3. Pretty cool how it stacks up over time reply sergiomattei 13 hours agorootparentprevSame but with macOS.The only cool thing about it is that it’s declarative: nix-darwin everything and a fully working and customized machine is up in 10 minutes with one command reply doublepg23 13 hours agorootparentDo you have some docs or writeups on your setup? I&#x27;m planning to move to macOS in a few weeks. reply zaknenou 20 hours agorootparentprevnext [4 more] [flagged] bayindirh 19 hours agorootparentOf course you customize your Debian installation over time.Over time is the key here. A package here, a small config there, and after some time, that installation becomes so unique that it starts reading your mind.Non-breaking updates is the icing on the cake.The biggest point is you install Debian once, or when your processor architecture changes.--Sent from my a 6 year old Debian installation. reply mad182 20 hours agorootparentprevNot sure if you mean that sarcastically, but it&#x27;s a really boring and stable OS, the setup is just a few clicks and hitting enter a few times. I think I haven&#x27;t even bothered to change the wallpaper on my newest laptop.There&#x27;s nothing really special about Debian as well, it could as well be Ubuntu, Mint or anything else that&#x27;s plug n play. I&#x27;m just used to Debian, and it comes with less junk I don&#x27;t need installed by default. reply doubled112 20 hours agorootparentDebian is my set and forget OS as well.After running my home servers on it, one release wasn&#x27;t too far behind to run as a desktop, and I&#x27;ve been happy here ever since.Xfce desktop, move the panel to the bottom, install applications. Use it for the next days&#x2F;months&#x2F;years until I decide to look around again. reply toyg 20 hours agoparentprevWhen every ounce of power mattered, fine-tuning your OS made sense.Nowadays most people are swimming in CPU cores and gigabytes of ram and terabytes of solid-state memory, so fine-tuning is a waste of time (unless you play bleeding-edge games). But it wasn&#x27;t always such. reply nottorp 20 hours agorootparent> so fine-tuning is a waste of time (unless you play bleeding-edge games).Unless you run javascript &#x27;applications&#x27;. Games are already optimized. reply hermitdev 18 hours agorootparent> Games are already optimized.Yeah, I&#x27;m calling bullshit on this one. At least, it doesn&#x27;t line up with my experience. In my experience, games are optimized _just enough_ for a decent playing experience (and not always then). Games devs, as a whole, are the worst offenders of expecting their users to just throw more money (hardware) at the software to achieve usable&#x2F;enjoyable experiences. There are, of course, exceptions. But, for every Carmack, there&#x27;s 10s of thousands of developers scrambling to make their deadline, doing just enough to ship. reply II2II 17 hours agorootparentI have heard of people recompiling the kernel to improve gaming performance (mostly to use a different scheduler or what be it), but don&#x27;t recall seeing anything beyond single digit percentage improvements in performance. Which makes sense, since you can only recompile the kernel and a subset of open source libraries that the game may use. Those are going to be fairly well optimized to start with.The games themselves though are a different story. Outside of open sources games (which are usually less demanding than commercial ones), you don&#x27;t have the source code to rebuild it. Even if you did, enabling optimizations beyond what the developer used risks breakage so you will have to do your own testing. Even then, simply rebuilding the software wouldn&#x27;t address the quality of the code created by those developers who are scrambling to meet a deadline with as little effort as possible. reply hermitdev 14 hours agorootparentI&#x27;ll be the first to admit that I&#x27;m not a game developer and my exposure to commercial games&#x27; source has been very limited. The most exposure I&#x27;ve had was to Civ4 due to Firaxis releasing the source for the core game DLL for modding. Civ4 also used Python for scripting and Python (undeservedly, here) gets the blame for the game being slow, especially during late-game play.Back in the day, I spent a fair amount of time working on gutting the DLL because frankly, it was atrocious. My memory is a little fuzzy as it&#x27;s been +10 years since I&#x27;ve looked at it, but things I remember seeing:* over use of C&#x2F;C++ preprocessor macros where an inline function would have been more appropriate to say, get the array&#x2F;list of CvPlots (tiles) that needed to be iterated over all the time. * lack of hoisting loop invariants out of loops. It is common to see usages of the non-trivial macros above in the bodies of tight loops, often tight nested loops. Optimizing compilers are great, but they&#x27;re not _that_ great. * the exposure of the C++ types to Python was done...poorly. It was done using Boost Python (which, while a great library for its day had a _huge_ learning curve). For every Cv type in the C++ DLL, there was a corresponding Cy type to expose it to Python. Collection types were _copied_ every call into Python code, which was quite frequent. The collections should have been done as proxies into the underlying C++ collections, instead of as copies.Most of the changes I made were quite small, but over the course of a month of part-time hacking on it, I&#x27;d gotten late game turns down to a couple of minutes from 30-minutes and memory usage was extremely reduced; and I never did get around to fixing the Python wrapper as it would have too intrusive to fix it properly. I could have made more aggressive changes if I had full access to the source, but being constrained by DLL boundaries and C++ types being exported limited what could be done w&#x2F;o breaking the ABI (had to be extremely careful about not changing object sizes, affecting vtable layout, etc).Frankly, I doubt the developers spent very much time at all, if any, with a profiler during the course of development with the game. reply nottorp 14 hours agorootparentYeah but you picked the one game where some dude patched the civ 4 binary for a 3-4 x increase in rendering performance :)Civ had been a 2d game until then, it was their first 3d title.Not to mention that it was turn based strategy, and the main performance problem was AI turn length in the endgame. reply Zambyte 18 hours agorootparentprev> Games are already optimized.What AAA titles have you played around launch in the last 5 or so years? reply Zambyte 18 hours agorootparentprevI think the biggest case where Gentoo still makes sense is when you have a large fleet of identical machines. In that case, the effort put into tuning your installation will be multiplied across the number of machines it&#x27;s applied to.For a single machine home install, the biggest value Gentoo has to offer is the learning experience. I ran it for about a year like 4 years ago, and I definitely learned a lot in that time. Hopped around a bit and I&#x27;ve since landed on GNU Guix, and I&#x27;m probably set for life now in the GNU world. reply MountainMan1312 11 hours agorootparentI&#x27;m a Gentoo daily driver and I&#x27;m also looking real hard at Guix. I already live inside of Emacs, having everything in Lisp seems kind of nice. reply dagw 18 hours agorootparentprevWhen every ounce of power mattered, fine-tuning your OS made senseI used to believe that and was a huge Gentoo user for years back when it was initially released. Then one day I benchmarked Gentoo and a default RedHat install on every work load I cared about, and everything was within the margin of error. reply lanstin 16 hours agorootparentI like gentoo because I have it set up to compile everything with symbols and the source code in the image, and I can gdb anything I am curious about. reply costco 16 hours agorootparentprevMade an ancient computer with very limited CPU usable for my siblings with Gentoo. The secret is to do USE=\"-* minimal\" and enable things that are required from there. Compiling a custom kernel was actually necessary because it had a really old NVIDIA card that was no longer supported and I had to patch something to do with MTTR. Installed XFCE 4 and it idled with 70 MB of RAM used. Could play Youtube videos without the whole thing freezing whereas Debian could not. Gentoo is great. reply pjmlp 20 hours agoparentprevThe time I wasted with Gentoo in 2004 was enough to never try it again.A full day compiling stuff only for the base install, let alone everything else I would eventually need.On my case, I decided to become another Scientific Linux tester. reply oblio 20 hours agorootparentThe thing is, that was 20 years ago. I basically did the same thing, at almost the same time, as you.But computing power is much higher now. The same compilation now would probably take 1-2 hours, max. Updates would be super fast.Gentoo itself is considered generally stable and a pretty solid distribution, or it used to be.I wonder if these days the flexibility and the engineering behind Gentoo might be worth taking another go at it. reply cogman10 18 hours agorootparentFor kicks and giggles, I just set it up on a new system a couple of weeks back.It&#x27;s really not much different than working with Arch in terms of complexity. Initial setup takes a bit, but if you&#x27;ve installed arch you are pretty familiar with everything you need (in fact, arch docs are helpful for a gentoo setup :D).The docs are VERY good and easy to google.Compilation time can be nasty depending on what you install but not terribly bad. I just rebuilt the world because a GCC update broke lto that I&#x27;m running. With about 2k packages that took about 6 hours to complete on a Ryzen 7950.General updates take almost no time at all (especially using git for syncing). Usually less than 10 minutes often less than 1. As I write this, I&#x27;m currently rebuilding kde (if you are using your computer, rebuilding doesn&#x27;t really get in the way. Especially if you are already working with a multicore system). reply massysett 17 hours agorootparentprev“But computing power is much higher now. The same compilation now would probably take 1-2 hours, max. Updates would be super fast.”I’m not so sure. A lot of the power comes from multiple cores. Years ago I had one core, now I have eight. A lot of the compiles don’t use all the cores.Software has also gotten bigger. rustc is huge, for example. It didn’t even exist when I used Gentoo years ago.These days I’m on the Mac and I just switched to Homebrew after using Macports for years. It was for one of the same reasons I stopped using Gentoo: compiling takes too long. Whenever I upgraded Mac OS versions, Macports required me to recompile everything. This was no problem at all for, say, tree. But something was pulling in gcc (emacs needed it for some reason??) and this took ages to compile.At least Macports worked though. When I used Gentoo, it took so long to compile things that I would leave it overnight, and of course often in the morning I would see that the compilation stopped halfway through because something was broken. Hopefully that’s improved. Or of course maybe the binary packages will help with this.But if I wanted a build-your-own, rolling-release binary system, I don’t see why I wouldn’t just use Arch. reply pjmlp 20 hours agorootparentprevEven 1-2 hours is too much for me.I rather use programming languages ecosystems that favour binary libraries for a reason. reply II2II 17 hours agorootparentA lot of the reason depends upon what you hope to get from the labour and the overall environment that you are working within.I was working with a 486 around 1995. Compiling your own software was the norm and compiling your own kernel could have significant performance benefits (even if it was just to conserve the limited memory supported by machines of the day, to head off some of the swapping). By the time I learned of Gentoo, that was not really the case: most of the software one could obtain was provided in binary form and compiler optimizations were much less relevant (unless you had a special workload).The tooling provided is important too. I was using NetBSD for a while. For the most part you just started the compilation process and walked away until it was done. (I imagine Portage is similar.) You didn&#x27;t get the instant gratification, but it was not time intensive in the sense that you had to attend to the process. That was very much unlike my earlier experiences in compiling software for Linux, stuff that isn&#x27;t in the repos, since it did have to be attended to. reply pjmlp 15 hours agorootparentIt surely wasn&#x27;t the norm for me, in 1995&#x27;s Summer, I got my first Linux distribution via Slackware 2.0, everything was already compiled and when chosing to download tarballs I would rather pick already compiled ones.Later on, to take advantage of my Pentium based computer, I would get Mandrake, with its i585 optimized packages.Most of my Linux based software would be sourced via Linux magazines CD-ROMs, or Walnut Creek CD-ROM collections. reply poszlem 17 hours agorootparentprevThe time I \"wasted\" with Gentoo in 2005 taught me enough about how Linux works to land me my first real IT job. I will forever be grateful to that distribution. reply pjmlp 15 hours agorootparentSure, but you didn&#x27;t need to suffer compiling stuff from scratch to know how UNIX works.I used first Xenix in 1993-1994, and naturally wasn&#x27;t compiling it from scratch. reply netdur 20 hours agoparentprevI had toshiba satellite with some whooping 96MB RAM... as main computer even... happily ran Windows 98... them I got the book \"Linux from scratch\" the rest is history... now I am happy mac user. reply BSDobelix 19 hours agorootparentHave you try&#x27;d \"MacOS from scratch\"? It&#x27;s even harder ;) reply Maken 19 hours agoparentprevI never used gentoo. Making sure the graphic and wifi card in a laptop worked after every upddate on Ubuntu was hard enough for me. reply skriticos2 19 hours agoparentprevYea, same here back in the day. Stage 1 installations for Gentoo really made me interact with the kernel and software in a different way. It did not just work, but while solving the issues, I learned a lot on how things worked internally. It&#x27;s a great thing to get really familiar with the workings under the hood.But yea, now-a-days I&#x27;m on Ubuntu LTS. reply goalieca 19 hours agoparentprevOn my first pc assembled from used parts, I was able to squeeze every bit of compute out of gentoo. Being able to build smaller binaries by excluding dependencies seemed to help a lot. I used it until the first ubuntu was released and it just worked and worked well. The only problem was that it was an ugly brown. reply dexterbt1 20 hours agoparentprevSame, this was around 2004-2006ish when I maintained a Gentoo build for my Pentium 4 box. There was this somewhat draw to me of compiling my own binaries highly optimized to my processor and that Portage mostly works. But my gosh, gcc build times are killing the fun. When Ubuntu arrived and saw my peers being productive, I switched. reply globular-toast 19 hours agoparentprevBut then you have to deal with big upgrades that might break your system and old packages (or start randomly adding PPAs etc.) A rolling distro means you can continually keep up with small changes and only adopt big new pieces (like systemd, pipewire, wayland etc.) if&#x2F;when you are ready to.I&#x27;ve installed Gentoo literally two times. Once per PC. Been using it for years. It&#x27;s not like you have to keep tweaking it. It does help if you run a basic system like me, though (no DE, simple tiling WM, don&#x27;t use much apart from Emacs and Firefox). reply gavinhoward 19 hours agoprevWow...I feel weird.A lot of the comments are saying that they tried Gentoo or used to use it.And here I am, using it as my daily driver and server workhorse.I wonder what makes me different such that Gentoo is the best for me.And I am not going to enable binary packages. reply connorgutman 18 hours agoparentYou’re not alone! I run Gentoo on all my devices. GPD Win 4, Pixelbook with coreboot, my NAS, and a VPS. Why (IMO)?1. Best documentation I have ever used.2. Infinite amount of customization. Your system can be as minimalist or maximalist as you’d like.3. Portage overlays > AUR any day of the week.4. Choose your own init system. Want SystemD? Fine. Want OpenRC? Go right ahead!5. It’s hard to understate the power of a good community with well-described values. I feel safe and at home with Gentoo because I know that it will always value the #1 thing I care about on Linux: freedom. Gentoo is like a choose your own adventure. Want to run SystemD + Gnome + PulseAudio + binary packages? You can do that. Want to run OpenRC + Hyprland + Pipewire + NetworkManager instead of netifrc? Go for it!Gentoo isn’t about compiling everything, it’s about choosing everything. Today’s announcement (although irrelevant to me) gives users more choices. reply aidenn0 16 hours agoparentprevI would still be using Gentoo if NixOS hadn&#x27;t come around. I like Gentoo just fine, but being a sysadmin for NixOS is much easier than being a sysadmin for Gentoo. reply the_af 19 hours agoparentprev> I wonder what makes me different such that Gentoo is the best for me.Ok, I&#x27;ll bite: why is Gentoo best for you? (I&#x27;m not going to try to refute any of your statements should you reply, I promise. I&#x27;m genuinely curious.)I&#x27;ll offer my own experience why Gentoo is not for me: I don&#x27;t run my own server, and at work I use servers in the cloud (Ubuntu based, I guess? Or AWS-flavored). No compiling any kernels or whatnot.For the desktop, a friend convinced me to try Gentoo 10+ years ago (\"it&#x27;s compiled for your specific hardware, it&#x27;ll be faster!\"), I gave it a try, wasted lots of time getting it running, then saw it wasn&#x27;t really noticeably faster for any task I did than regular Ubuntu, and took longer to get set up. So I ditched it.Another common claim I found to be false (in my case): \"I learned so much configuring Gentoo!\". Well, no. I mostly followed the recipes like almost everybody else, setting flags and touching config files I didn&#x27;t really understand, so I can safely say I learned nothing -- I just went through the motions.But that&#x27;s just my experience. reply gavinhoward 18 hours agorootparentThat&#x27;s what is weird; I don&#x27;t know why it&#x27;s better for me!I can understand that you didn&#x27;t learn anything from installing Gentoo. I did, but in my case, I was not like most new Linux users; instead of immediately trying to learn the details when I started, I just wanted to install Ubuntu and get working.It was only six years later that I installed Arch, then Linux from Scratch, then Gentoo, and in the process, I had many lightbulbs trigger. \"Oh, that&#x27;s why it&#x27;s like that!\"As much as I love Gentoo, I agree with you: the \"faster\" argument is a myth.I hope you can tell I&#x27;m not trying to evangelize. :) You said you weren&#x27;t going to refute that Gentoo is best for me; well, I&#x27;ll go the other way and not refute that Gentoo is really bad for nearly everybody!Anyway, I&#x27;ll take a stab at why Gentoo is best for me: extreme customization. See, there&#x27;s another way I&#x27;m not like most people; others start out with extreme customization and ricing, and they back off of it over time. I started with little and have only grown my customization over time.Maybe it&#x27;s because I&#x27;m on the spectrum, but computers are annoying to use for me by default. On Gentoo, however, I can make my setup fit me more than any other distro.Another thing that might contribute is that I like lean setups. Just checked with `ps --ppid 2 -p 2 --deselectwc -l`, and subtracting the terminal, shell, ps, and wc, there were only 30 processes. And that includes niceties like redshift and picom.Because I do heavy fuzzing, that matters to me. Also, it is responsive; slow responses bother me more than they do others.But to be clear, that doesn&#x27;t refute your experience that Gentoo isn&#x27;t faster; it took a lot of work to get Gentoo to this point, and it isn&#x27;t default.So I am weird. reply bombcar 18 hours agorootparentPeople obsess over the compiler flags (and there was a short time when everything else seemed to be x86-32bit and Gentoo was one of the very few distros where you could compile everything for amd64) but the real advantage of Portage is the USE flags; where you can turn off major things like X support, or disable IPv4 entirely, etc. Nothing else I&#x27;ve ever encountered allows that customizability.And that&#x27;s been much more important to me than cflags, which I set once and ignore, ever since trying to install a command-line MP3 player on an ancient RedHat brought in an entire X install. reply gavinhoward 18 hours agorootparentYes, I agree with this. I have used USE flags immensely. reply goku12 18 hours agorootparentprevNot the same person you&#x27;re replying to. But I have some ideas to share.> Another common claim I found to be false (in my case): \"I learned so much configuring Gentoo!\"This isn&#x27;t true for everyone. My general plan with anything is to follow the script exactly at the beginning. If that works out, start making incremental changes until something breaks or until you&#x27;re satisfied. Gentoo taught me a lot of things that way - especially the kernel compilation (which is important to me anyway, due to my profession). Before this, it was Arch. And a regular Linux distro before that. All of them taught me something with the same strategy during the initial phase. Eventually the struggle gives way to familiarity and learning rate starts to fade.> Well, no. I mostly followed the recipes like almost everybody else, setting flags and touching config files I didn&#x27;t really understandI don&#x27;t know about anyone else, but Gentoo config files were the easiest for me to understand. I feel very much in control with them. The Gentoo wiki doesn&#x27;t just give you recipes - they always tell you the exact reason for a configuration or flag (This is true for Arch as well). I have also created packages for myself - so I have a reasonable understanding of USE flags. I&#x27;ve been thinking about migrating away from Gentoo - but I like USE flags so much. Sometimes they allow you to access application features that are not available on other distros (because they have to choose build flags that suit the majority).Something that helps me with this level of control over the configuration is that I maintain them as literate org-mode files with explanations and even diagrams at times. This might seem like too much work for a desktop system. And often that is true. But this workflow suits my interests and profession really well. In fact, at least 3 custom programs are part of my desktop - configuring Gentoo to my liking is the least difficult part in it.> Ok, I&#x27;ll bite: why is Gentoo best for you?Finally. I said this in another comment. The most attractive part of Gentoo for me isn&#x27;t the custom compiled kernel or software (I use flatpaks for most desktop apps). It&#x27;s the package management system. Portage is by far the best system I&#x27;ve experienced for creating system packages. reply akira2501 12 hours agorootparentprev> why is Gentoo best for you?It has zero opinions about how my system should be configured or what options the installed software should have. It lets me build precisely what I want to build without any hassle or compromise.> I mostly followed the recipes like almost everybody elseThe thing I really like about gentoo is I have a virtual&#x2F;akira package. It brings up any system precisely the way I want it. If I have a new system I can just install that virtual from my overlay repo and be in business within a few minutes.I wrote this virtual 10 years ago. It still works perfectly. Ironically, I guess I could say, I choose gentoo because I want to spend _less_ time managing my system, and it enables that in a way that no other distribution has for me ever. reply bombcar 18 hours agorootparentprevI use Gentoo on my personal servers because it offers the customization I want with the ability to easily install from source some various strange things I have laying around.It&#x27;s a rolling release so it never has a \"major blow up day\" - at worst I have to spend 5-10 minutes on a \"breaking\" change.My desktop is just a Mac, but my Gentoo server provides the Linux I need.(Work servers are Ubuntu because work software wants something well-known.) reply theamk 10 hours agoparentprevDo you have machines which \"just work\"? Not a daily driver machine or server that you mess with every week, but something you set up long time ago and that has been working even since. Something that you may only log in few timers per year, if that.I stopped using Gentoo when I got my made my first \"production\" system: I made it, people started using it, and it was \"done\" - as in, there were no more any reasons to log into it daily, or even weekly. I&#x27;d still want my security updates and very very rarely there would be a new feature that required new package, but otherwise I wanted system to be steady. And Gentoo is a horrible fit for this usecase - even on my personal desktop where I updated weekly I had emerge build failures practically every time; and a system which might not be updated for months would be even worse. So I went to Debian, and it was good (especially with unattended upgrades), and I eventually switched all my machines to it.If you use Gentoo, do you have any machines that you don&#x27;t tinker with weekly? How often do you update them, and how often do you need manual intervention to finish the update? reply BenjiWiebe 3 hours agorootparentOddly enough, on my family&#x27;s farm, we got a DeLaval voluntary milking system, i.e. milking robot. Very pricey piece of gear. And several of it&#x27;s embedded computers for some reason run Gentoo. reply gavinhoward 8 hours agorootparentprevI use mine daily and update them about weekly, yes.Leaving a Gentoo system is, as you say, not something you should do. I&#x27;m lucky that I don&#x27;t have that. reply theamk 6 hours agorootparentWell, here is your answer why so few people use gentoo.I can count at least 6 linux computers in my household (this includes stuff like work laptop(s), fileserver, my family member&#x27;s PCs, and that raspberry pi which only acts as a camera server for my 3D printer). And the life is just easier if all of them run the same, or at least very similar, systems. reply gavinhoward 5 hours agorootparentOh, I know people don&#x27;t, and shouldn&#x27;t, use Gentoo.What&#x27;s weird to me is that it works for me. And it&#x27;s weird because I see how Gentoo is just not a good fit for most things, so fitting it feels weird. reply nubinetwork 19 hours agoparentprev> And here I am, using it as my daily driver and server workhorse. I wonder what makes me different such that Gentoo is the best for me.Persistence like a Saint? Hard to say. I&#x27;ve been using it for 20 years too. reply BeetleB 17 hours agoparentprev20+ years for me:https:&#x2F;&#x2F;blog.nawaz.org&#x2F;posts&#x2F;2023&#x2F;May&#x2F;20-years-of-gentoo&#x2F; reply juped 15 hours agoparentprevExtensive binary packages would have been nice back in the years when Gentoo was a meme OS; CPUs were a lot slower and not multicore. I don&#x27;t have a use for them now.Anyway, the best OS is the one you already know how to use. reply hansvm 12 hours agoprevThe thing that makes Gentoo fantastic is the fact that it&#x27;s designed from the ground up to make it easy and maintainable to add the one little tweak you want for your system. In all the other distros I&#x27;ve tried (all the major ones, many of the minor ones), they tend to work better out of the box, but straying from the beaten path results in a flogging like no other. A tuned Gentoo system just works™, whatever \"just working\" means for you personally. That might be a python 2to3 name collision when Arch decides to overwrite upstream, or a system-critical latency issue when SystemD does too much unnecessary garbage in kernel mode, or whatever, but all of its flaws aside I&#x27;m a very, very happy Gentoo user.Upstream binary packages are just another extension of that freedom. You already had binary versions available of a few major projects (or could roll your own build server), but making more of them easily available allows a lot more people to reap those benefits without having to worry about the huge time sink of building every little thing. If you need more flexibility (patches, use flags, ...) for a given package, that&#x27;s still available and easy to maintain. This is a huge win. reply vermaden 20 hours agoprevToo little too 15 years too late.One of the reasons I moved from Gentoo 15+ years ago to FreeBSD was that it was mandatory to compile everything while FreeBSD provided binary packages.It may be not that important today - but it was a game changer with single CPU core and 1GB RAM. reply rjzzleep 19 hours agoparentSad to say that you are completely right, for the longest time after being on gentoo for a long time I switched to macos for a few years, and was using gentoo prefix, which is vastly superior than Homebrew. I added patches to fix upstream llvm to work in gentoo prefix on macos almost a decade ago [1]I finally met someone at the GSoC reunion that wanted to get me maintainer status, but I never got them to follow through. He had already warned me that it would be a complicated task to accomplish. I kept mentioning prefix needing binaries as well. Imagine if gentoo prefix had been as easy to install packages as homebrew on a mac.It&#x27;s sad, but gentoo is a good example of why an open source project that is technically superior, cannot survive inferior solutions without good stewardship if they disregard some basic end user quality of life features. I would argue that that is also what killed opensolaris&#x2F;illumos(which is basically on life support), because the people in charge could never get past their elitism and decide that for community engagement the kernel build needs something more simple that 100 layers of nested incomprehensible makefile&#x2F;shell spaghetti.[1] https:&#x2F;&#x2F;github.com&#x2F;fishman&#x2F;timebomb-gentoo-osx-overlay&#x2F;tree&#x2F;... reply bee_rider 18 hours agoparentprevIs it too late? Gentoo has always been a niche within the already niche Linux community, but they seem to keep chugging along happily. Are they having some problems? reply BSDobelix 17 hours agorootparent~Gentoo is probably the most installed \"Desktop\"-Linux in the world -> ChromeBookshttps:&#x2F;&#x2F;wiki.gentoo.org&#x2F;wiki&#x2F;ChromeOS reply bee_rider 16 hours agorootparentI guess what I’m trying to say is, in order to define “too late” we need to define an objective and then figure out if we’ve passed the point where it is possible.From the outside it looks like the Gentoo community is happy and stable being small. And that’s good.I’m not sure Google copying some of their software and putting it on Chromebooks is a huge win for the community, although I bet some of the Gentoo devs are proud. reply galleywest200 17 hours agorootparentprev> ChromeOS is built using Portage, Gentoo&#x27;s package manager, and Gentoo-based chroots. ChromeOS uses the upstart init system.Sounds like they are just using parts of it and not Gentoo itself. reply TillE 10 hours agorootparentPortage is like 90% of what makes Gentoo Gentoo. reply goku12 18 hours agorootparentprevThey have had the binary packaging capability for a long time now. It just didn&#x27;t make sense to use it on a global scale considering the vast combination of packages they could generate (due to USE flags, profiles, etc) and the infrastructure needed to distribute them. They seem to have decided to offer binaries for the most common configurations. This isn&#x27;t a major change. Perhaps the infrastructure also makes more sense than before. reply chlorion 13 hours agoprevGentoo for me is not about compiling things from source, or \"performance\", or tweaking your OS for days, which seems to be the comment perception from people.Now that there is an officially supported binhost, you do not have to compile anything if you don&#x27;t want to. You can use a desktop profile with zero customization and have a system that works \"out of the box\". You do have the options to customize and compile but it&#x27;s not required.Gentoo&#x27;s benefits for me include the tooling around portage and the PMS (package manager spec). Gentoo&#x27;s software packaging tooling is (IMO) superior to what exists in Debian and other conventional (not nix) distros. The most similar would be Arch&#x27;s PKGBUILDs, which are also pretty nice.Packaging software for Debian, trying to create a .deb package is a fairly arcane process and the documentation was difficult to find and digest when I attempted this. Gentoo has a wiki section called \"ebuild writing guide\" that describes everything in great detail.There are also benefits like being able to select on a per-package-basis whether you want \"stable\" or \"unstable\" versions of software. Gentoo is a rolling release distro, but requires packages to meet a certain criteria before being marked as \"stable\". You are not forced to use \"stable\" packages if you want the most recent releases from upstream (basically what Arch does) but still have access to them if you&#x27;d like.Gentoo&#x27;s community is the most important feature for me though. Gentoo feels like a proper open source project. You don&#x27;t have to be a Gentoo developer to contribute, and you can interact directly with Gentoo developers when you need guidance or have questions. Gentoo&#x27;s community seems to have a lot less elitism compared to other distros also which is very important to me. reply intsunny 20 hours agoprevI shudder to think how many sky high electricity bills and greenhouse gases were released to needlessly compile the same software over and over again. reply Narishma 20 hours agoparentProbably orders of magnitude less than the amount of wasted energy and greenhouse gases emissions of needlessly compiling the same javascript software over and over again on the billions of devices used to access the web. reply tjoff 19 hours agorootparentNot to talk about all the batteries we burn out from it or the billions of devices that is replaced solely because we run so utterly inefficient software. reply mid-kid 20 hours agoparentprevcompared to the average power draw of activities like gaming? probably not a significant amount. reply hylaride 19 hours agoparentprevProbably not that much. It&#x27;s not like gentoo was widely used compared to Red Hat or Ubuntu. Also CPU power usage is a rounding error if the computer was on anyways compared to the spinning HDD, monitor etc. reply weberer 19 hours agoparentprevProbably a fraction of a fraction of 1% of the power used by Microsoft to constantly collect personal data from Windows 10 users 24&#x2F;7. reply bee_rider 19 hours agoparentprevNot to mention the wastefulness of general purpose CPUs. Most computer use is in the browser, why don’t we have a Firefox ASIC yet? reply pantantrant 14 hours agorootparentI&#x27;m happy to be annoucing my Program as a Processor product. Want to run another program? No need, it&#x27;s an app. reply serf 19 hours agoparentprevprobably less watts than the full screen videos and rendered play worlds that are drawn behind the main menu on any given AAA console game bought by millions of people.given the behavior one can only think that software people generally don&#x27;t care unless it bothers a user metric like &#x27;battery life&#x27;. reply phh 20 hours agoparentprevWanna talk about JS? Java? reply dordoka 19 hours agorootparentContrary to popular belief, Java is amongst the most efficient languages regarding power consumption. reply phh 19 hours agorootparentI was referencing directly the amount of compilation, not necessarily the power consumption. But with regards to ecological impact, I would guess you have Java on a server in your mind.But, the biggest Java user in term of number of devices is Android. And every time you install an app, you compile it. Including every time you update it (which nowadays is... everyday?). Also it&#x27;ll recompile in background to use pgo. Nowadays [1] Google could pretty much compile it server-side for most devices, and save a lot of battery wear (batteries wear when getting hot, and compiling, weirdly, heats), in addition to power consumption. reply Zambyte 18 hours agorootparentprevIt depends on how you use it. For long running applications like services, yeah, the JIT will get the bytecode down to some very high quality machine code. If you write lots of small applications and compose them using something like Unix, then Java is very inefficient. reply BSDobelix 19 hours agorootparentprevThat&#x27;s so funny that Java was also the first thing that i had in my mind...also python and \"LLM training\". reply blibble 17 hours agoparentprevnothing compared to msmpeng.exe, which spins several cores constantly 24&#x2F;7(microsoft&#x27;s anti-virus) reply lupusreal 19 hours agoparentprevAs far as hobbies go, it&#x27;s very far from the worst. Some people play games for 12 hours a day on kilowatt gamer PCs, or race their car around a track, or cruise around on a gas guzzling boat, etc. reply goalieca 20 hours agoparentprevWhat’s your standard build look like at work and how Many times a day does it run? Does it go all the way to deploying a dev cluster?? reply sambull 19 hours agoparentprevProbably less then they pay out for some TX based miners to shut down when grandma starts freezing. reply gosub100 19 hours agoparentprevDo you think that many people were even using it and compiling regularly? I left it around 2010 for linux mint. Just last summer I tried it again just out of curiosity, and couldn&#x27;t even get it installed. They had a broken release and of course there was some hack to re-update everything back to the last stable version, which gave me flashbacks of what Gentoo Life was all about and I stopped then and there. reply globular-toast 19 hours agoparentprevI heat my home. The difference between heating via compiling my kernel and heating via whatever heating you use is almost certainly negligible. But I get a custom built system out of it too.I don&#x27;t cool my home, though. reply tmoravec 19 hours agorootparentThe difference between a heat pump and resistive heating is certainly not negligible. reply globular-toast 18 hours agorootparentWell, even if everyone did have heat pumps, let&#x27;s think about it a bit. I upgrade my system about once a week. I reckon on average it&#x27;s about 1 hour of compiling per week but let&#x27;s say 2 to be safe. That&#x27;s no more than 1 kWh per week. A fridge-freezer will use that in a day. An electric oven will use that in 15 minutes. An electric car will use that travelling 3-4 miles. Most households use something like 30 kWh per day to heat. Even if a heat pump made that like 10 kWh it&#x27;s still a drop in the ocean. And don&#x27;t forget the resulting binaries run slightly more efficiently because they are compiled specifically for my CPU, plus I enjoy it. reply wirrbel 21 hours agoprevThere was this sweet spot for a while where Gentoo just worked like a breeze, Where there weren&#x27;t too many and too few useflags and when I would recompile Open Office when my dorm room was too cold in cold winter nights. reply jstummbillig 20 hours agoparentOnly on HN could any state that Gentoo was ever in be reflected upon as \"just worked like a breeze\" unironically and I mean that in a fond, loving way. reply luispauloml 19 hours agorootparentI know it&#x27;s off topic, but I find it interesting how this sentence was so hard for me to understand. I struggled for many seconds but couldn&#x27;t go beyond \"was ever\" because it felt like there was some mistake, like a word was missing here or there.In the first pass, my mind decided that \"state\" was a verb, and, therefore, there should be a subject appearing before it. But I only found \"any\", instead of \"anybody\" or \"anyone\". Then there is \"was ever in be\" which, by itself, is a weird construction. It does makes sense in the sentence, because it is \"[the] state that Gentoo was ever in\" + \"be reflected upon\". But since I was (unconsciously) dividing the sentence in smaller parts trying to identify the subject, the predicate, the verb, the object, or whatever would make sense for me, cutting the sentence like that only confused me even more. I kept going back and forth trying to imagine which word was missing, and only after pushing through until the quotation, the whole sentence finally made sense.Although I can&#x27;t think of any example right now, I know that it is common to use sentences with structure similar to this one, and I see them almost daily, probably multiple times a day. However, as a non-native speaker, this one was an actual struggle, and I feel so good for having overcome it that I am willing comment on it.For closure, if I was the one writing this sentence, I would probably use the active voice with an indefinite pronoun, which is also probably what my mind was expecting: Only on HN would anyone reflect upon any state that Gentoo was ever in as \"just worked like a breeze\".And I ask: were there native speakers that also couldn&#x27;t understand it in a single reading? reply Zambyte 18 hours agorootparentNative English speaker here. The sentence was definitely missing a word. I read it multiple times before I got it too. Your rewrite was easier to understand. reply mgdlbp 16 hours agorootparenta garden path sentence, but it parses.[only on HN] could [any state [that Gentoo was ever in]] be [reflected upon [as \"just worked like a breeze\"] unironically]_,_ and [I] mean [that] [in a fond, loving way] reply trashburger 20 hours agorootparentprevBut it does! Even with the weird packages I have installed (ROCm) and a ton of accept_keywords unmasks, all I have to do is overnight updated twice a month. I haven&#x27;t touched &#x2F;etc&#x2F;portage in months. reply oblio 20 hours agorootparentprevYou could start from a later stage, I think they had a mostly binary stage 3, and emerge was generally solid, albeit slower due to the compilations.So at least once upon a time (10+ years ago) there was this option of just using it as almost another regular distribution.Slackware on the other hand... (I say this is in a bad way, and I think it&#x27;s changed since; for Slackware for anything more complex you had to manage the entire dependency tree yourself, and it was a pain in the neck for anything not part of the not-that-many-regular-packages; nota bene: for the \"beaten path\" Slackware was more or less just another Linux distro, but the \"beaten path\" was quite narrow). reply wirrbel 14 hours agorootparentI have this botched up Debian desktop installation that I rarely use but never quite got around to make a clean install because I need an installation medium for that and i don’t have usb sticks anymore.Do change root installs still work? I would probably give it a try again. reply chpatrick 20 hours agoprevI think NixOS has kind of taken over anything I would use Gentoo for. reply 1oooqooq 20 hours agoparentyou build \"impure\" (in nixos speak) packages for all your software? if not this is not a close comparisson. gentoo shines when you need system wide control of build flags (for perf or security) reply chpatrick 19 hours agorootparentI don&#x27;t know what you mean by impure package. You can modify the build of any nix package and it will rebuild everything just like Gentoo.I don&#x27;t personally believe in the CFLAGS \"performance\" micro-optimization though so that&#x27;s not really something that matters to me. Security is pretty hardened by default on Nix: https:&#x2F;&#x2F;nixos.org&#x2F;manual&#x2F;nixpkgs&#x2F;stable&#x2F;#sec-hardening-flags... reply yjftsjthsd-h 15 hours agorootparentprev> gentoo shines when you need system wide control of build flagsI&#x27;m pretty sure nixos can do that, though?There&#x27;s this for CFLAGs on a single package: nix-shell -p &#x27;hello.overrideDerivation (old: { NIX_CFLAGS_COMPILE = \"-Ofoo\"; })&#x27; --run \"hello --version\"And this page seems to talk about overriding settings for all of nixpkgs, and even something kind of like USE flags (although I agree that it&#x27;s way less powerful and generic than USE flags; that&#x27;s one place Gentoo wins): https:&#x2F;&#x2F;nixos.org&#x2F;guides&#x2F;nix-pills&#x2F;nixpkgs-overriding-packag... reply __MatrixMan__ 18 hours agorootparentprevIt seems like a close comparison to me. The inputs to nix derivations aren&#x27;t always directly named after the build flags they control, but ultimately they control build flags. If you disable the binary cache then you&#x27;re building everything yourself with system wide control of build flags.Purity has to do with whether you let those builds depend on things that found lying around on the system versus things that are explicitly in the derivation (i.e. a pure build can tolerate missing files by building them, an impure one may get stuck for lack of a dependency or because a dependency was not as expected). reply laweijfmvo 20 hours agoprevInstalling Gentoo was always fun for me, to get it working and the dream of a fully custom optimized machine (I guess Arch offers similar experience, minus the full optimization), but getting everything polished afterwards was always just too much for me and I switched to a packaged distro.It seems like it would be cool for an SBC, but the compilation (or setting up cross-compilation) was always too much; now maybe it’s feasible again? But I’m too old to have the time to try! reply bachmeier 20 hours agoparent> minus the full optimizationBack when I used to mess around with this more, I never noticed much speed advantage from compiling my software vs installing binaries. What did help was understanding what was running and cutting out the things I didn&#x27;t need. I&#x27;d be surprised if Gentoo offered any advantages over Arch, Slackware, etc. for that. reply squarefoot 20 hours agoparentprev> It seems like it would be cool for an SBC, but the compilation (or setting up cross-compilation) was always too much;Back in the day I had a few relatively slow machines and used to compile my kernels using distcc to offload the tasks to them. I never used cross compilation but I see it is supported so it may be a possibility for small SBCs.https:&#x2F;&#x2F;www.distcc.org&#x2F; reply bayesianbot 17 hours agoprevSo apparently it would be possible to use mostly binary installations but compile some libraries, interpreters and cpu hogging apps with -march=native - and that wouldn&#x27;t cause any problems with ABI or anything?Would it be hard to set up a build server on another machine? What if that other machine was running Arch or some other system?I remember when running Gentoo ~20 years ago, it was quite often that package compilation failed and I had to go fix some package settings or something, don&#x27;t even remember well except how disappointing it was seeing a failed compilation again after waiting a long time - is this still common occurrence?edit: at least the build server doesn&#x27;t look too bad and the running other arch could be solved with a WM &#x2F; container.. Feeling really like wanting to give it a try, still remember the good times I had with Gentoo way back (even with the compilation problems) reply tommiegannert 19 hours agoprevAny background on why (now)? The post is a bit sparse.(I&#x27;m also one of those who left Gentoo for Ubuntu, both because compilation made it needlessly slow to wait for a tool I needed, and because emerge was just so slow compared to apt. Ebuilds were awesome at a time when dpkg build tooling seemed to change completely once a year.) reply iamthepieman 19 hours agoprevGentoo was the first distro that I grokked. The way they walked you through the build system (honestly can&#x27;t remember if it was official docs, community forum or what) was just the right balance of teetering on the edge of everything crashing and burning, and learning the deep secrets of how an OS actually works that it kept me intrigued for years. I ran it all through college but then got jobs at non-nix (un-nix?) companies and it gradually faded. This seems like a great development but if it had existed when I was exploring Gentoo, I might have taken the easy way and learned much less. reply Zambyte 18 hours agoparent> honestly can&#x27;t remember if it was official docs, community forum or whatNot sure how long this has been true for, but the Gentoo Wiki (which includes the Gentoo Handbook[0]) is an absolute goldmine of information, and is probably what you were looking at if I had to guess.[0] https:&#x2F;&#x2F;wiki.gentoo.org&#x2F;wiki&#x2F;Handbook:Main_Page reply iamthepieman 17 hours agorootparentyes that was it. It really is an example of fantastic documentation. I had tried to run a linux distro that I had bought when I was 16 or 17 (in a box from a store!) but failed to see the appeal. The throw you right in the deep end with plenty of support (good documentation in this case) is still how I learn best and I attribute my experience with Gentoo for helping me find the hacker mindset. Anything is possible if you&#x27;re willing to dig deep enough. reply nottorp 20 hours agoprevInteresting that this is still going on.I haven&#x27;t done it since Slackware came on floppies and you mostly had to recompile your kernel to get the right drivers in.Even did a linux from scratch once, later, to see how it goes. Then went along with my business. reply RuggedPineapple 19 hours agoparentThe Pi (at least 1-4, haven&#x27;t looked at 5) has an extremely weird set up where the GPU actually boots the system using a binary blob before handing it over to the CPU. When mainstream linux was first beginning to support the Pi this caused a lot of issues. One of which was that GPU acceleration was not available. It was in the Pi foundation&#x27;s own distro (Raspbian&#x2F;RaspberryPi OS) but not in others. One of the first to get it working was Gentoo. I really, really wanted to get Open Morrowind working so I tried it for a while.Then I had to re-emerge a software package to change my timezone and I was just done. Thats way more complicated then it should be or needs to be. reply 1oooqooq 20 hours agoparentprevwow. slackware is around since 93 and loadable modules since 95... i always remembered compiling drivers in to save RAM (i.e. i was more removing drivers than adding). But somewhere around kernel 1.2 we did have to compile driver modules in, indeed. reply nottorp 14 hours agorootparentLinux was very new. I was in the last years of high school, 93-95 exactly. I couldn&#x27;t even afford a 386 :)A friend of mine got us access to one at the work place of a relative of his and we spent nights fiddling with Slackware. We even made a serial cable so I could log in via serial from the other computer in the office, which was only a 286. reply krylon 11 hours agoprevI ran Gentoo on my desktop from about 2005 to 2008. The last year of that period I was without Internet access, and when I tried to update the system after being online again, it broke (not surprising, I heard similar stories from users of other rolling release distros since).But it just so happens I set up a virtual machine running Gentoo but a few days ago, with no clear idea of why or what for. What a remarkable coincidence.Gentoo requires the user&#x2F;admin to put in a lot more work than mainstream distros like Debian, but in return you get such a high degree of control and choice that the system, once it&#x27;s up and running, feels more like a pet than a piece of software. reply rany_ 20 hours agoprevDoes that mean they&#x27;re building the package about 2 times and maybe even more if systemd support needs to be there? As far as I know, Gentoo supports musl&#x2F;glibc and could support either systemd or openrc... (some packages link against systemd for some functionality) reply goku12 20 hours agoparentThey do mention that they support 3 different profiles - openrc, gnome&#x2F;systemd and plasma&#x2F;systemd. Nothing mentioned about glibc&#x2F;musl split. But they seem to be aiming only for a reasonable coverage. So I&#x27;d speculate that only glibc is targeted. reply theamk 14 hours agoprev> If you use useflag combinations deviating from the profile default, then you can’t and won’t use the packagesso.. they are basically never going to be used then? It has been a while since I run Gentoo, but I remeber USE flags to be the most useful and fun features of Gentoo, giving it the power other distributions cannot hope to match. I cannot imaging running Gentoo with default USE flags, might as well switch to debian in this case. reply _0xdd 19 hours agoprevAlways have a soft spot for Gentoo. The first distro I ever used was RH 6.1, but Gentoo helped me actually learn how the system works (e.g., partitioning, the FS layout, how a bootloader works, what an init system is, etc.) reply pxc 13 hours agoprevThis mix of a binary cache for common compilation options with transparent fallback to source builds when flags are customized is how the youngest generation of source-based Linux distros (NixOS and GuixSD) work, and it&#x27;s a really nice combination. Congrats to the Gentoo community! I think a lot of people will enjoy this kind of setup. reply jasoneckert 19 hours agoprevBack in the 2000s I would spend immense amounts of time compiling custom Gentoo on old SGI MIPS systems to obtain the best performance. However, even back then I remember thinking that as computing power evolved, there would be a point where Gentoo would either fade into obscurity, or release binary packages to allow users to quickly set up the distribution. As a result, I&#x27;m not surprised by this announcement in the least and half expected it to happen a decade ago. reply znpy 19 hours agoparent> Back in the 2000s I would spend immense amounts of time compiling custom Gentoo on old SGI MIPS systems to obtain the best performance.I&#x27;m curious, did you ran benchmarks? What kind of performance gains did you get, if you got any? reply jasoneckert 19 hours agorootparentI didn&#x27;t run any formal benchmarks back then, but the performance I would get was on par with the fastest x86 and x86_64 machines I had running Linux in the mid 2000s. For example, I would bring an SGI O2 with an R5000 CPU into my classroom running Gentoo with Fluxbox and let the students play with it. They would often comment about how fast it browsed the web and connected to our campus file and RDP servers, and were shocked when I told them the CPU only ran at 150Mhz! reply michaelcampbell 14 hours agoprevDoes no one remember the \"funroll-loops\" parody site? https:&#x2F;&#x2F;www.shlomifish.org&#x2F;humour&#x2F;by-others&#x2F;funroll-loops&#x2F;Ge... reply andix 18 hours agoprevGentoo was great fun, when I was young and had a lot of time. It worked surprisingly well. But I often skipped installing security updates, because it could take days to finish on a slow system.Maybe some day I will get nostalgic and try it out again, but I really don&#x27;t miss it yet. It&#x27;s also quite a waste of CPU power and energy to compile everything from scratch without a real need. reply krmboya 17 hours agoparentIt may look like a waste to compile from scratch but it&#x27;s great for reproducible builds.That can important in cases where you need to be sure you are running the source code that you can see reply totallywrong 20 hours agoprevI remember Gentoo taking more than a full day to compile on my computer back then. It did teach me a lot though, will always have a soft spot for it. reply entropie 20 hours agoparentYeah, A day was pretty normal.I installed gentoo on a workmachine at a new job, like a dual xeon something with 64gb ram back in a time where 8gb ram on a worstation was plenty. I had a blast. It took \"only\" like 3 hours to get my usual to go system.Unfortunately I need haskell on my workstation (xmonad) and ghc can easily take alone like 5-20+ hours compile time on older computers. I compiled ghc a few times on an old t41 - it took more than a day.Still, gentoo is my first choice. I run it on a few root servers and every workstation. reply yjftsjthsd-h 15 hours agorootparentYeah, IME it&#x27;s always a tiny number of packages; IIRC my system only took a day to build because webkit took most of that time, and I didn&#x27;t even try to compile firefox. (In fact, some of my setup retains the expectation of running firefox from a tarball from Mozilla specifically because I used to run gentoo) reply hyperpl 19 hours agoprevI went from Slackware -> FreeBSD -> Gentoo -> ArchLinux and haven&#x27;t looked back except that I now run OpenBSD for my router. reply INTPenis 20 hours agoprevI&#x27;ll never forget my first time building Gentoo, in school in 2003, on the school laptop.I started the build in the evening, and in the morning I waited for the current package to download, then closed the lid, put the laptop in the bag, took the train to school, connected to the school wifi, and continued the build in school.It was fun to try but that&#x27;s all I did, quickly moved on to something more sane. reply freetonik 20 hours agoparentSimilar, I did this in my first year at uni, and I felt invincible. The whole idea of building your own OS distro from parts is the epitome of a so-called IKEA effect. Simply formatting that hard drive and saying goodbye to my “unique” setup (in reality as vanilla as they come) was difficult! reply dec0dedab0de 18 hours agoprevI don&#x27;t use Gentoo, but isn&#x27;t the whole point that you compile everything? Could anyone explain some pros&#x2F;cons of Gentoo other than squeezing out performance for your specific machine? reply lucideer 17 hours agoparentIt&#x27;s got very similar benefits to Arch in terms of system setup and configuration with a few extras:- for the binary version the difference might not be as significant but bit for bit I&#x27;d say it&#x27;s still more configurable than Arch- in terms of choices they&#x27;ve made about conventional system defaults, there&#x27;s more interesting options on offer: especially when it comes to Systemd -vs- OpenRC & networkmanager -vs- netifrc- I&#x27;m not sure how true this last point is, but I get the impression the system of overlays & profiles is a little more expressive & powerful than equivalents in other distros. E.g. Manjaro is often considered to be something of bastardisation of Arch by virtue of some underlying design decisions made in \"forking &#x2F; extending\" it, & compatibility is limited. On the other hand things like Funtoo & Pentoo are really just Gentoo at heart, using its core features for packaging distro customisations. reply jaegrqualm 17 hours agoparentprevIt offers more customization than Arch and the like, and it allows you to fix bugs you might find annoying more quickly than maintainers might. This is in addition to some nebulous performance gains from optimizing the builds you compile yourself.The problem has always been that while you had all this choice, the one choice you didn&#x27;t have was to just use regular old binary packages for the things you didn&#x27;t have to customize. This complaint has finally culminated in TFA. reply kristianpaul 18 hours agoprev“ To speed up working with slow hardware and for overall convenience, ” reply nsagent 20 hours agoprevI tried Gentoo Prefix for a while on macOS, so I could use the same package manager across OSes, but always compiling from source got a bit tiresome on a laptop, so I went with MacPorts instead. reply nfeutry 20 hours agoprevWondeful, that means that you can use binary for most part of the system and just rebuild the parts you want to customize. A great saving of compile time ! reply Projectiboga 18 hours agoprevIsn&#x27;t this just over three months early?But this",
    "originSummary": [
      "Gentoo Linux, a source-based distribution, now provides binary packages for download and installation alongside source-based packages.",
      "The binary packages are available for amd64 and arm64 architectures and offer a wide range of software.",
      "Users can configure their existing Gentoo installation to use the binary packages by creating a configuration file.",
      "Binary packages are compiled with specific settings for different architectures and profiles.",
      "The packages are cryptographically signed, and users can verify the signatures before installation.",
      "Binary packages can be used with a merged-usr system but not with other profile versions.",
      "Currently, Gentoo does not have plans to offer binary packages for ~amd64.",
      "Users are encouraged to report any issues or bugs related to the binary package support."
    ],
    "commentSummary": [
      "Gentoo Linux is a distribution known for its flexibility and customization options, with strong community support.",
      "The article compares Gentoo to other popular distributions like Debian, Arch, and Ubuntu, discussing factors such as hardware support and ease of maintenance.",
      "There is a discussion on software compilation, gaming performance, efficiency, environmental impact, and the role of customization and configurability in operating systems. Overall, opinions about Gentoo vary, but it is generally praised for its customization options and educational value, although some find it time-consuming or prefer alternatives."
    ],
    "points": 319,
    "commentCount": 257,
    "retryCount": 0,
    "time": 1703851979
  },
  {
    "id": 38804603,
    "title": "Seeking Advanced Programming Courses for Experienced Coders: Recommendations for Learning Elixir",
    "originLink": "https://news.ycombinator.com/item?id=38804603",
    "originBody": "Lately I&#x27;ve been learning Elixir. Many of the popular resources seem beginner-focused, which feels a bit tedious when you&#x27;ve been coding for over a decade. Are there any popular programming books or courses for more experienced coders?",
    "commentLink": "https://news.ycombinator.com/item?id=38804603",
    "commentBody": "Programming Courses for Experienced Coders?Hacker NewspastloginProgramming Courses for Experienced Coders? 314 points by trwhite 20 hours ago| hidepastfavorite153 comments Lately I&#x27;ve been learning Elixir. Many of the popular resources seem beginner-focused, which feels a bit tedious when you&#x27;ve been coding for over a decade. Are there any popular programming books or courses for more experienced coders? onetimeuse92304 18 hours agoThere is a general lack of advanced materials in software engineering. This is one area where market economy works against common good, unfortunately. People who have the knowledge and want to capitalise on it are highly incentivised to preparing beginner level materials and maybe a tiny bit to prepare the something for people who already have a bit of knowledge. After that, the number of people who are your potential target drops off sharply and you would have to increase the price dramatically. And it happens that people will not buy a $200 book even if it had $100k worth knowledge in it.There are some positions available though. For example, I recently found Let over Lambda by Doug Hoyte. Excellent book if you are that kind of programmer.Another problem with advanced materials is that frequently people are even unable to recognise them as advanced or valuable. It is easy to look at and evaluate things you have experience with, it is much more difficult to impossible to do the same with stuff that is beyond your experience (also called The Blub Paradox: https:&#x2F;&#x2F;www.paulgraham.com&#x2F;avg.html).Personally, I just read a lot. I try to keep an open mind and even if I don&#x27;t agree fully with the contents of the book, I try to use the ideas as inspirations for my own thinking. You can read codebases written by other people and learn new ideas. Over time, you gather a library of solutions to problems and knowledge of when to and when not to apply them.If you do enough of it, you will find useful knowledge in unusual places. reply nerdponx 16 hours agoparentThe problem with a $200 book is that it&#x27;s hard to tell the difference between a book that contains $100k of knowledge and a book that contains $100 of knowledge. reply onetimeuse92304 14 hours agorootparentHey, here is a thought I realised while buying yet another cookbook. If you buy a cookbook and find ONE recipe that you will like and will be using regularly for the rest of your life, that cookbook was already worth it.Even if you found zero recipes, that does not mean you wasted time. Maybe if you think in terms of results for that particular cookbook investment, but it is pretty arbitrary way to look at it. What if you tried to measure your return for investment on a single page of a cookbook? For half a dozen coobkooks? For a shelf of cookbooks?If you continue buying and studying cookbooks, you will find some good recipes in some cookbooks and what you need to look for is the total return on the total of investment. I spent a ton of money on cookbooks but my life is much better for it and I have zero regrets even if many of these cookbooks yielded zero results.So now back to technical books.Some of these books will yield new knowledge and some may yield very little or even none at all. You may not know before you buy a book whether it will pay for itself, but you know if the whole business of buying books and learning from them is worth it. I chose to buy and read books even if some of them yield little results because I think I am a better person and a better developer for it. I learned a lot of useful stuff even if I completely ignore majority of everything I read and then forget majority of everything I thought valuable. reply emmo 13 hours agorootparentThis sort of seems to assume that time and money in infinite, though. Buying a recipe book for one recipe seems like a profound waste of resources. reply onetimeuse92304 12 hours agorootparentThis sort of seems to assume that it is at all possible to gain knowledge and experience without making mistakes.Hiring people who do not perform seems like a profound waste of resources. Doing projects that fail seems like a profound waste of resources. Learning technology that will not be helpful later in your life seems like a profound waste of resources.When was the last time you found a cookbook where a significant portion of recipes stayed with you for the rest of your life? reply thorin 13 hours agorootparentprevYou&#x27;ve just reminded me that o reilly used to publish quite a lot of cookbooks, I think there was a bash one, perl, python etc. That kind of book would probably be quite useful for getting advanced concepts I guess as a lot of more advanced books are quite abstract. reply quickthrower2 3 hours agorootparentprevNo programming book gives me $100k of knowledge. Let’s say the knowledge is 10% of the gain and 90% sweat. I need $1m in lifetime value I couldn’t have got otherwise. If it has that sort of value you build a business around that tech! See OpenAI for an example! reply johnnyanmac 14 hours agorootparentprevideally that&#x27;s what reviews and word of mouth are. But I imagine like other sectors that book publishers are risk averse.But if anyone is like me and interested in taking a risk on those $80-150 tomes, CRC Press&#x27; books have been more hits than misses for me. reply worthless-trash 2 hours agorootparentDo you have any recommendations ? reply psadauskas 15 hours agoparentprev> People who have the knowledge and want to capitalise on it are highly incentivised to preparing beginner level materials and maybe a tiny bit to prepare the something for people who already have a bit of knowledge. After that, the number of people who are your potential target drops off sharply and you would have to increase the price dramatically.I think, even aside from the incentives, its a matter of interest and time. Earlier in my career, when I was learning at a rapid pace, I also had the time and motivation to write blog posts about what I discovered, as did many of my peers. Its also easier to write about beginner concepts, because they&#x27;re simpler.As I&#x27;ve advanced in my career, the things I&#x27;m learning and doing are more and more complicated, and&#x2F;or require much more prerequisite knowledge. Not only do I have less time, there&#x27;s also a corresponding increase in difficulty in writing concisely and coherently about the more advanced concepts.About once a week I read a blog post in a field I&#x27;m familiar with, and recognize it as something I myself would have written 10 years ago. But I also now see in it the potential problems and pitfalls, and tweaks to make to avoid them. I think to myself \"I should write a blog post about that\", and sometimes even jot down some notes or a draft, but the topic is just so complicated, and the potential problems so subtle, that it takes a large amount of effort just organizing it, and I get bored&#x2F;distracted and move on, never finishing it.Talking to those same peers now, they feel much the same. And even with the experts that do manage to write about expert topics, that&#x27;s become their full time job. Then after a few years of full-time writing instead of full-time doing, I find when they write about a topic that I&#x27;ve been full-time doing while they were writing, there&#x27;s gaps in their knowledge or new techniques and tradeoffs they&#x27;ve missed.I don&#x27;t know how to solve this problem, except maybe figure out a way to give the knowledgeable experts a sabbatical or something, where they can take a break and write. Good luck getting a startup to agree to that, though. reply johnnyanmac 14 hours agorootparentThat seems to be why the most popular advanced topics are conference talks instead of books. Still takes prep and you lose the quality of a tome, but an hour long talk (possibly with decently annotated slides for later viewings) is probably better worth for the speakers and audiences alike. conference foots the bill with the costs gathered from participants, participants get a variety of talks (so they aren&#x27;t paying for one tome but maybe a dozen nuggets to serve as jumping off points. Which may be all an advanced participant needs), and the industry gets advertising&#x2F;potential recruiting from the staff they dispatch.Of course, I&#x27;m sure I don&#x27;t need to explain the downsides of a talk, Especially one presented by very knowledgable people but not ones who are necessarily expert speakers. But at the same time, that lack of polish may be a light in the darkness that is false platitudes in every other ad. reply tcmart14 13 hours agoparentprevFor me I think it is a little different. There are tons of beginner material and even quiet a few advanced materials, I find the middle (or intermediate) materials lacking. For example, with rendering. Tons of great content with indepth explanations of how to draw a triangle. You can also find lots of good tutorials about advanced procedural generate techniques, but they (correctly) assume a lot of knowledge that is above the simple task of rendering a triangle. There isn&#x27;t a lot of content to help you get from rendering a triangle to the knowledge you need for some advanced rendering techniques. reply bityard 12 hours agoparentprev> And it happens that people will not buy a $200 book even if it had $100k worth knowledge in it.It is not at all unfashionable to pay $50k for a master&#x27;s degree, however. reply wmil 8 hours agorootparentTurning a masters degree into a pay bump is straightforward. But reading books won&#x27;t get you hired at a new job or even get you a raise at most places. reply LouisSayers 16 hours agoparentprev> people will not buy a $200 book even if it had $100k worth knowledge in it.I remember paying crazy amounts for books at university and wouldn&#x27;t have been surprised to have to pay $200 for a book. As it was I bought mine second hand to save on cost.When people can expense such items then I also imagine a $200 book selling well when properly targeted and marketed.What sells even better though are courses, and there are many courses out there aimed at corporates which are in the thousands of dollars so again, $200 doesn&#x27;t seem crazy at all in comparison. reply dmoy 16 hours agoparentprev> There is a general lack of advanced materials in software engineeringI wonder how much this is true in other areas (not counting academic papers).It&#x27;s not exactly true in law due to the plethora of giant treatises in any specific subject of law, as far as I know that might be a one-off due to US law putting such an (insane?) emphasis on precedent. Basically need a professional lawyer+historian to compile all the relevant stuff.But other engineering or science disciples? I have no idea one way or the other. reply wheelinsupial 10 hours agorootparentI think it depends on what you mean by advanced and what exactly you&#x27;re calling engineering.As you&#x27;ve pointed out in other engineering disciplines, there are researchers who are pretty deeply immersed in the academic papers and help to move the state of the art forward.In other cases, there are the societies like ASME, IEEE, ASCE, etc. that put out standards or guidelines. These could either be viewed as documents that you use to plug numbers into formulas or you can try to derive the equations from first principles. Often there are training courses or workshops run by these societies, so that&#x27;s a place to learn. There are trade publications to learn from as well. reply elicksaur 14 hours agorootparentprevNot sure a treatise is exactly a good comparison. That would be more like documentation, where the question here is more “how to get better at programming?”. A treatise isn’t going to tell you how to be a better lawyer.I’m not sure there’s great content like this for lawyers either even with mandated continuing education requirements. They’re typically a bore and surface level. About the same quality as your average “What’s new in Elixir 1.15” blog post. reply aleph_minus_one 16 hours agoparentprev> And it happens that people will not buy a $200 book even if it had $100k worth knowledge in it.If this were the case, such a book would be an immediate buy for me. The problem is that you will have to show me that such a book will bring me this value in my culture&#x2F;country (i.e. not USA) with a very high likelihood - this will be very hard. reply someguy5281 14 hours agoparentprev> Personally, I just read a lot.What books would you recommend? I find it hard to learn real world patterns and codebases.Smaller patterns like inheritance, composition, traits, functional monads, async coroutines, RAII, MVVM, dependency injection, and semaphore synchronization, are easy to learn, but seemingly unhelpful in the grander scheme. Nobody seems to completely adhere to the rules. As a result, these small patterns never seem to lead to clean code.I loved coding as a child and pursued CompSci in college. But having worked a few years in the industry, I really think I do not know how to solve anything. At one point I felt as if everyone is gaslighting me. But if multiple people from different companies say I need to learn, then I should.Right now I want to read to know what exactly I am missing in my knowledge about how code works in large codebases(Edit: Thanks for the reply!) reply onetimeuse92304 14 hours agorootparent> (...) are easy to learn, but seemingly unhelpful in the grander scheme. Nobody seems to completely adhere to the rules. As a result, these small patterns never seem to lead to clean code.Good. That is very useful state of mind to be in.Because these things do not produce cleaner code. They are tools to solve problems.Coders are like photographers. The ones who like talk the most shoot walls head on and then peep at pixels and mostly discuss hardware. The ones who know how to make good photos do not care about who uses what kind of camera. A camera is just a tool to capture light field.Clean code is what happens in your mind and as a result of your mental process.Patterns, composition, inheritance tricks, and so on -- these are things you add to your toolbelt to use it when it is needed. You should continue adding tools to your toolbelt to be able to solve wider variety of problems and have more alternative solutions. But on its own, more tools do not mean mastery and do not lead to clean code.If anything, more tools without knowledge how to use them leads to less readable code. This happened to me for the first half of my career..What get you to mastery is by building taste for simple code and ability to self reflect on your work. Taste for simple code means you know why code needs to be simple and you know how clean code looks like.Self reflection is trying to be objective and judgy about your results. When you write the code, you keep refactoring until you like the results. You write a module, then you polish it until there is nothing else to remove and it does exactly what it is supposed to be doing. You do it long enough and you will have experience to start it closer to a clean result. You may never get to be able to write clean code on the first try but you might get better at having much cleaner first approximation.Ideally, you also take some responsibility longer term for what you produced. You compare your initial ideas with how they fared over time. You learn some things you thought smart weren&#x27;t so smart at all. You learn what works and what doesn&#x27;t work in general.There is one more way and this is mentoring. I do pair programming with people where we design and implement solutions to progress. Lets me get to know the person better and the person gets to observe the process which can be a huge shortcut in the process of learning to write clean code. reply ahoka 11 hours agorootparentThe next step is to realize that there is no clean or unclean code. reply jansimonek 12 hours agorootparentprevI found the book Growing Object-Oriented Software Guided by Tests to be very inspiring. The book demonstrates how to evolve the code architecture, how and when to add new concepts and abstractions, how to listen to the tests, when to refactor etc. The tech in the examples is dated, but the gist of the book is very relevant.Another classics are: - Working effectively with legacy code - Refactoring: Improving the Design of Existing CodeAll of these books have a lot of depth to them reply corethree 15 hours agoparentprevI highly disagree. There&#x27;s a huge amount of material out there for advanced software engineering. The problem is this material is exponentially harder and most programmers aren&#x27;t interested in this stuff.What OP is talking about is essentially learning a new language which is sort of trivial and not advanced. It&#x27;s sort of a parallel track and he likely wants something that doesn&#x27;t start at the very beginning. Something called: \"Learning elixir for experienced programmers\". This kind of thing isn&#x27;t \"advanced\". It&#x27;s more like \"convenient\". Most programmers spend there entire careers in this zone just learning new languages and new technologies. They are sort of moving horizontally through the field rather then upwards.True advanced stuff is hard af, and the more advanced you get the less and less relevant it is to your job. reply amadeuspagel 13 hours agoparentprevIs this a problem in other fields? reply spicemonger 18 hours agoprevI&#x27;ve taken 2 of David Beazley&#x27;s courses.[1] And, I highly recommend them. If you haven&#x27;t seen some of his talks, he&#x27;s very good at explaining things by building them from nothing.I took 2 courses: \"Rafting Trip\" and \"Write a Compiler\". Both were awesome. The Rafting Trip took us through implementing the Raft consensus algorithm from scratch. And the \"Write a Compiler\" course had us build a small language using LLVM.Both courses (but especially the Rafting trip one) were definitely for experienced programmers. In the courses I took, people generally had at least 5 years of professional work. And even still, there were a few people that really struggled to stay on pace in the course.But at the end, most people had a (kinda) working Raft library or compiler![1] https:&#x2F;&#x2F;dabeaz.com&#x2F; reply yla92 2 hours agoparentHave taken two of his courses (Compiler and SICP) and highly recommend them. The only thing that is stopping me from taking more is I live at the other end of the world and have to stay awake the whole night for 5 days. reply pajep 15 hours agoparentprevI enjoyed the \"Rafting Trip\" as well, side tracking a bit, anyone have course recommendation for ML ranking and ML recommendation courses or probabilistic data structures and algorithm (for example, bloom filter, Freivalds&#x27; algorithm that make multiplying matrix to O(n^2) etc.... etc)? reply TheAlchemist 3 hours agoparentprevI&#x27;ve taken 1 of his course, and I will definitely take more.As you say, he is very good at explaining things - it&#x27;s hard to describe, but it just clicks. Highly recommended.I was a bit worried about the online aspect, but there is a very good flow actually. reply zenburnmyface 12 hours agoparentprevI&#x27;ve also taken David&#x27;s compiler course, and it was excellent. I highly recommend taking any course he offers that interests you. reply notdonspaulding 13 hours agoparentprevI came here to mention Dave Beazley&#x27;s courses and talks.In particular, I recently prepped&#x2F;ran a week-long, in-house training session of Dave&#x27;s Python-Mastery[1] course at my day job. We had a group of 8 with a mix of junior and senior Software Engineers and while the juniors were generally able to follow along, it really benefited the senior SEs most. It covers the whole language in such depth and detail that you really feel like you&#x27;ve explored every nook and cranny by the time you&#x27;re done.[1] https:&#x2F;&#x2F;github.com&#x2F;dabeaz-course&#x2F;python-mastery&#x2F;(I enjoyed teaching the class so much that I&#x27;ve considered offering my services teaching it on a consulting basis to other orgs. If that interests anyone, feel free to reach out to the email in my profile.) reply ilrwbwrkhv 7 hours agoparentprev+1 to this. Back in the day I read his Python cookbook to get into ML and have found only a few other books since then with that quality. reply ayhanfuat 16 hours agoparentprevHow was the participation (number of people, interaction between the instructor and participants and interaction among participants)? reply zjmil 16 hours agoparentprevI have also taken the \"Rafting Trip\" course and enjoyed it. Second that it is for more experienced developers. reply najmlion 16 hours agoparentprevSold out :( reply cvhashim04 16 hours agoparentprevThank you, looking into it reply anonymoushn 17 hours agoprevHello, recently I&#x27;ve enjoyed Casey Muratori&#x27;s Performance-Aware Programming course[0]. You could read Algorithms for Modern Hardware[1] to learn similar set of stuff though. Casey&#x27;s course is aimed at bringing beginners all the way to a nearly-industry-leading understanding of performance issues while the book assumes a bit more knowledge, but I think a lot of people have trouble getting into this stuff using a book if they don&#x27;t have related experience.I&#x27;ve also found Hacker&#x27;s Delight Second Edition[2] to be a useful reference, and I really wish that I would get around to reading What Every Programmer Should Know About Memory[3] in full, because I end up reading a bunch of other things[4] to learn stuff that&#x27;s surely in there.[0]: https:&#x2F;&#x2F;www.computerenhance.com&#x2F;p&#x2F;welcome-to-the-performance...[1]: https:&#x2F;&#x2F;en.algorithmica.org&#x2F;hpc&#x2F;[2]: https:&#x2F;&#x2F;github.com&#x2F;lancetw&#x2F;ebook-1&#x2F;blob&#x2F;80eccb7f59bf102586ba...[3]: https:&#x2F;&#x2F;people.freebsd.org&#x2F;~lstewart&#x2F;articles&#x2F;cpumemory.pdf[4]: https:&#x2F;&#x2F;danluu.com&#x2F;3c-conflict&#x2F; reply zengid 15 hours agoparentI was going to say check out something from Casey too. Maybe just randomly jump into one of the 600+ videos for handmade hero and try to see what he&#x27;s doing. I think Casey is a great educator, even if he has a lot of hot takes. reply krapp 7 hours agorootparentIt&#x27;s worth pointing out that Handmade Hero is not an example of how anyone should approach making a video game, and even Casey would say that, since that isn&#x27;t his goal.Use libraries. Use frameworks. Don&#x27;t use C++ but reimplement the stdlib because you hate C++ and assume it was designed by morons. Don&#x27;t spend... how many years has it been now, nearly ten? On a debug room that looks like a high school CS project. reply ilrwbwrkhv 7 hours agoparentprevCasey Muratori and Jonathan Blow are some of the greatest minds countering the whole programming movement which creates sub par software imo. reply dyarosla 3 hours agorootparentWhile I like some of their opinions they do pontificate an awful lot for folks who haven’t delivered what could be considered “above par” source code- the stuff in their streams is pretty darn messy. reply imjonse 19 hours agoprevPeter Norvig&#x27;s programming course from 10 years ago. It says there \"no experience required\", but it&#x27;s intermediate-advanced actually.https:&#x2F;&#x2F;www.udacity.com&#x2F;course&#x2F;design-of-computer-programs--... reply dpflan 19 hours agoparentIt is also very interesting to watch an accomplished computer scientist write code and solve problems. reply cobertos 18 hours agoprevI&#x27;m still partial to LearnXinYMinutes[0]. It&#x27;s how I learned enough MatLab&#x2F;Octave in a couple hours to test out of an intro CS course.Usually I just use that site in addition to the official tutorial when a concept really stumps me.Here&#x27;s their article on Elixir[1][0]: https:&#x2F;&#x2F;learnxinyminutes.com[1]: https:&#x2F;&#x2F;learnxinyminutes.com&#x2F;docs&#x2F;elixir&#x2F; reply archsurface 16 hours agoparentYou consider this to be \"for experienced coders\"? reply nerdponx 16 hours agorootparentYes. It&#x27;s not useful for beginners. It&#x27;s a good way to get a quick introduction to the syntax and idioms of a language you&#x27;re unfamiliar with. reply soultrees 18 hours agoparentprevWhat an amazing resource. Thank you for this. And even more thanks to whoever put this together. reply culi 17 hours agorootparentThe project was created and is maintained by Adam Bard, but is open sourced with over 1.7k contributors since 2013https:&#x2F;&#x2F;github.com&#x2F;adambard&#x2F;learnxinyminutes-docs reply soultrees 17 hours agorootparentHaha yeah, I saw that after. In that case, great job everyone! reply sbarre 19 hours agoprevWhen I want to take the next step with a new language, I usually find a popular or interesting framework&#x2F;library for that language, and I do a deep dive into the codebase.Set it up for local development, then step through calls with a debugger, look at the whole call&#x2F;request lifecycle, start to tinker with the subsystems..It&#x27;s a great way to see the real-world usage of all the core concepts you&#x27;ve been introduced to in the intro&#x2F;learning courses or tutorials, and be exposed to more advanced patterns.For Elixir, the obvious choice would be Phoenix if you&#x27;re on the web app side of things.. reply carom 15 hours agoprevThis is something the computer security industry is great at [0][1] that the software industry really needs to catch up on. I would love a weekend to week long course on writing an SMT solver, building a 2D game, training a neural net on custom data, modern C++, peer to peer networking, integrating the lightning network into an app, building a darkweb application, etc. I have not seen many options for that.Two free resources are Karpathy&#x27;s neural net zero to hero [2] and Gamozolab&#x27;s fuzz week [3]. Even just recordings of people who are top in their domain writing code are so useful.0. https:&#x2F;&#x2F;ringzer0.training&#x2F;index.html1. https:&#x2F;&#x2F;www.blackhat.com&#x2F;tr-24&#x2F;training&#x2F;schedule&#x2F;index.html2. https:&#x2F;&#x2F;karpathy.ai&#x2F;zero-to-hero.html3. https:&#x2F;&#x2F;youtube.com&#x2F;playlist?list=PLSkhUfcCXvqH2sgJa8_gnB41_... reply __mp 19 hours agoprevWhen writing a small application in Go I had great success asking Chat GPT specific questions. It helped me figure out which packages I needed to use and how to interact with them.I understand that this might not fit your use-case, but it&#x27;s worth a try. Just be aware that it tends to hallucinate APIs. reply bidandanswer 19 hours agoparentIf it does hallucinate, you can just paste it the latest documentation and the hallucination rate goes to nearly zero. reply reactordev 19 hours agorootparentOr it apologizes profusely, tells you you’re correct, and fails to fix its reasoning or the examples it gives you. reply davidw 18 hours agorootparentI was trying to get it to write some Elixir code to handle Erlang B calculations and it just couldn&#x27;t figure it out, but it was pretty confident with the wrong things it was printing out. reply bidandanswer 18 hours agorootparentThe rust borrow checker often requires too much critical thinking for ChatGPT, as well.LLMs confer a tremendous productivity boost. You just have to understand their limits and know when it&#x27;s faster to think through and write the solution yourself. reply fauigerzigerk 19 hours agorootparentprevI told it to stop apologising all the time. It apologised for doing that :) reply Zambyte 18 hours agorootparentDid you also try telling it what it should do instead? reply fauigerzigerk 17 hours agorootparentNo, because there is nothing I wanted it to do instead. Perhaps what I should have done was to change the system prompt to something like \"You are a helpful assistant that will admit its mistakes but never apologise for them.\" reply ZeroClickOk 14 hours agorootparentprevprobably chatgpt is Canadian :) reply block_dagger 18 hours agorootparentprevSounds like you need to add some custom instructions to get the tone, accuracy, and concision you want. reply JimBlackwood 19 hours agorootparentprevDoes it actually consume the documentation? You could (haven’t tried in a while) also have ChatGPT tell you anything you wanted aslong as you stated “It’s been posted on Wikipedia after your last consumed date”.I feel like telling it that it’s wrong and linking documentation is the same as referring to Wikipedia? reply dpflan 10 hours agorootparentprevI mean you can just run the code and verify its functionality, right? I guess you could be in trouble if you get functioning hallucinations and try to build more around a false foundation. reply justin66 19 hours agoparentprevThe person asking the question seems to be looking for a course or textbook with which to challenge themselves.What you&#x27;ve described is the opposite of that. reply __mp 18 hours agorootparentI understand. But as a long-time programmer I like to explore new programming languages or APIs by working on a small project. I start with a first prototype to get the feel of the language, before I look into extended documentation. Chat GPT helps with this use-case.It&#x27;s also more fun to have something which can be easily extended than to start from scratch. reply adonese 19 hours agoparentprev>Just be aware that it tends to hallucinate APIs.It gives you nice little hints and a different perspective on your api design itself. I&#x27;d have a chat with chatgpt and sometimes I&#x27;d miss to share specific code and it just assumes or hallucinate on that. reply sltr 18 hours agoprevIt&#x27;s not Elixir, but I&#x27;m working through Jeremy Koppel&#x27;s \"Advanced Software Design Course\". I&#x27;ve been coding for 22 years and it is definitely growing me.https:&#x2F;&#x2F;www.mirdin.com&#x2F; reply miguendes 14 hours agoparentThat looks interesting. If you don’t mind me asking. What did you learn there that you couldn’t learn from books or talks? reply sltr 11 hours agorootparentSame difference between reading a textbook vs taking a lecture. He brings it to life. There&#x27;s also personal feedback. His content isn&#x27;t available in any other format I know of.Do have a look at the testimonials. He hits the nail on the head on many ideas I could intuit but didn&#x27;t have words for.I don&#x27;t know of any books or speakers to compare him to. He&#x27;s a unique bridge between academia and industry. I suppose you could say his teachings are available in the many academic papers he sources, but those papers are dense. His curation and contextualizion gets the ideas in a form I can comprehend and apply at work. reply tejohnso 15 hours agoparentprevI&#x27;d also recommend Mirdin. I did a short interactive course a while back, and I&#x27;m considering doing the full interactive course soon. reply kilroy123 18 hours agoparentprevI took his course as well. It is indeed advanced. I certainly learned a fair amount from it. (coding for 15 years) reply krilcebre 17 hours agoparentprevIs it programming language agnostic? reply 100k 16 hours agorootparentYes. The concepts apply to all programming languages. The exercises use multiple programming languages. For example, one exercise has you explore the Git source code (which is in C) to learn how to use a program&#x27;s data structures to understand an unfamiliar codebase. Other examples have you find bugs in Python code or write Java. reply AlchemistCamp 19 hours agoprevThe more advanced a book or course is, the less popular it can be. What are you looking for in a resource that you don’t get from the docs? Knowing that might help narrow down the recommendations.If you’re looking to understand what makes Elixir different from the most popular languages and how OTP works, I’d suggest Elixir in Action: https:&#x2F;&#x2F;www.manning.com&#x2F;books&#x2F;elixir-in-action-third-editionIf you’re already familiar with that, then take a look at the books from Pragmatic Bookshelf. They have quite a few books that cover different aspects of Elixir development including a recent ML-focused one: https:&#x2F;&#x2F;pragprog.com&#x2F;categories&#x2F;elixir-phoenix-and-otp&#x2F; reply Q6T46nT668w6i3m 19 hours agoprevThe obvious “next step” is experiential, i.e., write complicated programs. Books about writing compilers, databases, operating systems, emulators, file systems, etc. are all useful. reply whatamidoingyo 19 hours agoparentDo you have any book recommendations for those topics? reply NukedOne 2 minutes agorootparentBuilding githttps:&#x2F;&#x2F;shop.jcoglan.com&#x2F;building-git&#x2F;Build a blockchain from scratch in gohttps:&#x2F;&#x2F;web3coach.gumroad.com&#x2F;l&#x2F;build-a-blockchain-from-scra...Compiling to assembly from scratchhttps:&#x2F;&#x2F;keleshev.com&#x2F;compiling-to-assembly-from-scratch&#x2F;If anyone knows of similar books, don&#x27;t hesitate to comment below. reply blueblueue 13 hours agorootparentprevI really enjoyed http:&#x2F;&#x2F;www.emulator101.com&#x2F;. It is a hands on tutorial on how to write an emulator for an Intel 8080 CPU that can run the original Space Invaders. The code examples are in C, but you can find implementations in other languages on GitHub as well. reply hiyer 18 hours agorootparentprevCrafting interpreters [1] is one such, where the author walks you through writing an interpreter for a toy language. The book uses Kotlin IIRC, but you can write the interpreter in any language of your choice.1. https:&#x2F;&#x2F;craftinginterpreters.com&#x2F; reply Jtsummers 18 hours agorootparentIt uses Java and C. Strictly speaking there&#x27;s no reason you have to use either language, you can also use the design in the language of your choice (assuming sufficiently matching semantics, like C# or Kotlin instead of Java for Part 1, or a willingness to put in more legwork if you go further afield). reply hectormalot 17 hours agorootparentprevThere is a similar crafting an interpreter[1] and compiler[2] pair of books for Go. I&#x27;ve heard good things about it, didn&#x27;t work through it myself though:1. https:&#x2F;&#x2F;interpreterbook.com2. https:&#x2F;&#x2F;compilerbook.com reply Derbasti 16 hours agorootparentprevMan, that book was so good! What a tremendous achievement in clarity and insight. I loved it! reply __mp 18 hours agorootparentprev\"Computer Systems: A Programmer&#x27;s Perspective\" is a great introductory systems textbook on everything going on in a computer.For more advanced C&#x2F;C++ developers I can recommend \"The Linux Programming Interface\" - (https:&#x2F;&#x2F;man7.org&#x2F;tlpi&#x2F;). It is a GREAT compendium for writing code against the POSIX API. I have a (legal) copy of this book on all the computers I have access to.\"The Database Redbook\" - (http:&#x2F;&#x2F;www.redbook.io) is a valuable source on database implementations.\"Compilers: Principles, Techniques, and Tools\" (Dragon Book) is a great starting point. The book is out of print and but I think it should still hold up for most basic use-cases. reply 3pm 13 hours agorootparentprevThis is a really good fundamentals resource: https:&#x2F;&#x2F;teachyourselfcs.com&#x2F; They list books and videos. reply whalesalad 18 hours agoprevI don’t have any general course recommendations but the strange loop conference videos have been the most inspiring for me. I hope to attend soon, and a life goal of mine is to have something interesting enough to present there.For Elixir, Dave Thomas’ coding gnome “elixir for programmers” course skips all the menial BS about “what is a string” etc and goes right into the meat and potatoes of leveraging the language correctly. I loved it. reply codewithcheese 4 hours agoparentStrange loop, one of the few conferences where videos from 10 years ago are still relevant today, will be missed.Retrospective https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=suv76aL0NrA reply __oh_es 17 hours agoparentprevPossibly mistaken but from what I understand 2023 was the last year of Strange Loop - I was hoping to attend 2024’s reply gorjusborg 17 hours agorootparentWas there bot enough demand for the content? It seems like the strange loop content was often top notch. reply whalesalad 17 hours agorootparentprevI am going to pretend I didn’t see this =‘( reply epiccoleman 17 hours agoparentprev+1 for Dave Thomas&#x27;s course. Very refreshing and fun. reply eatonphil 18 hours agoprevDavid Beazley teaches weeklong intensive courses on Raft and interpreters and SICP. Good use of a corporate education budget.I have no affiliation with it I just love the idea and hope to see more like this.https:&#x2F;&#x2F;www.dabeaz.com&#x2F;courses.html reply angarg12 17 hours agoprevAs others have said, expert-level material tend to not lend itself well to a book format.Instead I find a lot of advance level knowledge is usually shared in tech blog posts and tech talks. The quality of these tends to vary a lot, so you need a bit of curation and due diligence. I myself found the videos from InfoQ very useful with 15 yoe (particularly in the software architecture track).And if you want the really advanced stuff, you can also read papers. The barrier to entry is higher and sometimes is difficult to connect them to your everyday problems, but it doesn&#x27;t get more cutting edge that than. reply dceddia 12 hours agoprevFor Elixir specifically, I liked Dave Thomas’ Elixir for Programmers. https:&#x2F;&#x2F;codestool.coding-gnome.com&#x2F;courses&#x2F;elixir-for-progra...For learning about performance and low level stuff, Casey Muratori’s Performance Aware Programming course has been great. https:&#x2F;&#x2F;computerenhance.comFor higher level software design stuff (beyond just GoF patterns), check out James Koppel’s writing at https:&#x2F;&#x2F;www.pathsensitive.com&#x2F; and his courses at https:&#x2F;&#x2F;mirdin.com reply jugjug 15 hours agoprevLately, I have been struck by pragmatism of approaches discussed in https:&#x2F;&#x2F;clojuredesign.club&#x2F;For example, one can start separating pure functions from side-effects, aiming for aggressively minimal side-effecting functions. Applying this approach to calling REST APIs, one would first create a full request (including the http method, body, headers, etc) in a pure function, and only then send that request to the wire. With such a split, one can easily unit-test the request construction (it&#x27;s a pure fn), and also explore the request.It was mind-bending to me when I first heard it. The podcast is full of such approaches and it seems that Chris and Nate, the hosts, have quite some battle scars to source from. reply eterps 15 hours agoparentI&#x27;m intrigued. Could you provide some pointers on where to start at https:&#x2F;&#x2F;clojuredesign.club? reply Jeaye 14 hours agoprevI&#x27;m hijacking a bit, but one thing I&#x27;ve been really wanting is some advanced optimization material for C++. There&#x27;s so much that goes into data locality, branch elimination, faster ways to copy word-aligned data, and so on. I&#x27;ve picked up all I know on the fly, but would love some great resources which go deep into this. Is there something more instructive than Intel manuals?Closest thing I found recently is this: https:&#x2F;&#x2F;agner.org&#x2F;optimize&#x2F;optimizing_cpp.pdf reply rramadass 5 hours agoparentYou might want to check out Fedor Pikus&#x27; The Art of Writing Efficient Programs.Also see Victor Eijkhout&#x27;s books : https:&#x2F;&#x2F;theartofhpc.com&#x2F; reply Jeaye 2 hours agorootparentPerfect. Thank you! reply rramadass 1 hour agorootparentAlso checkout the old (and slim) classic Writing Efficient Programs by Jon Bentley (of Programming Pearls fame). reply bboreham 12 hours agoparentprevI really enjoyed “Speed Is Found In The Minds Of People”, a talk by Andrei Alexandrescu.At first he is just showing simple stuff, but stay with it, he goes deep.https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=FJJTYQYB1JQ reply Jeaye 2 hours agorootparentAh, I&#x27;m familiar with Alexandrescu. Thanks for this suggestion! reply enduku 17 hours agoprevMirdin[0] can be helpful too.[0] https:&#x2F;&#x2F;self-service.mirdin.com&#x2F;products&#x2F;advanced-software-d... reply weinzierl 18 hours agoprevBesides most courses being beginner-focussed, there are at least two other related problems:- Most advanced courses sell themselves dishonestly as beginner-friendly even if they aren&#x27;t after the first couple of lessons. On the free market everyone is after every single potential customer, no matter what.- Everything is stretched out so much that information density is annoyingly low. Again free market pays per page and minute and not value.You might have more luck looking at resources produced free from these constraints and the best place I know is university courses. Still, good ones are hard to come by, but I learned a ton at uni I&#x27;d never had learned outside. reply bmac 15 hours agoprevhttps:&#x2F;&#x2F;www.destroyallsoftware.com&#x2F;screencasts are classics and the content and presentation have aged quite well over the years.The topics demonstrate intermediate&#x2F;advanced web development&#x2F;tdd concepts in Ruby but the lessons can easily be applied in other languages. There are even a few videos that show this approach with Python and C. reply dartos 19 hours agoprevhttps:&#x2F;&#x2F;www.manning.com&#x2F;books&#x2F;the-little-elixir-and-otp-guid...I read this book years ago when i was first learning elixir. I was coding for about 8 years at the time, so not new to the game.I liked it a lot. I don&#x27;t think i wrote every project in the book, but it helped me wrap my head around OTP and how erland&#x2F;elixir projects are structured.Highly recommend it. reply bentanweihao 18 hours agoparentYay author here, and this made my day <3. reply Arubis 17 hours agorootparentAny chance of a revised version one of these days? I, too, loved this book, but the ecosystem has moved far in the meantime & many of the examples no longer compile. reply sn9 9 hours agoprevA little known book that many might find interesting is Cristina Lopes&#x27;s Exercises in Programming Style which takes a single problem and solves it in 40 different ways using Python. [0]https:&#x2F;&#x2F;www.amazon.com&#x2F;Exercises-Programming-Style-Cristina-... reply christofosho 17 hours agoprevThe ideas shared in the post replies are great. My approach has been more hands-on and puzzled together recently. I&#x27;ve been digging into harder problems on sites like leetcode to learn more math, and solidify some algorithm and data structure concepts. It has really helped me feel more confident and erase some of the lingering imposter syndrome around the algorithm-side of programming. A great guide that got me started on this was the tech interview handbook[1], which helped me dig into specific concepts instead of just randomly targeting. I&#x27;ve found my ability to review code has also improved through this, as I am picking up different libraries and concepts that I wouldn&#x27;t have otherwise.1. https:&#x2F;&#x2F;www.techinterviewhandbook.org&#x2F;algorithms&#x2F;study-cheat... reply cassianoleal 19 hours agoprevSpecifically for Elixir, I found the first edition of Elixir for Programmers [0] by Dave Thomas to be very good.It&#x27;s now on the second edition which I haven&#x27;t done.[0] https:&#x2F;&#x2F;codestool.coding-gnome.com&#x2F;courses&#x2F;elixir-for-progra... reply trwhite 19 hours agoparentThanks! I&#x27;ll check that outEdit: Exactly the kind of thing I was looking for reply Arubis 17 hours agoprevGetting more focused on a particular subtopic seems to be a good way to get somewhat more advanced material. PragProg has a bunch of books for the Elixir ecosystem that might be up your alley (https:&#x2F;&#x2F;pragprog.com&#x2F;titles&#x2F;smgaelixir&#x2F;genetic-algorithms-in..., for example).I recently went through the Typescript resources over at https:&#x2F;&#x2F;executeprogram.com and found them really effective at solidifying the edges of that particular language for me; wish that was available for more ecosystems. reply archsurface 16 hours agoparentThat book is recommended quite often - I don&#x27;t agree, I was disappointed. If you know the just the basics of the two topics you won&#x27;t learn anything new. reply jasonjmcghee 10 hours agoprevBoth of Robert Nystrom’s books are awesome and don’t feel beginnery http:&#x2F;&#x2F;stuffwithstuff.com&#x2F;Building games and interpreters both feel niche, but i really love working on dramatically different problems than usual. Really helps expand my perspective reply dinvlad 19 hours agoprevFor Elixir, take a look at Phoenix LiveView course from https:&#x2F;&#x2F;pragmaticstudio.com - they don&#x27;t start from the beginning, and assume you are already familiar with Phoenix from the guides. reply Jtsummers 17 hours agoprevMore towards the advanced side I&#x27;d recommend Fred Hebert&#x27;s Property-Based Testing with PropEr, Erlang, and Elixir - https:&#x2F;&#x2F;pragprog.com&#x2F;titles&#x2F;fhproper&#x2F;property-based-testing-...It&#x27;s also one of the best resources I&#x27;ve come across on property-based testing and is something I recommend to non-Erlang&#x2F;Elixir programmers as well. reply 7sidedmarble 5 hours agoprevWhat would you be interested in learning about Elixir? I&#x27;m writing educational content now. reply barracutha 19 hours agoprevIn this regard, has anyone tried codecrafters.io? Is it worth it? reply simtel20 19 hours agoparentI&#x27;ve been using it and it&#x27;s been fun for learning rust. I&#x27;ve only done DNS and web server modules so far, but it has helped a lot in giving me goals that are achievable reply rohitpaulk 16 hours agoparentprev(disclaimer: I work at CodeCrafters)CodeCrafters is a bit light on Elixir support at the moment (only 2&#x2F;8 challenges support it so far). We&#x27;re planning on expanding Elixir support soon.Currently, we&#x27;re a great fit for Rust &#x2F; Python &#x2F; Go - we support a variety of challenges for those, and there&#x27;s lots of example code from other users to review and learn from. reply LouisSayers 16 hours agoprevI was quite impressed with Ardan Labs go tour[1] (free) for learning go.More of an intermediate resource, but was really good for helping to nail down some golang basics.Would be interesting to hear from people that have taken their courses.[1] https:&#x2F;&#x2F;tour.ardanlabs.com&#x2F;tour&#x2F;eng&#x2F;list reply sberens 14 hours agoprevI&#x27;ve heard good things about Bradfield CS[0] and CSPrimer[1] (both run by the same person).[0] https:&#x2F;&#x2F;bradfieldcs.com&#x2F;[1] https:&#x2F;&#x2F;csprimer.com&#x2F; reply weatherlight 18 hours agoprevDive into the BEAM VM. https:&#x2F;&#x2F;github.com&#x2F;happi&#x2F;theBeamBook reply roumenguha 7 hours agoprevOn a similar note, does anyone have any resources for traditional computer vision, SLAM, and computational geometry? reply weaksauce 7 hours agoprevthe Architecture of open source applications is a pretty interesting book to see real world examples of design https:&#x2F;&#x2F;aosabook.org&#x2F;en&#x2F;index.html reply theusus 16 hours agoprevI will suggest doing SICP. It teaches good FP patterns, and might complement another FP language well. reply andix 17 hours agoprevAdvent of Code (https:&#x2F;&#x2F;adventofcode.com&#x2F;)It&#x27;s not a programming course per-se, but it&#x27;s a great resource to master the skill of coding and problem solving.It&#x27;s just one part though, it won&#x27;t teach you anything about architecturing a bigger system. reply netbioserror 18 hours agoprevAside from raw theory, the best thing you can do is try to translate a project you have, maybe a toy project of reasonable complexity, and rewrite it idiomatically in the new language. Use all the preferred methods, semantics, and modeling tools of the target language. Use the language&#x27;s docs and X in Y Minutes page. reply bloaf 18 hours agoparentBut \"which methods, semantics, and modeling tools are preferred\" is probably what you&#x27;re trying to learn. reply netbioserror 18 hours agorootparentUnfortunately, that&#x27;s what language docs are for. Like I said, you can get some theory. If you&#x27;re learning functional programming, you can find books on lambda calculus and category theory and more. But ultimately you have to learn how to build good software, and there are scant few books about that out there. In the functional world, I know of two and they&#x27;re both in Scheme. So the best approach tends to be feet-first into the fire using the language&#x27;s docs, and hopefully the creator(s) has documents explaining theory and practice. reply cpursley 17 hours agoprevThis is a pretty great Elixir course that goes into advanced topics: https:&#x2F;&#x2F;kamilskowron.gumroad.com&#x2F;l&#x2F;cSGdY\"Hands-on Elixir & OTP: Cryptocurrency trading bot\" reply trmpakufnfee 18 hours agoprevI would not call myself experienced, but I often find myself glancing over documentation first (building a mental model and a map of what features are offered by this thing), then trying to build something, rather than sit through a slow paced course. This may or may not work for you. reply mmmBacon 16 hours agoprevIf you’re looking for advanced courses in Python, I highly recommend a workshop or class from James Powell.https:&#x2F;&#x2F;www.dontusethiscode.com reply drwl 19 hours agoprevMy take is that there’s such variance in people’s skill levels and so it’s really hard to cater educational programming content. Personally, I’ve found getting introductory books&#x2F;courses and skimming through it until you hit something that you don’t understand and then diving deeper into that bit. reply bee_rider 19 hours agoparentIt is interesting, though, that you can easily get intermediate training and coaching in basically any sport. But the thing you do for money? Take this very ad-hoc approach. reply 082349872349872 19 hours agorootparentPeople age out of being competitive athletes themselves and coaching is a natural way to stay involved with the sport.Old coders who love coding don&#x27;t have to do so indirectly. reply drwl 19 hours agorootparentprevI’m sure you can find individual coaches but the cost will exceed what most people are willing to spend or invest. Take executive coaches for example, they exist but from what I understand they’re largely private coaching and cost $$$. reply zihotki 19 hours agorootparentprevConcepts and excercises in sport don&#x27;t change that frequently comparing to programming. Programming is very ad-hoc indeed, it has way more variables for the intermediate level so that the coach won&#x27;t have enough time to digest the details. reply zaptheimpaler 17 hours agoprevThere are so many great books on specific domains but it depends on what you want to learn and you can then search for that. \"Programming\" is obviously too broad a category for experienced people. reply snicker7 19 hours agoprevSICP. Lectures are on YouTube. reply falcor84 15 hours agoprevHere&#x27;s a \"life hack\" - get a good book that doesn&#x27;t make any assumptions about your background but otherwise goes deep, and then use an LLM&#x27;s \"talk to pdf\" functionality (e.g. via the paid ChatGPT subscription). You can then describe the LLM your particular background and ask it to walk you through the book while skipping through \"the boring parts\" you are already familiar with. reply User23 18 hours agoprevWell it&#x27;s not popular (even though it ought to be), but Edsger Dijkstra&#x27;s A Discipline of Programming is an approachable and informal (by the author&#x27;s standards) look at solving non-trivial problems in a provably correct way. In fact the entire EWD archive[1] can be arranged into a course pretty readily, although you&#x27;ll definitely want to skim or skip large chunks of it. Also some of the papers are just acerbic observations on life, which some people like and some don&#x27;t.And there&#x27;s it&#x27;s big brother Predicate Calculus and Program Semantics by Dijkstra and Scholten that more rigorously formalizes the same approach.Dafny[2] is one approach out of Microsoft Research that attempts to provide automated tooling around some of those concepts.All of the above are excellent places to start if you&#x27;re interested in learning how to write better code with the imperative languages that you&#x27;re actually going to use professionally.[1] https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;EWD&#x2F;[2] https:&#x2F;&#x2F;dafny.org&#x2F; reply sn9 9 hours agoparentYou can use Coq to teach yourself many of the same ideas from ADP using Penn&#x27;s Software Foundations (probably the first two books are sufficient).A somewhat more modern approach of related concepts to ewd&#x27;s work is the work of Richard Bird which teaches you about using equational reasoning to take a correct naive algorithm and systematically optimize it into a correct and fast algorithm. [0] [1]Regarding Dafny, there&#x27;s actually a new book out that should be approachable to anyone with programming experience. [2][0] https:&#x2F;&#x2F;www.cs.ox.ac.uk&#x2F;publications&#x2F;books&#x2F;functional&#x2F;[1] https:&#x2F;&#x2F;www.cs.ox.ac.uk&#x2F;publications&#x2F;books&#x2F;adwh&#x2F;[2] https:&#x2F;&#x2F;mitpress.mit.edu&#x2F;9780262546232&#x2F;program-proofs&#x2F; reply User23 7 hours agorootparentOne of the real eye openers for me along that journey was realizing that Eclipse refactors were (mostly, except for some rough edges) automated semantic invariant preserving code transformations, which is to say a kind of simplified equational reasoning resting solely on the equivalence.Equational reasoning is borderline magical, and I look forward to the day when it&#x27;s as pervasively taught as basic logic.I have this fantasy in my mind of a language&#x2F;editor combination that begins with the empty program and only allows legal equational development. Without explicit predication that would just maintain the invariants of the language itself, so no undefined behaviors, but with user supplied predicates would allow a kind of exploratory programming with safety nets. One way to look at it would be explicit symmetry breaking, although I&#x27;m not sure that&#x27;s a good way to describe it to typical working programmers. The idea though is that for a program with the empty specification all code transformations are symmetrical with respect to the spec. And then as the specification is added to certain transformations become symmetry breaking and thus illegal, with said illegality being statically checked. Clearly I&#x27;m waving my hands quite a lot here, and I have no clue if it&#x27;s actually practicable. reply crabbone 18 hours agoprevI don&#x27;t know anything about learning Elixir, so, cannot help on that specific topic.But, here&#x27;s how I conceptually think about advancing one&#x27;s understanding in programming.First, you can aim for breadth or for depth of knowledge. So, learning another language is, in itself a kind of \"advance\". However, for depth, you&#x27;d usually want to pick a specific sub-discipline in programming and concentrate on that. You will have better luck with choosing theoretical disciplines, because those usually are rooted in math, and so have longer history and more systematic approach. The more \"practical\" aspects usually follow the arc of having an introductory course (at best) with the next learning stage being an \"area of active research\" (also, the intro course would typically be bad).To give you an example: in college I became interested in regular languages. So, I was looking to expand my knowledge on the subject. Out of all things that I could dig up on regular languages two seemed most appealing: approximation of context-free grammars with regular languages and equivalence between generating functions and regular languages. In a short while I realized that I don&#x27;t have enough of mathematical background to go after the generative functions part, but the approximation part turn out to be fun and interesting. I was able to dig up some papers and even try my hand in doing something with the concept.You&#x27;d have similar success if you went after types in programming languages. There are plenty of publications, and you will have multiple steps ahead of you before you run up the wall of \"area of active research\". My journey down that path started with Benjamin C. Pierce&#x27;s Types in Programming Languages.Another direction you may consider is to try to generalize your knowledge. For example, of programming languages. In this regard, I find the book by Shriram Krishamurthy, Programming Languages: Application and Interpretation to be a good introduction to the theoretical side of crafting and evaluation of programming languages.There are, also, unfortunately, some areas where a lot of programming work has been done, but very little learning material is available, and, especially nothing deep has been produced. For example, if you want to go down the system programming route... there are books, that&#x27;s true, but the level of generalization leaves a lot to be desired. It&#x27;s often boggled in all kinds of \"practical advise\" and very concrete examples based on Linux Kernel code long ago replaced by something else (or worse yet, but original Unix code etc.)Or, even worse, if you consider fields like testing. Or even GUI. It&#x27;s surprisingly common and yet surprisingly without much analysis or structure. reply GuestHNUser 17 hours agoprevcomputerenhance.com is a course focused on understanding how to measure and write high performance software. It&#x27;s a great course targeting experienced programmers. reply _ast 18 hours agoprevYeah, I see this problem. And I found some solution for this issue. We need to promote coaching relationships between starters and experienced engineers. reply jantypas2 17 hours agoprevWell, I don&#x27;t know how advanced I am (except in age). But I have found the best advanced training is the same as it was in college. No book makes up for the late night hours. I find other projects and offer to help -- there&#x27;s always a shortage of coding labor (especially if it&#x27;s free). You learn what the book or course never discusses -- things like \"What the heck was the person trying to do here?\" and \"Even God doesn&#x27;t know how this code works!\" You know you&#x27;re in trouble when you see \"Oh God! What an evil hack! But it works... don&#x27;t touch anything!\" or \"Good luck future Matt! You KNEW this was a horrible job, but did you come back in time and stop me?\" reply demon-code-999 16 hours agoprevare you talking about learning better ways to organize code &#x2F; systems to buy into? i mean literally just pick up books on IL and there you go. I dont understand what \"experienced\" coder means here... you should be able to research on your own @ 10 years+ reply pmarreck 19 hours agoprevI&#x27;m guessing that anything functional-language-focused aimed at a higher level would work. But I would also like to know the answer to this (and also am focused on Elixir).Informally I just tend to write tight modules that loosely adhere to hexagonal architecture and are easy to unit-test.I&#x27;m at a startup that is hiring, btw (I would be the one to reach out to). Seeking someone that is... probably like you, actually. Interested in Elixir but trying to aim higher as a self-motivated learner. reply Avicebron 19 hours agoparentHi, nice to meet you. I&#x27;m an early adopter of Elixir (coming from Haskell&#x2F;OCaml) and I will second another comment here, I&#x27;ve found the best way for me to learn Elixir is to be frequently building tooling that I either need or want, one habit I started was to migrate small projects I had previously over to Elixir, the community is a pretty good place to go for questions&#x2F;inspiration. https:&#x2F;&#x2F;elixirforum.com&#x2F; reply lobo_tuerto 19 hours agoparentprevHow do one get in touch with you? :) reply revskill 17 hours agoprev [–] No, there&#x27;s no such thing as \"advanced\". It&#x27;s just that people \"forgot\" fundamentals of things, then they see those \"fundamentals\" as advanced.I do think, teaching is hard. It basically turned \"advanced\" into \"fundamentals\" with correct teaching&#x2F;learning approach. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The person is currently learning Elixir and is looking for resources that are more advanced and suitable for experienced coders.",
      "They feel that the popular resources available are too beginner-focused and want to explore programming books or courses that cater to their level of expertise.",
      "The individual is seeking recommendations for resources that can help them further their understanding and skills in Elixir as a more experienced programmer."
    ],
    "commentSummary": [
      "The lack of advanced programming resources and the challenges in creating and pricing such resources are discussed.",
      "Participants recommend books, courses, and resources related to computer programming and systems for improving programming skills.",
      "The discussion explores the importance of practical knowledge, understanding large codebases, limitations of AI models for programming, and the value of mentoring and clean coding practices."
    ],
    "points": 314,
    "commentCount": 153,
    "retryCount": 0,
    "time": 1703856239
  },
  {
    "id": 38803556,
    "title": "Differences in Virtualization: Apple Silicon vs. Intel Macs",
    "originLink": "https://eclecticlight.co/2023/12/29/why-are-apple-silicon-vms-so-different/",
    "originBody": "hoakley December 29, 2023 Macs, Technology Why are Apple silicon VMs so different? Running macOS virtual machines (VMs) on Apple silicon Macs may not seem popular, but it has long been one of Apple’s important goals. Yet, if you do use a virtualiser on an M-series Mac, you’ll know how different it is from those that virtualise macOS and other operating systems on Intel Macs. This article explains why virtualisation is so important, and how it has become so different. Wind the clock back to WWDC in June 2020, when Craig Federighi announced details of what was coming in Apple silicon, and he identified three pillars to support a rich diversity of software on these new Macs: Universal apps that run on both architectures, Rosetta 2 to translate Intel code to Arm instructions, and virtualisation. At the time, before Apple had even released its Developer Transition Kit, Andreas Wendker went on to demonstrate a pre-release version of Parallels Desktop running Linux as a guest, but there was no mention at all of Windows. Although the first version of macOS to run on Apple silicon, Big Sur, didn’t support the lightweight virtualisation that was to follow in Monterey, it brought what Apple termed its Virtualization Extensions, an implementation of Arm’s AArch64 virtualization. In the early days of virtualisation, two distinct types were distinguished. Type 1 runs a hypervisor (the core of the virtualiser) direct on the computer’s hardware. Type 2, also known as hosted, runs a primary host operating system on the hardware, and hypervisors then run on top of, or in close conjunction with, that to deliver the same range of services to guest operating systems. A host operating system can run more than one hypervisor at a time, and each hypervisor in turn can run more than one guest, but ultimately all converge on the host operating system and its kernel. These are now much more common on desktop computers, and include popular products such as VMware Fusion, Parallels Desktop and Oracle VirtualBox. Running unmodified guest operating systems is made simpler when there’s hardware support for a hypervisor. Those adopting that approach can use Intel’s VT-x feature set, typically with Extended Page Tables (EPT) and Unrestricted Mode, or, in the case of Arm CPUs, AArch64 virtualization. This still leaves the problem of device support, which can either be left to the virtualiser to address, or can be provided for in a higher-level virtualisation framework. Over the years, and thanks to the effort of many engineers, virtualisers running on Intel hardware including Macs have grown extensive device support, and that’s what we rely on when virtualising macOS on Intel Macs. Every single hardware device in an Apple silicon chip is different from its equivalent (if there is one) in Intel Macs. Even if Apple had wanted to document them fully for external use, the engineering effort to match device support in Intel Macs would have been too costly for any third party. Thus starting with a hypervisor and expecting others to build a complete virtualiser wasn’t feasible, nor was it likely to result in the high performance that Apple and users expected. What Apple did instead was to build device support into macOS, in the form of Virtio drivers. Virtio is a standard originally developed by Rusty Russell that provides an abstraction layer over I/O devices. When the guest operating system calls to open a file, for example, that’s passed to a front-end Virtio storage device para-driver, and from there into a Virtio back-end driver that interacts with the storage device. Although this might seem less efficient than traditional virtualisation, in practice it can prove far more efficient. Its most obvious advantage is that creating a virtualiser app becomes a matter of configuring and opening the required Virtio devices, and letting the guest, Virtio and the host get on with it. And that’s essentially what an app using Apple’s Virtualisation framework does. Apple’s choice of Virtio was undoubtedly swayed by the fact that Linux already has good Virtio support, but at the time macOS had none. In the couple of years preceding the release of Monterey, Apple’s engineers thus set about building Virtio support into macOS, which explains why macOS lightweight virtualisation is only available on Monterey and later hosts, and when running Monterey and later guests. As implemented in macOS (both as guest and host), there are also extensions to support keyboard and pointing devices, a shared clipboard (‘Spice’), and high-performance graphics with Metal and GPU support. In the Virtio model, providing such support is the task of the operating system, not the virtualiser. For vendors like VMware and Parallels this reduces not only the cost of development, but also the commercial value of their products; there’s no scope for either of them to engineer better or faster graphics support, as that’s determined by features provided in both guest and host operating systems, via Virtio or an equivalent. That puts Apple in charge of what hardware and features are supported by virtualisation on Apple silicon, and the difficulties that have arisen over Apple ID access for VMs. On the other hand, it guarantees optimum performance in VMs. Not only is their CPU and GPU code run direct on the hardware, just as in the host, but Virtio devices deliver almost as good performance as on the host. The reward for Apple is flexibility in the future of macOS. Running older versions of macOS in a VM enables users to run Intel-only apps long after Rosetta 2 support is dropped from the current macOS, and for newer Apple silicon Macs to run software that’s incompatible with their minimum version of macOS. Using either Linux or macOS, developers can distribute Docker-like lightweight VM packages, something already done by Cirrus Labs’ Tart. Virtualisation may seem to be a minority pursuit just now. That’s likely to change over the coming few years, thanks to lightweight virtualisation and Virtio. Further reading Edouard Bugnion, Jason Nieh, Dan Tsafrir (2017) Hardware and Software Support for Virtualization, Morgan & Claypool. ISBN 978 1 62705 693 9. Bugnion was a co-founder of VMware. Share this: Twitter Facebook Reddit Pinterest Email Print Like Loading... Related Posted in Macs, Technology and tagged Apple silicon, hypervisor, Linux, macOS, Parallels, Virtio, virtualisation, VMware. Bookmark the permalink.",
    "commentLink": "https://news.ycombinator.com/item?id=38803556",
    "commentBody": "Why are Apple Silicon VMs so different?Hacker NewspastloginWhy are Apple Silicon VMs so different? (eclecticlight.co) 283 points by ingve 23 hours ago| hidepastfavorite225 comments andix 22 hours agoDoesn&#x27;t Windows do it more or less the same?A lot of Windows features depend on Hyper-V, once enabled Windows is not booted directly any more, Hyper-V is started and the main Windows system runs in a privileged VM.All other VMs need to utilize the Hyper-V hypervisor, because nested virtualization is not that well supported. So even VMware then is just a front-end for Hyper-V. reply RandomBK 16 hours agoparentBack when I ran Windows in a KVM VM for gaming, a lot of anti-cheat systems didn&#x27;t take kindly to running in a virtualized environment.Turning on HyperV to go KVM->HyperV->Windows effectively &#x27;laundered&#x27; my VM signature enough to satisfy the anticheats, though the overall perf hit was ~10-15%. reply beebeepka 14 hours agorootparentVery interesting. I wonder what sort of (available) CPU would be ideal for such a setup. A 7800x3D or 7950x. Also, was there any hit on the GPU side? reply RandomBK 14 hours agorootparentMore cache never hurts. I&#x27;d imagine there were GPU perf gaps, though they were hard to distinguish from CPU-based performance hits. The most notable issues were random latency spikes caused by the multiple layers of hypervisors, which interfered with some games and occasionally caused audio&#x2F;video desync on Youtube.I ultimately tore down that setup and just swapped to dual-boot. The steps needed to set up high-performance VFIO (i.e. clearing enough contiguous RAM for 1GB Hugepages) meant most of the benefits of VFIO never really materialized for me. reply declaredapple 14 hours agorootparentprevYeah I&#x27;m very curious as to how this effected 99% framerates and frame pacing.I suspect only a modest hit to average framerate, but I can only imagine it hurt the actual max frametimes which make it \"feel choppy\" even if the framerate is still higher then your monitor&#x27;s refresh rate. reply neilalexander 22 hours agoparentprevYou are right that Windows itself runs under Hyper-V as a guest when virtualisation-based security is enabled and it even has paravirtual devices that are not massively different to VirtIO.I think your statement about VMware Workstation is right as of today too with recent versions, although for a long time older versions would simply refuse to start if it detected that Hyper-V was enabled, presumably because it made assumptions about the host virtualisation support. reply andix 18 hours agorootparentIt&#x27;s not just security features that need Hyper-V. Also WSL (Linux on Windows) or the Android Subsystem (run any side loaded app or anything from the Amazon App Store) need Hyper-V. Both of them are super useful for me, more and more things are iOS&#x2F;Android App based only. Linux should speak for itself. reply ComputerGuru 15 hours agorootparentOnly WSLv2 needs (or uses) Hyper-V. reply andix 14 hours agorootparentBut WSL1 is de-facto dead, although it is still supported. reply ComputerGuru 5 hours agorootparentIt still makes more sense than v2 for certain patterns. replyrdedev 16 hours agoparentprevIs it possible to use hyper v directly? Like could I boot into linux but switch over to Windows with just a key press? I&#x27;m guessing no since its not in Microsoft interest to do so reply andix 14 hours agorootparentThat&#x27;s an interesting idea, to run Hyper-V completely without Windows. I think it&#x27;s not possible, at least not without some substantial amount of hacking.But it&#x27;s no problem to run Linux on Hyper-V. It&#x27;s a hypervisor, off course you can start nearly any operating system as a VM. You can also give the VM access to some hardware components. But I don&#x27;t think it&#x27;s possible to get a full native Linux desktop experience, with GPU&#x2F;Screen, Keyboard and Mouse connected to the host system.Edit: this post seems to answer your question, not sure if it&#x27;s correct: https:&#x2F;&#x2F;superuser.com&#x2F;a&#x2F;1531799 reply als0 11 hours agorootparentYou can soon run Linux on Hyper-V without Windows: https:&#x2F;&#x2F;www.theregister.com&#x2F;2021&#x2F;02&#x2F;17&#x2F;linux_as_root_partiti... reply superb_dev 3 hours agorootparentIt’s been a few years now, I wonder how far this project has gotten reply ComputerGuru 15 hours agorootparentprevNot with Hyper-V but the thing to be aware of is there is no difference which you initially “boot into” since each is essentially run at the same level.You can install ESXi (free) to do what you are asking, though. reply andix 14 hours agorootparentESXi is a completely headless system, except some minimal management UI&#x2F;CLI there is no possibility to directly interact with the VMs on the host system. At least that&#x27;s my understanding.And I think a very similar thing can be archived with Windows Server Core. Running Hyper-V with just a minimal Windows installation for management, without the full Windows UI. reply ComputerGuru 14 hours agorootparentYeah, but it&#x27;s configurable. I have it pull up the core on a VGA card and then boot up my primary VM on a GPU. reply josephg 21 hours agoparentprev> Hyper-V is started and the main Windows system runs in a privileged VM.What are the performance implications of that? reply abhinavk 21 hours agorootparentMinor performance loss. 5% fps on average. MS recommends turning it off if gaming is your primary use. reply overstay8930 19 hours agorootparentEven then it&#x27;s really not that much of a hit if you have half-decent hardware, I&#x27;ve kept it on and I think the only issue I saw was launch day BG3 and it would use much more power from the wall than when I turned it off. reply therein 15 hours agorootparentMake sure to have Intel VT-x or AMD-V enabled too.There are now a lot of BIOS flags that you can have set to off by default that&#x27;ll silently hinder performance. reply edude03 15 hours agoparentprev> A lot of Windows features depend on Hyper-V, once enabled Windows is not booted directly any more, Hyper-V is started and the main Windows system runs in a privileged VM.Got a source for this? Not that I don&#x27;t believe you but other than for the Xbox I haven&#x27;t seen&#x2F;can&#x27;t find any details about this. reply dgellow 14 hours agorootparentSurprised you didn’t find the information, it’s covered in details in Microsoft own docs: https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;virtualization&#x2F;hyper-v-on-...quote:“ In addition, if you have Hyper-V enabled, those latency-sensitive, high-precision applications may also have issues running in the host. This is because with virtualization enabled, the host OS also runs on top of the Hyper-V virtualization layer, just as guest operating systems do. However, unlike guests, the host OS is special in that it has direct access to all the hardware, which means that applications with special hardware requirements can still run without issues in the host OS.”From https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;virtualization&#x2F;hyper-v-on-... reply marshray 12 hours agorootparentprev\"Virtualization-Based Security: Enabled by Default\"https:&#x2F;&#x2F;techcommunity.microsoft.com&#x2F;t5&#x2F;virtualization&#x2F;virtua... reply moffkalast 16 hours agoparentprev> Hyper-V is started and the main Windows system runs in a privileged VMWait it&#x27;s all VMs? Always has been?! That is actual one sentence horror. reply deaddodo 14 hours agorootparentIt hasn&#x27;t always been, nor is it necessarily now. If you enable Hyper-V, that will act as Hypervisor for your machine and boot Windows by default. Applications that use it (VMWare, for instance, or Microsoft ones like WSL2) will add their own guests to the Hypervisor.It is not the default configuration. And it wasn&#x27;t even installed before Windows 8. reply andix 14 hours agorootparentIsn&#x27;t virtualization based security the default for Windows 11? I only have upgraded Win 11 systems, so no idea what&#x27;s the default on a fresh installation. reply deaddodo 10 hours agorootparentMy fresh install of Windows 11 definitely didn&#x27;t have Hyper-V active by default. It had to install it and reboot to activate the WSL2 subsystem.Now, to be fair, this was an install from a couple major updates back. So newer builds might have it active by default. reply lodovic 15 hours agorootparentprevSometimes it&#x27;s hard to tell how many VMs there are between my code and the actual hardware. It seems to be VMs all the way down. reply Erratic6576 13 hours agoprevI wish every OS user logged in their isolated VM of the OS. This way, Adobe could install all their bloatware and take control of their user and I could keep ownership of my Apple’s computer reply jdewerd 12 hours agoparentWhat&#x27;s sad is that processes are already virtual machines, they just need to have a better permissions system. What&#x27;s really sad is that for the most part those better permissions systems have been built (namespaces&#x2F;cgroups on linux, gatekeeper on Mac OS) but nobody figured out how to expose that to end users before the business people figured out that there were trillions of dollars available if you charged rent to centrally manage it.We were so close. Sigh. reply lox 12 hours agorootparentIs this not essentially what docker did with cgroups? It’s incredibly tricky securing containers, I’m not at all confident process only sandboxes would be adequate. reply xorcist 9 hours agorootparentDocker makes it really hard to do anything with cgroups. Unless you mean letting Docker manage everything about them, in which case you can configure nothing.Systemd did the cgroups thing right. Apart from the v1&#x2F;v2 thing, but if you can use only v2 then you do not need to think about it. reply theossuary 12 hours agorootparentprevThere&#x27;s a big difference between securing containers, and using them to prevent Adobe from polluting they entire system. Containers are an excellent way to provide lower guarantees of security (though still more than is there currently), with higher usability. Microvms also fit into the model very cleanly and could be used transparently when higher security was required.The fact that VMs are necessary has shown how much OSes have failed. That we need to take an OS and package it into multiple VMs to get any real isolation is a problem that OSes should solve for. reply PaulDavisThe1st 10 hours agorootparent> The fact that VMs are necessary has shown how much OSes have failed.The fact that VMs exist at all shows how much OSes have succeeded. reply GuB-42 8 hours agoparentprevEssentially shipping an entire OS with every app looks horribly inefficient to me. Especially if the only thing you need is sandboxing.Containers would be a more appropriate solution, and even containers would be somewhat overkill. Simply using UNIX-style permissions and an application-specific UID could do. I think it is how it is done in Android. reply curt15 13 hours agoparentprevIsn&#x27;t that roughly what Qubes OS provides? reply fulafel 58 minutes agorootparentDoes it support macOS VMs? reply rustcleaner 4 hours agorootparentprevI daily drive Qubes and will never go back to a normie system again if I can help it!! reply deusum 12 hours agorootparentprevQubes does allow creating a VM for just about any program or service. But, in my experience, it suffers from latency. So, while fine for web browsing, it wasn&#x27;t too keen on playing videos. YMMV of course, but Adobe products are already hogs without the emu layer. reply jacquesm 10 hours agoparentprev> I could keep ownership of my Apple’s computerThat&#x27;s a funny slip... reply tbenst 22 hours agoprevDoes anyone know the state of running Windows &#x2F; Linux x86-64 virtualization on Apple Silicon? This article is super interesting but dances around the most important application for VMs on Mac. reply tecleandor 22 hours agoparentFor Linux, and if you only need to run CLI tools, I&#x27;ve been very happy with Lima [0]. It runs x86-64 and ARM VMs using QEMU, but can also run ARM VMs using vz [1] (Apple virtualization framework[2]) that is very performant. Also, along with the project colima [3] you can easily start Docker&#x2F;Podman&#x2F;Kubernetes instances, totally substituting Docker Desktop for me.For desktop environments (Linux&#x2F;Windows) I&#x27;ve used UTM [4] with mixed success. Although it&#x27;s been almost a year since last time I used it, so maybe it runs better nowThere&#x27;s also Parallels, and people say it&#x27;s a good product, but it&#x27;s around USD&#x2F;EUR 100, and I haven&#x27;t tested it as I don&#x27;t have that need.And there&#x27;s VMWare Fusion but... who likes VMWare? ;) [0] - https:&#x2F;&#x2F;lima-vm.io [1] - https:&#x2F;&#x2F;lima-vm.io&#x2F;docs&#x2F;config&#x2F;vmtype&#x2F;#vz [2] - https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;virtualization?language=objc [3] - https:&#x2F;&#x2F;lima-vm.io&#x2F;docs&#x2F;faq&#x2F;colima&#x2F; [4] - https:&#x2F;&#x2F;mac.getutm.app&#x2F; [5] - https:&#x2F;&#x2F;www.parallels.com&#x2F;products&#x2F;desktop&#x2F; reply cangeroo 19 hours agorootparentParallels has a bad desktop user experience using Linux because of poor support for continuous scrolling. Lots of users have complained on their forums for years, but they refuse to do anything about it. I bought it for one year, and regretted the experience. It works well with Windows though.Generally, the experience with MacOS is mediocre thanks to Apple and their Virtualization Framework, with many basic features missing for years. reply deaddodo 14 hours agorootparentThis is ironic, considering Parallels was originally an Apple first product designed specifically for virtualizing Windows and running it&#x27;s apps \"seamlessly\" alongside native Mac ones. reply kergonath 13 hours agorootparentWhy is it ironic? The parent says that it works well with Windows, which you say is the original use case. Linux has nothing to do with this. reply deaddodo 10 hours agorootparentAh, I may have misread it. I thought they were saying Parallels on Windows runs Linux fine.But, re-reading it again, your interpretation is probably correct. reply a_vanderbilt 17 hours agorootparentprevCan you elaborate on the continuous scrolling? I&#x27;ve actually never noticed anything off about the scrolling. reply dada78641 20 hours agoparentprevMy personal experience is that Windows 11 for ARM runs extremely well on Parallels. It includes an emulation layer for x86 apps that&#x27;s completely invisible and just works. I can even still run Cakewalk, a program originally from the 90s, on my M1 Mac to edit midi files.With that being said, this is just my view as someone who uses simple consumer oriented programs, and I&#x27;m not sure how well it&#x27;ll work for more serious purposes. reply sydbarrett74 18 hours agorootparentHave you tried any Windows games on Apple Silicon? What kinds of Windows apps do you tend to run? I&#x27;ve used the macOS version of World of Warcraft on my &#x27;20 Mac Mini (16GB RAM) and even with utilities that adjust the mouse acceleration curve, I still find game play clunky. I was hoping I could run WoW under a VM and have it be somewhat performant. reply swozey 15 hours agorootparentWhen I first got it I tested a few games on my 2022 M1 Max 64GB 16\" MBP both natively and in Windows ARM.The only one that I remember is Crusader Kings II. It has a native MacOS version which I tried and it ran pretty rough. Very, very choppy on the map. I didn&#x27;t tweak any graphics settings from the defaults and put no effort into making it run better, FWIW.Next, I ran it via Windows ARM in Parallels. Now that I&#x27;m writing this I have no idea what I did to test it. I feel like it just ran but I don&#x27;t think I did anything specific to make an x86 process run on ARM. Maybe Windows ARM does that for you, I forget.Anyway, it ran really well. Absolutely much, much better than the native app. It felt completely smooth navigating the map, etc. I did NOT play it in a big game that lasted hundreds of years. I probably did 5 turns, mostly checking to see how smooth scrolling the map and the UI&#x2F;UX stuff was.I have a 4090&#x27;d gaming desktop so it wasn&#x27;t a big deal to me to be able to game on the mac which is why I put as much effort into this as you can see. lmao.It&#x27;s amazing at everything else! reply solardev 14 hours agorootparent> I feel like it just ran but I don&#x27;t think I did anything specific to make an x86 process run on ARM. Maybe Windows ARM does that for you, I forget.Yeah, Microsoft doesn&#x27;t get nearly enough credit for this, but Windows for Arm just automagically emulates x86 for you! Kinda like Rosetta, but for Windows.https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;arm&#x2F;apps-on-arm-x8... reply solardev 15 hours agorootparentprevFor gaming, you want to use Crossover or the FOSS Whisky app. Parallels only runs Arm Windows which then emulates x86. This is much much slower than using Wine to translate system calls and Apple&#x27;s Game Porting Toolkit to handle the Vulkan or DirectX graphics. Crossover and Whisky take care of the internals of those for you. Give those a shot, I think you&#x27;ll find it much better than a full VM. In my experience some games do run better this way than the MacOS versions, though that&#x27;s usually because the Mac client wasn&#x27;t compiled for Apple Silicon and so Rosetta is emulating. Unfortunately, I&#x27;m pretty sure WOW is already Apple Silicon native, so you probably won&#x27;t get better performance this way.Crossover is paid but has better compatibility: https:&#x2F;&#x2F;www.codeweavers.com&#x2F;crossover&#x2F; (or see https:&#x2F;&#x2F;www.codeweavers.com&#x2F;compatibility for compatible games)Whisky is free, and will work just as well for games it supports, but has compatibility with fewer games (no official list, so you just have to download it and try yourself): https:&#x2F;&#x2F;github.com&#x2F;Whisky-App&#x2F;WhiskyFor the mouse stuff, try a USB mouse if you&#x27;re not already using one, combined with https:&#x2F;&#x2F;github.com&#x2F;ther0n&#x2F;UnnaturalScrollWheels to disable acceleration and fix the scroll wheel.That works really well for me to get a Windows-like mouse curve.TLDR skip the emulation and go for translation layers via Crossover, Whisky, and GPT. It&#x27;ll be much faster. The mouse thing is separate and has nothing to do with the graphics layer.------Personally though, I&#x27;d just pay $20 a month for Geforce Now. It is much much faster than even the highest end Mac. I don&#x27;t think WOW is on there, but for supported games, it&#x27;s a phenomenal experience... sold my 3080 desktop and replaced it with GFN on my Macbook. It&#x27;s fantastic.Supported games: https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;geforce-now&#x2F;games&#x2F; reply ngcc_hk 15 hours agorootparentWhat is the bandwidth requirement I wonder. Seems too cheap to be true … must have some other catch. Latency as well? reply solardev 15 hours agorootparentFor GeForce Now? Not much:From https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;geforce-now&#x2F;system-reqs&#x2F;:- 15 Mbps for 720p @ 60FPS- 25 Mbps for 1080p- Up to 35 Mbps for 4k&#x2F;120 FPSInput latency is there, yes, but it&#x27;s not too bad especially if you turn on Nvidia Reflex and use the hardware cursor. Totally unnoticeable in many games. For first-person shooters it&#x27;s definitely noticeable, but IMO still playable as long as you&#x27;re not doing it competitively. I play shooters on it from time to time... and put it this way, I would much rather do that (on max graphics) and deal with the minor input lag, than to try to get them running on my Mac, all to get super low graphics with low draw distance, etc.It&#x27;s never going to beat a 4090 on your desk, but for $20&#x2F;mo...? It&#x27;s an incredible value.I don&#x27;t know that there really is a \"catch\" beyond basic network principles&#x2F;limitations. Game streaming has been developed for more than a decade now... when OnLive first came out, the technology (home internet and hardware encoding) wasn&#x27;t quite there. Now 35Mbps is commonplace, Nvidia has hardware encoding in all their cards, AND they control the entire stack of their data center like no one else can. Stadia&#x27;s failure was IMO a Google management problem more than any technical limitation. GeForce Now is a much much better service, both using your existing Steam library and supporting way more games.The pricing does seem really good, especially compared to Shadow.tech (where you rent a whole gaming VM with a 3070 Ti for $50&#x2F;mo, but can run anything you want) or AirGPU (similar service). But the games-as-a-service platforms like Amazon Luna, Xbox Cloud Streaming, and PS Plus are all comparably priced ($10-$20&#x2F;mo). There are other third party services like Boosteroid too. Cloud gaming is a maturing technology that&#x27;s largely already \"there\", in my experience (have tried nearly all of them over the last 10+ years).I think Nvidia is uniquely positioned as the only company in this space who can provide the graphics cards first-party instead of needing to buy them from, well, Nvidia. It&#x27;s possible that the current pricing is a loss leader, but they&#x27;ve already raised the prices from the Founders pricing they had a few years ago, and it&#x27;s still not too bad. It&#x27;s not like Nvidia is hurting for cash anyway. My main fear is not that there&#x27;s a \"catch\", but that they&#x27;ll gradually move out of the gaming segment and focus on AI.In the meantime, while it lasts, GeForce Now really is wonderfully, uh, game-changing :)----------Edit: PS they have a free tier, and you can even use it in a browser tab, no client download needed. That&#x27;s enough to give you a taste for free, no commitment. If you decide you like it, the Ultimate plan is very much worth it, and the desktop (or mobile) clients offer slightly better UX than the browser tab and higher resolutions. reply rogual 17 hours agorootparentprevNot OP, but I use Parallels on M2 and gaming is a bit hit-or-miss. I&#x27;d say maybe 80% of games work flawlessly, and 20% have some sort of issue ranging from the annoying to the unplayable.For non-gaming, Parallels is extremely solid. I use Visual Studio and various productivity apps and they all work perfectly -- although Parallels is enshittified scumware that pops up ads at every available opportunity, so if that kind of thing bothers you, it&#x27;s worth considering it before buying. reply plufz 16 hours agorootparentAds about what? Upgrading to a more expensive tier or like third party ads? reply timenova 22 hours agoparentprevYMMV, but from my own experiments, on an M1 Macbook Air, it did not work well for me. I was trying to compile an Elixir codebase on x86-64 Alpine Linux. Elixir does not have cross-compiling. I tried it in a Docker container, and in a Linux VM using OrbStack. Both approaches fail, as it just segfaults, even on the first `mix compile` of a blank project.This problem does not exist in ARM containers or VMs, as the same project compiles perfectly in an ARM Alpine Linux container&#x2F;VM.It&#x27;s definitely not plug-and-play for all scenarios. If anyone knows workarounds, let me know. reply cschmatzler 21 hours agorootparentThat’s an underlying QEMU bug, which is used by Lima [1]. Add `ENV ERL_FLAGS=\"+JPperf true\"` to your Dockerfile and it will build just fine cross platform. The flag just changes some things during build time and won’t affect runtime performance.[1] https:&#x2F;&#x2F;gitlab.com&#x2F;qemu-project&#x2F;qemu&#x2F;-&#x2F;issues&#x2F;1034 reply timenova 19 hours agorootparentThanks. I can confirm that this works. Compiling a new project no longer segfaults, and `Mix.install()` works in `iex` too. reply plufz 16 hours agorootparentHN just turned into Stack Overflow. :) reply giantrobot 16 hours agorootparentIn that case can this whole thread be deleted and replaced by a link to an almost completely unrelated issue that used some of the same English words in the description? Just trying to get the full effect here. reply thejosh 18 hours agorootparentprevFor anything that doesn&#x27;t need a UI, you&#x27;re FAR better off having some remote server than trying to emulate, it&#x27;s far to slow for ARM64x86-64 in both directions..Many things are just so much easier with a remote server&#x2F;workstation somewhere than trying to deal with VM shenanigans.ARM64 visualised on the otherhand (Linux works great, macos seems good(?), haven&#x27;t tried Windows) with UTM is pretty great. reply timenova 18 hours agorootparentI absolutely agree! I finally went in that direction. The only reason I was trying this whole ordeal was because I was trying to get some private dependencies included in the build without going through the whole hassle of git submodules. Now I just include those deps as a path include in mix.exs. Not a great solution I know... reply travisgriggs 17 hours agorootparentprevI’ve been able to do this (build x86&#x2F;ubuntu targeted elixir) with UTM on my M1 Mac. It ain’t fast, that’s for sure. But it works. Which is interesting because sibling responses to your Lima experience claim it’s because of a qemu “bug”, but utm runs qemu as well. reply toast0 21 hours agorootparentprev> Elixir does not have cross-compiling.Elixir compiles to beam files, like Erlang, right?I was pretty sure beam files are bytecode and not platform specific? reply timenova 19 hours agorootparentYou&#x27;re right that Elixir source code compiles to BEAM bytecode, however, if you run `mix release`, you need to ensure that the release runs on the same target OS and OpenSSL version. My aim was to build a `mix release` on my M1 Mac to run it on an x86-64 server.From the docs [0]:> Once a release is assembled, it can be packaged and deployed to a target, as long as the target runs on the same operating system (OS) distribution and version as the machine running the mix release command.The `mix release` command outputs a directory containing your compiled Elixir bytecode files, along with the ERTS (Erlang Runtime System). The ERTS it bundles is only for your host machine&#x27;s architecture. Another point to remember is that some dependencies use native NIFs, which means they need to be cross-compiled too. Hence it&#x27;s not as easy as replacing the ERTS folder with one for another architecture in most circumstances.There&#x27;s a project that aims to alleviate these issues called Burrito [1], but when I tried it, I had mixed success with it, and decided not to use it for my deployment approach. It looks like Burrito has matured since then, so it would be worth taking a look into if you need to cross-compile.The gist is, while possible, its significantly harder to get an Elixir release running on another architecture than say is the case for Go.[0] https:&#x2F;&#x2F;hexdocs.pm&#x2F;mix&#x2F;1.16.0&#x2F;Mix.Tasks.Release.html [1] https:&#x2F;&#x2F;github.com&#x2F;burrito-elixir&#x2F;burrito reply outcoldman 18 hours agoparentprevI do my work on Apple Silicon laptops since the first M1 came out.I use Docker Desktop that can run for me amd64 images as well.I do run Splunk in it (which is a very enterprise product, written mostly in C++), I was so shocked to see that I was able to run it on Rosetta pretty much from day 1. Splunk worked on macOS with Rosetta from day 1, but had some issues in Docker running under QEMU, now Docker uses Rosetta for Linux, which allows me to run Splunk for Linux in Docker as well.I use RedHat CodeReady Containers (local OpenShift), which works great as well.And I use Parallels to run mostly headless Linux to run Kubernetes. And sometimes Windows just to look at it.In a first two years of Apple Silicon architecture I definitely had to find some workaround to make things work. Right now I am 100% rely only on Apple Silicon, and deliver my software to large enterprise companies who use it on amd64&#x2F;arm64 architectures. reply deergomoo 22 hours agoparentprevYou can use Rosetta to run x86 Linux binaries with good performance under a virtualised ARM Linux [0], but if you want to run fully x86 Windows or Linux you’ll need to emulate, not virtualise. It’s possible, but there’s a big performance hit as you might expect.[0] https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;virtualization&#x2F;run... reply kamilner 22 hours agorootparentI&#x27;m not sure how OrbStack does it, but it can run a fully x64 Linux using Rosetta with quite good performance. reply AkshitGarg 20 hours agorootparentIIRC that runs a x86_64 userland (using Rosetta) on a arm64 kernel. reply kamilner 20 hours agorootparentInteresting. uname -a reports x86_64, and lscpu also reports x86_64, although perhaps that&#x27;s just the kernel being patched to lie about the architecture. reply selimnairb 22 hours agoparentprevI run full AMD64 containers using Docker Desktop, which uses Rosetta under the hood. On my M1 Pro they were a bit slow (maybe 25% slower than my work laptop, which is a 12th gen. i9), but good enough in general. I have since upgraded to an M3 Max and AMD64 VMs seem to be a lot faster, maybe even faster than my 12th gen. i9. I really hope Apple doesn’t get rid of Rosetta support in VMs, ever. It’s just too useful. reply donatj 22 hours agoparentprevYour mileage may vary, but I&#x27;ve been quite happy running x86-64 software in an ARM build of Windows 11 in UTM.Nothing graphical or all that intensive though, just some productivity tools I can&#x27;t live without. reply hypercube33 20 hours agorootparentWhat hardware are you running this on out of curiosity? reply donatj 18 hours agorootparentM1 Macbook Pro reply kamilner 22 hours agoparentprevI regularly use Orbstack to develop for x64 Linux (including kernel development). It works transparently as an x64 linux command line that uses Rosetta under the hood, so performance is reasonably good.It can also run docker containers, apparently faster than the normal docker client, although I haven&#x27;t used that feature much so I&#x27;m not sure. reply vbezhenar 22 hours agoparentprevVery slow using qemu. You can run arm64 Linux and run x86_x64 apps inside using Rosetta, if your virtual machine uses Virtualization.Framework (does not work with qemu, AFAIK). I suppose you can do the same with arm64 Windows and Microsoft x86_64 translation technology, but not really sure. reply rincebrain 20 hours agorootparentYou can use qemu -accel hvf. reply fulafel 19 hours agoparentprevThe article is about virtualization, not emulating x86-64, so I&#x27;d disagree it&#x27;s dancing around that. (Also, Windows and Linux have their own x86 emulations - if you boot virtualized Windows&#x2F;ARM or Linux&#x2F;ARM, you can get to the native emulation functionalities) reply nxobject 22 hours agoparentprevI wish there was a good GUI-based solution for Windows emulation via Rosetta. My use case isn’t development - it’s running software with an x64-only proprietary driver! (The Oculus remote link drivers, FWIW.) Fusion and Parallels don’t have that feature, so I’m wondering whether there are technical difficulties&#x2F;blockers there. reply maldev 18 hours agoparentprevI&#x27;m a big windows guy, pretty much windows only. Recently bought a macbook. I love windows so much that I set up my shell on the mac to be powershell and use Windows Terminal to SSH into the mac.I&#x27;m REALLY happy with parallel desktop. It runs any productivity or programming app I&#x27;ve needed. It also makes it as if it&#x27;s running natively on the mac, you can just open up some windows app and it pops up like a mac one. It works amazingly fast, and I can develop both x64, x32, ARM apps in visual studio on my VM. Games don&#x27;t work because of DRM, but I just use Parsec to stream my desktop if I want to game anyways, so it doesn&#x27;t affect my workflow. And any game I would actually play while traveling is on the mac natively.For linux I only emulate Kali, and it works good, I love how the VM&#x27;s pop up as a \"Virtual desktop\" so I can side swipe it, but linux vm&#x27;s don&#x27;t have the native integration like Windows. Once nested virtualization is enabled, i&#x27;ll probably stick it in WSL, I personally don&#x27;t use Linux that much since I think it&#x27;s shit.The only downside is some asshole at Apple won&#x27;t put in nested virtualization for the VM&#x27;s, even though M2 and M3 have support for it on linux. reply freedomben 16 hours agorootparentIf you don&#x27;t mind me asking, why did you buy a macbook? reply maldev 16 hours agorootparentIt&#x27;s my first Mac, and I bought it because the actual machine is magical. It&#x27;s so well built and has so many little things that make it great. I thought it was dumb and overhyped until my girlfriend got a M2. I then looked up the virtualization and played around with it a bit, and bar games, it&#x27;s the best laptop for running Windows apps. And even then, it runs every game I would play on the road.I also really liked the memory layout they have. I have been messing around a ton with ML&#x2F;AI, it&#x27;s able to do local models faster than chatgpt and get like 70% the accuracy. I have a pretty beastly desktop setup, and it&#x27;s a joy to use such a solid machine in bed while i&#x27;m watching TV. reply LASR 14 hours agoparentprevI was able to get a fully functional Windows 11 install using UTM on my M1 MBP. This really helped with some Windows-only android tools with USB passthrough.I&#x27;ve not tried Linux.Note: I am not associated with UTM in any way, just a satisfied user.[1] https:&#x2F;&#x2F;mac.getutm.app&#x2F; reply sneed_chucker 14 hours agorootparentProbably ARM Win 11 though right? reply xvector 14 hours agorootparentprevI&#x27;ve always wondered what the security posture is of UTM, QEMU, etc. Is an escape trivial or is there thought put into security? reply jxdxbx 20 hours agoparentprevARM Windows runs well with Parallels. And it can run x86 apps. reply stephen_g 20 hours agorootparentYes, this is the best way to do it if possible in my experience. I use some fairly heavy x86_64 apps in the Arm for Windows in Parallels, using Windows’ translation system (rosetta 2 equivalent), and it’s been quite good.Trying to emulate the whole x86_64 version of an OS (I tried some Docker images that only came in x86 before finding instructions to rebuild them on the ARM base OS) has been super slow on the other hand. This is on a quite decent M2 Pro. reply cangeroo 19 hours agorootparentprevSome x86 apps refuse to run on ARM, having platform detection built-in to their installer. reply zerkten 14 hours agorootparentIf it&#x27;s an MSI-based installer, it&#x27;s pretty easy to edit the MSI with Orca to remove the check. This is similar to how you&#x27;d get client software installs unblocked on Windows Server. In other cases, there are often ways to trick it, but it&#x27;s contextual. reply svdr 15 hours agoprevI wanted to use a MacOS VM with Parallels for development. It is very easy to install and runs fast, but it&#x27;s impossible to sign in with an Apple ID, which severely limits its use. reply naikrovek 10 hours agoparentThat’s Apple’s decision. It was intentional.Apple are very weird about MacOS VMs. reply sneak 14 hours agoparentprevSeverely? I use macOS directly on hardware without an Apple ID as my daily driver.It works fine. reply WanderPanda 19 hours agoprevMy great confusion is why docker —-platform linux&#x2F;amd64 is so much faster (almost native performance) than x86 UTM VMs. Can docker somehow leverage Rosetta? reply cpuguy83 18 hours agoparentYes, Docker can leverage Rosetta. I haven&#x27;t used Docker Desktop in a bit (b&#x2F;c I end up doing my work in a VM on Azure since I work on Azure), but not too long ago there was an option to enable it in the settings panel, not sure if it&#x27;s default or not these days.Any Linux VM can use Rosetta[1] you just need to enable it when booting the vm. This creates a shared directory in the vm that you need to mount and then register Rosetta with binfmt_misc (same way Docker uses qemu).[1] https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;virtualization&#x2F;run... reply MBCook 15 hours agorootparentI remember seeing it was out of beta in the release notes of Docker Desktop not too long ago. reply koenigdavidmj 19 hours agoparentprevDocker runs an ARM kernel and uses qemu in user mode on the individual binary level. Anything CPU-bound is emulated, but as soon as you do a system call, you’re back in native land, so I&#x2F;O bound stuff should run decently. reply arianvanp 18 hours agoparentprevNote that UTM also supports rosetta. Boot up an aarch64 image with Rosetta support and then load the mounted binfmt handler. Now you can run x86 binaries on your aarch64 UTM VM. Works flawlessly.If you use NixOS you can simply enable https:&#x2F;&#x2F;search.nixos.org&#x2F;options?channel=23.11&show=virtuali... reply steeve 18 hours agoparentprevIt does yes, Apple provides Rosetta for Linux: https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;virtualization&#x2F;run... reply naikrovek 10 hours agoparentprevMacOS on Apple Silicon does not allow whole VM Rosetta.You must run arm64 MacOS or Linux VMs and those VMs can run x86_64 binaries via Rosetta. Apple documented this.Running an x86_64 virtual machine on MacOS requires software emulation, which is why it is so slow. Docker sets it up correctly so that the Linux VM it uses is arm64 but the binaries in the containers are x86_64, so that Rosetta can be used on those binaries. reply jbverschoor 19 hours agoparentprevDitch Docket.. Orbstack is fast.. reply gnatolf 22 hours agoprevWhat&#x27;s the progress, or who&#x27;s behind a virtio layer for windows? Any hope that this will work in the foreseeable future? reply virtioliker 22 hours agoparentThere&#x27;s mature VirtIO drivers for just about everything already, under the virtio-win umbrella: https:&#x2F;&#x2F;github.com&#x2F;virtio-win&#x2F;kvm-guest-drivers-windowsMy desktop PC is using libvirt+qemu (on an Arch host. I use Arch, btw) to PCI passthru my RTX 4090 GPU to a Windows guest. I installed the guest initially with emulated SATA for the main drive. Once Windows was up and running, I installed virtio-win and the guest is now using virtIO accelerated drivers for the network interface + main disk. I&#x27;m also sharing some filesystems using virtio-fs. reply ComputerGuru 15 hours agorootparentDid you have to use any hacks to get a regular GTX&#x2F;RTX card to pass through? Last time I tried this with ESXi, it was insanely difficult and poorly documented to get non-Quadro cards to do pass thru (admittedly on a Windows guest). reply my123 14 hours agorootparentNVIDIA changed this in 2021: https:&#x2F;&#x2F;nvidia.custhelp.com&#x2F;app&#x2F;answers&#x2F;detail&#x2F;a_id&#x2F;5173&#x2F;~&#x2F;g... reply ComputerGuru 14 hours agorootparentThanks; that was after I tried to make things work and gave up. reply diffeomorphism 22 hours agoparentprevDo you mean windows using virtio? Then the answer would be red hat and since many years ago:https:&#x2F;&#x2F;pve.proxmox.com&#x2F;wiki&#x2F;Windows_VirtIO_Drivers reply virtioliker 22 hours agoparentprev(oh and to answer the other part of your question: I believe Red Hat contribute a lot to virtio-win) reply gnatolf 21 hours agorootparentThanks. I&#x27;m sorry if my question wasn&#x27;t particularly complex to answer ; - ) reply janandonly 15 hours agoprevOwh waawh. I see this article mentions drivers written by Rusty Russell, who I encourage everyone to follow on twitter (he is @rusty_twit) for his deep insights into software development. reply caycep 15 hours agoprevDo all commercial desktop VMs - VMWare fusion&#x2F;parallels&#x2F;UTM&#x2F;Vimy now use this virtio model?in theory win arm64 should run roughtly the same for all? reply naikrovek 10 hours agoparentIf they are using Virtualization.Framework they are all going to have the same performance. Apple made it very easy to create and use VMs with this framework, so I would expect most tools to use it. reply chaxor 15 hours agoprevMan is this the case.I have been trying to figure out how to have a single command to make a Qemu VM on an M2 Apple silicon chip for like a year without much luck.All I want is to run something like Alpine Linux + Sway WM on Qemu while on macOS or AsahiLinux with one command on cli.On x86-64 its fairly simple :( reply r-bar 14 hours agoparentLima (1) is a project that packages Linux distros for MacOS and executes them via qemu in the backend. Maybe you could solve your problem by launching one of their vms and inspecting the command line it generates. You might find an option you were missing.(1) https:&#x2F;&#x2F;github.com&#x2F;lima-vm&#x2F;lima reply chaxor 14 hours agorootparentI&#x27;ll check this out. There are many different systems out there like UTM and such, but I want the most basic &#x2F; minimal amount of dependencies, which will work basically anywhere - which is just QEMU. Not UTM, or maybe parallels, sometimes Lima, for Mac and then virtualbox for windows, and QEMU Linux type of nonsense. Just QEMU should suffice everywhere, and it&#x27;s much more secure that way. reply hinkley 14 hours agoparentprevI think this is basically what Colima is doing, if you’re willing to run docker containers to get it reply chaxor 11 hours agorootparentIt would be silly to install Colima for this though.If the argument is that Colima --calls--> Lima --calls--> {a ton of different things including kubernetes and docker and ...} --calls--> a QEMU command somewhere deep in the code, then the only thing that is required here is QEMU. Not kubernetes or any other junk on top that just adds complexity and potential insecurity.One QEMU command should be all that&#x27;s required. reply mschuster91 22 hours agoprev> Running older versions of macOS in a VM enables users to run Intel-only apps long after Rosetta 2 support is dropped from the current macOSNow if they&#x27;d offer that for x86 Windows guests... I mean, games are the obvious thing but I guess the architectural differences between Apple&#x27;s PowerVR-family GPU and NV&#x2F;AMD are just too large, but there&#x27;s a ton of software that only has Windows binaries available and which I still need either an Intel macOS device or an outright Windows device to run.Yes I know UTM exists but it&#x27;s unusably slow and the Windows virtio drivers it ships are outright broken. reply mort96 22 hours agoparentEven if you could get Windows working, what good would ARM Windows do?Honestly, running virtualized x86_64 Steam (using something like FEX) under Asahi Linux and using Proton seems like the most fruitful way to play Windows games on Apple Silicon hardware (at least once the GPU drivers mature). reply zamadatix 19 hours agorootparentARM Windows probably already does better than future Asahi+Proton+FEX in that it includes a Rosetta2&#x2F;FEX like layer of it&#x27;s own, is otherwise the native Windows without needing to fake that interface, and e.g. Parallels already has DX11 working through Metal without the need for a future version of Asahi drivers combined with the layer in Proton.The downside to either approach is anticheats. Games without them can run great today, games with them can&#x27;t run at all because they are kernel level x86 code and emulating the kernel architecture is too slow for games. It looks like Windows is doing another ARM push with higher end chips and less vendor exclusivity this time around - maybe that&#x27;ll finally get enough market penetration to make this less of an issue going forward, at which point virtualized ARM Windows could be nearly fully viable. reply nxobject 22 hours agorootparentprevThere’s one obscure use case that won’t work, sadly - people who have to use proprietary binary only drivers! I’ve been through hell trying to get Oculus Link to work. reply mschuster91 21 hours agorootparentprevI meant x86 Windows of course. No other way to flash Samsung or Mediatek phones, for example - the tools are all proprietary and only run on Windows. reply nottorp 17 hours agoparentprev> > Running older versions of macOS in a VM enables users to run Intel-only apps long after Rosetta 2 support is dropped from the current macOS> Now if they&#x27;d offer that for x86 Windows guests...Hmm the way i read it they&#x27;re running older ARM versions of Mac OS in the VMs. Not x86 versions. The virtualization infrastructure doesn&#x27;t do architecture translation, that is done in software by the OS running inside the VM.As for x86 games... they run pretty well with x86 crossover emulating x86 windows that is then translated by rosetta 2 to arm... is your head spinning yet? reply ngcc_hk 9 hours agorootparentIf the op premises partially is about when Rosette v2 no longer support, at least the older vm arm based macOS can run Apple intel App using the now current then obsolete Rosette v2.Never thought of that but it happened to power pc App …Tbh they should keep it as unlike powerpc not much people use, intel based app is all around. Having both intel and arm, only one upcoming platform is missing. But supporting a translator as said implicitly in other post is hard. New intel&#x2F;amd cpu code may appear, ignoring all those amd and nivdia Gpu code which are mostly not supported anyway. reply cactusplant7374 22 hours agoprevIs it possible to virtualize 32 bit? reply zamadatix 19 hours agoparentVirtualize no, there is no hardware support for 32 bit ARM on Apple Silicon. You can emulate it (32 bit ARM or x86) just fine though. Emulating the whole OS will be relatively slow compared to emulating just a userspace binary. reply transpute 22 hours agoprevApple Silicon M2+ has hardware support for nested virtualization.It&#x27;s rumored [1] that 2024 iPad Pro will see price hikes of $500-$700 to cover the OLED screen and increases in base memory&#x2F;storage. If a new iPad Magic Keyboard gains [2] an aluminum shell that looks like a Macbook, that could put iPad Pro into the price tier of Macbook Pros.If 2024 iPad Pro + Magic Keyboard costs > Macbook Air + Mac Mini, that may allow Apple to untie iPad Pro M3 nested virt for iOS, macOS and Linux VMs.[1] https:&#x2F;&#x2F;www.tomsguide.com&#x2F;news&#x2F;ipad-pro-2024[2] https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;9&#x2F;3&#x2F;23857409&#x2F;ipad-aluminum-mag... reply NikolaNovak 20 hours agoparentI am a person outside of apple ecosystem that has to use iPhone and occasionally iPad for work.Question : how do you manage your files?My wife hears a primeval scream from our home office every 3 months when I determine to try to get files off my iPhone (voice memos, photos, downloads, whatever) or God forbid put files on.Even worse screams when I try to manage files on device such as \"delete all photos\" (cannot.be.done).And I degenerate into gurgles when I try to find or manage different files (a downloaded jpeg is \"not\" a photo and cannot be found via photos app,has been my bitterly learned experience. Because reasons).I know modern generations are more comfy outside of hierarchical folder &#x2F; file structure and treat their device like a massive database, which, fine in principle. But after 4 years of iphone usage I still see it as a massive black hole where files go in but don&#x27;t come out. So I... Cringe with terrified shakes when people talk about iPads for work. How do your organize your files on them? How do you manage and transfer and version control?Or am I a dinosaur and everybody&#x27;s files are emephereally in the google or apple cloud and it&#x27;s just not a problem, things are magically right and where they need to be? reply oblio 19 hours agorootparentNo, people just suffer in silence.There are famous Youtubers like MKBHD that more or less every year say:\"The new iPad is great, the hardware is awesome, I use the iPad a ton, but I can&#x27;t use it to replace a laptop because of the lack of file management&#x2F;window management&#x2F;...\".I have heard this text in similar forms for at least 3 years.You can make do, but it is as awkward as you&#x27;d expect.The only winning entity is Apple, that gets people to also buy laptops and to be even more locked into this crippled setup, since as you said, younger generations aren&#x27;t as aware of the possibilities, anymore. reply teaearlgraycold 18 hours agorootparentI don’t think MacOS is a crippled system. Agreed that trying to use an iPad as a primary device is torture. But, compared to Windows, MacOS is comparably accommodating of my needs as a developer. reply rjzzleep 18 hours agorootparentIs it though? I mean, I do remember the same, but I just booted into Win 11 after buying a GPD Win and it looks nice. Microsoft seems to have resigned itself to the fact that as a developer you should use WSL2.If you do any kind of docker related development you will inevitably install something similar to WSL2 using docker desktop or whatever. Technically it now supports native containers, but we&#x27;re not there yet. reply nottorp 17 hours agorootparentWSL2 is also a virtual machine isn&#x27;t it? It virtualizes x86 linux on x86 windows, kinda seamlessly, but still that&#x27;s all it is. reply rjzzleep 16 hours agorootparentWhich is exactly what docker desktop on macOS does as well. Unless you&#x27;re doing iOS or macOS development, contrary to common belief WSL2 is actually integrated better than it&#x27;s mac counterparts. You can even mount other linux partitions into WSL. reply nottorp 16 hours agorootparentDocker desktop is a piece of crap on macOS. It allocates half your ram for a linux VM and then allocates other linux VMs inside it. If you&#x27;re doing servers, that&#x27;s 7+ Gb of ram wasted since your work VMs will at most use hundreds of megabytes.I sure hope WSL does better :) reply icedchai 11 hours agorootparentDocker desktop works fine. You can change the memory settings, you realize? reply teaearlgraycold 16 hours agorootparentprevI like the MBP hardware. I think once I feel comfortable relying on Asahi it would be nice to run that instead of MacOS. replyoblio 9 hours agorootparentprevI meant kids aren&#x27;t even moving to full blown computers and stay on crippled touch platforms like the iPad. reply jxdxbx 19 hours agorootparentprevI manage files on my iPad (and iPhone) with Files and iCloud Drive. It’s been around for a while! The problem is that many apps are still stuck in 2015. But for apps that support it, using the Files file picker is no different than using the Mac file picker and Finder. You open files, you save them, they sync. Some apps do default to their own folder in iCloud Drive, but that folder can be accessed by any other app and is also available on the desktop.Sadly third-party support for Files plugins is not what it should be (Google Drive is so incomplete I don’t know why they even bother). The major cloud services want you using their apps, I guess.But Secure Shellfish does it perfectly so my Windows media server is available as a “file system” on my iPhone and iPad via SFTP. reply TheCoreh 15 hours agorootparentprevThe Files app allows storing files locally, and mounting network shares. You can also seamlessly copy and paste files (via handoff) between macOS and iOS.I typically just hit Cmd+C on the Mac and long press+Paste on Files on the iPhone. If you are using the iPad with an external mouse or trackpad you can also drag and drop it directly to the Mac.As for the distinction between random JPEG files and the Photos app, I think that&#x27;s actually quite good. I don&#x27;t get my gallery littered with random images, and it also supports non destructive editing, among other features. Moving between the two is also fairly easy, you can use the Share sheet or just drag and drop.The one thing I would change is that screenshots end up in Photos.app by default, I&#x27;d rather have them go to Files. reply NikolaNovak 12 hours agorootparentThx for your reply!>> As for the distinction between random JPEG files and the Photos app, I think that&#x27;s actually quite good.Please don&#x27;t take this personally, but that always terrifies me. It&#x27;s like modern apple owner \"sour grapes\" fable - \"I actually love this random limits tion, it makes my life much easier \" and I hear it a lot! If I right click and save photo in some apps or websites it is in photos app, but in random other apps that same file is no longer a photo. How&#x27;s that good? There are a million ways to \"not clutter\" that are better. Folder might be one but if that&#x27;s anathema, then albums or tags. It&#x27;s a completely random subset of things that end up being photos vs not, seemingly based on location or tags that arr neither visible or accessible to me as a user. I get that this is \"good\" for some people, I am clearly not in that group though.Re ease of copying files, does any of that work if you don&#x27;t have a Mac? Context of conversation here is iPhone &#x2F; iPad as independent working devices and ability to transfer files without a Mac OS device. I am readily convinced that if I bought whole heartedly into apple ecosystem and only apple,my life would be easier along some axis, but that&#x27;s not a life I lead - I have the black box of iphone and I cannot for example delete all photos on it in any way that I could find including in the app, in the settings, via apple support or apple store creepily smiling people :-&#x2F;. reply VogonPoetry 10 hours agorootparentIt seems like you want to delete all of the photos on your iPhone.I have not tested this because I don&#x27;t want to delete all my photos and I don&#x27;t have a 2nd set of systems to try this out with, but I think this can be done by creating a \"shortcut\" to do it. (Shortcuts are like AppleScript for the iOS ecosystem).To do this, search for the \"Shortcuts\" app and run it (it usually isn&#x27;t visible).- Create a new shortcut - Add the \"Find Photos\" action - Add the \"Delete Photos\" actionThis should connect the two actions together. You can then run it using the play button.This will take a very long time to run if you&#x27;ve got a lot of photos and it will ask to confirm using a popup. It might be worth trying to remove items in smaller chunks by using a filter (perhaps based on date or some other criteria).I hope this helps. reply TheCoreh 11 hours agorootparentprevNot taking it personally, :-) I 100% understand why you might also prefer it the other way.The weird \"some apps save it to Photos while others save it to Files\" situation is a consequence of Files being a relatively late addition to the iOS ecosystem. A lot of apps are poorly maintained, use some cross platform framework that doesn&#x27;t support the Files feature well, or the developers are simply unaware of the distinction. It will probably get better over time.One thing Apple could do in the mean time is to also expose Photos as a folder view inside of Files (they do this on macOS, to some extent, on the file pickers. I&#x27;ve never actually used it)Re: Transferring it to a PC, the one thing that won&#x27;t work is the seamless copy and paste via handoff. You can plug in a USB stick into an iPad or iPhone (using an adapter for pre-15 models, or if the USB stick is USB-A) formatted as exFAT and it should just work.AFAIK, there isn&#x27;t a single button to delete all photos, probably to avoid people doing it accidentally. You&#x27;ll need to manually select all photos and hit delete. Or you can also write a small script via the Shortcuts app to delete them for you. reply GeekyBear 18 hours agorootparentprevThe Files app can connect to various cloud services &#x2F; local servers by adding locations.For example, you can add a location for a folder shared via SMB from your Windows based computer.https:&#x2F;&#x2F;osxdaily.com&#x2F;2019&#x2F;11&#x2F;04&#x2F;how-connect-smb-share-iphone... reply AnonymousPlanet 8 hours agorootparentprevI use KDE-Connect. Connects my Linux desktop and any Android or iOS device. It&#x27;s originally a Linux Application but runs on Windows and MacOS as well.You can send&#x2F;receive files, photos, clipboard, notifications etc. On Linux I can also use it to control media and use my phone as mouse or keyboard.The pairing is painless via QR-Code. You decide what is shared and what isn&#x27;t. It works directly over your local network. No cloud servers are involved. reply matwood 19 hours agorootparentprevFor photos, either Photos app or Lightroom cloud is what I have used. I have a usb-c sd card reader that I use to upload photos onto the iPad. From there they end up on all my devices. The nice thing is this works if I instead upload them onto my MBP or took pictures with my iPhone.For files, iCloud has worked fine.Personally, I don&#x27;t want to think about moving files from one device to another. I want them available on all devices regardless of where they were created&#x2F;added. reply jahewson 14 hours agorootparentprev> Or am I a dinosaur and everybody&#x27;s files are emephereally in the google or apple cloud and it&#x27;s just not a problem, things are magically right and where they need to be?Yep! Use iCloud and unburden yourself from ever thinking about files again. reply callalex 11 hours agorootparentprevI have a shared iCloud folder with my dad with a few .mp4s in it that will consistently cause a hard crash on any iOS device by just…viewing the folder in Files. It crashes so hard that the entire system locks up and you can’t close the app, and holding down the power button doesn’t work to restart. You have to wait for the device to actually overheat and then shut itself off to cool down before you can bring it up again. reply PlunderBunny 14 hours agorootparentprevRe: delete all photos, did you know that - if you are viewing a list of photos in an ‘album’ - you can click the Select button at the top right corner of the screen and then drag-select all the files? It’s quite tricky to do - you have to tap to select the first file, then touch and immediately drag to do the second file onwards. Took me years to discover this by accident - it’s the most fiddly&#x2F;weird&#x2F;hidden feature in an operating system that has become increasingly full of them. reply NikolaNovak 12 hours agorootparentI thank you for your reply, but are you trying to tell me drag selecting 50k photos is the way to go?(And if people start screaming \"why do you have 50k on your phone??!?\", I&#x27;ll start screaming right back \"because I cannot offload or manage or delete them!!!\" :-) reply zx8080 18 hours agorootparentprevWhy do you need files out? Just buy more iCloud storage. Or how is it supposed to work in iEcoSystem? reply NikolaNovak 12 hours agorootparentI assume you&#x27;re sarcastic but I already pay for Icloud and it doesn&#x27;t help me meaningfully manage files or move them out of apple ecosystem :-( reply alberth 18 hours agorootparentpreviCloud.Dropbox is a close 2nd, but won’t do everything you described (like download folder) - but iCloud will. reply NikolaNovak 12 hours agorootparent\"Icloud\" and... Then what? I pay for Icloud and I still cannot manage files or offload them easily. I have 50k photos by now because I&#x27;ve struggled for years, so any tip that starts with \"drag select photos and then...\" can bugger off :-))))I&#x27;ve installed the monster of iTunes on my windows and that shucked remaining life out of me. Then I installed Icloud for Windows or whatever it was called and I oscillated between murdering myself and others. It just doesn&#x27;t work. At best I was able to slowly drag and select 1000 photos at a time to get crippled small version of the files. reply foobiekr 17 hours agorootparentprevI spent yesterday recovering some files that had silently reverted to October 2023 versions on - no kidding - December 24th. I only noticed it yesterday morning when I opened a spreadsheet and was absolutely baffled.This is the second time iCloud has fucked me. As much as I want to use it I no longer trust it. reply r3d0c 18 hours agorootparentprevpay apple again to be able to manage your own files, lol.. reply alberth 18 hours agorootparentiCloud is free (up to 5GB). That seems fair.https:&#x2F;&#x2F;www.apple.com&#x2F;icloud&#x2F;#:~:text=Is%20there%20a%20free%....Which mobile platform provides unlimited&#x2F;better for no cost? reply smoldesu 17 hours agorootparent> Which mobile platform provides unlimited&#x2F;better for no cost?For one, Android. I use Syncthing; my phone reports that I&#x27;ve synced 27gb of local state to my PC and laptop without me paying a dime.Caveat being, you have to use a mobile platform that doesn&#x27;t prevent third-parties from integrating with the OS. iCloud&#x27;s quality is almost besides the point when Apple uses their software control to ensure a feature-complete alternative can&#x27;t exist. reply ylk 16 hours agorootparentYour phone manufacturer gave you a box with syncthing + storage for free with purchase of your device?Nextcloud also works on iOS, integrates with the Files app and was always able to sync photos right after I took them. reply InCityDreams 17 hours agorootparentprev>Which mobile platform provides unlimited&#x2F;better for no cost?Could you explain how it is free?I mean, could it be possible that the actual cost of the &#x27;free&#x27; icloud is built into the prices&#x2F; cost of the device(s) you originally purchased (so that you can store your stuff in the icloud)? reply DavidPastrnak 18 hours agorootparentprevicloud keeps everything synced across my devices seamlessly - M1 Air, iPhone, and iPad. reply overstay8930 18 hours agorootparentprevWhy would you use an iPhone if you don&#x27;t want to use iCloud? That is the entire point of buying into the Apple ecosystem. reply r3d0c 18 hours agorootparentso you have to pay apple an ongoing fee to be able to manage your own files?does that seem rational?also such a weird line of thought that buying a single apple product isn&#x27;t enough to be able to use it properly, and that any criticism of apple is just \"us plebs using it wrong and not paying them more money\" reply DavidPastrnak 18 hours agorootparentYou don’t have to pay Apple to manage your files. You can manage them with a traditional file manager if you’d like akin to any other device.If you want cloud storage, Apple provides free iCloud storage that will keep everything synced across your devices. There is an upper limit to the free tier space, at which you can purchase additional storage or move to a cloud platform of your choice. reply nottorp 17 hours agorootparentConsidering how much of a premium you pay for the iPhones, that upper limit is stingy like hell.And Apple&#x27;s marketing ain&#x27;t great either. They push your photos to iCloud by default, which fills the free space instantly, then when you try to turn that off they give you a vague and threatening message that your photos will be lost.Marketing by threats will make me to at best give money to the competition. reply DavidPastrnak 17 hours agorootparentDo you have the text from the message that says all of your photos will be lost? I’ve never seen it. reply nottorp 16 hours agorootparentYeah right, I&#x27;m hallucinating and so is my wife. More likely, you consider this type of sales copy normal and didn&#x27;t notice it. reply DavidPastrnak 15 hours agorootparentI use the Apple one family plan which is 2TB of storage so I’ve likely simply never seen it. replyNikolaNovak 11 hours agorootparentprevBut I do have and pay for Icloud.And then what? There&#x27;s a dozen messages here that say \"Icloud\" and I guess that&#x27;s the point, people use cloud and done care for details. But I do! I want to offload the files and put them on my NAS and on my backup off site drive and manage and organize them. Icloud is not a step in that direction (maybe it is if you have a Mac laptop but while point here is discussing iphone and iPad as their own devices.). reply danieldk 21 hours agoparentprevIt&#x27;s rumored [1] that 2024 iPad Pro will see price hikes of $500-$700 to cover the OLED screen and increases in base memory&#x2F;storage.I am surprised that such a price hike is necessary. You can buy a new Galaxy Tab S9 with an excellent OLED screen from Amazon for $740.If 2024 iPad Pro + Magic Keyboard costs as much as Macbook Air + Mac Mini, hopefully that will allow Apple to untie the iPad and allow it to run iOS, macOS and Linux VMs.Unlikely. Apple is in the business of selling you a MacBook, iPhone and iPad. Even more now update cycles are slowing down. So, it&#x27;s pretty unlikely that they&#x27;d go the route of Samsung DeX (which allows you to use a phone or tablet as a desktop).(Yes, I know that you can hook up an iPad to an external screen, but it is not really a full desktop experience.) reply jwells89 20 hours agorootparentIt’s rumored that the OLED panel used in the new iPad revision won’t be a bog standard OLED, but instead a variant that emphasizes longevity and burn-in resistance by stacking two OLED layers atop each other (on top of the usual binning Apple does). That makes the price hike sound more plausible. reply transpute 21 hours agorootparentprev> I am surprised that such a price hike is necessary.They are adding a 12.9 inch iPad Air, so they have an opportunity to differentiate iPad Pros from Air to justify the price difference, https:&#x2F;&#x2F;www.imore.com&#x2F;ipad&#x2F;ipad-air&#x2F;129-inch-ipad-air-on-tra... The grand plans include a supersized iPad Air for the first time, and it seems like we&#x27;re on track to see it launch in March 2024. Display analyst Ross Young has confirmed that the display shipments of the 12.9-inch iPad Air began in December.> you can hook up an iPad to an external screen, but it is not really a full desktop experience.Stage Manager does inch closer to a desktop experience, with apps in movable windows. Imagine a macOS VM in a large window on external monitor, alongside a small iOS app&#x2F;VM window. With a cheap USB-C capture card, an external video or camera input can appear in an app window.> Apple is in the business of selling you a Macbook, iPhone and iPadIf Apple can get same-or-better margins&#x2F;revenue than Macbook+iPad with an iPad Pro, with less physical hardware thanks to virtualization, why not save on atoms and shipping? The iPad Pro has long been overpowered for the few iOS-approved use cases. Virtualization would finally unlock that power. Avoids carrying multiple devices. Eliminates any dependency on sidecar Raspberry Pi or cloud VM for Linux workloads. reply Xylakant 20 hours agorootparentprev> (Yes, I know that you can hook up an iPad to an external screen, but it is not really a full desktop experience.)I have defaulted to iPad as mobile computer for a while now, instead of carrying a laptop around. It works well enough for most office tasks, with some trickery even for light on-call support. And it’s definitely improving over time. The major pain point for me is currently file management. reply overstay8930 19 hours agorootparentWhy not just use a MacBook Air or something? It&#x27;s basically the same price.I tried switching to iPad and the only thing I keep thinking about was \"this is just my Mac, but worse in every single way\" reply Marsymars 15 hours agorootparent> Why not just use a MacBook Air or something? It&#x27;s basically the same price.Not the person you posed the question to, but my reasoning is mostly that my MacBook Air is docked with my desktop peripherals when I&#x27;m home, and it&#x27;s cumbersome to undock&#x2F;redock it all the time, so I use my iPad if I&#x27;m not at my desk. If I need to do something that I can&#x27;t do on my iPad, then I walk to my desk where I have a proper mouse&#x2F;keyboard&#x2F;monitor. I only undock my MacBook every few months when I&#x27;m travelling and need a real computer on the go. reply Xylakant 18 hours agorootparentprevI use the small iPad Pro, even the MacBook Air doesn’t come close in terms of weight and form factor. I did use the tiny MacBook Air, and I’d love a 12” MacBook, but they no longer exist.On top of that, the combination of iPad, pen and paperlike screen protector is really nice for taking notes. The option to undock from the keyboard and just take the tablet is also nice.I agree that it’s worse on pretty much every other metric and that it’s an optimization for one specific metric, but it’s workable.And plugged into a decent screen, it’s pretty ok for most office tasks. reply beeboobaa 20 hours agorootparentprevOf course it&#x27;s not necessary, but when apple sees a way to gouge for more money, they do it. reply zamadatix 19 hours agoparentprev$500-$700 rumour sounds like something to get you to click and share their article rather than an honest estimate. Their logic for the two numbers is the panel is estimated to cost $250-$350 (depending on size) and they estimate a 50% profit margin on the iPads so the base model will be 2*$[250,350]=$[500,700] more... which means they must calculate the existing screen to be completely free? They don&#x27;t mention anything about the base specs increasing in that root article but even if they did that&#x27;s not clear to be an actual increase in production cost. It&#x27;s a newer device after all.I expect a price increase of some sort, it&#x27;s the safe thing to bet on and anybody else could safely write about that too, but I&#x27;m already disappointed how much time I&#x27;ve spent talking about a clickbait future Apple device rumour news article which attempts to create the worst possible number they think they can get away with claiming as realistic. reply raccoonDivider 22 hours agoparentprevI don&#x27;t understand why they&#x27;re trying to turn iPads into laptops. Just start from their existing laptops and make them more mobile instead of trying to inflate a phone OS into something that does the job? Is this about control over the apps people can run? reply neilalexander 21 hours agorootparentIn many ways, an iPad with a keyboard is probably the perfect home computer for people who don&#x27;t really care about computers and just have simple requirements. The apps that people generally expect to find are there and a keyboard just makes it that bit more comfortable to sit and bash out an email or letter. reply beeboobaa 19 hours agorootparentSure, if you want to breed even more generations of computer illiterates. We should be encouraging people to learn about the computers they use so they can do actually useful stuff with it later in their life. Not just \"hey here&#x27;s an app, now go make me more money by looking at ads\" reply matwood 19 hours agorootparentSomeone using their phone or tablet with a keyboard to get things done is far from computer illiterate. For the majority of the population computers are a tool. Knowing deeply how they work is about as important as knowing deeply how their car works. reply beeboobaa 18 hours agorootparentIf all they ever have access to is phones then their world consists solely of software they have been allowed to install by their platform overlords. Even if they had the urge to try and create something themselves, they would be forbidden from doing so.Just keep consuming those ads and don&#x27;t think about it. reply anonymousab 18 hours agorootparent> their world consists solely of software they have been allowed to install by their platform overlordsThe same will be true of most cars within a generation, and is effectively true for most car owners now; they do not really know how to do much with their car beyond drive it, use the infotainment as-is and bring it in for repair when anything seems off. reply beeboobaa 17 hours agorootparentYes, everything is being fucked by the drive for profit. reply RunSet 16 hours agorootparentprev> Someone using their phone or tablet with a keyboard to get things done is far from computer illiterate.Full literacy involves writing, not just reading. At one point the same held for computer literacy. I would not call someone \"literate\" if they could only read words they already recognized from viewing forms and their writing ability was limited to filling out those forms using a limited but appropriate vocabulary. I would likewise not consider someone computer literate if they were limited to using software written by others.For more eloquent words in this vein:https:&#x2F;&#x2F;citejournal.org&#x2F;volume-2&#x2F;issue-3-02&#x2F;seminal-articles... reply nottorp 17 hours agorootparentprev> Someone using their phone or tablet with a keyboard to get things done... if you don&#x27;t get a lot of things done.The main quality of a laptop is the keyboard is solidly attached to the screen. That means you can use it anywhere and you don&#x27;t need to dedicate a desk like space for the keyboard.With an iPad you need a stand, space for the keyboard and then you&#x27;re close to the space taken by a monitor with peripherals and a desktop under the desk. Might as well get a desktop then since it&#x27;s more powerful.It may be useful for tasks that only need a keyboard 1% of the time though. reply lotsofpulp 19 hours agorootparentprevDoes this apply to cars&#x2F;appliances&#x2F;medical equipment&#x2F;any other tools?I don’t see anything wrong with people excelling at some tasks, such as CAD&#x2F;medicine&#x2F;construction&#x2F;editing media&#x2F;law&#x2F;etc, and not excelling at understanding all the details about how their tools work. reply beeboobaa 18 hours agorootparentYes. Cars are turning into pieces of shit that need a subscription because techbros made them too complicated for an average person to understand. Appliances, same story. Techbros are turning goddamn printers into a subscription service. reply lotsofpulp 18 hours agorootparentI guess I will have to disagree. My cars have been lasting longer and longer, and the cost per mile keeps going down.My appliances have also been working fine for 5+ years. LG inverter motor is dead silent in my fridge, and I get the benefits of having a French door fridge on top and freezer drawer on the bottom. Same for all the other appliances I have too. I don’t expect them to last 20 years, but as long as I get 5 to 10, I’m ok with it considering the price I paid.My brother printers have been working fine for many years, and at least as of 2021, the MFC printers did not need a subscription.Maybe things have changed and I haven’t needed to buy anything in the last couple years. reply beeboobaa 17 hours agorootparent> Maybe things have changed and I haven’t needed to buy anything in the last couple years.It has. Good luck finding a new printer that doesn&#x27;t (figuratively) spit in your face repeatedly. reply lotsofpulp 16 hours agorootparentI have this one and it works great. No subscription needed or any funny business.https:&#x2F;&#x2F;www.brother-usa.com&#x2F;products&#x2F;mfcl2710dw replychongli 21 hours agorootparentprevYeah not to mention it’s way easier to use than macOS.Macs used to be so easy to use on Classic Mac OS. Mac OS X really left a lot of people behind on the usability front. It became much more of a power user OS. Then iPads came along and stole that group (of ordinary users) away.But now it seems they’re adding more and more power user features to iOS, complicating things again (with even less discoverability due to complex gestures). History seems to be repeating itself. reply user_7832 20 hours agorootparentAs someone who’s never used MacOS fulltime, what did OS9 do better than X? I’ve found modern MacOS fairly similar to windows in common tasks and interface. reply neilalexander 19 hours agorootparentGoing somewhat off-topic here but classic Mac OS had very precise human interface guidelines[1] which strongly emphasised repeatable behaviours and recognisable patterns. For that matter, so did earlier versions of Windows[2]. A lot of thought went into visual cues and design elements so that things looked and acted predictably system-wide and they were designed so that it would always be obvious which elements were and weren&#x27;t interactive.Both Apple and Microsoft have regressed in this respect. Minimalism and prettiness have taken priority over usability in both modern macOS and modern Windows and they are far more inconsistent and harder to learn to use as a result. Often something that you learn in one place place or app now doesn&#x27;t work in another.In Apple&#x27;s case this has been mostly as a result of their efforts to make macOS and iOS more alike and to share applications&#x2F;components across the two, which often creates weird-feeling results and awkward app designs. In Microsoft&#x27;s case this is mostly because they have more UI frameworks than sense and each new one introduces more problems than solutions. Electron-adjacent apps probably don&#x27;t help matters either, since they also generally break all of the platform rules and implement their own UI controls anyway.[1] https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.5555&#x2F;573097 [2] https:&#x2F;&#x2F;ics.uci.edu&#x2F;~kobsa&#x2F;courses&#x2F;ICS104&#x2F;course-notes&#x2F;Micro... reply chongli 10 hours agorootparentprevThe Classic Mac OS Finder used a spatial metaphor. When Apple moved to OS X, they copied Windows by switching to a browser metaphor [1]. For many Classic fans this was Apple&#x27;s biggest mistake. Ever since then people generally go out of their way to avoid using the Finder altogether because it&#x27;s so unpredictable and opaque.[1] https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2003&#x2F;04&#x2F;finder&#x2F; reply nottorp 19 hours agorootparentprevThat&#x27;s the problem. For example modern MacOS has in your face notifications and allows applications in the background to steal focus.I gather classic Mac OS was done so you can get on with whatever you&#x27;re doing and nothing bothered you. reply lotsofpulp 19 hours agorootparentThis is easily remedied.https:&#x2F;&#x2F;support.apple.com&#x2F;guide&#x2F;mac-help&#x2F;turn-a-focus-on-or-... reply nottorp 17 hours agorootparentNo. It should be the default. And I bet it only refers to notifications, not to other applications stealing focus (as in bring themselves to the foreground) from the one you&#x27;re into because they think they&#x27;re damn important. reply raccoonDivider 12 hours agorootparentDo Not Disturb by default is a question of taste and use case, they probably brought to the desktop what people seemed to like on mobile devices.Applications stealing focus is a plague though. Maybe Apple will finally figure out that it&#x27;s not worth having in their API. replyjxdxbx 20 hours agorootparentprevI use a combo of desktop computers with giant screens and an iPad. I like this better than having a laptop. I don’t think the traditional multi-window paradigm works well on a very small screen (though I am aware it was invented for tiny screens!). When I’m mobile I prefer to have just one app at a time, or at most Stage Manager.The biggest problems I run into with iPadOS are not related to the OS, but stripped-down apps, or apps that don’t use the file picker and other iPad features. In a few cases I have to use web apps (which work perfectly) instead of iPad apps, for example with Google Docs, since the iPad apps are more like stripped-down phone apps. reply jwells89 20 hours agorootparentAgree that my biggest gripe with iPadOS is third party apps that don’t take advantage of the platform. Cross-platform apps are the most notorious, usually being stretched out phone apps rather than proper tablet apps.It’s still a far sight better than the Android tablet situation though, where stretched out phone apps are the norm instead of the exception. reply ako 21 hours agorootparentprevHow would you make a laptop more mobile? I think they&#x27;ve gone too small and too thin in the past, now settling on larger laptops.If i didn&#x27;t need to program on my computer, i&#x27;d use an ipad as a single computing device for everything. It&#x27;s perfect for couch consumption, and with stage manager, an external bluetooth keyboard and mouse, it&#x27;s more than adequate for anything else you&#x27;d expect from a computer: office, photo and video editing and watching, internet browsing, email, etc.For 95% of all use cases, the ipad already is the best laptop. reply transpute 21 hours agorootparentprevPerhaps they are turning laptops into iPads. The price&#x2F;performance of Apple Silicon laptops was a descendant of early iPad Pro SoCs, with current iPad Pros on M2. A couple of years ago, MacOS on Apple Silicon gained the ability to run iOS apps, either via the Mac appstore or by copying .ipa files. reply rtpg 20 hours agorootparentprevipad touchscreen is good for reading documents and the like. While I&#x27;ve been a bit of a \"make Macbooks with touchscreens you cowards\" person, iOS (iPad OS but w&#x2F;e) has a _lot_ of nice affordances that are centered around getting you quickly to your work in a couple of taps, and not futzing about with typing things in.The thing I always think about: how fast it is to play an MP3 from \"device in pocket\" state with an MP3 player vs a computer (or my phone!). iOS affordances around that are good.Having said that... maybe there&#x27;s a new shell that MacOS could use to get there. They seem to be trying with some changes though I don&#x27;t really enjoy the changes so far reply gumby 20 hours agorootparent> The thing I always think about: how fast it is to play an MP3 from \"device in pocket\" state with an MP3 player vs a computer (or my phone!). iOS affordances around that are good.This is a very important metric! Jeff Hawkins famously walked around with a piece of wood in his pocket the planned size of the Palm Pilot, and when he wanted to do something (write down a note) he would work through how many key presses it would take on the new device. His limit was three.When I tried a BlackBerry I was infuriated by how many key presses everything took. What a horrible experience.> Having said that... maybe there&#x27;s a new shell that MacOS could use to get thereLike it or not, Apple’s plan for this remains Siri. reply rafaelmn 21 hours agoparentprev> hopefully that will allow Apple to untie the iPad and allow it to run iOS, macOS and Linux VMs.Why would they do that ? They want their 30% on everything you install on iOS. reply transpute 21 hours agorootparent> 30% on everything you install on iOSThat&#x27;s likely changing soon in EU and Japan.https:&#x2F;&#x2F;asia.nikkei.com&#x2F;Business&#x2F;Technology&#x2F;Japan-to-crack-d...> The Japanese government sees this model as solidifying the companies&#x27; dominance in the mobile market. The legislation aims to force them to allow third-party app stores and payment systems as long as they are secure and protect user privacy. Japanese companies would be able to run dedicated game stores on iOS devices, as well as use payment systems with lower fees from Japanese fintech companies.https:&#x2F;&#x2F;www.computerworld.com&#x2F;article&#x2F;3711375&#x2F;coming-soon-to...> You download an app from Apple’s App Store and then use it to access the enterprise app store. There’s still a step where Apple inserts itself — the enterprise app store is itself an app that Apple has vetted and allowed in its own App Store. Most likely Apple will want alternatives to its App Store to work the same way. reply ink404 21 hours agorootparentprevlikely they want to support using Xcode to develop apps on iPad reply MissTake 20 hours agorootparentprevThat changed years ago.Most developers now see a 15% hit, only going to 30% once they’ve hit certain thresholds. reply zamadatix 20 hours agorootparentWhile that&#x27;s somewhat not as horrible for new developers I wonder how far that actually puts Apple&#x27;s average cut from 30% (in terms of revenue not developer count) or how much it changes the point that it&#x27;s nowhere near 0%. reply eptcyka 18 hours agoprevDaily reminders that apple only allows 2 concurrent virtualised instances of macOS to run on their hardware. reply arianvanp 17 hours agoparentIs that a technical or a contractual limitation?edit: I fucked around and found out:The number of virtual machines exceeds the limit. The maximum supported number of active virtual machines has been reached. reply ComputerGuru 15 hours agorootparentUse a better hypervisor like ESXi (but I don’t think a different hyoervisor is available for Apple silicon). reply naikrovek 10 hours agorootparentWorkarounds or not, the MacOS license agreement only allows two concurrent MacOS VMs on a single physical machine.I’m sure you could patch qemu to use Hypervisor.Framework as a virtualization backend and then you could run more than two, but you’d still be violating their license agreement.Apple are very weird about MacOS virtual machines. reply nsteel 22 hours agoprevCould the title of this piece also be \"Why are arm VMs so different?\" or is this actually specific to Apple&#x27;s chips? Wouldn&#x27;t anyone transitioning between two architectures while maintaining compatibility be in the exact same situation?I&#x27;m just curious what&#x27;s special in this case (if anything). reply neilalexander 22 hours agoparentThe post is more about VirtIO than it is about the processor architecture. VirtIO is not ARM-specific. reply IMcD23 21 hours agorootparentIt’s not Apple Silicon specific either. I don’t understand the title. Maybe it should have been “Apple’s virtualization API and VirtIO driver support on Apple Silucon” reply JohnBooty 18 hours agorootparentTechnically, no. Effectively, sort of. When they transitioned to Apple Silicon they simultaneously transitioned to Virtio. reply bonzini 16 hours agorootparentIndeed, Virtualization.framework already supported virtio in guests before, but that&#x27;s when they added host drivers. By the way this:> In the Virtio model, providing such support is the task of the operating system, not the virtualiser.is wrong. Virtualization.framework is a standard implementation of a virtualiser that is shipped with macOS, and while it includes virtio, it does not have to be part of the OS; the same task can be done by anyone (for example QEMU).The low-level, OS-dependent part of virtualization support is called Hypervisor.framework and it does not have any knowledge of virtio. replysergiomattei 22 hours agoprevGreat post. This is a massive change: now we get macOS VMs with full graphics performance and QE&#x2F;CI.This was impossible on Intel machines without PCI passthrough of a compatible GPU (on a Hackintosh). reply blikdak 21 hours agoprev [–] AI generated nonsense, transition from intel to arm architecture has nothing to do with virtio. reply AshamedCaptain 21 hours agoparentThe article is literally contentless. It doesn&#x27;t answer its own question. I don&#x27;t know if its AI generated or a marketing fluff piece. Virtio is nothing but an interface&#x2F;protocol and won&#x27;t magically make your VMs any different -- in fact it was already commonly used in x86 VMs. reply Klonoar 21 hours agoparentprev [–] ...you actually think a notable Apple-centric blog is AI-generated nonsense?Am I reading this right? reply bonzini 16 hours agorootparent [–] It does have a lot of confusing or downright wrong content. Saying that it&#x27;s hallucinations is in some sense a compliment to the author... replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple has implemented device support in macOS using Virtio drivers, enabling efficient virtualization on Apple silicon Macs.",
      "This approach provides Apple with more control over hardware and feature support, but it diminishes the commercial value for third-party virtualization vendors.",
      "Virtualization on Apple silicon Macs offers flexibility for running older macOS versions and incompatible software, and the article suggests that the lightweight virtualization and Virtio could drive further growth in the field."
    ],
    "commentSummary": [
      "The article compares the performance and compatibility issues of running Windows on virtual machines (VMs) on Apple Silicon compared to Windows VMs reliant on Hyper-V.",
      "Participants discuss the use of Hyper-V for security features and compatibility with Linux and Windows operating systems but express mixed opinions on using VMs for securing software applications.",
      "Users share experiences and recommendations for running Windows apps and games on Mac, including translation layers like Crossover and cloud gaming services like GeForce Now. Additionally, frustrations about file management on Apple devices are discussed, with alternative apps and services suggested."
    ],
    "points": 283,
    "commentCount": 225,
    "retryCount": 0,
    "time": 1703845778
  },
  {
    "id": 38807720,
    "title": "US Steel's Decline: Lacking Innovation and Falling Behind Minimills",
    "originLink": "https://www.construction-physics.com/p/no-inventions-no-innovations-a-history",
    "originBody": "Share this post “No inventions; no innovations” A History of US Steel www.construction-physics.com Copy link Facebook Email Note Other “No inventions; no innovations” A History of US Steel Brian Potter Dec 29, 2023 89 Share this post “No inventions; no innovations” A History of US Steel www.construction-physics.com Copy link Facebook Email Note Other 7 Share Last week US Steel announced it was being acquired by Japanese steel company Nippon Steel. The milestone gives an opportunity to look back at what once was the largest and most important company in the US (and arguably the world), and how it slowly declined. Prior to the acquisition announcement, US Steel had a market cap of around $8 billion, not even enough to put it in the Fortune 500 (it would come in at around #690, slightly below the Texas Roadhouse restaurant chain).1 Over its lifespan, the company slowly but steadily lost market share and importance. When it was formed in 1901, it was by far the largest company in the world, and produced nearly 2/3rds of American steel. Today, it makes just 12% of American steel, around a third of the steel that it made in 1955, and employs around the same number of people as online pet retailer Chewy. How did a once mighty industrial titan fall so far? Let’s take a look. Origins and early years of US Steel By the turn of the 20th century, the American steel industry had eclipsed Britain to be the largest and most efficient in the world. In 1896, an engineer for the Pennsylvania Steel Company wrote that “within the last decade America has made marvelous developments in her iron industry, until she now leads the world in the quantity of her products and bows to none in their quality”. Relentless competitors like Andrew Carnegie of Carnegie Steel had, in the course of expanding their steelmaking capacity and acquiring market share, steadily driven down the price of steel, which fell by more than 80% between 1870 and 1896. But these successes had come at a cost. Steel production was subject to very large economies of scale - the larger your blast furnaces, and the more Bessemer converters and open hearth furnaces you operated, the cheaper making steel became. And because of the high fixed costs of these facilities, producers were incentivized to keep them full, cutting prices to just above the marginal cost of production in times of low demand. The result was chronic overcapacity and as steelmakers built ever-larger facilities and tried to underprice each other - at the end of the 19th century, nearly half of steelmaking capacity in the US went unused each year. There was thus a growing desire for consolidation of the industry. If many competing firms were combined, production volumes of the new firm would rise and duplicate plant could be eliminated, enabling even greater economies of scale and lowered costs. The instigating event for such a merger was a banquet of bankers and industrialists in 1900, where Carnegie Steel president Charles Schwab spoke of the potential benefits of industry rationalization via merger. Less than 4 months later, it was done. Organized by JP Morgan, a new company was created by combining Carnegie Steel and Federal Steel, along with a hodgepodge of smaller companies including National Steel, American Sheet Steel, and American Hoop.2 The new company, US Steel, was a monster. The first company to be worth more than $1 billion, the company had 168,000 employees and produced just under 9 million tons of steel annually, over 60% of all the steel in the US. And it quickly grew even larger. By 1917 US Steel (now up to 268,000 employees) was more than 3 times as large as the next largest company, AT&T, and had more than 7 times as many employees as Standard Oil when it was broken up. The decades prior to the formation of US Steel were characterized by cutthroat competition, but that ended with the formation of US Steel. The head of the new company, Judge Elbert Gary, was a fundamentally conservative businessman, and his desire was to bring stability to what was a cyclical and chaotic industry. Whereas previously companies like Carnegie Steel would constantly cut prices to achieve greater market share and drive their competitors out of business, Gary set (generally) higher prices, and kept them high even as demand for steel fluctuated. Though its size did give it production cost advantages, these savings were retained as higher profits rather than passed on to the consumer. Gary was, according to former managing director of Carnegie Steel James Gayley, “opposed to the old method of going out into the market and slashing prices in order to get business”. To maintain prices in the face of fluctuating demand, Gary would gather the industry’s leadership together in regular “Gary Dinners”, where price levels would be agreed upon and Gary would “exhort the chairmen, presidents, or other major owners of steel companies to maintain rank”. Those who refused to cooperate would be “disciplined” by the others. Many former Carnegie Steel executives, schooled in the intense competition of the Carnegie years, were allergic to this strategy, and departed for other steel companies. Via Temin 1964 and USGS Gary proved adept at navigating the company through a cultural climate that was increasingly hostile towards potential monopolies. Under his leadership, the company successfully fought off a Justice Department lawsuit to break up the company as an illegal monopoly. (Perversely, the court ruled that the Gary Dinners proved that US Steel did not have the power to set prices, and thus couldn’t be a monopoly). But other threats proved harder to cope with. US Steel was consistently behind on adopting the latest steelmaking technology, and was often forced to license technologies from more forward-looking companies (some of which were run by former Carnegie men). A 1936 magazine article described early US Steel company policy as “No inventions; no innovations”. In 1902, for instance, a man named Henry Grey invented what was known as a “Universal Beam Mill”. At the time, beams were made in the shape of an uppercase I, with a tall vertical web capped with narrow flanges at the top and the bottom. If you wanted wider flanges capable of carrying heavier loads, you would need to weld or rivet steel plates to the beam, an expensive and time-consuming process. The Universal Mill made it possible to roll wide-flange beams in a single operation, eliminating the process of attaching extra steel plates. Grey first brought his mill design to US Steel, but it was rejected by the finance committee. Instead, the first Universal Mill in the US was built by Bethlehem Steel, run by former Carnegie president Charles Schwab. Faced with declining market share in the growing construction steel market (which made heavy use of wide-flange steel beams), US Steel was ultimately forced to license the Universal Mill from Bethlehem in 1926. The same pattern repeated for other new technologies. In the 1920s, technology for making large-diameter pipe by electrical resistance welding was invented. Though it was (supposedly) first offered to US Steel, the company again passed, only to adopt it several years later to keep up with its competitors. Around the same time, technology for continuous rolling of wide sheets of steel was invented, which greatly reduced production costs. US Steel had investigated continuous sheet rolling as early as 1902, but had abandoned its efforts. Instead, it was once again forced to license the technology from others to remain competitive (though in fairness, it was one of the first manufacturers to license it). More generally, US Steel’s enormous size made it unwieldy and difficult to manage. It took time for information to filter up through the many layers of management between the factory floor and company leadership, and to get all of the disparate operations of the company moving in the same direction. This, on top of the fundamental conservative culture created by Gary, and the lack of top management talent as former Carnegie executives abandoned the company, caused US Steel to steadily lose market share to its more agile rivals. The company continued to grow, but other companies grew faster. By 1941 US Steel’s output had risen to nearly 30 million tons of steel annually, triple what it made when it was founded, but its market share had fallen from over 60% to around 35%. US Steel in the post-WWII era At the end of WWII, the American steel industry was an unchallenged juggernaut. During the war, American steel production had risen by more than a third, while the steel industries of most other countries had been nearly wiped out. By 1945, America was producing over 60% of the world’s steel. American steelmakers had the latest technology, unparalleled expertise, the largest economies of scale, and easy access to resources such as iron ore and coal. In 1945, the president of the US Steel Export Company George Wolf testified before congress that “the European steel industry is still far behind that of the United States, product, quality, and cost wise”, and that the gap was “an ever-widening one”. Japan, which would later emerge as a steel juggernaut, wasn’t even on the radar, producing just 0.56 million tons in 1946 against America’s 66 million tons. US Steel continued to dominate the industry, producing more than twice as much steel as the next largest company (Bethlehem Steel), and averaging around 30% of American steel output in the mid-1950s. With seemingly no rivals on the horizon, American steel companies, including US Steel, became complacent. The post-war years are sometimes described as the “dodo period” of the American steel industry, as it steadily raised prices (steel prices rose at 7% annually between 1947 and 1957) and enjoyed large profits while ignoring the progress that was taking place in other parts of the world. As American steelmakers built new capacity to meet post-war demand (by 1962 American steel capacity reached 154 million tons annually, nearly twice the production at the end of WWII), they did so using existing steelmaking technology, primarily the open hearth furnace. In an open hearth furnace, a combination of liquid pig iron from a blast furnace and steel scrap is placed in a large crucible, which is then heated using burning gas blown through a regenerative heat exchanger. In 1954, over 90% of steelmaking capacity in the US was open hearth furnaces, with the balance a mix of electric arc furnaces and Bessemer converters. But in 1952, a new steelmaking technology appeared, the Basic Oxygen Furnace (BOF). The BOF was in a sense a refinement of the earlier Bessemer converter, the first technology for mass-producing steel. Whereas the Bessemer converter worked by blowing air through liquid pig iron from below, the BOF worked by blowing pure oxygen into pig iron from above. Oxygen furnaces had been conceived as early as the mid-19th century, but weren’t feasible to build until pure oxygen began to be made on an industrial scale in the 20th century. The open hearth furnace replaced Bessemer converters because the latter was difficult to control, tended to result in nitrogen-embrittled steel, and was limited in the types of ore it could use. But open hearths came with drawbacks. They were more expensive to build, ran slower, and required much more fuel than Bessemer converters. Only cheap scrap metal, which could be remelted in an open hearth furnace along with liquid pig iron, made them economical to operate. The BOF, by contrast, had many of the benefits of the Bessemer converter (rapid conversion of iron to steel, low fuel and labor costs, comparatively cheaper to build) while eliminating many of its drawbacks (nitrogen embrittlement, limitations on types of ore it could use). The first commercial BOF was built in Austria in 1952, and diffused through the industry as other companies experimented with it. By the end of the 1950s, it was clear that the BOF could produce steel more cheaply than the open hearth furnace (and in fact, no new open hearth integrated steelworks were built in the US after 1958). But US Steel was once again slow to adopt the technology. Reluctant to abandon expensive open hearths that had years of useful life left, as late as 1962 US Steel executives were arguing that open hearth steel would remain competitive. US Steel didn’t build its first BOF until 1964, well behind other American companies like Kaiser Steel, which by 1964 was making 43% of its steel in BOFs. Though US Steel was once again a laggard, it was emblematic of the entire US Steel industry, which was comparatively slow to adopt the BOF. By the time US Steel built its first BOF, the BOF was producing around 17% of steel in the US, compared to around 55% of steel in Japan. And the BOF wasn’t the only steel technology where Japan was pushing ahead of America. In pursuit of ever-larger economies of scale, Japan went far beyond the US in the size of its blast furnaces. By the mid-1970s, average blast furnace size at Nippon Steel was 4 times the average at US Steel, and by 1977 more than half the blast furnaces in Japan had a volume of more than 2000 cubic meters, compared to just 2.6% of furnaces in the US. When US Steel had trouble with its own large blast furnace at its Gary Works, it was eventually forced to turn to the Japanese for help, who by then had much more experience than Americans operating very large furnaces. A similar trajectory occurred with continuous casting technology, which produced continuous slabs of steel instead of individual ingots, and thus eliminated much of the rolling required for steel production. Though US firms pioneered much of this research, other countries adopted it more quickly. By 1975, only 9% of steel was continuously cast in the US, compared to 31% in Japan and 24% in West Germany. Here again, US Steel lagged, and didn’t widely adopt continuous casting until the early 1990s. Thus, by the early 1960s cracks in America’s seemingly unassailable steel industry were beginning to show. Foreign producers like Japan had expanded their industries and adopted the newest steelmaking technology like the BOF and continuous casting. American steelmakers like US Steel, saddled with outdated technology and high labor costs, found themselves under threat from foreign producers for the first time. By 1958 some steelmakers in Germany and Japan were able to compete on price with US producers, and by the mid-1970s input costs for Japanese steel (ore, labor, coking coal, etc.) were nearly half those of US costs. Between 1955 and 1970 steel imports to the US increased by more than factor of 10, going from less than 2% of US production to more than 15%, and continued to rise. In the face of foreign pressure, US Steel could no longer maintain the “price leadership” it had in the past, setting whatever price it wanted and encouraging the rest of the industry to follow suit. The company continued to lose ground to its more nimble rivals, who now included overseas competitors. By 1971, it was still the largest steel company in the US, but had been eclipsed in size by Japanese steelmaker Nippon Steel. Its annual output of steel was just over 27 million tons, less than it had produced in 1941, and its share of the US market had fallen to around 24%. The American steel industry responded to the rise of foreign producers not by trying to improve their operations, but by demanding government protection from “unfair” foreign trade practices. In 1968, steel producers in Japan and Europe, at the behest of President Lyndon Johnson, agreed to artificially restrict their steel exports to the US. This was intended to give US producers “breathing room” to modernize their facilities and improve operations, though this didn’t occur (in fact, capital investment by American steelmakers declined after the agreements). And though pressure had been temporarily removed, things were about to get much worse for US Steel and the American steel industry. The steel crisis and the rise of the minimill Having temporarily fended off foreign threats, American steel production continued to climb. In 1973, right around when Hyman Roth was boasting that their mafia operations were “bigger than US Steel”, America produced 137 million tons of steel, more than any other country in the world. While not the unrivaled behemoth it once was, US Steel was still the 13th largest company in the US by revenue, and the largest steelmaker in the US. It was widely predicted that demand for steel would continue to rise, as it had for most of the 20th century. In 1972, chairman of US Steel Edwin Gott predicted that worldwide demand for steel would rise 25% by 1980. Countries around the world expanded their steelmaking capacity in anticipation. Instead, demand for steel stagnated. Between 1973 and 1984 worldwide demand for steel was essentially flat, and in industrialized countries it declined by around 25%. Steelmakers around the world, faced with large amounts of excess capacity, were incentivized to sell steel for just above the variable costs of production, “dumping” it on foreign markets. It’s somewhat unclear if such dumping actually occurred, but steel imports to the US continued to rise through the 1970s even as overall demand dropped. And US Steel was hurt most of all - nearly all of the loss in market share to foreign steelmakers came at the expense of US Steel. American producers once again demanded government protection from foreign competition. They filed dozens of “antidumping” cases under the Trade Act of 1974, and secured other protectionist measures such as a “trigger price mechanism” in 1978 (which prevented foreign steelmakers from selling below their total costs of production) and a new round of voluntary export restrictions in 1984. These measures helped stave off foreign competition - steel imports declined from 26 million tons in 1984 to 17 million tons in 1989. But they couldn’t stop a threat that was emerging from within the US: the minimill. Historically, steel had been produced in large, integrated steelworks. Iron ore would be turned into pig iron in a blast furnace, which would then be turned into steel in an open hearth or basic oxygen furnace. From there, the steel would be cast into ingots or slabs and then rolled into various shapes - wire, rods, plate, beams, sheets, and so on. But in the late 1960s, a new type of steelmaking facility began to appear, the minimill. The minimill made steel not by processing iron ore, but by remelting scrap steel in an electric arc furnace. By eliminating the blast furnaces which turned iron ore into pig iron, minimills were not only much cheaper to build than integrated steelworks (as little as 1/10th the cost per ton of steel they produced), but they could profitably be built much smaller. And the scrap steel they required was widely available thanks to the previous transition to the BOF, which used much less scrap than the open hearth it replaced. Because scrap steel was often contaminated with other metals such as copper that couldn’t be easily separated, minimill steel was initially lower quality than BOF steel, and minimills were only competitive in products where such quality didn’t matter, like concrete reinforcing steel. But as minimill technology improved, they began to take more and more share from large, integrated steelmakers like US Steel. Between 1974 and 1994 steelmaking capacity of integrated producers fell by more than 50%, while the capacity of minimills increased by 360%, reaching 30% of American steelmaking capacity. Rise of minimills between 1974 and 1994 Thus, by the early 1980s, US Steel was in trouble. Its market share had fallen to around 20% of the US market, and it was massively less efficient than both low-cost steelmakers abroad and increasingly capable minimills at home. Whereas for much of its history it had been one of the most profitable American steelmakers thanks to its scale, it was now one of the least profitable. US Steel responded to these threats with bitter medicine. It cut tens of thousands of jobs and closed dozens of plants, reducing employment from 171,000 in 1979 to less than 21,000 in 1995. It divested many of its auxiliary operations like mines, warehouses, and bridge construction. It abandoned market segments like rebar and heavy structural steel where it couldn’t compete with the minimills, and instead focused on things like sheet steel products (which minimills still had trouble with), concentrating operations in a small number of large plants. By 1985, US Steel had closed more than 150 facilities, and by 1998 its steelmaking capacity was down 71% from its peak in 1973. (US Steel was far from the only integrated producer that needed to take such drastic steps. The entire American steel industry (and indeed, the worldwide steel industry) shed hundreds of thousands of workers as it dealt with its overcapacity.) This tough medicine worked. Productivity increased enormously, and US Steel became one of the most efficient integrated steelmakers in the world (though it still had trouble matching the productivity of the most efficient minimills). The new, lean US Steel proved to be a scrappy competitor. As imports continued to rise (reaching 37% of US production in 1998), and minimills continued to take market share (electric arc furnaces reached 47% of US production by 2000), US Steel survived where many of its competitors didn’t. Kaiser Steel, the company that had beaten US Steel to the punch in installing BOFs, closed shop in 1983 after 18 quarters of losses. Between 1997 and 2001, 30 steel companies declared bankruptcy, including longtime rival Bethlehem Steel. But the new, lean US Steel still continued to be a step behind on technological innovation. US Steel didn’t adopt the minimill until 2020, when it acquired a minimill company and built its own minimill in Alabama. (Minimills now produce around 25% of US Steel’s domestic output.) Companies like Nucor beat it to the punch with things like thin slab casting technology. Today, the momentum in the American steel industry is clearly with the minimills. An ever-larger fraction of American steel is made in electric arc furnaces. Minimill company Nucor passed US Steel in production in 2015, and today is the largest steel producer in the US. Conclusion Arguably, US Steel has been a disappointment since the day it was formed. It was created as a fundamentally conservative reaction to the vicissitudes of the steel industry, and this guided its early years and shaped its culture. The economies of scale it achieved were never passed on to the consumer, and instead it used its size to bully other steelmakers and extract money from consumers. When this stopped working, it used its political influence to prevent consumers from buying low-cost foreign steel. Improving the efficiency of its operations was something it did as a last resort when left with no other options. The company’s large size made it unwieldy to manage, and it was late to every major advance in steelmaking technology of the last 100 years, from continuous rolling to the basic oxygen furnace to the minimill. When the company did try its hand at technology innovation, it reliably made missteps. In some cases, like with continuous rolling, it gave up too early, while in other cases it spent many years unsuccessfully developing a technology. In the 1950s, for instance, it spent many years trying to develop an alternative to the BOF that blew oxygen in from the side, long after other producers had given up on the technology. And in the 1970s it tried to develop another alternative to the BOF called Q-BOP that likewise didn’t seem to pan out. As far as I can tell, no major steelmaking technology over the last century came out of US Steel. The US Steel of today is a far cry from the industrial giant of the 20th century. But being transformed into a lean, competitive company doesn’t seem to have changed its fundamental culture, a company that's content to be a follower, rather than a leader in technological development and pushing the industry forward. 1 The acquisition announcement, however, has juiced the stock price a bit. 2 Carnegie himself would retire and spend the rest of his life philanthropically distributing the $225 million (over $8 billion in 2023 dollars) he earned from the buyout of Carnegie Steel. Subscribe to Construction Physics By Brian Potter · Hundreds of paid subscribers Why buildings are built the way they are. Subscribe 89 Share this post “No inventions; no innovations” A History of US Steel www.construction-physics.com Copy link Facebook Email Note Other 7 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=38807720",
    "commentBody": "\"No inventions; no innovations\" A History of US SteelHacker Newspastlogin\"No inventions; no innovations\" A History of US Steel (construction-physics.com) 234 points by gok 16 hours ago| hidepastfavorite199 comments pomian 14 hours agoOne of the ideas that seems to be missed from the study of history and economy, was the effect on the total destruction of the Japanese and German industrial base - during WWII. Britain and USA, were left with their archaic industrial systems. After WWII, the Japanese and the German steel industry had to be completely reborn. (At the cost of primarily USA financing.) Those two countries had no more relics of the past, and started over, necessarily, with the most modern technology and science. North American industry was in a slow evolution from the 1900&#x27;s, but Germany and Japan, had a hot start from the 1950-60&#x27;s. No wonder everyone was impressed by their modern approach to construction, design, manufacturing, which more or less started to out perform USA and Britain in the 70&#x27;s. China, started it&#x27;s industrial rebirth even later. reply ChuckMcM 10 hours agoparentIt is an interesting narrative but largely untrue. As the article points out, US Steel (and the US steel industry in general) was making enough profit, what they were not doing was re-investing those profits into upgrading their steel making. Instead, the \"People in Charge\" were giving themselves large bonuses and pay upgrades for being bosses of the \"big steel.\" This was especially true through the 80&#x27;s and 90&#x27;s in a post \"stagflation\" atmosphere a lot of industry became more focused on executive pay than product production. The poster child was the auto industry \"forgetting\" how to make quality cars. They didn&#x27;t forget of course, they just focused more on shoveling money into the C-suite and less on actually building a quality product. The massive erosion of the car market to Japanese automakers was a direct result of that shift of thinking. reply AnthonyMouse 2 hours agorootparent> This was especially true through the 80&#x27;s and 90&#x27;s in a post \"stagflation\" atmosphere a lot of industry became more focused on executive pay than product production.Really the thing that does this is market concentration. You have only three major companies in an industry and they can play \"conscious parallelism\" games to avoid competing with each other while pointing to the others as ostensible competition to fend off antitrust allegations.Then because they&#x27;re not actually subject to competitive pressure, they stagnate, and as soon as anyone else shows up who hasn&#x27;t forgotten how to satisfy the customer, the incumbents start to crumble.It isn&#x27;t a coincidence that the article is about something similar happening to US Steel, the infamous former monopolist. reply atoav 1 hour agorootparentprevWhich is precisely the culture people in Germany and Japan don&#x27;t have (or at least: had). If there are profits at least a chunk of them should be re-invested into making things better, who knows what the future brings after all. reply edmundsauto 9 hours agorootparentprevI’m not sure that narrative on “forgetting how to build quality cars” is fair. Germany and Japan introduced a lot of new technologies and processes that would naturally be a little rocky from time to time. It is the nature of immature services.Once those matured, the new stuff quickly caught up. But it needed time.What makes you think it was the increase in executive or shareholder comp that caused the degradation in quality? Do we even know if there was a degradation between a 1960 ford and 1985, or is it because the Japanese and German automakers raised our standards?This is a situation where a lot of narratives fit the evidence. reply mildchalupa 8 hours agorootparentHere&#x27;s an example, before WWII brake lines were made out of Monel, a corrosion proof material aside from galvanic. Nickel and copper being expensive and needed elsewhere the lines were made then made from painted steel. After the war steel lines became the standard.The cost difference is small but it&#x27;s indicative of a larger issue of disposability. One of the reasons Japanese vehicles started to dominate was that they lasted longer. This is due to better tolerances, more efficient engines, and more robust component design.The Toyota production method, lean and just-in-time were mostly methods for rationalizing better systems to eliminate waste within production and to minimize inventory. Such methods don&#x27;t inherently make a better car though they leave more io the table to spend on coatings, quality parts etc. reply zer00eyz 1 hour agorootparentJapan, Toyota Production System (TPS) is a merger of culture and the work of William Edwards Deming.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;W._Edwards_DemingThe wiki has this line that sums it all up perfectly:In 1982, Deming&#x27;s book Quality, Productivity, and Competitive Position was published by the MIT Center for Advanced Engineering, and was renamed Out of the Crisis in 1986. In it, he offers a theory of management based on his famous 14 Points for Management. Management&#x27;s failure to plan for the future brings about loss of market, which brings about loss of jobs. Management must be judged not only by the quarterly dividend, but also by innovative plans to stay in business, protect investment, ensure future dividends, and provide more jobs through improved products and services. \"Long-term commitment to new learning and new philosophy is required of any management that seeks transformation. The timid and the fainthearted, and the people that expect quick results, are doomed to disappointment.\" reply wombatpm 8 hours agorootparentprevHaving owned a 72 Vega and a 79 Mustang the quality drop off from earlier vehicles was obvious. No one is collecting late 70’s mustangs. No one is restoring Chevy Vegas. American car companies made crap cars and didn’t care until it was too late.I refer you to the movie Roger & Me. reply RcouF1uZ4gsC 4 hours agorootparentprev> The poster child was the auto industry \"forgetting\" how to make quality cars. They didn&#x27;t forget of course,They didn&#x27;t just forget. They rejected approaches that would have helped them improve quality.Deming tried to convince US industry for quality, but was rejected. He, however, was embraced in Japan. reply KMag 13 hours agoparentprevSee the movie \"The Mouse That Roared\". Having seen the results of Germany and Japan being bombed and rebuilt by the US, a small fictional country comes up with the brilliant development plan of intentionally starting and losing a war with the US. Spoiler alert: they end up having to deal with the tragedy of accidentally winning the war. As I recall, part of it was they just assumed their generals would lose, so the generals weren&#x27;t in on the plan. reply CrazyStat 12 hours agorootparentThey \"win\" the war by accidentally kidnapping a scientist with a powerful new nuclear weapon. No actual fighting happens.It&#x27;s an excellent movie though. reply vanderZwan 11 hours agorootparentA satirical movie involving nuclear weapons and Peter Sellers plays three roles? Where have I heard that one before?(actually, it looks like it is five years older than Dr. Strangelove? This looks like it could be turned into a very mean pubquiz trivia question, hahaha) reply selimthegrim 8 hours agorootparentIt’s based on an older book. reply fosk 10 hours agorootparentprevMaybe someone can clarify this to me: wasn’t US help essentially given as a loan that would carry interest and repayments? Or was it given for free with the agreement that they would follow a US-inspired democratic system and be allies (and in exchange of trade agreements)? Were military bases in those countries given as a repayment, perhaps in addition to repaying the loans? But even then, isn’t the US paying rent for those bases? reply AnthonyMouse 1 hour agorootparentLoans to a bombed out country in the aftermath of losing a major war can be hard to come by, for the same reasons as they are for a third world country with an unstable government.It&#x27;s not uncommon for a country to get stuck in a situation where they have no infrastructure that could attract investment so they have no industry so they have no economic base to build infrastructure. \"Get the US to loan us money\" is nominally a way out of this.The US noticed this too and makes loans to such countries de facto through the World Bank, providing them with an opening through which they can crawl up inside those countries and lay eggs. reply voidfunc 10 hours agorootparentprevThe repayment is that we took over the defense of Europe and basically eliminated the pre-war European powers as global competitors. This gives us tremendous leverage to do whatever the fuck we want either unilaterally or by coercing our European client-states to act on our behalf.Europe deciding to light itself on fire twice in the last century is the best thing that ever happened to America. reply ponector 9 hours agorootparentThere were reparations payed to the USA and then there were repayment of the loans provided as aid within Marshall plan. reply lisper 11 hours agorootparentprevReminds me of \"The Producers.\" reply l33t7332273 13 hours agorootparentprevA small country winning the war against post WW2 USA seems like it would be a bit of a plot hole. reply ben_w 13 hours agorootparentIt&#x27;s a comedy, one I feel I ought to watch at some point.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Mouse_That_Roared_(film) reply morkalork 12 hours agorootparentPeter Sellers playing 3 different roles. Poor guy really did get type cast in the weirdest way. reply robertlagrant 11 hours agorootparent\"Comedies involving diamonds and&#x2F;or nuclear weapons\" reply vanderZwan 11 hours agorootparentAlso one movie with him in brownface that oddly enough is actually extremely popular in India and Pakistan (to the point where I was introduced to it on a New Year&#x27;s Eve party hosted by a friend from Pakistan).https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Party_(1968_film)#Racial_c... reply selimthegrim 8 hours agorootparentI have never understood why all my Desi relations were gaga for this movie. All the American born ones roll their eyes without fail. reply lmm 2 hours agorootparentThe idea that it&#x27;s haram (I can&#x27;t quite think of the right word; not merely wrong but disgusting) for someone to make themselves up as a different skin tone is a distinctly American cultural quirk. replyachates 13 hours agorootparentprevThe diplomat in charge of central Europe thinks their declaration of war is a practical joke from another department, so nobody notices when their army (of about 10 crossbowmen) shows up in New York and grabs a fictional doomsday device out of a secret lab. reply sjfjsjdjwvwvc 13 hours agorootparentprevWhich war did the US win post WW2? Afaict they all ended in stalemates and eventual retreat (Korea,Iraq,Vietnam,Afghanistan,..) reply nostrademons 12 hours agorootparentThis all depends on victory conditions. The U.S. \"won\" Korea, Iraq, Vietnam, and Afghanistan in the same sense that they \"won\" WW2 - those countries were bombed back into the stone age and the existing governments fell. But WW2 was fought as the U.S. was an ascendant (but not dominant) power among a number of peer rivals, and the U.S. was not the aggressor. It felt like a victory, because we emerged as the dominant power, with the only industrial base that wasn&#x27;t destroyed, and then enjoyed the economic fruits of rebuilding a country that had been bombed back into the stone age and then capitulated. Additionally, Germany and Japan expected that U.S. occupation would be absolutely terrible, that we would be tyrants in the same way that their militaristic governments of the time were, and so when it turned out we just wanted to make money, that was a huge relief to them.With all the post-WW2 wars, we&#x27;ve gone in as a dominant power, as the aggressor, to a country that is far smaller and less developed. In terms of casualties, they&#x27;ve been even more lopsided victories than WW2. The second Iraq war killed about half a million Iraqis and displaced about 1.8M, vs.You can argue that it was a political win if the goal was defending South Korea, but militarily, it was a stalemate.Korea was a proxy war between the US and China (i.e. Communism). The US wanted a foothold on the mainland and to put a demonstration of the virtues of Capitalist Economics in the Communists&#x27; back yard, both of which it got in the form of South Korea. If the US hadn&#x27;t been involved, \"Democratic People&#x27;s Republic of Korea\" (i.e. North Korea) would have rolled south with China&#x27;s backing and \"Korea\" would have been a single communist country.Without China&#x27;s involvement the opposite would have happened. China was well aware of this and did not want a capitalist economy sharing a border with it -- Berlin Wall kind of problems -- so they weren&#x27;t going to stop supporting North Korea.Which is the kind of situation that makes unconditional military victory close to impossible. China just keeps giving North Korea assistance, most of the casualties are North Koreans rather than Chinese and the North Korean leadership won&#x27;t surrender as long as they have China&#x27;s backing because it&#x27;s not a democracy so staying in power is the only thing their leaders care about. \"Unconditional victory\" would effectively have to be a military victory over China.At the time the US had nuclear weapons and China didn&#x27;t, but China had half a billion in population even then, and they too were a non-democracy whose leaders cared more about staying in power than dead citizens. The US could have won militarily if they were willing to kill several hundred million people. It&#x27;s probably best that they weren&#x27;t.And it was largely the same for the other proxy wars. Unconditional victory would require victory over not just the nominal enemy but the ones propping them up. Hence Cold War. reply l33t7332273 7 hours agorootparentprev> You can argue that it was a political win if the goal was defending South Korea, but militarily, it was a stalemateDo you think that wars are not political? A political win is a military win. reply robertlagrant 11 hours agorootparentprevThe US picks extremely difficult battles to fight in a way its citizens deem acceptable, and they aren&#x27;t wars of conquest. Those factors are going to make it very hard to point at a simple winner. reply hef19898 11 hours agorootparentAll the wars mentioned have ckear, internationally acknowledged winners and loosers as well as treaties.Vietnam ended in US defeat, and French defeat before that.Afghanistan ended with US &#x2F; NATO defeat and the Taliban taking over the country.The Korean War isn&#x27;t over yet technically, all we have there is a cease fire. Stalemate, nobody surrendered.The US won the second Gulf War, Panama and some smaller stuff I am too lazy to look up. reply l33t7332273 7 hours agorootparentI’m not sure why people care to mention Vietnam and Afghanistan in the same breath as real wars. These are wars against insurgencies, populations, and ideologies, not governments.They’re inherently different and the only way to “””win”””(which is a loaded term with wars against insurgents. We accomplished important strategic goals in Vietnam before leaving the country to self determine) is to resort to tactics employed by folks like The Butcher did in Cuba. reply jltsiren 26 minutes agorootparentVietnam War was a conventional war between two governments and their international supporters. The government the US supported lost the war, because the US was unwilling to commit sufficient forces to win. Largely because they were afraid of a repeat of the Korean War, with China and possibly even the USSR joining the war for real. And because the US lacked the will to win, they eventually lost the will to maintain the status quo. reply hef19898 43 minutes agorootparentprevVietnam, the US war, was dought in South Vietnam against local insurgents and regular North Vietnamese forces. It ended with a formal peace treaty, in which the US agreed to withdraw all troops, not send further military aid to the South, return prisoners, remove mines from North Vietnamese harbours and pay, even if called differently, reparations. Vietnam was pretty much a war the US lost, and one that was not exclusively fought against insurgents.The US won zero strategic goals, Vietnam was reunited by force under the communist North. The US goal was to orevent exactly that. Nice spin so, calling Vietnam a defacto strategic victory for the US.And Afghanistan, seriously? Before, anti-Talobam forces controlled at least parts of zue country. Nowy the Taliban are in full controll after they over ran Kabul following a hasty retreat of NATO forces from the country, that is as clear cut a loss as it gets. Had the US led coalition left after Bin-Laden was killed, one could have declared a victory. By staying and tuening the whole affaire into a foreign occupation without clearly defined goals, the US-led coalition could only loose. And they did. reply herval 10 hours agorootparentprevWould you mind providing some “clear and internationally acknowledged” sources for the Korean War that stated it’s still ongoing or that Afghanistan was an undisputed loss? reply shagmin 6 hours agorootparentAs for the Korean war, it&#x27;s kind of a fuzzy topic. There was no formal peace deal, just an armistice that became the status quo. And there are still occasional clashes. These probably aren&#x27;t the kind of sources you&#x27;re looking for but it&#x27;s a weird topic. To me it&#x27;s more obvious that we lost Vietnam than Afghanistan yet I&#x27;ve heard so many arguments the US did win Vietnam (usually where \"winning\" means something different to different people). Either way Afghanistan was at least an embarrassment.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Korean_conflicthttps:&#x2F;&#x2F;www.google.com&#x2F;amp&#x2F;s&#x2F;amp.cnn.com&#x2F;cnn&#x2F;2021&#x2F;12&#x2F;30&#x2F;asia... reply FrustratedMonky 7 hours agorootparentprevIt &#x27;feels&#x27; like a loss. I&#x27;m sure there are other ways to spin it.The “clear and internationally acknowledged” criteria for a &#x27;loss&#x27; is being caught on camera while the last soldiers evacuate and the enemy takes over.So technically Vietnam and Afghanistan are losses. Because of the video&#x27;s. reply FrustratedMonky 7 hours agorootparentprevDeaths. US in Iraq. ~7000. I think the point still stands. Even though they &#x27;felt&#x27; like a loss, there was a ~500:1 kill ratio. So was it really a loss if you are mowing the enemy down?Also note. There were 4x as many deaths from suicide. If we really cared about soldiers, this would be the number one story every day.https:&#x2F;&#x2F;watson.brown.edu&#x2F;costsofwar&#x2F;costs&#x2F;human&#x2F;military&#x2F;kil...Over 7,000 U.S. service members and over 8,000 contractors have died in the post-9&#x2F;11 wars in Iraq, Afghanistan, and elsewhere.Over 30,177 U.S. service members and veterans of the post-9&#x2F;11 wars have died by suicide. Coalition partners have died in large numbers: approximately 177,000 uniformed Afghans, Pakistanis, Iraqis, and Syrian allies have died as of November 2019. reply nostrademons 10 hours agorootparentprevThe government of North & South Korea that I&#x27;m speaking of was Japan.The government of North & South Vietnam that I&#x27;m speaking of was France.Acknowledged on the undercount of U.S. casualties in Iraq. It doesn&#x27;t change my conclusion, though.There&#x27;s a sibling comment by delecti that&#x27;s making the point that I&#x27;m driving at: what is the goal of war? Is it regime change? Is it to sell arms? Is it to kill people? If it&#x27;s regime change, war in general is a remarkably ineffective way of doing that - it&#x27;s pretty rare that the victorious powers get the government they actually want, otherwise we would never have had to fight WW2. If it&#x27;s killing people or feeding the military-industrial complex, the U.S. was pretty good at that in all the wars listed.Incidentally, Vietnam is currently one of the most capitalist countries on Earth. So politically, the communists were defeated. All we had to do was let them govern the country for 15 years and everybody realized they didn&#x27;t want communism. reply specialist 9 hours agorootparentYes and: maintaining an Empire (Hegemony) requires Forever War. Were USA to achieve a Von Clauswitz style victory, Pax Americana would quickly implode. reply foooorsyth 9 hours agorootparentprev>Saying that Germany and Japan were “bombed back into the Stone Age” is a wild exaggerationJapan was firebombed pretty ruthlessly. McNamara openly stated that he would have been tried as a war criminal had the US lost the war. Here is a list of 67 Japanese cities that were firebombed with their destruction % and their US size equivalent:http:&#x2F;&#x2F;www.ditext.com&#x2F;japan&#x2F;napalm.html#google_vignetteSource: The Fog of War (Documentary)I guess this all depends on one’s definition “bombed back into the Stone Age”. I’m sure descendants of those that lived in Dresden might argue that they, in fact, were indiscriminately bombed to the point of ruin. reply FrustratedMonky 7 hours agorootparentNot super exaggerated. See the fire bombing of Dresden.Guess technically, &#x27;stone age&#x27; is hyperbole. They didn&#x27;t literally have to climb back up through a bronze age and steal age again.But point stands, they were severely crippled. reply kriro 12 hours agorootparentprevIn my opinion, this is a misrepresentation of the Vietnam war. The country was not \"bombed back into the stone age\". The U.S. merely used extremely despicable tactics like Agent Orange, My Lai etc.I&#x27;m also not aware of the existing Vietnamese government falling (depends on which one you consider the existing one but it was not the government the U.S. wanted to fall that fell). The country resisted a superior invader like it did in the past (China) and I&#x27;m pretty sure most people would consider Vietnam the winner of the war (if there&#x27;s a winner in war). reply mr_toad 9 hours agorootparentIf it was simply the case of a country resisting an invader then why did the winning side engage in a bit of ethnic cleansing of their ‘own’ people after the invaders had left? reply Supermancho 5 hours agorootparentOpportunity and perception. China didnt need to loseca war to initiate the actions against the Uyghurs. reply dmurray 12 hours agorootparentprev> The U.S. \"won\" Korea, Iraq, Vietnam, and Afghanistan in the same sense that they \"won\" WW2 - those countries were bombed back into the stone age and the existing governments fell.This might be taught in US schools, but outside the US we have a rather different take on how the Vietnam war ended - the US-backed government fell and the North Vietnamese government took over the rest of the country. And we learned that the Korean war ended in some kind of stalemate, where the government structures on both sides exist largely intact today. reply nostrademons 9 hours agorootparentSure, I agree with all of that.I&#x27;m arguing that this is the wrong litmus test for judging the winners of war, because war is an ineffective way of achieving regime change in the first place. Even in cases where it looks like a country \"wins\" a war by \"defeating\" their opponents, they usually collapse from the inside and then the true winners are those that either take advantage of the chaos to seize power, or stay out of the war entirely. The allies were victorious in WW1 - yet the government they wanted stayed in power for barely 10 years, and then we got Hitler and WW2. Russia was part of the victorious allies in WW1, but the country collapsed and the communists took over. The communists were the official victors of the Vietnam war, but Vietnam is now one of the most capitalist countries on earth, and so politically they were defeated by economics and not war. reply l33t7332273 7 hours agorootparentprevIt is not taught in US schools that the IS won Vietnam in the sense that the government we backed won. Why do you think it is?Also, the Korean war was clearly a victory for the US; we intended to defend SK and prevent the spread of Communism, and that’s what happened. reply Qwertious 2 hours agorootparent>Also, the Korean war was clearly a victory for the US; we intended to defend SK and prevent the spread of Communism, and that’s what happened.Nonsense. A clear victory would be if South Korea held its frontline near the Chinese border, instead of being pushed back to the 38th parallel.The Korean war was a stalemate. It wasn&#x27;t a loss, but it wasn&#x27;t a win. reply l33t7332273 2 hours agorootparentI think the meaning of clear victory is certainly murky, but to call it a stalemate is just incorrect; it was definitely a strategic victory. reply Gibbon1 11 hours agorootparentprevYou&#x27;re missing the forest for the trees. Reality is the US won it&#x27;s war against communism by the early 1970&#x27;s. There wasn&#x27;t much reason to continue the war in Vietnam after that. Not the least because the communists in Vietnam had no intention of being a Russian or Chinese vassal state. Suited the US just fine. reply l33t7332273 7 hours agorootparentThe reason to commit so ferociously to Vietnam was to show our other strategic partners and allies how far USA was willing to go for seemingly small things; if we’ll dump endless troves of blood and treasure for a small country in Asia, what will we do if tanks push into Berlin? reply Supermancho 5 hours agorootparentThats a benefit to the military response, but I dont believe the massive waste of political capital, manpower, and supplies looked impressive to other countries. Especially after the abrupt exit. The use of cruise missiles has been far more imoressive. reply dragonwriter 51 minutes agorootparentprev> The U.S. \"won\" Korea, Iraq, Vietnam, and Afghanistan in the same sense that they \"won\" WW2 - those countries were bombed back into the stone age and the existing governments fellThe US isn&#x27;t conventionally, even in the US. viewed as the victor in Korea (war is still in progress, with a long but imperfectly observed ceasefire), Vietnam (one of the few wars the US generally acknowledges as an unambiguous defeat, thought right wingers will often say that we won “militarily” and lost “politically” or some other cope), or Afghanistan (recently added to the list alongside Vietnam).Also, the prewar opposing in Korea, Vietnam. Iraq (1991), and Afghanistan did not fall, and in each case remained in power after the war (and in Vietnam and Afghanistan, the primary opposing government ended up in control of more territory as a result of the war, controlling undivided a country which it had controlled only a divided—formally in Vietnam’s case, de facto for the Taliban in Afghanistan—fraction of before the war.)> With all the post-WW2 wars, we&#x27;ve gone in as a dominant power, as the aggressorNo, we haven&#x27;t. Not even in a loose sense of “aggressor” in several of them. reply michaelt 10 hours agorootparentprev> The U.S. \"won\" Korea, Iraq, Vietnam, and Afghanistan in the same sense that they \"won\" WW2 - those countries were bombed back into the stone age and the existing governments fell.I feel you&#x27;re overlooking the critical fact that after the allies won WW2, Nazis didn&#x27;t control Germany.Whereas the Taliban remains in control of Afghanistan. To describe that as a US win doesn&#x27;t really seem accurate.The US did print a big &#x27;Mission Accomplished&#x27; banner though, so I agree the folks who started the war do claim it was a victory. reply l33t7332273 7 hours agorootparentThe mission accomplished banner was for Iraq, not Afghanistan. USA explicitly achieved its war aims in Iraq 2(and Iraq 1 for that matter). reply herval 9 hours agorootparentprevWW2 is an exceptional case in that it’s one of extremely few wars where the winners and losers are universally acknowledged reply scotty79 12 hours agorootparentprevI don&#x27;t think any other country considers any of those cases to be wins ... Perhaps maybe wars in Iraq or Jugoslavia.I think eventual loss in Ukraine will be last nail in the coffin of US military reputation. reply mikeyouse 12 hours agorootparentThe US isn’t fighting in Ukraine so that’d be a strange result. Especially since our surplus weapons with novice operators have laid waste to the invasion force of what was considered to be the 2nd or 3rd strongest army on the planet.. reply sbierwagen 10 hours agorootparent>surplus weaponsI&#x27;ll grant that for the Bradley, but Patriot batteries are in rather short supply. reply SoftTalker 12 hours agorootparentprevThe fight isn&#x27;t over yet and there is trouble in the Red Sea and it could all turn into WW-III at some point. reply scotty79 12 hours agorootparentprev> The US isn’t fighting in UkraineSame way that russia is not fighting a war but commencing special military operation.US is doing it with both hands tied behind their back but it will make defeat no less devastating.In reality US is fighting their penultimate war right now if it ends in a loss.> what was considered to be the 2nd or 3rd strongest army on the planet..That was summarily debunked about two weeks into the conflict. And noone but russians believes it today. reply hef19898 11 hours agorootparentHow much active fighting troops do NATO countries have in Ukraine, exactly? You need those to be counted a belligerent in a conflict. reply mikeyouse 9 hours agorootparentIt’s utterly bizarre that people keep insisting the US is fighting a war given we have zero troops and zero military hardware committed there.. Ukraine has destroyed hundreds of armored units and multiple $billion naval ships with literal 25-yr old weapons… all the fear of WWIII is premised on the US actually engaging since the Russians would suffer immediate and massive losses and the only way to save face would be to go nuclear. reply Qwertious 2 hours agorootparent>and zero military hardware committed there..Where&#x27;d those Bradleys come from, then? Zero troops yes, but the US has absolutely shipped a ton of military hardware to Ukraine.Ukraine would absolutely fold without the inflow of NATO military hardware, that&#x27;s the crux of Russia&#x27;s strategy for winning - hope it can undermine Western support for a Ukraine war, and then crush Ukraine once they&#x27;re on their own. That&#x27;s the entire reason why Russia blocked gas exports to Europe - they lost a lot of gas profits forcing the EU to choose between supporting Ukraine or having Russian gas for the winter.The US itself isn&#x27;t fighting in the war, but they&#x27;re undeniably a key participant in it. With military hardware and training and intel support and money. replyherval 9 hours agorootparentprevTangent to all that - whether the US reputation will change or not, it’s becoming clear the russian military reputation is way, way overestimated. I’m curious what effect this will have reply l33t7332273 7 hours agorootparentprevIt would be particularly strange for US’s military reputation to be damaged by a war it hasn’t lost a man in. reply AlbertCory 7 hours agorootparentprevThis is ignorant of Clausewitz&#x27;s definitions, which no one has managed to supplant. It has nothing to do with emerging as the dominant power or getting average citizens any benefits.\"winning\" the war means accomplishing your political objectives. Those objectives may change as the war goes on, and in the case of Korea, the objective became \"keep South Korea free from North Korean domination.\" That was achieved.It was unclear to MacArthur and he assumed it was \"destroy North Korea, whatever it takes.\" That became NOT the US objective, when it became clear that China stood in the way of it.In Vietnam the objectives were never clearly laid out, but \"keep South Vietnam free from North Vietnam domination\" was pretty clearly it. Thus, we lost.Your other points are wrong: the first Iraq war&#x27;s objective was to eject Iraq from Kuwait. Perhaps that goal was insufficient, but that was the goal the military undertook, and achieved.https:&#x2F;&#x2F;www.amazon.com&#x2F;Strategy-Critical-Analysis-Vietnam-Wa... reply delecti 13 hours agorootparentprevThat&#x27;s mostly because we keep going into situations with political goals, and military means (when all you have is a hammer, every problem looks like a nail). If a small country declared war on us, we could easily bomb them back into the iron age. Though in 20 years, the young adults there would probably provoke us into another dumb situation like the above. reply Beijinger 13 hours agorootparent\"with political goals, and military means\"True\"If a small country declared war on us, we could easily bomb them back into the iron age.\"Putin probably thought the same about Ukraine.Military objectives were achieved in Afghanistan, but not in Korea and Vietnam. And based on Clausewitz: \"War is politics by other means\" And our politics changed.... reply riversflow 11 hours agorootparent> Military objectives were achieved in Afghanistan, but not in KoreaHard disagree on Korea. South Korea is a hell of an ally, very inconvenient for the PRC. reply DoughnutHole 9 hours agorootparentJust because the US got a useful ally in the long run doesn’t mean that the military objective was met. That wasn’t the objective of the war.The objective of the US once they pushed back the initial invasion in the Korean War was to destroy the DPRK, force communism out of Korea, and unify the country. They failed. The war became a stalemate once China got directly involved.If Vietnam becomes a stalwart ally of the US over the next 20 years that won’t mean the Vietnam War was a success just because the US ended up eventually having a useful interest in the region. reply nostrademons 12 hours agorootparentprevMilitary objectives were achieved in Korea and Vietnam in the sense that we killed lots of people. What we learned (and really, already knew from WW2, where we very intentionally did not invade the Japanese home islands) is that killing lots of people on an enemy&#x27;s home soil just turns a lot more people against you. reply fodkodrasz 11 hours agorootparentprev> \"If a small country declared war on us, we could easily bomb them back into the iron age.\"> Putin probably thought the same about Ukraine.I doubt, as Ukraine is far from being a small country. It is the second largest European country (counting Russia as an European country, as is common in geography). (he most likely had other miscalculations leading to the current situation) reply l33t7332273 7 hours agorootparentprevAre you implying the US post WW2 would lose a war of aggression to any small country? Indeed, I’d posit USA easily wins a war of aggression against any non-top 5 country(by military spending or what have you). reply sbierwagen 11 hours agorootparentprevGulf War 1. Invasion of Panama. The 1994 invasion of Haiti, debatably. The Kosovo war, kinda. reply hef19898 11 hours agorootparentSmall correction: it was Gulf War 2, the first one was between Iran and Iraq (the US allied with Sadam back then, different story). reply sbierwagen 10 hours agorootparentI mean, if the numbering system has to include every war ever fought in Mesopotamia, it would be more like Gulf War 200. You can&#x27;t kick a rock in Iraq without hitting a cuneiform tablet describing a battle fought in 3500BC. reply KMag 13 hours agorootparentprevI would argue both Iraq wars and the war in Afghanistan were cases of losing the peace in equal measure to easily winning the war. reply beambot 12 hours agorootparentprevTechnically, the US hasn&#x27;t fought any wars since WWII -- as a declaration of war is a Congressional function. All of the other things people think of or refer to wars were technically \"conflicts\". reply tshaddox 12 hours agorootparentThat’s not even correct on a technicality, because you switch mid-sentence from discussing “war” to discussing “declaration of war.” A declaration of war is plainly different from a war. Declarations of war are frequently made well after a war has begun or well before any fighting takes place. And as you state, many wars are fought without any declaration of war. reply mongol 10 hours agorootparentprevYou are using the word technically wrong. Whether a country is at war or not is not a technical matter, it is dependent on the facts on the ground. A war is an intense armed conflict, usually between states. Just as the US did not declare war after WWII, they have not had problems calling Russia&#x27;s invasion of Ukraine a war, despite Russian statements it is just a special military operation. We all know it is a war, just like that the US has been involved in multiple wars after WWII. reply bejk22 12 hours agorootparentprevI&#x27;m unsure the US can unilaterally decide if it&#x27;s in a war or not. Do the opposite side consider it a war? Do the international community consider it a war? Semantic games internal to a single side do not matter much. reply skygazer 8 hours agorootparentUntil an outside observer collapses the wave function, it’s a Schrodinger’s conflict? A Tolstoy Superposition (of War and Peace)? reply mnau 13 hours agorootparentprevFirst gulf war would qualify. reply jonplackett 13 hours agorootparentI think that’s more like quitting while you’re ahead reply l33t7332273 7 hours agorootparentAnd we quit Vietnam while behind. reply anjel 12 hours agorootparentprevGranada and Panama. reply loloquwowndueo 13 hours agorootparentprevVietnam. reply ProjectArcturis 14 hours agoparentprevSimilar to many cities having catastrophic fires (New York, Chicago) which allowed them to rebuild their streets on a grid system. Vs Boston, which never had a great fire but which is known today for the phrase \"You can&#x27;t get there from here.\" reply turndown 14 hours agorootparentChicago had a grid system before the great fire, maybe NY is a good example of this but I do not know. See this[0] 1869 map which shows Chicago was already quite regularized0: https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;a&#x2F;a8&#x2F;1869_Bla... reply ericjmorey 13 hours agorootparentIt&#x27;s not. The older parts of NYC are still not on a grid, there was no massive fire in NYC that needed to be rebuilt. They may have been thinking of London `¯\\_(ツ)_&#x2F;¯` reply caboteria 13 hours agorootparentprevWe&#x27;ve had a couple of big fires: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Great_Boston_FireSomehow they didn&#x27;t result in exchanging grids for winding streets.The back bay, though, is newer than other parts of the city and it&#x27;s a grid. reply technofiend 13 hours agorootparentprevRe, Boston: The way I heard it was \"Three rights make left, except in Boston.\" reply mitthrowaway2 14 hours agoparentprevSolution for the future of American industry: Bomb US Steel?Maybe this would also be an effective way of clearing out NIMBYs blocking the routes for high-speed rail lines? reply yetanotherloss 13 hours agorootparentYou jest but my friend is a professor in Toyama, Japan, and makes the occasional dark joke about moving the people out for a few days and having the US raze it again from time to time.Sometimes getting rid of ossified organizations is a good thing, but there are probably better ways than high explosives. reply Aloha 12 hours agorootparentArguably the Occupation of Japan was more important than the carpet bombing for changing Japanese culture.We brought in many new ideas - both in business and in government - most of which persist in some form today - and the Japanese in many cases have taken those ideas, refined them mightily, infused them with some Japanese culture - and re-exported them to us.The best example of this that I can think of, is Kaizen - the various scientific management techniques exported to Japan by W. Edwards Deming - which was re-exported to us as Kaizen. reply nerdponx 11 hours agorootparentprevThis concept is related to the idea of \"creative destruction\": https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Creative_destruction reply JumpCrisscross 14 hours agorootparentprev> Solution for the future of American industry: Bomb US Steel?Plenty of places are bombed into oblivion. What makes the rebirth is the rebuilding. The marshalling of public resources. You can replicate that without the bombs with a public-spending initiative. reply Fatnino 14 hours agorootparentHow do you spend away a bunch of NIMBYs? Their whole thing is that they refuse to cooperate with that.Bombs would solve that, but come with a host of other... issues. reply pstuart 13 hours agorootparentEminent domain is the best we&#x27;ve got. reply mitthrowaway2 14 hours agorootparentprevThe OP&#x27;s contention was that Japan and Germany had an advantage due to their steel industry being bombed, which is apparently easier than upgrading legacy production facilities that haven&#x27;t been blown to rubble. reply sokoloff 13 hours agorootparentIt’s often easier to rebuild rubble using other people’s money than to modernize in-place using your own.Beyond just the “whose money is buying?” is that the latter costs you current production while the bombed-out scenario has no current production to forego. reply danans 9 hours agorootparentprev> The marshalling of public resources.Indeed, something like a Marshall Plan!> You can replicate that without the bombs with a public-spending initiative.You can, but the challenge isn&#x27;t in the implementation as much as gathering the political will to do that. Since the New Deal and WWII, that has rarely happened in the US, and only recently: The ACA, the 2021 Infrastructure Bill and the Inflation Reduction Act. All of those barely had enough support to pass and both are at risk if there is a significant shift in power. reply altairTF 13 hours agorootparentprevEvery 100 years, the government send a warning and carpet bomb a city to the ground. This for sure would remove all the past vicious of the region affected. That would be something reply tandr 12 hours agorootparentSounds like a good plot for a (short) sci-fi story. reply sjfjsjdjwvwvc 13 hours agoparentprevThe German industrial base was not totally destroyed after WW2. Especially in western Germany most of it was still good and they had to rebuild little to get it going again. The myth of the Trümmerfrau rebuilding Germany into a Wirtschaftswunder from total destruction is exactly that, a myth.Edit: also the effect of Marshallplan is usually vastly overstated - due to the massive bureaucracy involved it had relatively little impact. reply WalterBright 13 hours agorootparentMuch larger shares of the MP money went to Britain and France. reply hef19898 14 hours agoparentprev>> (At the cost of primarily USA financing.)The Marshall Plan was one of the best foreign policy ideas ever so. reply kevbin 13 hours agorootparentI’d like to see Chris Nolan follow-up “Oppenheimer” with “George (Marshall)” and “(William) Knudsen” reply robocat 13 hours agoparentprevCan we set up a system to occasionally bomb our industrial base to get these benefits?Your point is mostly irrelevant. US steel could have innovated or even copied - but it didn&#x27;t.The idea that poor US steel couldn&#x27;t compete because other startups had an advantage is assinine. That is a core message of the article itself. reply specialist 9 hours agorootparentJust need to target the executive suites. Bonus points if the board is meeting at the time. reply hulitu 12 hours agoparentprev> North American industry was in a slow evolution from the 1900&#x27;s, but Germany and Japan, had a hot start from the 1950-60&#x27;sYou forgot that US took every German and Japanese engineer as prisoner. As a friend said, US chemical industry worked 40 years after the WWII with German patents. reply Qwertious 2 hours agorootparent>You forgot that US took every German and Japanese engineer as prisoner.Do you mean took them prisoner for a few days to sort out paperwork, or do you mean took them prisoner and kept them?The latter is provably nonsense as indicated by e.g. Heckler and Koch, a German engineering company (and later gun company) formed by three former Mauser engineers. Mauser, the gun company. reply justrealist 12 hours agorootparentprev> You forgot that US took every German and Japanese engineer as prisoner.This is not even slightly true. reply boringuser2 12 hours agoparentprevI don&#x27;t really like this claim because we are perfectly capable of blowing up our own factories, but people generally agree that it probably isn&#x27;t a good idea. reply dhdudbd 11 hours agoparentprevwhite man&#x27;s burdennever change hn reply davidthewatson 15 hours agoprevI&#x27;m happy to see the link here as I was curious about the subject given that my career started in US Steel&#x27;s data center in Pittsburgh.I&#x27;m saddened by the fact that this retelling seems mostly negative and ignores a large part of US Steel&#x27;s evolution into USX. The retelling is subtractive whether you view Marathon Oil&#x27;s involvement as a positive complementary asset play at the time or a negative given the history of its divestiture.I can say that there was innovation in the data center where I worked in the evolution from manual human mainframe era data center operations to token ring networks of PC API&#x27;s along with abstraction and automation via glue code.The minimization of manual human labor as people retired is likely lost to the history books unless one of my old technical collaborators decides to write a book in retirement.The CMU kids I worked with at US Steel&#x27;s data center in Pittsburgh were just as smart as the ones I worked with in the software industry from Boston to Seattle. reply WhitneyLand 14 hours agoparentI don’t doubt you at all, but what kind of innovation was there?Did it tend to be strategic or tactical?How closely was tied to their core competencies?How many innovations were industry firsts?What percent impact did they have on profits &#x2F; growth &#x2F; market share?From the article it sounds like innovation and investment were consistently blocked by short term financial goals.I can easily believe there were lots of very smart people, with transformative ideas, that were never given a chance to thrive. reply bluGill 5 hours agoparentprevHow does technical innovation serve their core purpose of produting steel? The things you name are a distraction from the real business and should have been bought not done in house. or they should have done a pihiot to tech and got rid of the mills to someone who wanted to run then. reply Digory 14 hours agoparentprevI had the same feeling at the end. Of all the ways to spend the 20th century, being tied to US Steel wasn’t exactly a bad ride.“Arguably, the Harvard system was a disappointment every day since 1636…” reply a1o 15 hours agoprevWasn&#x27;t US Steel who commissioned the books from a SciFi illustrator to ensure that lots of reference drawing illustrations would have steel in the future and got these books for free available to anyone who called, and they ended up getting famous in the film industry so people ended up always designing futuristic movies with things made of steel like ships and vehicles? reply aresant 15 hours agoparentYes you are thinking of Syd Mead - here’s the serieshttps:&#x2F;&#x2F;sydmead.com&#x2F;category&#x2F;gallery&#x2F;us-steel&#x2F; reply fudged71 14 hours agorootparentYou’re telling me the Stanford torus space colony that has filled my dreams for my entire life was propaganda for US steel? reply marcosdumay 13 hours agorootparentAnybody since the 70s designing those radial supports out of anything that isn&#x27;t a polyester isn&#x27;t paying attention. reply scotty79 12 hours agorootparentStarships in After Earth have very cool designs with a lot of threads and fabrics and membranes instead of rigid steel. reply BasilPH 14 hours agorootparentprevSyd Mead did the designs for Blade Runner and Tron, and absolute legend. reply hn_throwaway_99 14 hours agorootparentprevGotta say, I&#x27;d live in that steel modular house in a heartbeat. reply sbierwagen 11 hours agorootparentThen you&#x27;ll love the article OP wrote about Lustron stamped-steel houses: https:&#x2F;&#x2F;www.construction-physics.com&#x2F;p&#x2F;the-lustron-home reply csours 14 hours agorootparentprevThere&#x27;s a reason that house isn&#x27;t pictured in the snow. reply genman 11 hours agorootparentI can be insulated. People have built small houses from marine containers for long time. reply k7sune 11 hours agoparentprevMakes me wonder who supplies the stainless steel used for the starships and cybertrucks. US steel might just turned out to be prescient. reply zokier 10 hours agorootparentIt&#x27;s rumored to be Outokumpu, but Steel Dynamics also getting mentioned. reply Animats 13 hours agoprevFor an overview of how Nucor became #1 in the US steel industry, see \"American Steel\" (1992) by Richard Preston. The author was present for the building and startup of Nucor&#x27;s first continuous thin sheet casting mini-mill. Nucor bought a new experimental continuous caster from a German company, after trying to build their own, and built a mill around it. This plant could turn scrap metal into sheet steel. \"You could punch garbage cans out of it all day.\" Gradually, the quality improved, and soon they were making steel for auto parts. Previously, steel recycling just produced lower grade steel - cars in, rebar out. So this closed the recycling cycle.The amount of steel in use per capita in developed countries seems to have reached a constant level. About 69% of steel produced in the US is the same steel going round and round. If you ignore rebar, low-grade steel stuck inside concrete, it&#x27;s even higher. It&#x27;s the developing countries that are still making and using new steel. They don&#x27;t have enough steel infrastructure yet.\"Mini-mill\", in this context, means \"smaller than a square mile\". Here&#x27;s Nucor&#x27;s Crawfordsville plant.[1] It&#x27;s not small. Compare with US Steel&#x27;s Gary Works.[2] That century old plant is just about their last remaining big plant in the US.US Steel somehow missed this change.[1] https:&#x2F;&#x2F;earth.google.com&#x2F;web&#x2F;@39.97805108,-86.8271336,264.41...[2] https:&#x2F;&#x2F;earth.google.com&#x2F;web&#x2F;@41.62932676,-87.36187513,174.7... reply pfdietz 12 hours agoparentA very good book, although being 31 years old some of the characters are dead now (like the then-head of Nucor.)The chapter describing the accident (where a ladle of molten steel fell and the steel drained into a depressed area with standing water) is horrific. It&#x27;s fortunate the body count wasn&#x27;t higher. reply chiph 10 hours agoparentprevNucor&#x27;s headquarters is near my office. It&#x27;s about the same size as the local Coca-Cola bottler&#x27;s. No huge edifice to the CEO&#x27;s ego - just a typical office building. reply newsclues 10 hours agoparentprevSpeaking of Gary...https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Magnitogorsk\"In 1928 a Soviet delegation arrived in Cleveland, Ohio, to discuss with American consulting company Arthur G. McKee a plan to set up in Magnitogorsk a copy of the US Steel steel-mill in Gary.\" reply ganzuul 15 hours agoprevSteel remains a technology with huge potential for future development. Some keywords: eutectic solution, bulk metallic glass, and boron steel.It seems crazy to me that any first world nation would let it&#x27;s steel production fall into foreign hands. As a machinist for 5 years in my country I would have been excempt from military service even in total war.If US steel is unable to innovate and foreign ownership is somehow not a problem, this development is probably a good thing. The Zaibatsu system is a good fit for what steel is. reply araes 14 hours agoparentAmerica has started to have the appearance of griefing their own enlisted. I&#x27;m not sure if America actually cares about those types of war and military considerations any longer. Have you looked at ship construction times lately?On the materials side. Totally agree. However the issue there, is that&#x27;s not what corporations tend to optimize. The story itself really spells it out pretty clearly (it may be biased, never worked at US Steel personally). However, the article&#x27;s description is:US Steel became a monopoly, and immediately acted like a monopoly. Innovation ceased. Money extraction began. Commanding obedience was the norm. Convincing themselves all competitors would fail was the norm. And US Steel did not want to invest in anything outside its own sunk costs. reply ren_engineer 12 hours agorootparent>I&#x27;m not sure if America actually cares about those types of war and military considerations any longer. Have you looked at ship construction times lately?it&#x27;s kind of funny because the US won WWII by the ability to churn out huge volumes of decent quality goods but now our military seems to be adopting the German idea that small numbers of expensive wunderwaffen will turn the tide reply sbierwagen 11 hours agorootparentWell, what will a future war look like?We&#x27;re never going to fight WW2 again, because all the great powers have nuclear weapons now. The US army will never take the field in a straight up slugfest against Russia or China.So that leaves non-nuclear regional powers. But you simply don&#x27;t need a whole-of-society mobilization to fight Iran. The phase of active combat against Iran will not take years-- it won&#x27;t take months.If we really needed to, we could be building millions of Jeeps again. But we don&#x27;t need to, and won&#x27;t need to. reply tcbawo 5 hours agorootparentMy guess is that the future of war looks like Ukraine, where small, cheap automated drones will overwhelm expensive, low volume hardware, munitions, and defensive positions. reply stjohnswarts 12 hours agorootparentprevAnd Chinese military build out seems to be \"perfect is the enemy of good enough\" and \"quantity has a quality all its own\", so soon we&#x27;re likely to be the ones playing catch up despite our huge military budget and outlays. reply Qwertious 2 hours agorootparent>And Chinese military build out seems to be \"perfect is the enemy of good enough\" and \"quantity has a quality all its own\",They&#x27;re moving away from that - they&#x27;re becoming more similar to the US in prioritizing quality. China is racing to build a first-world economy before their demographics shut down their growth, so relying on cheap, plentiful troops makes less and less sense as the years go on. reply vGPU 13 hours agorootparentprevWhich they are likely sorely regretting as tensions flare higher with China and our navy is struggling to protect shipping around Israel. reply stjohnswarts 12 hours agorootparentThey really aren&#x27;t struggling we have plenty of firepower there. But yeah, China will soon outstrip our naval numbers by a large amount within the end out of the decade. We still have better tech but what do we do when they launch 200 \"good enough\" cruise missiles at each aircraft carrier sitting in the Taiwan Straight? reply stjohnswarts 12 hours agorootparentprevThey really aren&#x27;t struggling. But yeah, China will soon outstrip our naval numbers by a large amount within the end out of the decade. We still have better tech but what do we do when they launch, at the same time, 200 \"good enough\" cruise missiles at each aircraft carrier sitting in the Taiwan Straight? reply andbberger 1 hour agorootparenttypical carrier complement has ~300-500 vertical launch tubes and sea sparrows can be quad packed into a tube, and CIWS. not sure i would bet against aegis reply mkoubaa 14 hours agoparentprevUS steel and steel production in the USA are not the same thing. The company&#x27;s name is that of a legal entity, not an accurate description of what it is. reply colechristensen 7 hours agoparentprevThe steel production will still be local... and it&#x27;s Japan, a close ally. It&#x27;s not like if war broke out the steel mills would evaporate. If for some crazy reason Japan was antagonistic, they would be nationalized or otherwise forced to behave correctly. We have the Defense Production Act which gives plenty of leeway for the government to change situations when national defense justifies it. reply ApolloFortyNine 14 hours agoprevI&#x27;m not sure innovation is really the issue, this article never actually goes into the alloys US Steel developed (of which there are many), and alloys are what makes steel steel really. It&#x27;s always been fascinating to me how the just a tiny percent of another element in a metal can have an absolutely dramatic affect on the strength&#x2F;resilience of the material.And they tried more complicated alloys, for instance they developed Corten steel, the biggest example of which is probably the US Steel building in Pittsburgh, [1]. It&#x27;s a steel where it&#x27;s &#x27;rust&#x27; essentially works as a protective layer.More than anything this article shows US Steel simply couldn&#x27;t compete with foreign suppliers. It&#x27;s interesting to me that they don&#x27;t even mention the Steel Workers Union, which was&#x2F;is one of the largest and most powerful unions in the U.S. I&#x27;m not saying the cause, but if you need 5% more steel to cover the deficiencies in foreign steel in strength, but it&#x27;s 20% cheaper, than it&#x27;s simply cheaper to import more foreign steel.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Weathering_steel reply hulitu 12 hours agoparent> More than anything this article shows US Steel simply couldn&#x27;t compete with foreign suppliersBecause the only inovation came from stolen patents from eastern europe.You can survive doing nothing until your competitor comes with something new. reply shrubble 12 hours agoparentprevIs Corten used in shipping containers (I seem to recall that it is)? reply rhapsodic 14 hours agoparentprev>And they tried more complicated alloys, for instance they developed Corten steel, the biggest example of which is probably the US Steel building in Pittsburgh. It&#x27;s a steel where it&#x27;s &#x27;rust&#x27; essentially works as a protective layer.Also worthy of mention is the New River Gorge Bridge. [1]And the recently-collapsed Fern Hollow Bridge in Pittsburgh. [2][1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;New_River_Gorge_Bridge[2] https:&#x2F;&#x2F;www.carboline.com&#x2F;solution-spot&#x2F;posts&#x2F;pittsburgh-bri... reply steveklabnik 11 hours agorootparentOr… the US Steel Tower https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;U.S._Steel_Tower reply legitster 15 hours agoprevMy grandfather worked in a metal shop for 30 years after he served in Korea. I remember him telling me they switched to Chinese made steel in the 70s because of the quality problems they were having with American made steel. Being out West, they were somewhat more free from the political&#x2F;social&#x2F;union pressure to use American commodities. reply SoftTalker 14 hours agoparentProbably meant Japanese, I don&#x27;t think China had much of a steel industry in the 1970s and given the politics of the time I don&#x27;t think it would have been imported in the USA even if available. reply margalabargala 12 hours agorootparentChina started pushing for a large domestic steel industry in the late 50s.It did not go well. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Backyard_furnace reply legitster 5 hours agorootparentprevHe specifically said Chinese. But given the era it might have well been conflated. reply altairTF 13 hours agoparentprevThe paradox of rights(don&#x27;t quote me, i just made this up). In wealthier societies, demands for greater worker rights from the government increase. This can lead to more bureaucracy and higher labor costs, potentially making third-world countries with lower regulations more attractive for importing goods or outsourcing production. Countries already operates in a libertarian interaction with each others. I find these global economics aspects fascinating reply bluGill 10 hours agorootparentWealthy countries tend to invest in automation. If you have 1&#x2F;10 as many workers you can pay them 5 times as much. reply pseudolus 13 hours agoprevFor individuals interested in the steel industry who are visiting or living in the Northeast of the United States, the National Museum of Industrial History (affiliated with the Smithsonian) situated in Bethlehem, PA is a great place to visit [0]. It&#x27;s located in one of the repair shops of the now defunct Bethlehem Steel plant and offers a wide ranging introduction to the production of steel in the US as well as various types of industrial machinery. As a bonus visitors can stroll the grounds of a largely intact, but derelict, steel plant. Interestingly, that particular area of Pennsylvania was also a center for the production of silk and more women were employed in the production of silk in that region then men were employed by steel plants.[0] https:&#x2F;&#x2F;www.nmih.org&#x2F; reply erehweb 15 hours agoprevReminiscent of the business school joke - What does US Steel make? The answer is not \"steel\", but \"money\", that being the point of any company. reply araes 14 hours agoparentHad a thought the other day, that the natural course of many businesses is towards becoming a bank and eventually a casino. If it were Pokemon, all corporations final form would be casinos with executives gambling investors money.It fits the economy surprisingly well. Harvard, arguably a bank not a school. US Steel, joke is they produce money not steel. Airlines are trying to avoid flying airplanes, and operate air miles banks. Hasbro no longer produces toys, only money. Article today where the main commentary on Intel was the finance bros took over a long time ago. reply feoren 14 hours agorootparentCar companies making their money off of financial instruments only loosely related to the cars people drive off their lots ... reply pjscott 15 hours agoparentprevAlthough it&#x27;s true, that&#x27;s a hazardous way of thinking. If they had put more of their focus on making steel, keeping up with the technological advances rather than being dragged along grudgingly, perhaps they&#x27;d be making more money these days. reply atrus 15 hours agorootparentIt reminds me of that old quote that democracies die when the citizens realize they can vote themselves money. It&#x27;s the same with these companies, the upper managements realizing they can just give themselves more money and coast on their companies momentum.It&#x27;s not the innovators dilemma, it&#x27;s the c-suite lines their pockets while the company burns dilemma. reply nradov 14 hours agorootparentIt&#x27;s the eternal principal&#x2F;agent problem. Those things go in cycles. When management goes too far off the rails then corporate raiders and private equity eventually take over to replace management and unlock latent value. Unfortunately, the managers who caused the problem still often end up fabulously wealthy while regular employees get screwed.This problem can be somewhat ameliorated by compensating executives primarily using equity with long vesting or lock-up periods. That keeps their interests aligned with long-term shareholders. reply scotty79 12 hours agorootparentprevIt doesn&#x27;t really fit the modern collapse. Democracies seem to decline when oligarchs extract so much wealth that the entire economy suffers and common people flock to strong political figures to bring back order and prosperity.> citizens realize they can vote themselves moneyAlthough this certainly sounds true if you consider just the richest citizens and by \"vote\" you mean inflence the votes. reply sgt101 15 hours agorootparentprevIt&#x27;s a very successful strategy in any corporate. Focus on the books, produce results, take the bonus and then jump.When things go pear shaped do not be found holding the bag. If later questioned: \"it was all great when I was there, it&#x27;s so sad that it went south - it was a great place and there was a lot of value on the table.\"Strangely the big investors don&#x27;t seem to ever cotton on to this - the big pension funds and sovereign wealth seem to respond by getting out of the equity markets and investing in things like property. reply hef19898 14 hours agorootparentThe oposite is true as well so: don&#x27;t focus on the books, bottom line and so on and the company goes bust as well.Any successful company has to do both. reply sgt101 12 hours agorootparentyou are quite correct - it becomes a plague when one is addressed to the exclusion of the other. As they say \"don&#x27;t run out of cash\". reply swexbe 14 hours agorootparentprevThis comment reads like it was written in the 70s. Even with the interest rate hike, this is still the age of VCs with infinite pockets, companies that don’t plan on going profitable for decades, every company in sp500 throwing money in the AI money hole, etc. reply sgt101 12 hours agorootparentYes, let us exclude tax scams and money laundering and address the real economy where things get made and real people get paid. reply feoren 14 hours agorootparentprevAs other replies have pointed out, your problem is with the word \"they\". There is no \"they\" at a publicly traded corporation. The key decision-makers are only there for 2 to 5 years, however long it takes them to suck out the blood of the company before they scurry off to parasitize a juicier host. Nobody with decision-making power ever gave two shits whether US Steel was going to make lots of money in N decades. reply latency-guy2 7 hours agorootparentprevNot a hazardous way of thinking. reply cafard 10 hours agoparentprevMany years ago, I read some that some executive of General Motors had said that GM was not in business to make cars but to make money. At the time, that seemed acute. Since then, I have wondered whether it was where GM lost its way. reply pfdietz 14 hours agoprevSomething like 70% of US steel production is now from scrap. Part of this was moderation of growth. In steady state, nearly all steel could come from scrap (limited by contaminants, I guess.)I expect aluminum to displace more steel in the future. Witness what&#x27;s happening with \"gigacasting\" at Tesla and elsewhere. reply kube-system 13 hours agoparentWhether or not an application can use aluminum or remelted scrap depends on how picky an application is for the material. reply bluGill 10 hours agoparentprevsteel has many useful properties hard to get out of aluminum. And vise versa, consult a metraligost to see what is right for you. reply hyperthesis 14 hours agoprevReminds me of the Wright Brothers. After their incredible invention of controlled heavier-than-air flight (using a wind-tunnel - did they invent that too?), the focus was on patent royalties, while others innovated. reply hammock 15 hours agoprevPlenty of innovation by US Steel. Just in operations, not in materials. reply darth_avocado 14 hours agoparentSo in other words, short term value for the shareholders? reply hammock 13 hours agorootparentDefine short term. 120 years?US Steel’s vertical integration practices changed the game and influence goes all the way to Apple and Tesla today.Their way of standardizing manufacturing processes inspired Henry Fords assembly line.Their Research Lab paved the way for the establishment of similar, famous labs at IBM, Bell, Xerox and others.Their corporate structure set a precedent for large-scale corporations, influencing the development of conglomerates like GE to diversify various industries under a single corporate umbrella reply cturner 15 hours agoprevClayton Christensen talk about the steel industry, and how businesses resist change - https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rpkoCZ4vBSI&ab_channel=Sa%C3... reply petermcneeley 12 hours agoprevThis analysis does not seem properly include wage&#x2F;labor costs.\"By 1958 some steelmakers in Germany and Japan were able to compete on price with US producers, and by the mid-1970s input costs for Japanese steel (ore, labor, coking coal, etc.) were nearly half those of US costs.\"So the input costs were half almost certainly all due to labor either directly or indirectly.Viewed in this light the fall of US Steel is no different than any other manufacturing process in the USA. reply 0xDEADFED5 4 hours agoparentpage 17 of http:&#x2F;&#x2F;www.sfu.ca&#x2F;~kawasaki&#x2F;Elbaum.pdf : In Japan between 1956 and 1976 the real cost of steel fell by some 39%. Although wages increased more than 10-fold, gains in labor productivity more than compensated, so that real unit labor costs fell by 16.3%, accounting for about 5 of the 39% drop in real costs. The rest of the 39% was due to a decline in unit materials and energy costs. reply petermcneeley 3 hours agorootparentThis does not tell us what the wages were compared to Americans at that time. It is very easy to have a 10 fold increase in wages if people are making $2 a day. reply 0xDEADFED5 2 hours agorootparentit&#x27;s a pretty interesting paper, it probably answers your questions. paragraph preceding my last quote: In the US between 1956 and 1976, while the dollar depreciated by 17.8%, the real domestic cost of producing steel grew by 37%. Although steel had historically been a materials-intensive sector, by 1956 in the US, labor and materials each accounted for about the same share of unit input costs. Each also grew in real terms by roughly the same rate from 1956 to 1976: unit labor costs by 35% and unit material costs by 38%. Each therefore made like contributions to a loss in US comparative advantage in steelmaking. The main reason for the rise in unit material costs was a 48% increase of the real price of iron ore and (with our 1976 end date falling in the aftermath of the 1973 oil price shock) a near dou- bling of the real price of coke. reply WhitneyLand 14 hours agoprevWell written, informative, worth reading.It’s fascinating how so many business mistakes from the last 100 years continue to be relevant and continue to be repeated. reply KMag 14 hours agoprevIt would have been interesting to get the take from my dad&#x27;s cousin, who did early powdered metallurgy work research as an MIT undergrad in the 1940s, and later became a VP at US Steel. Unfortunately, he&#x27;s no longer with us. reply hyperthesis 14 hours agoprevAt that propitious 1900 banquet, perhaps pricing power was discussed?People of the same trade seldom meet together, even for merriment and diversion, but the conversation ends in a conspiracy against the public, or in some contrivance to raise prices. reply jmclnx 11 hours agoprev>The American steel industry responded to the rise of foreign producers not by trying to improve their operations, but by demanding government protection from “unfair” foreign trade practicesI remember this happening in school. Plus the teacher of a class (70s) I was in blamed the Steel Problems on the Marshall Plan. Until I saw this article I believed that.Now I know it seems to point to the usual US trend of profits before anything else. reply aslgbb 11 hours agoprevTicker symbol is \"X\". I wonder who gets that when US Steel is no longer traded . . . reply yreg 9 hours agoparentI jokingly discussed with friends that we have to buy some when Elon renamed Twitter with the idea that people are going to invest thinking it&#x27;s Elon&#x27;s X.Not sure if that happened but the stock went up quite a bit since then. reply tmm 14 hours agoprevSeems fitting to leave this here:https:&#x2F;&#x2F;youtu.be&#x2F;1D2Q9-1EmB4?si=gXwIO3FzpSKkp3VVUS Steel, Tom Russell reply WhereIsTheTruth 14 hours agoprev [–] I wasn&#x27;t too far off https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38689019 replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "US Steel has faced a decline in market share and profitability in the American steel industry.",
      "Competition from low-cost steelmakers abroad and efficient minimills at home contributed to this decline.",
      "US Steel struggled to adopt new technologies and faced challenges in terms of efficiency and innovation."
    ],
    "commentSummary": [
      "The decline of the US steel industry compared to Germany and Japan is attributed to mismanagement and prioritization of executive pay over innovation and upgrades.",
      "The discussion also addresses the decline of the US auto industry and the impact of US money and loans provided to other countries.",
      "Different perspectives on the outcomes of wars, such as the Korean War, Vietnam War, and the ongoing conflict in Afghanistan, are explored, along with the importance of clear political objectives in warfare."
    ],
    "points": 234,
    "commentCount": 199,
    "retryCount": 0,
    "time": 1703871354
  },
  {
    "id": 38803614,
    "title": "Copyright: A Monetization Strategy, not a Moral Right",
    "originLink": "https://twitter.com/Plinz/status/1740597001652461895",
    "originBody": "Copyright is not a moral right, it&#39;s a monetization strategy that enables some information related business models at the expense of others.— Joscha Bach (@Plinz) December 29, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=38803614",
    "commentBody": "Copyright is not a moral right, it&#x27;s a monetization strategyHacker NewspastloginCopyright is not a moral right, it&#x27;s a monetization strategy (twitter.com/plinz) 209 points by tosh 23 hours ago| hidepastfavorite339 comments LocalH 22 hours agoCopyright was a way to give rightsholders incentive to do what they do, while still ensuring that the work enters the public domain, which is and always has been the absolute endgame of copyright.Today&#x27;s copyright is much different, due to massive lobbying efforts by those who would benefit most. Authors life + what, 100 years or so, is way too long (and that&#x27;s completely discounting those who would want copyright to be perpetual). So you have a situation where a person creates a work, a large rights owner buys it up, and then they get to milk all the benefits while preventing the public from having their rightful free access.Restore copyright to 14+14 years (with a single exception, the original artist and only the original artist gets 28+14, if they sell their rights to the work then the recipient only gets 14+14). Destroy the DMCA. Those are the two most important things to restore copyright to a proper moral standing. Otherwise, copyright as it stands right now is immoral and unjust, and thus (some of) those who pirate are doing so for fully ethical reasons, and are on the better side of history in the long run.Of course, some people just want free shit. However, those people will always exist no matter how reasonable copyright is. Hell, I don&#x27;t mind getting free shit when I pirate something, even if I&#x27;m doing it for moral reasons. reply loupol 21 hours agoparentAgreed on all points, the tweet feels like an allergic reaction to what copyright has morphed into, which is a tool for big corporations to extract money from existing IP for an insanely long duration.But it&#x27;s important to remember that no copyright at all would hurt small and medium creators immensely. Big corps could just pick things that are trending up, rip them off instantly and scale their ripoff much better thanks to their great workforce and marketing reach. That would create a big disincentive for independent creation of IP.Like you, I would just prefer if copyright was kept in place but its duration decreased drastically compared to now. It doesn&#x27;t seem likely given the lobbying strength of the corporations that benefit from the current situation sadly. reply sharperguy 20 hours agorootparentAs a consumer why would I pay spotify $15 a month for ripped off content that doesn&#x27;t even go to the artist, when I can just paypal my favourite artists $5 now and again, giving them infinitely more revenue from me than spotify does even now, while I torrent&#x2F;share&#x2F;sample&#x2F;remix&#x2F;cover their entire collection? reply sojuz151 20 hours agorootparentFor convince? There is a single app with all the music, recommendations, and everything. Also, Spotify would be far cheaper. Most people would just use the app with the biggest catalogue, and not a single dime would go to the artists. reply iwontberude 9 hours agorootparentUnfortunate not convince reply chrismcb 2 hours agorootparentprevYou can do that... But it isn&#x27;t easy, for you nor for the artist. reply naasking 18 hours agorootparentprev> But it&#x27;s important to remember that no copyright at all would hurt small and medium creators immensely. Big corps could just pick things that are trending up, rip them off instantly and scale their ripoff much better thanks to their great workforce and marketing reachThe big assumption in this argument is that large corporations that depend on copyright would still exist, but it seems pretty clear that they would not. If copyright didn&#x27;t exist, then there would be nothing for them to monetize. The first broadcast, distribution or performance of a work could recorded&#x2F;copied and redistributed with no penalty, so large corporations are just as disadvantaged as small players. How do you see large corporations forming and perpetuating themselves in order to exploit these smaller players? reply snowwrestler 17 hours agorootparentLarge-scale distribution is expensive and complicated. Companies don’t need to control copyright to make money doing it. In fact digital distribution companies make way more money today than copyright holders. Look how much Google, Apple, Amazon, etc are worth, compared to the labels. Spotify alone has more annual revenue than all the big music labels combined. Copyright is the legal lever that allows labels and artists to make a claim on some of that distribution revenue.This is in fact the origin of the legal concept of copyright: established printers who had more money and equipment than authors would print up copies of popular written works, distribute and sell them, and return nothing to the author. reply saaaaaam 12 hours agorootparent> Spotify alone has more annual revenue than all the big music labels combinedThis is very much not the case. In fact Spotify’s revenue in its last full year was less than half of the combined revenues of the three major record labels for a similar period.Spotify generated €11.7 billion in revenue in 2022, approx $12.9 billion.Universal Music Group generated €10.3 billion in revenue in 2022, approx $11.3 billionWarner Music Group generated $5.9 billion in their fiscal year ending Sep 30 2022Sony Music generated ¥1.3 trillion (approx $10.1 billion) in their fiscal year ending March 2023.Total revenues of major labels: $27.3 billion.Spotify: https:&#x2F;&#x2F;s29.q4cdn.com&#x2F;175625835&#x2F;files&#x2F;doc_financials&#x2F;2022&#x2F;q4...UMG: https:&#x2F;&#x2F;www.universalmusic.com&#x2F;universal-music-group-n-v-rep...WMG: https:&#x2F;&#x2F;investors.wmg.com&#x2F;static-files&#x2F;f35e3e8a-8ae2-4960-95...Sony: https:&#x2F;&#x2F;www.sony.com&#x2F;en&#x2F;SonyInfo&#x2F;IR&#x2F;library&#x2F;presen&#x2F;er&#x2F;pdf&#x2F;22... reply bloppe 16 hours agorootparentprev> Large-scale distribution is expensive and complicated.Not anymore. Most CDN&#x27;s will do that for free. Distribution is easy. Spotify makes money because open source pirating networks attract the attention of law enforcement. reply naasking 16 hours agorootparentprev> Large-scale distribution is expensive and complicated.It is in fact neither expensive, nor complicated, eg. napster. reply malwrar 12 hours agorootparentprev> Large-scale distribution is expensive and complicatedMaybe centralized distribution. Bittorrent appears to be a fairly cheap and reliable method for serving content that scales nicely as more people join the swarm. reply sensanaty 17 hours agorootparentprevMaybe if we lived in a world that never had copyright law to begin with, but we don&#x27;t. We live in a world where trillion dollar entities like Microsoft & Disney exist. Even if you abolished the concept of Copyright, Patents and all the rest of it now at this very moment, these companies would still have trillions in their coffers with which they could do plenty of harm to smaller entities. You think Microsoft, who already have a habit of harvesting as many things as they can en-masse, isn&#x27;t gonna be able to do anything to harm the smaller players? You think Disney isn&#x27;t just gonna go out and straight up hoover up every single byte of music & video in existence that they can get their hands on and start reselling it?> ...then there would be nothing for them to monetize...... other than the works of every single person they could possibly get their hands on, as is happening with the AI companies. reply naasking 17 hours agorootparent> Even if you abolished the concept of Copyright, Patents and all the rest of it now at this very moment, these companies would still have trillions in their coffers with which they could do plenty of harm to smaller entitiesAnd they would slowly dwindle and die as their revenue streams dried up. I&#x27;m still not seeing the issue. They&#x27;re already \"harming\" small creators in these same ways, particularly because extended copyright means we can&#x27;t have derivative works, thus stifling innovations of smaller creators right now.> You think Disney isn&#x27;t just gonna go out and straight up hoover up every single byte of music & video in existence that they can get their hands on and start reselling it?Reselling what? Something you would be able to download for free on the Internet if copyright didn&#x27;t exist? An open source Spotify would immediately pop up that would only charge you enough to cover hosting. What commercial enterprise do you think could compete with that long-term? reply snowwrestler 17 hours agorootparent> An open source Spotify would immediately pop up that would only charge you enough to cover hosting. What commercial enterprise do you think could compete with that long-term?What you are describing is a commercial enterprise. reply naasking 16 hours agorootparentOnly charging for hosting costs is not a commercial enterprise, more like a non-profit. reply sensanaty 17 hours agorootparentprev> And they would slowly dwindle and die as their revenue streams dried upPresumably they wouldn&#x27;t just do literally nothing in this new Copyrightless world, I imagine with their trillions of dollars they can come up with new business ideas in this new world devoid of intellectual property rights.> They&#x27;re already \"harming\" small creators in these same ways, particularly because extended copyright means we can&#x27;t have derivative works, thus stifling innovations of smaller creators right now.Okay, but I don&#x27;t get what type of innovations - other than AI chatbots, and I mention this with a huge asterisk because all people are asking for is for these trillion dollar corporations to pay the people who&#x27;s work they&#x27;re benefiting from - are being stifled right now? There&#x27;s more media than ever before and it&#x27;s only accelerating despite all the claims of stifled innovations. Genuine question, do you have a list of things that you&#x27;d say are being stifled by over-aggressive Copyright laws? Even if we venture out of Copyright and into Patents and Big Pharma, I especially can&#x27;t imagine many people who have the skills necessary to come up with new medicines doing their work for no compensation.> Reselling what? Something you would be able to download for free on the Internet if copyright didn&#x27;t exist?But who would create all of this free music for the open source Spotify to gobble up? Sure there&#x27;ll be a chunk of people out there still creating things because they want to create things, but they also have to put food on the table at the end of the day, how are they gonna do that if everything they ever produce just gets swallowed by the black hole known as the internet? Why would anyone create anything at all, if the moment they do it gets redistributed to everyone else for free? Even open source licenses often come with strings attached, I can&#x27;t imagine that most people would be happy with all their work being gobbled up without even acknowledgment of where the work comes from, which is already a part of the most commonly encountered OSS licenses.I just don&#x27;t see how this world you&#x27;re envisioning can exist in a non-Utopian non-post-scarcity world where the majority of people are living paycheck-to-paycheck and are barely scraping by as is.> An open source Spotify would immediately pop up that would only charge you enough to cover hosting.Who&#x27;d wanna pay for that, if you can just download the music yourself?> What commercial enterprise do you think could compete with that long-term?You&#x27;re literally describing a commercial enterprise here, the only difference being that the OSS version of Spotify just doesn&#x27;t pay artist&#x27;s for their music (ignoring that Spotify already barely pays artists anything). Spotify already charges people to cover hosting (+ employees and all the other associated costs), is an OSS version of Spotify that just pirates their catalogue really innovative to you? Cause that&#x27;s exactly what you&#x27;ve described here. reply naasking 7 hours agorootparent> Okay, but I don&#x27;t get what type of innovations - other than AI chatbots, and I mention this with a huge asterisk because all people are asking for is for these trillion dollar corporations to pay the people who&#x27;s work they&#x27;re benefiting from - are being stifled right now?Software, music, graphics are all subject to substantial restrictions on new works because of copyright. You don&#x27;t even notice it because it&#x27;s become so normalized.> Sure there&#x27;ll be a chunk of people out there still creating things because they want to create things, but they also have to put food on the table at the end of the day95%+ of musicians don&#x27;t make money from music distribution, they make it from performances when touring. Eliminating copyright would have no impact on this. It&#x27;s the same reason open source developers can still feed their families.Graphic artists would still be commissioned for custom works, although AI will now eat into that too somewhat.Many, many people would continue to write, compose and create art despite no financial incentives. Just look at all of the fanfiction and fan art out there.> Who&#x27;d wanna pay for that, if you can just download the music yourself?You absolutely could, but people often pay for extra convenience: an easily searchable index, music recommendations, playlists that can sync across devices, and so on.> You&#x27;re literally describing a commercial enterprise here,I&#x27;m more describing an almost non-profit that provides a convenient interface. Spotify isn&#x27;t just charging for hosting, it also has to pay licensing fees for music rights and profit margins for investors. Neither of those factor into this new fictional world we&#x27;re discussing.I&#x27;m not sure why this OSS version of Spotify has to be \"innovative\", the innovation is the low cost access to all of humanity&#x27;s musical creations.Copyright was intended to advance progress in the arts and sciences, but it&#x27;s honestly doing the opposite, and has been for quite some time. reply malwrar 11 hours agorootparentprev> Okay, but I don&#x27;t get what type of innovations are being stifled right now?I think they mean derivative works. If one wanted to make e.g. a fan-made Star Wars movie or open source version of a closed source game, under the current regime they could and have been sued into oblivion. Tons of examples of this occurring for media. In software, copyright is used by trillion dollar entities to bully smaller projects aiming for things like interoperability to be distributed. reply Mountain_Skies 13 hours agorootparentprevBreak them up as part of the process. It&#x27;s well beyond the time that should have been done anyway. Trillion dollar corporate entities shouldn&#x27;t exist nor should any of them have ever gotten anywhere close to being over 1% of GDP. Just look at the weirdness that happens in South Korea with Samsung having so much control. reply beej71 3 hours agorootparentprevIs the argument here that without copyright the works would be effectively worthless, so the corporations couldn&#x27;t make money off them? reply chrismcb 1 hour agorootparentBasically. Studios wouldn&#x27;t invest 100s of millions of Dollars to create blockbuster movies of they couldn&#x27;t get their money back. Personally I like watching blockbuster movies. Same goes for almost anything that takes time to create. reply Centigonal 17 hours agorootparentprevH&M, Shein, etc are not IP-based organizations - their value comes from their massive and highly efficient production, supplychain, and distribution networks. They steal or dupe many of their designs, usually from smaller creators, designers, and boutiques. A no copyright environment would benefit businesses like this. reply naasking 16 hours agorootparentSure, and benefit all of us in turn given the lower costs for nice clothing due to economies of scale. That&#x27;s ostensibly the point of copyright, no? To benefit society by advancing useful arts and sciences.So boutique designers that can charge high prices due to artificial scarcity would not be a thing. If there are no more boutique designers, then these firms will have to commission designers themselves, or we rely on open source design work, eg. students who are learning design, enthusiasts, and so on. I&#x27;m not really seeing a real downside here. Yes, things would be different, but would they be worse? I don&#x27;t think so. reply myaccountonhn 20 hours agorootparentprevThat’s what they already do today. OpenAI is an obvious example in our industry but another one is where companies steal clothing designs from indigenous communities all over America. These people can not really defend or assert themselves in US copyright system, so US companies just engage in full on resource extraction without any compensation. reply chrismcb 1 hour agorootparentClothing design is difficult to copyright. I&#x27;m pretty sure just about anyone can defend themselves in the US copyright system. Sure it becomes difficult when you are a small fry (whether indigenous or not) against a big fish, but it can be done. reply sveitly 20 hours agorootparentprevNew or traditional stuff? Copyright shouldn&#x27;t be how you defend your traditional stuff. It&#x27;s more something for the Protected Designation of Origin &#x2F; PDO reply soerxpso 13 hours agorootparentprev> But it&#x27;s important to remember that no copyright at all would hurt small and medium creators immensely.This doesn&#x27;t refute the tweet. You&#x27;re just saying you want a monetization strategy that enables an information business model that you like (small&#x2F;medium creators) instead of one that you don&#x27;t like. It&#x27;s still not a moral right, and your belief that the law should help artists is not innately superior to a belief that the law should help Disney or that the law should help corn farmers.Additionally, it&#x27;s arguable that copyright is needed for small&#x2F;medium creators to succeed. Most successful independent creators monetize through patreon or personal commissions, neither of which are actually hindered by lack of copyright. reply PH95VuimJjqBqy 8 hours agorootparentnowhere in that commenters post did they mention preference, they made an observation. You&#x27;re the one trying to apply feelings to it. reply grungydan 12 hours agorootparentprev>Big corps could just pick things that are trending up, rip them off instantly and scale their ripoff much better thanks to their great workforce and marketing reach.Except that they are already doing this, and since money == \"justice\" in this country and most of the rest of the world, their big bag of money means that you have zero realistic chance in winning any attempt at suing them for doing so.Copyright is a great conversation to have, but it&#x27;s a bit like sitting around talking about how to best dry off a plate in the Titanic dining room. reply LocalH 21 hours agorootparentprevThe DMCA has also been abused far past its original intentions. It was basically originally intended to stop people copying DVDs. Now it&#x27;s used to protect these asinine little \"digital locks\" inside of everything that prevents independent repair by pairing parts using cryptographic exchanges. reply dventimi 19 hours agorootparentprevHang on. How would there even be big corporations to hurt small and medium-sized creators, without copyright? reply User23 19 hours agorootparentTrade secrets can exist without any supporting legislation. reply GavinMcG 18 hours agorootparentBut copyright doesn’t deal with secrets. Just the opposite: it gives control over something that one has made public and attaches upon publication. reply User23 17 hours agorootparentSure, but it’s an alternative way a large SaaS provider might attempt to defend whatever competitive advantage its code provides. I’m not saying it’s equivalent or even as good, just that a company with good data controls could probably grow large absent IP laws.We’d probably also see rapid advances in homomorphic encryption to enable deployed software. reply dventimi 18 hours agorootparentprevTrade secrets are practically impossible to maintain without supporting legislation. reply chrismcb 1 hour agorootparentThis seems like an odd comment. Yes there is law that protects trade secrets. But you still need to keep the secret a secret. If the secret gets out, it is no longer a trade secret. reply Brian_K_White 17 hours agorootparentprevCorporations get everyone to do things they don&#x27;t have to against their own interests all the time.All day every day every industry every level.\"How?\" is infinite different ways not any particular one.Usually it&#x27;s down to something being 0.001% prettier or more convenient or even a totally fabricated impression that everyone else does it (which then becomes true but is only true after the idea was used).They sucessfully harness the desire for conformity in some people and also the desire for non-conformity in other people, at the same time for the same products.They completely effectively harness countless well studied aspects of human nature.If you&#x27;re like me, sitting here writing about how cynical and manipulative they all are, they have angles that work on that too. reply dventimi 17 hours agorootparent> Corporations get everyone to do things they don&#x27;t have to against their own interests all the time.Not without government intervention, they don&#x27;t. In fact, they wouldn&#x27;t even exist without government intervention. reply Brian_K_White 17 hours agorootparentHarnessing knowledge of human nature does not require government support. Governments don&#x27;t create human nature.They use the government where possible, but as just one of countless tools. They don&#x27;t always get what they want from the governemnent, yet they still make money. As often as not, corporations end up making more money as a result of losing some fight with a government.All they need to make money is activity. Any activity, even \"the government just took away something we were using and dinged us for $200M\" 6 months later they are worth twice what they were before, because that was big activity.If the government takes away a toy they were making money from, they just figure out some other new toy, and in the end the shake-up and (forced) opportunity for change was worth more than what they were making from the status quo. reply dventimi 17 hours agorootparent> Harnessing knowledge of human nature does not require government supportYeah but fencing it off from others does require government support. If it&#x27;s human nature to create, it&#x27;s also human nature to copy. If you create some new worthwhile invention, I&#x27;ll just copy your invention without even asking you, and without the government there&#x27;s nothing you can do to stop me. reply Brian_K_White 17 hours agorootparentNo, it does not. It&#x27;s just one of countless levers.There are countless things anyone can copy or do for free already right now, that countless people pay a company for, for no reason at all. No government enforcement of anything involved. reply dventimi 16 hours agorootparent> There are countless things anyone can copy or do for free already right now, that countless people pay a company for, for no reason at all.Name one. reply Brian_K_White 16 hours agorootparent3% tax on every transaction in your entire life going to Visa by using a debit card instead of cash.There are people (not huge corps in this case) selling CDRs of PDFs from archive.org on eBay. But more to the point, people buying them.The completely intangible nothing that differentiates a Burberry bag from any other medium quality bag.Now that last one almost sounds like the opposite point since the intangible nothing is exactly what the government is protecting there, but the government does not enforce that you need to pay Burberry to get a bag exactly like it in both quality and aesthetic, but people voluntarily do anyway. reply dventimi 16 hours agorootparent>>> There are countless things anyone can copy or do for free already right now, that countless people pay a company for, for no reason at all.>> Name one.> 3% tax on every transaction in your entire life going to Visa by using a debit card instead of cash.There&#x27;s a reason for that: Visa and MasterCard have monopoly power obtained via anti-competitive practices which were enabled by contract law. Visa and MasterCard prohibited member banks from issuing their own cards. Discover and American Express among others sued Visa and MasterCard for this about 15 years ago.> the government does not enforce that you need to pay Burberry to get a bag exactly like it in both quality and aestheticThe government does enforce that. A potential Burberry competitor cannot sell a bag \"exactly like\" Burberry&#x27;s because to do that it would have to have the Burberry logo, which is a trademark protected by federal law. reply Brian_K_White 13 hours agorootparentBut all the government is enforcing or protecting is an identity, not a thing. Burberry convinces people to do something they don&#x27;t have to do, EVEN to get a bag of the same style and quality.What do the customers get? They get nothing more than the social status of other people seeing them have it. That value is something that doesn&#x27;t exist except that Burberry created it out of thin air. The tools that Burberry uses to to produce those sales are not the government protection of the exclusive right to sell a bag of a certain style, it&#x27;s the knowledge of human nature, in this case, status displays. reply dventimi 8 hours agorootparent>What do the customers get? They get nothing more than the social status of other people seeing them have it. That value is something that doesn&#x27;t exist except that Burberry created it out of thin air.If those customers value that social status, how did you derive the authority to tell them they&#x27;re wrong?Of course, you don&#x27;t have that authority. Nobody is in a position to tell other people what they should or shouldn&#x27;t value. Some people value the exclusivity of fashionable brands, but that exclusivity cannot be maintained without government involvement. Without it, there&#x27;s nothing to stop somebody besides Burberry from creating indistinguishable copies of Burberry bags, complete with the Burberry logo, at lower prices. If they&#x27;re indistinguishable then who wouldn&#x27;t buy them at the lower price? If they&#x27;re indistinguishable, how would you even know it&#x27;s a copy? If it&#x27;s indistinguishable, is it EVEN a copy?No. Monopoly control of something--a resource or in this case an idea--requires the power to enforce that monopoly. Typically, governments have that power and among the ways to exercise it is to grant and protect patent and copyright and trademark monopolies. reply Brian_K_White 6 hours agorootparentI never said monopoly control. The entire point was that enforced monopoly control is not required. That is thinking way too simplistic. Companies get people to pay for things that they don&#x27;t have to _all the time_, by all kinds of different means. They only also use government granted monopoly because why not if it&#x27;s available. reply dventimi 4 hours agorootparent>I never said monopoly control.You didn&#x27;t have to. When asked to name one example of corporations getting people to pay for things they don&#x27;t have to, both of the examples you named were examples of a monopoly. Burberry, for instance, has a monopoly. Only Burberry can make bags with the iconic pattern and logo, and if you value the iconic pattern and logo for reasons you&#x27;re not obliged to justify to anyone else, you DO have pay Burberry for the privilege. That&#x27;s a monopoly, and it wouldn&#x27;t exist if the government didn&#x27;t maintain it. reply fsflover 15 hours agorootparentprev> But it&#x27;s important to remember that no copyright at all would hurt small and medium creators immensely.This is false: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15309950 reply chrismcb 1 hour agorootparentPiracy isn&#x27;t what the OP is talking about (at least I don&#x27;t think they are) what they mean is a bigger business will come and steal the small&#x2F;medium content. Since they are bigger they can do a better job of selling the content than the small&#x2F;medium person. reply FrustratedMonky 18 hours agorootparentprevIt is currently so pro-big business, I would not be surprised if a modern &#x27;free&#x27; market economy Republican, would think the original arguments to create copywrite, sound socialist. reply Mountain_Skies 13 hours agorootparentYou&#x27;ll never win against Big Business while you&#x27;re still stuck in the half century obsolete paradigm of the Democrats not being a wholly owned subsidiary of Big Business Inc just like that Republicans are. reply posix86 20 hours agorootparentprevNot just small, also big companies. Why spent billions of dollars researching a new technology when, as soon as you figure it out, someone else copies it for a millionth of the cost? The dominant strategy will be just to wait for others, which might slow progress a lot. reply thereddaikon 19 hours agorootparentYou&#x27;re thinking of patents which are similar but concern technical inventions whereas copyright is for creative works. And then you have trademarks which are for business branding. They are all government enforced monopolies on ideas but different kinds of ideas, each with limitations and with the intent to facilitate economic activity. Reforming copyright shouldn&#x27;t have an effect on patent law. reply LtWorf 20 hours agorootparentprevCopyright and patents aren&#x27;t the same at all. reply kasey_junk 19 hours agorootparentBut they protect creators in the same sorts of ways. reply goodpoint 18 hours agorootparentNo, they were supposed to protect creators. Now they protect big corps and harm small ones and people. reply chrismcb 1 hour agorootparentI&#x27;m not sure how they (copyrights? Patents?) Harm small ones and people?For copyrights it works be nice to get some of our culture sooner, but I don&#x27;t think it harms us. Same for patents. You can make an argument that both should be shorter. I would also accept that software patents shouldn&#x27;t exist)I think they are definitely rubber stamped to much) but how is the small creator harmed? reply LtWorf 56 minutes agorootparentThey patent anything, and DCMA takedowns are basically random. After that, if you can&#x27;t afford lawyers you&#x27;re screwed regardless of the validity of the claim. reply kasey_junk 16 hours agorootparentprevI don’t know how you can hold the position that copyrights do that but patents don’t. reply dventimi 19 hours agorootparentprevWell, they&#x27;re the same at least insofar as they&#x27;re government-granted monopolies. reply contravariant 21 hours agoparentprevDestroying the DMCA will go quite a way in improving the whole DRM situation, but frankly we&#x27;re at the point where we need to fight back hard. We&#x27;re heading in a direction where nobody can truly own their own devices, because some people want to be absolutely sure no unauthorized (e.g. free, open source) software runs on it, because god forbid anyone should have any agency.We should make clear that copyright is about publishing not using and that any usability infringements are infringements on ownership i.e. stealing. reply Chris_Newton 21 hours agorootparentI’d be in favour of excluding certain types of work from copyright protection entirely. Copyright isn’t there to create a barrier to interoperability or a way to artificially prevent competition in markets, so IMHO things like data formats and APIs should not be copyrightable. This isn’t to say that no real creative work is involved in producing those things, only that the damage to society from allowing them to be protected in this way is too great.Curiously, the US does recognise this concept to an extent in its legal treatment of fonts, but unfortunately that remains the exception rather than the rule, and even that exception is not recognised in the global copyright treaties. There is still a long list of software and hardware that is protected from otherwise normal competition by repurposing copyright to prevent things like easily exporting a user’s own data to use with different software or swapping out a failed hardware product for an after-market alternative that wasn’t made by the original manufacturer. In this respect, copyright now acts like bad patents, subverting the original intent and corrupting any reasonable moral&#x2F;economic arguments behind it, and instead merely providing a legal weapon for powerful incumbents to inhibit creative work it was supposed to encourage. reply contravariant 21 hours agorootparentI don&#x27;t think API copyright is recognized, even in the U.S. Logically speaking it shouldn&#x27;t constitute a copyrightable work, but you never know sometimes. The documentation could be considered copyrightable, though you could just reverse engineer it.It&#x27;s kind of the same category as fonts and recipes, which you can figure out by inspection mostly. If you want to protect something like that then a patent might be in order, though let&#x27;s not get into that mess... reply lebuffon 19 hours agorootparentAPI might go down the road travelled by music in the last 25 years or so.Music used to be a copyright on a melody. Arrangers&#x2F;producers had nothing. Recorded music now has expanded to have \"production rights\". This a copyright on the way the released recording \"sounds\". The recent Ed Sheeran court case is a good example how weird that can get.So APIs could be get there with the right crop of lawyers and court cases. ? reply marcosdumay 19 hours agorootparentprev> I don&#x27;t think API copyright is recognized, even in the U.S.That was what every single person thought right until Oracle won a suit against Google for use of the Java&#x27;s API. reply jfengel 18 hours agorootparentAs I recall that was a claim of a byte for byte copy. Google claimed there was no other meaningful way to use the API but the court said that copying the file itself was a step too far. reply contravariant 17 hours agorootparentprevI&#x27;m confused, all that I can find is that the final decision by the supreme court was that it fell under fair use. reply marcosdumay 15 hours agorootparent> it fell under fair useWhat means it&#x27;s protected by copyrights. Fair use on the US is a murky thing that only large corporations have the legal power to bet on.But the sibling saying it&#x27;s about the organization of the files may be on point. I remember it being about the organization into packages, but I can easily be wrong here. replyapi 20 hours agorootparentprevGet rid of copyright and the industry response will just be to move everything to the cloud. You don’t even get a device, just a thin client. Things will be more locked down, not less.People want information to be free and have largely price anchored on free, but information of any quality is extremely labor intensive and expensive. That radical mismatch between desire and reality has and will continue to twist the market into weird and increasingly hostile shapes. reply Zambyte 18 hours agorootparentYou can&#x27;t see or hear media in \"the cloud\". It has to be downloaded to your device to do that. \"Streaming\" is just downloading + (usually) DRM enforced deleting, which is exactly what the person you&#x27;re replying to is saying will improve when we repeal DMCA. reply api 14 hours agorootparentDRM can get much more restrictive, such as being locked down to only special purpose devices produced by the publisher or distributor &#x2F; streaming service. So imagine requiring a Netflix dongle to watch Netflix, etc. Without the DMCA these things would just get built like Yubikeys with internal HSMs that require clean room practices to unlock. That&#x27;ll be combined with watermarking so if you do pirate it and leak it they sue you by tracing it back to you.My real point is the second part: quality media of any kind is very expensive and labor intensive to produce. Piracy can result in only two possible things: (1) even more restrictive and arcane DRM, or (2) the media you like just stops getting produced at all.If people flatly refuse to pay and insist on pirating, option (2) is the final endpoint. You just won&#x27;t get the product because nobody will make it.Free is a lie. When you insist that things must be free, you&#x27;re lying to yourself. When companies tell you it&#x27;s free, they are lying to you (usually to get you hooked for surveillance or later monetization). reply joquarky 12 hours agorootparent> Without the DMCA these things would just get built like Yubikeys with internal HSMs that require clean room practices to unlock.There is always the Analog hole¹, and I don&#x27;t think that can ever be resolved. You only need one A&#x2F;D conversion where quality is paramount and after that you have DRM free copies.> the media you like just stops getting produced at all.This is a sacrifice I&#x27;d be willing to make if it means eliminating DRM and other schemes that do NOT promote the progress of science and the useful arts.¹ https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Analog_hole reply api 11 hours agorootparent> This is a sacrifice I&#x27;d be willing to makeI feel like I must not understand your point, because this seems odd to me. You’re saying it’s better to have no high quality media at all than to have any form of DRM?If it makes you feel any better, I think this is where we are headed because as you correctly point out DRM doesn’t really work. The future is cheap trash produced almost for free by AI, crowdsourced filler like TikTok, or ads and propaganda funded by someone other than the consumer to push a message.The period from the birth of recorded media to the birth of easy digital reproduction will be remembered as this weird golden age when massive amounts of incredible recorded art, film, and music were produced because there was an economic model for it.I used to have the “information wants to be free” view until I saw how much effort goes into producing any form of media of any quality. It’s not easy or cheap at all and if it’s not funded it won’t happen. replyPicassoCTs 19 hours agorootparentprevNo need to look far, the pathology is blatantly obvious visible in some despots - who think normal everyday people should never have agency and can ever be only puppets for the powerful. Secret services, governments and cooperations are filled to the brim with these types at the top. So the first step to remove that sort of policy, is a screening for the pathology among the powerful. reply snapplebobapple 1 hour agoparentprevMostly agree. You are missing the economic jargon as to why copyright is bad (it causes deadweight loss to society by arbitrarily pricing above marginal cost) and why it might be good (maybe more creative work production because profits from works are higher). From the economics springs the right tjing to do: minimize deadweight loss, which means always eyeing ways to reduce copyright while maintaining output. From that i would argue i stead ofa fixed term makerenewal cost exponential by term. I thibking setting term to 10 years, giving first term for free then swt renewal at 100^n for each additional n terms would do the trick and cause nearly all works to enter public domain after 1 or 2 terms and the enduri gly profitable stuff (i.e. mickey mouse type stuff) to be unprofitable to renew after 50 years or so, preserving the fantasy of a gargantuan paypff to stroke the egos of content creators. reply dexen 22 hours agoparentprevI subscribe to your and broadly follow your arguments. It&#x27;s also worth pointing out the speed of information propagation, speed of manufacturing, and speed of distribution & logistics has dramatically increased since 14+14 years was the pragmatic standard. At present, the same economic effect is achieved in much shorter cycles. Keeping economic and moral parity would entail shortening the protection times a fair bit. reply Lerc 19 hours agoparentprevThe underlying principle for all laws should be that they should serve society, not individuals. Unfortunately influence is more often wielded by individuals. I think this causes an inevitable drift away from societal benefit but there are people pushing against that tide.It is rather odd seeing a such a visceral reaction to a tweet that is, in essence, true. Copyright exists for the reason you give, and that&#x27;s what the tweet is alluding to. I think we should always be reassessing the reasons for our laws. I&#x27;m inclined to think that while copyright is \"a monetization strategy that enables some information related business models at the expense of others\" It is one that overall benefits society, albeit I would also like to see changes along the lines of what you suggest.Having said that I don&#x27;t think it inevitable that it will always be the case. Should there be one-day a post scarcity world where no-one is required to earn money to survive, perhaps it would be better to make all information un-owned.I am put in mind of a map I once saw showing the world map stretched to show an even population density, and then the birthplaces of the greatest intellectuals and artists that the world has produced. Realized creativity came from where the money already was. Vast populations of poor regions would have been producing individuals with a similar potential. Potential that went unrealized. That may seem unrelated to the topic of copyright, but I&#x27;m not sure it is. If we are to make decisions about which way we are headed, it is worth thinking about where we could go. reply Eddy_Viscosity2 20 hours agoparentprevWhich political candidate&#x2F;party do I vote for to make this happen? reply CaptainFever 20 hours agorootparentThe Pirate Party: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pirate_Party reply Eddy_Viscosity2 19 hours agorootparentThe pirate party does not appear to have a candidate running in my area. reply swayvil 20 hours agorootparentprevWe have the freedom to choose among the options that they allow us. reply Eddy_Viscosity2 20 hours agorootparentWhat&#x27;s more frustrating is that things like copyright terms are often part of trade agreements (negotiated in secret). So you don&#x27;t get any say in what they are in your own country AND they get forced down the throats of other countries if they want to participate in the global economy. reply WarOnPrivacy 19 hours agorootparentprevIt&#x27;s even better. Partyism is baked right into to local governments whoadminister and fund party electionsprotect party power thru primary voting restrictions - and then by doing all thatgift parties a veneer of gov authority (which helps lock out competition). reply iti7 17 hours agorootparentprevFortunately humans age out of life. Millennials and GenZ that grew up on the web are sticking to center and center-left political positions. Political policy of the day that deflates their buying power of necessities not just PS5, is not endearing them to preserve the status quo.Electoral turnovers flush corruption and improve economic power for general public: https:&#x2F;&#x2F;www.nber.org&#x2F;papers&#x2F;w29766The drop off in allegiance to religion since the 00s has broken thousands of years of socialized obligation to maintain a specific historical story.Our brains devalue sources of information after a certain amount of time: https:&#x2F;&#x2F;www.sciencenews.org&#x2F;article&#x2F;mom-voice-kid-brain-teen...It’s been roughly 15 years since the cloud app boom really took off with users, people are tired of social media. Seems likely this trend of getting sick of the same old holds throughout life. Lack of obligation to historical story is revealing interesting things about our ebbs and flows.See also dwindling interest in comic book movies, a trend that took off with iron man… 15 years agoiPhones -> goggles within a similar timeframe15 years before that, in the early 90s the information super highway was taking off15 years before that we got sick of the oil crisis triggered by policy 15-ish years before Reagan (3-5 year margin of error)Thomas Jefferson is said to have written the Constitution ought to be rewritten every 19 (15+4) years or else the dead rule by fiat decree. Today we find the philosophy is embedded in our biology. reply LtWorf 20 hours agorootparentprevCan you collect signatures and have a referendum in USA? reply BlarfMcFlarf 18 hours agorootparentYou can do it in some states, but no national level mechanism exists. The whole system is designed for the federal stuff to be filtered through states and state governments. reply edgyquant 18 hours agorootparentprevDepends on the state. Even then, in the last generation California has passed a few things by referendum that were overruled by the court. reply Eddy_Viscosity2 16 hours agorootparentCould a state unilaterally decrease the copyright length? reply ronsor 14 hours agorootparentNo. The federal Copyright Act explicitly preempts state laws that implement copyright or anything close to it. Only Congress can fix this mess. replyalberth 16 hours agoparentprev> ”So you have a situation where a person creates a work, a large rights owner buys it up, and then they get to milk all the benefits while preventing the public from having their rightful free access.”The root of your argument seems to be that the public has a right to free access of someone’s work, why? reply hn_acker 6 hours agorootparentI think what LocalH was referring to by \"rightful free access\" was that copyrighted works are supposed to fall into the public domain after the copyright term expires. The public is entitled to unrestricted access to works whose copyright terms have expired. Excessively long copyright terms impeded that access.In the US, copyright is the means to incentivize people to make creative works. Copyright is a limited-time exclusive right. When that right expires, the First Amendment is no longer constrained by the Copyright Clause. (I wrote more about the Copyright Clause in a different comment [1].) In other words, sharing and modifying public domain works is protected by freedom of expression.Copyright is a social contract. The author receives: exclusive copying rights to the author&#x27;s creative works. The public receives: the author&#x27;s creative works at some point in the future, though the author (or rather the rightsholders, should the author partially, temporarily, or permanently give the copyright to other people) can set licenses for certain people (such as buyers and remixers) to access&#x2F;copy the works now.[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38812469 reply zugi 20 hours agoparentprevMostly agreed, but can you elaborate on the \"exception\"? Why should selling the copyright extend its total duration from 28 to 42 years?I&#x27;d be fine with just a fixed 28-year duration of all copyrights. That way you don&#x27;t have to worry about whether the original author (who could be a company) is dead or alive. reply danaris 20 hours agorootparentYou have misread the GP. Their \"exception\" is the exact opposite: if the copyright is sold, its total duration cannot exceed 28 years, while if it is retained by the original (human) creator, it can extend to 42 years. reply zugi 20 hours agorootparentI see, thanks for the clarification.A fixed 28-year duration would be shorter and simpler. reply 127361 14 hours agoparentprevRestricting who can profit from work you created sounds reasonable to me. I don&#x27;t have any problem with that aspect of copyright. reply ForHackernews 20 hours agoparentprev> Of course, some people just want free shit.People deserve free shit. If it costs ~$0.00 to copy something, the morally just thing to do is to give everyone who wants it a copy. If we could generate infinite socks for free, it would be outrageous to tolerate anyone living with cold, blistery feet.The only reason copyright exists is to incentivize the creation of new shit. If we need to fund sock R&D, maybe we need a revenue stream for that. reply CaptainFever 20 hours agorootparent> People deserve free shit. If it costs ~$0.00 to copy something, the morally just thing to do is to give everyone who wants it a copy. If we could generate infinite socks for free, it would be outrageous to tolerate anyone living with cold, blistery feet.This is such a great way to put it. It&#x27;s as if we&#x27;re living in a world where a sci-fi replicator exists and abundance is possible, but we&#x27;re not allowed to use it, so nothing changes. Meanwhile, corporations just manufacture a product once, then copies it for free forevermore, selling each one for a price with an infinite profit margin. reply tzs 17 hours agorootparentprev> The only reason copyright exists is to incentivize the creation of new shit. If we need to fund sock R&D, maybe we need a revenue stream for thatOK. So how do we do that?There are two issues that need to be solved when it comes to funding the development of things that have zero marginal cost of production.1. How much total funding should we provide for a given category of such goods, such as music or movies or books or computer programs?2. How do we decide what particular creators or particular works to fund?Historically the answer has often been \"government does both\". For example the rulers of renaissance city states would choose artists to fund. These states would compete to have the best artists. The church was also often a major source of funding for music and painting and sculptures (I say \"the\" church because there was usually a state mandated religion).In the US \"government does both\" would probably not be politically tenable. It probably needs to be something that at least approximates a free market.The first thing that comes to mind is to actually have a free market in these goods. The issue there is that free markets work best when certain requirements are met. The mathematical economists can prove that if goods have some certain properties (\"rivalrous\" and \"excludable\" goods) then a free market in those goods leads to optimal production and consumption of those goods.Some examples of such goods are food, bicycles, washing machines, and basically most physical goods that an individual can own and use.That does assume a somewhat ideal free market with plenty of competition, low barriers to entry, and things like that, so real free markets usually aren&#x27;t actually optimum, but they can decently approximate it.When you have goods that are non-rivalrous and non-excludable free markets, even ideal free markets, are not optimum. The market under produces the good, in the sense that consumers can afford more of and want more of the good than the market produces.Digital music, movies, books, and software are generally in that category.There are a few approaches that have been suggested to handle this under production problem.1. Don&#x27;t handle it. Individual artists can make their money elsewhere and do art as a hobby, or make their money off of donations, or find a wealthy patron, or something like that. If it means we no longer have new big art, like movies that involve several hundred people working for several years to make, so be it.2. Government funds it and chooses which art to fund. Maybe based on polls or something, or maybe there is a Ministry of the Arts that picks. As mentioned earlier, this is probably too \"not free market\" for the US.3. Government funds it, but funding is distributed via some objective method that doesn&#x27;t involve the government deciding what art to fund. For example they could get statistics from major download sites and divvy up the money based on popularity.Richard Stallman has suggested such an approach for music copying, with the money distributed in proportion to the cube root of popularity.As far as how much to fund, that could also be somewhat tied to how much people download, probably by a tax on something that correlates somewhat well with downloading. Since the vast majority of copying will take place using the internet, and the vast majority of people consume art, the most common suggestion I&#x27;ve seen is for the tax to be on internet service.4. Legally treat these goods as if they were rivalrous and excludable. This is the approach taken under current copyright law. Then the free market does handle the production problem. The market determines how much money goes to new art and to which art that money goes. But it does mean the consumer pays more than the ideal free market price for that art which would be the marginal cost of production (which is essentially zero for digital goods). reply jpc0 20 hours agorootparentprevPlease let me know your address, I will send you storage medium and you copy all contents of your hard drive please. reply CaptainFever 20 hours agorootparentThat&#x27;s a privacy violation. Different things. reply jpc0 17 hours agorootparentThe property you own or rent is not private information.I never said the address needs to be your home address, you could send a PO Box register to a shell company and have a hobo pick it up and transport it across the country for you if anonymity is an issue but privacy certianly isn&#x27;t an issue here. reply CaptainFever 17 hours agorootparentI was referring to the hard drive part. Though yes, this would be more of an anonymity concern. reply jpc0 14 hours agorootparentAh that would be true, I didn&#x27;t have your private files in mind, more code, audio tracks, movies, series, artwork ( I want them monkeys ) you get the drift.Biggest one for this audience would be code (since in high likelihood if you have the other&#x27;s its likely pirated unless you are streaming your own ripped media ) if you would even be allowed to redistribute the code you have written, you probably would have some deep seated issue eith just handing it off to some stranger for free unless of course, you like me, are a big proponent of OSS.But even then I have several proprietary codebases that I own but sure wouldn&#x27;t share willingly. reply CaptainFever 12 hours agorootparentI think I would be fine with that tbh! I would assume most people here are big proponents of OSS. replyForHackernews 11 hours agorootparentprevI would happily seed torrents of all the media on my hard drive...except I&#x27;d get a copyright strike from my ISP. reply vcg3rd 19 hours agorootparentprevWhy not use an actual real example instead of a fantasy of infinite socks? And you bind your concept of \"morally just\" to cost.What if it cost $.05 to copy? Why is moral and just (related but not equivalent) bound to cost?As to a real example: a professor of medieval history has incurred the cost of his education, including reading handwriting in a foreign language, the cost of time, effort, travel and more in researching and writing and has now typed up a book on his computer and it is \"free\"[1] to distribute so it&#x27;s morally just to give a copy to all who want it?You pretty much cloaked your greed in a word salad of meaningless moralizing with a jargon dressing and no real context.[1] Even digital requires the cost of equipment, electricity, and access, so if you bind morally just to free it doesn&#x27;t exist. reply MacsHeadroom 19 hours agorootparentNobody has a moral right to be reimbursed for the costs they incurred to produce an original work. If they produce it under a contract then they have a contractual right to be paid as stipulated in the contract. reply vcg3rd 16 hours agorootparentWhere did I write one has a moral right to reimbursement? Your inferences are you own.Let me restate what I clearly and obviously wrote: The history professor has no moral obligation to give his book away digitally.The person I replied to stated unequivocally that people deserve free stuff if it cost nothing. I refuted both that argument and the argument that anything is distributed, let alone produced, without cost.Is that clear enough or do you want to build another strawman? reply jeremyjh 19 hours agorootparentprevWe aren&#x27;t talking about moral rights, we are talking about incentives to produce the work in the first place. reply Buttons840 20 hours agoparentprevWhat do you mean by 14 + 14? (Why not just say 28?) reply gwd 20 hours agorootparentDefault 14 years copyright, an extra 14 years if explicitly renewed. The explicit renewal was required when the Constitution was written; and renewal remained a requirement for some time, which is why (for example) many of Philip K. Dick&#x27;s stories from the 50&#x27;s are considered to be in the public domain (as I understand it).The current regime for individual creators is automatic life + 70 years, which is just way too long. For example, Lewis Carroll died in 1898; if the current regime had been in place, Disney would have had to get permission from Carroll&#x27;s estate to make Alice in Wonderland all the way through 1968. reply CaptainFever 20 hours agorootparentprev14 years by default, then an additional 14 if you renew it. reply xboxnolifes 18 hours agorootparentprevI believe the idea is that if the copyright holder doesn&#x27;t care enough to renew, it&#x27;ll enter the public domain faster. reply PicassoCTs 21 hours agoparentprevAnd lower resistance to that becoming reality, by allowing established copyright to happily smolder on.. its just new works. Worked perfectly well with driver licenses in the EU. Old folks kept all the licenses, the young pay through the noose for every variation. Zero resistance to that policy change. reply heliodor 21 hours agoparentprevYou talk about moral but I don&#x27;t see in your proposal what&#x27;s moral about not allowing two entities to trade things fair and square.To see what I mean, it rubs me the wrong way when I buy a used car and lose the warranty by doing so. reply WarOnPrivacy 19 hours agorootparent> it rubs me the wrong way when I buy a used car and lose the warranty by doing so.That is because the warranty grantor chose to write that into the warranty, is it not? Or do you see someone else being the cause? reply WarOnPrivacy 19 hours agorootparentprev> I don&#x27;t see in your proposal what&#x27;s moral about not allowing two entities to trade things fair and square.I&#x27;m not sure I follow your point. Copyright operates by preventing trade. If something is under copyright, ~every person possible is banned from freely trading that thing. reply mschuster91 20 hours agoparentprev> Restore copyright to 14+14 years (with a single exception, the original artist and only the original artist gets 28+14, if they sell their rights to the work then the recipient only gets 14+14). Destroy the DMCA.And then, dismantle the patent system just as well. Particularly \"patent trolls\" that just sit on patents and don&#x27;t even actively license them at all are a scourge on innovation, and companies like Qualcomm that do use their patents but refuse to license them in a way that can be described as \"reasonable\" to anyone but lawyers aren&#x27;t much better.Patents used to be a good thing but have devolved into just another way for the largest players in capitalism to bully competition, and (like copyright) it incentivizes predatory rent seeking behavior over what&#x27;s healthy for society at large. reply kibwen 20 hours agorootparentWe&#x27;d be gloriously ecstatic if the copyright system was only as broken as the patent system. At least parents expire after 20 years. reply CaptainFever 20 hours agorootparentIf this is true, it&#x27;s weird how on HN people tend to be more warm to the idea of patent abolishment even though it&#x27;s actually less broken than copyright. reply greyface- 19 hours agorootparentAlmost all of us derive income from copyright. Only some of us derive income from patents. reply mschuster91 19 hours agorootparentprevBecause copyright has enough fair-use bypasses that it actually doesn&#x27;t impede people that much, and where fair-use doesn&#x27;t apply (e.g. in music with remixes), there is a relatively working and reasonably fair and accessible system that&#x27;s been established for many decades. It&#x27;s not perfect as cases like the TV show \"Cold Case\" demonstrated that wasn&#x27;t ever distributed on VHS&#x2F;DVD due to licensing issues involving the soundtracks [1], but these are rare. And even if you stretch the boundaries of the law too much, unless you&#x27;re a large-scale commercial infringer, it&#x27;s a few hundreds to a few thousands of dollars in fines and that&#x27;s it.Patents however? There&#x27;s reports about lawsuits just about every month or two here on HN [2], more than enough cases of small shops getting sued left and right - and even corporate giants like Apple are virtually defenceless in patent courts. Software patents make the issue even worse.On top of that, patents are useless against China. The only thing Western companies can (and regularly) do is to prevent the sale of infringing stuff in Western markets, but no one in China and many other places of the world gives any fuck about Western patents.Another issue - particularly in pharmaceuticals - is: even if you patent a molecule, say an insulin derivative, you don&#x27;t have to patent and thus reveal how you produce the stuff. That means that even after the patent for a specific variety of insulin has expired, a generics competitor still can&#x27;t readily use that patent because the precise steps of manufacture are still proprietary. Meanwhile, the original manufacturer just makes a tiny variation of the insulin molecule, ceases production of the old one, and charges \"new medication\" prices from insurances because it&#x27;s technically not the old medication any more. It worked in the past for simple, chemically synthesizable molecules, but it doesn&#x27;t work for the more complex stuff that needs GMOs as a production&#x2F;harvest vector.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cold_Case#Home_media_and_strea...[2] https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&page=0&prefix=false&qu... reply rich_sasha 21 hours agoparentprevThese things are not unrelated. The author can only sell their rights for a sizeable chunk of money precisely because the big corpo can then milk the profits. You take that away, and suddenly creating valuable \"content\" doesn&#x27;t pay. reply kibwen 20 hours agorootparentPeople were creating art for thousands of years before copyright became perpetual. People were creating art for thousands of years before their art became feasibly monetizable. Hell, people were creating art for thousands of years before the concept of money even existed. I harbor precisely zero concern that shortening copyright to a \"mere\" 28 years will suddenly cause all human capacity for art to evaporate. reply melagonster 20 hours agorootparentbut this is first time authors and musicians can get their money reply jasonjayr 19 hours agorootparentAre independent artists and musicians getting any significant amount of money with the current policy? reply WarOnPrivacy 19 hours agorootparentprevI&#x27;m currently reading about how folks who produced & performed 14th century plays supported themselves. So much that govs took in a healthy amount of taxes. reply dmurray 20 hours agorootparentprevHistorically (and now, for the most part) books were copyright the author, but a large of the profits accrued to the publisher. That system worked fine. reply Alpha3031 19 hours agorootparentWould be nice if a higher proportion of the profits accrued to the author though. reply f1shy 20 hours agorootparentprevIt do pays. Means more work maybe. Doing recitals, or whatever… reply spit2wind 22 hours agoprevThe RMS essay on the \"Copyright Trade-off\" expands on what I think this post hints at.https:&#x2F;&#x2F;www.gnu.org&#x2F;philosophy&#x2F;misinterpreting-copyright.en....> When the US Constitution was drafted, the idea that authors were entitled to a copyright monopoly was proposed—and rejected. The founders of our country adopted a different premise, that copyright is not a natural right of authors, but an artificial concession made to them for the sake of progress.Specifically,> The copyright system works by providing privileges and thus benefits to publishers and authors; but it does not do this for their sake. Rather, it does this to modify their behavior: to provide an incentive for authors to write more and publish more. In effect, the government spends the public&#x27;s natural rights, on the public&#x27;s behalf, as part of a deal to bring the public more published works. Legal scholars call this concept the “copyright bargain.” It is like a government purchase of a highway or an airplane using taxpayers&#x27; money, except that the government spends our freedom instead of our money.But what \"natural rights\"?> the freedom to lend a book to your friend, to sell it to a used book store, to borrow it from a library, to buy it without giving your name to a corporate data bank, even the freedom to read it twice....as well as> fair useI find his perspective compelling. Yet, he only cites the Constitution and one court case. Copyright has a long and complex history. I wonder what the essay \"leaves out\" and what other \"categories of interpretation\" exist. reply psychoslave 17 hours agoparentCopyright is a common law perspective. In France for example, and I guess most of the Civil law area, authors do have morale rights, which are explicitly named thus, which are imprescriptible and inalienable. But that is in extra to patrimonial rights which cover most of what you get from a common law copyright. reply indrora 11 hours agorootparentJapan as well has the \"Moral right\" thing, but in a different perspective: give back to the original author off your profit with each sale when possible.It made sense in the days of Old Edo. it doesn&#x27;t make as much sense now. It complicated the used game market in Japan for ages. reply psychoslave 1 hour agorootparentThat&#x27;s yet a different matter.Morale rights in French law gives the author possibility to forbid the use of their work based on social reputation it might imply. For example a compositor can refuse that a song they wrote would be used publicly by a political party as an anthem because this party promote ideologies that the author doesn&#x27;t want to be associated with.It also allow author to forbid any further publication of what they published in the past, and ask for unsold copies to be destroyed.None of the things within morale rights is about making money, at least in intent. reply AnimalMuppet 19 hours agoparentprevA bunch of \"categories of interpretation\" are dead, because they lost court cases.And, the original intent is pretty clear, because the people back then left a record of why they made the law the way they did. reply andsoitis 23 hours agoprev> Copyright is not a moral right, it&#x27;s a monetization strategy that enables some information related business models at the expense of others.The reality is actually more interesting than this post.The US Copyright Act of 1790 was meant to provide an incentive to authors, artists, and scientists to create original works by providing creators with a monopoly. The monopoly was limited in order to stimulate creativity and the advancement of “science and the useful arts” through wide public access to works in the “public domain.” Major revisions happened in 1831, 1870, 1909, and 1976. Source: https:&#x2F;&#x2F;www.arl.org&#x2F;copyright-timeline&#x2F;As to moral right, from Wikipedia: \"Modern copyright law has been influenced by an array of older legal rights that have been recognized throughout history, including the moral rights of the author who created a work, the economic rights of a benefactor who paid to have a copy made, the property rights of the individual owner of a copy, and a sovereign&#x27;s right to censor and to regulate the printing industry. The origins of some of these rights can be traced back to ancient Greek culture, ancient Jewish law, and ancient Roman law.[2] In Greek society, during the sixth century B.C.E., there emerged the notion of the individual self, including personal ideals, ambition, and creativity.[3] The individual self is important in copyright because it distinguishes the creativity produced by an individual from the rest of society.[citation needed] In ancient Jewish Talmudic law there can be found recognition of the moral rights of the author and the economic or property rights of an author.[4]\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;History_of_copyright#Early_dev... reply pbhjpbhj 21 hours agoparent>Major revisions happened in 1831, 1870, 1909, and 1976.The 1980s (&#x27;88?) accession to the Berne Convention presumably was also a &#x27;major revision&#x27; as the convention, then over 100 year old, removes the registration requirement.Copyright is, somewhat ironically, essential for FOSS licenses that place conditions on a distributor. Governments could no doubt use some other instrument for this purpose. reply GalahiSimtam 20 hours agoparentprevIn copyright law, moral rights are the right to attribution, integrity, that sort of stuff. It&#x27;s a loan translation from French &#x27;droit moral&#x27;Unsure if the original baiter meant this reply hef19898 22 hours agoprevAs a hobby content creator, I absolutely disagree. My content is mine, if someone else wants to use it for whatever purposes, that party has to ask for, and get permission, to do so. Which I might or might not give.I absolutely do not want any AI model to use my stuff as training data, to be used on a website or for advertizing. You want to make money off my stuff, you pay. reply izacus 22 hours agoparentWell, yeah, if course you disagree. I also think every one of millions of users of my software should pay me 100$ a month until they die and then their children should take over. Carpenters would also agree that paying them rent for 200 years for each table they build would be an amazing idea.But maybe the opinions of rent seekers don&#x27;t always result in the best for economy and society. reply CaptainFever 20 hours agorootparentRelated text from https:&#x2F;&#x2F;www.techdirt.com&#x2F;2011&#x2F;04&#x2F;08&#x2F;if-youre-arguing-that-so...:> Julian Sanchez, who has been doing excellent work on copyright issues of late, has a nice post about how such arguments are totally irrelevant to copyright policy. He notes that it’s no surprise that artistic and creative people want greater copyright privileges, and fewer exceptions such as fair use, but that’s meaningless:> > These will often be wonderful, likeable, creative people, and the correct policy response to these objections as such is: Cry me a fucking river; now piss off.> And the reasoning why is that copyright policy is not about what someone deserves or about rewarding people based on some moral grounding. Its purpose is and has always been clear: to provide incentive to promote progress. reply ethanbond 20 hours agorootparentprevRent-seeking doesn’t mean “produces something and charges recurring fee for its use.”It actually refers to what you’re implicitly defending here: capturing the value of people’s production. Training AI on other people’s labor without paying them for it and then extracting value from it is rent-seeking.Consider what happens to AI companies’ value if they had to enter into mutual agreements with their data sources. AI company value will be almost solely determined by the AI company’s own value contribution, which is close to zero, while the data sources’ value would be unaffected or go up.If your business model relies on you leveraging the work of others without mutually agreeable remuneration, chances are good you’re the rent-seeker. reply sneed420 21 hours agorootparentprevYou literally can sell your software with a 100$&#x2F;month license and many companies do that. Likewise artists should have the permission to refuse their work from being scanned by for-profit companies without being compensated.A more apt comparison would be that a cracked version of your software is being used by OpenAI for profit without paying you for a license at all. Would you not object to that? reply jpc0 19 hours agorootparentprevYou could do the alternative.Pay the musician $400-$500 for each song and then you don&#x27;t need to pay a recurring fee for it in perpetuity.Also I&#x27;m pretty sure if your carpenter has been studing to produce the works they produce and each work takes several months to a yeah then my estimated price above is off be at least 1 but probably more factors or 10 reply izacus 19 hours agorootparentI&#x27;m happy to do that, let&#x27;s start with putting copyright back to 14 years and then we can pay more :) reply jpc0 17 hours agorootparentThese things are not mutually exclusive under current copyright law.If it wasn&#x27;t expensive time wise a musician can happily give you a no-redistribution license verbally, you may want it in writing to prove they gave it to you but that is about how loose these things are.If you ask me \"hey can I use this in my video for free\" and you respond \"yeah sure no problem\" that&#x27;s all it takes, if its in a text even better.Granted most musicians don&#x27;t currently have redistribution rights to their own music but that is between them and their lack of hiring a good contract lawyer, and this would fall under contract law not copyright.Understand you are paying a lawyer to make intent clear in the first place since how do you define \"use\", we I might define it differently and the court may as well unless it is made explicit what \"use\" means.There is a reason that even the licensing agreement you didn&#x27;t read for most software is so verbose and licensing for any creative work is about as obtuse.Intent is important though, if I intendef for you to use it to listen to at home but you instead played it tp a crowd of 20000 people and earned a fat paycheck there is a big difference there morally and under current law legally.Most musicians want their music listened to, they just want a little money back for that... Same with other art forms. reply Crosseye_Jack 22 hours agorootparentprevCopyright is different from Patents, but both are forms of IP protection.If Entity A has copyright on a application nothing is stopping Entity B from writing a competing application, they just have to invest their time and effort into the application like Entity A did. If Entity B thinks they can eat Entities A lunch with a different pricing model &#x2F; cheaper price they are free to do so.If Entity A has patent protection on a application then no one else can make the \"same application\"* (without licensing the patent) and Entity A has a monopoly on that market for the life of the patent (upto 20 years iirc).* This would all be down to how the patent is written, and personally I am against software patents, but patents can often be bypassed even if that means that the bypassed version could be incompatible with the patented version. I also think that the length of copyright protection is too long, but I don&#x27;t think it should be abolished either. reply oneplane 22 hours agorootparentprevBy that logic, nobody should own anything, or owners should not be allowed to set the rules about the things they own and on what terms they can share or transfer that ownership? reply ubercow13 22 hours agorootparentNo, that&#x27;s just the opposite extreme. reply ozim 20 hours agorootparentprevFunny enough we are not that far away from that - maybe not for tables but for electronics and cars.Well maybe calling it funny is not the right take. reply jprete 22 hours agorootparentprevIt&#x27;s not rent-seeking to want to charge for one&#x27;s own work through copyright. reply izacus 22 hours agorootparentIt absolutely is if they want to do that for 90 years instead of some more reasonable time. Noone is disputing the right to charge money for creative works. reply jprete 18 hours agorootparentThe relevant quote from the great-grandpost: \"As a hobby content creator, I absolutely disagree. My content is mine, if someone else wants to use it for whatever purposes, that party has to ask for, and get permission, to do so. Which I might or might not give.\"Your response isn&#x27;t addressing what my response was addressing. reply ikekkdcjkfke 12 hours agorootparentIf there is no copyright there is an incentive to monetize it as much as possible and provide as much service back to the public as possible. If there exists copyright then it should be taxed as property tax. reply hef19898 12 hours agorootparentLet me tell you a secret: revenue from royalities is taxed as income. reply ben_w 22 hours agorootparentprev${Build a house, Write a book} and get money for it for $years afterwards — smells like rent to me.This doesn&#x27;t mean it&#x27;s necessarily wrong, especially given the economic realities of the world we find ourselves in, but it does seem more than a bit rent-y. reply HDThoreaun 20 hours agorootparentprevCreating something is literally the opposite of rent seeking. reply izacus 19 hours agorootparentEverybody creates. Just copyright conglomerates and writers think they deserve rent until death for a thing they created vs. a limited payment. reply hef19898 19 hours agorootparentI know some small artists who think the same. And rightly so, because they earn their living that way. And obviously, those artists expect royalties as long as they have the rights, and as long as those rights are used by third parties. reply 4bpp 19 hours agorootparentprevA landlord developer who builds and furnishes an apartment complex with the intent of collecting passive income on it for a long time also creates something, but that is close to a central example of rent seeking. reply ChadNauseam 18 hours agorootparentJust because rent is involved doesn&#x27;t make it what is normally referred to as \"rent seeking\", and I don&#x27;t think your example would qualify by the traditional definition. From Google:> Rent seeking is an economic concept that occurs when an entity seeks to gain wealth without any reciprocal contribution of productivity. An example of rent seeking is when a company lobbies the government for grants, subsidies, or tariff protection. reply ethanbond 16 hours agorootparentprevNo it quite literally is not. reply HDThoreaun 16 hours agorootparentprevYou don’t understand what rent seeking is. reply ZoomZoomZoom 20 hours agorootparentprevPutting economy and society first is a way to put up a dystopia.This logic always comes to conclusion with things like disposing of disabled and elderly.In other words, not a great argument. reply izacus 19 hours agorootparentWhat a strange statement, what else should laws put first if not society and economy (economic prosperity)?!?! reply ZoomZoomZoom 19 hours agorootparent\"Putting society and economy first\" is meaningless in itself until it&#x27;s clear how it&#x27;s supposed to be done. Liberal democracies, for example, are supposed to put society first via personal rights and autonomy, which is the opposite of the tone of the GP&#x27;s message. reply ItsMonkk 18 hours agorootparentIt&#x27;s a relatively simple argument first made by Economist Paul Samuelson a century ago.> A businessman could not build it for a profit, since he cannot claim a price from each user. This certainly is the kind of activity that governments would naturally undertake. Even if the operators were able—say, by radar reconnaissance—to claim a toll from every nearby user, that fact would not necessarily make it socially optimal for this service to be provided like a private good at a market-determined individual price. Why not? Because it costs society zero extra cost to let one extra ship use the service; hence any ships discouraged from those waters by the requirement to pay a positive price will represent a social economic loss—even if the price charged to all is no more than enough to pay the long-run expenses of the lighthouse.> https:&#x2F;&#x2F;truthonthemarket.com&#x2F;2021&#x2F;06&#x2F;22&#x2F;the-virtues-and-pitf...If one is to pay any amount of money more than the marginal cost, and decides not to that is an example of dead-weight loss.I think the major issue here is that those that wish to abolish copyright and patents aren&#x27;t then actively promoting another idea, as you say. Let&#x27;s look at spaces where it is currently illegal to copyright or patent something, say, Maths formulas. Do we have no one working on new maths? No. We have plenty of Mathematicians. They either work for companies creating trade secrets, or they work for Universities releasing their work in journals for all to use.So let&#x27;s re-imagine a system where copyrights and patents are abolished. As Samuelson argues, as this is non-excludable and has zero marginal costs, it is a perfect fit for the government to take charge. Given that the current system produces dead-weight loss bringing down the production of the entire country, if we remove this dead-weight loss than the country will have much higher productivity. That higher productivity will drive higher taxes, and those higher taxes will allow the government to expand tenure track positions in public universities. Those tenure track positions can then go on and create new works and patents. The demand that those works create will then bring students to study and allow the works to proliferate.Would you prefer a system where copyrights and patents were abolished but there were 100x more tenure track positions? Where 10% of the population held such a role? reply lunarimiso 22 hours agorootparentprevStraw man much?If you want to make your software pay-to-use on a monthly basis, that is your perogative. But your 1 million users will quickly become 0 users.No one would buy tables from those carpenters...Perhaps you should understand basic market before making asinine statements such as, \"But maybe the opinions of rent seekers don&#x27;t always result in the best for economy and society.\"If you create something and I take that and I end up earning money off your work, without compensating you.. Then that is in fact morally wrong. It really isn&#x27;t harder than that. reply vimsee 21 hours agorootparent> Straw man much?An interesting way to start a comment.> No one would buy tables from those carpenters...There are companies owning large portions of markets where they can do exactly what you describe. Through market manipulation, political manipulation and probably other tactics I don`t know. This is a moral issue.> If you create something and I take that and I end up earning money off your work..While I agree with that statement, this is not the moral issue we are discussing. We are discussing the morals of rent seeking behavior, which you shrug of as \"your 1 million users will quickly become 0 users\" as if that will always be the case. reply hef19898 20 hours agorootparentprevIf your software has million users, cudos.Regarding monetization:- either you developed said software as an employee, than you were properly paid for it and it is not content- your user bought liscenses from you for whatever conditions you agreed upon- you sell it as a SaaS, and you get monthly subscription fees- you give it away for freeObviously, all those payments end when the conrtact with your customers end. If your next of kin runs the software 90 years from now, and customers still use it, of course cuatomers will pay, or not, as laid out above. reply pbhjpbhj 21 hours agoparentprevThat&#x27;s fine, keep your content. Once you distribute it and ask the state to control people&#x27;s ongoing sharing of that content though, then we (&#x27;we the people&#x27;) are not going to be so keen as you to lock up your content as we gain little from such laws.I support your right not to share your creations. reply nirvdrum 20 hours agorootparentI gain a lot from such laws. It helps stimulate the arts. Without the ability to earn money from creations, that will leave the arts to a patron model or as the purview of the already wealthy.Given the collective disdain for anyone pursuing a liberal arts education, I don’t see the patron model resurrecting en masse. I don’t feel entitled to the work output of anyone for anything. I’d much rather give creators a chance to become self-sufficient than live with the alternative. There I see only bland corporate cash grabs and a further shift to subscription-based access to any content.To be sure, we already have some of that. But, we also have artistic output from others that wouldn’t be able to subsist without the promise of protection afforded by copyright. reply hef19898 20 hours agorootparentprevI don&#x27;t share much, true. And I din&#x27;t ask the state to intervene. I expect the state to provide the legal framework for me to intervene if I choose so. And guess what, the states and nations did: copyright law, patents, trademarks... reply keiferski 22 hours agoparentprevYes, I find the anti-copyright position to be quite anti-creator and pro corporation that can afford to outspend competitors on marketing. In a world with limited copyright laws, I see no reason why Disney&#x2F;OpenAI&#x2F;Google&#x2F;etc. wouldn&#x27;t just end up functionally owning anything that a less well-funded person creates. reply Adverblessly 22 hours agorootparentWith no copyright law, how would they own anything?If anything, I&#x27;d expect the funding will go to those who can sustain themselves on things like Patreon and Kickstarter, where money is provided before the work is created, making the copyright protection irrelevant. And while the big corporations will get to own those Patreons and Kickstarters, hopefully there will be enough competition to keep their greed in check.(Also, maybe more people will learn to tip for content even when they receive it for free) reply keiferski 22 hours agorootparentThat&#x27;s why I wrote \"functionally own.\" They&#x27;d own the distribution, eyeballs, and sales of merchandise. Without copyright laws, large media corporations have the budget and institutional know-how to simply copy and out-market any new creations that are made by less well-funded people. Good luck making a new kids cartoon if Disney is going to copy it and put billions of marketing dollars promoting it.It would be a death sentence to any smaller creator and services like Patreon would probably not even exist. reply Adverblessly 22 hours agorootparentCopyright law does not stop large media corporations from copying because the original work itself is not what they need. Which would be more popular: Fortnite or \"PUBG - Epic&#x27;s Edition\"?Do you really envision a small creator publishing a new kids cartoon and then Disney swooping in and... Creating a 2nd season for it and making money off of it? reply keiferski 22 hours agorootparentIf there are no copyright laws, what&#x27;s stopping Disney from just outright stealing and outproducing any ideas they find? If I&#x27;m a small time creator and make a new kids cartoon series with an original concept, what&#x27;s stopping Disney from just outright publishing their own version, marketing it 100x more than I am capable of doing, and then selling merchandise for it? If there is no copyright, they don&#x27;t even need to change the name. reply sokoloff 21 hours agorootparent> If there is no copyright, they don&#x27;t even need to change the name.The absence of copyright does not necessarily mean the absence of trademark.If I want to make O-shaped oat cereal, I can. That doesn’t (and shouldn’t) give me the right to put it in a yellow box and call it Cheerios. reply Adverblessly 19 hours agorootparentprevLet&#x27;s say that I, John Doe, created and published an original kids cartoon about Goatman, a lovable half-goat half-man who has wonderful woodland adventures full of charm and wit. It is truly amazing and garners love and success, as shown by the Kickstarter funding for my 2nd season of Goatman blowing past the goal and getting 10x the funding I needed. This success attracts Disney&#x27;s attention.In our no copyright world, you say that they will publish their own \"Disney&#x27;s Goatman\" alternate 2nd season and market it heavily, and then sell merchandise to make back the money (because they won&#x27;t profit enough off of tips or Patreon or Kickstarter or just charging for it because people can just \"pirate\" it). My first question would be, why would they even bother with publishing a show? Why not just release the Goatman merchandise and profit directly without spending the cost of making their own show. They could even just advertise John Doe&#x27;s Goatman, it would be cheaper. But even if they do publish a \"Disney&#x27;s Goatman\", why would anyone who liked \"John Doe&#x27;s Goatman\" watch it, given that what they liked is John Doe&#x27;s writing and creativity? And if \"Disney&#x27;s Goatman\" was actually really good and popular, who was actually harmed by that? Do you think people will like \"John Doe&#x27;s Goatman\" any less? There would be less money going to the 3rd season Kickstarter for \"John Doe&#x27;s Goatman\"? Furthermore, in our current world, what really is stopping Disney from releasing their own original kids cartoon about Mangoat, a lovable half-man half-goat who has charming woodland adventures full of wit and wonder? Sorry, but it just doesn&#x27;t really make sense to me when I think about the details. reply keiferski 18 hours agorootparentI don&#x27;t think it&#x27;ll quite play out the way you envision. I think it would go like this, instead:Disney doesn&#x27;t make a second season, they make an entirely new show that uses the brand and appeal of the character. They frame it as being \"the foundation\" or \"the original.\"My first question would be, why would they even bother with publishing a show? Why not just release the Goatman merchandise and profit directly without spending the cost of making their own show.Because the amount of resources available to Disney is orders of magnitude larger than random John Doe, and thus the potential revenue and audience is larger.Why not just release the Goatman merchandise and profit directly without spending the cost of making their own show.Again, because Disney marketing a show is orders of magnitude more effective than random guy&#x27;s Kickstarter.But even if they do publish a \"Disney&#x27;s Goatman\", why would anyone who liked \"John Doe&#x27;s Goatman\" watch it, given that what they liked is John Doe&#x27;s writing and creativity?When it comes to big brands, the average person doesn&#x27;t care about the author. People go watch Disney or Pixar movies because the brand is well-known and the product is widely marketed. No one cares who the creators are.And if \"Disney&#x27;s Goatman\" was actually really good and popular, who was actually harmed by that? Do you think people will like \"John Doe&#x27;s Goatman\" any less? There would be less money going to the 3rd season Kickstarter for \"John Doe&#x27;s Goatman\"?You, John Doe, are, as there is now considerable confusion in the marketplace and you will be less able to capitalize on your creation and expand it into a media franchise.Furthermore, in our current world, what really is stopping Disney from releasing their own original kids cartoon about Mangoat, a lovable half-man half-goat who has charming woodland adventures full of wit and wonder?There are still issues with blatantly copying works in a way that is confusing to the consumer. It seems pretty likely that a multinational corporation with marketing budget measured in billions will out compete a small time creator.Let&#x27;s use a real-world example to illustrate the point better. Imagine that we have no copyright laws. J.K. Rowling writes the first Harry Potter book and it sells well. She only received 2500 pounds as an advance for the work. The publisher (or Disney, or whatever other company) realizes they have a winning franchise on their hands, commissions more books, movies, theme park rides, on and on. It eventually earns billions of dollars.That all happened in real life, except we do have some copyright laws and Rowling earns a percentage of whatever profits come from the use of her creation. She is now a billionaire. In a world without those laws, she gets what, a few hundred thousand dollars in sales from the first book, maybe? The rest goes to some corporation. reply Adverblessly 16 hours agorootparent> Disney doesn&#x27;t make a second season, they make an entirely new show that uses the brand and appeal of the character. They frame it as being \"the foundation\" or \"the original.\"Framing it as the original is false advertisement and illegal. Just because you get to use another&#x27;s creative work doesn&#x27;t mean you get to lie about who made what when.Is the brand and appeal of the characters the only value of the original work? If you made a bad show but with some appealing characters, is it even a problem if they are used to produce a much better work? I mean, surely for you who made the bad work, but in this case this doesn&#x27;t seem like that much of an injustice.> Because the amount of resources available to Disney is orders of magnitude larger than random John Doe, and thus the potential revenue and audience is larger.I don&#x27;t really feel like that answers my question, obviously they can afford it. If their new show really is better then it makes sense to boost the brand, but if the premise is that the independent creator came up with something new and good that the rich capitalist cannot replicate with their lack of creativity and thus can only exploit, then it seems like a waste of money to even bother.> When it comes to big brands, the average person doesn&#x27;t care about the author. People go watch Disney or Pixar movies because the brand is well-known and the product is widely marketed. No one cares who the creators are.Okay, but then why even bother with copying? If people don&#x27;t care about the branding of the original product, what is gained by copying that brand? If I never heard of Goatman then I wouldn&#x27;t care for Disney&#x27;s version of Goatman (except if what I care about is Disney&#x27;s brand and then Goatman is immaterial). Or if literally all that is copied is that idea of having a half-goat half-man have wonderful woodland adventures full of charm and wit, then I&#x27;m not sure if anything of value has really been copied.> You, John Doe, are, as there is now considerable confusion in the marketplace and you will be less able to capitalize on your creation and expand it into a media franchise.Or you, John Doe, are publicly recognized as the original creator of Goatman (as to claim otherwise would be either false advertising or outright fraud), drawing attention to the original vision, and if the original vision is actually better than the copies, to profit from it.As John Doe though, I don&#x27;t know about all that \"media franchise\" part. Certainly my skills are limited to Writing, Animation and Character Design, I&#x27;ve never actually designed a toy line and so have nothing to offer there.> There are still issues with blatantly copying works in a way that is confusing to the consumer.This I fully agree with, transparency, clarity, correct and clear attribution are all incredibly important and should be protected by law. It is also no accident that many people know who Toby Fox or Notch are, but no one can name say the lead designer of any Ubisoft game that outsold Undertale by 100x.> Let&#x27;s use a real-world example to illustrate the point better. Imagine that we have no copyright laws. J.K. Rowling writes the first Harry Potter book and it sells well. She only received 2500 pounds as an advance for the work. The publisher (or Disney, or whatever other company) realizes they have a winning franchise on their hands, commissions more books, movies, theme park rides, on and on. It eventually earns billions of dollars.> That all happened in real life, except we do have some copyright laws and Rowling earns a percentage of whatever profits come from the use of her creation. She is now a billionaire. In a world without those laws, she gets what, a few hundred thousands dollars in sales from the first book, maybe? The rest goes to some corporation.Thank you for a concrete and detailed example, I feel it really helps me understand your reasoning. (Sorry if that comes across sarcastic via the medium of HN comments, I mean it sincerely)Let&#x27;s say that the first book was published and was a great success. JKR&#x27;s publisher sees the opportunity and contracts out the work to author the 2nd Harry Potter book, hoping to capitalize on the first&#x27;s success. At the same time JKR either crowdfunds or contacts another publisher to work on the sequel. The audience who loved the original book then gets to make a choice between reading JKRs sequel which has a large sticker on it \"From the same author as Harry Potter!\" or a different sequel with a smaller sticker at the legally mandated font required for liability that says \"Consumer Protection Alert: This book was not written by the author of the original prequel\". Which do they choose? And if they chose and preferred the non-JKR sequel, then what exactly is the problem? And why isn&#x27;t the contracted author publishing their own books, given they can out-write JKR?As for making billions of dollars off of movies and theme parks - sure. JKR would not have made billions. But then, neither would anyone else. Theme park builders would all have to compete for building the actually best Harry Potter theme park instead of licensing a monopoly and so the only theme park builder making real money would be the one that creates the best theme park. Possibly, one of them would even pay JKR to advise in the creation of the park, so they could advertise it as \"From the original author of Harry Potter!\", and if JKRs branding as the author of Harry Potter was worthwhile, that would still be a lot of money. Certainly this is better for the public. And as certainly, this wouldn&#x27;t have prevented the creation of Harry Potter in the first place, because JKR is an author and not a theme park builder, and even if motivated by money rather than creativity, could not possibly imagine a Harry Potter theme park as motivation for her work.And to take it one step further, once JKR has made billions from her work, is there really an appreciable difference between the gigantic Disney corporation and the gigantic JKR estate? Is there any appreciable difference between JKR paying someone to produce a Harry Potter work (whether a book or a Theme Park) that has no creative input from her (and thus hardly her own work) vs. Disney currently underpaying someone for their work and giving them no (or negligible) credit.Beyond that, Eliezer Yudkowsky could start making money off of the hard work and creativity he put into Harry Potter and the Methods of Rationality, and countless other similar smaller authors (that you never heard of, because their brilliant Harry Potter fanfiction is non-monetisable) could also start benefiting from their creativity and hard work.Thinking about it further and trying to synthesize a more coherent view from both of our arguments, it seems that we don&#x27;t disagree that people will tend to spend on brands, but it is your opinion that the \"Harry Potter\" brand is a much stronger brand",
    "originSummary": [
      "Copyright is often viewed as a means to generate profit rather than as a moral entitlement, which can have implications for different business models.",
      "Certain business models may benefit from copyright protection, while others may face obstacles or limitations.",
      "The discussion around copyright highlights the tension between economic interests and ethical considerations."
    ],
    "commentSummary": [
      "The article discusses the impact of copyright laws on creators, corporations, and society.",
      "It explores the criticism of current copyright laws that favor corporations and limit public access to creative works.",
      "The conversation covers topics such as the benefits and drawbacks of abolishing copyright, the role of platforms like Spotify in the music industry, and the abuse of copyright laws by big corporations."
    ],
    "points": 209,
    "commentCount": 339,
    "retryCount": 0,
    "time": 1703846261
  },
  {
    "id": 38805439,
    "title": "The Linux Backdoor Attempt of 2003: A Deep Dive into Programming, Cybersecurity, and Open Source",
    "originLink": "https://freedom-to-tinker.com/2013/10/09/the-linux-backdoor-attempt-of-2003/",
    "originBody": "December 30, 2023 Posts Comments Freedom to Tinker Research and commentary on digital technologies in public life The Linux Backdoor Attempt of 2003 OCTOBER 9, 2013 BY ED FELTEN Josh wrote recently about a serious security bug that appeared in Debian Linux back in 2006, and whether it was really a backdoor inserted by the NSA. (He concluded that it probably was not.) Today I want to write about another incident, in 2003, in which someone tried to backdoor the Linux kernel. This one was definitely an attempt to insert a backdoor. But we don’t know who it was that made the attempt—and we probably never will. Back in 2003 Linux used a system called BitKeeper to store the master copy of the Linux source code. If a developer wanted to propose a modification to the Linux code, they would submit their proposed change, and it would go through an organized approval process to decide whether the change would be accepted into the master code. Every change to the master code would come with a short explanation, which always included a pointer to the record of its approval. But some people didn’t like BitKeeper, so a second copy of the source code was kept so that developers could get the code via another code system called CVS. The CVS copy of the code was a direct clone of the primary BitKeeper copy. But on Nov. 5, 2003, Larry McVoy noticed that there was a code change in the CVS copy that did not have a pointer to a record of approval. Investigation showed that the change had never been approved and, stranger yet, that this change did not appear in the primary BitKeeper repository at all. Further investigation determined that someone had apparently broken in (electronically) to the CVS server and inserted this change. What did the change do? This is where it gets really interesting. The change modified the code of a Linux function called wait4, which a program could use to wait for something to happen. Specifically, it added these two lines of code: if ((options == (__WCLONE|__WALL)) && (current->uid = 0)) retval = -EINVAL; [Exercise for readers who know the C programming language: What is unusual about this code? Answer appears below.] A casual reading by an expert would interpret this as innocuous error-checking code to make wait4 return an error code when wait4 was called in a certain way that was forbidden by the documentation. But a really careful expert reader would notice that, near the end of the first line, it said “= 0” rather than “== 0”. The normal thing to write in code like this is “== 0”, which tests whether the user ID of the currently running code (current->uid) is equal to zero, without modifying the user ID. But what actually appears is “= 0”, which has the effect of setting the user ID to zero. Setting the user ID to zero is a problem because user ID number zero is the “root” user, which is allowed to do absolutely anything it wants—to access all data, change the behavior of all code, and to compromise entirely the security of all parts of the system. So the effect of this code is to give root privileges to any piece of software that called wait4 in a particular way that is supposed to be invalid. In other words … it’s a classic backdoor. This is a very clever piece of work. It looks like innocuous error checking, but it’s really a back door. And it was slipped into the code outside the normal approval process, to avoid any possibility that the approval process would notice what was up. But the attempt didn’t work, because the Linux team was careful enough to notice that that this code was in the CVS repository without having gone through the normal approval process. Score one for Linux. Could this have been an NSA attack? Maybe. But there were many others who had the skill and motivation to carry out this attack. Unless somebody confesses, or a smoking-gun document turns up, we’ll never know. [Post edited (2013-10-09) to correct the spelling of Larry McVoy’s name.] Filed Under: Privacy & Security Tagged With: Backdoor, History, Linux, Security Comments BlindWanderer says: October 27, 2013 at 10:50 pm It’s definitely a back door. Any unit test you would write to test it would fail. retval never gets set to -EINVAL, even if it did, the next line of code retval = -ECHILD; would change it. エルメス カードケース says: October 26, 2013 at 9:49 pm I every time used to read article in news papers but now as I am a user of net so from now I am using net for posts, thanks to web. Nathan T. (or some dude who wishes he was coding again) says: October 10, 2013 at 2:49 pm Wow to read the number of posts on this topic is quite entertaining. My own perspective: I never learned C; but so many languages over the years I should have been able to decipher the problem without the explanation. Although I haven’t been coding for about 5 or 6 years so I didn’t catch the actual problem that was so obvious. I did catch that the user ID being “0” would be “root” and knowing this was a “back door” was some type of elevated privileges. Not knowing the variables I didn’t quite see how it played out despite the fact that == is correctly used at the beginning and = is correctly used upon true condition of the if statement. Should have been obvious that the = in the if statement was an assignment despite not knowing C; based on the line below it. So many different languages, as soon as I began reading the explanation and was reminded of the assignment statement it was obvious that the assignment was simply for the purpose of gaining root privileges if certain conditions were met in malicious code. But I still wondered the result of the assignment below the if statement. However, I found the noob question and all the varied answers quite entertaining and only reminded me yet again why there are so many mistakes because each language has its own unique conventions. In some languages == means [is congruent as] or [is equal to] or [is the very same as]. Slight subtle differences that make major differences in how they are used depending on the specific language and variable type. Same problem with = sometimes it means [assigns right value to left value] or [is equal to]. In the case of the languages that assigns the value as at least one person presumed and many people corrected (and me not knowing C) I would have also presumed that setting UID to 0 would return a value of “true” for the if statement upon a successful completion of the assignment. I am sure there are at least three languages this is the case. However, many people corrected that in C the returned value is simply the assigned value which at 0 would interpret as false. I would have presumed that “retval = -EINVAL” would have been called and wonder exactly what that would do. But apparently it would never have been touched in C. No matter either; what concerns me more however, is lack of checks, not on the code but in the elevation of user privileges. And perhaps I feel this way because I have learned other languages besides C that grant me a THIRD perspective despite the Boolean operation of the if statement. While I have learned so many languages over the years I can’t remember which are which but I am sure some return “true” or “1” upon successful completion of assignment; “false” or “0” upon unsuccessful completion of assignment or “null” on an error; and a “null” in an if statement should kick a fatal exception error of some sort. Thus with three actual possibilities to what would normally be a Boolean operation. I would fully expect that the statement “current->uid = 0” to return either “null” or an error itself regardless of what that does to the “if” statement in question. Now granted this was introduced into the kernel itself which would obviously at times need to grant root access to running applications. But it doesn’t particularly inspire great confidence in the Linux kernel that any routine in the kernel can successfully assign the current user ID to “root.” Why would it automatically make that assignment without some other checks in place. Especially if the routine (wait4 in this case) is nothing more than an API call available to any application running over the kernel. The arguments of purposeful backdoor or coding error aside (and I have made that type of error many a times); I am more worried about the ease of assignment of root privileges. Shouldn’t the variable “uid” for any running application be very protected within the kernel? It shouldn’t be able to be changed at a whim even by just any call to kernel level routines. At least that is my opinion. Again from the perspective of Boolean operations do not need to be Boolean in fact; and the idea that some variable should not be accessible to most routines even within the kernel itself. But, then again I admit to one thing, I don’t code operating systems; so maybe it’s not possible to protect certain variables at that level? Tim says: October 17, 2013 at 12:53 pm I think it is more about the kernel being assumed to be safe. Adding verification would be a bit of effort, probably mainly changing the instances of assignment and comparison to method calls. The set method being available only to trusted functions. I don’t think it is necessary however. It’s like having a roommate and nit giving them keys. Don’t give your keys to visitors, but the roommate ought to be able to unlock things as needed. Franky says: October 10, 2013 at 5:44 am … and that’s why people invented static code analysis: To give you warnings for this kind of stuff (in this case: assignment while comparison is expected) Some Dude who works in closed source sector says: October 10, 2013 at 3:08 am other questions for consideration: – Why is “Cyber” considered the 5th domain of warfare? – not just a bunch of geeks, but a quite high command organization in the US military structure under StratCOM (Strategic command) – http://en.wikipedia.org/wiki/Unified_Combatant_Command – domains 1 – 4 of warfare: land, sea, air, space.. cyber is considered so powerful as to be considered able to fundamentally change the nature of warfare as these original 4 did as they evolved over history – Given its considered strategic importance, what methods would be employed to enable this 5th domain of warfare? – I work at . And I see published vulnerabilities and what they are in our software and other people’s software.. operating system and application level, in closed source systems and applications, and I ask myself… – – “how would that get there?” -“who benefits from it, if it was malicious?” – “Who has the level of resources to take long term malicious actions to implant and manufacture the vulnerabilities to then exploit? … – ” “What are the top software application in terms of installed base, any category? – “Top OS installed base, any category?” – “How does this correlate with discovered and published security vulnerability rate and severity?” *I* would focus on it like a special action group from the NBC WMD days: insert who is not what they appear to be into places that they have trusted access, to get information out and put things in place inside, for future use, in the places I want, to achieve my goals in an age of the 5th domain of warfare… don’t the same methods of politics, economics, intelligence and warfare still work also? How does one create trusted systems in an environment where NIST goes back on previous crypto recommendations, after it is revealed that NSA weakened aspects of the crypto? http://spectrum.ieee.org/telecom/security/can-you-trust-nist Doesn’t everything need to be 1) public 2) fully disclosed as far as algorithms, source code and analysis/results of [secure coding audits|vulnerability scans|penetration tests|developer identities (with privacy protections for the devs, full data held in escrow)|statistics of vulnerabilities] 3) tracked as to identity of developer? Some Dude who works in closed source sector says: October 10, 2013 at 2:36 am so, I think the time is right now to discuss what I think is needed in the long run for all software development, but particularly open source: – a system which validates and uniquely identifies all developers who submit code to participating projects (with some identity attributes perhaps held at a trusted clearing house, like a certificate authority, but not generally publicly available to the world at large) Why is this needed? Just accept it… in the age of a successful Aurora Test (see youtube, http://youtu.be/fJyWngDco3g , search engines for “Aurora test”) Consider instead evidence . – Stuxnet – Duqu – Flame etc – DHS, S&T directorate, Supply Chain Hygiene is a practice and program area. It encompasses lots of things such as biologicals and poisoned food stuffs, but also including areas like counterfeit and subverted components in computer systems and software – NIST – SP 800-53 rev4, consider all the added things in there around supply chain of software and hardware for infrastructure supporting US civilian government IT. There is a *lot* in this ~500 page document around trusted source of development of code and trustworthiness of developer of software. – Years of allegations and some noise and publicity of undisclosed evidence of ZTE and Huawei equipment being subverted for Chinese government purposes, from the factory – Bans for many years now on Lenovo hardware use by US, UK and Australian intelligence community So, consider the motivations at work to drive .govvies to write all this into requirements at length in NIST SP 800-53 rev4. Why? – NIST is staffed with smart people. Physicists. Engineers. Why go to the trouble to write these extensive requirements and make it expensive to deliver IT services within the govt as well as for external service providers to deliver services meeting these requirements to the US govt (as these are requirements of FEDRAMP, the compliance framework for “cloud” and other services to govt (I think any external service to got could and will be classified as “cloud computing” nowadays)? In the current environment with debt ceiling and similar political arguments, can you imagine the internal pressure to reduce costs? How can requirements which massively increase costs survive? A: there is broad evidence of these sorts of threats from other nations as well as wide-enough spread internal knowledge among approving management approving these requirements documents that these same methods are how the US Govt and their people (DoE-managed evil geniuses at work in the National Labs, among other places) are able to deliver results such as Stuxnet for the US Govt against our geopolitical friends, enemies and otherwise innocent bystanders. (in the age of NSA disclosures, is there such a thing as “friend” between nations? Was there ever? Aren’t ‘friends’ just potential future enemies, someone to monitor in case an election or coup goes sideways for you (see Iran, Egypt, Greece, France, Italy, Indonesia, etc) For OSS to remain viable and actually completely prove that the bazaar is better than the pyramid/cathedral model, a system for trusted development is required. needed features (incomplete list): – public validation of the source developer of code insertion into OSS projects – trusted and irrefutable ID of said developer, with a multi-way trust system for validating identity of the developer prior to admission and acceptance of code submissions – long term tracking of every change across the OSS world for – what the change – identity of the change submission tracked back to a validated developer identity – long term tracking and association of developer code submissions to later discovered security vulnerabilities and “bugs” – PUBLIC SCOREBOARD of security bug score of every developer (their career “batting average”) which allows statistical discovery by data analysis of who and where software development supply chain threats are originating, and statistical exposure of the subverters. You can’t hide from data analysis. This is the public speaking of a thought I have had for quite some time, since starting to look at emerging US government requirements… the NSA disclosures just reinforced my opinions. Larry McVoy says: October 9, 2013 at 6:07 pm “But the attempt didn’t work, because the Linux team was careful enough to notice that that this code was in the CVS repository without having gone through the normal approval process. Score one for Linux.” Um, the Linux devs didn’t notice this. It was found as part of the validation process when exporting the tree from BK to CVS. Source: I’m the guy who found the problem. Ben Alabaster says: October 9, 2013 at 10:19 pm @Larry McVoy – Great catch 😀 Ed Felten says: October 10, 2013 at 6:04 pm Yes, Larry, great catch indeed! I suppose I should have said that the Linux *community* (rather than “team”) caught it. I did call you out by name as the person who noticed the bogus patch. Karl Fogel says: October 9, 2013 at 5:01 pm Indeed very interesting — thanks for telling the story! Minor correction: “Larry McVoy” (not “McAvoy”). Omair says: October 9, 2013 at 2:15 pm Clearly, it was the ac1db1tch3z http://www.phrack.org/issues.html?issue=64&id=15&mode=txt 😛 BenAlabaster says: October 9, 2013 at 2:06 pm It’s funny how many times I’ve seen this particular backdoor discussed, and it’s funny that every single conversation has devolved into a debate about the merits of Yoda comparison… aren’t you all forgetting the point? If a developer doesn’t adhere to Yoda comparison, the only thing that’s gonna find that is StyleCop. Just because you use Yoda comparison, doesn’t mean the next developer does, and unless this is checked for by the code reviewer prior to check-in, it could just as easily slip under the wire. So just because you all (for example) want to use Yoda comparison, but I want to sneak something like this in under the radar, do you think I’m going to use Yoda comparison? Unlikely… The point of discussion isn’t if the technique should be used or not, it’s the fact that this technique got used and appears to have been used with malicious intent. Nathan T. (or some dude who wishes he was coding again) says: October 10, 2013 at 3:15 pm Ben I haven’t seen this particular backdoor discussed; so I am glad it devolved into the debate about the merits of “Yoda comparison.” I never even thought of that before, but I have always admired the wisdom of Yoda and the reversal of parts of speech tend to make one think more clearly about what is being said. To see that used as a analogy to reversing the order of variable comparison from conventional way of thinking in coding does the same; it makes one think more clearly about what is being typed. I see a great value in this. Having learned so many different languages with different syntax I have made similar mistakes so often it is not funny. Because some languages = is ONLY a comparison; and yet other languages = is an assignment; and still others it is both but depends on where it is used. So, I now put that into my head (certainly was not taught to think that way in any of my coding classes–though I haven’t attended a class since 2001) to use swap my comparisons for my own benefit to avoid errors. Regardless if or if not the use of such would stop any others from introducing back doors 🙂 I am glad that people brought it up in the discussion so I can become a better coder regardless of which language I happen to be coding in. Jeena says: October 9, 2013 at 12:47 pm Wouldn’t the compiler warn you about this? I only wrote a little bit of C but I remember the compiler warning me about this. Anon says: October 9, 2013 at 1:34 pm Today, most compilers would issue a warning. A long time ago, it was less common. anon says: October 9, 2013 at 2:02 pm Back before the good days of GCC? Anonymous says: October 10, 2013 at 4:34 am GCC has warned on assignments in conditionals for a long time, but even today GCC (and, I think, CLANG) does not issue a warning if you put the assignment inside parenthesis, which are not syntactically required for either assignment or comparison. bork says: October 9, 2013 at 11:34 am A devious person would set the __WCLONE or __WALL options to true before running the wait4 method. Unless on of these options are true, the rest of the expression is not executed. Also guessing that the options are not in use so that only a hacker that knew about the backdoor would create a program that could use it. not-noob says: October 9, 2013 at 11:25 am this is php, but yes, it does set it to 1 (you can try it yourself via cmd line): php -r ‘$foo = 0; if($foo = 1) {} echo $foo;’ 1 Pigeon says: October 9, 2013 at 10:08 am @noob: This is C, where pointers make scope more complicated than today’s world of garbage collection. Read up on pointers. Nathan Marley says: October 9, 2013 at 10:32 am The fact that a pointer is used to access the uid is irrelevant to the question. He should read up on “how conditions are evaluated in C”, or just assignments or really just C in general. That and practice writing a lot of C code. Andro says: October 9, 2013 at 10:06 am @noob: If works on a Boolean value. To get it the given expression is evaluated. Normally the expression is comparison between two values. But in this case it’s an assignment of a value to a variable somewhere and the result of the assignment Success or Not is fed to the If(). You can just as well put a whole function in there, and all the code in the function will get executed to produce a single value in the end – True or False. Hope this kinda helps. noob says: October 9, 2013 at 9:40 am Maybe i’m a noob (actually, I definitely am), but how could it possibly set the uid to root (0) if it’s nested in an if-statement? does the mere appearance of the assignment lead to a change in the var, even if it’s just in an if statement? Anonymous says: October 9, 2013 at 9:52 am Essentially yes. martin says: October 9, 2013 at 9:55 am Even if it is in an if-statement the code is executed. Consider something like this: if(foo->bar()) { /*do_sth*/ } You would expect that foo->bar() is executed and returns some value that is used for the if-statement. Same goes for the assignment. By the way, an assignment returns the value that is assigned. cwillu says: October 9, 2013 at 9:58 am Assignment is an expression, so that you can do things like x = y = 1 to set x and y to the same value. And any expression is valid in an if statement. gcc says: October 9, 2013 at 9:59 am unsigned int result = 0; if ((result = checkStatus()) >= 1) { signalError(); } return result; szevvy says: October 9, 2013 at 10:00 am See here for an explanation: http://stackoverflow.com/questions/151850/why-would-you-use-an-assignment-in-a-condition ege says: October 9, 2013 at 10:01 am A peculiarity of most c-like languages. It’s not the mere appearance, but the condition part in if statement is evaluated. First the (options == (__WCLONE|__WALL)) part is checked and if it only resolves to true then the part that comes after && is evaluated. In c, an assignment is also an expression (with the value of the right hand side of the =) perfectly ok within a condition part. In this example it will resolve to 0, which means the whole condition will fail so only the condition part is executed, not the statement part. WRXRated says: October 9, 2013 at 10:01 am If (var = 0) will always be true and will set var to 0. Jonny says: October 9, 2013 at 10:12 am No it will not always be true, especially not if var is of type const. George says: October 9, 2013 at 3:18 pm It won’t compile if var is const. That’s the whole idea behind putting constants (literal or otherwise) on the left. Locoluis says: October 9, 2013 at 10:19 am Actually, the expression “var = 0” returns the value assigned (this makes the syntax “var1 = var2 = 0” possible), which in this case is 0. This makes the expression always false. Mike says: October 9, 2013 at 10:25 am (var = 0) will always be false since it returns the value that was assigned and 0 is equivalent to false. Nathan Marley says: October 9, 2013 at 10:28 am It is not true, as it returns 0 (the value of the assignment) and 0 is interpreted as false. This condition fails, even though var gets set to 0 (except in cases like Jonny suggested). WRXRated says: October 10, 2013 at 12:54 pm I stand corrected… and I recall this bug totally screwing me over in the past. I just tested this now in VS 2008. int main() { int n = 99; if (n = 1) { printf(“true”); } else { printf(“false”); } if (n = 0) { printf(“true”); } else { printf(“false”); } return 0; } In the first block, n gets set to 1 and “true” is printed. In the second block n gets set to 0 and “false” is printed. Always fun to have coder discussions! 🙂 Josh says: October 9, 2013 at 10:08 am @noob, Code is executed inside an if(), I know that normally you think of if() as a check and not something that executes but it’s what allows you to do something like this: //JS var testFunc = function(){ return 1; }; if(testFunc() == 1) { … } instead of having to do something like this: //JS var testFunc = function(){ return 1; }; var result = testFunc(); if(result == 1) { … } Some time people will even do something like this //JS var testFunc = function(){ if(something) return 1; return false; }; if(a = testFunc()) { //use ‘a’ in here } which allows them to make code look a little cleaner (This is obviously all opinion) by containing the definition of ‘a’ into the check to see if you even need to use ‘a’. I guess it would also help in the case that you wanted to remove the above if() you can just delete the whole block and you don’t still have an ‘a’ floating around that isn’t used (Of course you could just delete the definition line at the same time you delete the if block). Anonymous says: October 9, 2013 at 10:09 am The conditions get evaluated when the if statement is executed no matter if the result is true or false. So it is set to zero whilst the if statement is performing it’s equality checks. Anonymous says: October 9, 2013 at 10:10 am An if statement is just a function, you can do anything in it, assignment operator also return a value if the assignment worked properly, so if you test i = 0 in an if statement, the assignment would return true if it worked. So yes, a value assignment would work in it. in this particular case, the if statement use the && operator, in c and c++ it would check for options == (__WCLONE|__WALL) first, then, if this is already true, their is no need to test for the second statement. So, all the attacker had to do to execute the second statement was to ensure that the first test was false, it’s briefly explained in the text that normally when calling wait4 the option variable would match __WCLONE or __WALL, but if you use a malformed call to wait4 where option isn’t fulfilling the first statement then the second will. Then uid will be set to 0 then the = operator will return true and the program calling wait4 will continue but with as root. msouth says: October 9, 2013 at 11:57 am There are a large number of errors in this description, perhaps they are deliberate and the parent is trolling? The && operator works the opposite of what you are saying, you are describing the || operator. You make this mistake twice, but at least you are consistent. If the left side of the && is false, the right side is not evaluated, because there is no reason to, the whole thing can’t be true. If you test i=0 in an if statement it will return false, since the value of an assignment is what was assigned. If the options part of the test is false, the second part of the && would not be evaluated. If you thought the && was ||, both of your statements would be correct, though. To get the second half of an || to evaluate, you have to have the first half false, because if the first half is true there’s no need to check the second half. What would really happen is if the options == (__WCLONE|__WALL) evaluated to true, then the second half would execute, assigning 0 to the uid, and making the && evaluate to false. You would never get to the retval = -EINVAL; statement, because the second half of the && is always going to evaluate to zero, which is false. Before you make any other comments on something like this, please run some test code in a debugger. I’m really thinking you must be a troll to be so consistently incorrect. Godhimself says: October 9, 2013 at 10:10 am An if statement is just a function, you can do anything in it, assignment operator also return a value if the assignment worked properly, so if you test i = 0 in an if statement, the assignment would return true if it worked. So yes, a value assignment would work in it. in this particular case, the if statement use the && operator, in c and c++ it would check for options == (__WCLONE|__WALL) first, then, if this is already true, their is no need to test for the second statement. So, all the attacker had to do to execute the second statement was to ensure that the first test was false, it’s briefly explained in the text that normally when calling wait4 the option variable would match __WCLONE or __WALL, but if you use a malformed call to wait4 where option isn’t fulfilling the first statement then the second will. Then uid will be set to 0 then the = operator will return true and the program calling wait4 will continue but with as root. b_bop says: October 9, 2013 at 10:11 am this code is to handle calling wait4 in a way that was supposed to be invalid so no one stumbles upon it in regular use. it says IF it is call this particular way, give me root. trq says: October 9, 2013 at 10:11 am Of course it does. If statements just evaluate expressions, and this expression happens to set the uid equal to 0. JOHN says: October 9, 2013 at 10:14 am yes that is the case – in many many languages. you can do anything you like in the part that is determining true and false.. The variable has scope outside of the if. Such practice is very old school and is generally not recommended now as it is confusing. Infact as comment above its recommended to do bool comparisons backwards … ie 0 == fred because 0=fred will give a compile error if types mismatch. Phil says: October 9, 2013 at 10:16 am Any statement in the guard of an if statement will be executed & it’s result examined to decide which branch of the if to follow. Eg, you can call a function inside an if: if(is_the_world_pink()) { do_pink_things(); } else { do_blue_things(); } or any more complicated piece of C. An assignment has the value assigned as its result. So “i=3” has the value 3. This lets you do things like i=j=k=1; to set three variables to the same value. (Because this parses as i=(j=(k=1))) and the rvalues all fall out correctly.) An unfortunately side effect of this is that “if (i=3) {} else {}” is perfectly valid C. The compiler evaluates i=3 in order to get the rvalue of the statement (3), setting i to the value 3 as a side effect in the process. Most compilers will warn about this usage these days as it’s probably an error 999 times out of a 1000. However if you put extra brackets around the statement (as in the code sneaked into the kernel CVS above) then that suppresses the warning. digitalsushi says: October 9, 2013 at 10:17 am Right, it’s called a side-effect! It’s like the difference between these: If ( are you root? ) then do this If ( you are root! ) then do this The second one isn’t really a question, since it always give the same answer: “yup!” It’s a good trick to hide in plain sight. Nathan Marley says: October 9, 2013 at 10:18 am Doesn’t matter if the assignment is within an ‘if’ condition. Anything can be used as a conditional, as long as it’s syntactically valid. The assignment will still be evaluated. I’m confused by your “mere appearance of the assignment” statement. If it appears, then it’s there. The C compiler makes no assumption on the intent of your “if” statements. (As in, you INTEND for it to only check a condition vs make an assignment.) Robert Price says: October 9, 2013 at 10:20 am Yes, in C the expression “x = 0” evaluates to 0, but performs the variable assignment as a side effect. That is, the assignment happens whenever the assignment expression is evaluated. There’s another wrinkle that isn’t because of the “if”, exactly, but rather because of the && logical “and” operator. The part on the right of the && is only evaluated if the part on the left turns out to be true. This is called short-circuit evaluation and it takes some getting used to in the presence of expressions that have side effects. joe says: October 9, 2013 at 11:47 am C shortcuts the conditionals, so in this code the part after && will only run if the part before && evaluates to true. Brian says: October 9, 2013 at 9:19 pm noob, in C, = means assignment, it doesn’t matter if it’s in an if statement. if(uid=0) evaluates to true for the purposes of the if statement (the assignment was successful) and uid now has a value of 0. cm-t says: October 9, 2013 at 9:16 am A coding style rule can be using: (0==foo) instead of (foo==0). If there is a typing mistake 0=foo should not pass tests. if you really want foo=0, everyone will notice the change. Librement Martin says: October 9, 2013 at 9:41 am sure. if you’re Yoda, you can use 0 == foo. V-2 says: October 9, 2013 at 10:08 am A co-worker of mine even uses “0 != foo” syntax, even though I pointed out a few times that it serves no purpose whatsoever 🙂 I guess some people just like these inversions. Nathan Marley says: October 9, 2013 at 10:35 am I think your co-worker has the right idea. Makes it easier if you ever need to change the condition to “==”. Less stuff to change/move around and it prevents accidentally changing the operator to just “=”, as the reverse order will cause the compiler to complain in case of accidental assignment, as stated above. cm-t says: October 10, 2013 at 3:34 am That’s exactly what I suggested, but Vador was here before Yoda… fugaz says: October 9, 2013 at 10:10 am Yoda wise he is Anonymous says: October 9, 2013 at 4:19 pm Except when to compare two variables he needs. Then, order his operands he cannot, and mistakes he will now make. Darren says: October 11, 2013 at 1:19 am Actually, if you never use > or >= your code becomes easier to read. The smaller thing always comes first. Especially helpful when you’re comparing time ranges, like “is now after start but before start+timeout?” Plauger taught me that one. Peter N. M. Hansteen says: October 9, 2013 at 8:58 am Typing in an assignment (foo = 0) rather than the comparison you intended (foo == 0) is a fairly common error that you will likely see every now and then in hastily written code. Fortunately it’s also the kind of error you would be likely to recognize and correct very soon, say when re-reading your code after catching up on sleep. What makes this interesting is the backstory that somebody apparently broke into the server in order to introduce this rather nasty bug. BenAlabaster says: October 9, 2013 at 10:07 am The fact that someone broke into the server to insert this bug meant that they likely had the wherewithal to understand the difference between =0 and ==0. These two facts in concert make this look far more incriminating than a basic bug caused by programming on not enough sleep. But this could also just as easily be sloppy programming from a trusted developer that had credentials allowing them to commit code to CVS without going through the predefined approval process. You know, like developers with admin access to the source repo, who write code on a fork, issue a pull request to central and then log into central and complete the pull request without any peer review being completed. I’ve seen this happen many times, deadlines loom, everything goes to shit, people do what they need to in order to get things done; it’s 4am, you’re running on 2 hours sleep from the night before, nothing you can think of does what you need, corners get cut and discipline goes out of the window to get shit out of the door on time. Don’t discount a lack of discipline where it could feasibly be possible. Unless you know *their* specific CVS process and setup inside out, it’s difficult to hypothesize the actual reason this could have happened – though granted, it certainly looks highly suspicious. I certainly wouldn’t expect this kind of behaviour from trusted kernel developers… but having been in the real world for many years now, I’ve also learned to trust that even with the best of intentions and the tightest discipline, not everything always goes down the way you plan it. Do we definitively have evidence that the server must have been breached in order to do this? Or was this merely a breach of process and some sloppy code, which is an almost daily occurrence in software houses around the world? Anonymous says: October 9, 2013 at 12:30 pm The CVS tree was never used to push changes to Linus’ BK tree. The cracker was hoping that a trusted developer using CVS would pull a clone of Linus’ BK tree (receiving the backdoor in the process) and then submit patches via email to Linus containing the backdoor as part of a patch set containing other changes unknowingly. Many developers refused to use BitKeeper since it is a closed source program. Linus solved that problem by writing git. David says: October 9, 2013 at 3:15 pm Point of order: Linus only wrote git AFTER the owner of BitKeeper revoked his license, not BECAUSE people complained about it being closed source. (And the incident proved the closed-source naysayers to be correct.) https://lwn.net/Articles/130746/ pm says: October 9, 2013 at 12:34 pm this is not a mistake. Assume that the coder meant == 0 what is he trying to enforce. If these 2 bits (_WCLONE and _WALL) are set and your are root then the call is invalid. The bit combination is harmless (setting WALL implies WCLONE, its like saying “get all babies and all girl babies”), and why would you forbid it for root only. Plus the test looks for exactly those flags being set, not those flags along with some other flags like WNOHANG…. Finally if it was a coder with rights somebody would have fessed up by now BenAlabaster says: October 9, 2013 at 2:00 pm Assuming they weren’t a plant from the outset? Not trying to be a conspiracy theorist… though that would be an NSA play, they’re in it for the long game, that’s why they got in on the ground floor with the development of the NIST standards. You obviously can’t beat open source, it’s open source, so the only way to get something in is to try and sneak it in. The only way you can do that is either be someone that’s trusted or use someone that’s trusted… Of course, I’m just playing devil’s advocate on both sides of this fence. I agree it seems suspicious, but at the same time, many things can seem suspicious under a certain set of circumstances and be completely run of the mill in another. My inner conspiracy theorist is screaming that this was a plot to subvert Linux security…. but the realist in me that works with large teams of well trusted, talented developers that frequently find themselves in situations where all discipline goes out of the window and stuff like this happens all the time. Kratoklastes says: October 9, 2013 at 5:06 pm @BenAlabaster, the idea that the NSA (or any of the ludicrous, staffed-by-second-raters security-theatre alphagetti) is “in it for the long game” is naive in the extreme. Entry level are badly paid GS5s (~$45k a year) – of whom the talented ones leave on or before the second anniversary of their sign-on: above that, the entire ‘intelligence’ world is staffed by triangulators and ‘yes-men’ (and women, I s’pose) – and that psychotype does NOT have long forward horizons. It’s all about the ability of bureaucracies to ‘fail their way to bigger budgets’ – they dont bear the costs of their failures directly (failure has zero career impact – think of Clapper getting caught outright lying to Congress recently). And if you’re working in ‘national security’, you can always say “Oh, there are things we did that I can’t tell you about… all very hush-hush, you see. But we did a while bunch of smart stuff. Duqu? That was totally us. Stuxnet? Yeah, that was us too. So incentive structures simply do not reward horizons longer than a few months (the average time of a case officer on most projects). It’s a bit like the answer to the questions “Why was Apple’s fingerprint ID so easy to crack? And why was the ‘fix’ patch also so easy to crack?”. Answer: Apple does not bear even a miniscule proportion of the total cost of poor infosec in its iDreck – the USER bears the bulk (recovery from Apple would be a pipe-dream in any jurisdiction). So Apple has no incentive to pay the necessary premium to get genuinely talented coders to work in its barbed-wire-fenced garden: it can make do with journeyman plodders. And so it is with the security-theatre institutions – staffed at the very top with political appointees (the ULTIMATE yes-men), and at senior bureaucratic levels with… well… professional bureaucrats.And at the bottom, it’s Dumb and Dumber (e.g., TSA, police forces and the military, where an IQ above 120 would rule you out as a candidate – and in the TSA, a GED would probably do so too). If bureaucracy was remotely efficient, then Microsludge’s bloated operating system would be more secure than Linux: the bazaar beats the pyramid every time. And the ‘intelligence’ world is the ultimate in unaccountable bureaucracy. And as to ‘results’: they also are strong evidence against the ‘long game’ notion – can you name one single issue of genuine geopolitical importance (go back to the founding of OSS if you like) where the intelligence fraternity got it right? They had NO IDEA Russia was about to test a nuke in 1949; NO IDEA the Chinese were about to test in 1964 (ditto India, Pakistan… rince and repeat); NO IDEA that US involvement in VietNam was a waste of resources that would end badly; NO IDEA that funding the mujaheddin would lead to adverse consequences; NO IDEA that East Germany could implode within six weeks; NO IDEA that the Soviet Union could do likewise – and we can progress this litany of failure right through to 9/11, Bali, London, Iraq, Afghanistan, and the recent Boston bombing. By asserting that somehow these clowns (and they are clowns) have some superior insight and longer working horizons than the common man, is selling the common man short – most of the security-theatre’s manpower is made up of second-rate dummies whose horizon consists solely of getting a good personnel report so that they can try to arbitrage across to a higher-paid job at State. Almost 20 years ago I was involved in teaching a graduate course that included folks from JWAC (the CIA’s Joint Warfare Analysis Centre). The course sought to teach the methodology of computable general equilibrium computer modelling. The folks that JWAC sent to be trained were nice enough, but they were nowhere near the sharpest knives in the drawer: they weren’t even top decile among the course candidates. Doug says: October 9, 2013 at 5:56 pm This statement confuses me: “You obviously can’t beat open source, it’s open source, so the only way to get something in is to try and sneak it in. The only way you can do that is either be someone that’s trusted or use someone that’s trusted…” To me, the whole philosophy of open source is that anyone can try to sneak anything they want into the code. But because it is open source you have thousands of eyes reviewing the code and it is unlikely that anything that gets snuck in will not be detected and rejected. It is in closed source code where all you need to do is corrupt one trusted programmer working on the code and you can run roughshod over the whole system. If you assume that the definition of trusted means that people trust them ie they don’t look too hard at somebody’s work to detect deliberate back doors, then their back doors are unlikely to be detected. Very few people have the ability to look at the code and those that do won’t detect the backdoors because they don’t have the time or the inclination to look really hard for them. This is the inherent flaw in closed source. It requires you to trust everything to a multinational corporation and further trust that each and every one of their employees is uncorruptable. Human nature being what it is, that’s not a very good bet. BTW: Of course, good closed source systems will have people specifically looking for back doors. I’m sure many closed source programs are quite secure, but I am talking about the philosophical differences between closed and open source. Ben Alabaster says: October 9, 2013 at 10:17 pm @Doug Sorry I wasn’t clearer, that was my point – thousands of eyes reviewing the source code certainly makes it safer than proprietary software. Even if you do have appointed personnel scouring the code for security flaws and back doors, you can always be legally compelled to provide them. With open source software it’s not so easy – you can’t legally compel a whole community to let something under the radar, nor can you compel them all to keep quiet… well, you could try I suppose, but I doubt they’d all lay down silently like good little sheep – especially not loud-mouths like Torvalds and Stallman, nor the masses they influence. This is exactly the reason I’m in the process of making the switch from Windows to Linux at home and exactly why I will be leaning towards open source platforms and technologies for future projects …even though they’re a considerably larger pain in the ass than the far easier to configure and use than proprietary systems. Donald says: October 11, 2013 at 10:05 am A common error yes, but not for a kernel developer. Subsentient says: October 9, 2013 at 1:39 pm In addition, parentheses were not required for the final comparison. This was done to prevent compiler warnings. This looks deliberate. Doclogic says: October 10, 2013 at 1:27 am It’s deliberate. No doubt. I understand all the “worked for 3 days no sleep deadline mistakes are made” whining, but this really doesn’t look that way to me due to the nature of the “error”. Just think for a sec, a kernel expert (who else would write this and inject it) accidentally sets uid to zero? Really? Who the fuck would believe that? Like, no one? I’ve done lots of really queer things with my fingers, but I’d never screw that pooch., not even on a phone using that itty bitty screen and that itty bitty keyboard. Those things suck, and I make mistakes all the time using them, but not that kind of mistake. Think about it. The best example I can give is to frame it in the form of a question regarding “type” and “kind”, that being: Would you mistakenly try the big assed plastic keys the baby dropped because he didn’t want them any more for your house keys, even if you hadn’t slept in three days? No. You wouldn’t. Why? because they are the wrong kind. I could understand not being able to find the silver one amongst all the brass keys because it’s dark and they are all the same type, but you’d never mistake the baby’s keys for your own because they are the wrong kind. QED: It didn’t happen. Jeremy P says: October 11, 2013 at 6:46 am The parentheses around the last bit absolutely are required because assignment has a lower precedence than && which means that the code would attempt to assign 0 to a non l-value. This is an error in C. The parentheses would not be required if the == operator had been used, but people often put in redundant parentheses anyway to make it explicit how the precedence goes. So this could have been finger trouble – using = instead of == is an old and well known way of screwing up in C. This is why you sometimes see things like 0 == foo inside if statements. The smoking gun is how the code actually got into the CVS repository. The CVS repository was supposed to be an automated copy of the Bitkeeper repository. This code was somehow inserted after syncing Bitkeeper to CVS and so can only have been done maliciously. mike acker says: October 22, 2013 at 9:37 am if (you_are_a_C_programmer) { /* then you are going to write If( 0==foo and NOT If( foo==0 */ /* unless you want to set foo equal to zero,– as follows: */ foo=0; } Freedom to Tinker is hosted by Princeton's Center for Information Technology Policy, a research center that studies digital technologies in public life. Here you'll find comment and analysis from the digital frontier, written by the Center's faculty, students, and friends. What We Discuss AACS bitcoin CD Copy Protection censorship CITP Competition Computing in the Cloud Copyright Cross-Border Issues cybersecurity policy DMCA DRM Education ethics Events Facebook FCC Government Government transparency Grokster Case Humor Innovation Policy Internet Law Managing the Internet Media NSA Online Communities Peer-to-Peer Predictions Princeton Privacy Publishing Recommended Reading Secrecy Security Spam Super-DMCA surveillance Tech/Law/Policy Blogs Technology and Freedom transparency Voting Wiretapping WPM Contributors Select Author... Aleecia M. McDonald Alex Halderman and Nadia Heninger Alex Migicovsky Amy Winecoff Andrew Appel Angelina Wang Annemarie Bridy Annette Zimmermann Annie Edmundson Arunesh Mathur Arvind Narayanan Axel Arnbak Aylin Caliskan-Islam Bart Huffman Barton Gellman Ben Kaiser Bendert Zevenbergen Bill Zeller Blake Reid Brett Frischmann Bryan Ford Chong Xiang Christelle Tessono Dan Wallach Daniel Howe Dave Levine David Lukens David Robinson Diego Vicentin Dillon Reisman Ed Felten Eric Smith and Nina Kollars Ethan Heilman Gary McGraw Gina Neff Grace Cimaszewski Grayson Barber Gunes Acar Harlan Yu Harry Kalodner Henry Birge-Lee Hooman Mohajeri Moghaddam Ian Davey Ian Lundberg J. Alex Halderman Jake Shapiro James Grimmelmann Jared Ho Jasmine Peled Joanna Bryson JD Lasica Jeffrey Tignor Jennifer Rexford Jeremy Epstein Jerry Brito Jessica Su Joe Calandrino Joel Reidenberg Jon Penney Jonathan Mayer Joseph Bonneau Joseph Lorenzo Hall Joshua Goldstein Joshua Kroll Julia Stoyanovich Justin Curl Karen Eltis Karen Rouse Katherine Haenschen Kelvin Chen Kenny Peng Kevin Klyman Kevin Lee Kevin Munger Klaudia Jaźwińska Laura Cummings-Abdo Leonid Reyzin Liza Paudel Luis Villa Lukasz Olejnik Madelyn R Sanfilippo Marcela Melara Mark Braverman Mark Hass Marshini Chetty Matheus Ferreira Matthew Salganik Mihir Kshirsagar Mike Freedman Miles Carlsten Mitch Golden Nadia Heninger Nathan Matias Nick Feamster Nicky Robinson Paul Ellenbogen Paul Ohm Paulina Borsook Pete Zimmerman Philip N. Howard Philipp Winter Prateek Mittal Priya Kumar Rebecca MacKinnon Ron Hedges Ronaldo Lemos Ryan Amos Sam Ransbotham SG Stephen Schultze Steve Roosa Steven Englehardt Steven Goldfeder Suman Jana Tiffany Li Timothy B. Lee Vanessa Teague and J. Alex Halderman Vitaly Shmatikov Wendy Seltzer Will Clarkson Yan Shvartzshnaider Yoshi Kohno Yusuf Dahl Zeynep Tufekci Archives by Month 2023: J F M A M J J A S O N D 2022: J F M A M J J A S O N D 2021: J F M A M J J A S O N D 2020: J F M A M J J A S O N D 2019: J F M A M J J A S O N D 2018: J F M A M J J A S O N D 2017: J F M A M J J A S O N D 2016: J F M A M J J A S O N D 2015: J F M A M J J A S O N D 2014: J F M A M J J A S O N D 2013: J F M A M J J A S O N D 2012: J F M A M J J A S O N D 2011: J F M A M J J A S O N D 2010: J F M A M J J A S O N D 2009: J F M A M J J A S O N D 2008: J F M A M J J A S O N D 2007: J F M A M J J A S O N D 2006: J F M A M J J A S O N D 2005: J F M A M J J A S O N D 2004: J F M A M J J A S O N D 2003: J F M A M J J A S O N D 2002: J F M A M J J A S O N D author log in Return to top of page Copyright © 2023 ·Education Theme on Genesis Framework · WordPress · Log in",
    "commentLink": "https://news.ycombinator.com/item?id=38805439",
    "commentBody": "The Linux backdoor attempt of 2003 (2013)Hacker NewspastloginThe Linux backdoor attempt of 2003 (2013) (freedom-to-tinker.com) 200 points by zhan_eg 19 hours ago| hidepastfavorite85 comments grugq 18 hours agoI have the full story on that incident. It is actually really funny.If the guy who did it wants to come forward, that is his decision. [edit: I won&#x27;t name names.]He did provided me the full story. He told me with the understanding that the story would go public, so I will dig it up and post it.I also interviewed the sysadmins who were running the box at the time.1. it was not an NSA operation, it was done by a hacker.2. it was discovered by accident, not because of clever due diligence.Basically, there was a developer who had a flakey connection and one time his commits didn&#x27;t go through. To detect this in future he had a script that would download the entire tree from the server and compare it against his local copy to make sure that his changes had been committed.It was discovered because of the discrepancy between his local working copy and the upstream copy. Which was checked not for security reasons, but because sometimes the two were out of sync. That&#x27;s all. Just dumb luck.The sysadmins are still quite bitter about it. I know how it feels when your box is hacked and you really take it personally.The code wasn&#x27;t added by hacking the CVS, as far as I remember, but rather through a hacked developer with commit rights.that&#x27;s the story as I was told reply epcoa 9 hours agoparentGeez, this crowd. The clearest evidence that it was not an NSA attack is that it was not very good. It modified a CVS mirror. At no time was the source of truth (the bitkeeper repo) in any danger. Anybody that knew how this stuff worked at the time would have known it would be caught immediately. Not very state level expertise, pretty sad if it was the NSA. reply p-e-w 6 hours agorootparent> The clearest evidence that it was not an NSA attack is that it was not very good.I suspect you are being sarcastic, but in case you aren&#x27;t, you may want to reexamine your assumptions.The colossal incompetence that is synonymous with government work doesn&#x27;t magically stop at three-letter agencies. The FBI&#x2F;CIA communication fuckups before 9&#x2F;11 are just one famous example.The idea that the NSA is staffed with \"uber hackers\" is a Hollywood fantasy. A government job working as a hacker is still a government job. Why would someone with that skillset, who can get a job at FAANG for 10x the salary, submit to the bureaucracy and monitoring BS that comes with working for an intelligence agency? I&#x27;m sure there are a select few who find this appealing, but the vast majority are just going the take the money and the free life. reply joncrane 4 hours agorootparentNot to mention it being extremely difficult to travel internationally, and not being able to have close personal friendships with many people who live in other countries. Not being able to partake in THC consumption EVER, much less any other recreational substance besides alcohol. The list goes on.I understand that it pays very well and there&#x27;s decent work&#x2F;life balance in terms of hours. But you have to essentially work in a windowless cell with no internet. And for lots of people with the curious hacker mentality, it would be a chore to \"keep your nose clean\" as they say.I live in the DC area and the stereotype of the bland, khaki, polo, and white sneakers wearing boring person is true. reply hatenberg 3 hours agorootparentprevIranian centrifuges would disagree with you as does the conversation about the apple exploit chain from last week. reply stevehawk 6 hours agorootparentprevmaybe 10x the salary (probably not) but also a correlated increase in hours instead of a contractually mandated maximum of 40 hours, combined with the legal inability to do work from home, discuss work at home, and a lot of related perks. reply p-e-w 5 hours agorootparentAlso \"perks\" like having your life put under the microscope at regular intervals, going to prison if you talk about what you do, etc.And I strongly doubt that agencies that are known to routinely violate the law, the constitution, and human rights care about \"contractually mandated\" 40-hour workweeks. reply causal 17 hours agoparentprevWait was the guy you know the hacker or someone who discovered the hack by accident? If the latter, how do you know anything about the hacker&#x27;s identity or motive? reply ngneer 17 hours agorootparentThat confused me, too. They appear to know the person who accidentally discovered the issue, not the hacker. reply netsharc 17 hours agorootparentDeveloper tries to tell a story...Sounds like OP interviewed the person who uploaded the code, whose system was previously inflitrated (it can still be the NSA). So why say \"If the guy who did it wants to come forward, that is his decision. But he did provide me the full story\", it doesn&#x27;t sound like OP interviewed the \"guy who did it\"... reply AnimalMuppet 17 hours agorootparentI read that the other way. \"If the guy who did it wants to come forward, that is his decision. But he [still talking about the guy who did it] did provide me the full story.\"That is, the perpetrator gave him the full story, but he won&#x27;t name names, because it&#x27;s the perpetrator&#x27;s choice whether or not to reveal his identity. reply netsharc 17 hours agorootparentOK, makes sense.. so the interviewed hacker mentioned that he got the code in by infiltrating the computer of \"some developer\"... reply grugq 16 hours agorootparenthe was more specific, but I (a) don&#x27;t remember the name off the top of my head, and (b) don&#x27;t think it is beneficial to put them on blast. It isn&#x27;t their fault they got hacked 20 years ago. reply jstanley 17 hours agorootparentprevHow do they know it wasn&#x27;t the NSA then? reply _notreallyme_ 16 hours agorootparentThe guy who did it was quite vocal about it in some circles. It was a \"for the lulz\" kind of hack... reply grugq 16 hours agorootparentprevthe hacker. I interviewed the sysadmins about it. reply greggsy 14 hours agorootparentprevIt’s the grugq.. he knows everything and everyone reply mathverse 14 hours agoparentprevDid not cliph &#x2F; wojciech purczynski also try to backdoor the kernel? reply ajross 17 hours agoparentprevTo be clear: you&#x27;re telling us the full story of the discovery, not the full story of the exploit? You and your source don&#x27;t know who the attacker was, right? reply grugq 16 hours agorootparentWhat is there to say about the hack? Like everything back then it was probably accomplished by exploiting trust relationships. I can ask him, but it is not at interesting 20 years later. reply umanwizard 10 hours agorootparentThe part of your story that’s still unclear is whether you know the identify of who actually inserted the malicious code. reply spenczar5 16 hours agorootparentprevIt is very interesting to prove whether or not it was a state actor! Surely you can see that that mystery is interesting to many people. reply epcoa 9 hours agorootparentA state actor would have done a much better job. This was detected nearly immediately and anyone that knew how the system was setup (which was public knowledge) would have known this would be caught. The state level hackers are not that dumb.If there was a serious backdoor attempt, then this was the distractor.And seriously back in those days especially Linux didn’t need much help with getting root exploits in the tree. reply dspearson 10 hours agorootparentprevIt was not a state actor. There were plenty of high profile people and projects being owned just for the fun of it back then. reply _notreallyme_ 16 hours agorootparentprev> Like everything back then it was probably accomplished by exploiting trust relationshipsThat&#x27;s wrong on many levels. Bold and stupid \"hacks\" committed by teenagers using SE tend to get a lot of traction, because it is both bold and stupid. This hasn&#x27;t changed. But \"back then\" there was much more than that... reply ShamelessC 14 hours agorootparentprev> What is there to say about the hack?What is there to say about the [discovery]? Like everything back then it was probably accomplished by [a simple source code diff]...it is not at interesting 20 years later.You get the idea. The story you know might be interesting to you because you happen to know the person involved. And it is sort of interesting? But not really as interesting as the _full_ story would be. In particular because your grammar in your original comment kind of implies you knew the actual attacker.This all seems fairly obvious to me? Is there anything we&#x27;re missing about the discovery? It&#x27;s pretty mundane that one of hundreds of devs working on that source code happened to have a vanilla copy, especially in 2003 with a less reliable and slower internet. reply hulitu 11 hours agoparentprev> 1. it was not an NSA operation, it was done by a hacker.Just like the NPR is not financed by the US government, but by NGOs. reply unethical_ban 10 hours agorootparentNPR is not (majority) financed by the US government.https:&#x2F;&#x2F;www.npr.org&#x2F;about-npr&#x2F;178660742&#x2F;public-radio-finance...edit - removed some snark reply dspearson 10 hours agorootparentprevNext we&#x27;ll be hearing that ~el8 was a CIA front. :) reply ijustlovemath 17 hours agoprevAnother bit of cleverness not mentioned in the article is that assignment expressions always evaluate to the rvalue. So the expression `current->uid = 0` has the effect of making sure that entire conditional never actually runs (or at least, the return never runs), which means the overall behavior of wait4 doesn&#x27;t change in an observable way. Very clever if you&#x27;re trying to pass all of the existing tests reply GuB-42 16 hours agoparentBut that should be something the compiler could catch. The expression is always false and the condition would never be executed. You usually get a warning for that. And if the compiler doesn&#x27;t, linters do.This is a common mistake, and I believe most linters have rules for that. And I don&#x27;t think there is any situation where there is a good reason for code like this to exist. Either the expression is wrong, or it doesn&#x27;t belong in a \"if\". You may get stuff like that in legitimate code with macro expansion, but again, it is not the case here, and from my experience, you get a warning anyways. reply _notreallyme_ 16 hours agorootparentYou didn&#x27;t get a warning for that in 2003... or maybe with the pedantic flag. And even if it were the case, it would have been drowned by all the other unfixed warnings...The only people using linters at that time was because it was forced by regulation (like automotive, aeronautics, ...) reply rcxdude 7 hours agorootparentprevEven now, and I&#x27;m pretty sure back then, the relevant warning in GCC is suppressed if the assignment occurs in parentheses, like it did in the diff (clang I believe has the same behaviour). So you would likely need a static analysis tool to flag up the behaviour, and those are quite noisy at the best of times. reply lelanthran 14 hours agorootparentprev> But that should be something the compiler could catch.Today, certainly. My compiler even catches errors in the format strings to printf[1].But back then? I doubt it, even with all the warnings turned up.[1] Removing yet another common source of bugs. reply diath 12 hours agorootparentprevThe compiler does not, but tools like clang-tidy do, (bugprone-assignment-in-if-condition in clang-tidy) but that didn&#x27;t exist back then. reply AnimalMuppet 17 hours agoparentprevOhh, that is clever - unless someone writes a test for these two new lines, and finds that they never return -EINVAL. reply TheJoeMan 17 hours agorootparentMaybe the unit tests should test that the variables you intend to \"not touch\" do not in fact change. So record UID before and after the function. When I&#x27;m writing critical code like this, one gets a tingly feeling when typing in that variable name. reply marssaxman 10 hours agorootparentprevDid unit tests exist in 2003? I don&#x27;t clearly remember when that idea came along, but comprehensive unit testing certainly was not standard practice 20 years ago... not in any organization I knew about at the time, anyway! reply usr1106 17 minutes agorootparentI believe unit tests existed. I had a test training in 2000 and things were pretty systematic already then. Not 100% sure whether the exact term was used then.Edit: JUnit is from 1997. So the name was definitely in use in 2003. I attended a TDD tutorial before 2004 (don&#x27;t remember the exact year). CI wasn&#x27;t a thing yet, so you executed your unit tests manually. &#x2F;editDo unit tests exist in the kernel today? There is some (or some would say a lot) of automatic testing for the kernel, but I don&#x27;t remember seeing a single unit test. reply mmsc 17 hours agoprevWasn&#x27;t this done by Ac1dB1tch3z? See http:&#x2F;&#x2F;phrack.org&#x2F;issues&#x2F;64&#x2F;15.html for the CVS exploit from the same time. reply mathverse 14 hours agoparentNice find. Maybe sd? reply natas 10 hours agoparentprevyes reply a-dub 14 hours agoprevit still seems kinda weird to me that all it takes to elevate privileges for a user process to \"can arbitrarily write system level memory or disk\" is just the clearing of all the bits of a single integer in kernel space which can be done by pretty much any execution path in the kernel.it just seems like there could be a more tamper resistant mechanism around privilege elevations. reply umanwizard 10 hours agoparentYeah, everything in the kernel is trusted and lives in one address space, just like any normal program. This is part of what would be solved by a microkernel architecture. reply a-dub 8 hours agorootparentthat&#x27;s part of it. and is the basis of the classic tannenbaum v. torvalds debate, but only part of what i mean.it would be interesting if there were some kind of write protection on the process-privilege data where some effort is made to verify the provenance of updates before they&#x27;re allowed to go through or maybe even the whole privilege table is centralized and signed. reply worthless-trash 4 hours agorootparentprevCan you explain how..Its my understanding that if \"OS process\" runs with its own address space with privileges (as it needs to talk to hardware), once an attacker has code execution functionality, what stops them from mapping the memory they need then writing to the address to set uid ? reply ngneer 17 hours agoprevWe used to use this as a cautionary tale in the CS department security course at the Technion. First, to highlight trust relationships in the \"supply chain\" (as the notion is now known in contemporary usage). Second, to pose the question of whether open source is inherently more trustworthy. reply laweijfmvo 16 hours agoparentI guess you could argue that more [evil] people would try to backdoor the linux kernel than there are [malicious] people inside private companies, but the level of trust inside a private company is probably much higher? Seems complex reply ngneer 15 hours agorootparentYou hit the nail on the head. It is a complex question without a straightforward answer. reply zhan_eg 19 hours agoprevPrevious discussions - [1] [2][1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24106213[2] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18173173 reply dang 14 hours agoparentThanks! Macroexpanded:The Linux Backdoor Attempt of 2003 (2013) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24106213 - Aug 2020 (141 comments)The Linux Backdoor Attempt of 2003 - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18173173 - Oct 2018 (28 comments)The Linux Backdoor Attempt of 2003 - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=6520678 - Oct 2013 (63 comments) reply robblbobbl 17 hours agoprevI&#x27;m pretty sure there are tons on unreleased and unpublished backdoor exploits for linux and windows likewise. The problem is you can&#x27;t fix them yourself if the signature keeps unknown to anyone. reply cwillu 16 hours agoparent“Backdoor” means something deliberately and specifically added to enable the vulnerability. I.e., something can&#x27;t really be both a backdoor and an exploit. reply hn_go_brrrrr 14 hours agorootparentReally? I think of a backdoor as a deliberate vulnerability, and the exploit as the attack (or attack code) that makes use of any kind of vulnerability.Let&#x27;s say the NSA adds a backdoor. If someone else finds it, isn&#x27;t that an exploit? reply wrboyce 9 hours agorootparentVery similarly to yourself, but I would say backdoor and vulnerability are mutually exclusive (kinda? I guess a backdoor is a deliberate vulnerability but I think you know what I mean) yet both can be exploited (the exploit being the client side code, if you will). reply cwillu 5 hours agorootparentprevIrregular verb joke incoming:I log in. You backdoor. They exploit. reply IshKebab 16 hours agoprevI think the risk from this type of attack is probably near zero. You can&#x27;t hack into Github and add a commit to Linux.Probably most of the deliberate backdoors that are present in Linux have been inserted by well funded state sponsored developers performing useful work. Easy to sneak a vulnerability in that way. (There was a controversial incident a few years ago when some researchers proved as much.) reply LMYahooTFY 16 hours agoparentLink to the incident you&#x27;re referring to? reply richbell 15 hours agorootparentIf I had to guess it&#x27;s this, but it seems like the researcher&#x27;s claims didnt stand up to scrutiny.https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;HobbyDrama&#x2F;comments&#x2F;nku6bt&#x2F;kernel_d... reply charonn0 16 hours agoprev> it said \"= 0\" rather than \"== 0\"Why do so many programming languages have different equals&#x2F;assigns operators?There are languages that combine them and apparently don&#x27;t have any problems. Is it something to do with being strongly vs. weakly typed? reply layer8 14 hours agoparentThe C designers wanted to be able to write stuff like `while ((ch = getchar()) != EOF) { ... }`, so assignment needed to be an expression. Secondly, C had no boolean type, and instead integers were used for boolean values (zero is false, nonzero is true). The combination of these two facts entails that an integer assignment is also a valid boolean expression.To prevent accidental or malicious use of the assignment operator in place of the equals operator in a language, you either have to have a real boolean type, and no implicit conversion of other types to boolean, or make assignments not be an expression, or disallow assignment expressions in boolean contexts.Making both operators the same symbol is not a good solution IMO, because it makes it harder to distinguish which is which in arbitrary contexts. E.g. in `a = b = c`, presumably the first is an assignment and the second a comparison? Or maybe not? It would just be confusing. Not sure which languages you are referring to that do this. reply krylon 11 hours agorootparentA common idiom to defang this was the \"Yoda assignment\": if (0 == do_something(foo)) { ... }If one accidentally omits one equals-sign, it makes the compiler barf instead of becoming a silent-but-deadly kind of bug (whether intentional or not).In Go, an assignment is not an expression, so the whole thing becomes illegal. I found this approach a bit offensive at first, but I got used to it rather quickly. reply akira2501 12 hours agorootparentprev> or make assignments not be an expression,Or just reverse the expression: 0 == curent->uidSo that the bug case is an error: 0 = current->uid reply layer8 11 hours agorootparentYes, that is well known, but it doesn’t prevent the issue in TFA. reply akira2501 10 hours agorootparentHow does it not? Applied literally to the article, it would have turned this backdoor into a compile time error. reply umanwizard 10 hours agorootparentBecause you can’t trust the person backdooring your code to help you out by writing in this style. reply akira2501 9 hours agorootparentYes, they could literally violate the coding style, but presumably, that would draw more attention to what they&#x27;ve done, not less. replyklodolph 16 hours agoparentprevI don’t think strong&#x2F;weak typing is the culprit here.I think partly that being explicit is nice. Assignment and equality are two very different things, so it makes sense for there to be different syntax. You can easily prevent the code in the article from working—just disallow assignment inside of expressions. This is probably a good idea, and a lot of newer languages make that choice.Even when you read papers about programming, you often see different notation for assignment and equality. Assignment may beI would hate to see something like this in my code base:> a = x = y;> If that meant “set ‘a’ to true if ‘x’ is equal to ‘y’, and false otherwise.” I would, honestly, be a little pissed off.Would you find it more acceptable as `a = (x = y)`? To me, that is reasonably clear. reply klodolph 13 hours agorootparent> Would you find it more acceptable as `a = (x = y)`? To me, that is reasonably clear.No, I don’t consider that acceptable. It is not enough that it is clear to some people who know what they are looking at. The language should be more clear to more people. reply vidarh 14 hours agoparentprevSome to make them more distinct. Some because they treat assignment as an expression, and so either can occur in the same context.In the former you could combine them. In the latter you can&#x27;t (you need to be able to tell if \"if (a = b) ...\" contains a comparison or assignment).(EDIT: I agree with the sibling reply from klodolph there - there are many cases where reusing the same operator would get really confusing, and so I&#x27;d prefer the operators to be distinct even if the language do not allow them in the same context) reply charonn0 13 hours agorootparentIt&#x27;s been my impression over the years that = vs. == is one of the most common mistakes made in languages that use them. In which case, can it really be said to be less confusing? reply vidarh 11 hours agorootparentThere are two orthogonal issues here:1) Do you allow assignment as an expression?2) Do you use the same operator?If you answer \"yes\" to #1, you must answer no to #2, but if you answer no to #1 you can choose whether or not you use the same operator. Consider these examples (assuming that if they&#x27;re different, we use =&#x2F;==, but of course any other set of operators could be substituted): # A) if &#x27;yes&#x27; to 1 this would be a \"double assignment\", setting both a and b to c. a = b = c # B) if &#x27;no&#x27; to 1, and &#x27;yes&#x27; to 2, this would be an assignment of the comparison of b and c to a: a = b = c # C) if &#x27;no&#x27; to 1 and &#x27;no&#x27; to 2, this would be an assignment of the comparison of b and c to a: a = b == c # D) if &#x27;no&#x27; to 1 and &#x27;no&#x27; to 2, this would most likely be a syntax error: a = b = cWith respect to confusion, I&#x27;d argue that B) creates a lot of potential for confusion. You&#x27;d want \"a = b = c\" to either be \"double assignment\" (A) or a syntax error (D). If your language does not allow assignments as expressions, I&#x27;d go for C&#x2F;D exactly for the reason you give, as the main reason not to allow assignments as expressions tends to be exactly to avoid the mistake you mention (it&#x27;s trivial to support in a compiler&#x2F;interpreter, so it&#x27;s a question of whether you believe it&#x27;s more helpful or more damaging) reply layer8 13 hours agorootparentprevModern languages using that syntax tend to prevent that mistake by either outright disallowing assignments in boolean contexts, or by not having implicit conversion of other types to boolean, meaning that the mistake would be limited to the case of comparing a boolean variable to another value, which is quite rare. Some languages further limit the risk by making variables unmodifiable by default, meaning that it would have to be an explicitly modifiable boolean variable.Assignment is one of the most frequent operations in typical programming languages, so it makes sense for it to be a single-character symbol, and ‘=’ is about the only fitting ASCII symbol for that. (With non-ASCII, there would be ‘≔’ or ‘←’ (the latter being used by APL), but those are non-obvious to type.) reply nurettin 16 hours agoparentprevIt&#x27;s just syntax. Pascal had := and = reply agilob 18 hours agoprevThis is a single example of an unsuccessful attempt to backdoor Linux. There were successful attempts too https:&#x2F;&#x2F;www.bleepingcomputer.com&#x2F;news&#x2F;security&#x2F;nsa-linked-bv... reply grugq 18 hours agoparentAm I missing something? That seems to link to a Linux backdoor, not a backdoor in Linux. reply deelowe 17 hours agorootparentYeah. This doesn&#x27;t appear to be the same sort of thing (malicious code being integrated into the kernel itself). This is just a backdoor that leverages an exploit. reply aftbit 17 hours agoprev [–] While I&#x27;m here, does anyone know of a good trustworthy RAT for Windows machines that I can control from my Linux box? I have some relatives for whom I provide technical support. I&#x27;d love to just put an EXE on their desktop that would launch a VNC session and connect back to me (since they have the typical NAT + firewall of home users), but I don&#x27;t want to install a virus on their machines. reply gerdesj 9 hours agoparentOpenVPN, IPSEC, Wireguard can all be used to tether them to you. If you don&#x27;t have a static IP then a dynamic DNS service can fix that. Once you have a VPN then use whatever you fancy - RDP for example.You could use Teamviewer or the like.Self host a MeshCentral or RustDesk (MC for me!) reply ngneer 17 hours agoparentprevNot sure what your decision procedure is for \"a virus\" versus \"trustworthy RAT\", but there are plenty of open source options out there, in case that helps, https:&#x2F;&#x2F;medevel.com&#x2F;18-os-rat-list&#x2F; reply olivierduval 17 hours agoparentprevAFAIK, solutions like TeamViewer or AnyDesk may be securely connected from the outside and manage the NAT+firewall of home user... no need to have a RAT (in the \"virus&#x2F;backdoor\" meaning) reply avidiax 11 hours agoparentprevPut Tailscale on their machines and use a normal remote desktop application (probably the built-in RDP). Or put a RaspberryPi with Tailscale on their network.Just make sure you set the key for those clients to not expire. reply genpfault 15 hours agoparentprevTor + ssh onion service? reply popcalc 17 hours agoparentprev [–] Anydesk? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The passage covers topics such as attempted backdoor insertion into the Linux kernel and the importance of trusted systems in warfare.",
      "It discusses concerns about cryptographic methods and the need for validation and tracking in open source development.",
      "The author also talks about coding techniques, compiler behavior, and the use of if statements in programming languages, as well as the benefits of open source software and criticism of closed source code."
    ],
    "commentSummary": [
      "The discussion touches on a range of topics, including a previous hacking incident in Linux and challenges in government jobs.",
      "It also examines the effectiveness of compilers and tools in identifying errors, the security implications of privilege elevations in operating systems, and the debate between open-source and proprietary software regarding security.",
      "The concept of backdoors and their exploitation is explored, and recommendations are provided for a reliable remote administration tool for controlling Windows machines from a Linux box."
    ],
    "points": 200,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1703860956
  },
  {
    "id": 38805383,
    "title": "How the append-only b+tree improves data storage and access efficiency",
    "originLink": "https://www.bzero.se/ldapd/btree.html",
    "originBody": "how the append-only btree works Consider this 3-level b+tree. There are two levels of branch pages (the root is a branch page), and 5 leaf pages. Data and keys are stored in the leaf pages. Leaf chaining, ie links between leaf pages for easy sequential access, is not supported as it would require the whole tree to be rewritten on each update. The pages are stored sequentially in the database file. Increasing page numbers means increasing file offsets. The meta page includes a pointer to the root page, a SHA1 hash, and statistics counters. When the file is opened it is scanned backwards page by page to find a valid meta page, and thus the root of the tree. When updating a value in leaf page 8, instead of changing the page in-place, a whole new page is appended to the file (here as page 12). Because the location of the page is changed, each parent page needs to be updated to point to the new locations. Leaf page 7 is not affected. A new root page is created as a copy of the previous root page, only the pointer to branch page 6 is updated to point to branch page 11. Any cursor still having a pointer to root page 9 can traverse the tree unaffected by the change. It will see a consistent snapshot of the database. Dashed pages and pointers in the diagram are still there in the file, they are just not the last version. In the file, pages are written sequentially by appending new pages to the file. Already written pages are never modified. After each new generation of the tree is written, there is a meta page pointing to the new root. Changing one page (leaf page 8) resulted in 4 new pages being appended to the file. This wastes disk space, but writing consecutive pages to disk is more efficient than writing random locations. And there is no need for a transaction log, because the database file is the transaction log.",
    "commentLink": "https://news.ycombinator.com/item?id=38805383",
    "commentBody": "How the append-only btree works (2010)Hacker NewspastloginHow the append-only btree works (2010) (bzero.se) 196 points by kevmo314 19 hours ago| hidepastfavorite84 comments ww520 16 hours agoThe immutable b+tree is a great idea, except it generates a tremendous amount of garbage pages. Every update of a record generates several new pages along the path of the tree.There are two additional techniques to make immutable b+tree practical. One is to reclaim stale unreachable pages after the transactions using them have closed. Two is to use a pair of oscillating fixed location meta pages.A page is active if it’s in the latest snapshot or it is in use by an open transition; otherwise, it’s unreadable and can be reclaimed. When a transaction closes, it hands its list of in-use pages to the reclaimer. The reclaimer tracks these pages. When no other open transactions hold on to a page, it can be reclaimed.When the write transaction commits, it hands the list of old pages being overwritten to the reclaimer as candidates for freeing, pending no open transactions using them.The reclaimer can batch a number of reclaimed pages together to update the free page list, by appending to the end of the file a page of the newly freed page pointers. Update the meta page to point to the new head page of the free page list at the end of the file. This can be done as a write transaction to keep things consistent.At a crash all the pending reclaiming pages in memory are lost and the garbage pages in disk linger. This requires periodic garbage collection or compaction.The most frequently updated page in the tree is the meta page. Every write transaction updates it. This creates a lot of garbage pages. The second technique addresses this problem.One insight is that the meta page is only needed when a new transaction begins by finding the tree root of the latest committed snapshot. That means we only need two copies of the meta pages, one for the last committed snapshot and one for the new pending write transaction. When the write transaction commits, the pending meta page becomes the latest committed snapshot. The other page becomes available for the next pending transaction.We can have two fixed location meta pages, oscillating between the latest committed snapshot and the pending new transaction. The fixed location removes the need to search for the meta page from the end of file. reply ruuda 14 hours agoparentThe Hitchhiker Tree [1] addresses the write amplification at the cost of making lookups and scans slightly more expensive, by keeping a small array of \"pending writes\" in every non-leaf node. The entire thing is still immutable, but instead of writing a new leaf node + path from root to that leaf, in the majority of cases we only write a new root. When there is no space left in the array, all the pending values get pushed one level down at the same time, so the write amplification is amortized.[1]: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=jdn617M3-P4 reply senderista 9 hours agorootparentThis idea dates to at least 1996: https:&#x2F;&#x2F;citeseerx.ist.psu.edu&#x2F;document?repid=rep1&type=pdf&d... reply kragen 1 hour agorootparentcan you explain the similarities and differences between arge&#x27;s &#x27;buffer tree&#x27; and the fractal tree and hitchhiker tree?my tentative understanding is that arge&#x27;s work, applied to an ordinary search tree, yields something very similar to the fractal tree, except that it will opportunistically flush buffers before they are full if the child nodes they route to must be brought in from disk anyway to answer a read query. but arge is more concerned with applying the technique to hairier data structures like segment trees and binary decision diagrams. and the hitchhiker tree seems to be precisely a copy-on-write fractal tree, no more and no less. but possibly i am misunderstanding something?there&#x27;s a clear citation path, so maybe i can untangle this: greenberg describes the hitchhiker tree as &#x27;synthesizing fractal trees and functional data structures&#x27;, kuszmaul&#x27;s 02011 slide deck https:&#x2F;&#x2F;www.percona.com&#x2F;blog&#x2F;wp-content&#x2F;uploads&#x2F;2011&#x2F;11&#x2F;how-... cites brodal and fagerberg (02003, though he says 02002) and buchsbaum, goldwasser, venkatasubramanian, and westbrook (02000, though he says 02006), both of whom cite arge&#x27;s 01996 paper reply ww520 14 hours agorootparentprevI believe the Bw-tree does the same thing, caching new updates in the intermediate branch nodes and only pushing the updates in batch down to the lower layers when running out of room at the branch node. These are the newer wave of algorithms to address the write amplification. reply cryptonector 14 hours agoparentprevYes, CoW trees have tremendous write magnification. This is well-known. The fix is to amortize this cost by writing transactions as a log and then doing a b-tree update operation for numerous accumulated transactions.Think of ZFS and it&#x27;s ZIL (ZFS Intent Log). That&#x27;s exactly what the ZIL is: a CoW tree write magnification amortization mechanism. reply ww520 14 hours agorootparentI&#x27;m not familiar with ZFS internals beyond the deduplication part. Is that just the traditional transaction log + btree update approach most databases used? Does ZFS support transactional reading and writing? reply cryptonector 13 hours agorootparent> Does ZFS support transactional reading and writing?I don&#x27;t know that I understand your question. ZFS supports POSIX transactional semantics, which is not ACID, though ZFS&#x27;s design could support ACID.> Is that just the traditional transaction log + btree update approach most databases used?Basically, yes. The idea is to write a sequential log of a) data blocks, b) metadata operations (e.g., renames) to support fast crash recovery. During normal operation ZFS keeps in-memory data structures up to date but delays writing new tree transactions so as to amortize write magnification. On unclean shutdown ZFS checks that all transactions recorded in the ZIL have been written to the tree or else it will do it by a) rewinding the ZFS state to the newest fully-written transaction, b) loading the contents of the ZIL from that point forward.Because the ZIL was designed at a time when fast flash devices were expensive, it&#x27;s really a separate thing from the rest of the tree. If one were to start over one might integrate the ZIL with the rest of the system more tightly so as to further minimize write magnification (e.g., data blocks written to the ZIL need not be re-written to the tree). reply ww520 9 hours agorootparentGood to know. Thanks for the explanation.Basically ZFS maintains a ZIL based in-memory tree for the recent updates and a on-disk btree for the complete updates, minus the recent updates in ZIL. That&#x27;s consistent with most database transaction log implementation. Some newer approach adds a Bloom filter to do fast decision between looking in the in-memory tree or in the on-disk btree. reply wredue 14 hours agorootparentprevFFS. The fix for absolutely bonkers CoW costs is to stop buying in to this idiotic notion that “immutability is easier to reason about”.If you’re having difficulty reasoning about how to deal with major performance issues then your position is not easier to reason about. Full stop.Stop the madness! reply cryptonector 13 hours agorootparentIt is easy to reason about the performance issues of CoW, and it&#x27;s easy enough to reason about how to work around those issues. Ease of reasoning is a big deal when you&#x27;re dealing with hundreds of millions of lines of code, as some of us do. Cognitive load is a big deal in today&#x27;s world. It&#x27;s why we&#x27;re migrating from C to memory-safe languages. reply kragen 7 hours agorootparentprevit&#x27;s correct that aliasing makes performance harder to reason aboutbut there are other things you might want to reason about, such as whether a subroutine terminates at all and what it returns, and immutability does make it easier to reason about those things reply dragontamer 16 hours agoparentprev> except it generates a tremendous amount of garbage pagesNote that this is an advantage on say, controller-less NAND Flash (JFFS-like embedded NAND Flash).More modern Flash drives have embedded FTL (flash translation layers) that internalizes that garbage collection process. But *someone* had to write the FTL to begin with, and methinks this append-only btree would work very well for that.-------In NAND Flash, only 10,000ish erase&#x2F;write cycles are allowed before any block becomes unusable. (Depending on tech of course: could be 1000 on QLC could be 100k on SLC). All that garbage helps cycle the write&#x2F;erase cycles across the drive more evenly &#x2F; more consistently. Especially if you combine that garbage with TRIM-like commands.That might be a little bit too low level for a lot of folks though. reply epcoa 15 hours agorootparentThere are more appropriate and better data structures for an FTL especially with the assumption of some amount of auxiliary RAM (either host or on controller). Much of this literature is open access&#x2F;free.“All that garbage helps cycle the write&#x2F;erase cycles across the drive more evenly”Well no, you still want to minimize garbage production (and related GC overhead). Wear leveling doesn’t mean produce more garbage. reply dragontamer 15 hours agorootparent> Well no, you still want to minimize garbage production (and related GC overhead). Wear leveling doesn’t mean produce more garbage.Surely some % of garbage helps as you&#x27;re garbage collecting and reliably trying to shuffle data around to wear-level more evenly?Lets say you have 99% used data and 1% garbage. You have very little room for (static) wear leveling. Ex: If you&#x27;re writing 10MB of new data, and your drive is 99% full of allocated data... you&#x27;d have to move 1000MB of data around to \"find\" the 10MB of garbage, on the average.In the other extreme case: 0% data used and 100% garbage (say a TRIM operation just completed), then you simply just write 10MB without having to worry about moving any data around.The 50% data + 50% garbage scales as appropriate. 10MB of written new data will need you to find and coalesce 10MB of garbage to write the new data. This will happen after moving 20MB of overall data around.----------I&#x27;m oversimplifying of course. But even in a real life system, I&#x27;d expect that the more \"garbage\" you have sitting around, the better the various algorithms work for FTL &#x2F; SSD (static or dynamic) wear leveling. reply epcoa 14 hours agorootparentI can’t state any more clearly: minimize the production of garbage.“Surely some % of garbage helps as you&#x27;re garbage collecting ”Garbage that doesn’t exist doesn’t need collecting.The flaw here is confusing free space with garbage. You shouldn’t have written in the first place if you could have avoided it.Every environmentalist knows this: RRR, the first R is reduce not recycle. reply dragontamer 14 hours agorootparentAny append-only data-structure will have data that was true (when it was written), but has become false&#x2F;obsolete&#x2F;garbage at a later time. This is unavoidable.I&#x27;m not saying that we write needless garbage to the logs or filesystem or whatever. I&#x27;m saying that the amount of garbage in your stream that you leave behind aids in later steps of (static) wear-leveling. So therefore, its not a big deal. You&#x27;re going to be making plenty of this (initially true data, but later garbage data) as files get updated, moved around filesystems, or whatnot.\"Garbage\" in this sense is perhaps the wrong word. Its more like \"Obsolete data\", or \"outdated data\". reply ncruces 13 hours agorootparentIf you read the article though, many of the updated nodes (which are now garbage) don&#x27;t see any updates to their “data” but to internal tree pointers.So lots of “data” is being copied, and garbage is being generated, only for the benefit of tidying up tree structure, not because the actual “data” in those pages changed.Not generating such garbage in the first place is an obvious benefit. reply dragontamer 12 hours agorootparentI think I see what you mean now. Thanks. replythechao 15 hours agoparentprevI am immediately nerd-sniped by this. Is there any code out there you know of I can see? The dual meta-data pages sounds precisely like double-buffering (from graphics; my domain of expertise). I am also drawn to append-only-like and persistent data-structures. reply hoytech 14 hours agorootparentLMDB works like this. The MDB_env struct keeps pointers to both meta pages:https:&#x2F;&#x2F;github.com&#x2F;LMDB&#x2F;lmdb&#x2F;blob&#x2F;30288c72573ceac719627183f1...Which meta-page used is determined by the transaction ID (even IDs use first, odd IDs second).This is the earliest description I can find of the \"shadow paging\" concept: https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;320521.320540And I believe its first implementation was in IBM&#x27;s System R. reply ww520 10 hours agorootparentLMDB is the poster child of COW btree implementation. That paper is a good find. Thanks. reply codetrotter 16 hours agoparentprevThis description reminds me of a document I read about how ZFS is implemented. In particular, how snapshots work in ZFS, and what happens when you delete a snapshot. reply cryptonector 14 hours agorootparentIt&#x27;s a very similar idea.Traditional Unix-ish filesystems, with inodes and indirect nodes are a lot like b-trees, but ones where the indices are block numbers and where you can only append indices, trim, or replace indices.The ZIL (ZFS intent log) is a mechanism for amortizing the write magnification of copy on write trees. reply mamcx 14 hours agoparentprevNaively thinking here, how about a 2 immutable b+tree setup:The \"hot\" tree is the WAL: All the data is there and copies are generated.The \"cold\" tree is behind: In the background move from WAL and at the same time compact it. reply ww520 14 hours agorootparentThat&#x27;s how the traditional transaction log + btree approach work. The append-only btree removes the need for a separate transaction log. reply senderista 8 hours agorootparentI think you could view the append-only B-tree as a deamortized version of the traditional update-in-place B-tree + WAL. It&#x27;s true that it eliminates the problem of maintaining a separate WAL, but only by trading it for an arguably harder problem: compacting the B-tree. It&#x27;s easy and cheap to truncate the WAL; it&#x27;s difficult and expensive to compact the B-tree. I guess LMDB solves this problem by only updating at page-level granularity so it can reuse entire pages without compaction, although I haven&#x27;t studied its design in much detail. reply ww520 4 hours agorootparentIf by compacting the tree you meant compacting the space left over by the users deleting the records, it&#x27;s about the same amount of work between the two approaches. The WAL+btree case needs to merge the near empty pages and copy the remaining records while the append-only btree case needs to allocate new pages to merge in the near empty pages and fix up the parent path which can be done as a batching garbage collection phase.If by compacting you meant cleaning up the garbage pages, while the WAL+btree is simpler in truncating the WAL, the append-only btree is pretty easy. It&#x27;s just doing upkeep on the free-page list.There&#x27;re only three places to pay attention: 1. any page touched by a transaction (read&#x2F;write) is added to the in-use list. 2. When a transaction closes, removes its touched pages from the in-use list by decrementing their in-use counters. 3. When a write transaction commits, adds all the old overwritten pages to a pending-delete list. Periodically check the pending-delete list against the in-use list and any page not in use is moved to the deleted-queue. When the deleted-queue reaches a large enough batch, create a new free-page to contain the pointers of the deleted pages from the queue. Chain up to the existing free-page list in the meta page by storing the pointer to the existing head of list in the new free-page. Append the new free-page to the db file. Update the new free-page as the new head of the free-page list in the meta page. That&#x27;s it. reply senderista 15 hours agoparentprevOf course, this implies a single-writer design (not necessarily a bad thing!). reply ww520 15 hours agorootparentActually immutable btree has a single writer in general since there’s no translation log and the meta page pointing to one latest tree root. Even if two write transactions updating different parts of the tree, they both need to update the same meta page, causing a contention. This is one downside (or upside depending on your need since single writer simplifies things a lot and great for sequential writes). reply hoytech 17 hours agoprevLMDB [0] was derived from this project, as mentioned in its copyright notice section [1].[0] http:&#x2F;&#x2F;www.lmdb.tech&#x2F;doc&#x2F; [1] https:&#x2F;&#x2F;github.com&#x2F;LMDB&#x2F;lmdb&#x2F;blob&#x2F;30288c72573ceac719627183f1... reply jnwatson 17 hours agoparentImportantly, the LMDB API allows append mode, and it is very fast. reply btilly 18 hours agoprevOne of the advantages of this kind of architecture is that if one reader is reading the old tree while it is replaced, it just works. Databases are really good at having multiple versions of the same data structure be accessible at the same time.I hand rolled some similar data structures in higher order languages when I couldn&#x27;t find any Python packages that gave me the same capability. But I couldn&#x27;t figure out what a good name would be for, say, a dictionary that could have multiple concurrent versions. So I never went anything where with that. reply j-pb 16 hours agoparentThese things are usually called \"persistent\" versions of the thing, e.g. persistent Dictionary. Sometimes \"immutably persistent\" to distinguish it from the \"durable\" meaning of persistence i.e. written to disk. reply btilly 15 hours agorootparentIt seems odd to me to call a data structure \"persistent\" when its purpose is to allow you to cheaply keep many similar transitory copies around until you throw them away.The specific application was using dynamic programming to build up a complex data structure. You wind up with many copies of very similar data structures, and doing a deep copy each time is prohibitively expensive. reply ncruces 13 hours agorootparentStill that&#x27;s what they&#x27;re called: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Persistent_data_structurePrecisely because previous versions remain valid (persist) under modification. reply btilly 13 hours agorootparentThere is a key difference between that, and what I created.The difference is that persistent data structures allow you to traverse the history of the data structure to find past versions. By contrast I only allowed you to see the current version, returning a new version of the root any time you made a modification. As a result, the memory associated with the old version could be freed once nothing would want to access it again. And any version that you had could still be manipulated.For the example that I was dealing with, both made sense. On top of that it made it possible to create a hashable version of the data structure that had a good chance (not a guarantee) of matching hashes when you arrived at the same data structure through different histories. reply kragen 1 hour agorootparentthe data structures people normally call &#x27;persistent&#x27; behave like what you implemented; they&#x27;re ubiquitous in languages like clojure, haskell, and even ocaml that privilege immutability. the map module in ocaml&#x27;s standard library is an example, and so is almost everything built in to clojure. &#x27;traverse the history of the data structure to find past versions&#x27; is not how these persistent data structures normally workthere is an unfortunate terminology clash with &#x27;persistent&#x27; in the sense of &#x27;not vanishing after a power cycle&#x27;, so i typically use the term &#x27;fp-persistent&#x27;, because this sense of &#x27;persistent&#x27; is associated with functional programming. this has the disadvantage that it&#x27;s a term i made up, so nobody knows what i mean until i explain it reply j-pb 2 hours agorootparentprevnext [–]The difference is that persistent data structures allow you to traverse the history of the data structure to find past versions.That capabilities is not a required property of persisten datastructures. The copy on write and collect the unique parts when a root is freed semantic that you describe is exactly the common behaviour of persistent data-structures in the wild.Libraries like Rusts \"im\", even do some nifty optimisations where they combine the borrow checker with reference counting to only perform copy on write when the reference you have is non-unique.So based on your description you build a path-copying persistent data-structure.I&#x27;d recommend this book if you want to compare your work with the state of the art: https:&#x2F;&#x2F;books.google.de&#x2F;books&#x2F;about&#x2F;Purely_Functional_Data_S... reply ncruces 11 hours agorootparentprevWell, the difference isn&#x27;t that great. You wrote: “the memory associated with the old version could be freed once nothing would want to access it again.” So all it really takes is something wanting. replyHnrobert42 18 hours agoparentprevAs Phil Karlton, “There are only two hard things in computer science: cache validation and naming things.” Seems like you found a way to work on both!https:&#x2F;&#x2F;skeptics.stackexchange.com&#x2F;questions&#x2F;19836&#x2F;has-phil-... reply timeimp 18 hours agorootparentI always have to add the \"and off-by-one\" at the end.Nothing like reading memory you don&#x27;t own! reply airstrike 17 hours agorootparentThere are actually only two hard problems in computer science:0) Cache invalidation1) Naming things5) Asynchronous callbacks2) Off-by-one errors3) Scope creep6) Bounds checking reply Scarblac 15 hours agorootparentThere is only one hard problem in software engineering: people. reply alternative_a 16 hours agorootparentprevLet’s be pedantic here and get all religious over the words “Science” and “hard”.Computer “science” has a difficult conceptual problem with caching. The optimal cache, this science tells us, is indistinguishable from a fortune teller who is never wrong (oracle). Fortune telling is a “hard” problem for a science based on reasoning. The best we can do is hedge bets (which is what the science of caching focuses on).This same science also has a difficulty with naming things. Now numbering things is easy and science loves maths and maths love sciences, but science and letters have a more difficult history. Science would approve of “factoryfactoryfactoryImpl” btw ... it’s “a rational scheme of naming”. .Here we see a “science” that is facing actual difficulties.The rest of your list are difficult but not “hard”. The science of these matters is clear and the rest is up to the “scientists” struggling with “scope creep” and “bounds checking” .. reply pmarreck 14 hours agorootparentprevinvalidation? reply seunosewa 18 hours agoprevAppend-only struxtures are not efficient enough for general use. You&#x27;re paying a steep price for creating a snapshot of the data for every single operation.I saw this when I was playing with Scala&#x27;s immutable and mutable data structures - written by the same team - ages ago. The immutable structures were much slower for common operations.The fastest databases tend to use undo logs to re-construct snapshots when they are needed. reply scottlamb 17 hours agoparentYou can build real useful systems on them. For example, Gmail used to be implemented on top of GFS as two append-only files per user: the zlog and the index. The zlog held (zlib-compressed) operations such as \"add this message\" and \"delete this message\". The index was an append-only btree as described here. It was a true index in that it didn&#x27;t include the actual message contents. Those were simply pointers into the log. Thus, it was much smaller than the zlog and its efficiency was less of a concern. Also, periodically both files were \"compacted\" by writing a fresh log (that built the current state in the most efficient way) and matching index. This was primarily for privacy (otherwise your deleted messages would never actually go away) but also improved efficiency.Gmail&#x27;s now built on top of Spanner so uses a log-structured merge tree. Still append-only files but a bit different. Files aren&#x27;t per user but per some arbitrary key range boundary; no more btree; multiple layers with the top ones getting compacted more frequently; etc. reply pclmulqdq 17 hours agorootparentIt&#x27;s worth noting that Google&#x27;s internal scale-out filesystem is append-only (for various distributed systems and operational reasons), so you end up with append-only data structures proliferating in that ecosystem. That does not necessarily mean that the append-only data structures were chosen for any technical merit other than ability to be implemented on the append-only filesystem. reply btilly 16 hours agorootparentSo if we ignore the technical merits based on which append-only data structures were chosen in general, we don&#x27;t necessarily have additional technical merits for any particular time append-only was chosen.The reasons why append-only structures were chosen in general apply surprisingly often to any particular system that scales to large data, which would like to be robust to a wide variety of real world problems. You won&#x27;t see it in looking at the data structure because the hard parts are abstracted away from you. But you&#x27;ll see it if you try to reimplement the same system from scratch, then scale it up to production. reply _factor 9 hours agorootparentDon&#x27;t forget the security implications. If only root can run the compression&#x2F;deletion script, all the compromised user can do is try to write to theirs. If you get into writing other users&#x27; data, that sucks, but nothing is deleted or exfiltrated, and the log is baked in. Break into root, well good luck with that on any system. reply alternative_a 16 hours agorootparentprevThe subtle point I read in GP is the historic correlation between storage systems and data structures. Your point is equally valid in that this correlation is not indicative of non-general applicability.Both points need to be kept in mind imho in design. reply scottlamb 15 hours agorootparentprevGood point. It&#x27;s also worth noting that raw flash also is generally treated as roughly append-only for wear leveling. \"General-purpose\" may be in the eye of the beholder, but I&#x27;d say these append-only data structures are useful in at least two significant environments. reply pclmulqdq 2 hours agorootparentWear leveling generally happens well below the user-level filesystem (and is quite complicated!), and altering your user-level behavior because you think it helps is a little bit silly. Zoned NVMe is an obvious exception to this, where the FTL takes advantage of the append-only zones (even that is an abstraction only shown to filesystems), but it will frequently remap your blocks if you do a lot of read-modify-writes to keep the wear even. reply lloydatkinson 16 hours agorootparentprevWhat you described is called event sourcing reply slashdev 17 hours agoparentprevLMDB is a strong counter-example to your argument. Based on its benchmarks, you&#x27;re wrong. [1]Yes immutable in-memory data structures are slower than their mutable counterpoints, but we&#x27;re not talking about either of those here.Databases are not in-memory data structures.[1] http:&#x2F;&#x2F;www.lmdb.tech&#x2F;bench&#x2F;microbench&#x2F;july&#x2F; reply londons_explore 18 hours agoparentprevI don&#x27;t think this is generally true.Especially on mediums where sequentially writing large blocks is faster than random writes, you get much better performance to use a log-structured datastructure and put anything new&#x2F;any changes at the end.Due to the design of modern SSD&#x27;s, &#x27;write in place&#x27; is pretty much an illusion. reply joshlemer 12 hours agoparentprevScala&#x27;s immutable data structures, and especially the way they are used idiomatically and most obviously, have a lot of performance issues that are much bigger than the basic cost of persistent data structures. Namely, most commonly I will see developers default to pipelining strict operations like myList.map(...).filter(...).flatMap(...).collect(...).take(2) which forces&#x2F;materializes the whole collection in memory for every operation. Better would be to first transform the list into a lazy view or iterator with myList.view or myList.iterator, and then do the pipeline.They also lack the ability to perform multiple updates in a batch, except for some very limited cases. Other implementations like Clojure&#x27;s support \"transients\" where you get access to mutate the data structure over and over again (as well as do reads), and then freeze the structure in place as a new persistent collection. JavaScript libraries like immer allow for the same thing. Scala&#x27;s collections don&#x27;t generally support this except in the form of \"builders\" which don&#x27;t support reads and also don&#x27;t support all the write access patterns (such as updating a vector at a specific index, or removing a key from a map&#x2F;set, etc). reply vlovich123 18 hours agoparentprevA mutable approach requires a write ahead log meaning you have to copy the data twice on every write which seems worse. reply iambvk 12 hours agorootparentIt is: two writes for write ahead log vs. log-n (tree-height) writes for CoW reply vajrabum 18 hours agorootparentprevIn a well designed database system (hardware!) you aren&#x27;t going to use the same CPU, disk or controller to write the log and the data structure. So the performance penalty if any is going to be low. reply vlovich123 15 hours agorootparentLow but non 0 and you’re still duplicating the values written to disk. So your SSD lifetime is decreased and this is effectively a RAID0 layout which means data loss is risky too. reply loeg 17 hours agorootparentprevNow you’re paying for double the hardware. reply halayli 17 hours agoparentprevYou&#x27;re comparing apples and oranges. Anytime you write&#x2F;modify on an SSD you&#x27;re writing on a new page. So you might as well leverage this behavior to your advantage. It has several benefits when it comes to implementing transactions. reply cryptonector 14 hours agoparentprevThe biggest cost to copy-on-write data structures is write magnification. Adding a log to amortize that cost helps a lot. That&#x27;s what the ZFS intent log does. Any CoW b-tree scheme can benefit from that approach.The benefits of CoW data structures are tremendous: - easy to reason about - easy to multi-thread for reading - O(1) snashopts (and clones)The downsides of CoW data structures are mainly: - the need to amortize write magnification - difficulty in multi-threading writes reply hinkley 14 hours agoparentprevYou probably need something special for batch operations. Adding five records in one transaction shouldn’t pay >= 5 times as much as a single one.You need a different API for something like that however. reply joshlemer 12 hours agorootparentYes, for instance Clojure&#x27;s \"transients\" as well as the JavaScript \"immer\" library reply valenterry 17 hours agoparentprevThat usually happens when you try to work with immutable data structures like you are used to with mutable datastructures.For example, if you append to a mutable list then it&#x27;s going to be fast. But prepending to it is much slower. With immutable lists, it&#x27;s the other way around. Not knowing that will make you think that immutable datastructures are generally slow, but they are not.That being said, I would say they are generally more tricky, so it&#x27;s good to understand in which cases it&#x27;s worth to sacrifize safety and readability and switch to mutable datastructures for performance reasons. reply teo_zero 16 hours agoparentprevIF reads dominate writes, and multiple threads read the data concurrently, this approach may win by eliminating the need for locks. reply bob1029 17 hours agoprevSome years ago I was playing around with the idea of an append-only splay tree for storing a database file. The thinking was that the algorithm would keep recently-accessed items at the business end of the log (in addition to any new&#x2F;updated&#x2F;deleted items).This concept is clearly quite heavy on writes, but the tradeoff is that you would then have the ability to expire unused entries simply by picking an offset in the log and chopping everything that precedes it. Any record accessed more recently than that cutoff point would be guaranteed to have been written one or more times later on in the log.Any access would result in a new modified tree & root node being written, but the prototype did batch IO using an MPSC queue abstraction which meant that I could amortize things a bit. Multiple transactions could fit in a single IO command if they are issued from different threads, occur within a small slice of time and are smaller than the block size. reply o11c 12 hours agoprevOne thing that&#x27;s not explicitly mentioned: append-only trees necessarily lack `parent` pointers.This means that your \"iterator\" can&#x27;t be a lightweight type, it has to contain an array of parent nodes to visit (again) later. You can use a fixed-size array if you can reason about the maximum height of the tree (for a balanced binary tree, this means around `2*POINTER_BITS`, which is 1024 bytes on a 64-bit platform; it will be less for a B-tree (`2*log(POINTER_MAX, CHILDREN_PER_NODE)`) but you now have to either track or re-scan for the index of the child you just returned from). Beware buffer overflow logic bugs if your tree isn&#x27;t as balanced as you thought! reply hlship 14 hours agoprevIn such a system, how does a reader find the root node? I&#x27;d be concerned about a) readers seeing a partial write to disk (or page buffer) during an update or b) a crash while the writer writes to disk.I could imagine using two files, one containing the actual b-tree, the second containing the offset to the latest root note; the second file gets overwritten only after a successful write is verifiably written to disk.Datomic&#x27;s (https:&#x2F;&#x2F;www.datomic.com&#x2F;) architecture is similar to this, but uses many write-once \"segments\" (which could be files in EFS or S3, or rows in DynamoDB or other stores). reply ww520 13 hours agoparentThe pointer to the root tree node is stored in the last committed metadata page. A read transaction starts with the reading of the metadata page. Reaching the partial written pages is impossible as the writer has not committed the latest metadata page yet. The transaction is committed when the latest metadata page is written as the last step. reply twoodfin 14 hours agoparentprevThere’s a “canonical” pointer to the root node which is updated atomically on every append.For an in-memory database, a CAS is adequate. Persistent stores ultimately need some kind of file-level locking.If you look at the Apache Iceberg spec, you get a good idea of how this works: The only “mutability” in that universe is the root table pointer in the catalog. reply paulsutter 16 hours agoprevEvery historical version of the database is available as a snapshot. The metadata block should point back to previous meta block(s), not just the new rootAnd of course emphasizing the closing statement:> there is no need for a transaction log, because the database file is the transaction log reply zogomoox 17 hours agoprevInstead of cloning the branches all the way up to root, wouldn&#x27;t it be more efficient to just write delta-records, so a structure per inner node that says \"it&#x27;s like this original node, but with the following modifications x,y,z\" reply btilly 17 hours agoparentEfficient in what way?Yes, a delta record would be smaller. But now you have to fetch both the new and the old, plus do computation to figure out what you have. This is trading off space and time.I&#x27;m going to go to a reasonably low level to explain this.Databases have generally found that the right tradeoff between space and time is to always work in terms of pages of fixed size. Now all reads are memory aligned, of known size. All writes are as well. And inside the CPU, processing a page in the most straightforward and predictable way possible is very fast. In particular you want to avoid complex logic that introduces too many pipeline stalls.If you&#x27;re going to work with pages anyways, you want to find ways to fetch as few pages as possible. Only have this page point to that page where you really need to. And put as much as reasonable on each page. The name of the game is to fetch as few pages as you need, and get everything you need from a page when you fetch it. Because when you&#x27;re getting a new page, often you have to wait for a disk read. That&#x27;s slow. It used to be really slow, you needed to wait for the right part of the disk to rotate around. Those disks went at something like 7200 rpm, but that means 120 revolutions per second, which means you&#x27;re waiting for anywhere from 0 to 8.333... milliseconds for the read. Now consider reading through a million record database...That&#x27;s where a BTree comes in. It is a tree structure built around a page layout. When a page gets too full, it is split and one record is promoted to the level above. When the top page gets too full, a new level is created at top. That keeps it perfectly balanced at all times. So you can get to any record in very few reads. And if you&#x27;re walking the whole data structure, a single page generally has many records. reply PartiallyTyped 17 hours agoparentprevYou may want to rebalance a tree, or you may want to have multiple readers while a writer exists without relying on mutexes or locking. reply zogomoox 17 hours agorootparentas long as you don&#x27;t modify the original nodes that would still work, no locks needed. reply packetlost 16 hours agoprevOne enhancement I&#x27;ve toyed with in the past is to have subtrees beneath each leaf node for the historical data for each record. You switch the indexing value to the timestamp of record insertion time or a TX number and keep inserting in append-only fashion. There&#x27;s some enhancements you can make because the common case is for the key to be a monotonically increasing number, but nothing is strictly necessary. I&#x27;d love to build a datalog system on top of something like that at some point, even if it&#x27;s just for fun. reply toolslive 13 hours agoprevwith append-only data structures, you could turn the fact that you rewrite the path from the node to the root to your advantage and rebalance. This means you abandon the \"balanced\" property and use update statistics to get an optimal shape. Who cares about the length of paths to parts of the tree you never visit? reply kragen 1 hour agoparentthis is how splay trees work, but you might actually care about the lengths of paths to parts of the tree you rarely visit; for many real systems, worst-case response time is more important than throughput reply JensRantil 13 hours agoprev [–] IIRC, this is how CouchDB was implemented. The benefit is that it was resilient to crash without needing a write-ahead log. The downside was that it required running background compaction of the B+tree regularly. reply josephg 13 hours agoparent [–] LMDB works the same way. But it also stores a second data structure on disk listing all free blocks. Writes preferentially take free blocks when they’re available. The result is it doesn’t need any special compaction step. It sort of automatically compacts constantly. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The append-only b+tree is a data structure commonly used for efficient storage and retrieval of data.",
      "It is made up of branch and leaf pages, with the leaf pages containing the actual data and keys.",
      "Rather than supporting leaf chaining for sequential access, each update involves appending a new page to the file and updating the parent pages, ensuring a consistent snapshot of the database without the need for a transaction log. This approach is more efficient for writing consecutive pages to disk, but may waste disk space."
    ],
    "commentSummary": [
      "Various techniques and approaches are explored to optimize write amplification, reduce garbage data, and improve the efficiency of different tree structures.",
      "The use of append-only btrees, CoW data structures, and log-structured approaches in systems like ZFS and LMDB are discussed.",
      "The concept of persistent data structures and their benefits in functional programming is highlighted, along with the challenges and trade-offs involved in choosing these data structures. The importance of optimizing disk reads and maximizing page efficiency in databases is also emphasized."
    ],
    "points": 196,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1703860706
  },
  {
    "id": 38810073,
    "title": "Koka: Advanced Functional Language with Effect Types and Handlers",
    "originLink": "https://koka-lang.github.io/koka/doc/index.html",
    "originBody": "Koka A Functional Language with Effect Types and Handlers // A generator effect with one operation effect yield fun yield( x : a ) : () // Traverse a list and yield the elements fun traverse( xs : list ) : yield () match xs Cons(x,xx) -> { yield(x); traverse(xx) } Nil -> () fun main() : console () with fun yield(i : int) println(\"yielded \" ++ i.show) [1,2,3].traverse Welcome to Koka – a strongly typed functional-style language with effect types and handlers. Install Get Started Documentation Github Note: Koka v2 is a research language that is currently under development and not ready for production use. Nevertheless, the language is stable and the compiler implements the full specification. The main things lacking at the moment are libraries, package management, and deep IDE integration. News: 2023-07-03: Koka v2.4.2 released: add support for fip and fbip keywords described in “FP2: Fully in-Place Functional Programming” (ICFP'23) [pdf]. Various fixes and performance improvements. 2021-02-04 (pinned) The Context Free youtube channel posted a short and fun video about effects in Koka (and 12 (!) other languages). 2021-09-01 (pinned) The ICFP'21 tutorial “Programming with Effect Handlers and FBIP in Koka” is now available on youtube. 2022-02-07: Koka v2.4.0 released: improved specialization and int operations, add rbtree-fbip sample, improve grammar (pub (instead of public, remove private (as everything is private by default now)), final ctl (instead of brk), underscores in number literals, etc), rename double to float64, various bug fixes. 2021-12-27: Koka v2.3.8 released: improved int performance, various bug fixes, update wasm backend, initial conan support, fix js backend. 2021-11-26: Koka v2.3.6 released: maybe-like types are already value types, but now also no longer need heap allocation if not nested (and [Just(1)] uses the same heap space as [1]), improved atomic refcounting (by Anton Lorenzen), improved specialization (by Steven Fontanella), various small fixes, add std/os/readline, fix build on freeBSD 2021-10-15: Koka v2.3.2 released, with initial wasm support (use --target=wasm, and install emscripten and wasmtime), improved reuse specialization (by Anton Lorenzen), and various bug fixes. 2021-09-29: Koka v2.3.1 released, with improved TRMC optimizations, and improved reuse (the rbtree benchmark is as fast as C++ now), and faster effect operations. Experimental: allow elision of -> in anonymous function expressions (e.g. xs.map( fn(x) x + 1 )) and operation clauses. Command line options changed a bit with .koka as the standard output directory. 2021-09-20: Koka v2.3.0 released, with new brace elision and if/match conditions without parenthesis. Updated the javascript backend using ES6 modules and BigInt. new module std/num/int64, improved effect operation performance. 2021-09-05: Koka v2.2.1 released, with initial parallel tasks, the binary-trees benchmark, and brace elision. 2021-08-26: Koka v2.2.0 released, improved simplification (by Rashika B), cross-module specialization (Steven Fontanella), and borrowing annotations with improved reuse analysis (Anton Lorenzen). 2021-08-26: At 12:30 EST was the live Koka tutorial at ICFP'21, see it on youtube. 2021-08-23: “Generalized Evidence Passing for Effect Handlers”, by Ningning Xie and Daan Leijen presented at ICFP'21. See it on youtube or read the paper. 2021-08-22: “First-class Named Effect Handlers”, by Youyou Cong, Ningning Xie, and Daan Leijen presented at HOPE'21. See it on youtube or read the paper. 2021-06-23: Koka v2.1.9 released, initial cross-module specialization (by Steven Fontanella). 2021-06-17: Koka v2.1.8 released, initial Apple M1 support. The Perceus paper won a distinguished paper award at PLDI'21! 2021-06-10: Koka v2.1.6 released. 2021-05-31: Koka v2.1.4 released. 2021-05-01: Koka v2.1.2 released. 2021-03-08: Koka v2.1.1 released. 2021-02-14: Koka v2.0.16 released. 2020-12-12: Koka v2.0.14 released. 2020-12-02: Koka v2.0.12 released. 2020-11-29: Perceus technical report publised (pdf). Why Koka? Minimal but General The core of Koka consists of a small set of well-studied language features, like first-class functions, a polymorphic type- and effect system, algebraic data types, and effect handlers. Each of these is composable and avoid the addition of “special” extensions by being as general as possible. Effect Types Koka tracks the (side) effects of every function in its type, where pure and effectful computations are distinguished. The precise effect typing gives Koka rock-solid semantics backed by well-studied category theory, which makes Koka particularly easy to reason about for both humans and compilers. Effect Handlers Effect handlers let you define advanced control abstractions, like exceptions, async/await, iterators, parsers, ambient state, or probabilistic programs, as a user library in a typed and composable way. Perceus Reference Counting Perceus is an advanced compilation method for reference counting. This lets Koka compile directly to C code without needing a garbage collector or runtime system! This also gives Koka excellent performance in practice. Reuse Analysis Perceus also enables reuse analysis and lets Koka optimize functional-style programs to use in-place updates when possible. Learn more Read more about these core concepts Install On Windows (x64), open a cmd prompt and use: curl -sSL -o %tmp%\\install-koka.bat https://github.com/koka-lang/koka/releases/latest/download/install.bat && %tmp%\\install-koka.bat On Linux (x64) and macOS (x64, arm64 (M1/M2)), you can install Koka using: curl -sSL https://github.com/koka-lang/koka/releases/latest/download/install.shsh (If you previously installed Koka on macOS using brew, do an brew uninstall koka first). On other platforms it is usually easy to build Koka from source instead. After installation, verify if Koka installed correctly: $ koka _ _||| _ ___| _ __ _|/ / _ \\| |/ / _'welcome to the koka interactive compiler( (_)( (_|version 2.4.0, Feb 7 2022, libc x64 (gcc) |_|\\_\\___/|_|\\_\\__,_| type :? for help, and :q to quit loading: std/core loading: std/core/types loading: std/core/hnd > Type :q to exit the interactive environment. For detailed installation instructions and other platforms see the releases page. It is also straightforward to build the compiler from source. Get started with the compiler",
    "commentLink": "https://news.ycombinator.com/item?id=38810073",
    "commentBody": "Koka: Strongly typed functional-style language with effect types and handlersHacker NewspastloginKoka: Strongly typed functional-style language with effect types and handlers (koka-lang.github.io) 163 points by nateb2022 13 hours ago| hidepastfavorite45 comments dang 11 hours agoRelated. Others?Koka: A fast functional programming language with algebraic effects - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38421003 - Nov 2023 (2 comments)The Koka Programming Language - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28335043 - Aug 2021 (2 comments)Koka: A Functional Language with Effects - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=27710267 - July 2021 (12 comments)A Tour of Koka (an elegant programming language with Algebraic Effects) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26292411 - Feb 2021 (1 comment)An Introduction to the Koka Programming Language - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=14647415 - June 2017 (1 comment)Koka – A function-oriented programming language - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=10131071 - Aug 2015 (10 comments)Koka a function oriented language with effect inference - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=4407415 - Aug 2012 (1 comment) reply anfelor 8 hours agoparentSome of the research behind Koka was discussed at:FP2: Fully In-Place Functional Programming [pdf] - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36471591 - July 2023 (24 comments)Perceus: Garbage Free Reference Counting with Reuse [pdf] - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25464354 - Dec 2020 (44 comments)Implementing Algebraic Effects in C - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=14887341 - July 2017 (21 comments) reply nerdponx 9 hours agoparentprevNice to finally see this get some attention on the front page. reply codeflo 2 hours agoprevInterestingly, this seems to be sponsored by Microsoft Research, which has also heavily supported the development of and research on Haskell.(Source: https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;project&#x2F;koka&#x2F;) reply ReleaseCandidat 14 minutes agoparentAnd Lean (which also uses Perceus RC) too: https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;project&#x2F;lean&#x2F; reply ed_blackburn 1 hour agoparentprevDid they not effectively acqui-hire or plain hire the pony lang team too? reply djha-skin 4 hours agoprev> Koka tracks the (side) effects of every function in its type, where pure and effectful computations are distinguished. The precise effect typing gives Koka rock-solid semantics backed by well-studied category theory, which makes Koka particularly easy to reason about for both humans and compilers.Pretty comical to hear the words \"easy to reason about\" and \"category theory\" in the same sentence.With apologies to the Haskellers, any time \"category theory\" is mentioned I feel myself shying away, prior experience teaching me that those words mean \"you will spend the majority of your time working around the type system \"; and, \"we have more data types than individual bits of data that those types describe\".A little type system goes a long way, and there&#x27;s such a thing as too much in my opinion.I was initially interested because of algebraic effects in the language because I&#x27;m told they&#x27;re basically the same as common lisp conditions. I really liked learning about conditions and I wish they were in more languages. I must confess I am less interested now. reply galaxyLogic 4 minutes agoparent> A little type system goes a long way, and there&#x27;s such a thing as too much in my opinion.Agreed.The purpose of \"types\" is to make the program easier to understand, because the invariants expressed as type-declarations hold at all times of the program execution. That makes it EASIER to reason about the program, in other words makes it easier to understand your program and what it is doing, by understanding what it cannot be doing, meaning violating its type-constraints.But now IF the type-declarations-language becomes highly advanced and thus complex and difficult to understand, that potentially makes your program more DIFFICULT to understand.So it&#x27;s good to keep the purpose of type-systems in mind while thinking about their benefits. We declare them only so that we and others can better understand what our programs are doing.Of course if the program is small, it is typically easy to understand and may be easier to understand without them. reply yawaramin 4 hours agoparentprevCategory theory is not the headline news here, it&#x27;s safety without sacrificing performance by using a new technique to compile to binaries without a runtime or garbage collector, with potentially better deallocation performance than Rust, and functional-looking code like `array.map(func)` which performs as fast as a for-loop thanks to advanced analysis. reply galaxyLogic 10 minutes agorootparentSo this means we have benefits of Rust but in a nicer, functional syntax? reply djtango 4 hours agorootparentprevthanks for this, it wasn&#x27;t immediately obvious to me why the headline example was traversing a list but doing that fast without GC is cool reply nesarkvechnep 2 hours agoparentprevAlgebraic effects are not basically the same as Common Lisp conditions. reply jitl 11 hours agoprev> Effect handlers let you define advanced control abstractions, like exceptions, async&#x2F;await, iterators, parsers, ambient state, or probabilistic programs, as a user library in a typed and composable way.> Perceus is an advanced compilation method for reference counting. This lets Koka compile directly to C code without needing a garbage collector or runtime system! This also gives Koka excellent performance in practice.Effectful functional language that compiles to C? Sounds great. reply nerdponx 9 hours agoparentI&#x27;ve only played around with it a little bit, but it really is a cool language.I particularly enjoyed this presentation, which is what sold it to me as a good idea worth spending some time on: https:&#x2F;&#x2F;youtu.be&#x2F;6OFhD_mHtKAOCaml 5.0 also interestingly includes an effect handler system: https:&#x2F;&#x2F;v2.ocaml.org&#x2F;manual&#x2F;effects.html reply jitl 9 hours agorootparentI’ve been following the road to OCaml 5 and the effect system is nice, but getting to have effects without bringing in a garbage collector seems quite special. I’m working on a C-WASM wrapper library, and one issue I struggle with in C code is dealing with async IO outside the WASM module. I haven’t looked deep into the FFI story of this new language but it would directly solve this problem that currently needs the ASYNCIFY Binaryen transform which adds a 2x size increase and 50% performance penalty. Can Koka call C in an effect? Can it suspend a stack if C calls into it? I can presumably drop koka into my existing C-WASM project with a few new Makefile rules. These are definitely places OCaml can’t go; by compiling to C Koka makes these advanced FP features much more attainable (at least, for me). reply naasking 7 hours agorootparentThey have a C library for writing effect handlers in C, so you could potentially just use that directly:https:&#x2F;&#x2F;koka-lang.github.io&#x2F;nodec&#x2F;api&#x2F;group__effect.htmlOn GitHub:https:&#x2F;&#x2F;github.com&#x2F;koka-lang&#x2F;libhandlerI believe it grew out of this prior work:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=14887341 reply phoe-krk 10 hours agoparentprevAlso, as a Lisp programmer, I absolutely loved reading the section at https:&#x2F;&#x2F;koka-lang.github.io&#x2F;koka&#x2F;doc&#x2F;book.html#sec-handlers - it&#x27;s essentially like using the Common Lisp condition system, making me feel at home, except it&#x27;s more generalized (e.g. masking individual condition handlers is not generally possible in CL) and also working in a strongly statically typed environment. reply kazinator 8 hours agorootparentIn CL, you can control the visibility of restarts. In the restart-bind construct, there is a :test-function which serves as a predicate that determines whether the restart is visible. (Unfortunately, this function is referred to as a condition).While restarts can be visible or not, I believe there is no such mechanism for handlers. However, handlers can be effectively invisible&#x2F;transparent by declining to handle a condition, which they can do simply by returning instead of perpetrating a non-local transfer.Also, there doesn&#x27;t seem to be an API in Common Lisp for calculating the handlers visible at a given point for a condition of a given type. So that means that the ability of a handler to decline a condition is pretty much as good as a visibility mechanism. reply nerdponx 9 hours agorootparentprevAs far as I understand the PL theory involved, it&#x27;s statically-typed delimited continuation, which I think is even more general than the CL condition system. reply mrobot 1 hour agoprevI was confused because the reference counting in the \"Why Koka\" part (section 2) of the book [1] seemed mismatched, so i looked it up in their reference counting TR [2]. It turns out it uses a seemingly novel approach to reference counting where any function you pass a reference to is responsible for decrementing and possibly freeing that reference. If you need to pass a reference to two functions you have to dup it once.This makes it possible for fold to free all the Cons cells as it is mapping over it. The reuse analysis is cool, too, with in-place updates of structures that won&#x27;t be referenced again.[1] https:&#x2F;&#x2F;koka-lang.github.io&#x2F;koka&#x2F;doc&#x2F;book.html [2] https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;publication&#x2F;perceus... (see section 2.2) reply satvikpendem 9 hours agoprevFor those who&#x27;ve used effectful languages, how much of a spaghetti code mess do they create, potentially? That&#x27;s basically my only worry with effects that can be suspended and resumed at basically any point and any location in the codebase. reply naasking 6 hours agoparentPreemptive multitasking is an effect handler implemented by the operating system. There are very few instances where you have to worry about process switching by the OS, basically only when you have a handle to a resource outside of your process.A similar constraint will apply to algebraic effects, which is why they&#x27;re algebraic: only if your code directly depends on some effect will you have to care about effects. reply satvikpendem 6 hours agorootparentI don&#x27;t quite understand, let&#x27;s say I have a bunch of effects in my code, all depending on one another. Will that make a mess? If so, how does one avoid that, or is that an inherent issue in algebraic effects systems? reply nyssos 4 hours agorootparent> I don&#x27;t quite understand, let&#x27;s say I have a bunch of effects in my code, all depending on one another. Will that make a mess?It depends entirely on what those effects and dependencies actually are. If you really want to, you can use an effect system as a dynamically scoped imperative language, just like you can write all your Haskell code in `IO` or use exceptions for control-flow in C++. The value proposition is that you have to do it on purpose. reply nerdponx 6 hours agorootparentprevYou mean a situation where the effect handler for effect A invokes effect B? I wonder if the compiler is able to detect and prevent \"circular\" event invocations. reply solidsnack9000 7 hours agoprevSuch a thought provoking language. Docs are a good read, and reach a standard that few production languages come close to. reply scns 8 hours agoprevDot Selection looks neat, no pipeline operator needed. Enables extension functions a la Kotlin too. reply yawaramin 4 hours agoparentAlso known as Uniform Function Call Syntax (UFCS). reply codethief 6 hours agoprevDoes anyone here have any experience with Koka? I&#x27;ve been following the language from afar for a while but I have yet to read an account of what it&#x27;s like to use it in practice. reply helix278 59 minutes agoparentI&#x27;ve tried using it a bit. It works and it is fun to use, but there are quite a number of rough edges. I wouldn&#x27;t use it in production yet, I don&#x27;t think they are conservative there. reply xigoi 1 hour agoparentprevLast time I tried to use it (which was quite long ago), it barely had a print function, making it pretty much unusable. reply hamandcheese 5 hours agoparentprevIt is self-proclaimed as a \"research Language\" on the homepage, which has kept me away so far. reply Hugsun 4 hours agorootparentI share this sentiment, although I have wondered if they&#x27;re being overly conservative with this statement. reply pharmakom 11 hours agoprevWhat are the practical advantages of Koka over say “IO a” or “Async” in other languages? reply spicebox 10 hours agoparentMonads don’t compose, effects do. ‘IO a’ works great until until you need to add another effect, for example ‘Maybe’. Then you need to bring in monad transformers and create your own monad combining the two, then rewrite all your code to lift the effects from one monad to the other. And you have to do this every time you want to add a new effect. reply ChadNauseam 3 hours agorootparentNot to mention you need monadic and nonmonadic versions of every higher order function (or so it feels like) - map &#x2F; mapM, fold &#x2F; foldM, etc.This is even worse in Rust, which requires you to have a separate implementation for every effect as well (since it doesn&#x27;t have higher kinded types) reply jitl 11 hours agoparentprevMonads need to wrap each other, effects are more composable reply TheMatten 11 hours agorootparent> Monads need to wrap each other, effects are more composableIt&#x27;s really trickier than algebraic effects make it seem though. Haskell-ish \"monad transfomers\" as a stack of wrappers may pick concrete ordering of effects in advance (e.g. there&#x27;s difference between `State>` and `Result>`, using Rust syntax), but effect systems like one in Koka either have to do the same decision by using specific order of interpreters, or by sticking to single possible ordering, e.g. using one, more powerful monad. And then there&#x27;re questions around higher order effects - that is, effects with operations that take effectful arguments - because they have to be able to \"weave\" other effects through themselves while preserving their behaviour, and this weaving seems to be dependent on concrete choice of effects, thus not being easily composable. In a sense, languages like Koka or Unison have to be restricted in some way, giving up on some types of effects. I&#x27;m not saying that&#x27;s a bad thing though, it&#x27;s still a improvement over having single effect (IO) or no effects at all. reply spicebox 10 hours agorootparentBeing able to change the ordering of effects on the fly is a benefit of algebraic-effect systems. As you mentioned `State>` and `Result>`have very different effects. Algebraic-effects let you switch between the two behaviors when you run the effects, whereas with monad transformers you have to refactor all your code to use `State>` instead of `Result>` or vice-versa reply shirogane86x 2 hours agorootparentYou can recover the ability to reorder effects by using MTL-style type classes, so you could write that as M where M: MonadState + MonadError, in rust-ish syntax. But that makes the number of trait&#x2F;typeclass implementation for each transformer explode (given a trait and a type for each transformer, it&#x27;s O(N^2)), whereas algebraic effect systems don&#x27;t really have that issue. I also have a hunch that algebraic effects(or, well, delimited continuations in general) are probably easier to optimize than monad transformers, too. reply grumpyprole 10 hours agoparentprevIt&#x27;s much easier to adapt existing code. There&#x27;s no need to rewrite code to use monadic binds. reply cranberryturkey 9 hours agoprev [–] does this support WASM? reply nateb2022 9 hours agoparent [–] yes, use `--target=wasm` reply gpderetta 7 hours agorootparent [–] My understanding is that wasm doesn&#x27;t allow for stack switching yet. How does koka implements effects in wasm? Heap allocated stack frames? reply miclill 36 minutes agorootparent [–] I think the _magic_ here is emscripten:- https:&#x2F;&#x2F;emscripten.org&#x2F;docs&#x2F;porting&#x2F;setjmp-longjmp.html- https:&#x2F;&#x2F;github.com&#x2F;koka-lang&#x2F;libmprompt&#x2F;blob&#x2F;main&#x2F;src&#x2F;mpromp... replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Koka is a strongly typed functional language that incorporates effect types and handlers.",
      "The language offers advanced control abstractions and utilizes reference counting for efficient memory usage.",
      "Although Koka has a complete compiler implementation and is stable, it is still in the research phase and not recommended for production use."
    ],
    "commentSummary": [
      "The conversations revolve around the Koka programming language and its features, such as effect types, algebraic effects, and handlers.",
      "Topics covered include category theory, type systems, advanced compilation techniques for performance optimization, and the use of Koka in advanced functional programming projects.",
      "Koka's ability to compile to C code without a garbage collector, its integration with existing C-WASM projects, and its handling system similar to Common Lisp conditions are also mentioned.",
      "The advantages and limitations of Koka, its use of exceptions for control-flow, and its support for circular event invocations and extension functions are discussed.",
      "The difference between monad transformers and algebraic-effect systems and the potential ease of optimization are compared.",
      "There is a brief mention of Koka's wasm support and the suggestion of using emscripten for stack switching."
    ],
    "points": 163,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1703883196
  },
  {
    "id": 38807001,
    "title": "UK's Viking Link interconnector brings cheaper and greener power",
    "originLink": "https://www.nationalgrid.com/national-grid-announces-commercial-operations-viking-link-worlds-longest-land-and-subsea",
    "originBody": "National Grid announces commercial operations of Viking Link – the world’s longest land and subsea interconnector Breadcrumb Back to home Twitter Facebook LinkedIn Email 29th December 2023 - Press release National Grid’s record-breaking new interconnector was switched on today Viking Link can transport enough electricity to power up to 2.5 million UK homes £1.7 billion link stretches for 475 miles between UK and Denmark It will bring more than £500 million in savings for UK consumers in the first ten years The world’s longest land and subsea interconnector started commercial operations today (29th December 2023). National Grid’s new Viking Link electricity interconnector became operational this afternoon transporting power between the UK and Denmark. The link has a capacity of 1.4 GW and stretches for 475 miles under land and sea to join Bicker Fen substation in Lincolnshire with Revsing substation in southern Jutland, Denmark. The £1.7 billion project is a joint venture between National Grid and Danish System Operator, Energinet, and has the capacity to transport enough electricity for up to 2.5 million* UK homes, bringing over £500 million of cumulative savings for UK consumers over the next decade* due to cheaper imported power from Denmark. Construction on Viking Link, National Grid’s sixth interconnector, started in 2019, with more than four million working hours spent to get to this point. National Grid’s interconnector business is run by National Grid Ventures (NGV), which operates outside of National Grid’s core regulated businesses in the UK and US. NGV develops, operates and invests in energy projects, technologies and partnerships to accelerate the development of our clean energy future. Viking Link will bring huge benefits for UK consumers including cheaper, lower carbon power and increased energy security as the UK can call on additional power from Denmark when needed. Initially, Viking Link will be operating at a capacity of 800MW before increasing up over time to 1.4GW. National Grid and Energinet will be working together to bring the asset up to full capacity over the coming year. In its first year of operation Viking Link is expected to save approximately 600,000 tonnes of carbon emissions – this is equivalent to taking roughly 280,000 cars off the road.* President of National Grid Ventures Katie Jackson said: “This record-breaking new link is a fantastic example of engineering and collaboration with our partner, Energinet. As we deploy more wind power to meet our climate and energy security targets, connections to our neighbouring countries will play a vital role increasing security of supply and reducing prices for consumers. Stretching further across land and sea than any of our existing links, it connects the UK to clean, green Danish energy, improving security of supply and bringing huge carbon and cost savings for UK consumers.” Viking Link has a converter station on each end of the cable where the power is transformed into the correct frequency before being transported onto each country’s transmission systems. Principal Contractor Siemens Energy built the converter station in the UK while Energinet built the Danish converter station. Siemens Energy have designed, installed and commissioned the electrical assets on both sides. The HVDC offshore cable was manufactured and laid by Prysmian Group . The cable was laid on the seabed using a custom-made vessel The Leonardo Da Vinci which was then buried using Asso trenchers. Prysmian Group manufactured the HVDC land cable which was installed by Balfour Beatty, the offshore section was manufactured and installed by Prysmian Group in the UK and the Danish land section was manufactured by NKT and installed by Monck. The UK land cable was made up of 118 sections stretching for 67km between Bicker Fen and Sutton-on-Sea where the subsea cable begins. It was connected to the UK’s National Transmission Network by National Grid Electricity Transmission at the existing Bicker Fen substation. Interconnectors enable the fast, flexible sharing of energy between countries, making them the perfect tool for managing the intermittent nature of renewable energy sources. The switch on was a proud moment for Managing Director of National Grid Interconnectors Rebecca Sedler. She said: \"Viking Link is an achievement for both Denmark and the UK, and consumers in both countries will benefit from this infrastructure for many years to come. The hard work and collaboration of our teams, accounting for more than four million labour hours, highlights National Grid’s dedication to the UK’s clean energy transition.” National Grid launched the UK’s first interconnector (IFA) to France in 1986. Since then, it has built five more including a second link with France (IFA2) and further connections with The Netherlands (BritNed), Belgium (Nemo Link) and Norway (North Sea Link). Between 2020 and 2030, National Grid expects its interconnectors will have helped the UK to avoid around 100 million tonnes of carbon emissions and by 2030, 90% of the energy imported through the company’s interconnectors will be from zero carbon energy sources. Energy Security Secretary Claire Coutinho said: Great news today as the new Viking Link interconnector starts to transport energy between Denmark and the UK, under the North Sea. The 475-mile cable is the longest land and subsea electricity cable in the world and will provide cleaner, cheaper more secure energy to power up to 2.5 million homes in the UK. It will help British families save £500 million on their bills over the next decade, while cutting emissions. Earlier this year (2023) National Grid announced joint plans with TenneT for a new 1.8GW interconnector between the UK and The Netherlands, called LionLink. The link would not only join the two countries but also connect to offshore wind generation. LionLink would be the second link between the two countries and is expected to be operational in the early 2030s. A second new link called Nautilus, is also in the planning phase with the potential to connect with Belgium. To find out more about National Grid Interconnectors, how they work and their role in decarbonising the energy system click here. Similar reading Our love is electric - research reveals UK obsession with all things electric 28 December 2023 It’s official, we Brits are addicted to electrical gadgets. New research from National Grid has found that two thirds of respondents spend five hours or more with their gadgets on Christmas Day and Boxing Day and 36% confessed that they bought at least one electrical gadget in the Black Friday sales this year. You could call it love, after all there are 20 songs with the word electric in the title and 100’s featuring power. Read article Richborough Energy Park battery connects to grid on site of former coal plant 21 December 2023 National Grid has connected a 100MW battery project to the electricity transmission network at its Richborough substation. Read article National Grid fast-tracks overhead line upgrade project to help accelerate connection dates of 175 clean energy projects 15 December 2023 National Grid has accelerated engineering works to address a bottleneck in connecting low carbon projects to the electricity network in South West England and Wales. Read article",
    "commentLink": "https://news.ycombinator.com/item?id=38807001",
    "commentBody": "UK announces commercial operations of longest land&#x2F;subsea interconnectorHacker NewspastloginUK announces commercial operations of longest land&#x2F;subsea interconnector (nationalgrid.com) 162 points by Svip 17 hours ago| hidepastfavorite120 comments Havoc 14 hours agoThere is a nice UK grid dashboard too [0]. The UK grid is actually quite variable given the amount of wind input, so imports&#x2F;exports tend to be quite twitchy depending on what&#x27;s happening - frequently going to negative and also substantially positive on pricing.[0] https:&#x2F;&#x2F;grid.iamkate.com&#x2F; reply londons_explore 14 hours agoparentUnfortunately, Denmark has many of the same winds as the UK, so on the exact same days the UK has excess power, Denmark doesn&#x27;t need power... reply petermonsson 13 hours agorootparentThere is a 20 hour time difference from when the wind picks up in the UK until the wind picks up in Denmark reply lewj 11 hours agorootparentsource? interested for the details please. reply petermonsson 4 hours agorootparentIn Danish: https:&#x2F;&#x2F;ing.dk&#x2F;artikel&#x2F;efter-10-aars-arbejde-kom-der-stroem-... reply CPLX 10 hours agorootparentprevThe fact that it’s 800 miles away and the weather is moving at 40mph or so? reply OnlyMortal 10 hours agorootparent… and the UK has prevailing winds straight of the Atlantic. reply 4rt 9 hours agorootparentprevThat made me laugh reply quijoteuniv 1 hour agorootparentprevBut this promote better flow to Norway that has the hydropower, ultimately making more profit to the Energy Companies there. Sell expensive hydro energy when there is no wind, buy cheap wind energy at night when no pnes need it and pump water to the reservoirs. It would be good to see how this cable was lobbied and some estimates in the gain in profit the multinational energy companies are making. Or did everyone think this was made for bringing energy to UK homes and lower the carbon footprint. reply cinntaile 28 minutes agorootparentThe point of the cable is to balance out the energy flows across Europe and to make renewable energy less variable. If there is excess supply in the UK, they can make money by selling it to the continent and if there is too little energy they can buy it from the continent. It works the same the other way around. reply londons_explore 14 hours agoprev> with more than four million working hours spent to get to this point.A typical Brit might work 80,000 hours in a lifetime. So this interconnector is the lifetimes work of 50 people.It is approximately 1 millionth of the total work output of all people alive in the UK.Not sure if that is more or less than I expected... reply 7373737373 1 hour agoparentI wish there was a Wiki table&#x2F;website that showed total working hours for many projects reply toomuchtodo 16 hours agoprevTime to phase out that last coal generator!https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_active_coal-fired_powe...https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ratcliffe-on-Soar_Power_Statio...Edit: interconnector already ramped to 800MW importing to the UK. Let’s gooooooooo! reply Reason077 2 hours agoparent> ”Time to phase out that last coal generator!”Ratcliffe-on-Soar, the last coal fired power plant in the UK, is scheduled to close in September.They probably won&#x27;t close earlier as they will be under capacity market contracts until then, and will have a supply of coal that needs to be disposed of (ie: burned&#x2F;used). reply tomatocracy 1 hour agorootparentThey&#x27;re also making very good money in the balancing market, and generation margins are getting tighter. It wouldn&#x27;t surprise me if the closure gets delayed. reply labster 9 hours agoparentprevCan we just agree to a phase down of that coal generator instead? reply toomuchtodo 9 hours agorootparentFor how long would it need to be mothballed before you’d be comfortable demolishing it? Important to be conservative wrt grid stability, but at some point, burn the ships so there is no going back. reply Reason077 2 hours agorootparentMothballed plants don’t really contribute to grid stability. They need to be kept in a pretty active state, ready to spin up at a moments notice in order to be any use.The fuel (coal) left onsite is also an environmental liability and really needs to be gotten rid of by burning it, necessitating a carefully planned shutdown schedule.Realistically, there is no going back: once the last coal-fired plant is closed (in September 2024), it will be gone for good. reply NooneAtAll3 9 hours agorootparentprevthey can&#x27;t, or else british clocks wouldn&#x27;t be ahead :)https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=bij-JjzCa7o reply trebligdivad 8 hours agorootparentprevMeh we&#x27;re down to 1.1% of generation being coal in the last year; low enough to stop worrying about. Lets worry about how to get rid of the 33.5% of gas! reply cbzoiav 6 hours agorootparentWe&#x27;ve gone over a month without burning any coal, but then the Ukraine war happened and we were at risks of blackouts. That 1.1% is likely heavily concentrated as a much higher percentage on a relatively small days over winter and&#x2F;or where wind levels were low.We probably want to be sure if gas imports are heavily hit again or a major interconnector goes down we&#x27;ll be fine before entirely demolishing it. reply dubcanada 5 hours agorootparentCould you not just keep gas as a backup? I don’t see why you need both coal and gas? reply Reason077 1 hour agorootparentThere was an issue with gas in the 2022&#x2F;23 winter as the Russia-Ukraine war affected supplies and caused prices to skyrocket. More coal than usual was burnt because of this. But it&#x27;s a moot point now: coal-fired plants have continued to close in line with UK government policy, and the last remaining one is scheduled to close in September. reply cfn 1 hour agorootparentprevI suppose gas and coal are sourced from different places and that may give some resilience to the system. They also serve very different functions in the grid. Coal is for baseline and gas is for peak. reply Reason077 1 hour agorootparentCoal is very much used only for peak in the UK in recent years. The remaining plants (in fact, there’s only one still in service now) are kept on standby during winter and activated only when demand is forecast to exceed supply (plus a safety margin).Gas is now more of a year-round flexible baseload, taking up the slack whenever demand is high and&#x2F;or renewables production is low.Nuclear is now the only true baseload in the UK - always generating more-or-less the same output except when they have to be shut down for maintenance, refuelling, or decommissioning! reply tomatocracy 1 hour agorootparentprevThese days, it&#x27;s nearer the other way around in the UK - gas is closer to baseload (sort of) and coal has a profile closer to a peaker (especially when turning up&#x2F;down rather than on&#x2F;off). replyJonChesterfield 2 hours agoprevBetter electrical connection to the mainland seems good all round. Denmark is electrically connected to the other neighbouring countries so this yields (indirectly) a higher current link to the rest of Europe as well.Spikes in production and demand occurring at different places at different times and electrical storage being tricky makes this a good thing. reply hokkos 13 hours agoprevIf you want to see the exchange for the whole europe zone :https:&#x2F;&#x2F;energygraph.info&#x2F;d&#x2F;7dWs1mVVk&#x2F;interconnect-physical-f... reply dn3500 10 hours agoprevUnfortunate wording here: \"The cable was laid on the seabed using a custom-made vessel The Leonardo Da Vinci which was then buried using Asso trenchers.\" reply PeterStuer 2 hours agoparentThe cable was laid, then needed a post lay burial with an (optional) depressor.Inuendo by AssO (not a nsfw actor name). Who said trenchers don&#x27;t have a sense of humour? reply KaiserPro 14 hours agoprevThe good thing about this is that its bidirectional, so can be used to offload ~4.2% of the total generation capacity of the UK, or should they need it, import that amount.That should also cut the tops and bottoms off the spot price of electricity. reply zeristor 14 hours agoparentA large amount of wind power generation is in Scotland, four further interconnects are being built to transfer this to SE England, but it’ll take a while. reply KaiserPro 52 minutes agorootparentindeed! there is a bottleneck between scotland and england in terms of grid capacity that needs to be solved. Its not cheap, and probably not as lucrative. reply askvictor 2 hours agoparentprevWhat are the reasons why an interconnect might be uni directional? I get that a DC interconnect might only go in one direction at a time, but might it not be able to change such direction that is? reply tomatocracy 49 minutes agorootparentRegulation and tax are big ones. The ElecLink UK&#x2F;France interconnector was significantly delayed due to disagreements in how it should be regulated and taxed. Similarly, things like the EU Carbon Border Adjustment mechanism have the potential to make it unprofitable to import electricity into the EU without affecting exports (because even significant price discrepancies would be swallowed up by the additional carbon tax which might be imposed). (In the case of the UK the EU will likely agree to waive the carbon tax but it&#x27;s not certain even there). reply dukeyukey 12 hours agoparentprevWhich could theoretically supply nearly half of Denmark&#x27;s electricity demand, at peak transfer. reply silvestrov 11 hours agorootparentNo. The connection is 1.4 GW and Denmark typically use 4 to 5 GW. See https:&#x2F;&#x2F;energinet.dk&#x2F; (scroll down to map at look at \"FORBRUG I DK\") reply gary_0 13 hours agoprevLots of fun facts on the Wikipedia page: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Viking_Link> Annual transmission capacity of 12.3 TWh.> The actual cable is made of copper, steel, paper and plastic and weighs about 40 kg per meter.> It is similar in capacity and length to the UK–Norway North Sea Link. reply o11c 5 hours agoparent> weighs about 40 kg per meterFor reference, a human child weighs about 15 kg per meter. But since humans grow in 3 dimensions, a healthy adult human will be around 40–45 kg per meter, similar to the cable. In our current overweight society, realistic adult humans are of course heavier. reply chmod775 51 minutes agorootparentCan you convert that to football fields? reply CapitalistCartr 12 hours agoprevSo it has a payback period of over 3 decades. Lots of talk of all the resources spent building it, only the carbon savings of usage. I wonder how long the carbon payback is. In an era of rapidly improving renewables, this doesn&#x27;t seem like picking low-hanging fruit. reply justinclift 12 hours agoparentIt&#x27;s done by NGV, which was described as: NGV develops, operates and invests in energy projects, technologies and partnerships to accelerate the development of our clean energy future.Maybe they&#x27;re one of those places which invests in developing infrastructure tech, to pave the way for subsequent generations? reply allanrbo 16 hours agoprevHow come Denmark can produce cheaper and less carbon intensive energy than the UK? Can&#x27;t offshore windmills be built in the UK too? reply martin-adams 16 hours agoparentThey can, but the problem they face is that they can&#x27;t get any of the developers to want to build them because the ROI is set too low.https:&#x2F;&#x2F;www.theguardian.com&#x2F;environment&#x2F;2023&#x2F;sep&#x2F;08&#x2F;what-wen... reply VBprogrammer 15 hours agorootparentThat&#x27;s a very recent phenomenon though right? Presumably the current rates of inflation have a big impact. It wasn&#x27;t so long ago that we were seeing record low strike prices being accepted for offshore wind, down to about £40&#x2F;MWh. reply tomatocracy 33 minutes agorootparentThere are a couple of things which are misleading about the quoted CfD strike prices. Firstly they are in 2012 prices. Secondly, they are inflated by CPI every year which means the market prices then lower than a true fixed price would be priced. Thirdly, in all rounds prior to the most recent round, what was granted by govt were actually options to enter into a CfD, so developers were able to walk away and replace CfDs with higher-priced market PPAs at COD if they wanted to (and many have indeed done that). As a result, the business plan which some developers have followed has essentially been to bid the lowest price they could get sufficient debt financing with and then try to improve on that with higher priced PPAs or finding financing willing to take merchant risk at COD. reply VBprogrammer 5 minutes agorootparentI actually meant to say interest rates above but that these strike prices are inflated by CPI isn&#x27;t really obvious so I&#x27;m glad you&#x27;ve mentioned it.Is there anywhere I can read about these arrangements? It would be really interesting to know the true cost per mWh delivered from some of these schemes. reply doikor 53 minutes agorootparentprevIt’s mostly the fact that the more wind you build the less each turbine will make in profit as in general they all spin at the same time pushing the prices down when they produce electricity.The solution is to increase demand during those moments and thus these new interconnects to Denmark and Norway. Other option is to build storage or new consumption (hydrogen, ammonia, e fuels, etc) reply VBprogrammer 9 minutes agorootparentI don&#x27;t think that is particularly an issue, at least in the UK. They have a guaranteed price per unit. If the wholesale price is above that then it is paid back. reply dukeyukey 16 hours agoparentprevThey can, and the UK is one of the largest builders of offshore wind. But Denmark has spare capacity, and UK electricity prices are high. reply Angostura 15 hours agorootparentIt’s not really said in the story, but this is a bidirectional connector.There are certainly times when the UK generates excess energy from wind. reply toomuchtodo 15 hours agorootparentThis interconnector was already exporting to Denmark earlier in the week when the UK was experiencing record wind generation (per Electricity Maps [1]). The link currently can’t be maximized from West Denmark to the UK due to transmission constraints on the DK side.[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38807469 reply fbdab103 14 hours agorootparentprevIs that rare? I naively thought that all electrical infrastructure could flow in whichever direction there was need. reply adhesive_wombat 10 hours agorootparentFor things like inverters for example, yes, you have to measure things to decide when to turn on (say) transistors based on that information. You have to carefully design it if you want to be able to drive the exact same H-bridge in reverse as a rectifier. reply scrlk 13 hours agorootparentprevSee \"reverse power flow\": https:&#x2F;&#x2F;roadnighttaylor.co.uk&#x2F;connectology&#x2F;what-is-reverse-p... reply dukeyukey 15 hours agorootparentprevAbsolutely. It&#x27;s a given with stuff like wind. reply sgt101 13 hours agoparentprevAs further up in the thread, if a cyclone comes in on the gulf stream it arrives 20hrs earlier in the UK than in Denmark, and when significant wind power is built on the Irish Atlantic coast then there&#x27;s effects will be amplified and a smoother renewable provision for now Europe will be available. reply davedx 16 hours agoparentprevThere are… Hornsea for example is one of the bigger ones: https:&#x2F;&#x2F;hornseaproject3.co.uk&#x2F; reply DamonHD 15 hours agoparentprevGeographical spread of wind (and solar) generation helps produce a more consistent overall supply. reply ZeroGravitas 14 hours agoparentprevOffshore is more expensive than onshore wind, which has been effectively banned across most of the UK. reply swarnie 15 hours agoparentprevThe UK already has a stupid amount of offshore wind with multitudes more in the planning &#x2F; development phase.This is part of a wider strategy with The Netherlands, Denmark and Germany and interconnect their energy markets using the north sea renewables as both generation and interconnection.The future is bright. reply hkt 16 hours agoparentprevThe UK has very large amounts of offshore wind, unfortunately it isn&#x27;t enough and we&#x27;re not developing more capacity fast enough. It doesn&#x27;t help that the government has utterly screwed the incentives:https:&#x2F;&#x2F;www.theguardian.com&#x2F;environment&#x2F;2023&#x2F;sep&#x2F;08&#x2F;what-wen... reply alexchamberlain 15 hours agorootparentAlso, when the wind blows too much, we produce more than we need. Christmas day had negative prices before 7am, as we had too much wind power. reply patcon 17 hours agoprevRe: cutting undersea cables. For critical infrastructure like this during cold war style conditions (i.e. Russian-NATO conflict), I wonder how pros&#x2F;cons of public vs private ownership compare...For example: I could imagine sabotaging public infra is more akin to an attack on the state (therefore disincentized). But on the other hand, I suspect private interests can be more clever in protecting their investment, because they can more readily pay for high mitigation costs when consequent losses would be high.Anyone else know how to thing of the tradeoffs between public vs private infrastructure here? reply 1920musicman 16 hours agoparentAFAIK attacks on sufficiently important private infra is also considered an attack on the state. There is no meaningful difference, other than the scale of the attack and the importance of that infra.The main defining factor in how the state responds to such an attack is whether escalation is in the interest of the victim. E.g. recent attacks on commercial ships in the Red Sea. reply patcon 11 hours agorootparentYeah thanks for stating it clearly. That distinction makes sense to me too :) reply Scubabear68 15 hours agorootparentprevIndeed, I wonder if convoys will be resurrected to protect shipping in the Red Sea area. reply davedx 15 hours agoparentprevThink the bigger problem is proving beyond reasonable doubt who did it, see Nordstream reply mcfedr 15 hours agorootparentThat&#x27;s surely more about convience, i.e. no one in Europe wants to actually deal with Russia so will faff around pretending they don&#x27;t know who did it as long as possible reply WJW 15 hours agorootparentThere&#x27;s only three possible outcomes that have any chance of being true and none of them are good:- It was the Russians, and the demands by the public to respond might escalate the current situation from a nice contained proxy war into something that might get actual voters killed.- It was the Ukrainians, which would be politically awkward because we&#x27;re supposed to be allies.- It was the USA, which would also be politically awkward for the same reason.So in all cases it would be better to not find out in the first place, hence the current faffing about. reply fbdab103 13 hours agorootparentNot sure it had to be a state operation. The attack could have been done by just a handful of guys with practically no funding. The pipe is not that deep underwater and would not require much incentive to blow. reply treprinum 4 hours agorootparentIt&#x27;s the most monitored underwater area in the world and the magnitude of the explosion as detected by seismographic stations looked like a small nuke. Not sure a handful of guys would be capable of that. reply 0xDEADFED5 5 hours agorootparentprevWashington Post and Der Spiegel seem to be pretty sure it was Ukraine reply PeterStuer 2 hours agorootparentprevIf by Russia you mean the US with the (even more embarrassing) aid of the Swedes you are spott on. reply corradio 16 hours agoprevIf you’re curious, you can follow it live on https:&#x2F;&#x2F;app.electricitymaps.comRight now it’s transporting ~20MW from Denmark to the UK reply WJW 15 hours agoparentUp to 779 MW now, maybe they&#x27;re ramping it up slowly? reply justinclift 12 hours agoparentprevInteresting.The map seems buggy for Canada. The \"Country\" map for Canada seems to be wrong, as it looks like it&#x27;s subdivided into several pieces (as if it was the \"Zone\" view). reply corradio 49 minutes agorootparentIndeed I’ve reported the bug here: https:&#x2F;&#x2F;github.com&#x2F;electricitymaps&#x2F;electricitymaps-contrib&#x2F;i... reply greenbit 16 hours agoprevDidn&#x27;t see mention of the voltage&#x2F;current specs. 1400A at 1MV maybe? HVDC is morbidly fascinating, and the gear to transform has that awesome 1950s sci-fi look. reply davedx 15 hours agoparentI actually drove all the way out to the Britned HVDC transformer station near Rotterdam earlier this year. Absolutely huge bunch of gear, mostly enclosed though and no public tours :( reply mikeyouse 15 hours agoparentprevCurrently it’s only 800MW at 525KV but eventually they’ll bump that to 1,400MW so 2,700A if my math is right? reply formerly_proven 14 hours agorootparentIt’s bipolar (+- 525 kV) according to W reply malwrar 11 hours agoprevAre there any safety concerns around 1.4GW flowing through a cable surrounded by salt water? I’m entirely uneducated in the ways of zap juice, but that sounds like a recipe for spontaneous electrocution if that cable breaks anywhere. reply seabass-labrax 7 hours agoparentThe power cable operators don&#x27;t want to waste any energy and will have power monitoring systems at least at both ends, so much as with a fusebox at home, I&#x27;m sure that the cable would be switched off very quickly if that happened. In addition, the power would be dissipated close around the source of the breakage by the (not insignificant) electrical resistance of water. Since the North Sea is about 100 metres deep, the only victims in such event would be fish and other seabed creatures. I don&#x27;t tend to chew power lines, though, so don&#x27;t worry about me. reply chpatrick 10 hours agoparentprevI&#x27;m not a physicist but I would imagine it dissipates quite quickly due to some inverse square law. reply ivix 9 hours agoparentprevThink about how lightning exists, and you&#x27;ll see why this is not an issue. reply chx 10 hours agoprevI posted 1.5 months ago about how a potentially catastrophic grid event three years ago instead showed how successful the EU cooperation is: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38251463As an aside, the UK is still part of this but thanks to Brexit they got the opportunity to finance this interconnector by themselves and not avail themselves to the vast funds from the European Energy Programme for Recovery. For example Malta did and while the Malta Interconnector is not a world record in length but it certainly is in the relative amount: currently it&#x27;s already above a quarter of the energy needs of Malta and in 2015 the relevant EU report said it&#x27;s expected this to grow to 35% eventually. reply blibble 10 hours agoparent> but thanks to Brexit they got the opportunity to finance this interconnector by themselves and not avail themselves to the vast funds from the European Energy Programme for Recoverythe UK was a net contributor to the EUeverything it \"received\" from the EU was paid for by itselfhttps:&#x2F;&#x2F;www.bbc.co.uk&#x2F;news&#x2F;uk-politics-48256318 reply chx 9 hours agorootparentsighThe UK’s net EU budget contribution in 2014 was £9.8 billion https:&#x2F;&#x2F;commonslibrary.parliament.uk&#x2F;research-briefings&#x2F;cbp-...Meanwhile the UK exported $276 billion worth of good to the EU more than half of its total https:&#x2F;&#x2F;www.worldstopexports.com&#x2F;united-kingdoms-top-import-...Not counting anything else just these two we could say the EU had a 3.6% tariff on UK imports (insanely low! check https:&#x2F;&#x2F;www.wto.org&#x2F;english&#x2F;res_e&#x2F;statis_e&#x2F;daily_update_e&#x2F;ta... -- and note the post brexit deal only created a 0% tariff on goods and not services). And even that is a severe exaggeration because there were other benefits to this.On the other hand, the way these things went it&#x27;s very likely the UK already contributed to the EERP but got nothing out of it. reply throwaway167 9 hours agorootparent> very likely the UK already contributed to the EERP but *got nothing out of it*.I for one am swimming in all the surplus cash not going to the EU any more, as well as all those upwards revisions to GDP and employment, and our pound soaring to new highs improving my purchasing power. reply blibble 5 hours agorootparentprev> Not counting anything else just these two we could say the EU had a 3.6% tariff on UK importsthis \"analysis\" makes the infamous Boris bus number look almost honest by comparison reply kmlx 8 hours agorootparentprev> The UK’s net EU budget contribution in 2014 was £9.8 billionthe state’s budget contribution. aka taxes and other income of the state.> Meanwhile the UK exported $276 billion worth of good to the EU more than half of its totalnot the UK, not the state. the opposite. the private sector. reply tapland 15 hours agoprevI&#x27;d assume this also sets the lowest price in the danish zone to the UK market price when exporting?NordPool market zones are crazy and some zones see insane price surges when demand prices for neighboring zones are high. reply WJW 15 hours agoparentWell at least for the first 1.4 GW, yes. Also there is some nonzero transmission loss so that needs to be taken into account.The cable can move electricity from the UK to Denmark too btw, so some of the price surges in the Danish region could get damped because the UK will start exporting electricity and the additional supply will bring down the price again. reply DamonHD 17 hours agoprevHurrah! I thought that it wasn&#x27;t coming on-line until January! reply riffic 12 hours agoparentlook what&#x27;s three days away reply wifnxiwjfn 3 hours agoprevUK concerned that they’ll need the Scots to keep the lights on and so they’re ploughing British cash into a Danish interchange reply figmert 17 hours agoprevWill this finally decrease some of these bills? Or will this just mean even more record profit for the energy companies? reply altacc 16 hours agoparent“ It will bring more than £500 million in savings for UK consumers in the first ten years.”If you believe that I have a monorail to sell you! Why pass onto consumers what you can take as profit.Norwegian news is reporting that the price of electricity will increase a tiny amount (about £10-15 a year) in both Norway & the UK as an increased flow of power means more bidders for it, pushing the price up when power is scarce (which seems to be most of the time).https:&#x2F;&#x2F;www.nrk.no&#x2F;sorlandet&#x2F;ny-kraftkabel-kan-fore-til-dyre... reply tonyedgecombe 15 hours agorootparent>Why pass onto consumers what you can take as profit.Because other suppliers will outbid you.>Norwegian news is reporting that the price of electricity will increase a tiny amount (about £10-15 a year) in both Norway & the UK as an increased flow of power means more bidders for it, pushing the price up when power is scarce (which seems to be most of the time).That doesn&#x27;t make sense, if energy is scarce in one country but not the other then the additional supply should lower prices. reply ta1243 14 hours agorootparentIt will lower the peaks of the prices, but at the same time increase the depths. Currently those on market driven tarrifs can sometimes get paid to take electricity, and certainly have low prices. For example Octopus Agile is charging under 5p&#x2F;kWh from 2230-2300 today, and from 0600-0630 actually paid its customers 4p&#x2F;kWhWith Denmark able to buy 1GW this will increase demand at the cheapest bits (when supply is high), but with Denmark able to sell 1GW it will increase supply and thus drive lower price.The majority of people in the UK don&#x27;t pay market prices for electricity and instead pay a government set \"cap\" which is based around hiding the actual cost in the marketing material because far too many people in the UK don&#x27;t understand what you can do with 1kWh or how it affects your bill - some people on think that turning 30W of LED lights off makes a material difference to the monthly cost they pay for example. reply tonyedgecombe 45 minutes agorootparent>For example Octopus Agile is charging under 5p&#x2F;kWh from 2230-2300 today, and from 0600-0630 actually paid its customers 4p&#x2F;kWhI understand that, the market is trying to deal with an inefficiency in the system. What I don&#x27;t understand is how removing those inefficiencies will lead to higher prices. If anything it should do the opposite.The cap is incidental, I don&#x27;t like it but it doesn&#x27;t really have much to do with this. reply hardlianotion 15 hours agorootparentprevThe interconnector is with Denmark, which is connected to other countries as well. Presumably it is this extra demand that could cause the price rise. reply tonyedgecombe 39 minutes agorootparent>Presumably it is this extra demand that could cause the price rise.The cable works both ways.Having an inter-connector opens the market to a larger pool of suppliers on both sides of the cable. You would expect this to lower prices rather than increase them. reply OscarCunningham 15 hours agorootparentprev£500 million over 60 million people over 10 years is £1 per person per year. I can believe it. reply louthy 15 hours agorootparent67 million people (as of 2020, so probably not far off 70m now) reply jonplackett 14 hours agorootparentprevBut we had to spend 1.7 billion. So looks like we need to wait 37 years for payback… reply OscarCunningham 13 hours agorootparentI don&#x27;t know if the UK government paid the full amount, but £50 million a year for £1.7 billion is a 2.9% interest rate, which is approximately sensible. reply ta1243 14 hours agorootparentprev> If you believe that I have a monorail to sell you! Why pass onto consumers what you can take as profit.That&#x27;s about 15p per month per household reply Faaak 16 hours agoparentprevBoth reply _dain_ 16 hours agoparentprevheaven forbid someone build infrastructure because of the profit motive reply jameshart 16 hours agoprevThe ‘Viking Link’ connector, projecting Danish power across the North Sea… I feel a song coming on…Aaaa-aaaaaa-ah! Aaaa-aaaaaa-ah!It comes from the land of the ice and snowFrom the midnight sun where the cold winds blowThe hammer of the godsWill turn the blades of turbinesTo light the grid, sing and cryWind power, I am coming… reply londons_explore 14 hours agoparentMy attempt:(Tune begins with a rising, powerful melody)Verse 1: From the shores of the Danes to Britannia&#x27;s realm, Where the sea meets the sky in a watery helm, There&#x27;s a thread made of power, so silent and sleek, A marvel of might that the ancients would seek.Chorus: Oh, Viking Link, your cables enfold, Uniting the lands as the sagas once told. Electric currents, like legends of old, Viking Link, forging futures so bold.Verse 2: Beneath the North Sea where the mermaids do sing, Lies a path of pure energy, a power-bringing string, Connecting the heart of the emerald isles, With wind-harnessed force that covers the miles.Chorus: Oh, Viking Link, your currents so free, Dance &#x27;cross the depths of the deep, briny sea. Blending the watts as the mead-masters would, Viking Link, for the greater world&#x27;s good.Bridge: Hark! As the turbines spin round and round, A whisper of Odin, in kilowatts found. Thor might have thundered with fierce, mighty sound, But our silent giant lies under the ground.Verse 3: From the fjords where the Vikings once launched their great fleets, Comes a new age of conquest, where technology meets. And the UK replies with welcoming hands, Together they stand, where power demands.Final Chorus: Oh, Viking Link, your saga&#x27;s begun, Tales of electrons, from dusk until dawn. May your current flow steady and strong, Viking Link, in our hearts, your song lingers on.(End with a triumphant harmony) reply hkt 16 hours agoparentprevThis is genius tbh reply davedx 16 hours agoparentprevBeautiful. Rousing.Has the Danelaw come again? Someone call Uhtred! reply myself248 14 hours agoprev [–] The choice of a .com TLD instead of a .uk for a thing called \"national\" is really odd to me. reply lmm 3 hours agoparentLots of \"national\" US companies use .com instead of .us, it&#x27;s not that unusual. reply timthorn 14 hours agoparentprevNational Grid has operations in the USA these days, it&#x27;s no longer a UK only business. reply seabass-labrax 7 hours agoparentprev [–] .com is short for &#x27;commercial&#x27;. The USA has their own .us TLD, but for some reason the Americans never particularly wanted to use it.National Grid Ventures is registered in the UK, so they would be eligible for a *.uk domain if they wanted one. reply mkl 2 hours agorootparent [–] And .gov is short for \"government\", but only the USA&#x27;s, and .edu is short for \"education\", but only the USA&#x27;s.There were strange third-level-only location-based restrictions on .us for a long time, created by the one guy (!) responsible for it, so it didn&#x27;t get much use in the critical 1990s period. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;.us replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "National Grid has announced the completion of Viking Link, the world's longest land and subsea interconnector, stretching 475 miles between the UK and Denmark.",
      "The £1.7 billion project has the capacity to power up to 2.5 million homes and is expected to save UK consumers over £500 million in the first ten years through cheaper imported power from Denmark.",
      "Viking Link is also estimated to reduce carbon emissions by 600,000 tonnes in its first year of operation, while increasing security of supply and decreasing prices for consumers."
    ],
    "commentSummary": [
      "The UK has launched the longest land/subsea interconnector to balance energy flows and reduce variability of renewable energy.",
      "Closure of the last coal-fired power plant in the UK is discussed, highlighting the importance of grid stability and dependence on gas imports.",
      "Interconnects are being built with mainland Europe and Scotland to manage production and demand spikes, though grid capacity remains a bottleneck."
    ],
    "points": 162,
    "commentCount": 120,
    "retryCount": 0,
    "time": 1703868339
  },
  {
    "id": 38803589,
    "title": "Debunking the Myth: Roman Soldiers Were Not Paid in Salt",
    "originLink": "http://kiwihellenist.blogspot.com/2017/01/salt-and-salary.html",
    "originBody": "Kiwi Hellenist Modern myths about the ancient world. Wednesday 11 January 2017 Salt and salary: were Roman soldiers paid in salt? A few weeks ago, we looked at myths to do with ploughing over cities and salting the earth. Today we’re looking at a kind of companion myth. The basic idea is that Roman soldiers were paid in salt, or received an allowance of ‘salt money’. Salt money? (photo by Benreis; CC licence) A few other ancillary myths tend to come along with it too. Take a look at these gloriously mangled pieces of misinformation: I thought you might like to know just where your salary comes from. The word, at least. The source seems to be the Latin ‘salarium’ (‘sal’ being salt) which is a word tied to the payments made to soldiers in the early Roman salt trade. In those days, salt (regular ordinary table salt) was a prized and valuable commodity. If you’ve ever heard the phrase ‘you are the salt of the earth’ or ‘worth your salt’, both are referring to the high value of salt. -- a 2009 blog post A soldier’s pay -- consisting in part of salt -- came to be known as solarium argentum, from which we derive the word salary. A soldier’s salary was cut if he ‘was not worth his salt,’ a phrase that came into being because the Greeks and Romans often bought slaves with salt. -- Time, ‘A brief history of salt’, 15 Mar. 1982 (The blog post, in particular, has been uncritically copied, paraphrased, and plagiarised on many other parts of the web -- like this page offered up by the European Parliament’s Terminology Coordination Unit.) First, the accurate bits. (1) The English word ‘salary’ does indeed come from Latin salarium ‘stipend, money allowance’. (2) Salarium does indeed appear to be linked to sal ‘salt’, via the adjective salarius ‘pertaining to salt’. And there the accuracy ends. Here’s the simplest form of the myth. The word ‘salary’ comes from the Latin word for salt because the Roman Legions were sometimes paid in salt. -- Wikipedia, ‘History of salt’ Pure fantasy. There isn’t the tiniest scrap of evidence to suggest this. At all, to any extent, ever. The allure of this myth comes simply from the link between salarius and salarium. Naturally everyone wants to have the true explanation of what exactly the link is. Unfortunately no ancient source tells us one. And so we end up in the situation where people invent explanations for themselves. Folks who propagate this myth don’t usually try to cite sources, but when people do go looking for sources, they end up drawn to two pieces of ancient testimony. First is the 1st century CE writer Pliny the Elder: honoribus etiam militiaeque interponitur salariis inde dictis ... (Salt) is also related to magistracies and duty abroad, and that’s where we get the word ‘salaries’ ... -- Pliny Natural history 31.89 And second, testimony about state taxes on salt. For example, the historian Livy reports how the Roman censors imposed a new tax in 204 BCE: vectigal etiam novum ex salaria annona statuerunt. sextante sal et Romae et per totam Italiam erat; Romae pretio eodem, pluris in foris et conciliabulis et alio alibi pretio praebendum locaverunt. id vectigal commentum alterum ex censoribus satis credebant ... inde Salinatori Livio inditum cognomen. (The censors) also imposed a new tax on the annual salt production. Salt cost a sixth of an as in Rome and throughout Italy; they set it to be offered at the same price in Rome, but more in town squares and marketplaces, and at other rates in other places. It was widely believed that just one of the two censors devised this tax ... As a result (the censor) Marcus Livius was given the nickname ‘salt-dealer’. -- Livy 29.37.3 Elsewhere Cato the Elder is quoted as talking about salinatores aerarii, treasurers of the salt taxes, as a specialised post in the 190s BCE (reported in Servius auctus, commentary on Aeneid 4.244). These passages, along with Pliny, are close as we get to a link between salt and money in any extant Roman sources. The trouble with citing Pliny as a source for the myth is of course that Pliny doesn’t say anything of the kind. The problem is exacerbated by Wikipedia, which bald-facedly re-writes Pliny, and has been quoted very widely: the Roman historian Pliny the Elder, who stated as an aside in his Natural History’s discussion of sea water, that ‘[I]n Rome...the soldier’s pay was originally salt and the word salary derives from it...’. -- Wikipedia, ‘Salary’ (the addition of this line dates to 2004) This is a mistranslation, just to be clear. And this wording doesn’t even appear in the linked source. And Pliny isn’t writing about sea water, but about salt itself. None of that has stopped this fake quotation being repeated in countless books and websites. Note, 18 Jan.: this error, and the other Wikipedia excerpt quoted above, have since been corrected. However, some other parts of the articles are still inaccurate: see below. Brine refinery at Fuerteventura, Canary Islands (source: tourist blog). Ancient Roman salinae worked in more or less the same way: see Pliny Nat. hist. 31.81-83. If you take a global view, of course you’re bound to find some times and places where salt could act as a means of storing value and facilitating exchange. The most famous example is Ethiopia in the modern era. Here’s how it’s reported by Ray’s Travels, a classic 17th century piece of travel writing: In trading, they make no use of coined money, as the Europeans do, but their money are pieces of fifteen or twenty Pics of cloth, gold, which they give by weight, and a kind of salt, which they reduce into little square pieces like pieces of soap, and these pass for money. They cut out that salt upon the side of the Red Sea, five or six days journeys from Dangala, as you go from Cairo, and the places where they make it are called Arbo. -- John Ray, A collection of curious travels and voyages, vol. 2 (1st ed. 1693, 2nd ed. 1705), 1738 printing, p. 486 This 1949 book, this 1977 essay, and this 1994 book report that salt bars called amoléh continued to serve as an important medium for exchange -- one among many; others included Maria Theresa thalers, clothing, iron, gold, and cattle -- all the way up until the beginning of the 20th century. Reportedly the chief source of Ethiopian salt bars was the Afar depression, next to the Red Sea, a region that includes present-day Djibouti as well as slivers of Ethiopia, Eritrea, and Somalia. Just bear in mind that this has nothing at all to do with Roman soldiers. The fact that salt could mediate exchange in 17th-19th century Ethiopia has no bearing on ancient Rome. Salt money might be a plausible thing in and of itself, but we have absolutely no reason to imagine salt currency in Rome. It’s just that when you go hunting for something specific across the whole of human history, you’re likely to find it. A few more examples. This 2013 book claims that salt has also been used as ‘money’ (the word is tendentious: ‘a medium for trade’ and ‘money’ are not the same thing) in China, pre-Columbian Mexico, Borneo, and elsewhere. A person who uploaded this photo to Wikimedia.org claims it is a sample of salt currency from early 20th century Angola, held at the Royal Ontario Museum. And Wikipedia alleges that American soldiers were paid in brine during the War of 1812. This last one appears to be completely fictional, like the Roman case: apparently it’s some kind of distant distortion of the British salt embargo during the war, and the development of several important brine refineries in the USA throughout the 1800s-1810s. ‘Roman soldiers were paid in salt’ may be the simplest form of the myth, but it’s also a secondary form. I’ve done some searching around in Google Books with date constraints, and that seems to indicate that people first started writing about the idea around the 1860s (here, for example). The older, primary form of the myth is that soldiers were given ‘salt money’, that is, a monetary allowance for buying salt. This, too, is a modern invention. It isn’t nearly as daft as ‘soldiers were paid in salt’, but it’s still only a conjecture, unsupported by any ancient testimony. The phrase ‘salt money’, or in Latin salarium argentum, is an invention of 18th and 19th century Latin dictionaries. The phrase was coined by dictionary-writers as their best guess for how salarium ‘salary’ came from salarius ‘pertaining to salt’. Here’s one of the two standard Latin-English dictionaries, Lewis & Short, on the subject: B. sălārĭum, ii, n. (sc. argentum; cf.: calcearium, congiarium, vestiarium, etc.); orig., the money given to the soldiers for salt, salt-money; hence, post-Aug. (v. Dio Cass. 52, 23, and 78, 22), in gen., a pension, stipend, allowance, salary (cf.: honorarium, annuum, merces, stipendium) -- Lewis & Short, A Latin dictionary (1879), p. 1618, ‘Salarius’ The key bit is in the first line. The supposed meaning ‘salt money’ (‘sc[ilicet] argentum’, i.e. ‘with argentum implied’) is not actually attested anywhere. It’s inferred by analogy with some other, real, expressions: calcearium (‘shoe money’, from calceus ‘shoe’); congiarium (‘distribution of largesse’, from congius ‘half an amphora’s worth’); and vestiarium (‘clothing money’, from vestis ‘clothing’). Unlike salarium argentum, these terms actually do appear in various ancient sources, with the correct meanings. Lewis & Short didn’t invent the conjecture: it also appears in the older Latin-German dictionaries of Freund (1834) and Scheller (1804). It seems to have its origin in the 1st edition of Facciolati and Forcellini’s Totius Latinitatis lexicon (‘dictionary of the entire Latin language’): Salarium, ii ...: proprie est annona salis, quae olim dabatur militibus. ‘salary’ ...: strictly, the annual salt revenue, which was once given to soldiers. -- Totius Latinitatis lexicon (1st edition, 1771), vol. 4 p. 15, ‘Salarius’ This was already a very muddled rendering of the evidence. Facciolati-Forcellini go on to cite Pliny, though as we have seen Pliny doesn’t actually say this. It looks like what’s happened is that they've conflated the Pliny passage with the Livy passage. Livy referred to a tax on the salaria annona ‘annual salt production’. Annona can mean either ‘annual production’ or ‘annual revenue’, and Facciolati-Forcellini have taken Livy’s phrase and used it with the other meaning: annona salis ‘annual salt revenue’. Later on, Scheller and Freund realised that Pliny didn’t say what Facciolati-Forcellini claimed he did, but they liked the idea so they instead supported it with the analogies of ‘shoe money’, ‘clothing money’, and so on. And the idea stuck. All these dictionaries are engaging in conjecture. No ancient source ever actually uses salarium to mean ‘salt allowance’. It’s a guess. It isn’t a terrible guess, but it’s still a guess. One thing that weighs heavily against it is that even Pliny, who’s trying to link salarium to ‘salt’ as closely as he can, doesn’t try to get away with inventing ‘salt money’. The current standard, the Oxford Latin dictionary (1968), very properly avoids taking any view on the question. It just states that salarium comes from sal. Unlike the older dictionaries, it doesn’t make any inferences about how or why the two words are related. ‘Salt money’ certainly isn’t as ridiculous as the idea of paying soldiers in salt -- it does have parallels that make it at least a reasonable conjecture -- but there’s still no evidence for it. Sea water refinery in western France (source: ISSLR.org) I don’t have a perfect explanation for how the Latin word for ‘salty’ gave rise to the word for ‘salary’. Of course I don’t: that’s why we have this myth floating around. We don’t have the evidence to settle on a single explanation. As I said above, ‘salt allowance’ isn’t a terrible guess. But I strongly suspect it’s much more metaphorical than that. Compare how the Greek word for a salary was opsōnion, literally ‘(money) for buying opson’, where opson means ‘fish, relish, sauce’. That doesn’t mean Greek workers were given a ‘fish allowance’: it means that there was a generalised idea that wages went on traded goods like fish, and not on things like barley which land-owners would grow for themselves. Similarly, in Rome, grain allowances were a common thing; it could easily make sense to interpret salarium as ‘everything-else-money’. This interpretation is less specific, slightly metaphorical, and it’s still just a conjecture. But I’d say it’s more plausible, and certainly a more economical explanation, than inventing a specialised category of wages out of thin air. We still haven’t dealt with this: A soldier’s salary was cut if he ‘was not worth his salt,’ a phrase that came into being because the Greeks and Romans often bought slaves with salt. -- Time, ‘A brief history of salt’, 15 Mar. 1982 Oh dear oh dear. This one has made it into Wikipedia too (‘soldiers who did their job well were “worth their salt”’). Unfortunately for Time and for the thousands of people who have repeated this idea, the phrase ‘worth one’s salt’ is definitely not Roman. It is first attested in the 1830s (Etymonline.com; for sources, see OED under ‘salt’). The thing about buying slaves with salt is fictional too. And then there’s ‘salt of the earth’, which comes up in the 2009 blog post I quoted at the start. I mentioned this in my previous post on ‘salting the earth’. It’s nothing to do with Roman soldiers: it’s biblical, from Matthew 5:13 in the New Testament. This means that (1) it isn’t a Roman phrase, but at closest, Helleno-Christian; (2) it’s later than Pliny’s mention of salarium; (3) it’s about using salt as a fertiliser as much as anything else, as I argued in my earlier post. World salt production in 2012. That year, China produced between 22.5% and 27% of the world’s salt, well ahead of the USA, India, and Germany (in that order). (Generated using OpenHeatMap, based on Wikipedia figures) Salt was certainly a significant strategic resource in antiquity. But calling it ‘prized and valuable’ is silly. Yes, it’s the single most common preservative agent ever used, and it is by far the most common seasoning. The Roman salt trade was under state control from the earliest times (see e.g. Livy 1.33.9, 2.9.6); the Via Salaria or ‘Salt Road’ owed its name to its role in salt transportation; the Etruscan city of Veii owed much of its wealth to salt production; and access to salt even provoked a war between two German tribes at Bad Salzungen in the 1st century CE. But ‘prized and valuable’ -- no. That suggests a special cultural status which isn’t supported by any evidence. No one thought of salt as an heirloom, or used it for jewellery. No one talks about awarding salt as a prize for contests. There’s no evidence anyone used salt bars as money -- not even as one among many forms of exchange, as in 19th century Ethiopia. Salt was not a prestige object. Modern people who repeat these myths sometimes emphasise the high value of salt in the Roman world. Well, sure, the salt trade was valuable ... that’s because it was traded in such high volume. But in 204 BCE, when Marcus Livius ‘the salt-dealer’ imposed his tax on salt, Livy quotes the price of salt at a sextans: that is, one sixth of a copper as, or one 60th of a silver denarius (or in a civilian context, a sextans was one 96th of a denarius). Polybius, writing in the mid-100s BCE, quotes a foot-soldier’s pay as ‘two obols’ per day, that is to say, one third of a denarius (Polybius 6.39.12). In other words, a Roman pound of salt (ca. 330 grams) cost one twentieth of a foot-soldier’s daily wages. Important? Of course. Expensive by modern standards? Maybe, depending on the price of salt where you live. ‘Prized and valuable’? No. Actually that deserves more than a ‘no’. It deserves a hearty laugh followed by a ‘no’. Thus: ‘Ha ha ha ha! No.’ There, got it right now. Posted by Peter Gainsford at 07:30 Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest Labels: food, language, Roman, warfare 46 comments: Anonymous27 January 2017 at 07:52:00 GMT+13 Thanks. I recently \"learned\" about the origin of salary being the Roman soldier pay. Now i won't go around mentioning this factoid. In, fact now i can shoot it down if anyone else mentions it! ReplyDelete Replies Reply Fiona8 February 2017 at 11:17:00 GMT+13 Thank you, this was brilliant. You have convincingly corrected our misconceptions, and my family and I are pleased to have the truth (even though the myth really is a good story -- which is, of course, always the way). ReplyDelete Replies Peter Gainsford20 February 2017 at 14:47:00 GMT+13 Happy to be of use and/or interesting, Fiona. Glad you liked it! Delete Replies Reply Anonymous15 October 2022 at 06:51:00 GMT+13 Thanks for settling this in my mind and I will share this if the topic comes up! I am interested in the history of Judea during the first century AD, along with the Mosaic Law of the Torah. Salt was highly valued indeed in early Judaism, and Jesus famously referred to metaphorically; He encouraged His followers to shine with His Veritas and Agape and to be tasty snd pure as SALT! I read that the salt harvested from the Dead Sea has been more highly valued than other types. I know that I miss salt in my cooking in my elderly season; I tend to sneak it into bland soups and sauces defiantly! My blood pressure is very good, thank the Lord! I enjoyed your article very much; you seem to be a very learned and feisty gentleman! May God bless you abundantly! Knowledge and Wisdom are very important; more valuable than gold or silver! Delete Replies Reply Reply liza19 March 2017 at 01:36:00 GMT+13 nice blog ReplyDelete Replies Reply jeff house3 August 2017 at 01:16:00 GMT+12 So I guess Roman soldiers weren't given twenty pounds of salt a day or one hundred pounds of it a week to carry around on maneuvers! ReplyDelete Replies Rick4 August 2017 at 04:55:00 GMT+12 More like 16 pounds, actually, refer to statement above that a Roman pound was 330 grams whereas modern pound is 454 grams, but point taken even so. Delete Replies Reply Reply Kipala16 January 2018 at 00:34:00 GMT+13 I have an analogy from Swahili. Police and other officers asking for a bribe often use the word \"chai\" = tea. \"Give him some tea\" can be the recommendation when stopped by traffic police. Historians in a few hundred years may wonder if police salary in East Africa really included tea - in form of leaves or as a drink??? (not trying to imply that salarium had to do with bribes, but theer are many ways to use euphemisms) ReplyDelete Replies Anonymous9 December 2021 at 04:56:00 GMT+13 In Russia, a tip is called \"na chai\" -- FOR tea. I'm sure this is the same thing. \"Give him something FOR tea\", not \"give him tea\". Delete Replies Reply Reply Kipala16 January 2018 at 00:34:00 GMT+13 This comment has been removed by the author. ReplyDelete Replies Reply Kipala16 January 2018 at 00:34:00 GMT+13 This comment has been removed by the author. ReplyDelete Replies Reply Yoav7 March 2018 at 19:20:00 GMT+13 I want to add another story which stresses the importance of salt:the story sais that the king asked her daughters who loved him mostly.The first answered :like dimonds, the second said:like stars in the sky while the third said: like salt in soup. The king got angry and let her leave the castle. After many years ghe king had a feast and many cooks were invited to prapare food. One of them was his daughter who prepared him soup without salt. When the king tasted the soup he suddenly remembered his doughter and asked for forgivnes. I just wsnted to stress the very importance of salt since it appears even in folk stories. ReplyDelete Replies Reply Smart Saqib6 June 2018 at 07:24:00 GMT+12 Through interviews with experts, we learn that Jesus is not a historical figure, the events of Jesus' life were based on the Roman military campaign, his The Bible in Ancient greek language supposed second coming describes a historical event that already occurred, the theories of Christ came from the ancient pagan secret schools, and the Gospels were written by a family of Caesars and their supporters who left us documents to demonstrate it. roman inventions still used today ReplyDelete Replies Anonymous22 May 2019 at 11:19:00 GMT+12 And remind us again whether or not Jesus was a historical figure fits into this discussion of salt, salary & a Roman soldier's pay??? Delete Replies Reply Peter Gainsford22 May 2019 at 11:22:00 GMT+12 I get your objection, and Saqib's wrong of course, but don't waste your time - look at the date! That comment was left nearly a year ago. Delete Replies Reply Reply Insurance Plus22 December 2018 at 20:30:00 GMT+13 \"how the Latin word for ‘salty’ gave rise to the word for ‘salary’\" ... One Sweats salty when one labors., that salt needs to be replaced regularly. Hence,, Earning ones Salt,, and,, Being worth ones Salt, Paying compensation for the salt of anothers labor,, all become intertwined. Labor worthy of Sal/t. is valued in Den/ari,,, Saltari, Salary. ReplyDelete Replies Reply MaryV2 April 2019 at 00:16:00 GMT+13 Oh my! Now I can't trust my dictionary. Thanks for this very interesting and well-researched post. ReplyDelete Replies Reply Unknown24 December 2019 at 20:22:00 GMT+13 Interesting article but you did not include in your references nothing aboutt the importance of salt in suporting life. I will not give you references (you will have to look them for your self) but you should know that human muscles (starting with harth) or any animal life form cannot function without a certain quantity of salt which is not big but the need increases with the amount of effort put into daily activity and if you have to travel great distances on foot or horse (but the horse or elephant or bull or sheep or cow or goat has muscles like man, right?), like an army (not to mention the fact that an army needs strong muscles and to be in shape to fight with swords or spears or other weapons, not at all easy to carry or handle because they did not have buttons to push at that time) then the amount of salt required for a good condition is higher and the salt is massively eliminated by perspiration. The fact that nowadays doctors insist on reducing salt in the daily diet derives from the fact that we live in an industrial era where industrially processed food is supersaturated with salt to increase the taste because man is equipped mainly with specialized taste buds in salt and sweet without which the muscles and brain - as the main sugar consumer, cannot work. Nowadays pepper costs nothing because we live in other times, but now only a few hundred years ago European traders were getting rich with just a few bags of pepper. In the Netherlands of the 15-16-17 centuries you can buy a large villa with only a small bag of pepper or another spice that the spice traders had just started to bring from the east. The same situation was in Rome in its beginnings, in the 8th century before Christ when there is the road called VIA SALARIA, so named for traveling the AREA (ARIA) in which salt was extracted from seawater, an extremely expensive process at that time. Salt became much cheaper only in the 1st century BC, when the Roman empire expanded its territory by grabbing many other areas where salt was produced, especially south of the Danube, but not only when the price of salt dropped naturally if the sources have become multiple, under their own control directly and within reach. So, REACH becoms RICH and when you are REACH is kind of hard to imagine what was life like just few hundreds of years ago all over the world if you have no suficient archives...I can understand that! ReplyDelete Replies Anonymous9 April 2020 at 22:45:00 GMT+12 How can you expect anyone to take your 'information' seriously if you present unsupported information and then tell us to find our own sources, on a topic that is described in the main article as being hard to pin down, and full of inaccuracies. Delete Replies Reply Reply Unknown24 December 2019 at 20:36:00 GMT+13 The roman soldiers were indeed payd in salt (at least partially)but the payment in salt decreased over time as salt became more accessible to everyone and gold became the main target. The word SALVATION (SALVE .... SAVED) comes from the Latin expression SAL VE RA which means: Salt brings energy and light where RA (the god of sun, light and energy to the Egyptians but not only to them ... see the expression RADIOS which means RA GOD from which we have the words RADIO, IRADIATION and RADIO DE SUN but also RAI (heaven) in Romanian language but also the word ALTA RA in the Christian religion (altar) which means to offer food offerings to the god of light, which is why we have today the name : day of the sun (Sunday) and DOMINUS DEUS from which in the Romance languages the words: DUMINICA / DOMENICA / DIMANCHE. Jesus was called the Salt of the Earth and hence called the Savior for some but he is in fact the leap of mankind (in Romanian SARE means to jump and SALT means the same thing - you see what I said above about the physiological necessity of salt). Also from here we have in the Romanian language the word SALUT(hello). You haved it too from french: I SALUT YOU! I do not want to be a smart ass ... I just wanted to show you the importance of the word SALT in the history of LIFE and humanity ... element that you did not take into account when you wrote this article that started well but failed because you did not have taken into account several variables. In order to have a correct image of what happened 2000 years ago, you must also transpose yourself into the state of those then, beside the information of the archive, which may or may not be complete, otherwise you may draw the wrong conclusions. ... which unfortunately happens very often nowadays. The words like VESTIARIUM that you have mentioned means THE AREA were you keep you clothes...or shoes....or the amphora with olives, olive oil ad other persihable goods. In romanian language VESTIAR means dressing room. VESTI (clothes)+ ARIA/ARIUM(AREA). In latin ALTA means HIGHT (ALTI tude). TALL (which is an anagrame of ALTA) means INALT in romanian language (IN+ALTA in latin). The word Romanian People comes from being a citizen of ROMAN empire 2000 years ago (POPULUS ROMANUS). Populus - Population -People (from french)- Popor (in romanian). ReplyDelete Replies Anonymous9 October 2022 at 15:28:00 GMT+13 What rhetorical nonsense are you speaking? Is this a joke? Delete Replies Reply Reply Peter Gainsford1 January 2020 at 09:58:00 GMT+13 PSA: for the record I generally only remove comment if they're spam (I haven't had any promoting harming others, yes, but I'd remove those too). I don't plan on removing irrational posts unless they become seriously disruptive. ReplyDelete Replies Anonymous9 July 2020 at 10:02:00 GMT+12 Thanks, they are kind of entertaining. Very well researched article, I wish more of the information on the internet was like this. Delete Replies Reply Reply CorwinS1 April 2020 at 20:32:00 GMT+13 Apparently \"rhino\" is a British slang term for money that dates back to the 17th century. This obviously indicates that soldiers in the English Civil War were literally paid in rhinoceroses. ReplyDelete Replies markfili4 September 2020 at 11:57:00 GMT+12 Yesssss! Good, good... Delete Replies Reply Reply mr Gill25 December 2020 at 02:52:00 GMT+13 Well at one point most members of the army did not get paid as we think of it. The loot that was collected was doled out at the end of the campain, war paid for it self. Now even rich men might run out of money but with the army feeding you and repairing if not purchasing your gear a small amonth of coinage was not a big thing you wernt living on it. (emily) ReplyDelete Replies Reply mapalem12 February 2021 at 04:51:00 GMT+13 Thank you for your well researched blog. The salt for salary also made no sense to me. When I read about the salary stub found at Masada and saw how little a Roman soldier earned, I thought of the salary/salt connection as a phrase like pin money that might have been facetious or even critical ReplyDelete Replies Reply Nimbusaeta5 March 2021 at 08:31:00 GMT+13 Thank you for this! ReplyDelete Replies Reply ali D11 March 2021 at 21:01:00 GMT+13 Really excellent article - I've long thought the being paid in salt was completely unpractical - imagine tripping up on the way home and dropping your salary in a puddle.... Do you know anything about the \"sponge on a stick\" theory? I understand that that's pretty vague as well but every kid in school gets told this. Cheers ReplyDelete Replies Reply Fr.Ronald8 June 2021 at 01:53:00 GMT+12 Excellent article! Now I have to re-write an old homily that used that myth, but truth is always better than fiction. ReplyDelete Replies Reply ktschwarz27 June 2021 at 20:34:00 GMT+12 The American Heritage Dictionary must have updated its online entry for salary, since the etymology now appears to be taken directly from this post: \"... from Latin salārium, salary or stipend paid to a military or civil post holder (probably originally “money given to soldiers for buying salt and other such things, supplementing a grain ration”), from neuter of salārius, relating to salt ... For the semantic development, compare Greek opsōnion, salary, wages, from opson, relish, fish, or other tasty food to be eaten as accompaniment to bread, and ōneisthai, to buy.\" I bet you'd rather see \"possibly (although evidence is lacking)\" instead of \"probably\", but at least it's an improvement over all the print editions, which just stated \"money given to Roman soldiers to buy salt.\" Progress! Merriam-Webster used to have \"money given to Roman soldiers for salt\" in old editions, but that was removed quite a while ago: from the 10th Collegiate Edition (1993) onward, they say only \"... from Latin salarium pension, salary, from neuter of salarius of salt\", without giving any guess as to why, just like the Oxford Latin Dictionary. Maybe somebody there checked the Latin dictionary! Point to MW. ReplyDelete Replies Peter Gainsford27 June 2021 at 22:01:00 GMT+12 Thank you for telling me! This is awesome news! Though I'll say I'm surprised to see the comparison with opsonion taking hold: I don't think I've got enough authority to lay down the law on that by myself, so I hope they found someone else to corroborate the idea. Delete Replies Reply Reply Merennulli8 July 2021 at 07:55:00 GMT+12 The \"Pliny the Elder\" translation is wrong on Wikipedia, but also here. In the full section you both are translating a piece of, Pliny the Elder's Natural History is telling the story of Ancus Marcius, a king of Rome 600+ years before Pliny the Elder's time. According to Natural History, Ancus Marcius took a salt pit (\"salinas\", this probably refers to the ones at Ostia which he founded), had people (\"populis\", not specifically soldiers) carry the salt to Sabinos, and gave them a \"congiario\" (literally \"largess of the emperor\"...despite him being a pre-imperial king) of \"mola salsa\" (salted flower, most likely to be used ritually rather than as food). Natural History claims this was called the \"Salariae viae\" (\"salary of the way\") and that the etymology of \"salariis\" (\"salaries\") is clear from that name. In short, Natural History claims a 600+ year old one-time gift of salted flour after labor involving salt is the origin of the term. I'm not sure how reliable Natural History is on the subject. You may have noticed I credit the book and not Pliny the Elder with these claims - Pliny the Elder had his servant copy down things he dictated after another servant read them from other sources. We don't actually know who the source of this etymology is or how much it was affected by his dictation process. The \"salariae viae\" phrase is used as if it would reasonably be understood by the reader but requires 600+ year old historical knowledge, limiting the audience unless it was from a much older source. ReplyDelete Replies ...26 January 2022 at 13:45:00 GMT+13 Exacto!! esa es la verdadera traducción. Delete Replies Reply Peter Gainsford26 January 2022 at 17:56:00 GMT+13 This is a very muddled reading of the passage. Pliny reports many things in this passage, because it's a string of mini-anecdotes. They're mostly unrelated, except that they all involve salt in some way. Ancus Marcius is involved in only two of them. In order, starting from 31.88 and going to the end of 31.89, they are: 1. Farm animals enjoy salty pasture, and salty pasture produces better cheese 2. Salt is an important metaphor because it's important to civilised life 3. sales (literally 'salts') means 'wit' 4. Salt has something to do with magistracies and military service, and that's where the word 'salary' comes from 5. Salt was important in olden times, and that's why the trade route to Sabine territory, the Via Salaria, is named after it 6. Ancus Marcius once gave a largess of 6000 modii of salt to the people 7. Ancus Marcius was the first to build salt refineries (salinae) 8. In olden times people used salt as an accompaniment to bread 9. Sacrifices always involve the use of salted flour Your account mixes Ancus Marcius up with items 4, 5, and 9, but they're all distinct anecdotes. Only point 4 has anything to do with the popular modern myth that has arisen around that line, and if you want to make a case that I translated the line incorrectly, you'd better focus your attention on that line and not on a hodge-podge of unrelated anecdotes half a paragraph further down. Delete Replies Reply ...8 February 2022 at 13:44:00 GMT+13 No concuerdo con que los puntos 4 y 5 no estén relacionados. Quizás pueda parecer así en la traducción al inglés, puesto que han utilizado el punto y coma para separar ambas partes después de agregar horriblemente un paréntesis. Esa traducción honestamente es espantosa. Viendo el original en latín es más que claro que tanto el punto 4 y 5 son en realidad el mismo punto, se entienden tal cual y no hay motivo real para entenderlo como dos \"mini anécdotas\" separadas. Delete Replies Reply Reply Stavros22 July 2022 at 01:42:00 GMT+12 Thanks for the great article! The current 'salary' article in Wikipedia cites this article, but it would be nice to get some additional solid sources which agree with you, given the huge number of places -- including many serious dictionaries -- which continue to repeat the false story about payment in salt and the unsubstantiated story about a salt allowance. Are there scholarly articles on the topic? Or is this just folk knowledge among classicists with no citable source? (cf. the Sparkes article I cite in the Wikipedia article on 'idiot') ReplyDelete Replies Peter Gainsford22 July 2022 at 11:31:00 GMT+12 There's an important methodological point embedded in this question! It would be folk knowledge if people believed a thing like salt salary was real; in this case, it's not believing that it was real, because there's zero evidence for it. So the real situation is: most scholars are aware that a bit of folk knowledge is not true. The 1968 Oxford Latin Dictionary entry, for example, seems to be coming from that position. It sounds a bit like you're looking for evidence for the non-existence of salt salary, and evidence for non-existence is a non-trivial idea. The whole point is that there's nothing to point at. That's why in this piece, I focused on trying to work out how the idea did come into existence. It may be that evidence for the story can be pushed earlier than 1771, of course, and I'd be very interested if anyone could find something earlier! But as far as scholarly articles are concerned, it'd be an odd use of time to devote an article to showing that there's no evidence for an idea, when no one in the field is pushing for that idea anyway. It'd be like an article showing that there's no evidence that curse tablets work. I doubt any journal would print something along those lines. Delete Replies Reply Stavros23 July 2022 at 02:45:00 GMT+12 I understand that **classical** scholars do not believe this. The problem is that many others do, and the story is widely repeated not just in popular accounts, but even in the scholarly literature of geology, economics, chemistry, etc. So it is worth the effort to debunk. I agree that proof of non-existence is pretty much impossible. However, showing the origin of the idea (as you have done) is eminently possible, as is showing that something is implausible. As for what journals would publish it, you have yourself mentioned Ridley's article on the non-salting of Carthage (or more precisely the non-evidence for the salting of Carthage). There's also Sparkes's article on the non-use of \"idiot\" in Ancient Greek to mean \"civically inactive\" (which I cite in the Wikipedia article on 'idiot') (or more precisely the lack of attestation of such use). Journal publication is more archival, benefits from peer review, and invites scholarly discussion more than a blog does. I'd think this article was eminently suitable for publication in *some* journal, though probably not *Past & Present*, *TAPA*, or *AHR*. Delete Replies Reply Reply Anonymous15 August 2022 at 06:45:00 GMT+12 I spent a few minutes drafting a reply to one of the irrelevant and inexplicably self-confident comments above but it makes much more sense to just ignore them and thank you for an interesting and well written article. This myth has frustrated me for a while and I'm glad to have a place to point people for a comprehensive explanation. ReplyDelete Replies Reply MaryFrances Yaeger15 October 2022 at 07:37:00 GMT+13 I applaud this interesting article and every comment to date has helped in my understanding of the history of salt and its importance in our physiology, our cultural and religious heritage, including relevant myths and subsequent paradigms! Thank you so much for your collective insights and opinions! I believe that Jesus taught His followers to be “SALT AND LIGHT” in such simplicity and succinct wisdom! It was not a suggestion, but a loving mandate! I’ll definitely think about salt today, as I sprinkle some generously on my chicken, as I apply my hearty barbecue sauce, and as I bake my potato and slather it with salted butter! I will pray over my meal and thank the Lord, who is my eternal Salvation and my Best Friend! You are all very intelligent and very feisty gentlemen! Full of knowledge, yes, and this is impressive! Wisdom is Sophia, right? Knowledge must be respected and cherished as vital, developed from the outside in. Wisdom comes from within, however from the spiritual core! Ah, Wisdom and Womb! Hmm! Lord, help me grow in knowledge, and especially in wisdom! Wisdom takes humility, wisdom dances with laughter! Merry hearts, gentlemen, HEAL like a medicine!” ReplyDelete Replies Reply Anonymous20 January 2023 at 12:03:00 GMT+13 The plain meaning of \"the annual salt revenue, which was once given to soldiers.\" in the 1777 dictionary seems like it would be \"they paid the soldiers with revenues raised by taxing salt,\" and not any of the more fanciful interpretations spread since. ReplyDelete Replies Reply Hans11 February 2023 at 01:27:00 GMT+13 Last week's edition of the Economist quoted you on this issue; https://www.economist.com/culture/2023/02/02/some-well-known-etymologies-are-too-good-to-be-true (And please delete the previous version of this comment, I put the wrong link in it.) ReplyDelete Replies Peter Gainsford15 February 2023 at 16:49:00 GMT+13 Thanks for the heads-up! Delete Replies Reply Reply Anonymous10 April 2023 at 12:25:00 GMT+12 I'm just speculating here, but could salarium be used as a catch-all to describe salted fish and meat, etc.? According to the Oxford Roman Economy project the Romans produced a *lot* of salted fish which I presume was mostly traded for coin. Hence, it's the sort of thing for which there might be a monetary allowance and it would make \"salarium\" a sort of parallel to opsōnion. ReplyDelete Replies Peter Gainsford18 April 2023 at 09:32:00 GMT+12 I'd say that's very possible, yes -- it'd be nice to have the corroboration for it, of course. I agree that the idea of a parallel to opsōnion is attractive! Delete Replies Reply Reply Add comment Load more... Newer Post Older Post Home Subscribe to: Post Comments (Atom) About me Dr Peter Gainsford is a classicist based in Wellington, New Zealand. He is the author of Early Greek Hexameter Poetry (Cambridge, 2015) and has written articles on many aspects of Greek language, literature, and myth. ORCIDMastodonGoogle Scholar More about me Search This Blog Blog Archive ► 2023 (17) ► December (2) ► November (3) ► October (1) ► July (1) ► June (2) ► April (1) ► March (4) ► February (3) ► 2022 (18) ► December (3) ► November (1) ► August (1) ► July (3) ► June (1) ► May (2) ► March (3) ► February (2) ► January (2) ► 2021 (14) ► November (3) ► October (2) ► September (1) ► August (2) ► July (1) ► May (2) ► April (1) ► March (1) ► February (1) ► 2020 (21) ► December (1) ► October (1) ► September (1) ► August (2) ► July (2) ► June (3) ► May (3) ► April (4) ► March (1) ► February (2) ► January (1) ► 2019 (18) ► December (1) ► November (1) ► October (3) ► September (2) ► August (1) ► July (2) ► June (3) ► May (1) ► March (1) ► February (2) ► January (1) ► 2018 (23) ► December (2) ► November (3) ► October (1) ► September (2) ► August (2) ► July (2) ► June (2) ► May (2) ► April (3) ► March (3) ► February (1) ▼ 2017 (19) ► December (2) ► November (2) ► October (2) ► September (2) ► August (2) ► June (1) ► May (1) ► April (1) ► March (4) ► February (1) ▼ January (1) Salt and salary: were Roman soldiers paid in salt? ► 2016 (22) ► December (3) ► November (2) ► October (1) ► September (2) ► August (2) ► July (1) ► June (1) ► May (1) ► April (2) ► March (2) ► February (3) ► January (2) ► 2015 (6) ► December (2) ► November (3) ► October (1) Classics Blogs Classics at the Intersections (Rebecca Futo Kennedy) The Guardian - Graeco-Roman news It's All Greek to Me (@annapjudson) Mycenaean Miscellany (@theo_nash) The Philological Crocodile (@PhiloCrocodile) @res_australes rogueclassicism (@rogueclassicist) Sententiae antiquae (@sentantiq) Society for Classical Studies, USA Tales of Times Forgotten (@spencemcdaniel) Reviews BMCR Classics for All Reviews Simple theme. Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=38803589",
    "commentBody": "Salt and salary: Were Roman soldiers paid in salt? (2017)Hacker NewspastloginSalt and salary: Were Roman soldiers paid in salt? (2017) (kiwihellenist.blogspot.com) 151 points by throwaway167 22 hours ago| hidepastfavorite149 comments empath-nirvana 18 hours agoThere are lots of slang words for wages today that have to do with food: \"cheddar\", \"cabbage\", \"dough\", \"bread\" \"bacon\" -- maybe future etymologists will assume that we were paid in bread and cheese. reply throwaway1492 17 hours agoparentI never understood the \"salt scarcity in antiquity\" idea. As in they could just use a splash brine water from the sea if you physiologically need salt. And transport sea water inland as needed. reply Ekaros 10 hours agorootparentIt really wasn&#x27;t any more scarce than let&#x27;s say wheat. And price seems to have been around same level with wheat take or leave some depending on distance from production.What really made it special was that it was commodity with possibly limited production locations, that kept extremely well and was in steady demand. So it is one thing that everyone uses and is relatively easy to tax. And the price likely was much more stable compared to food and other goods.The large scale demand also lead to it being desirable as military target, once you control the production you are good. reply andrewflnr 17 hours agorootparentprevTransporting water from a well is already a pain in the neck. You think they&#x27;re going to transport bulk seawater deep inland just to make their food soggy? reply kuhewa 4 hours agorootparentSeawater is alive, from the sulfate reducing microbes you&#x27;ll have rotten eggs flavoured brine before the trip is over reply thaumasiotes 11 hours agorootparentprevNo, but it&#x27;s very easy to make the seawater into salt and then transport the salt. reply askvictor 2 hours agorootparentDefine easy reply andrewflnr 10 hours agorootparentprevNo kidding, that&#x27;s why that one actually happens. reply ponector 17 hours agorootparentprevMain purpose for salt was good preservation. That&#x27;s why it was extremely valuable. reply lolinder 6 hours agorootparentAccording to the article, a Roman soldier could buy about 15 (modern) pounds of salt with a single day&#x27;s wages.Comparisons are very hard, but to put that in a bit of perspective: at an average salary of $60k&#x2F;yr, a typical American today makes $165&#x2F;day. So the cost in time for a Roman to buy salt would be roughly equivalent to if the price for salt today were $11&#x2F;lb.That&#x27;s more expensive than it is today (I just bought salt for ~$2.50&#x2F;lb), but it&#x27;s a far cry from extremely valuable. reply Retric 5 hours agorootparent$60k&#x2F;yr is a poor comparison because even modern Solders get paid less than average at a base but get room and board + many benefits.An Army private starts at, $1,833&#x2F;mo that’s 21k&#x2F;year. Even corporal is only getting $2,393&#x2F;mo to start and cap at $2,906&#x2F;mo w&#x2F; 10 years. https:&#x2F;&#x2F;www.military-ranks.org&#x2F;army-paySo a modern soldier starts at ~24lb&#x2F;day of your 2.50$&#x2F;lb salt, but also has much cheaper alternatives. reply lolinder 4 hours agorootparentLike I said, comparisons are hard. The point is that it wasn&#x27;t a luxury good or an exceptionally valuable commodity, it was affordable. reply Ekaros 12 hours agorootparentprevI think in that statement the \"extremely\" carries way too much weight.As it would imply price to be very high. Which then would mean that regular people would not have access. But they also widely used salt. So it could not have been extremely valuable as we understand. Or maybe gasoline is extremely valuable commodity now... reply ponector 11 hours agorootparentYes, you can compare it with today&#x27;s oil trade.Salt production and trade have been restricted, usually state-owned monopoly. Cities with salt mines like Salzburg became extremely rich, like oil countries today.Is gasoline valuable? Yes. Is affordable? Yes, but not for everyone. Same with salt back than. reply Turing_Machine 15 hours agorootparentprevYep. Some combination of drying, salting, and smoking was pretty much it.No refrigeration. No freezing. No canning.They used a lot of salt. reply jvanderbot 16 hours agorootparentprevAbove comment said it was not scarce, not that it wasn&#x27;t valuable. reply ponector 12 hours agorootparentIf it is not scarce - the value is low. reply whoknowsidont 12 hours agorootparentBreathable air is pretty valuable, at least for me. And it&#x27;s definitely not that scarce on this planet. replyHayvok 16 hours agoparentprevNo doubt the phrase “bring home the bacon” will outlive our civilization and future historians will confidently assert to one another that we were all paid in rashers of bacon. reply geodel 15 hours agorootparentI wonder was that because bacon slices looked like currency notes? Or maybe crisp currency gave good feeling like the crisp bacon? reply breischl 15 hours agorootparentTangentially, this makes me think about how much recently-introduced slang is for basically-random reasons like \"it happened fit well into the rhyme and meter of a popular song\" or \"somebody attractive&#x2F;famous said it\" or \"it sounds cool and kids&#x27; parents hate it\".Maybe there&#x27;s a good reason for the bacon thing, or maybe some guy just tended to buy bacon on payday. &#x2F;shrug reply thaumasiotes 11 hours agorootparentprevPhrasing can matter. Here&#x27;s a lyric from the song \"Kilkelly\":Because of the dampness, there&#x27;s no turf to speak ofand now we have nothing to burn.This sounds a bit less serious to modern American ears than it should. We think of winter as being annoying, not dangerous.In China, where a common word for wages is 薪资 -- \"fuel and resources\" -- people are more likely to intuit that going without fuel is best not attempted, even though they&#x27;ve never experienced it either. It makes for an odd example of poetry coming across better in translation than it does in the original language. reply rex_lupi 17 hours agoparentprevAlso \"Peanuts\" reply KeplerBoy 10 hours agoparentprevweren&#x27;t people actually paid in bread and cheese? reply kouru225 13 hours agoprevI think the author goes a little too far here in equating the monetary value of salt with how valuable salt was. Salt had serious religious and mythological value. In nearly every single culture salt is said to ward off demons. Salt was used in courting rituals where new couples would process their love by licking the same salt rock. In some cultures, salt was part of the burial process. There isn’t a single salt production site in the world that isn’t named something like “salt place” or “place where the salt comes from.” Sure, the fact that salt was involved in all these traditions and rituals probably indicates that salt was generally available to most people, but clearly it was still very important to them.If you’re wondering why salt was so mythologically important, just go over to your counter, put some salt in your hand, lick it, and try to imagine how you’d describe the taste to someone who’s never tasted it before. You can’t. Salt is its own thing and there’s nothing else like it. reply vlz 3 hours agoparentPossibly, but just because there is a lot of scattered evidence for salt being used in religious&#x2F;ritualistic ways over the ages, doesn&#x27;t mean it was important everywhere for that reason and at every time.I like the article for sticking to the textual evidence we have and concluding \"we don&#x27;t know\" instead of speculating.However if you have anything concrete on the religious meaning of salt in Ancient Rome that would be interesting. reply zeroonetwothree 4 hours agoparentprevThat’s true of every flavor. reply tsimionescu 2 hours agorootparentNot sure that it really matters one way or the other, but all other taste buds are triggered by a whole class of substances, not just one or two. Any sugar is sweet (sucrose, fructose, even lactose), as are a bunch of other compounds (aspartame and the other artifical sweeteners). Any acid is sour. Savoriness&#x2F;umami is caused by any compound containing glutamate. Bitterness is harder to pin down, but there are many compounds which trigger the taste.However, saltiness is not actually limited to NaCl. The part that triggers the taste is the Na ion, and other similar ions actually trigger the same taste - KCl is the most common, but compounds from Li, Rb, Cs, Ca can all trigger salty tastes. reply xiconfjs 15 minutes agoprevhttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Betteridge%27s_law_of_headli...\"Any headline that ends in a question mark can be answered by the word no.\" reply tomaytotomato 21 hours agoprev\"Salarium Argentum\" - salt money (or money&#x2F;allowance towards salt)It would be funny to see my payslip with a tex deductable section saying \"Salt allowance\". Imagine if modern day employers paid their employees in other things rather than currency that were tangibly valuable (would be chaos I am sure). reply c54 21 hours agoparentWe have things like a fitness stipend, wellness stipend, learning stipend… all with cash value earmarked towards a specific use. reply GeoAtreides 3 hours agoparentprevMany countries have meal (food) vouchers besides salary:[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Meal_voucher[2] https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;AskEurope&#x2F;comments&#x2F;793aud&#x2F;are_meal_... reply joshspankit 20 hours agoparentprev> Imagine if modern day employers paid their employees in other things rather than currency that were tangibly valuableAs highlighted in other comments, this happens all the time. For example there are huge tax benefits to getting paid in “non-taxable” ways. reply ant6n 20 hours agorootparentOr suddenly there’s sales tax on top of the income tax. reply the_mitsuhiko 15 hours agoparentprevFun fact: there is a beer allowance (Haustrunk) in Austria if you a work in a brewery. And that allowance is tax free. reply Qwertious 3 hours agorootparentThat sort of thing makes more sense than you&#x27;d think - if people earn money at a brewery, spend money on beer, and handle decisions about large amounts of beer on a daily basis then it encourages them to perhaps find an excuse to &#x27;spoil&#x27; beer then take it home.If everyone gets a ton of free beer anyway, then most people won&#x27;t be tempted to nick some booze.Making it tax-free makes some sense, because nicking the booze is also tax-free. reply 0xDEADFED5 3 hours agorootparentbased on the breweries in the USA i&#x27;ve visited, it&#x27;s very common here, too. i somehow doubt it&#x27;s because all the employees would otherwise be beer thieving criminals, though. reply lr4444lr 19 hours agoparentprevLike stock options? Yeah, even that is contentious. reply Detrytus 21 hours agoparentprevMy mom worked as an accountant for a big meat-processing factory and she was partially paid in their products: meats and sausages :)Also, coal mine workers in my country used to get few tons of coal once a year (useful for heating their homes in winter).Unfortunately, for tax purposes your employer is supposed to calculate a cash value of those bonuses, so you don&#x27;t actually pay your tax in sausage. reply toyg 20 hours agorootparentEx-wife worked in a brewery and had a weekly beer allowance. She didn&#x27;t drink, so she&#x27;d pick it up once every 4 months and give it to her ecstatic flatmates. reply hef19898 15 hours agorootparentStill part of union negotiated salaries for brewery workers in Germany. Since almost all breweries also have non-alcoholics, that part of the payment is aparebtly more often taken in the form sparkling water and the likes.I can vividly imagine your wife&#x27;s flatmates joy once a quarter so! reply Turing_Machine 15 hours agorootparentprevI think it&#x27;s also common in candy factories. New employees tend to eat a lot, but after the first week or so consumption declines considerably. reply nradov 15 hours agorootparentprevIn the former USSR, jobs in meat processing and sausage making plants were desirable because even though wages weren&#x27;t very high the workers could steal a lot. This was so normalized that they didn&#x27;t even think of it as stealing, it was just \"carrying out\". reply SXX 2 hours agorootparentCarry from the factory every nail.You are the owner here not a guest.(c) Rough translation of USSR joke. reply doctoboggan 17 hours agoparentprevSounds exactly like health insurance. reply throwaway167 19 hours agoparentprevCentral bank digital currencies where the currency can be spent only on certain goods, and can be set to expire. reply ipsum2 21 hours agoparentprevYou might find a SALT deduction when filing taxes. reply nemo44x 21 hours agoparentprevA lot of people get paid in part with stock options that don’t even have tangible value necessarily. reply deusum 18 hours agoprevWe&#x27;re still finding roman coins in the UK, that should be enough physical evidence against the idea. But, I imagine salt would be useful for bartering with some of the \"barbari\" the soldiers encountered. reply Ekaros 11 hours agoparentOn other hand UK being island next to Ocean, a local salt production must have happened for long time. As such I&#x27;m not entirely sure the locals would have needed to source it from Romans. Then again if Romans took over the production it changes things. reply nonrandomstring 12 hours agoprevSalt tablets were issued to anyone marching long distances in hot conditions, as recently as Vietnam and Korea. Modern rations have \"isotonic\" packs.As for wages of ancient soldiers, the Greeks (dunno about Romans) took spoils, including women and slaves. reply thaumasiotes 11 hours agoparentSalt is necessary for anyone who&#x27;s producing a lot of sweat. The alternative is that you run out of salt and die.This is also why Gatorade is salted. reply jdjdjdkdksmdnd 20 hours agoprevi think the topic of salt is misunderstood. ancient people didnt eat as much and they worked more and harder and also didnt have access to air conditioning. sweating more would deplete electrolytes and entering into dietary ketosis frequently would lead to a major decline of electrolytes. i think ancient people needed salt because they would get sick without it. but eveyone says its because they liked the taste reply qwytw 11 hours agoparent> ketosis frequentlyConsidering their diets, which were very high in carbohydrates (not sugars though) compared to modern diets that seems highly unlikely. Can you actually ever enter \"ketosis\" if you&#x27;re mainly eating bread and other grain products?> i think ancient people needed salt because they would get sick without it. but eveyone says its because they liked the tasteThey needed salt because there weren&#x27;t that many other ways to preserve food. I doubt this has much to do with taste. Also modern people need salt too..> ancient people didnt eat as muchThat&#x27;s debatable. According to our records medieval people did sometimes eat quite a lot. I guess the problem is that it varied a lot. You either had too much or to little food all of the time. reply jdjdjdkdksmdnd 8 hours agorootparentancient rome and medieval europe are really different. you can enter ketosis every day on a diet of carbohydrates by eating one or two times a day, eating less or engaging in exercise would make that ketosis deeper and longer. all of this could have applied to most people until relatively recently. reply mastazi 8 hours agoparentprevI always thought it was important mostly for food preservation. reply Nik09 19 hours agoprevShould start a gold standard salary, a possible substitute to Roman era reply massifist 8 hours agoprevWhile I do appreciate historical accuracy, I find this news very disconcerting!And here I was hoarding all this salt, hoping for our return to the salt standard. :-(Well ya can&#x27;t win em all! And at least I&#x27;m not on a low sodium diet. reply aurizon 19 hours agoprevRome had a growing problem - the lack of fiat currency as the reserves of silver and gold were exhausted. They had to find alternates. They tried debasement(adding lead&#x2F;zinc&#x2F;copper( but that created the early quick tests. Same now. We have so little gold that going to a gold backed method in circulation = gold would wise to $20,000 more or less per ounce at which point sea water extraction is economic. (currently it costs more to pump 1 ton of water over a 2 foot hill that the value of the gold therein) reply qwebfdzsh 15 hours agoparent> They had to find alternates.Except they didn&#x27;t use salt for that. That&#x27;s a myth. Did you read the article?> the lack of fiat currency as the reserves of silver and gold were exhausted.True, they had the same problem in the middle ages. Since we know massively more about the middle ages than Ancient Rome AFAIK they partially solved it through a mix of barter and credit (accounting was done using currency bit might have never changed hands in reality). As long as most trade is local that must be a pretty effective system. reply aurizon 3 hours agorootparentsupplies, like salt, grain, etc were used because often there was not enough silver on hand = get salt etc - better than nothing and you can sell as you travel reply pferde 21 hours agoprevFrom the article:\"Later on, Scheller and Freund realised that Pliny didn’t say what Facciolati-Forcellini claimed he did, but they liked the idea so they instead supported it\"I wonder, how many historical so-called facts are similarly based on a whim of some historiographer or other? We should take everything with a grain of salary... I mean, salt. :) reply marcosdumay 19 hours agoparentOne of my favorite ones is that the entire idea that any ancient population ever believed that the Earth is flat seems to come from a semi-fictional biography of Columbus from the 18th century.Somehow people just read an interesting book and decided that their great-grandparents all believed on that stupid thing. reply bombcar 18 hours agorootparentThe sad thing is it obscures the real issue people were bringing up with Columbus - they said the world was much larger than he thought so he would starve and die before getting to the indies.They were right, too, as the size of the planet was pretty accurately known. He just lucked out that someone had left a continent for him to run into. reply PurpleRamen 18 hours agorootparentDidn&#x27;t he also never reached the continent, just some islands, which then were used as a starting point for more expeditions? reply hn_throwaway_99 18 hours agorootparentNo, he definitely reached Central and South America. But even until his death he had insisted he had reached the East Indies. reply runeofdoom 14 hours agorootparentI think I recall that his grants from the Spanish crown (and part of his fame) were tied to him having made it to the East Indies. So there would have been motivation for him to stick with his stories. reply PurpleRamen 18 hours agorootparentprevAh, according to Wikipedia, he only reached the continent on his third and fourth voyage. On his first two expeditions, he was only cruising around the Bahamas, Cuba, etc. reply throw0101b 15 hours agorootparentprev> One of my favorite ones is that the entire idea that any ancient population ever believed that the Earth is flat seems to come from a semi-fictional biography of Columbus from the 18th century.See also the religion-science conflict thesis, which was popularized by Draper and White:* https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Conflict_thesis> My guests today are David Hutchings and James C. Ungureanu, co-authors of Of Popes and Unicorns: Science, Christianity and How the Conflict Thesis Fooled the World. David is a physicist, science teacher and writer and James is a historian of science and religion. In this interview we discuss their book and the origin and impact of the Conflict Thesis - the pervasive but erroneous idea that religion and science have always been in conflict down the ages.* https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=PkfA4v8cwYM reply acqq 9 hours agorootparentOh, there&#x27;s now a bunch of accounts claiming that Galileo was \"just\" mean to pope and therefore \"guilty\", but this is an actual pro-religion propaganda. The real sentence is preserved up to this day and is completely clear:https:&#x2F;&#x2F;hti.osu.edu&#x2F;sites&#x2F;default&#x2F;files&#x2F;documents_in_the_cas...\"heresy\" ... \"that the earth does move, and is not the center of the world\" ... \"contrary to Holy Scripture\"More detailed:\"We pronounce, judge, and declare, that you, the said Galileo . . . have rendered yourself vehemently suspected by this Holy Office of heresy, that is, of having believed and held the doctrine (which is false and contrary to the Holy and Divine Scriptures) that the sun is the center of the world, and that it does not move from east to west, and that the earth does move, and is not the center of the world; also, that an opinion can be held and supported as probable, after it has been declared and finally decreed contrary to the Holy Scripture\".Additionally, Galileo&#x27;s and Copernicus&#x27; books were finally removed from the index of the banned books only in 1835, they were on the banned list for more than 200 years, since the 1616 Inquisition&#x27;s judgment.Context: Galileo was the first person to see with his own eyes with his first of the kind self-made telescope the moons that are today known as Galilean moons and recognized them as the satellites of Jupiter in March 1610. Which convinced him that the understanding of the church was wrong. The church sentenced him in 1633 to house arrest where he remained until his death in 1642.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Galilean_moons reply marcosdumay 6 hours agorootparentOh, the Galileo thing.Politics is a complex thing where people don&#x27;t mean what they say, and their meaning change depending on who are listening, how, why and when.Heliocentrism was discovered by a joint-enterprise of two enemy churches, and only became heresy post-facto when some very good evidence arrived. But by then it seemed to really become heresy, and was punished by itself. Almost certainly the Galileo&#x27;s posture was important for that, but the society&#x27;s context was way more important.Anyway, you won&#x27;t get any good conclusion if you insist on analyzing the politicians arguments on logic or expect coherence. reply throw0101b 8 hours agorootparentprev> Context: Galileo was the first person to see with his own eyes with his first of the kind self-made telescope the moons that are today known as Galilean moons and recognized them as the satellites of Jupiter in March 1610.Which was not evidence for heliocentrism.In the early 1600s there were seven models floating around: Heraclidean (geo-heliocentric), Ptolemaic, Copernican (heliocentric, pure circles with lots of epicycles), Gilbertian, Tychonic, Ursine, Keplerian.Newton, in his Principia (1687), did not use calculus to present his Universal Graviation: rather it was carefully structured in Aristotelian form, with axioms and deductive logic. Kepler&#x27;s laws can be deduced from principles. Still no coriollis or parallax.The first inkling of the Earth&#x27;s motion comes in 1728 when James Bradley detects stellar aberration in γ-Draconis. In 1791 Giovanni Guglielmini finds a 4 mm Coriolis deflection over a 29 m drop, thus providing empirical evidence of rotation. In 1806 Giuseppi Calandrelli publishes \"Ozzervatione e riflessione sulla paralasse annua dall’alfa della Lira,\" reporting parallax in α-Lyrae. So parallax, the chief evidence for the Earth&#x27;s motion came 250+ years after Galileo.Stellar parallax was considered since at least Aristotle, as he mentions in his On the Heavens (II.14), and since it is not observed then it is reasonable to conclude that there is no motion (it took several thousand years to develop instruments to actually measure it).Galileo&#x27;s chief problems were (a) he was an egotistical jackass, and (b) he had no evidence for what he was claiming to be true. He was allowed to put forward the Copernican model \"suppositionally\", i.e., as an hypothesis, and \"not absolutely\". The latter of which, (b), Galileo admitted in his first deposition (12 April 1633): it was concluded that his book put forward the idea &#x27;absolutely&#x27;, which is where his conviction comes from.By the late 1600s most folks had switched over to the Keplerian model: not necessarily because they thought it was what was actually happening in reality, but probably because it made the math easier.For a good timeline of events, see (recently late) Michael F. Flynn&#x27;s \"The Great Ptolemaic Smackdown\":* https:&#x2F;&#x2F;tofspot.blogspot.com&#x2F;2013&#x2F;10&#x2F;the-great-ptolemaic-sma...Daniel Whitten&#x27;s \"Matters of Faith and Morals Ex Suppositione\" is also an interesting read. reply acqq 7 hours agorootparentSo yes, that&#x27;s exactly an example of the \"guilty Galileo and the good church\" false narrative.Many useless claims which don&#x27;t disprove that his sentence was literally because of:\"heresy\" ... \"that the earth does move, and is not the center of the world\" ... \"contrary to Holy Scripture\"And the church forbade his book as \"heresy\" for 200 years.He was right. The church was wrong, directly referring to the effing \"Holy Scripture\" to support its claim and played fighting \"heresy\", keeping being wrong for 200 years afterwards. It&#x27;s so clear. reply throw0101b 5 hours agorootparent> He was right.Monkeys throwing darts can also (just happen to) be \"right\" when picking stocks that do well in the market. Galileo had as much evidence in believing Copernicus was right as the monkeys.If he had simply stuck to simply arguing both sides of an hypothesis in his Dialogue, which he was asked to do by the pope in the first place, it would have saved everyone a lot of trouble. Heck, Kepler&#x27;s stuff was already around for decades, and Galileo completely ignored it (along with Tycho):* https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dialogue_Concerning_the_Two_Ch...If you want to argue &#x27;for science&#x27; then Galileo is not a good example: the only thing he just happen to be right about was that the sun was the centre of things, whereas everything else in the Copernican system (including epicycles) was just as messy as in Ptolemy. There was no practical reason to switch systems, and no evidence to think it was correct.At the end of the day the person who actually got things right was Kepler, and he kept plugging away at the problem because of this belief that the physical world reflected the spiritual realm (KGW XIII, letter 23, 35; 1595)> In this way, then, the Sun, itself at rest in the middle and yet the fount of motion, carries the image of God the Father and creator. For what creation is to God, motion is to the Sun. Moreover, it moves [the planets] in a fixed place, as the Father creates in the Son. Unless the fixed stars offered a place, thanks to their motionlessness, no movement could exist. I defended this axiom while still in Tübingen. The Sun distributes motive virtue through the medium space, in which the planets are found: just as the Father creates by spirit or by the virtue of His spirit. And from the necessity of these presuppositions, it follows that motion is in proportion with distance.See Kozhamthadam&#x27;s \"The Religious Foundations of Kepler&#x27;s Science\" and \"Theological Foundations of Kepler&#x27;s Astronomy\" by Barker and Goldstein.Going further, one needs to believe in certain metaphysical assumptions before you can even start doing what we know call science:* https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Naturalism_(philosophy)#Provid...There were plenty ideas floating around at the time, but ideas are cheap. Galileo certainly made important improvements to telescope technology, but his efforts in moving forward new models (specifically Copernican) were a dead end, and he made no practical difference to things: Kepler was already defending Copernicus in his Mysterium Cosmographicum (1e 1596), and put forward his laws in Astronomia nova (1609), a copy of which he sent to Galileo, which Galileo promptly ignored even two decades later when he published his Dialogue (1632). reply acqq 5 hours agorootparentYou still can&#x27;t deny: the church was wrong, directly referring to the effing \"Holy Scripture\" to support its claim.The Earth was never the center around which the Sun rotated. Not in 1AD, not in 1600AD, not now.If the church claimed that the \"Holy Scripture\" says that the Earth is in the center, the church was still wrong, and moreover, the \"Holy Scripture\" was wrong.The church can&#x27;t be right to claim \"heresy\" to somebody who was right then and is still right now. reply defrost 4 hours agorootparentIt was never a grand Science Vs Church issue, not at the time at least, that came perhaps later with legend.It wan&#x27;t even the case that the Pope (in person) was mad with Galileo for being used as a Simplicio caricature and figure of fun in his work.All the data used came from church funded observatories and church backed astronomers, all the main ideas from both sides of the debate came from church funded theorists.The crux of the dispute and the trial was pretty much that Galileo was a dedicated edgelord who had decades of pissing people off and making enemies on his ledger.Think less about religion Vs science and more about maverick asshole vs. faction within giant bureaucracy.Once Galileo had \"insulted the Pope\" the knives came out and his enemies struck, it was a pure show trial fueled by personal vindictiveness that came from being the target of savage biting insults. reply acqq 4 hours agorootparentStill:- the church officially wrote that the Earth is the center- that the Holy Scripture says so and- whoever says differently is hereticand the Earth was never the center. reply defrost 4 hours agorootparentNone of which had much to do with the persecution of Galileo.The Catholic Church has changed its stance on many things through time, see [history].In this instance the Church itself had officially requested a presentation be made to demonstrate various arguments for and against different viewpoints .. one of which was that the heavens didn&#x27;t rotate about the earth.It wasn&#x27;t a surprise that such a well known hypothetical should appear in a book commissioned to outline such hypotheticals. replyemmelaich 2 hours agorootparentprevAnd yet ... Pope Urban VIII was a patron of Galileo and encouraged him to write his treatise.Those who judged Galileo were part of the Roman Inquisition.Pope Urban&#x27;s hands were tied when Galileo was seen to mock the pope and church through the figure and dialogue of Simplicio.This is of course does not make the adjudication OK but you&#x27;re going too far in the opposite direction. reply PurpleRamen 18 hours agorootparentprevDefine ancient. We do know that cultures from 2000+ years ago believed in a flat earth. There is enough proof for this, and Ancient Greece and Romans discovered this to be wrong. But independent of this, there is also the claim that people in medieval Europe believed in a flat earth, again, and this was brought up as a reason why Columbus expedition would fail when he searched for supporters and ships.And in fact, such stories are not that uncommon at the 18. Century. Thinkers and scientists were fighting against the church, and they made up many fake stories to show how stupid and dangerous the church is. And AFAIK the church believing in a flat earth was one of them. reply marcosdumay 17 hours agorootparent> We do know that cultures from 2000+ years ago believed in a flat earth.There are plenty of cultures from 2000+ years ago that never bothered thinking about the shape of Earth. I don&#x27;t know of any that did bother and decided it was flat (and would really like a pointer), even though I do know of some ambiguous texts that people keep interpreting as that, but are much better explained as they not caring about the shape.It&#x27;s quite hard to do astronomy on a larger area than a single city and not discover the planet isn&#x27;t flat. And it looks like people have been exchanging astronomic findings over some longish distances for longer than they have been writing texts that we can read today. reply hnfong 6 hours agorootparent> I don&#x27;t know of any that did bother and decided it was flat (and would really like a pointer)天圓地方 (literally: sky round, land square) was a serious core idea in ancient Chinese cosmology that was most popular 2000 years ago around the Han Dynasty period.As for some serious pointers as you requested, here: https:&#x2F;&#x2F;ctext.org&#x2F;lunheng&#x2F;shuo-ri&#x2F;zhThere&#x27;s a couple paragraphs arguing that the sky isn&#x27;t round (dome-like) but \"flat\" and parallel to the land because people have travelled and have never seen the sky merge with the land. (see the paras starting \"實者、天不在地中，日亦不隨天隱，天平正，與地無異。\")You might say the ancient Chinese didn&#x27;t care enough so they ended up with wrong conclusions (or parroted whatever was told to them by even more ancient sources that just made up its cosmology), but at any rate the Chinese definitely did believe in flat earth 2000 years ago, and we have very reliable records for that one. reply Zancarius 16 hours agorootparentprev> I don&#x27;t know of any that did bother and decided it was flat (and would really like a pointer)Semitic cultures very much believed the Earth was more or less a disc-like shape that consisted of the known world (mostly in their sphere of influence, if you pardon the expression). But they also had some other ideas, like that of chaotic universal waters above and below the Earth, separated by the firmament, and supported by pillars (though you can see this idea changing somewhat by the time the book of Job was written). This is a theme that is repeated in Genesis, Isaiah, Job, and one or more of the Psalms.Not coincidentally, it&#x27;s fairly well established in the scholarly literature that this was the view of ancient Near Eastern writers, and it wasn&#x27;t until Young Earth Creationists decided to apply a degree of scientific concordism to the text where we get a more distorted view of Hebrew words like hûg inferring something other than a disc or circular inscription. It&#x27;s true that they probably didn&#x27;t care so much about the shape (unlike us), but their cosmology is definitely inferred rather strongly in the biblical texts (and in some cases from their neighbors). John Walton&#x27;s \"Lost World\" series on Genesis are a good pointer in this direction, but I&#x27;d also suggest the IVP Bible Background Commentary (Walton is a contributor) which certainly touches on this motif and draws upon other creation accounts such as those in the Ugaritic tablets, Baal Cycle, etc. The late Dr. Michael Heiser has a great lecture series you can find on YT talking about biblical cosmology that might help if you&#x27;re into that format.The link with Columbus was indeed perpetrating a myth. You get this as recently as Ray Comfort&#x27;s \"Evidence Bible\" which is filled with complete buffoonery in that it attempts to explain Columbus&#x27; motives based on his mention of Isaiah; YECs like Comfort link this to Isaiah 40:22 (again, with an incorrect reading of hûg). Columbus was motivated by his eschatology and the only reason he ever cited Isaiah was because of its dual function as prophetic-apocalyptic literature. reply alex_young 9 hours agorootparentI wonder how this squares with the Babylonian concept of a round earth. It seems odd that so much was uh, “inspired” by their texts but the shape of the earth was exempted.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Babylonian_astronomy#:~:text.... reply D-Coder 7 hours agorootparentprevIf one is being literal:> Isaiah 11:12: He will raise a signal for the nations and will assemble the banished of Israel, and gather the dispersed of Judah from the four corners of the earth.Which clearly indicates some kind of quadrilateral, or perhaps a tetrahedron. reply marcosdumay 12 hours agorootparentprevThanks. Those people seem really interesting.They were right on the way when the idea of popular astronomy (instead of only god-appointed people doing it) reached Europe, and yet they seem oblivious from it.It&#x27;s a really good reminder that even on the era of large empires, culture was still very fractally distributed. reply lehi 15 hours agorootparentprevChinese astronomical records were extensive, accurate, and continual for more than 3000 years. They believed the Earth was flat and square until the 17th century. reply poizan42 9 hours agorootparentprevIn Old Norse mythology, the Midgard Serpent (Jörmungandr) encircles the earth, so it would seem they believed the earth to be a disc. reply genman 15 hours agorootparentprevIf you read a story about the edge of the world then you have an example of an \"flat earth\" believer.Regardless of shape, I think the most fundamental shift occurred with the idea that the Earth is not a center of the universe. reply hef19898 13 hours agorootparentEarth at the center of the universe was one, the sun turning around earth another. The latter so was known to be most likely true for a long time by everyone who worked on astronomy. reply WalterBright 10 hours agorootparentWhen I was little, I just assumed that the sun revolved around the Earth. I recall not believing it when I was told the converse. reply PurpleRamen 16 hours agorootparentprev> There are plenty of cultures from 2000+ years ago that never bothered thinking about the shape of Earth.For those, we don&#x27;t have any kind of indicator what they believed about earths shape, so why do they matter?The way you phrased your comment indicated that you kinda believe that now one at any point in early history really believed in a flat earth, and it&#x27;s all just a story made up later. reply robertlagrant 11 hours agorootparentHalf this comment section is saying that. reply russdill 12 hours agorootparentprevPeople thought Columbus&#x27;s voyage would fail precisely because they believed the Earth to be a sphere and thus the distance from Europe to India was too far to traverse. Columbus tried to convince everyone it would succeed by claiming the Earth was instead pear shaped.https:&#x2F;&#x2F;sacred-texts.com&#x2F;earth&#x2F;boe&#x2F;boe26.htm reply whiddershins 18 hours agorootparentprevFor example, many people today believe that we only use 10% of our brain.But it wouldn’t be fair to say that’s what our “culture” believes to be true. reply jncfhnb 16 hours agorootparentIt is true in the sense that we only use about 10%, at any given time, for some definitions of percent. People just fail to understand that the other 90% is just not useful, rather than unused potential. reply Ekaros 12 hours agorootparentIs it even usable at same time? There is some level of plasticity, but in general certain parts activate for certain tasks and you can&#x27;t really make it use more parts at one time. reply spigottoday 14 hours agorootparentprevThat is news to me and interesting. Do you have a citation or link to share? reply srinivgp 12 hours agorootparent\"people only use about 2% of the volume of their house at any given time\" reply melagonster 7 hours agorootparentprevscientists distinguished function of different regions in our brain, so human doesn&#x27;t need to use all region is a more obvious idea now.I can&#x27;t recommend literature, a probably source is in medical textbook. reply GuB-42 9 hours agorootparentprevI thought bullshit too. After all, why would chemical reactions in the brain stop? But after a bit of thinking and Googling, I stumbled upon thishttps:&#x2F;&#x2F;www.ucl.ac.uk&#x2F;news&#x2F;2020&#x2F;aug&#x2F;energy-demands-limit-our...Not saying that only X% of our brain is being used, but what it essentially says is that our brain is power-limited, and therefore, it has to be selective in which parts to \"turn on\". It makes a lot of sense, there are few things in life more important than energy and power management, and human brains already need lots of both. reply duskwuff 7 hours agorootparentWe have a word for what happens when people \"use 100% of their brain\".We call that an epileptic seizure. :) replySilasX 10 hours agorootparentprevAlso, when someone says, \"they knew the earth was round in the Middle Ages\", they mean \"educated people knew\". Guess what fraction were educated? Not many. So ... averaged over the population, it&#x27;s not entirely correct.It would be like saying, \"It&#x27;s a myth that in the 21st century, they were bunch of rubes who were unaware that every single object is exerting a gravitational force on them.\" Well, um, it&#x27;s a myth that physics-educated people were unaware of this, sure. But most people have no reason to learn about this or be aware of it; it&#x27;s not relevant to everyday life. reply refurb 6 hours agorootparentprev> Somehow people just read an interesting book and decided that their great-grandparents all believed on that stupid thing.Just go on social media today to see the same thing in action. reply gretch 8 hours agoparentprevI personally am very skeptical of the other salt theory - salting the earth as a means of disrupting enemy soil.I started to think about it in depth when thinking about weed control in my own backyard - should I salt my own soil?The I realized how quickly it would wash away in the next rain. And for ancient times, the sheer volume of salt one would need in order to disrupt a significant amount of land.I dunno maybe it happened, but it seems like a very very dumb way to go about it. Then again, humans repeatedly prove the magnitude of our stupidity. reply edaemon 7 hours agorootparentThis same site has an article on that very myth: http:&#x2F;&#x2F;kiwihellenist.blogspot.com&#x2F;2016&#x2F;12&#x2F;salting-earth.html...To summarize, salting an area was a real thing, but the evidence suggests that it was done to make the land more fertile and not less. The idea was to replace your enemy&#x27;s city with weeds and greenery, as if it had never existed. reply dexwiz 8 hours agorootparentprevYeah it would take a ludicrous amount to kill a field. Might as well put down gold foil weed barrier. reply giraffe_lady 7 hours agorootparentprevInterestingly, accidentally making the soil too salty to support agriculture is one of the consequences of improperly managed irrigation over longish time scales, like decades to centuries. It&#x27;s well understood now but I don&#x27;t believe that it was to ancient peoples.I don&#x27;t know how this could be weaponized or if anyone ever made a serious attempt at it. But it&#x27;s experienced as massive regional catastrophe when it does, and the cause is clear even if not understood. There are places in arizona and california where salt crystals visibly form on the surface of what used to be farmland: it&#x27;s plausible this would have been seen in the irrigation agriculture societies of mesopotamia and the ancient eastern mediterranean. Certainly easy to make the jump to fantasizing&#x2F;praying about it happening to your enemies, once you&#x27;ve seen or heard of it. reply newsclues 20 hours agoparentprevNot just history, but any field of study.Trusting experts is a problem, because experts are human. reply cf1241290841 18 hours agorootparentI would argue its over relying on complexity management solutions, so less a problem of experts but the inability or unwillingness to live with degrees of uncertainty and manage your confidence&#x2F;trust accordingly.The whole expert thing becomes really problematic once people start making circular arguments to justify their existence and then promptly overrely. So the perceived necessity or benefit of having an expert means there has to be somebody you can trust. Which you then promptly overdo without functioning checks and balances.Expertise cant stem from demonstrated conviction and cant require trust, thats is describing religion and Ponzi schemes. You arent doing experts any favor by treating them as such, at best they still all have incomplete expertise and cant shoulder your reckless levels of trust. reply toolz 20 hours agorootparentprevI believe life is most pleasant when you choose to err on the side of trusting experts, but there is something quite amusing (to me) that programmers have \"cargo culting\" and if you write code and deeply invest in any community you&#x27;ll meet experts all the time that do things just because others do them, but then you&#x27;ll meet someone in another highly trained field and just trust what they say is all evidence based.It&#x27;s rather curious how many high skill professions seem to default to trusting others based on their credentials. reply toyg 19 hours agorootparentThe problem is that, as soon as you hit moderate complexity, validating claims becomes difficult, time-consuming, and full of traps. Are you going to microscopically analyse every sausage you eat? If not, how do you trust that it&#x27;s safe? Experts. reply toolz 19 hours agorootparentI get what you&#x27;re saying, but I also understand that poses it&#x27;s own problems. Just because trusting the experts is the best we can do, does not mean that it doesn&#x27;t sometimes come with significant consequences. History is littered with people abusing their expert status to maliciously achieve some goal.The first example that comes to my mind is the Tuskegee experiment, where the US public health service and CDC allowed 100 of the 400 men in their study to die, refusing to provide them with treatment for syphilis and preventing them from getting treatment by other means, so they could study the men. They also never revealed to those men that they had syphilis. reply blowski 19 hours agorootparentIt’s why pluralism is so important, where experts hold each other to account. It’s worrying when not believing some dogma causes you personal problems. reply hef19898 13 hours agorootparentprevThat behaviour is morally beyond bankrupt and racist. Doesn&#x27;t mean the people involved were or were not experts in their fields so. reply cf1241290841 17 hours agorootparentprevYou cant actually trust that its safe to eat, you just like the idea and act accordingly. And deal with the consequences of the uncaught error cases. Tainted food still occurs.At best you can be confident &#x2F; trust that a product is created in a given process which manages certain risks through certain means.Also, finding faults in said process is a lot easier then coming up with it or executing it. Be it making a sausage that is unlikely to make you sick or coming to an expert judgement with sufficient confidence to act on.It doesnt require the ability to validate a process to find it faulty, you just have to detect an error case occurring that isnt dealt with. reply tim333 19 hours agorootparentprevI&#x27;m happy eating sausages in the UK but less so in say Nepal. No experts involved, just a look at the shops. reply hef19898 18 hours agorootparentAll things considered, Nepalese meat is propably saver than industrial one. After all, the remains seen on the sreetside are from the same day, meaning whatever meat you buy was still alive in the morning. reply tim333 18 hours agorootparentHaving eaten in both places and gotten ill in Nepal I&#x27;m skeptical. He&#x27;s a pic of a typical meat shop there. https:&#x2F;&#x2F;c8.alamy.com&#x2F;zooms&#x2F;6&#x2F;445bf76d991f4c27a9b88318955c2e3...and uk https:&#x2F;&#x2F;www.ballardsbutchers.co.uk&#x2F;sites&#x2F;default&#x2F;files&#x2F;style...note differences in cleanliness, refrigeration and so on. reply hef19898 17 hours agorootparentThe issue is not the meat, it&#x27;s the dust. Properly washing it before cooking takes care of that. Vegetables are no different in that regard. reply toyg 13 hours agorootparentprevWithout stringent laws, drafted with experts, your \"rule of looks\" wouldn&#x27;t be worth the bits it&#x27;s written on. reply cf1241290841 1 hour agorootparentThat sounds like a belief. Laws are obviously incomplete, circumvented and broken all the time. There is no substitute for common sense when it comes to not getting food poisoning. Some data:There is the show \"Bizarre Foods with Andrew Zimmern\" in which he ate weird stuff around the globe. He mentioned across several interviews that he never got sick on one of his trips by mostly just staying away from stuff like known bad tap water.At the same time he had gotten food poisoning 4 times in the last decade in the US. Including>Worst case was in Portland, Maine eating mussels at a crappy restaurant that I shouldn&#x27;t have been eating in in the first place. On the road, in the third and fourth world I have not gotten sick.https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230607121840&#x2F;https:&#x2F;&#x2F;starcasm....If i recall his rule of thumb is that if the local grandmas eat there and they dont use stuff you have to be exposed to for a while to tolerate it (rotten food &#x2F; tap water in India) you are good to go. reply toyg 18 minutes agorootparent> Laws are obviously incomplete, circumvented and broken all the time.If they were, we&#x27;d have rates of food poisoning close to Nepal&#x27;s. But we don&#x27;t. Obviously laws have to be enforced, with checks and all, but they largely work.Same for tap water: when it&#x27;s handled following expert-based rules, you don&#x27;t need to stay away from it. Sure, some can build resistance by repeated exposure, but not everyone; and anyway we don&#x27;t need to, if we employ the scientific knowledge we built over centuries. It&#x27;s like saying that births will happen even without expert assistance: sure, but chances that stuff will go horribly wrong are dramatically higher. replypaleotrope 19 hours agorootparentprev“Briefly stated, the Gell-Mann Amnesia effect is as follows. You open the newspaper to an article on some subject you know well. In Murray&#x27;s case, physics. In mine, show business. You read the article and see the journalist has absolutely no understanding of either the facts or the issues. Often, the article is so wrong it actually presents the story backward—reversing cause and effect. I call these the \"wet streets cause rain\" stories. Paper&#x27;s full of them. In any case, you read with exasperation or amusement the multiple errors in a story, and then turn the page to national or international affairs, and read as if the rest of the newspaper was somehow more accurate about Palestine than the baloney you just read. You turn the page, and forget what you know.” ― Michael Crichton reply yonaguska 19 hours agorootparentprevUsually, the cost of doing something the wrong way as a programmer is minimal compared to say- a doctor or a rocket scientist. There&#x27;s not really a governing body on how to do things, as in actual engineering- and most of the work we do is under the constraint of time. saving time means deferring decisions to others. I personally find that there&#x27;s a lot of value in sifting through and examining the myriad of opinions on a given subject, and coming to your own conclusion, but in the professional world, this a luxury for but a few. reply newsclues 15 hours agorootparentprevTrust but verify.Asking experts to provide rationale for their decisions should be welcomed by experts. reply hef19898 15 hours agorootparentAll the people I consider experts I ever encountered, where more than happy to explain their rationals. I learned a shit ton of things listening to them. reply tim333 18 hours agorootparentprevExperts plus common sense and direct evidence I think works. Experts sometimes come out with complete nonsense, especially if there&#x27;s politics involved. reply cies 20 hours agoparentprevAnd this is quite an irrelevant fact. How many people know&#x2F;remember that the US faked an attack on it&#x27;s fleet on August 4 to join the Vietnam war? reply lifeisstillgood 20 hours agorootparenthttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Gulf_of_Tonkin_incidentOne real attack August 2, one confused incident where actually no attack happened.It&#x27;s kind of true but really it&#x27;s hardly a false flag operation(paraphrasing and being kind to US administration - US Navy breached territorial waters, exchanged fire on Aug 2, then on Aug 4 they fired upon radar returns and possible signals traffic. The second time there was no actual North Vietnamese and the Navy expressed doubts but US administration wanted to escalate and both incidents became conflated. It&#x27;s not the same as for example pre-planned false flag operations. Is there a fine line? Yes. exactly where that is is hard to say. reply cies 17 hours agorootparentI&#x27;d say knowingly trying to claim an attack that did not happen is --not a false flag-- but certainly a govt conspiring to deceive \"the people\".Not sure what&#x27;s worse.And see the result: how many dead Viets? I recently heard a US TV anchor say \"all civilian deaths since WW2\" (they meant to say \"US civ deaths\").The Iraqi WMDs were a similar hoax to get \"involved\". I never trust a word 3 letter agencies again: professional lairs with zero accountability. reply jvanderbot 16 hours agorootparentI didn&#x27;t read the above to mean \"Knowingly trying to claim at attack that did not happen\". I read it as \"We aren&#x27;t sure there&#x27;s someone there, but please just shoot anyway we need to be aggressive\", and political officials portrayed that as \"Attack\" without proof.These are congruent accounts with different interpretations, and I tend to believe public speakers are opportunistic spin doctors, not world-shaping conspirators.As for \"never trust a 3 letter agency\", that&#x27;s quite an extreme viewpoint. CDC? EPA? FBI? CIA? NSA? All of them do good, even if their findings can be spun ( or are even directed to be spun ). reply cies 13 hours agorootparent> CDC? EPA? FBI? CIA? NSA? All of them do goodWhat do you think is the good? Maybe the FBI to some extend (my point was quite a hyperbole I know). But under the line they mostly do bad imho. And they spin their own \"findings\", that&#x27;s how we got into this conversation.I the plan was to invade Vietnam&#x2F;Iraq, and the attack are propped up to \"allow\" then to invade. If you think they govt was deceived by it&#x27;s own speakers, that to my is quite a stretch... The plan was to invade, and the story was made to match. That is a conspiracy in and of itself. reply jvanderbot 4 hours agorootparentIf you&#x27;re writing off CDC and EPA entirely then I think there&#x27;s no common ground here. reply genman 14 hours agorootparentprevAnd when Communists took power in Vietnam then exactly what was expected to happen happened - a large scale human tragedy with mass killings and torture and mass exodus.Saddam Hussein was a mass murderer who had murdered around 400 000 people - half of them after US refused to take him down after the Gulf War. Any excuse to get him removed was a good one. reply hef19898 13 hours agorootparentThe right time to take down Saddam was after the secind gulf war, the first one with US participation. In the first gulf war between Iraq and Iran, Iraq was an US ally.And no, removing Saddam with good reason was a really bad idea...To cut it short, in the frame on the war on terror 4.5 million people died, almost a million directly related to the war and 38 million people displaced. If you think all that was a just price to pay to get Saddam and Bin-Laden, because the Taliban pretty much won, you should re-adjust your moral compass. reply genman 12 hours agorootparentWhy should I readjust my moral compass? Because people could not forget their feuds and started to kill? The system held together by terror was pushed out of equilibrium. They had a change to find a better one but they chose not to. If anything then US has done not enough by letting sociopaths in Syria and Iran still run the show - a huge part of the deaths (I&#x27;m not going to dispute your claimed numbers) could have been averted and not only in Middle East. reply cf1241290841 41 minutes agorootparentInteresting>could haveDo you have a rule of thumb how many deaths you are willing to accept and risk on what probabilities for which futures? Also, you used the term sociopaths, do you consider yourself a psychopath? reply acqq 9 hours agorootparentprevYou probably don&#x27;t want to know:https:&#x2F;&#x2F;pulitzercenter.org&#x2F;stories&#x2F;how-us-secret-war-laos-st...\"From 1964 to 1973, the United States bombed Laos more heavily than any country on earth. The reason most Americans do not know this is because it was a secret war orchestrated by the CIA; it stands as the largest covert CIA operation to date.\"\"One team can find anywhere from three to 16 bombs in a day. The UXO Lao’s 2015 annual report states that since 1996, 1.4 million UXO have been cleared in Laos by a combined effort of UXO Lao and other UXO-clearing organizations, like MAG International. At this rate, it will take thousands of years before Laos is free of UXO.\"\"Forty percent of UXO victims are children who pick up the bombs, usually thinking they are toys.\"The US cluster bombs. Now also in Ukraine:https:&#x2F;&#x2F;edition.cnn.com&#x2F;2023&#x2F;07&#x2F;20&#x2F;politics&#x2F;ukraine-cluster-... reply hef19898 20 hours agorootparentprevThe Tonkin Incident. Well, Wikipedia has a decent enough write up of the whole Vietnam War, including the French one, including Tonkin.The problem is less that those crucial facts are not know, but rather that they get ignored in public discourse. reply cies 17 hours agorootparentExactly! Same for the --benign-- roman wages in salt story. It will get ignored. reply hef19898 17 hours agorootparentAnd than all kinds of people will use that bit for all kinds of weird theories. reply cies 13 hours agorootparentThat do not harm a million Iraqi or Viet civs.Plenty of groups believe&#x2F; promote all kinds of \"weird theories\". As long as they are harmless I dont care. The moment it involves cutting baby penisses or promoting hate I&#x27;m voicing myself out against. reply hef19898 13 hours agorootparentThe cutting baby penisis remark was totally uncalled for, and can be seen offensive. replyilovecurl 17 hours agorootparentprevIf you are ever in the Seattle area, you can tour one of the destroyers that was involved in that incident, the USS Turner Joy, which is docked right next to the ferry terminal in Bremerton. reply narag 16 hours agorootparentprevHow many people know&#x2F;remember that the US faked an attack on it&#x27;s fleet on August 4 to join the Vietnam war?BTW, have you heard of the Maine? reply buzzin__ 7 hours agoparentprevPliny the Elder and his son Pliny the Younger are also involved in debunking another historical fakery. This one is about Jesus actually existing and not being made up 100 years later and 1000 kms away, in a different country and in a different language.As much as you can prove a negative, this guys do it by never mentioning him, despite being at the right place and time, and writing about other religions and prophets.Even the wikipedia entry on this subject starts with a huge logical phalacy. reply gamblor956 6 hours agorootparentThe Elder Pliny would have been a small child when Jesus died. Not many 10 year olds in the historical record discussing religion. reply joenathanone 18 hours agoprev [–] From my research ‘Salt’ meant fool, ‘salt of the earth’ is like a ‘bless your heart sort’ of matter. The implications for Salary would mean that the soldiers were being fooled by accepting what they were given as payment. They also say the Lord works in mysterious ways. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This article debunks the popular myth that Roman soldiers were paid in salt, providing evidence that they were not actually compensated with it.",
      "It explores the origins of the word \"salary\" and its connection to salt, shedding light on the historical use of salt as currency in Ethiopia and other cultures.",
      "The article emphasizes the importance of knowledge and wisdom in debunking myths, and briefly mentions discussions and comments on related blog posts and social media accounts in the field of classical studies and ancient history."
    ],
    "commentSummary": [
      "The articles and discussions explore a variety of subjects, including the payment of Roman soldiers in salt and the significance of salt in ancient societies.",
      "They also touch on topics like the belief in a flat Earth in ancient cultures, the Galileo affair, and the conflict between religion and science.",
      "The discussions provide multiple perspectives and address related subjects such as food safety, historical events, and the reliability of information provided by experts and governments."
    ],
    "points": 151,
    "commentCount": 149,
    "retryCount": 0,
    "time": 1703846056
  },
  {
    "id": 38806473,
    "title": "Ghosting in Tech Recruiting: Lack of Communication Leaves Candidates in Limbo",
    "originLink": "https://medium.com/@k0ryk/everyones-getting-ghosted-dbf0fbaf161",
    "originBody": "Everyone’s Getting Ghosted The new normal in tech recruiting Kory Kirk · Follow 6 min read · Sep 3 -- The company I interviewed for was up and coming, YC funded, now profitable, a well-known player in the generative AI space. I was excited for the opportunity. They have many in-house recruiting resources and a policy to not ghost candidates. Yet, I got ghosted. I’m not alone in this, and we’re going to investigate why this happens. Ghosting is a term that refers to asynchronous communication that is abruptly cut off by one party. The term is typically used in dating, but works well for tech recruiting too. In order for it to be ghosting, the ghosted party has to expect the conversation will continue. This means that if you apply and never hear back from a job, that’s not ghosting, a conversation never started. It only becomes ghosting when there is an expected next step that never happens. Once ghosted, do we become ghosts? Source: Dall-E The effect of getting ghosted for me, and likely others, is impactful. The narrative in my head played out like this: “was I really so bad that they wouldn’t even tell me no?” It was also a chilling effect for my projects that involved generative AI. I didn’t want to work on them. Every time I logged into my Github, I was reminded of the company because I still had a fork of the take home assignment. It took about a month for my project work to feel normal again, but I shelved some of my LLM projects and moved on. The fire of disappointment had smoldered into an ember of resentment. The resentment that, months later, inspired me to write a message to the team’s Product Manager (PM). I didn’t reach out on Linkedin. It was much more intimate, using my gamer tag. Discord is a social media platform where you can join communities that have chat rooms, forums, video and voice chat. I am a member of 50+ Discord communities, and many of them are dedicated to open source projects or technologies. The PM was in a Discord I was active in for an open source LLM toolset. Seeing that they were still in the server, and not wanting them to entice others to get ghosted. I private messaged them with the story of my experience. Thankfully, he was receptive. I had finished a 3-hour take home test and went through hours of Zoom interviews. My loop was done. I didn’t feel great about my performance, but didn’t feel bad. I had done well, but not perfect. My experience was not well aligned with the role. They were looking for a senior level engineer they deemed an “AI hacker.” I had spent the previous decade in technical lead roles, spending as much time on technical direction as I have with coding. I was expecting a message in the next two weeks saying “no,” but would be pleasantly surprised if they said “yes.” Two weeks after my final interview, I hear nothing. I am tired of waiting for the no, so I send an email to the recruiter asking for more details. I thought that something probably happened in the process and I slipped through the cracks, and I want to give them the opportunity to do the right thing. The right thing being not ghosting me. The recruiter never responded. Ugh. That’s tough to hear. The PM says after I tell them about this part in the story. Let me go talk to our talent team and see what went on here. Really appreciate you sharing this with me. Ghost in an email shell. Source: Dall-E There is this sort of liminal space between interpreting behavior as a mistake or malicious, at least for the story I tell in my head. I remind myself of Hanlon’s razor, “never attribute to malice that which is adequately explained by [misunderstanding].” When the weeks passed by with no response from the recruiter, the story in my head solidified — it’s malice. However, after the PM’s attempted reconciliation and more context, it seems like it was probably misunderstanding, or in the actual quote from Wikipedia: “stupidity.” A week after the PM got back to me, I get this email: Mixed feelings of 🙏and🖕. Source: Me I didn’t bomb the interview and feedback was generally positive, though vague. Their policy is to not ghost candidates, and it seems I slipped through the cracks. This works for me as closure. I will not be responding to the recruiting manager. I thanked the PM for helping me out, solid person, really appreciate them. Ghost World I had not been ghosted by an interviewer before. I typically apply to positions where I know people, so that may be why. I reached out to some coding communities I am in to ask for their experiences. Here are the parts of the discussion that stuck out: Almost all of the people i know have been ghosted multiple times by potential employers I had an HR lady tell me ‘we’ll definitely reach out one way or another we’re not one of those places that will leave you hanging’ and then proceed to leave me hanging recruiters have really been shit the past couple years with actually responding in the tech world…. Literally every single company i interviewed at (including some really big well known ones) recently have ghosted me This is a small sample, and does not include all the people who weren’t ghosted. These people are smart and talented with varying levels of experience. Knowing the skillsets of others who got ghosted and my personal experience, here are some of my assumptions about the recruitment ghosting trend: It has nothing to do with how well you interviewed (you could be 2nd pick and still get ghosted). The majority of people who participated in the interview are not aware of the ghosting. The people who performed the interview probably assume they have humane recruiting practices. There is no perceived incentive from recruiters to not ghost Verifying or measuring contact with rejected candidates is not a priority and not done. Even if there is a policy, there is nobody checking to see if it is followed. Nonperformative policy the message that they intended to send to you never made it to your inbox. You were meant to receive our decision with a link to book time for feedback. In Sarah Ahmed’s Complaint! she talks about institutions and their policies, describing many as “nonperformative,” a term which “seemed to capture how saying something was not doing something.” Some policies exist simply to bolster the institutions image, like an anti-bullying policy or inclusion policy. Nonperformative policies are not necessarily malicious, they come about when there is no investment or measurement into the success of the policy. In my case, the policy to follow up with every candidate was not appropriately measured, therefore allowing me to “slip through the cracks.” I wonder how many other candidates have slipped through the cracks as well. From the recruiter’s perspective, there is very little incentive to do a final follow up with a rejected candidate. That person is no longer part of the pool of potential candidates, which is the group of people who will make them money. They will definitely prioritize profitable communications over closing the loop with a rejected candidate. Final Thoughts From a game theory perspective if “winning” is having a job that fits all your wants and needs. Then the best way to win is to look and apply for jobs constantly, not just when you are discontent with your current job. That means a lot of opportunities for learning and improving interviewing skills. When recruiters ghost us, it becomes incredibly difficult to gauge our performance in those interviews and continue to improve. The truth is that getting ghosted by recruiters (or anyone) feels bad. Spending 10 hours on an interview process to not even get the time of day for a “no” is an inhumane practice and should be stopped. It cannot just be stopped with a policy, there needs to be measurement of compliance to the policy. The whole thing could be automated rather simply. We all know that working with recruiters can be frustrating from both a candidate and interviewer perspective. Those of us who perform interviews regularly in our professional lives should be more aware and verify that loops are closed for candidates. I have so many great conversations with potential coworkers, but a lot of times it doesn’t work out. I want those people to be treated with dignity and respect, and the easiest way I can do that is to ping the recruiter I work with on Slack and verify. So, that’s what I’ll do. Thanks for reading.",
    "commentLink": "https://news.ycombinator.com/item?id=38806473",
    "commentBody": "Everyone&#x27;s getting ghosted, the new normal in tech recruitingHacker NewspastloginEveryone&#x27;s getting ghosted, the new normal in tech recruiting (medium.com/k0ryk) 144 points by koryk 17 hours ago| hidepastfavorite182 comments frognumber 16 hours agoOne of my top lessons I&#x27;ve learned doing successful startups:1. Recruiting is the #1 job of any startup CEO, and the #1 determiner of corporate success.2. Up market, down market, side market, it doesn&#x27;t matter: You will get better employees if you treat candidates with respect and you will be more competitive.3. It&#x27;s a lot of work for the 95% of clowns out there you interview, and there&#x27;s a push towards automated process, but it will hurt your business.4. There&#x27;s a lot more to recruiting than just treating candidates with respect. It involves how you present yourself as an employer (participating in conferences &#x2F; meetups &#x2F; etc.), how you find candidates, checking references, reviewing github repos, etc. It&#x27;s a crazy amount of work.5. This is hard, but if you can do this, you will have a huge edge.The flip side is that as an employee, doing a good job interviewing &#x2F; recruiting, especially at a big company, is one of the lowest value-add tasks you can bring on, from a purely selfish &#x2F; incentive structures perspective. This friction, I think, is a major reason why recruiting is handled so badly. There is absolutely no upside to doing a good job, and it takes a lot of time to do so. reply fnordpiglet 16 hours agoparentAlmost every place I’ve worked (early at startups later at megacorps) there were never enough recruiting resources. So eventually I just got my own recruiter account on LinkedIn and started using my knowledge of the industry and state of hiring to make my own search queries and reached out directly. I had an incredible conversion rate - the senior person actually hiring saying “hey you look like you could fit in here” was very powerful. It was a hell of a lot of work, but I could fill seats faster than anyone around me with high quality people and my projects landed successfully. At a certain point I became too senior for that as my direct teams became smaller and more senior, and I’ve never been able to convince a single other manager to do it. Not a single person. I’ve since switched back to IC at a super senior level so am even further removed from direct recruiting, but I can see even more broadly. All I see now is engineering managers whining about recruiting, and I’ve still yet to see another manager take direct control of their recruiting. Why? The only thing I can guess is they don’t actually care about being successful at what they’re doing, they just care about successfully doing what they were hired to do, which doesn’t include recruiting. reply ivan_gammel 15 hours agorootparentThis. I gave up working with external recruiting agencies even if budget allows it for exactly the same reason. They just can’t beat me and my team in productivity and opportunity costs are high. Investing 40 hours of personal time to close a vacancy 1 month earlier means being able to delegate 1 month worth of work earlier. reply plugin-baby 16 hours agorootparentprev100% with you, but it’s not always easy to get control of your own recruiting. I’ve also seen HR get upset when their involvement is not what they expected. reply fnordpiglet 15 hours agorootparentNote I used HR for everything other than sourcing. The bottleneck is usually identifying people and initial phone screens. The rest of the process is legitimately HRs job. reply ineptech 15 hours agorootparentprevAgreed, which is insane! We&#x27;ll spend untold zillions fine-tuning and A&#x2F;B testing the process by which we get users, but when it comes to getting coworkers it&#x27;s \"Well HR says the applicants have to make an account to apply so I guess that&#x27;s just the way it is\". reply mschuster91 15 hours agorootparentIt will be that way inevitably as an organization grows, usually it&#x27;s an investor requirement to prevent outright nepotism hires or discrimination. No one cares too much for the first few dozen employees but anything larger usually will come with \"ffs go and implement at least some basic hiring standards to reduce the legal risks\". reply ylee 13 hours agorootparentprev>Almost every place I’ve worked (early at startups later at megacorps) there were never enough recruiting resources. So eventually I just got my own recruiter account on LinkedIn and started using my knowledge of the industry and state of hiring to make my own search queries and reached out directly.I&#x27;ve never worked for a tech company. But I have worked for two bulge-bracket investment banks.In the first case I&#x27;m pretty sure the person looking to fill the role personally contacted my college&#x27;s recruiting service (because she had received her MBA from there). In the second case I saw a Craigslist listing either by the person himself or his assistant, talked to someone I knew at the firm who worked near him, then reached out to the person directly. In both cases the person hiring did my first (and, in the second case, the only) interview, and I never talked to a true HR person, let alone recruiter, before receiving the offer. Does this sort of hiring never happen in tech? reply JohnFen 12 hours agorootparentYes, it happens in tech. It&#x27;s not even all that rare, really (at least outside of the FAANG crowd). I&#x27;ve reached a place in my career where I&#x27;m well-established and known enough that I don&#x27;t really need to hustle to find good dev jobs anymore, but for most of my career, I got most of my jobs by being my own recruiter in that way. reply gumby 12 hours agorootparentprev> and I’ve never been able to convince a single other manager to do it.I agree with everything you wrote, but if your company is big enough to have in house recruiters they probably don&#x27;t want you doing it!Not because they don&#x27;t want to be able to hire great candidates efficiently. But because you presumably don&#x27;t maintain the statistics they need to keep. For example, say (I don&#x27;t know you and am just making this example up), you might without realizing it only be contacting candidates who have names that you recognize as male, while they want to make sure they are pulling from a wider pool to improve their chances of getting the best candidate They may have other reasons for collecting such statistics (SEC filings? Who knows?). reply fnordpiglet 12 hours agorootparentThere are only a few bigger companies than the ones I’ve been at.I only do the sourcing, and the HR side I leave to recruiting. The bottleneck in the process is the sourcing, and I have no desire to do HRs job.On statistics, they only care about the statistics for their own sourcers and processes. There is no regulatory reason for collecting such stats.Another thing to remember is HR is the weakest organization in any company, and recruiting the weakest in HR. Any manager worth their salt pushes recruiting around, not the other way around. reply gumby 9 hours agorootparentAh, you&#x27;re doing the sourcing part, which indeed is the thing i-house recruiters are worst at (despite being compensated just for that!). Sorry I misunderstood.With reference bonuses, I think most companies do appreciate having their employees do sourcing, which makes your observation that you can&#x27;t get others to do it all the stranger. reply fnordpiglet 4 hours agorootparentAlmost all companies prevent hiring managers from receiving bonuses for their own referrals. I was hiring my own team, rather than using internal or external recruiters. reply datadrivenangel 15 hours agorootparentprevIf you want something done right you often have to do it yourself.Especially when the communication required to delegate it well is more work than just doing it. reply gustavus 15 hours agorootparentprevI have a story from a client I worked with. They had 3 or 4 positions they needed filled for their team ranging from Jr to Sr level. But because they worked at a big company they couldn&#x27;t do the recruiting directly, everything had to be through HR. While the problem was their assigned HR person was unresponsive and slow on the uptake and then took a huge vacation. So they just started looking at the applications that had been submitted through the portal and asking the division&#x27;s secretary to reach out and start setting up appointments. Time to interview and make a decision on people went from literal months to 2-3 weeks.Unfortunately this isn&#x27;t a happy ending to the story. HR threw a hissy fit that they were being sidestepped (because they were completely incompetent ninnies.) And management had to come down on the manager who was doing it and tell him to follow the process. reply mikestew 15 hours agorootparentMy observation has been that if the company has fewer than 100 employees, yet has a director-level or VP-level \"...of HR\", you will watch candidates one after another slip through your fingers. When you don&#x27;t have enough employees worthy of a \"VP of HR\", then that VP will find busy work to justify their existence.My poster child is a company of about 30 that had a \"VP of HR\". I can&#x27;t count how many candidates took another offer while she sat on her ass \"checking references\". reply benhurmarcel 14 hours agorootparentprevMy experience in a large corp is that whenever we can hire, we never have any issue finding very good candidates. The difficulty is getting approval from HR and management to hire externally. reply oramit 15 hours agoparentprevIn the last year of my last job I took on recruiting and mentoring tasks as one of my goals for the year. Management encouraged it, I got great feedback all year, and in my annual retrospective I highlighted it as a big win. When the annual review came around though it got no mention and I was basically punished for taking on those tasks because my billable time went down a small amount, even though we all knew (and agreed) that would happen. That, and other reasons, are why I found a new job the next month.My experience is the same as yours OP. Hiring just isn&#x27;t treated with any respect and your career will probably suffer if you take it on.Now that i&#x27;ve finished complaining... I think there&#x27;s a good reason for this behavior. In the US, where most of our posters are from, you can fire people for any or no reason. It&#x27;s true that you can put in more effort at the beginning of the hiring process to find better candidates but you won&#x27;t really know if they are a good fit until they work there for a while. You can have great candidates on paper who don&#x27;t work out in person, and terrible candidates on paper who are great on the job. This randomness to the hiring process means that people don&#x27;t treat it as a real discipline. And if you do hire a dud, you just fire them. Is it really any wonder that most recruiting processes are so callous? reply mschuster91 14 hours agorootparent> This randomness to the hiring process means that people don&#x27;t treat it as a real discipline. And if you do hire a dud, you just fire them. Is it really any wonder that most recruiting processes are so callous?It&#x27;s just the same in Europe with way higher standards on recruiting (i.e. anti-discrimination laws are actually enforced) and employee protection laws. Recruiting, accounting and IT are usually seen as a \"cost center\" by the remainder of the employees instead of being respected as vital contributors to the business, so it&#x27;s inevitable that people eventually \"check out\". reply tracerbulletx 16 hours agoparentprevYou&#x27;d think if professional sports could figure out how important this is we could too, but for some reason there&#x27;s a tendency to give it lip service then put absolutely no effort into doing a good job. Scouting and screenings are done by people that companies treat like dirt, we incentivize them only for body count, interviews are done by low experience employees and employees who don&#x27;t even like interviewing. We don&#x27;t know how to identify and cultivate talent in this industry yet, it&#x27;s clearly dysfunctional and deprioritized. reply nradov 16 hours agorootparentIn professional sports there are objective metrics and performances are largely in public. In knowledge work there are no reliable metrics that apply on an individual level and you have to rely on candidates themselves (plus perhaps some unreliable references) to understand work history. It&#x27;s a fundamentally harder problem. reply tracerbulletx 16 hours agorootparentI agree it is much harder, but no less important. Engineering orgs at most companies are no where close to even being good at identifying success or talent in their existing employees. reply Solvency 16 hours agorootparentprevAnd yet those sports teams still spend millions of dollars and expertise on it (recruiting). So what&#x27;s techs excuse? It&#x27;s harder so we don&#x27;t bother? reply LargeTomato 15 hours agorootparentWe can watch players play previous games. We can see their progression as a player over time.For tech, it&#x27;s hard to know if someone is good in just a couple hours of interviewing. reply nradov 15 hours agorootparentprevIt&#x27;s not an excuse. Everyone in tech acknowledges that effective recruiting is important. But beyond doing basic stuff right like not ghosting candidates, that knowledge isn&#x27;t actionable. No one has found a reliable, repeatable, scalable solution. If you can figure that out then you&#x27;ll be a billionaire. reply philwelch 16 hours agorootparentprevIn professional sports you are selecting one of the few hundred or thousand best people in the world. Its not even remotely the same impact. reply tracerbulletx 16 hours agorootparentEvery successful athletic team makes player scouting and development a core function, not only the elite levels. In the US that means minor league baseball teams, university teams from volleyball and fencing, to football and basketball. Pro cycling teams that pay almost nothing and aren&#x27;t competitive world wide.. It&#x27;s not just the elite teams that make this top priority. They ALL know that the most driven talented players they can get at their level is what will make them win. reply philwelch 14 hours agorootparentMost of your examples are talent pipelines for the top leagues. Regardless I think you made my point for me at the end:> They ALL know that the most driven talented players they can get at their level is what will make them win.This is not how normal employment works. Sports are competitive by nature so the difference between a 60th percentile player and the best player is the difference between winning and losing. But for most businesses, the difference between the best frontend developer and the 60th percentile frontend developer is close to zero. reply tracerbulletx 13 hours agorootparentThe difference is being Stripe where part of the reason they won was that they are considered extremely excellent at developer experience and technical execution. Or Netflix where they beat all of the legacy companies to a great platform doing something no one had done before, and retained the advantage to the point where they seem like they&#x27;re going to make it through the die off of streaming platforms. reply philwelch 7 hours agorootparentNeither of those examples is particularly convincing. Stripe succeeded because it tackled a famously difficult and annoying set of business problems; the technology is important and they&#x27;re reputed to be top rate, but that&#x27;s not why they&#x27;ve succeeded as a business. Netflix has no technical moat either; there are half a dozen streaming services that, on a technical level, are completely interchangeable with Netflix. The only difference between them is their respective content catalogues, and while Netflix probably has some advantage in being able to drive content decisions with customer data, that only gets you so far. replyx0x0 15 hours agorootparentprev> We don&#x27;t know how to identify and cultivate talent in this industry yet, it&#x27;s clearly dysfunctional and deprioritized. Do you even watch professional sports? Professional sports is not great at identifying or cultivating or recruiting, and the incentives there are far simpler, and the performance metrics generally easier.Take the NBA. A handful of teams are famous for cultivating talent, but mostly because the modal nba team is terrible at it.Even the best regularly completely mess up. The team of folks that put together the Warriors -- a franchise that dominated the nba for a decade -- completely blew a #2 draft pick, who is close to being out of the league. They gave Jordan Poole a huge contract and then were forced to trade him because he decided to stop playing defense and start taking terrible shots. He&#x27;s busy being a tank commander in Washington.Hell, Michael Jordan -- my take for the greatest of all time, and, at worst, the 3rd best basketball player ever -- famously didn&#x27;t go #1, and that&#x27;s with one of the best college coaches of all time (Bobby Knight) telling anyone who would listen that he was an extraordinary basketball player. Hakeem went first (ok, that&#x27;s not a disaster) and a complete bust went second (complete disaster).Lots of GMs struggle with really basic roster construction issues (Russel Westbrook on the Lakers). etc.iirc, only 4 of 30 coaches (Pop, Spo, Steve Kerr, Malone) have held their jobs for more than 4 years.edit: It&#x27;s very common for top-25 all time players to not be drafted first, or often, even all that high. Steph Curry, with a decent shot at top 10 all time: #7. Jokic: 41st (!!! -- essentially every team passed on him). Giannis: 15th. Luka: 3rd, after winning euroleague mvp at 18 (yes, I&#x27;d confident he&#x27;ll end top 25). etc. reply sokoloff 16 hours agoparentprevI think the last paragraph is blunted in companies where teams recruit&#x2F;interview for their own team (as opposed to recruiting&#x2F;interviewing for the company in general).In the former, if I do a good job, at least I get better colleagues on my team, making my daily life better and giving my team more chance to be seen as successful. reply ivan_gammel 15 hours agoparentprev>It&#x27;s a lot of work for the 95% of clowns out there you interview, and there&#x27;s a push towards automated process, but it will hurt your business.If 20 candidates enter your funnel before you hire one in a screening-technical-offer flow, that’s maybe 5-6 hours per week spent on recruiting in 1 month. You can even send a few personalized rejections to worthy but unfit applicants, sending 80% of applications to trash. The effort is noticeable, but it is not a lot of work with good automation, planning and interview design. reply TheCaptain4815 16 hours agoparentprevThis is so true it hurts. If you get burnt once with hiring, it’s something you’ll never forget but could destroy the startup&#x2F;small business in the process reply realusername 16 hours agoparentprev> There is absolutely no upside to doing a good job, and it takes a lot of time to do so.I&#x27;ve also learned this the hard way. I&#x27;ve conducted about a 100 interviews in 2 years and didn&#x27;t get compensated any for it despite being one of the most critical part of the company.Conducting interviews is also very tiring and time consuming, I&#x27;m estimating that two interviews in a day and your day is gone. I also evaluate it 2x more tiring than coding personally.It wasn&#x27;t a complete waste of time though, I got a lot of experience from that which will be very valuable in future management positions. reply 1920musicman 15 hours agorootparentI have the same experience. Interviews are very time consuming (prep 30 min, interview 1h, fill out the feedback form 30m-1h), and having several interviews each week means I spend ~1 day weekly on something that&#x27;s not going to benefit me directly in any way (excluding the benefit of potentially working with good engineers I helped hiring).So unless the incentives change, I don&#x27;t see this process improving in big tech. reply realusername 15 hours agorootparentYes exactly, the interview itself is a bit less than half of the work surprisingly and then you do need a real break after all of that very intense concentration anyways. 1 interview = roughly half a day gone, that&#x27;s what I&#x27;ve experienced.And then it&#x27;s indeed never valued inside the company, worse than that, it might be counted against you since you will achieve less in your team where all the evaluation will take place...I really don&#x27;t understand why companies don&#x27;t value engineers capable of conducting interviews because it&#x27;s really not an easy task, you need much better than average interpersonal skills and much better than average tech knowledge as well. reply 1920musicman 15 hours agorootparentAgreed. I mostly conduct system design interviews which already have a smaller pool of interviewers at my company. This contribution has been included exactly zero times in the countless review cycles I went through. reply varispeed 15 hours agorootparentprev> and didn&#x27;t get compensated any for itSo why would you do it? Such a big red flag. It means that employers will expect doing work for free (in some countries this is illegal) and potential employees should know about it. reply realusername 15 hours agorootparentI&#x27;m doing it for my future career, not for them that&#x27;s for sure!Despite not helping in this current company, conducting a lot of interviews taught me a lot for sure. reply varispeed 15 hours agorootparentFair point. reply varispeed 15 hours agoparentprevYou forgot about fair remuneration. Most employers think that they are doing potential employee a favour and the employees should kiss their feet just for reading the offer.I don&#x27;t respond to job offers that don&#x27;t include _minimum_ pay (not \"up to\") and I don&#x27;t respond to low-ball offers.Fair remuneration should also include true equity in the business as otherwise it is exploitation, as at software business the profits are typically not linked to rewards for people creating those profits - it&#x27;s all get creamed by shareholders.So if your company generates millions or billions and you only offer a salary? No thanks. reply geraldhh 16 hours agoparentprevyour last paragraph resonated very well with me as i can&#x27;t remember the last time i spoke to a recruiter that seemed to have a horse in the race. reply tayo42 16 hours agoparentprev> is one of the lowest value-add tasks you can bring on, from a purely selfish &#x2F; incentive structures perspective.At one place I worked, interviewing was one of the check marks you could participate in for promotions. reply 1920musicman 15 hours agorootparentBut that&#x27;s exactly what it is then: a check mark. I can interview dozens of candidates, and then add a total count to my self review. There is no incentive to do a good job (how would that even be evaluated?..). reply filoleg 15 hours agorootparent> how would that even be evaluated?Depends on the company. At my current workplace, the candidate is given an option to provide feedback at the end (either through a form or an email), and all interviewers are also required to submit written notes on how everything went down.Given that a candidate at the onsite will get interviewed by 4-5 people, with each of them providing a very detailed set of notes, it would be fairly trivial to smell out a misbehaving interviewer, if one cared to do so. What actually happens at the end of the day with those notes and candidate feedback, that’s the part i am not sure about. Once they get submitted to the hiring committee (or HR), it is out of my hands.But just saying, they do have ways of evaluating it, just on a less precise scale and more on a “bad&#x2F;good enough&#x2F;amazing” scale. With only the “bad” outcome raising any eyebrows&#x2F;having any meaningful effect, and with 99% of them getting the “good enough”&#x2F;“amazing” ratings. And how often the signal for that “bad” rating gets caught is also not something I know much about.P.S. Your assessments and notes are all preserved in the centralized hub, so you (and some others) can always access them later as well. And, sometimes, you indeed have people checking them out for assessment or such. Especially during your first couple interviews, you have a person supervising you and taking notes in parallel as well, and then you discuss them and they give you improvement suggestions and such. reply 1920musicman 14 hours agorootparentI am sure what you are describing is being tracked at my company. The concern with misaligned incentives I have is that there is a vast gap between conducting a passable interview in terms of engaging with the candidate and actually investing yourself in the process. So it&#x27;s not really about being a bad interviewer (as in rude, openly biased, etc) but about having the energy to do the best job you can - which all candidates deserve IMO. reply drewcoo 16 hours agoparentprev> treat candidates with respect> 95% of clowns out there you interviewWhat&#x27;s your company affiliation? reply LargeTomato 15 hours agorootparentI worked at a rocket startup. 1&#x2F;3 ex SpaceX. 1&#x2F;3 ex Google&#x2F;Facebook.I interviewed a guy and asked him, essentially, \"find the biggest number in a 2D array\". This guy spent half an hour struggling because he \"wasn&#x27;t sure how to look through the grid in a circle pattern\".You&#x27;d be incredibly surprised who gets interviews. reply filoleg 15 hours agorootparent> \"wasn&#x27;t sure how to look through the grid in a circle pattern\"Now I am curious to know whether I am too dense to get it or if the candidate was just that off the mark.Is circle pattern sorta like iterating through a 2d array like a spiral (i.e., outer layer of the 2d-shape first, then one deeper, etc.)? And if yes, why would that ever be useful for just searching for a specific value in a 2d array?I get how it could be useful for some more niche&#x2F;specific problems where the layering of the 2d array would actually matter, but is it just entirely off the chain to recommend it here? Because I cannot for the life of me figure out why you would want to do that instead of just iterating, especially considering how significantly less trivial it is to code-up that “circular” iteration (as opposed to just a regular linear iteration).Sidenote: Is there even a more efficient way to solve that problem, other than just sequentially iterating through the 2d array and simply tracking the value&#x2F;position of the largest number until you finish iterating over the entire 2d array (assuming it is non-sorted)? It seems way too simple, so I feel like either I am missing something about the problem statement or there is a better solution than the one I proposed. reply chrismcb 3 hours agorootparentI think the point of the OP was A the person wanted to solve this in a spiral pattern, which isn&#x27;t necessary. And B didn&#x27;t know how to do a spiral pattern, which is fairly simple. As to your other question, unless it is directed you need to look at each element. The real question is how to iterate over a 2d array. There are several methods and really that is all this question is looking for. reply icedchai 13 hours agorootparentprevMost day-to-day coding is simple and boring. Your interview questions should be, too. I&#x27;ve interviewed 100&#x27;s of candidates over the years. Many of them had trouble writing a couple of for loops. This was for a somewhat similar problem. I would stress \"we don&#x27;t need an optimal solution, we just need a working solution.\" reply 1920musicman 15 hours agorootparentprevHonestly, at this point I stopped making strong conclusions about candidates based on a single interview. I won&#x27;t recommend to hire but also won&#x27;t judge their abilities overall. Interviews can be very stressful, candidates overthink and often get fixated on a random solution they think the interviewer expects, etc.I had several odd experiences myself in the past, as a candidate. The funniest one was when I interviewed at a prestigious company I thought was hiring only top talent. I spent an hour trying to come up with the most efficient Sudoku solver, got completely stuck on some arbitrary algorithm that I came up with on the spot. It wasn&#x27;t a \"circle pattern\" but close to that. Wanted to impress the interviewer and also did not sleep the night before overthinking the process. reply FredPret 16 hours agorootparentprevYou can and should treat someone with respect, even if your expectation is that there&#x27;s a 95% chance they will turn out to be a clown. reply foldr 14 hours agorootparentIMO this way of looking at it is indicative of a lack of respect for candidates, even if communication is superficially respectful. If someone performs badly in an interview then by all means don&#x27;t hire them, but it&#x27;s both unkind and irrational to jump to the conclusion that they&#x27;re a &#x27;clown&#x27;. If someone is making such a harsh judgment about 95% of their applicants, on the basis of an interview process which we know to be highly imperfect, then I would not want to work for them. reply michaelt 16 hours agorootparentprevHe works for Clownr, the Uber of children&#x27;s party entertainment. reply 1920musicman 15 hours agorootparentI had a good laugh reading your comment and then found this: https:&#x2F;&#x2F;www.clownr.com&#x2F; reply arcanemachiner 15 hours agorootparent> Are You a Clown?> Say hello to you new best friend. replyCM30 16 hours agoprevThe new normal? This has been the case for years for almost all job applications. Maybe if you&#x27;re lucky and the company has put you through like 4 rounds of interviews you might get feedback, but the general rule seems to be that useful feedback for an interview is the exception rather than the norm.And heck, even that isn&#x27;t something you can rely on. I&#x27;ve had a fair few interviews get about 80% of the way through the recruitment process, then just ghost me without a trace. The main reason I have my current job is because the other company I was interviewing with just kinda faded away at the end of the application process.If being ghosted is the new norm for you, you must be insanely lucky. reply rossdavidh 15 hours agoparentThis was my first thought; I&#x27;ve been working as a dev for almost 20 years, and \"ghosting\" is the norm.Having seen it from the other side, one oddity I&#x27;ve noticed is that being \"ghosted\" usually means you were being considered; if they get back to you with a \"no\" it means they don&#x27;t think the person would work out, and even if nobody else comes forward, they don&#x27;t intend to hire that person. \"Ghosting\" is typically what happens when they are thinking \"well, maybe\", and they keep looking for an even better fit and then find one. Either it&#x27;s been long enough that they&#x27;ve forgotten, or it&#x27;s been long enough that they assume (usually correctly) that the person has figured it out by then.I&#x27;m not saying it _should_ be that way, I&#x27;m just saying it&#x27;s not \"new\". Letting this mess with your head is a bad situation, because it won&#x27;t be that unusual. Just keep in mind that it typically means \"near miss\", and keep looking. reply piloto_ciego 16 hours agoparentprevWhen I was a pilot just starting out, in 2007 I’d see a human response maybe 1&#x2F;100 applications.It’s pretty gross out there for people other than tech employees.What’s pretty funny is now that I’m changing into this field it seems to be adjusting to treat people like crap. So, I guess you’re welcome guys and gals - it’s my fault. reply skeeter2020 16 hours agorootparentThis isn&#x27;t really about applications though - not getting a human response at the first step has always been common, especially with electronic postings. It&#x27;s as easy to apply as post, so you do get a lot of spam applicants.If you&#x27;ve had human contact and then radio silenc - IMO that&#x27;s inexcusable. reply piloto_ciego 14 hours agorootparentLiterally all the time in aviation. Now I’m just cynical and do a quantity over quality approach reply araes 15 hours agorootparentprevIt was not that great in tech. I interviewed during the same time in tech and got limited responses even after what appeared to be positive interviews. There wasn&#x27;t quite as much of \"we completely do not respond\", yet it still existed. Mostly a vacuum of rejections with no context. Very little other than \"no\" in most cases.At least in my case though, it just got to \"putting in 2-3 customized applications a day, can&#x27;t really stop to worry about the type of response, unless its helpful. No&#x27;s a no, next application.\"Plus, 2007 was the era where Google&#x27;s puzzles and homework assignments were what everybody was fighting about. Which company&#x27;s got the craziest hiring homework and weirdest math puzzles that have almost nothing to do with your job? reply nradov 16 hours agorootparentprevWith the higher market demand for pilots now I suspect that job applicants in that field are now being treated better than they were in 2007. reply piloto_ciego 14 hours agorootparentYeah, it sucks lol, I can’t fly anymore due to illness, and the demand is crazy I still get called to this day. Now if you can finish fogging a partially fogged mirror you’re hired at a major. reply doubleorseven 16 hours agoparentprevMaybe only now it reached America? In Israel, for the last 13 years atleast, I don&#x27;t think there was ever an alternative. reply varispeed 15 hours agoparentprevAlso why would you need feedback?Every company is different and will have different requirements and expectations.Unless you are doing something universally bad (like didn&#x27;t shower before going out), there is not much that feedback could help you with other than make you start acting like someone you are not.If you acted on the interview and you got hired, you&#x27;ll be expected to continue the act probably for as long as you work there. Which ultimately leads to quick burn out and self-hate. reply malfist 15 hours agoparentprevThe article is specifically talking about ghosting that happens after an interview. reply CM30 13 hours agorootparentThat&#x27;s also rather common. Happened to me with one of the jobs I was interviewing at in the early days of the pandemic, and a few more in the years before that. Sometimes it even gets to like, a telephone interview, take home test and 2 live interviews, and they still ghost you. reply solatic 16 hours agoprevBoth of the following can be true at the same time: 1. I personally owe a response to anyone who reaches out to me, in a timely manner, even if that answer is no, because to intentionally ignore a request is unethical. 2. I will strive to have built up many and different areas in my life, such that my sense of confidence is not impacted by the actions or inactions of people who are, in essence, strangers.If you want to inure yourself to people ghosting you, spend some time in a sales gig doing cold calls. The vast majority of people will not respond to you and that is not only OK, that is a good thing. You want to be with people who give you a positive return on your energy, not people who sink your energy.Move on. reply andy99 16 hours agoparentI mostly agree with the sentiment. But I&#x27;d say the difference with the kind of ghosting he&#x27;s talking about is you&#x27;re already (well) into the interview process. So you&#x27;ve invested a fair bit already. Like in sales, getting ignored or told to get lost immediately is easy to shake off, going deep into the process and then not hearing anything is different. You don&#x27;t know if you should still expect something, you&#x27;ve got to balance persistence with being professional, you were to some extent counting on it at least possibly happening.I&#x27;ve been looking for a job recently, and was also surprised about how much ghosting seems to to on, especially after already talking to people but also never hearing at all. The best application experience I&#x27;ve had recently was probably one that 24 hours after I applied told me no. There I at least could move it out of my mind entirely.I do agree for sure that it&#x27;s not personal. And how you&#x27;re treated during recruitment is also good information. reply skeeter2020 16 hours agoparentprevMy Rule: The nature of a negative (i.e. final) communications should equal how personal the relationship is. If you applied electronically and we stated \"only those considered will be contacted\" or you cold-emailed me some sales pitch, we don&#x27;t have a relationship at all and you deserve nothing. If you contacted me and I responded with a short email, my final answer should be a shjort email. If you spent several hours in face-to-face interviews and we&#x27;re not moving forward I owe you a phone call or email with some explanation of why - in a TIMELY manner.Seems pretty easy to me. reply chrismcb 2 hours agoparentprevI don&#x27;t think ghosting has anything to do with 1 or 2. And there is a huge difference between cold calling someone without getting a result and meeting someone face to face and then being ghosted. Personally I&#x27;m not to hung up on small companies that do it. But later companies or recruiters, there is just no excuse. Especially for a recruiter, who&#x27;s job it is to communicate with potential employees. Who knows maybe you&#x27;ll find a different job for me down the line? The problem really comes into play when they say they will get back to you in a few weeks... And then never do.. reply sk11001 16 hours agoparentprevNot getting responses from cold calls or applications isn&#x27;t ghosting according to the author of the article (and I agree):> In order for it to be ghosting, the ghosted party has to expect the conversation will continue. This means that if you apply and never hear back from a job, that’s not ghosting, a conversation never started. It only becomes ghosting when there is an expected next step that never happens. reply JohnFen 16 hours agoparentprevPeople not responding to cold calls isn&#x27;t ghosting, though. Ghosting happens when there&#x27;s an established communication that gets dropped without explanation. A cold call is not an established communication, it&#x27;s a solicitation to establish communication. reply lapcat 16 hours agoprevI once interviewed with the owner of a small tech company. I thought the interview went well, and the owner said he would get back to me the next week. But he never did. I never heard another peep from them.Months later, I saw that the company had hired someone for the position, and the person looked to be very qualified, impressive credentials, so I had no complaints about being passed over. All they had to say was that they decided to go with another candidate, yet they didn&#x27;t bother to treat me with a modicum of respect.Fast forward to a year later, it turns out that their new employee left the company, and they had to advertise for the same position again. Guess who did NOT apply this time. Karma. reply kleiba 16 hours agoprevThis is a pet peeve of mine. In the olden days, pre-digital, companies would send you a proper letter back together with your application documents so you could possibly reuse them. That was actually a little work for them to do!These days, sending an effing canned email to all applicants that didn&#x27;t get hired costs a company next to nothing, and still ghosting happens.It&#x27;s indecent. reply arprocter 13 hours agoparentI recall being taken aback to receive a dead tree rejection letter in the mid 2000s (the only interaction had been me emailing my CV)The recruiters that meet you in person, talk a big game and then complete silence annoy&#x2F;confuse me the most - possibly they get commissions for signups? reply forinti 15 hours agoparentprevI once applied to a position in the IAEA. I didn&#x27;t get the job, but they sent me a letter. I kept that letter for a decade because it was such an unusual occurrence in my carreer.I got a nice letter from a UN agency, when local companies couldn&#x27;t be bothered to send an email. reply firebat45 13 hours agoparentprevIn the olden days, people took time to apply for each job. Nowadays, they spam out applications to hundreds of places in a day.The problem is on both ends and stems from rewarding quantity over quality. reply barbazoo 15 hours agoparentprevCan’t expose the precious organization to any form of liability. What if you accidentally leak the internal communication about the candidate *eyeroll* reply kthejoker2 16 hours agoprev100% in the \"nobody remembers what you did, everyone remembers how you made them feel\" camp.An automated no is such a no-brainer versus the reputational risk of being seen as a bad or callous employer.And people have looooong memories &#x2F; impressions created in this space. I honestly have no idea if General Electric is a good or bad employer, but the Jack Welch era still lingers in my mind. reply atrettel 16 hours agoparentI completely agree with this. I have only been ghosted after the interview stage twice but both times left bad feelings in my mind. I did try to contact people but did not receive a response obviously. I never applied to any jobs at those organizations again. I think many companies and other organizations need to realize that even an automated denial is much better than nothing. reply boring_twenties 17 hours agoprevEvery time I&#x27;ve been passed over for a job, I&#x27;ve been notified in a timely manner. I would consider anything less to be grossly, outrageously unprofessional. It&#x27;s never happened to me, but if it ever does, you best believe I am going to name and shame the hell out of that company. Everyone deserves to know, and people like this don&#x27;t deserve to have anyone working for them. reply piloto_ciego 16 hours agoparentI’m trying to get started in this field, I am finishing grad school in the spring and applying everywhere…I mostly hear nothing. It is incredibly unprofessional. reply skeeter2020 16 hours agorootparentagain, hearing nothing is not the issue. I still think these companies should send you a canned automatic response, but it&#x27;s about relationships, and the degree of personalized communication needs to match this. reply drewcoo 16 hours agorootparentprevNo response to applications is normal.Ghosting is suddenly leaving you in the middle of a conversation. For example, not showing up for a scheduled Zoom interview then not replying to your concerned email, asking to reschedule. And yes, that&#x27;s happened to me. reply optymizer 15 hours agoparentprevThis is the right attitude. If highly qualified applicants start avoiding companies that ghost, ghosting will stop. reply zaptheimpaler 16 hours agoprevThe whole point thing about having half a breakdown after being ghosted is not going to be remedied by receiving a generic rejection email.. even the actual response he did get after pestering them was just \"you were good but another guy was better\" and yeah this is 90% of all anyone will ever say anyways. Like you can simply assume that 1. a better candidate was found or 2. the job opening was already closed because already hired or budget or role reconsidered if it helps you feel better.The whole idea that we can apply the norms of personal relationships to a business transaction like this with 100s of candidates, no pre-existing relationship beyond 1-2 calls at mostand at best a generic rejection is basically displaced disappointment turned to resentment and anger. To which I say, you can either rant about it online and hope they change, or you can learn to regulate your own emotions. reply Gazoche 11 hours agoparentIt&#x27;s not just about personal feelings. It&#x27;s about not being left in the dark for a decision that affects your whole life. When you&#x27;re ghosted you&#x27;re just left wondering whether or not you got the job, whether or not you should start to look for a new place to move to, whether or not it&#x27;s worth pursuing other opportunities. It adds uncertainty to one&#x27;s life and therefore unnecessary stress that could be avoided if the company bothered to write a 30 seconds email.Rejection emails still hurt feelings but at least you get a clear answer and can plan your life around it. reply kdmccormick 15 hours agoparentprevFTA:> The effect of getting ghosted for me, and likely others, is impactful. The narrative in my head played out like this: “was I really so bad that they wouldn’t even tell me no?” It was also a chilling effect for my projects that involved generative AI. I didn’t want to work on them. Every time I logged into my Github, I was reminded of the company because I still had a fork of the take home assignment. It took about a month for my project work to feel normal again, but I shelved some of my LLM projects and moved on.How is this \"half a breakdown\"? They stopped working on their side projects for a month. That&#x27;s a completely understandable result after having spend hours of your life interviewing and not even getting the courtesy of a \"no\". I&#x27;m happy with myself when I make any significant movement in my side projects within a month, so I can certainly imagine that a disappointing professional experience would zap my personal coding motivation for a few weeks. That&#x27;s a not a breakdown--that&#x27;s human nature. Most people aren&#x27;t coding on the side at all.> The whole idea that we can apply the norms of personal relationships to a business transaction like this with 100s of candidates, no pre-existing relationship beyond 1-2 calls at mostand at best a generic rejection is basically displaced disappointment turned to resentment and anger. To which I say, you can either rant about it online and hope they change, or you can learn to regulate your own emotions.What? We can&#x27;t extend common courtesy into business transactions? You sound... tough to work with. Yes, resiliency is important and great and we should all strive for it. That doesn&#x27;t mean we should throw away longstanding norms of professional decency.Also, calling this a \"rant\" is unfair. It was well-written and calmly worded. A lot of people are reading it. Some of them might go back to their job Monday and think \"oh, I should ask if the recruiter ever followed up with the rejected candidates\". reply zaptheimpaler 15 hours agorootparentThanks for the implications on my character in the work place. Written very courteously yet not so nice.. maybe that&#x27;s the key difference in perspective. I don&#x27;t care about politeness I care about kindness.When someone writes a kind or genuinely useful letter with feedback when I get rejected, I greatly appreciate it. An automatic or generic rejection is the same as nothing to me. It has no actual information in it. And requiring a recruiter to reply to all rejections personally might feel nicer to you and less nice to them. reply kdmccormick 15 hours agorootparent> I don&#x27;t care about politeness I care about kindness.Your original comment is not kind either, my friend.Genuinely useful feedback is the best, for sure. But hearing a generic \"no\" still makes it easier to take a deep breath and move on as soon as they decide against you rather than weeks later when you&#x27;ve decided it&#x27;s appropriate to give up hope. reply asmor 12 hours agoparentprevIt&#x27;s also a liability thing. Keeping the legal attack surface minimal. It can result in you getting junk responses if you ask for feedback too. I&#x27;ve referred several friends to jobs who then got rejected, and they&#x27;re almost always given the wrong reason when asking for feedback.Especially if there&#x27;s a perceived culture misfit or some manager thought we both would leave at the same time for a startup after a few months (this happened me more than once!) they all got insulted on their skillset instead by the recruiter. And sometimes even skills that weren&#x27;t even in the job description! reply Niksko 16 hours agoprevI&#x27;ve been ghosted twice lately. One was someone reaching out via Hacker News for an initial chat where they said \"let&#x27;s put you through our interview process\" and then ghosted. The other was for a startup, similar situation, \"let&#x27;s set up a chat with our CEO\", then ghosted.The commonality here is an inability to just be honest and say \"doesn&#x27;t seem like a good fit\". Disappointing, and ironically, an indication that it really wouldn&#x27;t be a good fit. I don&#x27;t want to work with people who are unable to deliver uncomfortable news respectfully, or even, at all. reply sjducb 14 hours agoprevIf you get ghosted it usually means that you were almost good enough, or it was something unrelated to you.The bad candidates get an immediate rejection.The best candidates get offers quickly.Candidates that are kind of good enough, but not amazing are the ones who get ghosted. The employer doesn’t want to say yes in case they find someone better. They also don’t want to say no because you are “good enough” and if the next 3 candidates suck then you’ll get the job. Then they forget to tell you when they hire #3.The other ghosting is when the whole project gets cancelled mid interview. Often people involved aren’t sure if the project will be cancelled so they don’t tell the candidates early. Then when the cancellation is in full swing everyone has forgotten about the candidates. reply skeeter2020 16 hours agoprevThis is NOT the new normal, and we should NOT accept it. Everyone here should do their part; for example, I&#x27;m more often a hiring manager and will not tolerate ghosting anyone who gets to any interaction with engineering. IME it&#x27;s lazy recruiters who are responsible for this, and they&#x27;re about as big of a part of tech as an accountant, i.e. very little.I don&#x27;t understand why many people are hesitant to name and shame the individuals and companies. It doesn&#x27;t matter if it was intentional or accidental the outcome is the same, and I have consistently been very vocal with my being ghosted experiences. Surprise, surprise: they&#x27;re consistent on a company-basis and highly corelated to other shitty experiences, both before and during employment. reply LargeTomato 15 hours agoparentSpaceX.I interned in 2018. I was told jobs were available for interns but I never heard back.They contacted 12 months later for an interview but ghosted after the phone screen.Contacted again in 2021 for an interview out of the blue. I complete the cycle and they say, verbatim, \"you made a great impression on the team and they would really like to hire you. ... We will send over the paperwork later this week.\"No paperwork. I get a call 2 weeks later that the VP thought my GPA was too low so my offer was reneged. The recruiter apologized and asked if I wanted to re interview for a new team. I ghosted her. reply y-c-o-m-b 16 hours agoparentprevThere should definitely be a larger effort to name and shame. I&#x27;m not currently job hunting, but in the past I have considered starting a site to document companies and the individual recruiters that are ghosting. It might be worth-while for anyone job hunting currently to start a google sheet or something along those lines to do just that since it&#x27;s so common now. reply sshine 16 hours agoparentprevAt the least, ghosting sends a clear signal that the employer is not a place where clear communication is valued.I would much rather be ghosted than waste time communicating poorly for an extended period of time. reply tayo42 16 hours agoparentprev> I don&#x27;t understand why many people are hesitant to name and shame the individuals and companies.The risk for repercussion isn&#x27;t worth it. Your going to go online and stir up shit? Corporate culture is way to risk averse. If you are the kind of person getting ghosted and rejected from interviews you evidently aren&#x27;t a person with any leverage in the hiring market. reply stevekemp 15 hours agorootparentIn my experience companies with bad practices (be they recruitment, or otherwise) are named and shamed frequently - but only locally, and in-person.I&#x27;m not going to relocate from my current city (Helsinki), though perhaps I could go back to working remotely 100% of the time in the future. So really the only companies I&#x27;m ever going to apply for are based locally.I think via IRC, facebook, random geeky chats in pubs, and other face to face conversations I&#x27;m slightly familiar with most of the big players. There are certainly companies I&#x27;ve heard of that I&#x27;d never consider applying to, and would outright reject if a recruiter tried to head-hunt me for. And I think the reverse is true - some companies are well known locally for having fun challenges, awesome people, and a good environment. reply ponderings 14 hours agorootparentprevI&#x27;ve had strange success walking up to the guy, telling him I want us to deliver fantastic work together, I want to work with you - together. What I don&#x27;t want is to go to the boss and tell him you&#x27;ve fucked up, you are lazy, that you are doing a poor job and that we would be better off without you. This is the last thing I want. The best outcome is that we get someone else who knows nothing. If that doesn&#x27;t happen you would blame me. Then I give him a detailed list of things I think they should have done differently.It costs a few relationship points but they do respect the practicality of it. reply zero-sharp 13 hours agorootparentprev>If you are the kind of person getting ghosted and rejected from interviews you evidently aren&#x27;t a person with any leverage in the hiring market.Haha wow. reply senderista 15 hours agoparentprevFine: Yugabyte. reply chrismcb 2 hours agoprevA \"few\" years ago (back in the last economic crisis) I was job hunting. Interviewed with a ton of people, and almost no one would get back to you. A few did, but most just ghosted me. I just found myself back in the job hunt, but this time almost everyone sent me a response. It sucks getting a \"no\" but I&#x27;d rather get a \"you are the worst programmer in the world, go jump on a lake\" than get boring at all. I did one interview, had to give a presentation, out a lot of time into it. Never to hear from them again. But the point is, it was definitely better this time around (well better in the housing sense) reply cco 16 hours agoprevWhat I&#x27;ve never really understood is that though recruiters (external but also in-house) are typically paid on hiring candidates, much like sales people.And in that paradigm, why is ghosting so common? As a recruiter, a lot of your value is your professional network that you can pull from to place candidates. Why would you ever ghost people that, while not a good fit for this role, could be a good fit for a different role in the future?Even as someone not in recruiting, I&#x27;ve made several connections with folks in the interviewing process (both as interviewer and interviewee) that have led to either new business deals or job placement later on.Just never really made sense to me, interviewing is \"free\" networking. ¯\\_(ツ)_&#x2F;¯ reply RajT88 15 hours agoparentIn-house recruiters I have much better experiences with. Recruiters working for staffing&#x2F;headhunting firms in my experience tend to:1. Be focused on high volume, hence the ghosting. They save time by ghosting you and spend that time on candidates which are sure bets.2. They ask to be friends on LinkedIn up front (probably even if they know they are going to ghost you). This is to take advantage of your network. They get something out of the interaction; you get nothing.3. They sometimes line you up for interviews which you aren&#x27;t a good fit for. What in the actual fuck.I don&#x27;t friend recruiters from anything but the tech companies hiring. At my career stage, I won&#x27;t even talk to recruiters which aren&#x27;t working directly for the company doing the hiring. I would happily go back to this if I was at the phase of \"I need a job; any job for now\".The icky feelings I&#x27;ve gotten from recruiters over the years is akin to the icky feelings I&#x27;ve gotten from car salesmen and real estate agents.As I am writing this now, I am reminded of some of the headhunting&#x2F;staffing firms I&#x27;ve talked to over the years. I just removed all of those recruiters from my network. reply foobiekr 12 hours agoparentprevMuch like sales the professionals establish a pretty rigorous funnel and pay no attention to anyone who looks like they will fall out of the funnel. They optimize for offer and if they sense the candidate won’t get there they are better off trying to find one who will.Also, honestly, a lot of recruiters suck. They are often hired with no qualifications and really not even any serious education. It is routine to hire people who formerly worked in retail or whatever for these jobs so on average they are themselves fodder for the process and often don’t last long. reply skeeter2020 16 hours agoparentprevrecruiters are a lot like real estate agents - there&#x27;s very little barrier to entry or exit, so the good ones do well in both hot and cold markets, while the shitty ones exit when times get tough and compete like crazy for limited supply when they are good. The former builds value in their network and will likely never ghost you; you&#x27;ll earn them several commissions over a career. The bad ones get fired or voluntarily leave so the network has no value. I suspect we&#x27;re seeing an uptick in ghosting right now because:1. There are a lot more newer people in tech today who are now going through their first downturn and involuntarily looking for a job. As a stereotype these people are far more public on the interwebs and sharing their experiences.2. We&#x27;re still in the downward trajectory so the crappy recruiters haven&#x27;t been purged and companies don&#x27;t have to compete on quality of service to get limited talent... yet. Wait until the next upswing - I&#x27;d bet far less people got ghosted on job applications 2-3 years ago. reply nonrandomstring 14 hours agoprevI think its cultural. Disrespect has become normalised as part of \"fast paced modern life\".If we give toddlers a TV or tablet to play with instead of attentive parenting they grow up with damaged attachment patterns. We stop them playing outside and interacting with other children. They go through metal detectors to attend hostile schools in a locked-down environment and communicate only through text messages. They are watched night and day by CCTV cameras. They&#x27;re made to feel ashamed of simply existing because they&#x27;re using up air and \"killing the planet\".Do this for 20 or 30 years and we have a generation of timid, avoidant people with no interpersonal skills who as Julia Roberts&#x27; character in Sam Esmail&#x27;s new movie \"Leave the World Behind\" puts it just \"hate other people\".And then we use dating apps that reduce other humans to a dismissibe swipe. Those are our peers today. We treat each other like machines and mutual threats, because that&#x27;s all we&#x27;ve ever experienced.Is it any wonder that people in companies are too terrified to engage in a risky human-human interaction? reply andy99 10 hours agoparenthttps:&#x2F;&#x2F;genius.com&#x2F;John-lennon-working-class-hero-lyricsSpot on but not new reply kstrauser 17 hours agoprevAlmost as bad IMO are companies that auto-respond with messages like \"we&#x27;ll contact you in the next few weeks to discuss next steps\". Next few weeks? Are you under the impression that your competitors are all sitting around twiddling their thumbs?Yes, it&#x27;s a hirer&#x27;s market today, but that doesn&#x27;t mean you can take your sweet time eventually getting around to interested applicants. reply dspillett 15 hours agoparent> under the impression that your competitors are all sitting around twiddling their thumbs?If the premise of the article is correct (that ghosting, and by inference other poor communication, is very much the norm) then that impression would hardly be massively inaccurate. reply hx8 17 hours agoprevI think this would matter more to me if the feedback was ever useful. The feedback from these \"no\" is more cliche and less useful than a Dear John letter. All I ever hear are \"another more qualified candidate\", or rarely the slightly more useful \"culture fit\". reply sokoloff 16 hours agoparentHaving been in industry 30 years, it’s only about 15% of the time that I have any specific constructive feedback that’s worth sharing. “When you’re talking about problem X, first make sure you understand the problem because it seemed like you jumped straight into a solution that missed a key element of the question.” Or something similarly specific.In most cases, it’s more “you did okay, but someone else did better in this noisy, wide error-bar interaction”; giving someone feedback of “just be better next time” isn’t particularly helpful or motivating and some candidates will take that as a invitation to press for further details which may not exist and certainly won’t be coming.In cases where there is specific feedback, I give it to recruiters who hopefully share it with the candidate. (Anecdotally, I have talked with recruiters who have done just that, which is how I know that some candidates just won’t drop it.)If you’re never getting specific feedback, it’s probably the case that you’re consistently doing fine in interviews and someone else just wiggled a little higher on that particular day. reply SoftTalker 15 hours agorootparentAnd often it boils down to an intangible \"we felt a better vibe with candidate B\" and that&#x27;s probably something they will never come out and say.My reaction to the feedback he got when he finally jogged someone into responding was like yours. It was the typical cliche rejection letter. Maybe a bit more personal than some. I&#x27;m sure that the \"wires got crossed\" and the \"email never made it to your inbox\" are just a smokescreen for \"we hired someone else and didn&#x27;t bother to let you know.\" reply k310 16 hours agoprevI recall getting feedback from one or two companies at which I attended day-long interviews. I had to ask, and after the \"well, we can&#x27;t tell you that,\" got a couple of \"we hired someone with more specific experience with XYZ\".Otherwise, many days sent down a black hole, followed by beers, to recover from the day&#x27;s ordeal.My wild-ass hyperbolic guess is that once they make a choice, staying silent is their way of grabbing a beer or two and forgetting their own ordeal, and having no regrets. \"We hired a genius. All the others were run of the mill\"They&#x27;re human, after all, but maybe humans won&#x27;t be involved any more. Machines decide who will serve them. reply wombat-man 16 hours agoparentSometimes they just aren’t sure, or they have a couple other candidates in the pipe that seem good. I’ve been on the other side where we didn’t hire the person for random reasons that I try to not take it personally. But it is frustrating for candidates for sure. reply vinay_ys 16 hours agoprevAt every company I worked at, each interviewer would have gone through training to do interviews, write feedback, participate in debriefs and actually do those things in a timely fashion. Senior people would have participated&#x2F;chaired in semi-formal or more-formal hiring committees and helped make hire&#x2F;no-hire decisions. And the hiring tools we used showed if the candidate has applied earlier for same&#x2F;different roles, and did interviews and what were the feedback earlier. There would also internal confidential ref-checks based on overlap in candidate&#x27;s work history with anyone at the company currently. If the candidate was rejected at any stage, the recruiter is informed and they have access to almost all of these data in the same tool. And they are expected to communicate that back to the candidate. And there is a feedback tool for the candidate to rate the whole experience and this feedback is analyzed carefully, and processes tweaked accordingly, especially if the company is growing and expected to continue to grow for a while.In companies that had to do aggressive cost cutting, the recruiters were the first to be impacted. These roles have had high churn and sufficient training and experience quality monitoring may have suffered during this period. That could be the reason why the certain steps in the process involving recruiters (like communicating back to the rejected candidates) may have suffered w.r.t quality of interactions. reply davedx 15 hours agoprevJeez. On one hand, sure, it’s frustrating when this happens. But for everyone trying to get hired out there: don’t just sit passively waiting for results - call your damn recruiter on the phone! Recruiters are human and yes they make mistakes. Sometimes a phone call may actually result in jiggling you along in the process and actually make the difference between getting hired or not.Don’t sit at home and feel sad and stroke your beard and neglect your side projects. Communicate, sell yourself, don’t be afraid to be a bit pushy! reply zero-sharp 16 hours agoprevHere&#x27;s a relevant experience. I applied for a job a few days ago. The recruiter reached out to me through email to schedule a phone call and I scheduled a time through her online calendar. To give you some more context in terms of timing: she emailed me in the morning and I responded in the afternoon. Less than 24 hours after she reached out to me, the job ad on the company page is taken down and now says the position is filled. No response from the recruiter.What the hell was that? reply firebat45 13 hours agoparent>she emailed me in the morning and I responded in the afternoonReal question: How long should she sit at her desk and twiddle her thumbs, waiting for your reply, before she moves on to emailing&#x2F;phoning&#x2F;interviewing a different candidate?Or perhaps she should pursue multiple candidates at the same time, to immensely speed up the process and reduce her time wasted just waiting to hear back from potential candidates who may never respond back? reply zero-sharp 12 hours agorootparent>Real question: How long should she sit at her desk and twiddle her thumbs, waiting for your reply, before she moves on to emailing&#x2F;phoning&#x2F;interviewing a different candidate? Or perhaps she should pursue multiple candidates at the same time, to immensely speed up the process and reduce her time wasted just waiting to hear back from potential candidates who may never respond back?Huh? Of course she should pursue multiple candidates at the same time. I can hold that belief and simultaneously express frustration with the above story. Do you think this is some sort of dichotomy? reply TheChaplain 15 hours agoparentprevMy guess, you were not fast enough? Imagining myself as a recruiter, I would probably be working with a fair number of leads at the same time. Any of those who are a good fit and quick to respond would get sent to HR.My recent job I landed because I was pushy, I called the recruiter by phone and sold them on me, got the contact for the company manager, called them too and got hired. I got decent technical skills, but talking to people is what gave results.Just one thing, \"talk the talk, walk the walk\" is real. Bullsh*ting will put you in a bind sooner or later. reply zero-sharp 15 hours agorootparentI&#x27;m not sure what part of the story I just described was \"bullshitting.\" reply TheChaplain 15 hours agorootparentI didn&#x27;t mean you were, I just said that talking to people is effective but some take it up a level where it likely ends badly. reply RobRivera 15 hours agorootparentprevThey were making a comment on the general case, not your specific case reply optymizer 15 hours agoparentprevHM hired someone they knew or recruiter was just sourcing candidates and didn&#x27;t have visibility into the pipeline. reply pcai 15 hours agoparentprevsorry that happened to you. most likely they had a candidate deep in the pipeline with an offer out, and they were hedging their bets in case they declined. this is surprisingly common, though its shitty behavior for the recruiter to ghost you reply JohnFen 16 hours agoprevIs it new, though?I&#x27;ve been in the industry for a very long time, and being ghosted has always been my standard experience when I didn&#x27;t get a position I interviewed for. reply senderista 15 hours agoprevThis has happened to me multiple times, including from large YC-funded companies. I&#x27;m tempted to name-and-shame, but I doubt it would do any good.I don&#x27;t consider anyone obligated to respond to a resume or application, but stopping communication in the middle of the interview process is utterly disrespectful and does not speak well of your company&#x27;s values and culture. reply firtoz 16 hours agoprevI got ghosted twice by the same guy working for Citi, for the initial chat. Needless to say, I or my friends won&#x27;t bother applying for any jobs there. reply billy_bitchtits 14 hours agoprevMy company ghosted me for an internal advancement opportunity. A short interview with the HR recruiter then silence for 2 months, even from my manager. I had to complain to the head of HR to get a response letting me know that they&#x27;re going with an external hire. reply teunispeters 11 hours agoprevCould be worse, interviewers could be bullying and degrading interviewees they don&#x27;t like ... on a wider level.sigh 2 really good interviews, and then that. Work in my field, with tech I&#x27;d had decades on, and that. Worse experience of the last year of unemployment, and worst interview ever. I had a rough time with google interviews because I don&#x27;t have a PHD (or degree of any kind) but nothing on that level, ever, before. reply satokema 11 hours agoprevThere is no excuse for not sending a no.Recruiters this day are using one of a dozen or so recruiting platforms, all of which either have or ought to have functionality that tracks communications.\"Slipping through the cracks\" isn&#x27;t the issue. The issue is a lack of professionalism and diligence, which aren&#x27;t things I want to see from someone that is going to filter out future employees. reply wutangisforever 17 hours agoprevAs someone who interviewed in the bay for the last 7 years, ghosting is pretty normal on both sides of the equation.It totally sucks when you go through the process and don&#x27;t hear anything back but I also take it as a sign of feedback that I didn&#x27;t kill on the interview.I have ghosted a ton on interviews, never purposely but things get lost in the shuffle if you aren&#x27;t super passionate about the companyIt isn&#x27;t a great habit&#x2F;practice on either side, but by any means this isn&#x27;t new reply ugh123 16 hours agoprevMy first rule of recruiting for my own technical teams: don&#x27;t let a recruiter be the face of your company or team. Set rules on interaction, follow-ups, etc. Insert yourself or someone directly from the team into the process as early as time allows.Also, don&#x27;t let a recruiter contact candidates on your behalf. I&#x27;ve seen this go sideways several times where unprofessionalism can be conducted IN YOUR NAME by the recruiter. reply momocowcow 15 hours agoprevI’ve been ghosted a few times over the years. It stung at first but now that I have grey hair, I no longer take it personally. I actually started ghosting job offers and invitation to continue to the next phase of the interview process. If they’re really interested, they’ll find an alternate way to contact me. reply optymizer 15 hours agoparentI sincerey hope nobody reads this and thinks \"I should start ghosting too\". We want less ghosting in this process, not more. reply sevagh 15 hours agorootparentOperating with integrity in a domain where nobody else is is a shortcut to getting fucked. reply dspillett 15 hours agorootparentResponding with “Yeah, nah.” or a more formal equivalent is hardly going out of your way to operate with integrity in a manner that will get you massively fucked.Yes, ghost those who don&#x27;t take “sorry, but no” (or more succinctly just “no”) for an answer, especially those who have done so repeatedly (those are a time sink and deserve ghosting), but I wouldn&#x27;t take it as my default position. Try not to become what you hate.OK, so I tell a bit of a lie: ignorance is my default and only response wrt LinkedIn these days. I see the “you have messages&#x2F;requests &#x2F; people are looking at you” alerts regularly by mail, but I&#x27;ve not logged into the site in half a decade or more which anyone looking at my profile (rather than just being a gattling-gun invite spammer or similar) could probably tell with ease (the fact they bother contacting me is a good indication that they aren&#x27;t a useful lead!). reply operatingthetan 15 hours agorootparentprevInterpersonal enshittification&#x27;s cause in a nutshell. reply pcai 15 hours agoprevone seldom-mentioned reason employers ghost candidates deep in the process SEEMS silly, but is selfishly rational: they are hedging their bets. you came in second place, and they are waiting to see if their preferred candidate accepts before getting back to you. reply cryptodan 16 hours agoprevIt&#x27;s all sector&#x27;s in the industry not just isolated to tech jobs. reply tennisflyi 9 hours agoprevBeen this was for everyone outside of tech. Welcome and enjoy the purgatory reply ponderings 14 hours agoprevHere is a truly stupid idea: What if you could buy a job interview. You pay say 100 bucks, you get the deluxe tour, all the attention and dedication you deserve, a nice letter highlighting your qualities. Make the process into a real product that is worth buying.I&#x27;ve never experienced it and I don&#x27;t know if they still do it but I hear in Belgium they do a lot of things over dinner. If nothing useful comes out of it you split the bill. reply jzebedee 14 hours agoparentExactly what an already exploitative labor market needs: paid interviews!I can&#x27;t see a single way this could go wrong. reply firebat45 13 hours agoparentprevWell, at least you got the first part right. That is a truly stupid idea. reply Brian_K_White 16 hours agoprevI don&#x27;t understand why companies don&#x27;t understand: \"If you don&#x27;t care about me, why should I care about you?\" reply hartator 14 hours agoprevLiterally give the middle finder emoji to the interviewer and wonder why they don&#x27;t want to hire him. reply zitterbewegung 16 hours agoprevThis is anecdotal but, if the new normal was 10 years ago then sure. I stared out my career then and ghosting was common…. reply dr_kiszonka 16 hours agoprevI know ghosting is \"normal,\" but what do you do when it means you are not getting reimbursed for travel? reply fragmede 16 hours agoparentThrowaway AskReddit&#x2F;AskHN thread for \"advice\" followed by small claims court. reply tayo42 16 hours agoprevSo glad I wrapped up my 6mo job search lol. It was an extremely dehumanizing process. I think Ive mostly repressed my feelings since it ended, part of me wishes I wrote about it.These companies acting like everything they do is so important and urgent that they cant be bothered to give real feedback or tell me no. I suspect part of the lack of real feedback is that most people have no clue how to interview, even these large companies.Companies should run their own employees through the hiring process or something. This would definitely show how bad almost all tech recruiters are along with their interview process in general.I ended my search with a couple companies I don&#x27;t want to work for now. Meaningless I guess, I&#x27;m sure they still have an endless stream of candidates and these large companies past a certain point will never actually die. They make worse decisions then being rude to applicants daily and have no real repercussion. reply tekla 17 hours agoprevGetting ghosted 95% of the time has been the default for decades. reply prettyStandard 16 hours agoparentAgreed. A bit besides the point, but I&#x27;m dating a recruiter that has worked in a few industries, but not tech. She says, as a rule, she doesn&#x27;t provide feedback. People just don&#x27;t listen or accept it, they just use it as an opportunity to argue. So what&#x27;s the point? reply fragmede 16 hours agorootparentDetailed \"here&#x27;s why you suck\" feedback, sure, but just a \"no\" would be more professional than this emotionally immature \"ghosting\" thing. reply SoftTalker 15 hours agorootparentThis isn&#x27;t personal on their part. It might be a bit rude but if you&#x27;re going into the corporate world, get used to that. Just move on, if you&#x27;re sitting there stewing about getting \"ghosted\", you can be sure they are not. I don&#x27;t think it would bother me any more than getting a templated, cliche rejection letter. reply fragmede 13 hours agorootparentIt&#x27;s less about the feelings going around, and more that the hiring committee managed to meet, decided against a candidate, and then the recruiter can&#x27;t be bothered to give a candidate a canned \"no\"? What does a recruiter do? Doom scroll on Linkedin for $70k&#x2F;yr? reply SoftTalker 6 hours agorootparentYeah the recruiter could spend some time preparing and sending communications back to all the candidates who were interviewed and then rejected, or could spend that time working on finding candidates for other positions he&#x27;s trying to fill. What is he going to be incentivized to do? replybachmeier 16 hours agoparentprevI had my first interview for a faculty job in 2001. I sent a couple of emails to find out where they are in the hiring process. Still waiting for a response. reply thaumasiotes 14 hours agorootparentMaybe it&#x27;s time to send one more email? ;D reply thr0way120 17 hours agoprevThe ghosting problem is fixable in 2 seconds with any LLM + AI recruiting pipelining tool. That is the solution to this. Someone will figure it out in the next three months or less.On the other hand, here is the reality:In a \"hiring company friendly\" environment, where they cut all their expensive recruiters &#x2F; or all recruiters &#x2F; or simply don&#x27;t care &#x2F; or treat recruiters as disposable this is what you get.We are seeing a rising trend, which may reverse in a \"Good Market\" but part of me wonders if it ever will.White Collar workers are becoming &#x2F; have become more and more \"disposable.\"As disposable as recruiters.White Collar workers are not used to being disposable, we think we are unique and special butterfly hires. And much of silicon valley used to be structured around the messaging: \"Your talent is so useful and valuable that we can&#x27;t live without you.\"That pretense was never really true but they sort of put on an act to keep things friendly.Lately, that pretense is completely gone. And dropping.At some point employers, in my opinion, are going to find hires speaking out publicly.And naming names directly.Why? No consequences and the employers have nothing, as a class, to offer.I got to this point in my own professional career. I was treated so badly at one company, I saw no point in not directly naming and shaming them. I didn&#x27;t even care if \"their friends\" didn&#x27;t want to hire me. I didnt want to work with anyone who would be friends with people that evil.Here is the deal silicon valley wants:\"We treat you like shit and you are expected to take it as a normal part of \"professionalism,\" or we will black ball you and you will be deemed unhireable because you are unwilling to take being treated like shit gracefully, and we (employers) need employees who we can shit on and dispose of. If you complain after this treatment, you are a liability since you think you are worth literally anything as a human being ... when we require disposable parts.\"Even THAT contract, which is a VERY BAD AND ONE SIDED DEAL is fraying.I expect that you are going to see more and more people speaking publicly and directly naming these companies.Once THAT happens then you KNOW ITS ON.I suspect venture capital portfolios are going to need to directly tell CEOs not to do this because they are \"angering the sheep.\"Shitting on applicants, if it continues to escalate, will become a net liability.Venture Capital companies and Venture Capital firms want to cut corners, access cheap talent and avoid treating employees like they are human beings. They will push this as far as possible until it becomes a net liability.I think we are going to see people getting so fed up they begin naming names, and THEN it will change. reply gchamonlive 16 hours agoparentMaybe everyone engaged in this perverse machine is to blame? I mean not the individuals. We all have to eat. But the category as a whole.Maybe if we would just avoid this SV hellish circle and find places or modes of work that are in direct opposition to that, we can at least reclaim our sanity back.I assume people accept these shitty conditions because this is where the money is at? So maybe we won&#x27;t get rich avoiding these working environments, but live comfortable enough? We might even have enough energy and time to devote to side projects, our community and such.Maybe the obscenely fat paycheck isn&#x27;t the only way to go. reply operatingthetan 15 hours agoparentprevWhy do you need an LLM for that? This functionality already exists in tools recruiters use. If a candidate is crossed off the list, the system sends them a pre-written rejection email. Simple. reply cratermoon 16 hours agoparentprevA few months ago I started keeping a list of times someone said \"AI will fix it\" mentions. You made the list. reply Beefin 17 hours agoprev [–] as a founder recruiting engineers of various skillsets&#x2F;expertise i simply don&#x27;t have the bandwidth to respond to every non-qualified candidate. reply georgespencer 16 hours agoparentThis logic doesn&#x27;t really scan for me. If you care about optimizing your time enough to not respond to the deluge of unqualified candidates you receive, then you might benefit from an ATS, which will make it trivial to send a polite rejection to multiple candidates with a single click, whilst also enabling you to progress others more efficiently.You don&#x27;t have to use Lever (just noped out of using them at our company bc they don&#x27;t put any pricing data on their website + refused to answer simple questions over email, insisting on a \"sales call\", which is an even clearer indicator of a company which doesn&#x27;t value my time than one which does not reply to an application altogether) – there are plugins like Streak which can make this effortless.If you are experiencing an influx of unqualified candidates, you might also consider making your requirements much more explicit, although you&#x27;ll still get people trying their luck. reply ivan_gammel 17 hours agoparentprevDo you ghost people whom you already talked to in the past or you misunderstood the article? Not responding after an interview cannot be a bandwidth problem. reply drewcoo 16 hours agorootparentI think at least half of the responses here just don&#x27;t understand what ghosting is. Which means they&#x27;ve probably never been ghosted - good for them! reply rrherr 17 hours agoparentprevWhat do you mean by candidate? Applicant, or interviewee?OP was writing about interviewees, not applicants:“In order for it to be ghosting, the ghosted party has to expect the conversation will continue. This means that if you apply and never hear back from a job, that’s not ghosting, a conversation never started. It only becomes ghosting when there is an expected next step that never happens.” reply ErikAugust 17 hours agoparentprevNot responding to every resume received isn’t ghosting by the article’s definition. But going through an interview process including a take home assignment and getting no response is. reply dvaun 17 hours agoparentprevFrom what I’ve read, hiring was already difficult prior to the wave of layoffs. Would you say it became more or less difficult to find a target skillset with a larger pool of candidates, regardless of comp? reply shkkmo 17 hours agoparentprev> This means that if you apply and never hear back from a job, that’s not ghosting, a conversation never started. It only becomes ghosting when there is an expected next step that never happens.The article is specifically talking about ghosting people after a mutual process has been started, such as after an interview or coding assignment. reply boltzmann-brain 15 hours agoparentprevonce you&#x27;ve identified an applicant is not suited, it takes zero time to to copy-paste thanksbutnothanksgoodluckinyourjobsearch.txt and hit send. don&#x27;t be a dbag. reply greatpostman 17 hours agoparentprev [–] Yup. It’s too much work, literally. Especially when 20% of the resumes you get end up being fake reply mattkrause 15 hours agorootparentReally?While I’d love personalized feedback, all I really want is for your Applicant Tracking System to fire off a short email from noreply@company.com, bcc’ing everyone other than the person getting an offer. Subject: Your Application to $company I regret to inform you that $position at $company has been filled by another candidate. We appreciate your interest in working with us. [Add other platitudes, info about reapplying or whether you’ll be considered for other jobs here…if you want]. —- $Name or HR departmentIf you’re too pressed for time to do that…maybe you should be hiring someone who can. reply georgespencer 16 hours agorootparentprev [–] If you have enough candidates that it&#x27;s too much work to reject people, you likely have enough candidates to merit at least a simple templating and candidate management app or plugin. It is absolutely trivial to one-click reject groups of people. reply greatpostman 16 hours agorootparent [–] 95% of candidates aren’t relevant or qualified reply kcbanner 16 hours agorootparent [–] Did you read the comment you replied to? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ghosting, the act of abruptly cutting off communication with candidates, is a prevalent issue in tech recruiting.",
      "The author shares their personal experience of being ghosted by a company and the impact it had on them.",
      "Lack of incentives and enforcement of a nonperformative policy contribute to the prevalence of ghosting in the tech industry.",
      "The author argues for the importance of closing the loop with rejected candidates and treating them with dignity and respect.",
      "Recruiters should be more aware and ensure that communication is maintained with candidates."
    ],
    "commentSummary": [
      "Ghosting, the practice of abruptly cutting off communication with job candidates, is a prevalent issue in the tech industry's hiring process.",
      "The discussion emphasizes the importance of treating candidates with respect and prioritizing the recruiting process.",
      "Challenges in hiring and talent identification are acknowledged, as well as the lack of recognition and compensation for interviewers. Timely and respectful communication during the recruitment process is deemed essential.",
      "The negative impact of ghosting on job applicants is discussed, including the decline in providing feedback to candidates.",
      "Companies that engage in ghosting may face potential consequences."
    ],
    "points": 144,
    "commentCount": 182,
    "retryCount": 0,
    "time": 1703866314
  }
]
